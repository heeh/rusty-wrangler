{"text": "We consider that twomethods converged to the same solution if the optimal costs f 1 and f 2 satisfythe following conditions .The problems which were discarded by this process were BROYDN7D , CHAINWOO , NONCVXUN and SENSORS .Figures 1 - 4 show the performance of the above four methods relative to the CPU time ( in second ) , the total number of iterations , the total number of function evaluations , the total number of gradient evaluations , respectively .", "label": "", "metadata": {}, "score": "43.189262"}
{"text": "In Table 2 , \" - \" means the method failed and \" NaN \" means that the cost function generated a \" NaN \" for the function value .We used the profiles by Dolan and Mor\u00e9 [ 24 ] to compare the performance of those methods .", "label": "", "metadata": {}, "score": "45.615128"}
{"text": "The algorithm keeps track of a set of currently active constraints , and ignores them when computing the minimum allowable step size .( The x 's associated with the active constraint are kept fixed . )If the maximum allowable step size is zero then a new constraint is added .", "label": "", "metadata": {}, "score": "45.921997"}
{"text": "In the later part of this section , we will devote to the global convergence ofthe Algorithm 2.1 under a modified strong Wolfe line search which determine the steplength \u03b1 k satisfying the following conditions .At first , we give the following useful lemma whichwas essentially proved by Zoutendijik [ 19 ] and Wolfe [ 20 , 21].", "label": "", "metadata": {}, "score": "46.41523"}
{"text": "- fc : int - Number of function evaluations made .- gc : int - Number of gradient evaluations made .- - Notes - ----- - Uses the line search algorithm to enforce strong Wolfe - conditions .See Wright and Nocedal , ' Numerical Optimization ' , - 1999 , pg .", "label": "", "metadata": {}, "score": "48.03058"}
{"text": "We determine the steplength \u03b1 k which satisfies the conditions in ( 3.3 ) or the following approximate conditions : .The first inequality in ( 5.1 ) is obtained by replacing \u03d5 ( \u03b1 ) with its approximation function .in the modified strong Wolfe conditions ( 3.3 ) .", "label": "", "metadata": {}, "score": "48.151566"}
{"text": "Maximum step for the line search .May be increased during call .If too small , it will be set to 10.0 .Defaults to 0 .accuracy : float .Relative precision for finite difference calculations .Defaults to 0 .", "label": "", "metadata": {}, "score": "49.04917"}
{"text": "The specific constraint removed is the one associated with the variable of largest index whose constraint is no longer active .Wright S. , Nocedal J. ( 2006 ) , ' Numerical Optimization ' .Nash S.G. ( 1984 ) , \" Newton - Type Minimization Via the Lanczos Method \" , SIAM Journal of Numerical Analysis 21 , pp .", "label": "", "metadata": {}, "score": "49.759895"}
{"text": "_ _ doc _ _ .Optimize the function , f , whose gradient is given by fprime using the .quasi - Newton method of Broyden , Fletcher , Goldfarb , and Shanno ( BFGS ) .See Wright , and Nocedal ' Numerical Optimization ' , 1999 , pg .", "label": "", "metadata": {}, "score": "49.881935"}
{"text": "( The x 's associated with the active constraint are kept fixed . )If the maximum allowable step size is zero then a new constraint is added .At the end of each iteration one of the constraints may be deemed no longer active and removed .", "label": "", "metadata": {}, "score": "50.26278"}
{"text": "However , Powell constructed an example which showed that the PRP method with exact line searches can cycle infinitely without approaching a solution point [ 8].In [ 10 ] , Zhang , Zhou and Li proposed a three term PRP method .", "label": "", "metadata": {}, "score": "50.39126"}
{"text": "Appl .Math .vol.31 no.1 S\u00e3o Carlos 2012 .Recently , Yu and Guan proposed a modified PRP method ( called DPRP method ) which can generate sufficient descent directions for the objective function .They established the global convergence of the DPRP method based on the assumption that stepsize is bounded away from zero .", "label": "", "metadata": {}, "score": "52.73495"}
{"text": "Owing to the large size of our problem , we compute approximate reduced Newton steps by using the conjugate gradient ( CG ) iteration .We introduce a limited - memory , quasi - Newton preconditioner that speeds up CG convergence .", "label": "", "metadata": {}, "score": "52.827415"}
{"text": "W. Feller , An Introduction to Probability Theory and Its Applications ( Wiley , New York , 1971 ) .J. Nocedal , J. L. Morales , \" Automatic preconditioning by limited memory quasi - Newton updating , \" SIAM J. Optim . [", "label": "", "metadata": {}, "score": "52.962475"}
{"text": "[Links ] .[14 ] M. Al - Baali , Descent property and global convergence of the Fletcher - Reeves method with inexact line search .IMA J. Numer .Anal . , 5 ( 1985 ) , 121 - 124 .", "label": "", "metadata": {}, "score": "53.06024"}
{"text": "Conjugate gradient methods : Fletcher - Reeves ; Polak - Ribiere ; Polak - Ribiere + ; and Hestenes - Stiefel .Miscellaneous : Grid search ; Simplex ; and Levenberg - Marquardt .The step selection subalgorithms include : Backtracking line search ; Nocedal and Wright interpolation based line search ; Nocedal and Wright line search for the Wolfe conditions ; More and Thuente line search ; and no line search .", "label": "", "metadata": {}, "score": "53.064842"}
{"text": "Step 4 .Step 5 .It is easy to see that Algorithm 2.1 is well - defined .We analyse the convergence in the next sections .We do not specify the line search to determine the steplength \u03b1 k .", "label": "", "metadata": {}, "score": "53.4861"}
{"text": "If too small , it will be set to 10.0 .Defaults to 0 .accuracy : float .Relative precision for finite difference calculations .Defaults to 0 .fmin : float .Precision goal for the value of f in the stoping criterion .", "label": "", "metadata": {}, "score": "54.349777"}
{"text": "@@ -1708,7 +1708,7 @@ If True , return a list of the solution at each iteration .ranges : tuple Each element is a tuple of parameters or a slice object to @@ -1839,7 +1839,7 @@ full_output : bool If True , return the evaluation grid . -", "label": "", "metadata": {}, "score": "54.652954"}
{"text": "In this paper we simply call it DPRP method .This modified formula is similar to the well - known CG_DESCENT method which was proposed by Hager and Zhang in [ 13].Yu and Guan has proved that the directions d k generated by the DPRP method can always satisfied the following sufficient descent condition .", "label": "", "metadata": {}, "score": "55.04158"}
{"text": "From the Theorem 1 in [ 11 ] we can obtain the following useful lemma directly .Proof .So , the directions generated by ( 2.2 ) and ( 2.3 ) are descent directions due to Lemma 2.1 .The update rule ( 2.3 ) to adjust the the lower bound on was originated in [ 13].", "label": "", "metadata": {}, "score": "55.93679"}
{"text": "Math . , 104 ( 2006 ) , 561 - 572 .[Links ] .[28 ] W.W. Hager and H. Zhang , Algorithm 851 : CG_DESCENT , A conjugate gradient method with guaranteed descent .ACM T. Math .", "label": "", "metadata": {}, "score": "56.05886"}
{"text": "The left side of the figure gives the percentage of the test problems for which a method is the fastest ; the right side gives the percentage of the test problems that were successfully solved by each of the methods .The top curve is the method that solved the most problems in a time that was within a factor \u03c4 of the best time .", "label": "", "metadata": {}, "score": "56.09111"}
{"text": "[CrossRef ] .J. M. Bardsley , C. R. Vogel , \" A nonnnegatively constrained convex programming method for image reconstruction , \" SIAM J. Sci .Comput .( USA ) 25(4 ) , ( 2003 ) .J. J. Mor\u00e9 , G. Toraldo , \" On the solution of large quadratic programming problems with bound constraints , \" SIAM J. Optim . [", "label": "", "metadata": {}, "score": "56.19247"}
{"text": "J. Nagy , Z. Strakos , \" Enforcing nonnegativity in image reconstruction algorithms , \" in Mathematical Modeling , Estimation , and Imaging , D. C. Wilson , H. D. Tagare , F. L. Bookstein , F. J. Preteux , E. R. Dougherty , eds . , Proc .", "label": "", "metadata": {}, "score": "57.016983"}
{"text": "Moreover , we establish the global convergence of the DPRP method with a Armijo - type line search .The numerical results show that the proposed algorithms are efficient .Mathematical subject classification : Primary : 90C30 ; Secondary : 65K05 .", "label": "", "metadata": {}, "score": "57.069122"}
{"text": "New York ( 1987 ) .[Links ] .[ 6 ] Y. Liu and C. Storey , Efficient generalized conjugate gradient algorithms , Part 1 : Theory .J. Optim .Theory Appl . , 69 ( 1991 ) , 177 - 182 .", "label": "", "metadata": {}, "score": "58.40847"}
{"text": "CrossRef ] .Morales , J. L. .J. Nocedal , J. L. Morales , \" Automatic preconditioning by limited memory quasi - Newton updating , \" SIAM J. Optim . [CrossRef ] .Mor\u00e9 , J. J. .J. J. Mor\u00e9 , G. Toraldo , \" On the solution of large quadratic programming problems with bound constraints , \" SIAM J. Optim . [", "label": "", "metadata": {}, "score": "58.4842"}
{"text": "It wraps a C implementation of the algorithm .The algorithm incoporates the bound constraints by determining the descent direction as in an unconstrained truncated Newton , but never taking a step - size large enough to leave the space of feasible x 's .", "label": "", "metadata": {}, "score": "58.68852"}
{"text": "C. T. Kelley , Iterative Methods for Optimization ( Society for Industrial and Applied Mathematics , Philadelphia , Pa. , 1999 ) .Lu , P. .R. H. Byrd , P. Lu , J. Nocedal , \" A limited memory algorithm for bound constrained optimization , \" SIAM J. Sci .", "label": "", "metadata": {}, "score": "58.70703"}
{"text": "SPIE4121 , 182 - 190 ( 2000 ) .[CrossRef ] .C. T. Kelley , Iterative Methods for Optimization ( Society for Industrial and Applied Mathematics , Philadelphia , Pa. , 1999 ) .J. Nocedal , S. J. Wright , Numerical Optimization ( Springer - Verlag , New York , 1999 ) .", "label": "", "metadata": {}, "score": "58.721794"}
{"text": "Links ] .[ 10 ] L. Zhang , W. Zhou and D. Li A descent modified Polak - Ribi\u00e8re - Polyak conjugate gradient method and its global convergence .IMA J. Numer .Anal . , 26 ( 2006 ) , 629 - 640 .", "label": "", "metadata": {}, "score": "58.80378"}
{"text": "This refactoring makes the logic in the functions slightly clearer , and makes reusing them in large - scale non - linear equation solvers ( which only need 1-d searches ) more straightforward .New tests for line searches are also added .", "label": "", "metadata": {}, "score": "59.35447"}
{"text": "CrossRef ] .J. Nocedal , S. J. Wright , Numerical Optimization ( Springer - Verlag , New York , 1999 ) .Snyder , D. L. .Strakos , Z. .J. Nagy , Z. Strakos , \" Enforcing nonnegativity in image reconstruction algorithms , \" in Mathematical Modeling , Estimation , and Imaging , D. C. Wilson , H. D. Tagare , F. L. Bookstein , F. J. Preteux , E. R. Dougherty , eds . , Proc .", "label": "", "metadata": {}, "score": "59.39074"}
{"text": "We have the following Lemma which was essentially proved by Gilbert and Nocedal in [ 9].Lemma 3.5 .Let x k and d k be generated by Algorithm 2.1 with the modified strong Wolfe line search .The proof of Lemma 3.5 is similar to that of the Lemma 4.2 in [ 9 ] , so we omit here .", "label": "", "metadata": {}, "score": "59.39135"}
{"text": "However , there are some problems with line_search docstring : .Find alpha that satisfies strong Wolfe conditions .Uses the line search algorithm to enforce strong Wolfe conditions .Wright and Nocedal , ' Numerical Optimization ' , 1999 , pg .", "label": "", "metadata": {}, "score": "59.474865"}
{"text": "+ + For the zoom phase it uses an algorithm by [ ... ] .Perhaps the increment has slipped below + # machine precision ?+ + Uses the interpolation algorithm ( Armijo backtracking ) as suggested by + Wright and Nocedal in ' Numerical Optimization ' , 1999 , pg .", "label": "", "metadata": {}, "score": "59.499374"}
{"text": "Minfx is complete , very stable , well tested , and has been spun off as its own project for the benefit of other scientific , analytical , or numerical projects .Current optimisation algorithms .Line search methods : Steepest descent ; Back - and - forth coordinate descent ; Quasi - Newton BFGS ; Newton ; and Newton - CG .", "label": "", "metadata": {}, "score": "59.517925"}
{"text": "Links ] .[11 ] G.H. Yu and L.T. Guan , Modified PRP Methods with sufficient desent propertyand their convergence properties .Acta Scientiarum Naturalium Universitatis Sunyatseni(Chinese ) , 45 ( 4 ) ( 2006 ) , 11 - 14 .", "label": "", "metadata": {}, "score": "60.020157"}
{"text": "[CrossRef ] .Nocedal , J. .J. Nocedal , J. L. Morales , \" Automatic preconditioning by limited memory quasi - Newton updating , \" SIAM J. Optim . [CrossRef ] .C. Zhu , R. H. Byrd , J. Nocedal , \" L - BFGS - B : FORTRAN subroutines for large scale bound constrained optimization , \" ACM Trans .", "label": "", "metadata": {}, "score": "60.066135"}
{"text": "Outputs : ( alpha0 , gc , fc ) .So , as you see , params are not described ; especially I 'm interested in old_fval and old_old_fval .So , there was a letter from NLPy developer about an alternative ( link to an article was provided ) , also , Matthieu proposed using one of his own solvers .", "label": "", "metadata": {}, "score": "60.461018"}
{"text": "If ' fhess ' is provided , then ' fhess_p ' will be ignored .@@ -1154,7 +1154,7 @@ 3 : print iteration results .( See ' brent ' @@ -1429,7 +1429,7 @@ return the minimum of the function isolated to a fractional precision of tol .", "label": "", "metadata": {}, "score": "60.4732"}
{"text": "CrossRef ] .J. J. Mor\u00e9 , G. Toraldo , \" On the solution of large quadratic programming problems with bound constraints , \" SIAM J. Optim . [CrossRef ] .Vogel , C. R. .J. M. Bardsley , C. R. Vogel , \" A nonnnegatively constrained convex programming method for image reconstruction , \" SIAM J. Sci .", "label": "", "metadata": {}, "score": "60.58847"}
{"text": "16 ] Y.H. Dai and Y. Yuan , Convergence Properties of the Conjugate Descent Method .Advances in Mathematics , 25 ( 6 ) ( 1996 ) , 552 - 562 .[Links ] .[17 ] L. Zhang , New versions of the Hestenes - Stiefel nolinear conjugate gradient method based on the secant condition for optimization .", "label": "", "metadata": {}, "score": "60.590027"}
{"text": "Nagy , J. .J. Nagy , Z. Strakos , \" Enforcing nonnegativity in image reconstruction algorithms , \" in Mathematical Modeling , Estimation , and Imaging , D. C. Wilson , H. D. Tagare , F. L. Bookstein , F. J. Preteux , E. R. Dougherty , eds . , Proc .", "label": "", "metadata": {}, "score": "60.694534"}
{"text": "Oper .Res . , 3 ( 1978 ) , 244 - 256 .[Links ] .[ 27 ] L. Zhang , W. Zhou and D. Li , Global convergence of a modified Fletcher - Reeves conjugate gradient method with Armijo - type line search .", "label": "", "metadata": {}, "score": "61.02928"}
{"text": "Links ] .[ 4 ] B.T. Polyak , The conjugate gradient method in extreme problems .USSR Comp .Math .Math .Phys . , 9 ( 1969 ) , 94 - 112 .[Links ] .[5 ] R. Flectcher Practical Method of Optimization I : Unconstrained Optimization .", "label": "", "metadata": {}, "score": "61.192703"}
{"text": "In the next two sections , we devote to the convergence of the Algorithm 2.1 with modified strong Wolfe line search and Armijo - type line search .3 Convergence of Algorithm 2.1 under modified strong Wolfe line search .In this section , we prove the global convergence of the Algorithm 2.1 .", "label": "", "metadata": {}, "score": "61.3135"}
{"text": "We have established the convergence of a sufficient descent PRP conjugate gradient method with a modified strong Wolfe line search method and an Armijo - type line search .It was shown from the numerical results that Algorithm 2.1 was efficient for solving the unconstrained problems in CUTEr library , and the numerical performance was similar to that of the wellknown CG_DESCENT and MPRP methods .", "label": "", "metadata": {}, "score": "61.538853"}
{"text": "But it can wait , it has one quite good and problem with lincher is more actual .So I think the next GSoC schedule step should be connection of 1 - 2 Matthieu 's solvers ( that take into account slope angle , like strong Wolfe conditions do ) to native openopt syntax .", "label": "", "metadata": {}, "score": "61.60826"}
{"text": "Nash S.G. ( 1984 ) , \" Newton - Type Minimization Via the Lanczos Method \" , SIAM Journal of Numerical Analysis 21 , pp .770 - 778 Description .This is the initial release of minfx .The minfx project is a Python module library for numerical optimisation , being a large collection of minimisation algorithms .", "label": "", "metadata": {}, "score": "61.832893"}
{"text": "@@ -1477,7 +1477,7 @@ return the minimum of the function isolated to a fractional precision of tol .@@ -1494,7 +1494,7 @@ full_output : bool If True , return optional outputs .@@ -1570,7 +1570,7 @@ maxiter : int Maximum number of iterations to perform .", "label": "", "metadata": {}, "score": "62.490128"}
{"text": "[Links ] .[ 18 ] D. Li and Fukushima , A modified BFGS method and its global convergence in nonconvex minimization .J. Comput .Appl .Math . , 129 ( 2001 ) , 15 - 35 .[", "label": "", "metadata": {}, "score": "62.55612"}
{"text": "First of all I 'm interested . 1 ) what solvers does it contain ? 2 ) how xtol , funtol , contol etc can be passed to the solvers ? then , secondary ( it can wait , maybe default parameters would be enough for now ) .", "label": "", "metadata": {}, "score": "62.64476"}
{"text": "[Links ] . # CAM-307/10 .Navigation . scipy.optimize .Minimize a function with variables subject to bounds , using gradient information in a truncated Newton algorithm .This method wraps a C implementation of the algorithm .Maximum step for the line search .", "label": "", "metadata": {}, "score": "62.875664"}
{"text": "A constraint is considered no longer active is if it is currently active but the gradient for that variable points inward from the constraint .The specific constraint removed is the one associated with the variable of largest index whose constraint is no longer active .", "label": "", "metadata": {}, "score": "63.037483"}
{"text": "20 ] P. Wolfe , Convergence conditions for ascent methods .SIAM Rev. , 11 ( 1969 ) , 226 - 235 .[Links ] .[21 ] P. Wolfe , Convergence conditions for ascent methods .II : Some corrections .", "label": "", "metadata": {}, "score": "63.043808"}
{"text": "SIAM J. Optim .J. J. Mor\u00e9 , G. Toraldo , \" On the solution of large quadratic programming problems with bound constraints , \" SIAM J. Optim . [CrossRef ] .J. Nocedal , J. L. Morales , \" Automatic preconditioning by limited memory quasi - Newton updating , \" SIAM J. Optim . [", "label": "", "metadata": {}, "score": "63.083363"}
{"text": "- For the zoom phase it uses an algorithm by [ ... ] .Perhaps the increment has slipped below - # machine precision ? - - Uses the interpolation algorithm ( Armiijo backtracking ) as suggested by - Wright and Nocedal in ' Numerical Optimization ' , 1999 , pg .", "label": "", "metadata": {}, "score": "63.09994"}
{"text": "CrossRef ] .J. M. Bardsley , C. R. Vogel , \" A nonnnegatively constrained convex programming method for image reconstruction , \" SIAM J. Sci .Comput .( USA ) 25(4 ) , ( 2003 ) .Other ( 7 ) .", "label": "", "metadata": {}, "score": "63.123676"}
{"text": "Math .Software , 21 ( 1995 ) , 123 - 160 .[Links ] .[ 24 ] E.D. Dolan and J.J. Mor\u00e9 , Benchmarking optimization software with performance profiles .Math .Program ., 91 ( 2002 ) , 201 - 213 .", "label": "", "metadata": {}, "score": "63.198696"}
{"text": "[Links ] .[ 22 ] J.J. Mor\u00e9 and D.J. Thuente , Line search algorithms with guaranteed suficient decrease .ACM Trans .Math .Software , 20 ( 1994 ) , 286 - 307 .[Links ] .[ 23 ] I. Bongartz , A.R. Conn , N.I.M. Gould and P.L. Toint , CUTE : Constrained and unconstrained testing environments .", "label": "", "metadata": {}, "score": "63.306572"}
{"text": "We refer to [ 13 ] for more details .The Algorithm 2.1 code is a modification of the CG_DESCENT subroutine proposed by Hager and Zhang [ 13].We use the default parameter there .We can see from Figures 1 - 4 that the curves \" Algorithm 2.1 \" , \" CG_DESCENT \" and \" MPRP \" are very close .", "label": "", "metadata": {}, "score": "63.729843"}
{"text": "@@ -850,7 +850,7 @@ retall : bool return a list of results at each iteration if True .@@ -979,7 +979,7 @@ each iteration .Called as callback(xk ) , where xk is the current parameter vector .@@ -1011,7 +1011,7 @@ retall : bool If True , return a list of results at each iteration . scikits.openopt offers a unified syntax to call this and other solvers .", "label": "", "metadata": {}, "score": "63.75489"}
{"text": "Optimize the function , f , whose gradient is given by fprime using the .Newton - CG method .fhess_p must compute the hessian times an arbitrary . vector .If it is not given , finite - differences on fprime are used to . compute it .", "label": "", "metadata": {}, "score": "63.77762"}
{"text": "Its features include generating hierarchical clusters from - distance matrices , computing distance matrices from observation vectors , - calculating statistics on clusters , cutting linkages to generate flat - clusters , and visualizing clusters with dendrograms . scipy.optimize .Minimize a function with variables subject to bounds , using gradient information in a truncated Newton algorithm .", "label": "", "metadata": {}, "score": "63.871456"}
{"text": "Links ] .[ 7 ] Y.H. Dai and Y. Yuan , A nonlinear conjugate gradient method with a strong global convergence property .SIAM J. Optim . , 10 ( 2000 ) , 177 - 182 .[Links ] .", "label": "", "metadata": {}, "score": "64.000145"}
{"text": "[19 ] G. Zoutendijk , Nonlinear Programming , Computational Methods , in : Integer and Nonlinear Programming .J. Abadie ( Ed . ) , North - Holland , Amsterdam ( 1970 ) , 37 - 86 .[Links ] .", "label": "", "metadata": {}, "score": "64.06312"}
{"text": "Softw . [CrossRef ] .R. H. Byrd , P. Lu , J. Nocedal , \" A limited memory algorithm for bound constrained optimization , \" SIAM J. Sci .Comput .( USA ) 16 , 1190 - 1208 ( 1995 ) .", "label": "", "metadata": {}, "score": "64.37053"}
{"text": "J. , 7 ( 1964 ) , 149 - 154 .[Links ] .[ 3 ] B. Polak and G. Ribi\u00e8re , Note sur la convergence de m\u00e9thodes de directions conjugu\u00e9es .Rev. Fran\u00e7aise Informat .Recherche Op\u00e9rationnelle , 16 ( 1969 ) , 35 - 43 .", "label": "", "metadata": {}, "score": "64.5663"}
{"text": "- # else : print \" Using quadratic .\" - # else : print \" Using cubic .\" - xk : ndarray - Starting point . - pk : ndarray - Search direction .- args : tuple - Additional arguments passed to objective function . - c1 : float - Parameter for Armijo condition rule .", "label": "", "metadata": {}, "score": "64.68765"}
{"text": "For each : math:'$i$ ' + A rectangular distance matrix ' ' Y ' ' is returned .- XB : ndarray - An : math:'$m_B$ ' by : math:'$n$ ' array of : math:'$m_B$ ' - original observations in an : math:'$n$'-dimensional space .", "label": "", "metadata": {}, "score": "64.71239"}
{"text": "Theorem 3.6 .This also yields contradiction , then the proof is completed .All codes were written inFortran77 and ran on a PC with 2.8 GHZ CPU processor and 2 GB RAM memory and Linux ( Fedora 10 with GCC 4.3.2 ) operation system .", "label": "", "metadata": {}, "score": "65.041794"}
{"text": "+ + Notes + ----- + Uses the line search algorithm to enforce strong Wolfe + conditions .See Wright and Nocedal , ' Numerical Optimization ' , + 1999 , pg .+ + For the zoom phase it uses an algorithm by [ ... ] .", "label": "", "metadata": {}, "score": "65.11992"}
{"text": "Setting it to 0.0 is not recommended .Defaults to -1 .rescale : float .Scaling factor ( in log10 ) used to trigger f value rescaling .If 0 , rescale at each iteration .If a large value , never rescale .", "label": "", "metadata": {}, "score": "65.36438"}
{"text": "Autom .Control 21 , 174 - 184 ( 1976 ) .[CrossRef ] .C. T. Kelley , Iterative Methods for Optimization ( Society for Industrial and Applied Mathematics , Philadelphia , Pa. , 1999 ) .J. Nocedal , S. J. Wright , Numerical Optimization ( Springer - Verlag , New York , 1999 ) .", "label": "", "metadata": {}, "score": "65.80406"}
{"text": "The authors would like to thank the referees and Prof. DongHui Li for giving us many valuable suggestions and comments , which improves this paper greatly .We are very grateful to Prof. W.W. Hager , H. Zhang and Prof. J. Nocedal for providing their codes , and Zhang , Zhou and Li forthe MPRP method as well .", "label": "", "metadata": {}, "score": "65.916084"}
{"text": "Defaults to -1 .rescale : float .Scaling factor ( in log10 ) used to trigger f value rescaling .If 0 , rescale at each iteration .If a large value , never rescale .callback : callable , optional .", "label": "", "metadata": {}, "score": "66.15968"}
{"text": "[ 1 ] M.R. Hestenes and E.L. Stiefel , Methods of conjugate gradients for solving linear systems .J. Research Nat .Bur .Standards . , 49 ( 1952 ) , 409 - 436 .[Links ] .[ 2 ] R. Fletcher and C. Reeves , Function minimization by conjugate gradients .", "label": "", "metadata": {}, "score": "66.46451"}
{"text": "( USA ) 25(4 ) , ( 2003 ) .C. R. Vogel , Computational Methods for Inverse Problems ( Society for Industrial and Applied Mathematics , Philadelphia , Pa. , 2002 ) .Optim .Methods Software ( 1 ) .", "label": "", "metadata": {}, "score": "66.95556"}
{"text": "That is to say , Algorithm 2.1 is efficient for solving unconstrained optimal problems since CG_DESCENT has been regarded as one of the most efficient nonlinear conjugate gradients .We also note that the performances of Algorithm 2.1 , CG_DESCENT and MPRP methods are much better than that of the PRP+ method .", "label": "", "metadata": {}, "score": "67.05649"}
{"text": "Sample scripts should ensure that users have informative .( and documented ! ) examples to work with .( 7 days ) .So , one of the main problems for lincher ( along with extern QP solver from cvxopt ) is : a good line - search minimizer , that takes into account slope angle , is absent ( we had discussed the problem some weeks ago in mailing lists ) .", "label": "", "metadata": {}, "score": "67.118576"}
{"text": "By the end of 2010 , there were 164 unconstrained optimization problems in the CUTEr library .Moreover , five of them failed to install on our system for the limit storage space .The problems and their dimensions are listed in Table 1 .", "label": "", "metadata": {}, "score": "67.21446"}
{"text": "CrossRef ] .C. Zhu , R. H. Byrd , J. Nocedal , \" L - BFGS - B : FORTRAN subroutines for large scale bound constrained optimization , \" ACM Trans .Math .Softw . [CrossRef ] .Hammoud , A. M. .", "label": "", "metadata": {}, "score": "67.27339"}
{"text": "@@ -146,7 +146,7 @@ retall : bool Set to True to return list of solutions at each iteration .@@ -426,7 +426,7 @@ c2 : float Parameter for curvature condition rule .@@ -435,7 +435,7 @@ gc : int Number of gradient evaluations made .", "label": "", "metadata": {}, "score": "67.55675"}
{"text": "Links ] .[ 12 ] Y.H. Dai , Conjugate Gradient Methods with Armijo - type Line Searches .Acta Mathematicae Applicatae Sinica ( English Series ) , 18 ( 2002 ) , 123 - 130 .[Links ] .[ 13 ] W.W. Hager and H. Zhang , A new conjugate gradient method with guaranteed descent and an efficient line search .", "label": "", "metadata": {}, "score": "67.80561"}
{"text": "SIAM J. Sci .Comput .( USA ) ( 2 ) .R. H. Byrd , P. Lu , J. Nocedal , \" A limited memory algorithm for bound constrained optimization , \" SIAM J. Sci .Comput .( USA ) 16 , 1190 - 1208 ( 1995 ) .", "label": "", "metadata": {}, "score": "67.82109"}
{"text": "Called after each iteration , as callback(xk ) , where xk is the current parameter vector .Interface to minimization algorithms for multivariate functions .See the ' TNC ' method in particular .Notes .The underlying algorithm is truncated Newton , also called Newton Conjugate - Gradient .", "label": "", "metadata": {}, "score": "68.21196"}
{"text": "Links ] .[ 25 ] W.W. Hager and H. Zhang , A survey of nonlinear conjugate gradient methods .Pacific J. Optim . , 2 ( 2006 ) , 35 - 58 .[Links ] .[26 ] D.F. Shanno , Conjugate gradient methods with inexact searchs .", "label": "", "metadata": {}, "score": "68.26912"}
{"text": "( USA ) 16 , 1190 - 1208 ( 1995 ) .[CrossRef ] .Lucy , L. B. . L. B. Lucy , \" An iterative technique for the rectification of observed distributions , \" Astron .J. 79 , 745 - 754 ( 1974 ) .", "label": "", "metadata": {}, "score": "68.283554"}
{"text": "Springer - Verlag , Berlin ( 1984 ) .[Links ] .[ 9 ] J.C. Gilbert and J. Nocedal , Global convergence properties of conjugate gradient methods for optimization .SIAM .J. Optim . , 2 ( 1992 ) , 21 - 42 .", "label": "", "metadata": {}, "score": "68.36138"}
{"text": "+ + Parameters + ---------- + phi : callable phi(alpha ) + Function at point ' alpha ' + derphi : callable dphi(alpha ) + Derivative 'd phi(alpha)/ds ' .Returns a scalar .+ xk : ndarray + Starting point .", "label": "", "metadata": {}, "score": "68.47555"}
{"text": "CrossRef ] .R. H. Byrd , P. Lu , J. Nocedal , \" A limited memory algorithm for bound constrained optimization , \" SIAM J. Sci .Comput .( USA ) 16 , 1190 - 1208 ( 1995 ) .", "label": "", "metadata": {}, "score": "68.484764"}
{"text": "Relative solution error versus FFTs for binary star data .Horizontal axis , cumulative FFTs ; vertical axis , relative solution error on a logarithmic scale .Top plot , convergence results for 1 % noise ; bottom plot , convergence results for 10 % noise .", "label": "", "metadata": {}, "score": "68.83865"}
{"text": "@@ -648,7 +648,7 @@ iteration .Called as callback(xk ) , where xk is the current parameter vector .@@ -679,7 +679,7 @@ retall : bool Return a list of results at each iteration if True .x0 : ndarray @@ -822,7 +822,7 @@ iteration .", "label": "", "metadata": {}, "score": "68.851494"}
{"text": "Links ] .[ 15 ] Y.H. Dai and Y. Yuan , Convergence properties of the Fletcher - Reeves method .IMA J. Numer .Anal . , 16 ( 2 ) ( 1996 ) , 155 - 164 .[Links ] .", "label": "", "metadata": {}, "score": "68.857254"}
{"text": "Well - known nonlinear conjugate method include Fletcher - Reeves ( FR ) , Polak - Ribi\u00e8re - Polyak ( PRP ) , Hestenes - Stiefel ( HS ) , Conjugate - Descent ( CD ) , Liu - Story ( LS ) and Dai - Yuan ( DY )", "label": "", "metadata": {}, "score": "69.16756"}
{"text": "Precision goal for the value of f in the stoping criterion .xtol : float .Precision goal for the value of x in the stopping criterion ( after applying x scaling factors ) .Defaults to -1 .pgtol : float .", "label": "", "metadata": {}, "score": "69.752304"}
{"text": "[CrossRef ] .Toraldo , G. .J. L. Barlow , G. Toraldo , \" The effect of diagonal scaling on projected gradient methods for bound constrained quadratic programming problems , \" Optim .Methods Software 5 , 235 - 245 ( 1995 ) .", "label": "", "metadata": {}, "score": "69.919586"}
{"text": "l .s .l .D .I . f .l .Hess .J .f .l .s .l . )D .A .f .l .s .l .This is a heads up to those interested in Python - implemented optimization algorithms .", "label": "", "metadata": {}, "score": "70.69394"}
{"text": "My opinion about next thing to do is using an appropriate solver from Matthieu 's package .However , Matthieu 's syntax differs too much from openopt one .I think first of all there should be an openopt binding to Matthieu 's package , that will allow for oo users to use same syntax : .", "label": "", "metadata": {}, "score": "71.187775"}
{"text": "Methods Software 5 , 235 - 245 ( 1995 ) .[CrossRef ] .SIAM J. Control Optim . D. P. Bertsekas , \" Projected Newton methods for optimization problems with simple constraints , \" SIAM J. Control Optim . [", "label": "", "metadata": {}, "score": "71.68917"}
{"text": "We are particularly interested in the PRP method in which the parameter \u03b2 k is defined by .The PRP method has been regarded as one of the most efficient conjugate gradient methods in practical computation and studied extensively .Now let us simply introduce some results on the PRP method .", "label": "", "metadata": {}, "score": "72.02138"}
{"text": "Interface to minimization algorithms for multivariate functions .See the ' TNC ' method in particular .Notes .The underlying algorithm is truncated Newton , also called Newton Conjugate - Gradient .This method differs from scipy.optimize.fmin_ncg in that .It wraps a C implementation of the algorithm .", "label": "", "metadata": {}, "score": "72.11768"}
{"text": "+ VI : ndarray + The inverse of the covariance matrix ( for Mahalanobis ) .+ + + : Returns : + Y : ndarray + A : math:'$m_A$ ' by : math:'$m_B$ ' distance matrix .\" The vq module only - supports vector quantization and the k - means algorithms .", "label": "", "metadata": {}, "score": "72.9435"}
{"text": "Precision goal for the value of x in the stopping criterion ( after applying x scaling factors ) .Defaults to -1 .pgtol : float .Precision goal for the value of the projected gradient in the stopping criterion ( after applying x scaling factors ) .", "label": "", "metadata": {}, "score": "73.11046"}
{"text": "-Calling Conventions ------------------- 1 . ''The points - are arranged as m n - dimensional row vectors in the matrix X. + Computes the distance between : math:'$m$ ' points using + Euclidean distance ( 2-norm ) as the distance metric between the + points .", "label": "", "metadata": {}, "score": "73.65117"}
{"text": "( McGraw - Hill , New York , 1996 ) .J. W. Goodman , Statistical Optics ( Wiley , New York , 1985 ) .C. R. Vogel , Computational Methods for Inverse Problems ( Society for Industrial and Applied Mathematics , Philadelphia , Pa. , 2002 ) .", "label": "", "metadata": {}, "score": "73.884285"}
{"text": "59 - 60 For the zoom phase it uses an algorithm by Outputs : ( alpha0 , gc , fc ) --------------------------------------------------------------------So I need to know what are other args , especially gfk ( is it a gradient in point xk ? ) , old_fval , old_old_fval ( I guess I know what do c1 & c2 mean )", "label": "", "metadata": {}, "score": "75.04503"}
{"text": "Will be recomputed if omitted .Will be recomputed if omitted .+ c1 : float , optional + Parameter for Armijo condition rule .+ c2 : float , optional + Parameter for curvature condition rule .+ fc : int + Number of function evaluations made .", "label": "", "metadata": {}, "score": "76.663956"}
{"text": "+ c2 : float + Parameter for curvature condition rule .+ + Returns + ------- + alpha_star : float + Best alpha + phi_star + phi at alpha_star + phi0 + phi at 0 + derphi_star + derphi at alpha_star + + Notes + ----- + Uses the line search algorithm to enforce strong Wolfe + conditions .", "label": "", "metadata": {}, "score": "76.67238"}
{"text": "Horizontal axis , cumulative FFTs ; vertical axis , relative solution error on a logarithmic scale .Top plot , convergence results for 1 % noise ; bottom plot , convergence results for 10 % noise .Solid curve , GPRNCG with preconditioning ; dashed - dotted curve , GPRNCG without preconditioning ; dashed curve , LBFGS - B. H .", "label": "", "metadata": {}, "score": "76.67648"}
{"text": "- p : double - The p - norm to apply ( for Minkowski , weighted and unweighted ) - V : ndarray - The variance vector ( for standardized Euclidean ) .- VI : ndarray - The inverse of the covariance matrix ( for Mahalanobis ) .", "label": "", "metadata": {}, "score": "77.23117"}
{"text": "Methods Software 5 , 235 - 245 ( 1995 ) .[CrossRef ] .J. W. Goodman , Introduction to Fourier Optics , 2nd ed .( McGraw - Hill , New York , 1996 ) .J. W. Goodman , Statistical Optics ( Wiley , New York , 1985 ) .", "label": "", "metadata": {}, "score": "78.56358"}
{"text": "Image reconstruction gives rise to some challenging large - scale constrained optimization problems .We consider a convex minimization problem with nonnegativity constraints that arises in astronomical imaging .To solve this problem , we use an efficient hybrid gradient projection - reduced Newton ( active - set ) method .", "label": "", "metadata": {}, "score": "78.87065"}
{"text": "All methods can be constrained by the Method of Multipliers ( also known as the Augmented Lagrangian ) .Download .Copyright ( C ) 2004 - 2006 , the Gna ! people .Posted items are owned by whoever posted them .", "label": "", "metadata": {}, "score": "79.0178"}
{"text": "hi all , this is a letter primarily for my mentors & Matthieu Brucher according to the schedule proposed by my mentors I should work on chapter 2 2 .Make existing openopt code usable by adding docstrings , unit tests , and documented sample scripts .", "label": "", "metadata": {}, "score": "80.06027"}
{"text": "InSection 2 , we describe the algorithm .The convergence properties of the algorithm are analyzed in Section 3 and 4 .In Section 5 , we report some numerical results to test the proposed algorithms by using the test problems in the CUTEr [ 23 ] library .", "label": "", "metadata": {}, "score": "82.16158"}
{"text": "Make sure ralg and lincher are fully functional , tested , and documented , . completing as much as possible of your projected work on ralg .as well as their derivatives ) .Documentation must be . such as to allow other developers to understand and .", "label": "", "metadata": {}, "score": "83.99444"}
{"text": "\" CG_DESCENT \" stands for the CG_DESCENT method with theapproximate Wolfe line search [ 13].Here we use the Fortran77 ( version 1.4 ) code obtained from Hager 's web site and use the default parameters there . \" PRP+ \" shows the PRP+ method with Wolfe line search proposed in [ 22]. \" MPRP \" means the three term PRP method proposed in [ 10 ] with thesame line search as \" CG_DESCENT \" method .", "label": "", "metadata": {}, "score": "84.16763"}
{"text": "+ XB : ndarray + An : math:'$m_B$ ' by : math:'$n$ ' array of : math:'$m_B$ ' + original observations in an : math:'$n$'-dimensional space .+ metric : string or function + The distance metric to use .+ w : ndarray + The weight vector ( for weighted Minkowski ) .", "label": "", "metadata": {}, "score": "85.70523"}
{"text": "and/or an example of usage ?Regards , D. Author : damian.eads Date : 2008 - 11 - 02 22:53:36 -0600 ( Sun , 02 Nov 2008 )New Revision : 4944 Modified : trunk / scipy / optimize / optimize.py trunk / scipy / spatial / distance.py trunk / scipy / spatial / info.py Log : Working on doc fixes .", "label": "", "metadata": {}, "score": "87.504585"}
{"text": "l .s .l .D .I . f .l .Hess .J .f .l .t .s .l . )D .I . f .l .s .l .D .", "label": "", "metadata": {}, "score": "95.37688"}
{"text": "m .T . ) s .l . m .s .l .m .T .V .l .m .V .l . )l .m .V .l .T .V .", "label": "", "metadata": {}, "score": "97.09541"}
{"text": "m .T . ) s .l . m .s .l .m .T .V .l .m .V .l . )l .s .l .s .l .D .", "label": "", "metadata": {}, "score": "98.04541"}
{"text": "V .l .T .V .l .m .T . )H .l .V .l .m .V .l . )l .m .V .l .T .V .", "label": "", "metadata": {}, "score": "101.455795"}
