{"text": "We submitted the results for Task 1.1 ( sentence - level quality estimation ) , Task 1.2 ( system selection ) and Task 2 ( word - level quality estimation ) ... \" .This paper is to introduce our participation in the WMT13 shared tasks on Quality Estimation for machine translation without using reference translations .", "label": "", "metadata": {}, "score": "32.77281"}
{"text": "Our experiments and their results are described in section 4 .Future directions and extensions of this work are discussed in section 5 . 2 Evaluation Metrics The metrics used in our evaluations , in addition to BLEU and NIST , are based on explicit word - to - word matches between the translation being evaluated and each of one or more reference translations .", "label": "", "metadata": {}, "score": "33.246212"}
{"text": "We further improve performance by combining indi - vidual evaluation metrics using maximum correlation training , which is shown to be better than the classification - based frame - work .Such metrics were shown to have better fluency evaluation performance than metrics based on n - grams such BLEU and NIST ( Doddington , 2002 ) . \" ...", "label": "", "metadata": {}, "score": "34.971245"}
{"text": "In Task 1.1 , we used an enhanced version of BLEU metric without using reference translations to evaluate the translation quality .In Task 1.2 , we utilized a probability model Na\u00efve Bayes ( NB ) as a classification algorithm with the features borrowed from the traditional evaluation metrics .", "label": "", "metadata": {}, "score": "36.04123"}
{"text": "The detailed results of WMT13 Metrics Task is introduced in the paper .[ 16 ] .^ While the metrics are described as for the evaluation of machine translation , in practice they may also be used to measure the quality of human translation .", "label": "", "metadata": {}, "score": "37.646492"}
{"text": "The study concluded that , \" highly reliable assessments can be made of the quality of human and machine translations \" .[ 4 ] .As part of the Human Language Technologies Program , the Advanced Research Projects Agency ( ARPA ) created a methodology to evaluate machine translation systems , and continues to perform evaluations based on this methodology .", "label": "", "metadata": {}, "score": "38.34733"}
{"text": "( 2006 ) and Koehn and Monz ( 2006 ) , revealed that , in certain cases , the BLEU metric may not be a reliable MT quality indicator .This happens , for instance , when the systems under evaluation are based on different paradigms , and therefore , ... \" .", "label": "", "metadata": {}, "score": "38.395607"}
{"text": "( 2006 ) and Koehn and Monz ( 2006 ) , revealed that , in certain cases , the BLEU metric may not be a reliable MT quality indicator .This happens , for instance , when the systems under evaluation are based on different paradigms , and therefore , ... \" .", "label": "", "metadata": {}, "score": "38.395607"}
{"text": "This paper investigates these concerns in the context of using regression to develop metrics for evaluating machine - translated sentences .We track a learned metric 's reliability across a 5 year period to measure the extent to which the learned metric can evaluate sentences produced by other systems .", "label": "", "metadata": {}, "score": "38.47045"}
{"text": "( 2006 ) and Koehn and Monz ( 2006 ) , revealed that , in certain cases , the BLEU metric may not be a reliable MT quality indicator .This happens , for instance , when the systems under evaluation are based on different paradigms , and therefore , do not share the same lexicon .", "label": "", "metadata": {}, "score": "38.47281"}
{"text": "( 2006 ) and Koehn and Monz ( 2006 ) , revealed that , in certain cases , the BLEU metric may not be a reliable MT quality indicator .This happens , for instance , when the systems under evaluation are based on different paradigms , and therefore , do not share the same lexicon .", "label": "", "metadata": {}, "score": "38.47281"}
{"text": "Differing provisions from the publisher 's actual policy or licence agreement may be applicable .\" Their experiment also revealed that the enhancement of both BLEU and NIST is correlated to human evaluation .To overcome the weaknesses of the above mentioned metrics , a new metric was proposed by Lavie et al .", "label": "", "metadata": {}, "score": "38.48927"}
{"text": "By measuring the output of the systems against the original corpus data for the target language the adequacy of the translation can be assessed .Koehn uses the BLEU metric by Papineni et al .( 2002 ) for this , which counts the coincidences of the two compared versions - SMT output and corpus data - and calculates a score on this basis .", "label": "", "metadata": {}, "score": "38.613327"}
{"text": "527 - 549 , 2006 .G. Lembersky , N. Ordan , and S. Wintner , \" Language Models for Machine Translation - Original vs. Translated Texts , \" J. Computational Linguistics , Volume 38 , Number 4 , 2012 .K. Papineni , S. Roukos , T. Ward , and W.-J. Zhu , \" BLEU : A Method for Auto - matic Evaluation of Machine Translation , \" In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , ACL Press , 2002 , pp . 311 - 318 .", "label": "", "metadata": {}, "score": "38.62811"}
{"text": "More information on the evaluation algorithm may be obtained from the paper detailing the algorithm : BLEU : a Method for Automatic Evaluation of Machine Translation ( Papineni et al , 2002 ) .The included scoring script is intended for use with SGML - formatted data files .", "label": "", "metadata": {}, "score": "38.80126"}
{"text": "Witten IH , Frank E ( 2005 )Data mining : practical machine learning tools and techniques , 2nd edn .Morgan Kaufmann , San Francisco .Ye Y , Zhou M , Lin C - Y ( 2007 ) Sentence level machine translation evaluation as a ranking .", "label": "", "metadata": {}, "score": "38.85668"}
{"text": "We further improve the evaluation performance by combining the individual metrics using maximum correlation training , which is shown to be better than the classification - based framework .Such metrics were shown to have better fluency evaluation performance than metrics based on n - grams such BLEU and NIST ( Doddington , 2002 ) . \" ...", "label": "", "metadata": {}, "score": "38.89806"}
{"text": "Using the IBM model one and the information of morphemes , lexicon probabilities , Part - of - Speech , etc . , Popovi\u0107 et al .[ 22 ] also introduces an unsupervised evaluation method , and show that the most promising setting comes from the IBM-1 scores calculated on morphemes and POS-4gram .", "label": "", "metadata": {}, "score": "38.99953"}
{"text": "Papineni K , Roukos S , Ward T , Zhu W - J ( 2002 ) Bleu : a method for automatic evaluation of machine translation .In : 40th annual meeting of the Association for Computational Linguistics ( ACL-2002 ) , Philadelphia , PA , pp 311 - 318 .", "label": "", "metadata": {}, "score": "39.231964"}
{"text": "Coughlin ( 2003 ) reports that comparing the candidate text against a single reference translation does not adversely affect the correlation of metrics when working in a restricted domain text .Even if a metric correlates well with human judgment in one study on one corpus , this successful correlation may not carry over to another corpus .", "label": "", "metadata": {}, "score": "39.36136"}
{"text": "We then use this approach to investigate the benefits of introducing linguistic features into evaluation metrics .Overall , our experiments show that ( i ) both lexical and linguistic metrics present complementary advantages and ( ii ) combining both kinds of metrics yields the most robust metaevaluation performance . ... 1 5 # casesassessed 347 447 266 272 Table 1 : NIST 2004/2005 MT Evaluation Campaigns .", "label": "", "metadata": {}, "score": "39.849056"}
{"text": "Presentation at DARPA / TIDES 2003 MT Workshop .NIST , Gathersberg , MD .July 2003 .Bo Pang , Kevin Knight and Daniel Marcu .Syntax - based Alignment of Multiple Translations : Extracting Paraphrases and Generating New Sentences .", "label": "", "metadata": {}, "score": "40.866875"}
{"text": "After this , we formalize and verify empirically a set of properties that every text evaluation measure based on similarity to human - produced references satisfies .These properties imply that corroborating system improvements with additional measures always increases the overall reliability of the evaluation process .", "label": "", "metadata": {}, "score": "41.052418"}
{"text": "After this , we formalize and verify empirically a set of properties that every text evaluation measure based on similarity to human - produced references satisfies .These properties imply that corroborating system improvements with additional measures always increases the overall reliability of the evaluation process .", "label": "", "metadata": {}, "score": "41.052418"}
{"text": "Furthermore , Koehn uses the SMT systems and the Europarl corpus data to investigate whether back translation is an adequate method for the evaluation of machine translation systems .[ 1 ] The results indicate that the scores for back translation are far higher than those for monodirectional translation and what is more important they do not correlate at all with the monodirectional scores .", "label": "", "metadata": {}, "score": "41.34811"}
{"text": "It has been shown to correlate highly with human judgments of quality at the corpus level .[ 9 ] .BLEU uses a modified form of precision to compare a candidate translation against multiple reference translations .The metric modifies simple precision since machine translation systems have been known to generate more words than appear in a reference text .", "label": "", "metadata": {}, "score": "41.399483"}
{"text": "We achieved the highest BLEU score in 2 out of 5 language pairs and had competitive results for the other language pairs . ... el alignments to induce phrasebased translation models is common practise in the statistical machine translation community .In contrast to this , Marcu and Wong ( 2002 ) have defined a method for directly estimating phrasal translation models from parallel corpora , rather than using heuristic methods to induce phrase align ... . \" ...", "label": "", "metadata": {}, "score": "41.448494"}
{"text": "We participated in the supplied co ... \" .Our participation in the IWSLT 2005 speech translation task is our first effort to work on limited domain speech data .We adapted our statistical machine translation system that performed successfully in previous DARPA competitions on open domain text translations .", "label": "", "metadata": {}, "score": "41.54812"}
{"text": "Finally , the metric must be general , that is it should work with different text domains , in a wide range of scenarios and MT tasks .The aim of this subsection is to give an overview of the state of the art in automatic metrics for evaluating machine translation .", "label": "", "metadata": {}, "score": "41.608067"}
{"text": "In this paper , the authors propose an unsupervised MT evaluation metric using universal Part - of - Speech tagset without relying on reference translations .The authors also explore the performances of the designed metric on traditional supervised evaluation tasks .", "label": "", "metadata": {}, "score": "41.876244"}
{"text": "Hastie T , Tibshirani R , Friedman J ( 2001 )The elements of statistical learning .Springer - Verlag , New York .Hovy E , King M , Popescu - Belis A ( 2002 )Principles of context - based machine translation evaluation .", "label": "", "metadata": {}, "score": "41.99463"}
{"text": "( 2003 ) point out that , \" Any MT evaluation measure is less reliable on shorter translations \" , and show that increasing the amount of data improves the reliability of a metric .However , they add that \" ... reliability on shorter texts , as short as one sentence or even one phrase , is highly desirable because a reliable MT evaluation measure can greatly accelerate exploratory data analysis \" .", "label": "", "metadata": {}, "score": "42.016308"}
{"text": "[ 1 ] After sentence splitting and tokenization the sentences were aligned across languages with the help of an algorithm developed by Gale & Church ( 1993 ) .[ 1 ] .Contents .In his paper \" Europarl : A Parallel Corpus for Statistical Machine Translation \" ( 2005 )", "label": "", "metadata": {}, "score": "42.02887"}
{"text": "In this work , we attempt to change their original calculation uni ... \" .String - based metrics of automatic machine translation ( MT ) evaluation are widely applied in MT research .Meanwhile , some linguistic motivated metrics have been suggested to improve the string - based metrics in sentencelevel evaluation .", "label": "", "metadata": {}, "score": "42.068253"}
{"text": "In the unsupervised design , the metric is measured on the source and target POS sequences instead of the surface words .In the exploration of its usage in the traditional supervised MT evaluations , we measure the score on the target translations ( system outputs ) and reference translations , i.e. measuring on the surface words .", "label": "", "metadata": {}, "score": "42.130875"}
{"text": "M. Felice and L. Specia , \" Linguistic Features for Quality Estimation , \" Proceedings of the 7th Workshop on Statistical Machine Translation , ACL Press , 2012 , pp .96 - 103 .S. Petrov , D. Das , and R. McDonald , \" A Universal Part - of - Speech Tagset , \" Proceedings of the Eight International Conference on Language Resources and Evaluation ( LREC'12 ) , European Language Resources Association ( ELRA ) Press .", "label": "", "metadata": {}, "score": "42.26501"}
{"text": "Measuring systems based on adequacy and fluency , along with informativeness is now the standard methodology for the ARPA evaluation program .[5 ] .In the context of this article , a metric is a measurement .A metric that evaluates machine translation output represents the quality of the output .", "label": "", "metadata": {}, "score": "42.729134"}
{"text": "COLING 2012 .K. Papineni , S. Roukos , T. Ward , and W. Zhu ( 2002 ) .BLEU : a Method for Automatic Evaluation of Machine Translation .ACL 2002 .Acknowledgments .Tools . by Ding Liu , Daniel Gildea - In Proceedings of the HLT / NAACL-2007 , 2007 . \" ...", "label": "", "metadata": {}, "score": "43.20185"}
{"text": "In this paper , we present a comparison between the widely used BLEU and NIST metrics , and a set of easily computable metrics based on unigram precision and recall .Using several empirical evaluation methods that have been proposed .Page 2 . in the recent literature as concrete means to assess the level of correlation of au- tomatic metrics and human judgments , we show that higher correlations can be obtained with fairly simple and straightforward metrics .", "label": "", "metadata": {}, "score": "43.99659"}
{"text": "The included scoring script was released with the original evaluation , intended for use with SGML - formatted data files , and is provided to ensure compatibility of user scoring results with results from the original evaluation .An updated scoring software package ( mteval-v13a-20091001.tar .", "label": "", "metadata": {}, "score": "44.009346"}
{"text": "Many MT methods and automatic MT systems were proposed in the past years [ 2][3][4].Traditionally , people use the human evaluation approaches for the quality estimation of MT systems , such as the adequacy and fluency criteria .However , the human evaluation is expensive and time consuming .", "label": "", "metadata": {}, "score": "44.169544"}
{"text": "Version 0.2 also includes implementations of PRO and risk minimization as well as several additional forms of ramp loss from Gimpel ( 2012 ) .Also included are the improved sentence - level BLEU approximations from Nakov et al .( 2012 ) , which are recommended for single - reference training .", "label": "", "metadata": {}, "score": "44.74157"}
{"text": "Previous work by Lin and Hovy [ 9 ] has shown that a recall - based automatic metric for evaluating summaries outperforms the BLEU metric on that task .We describe the metrics used in our evaluation in Section 2 .We also discuss certain characteristics of the BLEU and NIST metrics that may account for the advantage of metrics based on unigram recall .", "label": "", "metadata": {}, "score": "44.78781"}
{"text": "Several other automatic metrics for MT evaluation have been proposed since the early 1990s .The utility and attractiveness of automatic metrics for MT evaluation has been widely recognized by the MT community .Evaluating an MT system using such automatic metrics is much faster , easier and cheaper compared to human evaluations , which require trained bilingual evaluators .", "label": "", "metadata": {}, "score": "44.889656"}
{"text": "We further improve performance by combining individual evaluation metrics using maximum correlation training , which is shown to be better than the classification - based framework .Such metrics were Ding Liu and Daniel Gildea Department of Computer Science University of Rochester Rochester , NY 14627 shown to have better fluency evaluation performance than metrics based on n - g ... . \" ...", "label": "", "metadata": {}, "score": "45.02736"}
{"text": "We further improve performance by combining individual evaluation metrics using maximum correlation training , which is shown to be better than the classification - based framework .Such metrics were Ding Liu and Daniel Gildea Department of Computer Science University of Rochester Rochester , NY 14627 shown to have better fluency evaluation performance than metrics based on n - g ... \" ...", "label": "", "metadata": {}, "score": "45.043915"}
{"text": "Scoring Tools .This evaluation kit includes a single Perl script ( mteval - v11b . pl ) that may be used to produce a translation quality score for one ( or more ) MT systems .The script works by comparing the system output translation with a set of ( expert ) reference translations of the same source text .", "label": "", "metadata": {}, "score": "45.119816"}
{"text": "We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality , and give two significant counterexamples to Bleu 's ... \" .We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric .", "label": "", "metadata": {}, "score": "45.147476"}
{"text": "Training a sentence - level machine translation confidence measure .In : Proceedings of the international conference on language resources and evaluation ( LREC-2004 ) , Lisbon , Portugal , pp 825 - 828 .Riezler S , Maxwell JT III ( 2005 )", "label": "", "metadata": {}, "score": "45.289047"}
{"text": "A number of sentences were used as a testing dataset to evaluate both engines .Human translations by three bilingual speakers were used as a gold standard .A simple evaluation metric was proposed to calculate the translation accuracy of verb - noun collocations .", "label": "", "metadata": {}, "score": "45.394115"}
{"text": "^ Papineni , Kishore et al ( 2002 ) : BLEU .A method for automatic evaluation of machine translation , in : Proceedings of the 40th Annual Meeting of the Association of Computational Linguistics ( ACL ) , pp . 311 - 318 .", "label": "", "metadata": {}, "score": "45.409042"}
{"text": "BLEU was one of the first metrics to report high correlation with human judgments of quality .The metric is currently one of the most popular in the field .The central idea behind the metric is that \" the closer a machine translation is to a professional human translation , the better it is \" .", "label": "", "metadata": {}, "score": "45.89902"}
{"text": "International Conference on Computational Linguistics and 44th Annual Meeting of the ACL , ACL Press , 2006 , pp .433 - 440 .[ 31 ] M. Mach\u00e1\u010dek and O. Bojar , \" Results of the WMT13 Metrics Shared Task , \" Proceedings of the Eighth Workshop on Statistical Machine Transla - tion , ACL Press , 2013 , pp .", "label": "", "metadata": {}, "score": "46.065857"}
{"text": "The training experiments on the past WMT corpora showed that the designed methods of this paper yielded promising results especially the statistical models of CRF and NB .The official results show that our CRF model achieved the highest F - score 0.8297 in binary classification of Task 2 . \" ...", "label": "", "metadata": {}, "score": "46.23575"}
{"text": "In : HLT - NAACL 06Statistical machine translation , Proceedings of the workshop , New York City , pp 86 - 93 .Owczarzak K , Van Genabith J , Way A ( 2007 ) Dependency - based automatic evaluation for machine translation .", "label": "", "metadata": {}, "score": "46.42639"}
{"text": "These results support the use of heterogeneous measures in order to consolidate text evaluation results . \" ...A number of approaches to Automatic MT Evaluation based on deep linguistic knowledge have been suggested .However , n - gram based metrics are still today the dominant approach .", "label": "", "metadata": {}, "score": "46.44967"}
{"text": "65 - 72 .B. Chen and R. Kuhn , \" Amber : A modi - fied bleu , enhanced ranking metric , \" In Proceed - ings of the Sixth Workshop on Statistical Machine translation of the Association for Computational Linguistics , ACL Press , 2011 , pp 71 - 77 . Y. Zhang , S. Vogel , and A. Waibel , \" Interpreting BLEU / NIST scores : How much im - provement do we need to have a better system ? , \" In Proc . of LREC , ELRA Press , 2004 , Lisbon .", "label": "", "metadata": {}, "score": "46.486107"}
{"text": "Each document contains 3,003 sentences of source English or translated German .To avoid the over fitting problem , the WMT20112 corpora are used as training data and WMT20123 corpora are used for the testing .This paper conducts the experiments on the simplified version ( unigram precision and recall ) of the metric .", "label": "", "metadata": {}, "score": "46.58593"}
{"text": "The idea of quality panel evaluation was to submit translations to a panel of expert native English speakers who were professional translators and get them to evaluate them .The evaluations were done on the basis of a metric , modelled on a standard US government metric used to rate human translations .", "label": "", "metadata": {}, "score": "46.771133"}
{"text": "By aligning both t ... \" .We propose three new features for MT evaluation : source - sentence constrained n - gram precision , source - sentence re - ordering metrics , and discriminative un - igram precision , as well as a method of learning linear feature weights to directly maximize correlation with human judg - ments .", "label": "", "metadata": {}, "score": "46.854156"}
{"text": "A metric that only works for text in a specific domain is useful , but less useful than one that works across many domains - because creating a new metric for every new evaluation or domain is undesirable .Another important factor in the usefulness of an evaluation metric is to have good correlation , even when working with small amounts of data , that is candidate sentences and reference translations .", "label": "", "metadata": {}, "score": "46.87761"}
{"text": "This test bed presents the particularity of providing automatic translations produced by heterogeneous MT systems ( i.e. , systems belonging to different paradigms ) .Specifically , all systems are sta ... . \" ...String - based metrics of automatic machine translation ( MT ) evaluation are widely applied in MT research .", "label": "", "metadata": {}, "score": "47.004433"}
{"text": "313 - 330 , 1993 .W. Skut , B. Krenn , T. Brants , and H. Uszkoreit , \" An annotation scheme for free word order languages , \" In Proceedings of Applied Natural Language Processing , ACL Press , 1997 .", "label": "", "metadata": {}, "score": "47.016605"}
{"text": "The methods were ; comprehension evaluation , quality panel evaluation , and evaluation based on adequacy and fluency .Comprehension evaluation aimed to directly compare systems based on the results from multiple choice comprehension tests , as in Church et al .", "label": "", "metadata": {}, "score": "47.05283"}
{"text": "In Proceedings of the Second International Conference on Language Resources and Evaluation ( LREC-2000 ) .Athens , Greece .pp .39 - 45 .Gregor Leusch , Nicola Ueffing and Herman Ney .String - to - String Distance Measure with Applications to Machine Translation Evaluation .", "label": "", "metadata": {}, "score": "47.10076"}
{"text": "The package was compiled and scoring software was developed by researchers at NIST , making use of broadcast , newswire and web data and reference translations collected and developed by LDC .The objective of the NIST Open Machine Translation ( OpenMT ) evaluation series is to support research in , and help advance the state of the art of , machine translation ( MT ) technologies -- technologies that translate text between human languages .", "label": "", "metadata": {}, "score": "47.11239"}
{"text": "In this work , we suggest using metrics which take into account linguistic features at more abstract levels .We provide experimental results showing that metrics based on deeper linguistic information ( syntactic / shallow - semantic ) are able to produce more reliable system rankings than metrics based on lexical matching alone , specially when the systems under evaluation are of a different nature .", "label": "", "metadata": {}, "score": "47.32348"}
{"text": "The measure of evaluation for metrics is correlation with human judgment .This is generally done at two levels , at the sentence level , where scores are calculated by the metric for a set of translated sentences , and then correlated against human judgment for the same sentences .", "label": "", "metadata": {}, "score": "47.353218"}
{"text": "Sample data files and execution scripts are provided .Code for Statistical Significance Testing for MT Evaluation Metrics .P. Koehn ( 2004 ) .Statistical Significance Tests for Machine Translation Evaluation .EMNLP 2004 .P. Nakov , F. Guzm\u00e1n , and S. Vogel ( 2012 ) .", "label": "", "metadata": {}, "score": "47.42001"}
{"text": "In : 10th EAMT conference , practical applications of machine translation , proceedings , Budapest , Hungary , pp 103 - 111 .Gim\u00e9nez J , M\u00e0rquez L ( 2008 )A smorgasbord of features for automatic MT evaluation .In : ACL-08 : HLT third workshop on statistical machine translation , Columbus , Ohio , pp 195 - 198 .", "label": "", "metadata": {}, "score": "47.44573"}
{"text": "In : Intrinsic and extrinsic evaluation measures for machine translation and/or summarization , Proceedings of the ACL-05 workshop , Ann Arbor , MI , pp 57 - 64 .Snover M , Dorr B , Schwartz R , Micciulla L , Makhoul J ( 2006 )", "label": "", "metadata": {}, "score": "47.77428"}
{"text": "59 - 63 .G. Doddington , \" Automatic Evaluation of Machine Translation Quality using N - gram Co - occurrence Statistics , \" In Proceedings of the 2nd International Conference on Human Lan - guage Technology Research ( HLT-02 ) , Morgan Kaufmann Publishers Inc. Press , 2002 , pp .", "label": "", "metadata": {}, "score": "47.939297"}
{"text": "160 - 167 .J. B. Mari\u00f1 R. E. Banchs , J. M. Crego , A. de Gispert , P. Lambert , J. o , A. Fonollosa , and M. R. Costa - juss\u00e0 \" N - gram based machine , translation , \" Computational Linguistics , Vol .", "label": "", "metadata": {}, "score": "48.013336"}
{"text": "While this may seem unexpected , since BLEU and NIST focus on n - gram precision and disregard recall , our experiments show that correlation with human judgments is highest when almost all of the weight is assigned to recall .We also show that stemming is significantly beneficial not just to simpler unigram precision and recall based metrics , but also to BLEU and NIST . 1 Introduction Automatic Metrics for machine translation ( MT ) evaluation have been receiv- ing significant attention in the past two years , since IBM 's BLEU metric was proposed and made available [ 1].", "label": "", "metadata": {}, "score": "48.013474"}
{"text": "The aim of the training stage is to achieve higher correlation with human judgments .The experiments on WMT11 corpora show that yields the best correlation scores on the language pairs of CS - EN , ES - EN , EN - CS and EN - ES , which contributes to the highest average score 0.77 on all the eight language pairs .", "label": "", "metadata": {}, "score": "48.245556"}
{"text": "Kulesza A , Shieber SM ( 2004 )A learning approach to improving sentence - level MT evaluation .In : TMI-2004 : Proceedings of the tenth conference on theoretical and methodological issues in machine translation , Baltimore , MD , pp 75 - 84 .", "label": "", "metadata": {}, "score": "48.261494"}
{"text": "Doddington , G. ( 2002 ) \" Automatic evaluation of machine translation quality using n - gram cooccurrence statistics \" .Proceedings of the Human Language Technology Conference ( HLT ) , San Diego , CA pp .128 - 132 .", "label": "", "metadata": {}, "score": "48.416687"}
{"text": "III .RELATED WORKS As mentioned previously , the traditional evaluation metrics tend to estimate quality of the automatic MT output by measuring its closeness with the reference translations .Gamon et al .[ 18 ] conduct a research about reference - free MT evaluation approaches also at sentence level , which work utilizes the linear and nonlinear combinations of language model and SVM classifier to find the badly translated sentences .", "label": "", "metadata": {}, "score": "48.43004"}
{"text": "The experiments also show that the latest proposed metrics ( e.g. AMBER ) achieve higher correlation score than the earlier ones ( e.g. BLEU ) .TABLE IV .The number of participated MT systems that offer the output translations is shown in Table 5 for each language pair .", "label": "", "metadata": {}, "score": "48.43991"}
{"text": "The goal is for the output to be an adequate and fluent translation of the original .The MT evaluation series started in 2001 as part of the DARPA TIDES ( Translingual Information Detection , Extraction ) program .Beginning with the 2006 evaluation , the evaluations have been driven and coordinated by NIST as NIST OpenMT .", "label": "", "metadata": {}, "score": "48.489693"}
{"text": "119 - 131 , 2013 . A. L.-F. M. Gamon , A. Aue , and M. Smets , \" Sentence - level MT evaluation without refer - ence translations : Beyond language modeling , \" Proceedings of EAMT , EAMT Press , 2005 .", "label": "", "metadata": {}, "score": "48.5645"}
{"text": "We then propose a powerful string - based automatic MT evaluation metric , combining all the features with various granularities based on SVM rank and regression models .The experimental results show that i ) the new features with various granularities can contribute to the automatic evaluation of translation quality ; ii ) our proposed string - based metrics with multiple granularities based on SVM regression model can achieve higher correlations with human assessments than the stateof - art automatic metrics . \" ...", "label": "", "metadata": {}, "score": "48.571693"}
{"text": "The evaluation results in Table 9 and Table 10 show that the evaluation on the language pairs with English as the source language ( English - to - other ) is the main challenge at system - level performance .Fortunately , the designed .", "label": "", "metadata": {}, "score": "48.691597"}
{"text": "To this end , they are designed to be simple , to focus on core technology issues , and to be fully supported .The 2004 task was to evaluate translation from Chinese to English and from Arabic to English .Scoring Tools .", "label": "", "metadata": {}, "score": "48.840202"}
{"text": "The OpenMT evaluations are intended to be of interest to all researchers working on the general problem of automatic translation between human languages .To this end , they are designed to be simple , to focus on core technology issues and to be fully supported .", "label": "", "metadata": {}, "score": "48.95394"}
{"text": "A survey of current paradigms in machine translation .Adv Comput 49 : 2 - 68 .Duh K ( 2008 )Ranking vs. regression in machine translation evaluation .In : Proceedings of the third workshop on statistical machine translation , Columbus , Ohio , pp 191 - 194 .", "label": "", "metadata": {}, "score": "49.00245"}
{"text": "[ 23 ] use the cross - lingual textual entailment to push semantics into the MT evaluation without using reference translations , which work mainly focuses on the adequacy estimation .Avramidis [ 24 ] performs an automatic sentence - level ranking of multiple machine translations using the features of verbs , nouns , sentences , subordinate clauses and punctuation occurrences to derive the adequacy information .", "label": "", "metadata": {}, "score": "49.038948"}
{"text": "As mentioned previously , the language - bias problem is presented in the evaluation results of BLEU metric .BLEU yields the highest Spearman rank correlation score 0.99 on FR - EN ; however it never achieves the highest average correlation score on any translation direction , due to the very low correlation scores on RU - EN , EN - DE , EN - ES , and ENRU corpora .", "label": "", "metadata": {}, "score": "49.151733"}
{"text": "Springer - Verlag , New York .Blatz J , Fitzgerald E , Foster G , Gandrabur S , Goutte C , Kulesza A , Sanchis A , Ueffing N ( 2003 ) Confidence estimation for machine translation .Technical report natural language engineering workshop final report .", "label": "", "metadata": {}, "score": "49.30033"}
{"text": "We present a large parallel corpus of texts published by the United Nations Organization , which we exploit for the creation of phrasebased statistical machine translation ( SMT ) systems for new language pairs .We present a setup where phrase tables for these language pairs are used for translation between languages for which parallel corpora of sufficient size are so far not available .", "label": "", "metadata": {}, "score": "49.408234"}
{"text": "[ 1 ] The reason why it is such a poor predictor of quality is reasonably intuitive .A round - trip translation is not testing one system , but two systems : the language pair of the engine for translating into the target language , and the language pair translating back from the target language .", "label": "", "metadata": {}, "score": "49.410927"}
{"text": "Banerjee et al .( 2005 ) highlight five attributes that a good automatic metric must possess ; correlation , sensitivity , consistency , reliability and generality .Any good metric must correlate highly with human judgment , it must be consistent , giving similar results to the same MT system on similar text .", "label": "", "metadata": {}, "score": "49.46083"}
{"text": "B. Wong , and C. Kit , \" ATEC : auto - matic evaluation of machine translation via word choice and word order , \" Mach Translat , 23:141- 155 , 2009 . A. L.-F. A. L.-F. A L.-F. Han , D. F. Wong , L. S. Chao , L. He , S. Li and L. Zhu , \" Phrase Tagset Mapping for French and English Treebanks and Its Application in Machine Translation Evaluation , \" Lecture Notes in Computer Science ( LNCS ) , Vol .", "label": "", "metadata": {}, "score": "49.493805"}
{"text": "In this work , ... \" .A number of approaches to Automatic MT Evaluation based on deep linguistic knowledge have been suggested .However , n - gram based metrics are still today the dominant approach .The main reason is that the advantages of employing deeper linguistic information have not been clarified yet .", "label": "", "metadata": {}, "score": "49.674305"}
{"text": "Our source - sentence c ... \" .We propose three new features for MT evaluation : source - sentence constrained n - gram precision , source - sentence reordering metrics , and discriminative unigram precision , as well as a method of learning linear feature weights to directly maximize correlation with human judgments .", "label": "", "metadata": {}, "score": "49.679604"}
{"text": "This paper will propose an automatic evaluation approach for English - to - German translation by calculating the similarity between source and hypothesis translations without the using of reference translation .Furthermore , the potential usage of the proposed evaluation algorithms in the traditional reference - aware MT evaluation tasks will also be explored .", "label": "", "metadata": {}, "score": "49.99493"}
{"text": "The MT evaluation series started in 2001 as part of the DARPA TIDES ( Translingual Information Detection , Extraction ) program .Beginning with the 2006 evaluation , the evaluations have been driven and coordinated by NIST as NIST OpenMT .These evaluations provide an important contribution to the direction of research efforts and the calibration of technical capabilities in MT .", "label": "", "metadata": {}, "score": "50.020058"}
{"text": "In this work , we suggest using metrics which take into account linguistic features at more abstract levels .We provide experimental results showing that metrics based on deeper linguistic information ( syntactic / shallow - semantic ) are able to produce more reliable system rankings than metrics based on lexical matching alone , specially when the systems under evaluation are of a different nature . ... ystems .", "label": "", "metadata": {}, "score": "50.263992"}
{"text": "References .Albrecht JS , Hwa R ( 2007a )A re - examination of machine learning approaches for sentence - level MT evaluation .In : ACL 2007 Proceedings of the 45th annual meeting of the Association for Computational Linguistics , Prague , Czech Republic , pp 880 - 887 .", "label": "", "metadata": {}, "score": "50.338703"}
{"text": "386 - 393 .Chin - Yew Lin and Eduard Hovy .Automatic Evaluation of Summaries Using N - gram Co - occurrence Statistics .In Proceedings of HLT - NAACL 2003 .Edmonton , Canada .May 2003 .pp .", "label": "", "metadata": {}, "score": "50.562767"}
{"text": "By aligning both the ... \" .We propose three new features for MT evaluation : source - sentence constrained n - gram precision , source - sentence reordering metrics , and discriminative unigram precision , as well as a method of learning linear feature weights to directly maximize correlation with human judgments .", "label": "", "metadata": {}, "score": "50.63643"}
{"text": "By aligning both the ... \" .We propose three new features for MT evaluation : source - sentence constrained n - gram precision , source - sentence reordering metrics , and discriminative unigram precision , as well as a method of learning linear feature weights to directly maximize correlation with human judgments .", "label": "", "metadata": {}, "score": "50.63643"}
{"text": "Details of the programme can be found in White et al .( 1994 ) and White ( 1995 ) .The evaluation programme involved testing several systems based on different theoretical approaches ; statistical , rule - based and human - assisted .", "label": "", "metadata": {}, "score": "50.78212"}
{"text": "Full - text preview . cmu.edu Abstract .Recent research has shown that a balanced harmonic mean ( F1 measure ) of unigram precision and recall outperforms the widely used BLEU and NIST metrics for Machine Translation evaluation in terms of correlation with human judgments of translation quality .", "label": "", "metadata": {}, "score": "50.804848"}
{"text": "Philadelphia : Linguistic Data Consortium , 2010 .Introduction .NIST 2004 Open Machine Translation ( OpenMT ) Evaluation , is a package containing source data , reference translations , and scoring software used in the NIST 2004 OpenMT evaluation .It is designed to help evaluate the effectiveness of machine translation systems .", "label": "", "metadata": {}, "score": "50.966553"}
{"text": "In the WMT13 shared tasks , both Pearson correlation coefficient and the Spearman rank correlation coefficient are used as the evaluation criteria .So we list the official results in Table 9 and Table 10 respectively by using Spearman rank correlation and Pearson correlation criteria .", "label": "", "metadata": {}, "score": "51.049995"}
{"text": "Syntactic features for evaluation of machine translation .In : Intrinsic and extrinsic evaluation measures for machine translation and/or summarization , Proceedings of the ACL-05 workshop , Ann Arbor , MI , pp 25 - 32 .Liu D , Gildea D ( 2006 ) Stochastic iterative alignment for machine translation evaluation .", "label": "", "metadata": {}, "score": "51.290974"}
{"text": "Both of the two metrics are trained on WMT11 corpora .The tuned parameters of hLEPOR are shown in Table 7 , using default values for EN - RU and RU - EN .TABLE VII .THE NUMBER OF EFFECTIVE MT SYSTEMS IN WMT13 Number of evaluated MT systems Year WMT13 other - to - English CS - EN 12 DE - EN 23 TABLE IX .", "label": "", "metadata": {}, "score": "51.774574"}
{"text": "It was originally used for measuring the performance of speech recognition systems , but is also used in the evaluation of machine translation .The metric is based on the calculation of the number of words that differ between a piece of machine translated text and a reference translation .", "label": "", "metadata": {}, "score": "51.8154"}
{"text": "It is the similar approach for the source sentence .( 17 ) and ( 18 ) , where represents the number of matched n - gram chunks .The n - gram precision ( and recall ) is calculated on sentence - level not corpus - level used in BLEU ( ) .", "label": "", "metadata": {}, "score": "51.990803"}
{"text": "Furthermore , the automatic evaluation metrics can be used to II .It first computes the n - gram matches sentence by sentence , then adds the clipped n - gram counts for all the candidate sentences and divides by the number of candidate n - gram in the test corpus .", "label": "", "metadata": {}, "score": "52.039692"}
{"text": "Technical report natural language engineering workshop final report .Johns Hopkins University , Baltimore .Carbonell JG , Cullingford RE , Gershman AV ( 1981 ) Steps toward knowledge - based machine translation .IEEE Trans Pattern Anal Mach Intell 3(4 ) : 376 - 392 CrossRef .", "label": "", "metadata": {}, "score": "52.163204"}
{"text": "The application of the designed algorithms to the traditional supervised evaluation tasks is also explored .To address the language - bias problem in most of the existing metrics , tunable parameters are assigned to different subfactors .The experiment on WMT11 and WMT12 corpora shows our designed algorithms yields the highest average correlation score on eight language pairs as compared to the state - of - the - art reference - aware metrics METEOR , BLEU , and TER .", "label": "", "metadata": {}, "score": "52.3508"}
{"text": "This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores . ... by conducting a manual evaluation .As such , the NIST MT Eval provides an excellent source of data that allows Bleu 's correlation with human judgments to be verified .", "label": "", "metadata": {}, "score": "52.392048"}
{"text": "The performance of simplified variant will be tested using the reference translations .VI .EVALUATING THE EVALUATION METHOD The conventional method to evaluate the quality of different automatic MT evaluation metrics is to calculate their correlation scores with human judgments .", "label": "", "metadata": {}, "score": "52.393745"}
{"text": "Short Papers : pp .61 - 63 .Joseph P. Turian , Luke Shen and I. Dan Melamed .Evaluation of Machine Translation and its Evaluation .In Proceedings of MT Summit IX .New Orleans , LA .Sept. 2003 .", "label": "", "metadata": {}, "score": "52.42691"}
{"text": "The reason is that performing an accurate error analysis requires intensive human labor .In order to speed up the error analysis process , we suggest partially automatizing it by having automatic evaluation metrics play a more active role .For that purpose , we have compiled a large and heterogeneous set of features at different linguistic levels and at different levels of granularity .", "label": "", "metadata": {}, "score": "52.631046"}
{"text": "First , a learned metric is more reliable for translations that are similar to its training examples ; this calls into question whether it is as effective in evaluating translations from systems that are not its contemporaries .Second , metrics trained from different sets of training examples may exhibit variations in their evaluations .", "label": "", "metadata": {}, "score": "52.691254"}
{"text": "These results support the use of heterogeneous measures in order to consolidate text evaluation results .The lexical measure BLEU has been criticized in many ways .Some drawbacks of BLEU are the lack of interpretability ( Turian et al . , 2003a ) , the fa ... . by Aaron Li - feng Han , Lidia S. Chao , Yi Lu , Liangye He , Derek F. Wong , Junwen Xing . \" ...", "label": "", "metadata": {}, "score": "52.78038"}
{"text": "Evaluation of machine translation .Various methods for the evaluation for machine translation have been employed .This article focuses on the evaluation of the output of machine translation , rather than on performance or usability evaluation .A typical way for lay people to assess machine translation quality is to translate from a source language to a target language and back to the source language with the same engine .", "label": "", "metadata": {}, "score": "52.85645"}
{"text": "Human assessments of adequacy and fluency are available for a subset of sente ... . \" ...This work studies the viability of performing heterogeneous automatic MT error analyses .Error analysis is , undoubtly , one of the most crucial stages in the development cycle of an MT system .", "label": "", "metadata": {}, "score": "52.97473"}
{"text": "For each language , the test set consists of two files : a source and a reference file .Each file contains four independent translations of the data set .The evaluation year , source language , test set ( which , by default , is evalset ) , version of the data , and source vs. reference file ( with the latter being indicated by -ref ) are reflected in the file name .", "label": "", "metadata": {}, "score": "53.106224"}
{"text": "San Diego , CA . pp .128 - 132 .K.-Y. Su , M.-W. Wu , and J.-S. Chang .A New Quantitative Quality Measure for Machine Translation Systems .In Proceedings of the fifteenth International Conference on Computational Linguistics ( COLING-92 ) .", "label": "", "metadata": {}, "score": "53.13448"}
{"text": "No . 1 . L. Specia and J. Gimenez , \" Combining Confidence Estimation and Reference - based Met - rics for Segment - level MT Evaluation , \" Proceedings of The Ninth Conference of the Association for Machine Translation in the Americas , AMTA Press , 2010 .", "label": "", "metadata": {}, "score": "53.1382"}
{"text": "Therefore , any metric must assign quality scores so they correlate with human judgment of quality .That is , a metric should score highly translations that humans score highly , and give low scores to those humans give low scores .", "label": "", "metadata": {}, "score": "53.25006"}
{"text": "Lavie , A. , Sagae , K. and Jayaraman , S. ( 2004 ) \" The Significance of Recall in Automatic Metrics for MT Evaluation \" in Proceedings of AMTA 2004 , Washington DC .September 2004 .Papineni , K. , Roukos , S. , Ward , T. , and Zhu , W. J. ( 2002 ) .", "label": "", "metadata": {}, "score": "53.288307"}
{"text": "The reason is that performing an accurate error ... \" .This work studies the viability of performing heterogeneous automatic MT error analyses .Error analysis is , undoubtly , one of the most crucial stages in the development cycle of an MT system .", "label": "", "metadata": {}, "score": "53.291836"}
{"text": "In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics ( ACL ) , pages 311 - 318 , Philadelphia , PA , July .Doddington , George .Automatic Evaluation of Machine Translation Quality Using N - gram Co - Occurrence Statistics .", "label": "", "metadata": {}, "score": "53.502987"}
{"text": "[ 1 ] Koehn explains this with the fact that errors committed in the translation process might simply be reversed by back translation resulting in high coincidences of in- and output .[ 1 ] This , however , does not allow any conclusions about the quality of the text in the actual target language .", "label": "", "metadata": {}, "score": "53.515347"}
{"text": "[ 10 ] .The NIST metric is based on the BLEU metric , but with some alterations .Where BLEU simply calculates n - gram precision adding equal weight to each one , NIST also calculates how informative a particular n - gram is .", "label": "", "metadata": {}, "score": "53.59977"}
{"text": "by Amittai Axelrod , Ra Birch Mayne , Chris Callison - burch , Miles Osborne , David Talbot - In Proc .International Workshop on Spoken Language Translation ( IWSLT , 2005 . \" ...Our participation in the IWSLT 2005 speech translation task is our first effort to work on limited domain speech data .", "label": "", "metadata": {}, "score": "53.74108"}
{"text": "Intelligibility was measured without reference to the original , while fidelity was measured indirectly .The translated sentence was presented , and after reading it and absorbing the content , the original sentence was presented .The judges were asked to rate the original sentence on informativeness .", "label": "", "metadata": {}, "score": "54.00112"}
{"text": "Proceedings of the 1stConference of the Association for Machine Translation in the Americas .Columbia , MD pp .193 - 205 .White , J. ( 1995 ) \" Approaches to Black Box MT Evaluation \" .Proceedings of MT Summit V .", "label": "", "metadata": {}, "score": "54.012985"}
{"text": "The human judges were specially trained for the purpose .The evaluation study compared an MT system translating from Russian into English with human translators , on two variables .The variables studied were \" intelligibility \" and \" fidelity \" .", "label": "", "metadata": {}, "score": "54.404037"}
{"text": "In : AMTA 2006 : Proceedings of the 7th conference of the Association for Machine Translation in the Americas , visions of the future of machine translation , Cambridge , MA , pp 223 - 231 .Tillmann C , Vogel S , Ney H , Sawaf H , Zubiaga A ( 1997 )", "label": "", "metadata": {}, "score": "54.67129"}
{"text": "It implements training and decoding algorithms for several commonly - used models in machine translation .Inference for Monolingual and Bilingual Gappy Pattern Models .Below is a link to code that implements the models described by Gimpel and Smith ( 2011a ) .", "label": "", "metadata": {}, "score": "54.784527"}
{"text": "However , the evaluation systems and actually tell different results on the quality of the three MT systems .The evaluation system gives very high score 0.90 on system and similar low scores 0.35 and 0.40 respectively on and systems .On the other hand , the evaluation system yields similar scores 0.46 , 0.35 and 0.42 respectively on the three MT systems , which means that all the three automatic MT systems have low translation quality .", "label": "", "metadata": {}, "score": "54.80036"}
{"text": "Liu D , Gildea D ( 2007 )Source - language features and maximum correlation training for machine translation evaluation .In : NAACL HLT 2007 Human language technologies 2007 : the conference of the North American chapter of the Association for Computational Linguistics , Rochester , NY , pp 41 - 48 .", "label": "", "metadata": {}, "score": "54.842327"}
{"text": "115 - 132 .C. Callison - Burch , P. Koehn , C. Monz , M. Post , R. Soricut , and L. Specia , \" Findings of the 2012 Workshop on Statistical Machine Translation , \" In Proceedings of the Seventh Workshop on Statistical Machine Translation , ACL Press , 2012 , pp .", "label": "", "metadata": {}, "score": "54.86824"}
{"text": "Turian , J. , Shen , L. and Melamed , I. D. ( 2003 ) \" Evaluation of Machine Translation and its Evaluation \" .Proceedings of the MT Summit IX , New Orleans , USA , 2003 pp .386 - 393 .", "label": "", "metadata": {}, "score": "55.060387"}
{"text": "Then we calculate the hLEPOR score on the extracted POS sequences , i.e. , the closeness of the corresponding POS tags between hypothesis sentence and reference sentence .The final score is the combination of the two sub - scores and .", "label": "", "metadata": {}, "score": "55.171"}
{"text": "The Significance of Recall in Automatic Metrics for MT Evaluation .DOI : 10.1007/978 - 3 - 540 - 30194 - 3_16 Conference : Machine Translation : From Real Users to Research , 6th Conference of the Association for Machine Translation in the Americas , AMTA 2004 , Washington , DC , USA , September 28-October 2 , 2004 , Proceedings .", "label": "", "metadata": {}, "score": "55.60581"}
{"text": "( 2006 ) .Church , K. and Hovy , E. ( 1993 ) \" Good Applications for Crummy Machine Translation \" .Machine Translation , 8 pp .239 - 258 .Coughlin , D. ( 2003 ) \" Correlating Automated and Human Assessments of Machine Translation Quality \" in MT Summit IX , New Orleans , USA pp .", "label": "", "metadata": {}, "score": "55.613132"}
{"text": "In : Proceedings of the 5th European conference on speech communication and technology ( EuroSpeech'97 ) , Rhodes , Greece , pp 2667 - 2670 .Uchimoto K , Kotani K , Zhang Y , Isahara H ( 2007 )Automatic evaluation of machine translation based on rate of accomplishment of sub - goals .", "label": "", "metadata": {}, "score": "55.633396"}
{"text": "The evaluation year , source language , test set ( which , by default , is \" evalset \" ) , version of the data , and source vs. reference file ( with the latter being indicated by \" -ref \" ) are reflected in the file name .", "label": "", "metadata": {}, "score": "56.089417"}
{"text": "The script works by comparing the system output translation with a set of ( expert ) reference translations of the same source text .Comparison is based on finding sequences of words in the reference translations that match word sequences in the system output translation .", "label": "", "metadata": {}, "score": "56.25479"}
{"text": "Mach Learn 20(3 ) : 273 - 297 .Doddington G ( 2002 )Automatic evaluation of machine translation quality using n - gram co - occurrence statistics .In : Proceedings of the second conference on human language technology ( HLT-2002 ) , San Diego , California , pp 128 - 132 .", "label": "", "metadata": {}, "score": "56.283432"}
{"text": "On the other hand , the Pearson correlation coefficient uses the absolute scores yielded by the automatic MT evaluation systems as shown in Eq .22 , without the preconverting into rank values .IX .Last , this paper also makes a contribution on the complementary POS tagset mapping between German and English in the light of 12 universal tags .", "label": "", "metadata": {}, "score": "56.721264"}
{"text": "A machine learning approach to the automatic evaluation of machine translation .In : Association for Computational Linguistics , 39th annual meeting and 10th conference of the European chapter , proceedings of the conference , Toulouse , France , pp 148 - 155 .", "label": "", "metadata": {}, "score": "56.73046"}
{"text": "In this paper we first pre ... \" .Automatically produced texts ( e.g. translations or summaries ) are usually evaluated with n - gram based measures such as BLEU or ROUGE , while the wide set of more sophisticated measures that have been proposed in the last years remains largely ignored for practical purposes .", "label": "", "metadata": {}, "score": "56.818"}
{"text": "That is followed by de- scriptino of m - bleu and m - ter , enhanced ver- sions of two other widely used metrics bleu and ter , which extend the exact word match- ing used in these metrics with the flexible matching based on stemming and Wordnet in Meteor .", "label": "", "metadata": {}, "score": "56.905304"}
{"text": "CDER : efficient MT evaluation using block movements .In : EACL-2006 , 11th conference of the European chapter of the Association for Computational Linguistics , proceedings of the conference , Trento , Italy , pp 241 - 248 .Lin C - Y , Och FJ ( 2004a )", "label": "", "metadata": {}, "score": "56.97426"}
{"text": "C. van Rijsbergen .Information Retrieval .Butterworths .London , England .2nd Edition .Deborah Coughlin .Correlating Automated and Human Assessments of Machine Translation Quality .In Proceedings of MT Summit IX .New Orleans , LA .Sept. 2003 .", "label": "", "metadata": {}, "score": "57.102905"}
{"text": "We show that signican tly better correlations can be achieved by placing more weight on recall than on precision .While this may seem unexpected , since BLEU and NIST focus on n - gram precision and disregard recall , our experiments show that correlation with human judgments is highest when almost all of the weight is assigned to recall .", "label": "", "metadata": {}, "score": "57.764565"}
{"text": "Finally , we present an alternative formulation of metric training in which the features are based on comparisons against pseudo - references in order to reduce the demand on human produced resources .Our results confirm that regression is a useful approach for developing new metrics for MT evaluation at the sentence level .", "label": "", "metadata": {}, "score": "58.09603"}
{"text": "Five reference translations are available .System outputs consist of 1056 sentences .We obtai ... . \" ...Automatically produced texts ( e.g. translations or summaries ) are usually evaluated with n - gram based measures such as BLEU or ROUGE , while the wide set of more sophisticated measures that have been proposed in the last years remains largely ignored for practical purposes .", "label": "", "metadata": {}, "score": "58.407116"}
{"text": "We will introduce the sub - factors in the formula step by step .This approach is different with BLEU metric which assigns penalty for the shorter sentence compared with the human reference translation .Parameter and specify the length of candidate sentence ( hypothesis ) and source sentence respectively .", "label": "", "metadata": {}, "score": "58.46042"}
{"text": "Open source tool pp .441 - 450 .International Association for Machine Translation .Open source tool .Association for Computational Linguistics .Online paper pp .414 - 421 Abstract .Machine learning offers a systematic framework for developing metrics that use multiple criteria to assess the quality of machine translation ( MT ) .", "label": "", "metadata": {}, "score": "58.56048"}
{"text": "EXPERIMENTS A. Unsupervised Performances In the unsupervised MT evaluation , this paper uses the English - to - German machine translation corpora ( produced by around twenty English - to - German MT systems ) from UseReference ?Yes Yes Yes No", "label": "", "metadata": {}, "score": "58.635765"}
{"text": "The judges were presented with a segment , and asked to rate it for two variables , adequacy and fluency .Adequacy is a rating of how much information is transferred between the original and the translation , and fluency is a rating of how good the English is .", "label": "", "metadata": {}, "score": "59.025276"}
{"text": "The study showed that the variables were highly correlated when the human judgment was averaged per sentence .The variation among raters was small , but the researchers recommended that at the very least , three or four raters should be used .", "label": "", "metadata": {}, "score": "59.143265"}
{"text": "[ 1 ] The latest release ( 2012 ) [ 2 ] comprised up to 50 million words per language with the newly added languages being slightly underrepresented as data for them is only available from 2007 onwards .[ 1 ] .", "label": "", "metadata": {}, "score": "59.17334"}
{"text": "gz ) , with XML support , additional options and bug fixes , documentation , and example translations , may be downloaded from the NIST Multimodal Information Group Tools website .Data .This release contains 373 documents with corresponding sets of four separate human expert reference translations .", "label": "", "metadata": {}, "score": "59.20011"}
{"text": "Paraphrasing for automatic evaluation .In : HLT - NAACL 2006 Human language technology conference of the North American chapter of the Association for Computational Linguistics , New York , NY , pp 455 - 462 .Koehn P ( 2004 ) Statistical significance tests for machine translation evaluation .", "label": "", "metadata": {}, "score": "59.361015"}
{"text": "However , the quality panel evaluation was very difficult to set up logistically , as it necessitated having a number of experts together in one place for a week or more , and furthermore for them to reach consensus .This method was also abandoned .", "label": "", "metadata": {}, "score": "59.36963"}
{"text": "The metric is also includes a stemmer , which lemmatises words and matches on the lemmatised forms .The implementation of the metric is modular insofar as the algorithms that match words are implemented as modules , and new modules that implement different matching strategies may easily be added .", "label": "", "metadata": {}, "score": "59.45593"}
{"text": "Somers , H. , Gaspari , F. and Ana Ni\u00f1o ( 2006 ) \" Detecting Inappropriate Use of Free Online Machine Translation by Language Students - A Special Case of Plagiarism Detection \" .Proceedings of the 11th Annual Conference of the European Association of Machine Translation , Oslo University ( Norway ) pp .", "label": "", "metadata": {}, "score": "59.556004"}
{"text": "Unsupervised Quality Estimation Model for English to German Translation and Its Application in Extensive Supervised Evaluation Aaron L.-F. The conventional MT evaluation methods tend to calculate the similarity between hypothesis translations offered by automatic translation systems and reference translations offered by professional translators .", "label": "", "metadata": {}, "score": "59.70277"}
{"text": "Five reference translations are available .System outputs consist of 1056 sentences .We obtai ... . \" ...We present a large parallel corpus of texts published by the United Nations Organization , which we exploit for the creation of phrasebased statistical machine translation ( SMT ) systems for new language pairs .", "label": "", "metadata": {}, "score": "59.914223"}
{"text": "As a contrast and simplified version , Zhang et al .[ 9 ] proposes modified BLEU metric using the arithmetic mean of the n - gram precision .B. TER Metric TER [ 7 ] means translation edit rate , which is designed at sentence - level to calculate the amount of work needed to correct the hypothesis translation according to the closest reference translation ( assuming there are several reference translations ) .", "label": "", "metadata": {}, "score": "60.782997"}
{"text": "Combining different metrics into a single measure of quality seems the most direct and natural way to improve over the quality of individual metrics .Recently , several approaches have been suggested ( Kulesza and . ... nation schemes presented in Section 3 in the context of four different evaluation scenarios .", "label": "", "metadata": {}, "score": "60.80107"}
{"text": "Software .Rampion .Rampion ( Gimpel and Smith , 2012 ) is an algorithm for training statistical machine translation models based on minimizing structured ramp loss .The code provided here can be used with the Moses decoder or any other decoder that supports the same formats for configuration files and k - best lists .", "label": "", "metadata": {}, "score": "61.09498"}
{"text": "In the second example , the text translated back into English is perfect , but the Portuguese translation is meaningless .While round - trip translation may be useful to generate a \" surplus of fun , \" [ 2 ] the methodology is deficient for serious study of machine translation quality .", "label": "", "metadata": {}, "score": "61.134125"}
{"text": "Rich source - side context for statistical machine translation .In : ACL-08 : HLT third workshop on statistical machine translation , Columbus , Ohio , pp 9 - 17 .Goldberg Y , Elhadad M ( 2007 )SVM model tampering and anchored learning : a case study in Hebrew NP chunking .", "label": "", "metadata": {}, "score": "61.39258"}
{"text": "The objective of the NIST OpenMT evaluation series is to support research in , and help advance the state of the art of , machine translation ( MT ) technologies -- technologies that translate text between human languages .Input may include all forms of text .", "label": "", "metadata": {}, "score": "62.239555"}
{"text": "While this does not allow us to simultaneously match different portions of the translation with different references , it supports the use of recall as a component in scoring each possible match .In the second case , we stem both translation and references prior to matching and then require identity on stems .", "label": "", "metadata": {}, "score": "62.42499"}
{"text": "In : HLT - NAACL 2003 : conference combining human language technology conference series and the North American chapter of the Association for Computational Linguistics series , companion volume , Edmonton , Canada , pp 61 - 63 .Owczarzak K , Groves D , Van Genabith J , Way A ( 2006 )", "label": "", "metadata": {}, "score": "62.49517"}
{"text": "pp .433- 439 . Y. Akiba , K. Imamura , and E. Sumita .Using Multiple Edit Distances to Automatically Rank Machine Translation Output .In Proceedings of MT Summit VIII .Santiago de Compostela , Spain .pp .15 - 20 . S. Niessen , F. J. Och , G. Leusch , and H. Ney .", "label": "", "metadata": {}, "score": "62.549038"}
{"text": "KOUS PTKA VAFIN FM $ ( NN PDS PTKANT VAIMP ITJ $ , NNE PIAT PTKNEG VAINF TRUNC $ .The issues between the traditional supervised MT evaluations and the latest unsupervised MT evaluations are discussed in the work of [ 21].", "label": "", "metadata": {}, "score": "62.716476"}
{"text": "corpora of sufficient size are so far not available .We give some preliminary results for this novel application of SMT and discuss further refinements .NIST Multimodal Information Group .NIST 2004 Open Machine Translation ( OpenMT ) Evaluation LDC2010T12 .", "label": "", "metadata": {}, "score": "62.749466"}
{"text": "In : ACL 2007 Proceedings of the 45th annual meeting of the Association for Computational Linguistics , Prague , Czech Republic , pp 296 - 303 .Al - Onaizan Y , Curin J , Jahr M , Knight K , Lafferty J , Melamed ID , Och F - J , Purdy D , Smith NA , Yarowsky D ( 1999 ) Statistical machine translation .", "label": "", "metadata": {}, "score": "62.995583"}
{"text": "This paper employs the -gram method into the matching process , which means that the potential POS candidate will be assigned higher priority if it has neighbor matching .The nearest matching will be accepted as a backup choice if there are both neighbor matching or there is no other matched POS around the potential pairs .", "label": "", "metadata": {}, "score": "63.074318"}
{"text": "Joachims T ( 1999 )Making large - scale SVM learning practical .In : Sch\u00f6elkopf B , Burges C , Smola A(eds ) Advances in kernel methods - support vector learning .MIT Press , Cambridge , pp 169 - 184 .", "label": "", "metadata": {}, "score": "63.417294"}
{"text": "DARPA TIDES MT and NIST OpenMT evaluations used SGML - formatted test data until 2008 and XML - formatted test data thereafter .This files in this package are provided in both formats .Updates .Additional information , updates , bug fixes may be available in the LDC catalog entry for this corpus at LDC2010T23 .", "label": "", "metadata": {}, "score": "63.61344"}
{"text": "Johns Hopkins University , Baltimore .Banerjee S , Lavie A ( 2005 )METEOR : an automatic metric for MT evaluation with improved correlation with human judgments .In : Proceedings of the workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization , Ann Arbor , Michigan , pp 65 - 72 .", "label": "", "metadata": {}, "score": "63.816456"}
{"text": "This study and its results may help to shed some light on the problem and to develop new methods to improve Arabic verb noun collocability in the output translation of current machine translation engines .\" ORdering ) is an automatic evaluation metric for the machine translation output .", "label": "", "metadata": {}, "score": "63.925087"}
{"text": "[ 1 ] Results reflect that some SMT systems perform better than others , e.g. , Spanish - French ( 40.2 ) in comparison to Dutch - Finnish ( 10.3 ) .[ 1 ] Koehn states that the reason for this is that related languages are easier to translate into each other than those that are not .", "label": "", "metadata": {}, "score": "64.125336"}
{"text": "63 - 70 .Bradley Efron and Robert Tibshirani .Bootstrap Methods for Standard Er- rors , Confidence Intervals , and Other Measures of Statistical Accuracy .Statistical Science , 1(1 ) .pp .54 - 77 .George Doddington .", "label": "", "metadata": {}, "score": "64.41307"}
{"text": "New Orleans , LA .Sept. 2003 .pp .240 - 247 .I. Dan Melamed , R. Green and J. Turian .Precision and Recall of Machine Translation .In Proceedings of HLT - NAACL 2003 .Edmonton , Canada .", "label": "", "metadata": {}, "score": "64.51851"}
{"text": "Fidelity was a measure of how much information the translated sentence retained compared to the original , and was measured on a scale of 0 - 9 .Each point on the scale was associated with a textual description .For example , 3 on the intelligibility scale was described as \" Generally unintelligible ; it tends to read like nonsense but , with a considerable amount of reflection and study , one can at least hypothesize the idea intended by the sentence \" .", "label": "", "metadata": {}, "score": "64.640755"}
{"text": "Secondly , it is designed to combine the performances on words and POS together , and the final score is the combination of them .( 23 ) where is the harmonic mean of precision and recall as mentioned above .( 24 )", "label": "", "metadata": {}, "score": "64.714615"}
{"text": ", 2004 ; Banerjee and Lavie , 2005 ; Lavie and Agarwal , 2007 ) have described the details underlying the metric and have extensively compared its performance with Bleu and several other MT evaluation metrics .\" [ Show abstract ] [ Hide abstract ] ABSTRACT : We describe our submission to the NIST Met- rics for Machine Translation Challenge consist- ing of 4 metrics - two versions of meteor , m - bleu and m - ter .", "label": "", "metadata": {}, "score": "64.8588"}
{"text": "The document - level \u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305 is calculated as the arithmetic mean value of each sentence - level score in the document .On the other hand , the document - level \u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305 is calculated as the product of three document - level variables , and the document - level variable value is the corresponding arithmetic mean score of each sentence .", "label": "", "metadata": {}, "score": "64.90303"}
{"text": "The texts chosen were a set of articles in English on the subject of financial news .These articles were translated by professional translators into a series of language pairs , and then translated back into English using the machine translation systems .", "label": "", "metadata": {}, "score": "64.92619"}
{"text": "To address this problem , they proposed the Human - targeted TER ( HTER ) to consider the semantic equivalence , which is achieved by employing human annotators to generate a new targeted reference .However , HTER is very expensive due to that it requires around 3 to 7 minutes per sentence for a human to annotate , which means that it is more like a human judgment metric instead of an automatic one . D. AMBER Metric AMBER [ 8 ] declares a modified version of BLEU .", "label": "", "metadata": {}, "score": "65.02005"}
{"text": "Human assessments of adequacy and fluency , on a 1 - 5 scale , are available for ... . \" ...Combining different metrics into a single measure of quality seems the most direct and natural way to improve over the quality of individual metrics .", "label": "", "metadata": {}, "score": "65.0408"}
{"text": "METEOR scores machine translation hypotheses by aligning them to one or more reference translations . \" \" meteor , initially proposed and released in 2004 ( Lavie et al ., 2004 ) was explicitly designed to improve correlation with human judgments of MT quality at the segment level .", "label": "", "metadata": {}, "score": "65.46124"}
{"text": "The edit categories include the insertion , deletion , substitution of single words and the shifts of word chunks .TER uses the strict matching of words and word order , e.g. mis - capitalization is also counted as an edit .", "label": "", "metadata": {}, "score": "65.60753"}
{"text": "TABLE III .Yes Yes Yes No No 0.25 0.22 0.18 0.34 0.33 Testing result of the proposed evaluation approaches on the WMT2012 corpora is shown in Table 3 with the same parameter values obtained as in training , also compared with the state - of - the - art evaluation metrics .", "label": "", "metadata": {}, "score": "65.78684"}
{"text": "Figures for correlation at the sentence level are rarely reported , although Banerjee et al .( 2005 ) do give correlation figures that show that , at least for their metric , sentence level correlation is substantially worse than corpus level correlation .", "label": "", "metadata": {}, "score": "65.79711"}
{"text": "Employing the confidence estimation features and a learning mechanism trained on human .TABLE I. COMPLEMENTARY GERMAN POS MAPPINGS FOR UNIVERSAL POS TAGSET Language ADJ ADP ADV CONJ DET NOUN NUM PRON PRT VERB X .English JJ IN RB CC DT NN CD PRP POS MD FW # Penn Treebank JJR RBR EX NNP PRP$ RP VB LS $ [ 28 ] JJS RBS PDT NNPS WP TO VBD SYM \" WRB WDT NNS WP$ VBG UH .", "label": "", "metadata": {}, "score": "65.81119"}
{"text": "BLEU and NIST .[ 12 ] .METEOR also includes some other features not found in other metrics , such as synonymy matching , where instead of matching only on the exact word form , the metric also matches on synonyms .", "label": "", "metadata": {}, "score": "66.20516"}
{"text": "He uses the corpus to develop SMT systems translating each language into each of the other ten languages of the corpus making it 110 systems .This enables Koehn to establish SMT systems for uncommon language pairs that have not been considered by SMT developers beforehand , such as Finnish - Italian for example .", "label": "", "metadata": {}, "score": "66.5457"}
{"text": "Automatically produced texts ( e.g. translations or summaries ) are usually evaluated with n - gram based measures such as BLEU or ROUGE , while the wide set of more sophisticated measures that have been proposed in the last years remains largely ignored for practical purposes .", "label": "", "metadata": {}, "score": "66.93141"}
{"text": "Google scored a verb - noun collocation value of 0.75 ( 3 % higher than Bing ) with a trend estimation ranging between 0.63 and 0.85 .The results also showed that , in most cases , the Arabic translation output of both engines produced a one verb synonym which did not collocate with the different nouns in the testing data sentences .", "label": "", "metadata": {}, "score": "67.30371"}
{"text": "This is due to the fact that METEOR employs many external materials including stemming , synonyms vocabulary , and paraphrasing resources , etc .However , to make the evaluation model concise , our method nLEPOR only uses the surface words and hLEPOR only uses the combination of surface words and POS sequences .", "label": "", "metadata": {}, "score": "67.67728"}
{"text": "The parameter is designed to adjust the weights of different n - gram performances such as unigram , bigram , trigram , four - gram , etc . , which is different with the weight assignment in BLEU where each weight is equal as 1/N. In our model , higher weight value is designed for the high level n - gram .", "label": "", "metadata": {}, "score": "68.03676"}
{"text": "NIST 2009 Open Machine Translation ( OpenMT ) Evaluation LDC2010T23 .Web Download .Philadelphia : Linguistic Data Consortium , 2010 .Introduction .NIST 2009 Open Machine Translation ( OpenMT ) Evaluation , Linguistic Data Consortium ( LDC ) catalog number LDC2010T23 and isbn 1 - 58563 - 570 - 7 , is a package containing source data , reference translations and scoring software used in the NIST 2009 OpenMT evaluation .", "label": "", "metadata": {}, "score": "68.20208"}
{"text": "They serve to illustrate the effectiveness of stemming in MT evaluation .Page 10 .Acknowledgments This research was funded in part by NSF grant number IIS-0121631 .References 1 .Papineni , Kishore , Salim Roukos , Todd Ward , and Wei - Jing Zhu .", "label": "", "metadata": {}, "score": "68.86717"}
{"text": "Firstly , the designed incomprehensive factors result in language - bias problem , which means they perform well on some special language pairs but weak on other language pairs .Secondly , they tend to use no linguistic features or too many linguistic features , of which no usage of linguistic feature draws a lot of criticism from the linguists and too many linguistic features make the model weak in repeatability .", "label": "", "metadata": {}, "score": "69.0412"}
{"text": "Data .This corpus consists of 150 Arabic newswire documents , 150 Chinese newswire documents , and 29 Chinese \" prepared speech \" documents , and a corresponding set of four separate human expert reference translations .For each language , the test set consists of two files : a source and a reference file .", "label": "", "metadata": {}, "score": "69.48274"}
{"text": "Impersonations , Chinese Whispers and Fun with Machine Translation on the Internet \" in Proceedings of the 11th Annual Conference of the European Association of Machine Translation .Graham , Y. and T. Baldwin .( 2014 ) \" Testing for Significance of Increased Correlation with Human Judgment \" .", "label": "", "metadata": {}, "score": "70.16454"}
{"text": "Secondly , the Pearson correlation coefficient information is introduced as below .Given a sample of paired data ( X , Y ) as , , the Pearson correlation coefficient is : ( 22 ) ( ) where and specify the arithmetical means of discrete random variable X and Y respectively .", "label": "", "metadata": {}, "score": "70.214836"}
{"text": "After all of the complementary mapping , the universal POS tagset alignment for German Negra Treebank is shown in Table 1 with the boldface POS as the added ones .B. Calculation Algorithms The designed calculation algorithms of this paper are LEPOR series .", "label": "", "metadata": {}, "score": "70.32555"}
{"text": "METEOR : an automatic metric for MT evaluation with high levels of correlation with human judgments .In : ACL 2007 : Proceedings of the second workshop on statistical machine translation , Prague , Czech Republic , pp 228 - 231 .", "label": "", "metadata": {}, "score": "70.792885"}
{"text": "The concept of METEOR is rather different from that of the above metrics in that all the other metrics relied on unigram precision ( the two SL and TL identical word strings ) only , while METEOR gives more weight to unigram precision and unigram recall .", "label": "", "metadata": {}, "score": "71.23211"}
{"text": "[ 11 ] For example , if the bigram \" on the \" correctly matches , it receives lower weight than the correct matching of bigram \" interesting calculations , \" as this is less likely to occur .NIST also differs from BLEU in its calculation of the brevity penalty , insofar as small variations in translation length do not impact the overall score as much .", "label": "", "metadata": {}, "score": "71.59622"}
{"text": "The METEOR metric is designed to address some of the deficiencies inherent in the BLEU metric .The metric is based on the weighted harmonic mean of unigram precision and unigram recall .The metric was designed after research by Lavie ( 2004 ) into the significance of recall in evaluation metrics .", "label": "", "metadata": {}, "score": "71.86554"}
{"text": "Universal POS chunk matching example for bigram precision and recall .The variable means n - gram Position difference Penalty that is designed for the different order of successful matched POS in source and hypothesis sentence .The position difference factor has been proved to be helpful for the MT evaluation in the research work of [ 13].", "label": "", "metadata": {}, "score": "72.06983"}
{"text": "The commonly used automatic evaluation metrics include BLEU [ 5 ] , METEOR [ 6 ] , TER [ 7 ] and AMBER [ 8 ] , etc .However , most of the automatic MT evaluation metrics are referenceaware , which means they tend to employ different approaches to calculate the closeness between the hypothesis translations offered by MT systems and the reference translations provided by professional translators .", "label": "", "metadata": {}, "score": "72.59972"}
{"text": "IV .DESIGNED APPROACH To reduce the expensive reference translations provided by human labor and some external resources such as synonyms , this work employs the universal part - of - speech ( POS ) tagset containing 12 universal tags proposed by [ 27].", "label": "", "metadata": {}, "score": "73.30053"}
{"text": "Lita LV , Rogati M , Lavie A ( 2005 ) Blanc : learning evaluation metrics for MT .In : HLT / EMNLP 2005 Human language technology conference and conference on empirical methods in natural language processing , Vancouver , British Columbia , Canada , pp 740 - 747 .", "label": "", "metadata": {}, "score": "74.966774"}
{"text": "Selezioni questo collegamento per guardare il nostro Home Page .Translated back .Selections this connection in order to watch our Home Page .Original text .Tit for tat .Translated .Melharuco para o tat .Translated back .Tit for tat .", "label": "", "metadata": {}, "score": "75.77166"}
{"text": "In this section we will introduce an enhanced version of the proposed metric , which is called as hLEPOR ( harmonic mean of enhanced Length Penalty , Precision , n - gram Position difference Penalty and Recall ) .There are two contributions of this enhanced model .", "label": "", "metadata": {}, "score": "75.93553"}
{"text": "[ 13 ] An enhanced version of LEPOR metric , hLEPOR , is introduced in the paper .[14 ] hLEPOR utilizes the harmonic mean to combine the sub - factors of the designed metric .Furthermore , they design a set of parameters to tune the weights of the sub - factors according to different language pairs .", "label": "", "metadata": {}, "score": "76.51466"}
{"text": "It provides eight kinds of preparations on the corpus including whether the words are tokenized or not , extracting the stem , prefix and suffix on the words , and splitting the words into several parts with different ratios .Advanced version of AMBER was introduced in [ 11].", "label": "", "metadata": {}, "score": "76.59859"}
{"text": "ALPAC ( 1966 ) \" Languages and machines : computers in translation and linguistics \" .A report by the Automatic Language Processing Advisory Committee , Division of Behavioral Sciences , National Academy of Sciences , National Research Council .Washington , D.C. : National Academy of Sciences , National Research Council , 1966 .", "label": "", "metadata": {}, "score": "77.19638"}
{"text": "So the value of and equals 3/4 and 3/5 respectively .( 13 ) ( 17 ) ( 14 )The parameter means the length of hypothesis sentence , and as the matched POS position number in hypothesis and source sentence respectively .", "label": "", "metadata": {}, "score": "77.51071"}
{"text": "APPENDIX This appendix offers the POS tags and their occurrences number in one of WMT2012 German document containing 3,003 sentences , parsed by Berkeley parser for German language that is trained on German Negra Treebank .Number of different POS tags in the document is : 55 .", "label": "", "metadata": {}, "score": "78.73804"}
{"text": "ADV ( adverb ) category in the 12 universal POS tags .The German POS \" PIDAT \" has a similar function with English POS \" PDT \" which means Predeterminer labeling the German word jedem ( each ) , beide ( both ) , meisten ( most ) , etc .", "label": "", "metadata": {}, "score": "79.02374"}
{"text": "Generally , is selected as 4 , and uniform weight is assigned as .It puts more weight on recall ( R ) compared with precision ( P ) .The matching process involves computationally expensive word alignment due to the external tools for stemming or synonym matches .", "label": "", "metadata": {}, "score": "80.35832"}
{"text": "VIII .DISCUSSION The Spearman rank correlation coefficient is commonly used in the WMT shared tasks as the special case of Pearson correlation coefficient applied to ranks .However , there is some information that can not be reflected by using Spearman rank correlation coefficient instead of Pearson correlation coefficient .", "label": "", "metadata": {}, "score": "80.87853"}
{"text": "In :ACL-04 : 42nd annual meeting of the Association for Computational Linguistics , Barcelona , Spain , pp 605 - 612 .Lin C - Y , Och FJ ( 2004b )ORANGE : a method for evaluating automatic evaluation metrics for machine translation .", "label": "", "metadata": {}, "score": "81.26529"}
{"text": "This page is the home for machine translation research conducted by members of Noah 's ARK in the Language Technologies Institute at Carnegie Mellon University .Machine translation is an active research area that offers great promise to revolutionize the way we communicate .", "label": "", "metadata": {}, "score": "81.34461"}
{"text": "The files in this package are provided in both formats .Sample .Sample text file containing excerpts from different xml files included in this corpus , including reference translations and source text for a single newswire document .The file is encoded in UTF-8 .", "label": "", "metadata": {}, "score": "82.2858"}
{"text": "Secondly , the experiments using multireferences will be considered .Thirdly , how to handle the MT evaluation from the aspect of semantic similarity will be further explored .X. POS POS Frequency 213 PIS 824 $ , 4590 PPER 1827 $ .", "label": "", "metadata": {}, "score": "83.937996"}
{"text": "The source language is used as pseudo reference .We will also test this method by calculating the correlation coefficient of this approach with the human judgments in the experiment .Petrov et al .[ 27 ] describe that the English PennTreebank [ 28 ] has 45 tags and German Negra [ 29 ] has 54 tags .", "label": "", "metadata": {}, "score": "84.33469"}
{"text": "Edmonton , Canada .May 2003 .pp .102 - 109 .Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .", "label": "", "metadata": {}, "score": "85.082214"}
{"text": "So \" PWS \" is classified into the PRON ( pronoun ) category in the universal POS tags .The German POS \" PRF \" has a similar function with English POS \" RP \" and \" TO \" , which means Particle and to respectively , labeling the German word sich ( itself ) .", "label": "", "metadata": {}, "score": "85.29585"}
{"text": "It is generally defined by the syntactic or morphological behavior of the lexical item in question .For a simple example , \" there is a big bag \" and \" there is a large bag \" could be the same expression , \" big \" and \" large \" having the same POS as adjective .", "label": "", "metadata": {}, "score": "88.818954"}
{"text": "So , firstly this paper conducts a complementary mapping work for German Negra POS tagset and extends the mapped POS tags to 57 tags .This paper classifies the omissive German POS tags according to the English POS tagset classification since that the English PennTreebank 45 POS tags are completely mapped by the universal POS tagset , as shown in Table 1 .", "label": "", "metadata": {}, "score": "89.25966"}
{"text": "There are no updates available at this time .Copyright .Europarl corpus .The Europarl Corpus is a corpus ( set of documents ) that consists of the proceedings of the European Parliament from 1996 to the present .In its first release in 2001 , it covered eleven official languages of the European Union ( Danish , Dutch , English , Finnish , French , German , Greek , Italian , Portuguese , Spanish , and Swedish ) .", "label": "", "metadata": {}, "score": "89.29618"}
{"text": "In the training period , the parameter values of ( weight on recall ) and ( weight on precision ) are tuned to 1 and 9 respectively , which is different with the reference - aware metric METEOR ( more weight on recall ) .", "label": "", "metadata": {}, "score": "93.32457"}
{"text": "The German POS \" PROAV \" has a similar function with English POS \" RB \" which means Adverb labeling the German word Dadurch ( thereby ) , dabei ( there ) , etc .So this paper classifies \" PWAV \" and \" PROAV \" into the .", "label": "", "metadata": {}, "score": "97.50686"}
{"text": "Copyright .", "label": "", "metadata": {}, "score": "121.95813"}
