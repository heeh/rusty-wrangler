{"text": "This algorithm can also be deployed in batch - mode learning using standard online - to - batch conversion techniques , and it has comparable empirical performance to state - of - the - art learning techniques such as boosting .In some embodiments , the perceptron algorithm is the averaged perceptron .", "label": "", "metadata": {}, "score": "28.826096"}
{"text": "Indeed , if we had the prior constraint that the data come from equi - variant Gaussian distributions , the linear separation in the input space is optimal .Although the perceptron initially seemed promising , it was eventually proved that perceptrons could not be trained to recognise many classes of patterns .", "label": "", "metadata": {}, "score": "30.595043"}
{"text": "This approach , based on best - first truncated tree induction , often leads to better performance , and can provide interpretable descriptions of the aggregate decision rule .It is also much faster computationally , making it more suitable to large - scale data mining applications .", "label": "", "metadata": {}, "score": "30.95753"}
{"text": "In Machine Learning : Proceedings of the Thirteenth International Conference 148 - 156 .Morgan Kaufman , San Francisco . in a paper by Amit and Geman ( 1997 ) .Using this approach and 100 iterations gives the following test - set errors as compared to the best corresponding values for LogitBoost .", "label": "", "metadata": {}, "score": "32.757877"}
{"text": "This text was reprinted in 1987 as \" Perceptrons - Expanded Edition \" where some errors in the original text are shown and corrected .More recently , interest in the perceptron learning algorithm has increased again after Freund and Schapire ( 1998 ) presented a voted formulation of the original algorithm ( attaining large margin ) and suggested that one can apply the kernel trick to it .", "label": "", "metadata": {}, "score": "33.255955"}
{"text": "A linear classifier can then separate the data , as shown in the third figure .However the data may still not be completely separable in this space , in which the perceptron algorithm would not converge .In the example shown , stochastic steepest gradient descent was used to adapt the parameters .", "label": "", "metadata": {}, "score": "33.94241"}
{"text": "Most of the performance bounds for on - line algorithms in this framework assume a constant learning rate .To achieve these bounds the learning rate must be optimized based on a posteriori information .This information depends on the wh ... \" .", "label": "", "metadata": {}, "score": "34.270218"}
{"text": "Most of the performance bounds for on - line algorithms in this framework assume a constant learning rate .To achieve these bounds the learning rate must be optimized based on a posteriori information .This information depends on the whole sequence of examples and thus it is not available to any strictly on - line algorithm .", "label": "", "metadata": {}, "score": "36.07454"}
{"text": "Learning algorithm .The learning algorithm is the same across all neurons , therefore everything that follows is applied to a single neuron in isolation .We first define some variables : .y .i .w .x .i .", "label": "", "metadata": {}, "score": "37.220673"}
{"text": "At each iteration of the training process , a weight is assigned to each sample in the training set equal to the current error on that sample .These weights can be used to inform the training of the weak learner , for instance , decision trees can be grown that favor splitting sets of samples with high weights .", "label": "", "metadata": {}, "score": "37.57201"}
{"text": "Third , the averaged perceptron is a discriminative classifier with strong theoretical performance guarantees .Table 1 shows the averaged perceptron training algorithm for learning the parameters ( weights ) \u03b1 avg .Freund and Schapire have proved several theoretical properties of the algorithm , including the fact that the expected number of mistakes made by a classifier trained using the algorithm does not depend on the weight vector dimensionality .", "label": "", "metadata": {}, "score": "37.636723"}
{"text": "We present a simple algorithm for playing a repeated game .We show that a player using this algorithm suffers average loss that is guaranteed to come close to the minimum loss achievable by any fixed strategy .Our bounds are nonasymptotic and hold for any opponent .", "label": "", "metadata": {}, "score": "37.81435"}
{"text": "We present a simple algorithm for playing a repeated game .We show that a player using this algorithm suffers average loss that is guaranteed to come close to the minimum loss achievable by any fixed strategy .Our bounds are nonasymptotic and hold for any opponent .", "label": "", "metadata": {}, "score": "37.81435"}
{"text": "The original ensemble method is Bayesian averaging , but more recent algorithms include error - correcting output coding , Bagging , and boostin ... \" .Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a ( weighted ) vote of their predictions .", "label": "", "metadata": {}, "score": "37.89047"}
{"text": "Machine Learning 11 63 - 90 .proposed by Freund and Mason ( 1999 ) .They represent decision trees as sums of very simple functions and use boosting to simultaneously learn both the decision rules and the way to average them .", "label": "", "metadata": {}, "score": "37.960896"}
{"text": "This method has since been generalized , with a formula provided for choosing optimal thresholds at each stage to achieve some desired false positive and false negative rate .[ 7 ] .In the field of statistics , where AdaBoost is more commonly applied to problems of moderate dimensionality , early stopping is used as a strategy to reduce overfitting .", "label": "", "metadata": {}, "score": "38.106586"}
{"text": "Direct multiclass generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multiclass generalizations of boosting in most situations , and far superior in some .We suggest a minor modification to boosting that can reduce computation , often by factors of 10 to 50 .", "label": "", "metadata": {}, "score": "38.4627"}
{"text": "This general approach has been studied previously by Ali and Pazzani and by Dietterich and Kong .This paper compares the effectiveness of randomization , bagging , and boosting for improving the performance of the decision - tree algorithm C4.5 .The experiments show that in situations with little or no classification noise , randomization is competitive with ( and perhaps slightly superior to ) bagging but not as accurate as boosting .", "label": "", "metadata": {}, "score": "39.0711"}
{"text": "Our techniques allow us to prove essentially the same bounds as if we knew the optimal learning rate in advance .Moreover , such techniques apply to a wide class of on - line algorithms , including p - norm algorithms for generalized linear regression and Weighted Majority for linear regression with absolute loss .", "label": "", "metadata": {}, "score": "39.355373"}
{"text": "We have developed a framework for deriving and analysing simple on - line learning algorithms .A learning problem is given by a parameterized class of predictors , a divergence measure in the labelling domain ( loss function ) and a divergence measure in the parameter domain .", "label": "", "metadata": {}, "score": "39.453224"}
{"text": "A more recent tutorial ( July 25 , Dagstuhl , Germany ) that contrasts the two main families of algorithms : the geometric and information theoretic algorithms .The first one is motivated by using the squared Eucledian distance as the parameter divergence and the second family by the relative entropy .", "label": "", "metadata": {}, "score": "39.725044"}
{"text": "A more recent tutorial ( July 25 , Dagstuhl , Germany ) that contrasts the two main families of algorithms : the geometric and information theoretic algorithms .The first one is motivated by using the squared Eucledian distance as the parameter divergence and the second family by the relative entropy .", "label": "", "metadata": {}, "score": "39.725044"}
{"text": "Freund and Schapire have shown how a different function of the margin distribution can be used to bound the number of mistakes of an on - line learning algorithm for a perceptron , as well as an expected error bound .We show that a slight generalization of their construction can be used to give a pac style bound on the tail of the distribution of the generalization errors that arise from a given sample size .", "label": "", "metadata": {}, "score": "39.948875"}
{"text": "Wadsworth , Belmont , CA .Dietterich , T. ( 1998 ) .An experimental comparison of three methods for constructing ensembles of decision trees : bagging , boosting , and randomization .Machine Learning ?Freund , Y. ( 1995 ) .", "label": "", "metadata": {}, "score": "39.95046"}
{"text": "Previous research has shown that an ensemble is often more accurate than any of the single classifiers in the ensemble .Baggi ... \" .An ensemble consists of a set of individually trained classifiers ( such as neural networks or decision trees ) whose predictions are combined when classifying novel instances .", "label": "", "metadata": {}, "score": "40.26633"}
{"text": "Its goal is to predict almost as well as the best sequence of such experts chosen off - line by partit ... \" .In this paper , we examine on - line learning problems in which the target concept is allowed to change over time .", "label": "", "metadata": {}, "score": "40.383915"}
{"text": "We show how to transform algorithms that assume that all experts are always awake to algorithms that do not require this assumption .We also show how to derive corresponding loss bounds .Our method is very general , and can be applied to a large family of online learning algorithms .", "label": "", "metadata": {}, "score": "40.762527"}
{"text": "Novikoff ( 1962 ) proved that the perceptron algorithm converges after a finite number of iterations if the data set is linearly separable and the number of mistakes is bounded by .R .Variants .The pocket algorithm with ratchet ( Gallant , 1990 ) solves the stability problem of perceptron learning by keeping the best solution seen so far \" in its pocket \" .", "label": "", "metadata": {}, "score": "40.9873"}
{"text": "Its goal is to predict almost as well as the best sequence of such experts chosen off - line by partitioning the training sequence into k + 1 sections and then choosing the best expert for each section .We build on methods developed by Herbster and Warmuth and consider an open problem posed by Freund where the experts in the best partition are from a small pool of size m. We propose algorithms that solve this open problem by mixing the past posteriors maintained by the master algorithm .", "label": "", "metadata": {}, "score": "41.17067"}
{"text": "The .-perceptron further utilised a preprocessing layer of fixed random weights , with thresholded output units .This enabled the perceptron to classify analogue patterns , by projecting them into a binary space .In fact , for a projection space of sufficiently high dimension , patterns can become linearly separable .", "label": "", "metadata": {}, "score": "41.915676"}
{"text": "In such cases the choice of weak learner and coefficient can be condensed to a single step in which is chosen from all possible as the minimizer of by some numerical searching routine .The output of decision trees is a class probability estimate , the probability that is in the positive class .", "label": "", "metadata": {}, "score": "42.643898"}
{"text": "Abstract .Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest .The generalization error for forests converges a.s . to a limit as the number of trees in the forest becomes large .", "label": "", "metadata": {}, "score": "42.68071"}
{"text": "We also introduce a notion of \" matching loss \" associated with any increasing transfer function .When this loss is used the the algorithms can be analysed .\" Relative Loss Bounds for Single Neurons , \" IEEE Transactions on Neural Networks , Vol .", "label": "", "metadata": {}, "score": "42.689835"}
{"text": "We also introduce a notion of \" matching loss \" associated with any increasing transfer function .When this loss is used the the algorithms can be analysed .\" Relative Loss Bounds for Single Neurons , \" IEEE Transactions on Neural Networks , Vol .", "label": "", "metadata": {}, "score": "42.689835"}
{"text": "Hastie , T. and Tibshirani , R. ( 1998 ) .Bayesian backfitting .Technical report , Stanford Univ . .Heikkinen , J. ( 1998 ) .Curve and surface estimation using dynamic step functions .In Practical Nonparametric and Semiparametric Bayesian Statistics ( D. Dey , P. M \u00a8uller and D. Sinha , eds . )", "label": "", "metadata": {}, "score": "42.881996"}
{"text": "In the following paper we prove relative expected loss bounds for the last trial .We use the leave - one - out loss to do this .\" Relative Expected Instantaneous Loss Bounds , \" in a special issue of Journal of Computer and Systems Sciences for COLT00 , Vol . 64 - 1 , pp .", "label": "", "metadata": {}, "score": "42.992744"}
{"text": "In the following paper we prove relative expected loss bounds for the last trial .We use the leave - one - out loss to do this .\" Relative Expected Instantaneous Loss Bounds , \" in a special issue of Journal of Computer and Systems Sciences for COLT00 , Vol . 64 - 1 , pp .", "label": "", "metadata": {}, "score": "42.992744"}
{"text": "Polikar , R. ( 2006 ) .\" Ensemble Based Systems in Decision Making \" ( PDF ) .IEEE Circuits and Systems Magazine 6 ( 3 ) : 21 - 45 : a tutorial article on ensemble systems including pseudocode , block diagrams and implementation issues for AdaBoost and other ensemble learning algorithms . is the ' bias ' , a constant term that does not depend on any input value .", "label": "", "metadata": {}, "score": "43.039253"}
{"text": "The paper concludes with illustrations of current research directions . \" ...Abstract .Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest .", "label": "", "metadata": {}, "score": "43.04241"}
{"text": "They conjectured ( incorrectly ) that a similar result would hold for a perceptron with three or more layers .Three years later Stephen Grossberg published a series of papers introducing networks capable of modelling differential , contrast - enhancing and XOR functions .", "label": "", "metadata": {}, "score": "43.832153"}
{"text": ".. voting the decisions of the individual classifiers in the ensemble .Two of the most popular techniques for constructing ensembles are boostrap aggregation ( \" bagging \" ; Breiman , 1996a ) and the Adaboost family of algorithms ( \" boosting \" ; Freund & ... . \" ...", "label": "", "metadata": {}, "score": "43.956184"}
{"text": "In fact , further results show that Boosting ensembles may overfit noisy data sets , thus decreasing its performance .Finally , consistent with previous studies , our work suggests that most of the gain in an ensemble 's performance comes in the first few classifiers combined ; however , relatively large gains can be seen up to 25 classifiers when Boosting decision trees . tworks .", "label": "", "metadata": {}, "score": "44.025906"}
{"text": "An older paper for finding a distribution w.r.t . equality constraints .Among all distributions that satisfy the constraints , the one of maximum entropy ( i.e. min .relative entropy to the uniform distribution ) is targeted .This amounts to a projection onto the constraints where the divergence function is the relative entropy .", "label": "", "metadata": {}, "score": "44.498436"}
{"text": "An older paper for finding a distribution w.r.t . equality constraints .Among all distributions that satisfy the constraints , the one of maximum entropy ( i.e. min .relative entropy to the uniform distribution ) is targeted .This amounts to a projection onto the constraints where the divergence function is the relative entropy .", "label": "", "metadata": {}, "score": "44.498436"}
{"text": "The first paper in which we derive learning algorithms in a simple framework trading off the divergence in the parameter domain with the divergence in the labelling domain .The focus is on linear regression .We analyse the gradient descent algorithm ( Widrow Hoff ) and a corresponding algorithm that does muliplicative updates to its weights which is called the exponentiated gradient algorithm . \"", "label": "", "metadata": {}, "score": "44.673447"}
{"text": "The first paper in which we derive learning algorithms in a simple framework trading off the divergence in the parameter domain with the divergence in the labelling domain .The focus is on linear regression .We analyse the gradient descent algorithm ( Widrow Hoff ) and a corresponding algorithm that does muliplicative updates to its weights which is called the exponentiated gradient algorithm . \"", "label": "", "metadata": {}, "score": "44.673447"}
{"text": "1183 - 1210 , Dec. 1969 .Freund , Y. , et al . , \" Large Margin Classification Using the Perceptron Algorithm , \" Machine Learning , vol .37 , pp .277 - 296 , 1999 .Hernandez , M.A. , et al . , \" The Merge - Purge Problem for Large Databases , \" In Proceedings of ACM SIGMOS-1995 , pp .", "label": "", "metadata": {}, "score": "44.888733"}
{"text": "Rather than minimizing error with respect to y , weak learners are chosen to minimize the ( weighted least - squares ) error of with respect to . , where , and .That is is the Newton - Raphson approximation of the minimizer of the log - likelihood error at stage , and the weak learner is chosen as the learner that best approximates by weighted least squares .", "label": "", "metadata": {}, "score": "45.17312"}
{"text": "Friedman , J. H. and Hall , P. ( 1999 ) .On bagging and nonlinear estimation .J. Comput .Graph .Statist .To appear .Grove , A. and Schuurmans , D. ( 1998 ) .Boosting in the limit : maximizing the margin of learned ensembles .", "label": "", "metadata": {}, "score": "45.19096"}
{"text": "Widrow , B. , Lehr , M.A. , \" 30 years of Adaptive Neural Networks : Perceptron , Madaline , and Backpropagation , \" Proc .IEEE , vol 78 , no 9 , pp .1415 - 1442 , ( 1990 ) .", "label": "", "metadata": {}, "score": "45.297386"}
{"text": "We study online learning algorithms that predict by combining the predictions of several subordinate prediction algorithms , sometimes called \" experts . \" These simple algorithms belong to the multiplicative weights family of algorithms .The performance of these algorithms degrades only logarithmically with the number of experts , making them particularly useful in applications where the number of experts is very large .", "label": "", "metadata": {}, "score": "45.306145"}
{"text": "Collins , M. , \" Discriminative Training Methods for Hidden Markov Models : Theory and Experiments with Perceptron Algorithms , \" In Proceedings of EMNLP-2002 , pp . 1 - 8 , 2002 .Crammer , K. , et al . , \" Kernel Design Using Boosting , \" In Proceedings of the 15th Annual Conference on Neural Information Processing Systems , NIPS'01 , 2002 .", "label": "", "metadata": {}, "score": "45.376984"}
{"text": "Abstract .Boosting is one of the most important recent developments in classification methodology .Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced .", "label": "", "metadata": {}, "score": "45.6484"}
{"text": "Here is a small such data set , consisting of two points coming from two Gaussian distributions .Two class Gaussian data.png .Two class gaussian data .Linear classifier on Gaussian data.png .A linear classifier operating on the original space .", "label": "", "metadata": {}, "score": "45.858566"}
{"text": "For the two - class case , we prove a theorem that shows how to change the proportion of negative examples in a training set in order to make optimal cost - sensitive classification decisions using a classifier learned by a standard non - costsensitive learning method .", "label": "", "metadata": {}, "score": "45.868145"}
{"text": "Training a Composite Similarity Function for Record Linkage .As noted above , any binary classifier that produces confidence scores can be used to estimate the overall similarity of a record pair ( R i1 , R i2 ) by classifying the corresponding feature vector x i and treating classification confidence as similarity .", "label": "", "metadata": {}, "score": "46.18771"}
{"text": "After the -th iteration our boosted classifier is a linear combination of the weak classifiers of the form : .So it remains to determine which weak classifier is the best choice for , and what its weight should be .We define the total error of to be the sum of its exponential loss on each data point , given as follows : .", "label": "", "metadata": {}, "score": "46.188156"}
{"text": "First , while Bagging is almost always more accurate than a single classifier , it is sometimes much less accurate than Boosting .On the other hand , Boosting can create ensembles that are less accurate than a single classifier - especially when using neural networks .", "label": "", "metadata": {}, "score": "46.260403"}
{"text": "Abstract .Bagging and boosting are methods that generate a diverse ensemble of classifiers by manipulating the training data given to a \" base \" learning algorithm .Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm .", "label": "", "metadata": {}, "score": "46.336464"}
{"text": "Thus , an adaptive framework that can learn a composite similarity function , customized for a particular group of products ( e.g. , a product category ) , from training data is useful for a general purpose product normalization algorithm .Basis Functions .", "label": "", "metadata": {}, "score": "46.455193"}
{"text": "Having theoretical guarantees that such additions will not harm performance allows for experimentation with different basis functions 240 without the fear that bad local optima will arise due to correlations between attributes .The algorithm can also be viewed as minimizing the cumulative hinge loss suffered on a stream of examples .", "label": "", "metadata": {}, "score": "46.846413"}
{"text": "Unlike neural networks and SVMs , the AdaBoost training process selects only those features known to improve the predictive power of the model , reducing dimensionality and potentially improving execution time as irrelevant features do not need to be computed .where each is a weak learner that takes an object as input and returns a real valued result indicating the class of the object .", "label": "", "metadata": {}, "score": "46.874977"}
{"text": "Instead of paying log n for choosing the best expert in each section we first pay log bits in the bounds for identifying the pool of m experts and then log m bits per new section .In the bounds we also pay twice for encoding the boundaries of the sections .", "label": "", "metadata": {}, "score": "46.886726"}
{"text": "Abstract .One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large , and often is observed to decrease even after the training error reaches zero .", "label": "", "metadata": {}, "score": "46.92341"}
{"text": "The Last - step Minimax Algorithm \" , ALT 00 , Springer Verlag Lecture notes in AI , vol .1968 , pp .279 - -290 , December 2000 ( with Eiji Takimoto ) [ Postscript ] .The following papers focus on the expert model .", "label": "", "metadata": {}, "score": "47.033802"}
{"text": "The Last - step Minimax Algorithm \" , ALT 00 , Springer Verlag Lecture notes in AI , vol .1968 , pp .279 - -290 , December 2000 ( with Eiji Takimoto ) [ Postscript ] .The following papers focus on the expert model .", "label": "", "metadata": {}, "score": "47.033802"}
{"text": "Bagging and boosting are methods that generate a diverse ensemble of classifiers by manipulating the training data given to a \" base \" learning algorithm .Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm .", "label": "", "metadata": {}, "score": "47.106873"}
{"text": "Lanckriet , G.R.G. , et al . , \" Learning the Kernel Matrix with Semidefinite Programming , \" In Proceedings of the 19th Int'l Conf . on Machine Learning , Sydney , Australia , 2002 .Lawrence , S. , et al . , \" Autonomous Citation Matching , \" In Proceedings of Agents-1999 , pp .", "label": "", "metadata": {}, "score": "47.265533"}
{"text": "We give several examples illustrating problems for which this new algorithm provides advantages in performance .Keywords . boosting isotonic regression convergence document classification k nearest neighbors .Share .References .Apte , C. , Damerau , F. , & Weiss , S. ( 1998 ) .", "label": "", "metadata": {}, "score": "47.599686"}
{"text": "We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples .Finally , we compare our explanation to those based on the bias - variance decomposition . ... a , overly complex weak hypotheses or weak hypotheses which are too weak .", "label": "", "metadata": {}, "score": "47.781494"}
{"text": "Nock , R. , & Sebban , M. ( 2001 ) .A Bayesian boosting theorem .Pattern Recognition Letters , 22 , 413 - 419 .Pardalos , P. M. , & Xue , G. ( 1999 ) .Algorithms for a class of isotonic regression problems .", "label": "", "metadata": {}, "score": "47.978455"}
{"text": "The bounds are expressed in terms of an everage margin of the examples .Previous methods ( including the Perceptron Convergence Theorem ) used worst - case margins . \"Linear Hinge Loss and Average Margin , \" in NIPS 98 , pp .", "label": "", "metadata": {}, "score": "48.00287"}
{"text": "The bounds are expressed in terms of an everage margin of the examples .Previous methods ( including the Perceptron Convergence Theorem ) used worst - case margins . \"Linear Hinge Loss and Average Margin , \" in NIPS 98 , pp .", "label": "", "metadata": {}, "score": "48.00287"}
{"text": "In one embodiment , for a group of products in a plurality of products , a composite similarity function is constructed for the group of products from a weighted set of basis similarity functions .Training records are used to calculate the weights in the weighted set of basis similarity functions in the composite similarity function for the group of products .", "label": "", "metadata": {}, "score": "48.164444"}
{"text": "Recent pubs of Manfred K. Warmuth .We have developed a framework for deriving and analysing simple on - line learning algorithms .A learning problem is given by a parameterized class of predictors , a divergence measure in the labelling domain ( loss function ) and a divergence measure in the parameter domain .", "label": "", "metadata": {}, "score": "48.24686"}
{"text": "Machine Learning , 38 , 243 - 255 .CrossRef .McCallum , A. , & Nigam , K. ( 1998 ) .A comparison of event models for naive bayes text classification .Conference Proceedings AAAI-98 Workshop on Learning for Text Categorization .", "label": "", "metadata": {}, "score": "48.27414"}
{"text": "Bagging predictors .Machine Learning 24 123 - 140 .Friedman , J. and Stuetzle , W. ( 1981 ) .Projection pursuit regression .J. Amer .Statist .Assoc .76 817 - 823 .Holte , R. ( 1993 ) .", "label": "", "metadata": {}, "score": "48.279766"}
{"text": "Shawe - Taylor , J ; Cristianini , N ; ( 1998 )Robust Bounds on Generalization from the Margin Distribution .Full text not available from this repository .Abstract .A number of results have bounded generalization of a classifier in terms of its margin on the training points .", "label": "", "metadata": {}, "score": "48.359932"}
{"text": "Bagging ( Breiman , 1996c ) and Boosting ( Freund & Schapire , 1996 ; Schapire , 1990 ) are two relatively new but popular methods for producing ensembles .In this paper we evaluate these methods on 23 data sets using both neural networks and decision trees as our classification algorithm .", "label": "", "metadata": {}, "score": "48.558556"}
{"text": "One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large , and often is observed to decrease even after the training error reaches zero .", "label": "", "metadata": {}, "score": "48.583065"}
{"text": "We generalize the recent relative loss bounds for on - line algorithms where the additional loss of the algorithm on the whole sequence of examples over the loss of the best expert is bounded .The generalization allows the sequence to be partitioned into segments , and the goal is to bound the additional loss of the algorithm over the sum of the losses of the best experts for each segment .", "label": "", "metadata": {}, "score": "49.077744"}
{"text": "References .Freund , Y. and Schapire , R. E. 1998 .Large margin classification using the perceptron algorithm .In Proceedings of the 11th Annual Conference on Computational Learning Theory ( COLT ' 98 ) .ACM Press .Gallant , S. I. ( 1990 ) .", "label": "", "metadata": {}, "score": "49.099277"}
{"text": "A decision - theoretic generalization of on - line learning and an application to boosting .Journal or Computer and System Sciences , 55 :1 , 119 - 139 .MathSciNet .Friedman , J. , Hastie , T. , & Tibshirani , R. ( 2000 ) .", "label": "", "metadata": {}, "score": "49.131252"}
{"text": "Muller , K.R. , et al . , \" An Introduction to Kernel - Based Learning Algorithms , \" IEEE Transactions of Neural Networks , vol .12 , No . 2 , Mar. 2001 .Newcombe , H.B. , et al . , Automatic Linkage of Vital Records , Science , vol .", "label": "", "metadata": {}, "score": "49.132015"}
{"text": "From our own work [ B \u00a8uhlmann and Yu ( 2000 ) ] we know that stumps evaluated at x have high variances for x in a whole region of the covariate space .From an asymptotic point of view , this region is \" centered around \" the true optimal split point for a stump and has \" substantial \" size O n-1/3 .", "label": "", "metadata": {}, "score": "49.563866"}
{"text": "We show that this seemingly mysterious phenomenon can be understood in terms of well - known statistical principles , namely additive modeling and maximum likelihood .For the two - class problem , boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion .", "label": "", "metadata": {}, "score": "50.014202"}
{"text": "Furthermore , when applied to concrete on - line algorithms , our results yield tail bounds that in many cases are comparable or better than the best known bounds . ... sophisticated statistical tools required by risk analyses based on uniform convergence .", "label": "", "metadata": {}, "score": "50.396084"}
{"text": "The technology for building knowledge - based systems by inductive inference from examples has been demonstrated successfully in several practical applications .This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems , and it describes one such syste ... \" .", "label": "", "metadata": {}, "score": "50.532173"}
{"text": "The at least one computer is also configured to use a perceptron algorithm to modify the weights in the weighted set .Another aspect of the invention involves a machine readable medium having stored thereon data representing sequences of instructions , which when executed by a computer , cause the computer to apply a composite similarity function to pairs of training records .", "label": "", "metadata": {}, "score": "50.744896"}
{"text": "Abstract .We study online learning algorithms that predict by combining the predictions of several subordinate prediction algorithms , sometimes called \" experts . \" These simple algorithms belong to the multiplicative weights family of algorithms .The performance of these algorithms degrades only loga ... \" .", "label": "", "metadata": {}, "score": "51.02098"}
{"text": "We generalize the recent relative loss bounds for on - line algorithms where the additional loss of the algorithm on the whole sequence of examples over the loss of the best expert is bounded .The generalization allows the sequence to be partitioned into segments , and the goal is to bound th ... \" .", "label": "", "metadata": {}, "score": "51.12432"}
{"text": "It can be used in conjunction with many other types of learning algorithms to improve their performance .The output of the other learning algorithms ( ' weak learners ' ) is combined into a weighted sum that represents the final output of the boosted classifier .", "label": "", "metadata": {}, "score": "51.160645"}
{"text": "Statist .Friedman , J. H. ( 1999a ) .Greedy function approximation : a gradient boosting machine .Ann .Statist .To appear .Friedman , J. H. ( 1999b ) .Stochastic gradient boosting .Technical report , Dept .", "label": "", "metadata": {}, "score": "51.344406"}
{"text": "Quinlan , J. ( 1996 ) .Boosting first order learning .In Proceedings of the Seventh International Workshop on Algorithmic Learning Theory ( S. Arikawa and A. Sharma , eds . )Lecture Notes in Artificial Intelligence 1160 143 - 155 .", "label": "", "metadata": {}, "score": "51.503094"}
{"text": "[ 3 ] Specifically , the loss being minimized by AdaBoost is the exponential loss , whereas LogitBoost performs logistic regression , minimizing .Thus AdaBoost algorithms perform either Cauchy ( find with the steepest gradient , choose to minimize test error ) or Newton ( choose some target point , find that will bring closest to that point ) optimization of training error .", "label": "", "metadata": {}, "score": "51.634552"}
{"text": "Machine Learning , 48 :1 , 253 - 285 .CrossRef .Duda , R. O. , Hart , P. E. , & Stork , D. G. ( 2000 ) .Pattern Classification ( 2 edn . )New York : John Wiley & Sons , Inc. .", "label": "", "metadata": {}, "score": "51.853394"}
{"text": "846 - 850 , 1971 .Renders , J.M. , \" Kernel Methods in Natural Language Processing , \" Xerox Research Center Europe , ACL'04 Tutorial , 2004 .Sarawagi , S. , et al . , \" Interactive Deduplication Using Active Learning , \" In Proceedings ACM SIGKDD-2002 , pp .", "label": "", "metadata": {}, "score": "52.51543"}
{"text": "The methods disclosed herein do not require that the basis functions 240 only operate on single fields of records .Indeed , the methods presented here are general enough to make use of arbitrarily complex functions of two records , e.g. , concatenations of multiple attributes .", "label": "", "metadata": {}, "score": "52.52533"}
{"text": "The feature common to the a ..In particular , it has been shown that the Lempel - Ziv algorithm universally achieves quite a small regret when compared to the best finite state predictor [ 15].A more general analysis of universal p .. \" ...", "label": "", "metadata": {}, "score": "52.52806"}
{"text": "Annals of Mathematical Statistics , 26 , 641 - 647 .MathSciNet .Bennett , K. P. , Demiriz , A. , & Shawe - Taylor , J. ( 2000 ) .A column generation algorithm for boosting .Conference Proceedings 17th ICML .", "label": "", "metadata": {}, "score": "52.757545"}
{"text": "Conference Proceedings Tenth Conference on Uncertainty in Artificial Intelligence , Seattle , WA .Maclin , R. ( 1998 ) .Boosting classifiers locally .Conference Proceedings Proceedings of AAAI .Mason , L. , Bartlett , P. L. , & Baxter , J. ( 2000 ) .", "label": "", "metadata": {}, "score": "52.84588"}
{"text": ".. she does not even assume the existence of such a mechanism . by Olivier Bousquet , Manfred K. Warmuth - JOURNAL OF MACHINE LEARNING RESEARCH , 2002 . \" ...In this paper , we examine on - line learning problems in which the target concept is allowed to change over time .", "label": "", "metadata": {}, "score": "52.882973"}
{"text": "x i is the pair - space vector defined above .The averaged perceptron algorithm has several properties that make it particularly useful for large - scale streaming linkage tasks .First and foremost , it is an online learning algorithm : the similarity function parameters ( weights ) that it generates can be easily updated as more labeled examples become available without the need to retrain on all previously seen training data .", "label": "", "metadata": {}, "score": "53.13732"}
{"text": "A paper that gives a partial Baysian interpretation for Winnow - like updates for learning disjunctions . \"Direct and Indirect Algorithms for On - line Learning of Disjunctions \" , In a special issue of Theoretical Computer Science for EUROCOLT 99 , Vol .", "label": "", "metadata": {}, "score": "53.298637"}
{"text": "A paper that gives a partial Baysian interpretation for Winnow - like updates for learning disjunctions . \"Direct and Indirect Algorithms for On - line Learning of Disjunctions \" , In a special issue of Theoretical Computer Science for EUROCOLT 99 , Vol .", "label": "", "metadata": {}, "score": "53.298637"}
{"text": "We generalise the basic result to function classes with bounded fat- shattering dimension and the 1-norm of the slack variables which gives rise to Vapnik 's box constraint algorithm .We also extend the results to the regression case and obtain bounds on the probability that a randomly chosen test point will have error greater than a given value .", "label": "", "metadata": {}, "score": "53.417877"}
{"text": "f .R .R .o .i .K .i .f .i .R .R .In turn , S can be used with a similarity based clustering algorithm to determine clusters , each of which contains a set of records that presumably should be linked .", "label": "", "metadata": {}, "score": "53.553703"}
{"text": "This analysis yields a new , simple proof of the min - max theorem , as well as a provable method of approximately solving a game .A variant of our game - playing algorithm is proved to be optimal in a very strong sense . by Yoav Freund , Robert E. Schapire , Yoram Singer , Manfred K. Warmuth - In 29th", "label": "", "metadata": {}, "score": "53.830612"}
{"text": "Empirical observations about the good performance of GentleBoost appear to back up Schapire and Singer 's remark that allowing excessively large values of can lead to poor generalization performance .[ 4 ] [ 5 ] .A technique for speeding up processing of boosted classifiers , early termination refers to only testing each potential object with as many layers of the final classifier necessary to meet some confidence threshold , speeding up computation for cases where the class of the object can easily be determined .", "label": "", "metadata": {}, "score": "53.83133"}
{"text": "Similarly , the -layer classifier will be positive if the sample is believed to be in the positive class and negative otherwise .Each weak learner produces an output , hypothesis , for each sample in the training set .At each iteration , a weak learner is selected and assigned a coefficient such that the sum training error of the resulting -stage boost classifier is minimized .", "label": "", "metadata": {}, "score": "54.060913"}
{"text": "There is a lot of flexibility allowed in the choice of loss function .As long as the loss function is monotonic and continuously differentiable , the classifier will always be driven toward purer solutions .[ 2 ] Zhang ( 2004 ) provides a loss function based on least squares , a modified Huber loss function : .", "label": "", "metadata": {}, "score": "54.241943"}
{"text": "The additional loss becomes O(k log n + k log(L / k ) ) , where L is the loss of the best partition with k +1segments .Our algorithms for tracking the predictions of the best expert are simple adaptations of Vovk 's original algorithm for the single best expert case .", "label": "", "metadata": {}, "score": "54.247543"}
{"text": "The at least one computer is configured to apply a composite similarity function to pairs of training records .The application of the composite similarity function provides a number that can be used to indicate whether two records relate to a common subject .", "label": "", "metadata": {}, "score": "54.326996"}
{"text": "Our results are proven without requiring complicated concentration - of - measure arguments and they hold for arbitrary on - lin ... \" .In this paper we show that on - line algorithms for classification and regression can be naturally used to obtain hypotheses with good datadependent tail bounds on their risk .", "label": "", "metadata": {}, "score": "54.404434"}
{"text": "The composite similarity function includes a weighted set of basis similarity functions .When executed by a computer , the instructions also cause the computer to use a perceptron algorithm to modify the weights in the weighted set .Another aspect of the invention involves a system that comprises means for applying a composite similarity function to pairs of training records .", "label": "", "metadata": {}, "score": "54.4263"}
{"text": "In the single segment case , the additional loss is proportional to log n , where n is the number of experts and the constant of proportionality depends on the loss function .Our algorithms do not produce the best partition ; however the loss bound shows that our predictions are close to those of the best partition .", "label": "", "metadata": {}, "score": "54.635567"}
{"text": "For example , some items shown separately in FIG .2 could be implemented on single computers and single items could be implemented by one or more computers .Producing Composite Similarity Functions .In record linkage , a function is created that is used to determine the degree of similarity between records .", "label": "", "metadata": {}, "score": "54.775635"}
{"text": "The simplest methods , which can be particularly effective in conjunction with totally corrective training , are weight- or margin - trimming : when the coefficient , or the contribution to the total test error , of some weak classifier falls below a certain threshold , that classifier is dropped .", "label": "", "metadata": {}, "score": "54.936733"}
{"text": "Freund , Y. and Mason , L. ( 1999 ) .The alternating decision tree learning algorithm .In Machine Learning : Proceedings of the Sixteenth International Conference 124 - 133 .Friedman , J. H. ( 1991 ) .Multivariate adaptive regression splines ( with discussion ) .", "label": "", "metadata": {}, "score": "54.954433"}
{"text": "Delivery Method : .References .Breiman , L. ( 1997 ) .Prediction games and arcing algorithms .Technical Report 504 , Dept .Statistics , Univ .California , Berkeley .Breiman , L. ( 1998a ) .Arcing classifiers ( with discussion ) .", "label": "", "metadata": {}, "score": "54.970955"}
{"text": "A linear classifier operating on a high - dimensional projection .A linear classifier can only separate things with a hyperplane , so it 's not possible to perfectly classify all the examples .On the other hand , we may project the data into a large number of dimensions .", "label": "", "metadata": {}, "score": "55.108772"}
{"text": "- 278 , 2002 .Shalev - Shwartz , S. , et al . , Online and Batch Learning of Pseudo - Metrics , In Proceedings of ICML-2004 , pp .743 - 750 , 2004 .Signla , P. , et al . , \" Object Identification with Attribute - Mediated Dependencies , \" In Proceedings of PKDD-2005 , 2005 .", "label": "", "metadata": {}, "score": "55.113228"}
{"text": "In some embodiments , the composite similarity function is a transform of a weighted linear combination of basis similarity functions .In some embodiments , the basis similarity functions are kernel functions .In some embodiments , a perceptron algorithm is used to modify the weights in the weighted set ( e.g. , by training module 250 ) ( 404 ) .", "label": "", "metadata": {}, "score": "55.182983"}
{"text": "A special case of this bound gives a bound on the probabilities in terms of the least squares error on the training set showing a quadratic decline in probability with margin .Keywords : . learning algorithm , online learning , perceptron , support vector , Support Vector Machine AdaBoost .", "label": "", "metadata": {}, "score": "55.26712"}
{"text": "Such records may either come in a pre - structured form ( e.g. , XML or relational database records ) , or such fields may have been extracted from an underlying textual description .Hence , a method and system that provide for efficient production and training of similarity functions between offers with multiple fields is also needed .", "label": "", "metadata": {}, "score": "55.37243"}
{"text": "Linear smoothers and additive models .The Annals of Statistics , 17 : 2 453 - 555 .MathSciNet .Burges , C. J. C. ( 1999 ) .A tutorial on support vector machines for pattern recognition ( Available electronically from the author ) : Bell Laboratories , Lucent Technologies .", "label": "", "metadata": {}, "score": "55.44152"}
{"text": "The method of claim 2 , wherein the perceptron algorithm is a voted perceptron .f .R .R .f .transform .[ .i .K .i .f .i .R .R . .", "label": "", "metadata": {}, "score": "55.459064"}
{"text": "Minsky M L and Papert S A 1969 Perceptrons ( Cambridge , MA : MIT Press ) .Novikoff , A. B. ( 1962 ) .On convergence proofs on perceptrons .Symposium on the Mathematical Theory of Automata , 12 , 615 - 622 .", "label": "", "metadata": {}, "score": "55.473907"}
{"text": "Shape quantization and recognition with randomized trees .Neural Comput .Breiman , L. ( 1999a ) .Random forests .Breiman , L. ( 1999b ) .Prediction games and arcing algorithms .Neural Comput .Dietterich , T. ( 2000 ) .", "label": "", "metadata": {}, "score": "55.600975"}
{"text": "Managing Gigabytes ( 2 edn . )San Francisco : Morgan - Kaufmann Publishers , Inc. .Zhang , T. , & Oles , F. J. ( 2001 ) .Text categorization based on regularized linear classification methods .Information Retrieval , 4 :1 , 5 - 31 .", "label": "", "metadata": {}, "score": "55.65107"}
{"text": "Lee , J. ( 1999 ) .A computer program that plays a hunch .New York Times August 17 .Quinlan , J. ( 1996 ) .Bagging , boosting , and C4.5 .In Proceedings Thirteenth American Association for Artificial Intelligence National Conference on Artificial Intelligence 725 - 730 .", "label": "", "metadata": {}, "score": "55.668304"}
{"text": "b . is negative , then the weighted combination of inputs must produce a positive value greater than .b .in order to push the classifier neuron over the 0 threshold .Spatially , the bias alters the position ( though not the orientation ) of the decision boundary .", "label": "", "metadata": {}, "score": "55.722458"}
{"text": "414 - 420 , 1989 .Kamvar , S.D. , et al . , \" Interpreting and Extending Classical Agglomerative Clustering Algorithms Using a Model - Based Approach , \" In Proceedings of the 19th Int'l Conf . on Machine Learning ( ICML-2002 ) , 2002 .", "label": "", "metadata": {}, "score": "56.130424"}
{"text": "132 , no . 1 , pp . 1 - 64 , January 1997 , an extended abstract appeared in STOC 95 ( with J. Kivinen ) [ Postscript file ] .In the following paper we discuss when gradient descent and related algorithms can be much worse than the new exponentiated gradient algorithm .", "label": "", "metadata": {}, "score": "56.79666"}
{"text": "132 , no . 1 , pp . 1 - 64 , January 1997 , an extended abstract appeared in STOC 95 ( with J. Kivinen ) [ Postscript file ] .In the following paper we discuss when gradient descent and related algorithms can be much worse than the new exponentiated gradient algorithm .", "label": "", "metadata": {}, "score": "56.79666"}
{"text": "The application of the composite similarity function provides a number that can be used to indicate whether two records relate to a common subject .The composite similarity function includes a weighted set of basis similarity functions .A perceptron algorithm is used to modify the weights in the weighted set .", "label": "", "metadata": {}, "score": "56.87516"}
{"text": "The application of the composite similarity function provides a number that can be used to indicate whether two records relate to a common subject .The composite similarity function includes a weighted set of basis similarity functions .A perceptron algorithm is used to modify the weights in the weighted set .", "label": "", "metadata": {}, "score": "56.87516"}
{"text": "as either a positive or a negative instance , in the case of a binary classification problem .The bias can be thought of as offsetting the activation function , or giving the output neuron a \" base \" level of activity .", "label": "", "metadata": {}, "score": "56.97921"}
{"text": "359 - 367 , 1998 .McCallum , A. , et al . , \" Efficient Clustering of High - Dimensional Data Sets with Application to Reference Matching , \" In Proceedings of SCM SIGKDD-2000 , pp . 169 - 178 , 2000 .", "label": "", "metadata": {}, "score": "57.066414"}
{"text": "Focusing primarily on the AdaBoost algorithm , this chapter overviews some of the recent work on boosting including analyses of AdaBoost 's training error and generalization error ; boosting 's connecti ... \" .Boosting is a general method for improving the accuracy of any given learning algorithm . by Charles Elkan - In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence , 2001 . \" ...", "label": "", "metadata": {}, "score": "57.1158"}
{"text": "97(1 - -2 ) , pp .325 - -343 , December 1997 .( with J. Kivinen and P. Auer ) [ Postscript file ] .In the following paper we prove relative loss bounds for single neurons with one output .", "label": "", "metadata": {}, "score": "57.132286"}
{"text": "97(1 - -2 ) , pp .325 - -343 , December 1997 .( with J. Kivinen and P. Auer ) [ Postscript file ] .In the following paper we prove relative loss bounds for single neurons with one output .", "label": "", "metadata": {}, "score": "57.132286"}
{"text": "Statist .Breiman , L. ( 1998b ) .Combining predictors .Technical report , Dept .Statistics , Univ .California , Berkeley .Breiman , L. , Friedman , J. , Olshen , R. and Stone , C. ( 1984 ) .", "label": "", "metadata": {}, "score": "57.571182"}
{"text": "It will be appreciated by those of ordinary skill in the art that one or more of the acts described may be performed by hardware , software , or a combination thereof , as may be embodied in one or more computing systems .", "label": "", "metadata": {}, "score": "57.784096"}
{"text": "It will be appreciated by those of ordinary skill in the art that one or more of the acts described may be performed by hardware , software , or a combination thereof , as may be embodied in one or more computing systems .", "label": "", "metadata": {}, "score": "57.784096"}
{"text": "As a result , a learning approach to the linkage problem in such settings should be able to readily use new training data without having to retrain on previously seen data .Thus , it would be highly desirable to develop methods and systems that efficiently produce and train composite similarity functions for record linkage problems , including product normalization problems .", "label": "", "metadata": {}, "score": "57.82421"}
{"text": "The Annals of Statistics , 38 : 2 , 337 - 374 .MathSciNet .Hardle , W. ( 1991 ) .Smoothing Techniques : With Implementation in S. New York : Springer - Verlag .Johnson , M. , Geman , S. , Canon , S. , Chi , Z. , & Riezler , S. ( 1999 ) .", "label": "", "metadata": {}, "score": "57.91984"}
{"text": "Continuous Versus Discrete - Time Non - linear Gradient Descent : Relative Loss Bounds and Convergence , \" in Fifth International Symposium on Artificial Intelligence and Mathematics , Florida , January 1997 ( with A. K. Jagota ) [ Postscript file ] .", "label": "", "metadata": {}, "score": "58.10339"}
{"text": "Continuous Versus Discrete - Time Non - linear Gradient Descent : Relative Loss Bounds and Convergence , \" in Fifth International Symposium on Artificial Intelligence and Mathematics , Florida , January 1997 ( with A. K. Jagota ) [ Postscript file ] .", "label": "", "metadata": {}, "score": "58.10339"}
{"text": "Thus , boosting is seen to be a specific type of linear regression .However , the exponential increase in the error for sample as increases results in excessive weight being assigned to outliers .One feature of the choice of exponential error function is that the error of the final additive model is the product of the error of each stage , that is , .", "label": "", "metadata": {}, "score": "58.12802"}
{"text": "The Pool Adjacent Violators ( PAV ) algorithm is a method for converting a score into a probability .We show how PAV may be applied to a weak hypothesis to yield a new weak hypothesis which is in a sense an ideal confidence rated prediction and that this leads to an optimal updating for AdaBoost .", "label": "", "metadata": {}, "score": "58.170708"}
{"text": "Kivinen , J. , et al . \"Online Learning with Kernels , \" IEEE Transactions on Signal Processing , vol .52 , No . 8 , pp .2165 - 2176 , 2002 .Koller , D. , et al . , \" Hierarchically Classifying Documents Using Very Few Words , \" In Proceedings of the 14th Intl Conf . on Machine Learning ( ICML-97 ) , pp .", "label": "", "metadata": {}, "score": "58.234825"}
{"text": "350 - 359 , 2002 .Verykios , V.S. , et al . , \" A Bayesian Decision Model for Cost Optimal Record Matching , \" The VLDB Journal , vol .12 , No . 1 , pp .28 - 40 , 2003 .", "label": "", "metadata": {}, "score": "58.332855"}
{"text": "Robertson , S. E. , & Walker , S. ( 1994 ) .Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval .Conference Proceedings 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval .", "label": "", "metadata": {}, "score": "58.351383"}
{"text": "An extended abstract appeared in COLT 89 ( with N. Littlestone ) [ Postscript file ] .When learning for example orthogonal rectangles then a subsample of up to four examples ( the left- , right- , top- and bottom- most pos . example ) represent a hypothesis that is consistent with the whole sample .", "label": "", "metadata": {}, "score": "58.357178"}
{"text": "An extended abstract appeared in COLT 89 ( with N. Littlestone ) [ Postscript file ] .When learning for example orthogonal rectangles then a subsample of up to four examples ( the left- , right- , top- and bottom- most pos . example ) represent a hypothesis that is consistent with the whole sample .", "label": "", "metadata": {}, "score": "58.357178"}
{"text": "AdaBoost is sensitive to noisy data and outliers .In some problems , however , it can be less susceptible to the overfitting problem than other learning algorithms .The individual learners can be weak , but as long as the performance of each one is slightly better than random guessing ( e.g. , their error rate is smaller than 0.5 for binary classification ) , the final model can be proven to converge to a strong learner .", "label": "", "metadata": {}, "score": "58.407288"}
{"text": "Mach .Learning 40 139 - 158 .Wheway , V. ( 1999 ) .Variance reduction trends on ' boosted ' classifiers .Available from virg@cse . unsw.edu.au .Friedman ( 1999 ) .It does not involve any \" reweighting \" .", "label": "", "metadata": {}, "score": "58.432304"}
{"text": "Averaging Expert Predictions , \" in EUROCOLT 99 , Springer Verlag , pp .153 - -167 , March 1999 ( with J. Kivinen )[ PDF file ] .\" Predicting Nearly as Well as the Best Pruning of a Planar Decision Graph \" , Theoretical Computer Science 288(2 ) : 217 - 235 ( 2002 ) ( with E. Takimoto ) [ Postscript file ] . \"", "label": "", "metadata": {}, "score": "58.501503"}
{"text": "Averaging Expert Predictions , \" in EUROCOLT 99 , Springer Verlag , pp .153 - -167 , March 1999 ( with J. Kivinen )[ PDF file ] .\" Predicting Nearly as Well as the Best Pruning of a Planar Decision Graph \" , Theoretical Computer Science 288(2 ) : 217 - 235 ( 2002 ) ( with E. Takimoto ) [ Postscript file ] . \"", "label": "", "metadata": {}, "score": "58.501503"}
{"text": "IEEE Transactions on Neural Networks , vol .1 , no . 2 , pp .179 - 191 .Rosenblatt , Frank ( 1958 ) , The Perceptron : A Probabilistic Model for Information Storage and Organization in the Brain , Cornell Aeronautical Laboratory , Psychological Review , v65 , No . 6 , pp .", "label": "", "metadata": {}, "score": "58.657486"}
{"text": "Hastie , T. and Tibshirani , R. ( 1990 ) .Generalized Additive Models .Chapman and Hall , London .Seminar f \u00a8ur Statistik ETH - Zentrum , LEO D72 CH-8092 Zurich Switzerland E - mail buhlmann@stat.math.ethz.ch Department of Statistics University of California Berkeley , California 94720 - 3860 .", "label": "", "metadata": {}, "score": "58.872356"}
{"text": "Friedman , Jerome ; Hastie , Trevor ; Tibshirani , Robert .Additive logistic regression : a statistical view of boosting ( With discussion and a rejoinder by the authors ) .Ann .Statist .28 ( 2000 ) , no . 2 , 337 - -407 .", "label": "", "metadata": {}, "score": "58.89528"}
{"text": "211 - -246 , June 2001 ( with K. Azoury ) [ Postscript file ] .We did one case of density estimation with an exponential family optimally . \" The Minimax Algorithm for Gaussian Density Estimation , \" in COLT 00 , June 28 - July 1 , 2000 , pp .", "label": "", "metadata": {}, "score": "59.2413"}
{"text": "211 - -246 , June 2001 ( with K. Azoury ) [ Postscript file ] .We did one case of density estimation with an exponential family optimally . \" The Minimax Algorithm for Gaussian Density Estimation , \" in COLT 00 , June 28 - July 1 , 2000 , pp .", "label": "", "metadata": {}, "score": "59.2413"}
{"text": "170 - 179 , June 28 - July 1 , 2000 , Stanford University ( with G. Raetsch , S. Mika , T. Onoda , S. Lemm , K. R. Mueller ) [ PDF file ] .In the following paper ( preliminary version ) we explain the most simple learning algorithms in terms of link functions and prove relative loss bound for continuous time versions of the algorithms .", "label": "", "metadata": {}, "score": "59.266434"}
{"text": "170 - 179 , June 28 - July 1 , 2000 , Stanford University ( with G. Raetsch , S. Mika , T. Onoda , S. Lemm , K. R. Mueller ) [ PDF file ] .In the following paper ( preliminary version ) we explain the most simple learning algorithms in terms of link functions and prove relative loss bound for continuous time versions of the algorithms .", "label": "", "metadata": {}, "score": "59.266434"}
{"text": "Improved boosting algorithms using confidence - rated predictions .Machine Learning , 37 :3 , 297 - 336 .CrossRef .Vapnik , V. ( 1998 ) .Statistical Learning Theory .New York : John Wiley & Sons , Inc. .", "label": "", "metadata": {}, "score": "59.331505"}
{"text": "However , it will be apparent to one of ordinary skill in the art that the invention may be practiced without these particular details .In other instances , methods , procedures , components , and networks that are well - known to those of ordinary skill in the art are not described in detail to avoid obscuring aspects of the present invention .", "label": "", "metadata": {}, "score": "59.430504"}
{"text": "Conference Proceedings Proceedings ACL'99 .Univ .Maryland .Kim , W. , Aronson , A. R. , & Wilbur , W. J. ( 2001 ) .Automatic MeSH term assignment and quality assessment .Conference Proceedings Proc .AMIA Symp .", "label": "", "metadata": {}, "score": "59.71419"}
{"text": "Customized similarity functions can be used for categorical attributes , e.g. , tree proximity can be used as a similarity measure for a categorical attribute that corresponds to the location of an item in the product category hierarchy .( See the discussion of Offers A and B below for more details . )", "label": "", "metadata": {}, "score": "59.978966"}
{"text": "This paper reviews these methods and explains why ensembles can often perform better than any single classifier .Some previous studies comparing ensemble methods are reviewed , and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly . by Thomas G. Dietterich , Doug Fisher - Bagging , boosting , and randomization .", "label": "", "metadata": {}, "score": "60.0468"}
{"text": "Vol .44(5 ) , pp .1906 - 1925 , September 1998 ( with D. Haussler and J. Kivinen ) [ Postscript file ] . \"The Weighted Majority Algorithm , \" in Information and Computation , Vol .108 , No . 2 , pp .", "label": "", "metadata": {}, "score": "60.282394"}
{"text": "Vol .44(5 ) , pp .1906 - 1925 , September 1998 ( with D. Haussler and J. Kivinen ) [ Postscript file ] . \"The Weighted Majority Algorithm , \" in Information and Computation , Vol .108 , No . 2 , pp .", "label": "", "metadata": {}, "score": "60.282394"}
{"text": "Kim , W. G. , & Wilbur , W. J. ( 2001 ) .Corpus - based statistical screening for content - bearing terms .Journal of the American Society for Information Science , 52 :3 , 247 - 259 .Langley , P. , & Sage , S. ( 1994 ) .", "label": "", "metadata": {}, "score": "60.300488"}
{"text": "1291 - -1304 , November 1999 ( with D.P. Helmbold and J. Kivinen ) [ Postscript file ] .We have generalized the above to multiclass outputs including multiclass logistic regression .In this paper we also make a connection between the matching loss and Bregmann divergences .", "label": "", "metadata": {}, "score": "60.42003"}
{"text": "1291 - -1304 , November 1999 ( with D.P. Helmbold and J. Kivinen ) [ Postscript file ] .We have generalized the above to multiclass outputs including multiclass logistic regression .In this paper we also make a connection between the matching loss and Bregmann divergences .", "label": "", "metadata": {}, "score": "60.42003"}
{"text": "McCallum , A. , et al . , \" Conditionally Models of Identity Uncertainty with Application to Noun Coreference , \" NIPS 17 , pp .905 - 912 , 2005 .Monge , A.E. , et al . , \" An Efficient Domain - Independent Algorithm for Detecting Approximately Duplicate Database Records , \" In Proceedings of SIGMOD 1997 Workshop on Research Issues on Data Mining and Knowledge Discovery , pp .", "label": "", "metadata": {}, "score": "60.57834"}
{"text": "We characterize precisely but intuitively when a cost matrix is reasonable , and we show how to avoid the mistake of defining a cost matrix that is economically i ... \" .This paper revisits the problem of optimal learning and decision - making when different misclassification errors incur different penalties .", "label": "", "metadata": {}, "score": "60.647083"}
{"text": "v .v .v .v .Note that one of ordinary skill in the art would recognize that other types of basis functions could be used beyond the three illustrated here .For example , other token - based or sequence - based string similarity functions , such as the string edit distance , could also be used to determine the similarity of product names and/or product descriptions .", "label": "", "metadata": {}, "score": "60.753452"}
{"text": "The present invention overcomes the problems described above .A basis similarity function provides a numerical indication of the similarity of entries in corresponding fields in data records for products in the group of products .Another aspect of the invention is a system comprising at least one computer .", "label": "", "metadata": {}, "score": "60.78795"}
{"text": "Cohen , W.W. , et al . , \" Hardening Soft Information Sources , \" In Proceedings of ACM SIGKDD 2000 , pp .255 - 259 , 2000 .Cohen , W.W. , et al ., Learning to Match and Cluster Large High - Dimensional Data Sets for Data Integration , In Proceedings of ACM - SIGKDD 2002 , pp .", "label": "", "metadata": {}, "score": "60.982765"}
{"text": "Referring to FIGS . 1 and 2 , record linkage computer 102 collects records ( e.g. , web pages from multiple websites 108 - 110 ) and stores them in records database 220 .In some embodiments , the records contain information for a plurality of products .", "label": "", "metadata": {}, "score": "61.177765"}
{"text": "Conference Proceedings The Conference on Automated Learning and Discovery , CMU .Aslam , J. ( 2000 ) .Improving algorithms for boosting .Conference Proceedings 13thCOLT .Palo Alto , California .Ayer , M. , Brunk , H. D. , Ewing , G. M. , Reid , W. T. , & Silverman , E. ( 1954 ) .", "label": "", "metadata": {}, "score": "61.450043"}
{"text": "Solving this product normalization problem allows the shopping engine to display multiple offers for the same product to a user who is trying to determine from which vendor to purchase the product .Accurate product normalization is also important for data mining tasks , such as analysis of pricing trends .", "label": "", "metadata": {}, "score": "61.602295"}
{"text": "[ PDF ] .A paper that contains some of the details of how we use Bregman divergences for proving relative loss bounds .In particular we make the connection between Bregman divergences and the exponential family of distributions .\" Relative Loss Bounds for On - line Density Estimation with the Exponential Family of Distributions \" , in a special issue of the Journal of Machine Learning on Theoretical Advances in On - line Learning , Game Theory and Boosting , edited by Yoram Singer , Vol .", "label": "", "metadata": {}, "score": "61.60579"}
{"text": "[ PDF ] .A paper that contains some of the details of how we use Bregman divergences for proving relative loss bounds .In particular we make the connection between Bregman divergences and the exponential family of distributions .\" Relative Loss Bounds for On - line Density Estimation with the Exponential Family of Distributions \" , in a special issue of the Journal of Machine Learning on Theoretical Advances in On - line Learning , Game Theory and Boosting , edited by Yoram Singer , Vol .", "label": "", "metadata": {}, "score": "61.60579"}
{"text": "Pasula , H. , et al . , \" Identity Uncertainty and Citation Matching , \" In NIPS 15 , pp .1401 - 1408 , 2003 .Rand , W.M. , \" Objective Criteria for the Evaluation of Clustering Methods , \" J. of the American Statistical Assoc . , vol .", "label": "", "metadata": {}, "score": "61.79853"}
{"text": "Accordingly , the recommended way of applying one of these methods in a domain with differing misclassification costs is to learn a classifier from the training set as given , and then to compute optimal decisions ... . \" ...In this paper we present a component based person detection system that is capable of detecting frontal , rear and near side views of people , and partially occluded persons in cluttered scenes .", "label": "", "metadata": {}, "score": "61.849087"}
{"text": "Li , X. , et al . , \" Robust Reading : Identification and Tracing of Ambiguous Names , \" In Proceedings of NAACL-2004 , pp .17 - 24 , 2004 .Li , Y. , \" Selective Voting for Perceptron - Like Online Learning , \" In Proceedings of 19th Int'l Conf . on Machine Learning , 2002 .", "label": "", "metadata": {}, "score": "61.913246"}
{"text": "Schapire and Singer 's improved version of AdaBoost for handling weak hypotheses with confidence rated predictions represents an important advance in the theory and practice of boosting .Its success results from a more efficient use of information in weak hypotheses during updating .", "label": "", "metadata": {}, "score": "62.043297"}
{"text": "The composite similarity function includes a weighted set of basis similarity functions .The system also comprises means for using a perceptron algorithm to modify the weights in the weighted set .Thus , the invention efficiently produces and trains composite similarity functions for record linkage problems , including product normalization problems .", "label": "", "metadata": {}, "score": "62.27495"}
{"text": "This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems , and it describes one such system , ID3 , in detail .Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete .", "label": "", "metadata": {}, "score": "62.391388"}
{"text": "Totally corrective algorithms , such as LPBoost , optimize the value of every coefficient after each step , such that new layers added are always maximally independent of every previous layer .This can be accomplished by backfitting , linear programming or some other method .", "label": "", "metadata": {}, "score": "62.643448"}
{"text": "Studies in Applied Mathematics , 52 ( 1973 ) , 213 - 257 , online [ 1 ] ) .Nevertheless the often - cited Minsky / Papert text caused a significant decline in interest and funding of neural network research .", "label": "", "metadata": {}, "score": "62.656357"}
{"text": "156 - 159 , 2003 .Charikar , M. , et al . , \" Clustering with Qualitative Information , \" In Proceedings of the 44th Annual IEEE Symposium on Foundations of Computer Science , 2003 .Chaudhuri , S. , et al . , \" Robust and Efficient Fuzzy Match for Online Data Cleaning , \" In Proceedings of ACM SIGMOD 2003 , pp .", "label": "", "metadata": {}, "score": "62.73313"}
{"text": "The averaged perceptron algorithm , described in Table 1 , is a space - efficient variation of the voted perceptron algorithm proposed and analyzed by Freund and Schapire .Each of the weights corresponds to the importance of the corresponding basis similarity function .", "label": "", "metadata": {}, "score": "62.85459"}
{"text": "Journal of Machine Learning Vol .45(3 ) , pp .301 - 329 ( with J. Kivinen )[ PDF file ] .We also have applied our methods for proving relative loss bounds to classification with a linear threshold function .", "label": "", "metadata": {}, "score": "63.06845"}
{"text": "Journal of Machine Learning Vol .45(3 ) , pp .301 - 329 ( with J. Kivinen )[ PDF file ] .We also have applied our methods for proving relative loss bounds to classification with a linear threshold function .", "label": "", "metadata": {}, "score": "63.06845"}
{"text": "It is based on AdaBoost .MH but also implements popular cascade classifiers and FilterBoost along with a batch of common multi - class base learners ( stumps , trees , products , Haar filters ) .NPatternRecognizer , a fast machine learning algorithm library written in C#.", "label": "", "metadata": {}, "score": "63.070805"}
{"text": "This architecture is known as Adaptive Combination of Classi ers ( ACC ) .The system performs very well and is capable of detecting people even when all components of a person are not found .The performance of the system is signi cantly better than a full body . by David Opitz , Richard Maclin - Journal of Artificial Intelligence Research , 1999 . \" ...", "label": "", "metadata": {}, "score": "63.0758"}
{"text": "Fairfax , VA . .Breiman , L. ( 1999 ) .Using adaptive bagging to debias regressions .Technical Report 547 , Dept .Statistics , Univ .California , Berkeley .Freund , Y. ( 1999 ) .An adaptive version of boost by majority algorithm .", "label": "", "metadata": {}, "score": "63.15629"}
{"text": "Here , we present results comparing RankBoost to nearest - neighbor and regression algorithms . by Mark Herbster , Manfred , K. Warmuth , Gerhard Widmer , Miroslav Kubat - In Proceedings of the 12th International Conference on Machine Learning , 1995 . \" ...", "label": "", "metadata": {}, "score": "63.17298"}
{"text": "MathSciNet .Ratsch , G. , Mika , S. , & Warmuth , M. K. ( 2001 ) .On the Convergence of Leveraging ( NeuroCOLT2 Technical Report 98 ) .London : Royal Holloway College .Ratsch , G. , Onoda , T. , & Muller , K.-R. Soft margins for AdaBoost .", "label": "", "metadata": {}, "score": "63.78169"}
{"text": "7 - 12 , 2003 .Bilenko , M. , et al . , \" Adaptive Duplicate Detection Using Learnable String Similarity Measures , \" In Proceedings of the 9th ACM - SIGKDD Int'l Conf . on Knowledge Discovery and Data Mining ( KKD-2003 ) , Washington DC , pp .", "label": "", "metadata": {}, "score": "63.99894"}
{"text": "For example , if the entire vocabulary has 20,000 different tokens , a string \" Canon EOS \" is represented by a 20,000-dimensional vector that has only two non - zero components , those corresponding to ' Canon ' and to ' EOS ' tokens .", "label": "", "metadata": {}, "score": "64.064926"}
{"text": "Historically , one of the most studied problems in record linkage is determining whether two database records for a person are referring to the same real - life individual .In applications from direct marketing to survey response ( e.g. , the U.S. Census ) , record linkage is often seen as an important step in data cleaning in order to avoid waste and maintain consistent data .", "label": "", "metadata": {}, "score": "64.13621"}
{"text": "R .f .transform .[ .i .K .i .f .i .R .R . . ]f .R .R .f .transform .[ .i .K .i .", "label": "", "metadata": {}, "score": "64.17068"}
{"text": "A tutorial that shows the usefulness of the Bregman divergences .\" Proving Relative Loss Bounds for On - Line Learning Algorithms Using Bregman Divergences \" , COLT 00 , June 28 - July 1 , 2000 , Stanford University .( with C. Gentile )", "label": "", "metadata": {}, "score": "64.30162"}
{"text": "A tutorial that shows the usefulness of the Bregman divergences .\" Proving Relative Loss Bounds for On - Line Learning Algorithms Using Bregman Divergences \" , COLT 00 , June 28 - July 1 , 2000 , Stanford University .( with C. Gentile )", "label": "", "metadata": {}, "score": "64.30162"}
{"text": "A new record linkage problem - called product normalization - arises in online comparison shopping .Here , two different websites may sell the same product , but provide different descriptions of that product to a comparison shopping database .( Note : Records containing product descriptions are also called \" offers \" herein . )", "label": "", "metadata": {}, "score": "64.384674"}
{"text": "Inform . and Comput .Freund , Y. and Schapire , R. ( 1996a ) .Game theory , on - line prediction and boosting .In Proceedings of the Ninth Annual Conference on Computational Learning Theory 325 - 332 .Freund , Y. and Schapire , R. E. ( 1996b ) .", "label": "", "metadata": {}, "score": "64.42313"}
{"text": "For instance , Freund and Schapire [ 16 ] tested AdaBoost on a set of UCI benchmark datasets [ 27 ] using C4.5 [ 29 ] as a weak learning algorithm , as well as an algorithm whichsFigure 5 : A sample of the e .. by Thomas G. Dietterich - MULTIPLE CLASSIFIER SYSTEMS , LBCS-1857 , 2000 . \" ...", "label": "", "metadata": {}, "score": "64.655495"}
{"text": "Internal estimates monitor error , strength , and correlation and these are used to show the response to increasing the number of features used in the splitting .Internal estimates are also used to measure variable importance .These ideas are also applicable to regression . by Robert E. Schapire , Peter Bartlett , Yoav Freund , Wee Sun Lee - In Proceedings International Conference on Machine Learning , 1997 . \" ...", "label": "", "metadata": {}, "score": "64.70795"}
{"text": "109 - 142 , July 2002 ( with D. P. Helmbold and S. Panizza )\\item [ Postscript file ] . \"Boosting as Entropy Projection , \" in COLT 99 , pp .134 - 144 , July 1999 , UC Santa Cruz ( with J. Kivinen )", "label": "", "metadata": {}, "score": "64.860275"}
{"text": "109 - 142 , July 2002 ( with D. P. Helmbold and S. Panizza )\\item [ Postscript file ] . \"Boosting as Entropy Projection , \" in COLT 99 , pp .134 - 144 , July 1999 , UC Santa Cruz ( with J. Kivinen )", "label": "", "metadata": {}, "score": "64.860275"}
{"text": "Collins , M. , \" Ranking Algorithms for Named - Entity Extraction : Boosting and the Voted Perception , \" In Proceedings of the Annual Meeting of the Association for Computational Linguistics ( ACL-02 ) , Philadelphia , PA , p. 489", "label": "", "metadata": {}, "score": "64.89931"}
{"text": "The problem of combining preferences arises in several applications , such as combining the results of different search engines .This work describes an efficient algorithm for combining multiple preferences .We first give a formal framework for the problem .We then describe and analyze a new boosting ... \" .", "label": "", "metadata": {}, "score": "65.06187"}
{"text": "Doorenbos , R .. B. , et al . , \" A Scalable Comparison - Shopping Agent for the World - Wide Web , \" In Proceedings of Agents-1997 , pp .39 - 48 , 1997 .Farahbod , R. , \" Modified Voted Perceptron : A Re - Ranking Method for NP - Chunking , \" Simon Fraser University , Dec. 2002 .", "label": "", "metadata": {}, "score": "65.36113"}
{"text": "\" A decision - theoretic generalization of on - line learning and an application to boosting \" .Journal of Computer and System Sciences 55 .doi : 10.1006/jcss.1997.1504 .CiteSeerX : 10 .1 .1 .32 .8918 : original paper of Yoav Freund and Robert E.Schapire where AdaBoost is first introduced . \" Boosting.org \" : a site on boosting and related ensemble learning methods .", "label": "", "metadata": {}, "score": "65.68706"}
{"text": "While the invention will be described in conjunction with the embodiments , it will be understood that it is not intended to limit the invention to these particular embodiments alone .On the contrary , the invention is intended to cover alternatives , modifications and equivalents that are within the spirit and scope of the invention as defined by the appended claims .", "label": "", "metadata": {}, "score": "66.067215"}
{"text": ".. roblem domain .It simply keeps one weight per expert , representing the belief in the expert 's prediction , and then decreases the weight as a function of the loss of the expert . by Nicolo Cesa - Bianchi , Alex Conconi , Claudio Gentile - IEEE Transactions on Information Theory , 2001 . \" ...", "label": "", "metadata": {}, "score": "66.26746"}
{"text": "Potential boosters ?Conference Proceedings Advances in Neural Information Processing Systems 11 .Duffy , N. , & Helmbold , D. ( 2000 ) .Leveraging for regression .Conference Proceedings 13th Annual Conference on Computational Learning Theory .San Francisco .", "label": "", "metadata": {}, "score": "66.54904"}
{"text": "The motivatio ... \" .In this paper we present a component based person detection system that is capable of detecting frontal , rear and near side views of people , and partially occluded persons in cluttered scenes .The framework that is described here for people is easily applied to other objects as well .", "label": "", "metadata": {}, "score": "66.60095"}
{"text": "In order to determine the desired weight that minimizes with the that we just determined , we differentiate : .Thus we have derived the AdaBoost algorithm : At each iteration , choose the classifier which minimizes the total weighted error , use this to calculate the error rate , use this to calculate the weight , and finally use this to improve the boosted classifier to .", "label": "", "metadata": {}, "score": "66.771164"}
{"text": "As explained above and illustrated by example below , a basis similarity function 240 provides a numerical indication of the similarity of entries in corresponding fields in two data records for products in the group of products .In some embodiments , the composite similarity function is a transform of a weighted linear combination of basis similarity functions , such as a sigmoid function .", "label": "", "metadata": {}, "score": "66.84233"}
{"text": "3 is a flowchart representing a computer - implemented method of producing a composite similarity function for product normalization according to one embodiment of the present invention .The process shown in FIG .3 is performed by record linkage computer 102 ( FIG .", "label": "", "metadata": {}, "score": "67.06432"}
{"text": "In other words , composite similarity function generator 230 constructs a composite similarity function for a group of products in the plurality of products .In some embodiments , the group of products is a product category .In some embodiments , the group of products is coextensive with the plurality of products .", "label": "", "metadata": {}, "score": "67.443344"}
{"text": "i .K .i .f .i .R .R . is the weighted linear combination of basis similarity functions .The method of claim 10 , wherein the basis similarity functions \u0192 i ( R 1 , R 2 ) are kernel functions .", "label": "", "metadata": {}, "score": "67.498764"}
{"text": "Pair Space Representation .The exponent T here is shorthand for ' matrix transpose ' , which makes x i a column vector ( k - by-1 matrix ) , as opposed to a row vector ( 1-by - k matrix)].", "label": "", "metadata": {}, "score": "67.52291"}
{"text": "In the first experiment , we used the algorithm to combine different WWW search strategies , each of which is a query expansion for a given domain .For this task , we compare the performance of RankBoost to the individual search strategies .", "label": "", "metadata": {}, "score": "67.609726"}
{"text": "Bansal , N. , et al . , \" Correlation Clustering , \" 43rd Annual Symposium on Foundations of Computer Science ( FOCS ) , pp .238 - 247 , 2002 .Baxter , R. , et al . , \" A Comparison of Fast Blocking Methods for Record Linkage , \" In Proceeding of the KDD-2003 Workshop on Data Cleaning , Record Linkage , and Object Consolidation , pp .", "label": "", "metadata": {}, "score": "67.72343"}
{"text": "Now , assume that the following weights corresponding to basis similarity functions 240 for particular attributes have been learned : .As described below , training records are used to calculate the weights in the weighted set of basis similarity functions in the composite similarity function ( e.g. , by training module 250 ) ( 304 ) .", "label": "", "metadata": {}, "score": "68.0399"}
{"text": "Winkler , W. , \" Using the EM Algorithm for Weight Computation in the Fellegi - Sunter Model of Record Linkage , \" American Statistical Assoc . , Proceedings of the Section on Survey Research Methods , pp .667 - 671 , 1988 .", "label": "", "metadata": {}, "score": "68.04762"}
{"text": "FIG .4 is a flowchart representing a computer - implemented method of training a composite similarity function for record linkage according to one embodiment of the present invention .DESCRIPTION OF EMBODIMENTS .Methods and systems are described that show how to produce and train composite similarity functions for record linkage problems , including product normalization problems .", "label": "", "metadata": {}, "score": "68.15731"}
{"text": "641 - 647 ( with Y. Singer ) [ Postscript file ] .Slides of talk on the new learning algorithms for Hidden Markov Models .[ Postscript file ] .The following paper gives an alternate method for deriving on - line learning algorithms that does not use Bregman divergences .", "label": "", "metadata": {}, "score": "69.02729"}
{"text": "641 - 647 ( with Y. Singer ) [ Postscript file ] .Slides of talk on the new learning algorithms for Hidden Markov Models .[ Postscript file ] .The following paper gives an alternate method for deriving on - line learning algorithms that does not use Bregman divergences .", "label": "", "metadata": {}, "score": "69.02729"}
{"text": "This can be overcome by enforcing some limit on the absolute value of z and the minimum value of w. .While previous boosting algorithms choose greedily , minimizing the overall test error as much as possible at each step GentleBoost features a bounded step size . is chosen to minimize , and no further coefficient is applied .", "label": "", "metadata": {}, "score": "69.4376"}
{"text": "Records in a product database generally have multiple attributes of different types , each of which has an associated similarity measure .For instance , string similarity measures like edit distance or cosine similarity can be used to compare textual attributes like product name and description .", "label": "", "metadata": {}, "score": "69.46547"}
{"text": "40 , No . 4 , pp .1215 - -1220 ( July 1994 ) ( with N. Cesa - Bianchi and A. Krogh [ Postscript file ] .My most recent work on boosting .We start with a linear programming problem for maximizing the margin and then add a relative entropy as a regularization . \"", "label": "", "metadata": {}, "score": "69.69003"}
{"text": "40 , No . 4 , pp .1215 - -1220 ( July 1994 ) ( with N. Cesa - Bianchi and A. Krogh [ Postscript file ] .My most recent work on boosting .We start with a linear programming problem for maximizing the margin and then add a relative entropy as a regularization . \"", "label": "", "metadata": {}, "score": "69.69003"}
{"text": "Moreover , for different categories of products , different similarity functions may be needed that capture the notion of equivalence for each category .Hence , a method and system that provide for efficient production and training of similarity functions between offers and/or between product categories is needed .", "label": "", "metadata": {}, "score": "69.7559"}
{"text": "Ridgeway , G. ( 1999 ) .The state of boosting .In Proceedings of the Thirty - first Symposium on the Interface 172 - 181 .Ridgeway , G. ( 1999 ) .The state of boosting .In Computing Science and Statistics 31 ( K. Berk , M. Pourahmadi , eds . )", "label": "", "metadata": {}, "score": "70.35529"}
{"text": "FIG .4 is a flowchart representing a computer - implemented method of training a composite similarity function for record linkage according to one embodiment of the present invention .The process shown in FIG .4 may be performed by record linkage computer 102 ( FIG .", "label": "", "metadata": {}, "score": "70.5391"}
{"text": "954 - 959 , 1959 .Ng , A. , \" CS229 Lecture Notes , \" Simon Fraser University , 2004 .Olsen , C.F. , \" Parallel Algorithm for Hierarchical Clustering , \" Parallel Computing , vol .21 , pp .", "label": "", "metadata": {}, "score": "70.991165"}
{"text": "This work describes an efficient algorithm for combining multiple preferences .We first give a formal framework for the problem .We then describe and analyze a new boosting algorithm for combining preferences called RankBoost .We also describe an efficient implementation of the algorithm for certain natural cases .", "label": "", "metadata": {}, "score": "70.997955"}
{"text": "This combined product database is then used to provide one common access point for the customer to compare product specifications , pricing , shipping , and other information .In such cases , two websites may have two different product offers that refer to the same underlying product , e.g. , \" Canon ZR 65 MC Camcorder \" and \" Canon ZR65 Digital MiniDV Camcorder . \"", "label": "", "metadata": {}, "score": "71.0663"}
{"text": "Ridgeway , G. ( 1999 ) .The state of boosting .In Proceedings of the Thirty - first Symposium on the Interface 172 - 181 .", "label": "", "metadata": {}, "score": "71.21689"}
{"text": "1 is a block diagram illustrating an exemplary distributed computer system according to an embodiment of the invention .FIG .2 is a block diagram illustrating record linkage computer 102 in accordance with one embodiment of the present invention .FIG .", "label": "", "metadata": {}, "score": "71.27272"}
{"text": "Whereas the doubling trick restarts the on - line algorithm several ti ... .This paper reviews some results in this area ; the new material in it includes the proofs for the performance of the Aggregating Algorithm in the problem of linear regression with square loss .", "label": "", "metadata": {}, "score": "71.40767"}
{"text": "If two weak learners produce very similar outputs , efficiency can be improved by removing one of them and increasing the coefficient of the remaining weak learner .[ 11 ] .MPBoost++ , a C++ implementation of the original AdaBoost .", "label": "", "metadata": {}, "score": "72.042656"}
{"text": "Localized boosting .Conference Proceedings 13thCOLT .Palo Alto , California .Mitchell , T. M. ( 1997 ) .Machine learning .Boston : WCB / McGraw - Hill .Moerland , P. , & Mayoraz , E. ( 1999 ) .", "label": "", "metadata": {}, "score": "72.43518"}
{"text": "97 - 99 , Winter 1997 .Doan , A. , et al . , \" Object Matching for Information Integration : A Profiler - Based Approach , \" In Proceedings of the IJCAI-2003 Workshop on Information Integration on the Web , Acapulco , Mexico , pp .", "label": "", "metadata": {}, "score": "72.56645"}
{"text": "The method of claim 10 , wherein the group of subjects is a product category .The method of claim 10 , wherein the group of subjects is coextensive with the plurality of subjects .TECHNICAL FIELD .The disclosed embodiments relate generally to machine learning .", "label": "", "metadata": {}, "score": "72.88429"}
{"text": "Bilenko , M. , \" Learnable Similarity Functions and Their Application to Clustering and Record Linkage , \" Proceedings of the 9th AAAAI / SIGART Doctoral Consortium , San Jose , CA , pp .981 - 982 , Jul. 2004 .", "label": "", "metadata": {}, "score": "73.2572"}
{"text": "It has been observed by several authors , including those of the current paper , that AdaBoost is not an optimal method in this case .The problem seems to be that AdaBoost overemphasizes the atypical examples which eventually results in inferior rules .", "label": "", "metadata": {}, "score": "74.19351"}
{"text": "i .R .R . .] wherein \u0192 transform [ ] function , and .i .K .i .f .i .R .R . is the weighted linear combination of basis similarity functions .The method of claim 2 , wherein the basis similarity functions \u0192 i ( R 1 , R 2 ) are kernel functions .", "label": "", "metadata": {}, "score": "74.75917"}
{"text": "367 - 373 , 2002 .Cunningham , H. , et al . , \" Human Language Technology for the Semantic Web , \" ESWS , Crete , Greece , May 2004 .Dekel , O. , et al . , \" Large Margin Hierarchical Classification , \" In Proceedings of 21st Int'l Conf . on Machine Learning ( ICML-2004 ) , Banff , Canada , 2004 .", "label": "", "metadata": {}, "score": "74.76373"}
{"text": "For a better understanding of the aforementioned aspects of the invention as well as additional aspects and embodiments thereof , reference should be made to the Description of Embodiments below , in conjunction with the following drawings in which like reference numerals refer to corresponding parts throughout the figures .", "label": "", "metadata": {}, "score": "75.4892"}
{"text": "The foregoing description , for purpose of explanation , has been described with reference to specific embodiments .However , the illustrative discussions above are not intended to be exhaustive or to limit the invention to the precise forms disclosed .Many modifications and variations are possible in view of the above teachings .", "label": "", "metadata": {}, "score": "75.50305"}
{"text": "31 - 48 , Aug. 2004 .Strehl , A. , et al . , \" Impact of Similarity Measures on Web - Page Clustering , \" In AAAI-2000 Workshop on Artificial Intelligence for Web Search , pp .58 - 64 , 2000 .", "label": "", "metadata": {}, "score": "75.97"}
{"text": "In the above example , the value \" 474 \" in attr 4 is just an identifier , whose value corresponds to a specific category in a product hierarchy tree .For product offers with these attributes , three types of basis functions 240 may be used - f cos , f num , and f cat -each of which operates on attribute values of a particular type : . f cos ( str 1 , str 2 ): cosine similarity between string values str 1 and str 2 : .", "label": "", "metadata": {}, "score": "76.046814"}
{"text": "Ratsch , G. ( 1998 ) .Ensemble learning methods for classification .Masters thesis , Dept .Computer Science , Univ .Potsdam .Ratsch , G. , Onoda , T. and Muller , K. R. ( 2000 ) .Soft margins for AdaBoost .", "label": "", "metadata": {}, "score": "76.25636"}
{"text": "training module 250 that uses training records 260 to calculate and/or modify the weights in the weighted set of basis similarity functions for a given composite similarity function ; and .composite similarity functions 270 that are created by function generator 230 and used to identify co - referent records in records database 220 .", "label": "", "metadata": {}, "score": "77.08472"}
{"text": "These modules ( i.e. , sets of instructions ) need not be implemented as separate software programs , procedures or modules , and thus various subsets of these modules may be combined or otherwise re - arranged in various embodiments .In some embodiments , memory 206 may store a subset of the modules and data structures identified above .", "label": "", "metadata": {}, "score": "77.3923"}
{"text": "Bhattacharya , I. , et al . , \" Iterative Record Linkage for Cleaning and Integration , \" In Proceeding of the SIGMOD-2004 Workshop on Research Issues on Data Mining and Knowledge Discovery ( DMKD-04 ) , pp .11 - 18 , 2004 .", "label": "", "metadata": {}, "score": "77.86551"}
{"text": "The composite similarity functions 270 can then be used to identify records in database 220 that correspond to the same product .In some embodiments , some or all of the records that correspond to the same product are sent to client 104 for display in GUI 112 .", "label": "", "metadata": {}, "score": "78.50884"}
{"text": "September 5 - 7 , 2001 .Boosting trees for anti - spam email filtering .Conference Proceedings RANLP2001 , Tzigov Chark , Bulgaria .Collins , M. , Schapire , R. E. , & Singer , Y. ( 2002 ) .", "label": "", "metadata": {}, "score": "79.660385"}
{"text": "334 - -343 ( with Y. Freund , R.E. Schapire and Y. Singer ) [ Postscript file ] .\" How to Use Expert Advice , \" in Journal of the ACM , Vol .44(3 ) , pp .427 - 485 , May 1997 ( with N. Cesa - Bianchi , Y. Freund , D.P. Helmbold , D. Haussler , and R.E. Schapire ) [ Postscript file ] . \"", "label": "", "metadata": {}, "score": "79.66356"}
{"text": "334 - -343 ( with Y. Freund , R.E. Schapire and Y. Singer ) [ Postscript file ] .\" How to Use Expert Advice , \" in Journal of the ACM , Vol .44(3 ) , pp .427 - 485 , May 1997 ( with N. Cesa - Bianchi , Y. Freund , D.P. Helmbold , D. Haussler , and R.E. Schapire ) [ Postscript file ] . \"", "label": "", "metadata": {}, "score": "79.66356"}
{"text": "Primary Examiner : .Robinson , Greta L. .Assistant Examiner : .Wilcox , James J. .Attorney , Agent or Firm : .Morgan , Lewis & Bockius LLP .Claims : .What is claimed is : . f .", "label": "", "metadata": {}, "score": "79.75186"}
{"text": "Consider the following example .When performing record linkage for product normalization , equivalence of book titles and author names is generally highly indicative of co - referent book records .So , the weight of the string similarity measure corresponding to the product name attribute should be high for the book domain .", "label": "", "metadata": {}, "score": "80.812325"}
{"text": "Although FIG .2 shows record linkage computer 102 as a number of discrete items , FIG .2 is intended more as a functional description of the various features which may be present in computer 102 rather than as a structural schematic of the embodiments described herein .", "label": "", "metadata": {}, "score": "81.42325"}
{"text": "The application of the composite similarity function provides a number that can be used to indicate whether two records relate to a common subject .In some embodiments , the common subject is a product .In other embodiments , the common subject is , without limitation : a seller ; a person ; a category , class , or other group of products ; or a reference .", "label": "", "metadata": {}, "score": "81.93198"}
{"text": "Jain , A.K. , et al . , \" Data Clustering :A Review , \" ACM Computing Surveys , vol .31 , No . 3 , pp .264 - 323 , 1999 .Jaro , M. , \" Advances in Record - Linkage Methodology as Applied to Matching the 1985 Census of Tampa , Florida , \" J. of the American Statistical Assoc . , vol .", "label": "", "metadata": {}, "score": "83.18307"}
{"text": "One of my favorite conjectures is the following ( posted as an open problem on the COLT web page ) : For any concept class of VC dimension d there is compression scheme of size d. In the case of orthogonal rectangles the VC dimension is four and the size the simple compression scheme is four as well .", "label": "", "metadata": {}, "score": "83.29762"}
{"text": "One of my favorite conjectures is the following ( posted as an open problem on the COLT web page ) : For any concept class of VC dimension d there is compression scheme of size d. In the case of orthogonal rectangles the VC dimension is four and the size the simple compression scheme is four as well .", "label": "", "metadata": {}, "score": "83.29762"}
{"text": "\" Sample Compression , Learnability , and the Vapnik - Chervonenkis Dimension \" , Machine Learning , Vol.21 ( 3 ) , pp .269- -304 , December 1995 ( with Sally Floyd ) [ Postscript file ] .The orginal unpublished manuscript that introduces compression schemes and gives sample complexity bounds . \"", "label": "", "metadata": {}, "score": "83.64567"}
{"text": "\" Sample Compression , Learnability , and the Vapnik - Chervonenkis Dimension \" , Machine Learning , Vol.21 ( 3 ) , pp .269- -304 , December 1995 ( with Sally Floyd ) [ Postscript file ] .The orginal unpublished manuscript that introduces compression schemes and gives sample complexity bounds . \"", "label": "", "metadata": {}, "score": "83.64567"}
{"text": "1 is a block diagram illustrating an exemplary distributed computer system according to an embodiment of the invention .This system includes record linkage computer 102 , multiple websites such as websites 108 and 110 , client computer 104 , and communication network(s ) 106 for interconnecting these components .", "label": "", "metadata": {}, "score": "84.46695"}
{"text": "BACKGROUND .Record linkage is the problem of identifying when two ( or more ) references to an object are referring to the same entity ( i.e. , the references are \" co - referent \" ) .One example of record linkage is identifying whether two paper citations ( which may be in different styles and formats ) refer to the same actual paper .", "label": "", "metadata": {}, "score": "87.455"}
{"text": "Memory 206 may include high speed random access memory and may also include non - volatile memory , such as one or more magnetic disk storage devices .Memory 206 may optionally include one or more storage devices remotely located from the CPU(s ) 202 .", "label": "", "metadata": {}, "score": "92.873474"}
{"text": "2 is a block diagram illustrating record linkage computer 102 in accordance with one embodiment of the present invention .Computer 102 typically includes one or more processing units ( CPU 's ) 202 , one or more network or other communications interfaces 204 , memory 206 , and one or more communication buses 214 for interconnecting these components .", "label": "", "metadata": {}, "score": "94.569916"}
{"text": "For example , consider the following two offers with four attributes each : .Offer A : . attr 1 , Product Name : Canon EOS 20D Digital SLR Body Kit ( Req .Lens ) USA . attr 2 , Product Price : $ 1499.00 .", "label": "", "metadata": {}, "score": "95.012825"}
{"text": "For example , \" Toshiba Satellite M35X - S309 notebook \" and \" Toshiba Satellite M35X - S309 notebook battery \" have a high textual similarity but refer to different products .At the same time , for high - end electronic items , price similarity is an important indicator of offer equivalence - the notebook and the battery records have very different prices , indicating that they are not co - referent .", "label": "", "metadata": {}, "score": "95.56033"}
{"text": "Offer B : . attr 1 , Product Name : Canon EOS 20d Digital Camera Body USA - Lens sold separately . attr 2 , Product Price : $ 1313.75 .attr 3 , Product Description : Canon EOS 20D is a digital , single - lens reflex , AF / AE camera with built - in flash , providing 8.2 megapixel resolution and up to 23 consecutive frames at 5 fps .", "label": "", "metadata": {}, "score": "97.85064"}
