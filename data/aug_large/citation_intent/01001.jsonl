{"text": "Each nugget will be assigned a weight equal to the number of assessors who judged it to be vital ; nugget weights will then be normalized so that the maximum weight of nuggets for each squishy list question is 1.0 .See ( Lin and Demner - Fushman , HLT / NAACL 2006 ) for details .", "label": "", "metadata": {}, "score": "37.714153"}
{"text": "Once the list of acceptable nuggets is created , the assessor will mark the nuggets contained in each [ answer - string , docid ] pair .Each nugget that is present will be counted only once .Some of the acceptable nuggets will be deemed vital , while other nuggets on the list are merely okay .", "label": "", "metadata": {}, "score": "47.250557"}
{"text": "In each case the goal was to retrieve small snippets of text that contain the actual answer to a question rather than the document lists traditionally returned by text retrieval systems .The best performing systems were able to answer about seventy per cent of the questions in TREC-8 and about sixty- ve per cent of the questions in TREC-9 .", "label": "", "metadata": {}, "score": "50.423"}
{"text": "The track established a common task for the retrieval and natural language processing research communities , creating a renaissance in questionanswering research .This wave of research has created significant progress in automatic natural language understanding as researchers incorporated sophisticated language processing into their question - answering systems .", "label": "", "metadata": {}, "score": "50.43087"}
{"text": "The track established a common task for the retrieval and natural language processing research communities , creating a renaissance in questionanswering research .This wave of research has created significant progress in automatic natural language understanding as researchers incorporated sophisticated language processing into their question - answering systems .", "label": "", "metadata": {}, "score": "50.43087"}
{"text": "Note that this means that if an answer - string contains multiple answer items for the question it will be marked inexact and will thus will not help the question 's score .In addition to judging the individual instances in a response , the assessors will also group correct instances into equivalence classes , where each equivalence class is considered a distinct answer item .", "label": "", "metadata": {}, "score": "50.976425"}
{"text": "In quantitative terms , the study estimated that the return on investment for every dollar spent on TREC was US$ 3 to $ 5 of benefits that accrued to information retrieval researchers .The study also enumerated a variety of qualitative benefits , concluding , in part , the following : .", "label": "", "metadata": {}, "score": "51.07619"}
{"text": "In quantitative terms , the study estimated that the return on investment for every dollar spent on TREC was US$ 3 to $ 5 of benefits that accrued to information retrieval researchers .The study also enumerated a variety of qualitative benefits , concluding , in part , the following : .", "label": "", "metadata": {}, "score": "51.07619"}
{"text": "In a separate body of QA research , no attempt was made to have systems understand text .Instead , the goal was to extract a small piece of text that answers the user 's question from a much larger bod ... . \" ...", "label": "", "metadata": {}, "score": "51.68548"}
{"text": "This new type of \" other \" questions puts more emphasis on the user aspect in the QA process -- an issue that has mostly been neglected in the QA community .The TREC criteria for what is a good answer to a given question has so far been rather vague , but QA systems dealt with this vagueness fairly effectively for factoid questions .", "label": "", "metadata": {}, "score": "51.877857"}
{"text": "The TRECVID community has contributed more than just research .They have donated essential parts of the evaluation infrastructure , including ground truth annotation systems and judgments , shot segmentation , automatic speech recognition , evaluation software , data hosting , and trained detectors .", "label": "", "metadata": {}, "score": "51.965237"}
{"text": "The TRECVID community has contributed more than just research .They have donated essential parts of the evaluation infrastructure , including ground truth annotation systems and judgments , shot segmentation , automatic speech recognition , evaluation software , data hosting , and trained detectors .", "label": "", "metadata": {}, "score": "51.965237"}
{"text": ".. s of domains is impractical .In addition , several theories of Q / A have been developed in the context of NLP or cognitive sciences .First , we have Permission to make digital o .. \" ...The Text REtrieval Conference ( TREC ) question answering track is an eort to bring the bene ts of large - scale evaluation to bear on a question answering ( QA ) task .", "label": "", "metadata": {}, "score": "52.241196"}
{"text": "A 2009 review article in Foundations and Trends in Information Retrieval 10 found the following : .Due to its widespread acceptance in the field , resulting in large participation of international teams from universities , research institutes , and corporate research labs , the TRECVID benchmark can be regarded as the de facto standard to evaluate performance of concept - based video retrieval research .", "label": "", "metadata": {}, "score": "52.98245"}
{"text": "A 2009 review article in Foundations and Trends in Information Retrieval 10 found the following : .Due to its widespread acceptance in the field , resulting in large participation of international teams from universities , research institutes , and corporate research labs , the TRECVID benchmark can be regarded as the de facto standard to evaluate performance of concept - based video retrieval research .", "label": "", "metadata": {}, "score": "52.98245"}
{"text": "As the track evolved , it was argued that this type of questions does not accurately model the needs of real users of QA technology .In addition to named entities as answers , users often search for definitions of concepts , or for summaries of important information about them .", "label": "", "metadata": {}, "score": "53.054573"}
{"text": "The researchers are also drawn in by the availability of data and scoring procedures that let them focus on the research task rather than infrastructure , and by the open forum for scientific comparison .The number of groups worldwide that are able to complete one of the tasks has grown , and new top performers continue to emerge .", "label": "", "metadata": {}, "score": "53.21144"}
{"text": "The researchers are also drawn in by the availability of data and scoring procedures that let them focus on the research task rather than infrastructure , and by the open forum for scientific comparison .The number of groups worldwide that are able to complete one of the tasks has grown , and new top performers continue to emerge .", "label": "", "metadata": {}, "score": "53.21144"}
{"text": "There is no expectation of an exact answer to squishy list questions , although responses will be penalized for excessive length .For each squishy list question , the assessor will create a list of acceptable information nuggets from the union of the returned responses and the information discovered during question development .", "label": "", "metadata": {}, "score": "53.622673"}
{"text": "This effectiveness has been demonstrated both in the laboratory on TREC test collections and by today 's operational systems that incorporate the techniques .Further , the techniques are routinely used on collections far larger than what was considered large in 1992 .", "label": "", "metadata": {}, "score": "53.924118"}
{"text": "This effectiveness has been demonstrated both in the laboratory on TREC test collections and by today 's operational systems that incorporate the techniques .Further , the techniques are routinely used on collections far larger than what was considered large in 1992 .", "label": "", "metadata": {}, "score": "53.924118"}
{"text": "TREC accomplished the original goal of building a large test collection early on ; indeed , it has now built dozens of test collections that are in use throughout the international research community .TREC 's greater accomplishment has been the establishment and validation of a research paradigm that continues to be extended to new tasks and application contexts every year .", "label": "", "metadata": {}, "score": "54.023804"}
{"text": "TREC accomplished the original goal of building a large test collection early on ; indeed , it has now built dozens of test collections that are in use throughout the international research community .TREC 's greater accomplishment has been the establishment and validation of a research paradigm that continues to be extended to new tasks and application contexts every year .", "label": "", "metadata": {}, "score": "54.023804"}
{"text": "Question Answering .Although a list of on - topic documents is undoubtedly useful , even that can be more information than a user wants to examine .The TREC question - answering track was introduced in 1999 to focus attention on the problem of returning the exact answer to a question .", "label": "", "metadata": {}, "score": "54.38479"}
{"text": "Question Answering .Although a list of on - topic documents is undoubtedly useful , even that can be more information than a user wants to examine .The TREC question - answering track was introduced in 1999 to focus attention on the problem of returning the exact answer to a question .", "label": "", "metadata": {}, "score": "54.38479"}
{"text": "The performance of the algorithms was verified on online product review articles ( \" digital camera \" and \" music \" reviews ) , and more general documents including general webpages and news articles .As a preprocessing step to our sentiment analysis , we extract sentences from input documents containing mentions of subject terms of interest .", "label": "", "metadata": {}, "score": "54.877953"}
{"text": "Voorhees admitted that success in the question - answering category has leveled off in the last few years in the area of traditional ad hoc tests , i.e. , new queries seeking document - based answers .In this area , top scores have shown little improvement .", "label": "", "metadata": {}, "score": "55.14566"}
{"text": "Annotation hierarchy subtask .The annotation hierarchy subtask results are shown in 7 , while the annotation hierarchy subtask plus evidence code results are shown in Table 8 .The primary evaluation measure for this task was the F - measure .", "label": "", "metadata": {}, "score": "55.279716"}
{"text": "Organizations can participate in TREC by responding to the call for participation issued each winter .Acknowledgments .C e r t ai n c o m m e r c i a l e n t ities , equipment , o r materials may be identified in this document to describe an experimental procedure or concept adequately .", "label": "", "metadata": {}, "score": "55.412666"}
{"text": "Organizations can participate in TREC by responding to the call for participation issued each winter .Acknowledgments .C e r t ai n c o m m e r c i a l e n t ities , equipment , o r materials may be identified in this document to describe an experimental procedure or concept adequately .", "label": "", "metadata": {}, "score": "55.412666"}
{"text": "We explain the accuracy of our system through the unique characteristics of its architecture : ( 1 ) usage of a wide - coverage answer type taxonomy ; ( 2 ) repeated passa ... \" .In this paper we present the features of a Question / Answering ( Q / A ) system that had unparalleled performance in the TREC-9 evaluations .", "label": "", "metadata": {}, "score": "55.59044"}
{"text": "Sample coverage of GO terms assigned to papers in the collection was very sparse .Determining papers containing GO term evidence will likely need to be treated as separate tasks for each concept represented in GO , and therefore require much denser sampling than was available in the data sets .", "label": "", "metadata": {}, "score": "55.66944"}
{"text": "F i g u r e 2 .T h e evolution of TRECVID in terms of ( a ) data and tasks and ( b ) participants .The digital video used in TRECVID has included broadcast news reports , unedited television program material , surveillance video , and nonprofessional Internet video .", "label": "", "metadata": {}, "score": "55.743095"}
{"text": "F i g u r e 2 .T h e evolution of TRECVID in terms of ( a ) data and tasks and ( b ) participants .The digital video used in TRECVID has included broadcast news reports , unedited television program material , surveillance video , and nonprofessional Internet video .", "label": "", "metadata": {}, "score": "55.743095"}
{"text": "Because of these results we further analyzed the text collections , comparing the features identified as strong predictors in the training data ( papers from the year 2002 ) with those in the test data ( papers from the year 2003 ) .", "label": "", "metadata": {}, "score": "55.915237"}
{"text": "To some extent all of these explanations may play a factor , but the last is probably the dominant factor .The GO triage task appears significantly more difficult than previously studied biomedical document triage tasks .In the 2002 Knowledge Discovery and Data Mining ( KDD )", "label": "", "metadata": {}, "score": "56.135345"}
{"text": "In each case the goal was to retrieve small snippets of text that co ... \" .The Text REtrieval Conference ( TREC ) question answering track is an eort to bring the bene ts of large - scale evaluation to bear on a question answering ( QA ) task .", "label": "", "metadata": {}, "score": "56.213264"}
{"text": "There will be two types of questions , each with its own evaluation metric . \"Which countries would like to build nuclear power plants ? \" return : strings containing an answer to the question .evaluation : nugget pyramid evaluation used for \" Other \" questions in the TREC 2006 - 2007 QA track .", "label": "", "metadata": {}, "score": "56.268715"}
{"text": "The final score for a run will be the mean of the per - series combined scores .Assessment environment .NIST assessors assess the answer - strings with respect to the viewable text of the supporting document as viewed from a web browser .", "label": "", "metadata": {}, "score": "56.50472"}
{"text": "Nonetheless she is hopeful and , when such brilliant breakthroughs finally occur , she expects that some of the first glimmers will appear at TREC .Major Progress .In areas outside question - answering , Voorhees has seen major progress .", "label": "", "metadata": {}, "score": "56.594112"}
{"text": "In particular , this means that it 's not valid to compare the scores from different years in TREC , because each TREC built a new ( different ) test collection .For pooling to be an effective strategy , it 's necessary to have a wide diversity of retrieval approaches contributing to the pools .", "label": "", "metadata": {}, "score": "56.86293"}
{"text": "In particular , this means that it 's not valid to compare the scores from different years in TREC , because each TREC built a new ( different ) test collection .For pooling to be an effective strategy , it 's necessary to have a wide diversity of retrieval approaches contributing to the pools .", "label": "", "metadata": {}, "score": "56.86293"}
{"text": "Therefore the KDD measure did not take into account a curator preference for not missing many positive articles as we have done here , equally weighted correct prediction of positives and negatives , and had a proportion of positives approaching 50 % in the test collection .", "label": "", "metadata": {}, "score": "56.88703"}
{"text": "Beyond the TREC workshops but still at NIST , TRECVID has evolved in many ways since its inception as a TREC track ( see Figure 2 ) .System tasks included search using multimedia topics , high - level feature extraction , shot and story boundary determination , and camera motion detection .", "label": "", "metadata": {}, "score": "57.122246"}
{"text": "Beyond the TREC workshops but still at NIST , TRECVID has evolved in many ways since its inception as a TREC track ( see Figure 2 ) .System tasks included search using multimedia topics , high - level feature extraction , shot and story boundary determination , and camera motion detection .", "label": "", "metadata": {}, "score": "57.122246"}
{"text": "Lack of Publicity .In Internet time , 10 years counts as a century , at least .The conferences have held developers to the grindstone of the state of the art , not just the acceptable state of the market .", "label": "", "metadata": {}, "score": "57.18765"}
{"text": "TREC also has a Web track that works with a snapshot of the Web as a document set for search engines .Voorhees discussed the conference 's role in advancing text retrieval services .She pointed to participation in TREC as a grounding for future start - ups coming out of academic settings .", "label": "", "metadata": {}, "score": "57.28646"}
{"text": "In many cases , the technology to support these varied types of search is still maturing .How is progress made in search technology ?How do search engine developers know what works and why ?Origins of TREC .Search algorithms are generally developed by comparing alternative approaches on benchmark tasks called test collections .", "label": "", "metadata": {}, "score": "57.84909"}
{"text": "In many cases , the technology to support these varied types of search is still maturing .How is progress made in search technology ?How do search engine developers know what works and why ?Origins of TREC .Search algorithms are generally developed by comparing alternative approaches on benchmark tasks called test collections .", "label": "", "metadata": {}, "score": "57.84909"}
{"text": "Finally , community members are frequently a source of data and use cases for new tasks .When TREC began , there was real doubt as to whether the statistical systems that had been developed in the research labs ( as opposed to the operational systems that used Boolean searches on manually indexed collections ) could effectively retrieve documents from large collections .", "label": "", "metadata": {}, "score": "57.96045"}
{"text": "Finally , community members are frequently a source of data and use cases for new tasks .When TREC began , there was real doubt as to whether the statistical systems that had been developed in the research labs ( as opposed to the operational systems that used Boolean searches on manually indexed collections ) could effectively retrieve documents from large collections .", "label": "", "metadata": {}, "score": "57.96045"}
{"text": "The second caveat is that images and stylesheets are loaded from the web , rather than from a local cache , so if that data has changed then the page appearance can be different .For blogs this last is less of a problem than for the general Web , based on past NIST experience .", "label": "", "metadata": {}, "score": "57.966946"}
{"text": "It automatica ... \" .It is , however , very costly to collect and analyze survey data manually .This paper presents a new framework for mining product reputations on the Internet .It automatically collects people 's opinions about target products from Web pages , and uses text mining techniques to obtain reputations of the products .", "label": "", "metadata": {}, "score": "58.055954"}
{"text": "The ability of search engines to point users to the information they seek has been fundamental to the Web 's success .As noted earlier , improvement in retrieval effectiveness ca n't be determined simply by looking at TREC scores from year to year .", "label": "", "metadata": {}, "score": "58.092796"}
{"text": "The ability of search engines to point users to the information they seek has been fundamental to the Web 's success .As noted earlier , improvement in retrieval effectiveness ca n't be determined simply by looking at TREC scores from year to year .", "label": "", "metadata": {}, "score": "58.092796"}
{"text": "Having a standard , widely available , and carefully constructed set of data laid the groundwork for further innovation in this field .The yearly TREC conference fostered collaboration , innovation , and a measured dose of competition ( and bragging rights ) that led to better information retrieval .", "label": "", "metadata": {}, "score": "58.218826"}
{"text": "Having a standard , widely available , and carefully constructed set of data laid the groundwork for further innovation in this field .The yearly TREC conference fostered collaboration , innovation , and a measured dose of competition ( and bragging rights ) that led to better information retrieval .", "label": "", "metadata": {}, "score": "58.218826"}
{"text": "TREC 's approach of evaluating competing technologies on a common problem set has proved to be a powerful way to improve current technology and hasten technology transfer .Hal Varian , Google 's chief economist , described TREC 's impact in a 2008 post 13 on the Google blog : .", "label": "", "metadata": {}, "score": "58.417088"}
{"text": "TREC 's approach of evaluating competing technologies on a common problem set has proved to be a powerful way to improve current technology and hasten technology transfer .Hal Varian , Google 's chief economist , described TREC 's impact in a 2008 post 13 on the Google blog : .", "label": "", "metadata": {}, "score": "58.417088"}
{"text": "From the Web pages , AnswerBus extracts sentences that are determined to contain answers .Its current rate of correct answers to TREC-8 's 200 questions is 70.5 % with the average response time to the questions being seven seconds .The performance of AnswerBus in terms of accuracy and response time is better than other similar systems .", "label": "", "metadata": {}, "score": "58.556973"}
{"text": "That includes META tags , comments , and other text that is only visible by viewing the HTML source .The NIST assessment platform displays the blog documents to the assessors using a web browser .It tries to show the document as closely as possible to how it would be seen during normal browsing , with two caveats .", "label": "", "metadata": {}, "score": "58.78689"}
{"text": "Technology transfer occurs across research teams within TRECVID and the wider video analytics community .Approaches that work for one system in one year 's task are commonly adopted with variations by other systems in the next year 's work .As a laboratory exercise with prototype systems , TRECVID results tend to be indicative rather than conclusive .", "label": "", "metadata": {}, "score": "58.85492"}
{"text": "Technology transfer occurs across research teams within TRECVID and the wider video analytics community .Approaches that work for one system in one year 's task are commonly adopted with variations by other systems in the next year 's work .As a laboratory exercise with prototype systems , TRECVID results tend to be indicative rather than conclusive .", "label": "", "metadata": {}, "score": "58.85492"}
{"text": "The QA track at the annual Text REtrieval Conferences ( TREC , [ 20 ] ) has become an important factor in shaping and giving direction to QA research .Introduced in 1999 , this track attracts a significant number of participants each year , and provides a focal point for much modern QA research .", "label": "", "metadata": {}, "score": "59.06386"}
{"text": "Figures 4 and 5 show that the sampling and coverage of GO terms in the training and testing sets , as well as the combined collection , is very sparse , both in terms of individual GO terms , and for papers containing evidence for common GO terms .", "label": "", "metadata": {}, "score": "59.119835"}
{"text": "TREC therefore introduced a track structure whereby a given TREC contained several retrieval subtasks that were each the focus of their own evaluation challenges .Figure 1 shows ( most of ) the tracks that were run in the different years of TREC , grouping the tracks by the dimension that differentiates them from one another .", "label": "", "metadata": {}, "score": "59.607796"}
{"text": "TREC therefore introduced a track structure whereby a given TREC contained several retrieval subtasks that were each the focus of their own evaluation challenges .Figure 1 shows ( most of ) the tracks that were run in the different years of TREC , grouping the tracks by the dimension that differentiates them from one another .", "label": "", "metadata": {}, "score": "59.607796"}
{"text": "Under the utility model , a system is penalized for returning nonrelevant information .In the filtering track collections , as in real life , there tends to be only a small number of relevant documents in a stream of millions of documents .", "label": "", "metadata": {}, "score": "59.91607"}
{"text": "Under the utility model , a system is penalized for returning nonrelevant information .In the filtering track collections , as in real life , there tends to be only a small number of relevant documents in a stream of millions of documents .", "label": "", "metadata": {}, "score": "59.91607"}
{"text": "The evaluation measures for the annotation subtasks were based on the notion of identifying tuples of data .Given the article and gene , systems designated one or both of the following tuples : .We employed a global recall , precision , and F - measure evaluation measure for each subtask : .", "label": "", "metadata": {}, "score": "60.08727"}
{"text": "Mailing List .The mailing list for the Question Answering Track is trec-qa@nist.gov .The list is used to discuss and define the task guidelines for the track , as well as for general discussion related to QA and its evaluation .", "label": "", "metadata": {}, "score": "60.45982"}
{"text": "There is no guarantee that these rankings will contain all or even any of the documents that actually answer the questions in a series .The document lists will be in the same format as previous years of TREC QA : . qnum rank docid rsv .", "label": "", "metadata": {}, "score": "60.657146"}
{"text": "Their hope is that a library of standard ontologies will come into common usage , enabling agents such as Expose to learn the information encoded on the Web .The knowledge base contains meta - information about the content of the Web , so that a query to START returns relevant ... .", "label": "", "metadata": {}, "score": "60.812035"}
{"text": "There is a large potential benefit to biomedical curation , and work in this area must continue to realize fully the advantages the automated biomedical document classification and text mining could bring to biomedical research .Competing interests .Authors ' contributions .", "label": "", "metadata": {}, "score": "60.87731"}
{"text": "Over time , MGI will collect vast amounts of data during the natural course of curating documents each year , but it may be a very long time before adequate numbers of samples are available for all GO codes .Selecting data specifically to train and test classification systems for identifying papers containing evidence for the most common GO codes and other , more specifically defined triage scenarios ( such as embryological expression ) may be more tractable tasks to address in the near term .", "label": "", "metadata": {}, "score": "61.097046"}
{"text": "One subtask of the categorization task required the triage of articles likely to have experimental evidence warranting the assignment of GO terms , while the other two subtasks were concerned with the assignment of the three top - level GO categories to each paper containing evidence for these categories .", "label": "", "metadata": {}, "score": "61.354683"}
{"text": "Overall , analysis of the results showed that systems did quite well , with the best system achieving an F - measure of 78 % on making yes / no decisions on papers , similar to the triage decision required in the TREC task [ 9 ] .", "label": "", "metadata": {}, "score": "61.471615"}
{"text": "When the track began , it was common for the two sides involved in litigation to negotiate a Boolean expression that defined the discovery result set .Then , humans would examine each document retrieved to determine its responsiveness to the discovery request .", "label": "", "metadata": {}, "score": "61.61065"}
{"text": "When the track began , it was common for the two sides involved in litigation to negotiate a Boolean expression that defined the discovery result set .Then , humans would examine each document retrieved to determine its responsiveness to the discovery request .", "label": "", "metadata": {}, "score": "61.61065"}
{"text": "5 After every TREC , they ran each system on each test collection .For every test collection , the later versions of the SMART system were much more effective than the earlier versions , with the later scores approximately twice that of the earlier scores .", "label": "", "metadata": {}, "score": "61.655025"}
{"text": "5 After every TREC , they ran each system on each test collection .For every test collection , the later versions of the SMART system were much more effective than the earlier versions , with the later scores approximately twice that of the earlier scores .", "label": "", "metadata": {}, "score": "61.655025"}
{"text": "An evaluation methodology encompasses the task , the metrics , and a statement of the valid interpretations of the metrics ' scores .A standard evaluation methodology allows results to be compared across different systems , which is important not so there can be winners of retrieval competitions , but because it facilitates the consolidation of a wider variety of results than any one research group can tackle .", "label": "", "metadata": {}, "score": "61.749687"}
{"text": "An evaluation methodology encompasses the task , the metrics , and a statement of the valid interpretations of the metrics ' scores .A standard evaluation methodology allows results to be compared across different systems , which is important not so there can be winners of retrieval competitions , but because it facilitates the consolidation of a wider variety of results than any one research group can tackle .", "label": "", "metadata": {}, "score": "61.749687"}
{"text": "Released : June 11 , 2005 .Aranea is a Web - based factoid question answering system that uses a combination of data redundancy and database techniques .Its performance in TREC 2002 , TREC 2003 , and TREC 2004 was competitive .", "label": "", "metadata": {}, "score": "61.80333"}
{"text": "As a service to the track , for each target , NIST will provide the ranking of the top 1000 documents retrieved by the PRISE search engine when using the target as the query .NIST will not provide document lists for individual questions .", "label": "", "metadata": {}, "score": "62.053726"}
{"text": "Amid this discontent , NIST was asked to build a large test collection for use in evaluating text retrieval technology developed as part of the US Defense Advanced Research Projects Agency 's ( DARPA )Tipster project .2 NIST agreed to construct a large test collection using a workshop format that would also support examination of the larger issues surrounding test collection use .", "label": "", "metadata": {}, "score": "62.258522"}
{"text": "Amid this discontent , NIST was asked to build a large test collection for use in evaluating text retrieval technology developed as part of the US Defense Advanced Research Projects Agency 's ( DARPA )Tipster project .2 NIST agreed to construct a large test collection using a workshop format that would also support examination of the larger issues surrounding test collection use .", "label": "", "metadata": {}, "score": "62.258522"}
{"text": "TREC 's innovation was to use pooling 3 to build the relevance sets for large document sets .A pool is the union of the top X documents retrieved by each of the participating systems ' searches for a given topic .", "label": "", "metadata": {}, "score": "62.337673"}
{"text": "TREC 's innovation was to use pooling 3 to build the relevance sets for large document sets .A pool is the union of the top X documents retrieved by each of the participating systems ' searches for a given topic .", "label": "", "metadata": {}, "score": "62.337673"}
{"text": "Five search engines and directories are used to retr ... \" .AnswerBus is an open - domain question answering system based on sentence level Web information retrieval .It accepts users ' natural - language questions in English , German , French , Spanish , Italian and Portuguese and provides answers in English .", "label": "", "metadata": {}, "score": "62.337677"}
{"text": "TAC 2008 Question Answering Track Guidelines .Contents : . I. Overview .The goal of the TAC QA track is to foster research on systems that search large document collections and retrieve precise answers to questions ( rather than entire documents ) .", "label": "", "metadata": {}, "score": "62.492996"}
{"text": "Questions within a series must be processed in order , without looking ahead .That is , the system may use the information in the questions and system - produced answers of earlier questions in a series to answer later questions in the series , but the system may not look at a later question in the series before answering the current question .", "label": "", "metadata": {}, "score": "62.497864"}
{"text": "The mean F - measure for the annotation plus evidence codes subtask was 0.3676 , with a top score of 0.4224 .Gene name recognition was found to be of benefit for this task .Conclusion .Automated classification of documents for GO annotation is a challenging task , as was the automated extraction of GO code hierarchies and evidence codes .", "label": "", "metadata": {}, "score": "62.547997"}
{"text": "9 The copy detection test data was the same in 2010 and 2011 , whereas the test queries ( 11,256 ) were randomly created .This allows comparison of systems .Top teams ' average scores for both detection and localization were better in 2011 than in 2010 .", "label": "", "metadata": {}, "score": "62.64341"}
{"text": "9 The copy detection test data was the same in 2010 and 2011 , whereas the test queries ( 11,256 ) were randomly created .This allows comparison of systems .Top teams ' average scores for both detection and localization were better in 2011 than in 2010 .", "label": "", "metadata": {}, "score": "62.64341"}
{"text": "So why did more elaborate systems not outperform this ?There are a variety of possible explanations : .How do we determine a more appropriate means of computing utility that more accurately reflects the needs of the MGI curators ?Perhaps an online - style ( incremental ) training and evaluation method would be more appropriate than the batch method that we used here .", "label": "", "metadata": {}, "score": "62.918427"}
{"text": "91 - 104 .12 P. Over , \" Instance Search , Copy Detection , Semantic Indexing @TRECVID , \" US Nat'l Inst .14 RTI Int'l , Economic Impact Assessment of NIST 's Text Retrieval Conf .gov / director / planning / impact_assessment . cfm .", "label": "", "metadata": {}, "score": "62.942112"}
{"text": "91 - 104 .12 P. Over , \" Instance Search , Copy Detection , Semantic Indexing @TRECVID , \" US Nat'l Inst .14 RTI Int'l , Economic Impact Assessment of NIST 's Text Retrieval Conf .gov / director / planning / impact_assessment . cfm .", "label": "", "metadata": {}, "score": "62.942112"}
{"text": "We have developed an architecture that augments existing search engines so that they support natural language question answering .The process entails five step ... \" .Web - based search engines such as Google and NorthernLight return documents that are relevant to a user query , not answers to user questions .", "label": "", "metadata": {}, "score": "62.99716"}
{"text": "\" This last requirement has been dubbed \" other \" questions [ 20].In our view , the task presented at the TREC 2004 QA track , and the introduction of the \" other ' questions makes a big step towards more realistic user scenarios .", "label": "", "metadata": {}, "score": "62.998863"}
{"text": "E - Discovery .The legal track was started in 2006 to focus specifically on the problem of e - discovery , the effective production of electronically stored information as evidence in litigation and regulatory settings .Today 's organizations depend on electronic records rather than paper records , but the volume of data and its potentially ephemeral nature have overwhelmed traditional legal discovery procedures and practices .", "label": "", "metadata": {}, "score": "63.00759"}
{"text": "E - Discovery .The legal track was started in 2006 to focus specifically on the problem of e - discovery , the effective production of electronically stored information as evidence in litigation and regulatory settings .Today 's organizations depend on electronic records rather than paper records , but the volume of data and its potentially ephemeral nature have overwhelmed traditional legal discovery procedures and practices .", "label": "", "metadata": {}, "score": "63.00759"}
{"text": "A description of the manual processing ( if any ) will also be requested .whether the entire Blog06 collection was searched for each target , or whether the search was limited to the text of the top 50 documents distributed by NIST .", "label": "", "metadata": {}, "score": "63.079132"}
{"text": "F i g u r e 1 .T e x t Retrieval Conference ( TREC ) tracks and the years in which they were held .The track name is listed on the right , and the track 's focus is listed on the left .", "label": "", "metadata": {}, "score": "63.10671"}
{"text": "F i g u r e 1 .T e x t Retrieval Conference ( TREC ) tracks and the years in which they were held .The track name is listed on the right , and the track 's focus is listed on the left .", "label": "", "metadata": {}, "score": "63.10671"}
{"text": "Restrictions .For automatic runs , no changes can be made to any component of the QA system or any resource used by the system in response to the test targets and questions .If there is any manual processing of questions , answers or any other part of a system or the resources that it uses , then the resulting run must be classified as a manual run .", "label": "", "metadata": {}, "score": "63.131775"}
{"text": "An individual squishy list question will be scored using nugget recall ( NR ) and an approximation to nugget precision ( NP ) based on length .In particular : .Let : .Then : .Overall score .The different types of questions ( rigid list and squishy list ) have different scoring metrics , but each of the two scores has a range of [ 0.0 , 1.0 ] with 1.0 being the high score .", "label": "", "metadata": {}, "score": "63.231884"}
{"text": "Soc . of Information Sci\u00ad ence and Technolog y , vol .62 , no .4 , 2011 , pp .613 - 627 .9 C.G.M. Snoek et al . , \" Any Hope for Cross - Domain Concept Detection in Internet Video , \" MediaMill TRECVID 2010 , www - nlpir . nist.gov/projects/tvpubs/ tv10.slides/mediamill.tv10.slides.pdf . 10 C.G.M. Snoek and M. Worring , \" Concept - based Video Retrieval , \" Foundations and Trends in Information Retrieval , vol .", "label": "", "metadata": {}, "score": "63.24245"}
{"text": "Soc . of Information Sci\u00ad ence and Technolog y , vol .62 , no .4 , 2011 , pp .613 - 627 .9 C.G.M. Snoek et al . , \" Any Hope for Cross - Domain Concept Detection in Internet Video , \" MediaMill TRECVID 2010 , www - nlpir . nist.gov/projects/tvpubs/ tv10.slides/mediamill.tv10.slides.pdf . 10 C.G.M. Snoek and M. Worring , \" Concept - based Video Retrieval , \" Foundations and Trends in Information Retrieval , vol .", "label": "", "metadata": {}, "score": "63.24245"}
{"text": "However , in February 2005 , NIST will publish the proceedings on the Web site .And this reporter / editor , for one , expects to produce copy on the 2004 conference as soon as humanly possible .TAC 2008 Question Answering Track .", "label": "", "metadata": {}, "score": "63.541054"}
{"text": "However , the new genomics track established last year does tap into the National Library of Medicine 's PubMed collection of text .In this area , an NSF grant helps fund the judging process .It has become harder and harder to crack the final steps to complete answer extraction .", "label": "", "metadata": {}, "score": "63.55411"}
{"text": "The track had a major impact in the legal community , including citations in judicial opinions ( see en.wikipedia.org/wiki/Paul_W._Grimm ) .Its main result was engendering conversation on the process by which e - discovery should be done by showing that an iterative process that included a human in the search loop almost always outperformed one - off searches .", "label": "", "metadata": {}, "score": "63.809155"}
{"text": "The track had a major impact in the legal community , including citations in judicial opinions ( see en.wikipedia.org/wiki/Paul_W._Grimm ) .Its main result was engendering conversation on the process by which e - discovery should be done by showing that an iterative process that included a human in the search loop almost always outperformed one - off searches .", "label": "", "metadata": {}, "score": "63.809155"}
{"text": "Rigid - list - score .The final answer set for a rigid list question will be created from the union of the distinct answer items returned by all participants and answer items found by the NIST assessor during question development .", "label": "", "metadata": {}, "score": "63.908478"}
{"text": "While this is most likely due to the sparse sampling of individual GO topics , there is currently insufficient evidence to determine the practical significance and generality of this , and whether this is a general problem for biomedical document classification .", "label": "", "metadata": {}, "score": "63.91025"}
{"text": "3 , pp .53 - 75 .5 C. Buckley and J. Walz , \" SMART at TREC-8 , \" Proc . 8 th Text Retrieval Conf .( TREC 99 ) , 1999 , pp .577 - 582 .", "label": "", "metadata": {}, "score": "63.944916"}
{"text": "3 , pp .53 - 75 .5 C. Buckley and J. Walz , \" SMART at TREC-8 , \" Proc . 8 th Text Retrieval Conf .( TREC 99 ) , 1999 , pp .577 - 582 .", "label": "", "metadata": {}, "score": "63.944916"}
{"text": "The answer - string does not have to appear literally in a document in order for the document to support it as being a correct answer item .An answer - string must contain a complete , exact answer item and nothing else .", "label": "", "metadata": {}, "score": "63.99026"}
{"text": "The TREC-8 Question Answering ( QA )Track was the first large - scale evaluation of domain - independent question answering systems .In addition to fostering research on the QA task , the track was used to investigate whether the evaluation methodology used for document retrieval is appropriate for a different natural language processing task .", "label": "", "metadata": {}, "score": "64.06614"}
{"text": "At present , articles are searched in a Web browser one at a time because full - text searching is not available for all of the journals included in MGI .Triage .The second step is to determine whether the identified articles should be sent for curation .", "label": "", "metadata": {}, "score": "64.08628"}
{"text": "Because the system has only a little training data at the beginning of the stream , its initial performance tends to be poor .To refine its user model , it must show many promising , but ultimately nonrelevant , documents to the user .", "label": "", "metadata": {}, "score": "64.18225"}
{"text": "Because the system has only a little training data at the beginning of the stream , its initial performance tends to be poor .To refine its user model , it must show many promising , but ultimately nonrelevant , documents to the user .", "label": "", "metadata": {}, "score": "64.18225"}
{"text": "This data includes judgment files , answer patterns , top ranked document lists , and sentence files .Document set .Answers for all questions in the test set must be drawn from the TREC Blog06 collection .The TREC Blog06 collection is a large sample of the blogsphere , and contains spam as well as possibly non - blogs , e.g. RSS feeds from news broadcasters .", "label": "", "metadata": {}, "score": "64.32648"}
{"text": "As noted above , some of these papers had not yet been annotated .Negative examples were all papers not designated for GO annotation in the operational MGI system .For the training data ( 2002 ) , there were 375 positive examples , and 5462 negative examples .", "label": "", "metadata": {}, "score": "64.35455"}
{"text": "Despite this success , much remains to be done .Computers are still unable to truly comprehend content generated for human consumption even as content stores grow ever larger .The TREC and TRECVID workshops will continue for the foreseeable future , focusing retrieval research on problems that have significant impact for both the retrieval research community and the broader user community .", "label": "", "metadata": {}, "score": "64.37718"}
{"text": "Despite this success , much remains to be done .Computers are still unable to truly comprehend content generated for human consumption even as content stores grow ever larger .The TREC and TRECVID workshops will continue for the foreseeable future , focusing retrieval research on problems that have significant impact for both the retrieval research community and the broader user community .", "label": "", "metadata": {}, "score": "64.37718"}
{"text": "The rigid list questions will be evaluated using the same methodology used to evaluate list questions in past TREC QA Track tasks .The squishy list questions will be evaluated with the nugget Pyramid method used to evaluate complex questions as described in ( Dang and Lin , 2007 ) .", "label": "", "metadata": {}, "score": "64.38626"}
{"text": "Participants in the testing mainly come from universities ( well over half , according to Ellen Voorhees , TREC project manager at NIST ) , but commercial operations ( such as Microsoft ) can also participate .In any case , as anyone saving pennies to buy into Google 's IPO knows , universities very often nurture the future talent that can take the information industry by storm .", "label": "", "metadata": {}, "score": "64.434975"}
{"text": "The rest of the articles are not entered into MGI .Our triage task involved correctly classifying which documents had been selected for GO annotation in this process .Annotation .The third step is the actual curation with GO terms .", "label": "", "metadata": {}, "score": "64.79218"}
{"text": "The TAC QA track evolved from the TREC QA track .Please visit the TREC website for additional information about Question Answering at TREC , including an open archive of TREC QA data and instructions on how to obtain additional detailed Past TREC Results for individual submitted runs .", "label": "", "metadata": {}, "score": "64.854095"}
{"text": "This article first appeared in IT Professional magazine and is brought to you by InfoQ & IEEE Computer Society .Search engines are developed using standard sets of realistic test cases that let developers measure the relative effectiveness of alternative approaches .", "label": "", "metadata": {}, "score": "64.940796"}
{"text": "NIST uses NekoHTML when parsing documents for indexing , and also to remove SCRIPT elements before sending documents to the browser .Building Better Search Engines by Measuring Search Quality .This article first appeared in IT Professional magazine and is brought to you by InfoQ & IEEE Computer Society .", "label": "", "metadata": {}, "score": "64.94751"}
{"text": "A rough count of workshop paper coauthors indicates that about 400 researchers are engaged in each year 's TRECVID experiments ( see Figure 2b ) .Although academic teams have predominated , commercial research laboratories have always been part of the mix .", "label": "", "metadata": {}, "score": "65.00254"}
{"text": "A rough count of workshop paper coauthors indicates that about 400 researchers are engaged in each year 's TRECVID experiments ( see Figure 2b ) .Although academic teams have predominated , commercial research laboratories have always been part of the mix .", "label": "", "metadata": {}, "score": "65.00254"}
{"text": "Table 1 shows the total number of articles in each journal and the number in each journal included in the subset used by the track .The SGML training document collection was 150 megabytes in size compressed and 449 megabytes uncompressed .", "label": "", "metadata": {}, "score": "65.08475"}
{"text": "We therefore highlight a sampling of tracks - filtering , question answering , and legal e - discovery - that have addressed particularly pressing search problems .Also included is the video retrieval track , which , driven by the growing availability of digital video , has grown into its own NIST workshop series : TRECVID .", "label": "", "metadata": {}, "score": "65.447296"}
{"text": "We therefore highlight a sampling of tracks - filtering , question answering , and legal e - discovery - that have addressed particularly pressing search problems .Also included is the video retrieval track , which , driven by the growing availability of digital video , has grown into its own NIST workshop series : TRECVID .", "label": "", "metadata": {}, "score": "65.447296"}
{"text": "National Institute of Standards and Technology ; 2004 .Darwish K , Madkour A : The GUC goes to TREC 2004 : using whole or partial documents for retrieval and classification in the Genomics Track : ; Gaithersburg , MD .Edited by Voorhees EM and Buckland LP .", "label": "", "metadata": {}, "score": "65.47766"}
{"text": "See Table 2 .It should also be noted that the MGI system is , like most operational databases , continuously updated , so the data for the track represented a snapshot of the database obtained in May , 2004 .The evaluation measure for the triage task was the utility measure often applied in text categorization research and used by the former TREC Filtering Track .", "label": "", "metadata": {}, "score": "65.49584"}
{"text": "National Institute of Standards and Technology ; 2004 .Using a Reference Corpus as a User Model for Focused Information Retrieval .ABSTRACT .We propose a method for ranking short information nuggets extracted from a text corpus , using another , reliable reference corpus as a user model .", "label": "", "metadata": {}, "score": "65.68544"}
{"text": "Subsequent testing verified that pooling as implemented in TREC finds a large majority of the relevant documents in a document set despite looking at only a tiny fraction of the whole collection .In addition , the testing further validated that retrieval systems that get higher scores on test collections built through pooling are generally more effective in practice than those that get lower scores .", "label": "", "metadata": {}, "score": "65.713"}
{"text": "Subsequent testing verified that pooling as implemented in TREC finds a large majority of the relevant documents in a document set despite looking at only a tiny fraction of the whole collection .In addition , the testing further validated that retrieval systems that get higher scores on test collections built through pooling are generally more effective in practice than those that get lower scores .", "label": "", "metadata": {}, "score": "65.713"}
{"text": "The TREC task was to determine whether the paper contained evidence for assignment of GO codes , any GO code .Currently , there are about 20,000 different terms in the GO , in the areas of cellular component , molecular function , and biological process .", "label": "", "metadata": {}, "score": "65.900635"}
{"text": "We first collect statements regarding target products using a general search engine , then , using the rules , extract opinions from them and attach to each of the opinions the labels . ... ions about specific products as expressed on the web .", "label": "", "metadata": {}, "score": "65.961044"}
{"text": "Details about the submission procedure will be emailed to the trec-qa@nist.gov mailing list when the test data is released .At that time , NIST will release a routine that checks for common errors in submission files including such things as invalid document numbers , wrong formats , missing data , etc .", "label": "", "metadata": {}, "score": "65.972244"}
{"text": "Speech retrieval has also made significant progress .In fact , Voorhees believes it has reached the point where it can become usable in large - scale services .Oddly , Web search engines , such as Google or Yahoo !Search , do not participate in TREC , although Voorhees assures us that they do follow TREC closely .", "label": "", "metadata": {}, "score": "66.1252"}
{"text": "References . 1 C.W. Cleverdon , \" The Cranfield Tests on Index Language Devices , \" Aslib Proc . , vol .19 , no .6 , 1967 , pp .173 - 192 .( Reprinted in Readings in Information Re\u00ad trieval , K. Sp\u00e4rck - Jones and P. Willett , eds . , Morgan Kaufmann , 1997 . ) 2 D. Harman , \" The DARPA TIPSTER Project , \" ACM SIGIR Forum , vol .", "label": "", "metadata": {}, "score": "66.135284"}
{"text": "References . 1 C.W. Cleverdon , \" The Cranfield Tests on Index Language Devices , \" Aslib Proc . , vol .19 , no .6 , 1967 , pp .173 - 192 .( Reprinted in Readings in Information Re\u00ad trieval , K. Sp\u00e4rck - Jones and P. Willett , eds . , Morgan Kaufmann , 1997 . ) 2 D. Harman , \" The DARPA TIPSTER Project , \" ACM SIGIR Forum , vol .", "label": "", "metadata": {}, "score": "66.135284"}
{"text": "by Ellen M. Voorhees , Dawn M. Tice - Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval . \" ...The TREC-8 Question Answering ( QA )Track was the first large - scale evaluation of domain - independent question answering systems .", "label": "", "metadata": {}, "score": "66.253365"}
{"text": "It also called for systems that would work beyond the English language or answer spoken questions .Humans assess the success of the automated retrieval processes and hold to strict standards .Other elements of the search process also receive careful evaluation .", "label": "", "metadata": {}, "score": "66.268234"}
{"text": "Additional experience will allow comparison and further analysis about which algorithmic features are most useful in biomedical document classification , and better understanding of the task characteristics that make automated classification feasible and useful for biomedical document curation .The TREC Genomics Track will be continuing in 2005 focusing on a wider range of triage tasks and improving results from 2004 .", "label": "", "metadata": {}, "score": "66.28899"}
{"text": "A few times , the track has spun off from TREC , and a community of interest established its own evaluation conference .de ) ; and the Forum for Information Retrieval Evaluation ( FIRE ) , which focuses on languages of the Indian subcontinent .", "label": "", "metadata": {}, "score": "66.311554"}
{"text": "A few times , the track has spun off from TREC , and a community of interest established its own evaluation conference .de ) ; and the Forum for Information Retrieval Evaluation ( FIRE ) , which focuses on languages of the Indian subcontinent .", "label": "", "metadata": {}, "score": "66.311554"}
{"text": "Rigid list questions require exact answers to be returned .Responses to \" squishy \" list questions need not be exact , though excessive length will be penalized .The questions used in previous TREC QA tracks are in the Data / QA section of the TREC web site .", "label": "", "metadata": {}, "score": "66.63631"}
{"text": "In the question - answering category , questions can extend from \" factoids , \" like the name of the river called the \" Big Muddy , \" to layered questions , like a list of chewing gum manufacturers .Other areas of investigation involve tasks that require judgment , like evaluating which articles take a different stance on an issue or what specific information new articles contribute to a breaking story .", "label": "", "metadata": {}, "score": "66.69374"}
{"text": "Therefore the corpus may contain many GO topics for which there are an inadequate number of cases to provide meaningful samples in both the test and training sets .For about 85 % of documents , the most common GO code associated with a document is found associated with two or more documents .", "label": "", "metadata": {}, "score": "66.71867"}
{"text": "The annotation decision was also not affected by this since the positive and negative samples were not exhaustive by design , that is , the data set for the annotation task did not include all article GO annotations made by MGI during this time period .", "label": "", "metadata": {}, "score": "66.79544"}
{"text": "The ciQA task provided a framework for participants to investigate interaction in the context of complex information needs , and was a blend of the TREC 2005 QA relationship task and the TREC 2005 HARD track .Multiple assessors judged the importance of information nuggets used to evaluate the ' ' Other ' ' responses and ciQA responses , resulting in an evaluation that is more stable and discriminative than using only a single assessor to judge nugget importance .", "label": "", "metadata": {}, "score": "67.00733"}
{"text": "The proposed ranking method makes a substantial improvement in the performance of our system .Categories and Subject Descriptors .General Terms .Information Retrieval .Keywords : Question Answering , Information Retrieval .INTRODUCTION .The area of Question Answering ( QA ) is at the focus of a lot of research interest lately , both in the Information Retrieval ( IR ) community and among Computational Linguists .", "label": "", "metadata": {}, "score": "67.174385"}
{"text": "The track used hypothetical complaints and corresponding requests to produce documents developed by practicing lawyers as topics .A designated \" topic authority \" played the role of the lead attorney in a case , setting forth a general strategy and guidelines for what made a document responsive to the request .", "label": "", "metadata": {}, "score": "67.22358"}
{"text": "The track used hypothetical complaints and corresponding requests to produce documents developed by practicing lawyers as topics .A designated \" topic authority \" played the role of the lead attorney in a case , setting forth a general strategy and guidelines for what made a document responsive to the request .", "label": "", "metadata": {}, "score": "67.22358"}
{"text": "For GO curation , MGI strives to select only the articles that contain evidence supporting assignment of a GO code to a specific gene .The goal of this triage process is to limit the number of articles sent to human curators for more exhaustive and specific analysis .", "label": "", "metadata": {}, "score": "67.24974"}
{"text": "Test data increased to 280 hours by 2009 .A summarization task was added against BBC rushes ( unedited program material ) , and an event detection task was added against airport surveillance video provided by the UK Home Office .Since 2010 , TRECVID has focused on diverse , often nonprofessional Internet video from various communitydonated sources in quantities from several hundred up to several thousand hours , extending the search and feature / event detection tasks while adding known - item and instance search to the evaluations ( see Figure 2a ) .", "label": "", "metadata": {}, "score": "67.25031"}
{"text": "Test data increased to 280 hours by 2009 .A summarization task was added against BBC rushes ( unedited program material ) , and an event detection task was added against airport surveillance video provided by the UK Home Office .Since 2010 , TRECVID has focused on diverse , often nonprofessional Internet video from various communitydonated sources in quantities from several hundred up to several thousand hours , extending the search and feature / event detection tasks while adding known - item and instance search to the evaluations ( see Figure 2a ) .", "label": "", "metadata": {}, "score": "67.25031"}
{"text": "We used values for u r and u nr that were driven by boundary cases for different results .In particular , we thought it was important that the measure have the following characteristics : .We fixed u nr at -1 as is typically done .", "label": "", "metadata": {}, "score": "67.25234"}
{"text": "Participants who submit a manual run must also submit a fully automatic run .Joint Blog Task The TAC 2008 QA task is part of a larger information retrieval , analysis , and synthesis task involving the TREC Blog Track and the TAC Summarization Track .", "label": "", "metadata": {}, "score": "67.52675"}
{"text": "An increasingly common approach is to develop \" model organism databases \" that bring together all the information for a specific organism into an easy to use format .These databases require extensive human effort for curation and annotation , which is usually done by PhD - level researchers .", "label": "", "metadata": {}, "score": "67.63652"}
{"text": "It takes real questions , even some supplied by virtual reference operations , and real problems , like detecting the novel or finding relevant information in non - English documents .One year they even required systems to assign confidence values to the answers detected .", "label": "", "metadata": {}, "score": "67.65608"}
{"text": "The TREC Genomics Track will be continuing in 2005 .The categorization task will consist of selecting papers for a set of four triage categories relevant to MGI curation , including allele phenotypes , embryologic expression , and tumor biology as well as repeating the GO triage categorization task with updated data .", "label": "", "metadata": {}, "score": "67.694115"}
{"text": "The community aspect is important to TREC 's success in other respects as well .TREC can benchmark current technology only if all retrieval approaches are represented .The annual TREC meeting facilitates technology transfer among different research groups as well as between research and development organizations .", "label": "", "metadata": {}, "score": "67.70163"}
{"text": "The community aspect is important to TREC 's success in other respects as well .TREC can benchmark current technology only if all retrieval approaches are represented .The annual TREC meeting facilitates technology transfer among different research groups as well as between research and development organizations .", "label": "", "metadata": {}, "score": "67.70163"}
{"text": "However , with the rising interest in \" answer products , \" as outlined in Microsoft 's new anti - Google strategizing and demanded by the small screens of wireless technology , it would seem that TREC 's approach would have more interest now than ever before .", "label": "", "metadata": {}, "score": "67.70475"}
{"text": "Submissions are due at NIST on or before July 1 , 2008 .Each team may submit up to three runs ( submission files ) for the QA task , ranked by priority .NIST will judge the first- and second - priority runs from each team and ( if resources allow ) up to one additional run from each team .", "label": "", "metadata": {}, "score": "67.79636"}
{"text": "The 2008 QA task focuses on finding answers to opinion questions .The 2008 QA task is similar to the main QA task in TREC 2007 in that the test set will consist of question series .However , each series in 2008 asks for people 's opinions about a particular target ( rather than general information about the target ) , and the questions will be asked over only blog documents .", "label": "", "metadata": {}, "score": "67.87407"}
{"text": "The system can then immediately adapt itself , based on that information .If the system decides not to show the document , any relevance information is missed .A filtering system 's effectiveness is scored using utility , a measure that rewards the system based on the number of relevant documents returned while penalizing it based on the number of nonrelevant documents returned .", "label": "", "metadata": {}, "score": "67.91847"}
{"text": "The system can then immediately adapt itself , based on that information .If the system decides not to show the document , any relevance information is missed .A filtering system 's effectiveness is scored using utility , a measure that rewards the system based on the number of relevant documents returned while penalizing it based on the number of nonrelevant documents returned .", "label": "", "metadata": {}, "score": "67.91847"}
{"text": "It did not contain any other synonyms . )There were 1418 unique possible document - gene pairs in the training data .The data from the first three rows of Table 4 differ from the rest in that they contained data merged from positive and negative examples .", "label": "", "metadata": {}, "score": "67.92485"}
{"text": "The best runs , i.e. , those with the highest F - measures , had medium levels of recall and precision .The top run came from Indiana University and used a variety of approaches , including a k - nearest - neighbor model , mapping terms to MeSH , using keyword and glossary fields of documents , and recognizing gene names [ 7 ] .", "label": "", "metadata": {}, "score": "68.047516"}
{"text": "Cohen AM , Bhuptiraju RT , Hersh W : Feature generation , feature selection , classifiers , and conceptual drift for biomedical document triage : ; Gaithersburg , MD .Edited by Voorhees EM and Buckland LP .National Institute of Standards and Technology ; 2004 .", "label": "", "metadata": {}, "score": "68.10423"}
{"text": "For the secondary subtask , systems must identify the evidence type code as well .Methods .Highwire is a \" value added \" electronic publisher of scientific journals .Most journals in their collection are published by professional associations , with the copyright remaining with the associations .", "label": "", "metadata": {}, "score": "68.21266"}
{"text": "The collection is 148 GB in size , consisting of : .38.6 GB of feeds .88.8 GB of permalink documents .28.8 GB of homepages .For the TAC QA task , each instance of an answer must be supported by a document from the permalinks component of the Blog06 collection .", "label": "", "metadata": {}, "score": "68.392975"}
{"text": "5 , 2005 , pp .99 - 122 .7 D. Ferrucci et al . , \" Building Watson : An Overview of the DeepQA Project , \" AI Magazine , vol .31 , no . 3 , 2010 , pp .", "label": "", "metadata": {}, "score": "68.65661"}
{"text": "5 , 2005 , pp .99 - 122 .7 D. Ferrucci et al . , \" Building Watson : An Overview of the DeepQA Project , \" AI Magazine , vol .31 , no . 3 , 2010 , pp .", "label": "", "metadata": {}, "score": "68.65661"}
{"text": "Because of the growing size and complexity of the biomedical literature , there is increasing effort devoted to structuring knowledge in databases .One of the many key efforts is to annotate the function of genes .A major use of the GO has been to annotate the genomes of organisms used in biological research .", "label": "", "metadata": {}, "score": "68.78142"}
{"text": "Each of these individual topics can be viewed as a separate yes / no classification task in itself .The GO triage categorization task may better be thought of as many subtasks , where classification of the presence / absence of each GO code is done individually , and the document is triaged for GO if classified as positive for any of the GO codes .", "label": "", "metadata": {}, "score": "68.811676"}
{"text": "NIST will evaluate the first- and second - priority runs from each team .If resources allow , NIST will evaluate the third - priority run from each team .NIST will post the questions on the TAC QA web site on June 24 and results will have to be submitted to NIST by 11:59 p.m. ( EDT ) on July 1 , 2008 .", "label": "", "metadata": {}, "score": "68.89465"}
{"text": "The process entails five steps : query modulation , document retrieval , passage extraction , phrase extraction , and answer ranking .In this paper we describe some probabilistic approaches to the last three of these stages .We show how our techniques apply to a number of existing search en-1 Radev et al .", "label": "", "metadata": {}, "score": "68.90018"}
{"text": "The rigid - list - score for a series is the mean of the F scores of the rigid list questions in the series .The squishy - list - score for a series is the mean of the F scores of the squishy list questions in the series .", "label": "", "metadata": {}, "score": "68.96391"}
{"text": "The only defect I can see in their strategy is one seen all too often by students of government information science and technology projects : lack of publicity .Well , that stops now .Most of the proceedings are available for downloading .", "label": "", "metadata": {}, "score": "69.16907"}
{"text": "To facilitate stable evaluations , especially when using test collections built from pooling , the standard methodology relies on average effectiveness over a set of topics in which each topic has relatively few relevant documents .But the real use case in e - discovery is gauging the effectiveness of a single response set when the number of responsive documents can be very large .", "label": "", "metadata": {}, "score": "69.181816"}
{"text": "To facilitate stable evaluations , especially when using test collections built from pooling , the standard methodology relies on average effectiveness over a set of topics in which each topic has relatively few relevant documents .But the real use case in e - discovery is gauging the effectiveness of a single response set when the number of responsive documents can be very large .", "label": "", "metadata": {}, "score": "69.181816"}
{"text": "Since the current process has value , but also leaves much room for improvement in efficiency , we estimated that a U norm in the range of 0.25 - 0.3 for the triage - everything condition would be appropriate .3 shows the value of U norm for all four boundary cases .", "label": "", "metadata": {}, "score": "69.31149"}
{"text": "For kernel sentences , SA extracts the following types of ternary expressions ( T - expressions)[7 ] : positive ornegative sentiment ... . by Satoshi Morinaga , Kenji Yamanishi , Kenji Tateishi , Toshikazu Fukushima , 2002 .It is , however , very costly to collect and analyze survey data manually .", "label": "", "metadata": {}, "score": "69.42105"}
{"text": "TREC helped educate graduate and undergraduate students , some who went on to lead IR companies and others who stayed in academia to teach and conduct research .TREC benefited IR product quality and availability - our research suggests that TREC motivated a large expansion in IR research that has enabled high quality applications such as web search , enterprise search , and domain - specific search products and services ( e.g. , for genomic analysis ) .", "label": "", "metadata": {}, "score": "69.43977"}
{"text": "TREC helped educate graduate and undergraduate students , some who went on to lead IR companies and others who stayed in academia to teach and conduct research .TREC benefited IR product quality and availability - our research suggests that TREC motivated a large expansion in IR research that has enabled high quality applications such as web search , enterprise search , and domain - specific search products and services ( e.g. , for genomic analysis ) .", "label": "", "metadata": {}, "score": "69.43977"}
{"text": "One of the goals of MGI is to provide structured , coded annotation of gene function from the biological literature .Human curators identify genes and assign GO codes about gene function with another code describing the type of experimental evidence supporting assignment of the GO code .", "label": "", "metadata": {}, "score": "69.478096"}
{"text": "The research paradigm is centered on community - based evaluations , called \" coopetitions , \" borrowing the neologism that reflects cooperation among competitors that leads to a greater good .The main element of the paradigm is the evaluation task , which is generally an abstraction of a user task that defines exactly what a system is expected to do .", "label": "", "metadata": {}, "score": "69.655914"}
{"text": "The research paradigm is centered on community - based evaluations , called \" coopetitions , \" borrowing the neologism that reflects cooperation among competitors that leads to a greater good .The main element of the paradigm is the evaluation task , which is generally an abstraction of a user task that defines exactly what a system is expected to do .", "label": "", "metadata": {}, "score": "69.655914"}
{"text": "III .Submission guidelines .Submission format .A submission file for the QA task must be a plain text file containing at least one line for each question in the QA task .Each line in the file must have the form : . qid run - tag docid answer - string .", "label": "", "metadata": {}, "score": "69.75248"}
{"text": "Other research groups began to follow the experimental methodology introduced by the Cranfield tests , producing several other test collections that were used in the 1970s and 1980s .But by 1990 , there was growing dissatisfaction with the methodology .Although some research groups used the same test collections , there was no concerted effort to work with the same data , to use the same evaluation measures , or to compare results across search systems to consolidate findings .", "label": "", "metadata": {}, "score": "69.77884"}
{"text": "Other research groups began to follow the experimental methodology introduced by the Cranfield tests , producing several other test collections that were used in the 1970s and 1980s .But by 1990 , there was growing dissatisfaction with the methodology .Although some research groups used the same test collections , there was no concerted effort to work with the same data , to use the same evaluation measures , or to compare results across search systems to consolidate findings .", "label": "", "metadata": {}, "score": "69.77884"}
{"text": "Our algorithm , probabilistic phrase reranking ( PPR ) , uses proximity and question type features and achieves a total reciprocal document rank of.20 on the TREC8 corpus .Our techniques have been implemented as a Web - accessible system , called NSIR . \" ... AnswerBus is an open - domain question answering system based on sentence level Web information retrieval .", "label": "", "metadata": {}, "score": "69.80243"}
{"text": "The track had 33 participating groups .The mean and maximum utility measure for the triage subtask was 0.3303 , with a top score of 0.6512 .No system was able to substantially improve results over simply using the MeSH term Mice .", "label": "", "metadata": {}, "score": "69.94704"}
{"text": "The Novelty Issue .TREC workshops build around tracks representing specific problem areas .Results from tracks may differ from year to year .The novelty issue also receives attention from related but non - TREC research conducted by DARPA called Topic Detection and Tracking .", "label": "", "metadata": {}, "score": "70.420715"}
{"text": "These evidence codes distinguish the type of evidence that the article provides for assigning the GO code , such as IDA ( inferred from direct assay ) , or IMP ( inferred from mutant phenotype ) .Only two groups took part in this subtask .", "label": "", "metadata": {}, "score": "70.507706"}
{"text": "Tracks shown in similar colors are closely related to one another .Today , each TREC contains seven or eight tracks that change frequently to keep TREC fresh and to support new communities .Several of the TREC tracks have been the first large - scale evaluations in that area .", "label": "", "metadata": {}, "score": "70.55345"}
{"text": "Tracks shown in similar colors are closely related to one another .Today , each TREC contains seven or eight tracks that change frequently to keep TREC fresh and to support new communities .Several of the TREC tracks have been the first large - scale evaluations in that area .", "label": "", "metadata": {}, "score": "70.55345"}
{"text": "Ellen Voorhees is a computer scientist at the US Na\u00ad tional Institute of Standards and Technology , where her primary responsibility is managing the TREC project .Her research focuses on developing and validating appropriate evaluation schemes to measure system effectiveness for di\u00ad verse user search tasks and for natural language processing tasks .", "label": "", "metadata": {}, "score": "70.61154"}
{"text": "Ellen Voorhees is a computer scientist at the US Na\u00ad tional Institute of Standards and Technology , where her primary responsibility is managing the TREC project .Her research focuses on developing and validating appropriate evaluation schemes to measure system effectiveness for di\u00ad verse user search tasks and for natural language processing tasks .", "label": "", "metadata": {}, "score": "70.61154"}
{"text": "The format will explicitly tag the target as the target , as well as the type of each question in the series ( type is one of RigidList and SquishyList ) .Each question will have an ID of the form X.Y where X is the target number and Y is the number of the question in the series ( so question 3.4 is the fourth question of the third target ) .", "label": "", "metadata": {}, "score": "70.651825"}
{"text": "Creating a reusable QA test collection is fundamentally more difficult than creating a document retrieval test collection since the QA task has no equivalent to document identifiers . 1 Introduction The Text REtrieval Conference ( TREC ) is a series of workshops organized by the National Institute of Standards and Technology ( NIST ) and designed to advance the state - of - the - art in information retrieval ( IR ) [ 15].", "label": "", "metadata": {}, "score": "70.66107"}
{"text": "Figure 5 shows the annotation hierarchy subtask results graphically .Table 8 .Annotation hierarchy plus evidence code subtask , sorted by F - score .Figure 5 .Annotation hierarchy subtask .Annotation hierarchy subtask results sorted by F - score .", "label": "", "metadata": {}, "score": "70.8298"}
{"text": "Branching Out .Although the initial intent for TREC was simply to build one or two large test collections for ad hoc retrieval and to explore methodological questions related to pooling , it soon became obvious that the ad hoc task could be tweaked along several dimensions .", "label": "", "metadata": {}, "score": "70.88368"}
{"text": "Branching Out .Although the initial intent for TREC was simply to build one or two large test collections for ad hoc retrieval and to explore methodological questions related to pooling , it soon became obvious that the ad hoc task could be tweaked along several dimensions .", "label": "", "metadata": {}, "score": "70.88368"}
{"text": "When classifying a biomedical text , the available training documents must have been written before the text to be classified .This is required for the TREC tasks to realistically simulate automation of the triage task of the GO curators .Papers written after a given article would not be available to the system for training prior to classifying that article .", "label": "", "metadata": {}, "score": "70.93686"}
{"text": "Background .The TREC 2004 Genomics Track focused on applying information retrieval and text mining techniques to improve the use of genomic information in biomedicine .The Genomics Track consisted of two main tasks , ad hoc retrieval and document categorization .", "label": "", "metadata": {}, "score": "70.99828"}
{"text": "The TREC-9 task was considerably harder than the TREC-8 task because TREC-9 used actual users ' questions while TREC-8 used questions constructed for the track .Future tracks will continue to challenge the QA community with more dicult , and more realistic , question answering tasks . by Dragomir Radev , Weiguo Fan , Hong Qi , Harris Wu , Amardeep Grewal - Journal of the American Society for Information Science and Technology , 2002 . \" ...", "label": "", "metadata": {}, "score": "71.14183"}
{"text": "Figure 4 analyzes this situation in the combined corpus .The figure displays the number of documents in the corpus , whose most common associated GO code is given ( by the GO codes frequency in the corpus ) on the x - axis .", "label": "", "metadata": {}, "score": "71.27963"}
{"text": "Edited by Voorhees EM and Buckland LP .National Institute of Standards and Technology ; 2004 .Yang K , Yu N , Wead A , LaRowe G , Li YH , Friend C , Lee Y : WIDIT in TREC 2004 Genomics , Hard , Robust and Web Tracks : ; Gaithersburg , MD .", "label": "", "metadata": {}, "score": "71.349"}
{"text": "National Institute of Standards and Technology ; 2004 .Lee C , Hou WJ , Chen HH : Identifying relevant full - text articles for GO annotation without MeSH terms : ; Gaithersburg , MD .Edited by Voorhees EM and Buckland LP .", "label": "", "metadata": {}, "score": "71.42692"}
{"text": "Edited by Voorhees EM and Buckland LP .National Institute of Standards and Technology ; 2004 .Settles B , Craven M : Exploiting zone information , syntactic rules , and informative terms in Gene Ontology annotation of biomedical documents : ; Gaithersburg , MD .", "label": "", "metadata": {}, "score": "71.49966"}
{"text": "Significant amounts of engineering and , in some cases , usability testing are required to make laboratory successes available in realworld applications .The Netherlands Institute for Sound and Vision , a major data and use case donor to TRECVID , has documented TRECVID 's role in allowing them to engage a wide community of researchers at a low cost to explore tasks of interest to them on their own data .", "label": "", "metadata": {}, "score": "71.502335"}
{"text": "Significant amounts of engineering and , in some cases , usability testing are required to make laboratory successes available in realworld applications .The Netherlands Institute for Sound and Vision , a major data and use case donor to TRECVID , has documented TRECVID 's role in allowing them to engage a wide community of researchers at a low cost to explore tasks of interest to them on their own data .", "label": "", "metadata": {}, "score": "71.502335"}
{"text": "Another difference between the TREC and KDD shared tasks may be even more important .The KDD FlyBase triage task was to \" determine whether the paper meets the FlyBase gene expression curation criteria , and for each gene , indicate whether the full paper has experimental evidence for gene products ( mRNA and/or protein ) \" [ 9 ] .", "label": "", "metadata": {}, "score": "71.50597"}
{"text": "National Institute of Standards and Technology ; 2004 .Kraaij W , Raaijmakers S , Weeber M , Jelier R : MeSH based feedback , concept recognition and stacked classification for curation tasks : ; Gaithersburg , MD .Edited by Voorhees EM and Buckland LP .", "label": "", "metadata": {}, "score": "71.524734"}
{"text": "One specific case of the transition to real - world use is the development and licensing of feature/ concept detectors to a company in the Netherlands , which will integrate them into software tools that allow police to search confiscated video for illicit material .", "label": "", "metadata": {}, "score": "71.60699"}
{"text": "One specific case of the transition to real - world use is the development and licensing of feature/ concept detectors to a company in the Netherlands , which will integrate them into software tools that allow police to search confiscated video for illicit material .", "label": "", "metadata": {}, "score": "71.60699"}
{"text": "We wanted to begin to understand this potentially important issue of terminological drift in the biomedical literature .The process generated a set of 1885 features on the training collection and 1899 significant features on the test collection .We then measured how well the training collection feature set represented the test collection feature set by computing similarity metrics between the two sets [ 6 ] .", "label": "", "metadata": {}, "score": "71.876366"}
{"text": "Edited by Voorhees EM and Buckland LP .National Institute of Standards and Technology ; 2004 .Fujita S : Revisiting again document length hypotheses - TREC 2004 Genomics Track experiments at Patolis : ; Gaithersburg , MD .Edited by Voorhees EM and Buckland LP .", "label": "", "metadata": {}, "score": "71.92"}
{"text": "We were also interested in how well the GO codes assigned to the documents using training and test collections overlapped .Figure 3 shows a plot of the number of GO codes in the combined ( training plus testing ) corpus , as a function of the number of documents associated with each of those GO codes .", "label": "", "metadata": {}, "score": "71.95149"}
{"text": "The triage subtask was limited by the fact that using the MeSH term Mice assigned by the MEDLINE indexers was a better predictor of the MGI triage decision than anything else , including the complex feature extraction and machine learning algorithms of many participating groups .", "label": "", "metadata": {}, "score": "72.39544"}
{"text": "Our annotation task involved a simplification of this annotation step .The goal of this task was not to select the actual GO term , but rather to automatically select the one or more GO hierarchies ( molecular function , biological process , or cellular component ) from which terms had been selected to annotate the gene for the article .", "label": "", "metadata": {}, "score": "72.610794"}
{"text": "We used a version that was normalized by the best possible score : . where U norm was the normalized score , U raw the raw score , and U max the best possible score .The coefficients for the utility measure were derived as follows .", "label": "", "metadata": {}, "score": "72.62092"}
{"text": "Across a number of groups , benefit was found from matching gene names appropriately .University of Wisconsin also found identifying gene names in sentences and modeling features in those sentences provided value [ 8 ] .Discussion .The TREC 2004 Genomics Track categorization task featured a wide diversity of approaches , resulting in substantial variation across the results .", "label": "", "metadata": {}, "score": "72.7921"}
{"text": "Examples of the required submission format for each subtask are shown in 5 .Results .There were 98 runs submitted from 20 groups for the categorization task .These were distributed across the subtasks of the categorization task as follows : 59 for the triage subtask , 36 for the annotation hierarchy subtask , and three for the annotation hierarchy plus evidence code subtask .", "label": "", "metadata": {}, "score": "72.91316"}
{"text": "The number of authors of papers in the TRECVID proceedings is a measure of the breadth of participation in TRECVID .In its first three years as an independent workshop series , the TRECVID community grew rapidly , tripling applications from 20 to 60 groups , of which 40 completed at least one task .", "label": "", "metadata": {}, "score": "72.941696"}
{"text": "The number of authors of papers in the TRECVID proceedings is a measure of the breadth of participation in TRECVID .In its first three years as an independent workshop series , the TRECVID community grew rapidly , tripling applications from 20 to 60 groups , of which 40 completed at least one task .", "label": "", "metadata": {}, "score": "72.941696"}
{"text": "Jimmy Lin and Dina Demner - Fushman .Automatically Evaluating Answers to Definition Questions .Proceedings of the 2005 Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing ( HLT / EMNLP 2005 ) , pages 931 - 938 , October 2005 , Vancouver , Canada .", "label": "", "metadata": {}, "score": "73.26735"}
{"text": "Instances will be judged by human assessors who will assign one of four possible judgments to an instance : . incorrect : the answer - string does not contain a correct answer item ; . unsupported : the answer - string contains a correct answer item but the document returned does not support that answer item ; . non - exact : the answer - string contains a correct answer item and the document supports that answer item , but the string contains more than just the answer item ( or is missing bits of the answer item ) ; .", "label": "", "metadata": {}, "score": "73.271805"}
{"text": "Edited by Voorhees EM and Buckland LP .National Institute of Standards and Technology ; 2004 .Zhang D , Lee WS : Experience of using SVM for the triage task in TREC 2004 Genomics Track : ; Gaithersburg , MD .", "label": "", "metadata": {}, "score": "73.273994"}
{"text": "Additional research into other tasks will provide more information about the performance expectations for biomedical document classification .This task is likely not representative of document classification for biomedical curation tasks .The Mouse Genome Institute also curates articles for purposes other than GO annotation .", "label": "", "metadata": {}, "score": "73.33222"}
{"text": "Boundary cases for utility measure of triage task for training and test data .Thus , for the training data , .Likewise , for the test data , .Annotation subtasks .The primary goal of annotation subtask was , given an article and gene name , to correctly identify which of the GO hierarchies ( also called domains ) had terms within them that were annotated by the MGI curators .", "label": "", "metadata": {}, "score": "73.48398"}
{"text": "Figure 1 shows the groups of documents and how they were assigned into being positive and negative examples for the subtasks .Figure 1 .Document grouping .Grouping of documents for categorization subtasks .Triage subtask .The goal of the triage subtask was to correctly identify papers that were deemed to have experimental evidence warranting annotation with GO codes .", "label": "", "metadata": {}, "score": "73.522736"}
{"text": "Together they sponsor a text retrieval conference known as TREC .The conference workshops evaluate the efforts of participants to complete difficult tests designed to advance text retrieval systems in different problem categories .Real Questions .Working searchers should have a natural affinity for TREC 's approach to improving text retrieval .", "label": "", "metadata": {}, "score": "73.52951"}
{"text": "Even now , when the moon is full and the wind moves the clouds across its shining face , the ghosts of the ancients meet and ponder their creation and judge its works .Even now , if you listen silently , you can hear them whisper wisdom on distances traveled and distances yet unspanned .", "label": "", "metadata": {}, "score": "73.63967"}
{"text": "Our approach is to develop a trainable information extraction system that takes two inputs .The first is an ontology that defines the classes ( e.g. , company , person , employee , product ) and relations ( e.g. , employed_by , produced_by ) of interest when creating the knowledge base .", "label": "", "metadata": {}, "score": "73.94948"}
{"text": "Soboroff has developed evaluation methods and test collections for a wide range of data and user tasks .Contact him at Ian . soboroff@nist.gov .This article first appeared in IT Professional magazine .IT Professional offers solid , peer - reviewed information about today 's strategic technology issues .", "label": "", "metadata": {}, "score": "73.97197"}
{"text": "Soboroff has developed evaluation methods and test collections for a wide range of data and user tasks .Contact him at Ian . soboroff@nist.gov .This article first appeared in IT Professional magazine .IT Professional offers solid , peer - reviewed information about today 's strategic technology issues .", "label": "", "metadata": {}, "score": "73.97197"}
{"text": "It appears that the MeSH term Mice meets this description .The terminological drift showing a difference in significant features between the training and test collection may simply be due to the very sparse sampling of the range of GO topics over both years .", "label": "", "metadata": {}, "score": "74.04647"}
{"text": "Number of GO codes by document frequency .This graph shows the number of GO codes at increasing levels of frequency that appear in the combined ( test + training ) corpus .Recall that a paper should be triaged positive for GO if there is evidence for any of the topics contained in the 20,000 GO codes .", "label": "", "metadata": {}, "score": "74.16142"}
{"text": "NIST 's Text Retrieval Conference ( TREC ) has been instrumental in creating the necessary infrastructure to measure the quality of search results .We often take search for text documents in our native language for granted , but Web search engines such as Yahoo , Google , and Bing were not built in a day , nor is Web content the only area where we need search .", "label": "", "metadata": {}, "score": "74.25307"}
{"text": "Table 1 .Number of papers total and available in the mouse , mus , or murine subset .Since MGI annotation lags behind article publication , a substantial number of papers had been selected for annotation but not yet annotated .", "label": "", "metadata": {}, "score": "74.39285"}
{"text": "II .Test Data .Questions .The test set consists of 50 targets , each with a series of 2 - 4 questions about that target .Each series is an abstraction of a user session with a QA system .", "label": "", "metadata": {}, "score": "74.45878"}
{"text": "However , we could not use the articles not yet annotated for the annotation hierarchy task , since we did not have the annotations .We also needed a set of negative examples for the annotation hierarchy task and chose to use articles selected for action by MGI for other ( i.e. , non - GO annotation ) actions .", "label": "", "metadata": {}, "score": "74.50101"}
{"text": "( For example , if the Team ID is \" NISTAssessor \" then the run - tag for the first - priority run should be \" NISTAssessor1 \" . )Submission procedure .Each team may submit up to three runs , ranked by priority ( 1 - 3 ) .", "label": "", "metadata": {}, "score": "74.8222"}
{"text": "Instead of classifying the sentiment of an entire document about a subject , SA detects all references to the given subject , and determines sentiment in each of the references using natural language processing ( NLP ) techniques .Our sentiment analysis consists of 1 ) a topic specific feature term extraction , 2 ) sentiment extraction , and 3 ) ( subject , sentiment ) association by relationship analysis .", "label": "", "metadata": {}, "score": "74.82333"}
{"text": "The automated classification of documents for GO annotation proved to be a challenging task .Automated extraction of GO hierarchy codes was even more challenging .This was the first year that the TREC Genomcs Track included a classification task , and so our understanding of the best way to approach these tasks for biomedical curation is just beginning .", "label": "", "metadata": {}, "score": "74.98236"}
{"text": "However , she also admitted that nobody yet has appeared at TREC with the brilliant insights that will take us to the next state of the art .Natural language processing , according to Voorhees , is a hard problem to beat , especially if you insist it operate as effectively as a Mr. Spock computer dialog .", "label": "", "metadata": {}, "score": "74.991684"}
{"text": "Error on line 43 , column 13 in template / simple / select .ftl stack.findString(parameters.listValue ) is undefined . ftl ] in user - directive ww.iterator [ on line 35 , column 1 in template / simple / select .ftl ] ----------", "label": "", "metadata": {}, "score": "74.9923"}
{"text": "When the test data were released , these three files were the only ones that were provided .Table 4 .Data file contents and counts for annotation hierarchy subtasks .For the positive examples in the training data , there were 178 documents and 346 document - gene pairs .", "label": "", "metadata": {}, "score": "75.739"}
{"text": "The ad hoc task and a routing task were the only two tasks in the first years of TREC .The routing task was designed to simulate a user monitoring a stream of documents , selecting relevant documents and ignoring unwanted ones .", "label": "", "metadata": {}, "score": "75.89447"}
{"text": "The ad hoc task and a routing task were the only two tasks in the first years of TREC .The routing task was designed to simulate a user monitoring a stream of documents , selecting relevant documents and ignoring unwanted ones .", "label": "", "metadata": {}, "score": "75.89447"}
{"text": "Another concern about the MGI data was whether the snapshot obtained in mid-2004 was significantly updated by the time the track was completed .We re - ran our submitted methods on the updated data and obtained virtually identical results .The major question for the triage subtask is why systems were unable to outperform the single MeSH term Mice .", "label": "", "metadata": {}, "score": "75.92829"}
{"text": "IV .Evaluation .Rigid list .For each rigid list question , the system should return an unordered , non - empty set of [ answer - string , docid ] pairs , where each pair is called an instance .", "label": "", "metadata": {}, "score": "75.96893"}
{"text": "However , time and resource constraints did not allow this .We decided that the triage - everything approach must have a higher score than the triage - nothing approach , since the current practice at MGI is to examine ( triage ) everything for GO evidence and that practice certainly has value to MGI and the many users of its database .", "label": "", "metadata": {}, "score": "76.02978"}
{"text": "However , the research arm of Microsoft has participated in TREC in the past .Speaking of proprietary interests , we asked Voorhees about the role of the intelligence community , particularly ARDA , the latest TREC sponsor .Both ARDA and DARPA primarily support TREC through monetary contributions , Voorhees said .", "label": "", "metadata": {}, "score": "76.05002"}
{"text": "For the training data , there were a total of 504 documents that were either positive ( one or more GO terms assigned ) or negative ( no GO terms assigned ) examples .From these documents , a total of 1291 genes had been assigned by MGI .", "label": "", "metadata": {}, "score": "76.60281"}
{"text": "All similarity measures show a low level of similarity between the two sets .We performed equivalent similarity measures on the individual word frequencies in the training and test collection , filtered out common English words as before , and sorted the words most frequent to least frequent for both sets .", "label": "", "metadata": {}, "score": "76.646225"}
{"text": "The World Wide Web is a vast source of information accessible to computers , but understandable only to humans .The goal of the research described here is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the World Wide Web .", "label": "", "metadata": {}, "score": "76.84252"}
{"text": "The World Wide Web is a vast source of information accessible to computers , but understandable only to humans .The goal of the research described here is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the World Wide Web .", "label": "", "metadata": {}, "score": "76.84252"}
{"text": "The World Wide Web is a vast source of information accessible to computers , but understandable only to humans .The goal of the research described here is to automatically create a computer understandable knowledge base whose content mirrors that of the World Wide Web .", "label": "", "metadata": {}, "score": "76.912"}
{"text": "The World Wide Web is a vast source of information accessible to computers , but understandable only to humans .The goal of the research described here is to automatically create a computer understandable knowledge base whose content mirrors that of the World Wide Web .", "label": "", "metadata": {}, "score": "76.912"}
{"text": "However , the asymmetric utility measure used in the triage task was heavily weighted towards recall .This reflected the priorities of the document curators .It is likely that further experience optimizing for this type of utility measure will provide improved results .", "label": "", "metadata": {}, "score": "76.916885"}
{"text": "Nakov PI , Schwartz AS , Stoica E , Hearst MA : BioText team experiments for the TREC 2004 Genomics Track : ; Gaithersburg , MD .Edited by Voorhees EM and Buckland LP .National Institute of Standards and Technology ; 2004 .", "label": "", "metadata": {}, "score": "77.166245"}
{"text": "However , this group also noted that the MeSH term Mice alone scored better than all but the single top run , with a Unorm of 0.6404 .This meant that no other approach was better able to classify documents for triage than simply using the MeSH term Mice from the MEDLINE record .", "label": "", "metadata": {}, "score": "77.22913"}
{"text": "The TREC 2004 Genomics Track would like to acknowledge the assistance of Judith Blake and her staff at the Mouse Genome Institute for their support in creating the tasks and preparing the data for this research .A second version of data was released in early 2005 that updated the 2004 data to correct some minor errors .", "label": "", "metadata": {}, "score": "77.26395"}
{"text": "We were assured by the MGI director ( J. Blake , personal communication ) that the initial triage decision for an article was made independent of the prior coverage of gene , even though priority decisions made later in the pipeline did take coverage into account .", "label": "", "metadata": {}, "score": "77.30391"}
{"text": "4 , 2009 , pp .215 - 322 .11 J. Oomen et al . , \" Symbiosis Between the TRECVID Benchmark and Video Libraries at the Netherlands Institute for Sound and Vision , \" Int'l J. Digital Libraries , vol .", "label": "", "metadata": {}, "score": "77.5357"}
{"text": "4 , 2009 , pp .215 - 322 .11 J. Oomen et al . , \" Symbiosis Between the TRECVID Benchmark and Video Libraries at the Netherlands Institute for Sound and Vision , \" Int'l J. Digital Libraries , vol .", "label": "", "metadata": {}, "score": "77.5357"}
{"text": "However , there would be great value to systems that could perform automated GO annotation , even though the task is very challenging [ 10 ] .These results demonstrated value in identifying gene names and other controlled vocabulary terms in documents for this task .", "label": "", "metadata": {}, "score": "77.57724"}
{"text": "All of the triage subtask results are shown graphically in Figure 2 , along with the utility for the MeSH term Mice and the decision to select all articles .Figure 2 .Triage subtask .Triage subtask runs sorted by Unorm score .", "label": "", "metadata": {}, "score": "77.60405"}
{"text": "He received the US Department of Commerce 's Bronze Medal for Superior Federal Service in 2011 .Con\u00ad tact him at over@nist.gov .Ia n Soboroff is a computer scientist and manager of the Retrieval Group at the National Institute of Standards and Technology ( NIST ) .", "label": "", "metadata": {}, "score": "77.63523"}
{"text": "He received the US Department of Commerce 's Bronze Medal for Superior Federal Service in 2011 .Con\u00ad tact him at over@nist.gov .Ia n Soboroff is a computer scientist and manager of the Retrieval Group at the National Institute of Standards and Technology ( NIST ) .", "label": "", "metadata": {}, "score": "77.63523"}
{"text": "In the categorization task , using data extracted for us from the MGI databases by the MGI staff , we simulated two of the classification activities carried out by human annotators for the MGI system : a triage task and two simplified variations of MGI 's annotation task .", "label": "", "metadata": {}, "score": "77.672165"}
{"text": "We present Sentiment Analyzer ( SA ) that extracts sentiment ( or opinion ) about a subject from online text documents .Instead of classifying the sentiment of an entire document about a subject , SA detects all references to the given subject , and determines sentiment in each of the references using nat ... \" .", "label": "", "metadata": {}, "score": "77.96717"}
{"text": "WR chairs the TREC Genomics Track and initially suggested the track tasks described here .Both authors serve on the TREC Genomics Track steering committee and reviewed and approved the final manuscript .Acknowledgements .The TREC 2004 Genomics Track was supported by NSF Grant ITR-0325160 .", "label": "", "metadata": {}, "score": "78.36983"}
{"text": "Contact her at ellen . voorhees@nist.gov .P au l Over is a computer scientist at the US National In\u00ad stitute of Standards and Technology and founding project leader for the TREC Video Retrieval Evaluations ( TREC\u00ad VID ) .He has also been responsible at NIST for evalu\u00ad ation of interactive text retrieval systems within the Text Retrieval Evaluations ( TREC ) and has supported natu\u00ad ral language processing researchers in evaluation of text summarization technology .", "label": "", "metadata": {}, "score": "78.62073"}
{"text": "Contact her at ellen . voorhees@nist.gov .P au l Over is a computer scientist at the US National In\u00ad stitute of Standards and Technology and founding project leader for the TREC Video Retrieval Evaluations ( TREC\u00ad VID ) .He has also been responsible at NIST for evalu\u00ad ation of interactive text retrieval systems within the Text Retrieval Evaluations ( TREC ) and has supported natu\u00ad ral language processing researchers in evaluation of text summarization technology .", "label": "", "metadata": {}, "score": "78.62073"}
{"text": "Publication Citation : Overview of the TREC 2006 Question Answering Track .NIST Authors in Bold .The TREC 2006 question answering track contained two tasks : a main task and a complex , interactive question answering ( ciQA ) task .", "label": "", "metadata": {}, "score": "78.81152"}
{"text": "Later tracks incorporated more difficult question types , such as list questions ( a question whose answer is a distinct set of instances of the type requested such as , \" What actors have played Tevye in Fid\u00ad dler on the Roof ? \") and definitional or biographical questions ( for example , \" What is a golden parachute ? \" or \" Who is Vlad the Impaler ? \" )", "label": "", "metadata": {}, "score": "78.86522"}
{"text": "Later tracks incorporated more difficult question types , such as list questions ( a question whose answer is a distinct set of instances of the type requested such as , \" What actors have played Tevye in Fid\u00ad dler on the Roof ? \") and definitional or biographical questions ( for example , \" What is a golden parachute ? \" or \" Who is Vlad the Impaler ? \" )", "label": "", "metadata": {}, "score": "78.86522"}
{"text": "Those GO codes are assigned , along with an additional code for each GO code indicating the type of experimental evidence .There can more than one gene assigned specific GO codes in a given paper , and there can be more than one GO code assigned to a gene .", "label": "", "metadata": {}, "score": "78.973434"}
{"text": "The results of the triage subtask are shown in 6 .A variety of groups used classifiers based on many different machine learning techniques .The higher scoring runs tended to make use of MeSH terms in some fashion .The best performing run came from Rutgers , using the MEDLINE record , weighting , and filtering by the MeSH term Mice [ 3 ] .", "label": "", "metadata": {}, "score": "79.021614"}
{"text": "1 The Cranfield test collection consists of a set of abstracts from journal articles on aeronautics , a set of queries against these abstracts , and an answer key of correct responses for each query .Though minuscule by today 's standards , the Cranfield collection broke new ground by creating the first shared measurement tool for information retrieval systems .", "label": "", "metadata": {}, "score": "79.09201"}
{"text": "1 The Cranfield test collection consists of a set of abstracts from journal articles on aeronautics , a set of queries against these abstracts , and an answer key of correct responses for each query .Though minuscule by today 's standards , the Cranfield collection broke new ground by creating the first shared measurement tool for information retrieval systems .", "label": "", "metadata": {}, "score": "79.09201"}
{"text": "The wealth of information on the web makes it an attractive resource for seeking quick answers to simple , factual questions such as \" who was the first American in space ? \" or \" what is the second tallest mountain in the world ? \"", "label": "", "metadata": {}, "score": "79.71992"}
{"text": "The wealth of information on the web makes it an attractive resource for seeking quick answers to simple , factual questions such as \" who was the first American in space ? \" or \" what is the second tallest mountain in the world ? \"", "label": "", "metadata": {}, "score": "79.71992"}
{"text": "26 - 28 .3 K. Sp\u00e4rck Jones and C. van Rijsbergen , Report on the Need for and Provision of an \" Ideal \" Information Retrieval Test Collection , report 5266 , British Library Research and Development , Computer Laboratory , Univ . of Cambridge , 1975 .", "label": "", "metadata": {}, "score": "80.77486"}
{"text": "26 - 28 .3 K. Sp\u00e4rck Jones and C. van Rijsbergen , Report on the Need for and Provision of an \" Ideal \" Information Retrieval Test Collection , report 5266 , British Library Research and Development , Computer Laboratory , Univ . of Cambridge , 1975 .", "label": "", "metadata": {}, "score": "80.77486"}
{"text": "We often take search for text documents in our native language for granted , but Web search engines such as Yahoo , Google , and Bing were not built in a day , nor is Web content the only area where we need search .", "label": "", "metadata": {}, "score": "80.84814"}
{"text": "A 2009 bibliometric study of TRECVID data by library scientists at Dublin City University found that TRECVID participants produced 310 ( unrefereed ) workshop papers between 2003 and 2009 as well as 2,073 peer - reviewed journal articles and conference papers .", "label": "", "metadata": {}, "score": "81.47642"}
{"text": "A 2009 bibliometric study of TRECVID data by library scientists at Dublin City University found that TRECVID participants produced 310 ( unrefereed ) workshop papers between 2003 and 2009 as well as 2,073 peer - reviewed journal articles and conference papers .", "label": "", "metadata": {}, "score": "81.47642"}
{"text": "They have also supported IR ( information retrieval ) and related research by acting as an intermediary between consenting publishers and information systems research groups who want to use their journals , such as the TREC Genomics Track .The journals available and used by our track this year were Journal of Biological Chemistry ( JBC ) , Journal of Cell Biology ( JCB ) , and Proceedings of the National Academy of Science ( PNAS ) .", "label": "", "metadata": {}, "score": "81.4837"}
{"text": "In this paper , we extend question - answering techniques , first studied in the information retrieval literature , to the web and experimentally evaluate their performance .First we introduce MULDER , which we believe to be the first general - purpose , fully - automated question - answering system available on the web .", "label": "", "metadata": {}, "score": "82.15112"}
{"text": "Eichmann D , Zhang Y , Bradshaw S , Qiu XY , Zhou L , Srinivasan P , Sehgal AK , Wong H : Novelty , question answering and genomics : the University of Iowa response : ; Gaithersburg , MD .", "label": "", "metadata": {}, "score": "82.375534"}
{"text": "The rest of this article is only available to active members of Questia .Notes for this article .Questia , a part of Gale , Cengage Learning .Contributors : Mishne , Gilad - Author , de Rijke , Maarten - Author , Jijkoun , Valentin - Author .", "label": "", "metadata": {}, "score": "82.47929"}
{"text": "That is : .Squishy list .The response for a squishy list question is syntactically the same as for a rigid list question : an unordered , non - empty set of [ answer - string , docid ] pairs .", "label": "", "metadata": {}, "score": "83.00736"}
{"text": "Given these inputs , the system learns to extract information from other pages and hyperlinks on the Web .This article describes our general a .. owledge base .Their hope is that a library of standard ontologies will come into common usage , enabling agents such as Expose to learn the information encoded on the Web .", "label": "", "metadata": {}, "score": "83.4475"}
{"text": "A total of 872 GO plus evidence codes had been assigned to these documents .For the negative examples , there were 326 documents and 1072 document - gene pairs .Note that MGI evidence codes refer to the type of evidence , not the specific thing that there is evidence for .", "label": "", "metadata": {}, "score": "83.72507"}
{"text": "For negative examples , we used 555 papers that had a gene name assigned but were used for other purposes by MGI .As such , these papers had no GO annotations .These papers did , however , have one or more genes assigned by MGI for the other annotation purposes .", "label": "", "metadata": {}, "score": "83.829445"}
{"text": "Each of the papers from these journals was provided in SGML format based on Highwire 's Document Type Definition ( DTD ) .We used articles from the year 2002 for training data and from 2003 for test data .The documents for the categorization tasks came from a subset of articles having the words mouse , mice or murine as described above .", "label": "", "metadata": {}, "score": "83.93742"}
{"text": "Jimmy Lin .Resources .TREC .Raw nugget pyramids data .Released : April 13 , 2006 ( Last update : September 9 , 2006 ) .Jimmy Lin and Dina Demner - Fushman .Will Pyramids Built of Nuggets Topple Over ?", "label": "", "metadata": {}, "score": "84.08946"}
{"text": "rank is the rank at which PRISE retrieved the doc ( most similar doc is rank 1 ) .docid is the document identifier .rsv is the \" retrieval status value \" or similarity between the query and document , where larger is better .", "label": "", "metadata": {}, "score": "84.35092"}
{"text": "All of the above lends support to the theory that the GO triage task is difficult because it contains many sub - problems which are very sparsely sampled .There aremany GO codes having only one associated document contained in the corpus , and there are many , many GO codes that are completely missing from the corpus .", "label": "", "metadata": {}, "score": "84.54379"}
{"text": "Number of documents with frequency of most common GO code .This graph shows the number of combined corpus documents having a most common GO code whose frequency is given on the x - axis .It is clear that a significant number of documents ( 48 out of 328 , about 15 % ) have a \" most common \" GO code that appears only once in the entire corpus .", "label": "", "metadata": {}, "score": "86.33296"}
{"text": "Whereas the routing evaluation task lets systems process all documents in the collection in a batch fashion , the filtering evaluation task requires a system to process documents as they arrive in a stream and to adapt the user model online .", "label": "", "metadata": {}, "score": "86.46117"}
{"text": "Whereas the routing evaluation task lets systems process all documents in the collection in a batch fashion , the filtering evaluation task requires a system to process documents as they arrive in a stream and to adapt the user model online .", "label": "", "metadata": {}, "score": "86.46117"}
{"text": "Java version of Brill 's Part - of - Speech Tagger .Released : December 27 , 2004 .Eric Brill 's part - of - speech tagger ported to Java via the Java Native Interface ( JNI ) .In actuality , it 's based on Benjamin Han 's ePost package , which is a cleaned - up version of Brill 's original code .", "label": "", "metadata": {}, "score": "87.581055"}
{"text": "LPost : Perl version of Brill 's Part - of - Speech Tagger .Released : December 27 , 2004 .Eric Brill 's part - of - speech tagger as a Perl Module .Just like the Java version , it 's based on Benjamin Han 's ePost package .", "label": "", "metadata": {}, "score": "87.63115"}
{"text": "For example , for the topic Clinton , his birthday might be considered important , while the day of the week when he left Mexico probably is not .In order to give reasonable responses to \" other \" questions , a QA system needs to model such preferences .", "label": "", "metadata": {}, "score": "87.918045"}
{"text": "InvalidReferenceException : Error on line 43 , column 13 in template / simple / select .ftl stack.findString(parameters.listValue ) is undefined ._ jspx_meth_ww_005fselect_005f0(forceUpdateProfileForm_jsp .java:773 ) at org.apache.jsp.user.forceUpdateProfileForm_jsp ._ jspService(forceUpdateProfileForm_jsp ._ jspx_meth_ww_005faction_005f2(decorator_005fmain_jsp .java:8086 ) at org.apache.jsp.decorators.decorator_005fmain_jsp ._", "label": "", "metadata": {}, "score": "88.25053"}
{"text": "Finally , we compare MULDER 's performance to that of Google and AskJeeves on questions drawn from the TREC-8 question track .We find that MULDER 's recall is more than a factor of three higher than that of AskJeeves .In addition , we find that Google requires 6.6 times as much user effort to achieve the same level of recall as MULDER . by Jeonghee Yi , Tetsuya Nasukawa , Razvan Bunescu , Wayne Niblack - In IEEE Intl .", "label": "", "metadata": {}, "score": "88.97897"}
{"text": "And the midwives at the birthing of the infant that would one day rise to hold the world in its mighty Web were ARPA ( now known as DARPA , the Defense Advanced Research Projects Agency ) and the National Science Foundation ( NSF ) .", "label": "", "metadata": {}, "score": "89.928375"}
{"text": "Answers must be retrieved from documents in the Blog06 corpus .Participants should obtain the Blog06 collection directly from the University of Glasgow .To improve the quality and completeness of the pool of answers in the QA track , NIST will accept manual runs in addition to fully automatic ones .", "label": "", "metadata": {}, "score": "89.99102"}
{"text": "There are over 3.2 million permalink documents in the Blog06 collection .More details can be found in the Blog06 README .The TREC Blog06 collection was created by the University of Glasgow for the TREC 2006 Blog Track .The collection is currently distributed only by the University of Glasgow .", "label": "", "metadata": {}, "score": "90.19806"}
{"text": "Volume : 3 .Issue : 1 Publication date : March 2005 .Page number : 47 + .\u00a9 Digital Information Research Foundation .COPYRIGHT 2005 Gale Group .This material is protected by copyright and , with the exception of fair use , may not be further copied , distributed or transmitted in any form or by any means .", "label": "", "metadata": {}, "score": "90.23596"}
{"text": "However , there are several important differences between the TREC and the KDD triage tasks , besides the obvious , but possibly important difference , that the KDD task focused on fly genomics and the TREC task on mouse .First of all , both the training and test collections for the KDD task had a relatively high proportion of positives ( 33 % and 43 % , respectively ) as compared to the TREC task ( 6.5 % and 7 % ) .", "label": "", "metadata": {}, "score": "91.18254"}
{"text": "In the TREC 2004 QA track this was taken a step further .The questions were now clustered in small groups , organized around the same topic .For example , the topic Concorde included questions such as How many seats are in the cabin of a Concorde ? and What airlines have Concordes in their fleets ?", "label": "", "metadata": {}, "score": "92.21373"}
{"text": "The answer - string can not contain any line breaks , but should be immediately followed by exactly one line break .Other white space is allowed in answer - string .The total length of all answer - strings for each question can not exceed 7000 non - white - space characters .", "label": "", "metadata": {}, "score": "93.68443"}
{"text": "As such , they employ a three - step process to identify the papers most likely to describe gene function : .About mouse .The first step is to identify articles about mouse genomics biology .The full text of articles from several hundred journals is searched for the words mouse , mice , or murine .", "label": "", "metadata": {}, "score": "94.148834"}
{"text": "Participants who would like to receive the text of the top 50 documents must submit or have already submitted to NIST a signed user agreement form for the Blog06 collection .Participating teams that have already obtained the Blog06 collection from the University of Glasgow should fax to NIST their existing ( signed )", "label": "", "metadata": {}, "score": "100.590576"}
{"text": "INVETIGATIVE REPORT On the TREC Trail By Barbara Quint Come , my children , and gather round the campfire .Listen to the old ones tell the tales of the long - forgotten ancient times and of the great deeds done when the world was young .", "label": "", "metadata": {}, "score": "105.30326"}
