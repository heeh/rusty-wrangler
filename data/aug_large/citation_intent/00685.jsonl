{"text": "In this report we describe our attempts to improve the discrimination accuracy of the Yarowsky word sense disambiguation algorithm .The first of these experiments used an iterative approach to re - train the classifier .Our hope was that a corpus labeled by an imperfect classifier would make training material superior to an unlabeled corpus .", "label": "", "metadata": {}, "score": "36.506943"}
{"text": "We present a detailed case study of this learning method applied to part of speech tagging . by David Yarowsky - IN PROCEEDINGS OF THE 33RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS , 1995 . \" ...This paper presents an unsupervised learning algorithm for sense disambiguation that , when trained on unannotated English text , rivals the performance of supervised techniques that require time - consuming hand annotations .", "label": "", "metadata": {}, "score": "39.40172"}
{"text": "This paper presents a statistical decision procedure for lexical ambiguity resolution .The algorithm exploits both local syntactic patterns and more distant collocational evidence , generating an efficient , effective , and highly perspicuous recipe for resolving a given ambiguity .By identifying and utilizing only the single best disambiguating evidence in a target context , the algorithm avoids the problematic complex modeling of statistical dependencies .", "label": "", "metadata": {}, "score": "39.683907"}
{"text": "Thus , we draw on work done at AT&T Bell Laboratories by Gale and Church ( 1991a ; 1991b ... . \" ...This paper presents a statistical decision procedure for lexical ambiguity resolution .The algorithm exploits both local syntactic patterns and more distant collocational evidence , generating an efficient , effective , and highly perspicuous recipe for resolving a given ambiguity .", "label": "", "metadata": {}, "score": "39.685673"}
{"text": "In 1975 Kelly and Stone published a book explicitly listing their rules for disambiguation of word senses .These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .A good example of this is Luk 's system A. Luk .", "label": "", "metadata": {}, "score": "39.871883"}
{"text": "In 1975 Kelly and Stone published a book explicitly listing their rules for disambiguation of word senses .These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .A good example of this is Luk 's system A. Luk .", "label": "", "metadata": {}, "score": "39.871883"}
{"text": "The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet Agirre , E. and Rigau , G. ( 1996 ) .", "label": "", "metadata": {}, "score": "39.97984"}
{"text": "This attempt used as data a punched - card version of Roget 's Thesaurus and its numbered \" heads \" , as an indicator of topics and looked for repetitions in text , using a set intersection algorithm .It was not very successful , as is described in some detail in ( Wilks , Y. et al .", "label": "", "metadata": {}, "score": "40.063423"}
{"text": "Thus , many word sense disambiguation algorithms use semi - supervised learning , which allows both labeled and unlabeled data .The Yarowsky algorithm was an early example of such an algorithm .Yarowsky 's unsupervised algorithm uses the ' One sense per collocation ' and the ' One sense per discourse ' properties of human languages for word sense disambiguation .", "label": "", "metadata": {}, "score": "40.27281"}
{"text": "Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .Unsupervised word - sense disambiguation rivaling supervised methods .In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .", "label": "", "metadata": {}, "score": "40.675613"}
{"text": "We test this empirical hypothesis for several definitions of sense and collocation , and discover that it holds with 90 - 99 % accuracy for binary ambiguities .We utilize this property in a disambiguation algorithm that achieves precision of 92 % using combined models of very local context .", "label": "", "metadata": {}, "score": "41.609745"}
{"text": "( 1996 ) which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .A similarity matrix is thus formed which is subject to cluster analysis to determine groups of semantically related instances of terms .", "label": "", "metadata": {}, "score": "42.321384"}
{"text": "These operations capture linguistic differences such as word order and case marking .Model parameters are es ... \" .We present a syntax - based statistical translation model .Our model transforms a source - language parse tree into a target - language string by applying stochastic operations at each node .", "label": "", "metadata": {}, "score": "44.469696"}
{"text": "Unfortunately , it is a challenge to design a system that can accurately cope with the idiosyncrasies of human language .In this report we describe our attempts to improve the discrimination accuracy of the Yarowsky word sense disambiguation algorithm .The first of these experiments used an iterative approach to re - train the classifier .", "label": "", "metadata": {}, "score": "44.4916"}
{"text": "Yarowsky reports that the system correctly classifies senses 96 % of the time .Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .Unsupervised word - sense disambiguation rivaling supervised methods .In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .", "label": "", "metadata": {}, "score": "44.595383"}
{"text": "Yarowsky reports that the system correctly classifies senses 96 % of the time .Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .Unsupervised word - sense disambiguation rivaling supervised methods .In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .", "label": "", "metadata": {}, "score": "44.595383"}
{"text": "Yarowsky reports that the system correctly classifies senses 96 % of the time .Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .Unsupervised word - sense disambiguation rivaling supervised methods .In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .", "label": "", "metadata": {}, "score": "44.595383"}
{"text": "Yarowsky reports that the system correctly classifies senses 96 % of the time .Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .Unsupervised word - sense disambiguation rivaling supervised methods .In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .", "label": "", "metadata": {}, "score": "44.595383"}
{"text": "Yarowsky reports that the system correctly classifies senses 96 % of the time .Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .Unsupervised word - sense disambiguation rivaling supervised methods .In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .", "label": "", "metadata": {}, "score": "44.595383"}
{"text": "As with many problems in Natural Language Processing , word sense disambiguation is a difficult yet potentially very useful capability .Automatically determining the meanings of words with multiple definitions could benefit document classification , keyword searching , OCR , and many other applications that process text .", "label": "", "metadata": {}, "score": "45.528015"}
{"text": "The third experiment modified the training phase of the Yarowsky algorithm by replacing its assumption of a uniform distribution of senses for a word with a more realistic one .We exploit the fact that our dictionary lists senses roughly in order by frequency of use to create a distribution that allows more accurate training .", "label": "", "metadata": {}, "score": "45.678734"}
{"text": "This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .", "label": "", "metadata": {}, "score": "45.988953"}
{"text": "This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .", "label": "", "metadata": {}, "score": "45.988953"}
{"text": "This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .", "label": "", "metadata": {}, "score": "45.988953"}
{"text": "This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .", "label": "", "metadata": {}, "score": "45.988953"}
{"text": "This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky [ 13 ] .", "label": "", "metadata": {}, "score": "46.860825"}
{"text": "This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky [ 13 ] .", "label": "", "metadata": {}, "score": "46.860825"}
{"text": "In : Proceedings of the ARPA Human Language Technology Worskshop , Morgan Kaufmann , San Francisco ( 1993 ) .Sch\u00fctze , H. : Automatic word sense discrimination .Computational Linguistics 24(1 ) ( 1998 ) .Yarowsky , D. : Unsupervised word sense disambiguation rivaling supervised methods .", "label": "", "metadata": {}, "score": "47.04554"}
{"text": "This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky [ 12 ] .", "label": "", "metadata": {}, "score": "47.074455"}
{"text": "This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky [ 12 ] .", "label": "", "metadata": {}, "score": "47.074455"}
{"text": "This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky [ 12 ] .", "label": "", "metadata": {}, "score": "47.074455"}
{"text": "This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky [ 12 ] .", "label": "", "metadata": {}, "score": "47.074455"}
{"text": "Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .In Proceedings of the 14th International Conference on Computational Linguistics ( COLING-92 ) , pages 454 - 460 , Nantes , France , 1992 .An experiment in semantic tagging using hidden markov model tagging .", "label": "", "metadata": {}, "score": "47.23062"}
{"text": "Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .In Proceedings of the 14th International Conference on Computational Linguistics ( COLING-92 ) , pages 454 - 460 , Nantes , France , 1992 .An experiment in semantic tagging using hidden markov model tagging .", "label": "", "metadata": {}, "score": "47.23062"}
{"text": "Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .In Proceedings of the 14th International Conference on Computational Linguistics ( COLING-92 ) , pages 454 - 460 , Nantes , France , 1992 .An experiment in semantic tagging using hidden markov model tagging .", "label": "", "metadata": {}, "score": "47.23062"}
{"text": "Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .In Proceedings of the 14th International Conference on Computational Linguistics ( COLING-92 ) , pages 454 - 460 , Nantes , France , 1992 .An experiment in semantic tagging using hidden markov model tagging .", "label": "", "metadata": {}, "score": "47.23062"}
{"text": "This paper presents an unsupervised learning algorithm for sense disambiguation that , when trained on unannotated English text , rivals the performance of supervised techniques that require time - consuming hand annotations .The algorithm is based on two powerful constraints -- that words tend to have one sense per discourse and one sense per collocation -- exploited in an iterative bootstrapping procedure .", "label": "", "metadata": {}, "score": "47.77648"}
{"text": "Our second experiment used part - of - speech information as an additional knowledge source for the Yarowsky algorithm .We pre - processed our training and test corpora with a part - of - speech tagger and used these tags to filter possible senses and improve the predictive power of words ' contexts .", "label": "", "metadata": {}, "score": "48.139732"}
{"text": "We have investigated the performances of some statistical language model types and parameters in lexical ambiguity resolution for our direct transfer MT system .Hirst , G. , Charniak , E. : Word Sense and Case Slot Disambiguation .In : AIII 1982 , pp .", "label": "", "metadata": {}, "score": "48.718018"}
{"text": "As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .With the rise of statistical methods in CL in the 1990s , WSD became one of the main focus ' of supervised learning techniques .", "label": "", "metadata": {}, "score": "48.943127"}
{"text": "This could give the word different rankings or even different classifications .Alternatively , it can be done by identifying a single defining collocate for each class , and using for seeds only those contexts containing one of these defining words .", "label": "", "metadata": {}, "score": "49.445126"}
{"text": "The meaning of a word in a particular usage can only be determined by examining its context .This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : .", "label": "", "metadata": {}, "score": "49.540325"}
{"text": "The meaning of a word in a particular usage can only be determined by examining its context .This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : .", "label": "", "metadata": {}, "score": "49.540325"}
{"text": "The meaning of a word in a particular usage can only be determined by examining its context .This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : .", "label": "", "metadata": {}, "score": "49.540325"}
{"text": "Model parameters are estimated in polynomial time using an EM algorithm .The model produces word alignments that are better than those produced by IBM Model 5 .Developing a better TM is a fundamental issue for those applications .Researchers at IBM first described such a statistical TM in ( Brown et al . , 1988 ) .", "label": "", "metadata": {}, "score": "49.995445"}
{"text": "This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .", "label": "", "metadata": {}, "score": "50.101784"}
{"text": "This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .", "label": "", "metadata": {}, "score": "50.101784"}
{"text": "This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .", "label": "", "metadata": {}, "score": "50.101784"}
{"text": "This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .", "label": "", "metadata": {}, "score": "50.101784"}
{"text": "This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .", "label": "", "metadata": {}, "score": "50.101784"}
{"text": "This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .", "label": "", "metadata": {}, "score": "50.101784"}
{"text": "This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .", "label": "", "metadata": {}, "score": "50.101784"}
{"text": "In Proceedings of the 14th International Conference on Computational Linguistics ( COLING-92 ) , pages 454 - 460 , Nantes , France , 1992 .This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .", "label": "", "metadata": {}, "score": "50.11753"}
{"text": "However , if such knowledge did exist , then deep approaches would be much more accurate than the shallow approaches .Also , there is a long tradition in computational linguistics , of trying such approaches in terms of coded knowledge and in some cases , it is hard to say clearly whether the knowledge involved is linguistic or world knowledge .", "label": "", "metadata": {}, "score": "50.49765"}
{"text": "Yarowsky reports that the system correctly classifies senses 96 % of the time .Revision as of 04:20 , 25 June 2012 .Word Sense Disambiguation ( WSD ) is the process of identifying the sense of a polysemic word .In modern WSD systems , the senses of a word are typically taken from some specified dictionary .", "label": "", "metadata": {}, "score": "50.732235"}
{"text": "The problem of WSD was first introduced by Warren weaver in 1949 [ 3 ] In 1975 Kelly and Stone published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .", "label": "", "metadata": {}, "score": "51.018723"}
{"text": "The problem of WSD was first introduced by Warren weaver in 1949 [ 3 ] In 1975 Kelly and Stone published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .", "label": "", "metadata": {}, "score": "51.018723"}
{"text": "The problem of WSD was first introduced by Warren weaver in 1949 [ 3 ] In 1975 Kelly and Stone published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .", "label": "", "metadata": {}, "score": "51.018723"}
{"text": "The problem of WSD was first introduced by Warren weaver in 1949 [ 3 ] In 1975 Kelly and Stone published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .", "label": "", "metadata": {}, "score": "51.018723"}
{"text": "Chapter 4 Transformation - Based Error - Driven Learning Applied to Natural Language 4.1 Introduction In this section , we describe a framew ... . by David Yarowsky - In Proceedings of the ARPA Human Language Technology Workshop , 1993 . \" ...", "label": "", "metadata": {}, "score": "51.053005"}
{"text": "An experiment in semantic tagging using hidden markov model tagging .In Vossen , P. , Adriaens , G. , Calzolari , N. , Sanfilippo , A. , and Wilks , Y. , editors , Proceedings of the ACL / EACL'97 Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources .", "label": "", "metadata": {}, "score": "51.114365"}
{"text": "An experiment in semantic tagging using hidden markov model tagging .In Vossen , P. , Adriaens , G. , Calzolari , N. , Sanfilippo , A. , and Wilks , Y. , editors , Proceedings of the ACL / EACL'97 Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources .", "label": "", "metadata": {}, "score": "51.114365"}
{"text": "Yarowsky reports that the system correctly classifies senses 96 % of the time .Revision as of 07:00 , 4 January 2011 .Word Sense Disambiguation ( WSD ) is the process of identifying the sense of a polysemic word .In modern WSD systems , the senses of a word are typically taken from some specified dictionary .", "label": "", "metadata": {}, "score": "51.12106"}
{"text": "The problem of WSD was first introduced by Warren Weaver in 1949 [ 3 ] .In 1975 Kelly and Stone [ 4 ] published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .", "label": "", "metadata": {}, "score": "51.143913"}
{"text": "The problem of WSD was first introduced by Warren Weaver in 1949 [ 3 ] .In 1975 Kelly and Stone [ 4 ] published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .", "label": "", "metadata": {}, "score": "51.143913"}
{"text": "By using the classifier 's output from one iteration as its training input in the next , we tried to boost the accuracy of each successive cycle .Our second experiment used part - of - speech information as an additional knowledge source for the Yarowsky algorithm .", "label": "", "metadata": {}, "score": "51.305336"}
{"text": "What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching technique [ 10 ] which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .", "label": "", "metadata": {}, "score": "51.33598"}
{"text": "What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching technique [ 10 ] which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .", "label": "", "metadata": {}, "score": "51.33598"}
{"text": "What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching technique [ 9 ] which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .", "label": "", "metadata": {}, "score": "51.356285"}
{"text": "What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching technique [ 9 ] which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .", "label": "", "metadata": {}, "score": "51.356285"}
{"text": "What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching technique [ 9 ] which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .", "label": "", "metadata": {}, "score": "51.356285"}
{"text": "What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching technique [ 9 ] which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .", "label": "", "metadata": {}, "score": "51.356285"}
{"text": "The corpus is initially untagged .The algorithm starts with a large corpus , in which it identifies examples of the given polysemous word , and stores all the relevant sentences as lines .For instance , Yarowsky uses the word ' plant ' in his 1995 paper to demonstrate the algorithm .", "label": "", "metadata": {}, "score": "51.42508"}
{"text": "The selection of categories is accomplished by identifying and weighting words that are indicative of each category when seen in context , using a Bayesian theoretical framework .Other . \" ...We present a syntax - based statistical translation model .", "label": "", "metadata": {}, "score": "52.123528"}
{"text": "In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .", "label": "", "metadata": {}, "score": "52.288506"}
{"text": "In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .This takes a small number of seed definitions of the senses of some word ( the seeds could be WordNet synsets or definitions from some lexicon ) and uses these to classify the ' ' obvious ' ' cases in a corpus .", "label": "", "metadata": {}, "score": "52.391457"}
{"text": "In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .This takes a small number of seed definitions of the senses of some word ( the seeds could be WordNet synsets or definitions from some lexicon ) and uses these to classify the ' ' obvious ' ' cases in a corpus .", "label": "", "metadata": {}, "score": "52.391457"}
{"text": "In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .This takes a small number of seed definitions of the senses of some word ( the seeds could be WordNet synsets or definitions from some lexicon ) and uses these to classify the ' ' obvious ' ' cases in a corpus .", "label": "", "metadata": {}, "score": "52.391457"}
{"text": "The residual examples ( 85 % - 98 % according to Yarowsky ) remain untagged .The algorithm should initially choose seed collocations representative that will distinguish sense A and B accurately and productively .This can be done by selecting seed words from a dictionary 's entry for that sense .", "label": "", "metadata": {}, "score": "52.664043"}
{"text": "In addition , words that occur near the target word in great frequency can be selected as seed collocations representative .This approach is not fully automatic , a human judge must decide which word will be selected for each target word 's sense , the outputs will be reliable indicators of the senses .", "label": "", "metadata": {}, "score": "54.049065"}
{"text": "The algorithm will continue to iterate until no more reliable collocations are found .The ' One sense per discourse ' property can be used here for error correction .For a target word that has a binary sense partition , if the occurrences of the majority sense A exceed that of the minor sense B by a certain threshold , the minority ones will be relabeled as A. According to Yarowsky , for any sense to be clearly dominant , the occurrences of the target word should not be less than 4 .", "label": "", "metadata": {}, "score": "54.08713"}
{"text": "the work . of Kelly and Stone .( 1975 ) who . published a book explicitly listing their rules for disambiguation of word senses .The problem of WSD . was .first introduced by Warren weaver in 1949 W. Weaver .", "label": "", "metadata": {}, "score": "54.156013"}
{"text": "Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , Segond , F. , Schiller , A. , Grefenstette , G. , and Chanod , J. ( 1997 ) .An experiment in semantic tagging using hidden markov model tagging .", "label": "", "metadata": {}, "score": "54.38944"}
{"text": "In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .This takes a small number of seed definitions of the senses of some word ( the seeds could be WordNet synsets or definitions from some lexicon ) and uses these to classify the . ''", "label": "", "metadata": {}, "score": "54.453506"}
{"text": "In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .This takes a small number of seed definitions of the senses of some word ( the seeds could be WordNet synsets or definitions from some lexicon ) and uses these to classify the . ''", "label": "", "metadata": {}, "score": "54.453506"}
{"text": "According to the criteria given in Yarowsky ( 1993 ) , seed words that appear in the most reliable collocational relationships with the target word will be selected .The effect is much stronger for words in a predicate - argument relationship than for arbitrary associations at the same distance to the target word , and is much stronger for collocations with content words than with function words .", "label": "", "metadata": {}, "score": "54.66731"}
{"text": "This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .", "label": "", "metadata": {}, "score": "54.703743"}
{"text": "This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .", "label": "", "metadata": {}, "score": "54.703743"}
{"text": "This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .", "label": "", "metadata": {}, "score": "54.703743"}
{"text": "This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .", "label": "", "metadata": {}, "score": "54.703743"}
{"text": "This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .", "label": "", "metadata": {}, "score": "54.703743"}
{"text": "This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .", "label": "", "metadata": {}, "score": "54.703743"}
{"text": "These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .", "label": "", "metadata": {}, "score": "54.77825"}
{"text": "These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .", "label": "", "metadata": {}, "score": "54.77825"}
{"text": "These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .", "label": "", "metadata": {}, "score": "54.77825"}
{"text": "These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .", "label": "", "metadata": {}, "score": "54.77825"}
{"text": "These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .", "label": "", "metadata": {}, "score": "54.77825"}
{"text": "These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .", "label": "", "metadata": {}, "score": "54.77825"}
{"text": "These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .", "label": "", "metadata": {}, "score": "54.77825"}
{"text": "A good example of this is Luk 's system A. Luk .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 . which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .", "label": "", "metadata": {}, "score": "54.78191"}
{"text": "These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .A good example of this is Luk 's system [ 11 ] which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .", "label": "", "metadata": {}, "score": "55.018253"}
{"text": "These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .A good example of this is Luk 's system [ 11 ] which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .", "label": "", "metadata": {}, "score": "55.018253"}
{"text": "These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .A good example of this is Luk 's system [ 11 ] which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .", "label": "", "metadata": {}, "score": "55.018253"}
{"text": "These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .A good example of this is Luk 's system [ 11 ] which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .", "label": "", "metadata": {}, "score": "55.018253"}
{"text": "At the end of each iteration , the ' One sense per discourse ' property can be used to help preventing initially mistagged collocates and hence improving the purity of the seed sets .In order to avoid strong collocates becoming indicators for the wrong class , the class - inclusion threshold needs to be randomly altered .", "label": "", "metadata": {}, "score": "55.170128"}
{"text": "Distinguishing word senses in untagged text .In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 .", "label": "", "metadata": {}, "score": "55.51779"}
{"text": "Distinguishing word senses in untagged text .In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 .", "label": "", "metadata": {}, "score": "55.51779"}
{"text": "Distinguishing word senses in untagged text .In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 .", "label": "", "metadata": {}, "score": "55.51779"}
{"text": "Distinguishing word senses in untagged text .In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 .", "label": "", "metadata": {}, "score": "55.51779"}
{"text": "decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .This experiment involved the following steps : . # deriving a lexicon from the WordNet data files which contains all possible semantic tags for each noun , adjective , adverb and verb .", "label": "", "metadata": {}, "score": "55.526215"}
{"text": "These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .A good example of this is Luk 's system [ 12 ] which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .", "label": "", "metadata": {}, "score": "55.58663"}
{"text": "These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .A good example of this is Luk 's system [ 12 ] which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .", "label": "", "metadata": {}, "score": "55.58663"}
{"text": "Revision as of 09:01 , 1 December 2010 .Word Sense Disambiguation ( WSD ) is the process of identifying the sense of a polysemic word .In modern WSD systems , the senses of a word are typically taken from some specified dictionary .", "label": "", "metadata": {}, "score": "55.80754"}
{"text": "Add those examples in the residual that are tagged as A or B with probability above a reasonable threshold to the seed sets .Apply the decision - list algorithm and the above adding step iteratively .As more newly - learned collocations are added to the seed sets , the sense A or sense B set will grow , and the original residual will shrink .", "label": "", "metadata": {}, "score": "55.817345"}
{"text": "In this paper we show that for certain definitions of collocation , a polysemous word exhibits essentially only one sense per collocation .We test this empirical hypothesis ... \" .Previous work [ Gale , Church and Yarowsky , 1992 ] showed that with high probability a polysemous word has one sense per discourse .", "label": "", "metadata": {}, "score": "55.989273"}
{"text": "[ 2 ] However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .", "label": "", "metadata": {}, "score": "56.287605"}
{"text": "[ 2 ] However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .", "label": "", "metadata": {}, "score": "56.287605"}
{"text": "[ 2 ] However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .", "label": "", "metadata": {}, "score": "56.287605"}
{"text": "[ 2 ] However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .", "label": "", "metadata": {}, "score": "56.287605"}
{"text": "[ 2 ] However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .", "label": "", "metadata": {}, "score": "56.287605"}
{"text": "[ 2 ] However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .", "label": "", "metadata": {}, "score": "56.287605"}
{"text": "Word Sense Disambiguation ( WSD ) is the process of identifying the sense of a polysemic word .In modern WSD systems , the senses of a word are typically taken from some specified dictionary .These days WordNet is the usual dictionary in question .", "label": "", "metadata": {}, "score": "56.548847"}
{"text": "Word Sense Disambiguation ( WSD ) is the process of identifying the sense of a polysemic word .In modern WSD systems , the senses of a word are typically taken from some specified dictionary .These days WordNet is the usual dictionary in question .", "label": "", "metadata": {}, "score": "56.548847"}
{"text": "Word Sense Disambiguation ( WSD ) is the process of identifying the sense of a polysemic word .In modern WSD systems , the senses of a word are typically taken from some specified dictionary .These days WordNet is the usual dictionary in question .", "label": "", "metadata": {}, "score": "56.548847"}
{"text": "Approaches .Knowledge based .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet [ 5 ] and LDOCE [ 6 ] .", "label": "", "metadata": {}, "score": "56.712265"}
{"text": "Approaches .Knowledge based .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet [ 5 ] and LDOCE [ 6 ] .", "label": "", "metadata": {}, "score": "56.712265"}
{"text": "Yarowsky reports that the system correctly classifies senses 96 % of the time .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .", "label": "", "metadata": {}, "score": "56.879036"}
{"text": "Yarowsky reports that the system correctly classifies senses 96 % of the time .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .", "label": "", "metadata": {}, "score": "56.879036"}
{"text": "Yarowsky reports that the system correctly classifies senses 96 % of the time .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .", "label": "", "metadata": {}, "score": "56.879036"}
{"text": "Yarowsky reports that the system correctly classifies senses 96 % of the time .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .", "label": "", "metadata": {}, "score": "56.879036"}
{"text": "Approaches .Knowledge based .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet [ 4 ] and LDOCE [ 5 ] .", "label": "", "metadata": {}, "score": "56.882"}
{"text": "Approaches .Knowledge based .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet [ 4 ] and LDOCE [ 5 ] .", "label": "", "metadata": {}, "score": "56.882"}
{"text": "Approaches .Knowledge based .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet [ 4 ] and LDOCE [ 5 ] .", "label": "", "metadata": {}, "score": "56.882"}
{"text": "Approaches .Knowledge based .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet [ 4 ] and LDOCE [ 5 ] .", "label": "", "metadata": {}, "score": "56.882"}
{"text": "The resolution of a word 's syntactic ambiguity has largely been solved in language processing by part - of - speech taggers which predict the syntactic category of words in text with high levels of accuracy . E. Brill .Transformation - based error - driven learning and natural language processing : A case study in part of speech tagging .", "label": "", "metadata": {}, "score": "56.9507"}
{"text": "Discussions .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .", "label": "", "metadata": {}, "score": "57.55775"}
{"text": "Discussions .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .", "label": "", "metadata": {}, "score": "57.55775"}
{"text": "Discussions .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .", "label": "", "metadata": {}, "score": "57.55775"}
{"text": "Discussions .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .", "label": "", "metadata": {}, "score": "57.55775"}
{"text": "In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 .", "label": "", "metadata": {}, "score": "57.575222"}
{"text": "In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 .", "label": "", "metadata": {}, "score": "57.575222"}
{"text": "[ 1 ] The problem is that words often have more than one meaning , sometimes fairly similar and sometimes completely different .The meaning of a word in a particular usage can only be determined by examining its context .This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : .", "label": "", "metadata": {}, "score": "57.621567"}
{"text": "[ 1 ] The problem is that words often have more than one meaning , sometimes fairly similar and sometimes completely different .The meaning of a word in a particular usage can only be determined by examining its context .This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : .", "label": "", "metadata": {}, "score": "57.621567"}
{"text": "[ 1 ] The problem is that words often have more than one meaning , sometimes fairly similar and sometimes completely different .The meaning of a word in a particular usage can only be determined by examining its context .This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : .", "label": "", "metadata": {}, "score": "57.621567"}
{"text": "WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .One of the first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .", "label": "", "metadata": {}, "score": "57.747948"}
{"text": "WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .In modern WSD systems , the senses of a word are typically taken from some specified dictionary .These days [ [ WordNet ] ] is the usual dictionary in question .", "label": "", "metadata": {}, "score": "57.821564"}
{"text": "This approach , while theoretically not as powerful as deep approaches , gives superior results in practice , due to the computer 's limited world knowledge .Two shallow approaches used to train and then disambiguate are Na\u00efve Bayes classifiers and decision trees .", "label": "", "metadata": {}, "score": "57.827614"}
{"text": "In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .Word sense disambiguation using conceptual density .In Proceedings of COLING'96 .The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .", "label": "", "metadata": {}, "score": "57.87933"}
{"text": "In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .Word sense disambiguation using conceptual density .In Proceedings of COLING'96 .The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .", "label": "", "metadata": {}, "score": "57.87933"}
{"text": "The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .To appear in Journal of Natural Language Engineering , 4(3 ) .Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .", "label": "", "metadata": {}, "score": "58.05779"}
{"text": "The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .To appear in Journal of Natural Language Engineering , 4(3 ) .Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .", "label": "", "metadata": {}, "score": "58.05779"}
{"text": "A smoothing algorithm will then be used to avoid 0 values .The decision - list algorithm resolves many problems in a large set of non - independent evidence source by using only the most reliable piece of evidence rather than the whole matching collocation set .", "label": "", "metadata": {}, "score": "58.09704"}
{"text": "But over the last few years , there has n't been any major improvement in performance of any of these methods .It is instructive to compare the word sense disambiguation problem with the problem of part - of - speech tagging .", "label": "", "metadata": {}, "score": "58.196022"}
{"text": "WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .Contents .One of the first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .", "label": "", "metadata": {}, "score": "58.270256"}
{"text": "WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .Contents .One of the first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .", "label": "", "metadata": {}, "score": "58.270256"}
{"text": "This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .", "label": "", "metadata": {}, "score": "58.281357"}
{"text": "WSD systems are normally tested by having their results on a task compared against those of a human .However , humans do not agree on the task at hand - give a list of senses and sentences , and humans will not always agree on which word belongs in which sense .", "label": "", "metadata": {}, "score": "59.030785"}
{"text": "The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .", "label": "", "metadata": {}, "score": "59.060974"}
{"text": "The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .", "label": "", "metadata": {}, "score": "59.060974"}
{"text": "The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .", "label": "", "metadata": {}, "score": "59.060974"}
{"text": "The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .", "label": "", "metadata": {}, "score": "59.060974"}
{"text": "The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .", "label": "", "metadata": {}, "score": "59.060974"}
{"text": "The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .", "label": "", "metadata": {}, "score": "59.060974"}
{"text": "The most reliable collocations are at the top of the new list instead of the original seed words .The original untagged corpus is then tagged with sense labels and probabilities .The final decision list may now be applied to new data , the collocation with the highest rank in the list is used to classify the new data .", "label": "", "metadata": {}, "score": "59.173508"}
{"text": "This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance .We present a detailed case study of this learni ... \" .this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .", "label": "", "metadata": {}, "score": "59.295486"}
{"text": "Translation .In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .Word sense disambiguation using conceptual density .In Proceedings of COLING'96 .The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .", "label": "", "metadata": {}, "score": "59.50205"}
{"text": "Translation .In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .Word sense disambiguation using conceptual density .In Proceedings of COLING'96 .The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .", "label": "", "metadata": {}, "score": "59.50205"}
{"text": "This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .", "label": "", "metadata": {}, "score": "59.644592"}
{"text": "This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .", "label": "", "metadata": {}, "score": "59.644592"}
{"text": "This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .", "label": "", "metadata": {}, "score": "59.644592"}
{"text": "To appear in Journal of Natural Language Engineering , 4(3 ) .use large lexicons ( generally machine readable dictionaries ) and the information associated with the senses ( such as part - of - speech tags , topical guides and selectional preferences ) to indicate the correct sense .", "label": "", "metadata": {}, "score": "59.734825"}
{"text": "Computational Linguistics 18(1 ) , 1 - 30 ( 1992 ) .Clarkson , P.R. , Rosenfeld , R. : Statistical Language Modeling Using the CMU - Cambridge Toolkit .In : Proceedings ESCA Eurospeech ( 1997 ) .Fomey Jr. , G.D. : The Viterbi Algorithm .", "label": "", "metadata": {}, "score": "59.974174"}
{"text": "It should be noted that unsupervised disambiguation can not actually label specific terms as a referring to a specific concept : that would require more information than is available .What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .", "label": "", "metadata": {}, "score": "60.147713"}
{"text": "The categories listed for a word in Roget 's index tend to ... \" .This paper describes a program that disambiguates English word senses in unrestricted text using statistical models of the major Roget 's Thesaurus categories .Roget 's categories serve as approximations of conceptual classes .", "label": "", "metadata": {}, "score": "60.24483"}
{"text": "Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .", "label": "", "metadata": {}, "score": "60.39822"}
{"text": "Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .", "label": "", "metadata": {}, "score": "60.39822"}
{"text": "Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .", "label": "", "metadata": {}, "score": "60.39822"}
{"text": "Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .", "label": "", "metadata": {}, "score": "60.39822"}
{"text": "Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .", "label": "", "metadata": {}, "score": "60.39822"}
{"text": "Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .", "label": "", "metadata": {}, "score": "60.39822"}
{"text": "Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .", "label": "", "metadata": {}, "score": "60.39822"}
{"text": "Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , [ 8 ] decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .", "label": "", "metadata": {}, "score": "60.887726"}
{"text": "Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , [ 8 ] decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .", "label": "", "metadata": {}, "score": "60.887726"}
{"text": "Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , [ 8 ] decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .", "label": "", "metadata": {}, "score": "60.887726"}
{"text": "Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , [ 8 ] decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .", "label": "", "metadata": {}, "score": "60.887726"}
{"text": "The success rate for part - of - speech tagging algorithms is at present much higher than that for WSD , state - of - the art being around 95 % accuracy or better , as compared to less than 75 % accuracy in word sense disambiguation with supervised learning .", "label": "", "metadata": {}, "score": "61.230446"}
{"text": "In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 . which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .", "label": "", "metadata": {}, "score": "61.27976"}
{"text": "In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 . which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .", "label": "", "metadata": {}, "score": "61.27976"}
{"text": "Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .For example , does it make sense to describe an overall percentage accuracy for a WSD system or does evaluation require specific comparison of system performance on a word by word basis .", "label": "", "metadata": {}, "score": "61.59598"}
{"text": "Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .For example , does it make sense to describe an overall percentage accuracy for a WSD system or does evaluation require specific comparison of system performance on a word by word basis .", "label": "", "metadata": {}, "score": "61.59598"}
{"text": "Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .For example , does it make sense to describe an overall percentage accuracy for a WSD system or does evaluation require specific comparison of system performance on a word by word basis .", "label": "", "metadata": {}, "score": "61.59598"}
{"text": "Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .For example , does it make sense to describe an overall percentage accuracy for a WSD system or does evaluation require specific comparison of system performance on a word by word basis .", "label": "", "metadata": {}, "score": "61.59598"}
{"text": "Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .For example , does it make sense to describe an overall percentage accuracy for a WSD system or does evaluation require specific comparison of system performance on a word by word basis .", "label": "", "metadata": {}, "score": "61.59598"}
{"text": "Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .For example , does it make sense to describe an overall percentage accuracy for a WSD system or does evaluation require specific comparison of system performance on a word by word basis .", "label": "", "metadata": {}, "score": "61.59598"}
{"text": "Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , [ 9 ] decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .", "label": "", "metadata": {}, "score": "61.929"}
{"text": "Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , [ 9 ] decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .", "label": "", "metadata": {}, "score": "61.929"}
{"text": "One of the first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .The resolution of a word 's syntactic ambiguity has largely been solved in language processing by part - of - speech taggers which predict the syntactic category of words in text with high levels of accuracy .", "label": "", "metadata": {}, "score": "62.051254"}
{"text": "Black , E. : An Experiment in Computational Discrimination of English Word Senses .IBM Journal of Research and Development 32(2 ) , 185 - 194 ( 1988 ) CrossRef .Gale , W. , Church , K.W. , Yarowsky , D. : A Method for Disambiguating Word Senses in a Large Corpus , Statistical Research Report 104 , Bell Laboratories ( 1992 ) .", "label": "", "metadata": {}, "score": "62.168144"}
{"text": "Different dictionaries will provide different divisions of words into senses .One solution some researchers have used is to choose a particular dictionary , and just use its set of senses .Generally , however , research results using broad distinctions in senses have been much better than those using narrow , so most researchers ignore the fine - grained distinctions in their work .", "label": "", "metadata": {}, "score": "62.41058"}
{"text": "Another example is the work of Pedersen [ 11 ] who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .", "label": "", "metadata": {}, "score": "62.63013"}
{"text": "Another example is the work of Pedersen [ 11 ] who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .", "label": "", "metadata": {}, "score": "62.63013"}
{"text": "In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .", "label": "", "metadata": {}, "score": "62.76195"}
{"text": "Another example is the work of Pedersen [ 10 ] who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .", "label": "", "metadata": {}, "score": "62.919235"}
{"text": "Another example is the work of Pedersen [ 10 ] who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .", "label": "", "metadata": {}, "score": "62.919235"}
{"text": "Another example is the work of Pedersen [ 10 ] who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .", "label": "", "metadata": {}, "score": "62.919235"}
{"text": "Another example is the work of Pedersen [ 10 ] who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .", "label": "", "metadata": {}, "score": "62.919235"}
{"text": "Distinguishing word senses in untagged text .In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .", "label": "", "metadata": {}, "score": "62.951183"}
{"text": "Human performance , however , is much better on coarse - grained than fine - grained distinctions , so this again is why research on coarse - grained distinctions is most useful .Approaches .Deep approaches presume access to a comprehensive body of world knowledge .", "label": "", "metadata": {}, "score": "63.1885"}
{"text": "Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 . which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .", "label": "", "metadata": {}, "score": "64.46527"}
{"text": "Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 . which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .", "label": "", "metadata": {}, "score": "64.46527"}
{"text": "Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 . which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .", "label": "", "metadata": {}, "score": "64.46527"}
{"text": "Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 . which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .", "label": "", "metadata": {}, "score": "64.46527"}
{"text": "obvious .cases in a corpus .Decision lists R. Rivest .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .", "label": "", "metadata": {}, "score": "64.48015"}
{"text": "obvious .cases in a corpus .Decision lists R. Rivest .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .", "label": "", "metadata": {}, "score": "64.48015"}
{"text": "In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 Jonathan Traupman and Robert Wilensky .", "label": "", "metadata": {}, "score": "64.756424"}
{"text": "Covers the entire field with chapters contributed by leading researchers . is being used in a textual context .In modern WSD systems , the senses of a word are typically taken from some specified dictionary .In earlier systems the senses were more typically generic senses selected by the originators of the system .", "label": "", "metadata": {}, "score": "64.839874"}
{"text": ".. training sets .It thrives on raw , unannotated monolingual corpora - the more the merrier .The use of dictionary ... . \" ...This paper describes a program that disambiguates English word senses in unrestricted text using statistical models of the major Roget 's Thesaurus categories .", "label": "", "metadata": {}, "score": "65.080246"}
{"text": "Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored . constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .", "label": "", "metadata": {}, "score": "65.26301"}
{"text": "Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored . constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .", "label": "", "metadata": {}, "score": "65.26301"}
{"text": "Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored . constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .", "label": "", "metadata": {}, "score": "65.26301"}
{"text": "Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored . constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .", "label": "", "metadata": {}, "score": "65.26301"}
{"text": "Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored . constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .", "label": "", "metadata": {}, "score": "65.26301"}
{"text": "Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored . constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .", "label": "", "metadata": {}, "score": "65.26301"}
{"text": "In 1975 Kelly and Stone .E.F. Kelly and P.J. Stone .Computer Recognition of English Word Senses , Amsterdam : North - Holland . published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .", "label": "", "metadata": {}, "score": "65.6469"}
{"text": "In 1975 Kelly and Stone .E.F. Kelly and P.J. Stone .Computer Recognition of English Word Senses , Amsterdam : North - Holland . published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .", "label": "", "metadata": {}, "score": "65.6469"}
{"text": "Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .", "label": "", "metadata": {}, "score": "65.78369"}
{"text": "Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .", "label": "", "metadata": {}, "score": "65.78369"}
{"text": "Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .", "label": "", "metadata": {}, "score": "65.78369"}
{"text": "In Proceedings of COLING'96 and LDOCE J. Guthrie , L. Guthrie , Y. Wilks and H. Aidinejad , Subject - Dependent Co - Occurrence and Word Sense Disambiguation , ACL-91 , pp .146 - 152 .The information in these resources has been used in several ways , for example Wilks and Stevenson Y. Wilks and M. Stevenson .", "label": "", "metadata": {}, "score": "65.918625"}
{"text": "Since part - of - speech tagging is a relatively mature technology with high accuracy , we expected it to improve the accuracy of the much more difficult word sense disambiguation process .The third experiment modified the training phase of the Yarowsky algorithm by replacing its assumption of a uniform distribution of senses for a word with a more realistic one .", "label": "", "metadata": {}, "score": "66.50386"}
{"text": "Another aspect of word sense disambiguation that differentiates it from part - of - speech tagging is the availability of training data .While it is relatively easy to assign parts of speech to text , training people to tag senses is far more difficult .", "label": "", "metadata": {}, "score": "66.5727"}
{"text": "cases in a corpus .Decision lists R. Rivest .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .", "label": "", "metadata": {}, "score": "67.651436"}
{"text": "cases in a corpus .Decision lists R. Rivest .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .", "label": "", "metadata": {}, "score": "67.651436"}
{"text": "It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .", "label": "", "metadata": {}, "score": "67.9147"}
{"text": "It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .", "label": "", "metadata": {}, "score": "67.9147"}
{"text": "It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .", "label": "", "metadata": {}, "score": "67.9147"}
{"text": "It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .", "label": "", "metadata": {}, "score": "67.9147"}
{"text": "It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .", "label": "", "metadata": {}, "score": "67.9147"}
{"text": "It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .", "label": "", "metadata": {}, "score": "67.9147"}
{"text": "Y. Bar - Hillel .Language and Information .Addison - Wesley , 1964 .However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .", "label": "", "metadata": {}, "score": "68.879105"}
{"text": "An early sceptic was Bar - Hillel who famously proclaimed that \" sense ambiguity could not be resolved by electronic computer either current or imaginable . ''Y. Bar - Hillel .Language and Information .Addison - Wesley , 1964 .", "label": "", "metadata": {}, "score": "69.01135"}
{"text": "An early sceptic was Bar - Hillel who famously proclaimed that \" sense ambiguity could not be resolved by electronic computer either current or imaginable . ''Y. Bar - Hillel .Language and Information .Addison - Wesley , 1964 .", "label": "", "metadata": {}, "score": "69.01135"}
{"text": "Decision lists R. Rivest .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .", "label": "", "metadata": {}, "score": "69.39563"}
{"text": "Decision lists R. Rivest .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .", "label": "", "metadata": {}, "score": "69.39563"}
{"text": "Decision lists R. Rivest .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .", "label": "", "metadata": {}, "score": "69.39563"}
{"text": "Decision lists R. Rivest .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .", "label": "", "metadata": {}, "score": "69.39563"}
{"text": "In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987", "label": "", "metadata": {}, "score": "71.99786"}
{"text": "We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .", "label": "", "metadata": {}, "score": "72.167206"}
{"text": "However , the task has proved to be difficult for computer and some have believed that it would never be solved .An early sceptic was Bar - Hillel who famously proclaimed that \" sense ambiguity could not be resolved by electronic computer either current or imaginable . Y. Bar - Hillel .", "label": "", "metadata": {}, "score": "73.06014"}
{"text": "However , the task has proved to be difficult for computer and some have believed that it would never be solved .An early sceptic was Bar - Hillel who famously proclaimed that \" sense ambiguity could not be resolved by electronic computer either current or imaginable . Y. Bar - Hillel .", "label": "", "metadata": {}, "score": "73.06014"}
{"text": "Hearst , M.A. : Noun homograph disambiguation using local context in large corpora .In : Proceedings of the 7th Annual Conf . of the University of Waterloo Centre for the New OED and Text Research , Oxford , United Kingdom , pp . 1 - 19 ( 1991 ) .", "label": "", "metadata": {}, "score": "73.39325"}
{"text": "In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .", "label": "", "metadata": {}, "score": "73.40994"}
{"text": "Developing algorithms to replicate this human ability can often be a difficult task .Contents .Difficulties .One problem with word sense disambiguation is deciding what the senses are .In cases like the word bass above , at least some senses are obviously different .", "label": "", "metadata": {}, "score": "73.76479"}
{"text": "Addison - Wesley , 1964 .However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .", "label": "", "metadata": {}, "score": "74.65891"}
{"text": "Addison - Wesley , 1964 .However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .", "label": "", "metadata": {}, "score": "74.65891"}
{"text": "Translation .In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .Computer Recognition of English Word Senses , Amsterdam : North - Holland .Word sense disambiguation using conceptual density .", "label": "", "metadata": {}, "score": "74.73308"}
{"text": "Translation .In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .Computer Recognition of English Word Senses , Amsterdam : North - Holland .Word sense disambiguation using conceptual density .", "label": "", "metadata": {}, "score": "74.73308"}
{"text": "In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 aining algorithm still converges .", "label": "", "metadata": {}, "score": "74.73732"}
{"text": "in 1949 W. Weaver .In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .The problem of WSD was first introduced by Warren .Weaver . in 1949 W. Weaver .", "label": "", "metadata": {}, "score": "74.91797"}
{"text": "in 1949 W. Weaver .In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .The problem of WSD was first introduced by Warren .Weaver . in 1949 W. Weaver .", "label": "", "metadata": {}, "score": "74.91797"}
{"text": "The problem is that words often have more than one meaning , sometimes fairly similar and sometimes completely different .The meaning of a word in a particular usage can only be determined by examining its context .This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : . # The boy leapt from the bank into the cold water .", "label": "", "metadata": {}, "score": "76.03615"}
{"text": "Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .In 1975 .Kelly and Stone published a book explicitly listing their rules for disambiguation of word senses . . .", "label": "", "metadata": {}, "score": "76.47346"}
{"text": "Decision lists [ 13 ] are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .The learning proceeds in this way until all corpus instances are classified .", "label": "", "metadata": {}, "score": "76.563896"}
{"text": "Decision lists [ 13 ] are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .The learning proceeds in this way until all corpus instances are classified .", "label": "", "metadata": {}, "score": "76.563896"}
{"text": "Decision lists [ 13 ] are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .The learning proceeds in this way until all corpus instances are classified .", "label": "", "metadata": {}, "score": "76.563896"}
{"text": "Decision lists [ 13 ] are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .The learning proceeds in this way until all corpus instances are classified .", "label": "", "metadata": {}, "score": "76.563896"}
{"text": "Decision lists [ 14 ] are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .The learning proceeds in this way until all corpus instances are classified .", "label": "", "metadata": {}, "score": "78.21297"}
{"text": "Decision lists [ 14 ] are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .The learning proceeds in this way until all corpus instances are classified .", "label": "", "metadata": {}, "score": "78.21297"}
{"text": "Current accuracy exceeds 99 % on the full task , and typically is over 90 % for even the most difficult ambiguities .We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .", "label": "", "metadata": {}, "score": "78.36539"}
{"text": "# constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .# computing a HMM model based on the training corpus , runnig the tagger on the test corpus and comparing the results with the original tags in the test corpus .", "label": "", "metadata": {}, "score": "78.58198"}
{"text": "Contents .One of the first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .The resolution of a word 's syntactic ambiguity has largely been solved in language processing by part - of - speech taggers which predict the syntactic category of words in text with high levels of accuracy .", "label": "", "metadata": {}, "score": "84.2032"}
{"text": "Contents .One of the first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .The resolution of a word 's syntactic ambiguity has largely been solved in language processing by part - of - speech taggers which predict the syntactic category of words in text with high levels of accuracy .", "label": "", "metadata": {}, "score": "84.2032"}
{"text": "The van pulled up outside the bank and three masked men got out .We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .", "label": "", "metadata": {}, "score": "85.865295"}
{"text": "The van pulled up outside the bank and three masked men got out .We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .", "label": "", "metadata": {}, "score": "85.865295"}
{"text": "The van pulled up outside the bank and three masked men got out .We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .", "label": "", "metadata": {}, "score": "85.865295"}
{"text": "The van pulled up outside the bank and three masked men got out .We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .", "label": "", "metadata": {}, "score": "85.865295"}
{"text": "The van pulled up outside the bank and three masked men got out .We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .", "label": "", "metadata": {}, "score": "85.865295"}
{"text": "The van pulled up outside the bank and three masked men got out .We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .", "label": "", "metadata": {}, "score": "85.865295"}
{"text": "Shallow approaches do n't try to understand the text .They just consider the surrounding words , using information such as \" if bass has words sea or fishing nearby , it probably is in the fish sense ; if bass has the words music or song nearby , it is probably in the music sense . \"", "label": "", "metadata": {}, "score": "94.36021"}
{"text": "Abstract .This paper presents a statistical lexical ambiguity resolution method in direct transfer machine translation models in which the target language is Turkish .Since direct transfer MT models do not have full syntactic information , most of the lexical ambiguity resolution methods are not very helpful .", "label": "", "metadata": {}, "score": "99.18714"}
{"text": "For example , consider two examples of the distinct senses that exist for the word bass : . a type of fish . tones of low frequency .and the sentences : .I went fishing for some sea bass .The bass line of the song is very moving .", "label": "", "metadata": {}, "score": "104.47357"}
