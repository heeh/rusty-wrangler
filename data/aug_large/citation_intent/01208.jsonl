{"text": "[ Charniak , Eugene , \" Statistical Language Learning \" , 1993 , MIT Press ] .Very generally in the VSM , the n - dimensional vector used to characterize the vocabulary for a particular document can be viewed as a signal , although the order of the terms in the vector is not related to chronological or narrative order .", "label": "", "metadata": {}, "score": "32.474007"}
{"text": "[ 0009 ] .LSA applies a well - known mathematical technique called Singular Value Decomposition ( SVD ) to a word - by - document matrix .SVD is a form of factor analysis , or the mathematical generalization of which factor analysis is a special case .", "label": "", "metadata": {}, "score": "33.284904"}
{"text": "Additional flexibility may also be gained by carrying out these procedures within the wavelet transform so that locally significant coefficients are retained .This effectively produces a \" local \" reduced vocabulary .In the preferred embodiment , the resultant matrix contains rows associated with the N topics and columns associated with the ( N+M ) topics and cross - terms .", "label": "", "metadata": {}, "score": "33.506104"}
{"text": "Another approach is to reuse existing on - line lexicographic databases , such as WordNet ( see Voorhees et al . ; \" Vector Expansion in a Large Collection \" ; Proceedings of TREC , 1992 . ) or Longman 's subject codes ( see Liddy et al . ; \" Statistically - guided Word Sense Disambiguation \" ; Working Notes of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language ; 1992 AAAI Press ) .", "label": "", "metadata": {}, "score": "33.80948"}
{"text": "By scoring each factor separately and recombining them appropriately , documents are scored highly on all factors , and thus introduce a conjunctive constraint .For example , a document may score high for a query as a whole although it deals with only one of the subtopics of the query .", "label": "", "metadata": {}, "score": "34.207912"}
{"text": "Also note that all the vectors mentioned here are of the same size , the size of the vocabulary .An element of a vector is the frequency of occurrence of the word corresponding to that position in the vector .LSA modeling is a small variation on VSM modeling .", "label": "", "metadata": {}, "score": "35.563072"}
{"text": "Also note that all the vectors mentioned here are of the same size , the size of the vocabulary .An element of a vector is the frequency of occurrence of the word corresponding to that position in the vector .LSA modeling is a small variation on VSM modeling .", "label": "", "metadata": {}, "score": "35.563072"}
{"text": "Also note that all the vectors mentioned here are of the same size , the size of the vocabulary .An element of a vector is the frequency of occurrence of the word corresponding to that position in the vector .LSA modeling is a small variation on VSM modeling .", "label": "", "metadata": {}, "score": "35.563072"}
{"text": "In another embodiment , term sets can be extracted from the distance matrix by thresholding .The distance matrix can be represented as a graph where each term is a node and the distances between terms are weights on edges between respective nodes .", "label": "", "metadata": {}, "score": "35.751472"}
{"text": "The method may also include clustering documents that contain concept term sets together .In various embodiments , the filtering may be performed with reference to a set of stop words .The method may use a processor and memory , and the plurality of documents may be stored in the memory .", "label": "", "metadata": {}, "score": "36.113403"}
{"text": "[ 0006 ] .An information retrieval method with rising popularity is the use of the vector space model , where documents and queries are represented in a high - dimensional space in which each dimension of the space corresponds to a word in the document collection .", "label": "", "metadata": {}, "score": "36.19744"}
{"text": "A Boolean retrieval query passes through a corpus of documents by linking search terms together with Boolean operators such as AND , OR and NOT .The solution set of documents is generally smaller than the set that would result from single term searches , and all returned documents are typically ranked equally with respect to relevance .", "label": "", "metadata": {}, "score": "36.265743"}
{"text": "VSM modeling consists of : ( 1 ) Extracting the vocabulary used in a corpus .( 2 )Stemming the words so extracted and eliminating the designated stop words from the vocabulary .Stemming means that closely related words like ' programming ' and ' programs ' are reduced to the common root word ' program ' and the stop words are the non - discriminating words that can be expected to exist in virtually all the documents .", "label": "", "metadata": {}, "score": "36.391617"}
{"text": "VSM modeling consists of : ( 1 ) Extracting the vocabulary used in a corpus .( 2 )Stemming the words so extracted and eliminating the designated stop words from the vocabulary .Stemming means that closely related words like ' programming ' and ' programs ' are reduced to the common root word ' program ' and the stop words are the non - discriminating words that can be expected to exist in virtually all the documents .", "label": "", "metadata": {}, "score": "36.391617"}
{"text": "The array on the left will contain an alphabetized list of the files .generate_document_vectors ( ) : .This is a necessary step after the vocabulary used by a corpus is constructed .( Of course , if you will be doing document retrieval through a disk - stored VSM or LSA model , then you do not need to call this method .", "label": "", "metadata": {}, "score": "37.034035"}
{"text": "By default , a call to any of the constructors will calculate normalized term - frequency vectors for the documents .Note that ' word ' and ' term ' mean the same thing .In the calls above , the constructor parameter lsa_svd_threshold determines how many of the singular values will be retained after we have carried out an SVD decomposition of the term - frequency matrix for the documents in the corpus .", "label": "", "metadata": {}, "score": "37.365517"}
{"text": "A look at their nearest neighbors is usually sufficient to get a good idea of the topic that the word pertains to as demonstrated in Table 1 .With higher - order word representations , the user examines nearest neighbors as direct diagnostics for each individual term , thereby composing a query that matches the information needs of the user more closely .", "label": "", "metadata": {}, "score": "37.386684"}
{"text": "( 5 ) Constructing a query vector for the search query after the query is subject to the same stemming and stop - word elimination rules that were applied to the corpus .And , lastly , ( 6 ) Using a similarity metric to return the set of documents that are most similar to the query vector .", "label": "", "metadata": {}, "score": "37.387726"}
{"text": "( 5 ) Constructing a query vector for the search query after the query is subject to the same stemming and stop - word elimination rules that were applied to the corpus .And , lastly , ( 6 ) Using a similarity metric to return the set of documents that are most similar to the query vector .", "label": "", "metadata": {}, "score": "37.387726"}
{"text": "( 5 ) Constructing a query vector for the search query after the query is subject to the same stemming and stop - word elimination rules that were applied to the corpus .And , lastly , ( 6 ) Using a similarity metric to return the set of documents that are most similar to the query vector .", "label": "", "metadata": {}, "score": "37.387726"}
{"text": "Instead of evaluating the query as a whole , each subtopic should be evaluated individually and the results combined .If a document is irrelevant to one of the important subtopics of the query , then it often is irrelevant as a whole .", "label": "", "metadata": {}, "score": "37.718433"}
{"text": "Some of these developments are described below .Classification .Snob is a classification program .It relies on MML to form the best set of classes ( groups , clusters , species , types , ... ) to describe a given set of \" things \" ( individuals , items , measurements , observations , ... ) .", "label": "", "metadata": {}, "score": "38.046165"}
{"text": "( 3 ) creating a mathematical signal from the chronologically ordered , reduced vocabulary and the first several principal components from the association matrix or the full association matrix , .( 4 ) applying a discrete wavelet transform to this signal , and .", "label": "", "metadata": {}, "score": "38.103928"}
{"text": "For example , a hierarchical thesaurus is formed from a computer list of complex noun phrases where subsumption roughly corresponds to the subset relation defined on terms , e.g. , \" intelligence \" subsumes \" artificial intelligence \" .See Evans et al . ; \" Automatic Indexing Using Selective NLP and First - order Thesauri \" ; Proceedings of the RIAO ; Vol . 2 , pp .", "label": "", "metadata": {}, "score": "38.357418"}
{"text": "[ 0004 ] .Other information retrieval processes have extended and refined the Boolean term searching method to attempt to rank the resultant documents .Such processes do not necessarily reduce the size of the solution set .One such method of ranking the documents is by term weighting the query terms and/or term weighting the occurrence of terms in the solution set of documents by frequency .", "label": "", "metadata": {}, "score": "38.526623"}
{"text": "By retaining only a subset of the singular values ( usually the N largest for some value of N ) , you can construct reduced - dimensionality vectors for the documents and the queries .In VSM , as mentioned above , the size of the document and the query vectors is equal to the size of the vocabulary .", "label": "", "metadata": {}, "score": "38.594696"}
{"text": "By retaining only a subset of the singular values ( usually the N largest for some value of N ) , you can construct reduced - dimensionality vectors for the documents and the queries .In VSM , as mentioned above , the size of the document and the query vectors is equal to the size of the vocabulary .", "label": "", "metadata": {}, "score": "38.594696"}
{"text": "By retaining only a subset of the singular values ( usually the N largest for some value of N ) , you can construct reduced - dimensionality vectors for the documents and the queries .In VSM , as mentioned above , the size of the document and the query vectors is equal to the size of the vocabulary .", "label": "", "metadata": {}, "score": "38.594696"}
{"text": "Stemming will reduce all words such as ' programming , ' ' programs , ' ' program , ' etc . to the same root word ' program . 'The functions display_corpus_vocab ( ) and display_doc_vectors ( ) are there only for testing purposes with small corpora .", "label": "", "metadata": {}, "score": "38.63233"}
{"text": "For more general text retrieval , you would need to replace the simple stemmer used in the module by one based on , say , Porter 's Stemming Algorithm .You would also need to vastly expand the list of stop words appropriate to the text corpora of interest to you .", "label": "", "metadata": {}, "score": "38.86862"}
{"text": "For more general text retrieval , you would need to replace the simple stemmer used in the module by one based on , say , Porter 's Stemming Algorithm .You would also need to vastly expand the list of stop words appropriate to the text corpora of interest to you .", "label": "", "metadata": {}, "score": "38.86862"}
{"text": "As mentioned earlier , the module estimates the relevancies of the documents to the queries and dumps the relevancies in a file named by the ' relevancy_file ' constructor parameter .The constructor parameter ' relevancy_threshold ' is used in deciding which of the documents are considered to be relevant to a query .", "label": "", "metadata": {}, "score": "38.9012"}
{"text": "As mentioned earlier , the module estimates the relevancies of the documents to the queries and dumps the relevancies in a file named by the ' relevancy_file ' constructor parameter .The constructor parameter ' relevancy_threshold ' is used in deciding which of the documents are considered to be relevant to a query .", "label": "", "metadata": {}, "score": "38.9012"}
{"text": "If such judgments are available , run the script : . calculate_precision_and_recall_from_file_based_relevancies_for_VSM.pl .This script will print out the average precisions for the different test queries and calculate the MAP metric of retrieval accuracy .Significance testing consists of forming a null hypothesis that the two retrieval algorithms you are considering are the same from a black - box perspective and then calculating what is known as a p - value .", "label": "", "metadata": {}, "score": "38.950676"}
{"text": "Sequence or ( time- ) series data may contain hidden structure ( correlations ) .The MML classification method above has been extended to allow the ' class ' of a thing ( item , measurement , observation , ... ) to be influenced by other things , .", "label": "", "metadata": {}, "score": "39.147285"}
{"text": "The text engine will also reduce the amount of words contained within the document corpus by a variety of methods .For example , a text engine might remove stop words , stem words , filter the corpus according to word frequency or topicality , or perform some combination of these functions .", "label": "", "metadata": {}, "score": "39.163734"}
{"text": "This plays a critical role in creating reduced - dimensionality document vectors in LSA modeling of a corpus .The parameter query_file points to a file that contains the queries to be used for calculating retrieval performance with Precision and Recall numbers .", "label": "", "metadata": {}, "score": "39.199276"}
{"text": "One solution to this problem is to perform a dimensionality reduction of the order-1 vectors by means of a singular value decomposition , which is disclosed in Deerwester et al . cited above .It can be used to find a linear approximation of the original high - dimensional space ( one dimension for each word ) in an r - dimensional , reduced space , for an r on the order of 10 . sup.2 .", "label": "", "metadata": {}, "score": "39.27817"}
{"text": "The parameter corpus_vocab_db is for naming the DBM in which the corpus vocabulary will be stored after it is subject to stemming and the elimination of stop words .Once a disk - based VSM model is created and stored away in the file named by this parameter and the parameter to be described next , it can subsequently be used directly for speedier retrieval .", "label": "", "metadata": {}, "score": "39.37063"}
{"text": "See McCune et al . ; \" Rubric , A System for Rule - based Information Retrieval \" ; IEEE Transactions on Software Engineering 9 ; pp .939 - 44 ; 1985 .Each topic is a boolean combination of other topics and search terms .", "label": "", "metadata": {}, "score": "39.374603"}
{"text": "For example , the terms \" crash , \" \" spooler , \" and \" memory \" could be computed in a vector space with three dimensions .A term - frequency matrix A is constructed , where the set of term - frequency vectors in a corpus is maintained as rows and columns 120 .", "label": "", "metadata": {}, "score": "39.669518"}
{"text": "It may also rank the documents within each cluster according to the frequency of term co - occurrence within the concepts .A computerized method of analyzing a plurality of documents , comprising : . collecting and filtering term - frequency vector from a plurality of documents , wherein the filtering is performed with reference to a set of stop words ; . identifying a term - frequency vector for each of the documents ; . identifying a term - frequency matrix , wherein rows of the matrix comprise values for the term - frequency vectors ; . projecting the term - frequency matrix onto a lower dimensional space using latent semantic analysis , to create a transformed term matrix ; . developing a correlation matrix using the rows of the transformed term matrix ; . creating a concept graph of connected components using a concept threshold by identifying , from the transformed term matrix , groups of terms that exceed the correlation threshold to form a concept term set ; and .", "label": "", "metadata": {}, "score": "39.793053"}
{"text": "For example , the terms \" crash , \" \" spooler , \" and \" memory \" could be computed in a vector space with three dimensions .[ 0032 ] .A term - frequency matrix A is constructed , where the set of term - frequency vectors in a corpus is maintained as rows and columns 120 .", "label": "", "metadata": {}, "score": "39.794876"}
{"text": "An additional constructor parameter introduced in this version is case_sensitive .If you set it to 1 , that will force the database model and query matching to become case sensitive .Version 1.60 reflects the fact that people are now more likely to use this module by keeping the model constructed for a corpus in the fast memory ( as opposed to storing the models in disk - based hash tables ) for its repeated invocation for different queries .", "label": "", "metadata": {}, "score": "39.833717"}
{"text": "Note that ' word ' and ' term ' mean the same thing .In the calls above , the constructor parameter lsa_svd_threshold determines how many of the singular values will be retained after we have carried out an SVD decomposition of the term - frequency matrix for the documents in the corpus .", "label": "", "metadata": {}, "score": "39.865105"}
{"text": "The module estimates the relevancies of the documents to the queries and dumps the relevancies in a file named by the ' relevancy_file ' constructor parameter .The constructor parameter ' relevancy_threshold ' is used in deciding which of the documents are considered to be relevant to a query .", "label": "", "metadata": {}, "score": "39.866653"}
{"text": "The module estimates the relevancies of the documents to the queries and dumps the relevancies in a file named by the ' relevancy_file ' constructor parameter .The constructor parameter ' relevancy_threshold ' is used in deciding which of the documents are considered to be relevant to a query .", "label": "", "metadata": {}, "score": "39.866653"}
{"text": "The method may also include projecting the term - frequency matrix onto a lower dimensional space using latent semantic analysis to create a transformed term matrix .The method may also include determining a correlation matrix using the rows of the transformed term matrix , and creating a concept graph of connected components using a concept threshold , where each connected component comprises a set of terms that corresponds to a concept .", "label": "", "metadata": {}, "score": "39.933456"}
{"text": "A document is considered relevant to a query only when the document contains at least relevancy_threshold number of query words .The constructor parameter save_model_on_disk will cause the basic information about the VSM and the LSA models to be stored on the disk .", "label": "", "metadata": {}, "score": "40.185165"}
{"text": "A document must contain at least the ' relevancy_threshold ' occurrences of query words in order to be considered relevant to a query . txt ' , ' .As mentioned earlier , the module estimates the relevancies of the documents to the queries and dumps the relevancies in a file named by the ' relevancy_file ' constructor parameter .", "label": "", "metadata": {}, "score": "40.414364"}
{"text": "See : C. S. Wallace & J. D. Patrick .Coding decision trees .Machine Learning , 11 , pp.7 - 22 , 1993 .Decision Graphs are generalisations of decision trees .They model the same class of decision functions , but can express disjunctive conditions ( \" or \" ) much more economically .", "label": "", "metadata": {}, "score": "40.510178"}
{"text": "A large - scale singular value decomposition can be used for information retrieval .See Deerwester et al . ; \" Indexing by Latent Semantic Analysis \" ; Journal of the American Society of Information Science 41 ( 6 ) ; pp .", "label": "", "metadata": {}, "score": "40.585197"}
{"text": "Moreover , if the user does not have an understanding of the subject matter being searched , the combination of Boolean logic and term weighting may exclude the most relevant documents from the solution set and under - rank the documents that are most relevant to the subject matter .", "label": "", "metadata": {}, "score": "40.638107"}
{"text": "Moreover , if the user does not have an understanding of the subject matter being searched , the combination of Boolean logic and term weighting may exclude the most relevant documents from the solution set and under - rank the documents that are most relevant to the subject matter .", "label": "", "metadata": {}, "score": "40.638107"}
{"text": "VSM and LSA models have been around for a long time in the Information Retrieval ( IR ) community .More recently such models have been shown to be effective in retrieving files / documents from software libraries .VSM modeling consists of : ( 1 ) Extracting the vocabulary used in a corpus .", "label": "", "metadata": {}, "score": "40.711678"}
{"text": "The second subprocedure determines a trimmed sum profile from selected documents closest to a document group centroid .Given a set of k document groups that are to be treated as k centers for the purpose of attracting other documents , it is necessary to define a centroid for each group .", "label": "", "metadata": {}, "score": "40.739708"}
{"text": "But if such human - supplied relevance judgments are not available , you can invoke the following method to estimate them : .For the above method call , a document is considered to be relevant to a query if it contains several of the query words .", "label": "", "metadata": {}, "score": "40.87143"}
{"text": "But if such human - supplied relevance judgments are not available , you can invoke the following method to estimate them : .For the above method call , a document is considered to be relevant to a query if it contains several of the query words .", "label": "", "metadata": {}, "score": "40.87143"}
{"text": "This is a necessary step after the vocabulary used by a corpus is constructed .( Of course , if you will be doing document retrieval through a disk - stored VSM or LSA model , then you do not need to call this method .", "label": "", "metadata": {}, "score": "41.04497"}
{"text": "If human - supplied relevancy judgments are not available , the module will be happy to estimate relevancies for you just by determining the number of query words that exist in a document .Note , however , that relevancy judgments estimated in this manner can not be trusted .", "label": "", "metadata": {}, "score": "41.196495"}
{"text": "If human - supplied relevancy judgments are not available , the module will be happy to estimate relevancies for you just by determining the number of query words that exist in a document .Note , however , that relevancy judgments estimated in this manner can not be trusted .", "label": "", "metadata": {}, "score": "41.196495"}
{"text": "A concept graph of connected components may be created , using a concept threshold , where each connected component is a set of terms that corresponds to a concept .The method may also include clustering documents that contain concept term sets together .", "label": "", "metadata": {}, "score": "41.226387"}
{"text": "Words and documents are represented as vectors in the same multi - dimensional space that is derived from global lexical co - occurrence patterns .The method forms an improved retrieval performance for non - literal matches with queries .The computation of the lexical co - occurrence thesaurus proceeds in two phases .", "label": "", "metadata": {}, "score": "41.264297"}
{"text": "The second goal of word factorization is to eliminate irrelevant words semi - automatically .Many words in the Tipster topic descriptions are not relevant for the query in question , but they should not be placed on a stop list either because they could be relevant for other queries .", "label": "", "metadata": {}, "score": "41.30133"}
{"text": "Figure 1.1 graph of Minimum Term frequency Na\u00efve Bayesian model A word vector is created based on the training data .A Naive Bayesian classifier was used as an alternative approach to Bag of Words approach .The dimensions in the vector indicated the presence of the word and no special weight age parameter was used in classification .", "label": "", "metadata": {}, "score": "41.541386"}
{"text": "Thus the abscissa of the signal , in the preferred embodiment , is the narrative index in a view of the document without stop words which starts at one and goes to K. The terms that are either a topic or cross term ( i.e. survived the various filters : stop word list , stemming , document frequency and topicality ) are assigned their matching column of the Association Matrix .", "label": "", "metadata": {}, "score": "41.70014"}
{"text": "So if a word were to appear in all the documents , its IDF multiplier would be zero in the vector representation of a document .If so desired , you can turn off the IDF weighting of the words by explicitly setting the constructor parameter use_idf_filter to zero .", "label": "", "metadata": {}, "score": "41.747635"}
{"text": "Note that ' word ' and ' term ' mean the same thing . txt ' , ' .In the call above , the constructor parameter ' lsa_svd_threshold ' determines how many of the singular values will be retained after we have carried out an SVD decomposition of the term - frequency matrix for the documents in the corpus .", "label": "", "metadata": {}, "score": "41.748867"}
{"text": "Thus , the procedure simply maps from one word to other closely related words .For a thesaurus to be useful in information retrieval , it must be specific enough to offer synonyms for words as used in the corpus of interest .", "label": "", "metadata": {}, "score": "41.771904"}
{"text": "One way of conducting a query is to select a particular topic word or set of words and magnify the wavelet energy contained in the channels associated with those words .If the query words are topic terms for the article then there is little change required in the computational algorithm .", "label": "", "metadata": {}, "score": "41.786842"}
{"text": "Algorithm::VSM --- A Perl module for retrieving files and documents from a software library with the VSM ( Vector Space Model ) and LSA ( Latent Semantic Analysis ) algorithms in response to search words and phrases . txt ' , ' .", "label": "", "metadata": {}, "score": "41.816128"}
{"text": "Finally , when you set the boolean parameter debug , the module outputs a very large amount of intermediate results that are generated during model construction and during matching a query with the document vectors .If you would like to see corpus vocabulary as constructed by the previous call , make the call .", "label": "", "metadata": {}, "score": "41.8246"}
{"text": "This module has only been tested for software retrieval .For more general text retrieval , you would need to replace the simple stemmer used in the module by one based on , say , Porter 's Stemming Algorithm .You would also need to vastly expand the list of stop words appropriate to the text corpora of interest to you .", "label": "", "metadata": {}, "score": "41.82949"}
{"text": "Alternatively , problems may be identified based on an identification of terms having high numbers of occurrences in a concept set .[ 0046 ] .FIG .6 is a block diagram of exemplary hardware that may be used to contain and/or implement the program instructions of a system embodiment .", "label": "", "metadata": {}, "score": "41.98683"}
{"text": "An information retrieval method with rising popularity is the use of the vector space model , where documents and queries are represented in a high - dimensional space in which each dimension of the space corresponds to a word in the document collection .", "label": "", "metadata": {}, "score": "42.08336"}
{"text": "The values shown on the right side of the big arrows are the default values for the parameters .The following nested list will now describe each of the constructor parameters : .corpus_directory : .The parameter corpus_directory points to the root of the directory of documents for which you want to create a VSM or LSA model .", "label": "", "metadata": {}, "score": "42.083733"}
{"text": "The reference pattern may then be extracted from the given article or from a completely different context , e.g. one or more similar articles .More specifically , instead of taking a difference between two adjacent windows , the second vector in the difference is the sensor values averaged over the query terms .", "label": "", "metadata": {}, "score": "42.194046"}
{"text": "A .XP . puffs . spooler .crashes .memory . toner .d .d .d .d .Matrix A may be projected onto a lower dimensional space using Latent Semantic Analysis 125 .In other words , a six - dimensional space such as that of the above example could be projected onto a three - dimensional space using LSA .", "label": "", "metadata": {}, "score": "42.28521"}
{"text": "The net effect of this computation is to produce for each unique term a dense p - dimensional vector that characterizes its co - occurrence neighborhoods .These vectors then define a thesaurus by associating each word with its nearest neighbors .", "label": "", "metadata": {}, "score": "42.339592"}
{"text": "This step helps to reduce the dimensionally of the vocabulary needed to describe the original document and produce a more focused list of words .Also in the text engine utilized in a preferred embodiment , a suffix and a prefix list may be used to help reduce each word to its stem .", "label": "", "metadata": {}, "score": "42.382168"}
{"text": "With LSA , a database is searched using a query , which produces result items .The result items are clustered into logical categories , and the result items within each category are ranked based on the frequency of the occurrence of relevant words in each of the result items .", "label": "", "metadata": {}, "score": "42.405113"}
{"text": "In such a case , the concept threshold may be less than or equal to 1 , and terms that have correlation higher than the concept threshold may form the vertices of the concept graph .[ 0019 ] .The method may also include projecting the term - frequency matrix onto a lower dimensional space using latent semantic analysis to create a transformed term matrix .", "label": "", "metadata": {}, "score": "42.509354"}
{"text": "Document information retrieval using global word co - occurrence patterns .Abstract .A method and apparatus accesses relevant documents based on a query .A thesaurus of word vectors is formed for the words in the corpus of documents .The word vectors represent global lexical co - occurrence patterns and relationships between word neighbors .", "label": "", "metadata": {}, "score": "42.60167"}
{"text": "Before you can carry out precision and recall calculations to test the accuracy of VSM and LSA based retrievals from a corpus , you need to have available the relevancy judgments for the queries .( A relevancy judgment for a query is simply the list of documents relevant to that query . )", "label": "", "metadata": {}, "score": "42.62785"}
{"text": "2 depicts a flow diagram for an exemplary method of analyzing documents according to an embodiment .FIG .3 is a graphic illustration of an example of hierarchical agglomerative clustering .FIGS .4 a - 4 d are diagrams showing a term matrix in graphic form .", "label": "", "metadata": {}, "score": "42.688164"}
{"text": "Alternatively , problems may be identified based on an identification of terms having high numbers of occurrences in a concept set .FIG .6 is a block diagram of exemplary hardware that may be used to contain and/or implement the program instructions of a system embodiment .", "label": "", "metadata": {}, "score": "42.717575"}
{"text": "Formally , document ranks of the form are considered : . where r.sub.cv is the context vector rank ; r.sub.tf.idf is the tf.idf rank ; and . alpha . is a free parameter between 0 and 1 .FIG .19 shows a precision graph for 11 points of recall .", "label": "", "metadata": {}, "score": "42.753365"}
{"text": "Ideally , if a word appears in all the documents , its idf would be small , close to zero .Words with small idf values are non - discriminatory and should get reduced weighting in document retrieval .This is a necessary step after the vocabulary used by a corpus is constructed .", "label": "", "metadata": {}, "score": "42.771667"}
{"text": "Because of DocumentSpace 's independence from literal matches , it also does well even on one - word queries .There is a benefit of the parallel design of WordSpace and DocumentSpace .For a query consisting of words , the topic and content of a document can be described by its near neighbors in WordSpace , in complete analogy to the retrieval of document neighbors in DocumentSpace .", "label": "", "metadata": {}, "score": "42.796844"}
{"text": "You call this subroutine for constructing an LSA model for your corpus after you have extracted the corpus vocabulary and constructed document vectors : .The SVD decomposition that is carried out in LSA model construction uses the constructor parameter lsa_svd_threshold to decide how many of the singular values to retain for the LSA model .", "label": "", "metadata": {}, "score": "42.883392"}
{"text": "In the graph of FIG .19 , the bottom line uses the recall points for tf.idf .The middle line uses the recall points for linear combination for the optimal choice of . alpha . , which is 0.7 .Thus the average precision for tf.idf is 0.271 and the average precision for the linear combination of tf.idf and context vectors is 0.300 .", "label": "", "metadata": {}, "score": "42.99399"}
{"text": "In Version 1.1 , the new script significance_testing.pl in the ' examples ' directory illustrates significance testing with Randomization and with Student 's Paired t - Test .VSM and LSA models have been around for a long time in the Information Retrieval ( IR ) community .", "label": "", "metadata": {}, "score": "43.07091"}
{"text": "The definitions for encoding words are analogous to those in equations 1 - 2 and 4 - 5 except that the restricted vocabulary V.sub .N containing the N most frequent content words is used .In the second step , the representations for the total vocabulary are computed by summing up the reduced order-1 vectors of all neighbors of a given word .", "label": "", "metadata": {}, "score": "43.17468"}
{"text": "An alternative approach would be to simply delete the terms not in the reduced vocabulary and use the narrative index of the resulting compressed article as the abscissa .The critical element in creating the signal is that each word is assigned a vector of values that contains the interrelationships to all or an important subset of words in the document .", "label": "", "metadata": {}, "score": "43.306477"}
{"text": "See Grefenstette , G. ; \" Use of Syntactic Context to Produce Term Association Lists for Text Retrieval \" ; Proceedings of SIGIR 1992 ; pp .89 - 97 .See Ruge , G. ; \" Experiments on Linguistically - based Term Associations \" ; Information Processing & Management 28 ( 3 ) ; pp .", "label": "", "metadata": {}, "score": "43.33557"}
{"text": "If you want to turn off the normalization of the document vectors , including turning off the weighting of the term frequencies of the words by their idf values , you must set this parameter explicitly to 0 .The parameter stop_words_file is for naming the file that contains the stop words that you do not wish to include in the corpus vocabulary .", "label": "", "metadata": {}, "score": "43.338867"}
{"text": "Word factors containing nuisance or non - topical terms can be deleted from the query .FIG .14 shows the process for query factorization .In step 300 , the query is input into the processor .The processor retrieves the thesaurus vectors for the words in the query in step 302 .", "label": "", "metadata": {}, "score": "43.56433"}
{"text": "A document must contain at least the ' relevancy_threshold ' occurrences of query words in order to be considered relevant to a query .We have previously explained the role of the constructor parameter ' lsa_svd_threshold ' .txt ' , ' .", "label": "", "metadata": {}, "score": "43.60436"}
{"text": "The method of .claim 1 , wherein the filtering is performed with reference to a set of stop words .The method of .claim 1 , wherein the method uses a processor and memory , and the plurality of documents is stored in the memory .", "label": "", "metadata": {}, "score": "43.66426"}
{"text": "Queries may be viewed as short documents and hence may also be represented as vectors .Search proceeds by searching near neighbors to the query vector in document space .The assumption is that queries and documents are similar to the extent that they contain the same words .", "label": "", "metadata": {}, "score": "43.713455"}
{"text": "In various embodiments , the method may include ranking the documents within each cluster according to the number of occurrences of concept terms within a term set .Singular value decomposition or another appropriate method may be used to express the term - frequency matrix as a product of at least a document matrix , a diagonal matrix of eigenvalues , and a term matrix .", "label": "", "metadata": {}, "score": "43.780838"}
{"text": "Stemming helps to reduce the dimensionality of the vocabulary needed to describe the original document and produce a more focused list of words .After stemming words may be referred to as terms ; two initially different words may now be mapped into the same term .", "label": "", "metadata": {}, "score": "43.821625"}
{"text": "In Version 1.1 , the new script significance_testing.pl in the ' examples ' directory illustrates significance testing with Randomization and with Student 's Paired t - Test .DESCRIPTION .VSM and LSA models have been around for a long time in the Information Retrieval ( IR ) community .", "label": "", "metadata": {}, "score": "43.834343"}
{"text": "The format of the query file must be as shown in the sample file test_queries.txt in the ' examples ' directory .The constructor parameter relevancy_threshold is used for automatic determination of document relevancies to queries on the basis of the number of occurrences of query words in a document .", "label": "", "metadata": {}, "score": "43.895844"}
{"text": "Subsequently you call .for retrieval and for displaying the results . estimate_doc_relevancies ( ) : .Before you can carry out precision and recall calculations to test the accuracy of VSM and LSA based retrievals from a corpus , you need to have available the relevancy judgments for the queries .", "label": "", "metadata": {}, "score": "43.907406"}
{"text": "The database named by normalized_doc_vecs_db stores the normalized document vectors .Normalization consists of factoring out the size of the documents by dividing the term frequency for each word in a document by the number of words in the document , and then multiplying the result by the idf ( Inverse Document Frequency ) value for the word .", "label": "", "metadata": {}, "score": "43.945763"}
{"text": "The database named by normalized_doc_vecs_db stores the normalized document vectors .Normalization consists of factoring out the size of the documents by dividing the term frequency for each word in a document by the number of words in the document , and then multiplying the result by the idf ( Inverse Document Frequency ) value for the word .", "label": "", "metadata": {}, "score": "43.945763"}
{"text": "The database named by normalized_doc_vecs_db stores the normalized document vectors .Normalization consists of factoring out the size of the documents by dividing the term frequency for each word in a document by the number of words in the document , and then multiplying the result by the idf ( Inverse Document Frequency ) value for the word .", "label": "", "metadata": {}, "score": "43.945763"}
{"text": "The crucial operation of the user interface is retrieval of nearest neighbors , either a small number ( e.g. 10 ) that is presented directly for inspection , or a large number that is clustered and presented to the user in digested form .", "label": "", "metadata": {}, "score": "43.95868"}
{"text": "A singular value decomposition is used to reduce the dimensionality of the document vectors .A query vector is formed from the combination of word vectors associated with the words in the query .The query vector and document vectors are compared to determine the relevant documents .", "label": "", "metadata": {}, "score": "43.968037"}
{"text": "The testing is performed on a corpus of Wikipedia articles .They also present a sentence insertion model which determines the best paragraph within which to place a sentence .Alex Smola Et al(2007 ) worked on Semi Markov model using Max Margin method .", "label": "", "metadata": {}, "score": "43.996445"}
{"text": "However , increasing the number of search terms in the query narrows the scope of the search and thereby increases the risk that a relevant document is missed .Still again , all documents in the solution set are ranked equally .", "label": "", "metadata": {}, "score": "43.99652"}
{"text": "[0002 ] .Information retrieval methods exist for analyzing documents in a document repository and retrieving specific documents that are responsive to a query .For example , a simple information retrieval technique is word or term searching .Term searching involves a user querying a corpus of documents containing information for a specific term or word .", "label": "", "metadata": {}, "score": "44.024704"}
{"text": "This approach has been shown to be effective with certain types of signals .In a quantization type scheme , the significant wavelet coefficients may be retained to a small precision ( i.e. if the original signal is in double precision and the wavelet coefficients are stored in single precision ) .", "label": "", "metadata": {}, "score": "44.133"}
{"text": "We apply dimensionality reduction technique using Minimum term frequency , stop word identification and elimination methods for achieving the task .It is evident that Na\u00efve Bayesian Multinomial model outperforms simple Na\u00efve Bayesian approach in paragraph classification tasks .KEYWORDS performance , classifier , paragraph level classification , text classification , Na\u00efve Bayesian , Multinomial .", "label": "", "metadata": {}, "score": "44.194733"}
{"text": "98 - 107 .McCune et al . , \" Rubric : A System for Rule - Based Information Retrieval \" , IEEE Transactions on Software Engineering , vol .SE-11 , No . 9 , 1985 , pp .939 - 945 .", "label": "", "metadata": {}, "score": "44.200127"}
{"text": "The first call uses the regular document vectors and the second the normalized document vectors .After you have constructed a VSM model , you call this method for document retrieval for a given query @query .The call syntax is : .", "label": "", "metadata": {}, "score": "44.331924"}
{"text": "The technique of singular value decomposition ( SVD ) is used to achieve a dimensional reduction by obtaining a compact and tractable representation for search purposes .The uniform representation for words and documents provides a simple and elegant user interface for query focusing and expansion .", "label": "", "metadata": {}, "score": "44.37895"}
{"text": "LSA applies a well - known mathematical technique called Singular Value Decomposition ( SVD ) to a word - by - document matrix .SVD is a form of factor analysis , or the mathematical generalization of which factor analysis is a special case .", "label": "", "metadata": {}, "score": "44.418457"}
{"text": "And , along the same lines , Recall at a given rank r is the ratio of the number of retrieved documents that are relevant to the total number of relevant documents .The area under the Precision -- Recall curve is called the Average Precision for a query .", "label": "", "metadata": {}, "score": "44.428925"}
{"text": "The simplest , and perhaps most conventional , approach to thesaurus construction is to manually build an explicit semantic mapping table .This is clearly labor - intensive , and hence only possible in specialized domains where repeated use may justify the cost .", "label": "", "metadata": {}, "score": "44.439125"}
{"text": "This reorganization is reflected in the description of the examples directory in this documentation .The basic logic of constructing VSM and LSA models and how these are used for retrievals remains unchanged .Version 1.50 incorporates a couple of new features : ( 1 )", "label": "", "metadata": {}, "score": "44.471474"}
{"text": "Such processes do not necessarily reduce the size of the solution set .One such method of ranking the documents is by term weighting the query terms and/or term weighting the occurrence of terms in the solution set of documents by frequency .", "label": "", "metadata": {}, "score": "44.47608"}
{"text": "The new methods allow the true class structure to be recovered on less data than before and indicates where and why class membership ( probably ) changed .( A thing can have many attributes ; an attribute can be drawn from normal ( Gaussian ) , Poisson , von - Mises , etc . distributions . )", "label": "", "metadata": {}, "score": "44.48719"}
{"text": "The third subprocedure assigns individual documents to the closest center represented by one of these trimmed sum profiles .Referring to FIG .2 , the steps of the Buckshot method are shown .In step 30 , a random sample of C ' is constructed from corpus C of size . sqroot.kN.", "label": "", "metadata": {}, "score": "44.4936"}
{"text": "Information retrieval methods exist for analyzing documents in a document repository and retrieving specific documents that are responsive to a query .For example , a simple information retrieval technique is word or term searching .Term searching involves a user querying a corpus of documents containing information for a specific term or word .", "label": "", "metadata": {}, "score": "44.604103"}
{"text": "The tree structure can be drawn and examined , and it represents a kind of explanation of the data .Decision trees are used in expert systems to \" learn \" how a human expert does what she or he does well , given a training set of examples .", "label": "", "metadata": {}, "score": "44.643867"}
{"text": "Jason .D.M.Rennie Et.al(2003 ) proposes heuristic solutions to some of the problems with Na\u00efve Bayes classifiers .They first review Multinomial Na\u00efve Bayes model for classification and discuss several systemic problems with it .Applications like sentiment analysis , student online essay scoring , ext summarization etc are predominant for the present information era .", "label": "", "metadata": {}, "score": "44.651978"}
{"text": "There is also a need to update these data regularly .The problem of finding the correct location to insert new updated information in a hierarchical text is quite a challenging task .Normally , the structure of a given document is .", "label": "", "metadata": {}, "score": "44.663063"}
{"text": "Any particular class size will either separate some words from close neighbors or lump together some words with distant terms .A thesaurus can be constructed by defining a similarity measure on terms within the document .See Qiu et al . ; \" Concept Based Query Expansion \" ; Proceedings of SIGIR 1993 .", "label": "", "metadata": {}, "score": "44.698067"}
{"text": "After you have constructed a VSM model , you call this method for document retrieval for a given query @query .The call syntax is : .The argument , @query , is simply a list of words that you wish to use for retrieval .", "label": "", "metadata": {}, "score": "44.73252"}
{"text": "After you have constructed a VSM model , you call this method for document retrieval for a given query @query .The call syntax is : .The argument , @query , is simply a list of words that you wish to use for retrieval .", "label": "", "metadata": {}, "score": "44.73252"}
{"text": "To experiment with precision and recall calculations for LSA retrieval , run the script : . calculate_precision_and_recall_for_LSA.pl .Note that this script will carry out its own estimation of relevancy judgments --- which in most cases would not be a safe thing to do .", "label": "", "metadata": {}, "score": "44.80114"}
{"text": "The mathematical basis for this approach is the standard Vector Space Model ( VSM ) used in IR .In the VSM each document is represented as a vector of weights with each weight corresponding to a particular word or concept in the text .", "label": "", "metadata": {}, "score": "44.83736"}
{"text": "With Version 1.1 , you can access the retrieval precision results so that you can compare two different retrieval algorithms ( VSM or LSA with different choices for some of the constructor parameters ) with significance testing .( Version 1.0 merely sent those results to standard output , typically your terminal window . )", "label": "", "metadata": {}, "score": "44.8638"}
{"text": "Once a disk - based VSM model is created and stored away in the file named by this parameter and the parameter to be described next , it can subsequently be used directly for speedier retrieval .The database named by doc_vectors_db stores the document vector representation for each document in the corpus .", "label": "", "metadata": {}, "score": "44.921597"}
{"text": "No .07/790,316 to Pedersen et al . , which is incorporated herein by reference .The Buckshot method , which employs three subprocedures , will be described briefly .The first subprocedure , truncated group average agglomerate clustering , merges disjoint document sets , or groups , starting with individuals until only k groups remain .", "label": "", "metadata": {}, "score": "44.93081"}
{"text": "Documents that are returned with higher ranking search terms are themselves ranked higher in relevance .However , a more useful variation of term weighting is by occurrence frequency in a resultant document database , thereby allowing the documents in the solution set to be ranked .", "label": "", "metadata": {}, "score": "45.055416"}
{"text": "Documents that are returned with higher ranking search terms are themselves ranked higher in relevance .However , a more useful variation of term weighting is by occurrence frequency in a resultant document database , thereby allowing the documents in the solution set to be ranked .", "label": "", "metadata": {}, "score": "45.055416"}
{"text": "Setting case_sensitive to 1 also causes the query matching to become case sensitive .( This constructor parameter was introduced in Version 1.61 . )The parameter corpus_vocab_db is for naming the DBM in which the corpus vocabulary will be stored after it is subject to stemming and the elimination of stop words .", "label": "", "metadata": {}, "score": "45.11061"}
{"text": "A typical set of rules might include having at least three intervening token sequences between boundaries and specifying that all boundaries must be moved to the end of the nearest paragraph .In the VSM , certain \" filters \" are often used to identify the best words to characterize a document .", "label": "", "metadata": {}, "score": "45.195366"}
{"text": "The decision whether a word factor was relevant or not was made manually .The word factors that were judged relevant were then combined according to the algorithm described above .The top line in FIG .19 shows the precision for 11 recall points for .", "label": "", "metadata": {}, "score": "45.276985"}
{"text": "Thus , LSA provides a means for extracting and representing the contextual - usage meaning of words by statistical computations applied to a large corpus of text .Information content is defined by more than just its mere terms but is also characterized by the context in which a specific term is given .", "label": "", "metadata": {}, "score": "45.290096"}
{"text": "Specifically , the document is converted to an electronic signal and a wavelet transform is then performed on the signal .A method for automatically partitioning an unstructured electronically formatted natural language document into its sub - topic structure .Specifically , the document is converted to an electronic signal and a wavelet transform is then performed on the signal .", "label": "", "metadata": {}, "score": "45.33185"}
{"text": "7 shows Matrix B , which has rows corresponding to A - classes , i.e. , columns to words .For example , the B - subset contains the 20,000 most frequent words , excluding stop words .In step 110 , this B - subset is again partitioned into 200 word classes by clustering the columns of matrix B. The purpose of this second iteration is to ensure that each word in the corpus has a sufficient number of neighbors from at least one word class .", "label": "", "metadata": {}, "score": "45.400146"}
{"text": "[0008 ] .Latent Semantic Analysis ( LSA ) is a technique that helps to identify the contextual meaning of words or phrases in a document by using vector analysis to project queries and documents into a space which helps to infer semantic relations between terms .", "label": "", "metadata": {}, "score": "45.406178"}
{"text": "Subsequently , these databases can be used for much faster retrieval from the same corpus .The parameter ' want_stemming ' means that you would want the words in the documents to be stemmed to their root forms before the VSM model is constructed .", "label": "", "metadata": {}, "score": "45.432915"}
{"text": "The idea is to identify a class or a group to which a particular \" document \" belongs to .The supervised form of this task requires the availability of \" training data \" - the data that can \" train \" a classifier to correctly identify the class of an unknown text - which is called the \" test data \" .", "label": "", "metadata": {}, "score": "45.633846"}
{"text": "Thus , the weight of the word drops because it has a low inverse document frequency .The context vectors d.sub.j depend only on the underlying thesaurus vectors .Thus , this method is automatic .The document vectors are a derivation from the corpus .", "label": "", "metadata": {}, "score": "45.63882"}
{"text": "The distance matrix can be represented as a graph where each term is a node and the distances between terms are weights on edges between respective nodes .This is shown in .FIG .5 a .FIG .5 b .", "label": "", "metadata": {}, "score": "45.7092"}
{"text": "In traditional information retrieval systems , it is hard to assess the impact of the terms used in a query .The user communicates with the system on the level of document descriptions .In the case of relevance feedback , one specifies which documents returned as response to the original query should be the basis for the next search iteration .", "label": "", "metadata": {}, "score": "45.754738"}
{"text": "For each document in the corpus , a term - frequency vector is computed that contains the frequency of filtered terms in the document 115 .For example , .FIG .1 shows the terms \" crash , \" and \" spooler , \" in a vector space with two dimensions .", "label": "", "metadata": {}, "score": "45.76477"}
{"text": "Ideally , if a word appears in all the documents , its idf would be small , close to zero .Words with small idf values are non - discriminatory and should get reduced weighting in document retrieval . get_all_document_names ( ) : .", "label": "", "metadata": {}, "score": "45.805138"}
{"text": "Thus , synonyms are not required to co - occur , but they must have similar co - occurrence patterns .A multi - dimensional continuous space is formed where each word 's thesaurus vector represents its individual position .A continuous space does not force a classification choice , and hence avoids some of the ensuing problems .", "label": "", "metadata": {}, "score": "45.8386"}
{"text": "In other words , through the pattern of co - occurrences of words , LSI is able to infer the structure of relationships between articles and words that define context and meaning by the relationship between structure and concept .For more information about LSI , see e.g. , Foundations of Statistical Natural Language Processing , Christopher D. Manning , Hinrich Schutze , The MIT Press , 1999 .", "label": "", "metadata": {}, "score": "45.849697"}
{"text": "In other words , through the pattern of co - occurrences of words , LSI is able to infer the structure of relationships between articles and words that define context and meaning by the relationship between structure and concept .For more information about LSI , see e.g. , Foundations of Statistical Natural Language Processing , Christopher D. Manning , Hinrich Schutze , The MIT Press , 1999 .", "label": "", "metadata": {}, "score": "45.849697"}
{"text": "The method of . claim 15 , wherein the filtering is performed with reference to a set of stop words .The method of . claim 15 , wherein the method uses a processor and memory , and the plurality of documents is stored in the memory .", "label": "", "metadata": {}, "score": "45.849754"}
{"text": "In the order-0 scheme , the word vector representation was constructed from one entry for the word .The generalization of order-1 representations is to construct a word vector representation from entries for the word 's neighbors in the document collection .", "label": "", "metadata": {}, "score": "45.862312"}
{"text": "A document frequency filter specifies that a term must occur in at least A% of documents and in no more than B% of documents in order to be kept in the VSM vocabulary .Zipf 's Law states that the majority of words will occur once or twice , a few words will occur very often , but the most useful words for document discrimination occur a moderate amount of times .", "label": "", "metadata": {}, "score": "45.872917"}
{"text": "Application of the wavelet transform to the signal .Mathematically the definition of the Haar wavelet coefficients is # # EQU4 # # where m is the channel , k is the multi - resolution level , and j corresponds approximately to the narrative index at which the filter is centered .", "label": "", "metadata": {}, "score": "45.94906"}
{"text": "These connected components are extracted algorithmically by methods known in the art .FIG .5 c .FIG .5 d .In .It can be seen that the higher the threshold , the broader the concepts that are extracted from the distance matrix or corresponding graph .", "label": "", "metadata": {}, "score": "45.949337"}
{"text": "Each term of the documents is associated with a vector that represents the term 's pattern of local co - occurrences .This vector can then be compared with others to measure the co - occurrence similarity , and hence semantic similarity of terms .", "label": "", "metadata": {}, "score": "45.9543"}
{"text": "( 2 ) Where is the probability vector of the characteristic word that belongs to every class .To find the class that the text belongs to , the following relation must be maximized : .International Journal of Artificial Intelligence & Applications ( IJAIA ) , Vol .", "label": "", "metadata": {}, "score": "46.002987"}
{"text": "Furthermore , since the returned documents are not ranked , the user may be tempted to reduce the size of the solution set by including more Boolean linked search terms .However , increasing the number of search terms in the query narrows the scope of the search and thereby increases the risk that a relevant document is missed .", "label": "", "metadata": {}, "score": "46.071777"}
{"text": "[ 0038 ] .Terms that are highly correlated are closer together in a latent , concept space .Correlations are converted to distances by subtracting them from 1 so that a correlation of 1 corresponds to a distance of 0 as seen in the distance matrix below . toner .", "label": "", "metadata": {}, "score": "46.394066"}
{"text": "When you invoke the methods get_corpus_vocabulary_and_word_counts ( ) and generate_document_vectors ( ) , that automatically deposits the VSM model in the database files named with the constructor parameters corpus_vocab_db , doc_vectors_db and normalized_doc_vecs_db .Subsequently , you can carry out retrieval by directly using this disk - based VSM model for speedier performance .", "label": "", "metadata": {}, "score": "46.426403"}
{"text": "When you invoke the methods get_corpus_vocabulary_and_word_counts ( ) and generate_document_vectors ( ) , that automatically deposits the VSM model in the database files named with the constructor parameters corpus_vocab_db , doc_vectors_db and normalized_doc_vecs_db .Subsequently , you can carry out retrieval by directly using this disk - based VSM model for speedier performance .", "label": "", "metadata": {}, "score": "46.426403"}
{"text": "One solution is to lengthen the query through relevance feedback .After conducting the first search using an initial query .Additional words are added to the query to narrow the search for the next search iteration .Another solution is to expand a query through synonym relations as found in thesaurus .", "label": "", "metadata": {}, "score": "46.608315"}
{"text": "Example applications include diagnosis of telecommunication faults , pipeline monitoring and control , and medical diagnosis .Bayesian networks are beginning to be widely used in expert systems as an alternative to neural networks .MML is used to cost the models and perform a trade - off between model complexity and accuracy of fit to the observational data .", "label": "", "metadata": {}, "score": "46.73221"}
{"text": "The parameters ' corpus_vocab_db ' , ' doc_vectors_db ' , and ' normalized_doc_vecs_db ' are for naming disk - based databases in which the VSM model will be stored .Subsequently , these databases can be used for much faster retrieval from the same corpus .", "label": "", "metadata": {}, "score": "46.74433"}
{"text": "Generating the visualization .A 2-D signal may also be created by choosing narrative index to be one variable and date of publication to be a second variable .Wavelet analysis is readily applicable to such multi - D signals .", "label": "", "metadata": {}, "score": "46.747337"}
{"text": "The Average Precision values for the queries and the overall MAP can be printed out by calling .upload_document_relevancies_from_file ( ) : .When human - supplied relevancies are available , you can upload them into the program by calling .These relevance judgments will be read from a file that is named with the relevancy_file constructor parameter .", "label": "", "metadata": {}, "score": "46.770412"}
{"text": "The method of claim 32 , wherein the rank of the most relevant document is output .An apparatus for generating a thesaurus of word vectors for each word in a corpus of documents , the word vectors being based on the lexical co - occurrence of words within each of the documents , comprising : . a memory containing the corpus of documents ; . an extractor for retrieving a word from the corpus ; . a generator generating a word vector for the word based on every recorded number ; and .", "label": "", "metadata": {}, "score": "46.783226"}
{"text": "In the preferred embodiment a mathematical signal is created from the narrative order of the words in the text .For example , suppose that there are K total words including duplicates left in the document after removal of stop words .", "label": "", "metadata": {}, "score": "46.802887"}
{"text": "Further speedup in retrieval can be achieved by using LSA to create reduced - dimensionality representations for the documents and by basing retrievals on the stored versions of such reduced - dimensionality representations .The performance of a retrieval algorithm is typically measured by two properties : Precision at rank and Recall at rank .", "label": "", "metadata": {}, "score": "46.816757"}
{"text": "Further speedup in retrieval can be achieved by using LSA to create reduced - dimensionality representations for the documents and by basing retrievals on the stored versions of such reduced - dimensionality representations .The performance of a retrieval algorithm is typically measured by two properties : Precision at rank and Recall at rank .", "label": "", "metadata": {}, "score": "46.816757"}
{"text": "Efforts to speed this process has led to research in the area of Information Retrieval ( IR ) , which has set a precedent for certain approaches as has research in applied Mathematics and Statistics .An example of this work is in automatic text theme identification with the end being to provide automated textual summaries of documents .", "label": "", "metadata": {}, "score": "46.84788"}
{"text": "Stemming the words so extracted and eliminating the designated stop words from the vocabulary .Stemming means that closely related words like ' programming ' and ' programs ' are reduced to the common root word ' program ' and the stop words are the non - discriminating words that can be expected to exist in virtually all the documents .", "label": "", "metadata": {}, "score": "46.92833"}
{"text": "FIG .1 is a block diagram of an apparatus for determining lexical co - occurrence of terms within a document or query ; .FIG .2 is a flow diagram of the Buckshot clustering algorithm ; .FIG .3 shows the query formulation using WordSpace ; .", "label": "", "metadata": {}, "score": "46.932343"}
{"text": "The format of this file must be according to what is shown in the sample file ' relevancy.txt ' in the ' examples ' directory .We have already explained the roles played by the constructor parameters such as ' lsa_svd_threshold ' .", "label": "", "metadata": {}, "score": "46.94404"}
{"text": "You can display the idf value associated with each word in the corpus by .The idf of a word in the corpus is calculated typically as the logarithm of the ratio of the total number of documents in the corpus to the number of documents in which the word appears ( with protection built in to prevent division by zero ) .", "label": "", "metadata": {}, "score": "46.99109"}
{"text": "FIG .2 depicts a flow diagram for an exemplary method of comparing concepts for similarity within a corpus of documents and ranking the documents according to concept similarity .Initially , a corpus , or collection , of documents may be identified as relevant to a particular query 105 .", "label": "", "metadata": {}, "score": "47.059563"}
{"text": "The former uses regular document vectors for calculating the similarity between every pair of documents in the corpus .And the latter uses normalized document vectors for the same purpose .The document order used for row and column indexing of the matrix corresponds to the alphabetic ordering of the document names in the corpus directory .", "label": "", "metadata": {}, "score": "47.071747"}
{"text": "We have previously explained the role of the constructor parameter ' lsa_svd_threshold ' .The format of these two files must be according to what is shown in the sample files ' test_queries . txt ' and ' relevancy.txt ' in the ' examples ' directory .", "label": "", "metadata": {}, "score": "47.08442"}
{"text": "We have previously explained the role of the constructor parameter ' lsa_svd_threshold ' .The format of these two files must be according to what is shown in the sample files ' test_queries . txt ' and ' relevancy.txt ' in the ' examples ' directory .", "label": "", "metadata": {}, "score": "47.08442"}
{"text": "A call to new ( ) constructs a new instance of the Algorithm::VSM class : .The values shown on the right side of the big arrows are the default values for the parameters .The following nested list will now describe each of the constructor parameters : .", "label": "", "metadata": {}, "score": "47.12945"}
{"text": "We have already explained the roles played by the constructor parameters such as ' lsa_svd_threshold ' .Version 1.4 makes it easier for a user to calculate a similarity matrix over all the documents in the corpus .The elements of such a matrix express pairwise similarities between the documents .", "label": "", "metadata": {}, "score": "47.174007"}
{"text": "or .calculate_similarity_matrix_for_all_normalized_docs.pl .The former uses regular document vectors for calculating the similarity between every pair of documents in the corpus .And the latter uses normalized document vectors for the same purpose .The document order used for row and column indexing of the matrix corresponds to the alphabetic ordering of the document names in the corpus directory .", "label": "", "metadata": {}, "score": "47.281967"}
{"text": "The method of .the method of .claim 1 , wherein the concept threshold is less than or equal to 1 , and terms that have correlation higher than the concept threshold form the vertices of the concept graph .A computerized method for determining product problems from field service logs , comprising : . collecting and filtering terms from a plurality of service logs , wherein the filtering is performed with reference to a set of stop words ; . identifying a term - frequency vector for each of the logs ; . identifying a term - frequency matrix , wherein rows of the matrix comprise values for the term - frequency vectors ; . projecting the term - frequency matrix onto a lower dimensional space using latent semantic analysis , to create a transformed term matrix ; . developing a correlation matrix using the rows of the transformed term matrix ; . creating a concept graph of connected components using a concept threshold by identifying , from the transformed term matrix , groups of terms that exceed the correlation threshold to form a concept term set ; and .", "label": "", "metadata": {}, "score": "47.33691"}
{"text": "We have already explained the roles played by the constructor parameters such as ' lsa_svd_threshold ' .CHANGES .Version 1.4 makes it easier for a user to calculate a similarity matrix over all the documents in the corpus .The elements of such a matrix express pairwise similarities between the documents .", "label": "", "metadata": {}, "score": "47.356094"}
{"text": "In the preferred embodiment , after the application of the stop word filter , the stemming filter and the document frequency filter , a topicality filter is applied .For each pseudo corpus window , a word frequency count expectation is calculated .", "label": "", "metadata": {}, "score": "47.458538"}
{"text": "Correlations are converted to distances by subtracting them from 1 so that a correlation of 1 corresponds to a distance of 0 as seen in the distance matrix below .The distances can be used to plot a tree or graphic representation of a branching diagram representing a hierarchy of categories based on degress of similarity or shared characteristics ( i.e. , a dendrogram ) using a method known in the art as hierarchical agglomerative clustering .", "label": "", "metadata": {}, "score": "47.534187"}
{"text": "The database named by doc_vectors_db stores the document vector representation for each document in the corpus .Each document vector has the same size as the corpus - wide vocabulary ; each element of such a vector is the number of occurrences of the word that corresponds to that position in the vocabulary vector .", "label": "", "metadata": {}, "score": "47.5394"}
{"text": "The database named by doc_vectors_db stores the document vector representation for each document in the corpus .Each document vector has the same size as the corpus - wide vocabulary ; each element of such a vector is the number of occurrences of the word that corresponds to that position in the vocabulary vector .", "label": "", "metadata": {}, "score": "47.5394"}
{"text": "This calculation computes how well the occurrence of a term correlates to both the query and the document .For additional background relating to space modeling , see e.g. , Understanding Search Engines Mathematical Modeling and Text Retrieval , Michael W. Berry , Murray Browne , Society for Industrial and Applied Mathematics , ( 1999 ) .", "label": "", "metadata": {}, "score": "47.55026"}
{"text": "In an embodiment , each concept corresponds to a term set .The number of times the terms in the term set occur in each document may be tallied .In a retrieval system , the term set may be entered into a retrieval system that returns the documents containing some or all of the terms in the term set and displayed in decreasing order .", "label": "", "metadata": {}, "score": "47.55754"}
{"text": "In an embodiment , each concept corresponds to a term set .The number of times the terms in the term set occur in each document may be tallied .In a retrieval system , the term set may be entered into a retrieval system that returns the documents containing some or all of the terms in the term set and displayed in decreasing order .", "label": "", "metadata": {}, "score": "47.55754"}
{"text": "Table 2 shows that the word \" tank \" can be disambiguated both ways in WordSpace .A query consisting of the terms \" tank \" and \" water \" retrieves only words relevant to the \" receptacle \" sense of the word \" tank .", "label": "", "metadata": {}, "score": "47.605743"}
{"text": "[ 0031 ] .For each document in the corpus , a term - frequency vector is computed that contains the frequency of filtered terms in the document 115 .For example , .FIG .1 shows the terms \" crash , \" and \" spooler , \" in a vector space with two dimensions .", "label": "", "metadata": {}, "score": "47.610077"}
{"text": "Queries in the preferred embodiment .It is in the implementation of queries that the importance of the association matrix becomes clear .If zero - order statistics such as word frequency are used , as in the approach of Hearst , there will be no recognition of the synonymous use of different words .", "label": "", "metadata": {}, "score": "47.632378"}
{"text": "If human - supplied relevancy judgments are available and you wish to experiment with precision and recall calculations for LSA - based retrieval , run the script : . calculate_precision_and_recall_from_file_based_relevancies_for_LSA.pl .This script will print out the average precisions for the different test queries and calculate the MAP metric of retrieval accuracy .", "label": "", "metadata": {}, "score": "47.636574"}
{"text": "The matrix C has v.sup.2 /2 distinct entries , where v is the size of the vocabulary .Although this matrix is sparse , v is expected to be very large .Therefore , the overall storage requirement needed to form the co - occurrence thesaurus is unworkable .", "label": "", "metadata": {}, "score": "47.670303"}
{"text": "The functions display_corpus_vocab ( ) and display_doc_vectors ( ) are there only for testing purposes with small corpora .If you must use them for large libraries / corpora , you might wish to redirect the output to a file .The ' debug ' option , when turned on , will output a large number of intermediate results in the calculation of the model .", "label": "", "metadata": {}, "score": "47.692326"}
{"text": "If after you have extracted the corpus vocabulary and constructed document vectors , you would do your retrieval with LSA modeling , you need to make the following call : .The SVD decomposition that is carried out in LSA model construction uses the constructor parameter lsa_svd_threshold to decide how many of the singular values to retain for the LSA model .", "label": "", "metadata": {}, "score": "47.768394"}
{"text": "Relevancy judgments are commonly supplied by the humans who are familiar with the corpus .But if such human - supplied relevance judgments are not available , you can invoke the following method to estimate them : .For the above method call , a document is considered to be relevant to a query if it contains several of the query words .", "label": "", "metadata": {}, "score": "47.78692"}
{"text": "The method of . claim 17 , wherein the filtering is performed with reference to a set of stop words .The method of . claim 17 , wherein the method uses a processor and memory , and the plurality of documents is stored in the memory .", "label": "", "metadata": {}, "score": "47.79887"}
{"text": "DESCRIPTION OF THE PREFERRED EMBODIMENT(S ) .The preferred embodiment of the present invention utilizes the following steps : .( 1 ) Creation of a pseudo - corpus of words from an individual document using an overlapping window partition , .", "label": "", "metadata": {}, "score": "47.81965"}
{"text": "11 shows the normalized context vector for the query or document .Each context vector has 20 real - valued dimensions .FIG .12 shows the process of using context vectors to retrieve relevant documents for a query .In step 230 , a query is entered into the processor by the user .", "label": "", "metadata": {}, "score": "47.86214"}
{"text": "The columns and rows of the correlation matrix may correspond to terms that are the terms of the transformed term matrix , and a concept may include highly correlated terms .The correlation matrix may include a correlation higher than the concept threshold become vertices of a concept graph .", "label": "", "metadata": {}, "score": "47.899284"}
{"text": "The columns and rows of the correlation matrix may correspond to terms that are the terms of the transformed term matrix , and a concept may include highly correlated terms .The correlation matrix may include a correlation higher than the concept threshold become vertices of a concept graph .", "label": "", "metadata": {}, "score": "47.899284"}
{"text": "Because the length of an order-1 representation is equal to the number of unique terms in the vocabulary , the order-1 representations are dense and require a lot of storage , which is a severe disadvantage .The order-1 vector of a long document will hardly have any zeros since almost every word is a neighbor of one of the document 's terms somewhere in the corpus .", "label": "", "metadata": {}, "score": "47.92597"}
{"text": "Thus , LSA allows for information retrieval on the basis of concepts contained in the document with respect to other documents in the corpus rather than terms in the documents of the corpus .[ 0012 ] .Latent Semantic Indexing ( LSI ) is the application of LSA principles in a statistical information retrieval methodology .", "label": "", "metadata": {}, "score": "47.949047"}
{"text": "A .XP . puffs . spooler .crashes .memory . toner .d .d .d .d .[ 0033 ] .Matrix A may be projected onto a lower dimensional space using Latent Semantic Analysis 125 .", "label": "", "metadata": {}, "score": "47.96422"}
{"text": "By default this parameter is set to 1 .If you want to turn off the normalization of the document vectors , including turning off the weighting of the term frequencies of the words by their idf values , you must set this parameter explicitly to 0 . stop_words_file .", "label": "", "metadata": {}, "score": "48.004364"}
{"text": "An apparatus for retrieving relevant documents from a corpus of documents based on a query , comprising : . a memory containing the corpus of documents ; . a thesaurus of word vectors for each word of the corpus , the word vectors being based on lexical co - occurrence of words within each of the documents ; . determining means for determining a co - occurrence correlation relationship between the document vectors and the query vector ; and .", "label": "", "metadata": {}, "score": "48.03858"}
{"text": "The method may also include identifying one or more logs that relate to a product problem based on the ranking of the log or logs , and/or identifying one or more problems based on the frequency of occurrence of terms in the concept term set .", "label": "", "metadata": {}, "score": "48.068935"}
{"text": "Storing the VSM and LSA models in database files on the disk is now optional .The second feature , in particular , should prove useful to those who are using this module for large collections of documents .Version 1.42 includes two new methods , display_corpus_vocab_size ( ) and write_corpus_vocab_to_file ( ) , for those folks who deal with very large datasets .", "label": "", "metadata": {}, "score": "48.145836"}
{"text": "The equation to normalize the context vector is : # # EQU14 # # where p is the number of dimensions in the reduced space ; and d.sub.j is the context vector .By normalizing the context vectors , all of the context vectors will have the same length regardless of the size of the document .", "label": "", "metadata": {}, "score": "48.15745"}
{"text": "Version 1.2 includes a code correction and some general code and documentation cleanup .With Version 1.1 , you can access the retrieval precision results so that you can compare two different retrieval algorithms ( VSM or LSA with different choices for some of the constructor parameters ) with significance testing .", "label": "", "metadata": {}, "score": "48.18494"}
{"text": "Version 1.2 includes a code correction and some general code and documentation cleanup .With Version 1.1 , you can access the retrieval precision results so that you can compare two different retrieval algorithms ( VSM or LSA with different choices for some of the constructor parameters ) with significance testing .", "label": "", "metadata": {}, "score": "48.18494"}
{"text": "The construction of the thesaurus will be described with reference to FIGS .5 - 9 .The goal is to apply a singular value decomposition to reduce the dimensionality of the matrix in a disciplined fashion and in the process produce more compact representations .", "label": "", "metadata": {}, "score": "48.187492"}
{"text": "The humans bring to bear semantic considerations on the relevancy determination problem that are beyond the scope of this module .The module provides the following methods for constructing VSM and LSA models of a corpus , for using the models thus constructed for retrieval , and for carrying out precision versus recall calculations for the determination of retrieval accuracy on the corpora of interest to you .", "label": "", "metadata": {}, "score": "48.189125"}
{"text": "SUMMARY .The method may also include projecting the term - frequency matrix onto a lower dimensional space using latent semantic analysis to create a transformed term matrix .A correlation matrix may be created using the rows of the transformed term matrix .", "label": "", "metadata": {}, "score": "48.294724"}
{"text": "Although these vectors are somewhat sparse , this implies that word comparisons are an order v operation , which is prohibitively expensive for large scale application .Thus , the dimensionality of the problem must be reduced to a workable size by using a singular value decomposition of a matrix of co - occurrence counts .", "label": "", "metadata": {}, "score": "48.405247"}
{"text": "This problem is not particular to higher - order representations .A computationally simple approach to segment long documents is to cluster the set of tokens in a document into a set of coherent subtopic clusters .A token is a single item in full text .", "label": "", "metadata": {}, "score": "48.42782"}
{"text": "The distances can be used to plot a tree or graphic representation of a branching diagram representing a hierarchy of categories based on degress of similarity or shared characteristics ( i.e. , a dendrogram ) using a method known in the art as hierarchical agglomerative clustering .", "label": "", "metadata": {}, "score": "48.481827"}
{"text": "Since document d 2 has the smallest angle with q , it is the top - ranked document in response to the query crash spooler .Both terms ( crash and spooler ) are salient in d 2 and therefore have high weights , and although the other two documents also mention both terms , one of the terms is not a centrally important term in the overall document .", "label": "", "metadata": {}, "score": "48.509987"}
{"text": "Since document d 2 has the smallest angle with q , it is the top - ranked document in response to the query crash spooler .Both terms ( crash and spooler ) are salient in d 2 and therefore have high weights , and although the other two documents also mention both terms , one of the terms is not a centrally important term in the overall document .", "label": "", "metadata": {}, "score": "48.509987"}
{"text": "A clustering algorithm is used in step 304 to cluster the retrieved thesaurus vectors based on different topics or factors .The clustered vectors are stored in the RAM in step 306 according to the relevant factor .In FIG .15 , the memory locations are divided into factors .", "label": "", "metadata": {}, "score": "48.54885"}
{"text": "M.Kibriya , EibeFrank , Bernard .P , Geoffrey Holm'es,'Multinomial Naive Bayes for Text',lecture notes in Artificial Intelligence , pp 488 - 499,2004 .[ 9 ] Erdong Chen , Benjamin Snyder and Regina Barzilay,'Incremental Text Structuring with Online Hierarchical Ranking',Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational linguistics , pp .", "label": "", "metadata": {}, "score": "48.550613"}
{"text": "The humans bring to bear semantic considerations on the relevancy determination problem that are beyond the scope of this module .METHODS .The module provides the following methods for constructing VSM and LSA models of a corpus , for using the models thus constructed for retrieval , and for carrying out precision versus recall calculations for the determination of retrieval accuracy on the corpora of interest to you . new ( ) : .", "label": "", "metadata": {}, "score": "48.627518"}
{"text": "An improved retrieval performance results by inducing representations for documents that reflect term dependencies and remedy the bumpiness of small counts .However , undetected term dependencies and small counts are a problem if document occurrence is the basis of representation .", "label": "", "metadata": {}, "score": "48.651123"}
{"text": "You can exercise control over the process of determining relevancy of a document to a query by giving a suitable value to the constructor parameter relevancy_threshold .A document is considered relevant to a query only when the document contains at least relevancy_threshold number of query words .", "label": "", "metadata": {}, "score": "48.670322"}
{"text": "The document vectors are shown automatically when debug option is turned on .If you would like to compare in your own script any two documents in the corpus , you can call .or .Both these calls return a number that is the dot product of the two document vectors normalized by the product of their magnitudes .", "label": "", "metadata": {}, "score": "48.671585"}
{"text": "1 is a block diagram of a preferred embodiment according to the invention of an apparatus 10 for determining lexical co - occurrence of terms within a document or query and forming a thesaurus .The apparatus 10 includes a user input device 12 which includes , for example , one or more of an image scanner ( optical or electronic ) , a keyboard , a touchscreen , a mouse , etc .", "label": "", "metadata": {}, "score": "48.695827"}
{"text": "However , LSI is based on document occurrence .Decomposing a term - by - document matrix of a large collection can take days or even weeks because the time complexity is quadratic in the number of documents to process .Documents can be represented as vectors whose entries correspond to microfeatures such as finance , animal kingdom , etc .", "label": "", "metadata": {}, "score": "48.737362"}
{"text": "The disk file for storing the relevancy judgments .max_number_retrievals .The constructor parameter max_number_retrievals stands for what it means .debug .Finally , when you set the boolean parameter debug , the module outputs a very large amount of intermediate results that are generated during model construction and during matching a query with the document vectors .", "label": "", "metadata": {}, "score": "48.806335"}
{"text": "The method compares concepts consisting of groups of terms for similarity within a corpus of document , clusters documents that contain certain concept term sets together .It may also rank the documents within each cluster according to the frequency of term co - occurrence within the concepts . identifying a term - frequency matrix , wherein rows of the matrix comprise values for the term - frequency vectors ; . projecting the term - frequency matrix onto a lower dimensional space using latent semantic analysis , to create a transformed term matrix ; . developing a correlation matrix using the rows of the transformed term matrix ; . creating a concept graph of connected components using a concept threshold , where each connected component is a set of terms that corresponds to a concept ; and .", "label": "", "metadata": {}, "score": "48.845055"}
{"text": "4 shows the query formulation using DocumentSpace ; .FIG .5 is a flow diagram for computing the word vectors for the thesaurus ; .FIG .6 shows the Matrix A computed in the flow diagram of FIG .5 ; .", "label": "", "metadata": {}, "score": "48.91623"}
{"text": "The Boolean method of term searching is a widely used information retrieval process , and it is often used in Internet search engines because it is fast , uncomplicated and easy to implement in a remote online environment .However , the Boolean search method carries with it many of the shortcomings of term searching .", "label": "", "metadata": {}, "score": "48.935604"}
{"text": "In the alternative , the ranking can be stored in a RAM or permanent storage device .Referring to FIG .13 , each memory location stores the rank , the document identification and the correlation coefficient .The process described in FIG .", "label": "", "metadata": {}, "score": "49.00384"}
{"text": "Boolean searching variants that allow for term frequency based document ranking have a number of drawbacks , one of which is that longer documents have a higher probability of being ranked higher in the solution set without a corresponding increase in relevance .", "label": "", "metadata": {}, "score": "49.023315"}
{"text": "Boolean searching variants that allow for term frequency based document ranking have a number of drawbacks , one of which is that longer documents have a higher probability of being ranked higher in the solution set without a corresponding increase in relevance .", "label": "", "metadata": {}, "score": "49.023315"}
{"text": "The Average Precision for a query is the average of the Precision - at - rank values associated with each of the corpus documents relevant to the query .The mean of the Average Precision values for all the queries is the Mean Average Precision ( MAP ) .", "label": "", "metadata": {}, "score": "49.029167"}
{"text": "In a preferred embodiment , a principal component analysis ( PCA ) is then performed on the ( N ) rows in the Association Matrix .In proof of principal experiments designed to demonstrate the efficacy of the present invention , the mean was not subtracted out ; however this might be advantageous , especially since wavelet analysis is insensitive to the mean .", "label": "", "metadata": {}, "score": "49.114872"}
{"text": "Information retrieval methods have been devised that combine Boolean logic with other techniques such as content - based navigation , where shared terms from previously obtained documents are used to refine and expand the query .While each of the above described improvements provide some benefit to the user , the solution set of documents does not optimally convey the right information to a user if the user does not have an understanding of the subject matter being searched .", "label": "", "metadata": {}, "score": "49.120377"}
{"text": "The terms which have survived the previous filters then go on to be classified as topics or cross - terms .In the preferred embodiment there is a different document frequency filter for topics and cross - terms .Those terms with the largest topicality measure are called topics .", "label": "", "metadata": {}, "score": "49.222034"}
{"text": "The TF - IDF gives how important is a word to a document in a collection , since it takes in consideration not only the isolated term but also the term within the document collection .The intuition is that a term that occurs frequently in many documents is not a good discriminator ( why emphasize a term which is almost present in the entire corpus of your documents ? )", "label": "", "metadata": {}, "score": "49.292084"}
{"text": "The topicality filter is in part a denoising algorithm as is the application of stop word list , stemming algorithms , and document frequency filters .However , denoising may be accomplished via the wavelet transform itself , so the application of these filters may not be necessary .", "label": "", "metadata": {}, "score": "49.30863"}
{"text": "Note that the final reduction in dimensionality was performed because smoothing and improved generality results from a singular value decomposition reduction .Similarity between b - component vectors can contain a large error measure of semantic similarity since there may be several word classes with similar topics .", "label": "", "metadata": {}, "score": "49.331608"}
{"text": "The area under the precision vs. recall curve for a given query is called Average Precision for that query .When this area is averaged over all the queries , you get MAP ( Mean Average Precision ) as a measure of the accuracy of the retrieval algorithm .", "label": "", "metadata": {}, "score": "49.333717"}
{"text": "The result items are clustered into logical categories , and the result items within each category are ranked based on the frequency of the occurrence of relevant words in each of the result items .The frequency of the occurrence of words for a given result item may correspond to the number of unique terms that are contained in the given result item .", "label": "", "metadata": {}, "score": "49.34598"}
{"text": "Thus significant advantage in computational speed is gained by compressing via thresholding ( replacing small entries with zeros ) .Multi - dimensional Scaling ( MDS ) is a standard statistical method used on multi - variate data .In MDS , N objects are represented as d - dimensional vectors with all pairwise similarities or dissimilarities ( distances ) defined between the N objects .", "label": "", "metadata": {}, "score": "49.366608"}
{"text": "A vector for each query sub - topic is formed and compared to the document vectors .The document vectors are then scored and ranked by the degree to which they simultaneously match the subtopics of the query .BRIEF DESCRIPTION OF THE DRAWINGS .", "label": "", "metadata": {}, "score": "49.404617"}
{"text": "Machine - readable dictionaries can be used to derive \" context vectors . \" See Wilks et al . , \" Providing Machine Tractable Dictionary Tools \" ; Machine Translation ; Vol .5 , No . 2 , pp .99 - 154 ; 1990 .", "label": "", "metadata": {}, "score": "49.406258"}
{"text": "A third preferred embodiment uses the thesaurus vectors to analyze the query into topic - coherent word groups , which are called word factors .The goal is to ensure that documents are relevant to the entire query such that their score with respect to each factor is high .", "label": "", "metadata": {}, "score": "49.41493"}
{"text": "The reduced matrix C is shown in FIG .9 .To reduce compute time in the example , only a subset of the matrix , corresponding to the 1000th through 6000th most frequent word , was decomposed .This decomposition defined a mapping from the 200 dimensional B - class space to a 20 dimensional reduced space .", "label": "", "metadata": {}, "score": "49.46414"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .Aspects , features , benefits and advantages of the embodiments of the present invention will be apparent with regard to the following description , appended claims and accompanying drawings where : .FIG .1 is a graphic illustration of an example of vector analysis .", "label": "", "metadata": {}, "score": "49.47815"}
{"text": "Further speedup in retrieval can be achieved by using LSA to create reduced - dimensionality representations for the documents and by basing retrievals on the stored versions of such reduced - dimensionality representations .ESTIMATING RETRIEVAL PERFORMANCE WITH PRECISION VS .RECALL CALCULATIONS .", "label": "", "metadata": {}, "score": "49.506767"}
{"text": "One problem with current methods of information retrieval as described above is that , even with traditional LSA or LSI methods , the searches often retrieve semantically uninteresting words .A need exists for a method to compare documents not simply based on word occurrences , but rather on a higher level of concept occurrence and co - occurrence , where a concept is defined by two or more correlated terms .", "label": "", "metadata": {}, "score": "49.55213"}
{"text": "Further , even if all of the resultant documents use the search term in the proper context , the user has no way of judging which documents in the solution set are more relevant than others .Thus , the user often must perform a secondary search for a more relevant solution set of documents .", "label": "", "metadata": {}, "score": "49.558212"}
{"text": "Further , even if all of the resultant documents use the search term in the proper context , the user has no way of judging which documents in the solution set are more relevant than others .Thus , the user often must perform a secondary search for a more relevant solution set of documents .", "label": "", "metadata": {}, "score": "49.558212"}
{"text": "More precisely , the space of word vectors can be viewed as the surface of a partial hypersphere in a multidimensional space that is centered around the global centroid .Vectors in the area of the global centroid are equally close to everything .", "label": "", "metadata": {}, "score": "49.560173"}
{"text": "The value of each component is a function of the frequency the term has in that document .They show that query expansion using the cosine similarity measure on these vectors improves retrieval performance .However , the time complexity for computing the similarity between terms is related to the size of the corpus because the term vectors are high - dimensional .", "label": "", "metadata": {}, "score": "49.598366"}
{"text": "See : J. Oliver .Decision graphs - an extension of decision trees .4thInt .Conf on Artificial Intelligence , pp.343 - 359 , 1993 .NB .A decision tree ( graph ) is more correctly known as a classification tree ( graph ) and the problem addressed is then called supervised classification .", "label": "", "metadata": {}, "score": "49.644157"}
{"text": "After clustering the document , each document can then be described by the centroids of its subtopic clusters .The derivation of the structured document representations of order-2 are : .Order-2 clustering function : .Order-2 encoding function : # # EQU11 # # where .", "label": "", "metadata": {}, "score": "49.655853"}
{"text": "The method may also include clustering documents that contain concept term sets together , and for each cluster and corresponding term sets , ranking logs in each cluster by frequency of occurrence of terms in the concept term set .The method may also include identifying one or more logs that relate to a product problem based on the ranking of the log or logs , and/or identifying one or more problems based on the frequency of occurrence of terms in the concept term set .", "label": "", "metadata": {}, "score": "49.657413"}
{"text": "You can get hold of this data by calling .The script significance_testing.pl in the ' examples ' directory shows how you can use this method for significance testing .The first two of these are needed for creating disk - based database records for the VSM and LSA models .", "label": "", "metadata": {}, "score": "49.663963"}
{"text": "The sum of all words in a document is a good topic descriptor for short documents .However , long documents tend to contain words from different topics .If too many topics enter in the computation of a document vector , then the document vector will be in a region that is at an intermediate distance to all its topics , but not particularly close to any of them .", "label": "", "metadata": {}, "score": "49.687958"}
{"text": "A typical formula that is used to calculate the IDF weight for a word is the logarithm of the ratio of the total number of documents to the number of documents in which the word appears .So if a word were to appear in all the documents , its IDF multiplier would be zero in the vector representation of a document .", "label": "", "metadata": {}, "score": "49.6923"}
{"text": "A typical formula that is used to calculate the IDF weight for a word is the logarithm of the ratio of the total number of documents to the number of documents in which the word appears .So if a word were to appear in all the documents , its IDF multiplier would be zero in the vector representation of a document .", "label": "", "metadata": {}, "score": "49.6923"}
{"text": "4 .In the preferred embodiment , the 3-D representation is created by first selecting several energy level and multi - resolution level pairs for various locations on the \" Wave .\" This in turn will define a collection of thematic chunks at each multi - resolution level as described above .", "label": "", "metadata": {}, "score": "49.695786"}
{"text": "Subsequently you call .Before you can carry out precision and recall calculations to test the accuracy of VSM and LSA based retrievals from a corpus , you need to have available the relevancy judgments for the queries .( A relevancy judgment for a query is simply the list of documents relevant to that query . )", "label": "", "metadata": {}, "score": "49.759254"}
{"text": "Thus , the equation for the correlation coefficient is : # # EQU15 # # where d.sub.i is the query vector and d.sub.j is the document vector .After calculating all of the correlation coefficients , the documents are ranked in step 252 from most relevant to least relevant .", "label": "", "metadata": {}, "score": "49.87877"}
{"text": "Aust .Comp .Sci .Theory Symp . '98 , pp.215 - 230 , Springer Verlag , 1998 .Decision Trees and Graphs .expert system , noise , rule extraction , simple v. complex , supervised learning , uncertainty .", "label": "", "metadata": {}, "score": "49.91127"}
{"text": "The closer one gets to the root of the tree , the more terms are included and the broader the concepts that are represented .[ 0040 ] .Each concept is represented by a set of terms that are close together in a concept space .", "label": "", "metadata": {}, "score": "49.924652"}
{"text": "Thus , the dimensionality of the matrix fed into singular value decomposition can not be too high .In particular , the original matrix C can not be used .Instead , a two stage computation is performed that derives two sets of topical word classes from the corpus : 200 word clusters of low coverage ; and 200 word clusters of high coverage .", "label": "", "metadata": {}, "score": "49.94685"}
{"text": "118 - 155 .Crouch , C.J. , \" An Approach to the Automatic Construction of Global Thesauri \" , Information Processing & Management , vol .26 , No . 5 , pp .629 - 640 , 1990 .Deerwester et al . , \" Indexing by Latent Semantic Analysis \" , Journal of the American Society for Information Science 41(6 ) , pp .", "label": "", "metadata": {}, "score": "49.997684"}
{"text": "In the preferred embodiment , only the first several principal components of the Association Matrix were utilized , and the columns were selected from this compressed matrix representation .Each channel is then identified with a PCA component rather than a specific term .", "label": "", "metadata": {}, "score": "50.056847"}
{"text": "Context vectors are then computed from these dictionary - based word representations .This method has the same problems as other dictionary - based approaches .In particular , a genre - specific distinction that is not covered in the dictionary is not adequately represented in the dictionary - based representations .", "label": "", "metadata": {}, "score": "50.166237"}
{"text": "Dimensions of Meaning \" , Hinrich Schuetze , Proceedings Supercomputing ' 92 , Nov. 16 - 20 , 1992 , pp .787 - 796 .Douglas R. Cutting et al . , Scatter / Gather : A Cluster - based Approach to Browsing Large Document Collections , pp . 1 - 12 , 15th Ann Int'l SIGIR ' 92 ( 1992 ) .", "label": "", "metadata": {}, "score": "50.19259"}
{"text": "The method of .claim 1 , wherein the method uses a processor and memory , and the plurality of documents is stored in the memory .The method of .claim 1 , wherein the set of term - frequency vectors for the documents is maintained as rows and columns in a term - frequency matrix .", "label": "", "metadata": {}, "score": "50.20092"}
{"text": "Then the derivation of compact representations for the whole vocabulary proceeds in two steps .In the first step , an order-1 representation . phi . ' sub.1 is derived using a restricted vocabulary of N words , and a singular value decomposition computes a low - dimensional encoding .", "label": "", "metadata": {}, "score": "50.20829"}
{"text": "The first preferred embodiment described above is a rich representation of words and documents that is based on global information about the document collection .The first preferred embodiment is superior to the literal representation used in classical vector similarity search .", "label": "", "metadata": {}, "score": "50.244392"}
{"text": "Thus , LSA allows for information retrieval on the basis of concepts contained in the document with respect to other documents in the corpus rather than terms in the documents of the corpus .Latent Semantic Indexing ( LSI ) is the application of LSA principles in a statistical information retrieval methodology .", "label": "", "metadata": {}, "score": "50.32199"}
{"text": "Formally , # # EQU12 # # where d.sub.j is the vector for document j ; w.sub.ij is the weight for word i in document j ; and v.sub.i is the thesaurus vector for word i. Queries may be represented as vectors by using equation 15 .", "label": "", "metadata": {}, "score": "50.368496"}
{"text": "A logical improvement to single term searching is multiple , simultaneous term searching using \" Boolean term operators , \" or simply , Boolean operators .A Boolean retrieval query passes through a corpus of documents by linking search terms together with Boolean operators such as AND , OR and NOT .", "label": "", "metadata": {}, "score": "50.404076"}
{"text": "LSA represents the terms taken from an original corpus of information or any subset of terms contained in the original corpus , as individual points in a high dimensional semantic space .A semantic space is a mathematical representation of a large body of terms , and every term , combination of terms or sub - combinations of a term can be represented as a unique high - dimensional vector at a point in semantic space .", "label": "", "metadata": {}, "score": "50.455963"}
{"text": "FIG .1 is a graphic illustration of an example of vector analysis .[ 0023 ] .FIG .2 depicts a flow diagram for an exemplary method of analyzing documents according to an embodiment .[ 0024 ] .FIG .", "label": "", "metadata": {}, "score": "50.52167"}
{"text": "Second , a similarity measure is induced on words by comparing these vectors .Given a particular word its synonyms are then defined to be its nearest neighbors with respect to the similarity measure .This method of exploiting a lexical co - occurrence structure of words , i.e. , forming a word 's vector representation from entries of its near lexical neighbors rather than from only itself is superior to conventional methods .", "label": "", "metadata": {}, "score": "50.528675"}
{"text": "This you do by : .The only time you do NOT need to call this method is when you are using a previously constructed disk - stored VSM model for retrieval .display_corpus_vocab ( ) : .If you would like to see corpus vocabulary as constructed by the previous call , make the call .", "label": "", "metadata": {}, "score": "50.543205"}
{"text": "The context vector is a combination of the weighted sums of the thesaurus vectors of all the words contained in the document .These context vectors then induce a similarity measure on documents and queries that can be directly compared to standard vector - space methods .", "label": "", "metadata": {}, "score": "50.594086"}
{"text": "3 .An order-0 retrieval system will only do well on documents that contain both the ambiguous and the disambiguating term , but it will give the same ranking to documents that contain only one of them ( e.g. only water or only tank ) .", "label": "", "metadata": {}, "score": "50.72467"}
{"text": "[ 0034 ] .In an embodiment , the entries of the diagonal matrix S may include the eigenvalues of matrix A in decreasing order .The rows of T may be vectors corresponding to term frequencies in a transformed space .", "label": "", "metadata": {}, "score": "50.74963"}
{"text": "However , the Boolean search method carries with it many of the shortcomings of term searching .The user has to have some knowledge of the search topic for the search to be efficient in order to avoid relevant documents being ranked as non - relevant and visa versa .", "label": "", "metadata": {}, "score": "50.809025"}
{"text": "For example , a compression algorithm such as hard thresholding would be an example of a straightforward approach .Alternatively , a particular type of soft thresholding may be best suited for certain signals .As an alternative to the preferred embodiment , the fast wavelet transform algorithm is important to improve the efficiency of the procedure , especially for large documents where long filters need to be applied .", "label": "", "metadata": {}, "score": "50.855103"}
{"text": "Clustering may include , for example , tallying and/or ranking documents according to the number of terms in a concept they contain .Documents may be rank ordered within each cluster according to the number of occurrences of terms in a concept set 145 and presented to a user in a format corresponding to the rank order as seen the previous example .", "label": "", "metadata": {}, "score": "50.924553"}
{"text": "Such a characterization makes the user less dependent on well chosen document titles .It is easier to process than a long title , particularly if only the coarse topic of a document is of interest .In general , the nearest neighbors provide additional information on the content of a document that can be used for document selection or relevance feedback .", "label": "", "metadata": {}, "score": "50.93411"}
{"text": "( d.sub.j ) is the context vector for document d.sub.j .In step 342 , the documents are ranked based on the correlation coefficient assigned and the appropriate factor .The ranking of the documents within a factor is based on correlation : .", "label": "", "metadata": {}, "score": "50.935295"}
{"text": "The constructor parameter relevancy_threshold is used for automatic determination of document relevancies to queries on the basis of the number of occurrences of query words in a document .You can exercise control over the process of determining relevancy of a document to a query by giving a suitable value to the constructor parameter relevancy_threshold .", "label": "", "metadata": {}, "score": "50.943985"}
{"text": "The values shown on the right side of the big arrows are the default values for the parameters .The value supplied through the variable $ my_file_types would be something like [ ' .java ' , ' .txt ' ] if , say , you wanted only Java and text files to be included in creating the database model .", "label": "", "metadata": {}, "score": "50.944862"}
{"text": "Document titles are often uninformative or do not represent crucial parts of the content of a document .It also takes a relatively long time to read and evaluate them with respect to the user 's information needs .In systems based on order-0 representations , the user can only assess the impact of search terms indirectly by analyzing the retrieval results for varying search terms .", "label": "", "metadata": {}, "score": "50.945576"}
{"text": "The thesaurus vector is added to the context vector for the document in step 206 .If there are more words to process from the document , then the flow returns to step 204 to retrieve the thesaurus vector for the next word .", "label": "", "metadata": {}, "score": "51.199722"}
{"text": "[ 0025 ] .FIGS .4 a - 4 d are diagrams showing a term matrix in graphic form .[ 0026 ] .FIG .5 is a block diagram of exemplary hardware that may be used to contain and/or implement the program instructions of a system embodiment .", "label": "", "metadata": {}, "score": "51.218956"}
{"text": "Description .TECHNICAL FIELD .[ 0001 ] .The disclosed embodiments generally relate to methods of analyzing groups of documents .More particularly , the disclosed embodiments relate to methods for searching and organizing a collection or corpus of documents into various groups or clusters according to concepts that occur within the documents .", "label": "", "metadata": {}, "score": "51.26114"}
{"text": "Terms are then grouped by their occurrence in these document clusters .Since a complete - link document clustering is performed , the procedure is very computationally intensive and does not scale to a large reference corpus .Further , the central assumption that terms are related if they often occur in the same documents seems problematic for corpora with long documents .", "label": "", "metadata": {}, "score": "51.280144"}
{"text": "Furthermore , many researchers use measures for defining closeness that will group words according to frequency .By using these measures , it is impossible for a frequent word to have an infrequent neighbor .SUMMARY OF THE INVENTION .An object of the invention is to form a new corpus based method for constructing a thesaurus based on lexical co - occurrence of terms in the corpus .", "label": "", "metadata": {}, "score": "51.32745"}
{"text": "Each row displays a word and its nine nearest neighbors .For example , \" repair \" is the nearest neighbor of \" accident \" .Word pairs used as terms are displayed without being separated by a semicolon .Words in upper case are hand selected synonyms as might be found in a manually constructed thesaurus .", "label": "", "metadata": {}, "score": "51.359528"}
{"text": "or .Both these calls return a number that is the dot product of the two document vectors normalized by the product of their magnitudes .The first call uses the regular document vectors and the second the normalized document vectors . retrieve_with_vsm", "label": "", "metadata": {}, "score": "51.378246"}
{"text": "query_file .The parameter query_file points to a file that contains the queries to be used for calculating retrieval performance with Precision and Recall numbers .The format of the query file must be as shown in the sample file test_queries.txt in the ' examples ' directory . relevancy_threshold .", "label": "", "metadata": {}, "score": "51.399025"}
{"text": "A semantic space is a mathematical representation of a large body of terms , and every term , combination of terms or sub - combinations of a term can be represented as a unique high - dimensional vector at a point in semantic space .", "label": "", "metadata": {}, "score": "51.44203"}
{"text": "The coordinates , or term weights , of each vector are derived from occurrence counts within the documents .For example , crash may have only a passing reference in d 1 , but there may be several occurrences of spooler .", "label": "", "metadata": {}, "score": "51.45478"}
{"text": "The coordinates , or term weights , of each vector are derived from occurrence counts within the documents .For example , crash may have only a passing reference in d 1 , but there may be several occurrences of spooler .", "label": "", "metadata": {}, "score": "51.45478"}
{"text": "Creating the Pseudo - corpus .To get the best terms possible for individual document analysis , a pseudo - corpus for an individual document is created prior to making a digital signal .The pseudo - corpus is created as the original document is partitioned into overlapping windows of a fixed word size and word overlap .", "label": "", "metadata": {}, "score": "51.503304"}
{"text": "As part of a service support system , in an embodiment each document may include a log written by product support staff describing a problem .There may be more than one problem described in a log .Each concept may correspond to a problem with a product or a piece of equipment , part or human action in a process .", "label": "", "metadata": {}, "score": "51.566704"}
{"text": "As part of a service support system , in an embodiment each document may include a log written by product support staff describing a problem .There may be more than one problem described in a log .Each concept may correspond to a problem with a product or a piece of equipment , part or human action in a process .", "label": "", "metadata": {}, "score": "51.566704"}
{"text": "FIG .10 is a flow diagram for computing context vectors for documents ; .FIG .11 shows a document context vector ; .FIG .12 is a flow diagram for ranking the documents based on the query context vector and the document context vectors ; .", "label": "", "metadata": {}, "score": "51.598873"}
{"text": "If you must call this method on a large corpus , you might wish to direct the output to a file .The corpus vocabulary is shown automatically when debug option is turned on .You can display the idf value associated with each word in the corpus by .", "label": "", "metadata": {}, "score": "51.690025"}
{"text": "The code shown below will tokenize each document in the corpus and compute the term frequencies .Now that each of the documents in the corpus has been tokenized , the next step is to compute the document frequency quantity , that is , for each term , how many documents that term appears in .", "label": "", "metadata": {}, "score": "51.696156"}
{"text": "The wavelet transform may be a fast wavelet transform , a redundant wavelet transform , a non - orthogonal wavelet transform , a local cosine transform , or a local sine transform .The output of the wavelet transform may then be utilized to generate a visual representation of the semantic structure of the document .", "label": "", "metadata": {}, "score": "51.703766"}
{"text": "As is commonly the case with VSM , this module uses the cosine similarity distance when comparing a document vector with the query vector .You can display the retrieved document names by calling this method using the syntax : . where $ retrievals is a reference to the hash returned by a call to one of the retrieve methods .", "label": "", "metadata": {}, "score": "51.724663"}
{"text": "The result of the maximum rank is stored in step 346 of FIG .16 .The memory locations of the final ranking is shown in FIG .18 .The highest ranking document is most relevant to the query .The memory lists the rank , the result of the maximum ranking of equation 23 , and the document identification number .", "label": "", "metadata": {}, "score": "51.758865"}
{"text": "It is a further object of the present invention to utilize the output of the wavelet transform to partition the document .The partition maybe according to the semantic content of the document at a single level , or at multiple levels to produce an outline of the document .", "label": "", "metadata": {}, "score": "51.81214"}
{"text": "A method for automatically determining a semantic structure of an electronically formatted natural language based document consisting essentially of words , the method comprising the steps of : . a ) providing a numerical representation as a digital signal of the words within the document wherein said numerical representation contains some information relating the semantic content of the word to the semantic content of the document .", "label": "", "metadata": {}, "score": "51.85991"}
{"text": "So you are very likely to get faster performance with retrieval based on LSA modeling , especially if you store the model once constructed in a database file on the disk and carry out retrievals using the disk - based model .", "label": "", "metadata": {}, "score": "51.86445"}
{"text": "So you are very likely to get faster performance with retrieval based on LSA modeling , especially if you store the model once constructed in a database file on the disk and carry out retrievals using the disk - based model .", "label": "", "metadata": {}, "score": "51.86445"}
{"text": "So you are very likely to get faster performance with retrieval based on LSA modeling , especially if you store the model once constructed in a database file on the disk and carry out retrievals using the disk - based model .", "label": "", "metadata": {}, "score": "51.86445"}
{"text": "And , along the same lines , Recall at a given rank r is the ratio of the number of retrieved documents that are relevant to the total number of relevant documents .The Average Precision associated with a query is the average of all the Precision - at - rank values for all the documents relevant to that query .", "label": "", "metadata": {}, "score": "51.923893"}
{"text": "Table 2 shows ambiguity resolution with word vectors of order-2 .The nearest neighbors suggest that higher - order vectors deal with ambiguity and synonymy to some extent , even without user interaction .The example of \" tank \" shows that the information present in higher - order vectors can be used to resolve ambiguity , which is one of the main problems for representations of order-0 .", "label": "", "metadata": {}, "score": "52.04863"}
{"text": "The closer one gets to the root of the tree , the more terms are included and the broader the concepts that are represented .Each concept is represented by a set of terms that are close together in a concept space .", "label": "", "metadata": {}, "score": "52.07767"}
{"text": "[ 0015 ] .The present disclosure describes attempts to solve one or more of the above - listed problems .SUMMARY .[ 0016 ] .The method may also include projecting the term - frequency matrix onto a lower dimensional space using latent semantic analysis to create a transformed term matrix .", "label": "", "metadata": {}, "score": "52.104675"}
{"text": "Table 4 displays the nearest neighbors of the articles displayed in Table 3 .These nearest neighbors show that the neighborhood of a document in the space of word vectors is a good characterization of its topic for short , topically focused documents .", "label": "", "metadata": {}, "score": "52.10739"}
{"text": "3 .A query is shown in the section CURRENT QUERY .These words are collected in the pool of words to be examined in the section called POOL .The user can add or delete words from this pool depending on the output of the search .", "label": "", "metadata": {}, "score": "52.13144"}
{"text": "0017 ] .In various embodiments , the filtering may be performed with reference to a set of stop words .The method may use a processor and memory , and the plurality of documents may be stored in the memory .", "label": "", "metadata": {}, "score": "52.158165"}
{"text": "2 depicts a flow diagram for an exemplary method of comparing concepts for similarity within a corpus of documents and ranking the documents according to concept similarity .Initially , a corpus , or collection , of documents may be identified as relevant to a particular query 105 .", "label": "", "metadata": {}, "score": "52.161537"}
{"text": "Note that the methods display_corpus_vocab ( ) and display_doc_vectors ( ) are there only for testing purposes with small corpora .If you must use them for large libraries / corpora , you might wish to redirect the output to a file .", "label": "", "metadata": {}, "score": "52.178555"}
{"text": "The method of claim 14 , wherein the number of intervening words is 50 .The method of claim 1 , further comprising generating a context vector for each document of the corpus , the context vector for a document based on the word vectors from the thesaurus for each word located in the document .", "label": "", "metadata": {}, "score": "52.240948"}
{"text": "5 d .In .It can be seen that the higher the threshold , the broader the concepts that are extracted from the distance matrix or corresponding graph .A user or analyst can control the breadth of searching by selecting a higher or lower threshold to obtain respectively broader or narrower concepts and term sets .", "label": "", "metadata": {}, "score": "52.25202"}
{"text": "293 - 309 , 1991 .Grefenstette , Gregory , \" Use of Syntactic Context to Produce Term Association Lists for Text Retrievel \" , Computer Science Department , University of Pittsburgh , Pittsburgh , PA , pp .89 - 97 , 1992 .", "label": "", "metadata": {}, "score": "52.33654"}
{"text": "The rows of T may be vectors corresponding to term frequencies in a transformed space .In the above example , the resulting matrices are : .D .S .T . and .T .Dimensionality reduction may be performed by selecting a number m .", "label": "", "metadata": {}, "score": "52.404823"}
{"text": "Description .TECHNICAL FIELD .The disclosed embodiments generally relate to methods of analyzing groups of documents .More particularly , the disclosed embodiments relate to methods for searching and organizing a collection or corpus of documents into various groups or clusters according to concepts that occur within the documents .", "label": "", "metadata": {}, "score": "52.46708"}
{"text": "FIG .17 shows the memory locations for the ranking of document vectors by factor clusters ; .FIG .18 shows the final ranking of documents based on the factor cluster ranking ; and .FIG .19 is a graph showing the precision points computed by the context vector and the factor cluster vector methods .", "label": "", "metadata": {}, "score": "52.477146"}
{"text": "Certain articles of speech , conjunctions , certain adverbs ( collectively called stop words ) are thought to be devoid of theme content and are usually omitted from the document in VSM - based analysis .Various methods in IR have been also been used to compress vocabulary by looking at how words are associated with one another .", "label": "", "metadata": {}, "score": "52.526817"}
{"text": "If you want to turn off the normalization of the document vectors , including turning off the weighting of the term frequencies of the words by their idf values , you must set this parameter explicitly to 0 .The boolean parameter want_stemming determines whether or not the words extracted from the documents would be subject to stemming .", "label": "", "metadata": {}, "score": "52.536186"}
{"text": "For example a dilation factor of the square root of two would provide information from averaging over window sizes intermediate between those computed in the preferred embodiment .This would produce an image of the CWT which is smoother than that obtained in the preferred embodiment .", "label": "", "metadata": {}, "score": "52.59474"}
{"text": "To illustrate this approach , consider the N topics sensors attached to each word in the current reduced vocabulary as a set , W. The sensors attached to the query are members of another subset Q -- a fixed set .Let w be a number 0 .", "label": "", "metadata": {}, "score": "52.625614"}
{"text": "construct_lsa_model ( ) : .If after you have extracted the corpus vocabulary and constructed document vectors , you would do your retrieval with LSA modeling , you need to make the following call : .The SVD decomposition that is carried out in LSA model construction uses the constructor parameter lsa_svd_threshold to decide how many of the singular values to retain for the LSA model .", "label": "", "metadata": {}, "score": "52.63582"}
{"text": "The factor vectors are then compared to the document vectors to determine the ranking of the documents within the factor cluster . \" Cluster Algorithm for Vector Libraries Having Multiple Dimensions \" , IBM Technical Disclosure Bulletin , vol .37 , No .", "label": "", "metadata": {}, "score": "52.68112"}
{"text": "For Precision and Recall Calculations with VSM : .To experiment with precision and recall calculations for VSM retrieval , run the script : . calculate_precision_and_recall_for_VSM.pl .Note that this script will carry out its own estimation of relevancy judgments --- which in most cases would not be a safe thing to do .", "label": "", "metadata": {}, "score": "52.71263"}
{"text": "for each cluster and corresponding term sets , ranking logs in each cluster by frequency of occurrence of terms in the concept term set .The method of .claim 12 , further comprising identifying one or more logs that relate to a product problem based on the ranking of the log or logs .", "label": "", "metadata": {}, "score": "52.766624"}
{"text": "These might be hand built for a restricted domain or computed from the text of corpus itself .A thesaurus is a data structure that defines semantic relatedness between words .It is typically used in information retrieval to expand search terms with other closely related words .", "label": "", "metadata": {}, "score": "52.78513"}
{"text": "The boolean parameter want_stemming determines whether or not the words extracted from the documents would be subject to stemming .As mentioned elsewhere , stemming means that related words like ' programming ' and ' programs ' would both be reduced to the root word ' program ' .", "label": "", "metadata": {}, "score": "52.85084"}
{"text": "FIG .16 shows the retrieval of documents using the word factor method .In step 320 , the three factors of the query computed by the process shown in FIG .14 are retrieved .The factor vector for each factor cluster is computed in step 322 .", "label": "", "metadata": {}, "score": "52.893925"}
{"text": "A need exists for a method to compare documents not simply based on word occurrences , but rather on a higher level of concept occurrence and co - occurrence , where a concept is defined by two or more correlated terms .", "label": "", "metadata": {}, "score": "52.899734"}
{"text": "For those who still wish to store on a disk the model that is constructed , the script retrieve_with_VSM_and_also_create_disk_based_model.pl shows how you can do that .Other changes in 1.60 include a slight reorganization of the scripts in the examples directory .", "label": "", "metadata": {}, "score": "52.94311"}
{"text": "When the PC has or . had many different printers loaded on it , the Windows spooler will crash .\" [ 0045 ] .In an embodiment , the terms in the concept set may be semantically meaningful and may readily identify the concept .", "label": "", "metadata": {}, "score": "53.069878"}
{"text": "The entries contain the conditional probabilities modified by the independent probabilities .In the preferred embodiment B was taken as 2.0 .The window frequency count is then incorporated as a penalty term .It is not necessary to include any information about how many times a word appears in a window , only whether it appears or not .", "label": "", "metadata": {}, "score": "53.116714"}
{"text": "No . 08/713313 , filed Sep. 13 , 1996 entitled \" System for information Discovery \" and available from ThemeMedia Inc. , Richland Wash. is utilized .Stop words are very common words such as articles of speech , prepositions , and some adverbs .", "label": "", "metadata": {}, "score": "53.171013"}
{"text": "Hence word frequencies can be accommodated by applying a modified form of Na\u00efve Bayes which is Na\u00efve Bayes Multinomial .International Journal of Artificial Intelligence & Applications ( IJAIA ) , Vol .Dimensionality Reduction : We have tried to look at the performance of the classifier using dimensionality reduction technique .", "label": "", "metadata": {}, "score": "53.347008"}
{"text": "The stop word list may be a list of grammatical or function words that are deemed unlikely to be useful for searching .These words may have important semantic functions in English , but they may be considered to rarely contribute information if the search criterion is a simple word - by - word match .", "label": "", "metadata": {}, "score": "53.357555"}
{"text": "The stop word list may be a list of grammatical or function words that are deemed unlikely to be useful for searching .These words may have important semantic functions in English , but they may be considered to rarely contribute information if the search criterion is a simple word - by - word match .", "label": "", "metadata": {}, "score": "53.357555"}
{"text": "Version 1.41 downshifts the required version of the PDL module .Also cleaned up are the dependencies between this module and the submodules of PDL .Version 1.4 makes it easier for a user to calculate a similarity matrix over all the documents in the corpus .", "label": "", "metadata": {}, "score": "53.366646"}
{"text": "The ' debug ' option , when turned on , will output a large number of intermediate results in the calculation of the model .It is best to redirect the output to a file if ' debug ' is on .", "label": "", "metadata": {}, "score": "53.41108"}
{"text": "Precision and recall calculations for retrieval accuracy determination are best carried out with human - supplied judgments of relevancies of the documents to queries .If such judgments are available , run the script : . calculate_precision_and_recall_from_file_based_relevancies_for_VSM.pl .This script will print out the average precisions for the different test queries and calculate the MAP metric of retrieval accuracy .", "label": "", "metadata": {}, "score": "53.45409"}
{"text": "The definitions of the parameters are as shown : ....................... ( 1 ) ..............( 2 ) ................( 3 ) ...............( 4 ) VI .We have also compared the results obtained with that of the sentence level text classification .", "label": "", "metadata": {}, "score": "53.484795"}
{"text": "Extrapolating these numbers to really large libraries / corpora , we are obviously talking about very large matrices for SVD decomposition .For large libraries / corpora , it would be best to store away the model in a disk file and to base all subsequent retrievals on the disk - stored models .", "label": "", "metadata": {}, "score": "53.52377"}
{"text": "Extrapolating these numbers to really large libraries / corpora , we are obviously talking about very large matrices for SVD decomposition .For large libraries / corpora , it would be best to store away the model in a disk file and to base all subsequent retrievals on the disk - stored models .", "label": "", "metadata": {}, "score": "53.52377"}
{"text": "Extrapolating these numbers to really large libraries / corpora , we are obviously talking about very large matrices for SVD decomposition .For large libraries / corpora , it would be best to store away the model in a disk file and to base all subsequent retrievals on the disk - stored models .", "label": "", "metadata": {}, "score": "53.52377"}
{"text": "The documents are analyzed to determine the number that are actually relevant to the query .The precision of the search is the ratio of the number of relevant documents to the number of retrieved documents .The recall of the search is the ratio of the number of relevant documents to the number of relevant documents in the corpus .", "label": "", "metadata": {}, "score": "53.62589"}
{"text": "The parameters that end in ' _ db ' are for naming the database files in which the basic information about the model is stored , as explained for the previous constructor call .These are supplied through the constructor parameter ' query_file ' .", "label": "", "metadata": {}, "score": "53.6593"}
{"text": "The parameters that end in ' _ db ' are for naming the database files in which the basic information about the model is stored , as explained for the previous constructor call .These are supplied through the constructor parameter ' query_file ' .", "label": "", "metadata": {}, "score": "53.6593"}
{"text": "The method of claim 3 wherein the visual representation of the semantic structure of the document is selected from the group comprising a text based representation and a graphical representation and combinations thereof .The method of claim 1 further comprising the step of utilizing the output of the wavelet transform to partition the document .", "label": "", "metadata": {}, "score": "53.669086"}
{"text": "display_doc_relevancies ( ) : .If you would like to see the document relevancies generated by the previous method , you can call . precision_and_recall_calculator ( ) : .After you have created or obtained the relevancy judgments for your test queries , you can make the following call to calculate Precision@rank and Recall@rank : . or .", "label": "", "metadata": {}, "score": "53.71757"}
{"text": "The third is needed for calculating the SVD of the term - frequency matrix .( PDL stands for Perl Data Language . )EXAMPLES .See the ' examples ' directory in the distribution for the scripts listed below : .", "label": "", "metadata": {}, "score": "53.74472"}
{"text": "While each of the above described improvements provide some benefit to the user , the solution set of documents does not optimally convey the right information to a user if the user does not have an understanding of the subject matter being searched .", "label": "", "metadata": {}, "score": "53.90106"}
{"text": "They can be used to represent any joint probability function .They are particularly useful when the direct ( causal ) relationships between variables are sparse , because the update procedures for accomodating new observations are efficient .Approximation inference techniques are available for complex , non - sparse domains .", "label": "", "metadata": {}, "score": "53.931206"}
{"text": "The topical word classes agglomerate information over similar words .In FIG .5 , step 100 computes the word and word pair frequencies in the corpus .A word pair is two consecutive words in the corpus .For example , the title \" The Journal of Computer Science \" has four word pairs : The Journal ; Journal of ; of Computer ; and Computer Science .", "label": "", "metadata": {}, "score": "53.93949"}
{"text": "As shown in FIG .3 , the x - axis is the narrative word order , the y - axis is the multi - resolution level on a log scale , and the z - axis is the energy level .", "label": "", "metadata": {}, "score": "53.95304"}
{"text": "[0010 ] .Thus , LSA provides a means for extracting and representing the contextual - usage meaning of words by statistical computations applied to a large corpus of text .Information content is defined by more than just its mere terms but is also characterized by the context in which a specific term is given .", "label": "", "metadata": {}, "score": "54.04113"}
{"text": "min_word_length .The parameter min_word_length sets the minimum number of characters in a word in order for it be included in the corpus vocabulary .lsa_svd_threshold .The parameter lsa_svd_threshold is used for rejecting singular values that are smaller than this threshold fraction of the largest singular value .", "label": "", "metadata": {}, "score": "54.063934"}
{"text": "An important concept is that the similarity of meaning of terms to each other is based on the context in which that term appears and the context in which that term does not appear .LSA determines similarity of meaning of terms by context in which the terms are used and represents the terms and passages by statistical analysis of a large information corpus .", "label": "", "metadata": {}, "score": "54.11303"}
{"text": "An important concept is that the similarity of meaning of terms to each other is based on the context in which that term appears and the context in which that term does not appear .LSA determines similarity of meaning of terms by context in which the terms are used and represents the terms and passages by statistical analysis of a large information corpus .", "label": "", "metadata": {}, "score": "54.11303"}
{"text": "Each element b.sub.ij records the number of times the w.sub.j co - occurs with any of the medium - frequency words from class g.sub.Ai .This is similar to the usual co - occurrence matrix construction except that the matrix is no longer symmetric .", "label": "", "metadata": {}, "score": "54.126595"}
{"text": "Semantic structure is the order in which the topics are discussed in the document narrative .As will be further apparent to those skilled in the art , different methods of producing the signal will provide varying levels of noise in the resultant signal .", "label": "", "metadata": {}, "score": "54.127724"}
{"text": "Step 106 forms the first set of topical word classes by clustering Matrix A into groups .The clustering algorithm is based on the cosine similarity between the columns of matrix A. For example , 200 A - classes g.sub.A1 , g.sub.", "label": "", "metadata": {}, "score": "54.166847"}
{"text": "( PDL stands for Perl Data Language . )Precision and recall calculations for retrieval accuracy determination are best carried out with human - supplied judgments of relevancies of the documents to queries .If such judgments are available , run the script : . calculate_precision_and_recall_from_file_based_relevancies_for_VSM.pl .", "label": "", "metadata": {}, "score": "54.18548"}
{"text": "One component matrix is a two - dimensional matrix in which rows correspond to search terms , columns correspond to documents , and values correspond to numbers of occurrences of the terms within the documents .This matrix may be multiplied by other matrices that , for example , predict or identify the expected number of terms or related terms in the document in order to form a reconstructed matrix that measures the contextual relation of search terms .", "label": "", "metadata": {}, "score": "54.200394"}
{"text": "One component matrix is a two - dimensional matrix in which rows correspond to search terms , columns correspond to documents , and values correspond to numbers of occurrences of the terms within the documents .This matrix may be multiplied by other matrices that , for example , predict or identify the expected number of terms or related terms in the document in order to form a reconstructed matrix that measures the contextual relation of search terms .", "label": "", "metadata": {}, "score": "54.200394"}
{"text": "As is commonly the case with VSM , this module uses the cosine similarity distance when comparing a document vector with the query vector . display_retrievals( $ retrievals ): .You can display the retrieved document names by calling this method using the syntax : . where $ retrievals is a reference to the hash returned by a call to one of the retrieve methods .", "label": "", "metadata": {}, "score": "54.282913"}
{"text": "Documents 13609 , 22872 , and 27081 are long documents with more than one topic .Therefore , their document vectors are closer to the global centroid .Their nearest neighbors are function words , because function words share the characteristic of having a large number of words from different topics as their neighbors .", "label": "", "metadata": {}, "score": "54.316437"}
{"text": "These are supplied through the constructor parameter ' query_file ' .The format of the this file must be according to the sample file ' test_queries . txt ' in the ' examples ' directory .The module estimates the relevancies of the documents to the queries and dumps the relevancies in a file named by the ' relevancy_file ' constructor parameter .", "label": "", "metadata": {}, "score": "54.45755"}
{"text": "For an oracle , the value of MAP should be 1.0 .On the other hand , for purely random retrieval from a corpus , the value of MAP will be inversely proportional to the size of the corpus .( See the discussion in https://engineering.purdue.edu/kak/SignificanceTesting.pdf for further explanation on these retrieval precision evaluators . )", "label": "", "metadata": {}, "score": "54.532516"}
{"text": "13 shows the memory locations for the ranking of documents ; .FIG .14 is a flow diagram for forming factor clusters of document vectors ; .FIG .15 shows the memory locations for the factor clusters ; .FIG .", "label": "", "metadata": {}, "score": "54.554703"}
{"text": "The resulting collection of document windows is then treated as a document corpus -- thus the name pseudo - corpus .In a preferred embodiment of the present invention , the pseudo - corpus is fed into a text engine known in the art for further processing .", "label": "", "metadata": {}, "score": "54.59629"}
{"text": "An extended cosine formula is preferred in this circumstance .The extended cosine procedure is nearly identical to the composite wavelet energy except that the normalized dot product is used to operate on the vectors to be compared , thus emphasizing the pattern of usage and de - emphasizing some information about frequency of usage .", "label": "", "metadata": {}, "score": "54.601105"}
{"text": "Single term searching is extremely fast and efficient because little computational effort is involved in the query process , but it can result in a relatively large solution set of unranked documents being returned .Alternatively , the results may be ranked based on a weighted sum of matching words .", "label": "", "metadata": {}, "score": "54.679813"}
{"text": "Single term searching is extremely fast and efficient because little computational effort is involved in the query process , but it can result in a relatively large solution set of unranked documents being returned .Alternatively , the results may be ranked based on a weighted sum of matching words .", "label": "", "metadata": {}, "score": "54.679813"}
{"text": "TwinSpaces uses the methods of thesaurus vectors and document vectors defined in this application .TwinSpaces has two parts : WordSpace ( generation of word vectors ) and DocumentSpace ( generation of document vectors ) .TwinSpaces is ideal for a user who has a clearly defined information need , but who may not be fully aware of the vocabulary that is used for the topic of interest in the document collection .", "label": "", "metadata": {}, "score": "54.767662"}
{"text": "for each cluster and corresponding term sets , ranking logs in each cluster by frequency of occurrence of terms in the concept term set .The method of .claim 1 , further comprising identifying one or more logs that relate to a product problem based on the ranking of the log or logs .", "label": "", "metadata": {}, "score": "54.78382"}
{"text": "The ' examples ' directory contains two scripts to illustrate how such matrices can be calculated by the user .The similarity matrix is output as a CSV file .Version 1.3 incorporates IDF ( Inverse Document Frequency ) weighting of the words in a document file .", "label": "", "metadata": {}, "score": "54.7864"}
{"text": "The ' examples ' directory contains two scripts to illustrate how such matrices can be calculated by the user .The similarity matrix is output as a CSV file .Version 1.3 incorporates IDF ( Inverse Document Frequency ) weighting of the words in a document file .", "label": "", "metadata": {}, "score": "54.7864"}
{"text": "Subsequently , you can carry out retrieval by directly using this disk - based VSM model for speedier performance .In order to do so , you must upload the disk - based model by .Subsequently you call .The first two of these are needed for creating disk - based database records for the VSM and LSA models .", "label": "", "metadata": {}, "score": "54.791786"}
{"text": "The representation of documents gives rise to a similarity measure that reflects topical relatedness better than a scheme based on literal matches .Topical or semantic similarity between two words can then be defined as the cosine between the corresponding columns of the matrix C as defined in equation 3a .", "label": "", "metadata": {}, "score": "54.851116"}
{"text": "i.e. locating major thematic breaks .However , there may be information relevant for particular queries in the neglected channels .Thus , in certain implementations of the present invention , many more PCA channels might be kept to provide additional information as may be required by the particular user .", "label": "", "metadata": {}, "score": "54.939796"}
{"text": "Closeness may be calculated by looking at angles and choosing documents that enclose the smallest angle with the query vector .For example , the graph shown in .FIG .1 depicts a vector space with two dimensions corresponding to the words \" crash \" and \" spooler . \"", "label": "", "metadata": {}, "score": "55.002678"}
{"text": "Closeness may be calculated by looking at angles and choosing documents that enclose the smallest angle with the query vector .For example , the graph shown in .FIG .1 depicts a vector space with two dimensions corresponding to the words \" crash \" and \" spooler . \"", "label": "", "metadata": {}, "score": "55.002678"}
{"text": "The method may also include clustering documents that contain concept term sets together .BRIEF DESCRIPTION OF THE DRAWINGS .[ 0021 ] .Aspects , features , benefits and advantages of the embodiments of the present invention will be apparent with regard to the following description , appended claims and accompanying drawings where : .", "label": "", "metadata": {}, "score": "55.07163"}
{"text": "partitions the corpus of documents D into subtopic clusters C ; and . psi . sub.2 ' assigns centroids of subtopic clusters to documents .The partitioning function makes sure that words pertaining to different topics remain separated in different clusters , thus avoiding document vectors that are too close to the global centroid .", "label": "", "metadata": {}, "score": "55.080162"}
{"text": "[ 0029 ] .The disclosed embodiments relate to methods for searching and organizing a collection or corpus of documents into various groups or clusters according to concepts that occur within the documents .A document may include a physical document , such as a paper , book or article , or it may include an electronic source of information , such as a computer file , a representation of a video screen or another electronic storage medium .", "label": "", "metadata": {}, "score": "55.22284"}
{"text": "The ratio of actual word frequency to expected word frequency is used to pinpoint words of \" greatest topicality \" and produces a topicality measure for each term .[ Bookstein , A. et.al . , 1995 ] \" SID \" actually uses the reciprocal of a ratio related to Bookstein 's to assign topicality to terms .", "label": "", "metadata": {}, "score": "55.223373"}
{"text": "For computing the TF - IDF weights for each document in the corpus , it is required in the corpus a series of steps : 1 ) Tokenize the corpus 2 ) Model the Vector Space and 3 ) Compute the TF - IDF weight for each document in the corpus .", "label": "", "metadata": {}, "score": "55.254276"}
{"text": "Words with small idf values are non - discriminatory and should get reduced weighting in document retrieval .You can display the retrieved document names by calling this method using the syntax : . where $ retrievals is a reference to the hash returned by a call to one of the retrieve methods .", "label": "", "metadata": {}, "score": "55.28245"}
{"text": "Section-2.2 describes how the corpus was prepared for use in this work .Section-3 discusses the methodology of our work .Section -4 is about Results and Discussion .LITERATURE SURVEY Text Classification is an important Natural Language Processing(NLP ) application .", "label": "", "metadata": {}, "score": "55.32057"}
{"text": "The format of this file must be as shown in the sample file stop_words.txt in the ' examples ' directory .want_stemming .The boolean parameter want_stemming determines whether or not the words extracted from the documents would be subject to stemming .", "label": "", "metadata": {}, "score": "55.37423"}
{"text": "Version 1.3 incorporates IDF ( Inverse Document Frequency ) weighting of the words in a document file .What that means is that the words that appear in most of the documents get reduced weighting since such words are non - discriminatory with respect to the retrieval of the documents .", "label": "", "metadata": {}, "score": "55.458572"}
{"text": "The parameter stop_words_file is for naming the file that contains the stop words that you do not wish to include in the corpus vocabulary .The format of this file must be as shown in the sample file stop_words.txt in the ' examples ' directory .", "label": "", "metadata": {}, "score": "55.465023"}
{"text": "If you must call this method on a large corpus , you might wish to direct the output to a file .The corpus vocabulary is shown automatically when debug option is turned on .display_inverse_document_frequencies ( ) : .You can display the idf value associated with each word in the corpus by .", "label": "", "metadata": {}, "score": "55.531956"}
{"text": "N ; and r are the dimensions of the reduced space .A singular value decomposition is an expensive operation .Therefore , compact representations can only be derived for a small part of the vocabulary using singular value decomposition .Let N be the number of words for which a singular value decomposition is feasible given the computational means available in a particular setting .", "label": "", "metadata": {}, "score": "55.548775"}
{"text": "These logs provide a source of information for discovering and tracking systemic product problems .Information retrieval methods are used because the information is in textual form .However , to date methods of analysis of service logs for discovering product have been limited to manual analysis , which is time - consuming , expensive and prone to error .", "label": "", "metadata": {}, "score": "55.5614"}
{"text": "Co- occurring terms are projected onto the same dimensions , non - co - occurring terms are projected onto different dimensions .Thus , the product of a query vector and a document vector may be relatively high ( in terms of cosine similarity ) even if they do not share any terms , as long as their terms are semantically similar according to the co - occurrence analysis .", "label": "", "metadata": {}, "score": "55.62129"}
{"text": "Many visualization systems have been built to help the information analyst sift though massive quantities of expository language text found in an electronic format in computer databases and the like .These types of systems have been critically important to identify key documents for intensive analysis .", "label": "", "metadata": {}, "score": "55.652702"}
{"text": "The simplest approach is hard thresholding : replacing small wavelet coefficients by zeros .This gives the greatest compression and speed - up , but is not necessarily the most effective denoising method .More complex denoising approaches have been developed ( such as the SURE algorithm of Donaho ) and shown effective in many cases .", "label": "", "metadata": {}, "score": "55.775875"}
{"text": "i.e. each observation is assumed to be independent of the others .It has been shown how to incorporate a wide class of \" models \" of sequences into similarity measures : .See : . D. R. Powell , L. Allison , T. I. Dix , D. L. Dowe .", "label": "", "metadata": {}, "score": "55.817234"}
{"text": "display_precision_vs_recall_for_queries ( ) : .A call to precision_and_recall_calculator ( ) will normally be followed by the following call .for displaying the Precision@rank and Recall@rank values . display_map_values_for_queries ( ) : .The area under the precision vs. recall curve for a given query is called Average Precision for that query .", "label": "", "metadata": {}, "score": "55.97534"}
{"text": "The full co - occurrence matrix is constructed for a subset of terms in the corpus .For example , 3,000 medium frequency words ( frequency ranks 2,000 through 5,000 ) are chosen for this subset .FIG .6 shows Matrix A with the dimensions of 3000 words by 3000 words .", "label": "", "metadata": {}, "score": "56.044853"}
{"text": "Accordingly , it is an object of the present invention to provide a method for automatically determining the semantic structure of an electronically formatted natural language based document .It is then a further object of the present invention to utilize spectral analysis of the digital signal as a method of characterizing the document .", "label": "", "metadata": {}, "score": "56.145325"}
{"text": "The value supplied for this parameter is an anonymous list of the file suffixes for the file types .For example , if you wanted only Java and text files to be scanned , you will set this parameter to [ ' .", "label": "", "metadata": {}, "score": "56.16228"}
{"text": "The present invention utilizes spectral analysis of a waveform or digital signal created from written words contained in an electronically formatted natural language document as a method for providing document characterization .As practiced by the present invention , the digital signal retains the order of the words within the document .", "label": "", "metadata": {}, "score": "56.190224"}
{"text": "It is composed by two terms : one first computes the normalized Term Frequency , which is the number of times a word appears in a documnet , divided by the total number of words in that document .Then , the second term is the Inverse Document Frequency , which is computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the term ti appears .", "label": "", "metadata": {}, "score": "56.196808"}
{"text": "To do this , we can use the NLTK library which is a collection of natural language processing algorithms written in Python .The process of tokenizing the documents in the corpous is a two steps : First the text is splint into sentences , and then the sentences are split into the individual words .", "label": "", "metadata": {}, "score": "56.258602"}
{"text": "( See the discussion in https://engineering.purdue.edu/kak/SignificanceTesting.pdf for further explanation on these retrieval precision evaluators . )This module includes methods that allow you to carry out these retrieval accuracy measurements using the relevancy judgments supplied through a disk file .If human - supplied relevancy judgments are not available , the module will be happy to estimate relevancies for you just by determining the number of query words that exist in a document .", "label": "", "metadata": {}, "score": "56.28275"}
{"text": "These visual \" Waves \" thus provide the user with the information present in a written outline .Further , the surface representation is more flexible than a standard outline or tree because instead of requiring each sub - section to be strictly contained in one and only one higher level section , subsections may be \" fuzzily \" contained in a section or more than one section .", "label": "", "metadata": {}, "score": "56.37562"}
{"text": "METHODOLOGY Na\u00efve Bayes Multinomial Presence or absence of words is an important point to be considered while characterizing documents .Treating this parameter as Boolean attribute helps in applying machine learning to paragraph classification .Na\u00efve Baysian does not consider the number of occurrences of each word , which is potential information in determining the category of the document .", "label": "", "metadata": {}, "score": "56.443897"}
{"text": "The area under the Precision -- Recall curve is called the Average Precision for a query .When the Average Precision is averaged over all the queries , we obtain what is known as Mean Average Precision ( MAP ) .For an oracle , the value of MAP should be 1.0 .", "label": "", "metadata": {}, "score": "56.45861"}
{"text": "For each triangle , a centroid vector is created .A theme similarity parameter may then be used to merge triangles .The merging stops when further merges would fall outside the parameter range specified .The resulting merged triangles may then be associated with themes .", "label": "", "metadata": {}, "score": "56.468964"}
{"text": "See the file ' stop_words . txt ' in the ' examples ' directory for how such a file must be formatted .HOW DOES ONE DEAL WITH VERY LARGE LIBRARIES / CORPORA ?It is not uncommon for large software libraries to consist of tens of thousands of documents that include source - code files , documentation files , README files , configuration files , etc .", "label": "", "metadata": {}, "score": "56.594467"}
{"text": "For an oracle , the value of MAP should be 1.0 .On the other hand , for purely random retrieval from a corpus , the value of MAP will be inversely proportional to the size of the corpus .( See the discussion in https://engineering.purdue.edu/kak/Tutorials/SignificanceTesting.pdf for further explanation on these retrieval precision evaluators . )", "label": "", "metadata": {}, "score": "56.6454"}
{"text": "However , the term N / n .sub.i is inversely proportional to document frequency such that high frequency words receive less weight .For example , the frequency of the word \" the \" is high for a document .Therefore , this word has a high weight for the document .", "label": "", "metadata": {}, "score": "56.70909"}
{"text": "But there are many pieces of information that are relevant , yet can not be searched for , because the journalist does n't know about them yet .Thus TwinSpaces is ideal for this type of search , which is intermediate between literal retrieval and simply browsing all of the documents .", "label": "", "metadata": {}, "score": "56.85029"}
{"text": "retrieve_with_lsa ( ) : .After you have built an LSA model through the call to construct_lsa_model ( ) , you can retrieve the document names most similar to the query by : .Subsequently , you can display the retrievals by calling the display_retrievals($retrieval ) method described previously .", "label": "", "metadata": {}, "score": "56.91614"}
{"text": "For a word occurring n times in the document collection and for a definition of co - occurrence as occurring in a window of k words , there are nk co - occurrence events .However , there are only n occurrence - in - document events .", "label": "", "metadata": {}, "score": "56.930084"}
{"text": "- occurring terms are projected onto the same dimensions , non - co - occurring terms are projected onto different dimensions .Thus , the product of a query vector and a document vector may be relatively high ( in terms of cosine similarity ) even if they do not share any terms , as long as their terms are semantically similar according to the co - occurrence analysis .", "label": "", "metadata": {}, "score": "57.024876"}
{"text": "No words are omitted as the document is partitioned into windows .The ith window overlaps the ( I-1)st window by a fixed number of words .The last window is usually incomplete and the first window may also be incomplete if the windowing starts at a location other than the first word of the document .", "label": "", "metadata": {}, "score": "57.08198"}
{"text": "To remove redundancy a subset of j 's which differ by multiples of 2 k would be computed .The main advantage of the redundancy , which is most commonly used in edge detection , is the accurate location of features with sharp edges .", "label": "", "metadata": {}, "score": "57.16068"}
{"text": "See the file ' stop_words . txt ' in the ' examples ' directory for how such a file must be formatted .It is not uncommon for large software libraries to consist of tens of thousands of documents that include source - code files , documentation files , README files , configuration files , etc .", "label": "", "metadata": {}, "score": "57.17447"}
{"text": "See the file ' stop_words . txt ' in the ' examples ' directory for how such a file must be formatted .It is not uncommon for large software libraries to consist of tens of thousands of documents that include source - code files , documentation files , README files , configuration files , etc .", "label": "", "metadata": {}, "score": "57.17447"}
{"text": "A.Hearst Et al(1997 ) .TextTiling is an approach for subdividing text into multi - paragraph units that represent passages , paragraphs or subtopics .This basically plays an important role in many Information Retrieval and Text Summarization tasks . 'Genre Based Paragraph for Sentiment Analysis ' classification system for differentiating different paragraphs within movie reviews is the work carried out by Maite Taboada Et.al ( 2009 ) .", "label": "", "metadata": {}, "score": "57.281616"}
{"text": "A method for analyzing documents is disclosed .The method compares concepts consisting of groups of terms for similarity within a corpus of document , clusters documents that contain certain concept term sets together .A method for analyzing documents is disclosed .", "label": "", "metadata": {}, "score": "57.3284"}
{"text": "display_doc_vectors ( ) : .If you would like to see the document vectors constructed by the previous call , make the call : .Note that this is a useful thing to do only on small test corpora .If you must call this method on a large corpus , you might wish to direct the output to a file .", "label": "", "metadata": {}, "score": "57.331436"}
{"text": "4 , No . 5 , September 2013 123 Multinomial model for Text Categorization is a novel work of Ashraf .M.Kibriya Et.al(2004 ) .They discuss about transformations from Multinomial Na\u00efve Bayes to Transformed Weight Normalized Complement Na\u00efve Bayes .Reversing and smoothing the Multinomial Na\u00efve Bayes Text classifier is the work of Alfons Juan Et.al(2001 ) .", "label": "", "metadata": {}, "score": "57.347927"}
{"text": "Description of Related Art .Information retrieval systems typically define similarity between queries and documents in terms of a weighted sum of matching words .The usual approach is to represent documents and queries as long vectors and use similarity search techniques .", "label": "", "metadata": {}, "score": "57.395668"}
{"text": "It is the business of trying to put order on a mass of data , i.e. coming up with an explanation for it .The data are often contradictory and contain measurement errors and other uncertainties .People do inference all of the time ; it is one of the hall - marks of intelligence .", "label": "", "metadata": {}, "score": "57.47151"}
{"text": "The method of claim 3 , wherein a number of low coverage word clusters is 200 .The method of claim 3 , wherein the second subset contains 20,000 words .The method of claim 10 , wherein the words within the second subset are the most frequent words excluding stop words .", "label": "", "metadata": {}, "score": "57.490326"}
{"text": "txt ' ] .The module throws an exception if this parameter is left unspecified .( This constructor parameter was introduced in Version 1.61 . )The parameter lsa_svd_threshold is used for rejecting singular values that are smaller than this threshold fraction of the largest singular value .", "label": "", "metadata": {}, "score": "57.610077"}
{"text": "METHODOLOGY K - fold Cross Validation is used in this work for evaluation of the classifier performance .This technique involves splitting the document into K disjoint partitions and carrying out K rounds of testing with one of the partitions as the test set and the remaining as training .", "label": "", "metadata": {}, "score": "57.64123"}
{"text": "Element c.sub.i,j contains the number of times that term j co - occurs in a window of k words with any word in class g.sub.Bi .Referring to FIG .8 , matrix C has b rows and v columns .", "label": "", "metadata": {}, "score": "57.70115"}
{"text": "There is no overlap between word vectors .In the order-0 encoding , each word corresponds to a vector with exactly one non - zero weight ; that is , one entry for its own dimension .In the simplest case , this weight is one .", "label": "", "metadata": {}, "score": "57.74211"}
{"text": "Significance testing consists of forming a null hypothesis that the two retrieval algorithms you are considering are the same from a black - box perspective and then calculating what is known as a p - value .If the p - value is less than , say , 0.05 , you reject the null hypothesis .", "label": "", "metadata": {}, "score": "57.833733"}
{"text": "Significance testing consists of forming a null hypothesis that the two retrieval algorithms you are considering are the same from a black - box perspective and then calculating what is known as a p - value .If the p - value is less than , say , 0.05 , you reject the null hypothesis .", "label": "", "metadata": {}, "score": "57.833733"}
{"text": "An exemplary touch - screen display is disclosed in U.S. Pat .No .4,821,029 to Logan et al . , which is incorporated herein by reference in its entirety .[ 0051 ] .An embedded system may optionally be used to perform one , some or all of the operations of the methods described .", "label": "", "metadata": {}, "score": "57.87847"}
{"text": "An exemplary touch - screen display is disclosed in U.S. Pat .No .4,821,029 to Logan et al . , which is incorporated herein by reference in its entirety .An embedded system may optionally be used to perform one , some or all of the operations of the methods described .", "label": "", "metadata": {}, "score": "57.940628"}
{"text": "The task is to determine salient problems .Term set : spooler driver windows crashes printers crash XP .45 documents in cluster : .To get an idea of how well the term sets and log texts relate , here are fragments of two logs that . occur in all the clusters : .", "label": "", "metadata": {}, "score": "58.005585"}
{"text": "See Crouch , C. ; \" An Approach to the Automatic Construction of Global Thesauri \" ; Information Processing & Management 26 ( 5 ) ; pp .629 - 40 ; 1990 .Documents are clustered into small groups based on similarity measure .", "label": "", "metadata": {}, "score": "58.02701"}
{"text": "You construct document vectors through the following call : .If you would like to see the document vectors constructed by the previous call , make the call : .Note that this is a useful thing to do only on small test corpora .", "label": "", "metadata": {}, "score": "58.1494"}
{"text": "In a preferred embodiment , the true value may also be approximated by taking the dominant PCA components .This approach dramatically enhances computational efficiency .In the preferred embodiment , only those PCA components with singular values greater than about 1/100 times the maximum singular value are retained .", "label": "", "metadata": {}, "score": "58.266006"}
{"text": "The method of claim 1 , wherein a matrix of word vectors is formed using all of the generated word vectors of the corpus .The method of claim 2 , wherein forming the matrix comprises the steps of : . forming a first matrix from a first subset of words within the corpus , each element of the first matrix recording the number of times that two words within the first subset co - occur in the predetermined range ; . clustering the first matrix into groups to form a set of low coverage word clusters ; . clustering the second matrix into groups to form a set of high coverage word clusters ; . reducing dimensionality of the third matrix to represent each element of the third matrix as a compact vector .", "label": "", "metadata": {}, "score": "58.391533"}
{"text": "See Salton et al . ; \" Introduction to Modern Information Retrieval \" ; McGraw - Hill , New York ; 1983 .These vectors can be represented as an encoding scheme of order-0 , which is defined as : .Order-0 encoding for words : # # EQU1 # # .", "label": "", "metadata": {}, "score": "58.623962"}
{"text": "The second preferred embodiment uses the computed thesaurus vectors to perform a search for relevant documents .To use this information directly in the search , a similar representation for documents is needed .The document vectors that are computed are called \" context vectors .", "label": "", "metadata": {}, "score": "58.726234"}
{"text": "The user can also select a text location of interest for \" Topic Island \" generation or retrieval .Any location on the \" Wave \" visualization will have a specific multi - resolution level and energy level .By selecting a given point , and thereby specifying a multi - resolution level and energy level , the user then defines a cut off value of energy which may be used to partition the document .", "label": "", "metadata": {}, "score": "58.793648"}
{"text": "The present invention is thus a method for identifying the sub - topic structure of a document and visualizing those sub - topics .The invention is carried out as a series of instructions provided as a code for a programmable computer .", "label": "", "metadata": {}, "score": "58.798996"}
{"text": "D .S .T . and .T .[ 0035 ] .Dimensionality reduction may be performed by selecting a number m .S .T .[ 0036 ] .[ 0037 ] .A correlation matrix Z may be computed from the rows or columns of transformed term matrix X 130 .", "label": "", "metadata": {}, "score": "58.940105"}
{"text": "The method of claim 1 , wherein the similarity between two words uses a cosine similarity function of : # # EQU18 # # where V is a set of words ; w.sub.i is the word ; w.sub.j is the co - occurring word ; . phi . is a word encoding vector of the word and the co - occurring word .", "label": "", "metadata": {}, "score": "58.99424"}
{"text": "Another problem is that a query and a document can share ambiguous words .Thus , the word may be used in a different sense in the document than in the query .In this case , the query and the document may have a high degree of similarity according to a measurement of the cosine function of equation 3b even though the query and the document do not overlap in the intended topic .", "label": "", "metadata": {}, "score": "59.05656"}
{"text": "Order-2 encoding for words : # # EQU9 # # .Order-2 encoding for documents : # # EQU10 # # .Since order-1 vectors are based on word neighbors , the representation for a word w.sub.i that is derived in this way contains information about the neighbors of the neighbors of w.sub.i in the document collection .", "label": "", "metadata": {}, "score": "59.174515"}
{"text": "In this manner , the present invention allows the user to quickly identify changes in the theme in the document narration , define meaningful subdocuments , enhance queries of the document , and provide visual summaries of the topic evolution within the document without necessarily reading the document .", "label": "", "metadata": {}, "score": "59.17749"}
{"text": "Thus , a more compact word representation of order-0 and order-1 are as follows : .Compact order-0 encoding : # # EQU7 # # .Compact order-1 encoding : # # EQU8 # # .Reduced order-1 encoding : . where V.sub .", "label": "", "metadata": {}, "score": "59.387955"}
{"text": "Modelling organization in student Essays ' carried out by Isaac Persing Et.al(2010 ) discusses the structure of the essay .The authors develop computational model for the organization of the student essays .They adopt heuristic approach to paragraph function labelling . '", "label": "", "metadata": {}, "score": "59.40979"}
{"text": "d k and c k are filters .Then the following identities , called the two - scale relations , hold : # # EQU3 # # and similarly for \u03c6 .That is , if the scaling function coefficients are known at index m-1 , then the wavelet and scaling function coefficients at index m can be determined .", "label": "", "metadata": {}, "score": "59.554623"}
{"text": "FIG .17 shows the memory divided into sections for each factor .In each section , the documents are ranked from highest to lowest .Each factor rank is associated with the document identification and the correlation coefficient .In step 344 , the documents are ranked based on the maximum rank of the factors .", "label": "", "metadata": {}, "score": "59.648277"}
{"text": "Version 1.70 : All of the changes made in this version affect only that part of the module that is used for calculating precision - vs .-recall curve for the estimation of MAP ( Mean Average Precision ) .The new formulas that go into estimating MAP are presented in the author 's tutorial on significance testing .", "label": "", "metadata": {}, "score": "59.686783"}
{"text": "Processing results and user input information can be monitored on a CRT display monitor 14 .After processor 16 has completed processing the documents , the results can be output to an output device 18 , which includes , for example , a storage means ( hard or floppy disk ) , a printer , a photocopier , a facsimile machine or a CRT display .", "label": "", "metadata": {}, "score": "59.686882"}
{"text": "[ 0014 ] .An important application of information retrieval is to extract information from service logs written by product support staff .In an enterprise that manufactures complex and expensive machines and systems , service contracts are offered to customers .", "label": "", "metadata": {}, "score": "59.786877"}
{"text": "The method of .The method of .The method of .claim 1 , wherein the concept threshold is less than or equal to 1 , and terms that have correlation higher than the concept threshold form the vertices of the concept graph .", "label": "", "metadata": {}, "score": "59.790787"}
{"text": "The method of claim 1 wherein said wavelet transform is selected from the group comprising a fast wavelet transform , a redundant wavelet transform , a non - orthogonal wavelet transform , a local cosine transform , and a local sine transform .", "label": "", "metadata": {}, "score": "60.053978"}
{"text": "The method returns a hash whose keys are the document names and whose values the similarity distance between the document and the query .As is commonly the case with VSM , this module uses the cosine similarity distance when comparing a document vector with the query vector .", "label": "", "metadata": {}, "score": "60.067192"}
{"text": "7 shows the Matrix B computed in the flow diagram of FIG .5 ; .FIG .8 shows the Matrix C computed in the flow diagram of FIG .5 ; .FIG .9 shows the reduced Matrix C computed in the flow diagram of FIG .", "label": "", "metadata": {}, "score": "60.144974"}
{"text": "B200 ) generated are of high coverage .The Buckshot method is one example of a method to cluster the groups .In step 112 , a third co - occurrence matrix C is collected for the full corpus vocabulary versus the B - classes .", "label": "", "metadata": {}, "score": "60.17015"}
{"text": "An important application of information retrieval is to extract information from service logs written by product support staff .In an enterprise that manufactures complex and expensive machines and systems , service contracts are offered to customers .When machines or systems encounter problems , customers notify customer support staff who enter pertinent information about the problem into a service log .", "label": "", "metadata": {}, "score": "60.191616"}
{"text": "B47 , then they would not be similar in the 200-dimensional space ; but they are similar in the reduced space .This is because the singular value decomposition algorithm recognizes and eliminates such redundancies .Four passes through the corpus are required to complete the computation .", "label": "", "metadata": {}, "score": "60.301983"}
{"text": "Crouch constructs thesaurus classes by grouping words into bins of related words .Unfortunately , the boundaries between classes will be inevitably somewhat artificial .If classes are made too small , some words will be cut off from part of their topical neighborhood .", "label": "", "metadata": {}, "score": "60.329918"}
{"text": "In step 32 , sample C ' is partitioned into k groups by truncated group average agglomerative clustering .This partition is called partition G. In step 34 , partition P is constructed from corpus C by assigning each individual document to one of the centers in partition G. This is accomplished by applying assign - to - nearest over the corpus C and the k centers of partition G. In step 36 , partition G is replaced with partition P. Steps 34 and 36 are repeated once .", "label": "", "metadata": {}, "score": "60.41832"}
{"text": "By using the order-2 scheme , 55,000 word vectors are computed .An alternative method is to compute vectors for 5,000 letter fourgrams instead of words in iteration 1 ( for . psi . ' sub.1 ) .A fourgram is a sub - word fragment of four letters .", "label": "", "metadata": {}, "score": "60.454754"}
{"text": "A thesaurus must also cover all or most of the words found in queries , including the potentially unbounded set of proper nouns .These two considerations suggest that generic thesauri , which are restricted to common usage , are unlikely to be helpful .", "label": "", "metadata": {}, "score": "60.6366"}
{"text": "It must also be noted that as used herein and in the appended claims , the singular forms \" a , \" \" an , \" and \" the \" include plural references unless the context clearly dictates otherwise .Thus , for example , reference to a \" document \" is a reference to one or more physical documents , electronic files , and equivalents thereof known to those skilled in the art , and so forth .", "label": "", "metadata": {}, "score": "60.646202"}
{"text": "It must also be noted that as used herein and in the appended claims , the singular forms \" a , \" \" an , \" and \" the \" include plural references unless the context clearly dictates otherwise .Thus , for example , reference to a \" document \" is a reference to one or more physical documents , electronic files , and equivalents thereof known to those skilled in the art , and so forth .", "label": "", "metadata": {}, "score": "60.646202"}
{"text": "Hearst creates a smoothed token gap sequence that corresponds to the narrative order of the text .Merged paragraphs may also form a narrative based signal .While all of these methods have advantages for IR , there still exists a need for an improved method of automatically partitioning an unstructured electronically formatted natural language document into its sub - topic structure .", "label": "", "metadata": {}, "score": "60.681286"}
{"text": "claim 12 , further comprising identifying one or more problems based on the frequency of occurrence of terms in the concept term set .A computerized method of analyzing a plurality of documents , comprising : . collecting and filtering term - frequency vector from a plurality of documents , wherein the filtering is performed with reference to a set of stop words ; . identifying a term - frequency vector for each of the documents ; . identifying a term - frequency matrix , wherein rows of the matrix comprise values for the term - frequency vectors ; . projecting the term - frequency matrix onto a lower dimensional space using latent semantic analysis , to create a transformed term matrix ; . developing a correlation matrix using the rows of the transformed term matrix ; . creating a dendrogram of related concepts using a function of the correlation matrix by identifying , from the transformed term matrix , groups of terms that exceed a correlation threshold and connecting the groups of terms that exceed the correlation threshold to form a concept term set ; . identifying branches of the dendrogram corresponding to related concepts ; and .", "label": "", "metadata": {}, "score": "60.744705"}
{"text": "4 , No . 5 , September 2013 122 hierarchically divided into sections , paragraphs and sentences .Hence finding the correct location to insert updated information takes due importance which can be done using paragraph classification .As mentioned in the Literature , Topic shift detection , discourse parsing , text segmentation etc may be treated as paragraph classification tasks .", "label": "", "metadata": {}, "score": "60.7554"}
{"text": "( d.sub.j ) is computed for document d.sub.j by summing up the vectors of all tokens occurring in it .Similarity between the vector representations for words is measured by the cosine function : # # EQU3 # # Equation 3a is used to determine topical or semantic similarities between two words .", "label": "", "metadata": {}, "score": "60.76059"}
{"text": "g.sub.A200 using group average agglomerative clustering are found .These 200 word clusters are considered low coverage clusters .The Buckshot method is one example of a method to cluster the groups .A second matrix B is formed in step 108 by considering a larger vocabulary subset .", "label": "", "metadata": {}, "score": "60.787895"}
{"text": "FIG .5 a .FIG .5 b .This results in two connected components or term sets .These connected components are extracted algorithmically by methods known in the art .FIG .5 c .[ 0042 ] .", "label": "", "metadata": {}, "score": "60.807087"}
{"text": "Singular value decomposition or another appropriate method may be used to express the term - frequency matrix as a product of at least a document matrix , a diagonal matrix of eigenvalues , and a term matrix .The entries of the diagonal matrix may be the eigenvalues of the term - frequency matrix in decreasing order , and the rows of the term matrix may correspond to vectors corresponding to term frequencies in a transformed space .", "label": "", "metadata": {}, "score": "60.875458"}
{"text": "4 , No . 5 , September 2013 126 the document , the amount of words that occur once is very large , which add unrealistic requirement to the training data .The points are plotted on the graph in an excel sheet and a best fit curve is derived which makes it possible to interpolate and infer co - ordinates for other points .", "label": "", "metadata": {}, "score": "60.887123"}
{"text": "The method of claim 32 , wherein computing the correlation coefficient for each document uses the following formula : # # EQU26 # # where . psi .( f.sub.m ) is the factor vector for factor cluster f.sub.m and . psi .", "label": "", "metadata": {}, "score": "60.918648"}
{"text": "For basic VSM - based model construction and retrieval , run the script : . retrieve_with_VSM.pl .For Basic LSA - Based Retrieval : .For basic LSA - based model construction and retrieval , run the script : . retrieve_with_LSA.pl .", "label": "", "metadata": {}, "score": "60.96456"}
{"text": "The primary location may be found using a coarser multi - resolution level ( a higher value for k ) while the minor discussion would be located using a finer multi - resolution level(a lower value for k ) .Thus , \" fuzzily \" located refers to the phenomenon where discussions of a single topic are scattered throughout a document .", "label": "", "metadata": {}, "score": "60.969326"}
{"text": "The apparatus of claim 40 , wherein the thesaurus comprises : . an extractor for retrieving a word from the corpus ; . a generator generating a word vector for the word based on every recorded number .The apparatus of claim 40 , wherein the determining means ranks the documents based on the correlation relationship .", "label": "", "metadata": {}, "score": "61.033436"}
{"text": "The method of claim 32 , wherein ranking of the documents within a factor cluster is based on correlation : . where the rank of d.sub.j according to this ranking is r.sub.m ( j ) ; and corr(f.sub.m , d.sub.j ) is the correlation of factor m and document j. .", "label": "", "metadata": {}, "score": "61.05577"}
{"text": "Table 2 gives two examples of words whose synonyms have almost identical direction in the multidimensional space .DocumentSpace , which is the second part of TwinSpaces , contains 34,000 articles from the New York Times newswire between the months of June and November of 1990 .", "label": "", "metadata": {}, "score": "61.063255"}
{"text": "The query \" Japanese American research \" was performed using DocumentSpace .In the SEARCH RESULT section of FIG .4 , the ten top ranking documents are shown .These documents seem to conform well with the query although there are few literal matches .", "label": "", "metadata": {}, "score": "61.150497"}
{"text": "Each file may include representations of some or all of the text of the original documents .The corpus may be stored on a computer - readable or other medium , such as a database , on a server or otherwise .", "label": "", "metadata": {}, "score": "61.243164"}
{"text": "Each file may include representations of some or all of the text of the original documents .The corpus may be stored on a computer - readable or other medium , such as a database , on a server or otherwise .", "label": "", "metadata": {}, "score": "61.243164"}
{"text": "In order-0 retrieval , the occurrence of the word \" coast \" in one document and the use of its synonym \" shoreline \" in a related document will not increase the similarity of the two documents .The higher - order scheme can exploit synonymy since \" coast \" and \" shoreline \" have similar neighbors .", "label": "", "metadata": {}, "score": "61.313183"}
{"text": "0007 ] .Documents are ranked according to similarity with the query as measured by the cosine measurement , or the normalized correlation coefficient .This calculation computes how well the occurrence of a term correlates to both the query and the document .", "label": "", "metadata": {}, "score": "61.5401"}
{"text": "claim 1 , wherein the set of term - frequency vectors for the documents is maintained as rows and columns in a term - frequency matrix .The method of .claim 1 , wherein the columns and rows of the correlation matrix correspond to terms that are the terms of the transformed term matrix , and a concept comprises highly correlated terms .", "label": "", "metadata": {}, "score": "61.58869"}
{"text": "Mining Software Repository conference ( MSR11 ) was based on a relatively small iBUGS dataset involving 6546 documents and a vocabulary size of 7553 unique words .Also note that the iBUGS dataset was originally put together by V. Dallmeier and T. Zimmermann for the evaluation of automated bug detection and localization tools . )", "label": "", "metadata": {}, "score": "61.62712"}
{"text": "Mining Software Repository conference ( MSR11 ) was based on a relatively small iBUGS dataset involving 6546 documents and a vocabulary size of 7553 unique words .Also note that the iBUGS dataset was originally put together by V. Dallmeier and T. Zimmermann for the evaluation of automated bug detection and localization tools . )", "label": "", "metadata": {}, "score": "61.62712"}
{"text": "Mining Software Repository conference ( MSR11 ) was based on a relatively small iBUGS dataset involving 6546 documents and a vocabulary size of 7553 unique words .Also note that the iBUGS dataset was originally put together by V. Dallmeier and T. Zimmermann for the evaluation of automated bug detection and localization tools . )", "label": "", "metadata": {}, "score": "61.62712"}
{"text": "The second pass computes Matrix A and the A - classes .The third pass computes Matrix B and the B - classes .Finally , the fourth pass computes Matrix C. In addition , Matrix C is decomposed by using singular value decomposition to compute the thesaurus vectors .", "label": "", "metadata": {}, "score": "61.725143"}
{"text": "Documents containing the terms of a concept set may be clustered together 140 .Clustering may include , for example , tallying and/or ranking documents according to the number of terms in a concept they contain .Documents may be rank ordered within each cluster according to the number of occurrences of terms in a concept set 145 and presented to a user in a format corresponding to the rank order as seen the previous example .", "label": "", "metadata": {}, "score": "61.835014"}
{"text": "And the latter uses normalized document vectors for the same purpose .The document order used for row and column indexing of the matrix corresponds to the alphabetic ordering of the document names in the corpus directory .You have to be careful when carrying out Precision verses Recall calculations if you do not wish to lose the previously created relevancy judgments .", "label": "", "metadata": {}, "score": "61.875122"}
{"text": "1 .These separate multi - resolution levels are then combined as illustrated in FIG .2 . using a color shade or gray - scale to indicate the energy level .As illustrated in FIG .3 , this visualization may then be extended to a 3-D colored or grey scale surface plot .", "label": "", "metadata": {}, "score": "61.947136"}
{"text": "For example , any word with a frequency of one and any word pair with a frequency of less than five are not important .The word pair \" computer science \" will appear throughout the text .Therefore , it will probably appear more times than the other three word pairs , which will only appear in the title .", "label": "", "metadata": {}, "score": "62.01999"}
{"text": "Two of these values are calculated using an MDS projection on the centoids for the collection of thematic chunks .These values are used to determine the placement of the thematic chunk in the x - y plane .The multi - resolution level is then used to determine the placement of the thematic chunk in the z plane .", "label": "", "metadata": {}, "score": "62.04071"}
{"text": "In contrast , every word co - occurs with several words in the B - subset and hence will have many co - occurrence events with respect to B - classes .The 200 word clusters ( g.sub.B1,g.sub.B2 . . .", "label": "", "metadata": {}, "score": "62.054924"}
{"text": "The method of claim 3 , wherein a number of words selected from the corpus to form the first matrix is 3000 words .The method of claim 5 , wherein the selected words are medium frequency words .The method of claim 3 , wherein the clustering of the first matrix and the second matrix use a Buckshot fast clustering algorithm .", "label": "", "metadata": {}, "score": "62.106216"}
{"text": "See Peat et al . ; \" The Limitations of Term Co - occurrence Data for Query Expansion in Document Retrieval Systems \" ; Journal of the American Society for Information Science 42 ( 5 ) ; pp .378 - 83 ; 1991 .", "label": "", "metadata": {}, "score": "62.162994"}
{"text": "FIG .4 shows the user interface for DocumentSpace , which is parallel to the user interface for WordSpace .The user interface has functions for adding to and deleting from the pool and for looking at the nearest neighbors of the pool or an individual document .", "label": "", "metadata": {}, "score": "62.200203"}
{"text": "Therefore , a false similarity match could occur ( ambiguity problem ) .A second problem is that the same content may be expressed in different words .Therefore , a short query may miss a relevant document ( synonymy problem ) .", "label": "", "metadata": {}, "score": "62.23169"}
{"text": "The context vectors were computed for the 25 Category B topics of the Tipster collection .For each query , documents were ranked according to vector similarity as computed by the correlation coefficient and precision / recall statistics collected .The results of the invention were compared against a baseline standard vector space similarity search with augmented tf.idf term weighting .", "label": "", "metadata": {}, "score": "62.36046"}
{"text": "Primary Examiner : Hayes ; Gail O. Assistant Examiner : Kalidindi ; Krishna Attorney , Agent or Firm : . Oliff & Berridge Claims .I claim : .A method , using a processor and memory , for generating a thesaurus of word vectors based on lexical co - occurrence of words within documents of a corpus of documents , the corpus stored in the memory , the method comprising : . retrieving into the processor a retrieved word from the corpus ; . recording a number of times a co - occuring word co - occurs in a same document within a predetermined range of the retrieved word ; . repeating the recording step for every co - occurring word located within the predetermined range for each occurrence of the retrieved word in the corpus ; . generating a word vector for the word based on every recorded number ; . repeating the retrieving , recording , recording repeating and generating steps for each word in the corpus , and .", "label": "", "metadata": {}, "score": "62.384796"}
{"text": "Several methods have been used to visualize theme breaks found in electronically formatted text .Salton 's \" tour \" is a graph with links and nodes .[ Salton , 1994 ] Heart has developed a system called \" TileBars \" which allows the user to define specific topics of interest and then produces a linear color block map to show where chunks of the document are likely to contain these topics .", "label": "", "metadata": {}, "score": "62.479874"}
{"text": "Evans et al . , \" Automatic Indexing Using Selective NLP And First - Order Thesauri \" , Departments of Philosophy and Computer Science Laboratory for Computational Linguistics , Carnegie Mellon University , Pittsburgh , PA , pp .624 - 639 .", "label": "", "metadata": {}, "score": "62.536545"}
{"text": "FIG .3 is a 3-D rendering of the smoothed plot of FIG .2 where the z - axis is formed using energy intensity .FIG .4 depicts a graph of the energy levels from three different multi - resolution levels .", "label": "", "metadata": {}, "score": "62.57048"}
{"text": "Thus , a query and a document could have a similarity measure of zero in this simple scheme even though the query content can be understood as a reasonable description of the topic of the document .This is the problem of synonymy of words .", "label": "", "metadata": {}, "score": "62.579807"}
{"text": "Similarities between paragraphs are calculated using a cosine measurement ( normalized dot product ) and are used to create a text relationship map .In the text relationship map , nodes are the paragraphs and links are the paragraph similarities .All groups of three mutually related ( based on the similarity measure ) paragraphs are identified and merged .", "label": "", "metadata": {}, "score": "62.6346"}
{"text": "The method of claim 19 , wherein the word vectors are weighted before being added together .The method of claim 19 further comprising the step of normalizing the context vector .The method of claim 18 , wherein generating the context vector uses the equation : # # EQU21 # # where d.sub.j is the vector for document j ; w.sub.ij is the weight for word i in document j ; and v.sub.i is the thesaurus vector for word i. .", "label": "", "metadata": {}, "score": "62.70338"}
{"text": "The approximation of X in the k - dimensional space amounts to a dimension reduction from the original .vertline .V.vertline . dimensional to the k - dimensional space .The new lower dimensional representations for words are the rows of matrix T. Row i of matrix T is the reduced vector representation of word w.sub.i .", "label": "", "metadata": {}, "score": "62.73047"}
{"text": "For example , in Voorhees et al . , \" acts \" is expanded with the meaning \" acts of the apostles \" in a corpus of legal documents .In addition , they frequently do not record information about proper nouns , yet proper nouns are often excellent retrieval cues .", "label": "", "metadata": {}, "score": "62.74945"}
{"text": "display_normalized_doc_vectors ( ) : .If you would like to see the normalized document vectors , make the call : .See the comment made previously as to what is meant by the normalization of a document vector .pairwise_similarity_for_docs ( ) : . pairwise_similarity_for_normalized_docs ( ) : .", "label": "", "metadata": {}, "score": "62.773186"}
{"text": "Synonymous terms have similar neighbors and hence will contribute a similar \" direction \" in the multidimensional space of document vectors .Ambiguous terms have two different sorts of neighbors .In computing a document vector , those terms that correspond to the sense used in the document will be reinforced whereas the direction represented by the inappropriate sense will not be present in other words .", "label": "", "metadata": {}, "score": "62.7858"}
{"text": "FIG .1 is a series of plots of one minus the normalized composite wavelet energy at 3 fixed multi - resolution levels of an exemplary narrative .FIG .2 is a grey scale plot combining nine plots as created in FIG .", "label": "", "metadata": {}, "score": "62.869755"}
{"text": "317 - 332 , 1992 .Voorhees et al . , \" Vector Expansion in a Large Collection \" , Siemens Corporate Research , Inc. , Princeton , New Jersey .Wilks et al . , \" Providing Machine Tractable Dictionary Tools \" , Computer Research Laboratory , New Mexico State University , Las Cruces , New Mexico , pp .", "label": "", "metadata": {}, "score": "62.86976"}
{"text": "( PDL stands for Perl Data Language . )The last two are needed by the directory scanner to make pathnames platform independent .Starting with version 1.60 , this script does not store away the VSM model in disk - based hash tables .", "label": "", "metadata": {}, "score": "62.94008"}
{"text": "The approximate - repeats model converges more rapidly on DNA and other difficult data and can be used to find better explanations of the data : .There are many measures for the similarity of two sequences : longest common subsequence ( LCS ) , edit - distance , time - warps , etc ..", "label": "", "metadata": {}, "score": "63.002373"}
{"text": "For example , 20 words may be assigned as a token - sequence , which may then be described as a pseudo - sentence , and 6 token sequences may then be assigned as a block , which may then be described as a pseudo paragraph .", "label": "", "metadata": {}, "score": "63.11196"}
{"text": "By convention the diagonal elements of matrix S.sub.0 are constructed to be all positive and ordered in decreasing magnitude .Singular value decomposition allows a simple strategy for optimal approximate fit using smaller matrices .If the singular values in matrix S.sub.0 are ordered by size , the first k largest may be kept and the remaining smaller ones set to zero .", "label": "", "metadata": {}, "score": "63.133675"}
{"text": "293 - 309 ; 1991 .Word vectors are manually encoded for a medium number of words .Then , the document vectors are computed as sums of word vectors .However , this hand - encoding of documents is labor - intensive .", "label": "", "metadata": {}, "score": "63.162407"}
{"text": "The method of claim 25 , further comprising outputting at least one document according to its ranking . inputting a query ; . generating a factor vector for the query based on a clustering of the word vectors of the query ; . determining a correlation coefficient for each document based on the factor vector and the context vector for that document ; . ranking each document within a factor cluster based on the determined correlation coefficients ; and . determining a maximum rank of each document based on a combination of the ranks of the documents in each factor cluster ; and . outputting a final rank for at least one of the documents .", "label": "", "metadata": {}, "score": "63.216805"}
{"text": "378 - 383 , 1991 .Qui et al . , \" Concept Based Query Expansion \" , Department of Computer Science , Swiss Federal Institute of Technology , Zurich , Switzerland , pp .160 - 169 .Ruge , Gerda , \" Experiments on Linguistically - Based Term Associations \" , Information Processing & Management , vol .", "label": "", "metadata": {}, "score": "63.28106"}
{"text": "Two adjacent blocks form a window .By shifting each window over by one token sequence , a comparison may be made for the next pair of adjacent windows .The cosine calculation for each window is centered over the gap between the blocks .", "label": "", "metadata": {}, "score": "63.347633"}
{"text": "Ignoring all 2 lines of content .Module Install Instructions .To install Algorithm::VSM , simply copy and paste either of the commands in to your terminal A method for analyzing documents is disclosed .The method compares concepts consisting of groups of terms for similarity within a corpus of document , clusters documents that contain certain concept term sets together . 11127679 , 127679 , US 2006/0259481 A1 , US 2006/259481 A1 , US 20060259481 A1 , US 20060259481A1 , US 2006259481 A1 , US 2006259481A1 , US - A1 - 20060259481 , US - A1 - 2006259481 , US2006/0259481A1 , US2006/259481A1 , US20060259481 A1 , US20060259481A1 , US2006259481 A1 , US2006259481A1 .", "label": "", "metadata": {}, "score": "63.35177"}
{"text": "You can get hold of this data by calling .The script significance_testing.pl in the ' examples ' directory shows how you can use this method for significance testing .If you would like to compare in your own script any two documents in the corpus , you can call .", "label": "", "metadata": {}, "score": "63.56999"}
{"text": "claim 1 , further comprising ranking the documents within each cluster according to the number of occurrences of concept terms within a term set .The method of .claim 1 , wherein the at least three matrices include a document matrix , a diagonal matrix as a product of at least three matrices .", "label": "", "metadata": {}, "score": "63.577717"}
{"text": "The disclosed embodiments relate to methods for searching and organizing a collection or corpus of documents into various groups or clusters according to concepts that occur within the documents .A document may include a physical document , such as a paper , book or article , or it may include an electronic source of information , such as a computer file , a representation of a video screen or another electronic storage medium .", "label": "", "metadata": {}, "score": "63.86975"}
{"text": "\" LSI meets TREC :A Status Report \" , Susan T. Dumais , NIST Special Publication 500 - 207 , The First Text Retrieval Conference ( TREC-1 ) , Mar. , 1993 , pp .137 - 152 . \"Full Text Indexing Based on Lexical Relations An Application : Software Libraries \" , Yoelle S. Maarek et al . , Proceedings of the Twelfth Annual International ACMSIGIR Conference on Research and Development in Information Retrieval , Jun. 25 - 28 , 1989 , pp .", "label": "", "metadata": {}, "score": "63.873642"}
{"text": "A linear - time clustering algorithm such as Buckshot ( Cutting et al .1992 ) can be used .See Cutting et al . ; \" Scatter - gather : A Cluster - Based Approach to Browsing Large Document Collections \" ; Proceedings of SIGIR 1992 .", "label": "", "metadata": {}, "score": "63.93152"}
{"text": "The neighbors of the word \" Societies \" suggest that it is related to political and religious organizations , but no clear topic emerges .This indicates that the word is an ambiguous and , consequently , less useful search term .", "label": "", "metadata": {}, "score": "63.945595"}
{"text": "You can get hold of this data by calling .The script significance_testing.pl in the ' examples ' directory shows how you can use this method for significance testing .REQUIRED .This module requires the following modules : .SDBM_File Storable PDL .", "label": "", "metadata": {}, "score": "63.971413"}
{"text": "5 is a block diagram of exemplary hardware that may be used to contain and/or implement the program instructions of a system embodiment .DETAILED DESCRIPTION .Before the present methods , systems and materials are described , it is to be understood that this disclosure is not limited to the particular methodologies , systems and materials described , as these may vary .", "label": "", "metadata": {}, "score": "64.10337"}
{"text": "Fig 1.1 A sample paragraph which contains insufficient information about its category The above paragraph belongs to category Commerce but does not contain key words belonging to commerce category , which may lead to possible misclassification since the information may pertain to different categories .", "label": "", "metadata": {}, "score": "64.16822"}
{"text": "Cleaning up the data involved sentence separation and removing headings in the document .A total of 1791 paragraphs belonging to different category documents were used for the classification process .The table below shows the class wise distribution of paragraphs .", "label": "", "metadata": {}, "score": "64.217766"}
{"text": "Version 1.62 removes the Perl version restriction on the module .This version also fixes two bugs , one in the file scanner code and the other in the precision - and - recall calculator .The file scanner bug was related to the new constructor parameter case_sensitive that was introduced in Version 1.61 .", "label": "", "metadata": {}, "score": "64.25052"}
{"text": "claim 1 , wherein terms of the correlation matrix comprising a correlation higher than the concept threshold become vertices of a concept graph .The method of .claim 1 , further comprising ranking the documents within each cluster according to the number of occurrences of concept terms within a term set .", "label": "", "metadata": {}, "score": "64.49075"}
{"text": "Co- occurrences of terms are given higher weights than singular occurrences because the object of semantic indexing is to recognize which terms reappear most often in which parts of a document 's structure .It has been demonstrated that there are a few constantly recurring structural patterns within sentences and these patterns occur at the level in which terms and their articles interrelate .", "label": "", "metadata": {}, "score": "64.58411"}
{"text": "Co- occurrences of terms are given higher weights than singular occurrences because the object of semantic indexing is to recognize which terms reappear most often in which parts of a document 's structure .It has been demonstrated that there are a few constantly recurring structural patterns within sentences and these patterns occur at the level in which terms and their articles interrelate .", "label": "", "metadata": {}, "score": "64.58411"}
{"text": "If you have created a relevancy database and stored it in a file called , say , relevancy.txt , you should make a backup copy of this file before executing a script that calls estimate_doc_relevancies ( ) .Many thanks are owed to Shivani Rao and Bunyamin Sisman for sharing with me their deep insights in IR .", "label": "", "metadata": {}, "score": "64.616974"}
{"text": "R Et.al(2011 ) have investigated two classical approaches such as Na\u00efve Bayesian and Bag of Words to Sentence Level Text Classification in the Kannada Language and looked at the possibility of extending sentence level classification task to Paragraph Level Text Classification in their future work .", "label": "", "metadata": {}, "score": "64.6286"}
{"text": "Why ?Imagine that we have a repeated term in document with porpuse of improving its ranking on an Information Retrieval System or even create a bias torwards long documents , making them look more important than they are just because of the high frequency of the term in the document .", "label": "", "metadata": {}, "score": "64.67114"}
{"text": "BUGS .Please notify the author if you encounter any bugs .When sending email , please place the string ' VSM ' in the subject line to get past my spam filter .INSTALLATION .The usual . perl Makefile .", "label": "", "metadata": {}, "score": "64.72226"}
{"text": "Reducing this redundancy should enhance computational efficiency .The composite wavelet energy is calculated by taking the sum of squares across all channels ( index m ) for a fixed location ( index j ) and fixed multiresolution level ( index k ) .", "label": "", "metadata": {}, "score": "64.82956"}
{"text": "The apparatus of claim 46 , wherein the relevant documents are determined by combining the ranking of the document within each factor cluster .Description .BACKGROUND OF THE INVENTION .Field of the Invention .This invention relates to improvements in retrieving relevant documents from a corpus of documents .", "label": "", "metadata": {}, "score": "64.96756"}
{"text": "In step 250 , the correlation coefficient is computed based on the context vector of the query and the context vectors of the corpus of documents .The correlation coefficient is computed using the cosine function described earlier ( see equation 3 ) .", "label": "", "metadata": {}, "score": "64.978905"}
{"text": "The method of claim 6 wherein the document is partitioned according to the semantic structure of the document at multiple levels to produce an outline of the document .The method of claim 6 wherein the document is partitioned according to the semantic structure of the document at multiple levels to produce a fuzzy outline of the document .", "label": "", "metadata": {}, "score": "64.9884"}
{"text": "Order-1 encoding for words : # # EQU5 # # .A reasonable setting of W is 50 words .So a word is represented as the sum of its neighbors .To make the similarity between the two approaches explicit , an occurrence of a word is defined to be its own neighbor of order-0 and ordinary neighbors are defined to be neighbors of order-1 .", "label": "", "metadata": {}, "score": "65.01641"}
{"text": "Starting with version 1.60 , this script does not store away the model information in disk - based hash tables .If you want your model to be stored on the disk , you must run the script retrieve_with_VSM_and_also_create_disk_based_model.pl for that .", "label": "", "metadata": {}, "score": "65.09664"}
{"text": "The two dimensions correspond to the terms \" crash , \" and \" spooler . \"Since the vector 14 document d 2 has the smallest angle with the vector 10 for q , d 2 is the top - ranked document in response to the query \" crash , spooler .", "label": "", "metadata": {}, "score": "65.11449"}
{"text": "The two dimensions correspond to the terms \" crash , \" and \" spooler . \"Since the vector 14 document d 2 has the smallest angle with the vector 10 for q , d 2 is the top - ranked document in response to the query \" crash , spooler .", "label": "", "metadata": {}, "score": "65.11449"}
{"text": "That is because ultimately it is the humans who are the best judges of the relevancies of documents to queries .The humans bring to bear semantic considerations on the relevancy determination problem that are beyond the scope of this module .", "label": "", "metadata": {}, "score": "65.1548"}
{"text": "There are two types of compression commonly used , often simultaneously .Both are lossy -- some information is lost in the compression procedure .In a truncation type scheme , wavelet coefficients less than a specified cutoff value are replaced by zeros .", "label": "", "metadata": {}, "score": "65.25002"}
{"text": "Invoking the method estimate_doc_relevancies ( ) in your own script will cause the file relevancy.txt to be overwritten .If you have created a relevancy database and stored it in a file called , say , relevancy.txt , you should make a backup copy of this file before executing a script that calls estimate_doc_relevancies ( ) .", "label": "", "metadata": {}, "score": "65.267365"}
{"text": "claim 1 , wherein singular value decomposition is used to express the term - frequency matrix as a product of at least three matrices .The method of .claim 7 , wherein the at least three matrices include a document matrix , a diagonal matrix of eigenvalues , and a term matrix .", "label": "", "metadata": {}, "score": "65.3134"}
{"text": "Field service engineers read these logs and update them with technical observations and remedies .These logs provide a source of information for discovering and tracking systemic product problems .Information retrieval methods are used because the information is in textual form .", "label": "", "metadata": {}, "score": "65.37003"}
{"text": "[ 10 ] Qinfeng Shi , Yasemin Altun , Alex Smola , S. V. N. Vishwanathan,'Semi - Markov Models for Sequence Segmentation',Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational linguistics , pp .640 - 648 , Prague , 2007 .", "label": "", "metadata": {}, "score": "65.4725"}
{"text": "Average precision is 0.3218 .This is a five percent improvement over the tf.idf result of 0.271 .Although the invention has been illustrated with particularity , it is intended to be illustrative of the preferred embodiments .It is understood that the disclosure has been made byway of example only .", "label": "", "metadata": {}, "score": "65.56411"}
{"text": "But note that this estimation of document relevancies to queries is NOT for serious work .The reason for that is because ultimately it is the humans who are the best judges of the relevancies of documents to queries .The humans bring to bear semantic considerations on the relevancy determination problem that are beyond the scope of this module .", "label": "", "metadata": {}, "score": "65.73857"}
{"text": "But note that this estimation of document relevancies to queries is NOT for serious work .The reason for that is because ultimately it is the humans who are the best judges of the relevancies of documents to queries .The humans bring to bear semantic considerations on the relevancy determination problem that are beyond the scope of this module .", "label": "", "metadata": {}, "score": "65.73857"}
{"text": "But note that this estimation of document relevancies to queries is NOT for serious work .The reason for that is because ultimately it is the humans who are the best judges of the relevancies of documents to queries .The humans bring to bear semantic considerations on the relevancy determination problem that are beyond the scope of this module .", "label": "", "metadata": {}, "score": "65.73857"}
{"text": "For the purpose of the singular value decomposition , all order-1 representations of the vocabulary V are collected into a . vertline .V.vertline.x.vertline . V.vertline . matrix X such that row i of matrix X contains the order-1 vector of word i , i.e. , . phi .", "label": "", "metadata": {}, "score": "65.96776"}
{"text": "Visualization may be dramatically enhanced by then allowing the user to rotate the orientation angle .This dynamic surface shows at a glance the entire thematic complexity of the article at all the multi - resolution levels including major sections of topics , subsections , and transition paragraphs .", "label": "", "metadata": {}, "score": "66.027176"}
{"text": "Similar to the order-0 and order-1 functions ( . psi . sub.0 and . psi . sub.1 ) , order-2 function . psi . sub.2( d.sub.j ) is computed by summing up the vectors of the tokens occurring in d.sub.j .", "label": "", "metadata": {}, "score": "66.18285"}
{"text": "The closeness of terms with equal frequency occurs because the terms have about the same number of zero entries in their term vectors .For a given term , singular value decomposition assigns values to all dimensions of the space , so that frequent and infrequent terms can be close in the reduced space if they occur with similar terms .", "label": "", "metadata": {}, "score": "66.4591"}
{"text": "A high correlation can be any desired threshold , such as a correlation value of 0.7 or higher , 0.75 or higher , 0.8 or higher , 0.85 or higher , 0.9 or higher , 0.95 or higher , or any desired threshold .", "label": "", "metadata": {}, "score": "66.496"}
{"text": "A high correlation can be any desired threshold , such as a correlation value of 0.7 or higher , 0.75 or higher , 0.8 or higher , 0.85 or higher , 0.9 or higher , 0.95 or higher , or any desired threshold .", "label": "", "metadata": {}, "score": "66.496"}
{"text": "Version 1.61 improves the implementation of the directory scanner to make it more platform independent .Additionally , you are now required to specify in the constructor call the file types to be considered for computing the database model . java ' , ' .", "label": "", "metadata": {}, "score": "66.528854"}
{"text": "Below is the list of stop words and their meaning in English .International Journal of Artificial Intelligence & Applications ( IJAIA ) , Vol .But it is evident from our results that , words which occur once may definitely matter to the document in deciding the category of .", "label": "", "metadata": {}, "score": "66.59647"}
{"text": "Its computational complexity is O(N ) , which is slightly faster than the fast fourier transform .The filters c k and d k are called low- and high - pass filters , respectively .This refers to the part of the frequency spectrum that they are biased towards- low or high frequencies .", "label": "", "metadata": {}, "score": "66.6671"}
{"text": "As discussed above , fast algorithms exist for computing the wavelet transform .The algorithm is based on the two - scale relation ( 1 ) and is of similar complexity , O(N ) , as the fast fourier transform , where N is the number of elements in the vector or signal .", "label": "", "metadata": {}, "score": "66.707855"}
{"text": "claim 7 , wherein entries of the diagonal matrix are the cigenvalues of the term - frequency matrix in decreasing order , and .wherein the rows of the term - frequency matrix comprise vectors corresponding to term frequencies in a transformed space .", "label": "", "metadata": {}, "score": "66.88412"}
{"text": "The pairwise similarities are based on the dot product of two document vectors divided by the product of the vector magnitudes .The ' examples ' directory contains two scripts to illustrate how such matrices can be calculated by the user .", "label": "", "metadata": {}, "score": "67.01993"}
{"text": "For LSA - Based Retrieval with a Disk - Stored Model : .If you have previously run a script like retrieve_with_LSA.pl and no intervening code has modified the disk - stored LSA model of the corpus , you can run the script . retrieve_with_disk_based_LSA.pl .", "label": "", "metadata": {}, "score": "67.07676"}
{"text": "T .A correlation matrix Z may be computed from the rows or columns of transformed term matrix X 130 .The columns and rows of correlation matrix Z correspond to terms that are the terms of the transformed term matrix .", "label": "", "metadata": {}, "score": "67.167984"}
{"text": "While the above themes have been explained in detail for illustrative purposes , the present invention should in no way be limited to those precise schemes .Many other wavelets and corresponding subband coding schemes have been generated in recent years , and the use of these schemes in the method of the present invention is fully contemplated by the present invention .", "label": "", "metadata": {}, "score": "67.24722"}
{"text": "Quantitative similarity values are defined by the cosine of the angle between the term vectors representing the terms in the semantic space .More importantly , this comparison is valid only in the defined semantic space for the corpus of information .", "label": "", "metadata": {}, "score": "67.81258"}
{"text": "Quantitative similarity values are defined by the cosine of the angle between the term vectors representing the terms in the semantic space .More importantly , this comparison is valid only in the defined semantic space for the corpus of information .", "label": "", "metadata": {}, "score": "67.81258"}
{"text": "Tipster is a corpus of documents controlled by the government ( NIST - National Institute of Standards and Technology ) to further information retrieval methods .Note that these computations could have been accelerated by using loosely coupled coarse - grained parallelism to effect a linear reduction in compute time .", "label": "", "metadata": {}, "score": "68.00501"}
{"text": "FIG .10 shows the process of computation of context vectors .In step 200 , the query or document is loaded into the processor 16 ( see FIG .1 ) .All of the words in the query or document are extracted in step 202 .", "label": "", "metadata": {}, "score": "68.01634"}
{"text": "The code .The TF - IDF is the product between the TF and IDF .So a high weight of the tf - idf is reached when you have a high term frequency ( tf ) in the given document and low document frequency of the term in the whole collection .", "label": "", "metadata": {}, "score": "68.086464"}
{"text": "Any rectangular matrix ( including square matrices such as matrix X ) can be decomposed into the product of three matrices : . such that matrices T.sub.0 and D.sub.0 have orthonormal columns and matrix S.sub.0 is diagonal .This is called the singular value decomposition of matrix X. Matrices T.sub.0 and D.sub.0 are the matrices of left and right singular vectors , respectively , and matrix S.sub.0 is the diagonal matrix of singular values .", "label": "", "metadata": {}, "score": "68.186005"}
{"text": "The method of claim 25 , wherein computing the correlation coefficient uses the following formula : # # EQU24 # # where . psi .( d.sub.i ) is the query vector and . psi .( d.sub.j ) is the document vector .", "label": "", "metadata": {}, "score": "68.231155"}
{"text": "In step 330 , the documents of the corpus are retrieved into the processor .The document vectors for each document are computed in step 332 by using equation 15 .The document vectors are stored in step 334 .In step 340 , the correlation coefficients between the computed document vectors and the factor vector are computed by using the following equation : # # EQU17 # # where . psi .", "label": "", "metadata": {}, "score": "68.29581"}
{"text": "claim 1 , wherein the columns and rows of the correlation matrix correspond to terms that are the terms of the transformed term matrix , and a concept comprises highly correlated terms .The method of .claim 1 , wherein terms of the correlation matrix comprising a correlation higher than the concept threshold become vertices of a concept graph .", "label": "", "metadata": {}, "score": "68.53432"}
{"text": "if you have root access .If not , .THANKS .Many thanks are owed to Shivani Rao and Bunyamin Sisman for sharing with me their deep insights in IR .Version 1.4 was prompted by Zahn Bozanic 's interest in similarity matrix characterization of a corpus .", "label": "", "metadata": {}, "score": "68.61315"}
{"text": "claim 1 , further comprising identifying one or more problems based on the frequency of occurrence of terms in the concept term set .A method analyzing a plurality of documents , comprising : . collecting and filtering terms from a plurality of documents ; . identifying a term - frequency vector for each of the documents ; . identifying a term - frequency matrix , wherein rows of the matrix comprise values for the term - frequency vectors ; . projecting the term - frequency matrix onto a lower dimensional space using latent semantic analysis , to create a transformed term matrix ; . developing a correlation matrix using the rows of the transformed term matrix ; . creating a dendrogram of related concepts using a function of the correlation matrix ; . identifying branches of the dendrogram corresponding to related concepts ; and .", "label": "", "metadata": {}, "score": "68.82084"}
{"text": "D -- a set of documents ; . d.sub.j--a document j in D ; . psi . --document encoding ; . vertline.d.sub.j .vertline .--the number of tokens in d.sub.j ; and . t.sub.j,k --the k.sup.th token in document d.sub.j .", "label": "", "metadata": {}, "score": "69.163956"}
{"text": "[ 0052 ] .It will be appreciated that various of the above - disclosed and other features and functions , or alternatives thereof , may be desirably combined into many other different systems or applications .Also that various presently unforeseen or unanticipated alternatives , modifications , variations or improvements therein may be subsequently made by those skilled in the art which are also intended to be encompassed by the following claims .", "label": "", "metadata": {}, "score": "69.17856"}
{"text": "None by design .SO THAT YOU DO NOT LOSE RELEVANCY JUDGMENTS .You have to be careful when carrying out Precision verses Recall calculations if you do not wish to lose the previously created relevancy judgments .Invoking the method estimate_doc_relevancies ( ) in your own script will cause the file relevancy.txt to be overwritten .", "label": "", "metadata": {}, "score": "69.66387"}
{"text": "[5 ] Isaac Persing and Alan Davis and Vincent Ng,'Modeling Organization in Student Essays',Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing , pp 229 - 239,MIT , Massachusetts , USA , pp 9 - 11 , 2010 .", "label": "", "metadata": {}, "score": "69.69862"}
{"text": "While this invention is described in some detail herein , with specific reference to illustrated embodiments , it is to be understood that there is no intent to be limited to these embodiments .On the contrary , the aim is to cover all the modifications , alternatives and equivalents falling within the spirit and scope of the invention as defined by the claims .", "label": "", "metadata": {}, "score": "69.73354"}
{"text": "The method of claim 25 , wherein the step of outputting comprises outputting , instead of the ranking for at least one document , at least one document according to its corresponding ranking .The method of claim 25 , further comprising outputting at least one document according to its ranking .", "label": "", "metadata": {}, "score": "70.04761"}
{"text": "There are fewer fourgram combinations than there are words in the English language .The informational significance of individual terms can be evaluated by looking at their nearest neighbors .The topical characteristics of the selected words are obvious from looking at the few nearest neighbors given .", "label": "", "metadata": {}, "score": "70.07327"}
{"text": "Appendix A shows the ten highest ranking documents for each query term in Table 1 .With the exception of Societies , the document titles generally correspond to the topic described by the nearest neighbors of the query term .The cause for the religious articles retrieved for Societies may be the use of this term in the phrase \" Center for Religion and Human Rights in Closed Societies .", "label": "", "metadata": {}, "score": "70.08079"}
{"text": "An elevation or ( x - y ) location can be specified from graphical user input to perform certain functions .For example , the user can specify the elevation , or energy level , used in selection of text breaks by GUI on the \" Waves \" visualization .", "label": "", "metadata": {}, "score": "70.18971"}
{"text": "If the word \" tank \" occurs in two documents , but water does n't , the document containing words related to water ( such as pipes or flush ) will be rated higher than the one pertaining to a different topic .", "label": "", "metadata": {}, "score": "70.30762"}
{"text": "Machine Learning with Python : Meeting TF - IDF for Text Mining .Monday , December 19 , 2011 .This month I was studying about information retrieval and text mining , specially how to convert the textual representation of information into a Vector Space Model ( VSM ) .", "label": "", "metadata": {}, "score": "70.38527"}
{"text": "From a more practical standpoint , there are many other requirements on the function to ensure that the resulting transform is useful .However , the requirements are quite variable depending on the application and the data \u0192(x ) that it will be applied to .", "label": "", "metadata": {}, "score": "70.409805"}
{"text": "This invention was made with Government support under Contract DE - AC06 - 76RL0 1830 awarded by the U.S. Department of Energy .The Government has certain rights in the invention .FIELD OF THE INVENTION .The present invention relates generally to a method for automatically partitioning an unstructured electronically formatted natural language document into its sub - topic structure and specifies a device that may be used to graphically display and interact with the sub - topic structure of the document .", "label": "", "metadata": {}, "score": "70.50256"}
{"text": "Although any methods , materials , and devices similar or equivalent to those described herein can be used in the practice or testing of embodiments of the invention , the preferred methods , materials , and devices are now described .All publications mentioned herein are incorporated by reference .", "label": "", "metadata": {}, "score": "70.64047"}
{"text": "Although any methods , materials , and devices similar or equivalent to those described herein can be used in the practice or testing of embodiments of the invention , the preferred methods , materials , and devices are now described .All publications mentioned herein are incorporated by reference .", "label": "", "metadata": {}, "score": "70.64047"}
{"text": "As used herein , the visualization of sub - topic structure includes an energy surface device called \" Waves \" and a topographical surface called \" Topic Islands \" .Also as described herein , this approach to sub - topic structure is called \" topic - o - graphy \" .", "label": "", "metadata": {}, "score": "70.67091"}
{"text": "The parameter break_camelcased_and_underscored when set causes the underscored and camel - cased words to be split .By default the parameter is set .So if you do n't want such words to be split , you must set it explicitly to 0 .", "label": "", "metadata": {}, "score": "70.74974"}
{"text": "0044 ] .In the following example , a corpus consists of about 1,200 logs from a customer support Each log is uniquely identified by a number .The task is to determine salient problems .Term set : spooler driver windows crashes printers crash XP .", "label": "", "metadata": {}, "score": "70.77367"}
{"text": "The second concept refers to a software problem in which a print spooler somehow uses too much memory and causes some software program to crash under the XP operating system .Further , we see two sub - concepts : crashes and memory and XP and spooler .", "label": "", "metadata": {}, "score": "70.826706"}
{"text": "The second concept refers to a software problem in which a print spooler somehow uses too much memory and causes some software program to crash under the XP operating system .Further , we see two sub - concepts : crashes and memory and XP and spooler .", "label": "", "metadata": {}, "score": "70.826706"}
{"text": "Hence depending on the requirements of an application - whether it requires high precision / recall , the appropriate methods can be chosen .ACKNOWLEDGEMENTS The authors thank Dr Anandarama Upadhyaya for his suggestions .International Journal of Artificial Intelligence & Applications ( IJAIA ) , Vol .", "label": "", "metadata": {}, "score": "70.92695"}
{"text": "phi . sub.0 is an order-0 vector for tokens t.sub.j,l . .The method of claim 14 , wherein the word vector is generated based on : # # EQU20 # # where . phi .phi .sub.1 is an order-1 vector for token t.sub.j,l . .", "label": "", "metadata": {}, "score": "71.03413"}
{"text": "While a preferred embodiment of the present invention has been shown and described , it will be apparent to those skilled in the art that many changes and modifications may be made without departing from the invention in its broader aspects .", "label": "", "metadata": {}, "score": "71.25581"}
{"text": "Work by Andrew Mccallum Et.al ( 2007 ) makes an attempt to clarify the confusion between Na\u00efve Bayesian models ; Multi variant Bernoulli model and multinomial model .They claim that multinomial model is better than the multi variant Bernoulli model .", "label": "", "metadata": {}, "score": "71.31227"}
{"text": "Snob found seven classes which correspond well to the male and female groups of those species that were represented in reasonable numbers - remember that Snob knows nothing about biology and was not told the sex of the seals .Since then , Snob has been applied to data from earth sciences , medicine , molecular biology , psychology and many other areas .", "label": "", "metadata": {}, "score": "71.32086"}
{"text": "The topic is identified as belonging to the general area of \" Science and Technology \" .Therefore , \" science \" is one of the terms of the query .However , it is not relevant for the query .One of the word factors of the topic 75 is the following : . failed ; instance ; force ; conversely ; science .", "label": "", "metadata": {}, "score": "71.43317"}
{"text": "For VSM - Based Retrieval with a Disk - Stored Model : .If you have previously run a script like retrieve_with_VSM.pl and no intervening code has modified the disk - stored VSM model of the corpus , you can run the script . retrieve_with_disk_based_VSM.pl .", "label": "", "metadata": {}, "score": "71.456535"}
{"text": "FIG .4 shows such a dendrogram for this example .In the example of .FIG .4 , we see that there are two major concepts , one associated with toner and puffs and another having to do with spooler , XP , crashes and memory .", "label": "", "metadata": {}, "score": "71.57414"}
{"text": "FIG .4 shows such a dendrogram for this example .In the example of .FIG .4 , we see that there are two major concepts , one associated with toner and puffs and another having to do with spooler , XP , crashes and memory .", "label": "", "metadata": {}, "score": "71.57414"}
{"text": "Two terms lexically co - occur if they appear in text within some distance of each other , i.e. , a window of k words .Qualitatively , the fact that two words often occur close to each other is more likely to be significant than the fact that they occur in the same documents .", "label": "", "metadata": {}, "score": "71.782425"}
{"text": "2.1 CORPUS A custom built corpus called TDIL(Technology for Development of Indian Languages ) is a comprehensive Kannada text resource , which is developed by Central Institute of Indian Languages ( CIIL ) .The text resource is manually categorized which makes it a ready source of data for this problem .", "label": "", "metadata": {}, "score": "71.866234"}
{"text": "The subject matter of the present invention is particularly pointed out and distinctly claimed in the concluding portion of this specification .However , both the organization and method of operation , together with further advantages and objects thereof , may best be understood by reference to the following description taken in connection with accompanying drawings wherein like reference characters refer to like elements .", "label": "", "metadata": {}, "score": "71.872314"}
{"text": "B4 contains words like \" navy \" , radar \" , and \" missile \" , while some of the member of class g.sub.B47 are \" tanks \" , \" missiles \" , and \" helicopters \" .If one of two words has many neighbors in g.sub.", "label": "", "metadata": {}, "score": "71.973076"}
{"text": "10 .The computed context vectors are stored by the processor in step 234 .The documents are retrieved by the processor in step 240 .This step can be performed before or in parallel with the query processing .The context vectors for each document are computed in step 242 .", "label": "", "metadata": {}, "score": "72.257095"}
{"text": "[ 0027 ] .Before the present methods , systems and materials are described , it is to be understood that this disclosure is not limited to the particular methodologies , systems and materials described , as these may vary .It is also to be understood that the terminology used in the description is for the purpose of describing the particular versions or embodiments only , and is not intended to limit the scope .", "label": "", "metadata": {}, "score": "72.390915"}
{"text": "He is also a member of various Board of Studies and Board of Examiners for different universities .His research areas include Image Processing , Document Image analysis , Pattern Recognition , Character Recognition , Data Mining and Artificial Intelligence .Basavaraj S Anami is presently working as the Principal , K.L.E 's Institute of Technology , Hubli , since August 2008 .", "label": "", "metadata": {}, "score": "72.42255"}
{"text": "Ney,'Reversing and Smoothing the Multinomial Naive Bayes Text Classifier',Work supported by the Spanish \" Ministerio de Ciencia y Tecnolog\u00b4ia \" under grant TIC2000 - 1703-CO3 - 01 .[ 2 ] Andrew McCallumzy , Kamal Nigamy,'A Comparison of Event Models for Naive Bayes Text Classication',AAAI-98 , Workshop on learning for Text Categorization.1998 .", "label": "", "metadata": {}, "score": "72.46479"}
{"text": "Paragraphs may use neighbouring paragraphs to get the class information .This can be captured to increase the performance of the classifier .The work can be made use of in customer reviews in Kannada blogs .It can also be used in extracting opinions in posted Kannada articles online .", "label": "", "metadata": {}, "score": "72.81598"}
{"text": "Those words are commonly called stop words and they are present in almost all documents , so it is not relevant for us .In portuguese we also have those stop words such as ( a , os , as , os , um , umas , que , etc . ) .", "label": "", "metadata": {}, "score": "72.88011"}
{"text": "The razor is rather vague , but it can be made precise and Minimum Message Length encoding ( MML ) does just that : .Note that prediction is a related but different problem to explanation and requires correctly weighting the predictions of many hypotheses to give optimal results .", "label": "", "metadata": {}, "score": "72.905075"}
{"text": "As will be apparent to those skilled in the art , other filters could also be used ; the optimal filter being dependant on the particular user needs .In the preferred embodiment of the present invention , a dilation factor of 2 is used .", "label": "", "metadata": {}, "score": "73.11546"}
{"text": "Another example used in IR is an algorithm for finding sub - topic structure in expository text that uses a moving window approach .[ Multi - Paragraph Segmentation Of Expository Text , Marti A. Hearst , ACL ' 94 , Las Cruces , NM].", "label": "", "metadata": {}, "score": "73.642136"}
{"text": "This solution is costly since parsing technology is required to determine head - modifier relations in sentences .It is also unclear to what extent words with similar heads or modifiers are good candidates for expansion .For example , adjectives referring to countries have similar heads ( \" the Japanese / Chilean capital \" , \" the Japanese / Chilean government \" ) , but adding \" Japanese \" to a query that contains \" Chilean \" will rarely produce good results .", "label": "", "metadata": {}, "score": "73.70552"}
{"text": "At Patents you can conduct a Patent Search , File a Patent Application , find a Patent Attorney , or search available technology through our Patent Exchange .Patents are available using simple keyword or date criteria .If you are looking to hire a patent attorney , you 've come to the right place .", "label": "", "metadata": {}, "score": "73.76532"}
{"text": "If there are two classes involved , then it is called binary classification , if a single class is involved , then it is single label classification ; if there are more classes involved then it is multi label classification .A value of T assigned to(dj , ci ) indicates a decision to file dj under ci , while a value of F indicates a decision not to file dj under ci .", "label": "", "metadata": {}, "score": "73.77202"}
{"text": "Causal models ( Bayesian networks ) are used by medical , biological and social scientists to explain a large variety of phenomena , e.g. does smoking cause lung cancer , or does increased public spending on education lead to a long - term increase in national wealth ?", "label": "", "metadata": {}, "score": "73.836395"}
{"text": "If you do not have root privileges , you can carry out a non - standard install the module in any directory of your choice by : .With a non - standard install , you may also have to set your PERL5LIB environment variable so that this module can find the required other modules .", "label": "", "metadata": {}, "score": "74.20525"}
{"text": "Synonyms rarely co - occur .Synonyms tend to share neighbors that occur with both .For example , \" litigation \" and \" lawsuit \" share neighbors such as \" court \" , \" judge \" , and \" proceedings \" .", "label": "", "metadata": {}, "score": "74.32956"}
{"text": "Unpack the archive with a command that on a Linux machine would look like : . tar zxvf Algorithm - VSM-1.70.tar.gz .This will create an installation directory for you whose name will be Algorithm - VSM-1.70 .Enter this directory and execute the following commands for a standard install of the module if you have root privileges : . perl Makefile .", "label": "", "metadata": {}, "score": "74.59045"}
{"text": "Thanks , Zahn !This library is free software ; you can redistribute it and/or modify it under the same terms as Perl itself .Copyright 2012 Avinash Kak . syntax highlighting : no syntax highlighting acid berries - dark berries - light bipolar blacknblue bright contrast cpan darkblue darkness desert dull easter emacs golden greenlcd ide - anjuta ide - codewarrior ide - devcpp ide - eclipse ide - kdev ide - msvcpp kwrite matlab navy nedit neon night pablo peachpuff print rand01 solarized - dark solarized - light style the typical vampire vim - dark vim whatis whitengrey zellner Algorithm::VSM --- A Perl module for retrieving files and documents from a software library with the VSM ( Vector Space Model ) and LSA ( Latent Semantic Analysis ) algorithms in response to search words .", "label": "", "metadata": {}, "score": "74.59105"}
{"text": "Understanding protein structure is important in medicine and drug design .Computer Science .The MML research group carries out research into Inductive Inference .Computer Science units in Computer Programming , Algorithms and Data Structures , Foundations of C.S. and Artificial Intelligence give a good background for this work .", "label": "", "metadata": {}, "score": "74.91645"}
{"text": "To get an idea of how well the term sets and log texts relate , here are fragments of two logs that . occur in all the clusters : .Fragment of log 918189799 : . \" Rob unable to load the latest driver from the web or the driver from his CD on either his lap- . top or one of the customer 's workstations .", "label": "", "metadata": {}, "score": "75.216446"}
{"text": "The morphological richness of the Kannada language has led to the feature dimensions to be in the order of tens of thousands .For practical classification considerations , large amounts of training samples are required to train the classifier .1)Using stopwords The first approach for dimensionality reduction is identifying and eliminating stop words .", "label": "", "metadata": {}, "score": "75.4991"}
{"text": "J. D. Patrick .An Information Measure Comparative Analysis of Megalithic Geometries .PhD Thesis , Computer Science , Monash University , 1979 .J. D. Patrick & C. S. Wallace .Stone circle geometries : an information theory approach . in Archaeoastronomy in the Old World , D.Heggie ( ed ) , C.U.P. , 1982 .", "label": "", "metadata": {}, "score": "75.629"}
{"text": "Addison Wesley , Cambridge , Mass. , 1949].A document frequency filter is used to omit words that occur too frequently to usefully discriminate the topics between pseudo - corpus windows .Some infrequent words may also be eliminated .", "label": "", "metadata": {}, "score": "75.98587"}
{"text": "It will be appreciated that various of the above - disclosed and other features and functions , or alternatives thereof , may be desirably combined into many other different systems or applications .Also that various presently unforeseen or unanticipated alternatives , modifications , variations or improvements therein may be subsequently made by those skilled in the art which are also intended to be encompassed by the following claims .", "label": "", "metadata": {}, "score": "76.260056"}
{"text": "I 'd like to mention the excellent post from the researcher Christian Perone at his blog Pyevolve about Machine learning and Text Mining with TF - IDF , a great post to read .Term Frequency - Inverse Document Frequency is a weighting scheme that is commonly used in information retrieval tasks .", "label": "", "metadata": {}, "score": "76.31883"}
{"text": "I am also interested in distributed computing , high performance and data visualization , educational and bioinformatics ventures .In 2014 , I assumed a new position at Genomika Diagn\u00f3sticos , a brazilian genetics tests laboratory , as CTO .Introduction to MML .", "label": "", "metadata": {}, "score": "76.38132"}
{"text": "While this method is superior to approaches that treat phrase terms as unanalyzed segments , there is no notion of semantic similarity of basic terms .For example , the semantic similarity of \" astronaut \" and \" cosmonaut \" is not represented in the hierarchy .", "label": "", "metadata": {}, "score": "76.42912"}
{"text": "The question is , are these elaborate theories correct , or are the \" circles \" simply inaccurate - made by ancient peoples with rough measuring instruments on rough ground ?MML comes down firmly in favour of the latter - they are just rough circles .", "label": "", "metadata": {}, "score": "76.55197"}
{"text": "Program instructions may be stored in the ROM 318 and/or the RAM 320 .Optionally , program instructions may be stored on a computer readable medium such as a floppy disk or a digital disk or other recording medium , a communications signal or a carrier wave .", "label": "", "metadata": {}, "score": "78.12196"}
{"text": "A group average agglomerative clustering was used to group query terms into factors based on their thesaurus vectors .Each topic was clustered into three word factors .All directly juxtaposed words occurring at least five times in the corpus were used as terms . trade conflicts - airbus subsidies , anti dumping , countervailing , countervailing duty , dumping , dumping duty , federal subsidies , gatt , general agreement , review group , subsidies , tariffs , trade dispute , trade policy , trade tension .", "label": "", "metadata": {}, "score": "78.16215"}
{"text": "The . customer does have several XP and Win2000 workstations that the driver will load on .On the . laptop and the other workstation , the driver is crashing the spooler .When he restarts the .window , the printer is there but , when selected , he gets a general protection fault .", "label": "", "metadata": {}, "score": "78.27083"}
{"text": "Many thanks are owed to Shivani Rao and Bunyamin Sisman for sharing with me their deep insights in IR .Version 1.4 was prompted by Zahn Bozanic 's interest in similarity matrix characterization of a corpus .Thanks , Zahn !Several of the recent changes to the module are a result of the feedback I have received from Naveen Kulkarni of Infosys Labs .", "label": "", "metadata": {}, "score": "79.118744"}
{"text": "International Journal of Artificial Intelligence & Applications ( IJAIA ) , Vol .4 , No . 5 , September 2013 129 Error Analysis here indicates confusion between Natural Sciences and Aesthetics classes and Commerce and Natural Sciences classes .The Aesthetics class seems to have low precision and high recall .", "label": "", "metadata": {}, "score": "79.24688"}
{"text": "Version 1.62 was a result of Slaven Rezic 's recommendation that I remove the Perl version restriction on the module since he was able to run it with Perl version 5.8.9 .Another important reason for v. 1.62 was the discovery of the two bugs mentioned in Changes , one of them brought to my attention by Naveen Kulkarni .", "label": "", "metadata": {}, "score": "79.641266"}
{"text": "These post were really helpful in understanding the meaning of text mining .It help in putting the unstructured text in a structured form .Welcome to Wiztech Automation - Embedded System Training in Chennai .We have knowledgeable Team for Embedded Courses handling and we also are after Job Placements offer provide once your Successful Completion of Course .", "label": "", "metadata": {}, "score": "79.831085"}
{"text": "To appreciate the operation of the present invention , it is useful to review some of the mathematical theory behind the wavelet transform .The continuous wavelet transform of a function f(x ) is defined as # # EQU1 # # where \u03c8(x ) is the wavelet .", "label": "", "metadata": {}, "score": "80.03491"}
{"text": "He was looking for Alex but just missed him .The . customer does have several XP and Win2000 workstations that the driver will load on .On the . laptop and the other workstation , the driver is crashing the spooler .", "label": "", "metadata": {}, "score": "80.5421"}
{"text": "With this background , we have made a new attempt to analyze how paragraph level text classification works for the Kannada language using Na\u00efve Bayesian and Na\u00efve Bayesian Multinomial methods .The rest of the paper is organized as follows . Section-", "label": "", "metadata": {}, "score": "80.68386"}
{"text": "Natural Sciences class has equal precision and recall values , which is an interesting pointer for further research .International Journal of Artificial Intelligence & Applications ( IJAIA ) , Vol .4 , No . 5 , September 2013 130 Error analysis in this case shows the confusion between the Natural Sciences and Commerce Classes and also Aesthetics and Commerce classes .", "label": "", "metadata": {}, "score": "80.7693"}
{"text": "Communication with external devices may optionally occur using various communication ports 326 .An exemplary communication port 326 may be attached to a communications network , such as the Internet or an intranet .In addition to the standard computer - type components , the hardware may also include an interface 312 which allows for receipt of data from input devices such as a keyboard 314 or other input device 316 such as a remote control , pointer and/or joystick .", "label": "", "metadata": {}, "score": "81.13007"}
{"text": "litigation LAWSUITS ; audit ; lawsuit ; file ; auditors ; auditor ; suit ; sued ; proceedings .tax taxes ; income tax ; new tax ; income taxes ; taxpayers ; incentives ; LEVIES ; taxpayer ; corporate taxes .", "label": "", "metadata": {}, "score": "81.13189"}
{"text": "The apparatus of claim 40 , wherein the memory is one of a RAM , a ROM and an external storage unit .The apparatus of claim 40 , wherein the query vector is a plurality of query factor vectors , each query factor vector based on a cluster factor of the query .", "label": "", "metadata": {}, "score": "81.372635"}
{"text": "window , the printer is there but , when selected , he gets a general protection fault .\" Fragment of log 406538483 : . \" Windows spooler crashes loading XP driver ----------------------- Recreation Steps and Problem .When the PC has or . had many different printers loaded on it , the Windows spooler will crash . \"", "label": "", "metadata": {}, "score": "81.81904"}
{"text": "An optional display interface 322 may permit information from the bus 328 to be displayed on the display 324 in audio , graphic or alphanumeric format .Communication with external devices may optionally occur using various communication ports 326 .An exemplary communication port 326 may be attached to a communications network , such as the Internet or an intranet .", "label": "", "metadata": {}, "score": "82.337944"}
{"text": "The nearest neighbors of the pool ( tank , artillery ) are printed in the section SEARCH RESULT .By inspecting these neighbors , the user can make sure that the \" receptacle \" sense of the word \" tank \" will not interfere with a query on military equipment .", "label": "", "metadata": {}, "score": "82.68921"}
{"text": "The trouble is that an unlimited number of theories could explain some given facts .Which is the best one ?William of Ockham ( 1285 - 1349 ) is often credited with saying something like , \" if two theories explain the facts equally well then the simpler theory is to be preferred \" .", "label": "", "metadata": {}, "score": "83.93009"}
{"text": "FIG .6 , a bus 328 serves as the main information highway interconnecting the other illustrated components of the hardware .CPU 302 is a central processing unit of the system , performing calculations and logic operations required to execute a program .", "label": "", "metadata": {}, "score": "84.547165"}
{"text": "FIG .6 , a bus 328 serves as the main information highway interconnecting the other illustrated components of the hardware .CPU 302 is a central processing unit of the system , performing calculations and logic operations required to execute a program .", "label": "", "metadata": {}, "score": "84.547165"}
{"text": "The function words of a language are usually identified as stop words .These words are considered as noise and are removed before the classification process .There is no standard stop word list available in the Kannada language .Hence , we sought the help of subject expert in Kannada for identifying stop words in our corpus manually .", "label": "", "metadata": {}, "score": "84.72204"}
{"text": "Tech . , ' 89 was .Department of Computer Science , .Fac .Sci . , ' 68-'71 was .Department of Information Science , .Fac .Sci . ) fetched Wednesday , 10-Feb-2016 19:40:51 EST .", "label": "", "metadata": {}, "score": "84.77787"}
{"text": "The study also includes standard microcontrollers such as Intel 8051 , PIC , AVR , ARM , ARMCotex , Arduino , etc . .Hi admin thanks for sharing informative article on hadoop technology .In coming years , hadoop and big data handling is going to be future of computing world .", "label": "", "metadata": {}, "score": "85.60173"}
{"text": "Free Accommodation , Individual Focus , Best Lab facilities , 100 % Practical Training and Job opportunities .WIZTECH Automation , Anna Nagar , Chennai , has earned reputation offering the best automation training in Chennai in the field of industrial automation .", "label": "", "metadata": {}, "score": "85.70325"}
{"text": "R is an Associate Professor in the Department of Computer Science and Engineering , at PES Institute of Technology , Bangalore .She has over 18 Years of teaching experience .She has published several papers in international conferences and journals .", "label": "", "metadata": {}, "score": "85.76884"}
{"text": "You only have partial information and your opponents are actively trying to deceive you .Are they bluffing ?It is easy to calculate the probability that a good hand or a bad hand was dealt to someone .But your opponents know that you can do that .", "label": "", "metadata": {}, "score": "85.93414"}
{"text": "In order to install this module in a Linux machine on which I use tcsh for the shell , I set the PERL5LIB environment variable by .setenv PERL5LIB /some / other / directory / lib64/perl5/:/some / other / directory / share / perl5/ .", "label": "", "metadata": {}, "score": "86.49431"}
{"text": "[0048 ] .Program instructions may be stored in the ROM 318 and/or the RAM 320 .Optionally , program instructions may be stored on a computer readable medium such as a floppy disk or a digital disk or other recording medium , a communications signal or a carrier wave .", "label": "", "metadata": {}, "score": "86.77438"}
{"text": "For all issues related to this module , contact the author at kak@purdue.edu .If you send email , please place the string \" VSM \" in your subject line to get past the author 's spam filter .This library is free software ; you can redistribute it and/or modify it under the same terms as Perl itself .", "label": "", "metadata": {}, "score": "87.38544"}
{"text": "A processor 16 is connected to the input device 12 for processing the document image into co - occurrence vectors and comparing the vectors .Processor 16 operates to perform these functions in accordance with operating programs read from read only memory ( ROM ) 20 , and by using random access memory ( RAM ) 22 .", "label": "", "metadata": {}, "score": "87.63048"}
{"text": "See : C. S. Wallace & K. B. Korb .Learning causal models by MML sampling .In Causal Models and Intelligent Data Management , . A. Gammerman ( ed ) , Springer Verlag , 1998 .There are dozens of stone circles in the British Isles .", "label": "", "metadata": {}, "score": "87.68631"}
{"text": "In addition to the standard computer - type components , the hardware may also include an interface 312 which allows for receipt of data from input devices such as a keyboard 314 or other input device 316 such as a remote control , pointer and/or joystick .", "label": "", "metadata": {}, "score": "89.83795"}
{"text": "Then he completed his M.Tech in Computer Science at IIT Madras in March 1986 .Later he received his Doctrine(PhD ) in Computer Science at University of Mysore in January 2003 .He began his academic journey as the Lecturer in Electrical department in BEC , Bagalkot from September 1983 up to December 1985 .", "label": "", "metadata": {}, "score": "92.276215"}
{"text": "The Poker project is a test - bed for machine learning methods such as Bayesian networks .( Bayes published ' An Essay Towards Solving a Problem in the Doctrine of Chances ' in 1763 . )Molecular Biology .Molecular - Biology data contain experimental error and random \" noise \" which make analysis difficult .", "label": "", "metadata": {}, "score": "92.37344"}
{"text": "We apply dimensionality reduction technique using Minimum term frequency , stop word identification and elimination methods for achieving the task .It is evident that Na\u00efve Bayesian Multinomial model outperforms simple Na\u00efve Bayesian approach in paragraph classification tasks .Suitability of na\u00efve bayesian methods for paragraph level text classification in the kannada language using dimensionality reduction technique .", "label": "", "metadata": {}, "score": "94.09425"}
{"text": "The Information Retrieval ( IR ) techniques act as an aid in assisting users in obtaining relevant information .IR in the Indian context is very relevant as there are several blogs , news publications in Indian languages present online .This work looks at the suitability of Na\u00efve Bayesian methods for paragraph level text classification in the Kannada language .", "label": "", "metadata": {}, "score": "94.6013"}
{"text": "The Information Retrieval ( IR ) techniques act as an aid in assisting users in obtaining relevant information .IR in the Indian context is very relevant as there are several blogs , news publications in Indian languages present online .This work looks at the suitability of Na\u00efve Bayesian methods for paragraph level text classification in the Kannada language .", "label": "", "metadata": {}, "score": "94.6013"}
{"text": "If interested , check out his web page at Purdue to find out what the Objects Trilogy project was all about .You might like \" Designing with Objects \" especially if you enjoyed reading Harry Potter as a kid ( or even as an adult , for that matter ) .", "label": "", "metadata": {}, "score": "95.65699"}
{"text": "The candidates are given enhanced job oriented practical training in all major brands of PLCs ( AB , Keyence , ABB , GE - FANUC , OMRON , DELTA , SIEMENS , MITSUBISHI , SCHNEIDER , and MESSUNG ) .Embedded system training : Wiztech Automation Provides Excellent training in embedded system training in Chennai - IEEE Projects - Mechanical projects in Chennai .", "label": "", "metadata": {}, "score": "95.995575"}
{"text": "AUTHOR .Avinash Kak , kak@purdue.edu .If you send email , please place the string \" VSM \" in your subject line to get past my spam filter .COPYRIGHT .This library is free software ; you can redistribute it and/or modify it under the same terms as Perl itself .", "label": "", "metadata": {}, "score": "99.46806"}
{"text": "Dr .K.Srikanta Murthy is a Professor and Head , Department of Computer Science and Engineering , at PES School of Engineering , Bangalore .He has put in 24 years of service in teaching and 5 years in research .He has published 8 papers in reputed International Journals ; 41 papers at various International Conferences .", "label": "", "metadata": {}, "score": "99.96385"}
{"text": "The user can also check whether a proper name like \" Eschenbach \" is used for a specific person in the corpus ( here it is the conductor Christopher Eschenbach ) .If there were a tennis player of the same name , then it would make Eschenbach less useful in a search for documents on classical music .", "label": "", "metadata": {}, "score": "101.28831"}
{"text": "interfaith effort aims to ease children 's suffering .blahoslav s. hruby , presbyterian minister , dies at 78 . rabbi kelman , leader of conservative judaism , dies at 66 . greek orthodox group wants to permit married bishops .vatican , jewish groups see need to fight anti - semitism in east .", "label": "", "metadata": {}, "score": "106.29575"}
{"text": "Monash University , .Australia 3800 ( 6/'05 was .School of Computer Science and Software Engineering , .Fac .Info .Tech . , .Monash University , was .Department of Computer Science , .Fac .Comp .", "label": "", "metadata": {}, "score": "107.20755"}
{"text": "Thus , taking Hadoop & Spark Training in Hyderabad will help you to enter big data hadoop & spark technology .Search in this blog .Join the Brazilian Python Conference PythonBrasil 2013 .Marcel Caraciolo .I am a brazilian data scientist , entrepreneur , python hacker and technology consultant .", "label": "", "metadata": {}, "score": "110.696625"}
{"text": "[ 0047 ] .A disk controller 304 may interface with one or more optional disk drives to the system bus 328 .These disk drives may be external or internal memory keys , zip drives , flash memory devices , floppy disk drives or other memory media such as 310 , CD ROM drives 306 , or external or internal hard drives 308 .", "label": "", "metadata": {}, "score": "114.853775"}
{"text": "A disk controller 304 may interface with one or more optional disk drives to the system bus 328 .These disk drives may be external or internal memory keys , zip drives , flash memory devices , floppy disk drives or other memory media such as 310 , CD ROM drives 306 , or external or internal hard drives 308 .", "label": "", "metadata": {}, "score": "117.32551"}
{"text": "pope issue key document on roman catholic higher education .churches , s.f . mayor begin new push to enlist aids volunteers .leader of eastern orthodox christians begins visit to u.s . .leader of easter orthodox christians begins visit to u.s . .", "label": "", "metadata": {}, "score": "118.03213"}
