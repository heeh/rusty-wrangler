{"text": "A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence .The strength of our approach is that it allows a tree to be represented as an arbitrary set of features , without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account .", "label": "", "metadata": {}, "score": "31.924717"}
{"text": "However , this may not have been the central motivation for introducing deep structure .Transformations had been proposed prior to the development of deep structure as a means of increasing the mathematical and descriptive power of context - free grammars .", "label": "", "metadata": {}, "score": "33.596462"}
{"text": "However , this was perhaps not the central motivation for introducing deep structure .Transformations had been proposed prior to the development of deep structure as a means of increasing the mathematical and descriptive power of Context - free grammars .Similarly , deep structure was devised largely for technical reasons relating to early semantic theory .", "label": "", "metadata": {}, "score": "33.679672"}
{"text": "The parallel sentence pairs which can not be converted from the one to the other by using the above fifteen atomic paraphrasing classes will be filtered out .The collected sentence pairs are associated with one or more feature functions defined above .", "label": "", "metadata": {}, "score": "34.181355"}
{"text": "The strength of our approach is that it allows a tree to be represented as an arbitrary set of features , without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account .", "label": "", "metadata": {}, "score": "34.652214"}
{"text": "Chomsky believed that there would be considerable similarities between languages ' deep structures , and that these structures would reveal properties , common to all languages , which were concealed by their surface structures .However , this was perhaps not the central motivation for introducing deep structure .", "label": "", "metadata": {}, "score": "37.15686"}
{"text": "Chomsky believed that there would be considerable similarities between languages ' deep structures , and that these structures would reveal properties , common to all languages , which were concealed by their surface structures .However , this was perhaps not the central motivation for introducing deep structure .", "label": "", "metadata": {}, "score": "37.15686"}
{"text": "Returning to the more general mathematical notion of a grammar , an important feature of all transformational grammars is that they are more powerful than context free grammars .[ 12 ] This idea was formalized by Chomsky in the Chomsky hierarchy .", "label": "", "metadata": {}, "score": "37.429268"}
{"text": "Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand - annotated training data . \" ...Discriminative reranking is one method for constructing high - performance statistical parsers ( Collins , 2000 ) .", "label": "", "metadata": {}, "score": "37.731815"}
{"text": "These results suggest that the features used are sufficient for determining the hierarchical structure of texts and the nuclearity statuses of discourse segments .[0165 ] .Alternative embodiments of the discourse parser and its parsing procedure are possible .For example , probabilities could be incorporated into the process that builds the discourse trees .", "label": "", "metadata": {}, "score": "37.919167"}
{"text": "In that case , generating the set of decision rules may include applying a learning algorithm to the plurality of learning cases .Moreover , one or more features may be associated with each of the learning cases to reflect context .", "label": "", "metadata": {}, "score": "38.404156"}
{"text": "[ 0250 ] .Next , in step 1904 , the input text is parsed to produce a syntactic tree in the style of FIG .11 .If a full text is used , one can use a discourse parse to build the discourse tree of the text .", "label": "", "metadata": {}, "score": "38.499763"}
{"text": "[ 0006 ] This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description .This Summary is not intended to identify key features or essential features of the claimed subject matter , nor is it intended to be used as an aid in determining the scope of the claimed subject matter .", "label": "", "metadata": {}, "score": "38.58129"}
{"text": "Semantic - Similarity - Based Features .[ 0142 ] .Semantic - similarity - based features include the following : . [0143 ] .( 1 ) Features that denote the semantic similarity between the textual segments subsumed by the trees in focus .", "label": "", "metadata": {}, "score": "39.26104"}
{"text": "Moreover , the time , expense , and inconsistencies associated with manually built discourse tree derivation rules are reduced dramatically .[ 0020 ] .The ability to automatically derive discourse trees is useful not only in its standalone form ( e.g. , as a tool for linguistic researchers ) but also as a component of a larger system , such as a discourse - based machine translation system .", "label": "", "metadata": {}, "score": "39.499695"}
{"text": "Lexical features include the following : .[ 0137 ] .( 1 ) Features that denote the actual words and POS tags of the first and last two lexemes of the text spans subsumed by the trees in focus .[ 0138 ] .", "label": "", "metadata": {}, "score": "39.77028"}
{"text": "We present a system for identifying the semantic relationships , or semantic roles , filled by constituents of a sentence within a semantic frame .Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand - annotated training data .", "label": "", "metadata": {}, "score": "39.87182"}
{"text": "[ 0214 ] .Next , a set of high - scoring trees is extracted from the forest , taking into account both expansion - template probabilities and word - bigram probabilities .A generic extractor such as described by I. Langkilde , \" Forest - based statistical sentence generation , \" Proceedings of the 1 st Annual Meeting of the North American Chapter of the Association for Computational Linguistics ( 2000 ) can be used for this purpose .", "label": "", "metadata": {}, "score": "40.873775"}
{"text": "This completes the building and training of the paraphrasing model .The final paraphrasing model may be then incorporated in a paraphrasing program for application .[ 0129 ]According to one aspect of the present atomic paraphrasing technique , multiple atomic paraphrasing pairs each having atomic linguistic element and a matching atomic paraphrasing element are identified and evaluated using individual feature functions described above that are compatible with the respective atomic paraphrasing pair .", "label": "", "metadata": {}, "score": "40.935413"}
{"text": "Similarly , deep structure was devised largely for technical reasons relating to early semantic theory .Chomsky emphasizes the importance of modern formal mathematical devices in the development of grammatical theory : .But the fundamental reason for [ the ] inadequacy of traditional grammars is a more technical one .", "label": "", "metadata": {}, "score": "40.94609"}
{"text": "Similarly , deep structure was devised largely for technical reasons relating to early semantic theory .Chomsky emphasizes the importance of modern formal mathematical devices in the development of grammatical theory : .But the fundamental reason for [ the ] inadequacy of traditional grammars is a more technical one .", "label": "", "metadata": {}, "score": "40.94609"}
{"text": "The issues of consistency of argument structure across both polysemous and synonymous verbs are also discussed and we present our actual guidelines for these types of phenomena , along with numerous examples of tagged sentences and verb frames .We conclude with a summary of the current status of annotation process .", "label": "", "metadata": {}, "score": "41.029167"}
{"text": "Finally , SC ( S OUT , S IN ) is represented as a log linear function of the features being involved , as expressed as follows : .[ 0035 ]The task of building a paraphrasing model is divided into three subtasks : .", "label": "", "metadata": {}, "score": "41.045105"}
{"text": "[ 0221 ] .Next , in step 1704 , the input text is parsed to produce a syntactic tree in the style of FIG .11 , which is used in step 1706 as the basis of generating multiple possible solutions ( e.g. , the shared - forest structure described above ) .", "label": "", "metadata": {}, "score": "41.233036"}
{"text": "These may include the number of immediate children of the root nodes , the rhetorical relations that link the immediate children of the root nodes , and the like .[ 0135 ] .Lexical ( Cue - Phrase - Like ) and Syntactic Features .", "label": "", "metadata": {}, "score": "41.80187"}
{"text": "0099 ] .Next , in step 604 , for each lexeme , a set of one or more features was associated to each of the edu boundary decisions , based on the context in which these decisions were made .The result of such association is a set of learning cases - essentially , discrete instances that capture the edu - boundary decision - making process for a particular lexeme in a particular context .", "label": "", "metadata": {}, "score": "41.91003"}
{"text": "To each learning example , a set of 99 features was associated from the following two classes : operation features and original - tree - specific features .[ 0241 ] .Operational features reflect the number of trees in the stack , the input list , and the types of the last five operations performed .", "label": "", "metadata": {}, "score": "42.184162"}
{"text": "In 1998 , Chomsky suggested that derivations proceed in phases .The distinction of Deep Structure vs. Surface Structure is not present in Minimalist theories of syntax , and the most recent phase - based theories also eliminate LF and PF as unitary levels of representation .", "label": "", "metadata": {}, "score": "42.330704"}
{"text": "But the fundamental reason for [ the ] inadequacy of traditional grammars is a more technical one .Although it was well understood that linguistic processes are in some sense \" creative \" , the technical devices for expressing a system of recursive processes were simply not available until much more recently .", "label": "", "metadata": {}, "score": "43.035057"}
{"text": "[ need quotation to verify ] Chomsky developed a formal theory of grammar where transformations manipulated not just the surface strings , but the parse tree associated with them , making transformational grammar a system of tree automaton .( For more details see the Transformations section below . )", "label": "", "metadata": {}, "score": "43.15647"}
{"text": "In one embodiment , the paraphrasing of classes 2 - 4 is modeled as a two step procedure : ( i ) transform dependency tree of the original sentence into a new dependency tree ; and ( ii ) generate paraphrased sentences using the new dependency tree .", "label": "", "metadata": {}, "score": "43.238647"}
{"text": "1 , the process of automatic paraphrasing may use a paraphrasing program which incorporates a paraphrasing model built and trained as described above .In each specific instance of paraphrasing application on an input text , usually some but not all of the above defined feature functions are applicable .", "label": "", "metadata": {}, "score": "43.27381"}
{"text": "This paper presents a new approach to phrase - level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions .With this approach , the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions , achieving results that are significantly better than baseline . ...", "label": "", "metadata": {}, "score": "43.277023"}
{"text": "Moreover , the incremental building of the discourse tree is based on predetermined decision rules , such as automatically learned decision rules generated by analyzing a training set of annotated discourse trees .[ 0018 ] .The system may further include a discourse segmenter for partitioning the input text segment into edus and inserting the edus into the input list .", "label": "", "metadata": {}, "score": "43.340622"}
{"text": "3 is a list of fifteen exemplary paraphrasing transformation classes .[ 0011 ] FIG .4 is a flowchart of an exemplary process of acquiring semantically related lexicons using the algorithm of mutual induction for paraphrasing patterns and lexical relations .", "label": "", "metadata": {}, "score": "43.406662"}
{"text": "[ 0038 ] .The details of one or more embodiments are set forth in the accompanying drawings and the description below .Other features , objects , and advantages will be apparent from the description and drawings , and from the claims .", "label": "", "metadata": {}, "score": "43.718063"}
{"text": "In 1998 , Chomsky suggested that derivations proceed in \" phases \" .The distinction of Deep Structure vs. Surface Structure is not present in Minimalist theories of syntax , and the most recent phase - based theories also eliminate LF and PF as unitary levels of representation .", "label": "", "metadata": {}, "score": "44.082703"}
{"text": "In 1998 , Chomsky suggested that derivations proceed in \" phases \" .The distinction of Deep Structure vs. Surface Structure is not present in Minimalist theories of syntax , and the most recent phase - based theories also eliminate LF and PF as unitary levels of representation .", "label": "", "metadata": {}, "score": "44.082703"}
{"text": "In 1998 , Chomsky suggested that derivations proceed in \" phases \" .The distinction of Deep Structure vs. Surface Structure is not present in Minimalist theories of syntax , and the most recent phase - based theories also eliminate LF and PF as unitary levels of representation .", "label": "", "metadata": {}, "score": "44.082703"}
{"text": "[ 0199 ] .Learning Model Parameters For Channel - Based Summarizer .[ 0200 ] .Expansion - template probabilities were collected from parallel corpus .First , both sides of the parallel corpus were parsed , and then corresponding syntactic nodes were identified .", "label": "", "metadata": {}, "score": "44.272503"}
{"text": "\" The identifier relies on the following classes of features : structural features , lexical ( cue - phase - like ) features , operational features , and semantic - similarity - based features .Each is described in turn .[ 0131 ] .", "label": "", "metadata": {}, "score": "44.34468"}
{"text": "Consequently , the linguist can study an idealised version of language , greatly simplifying linguistic analysis ( see the \" Grammaticalness \" section below ) .The second idea related directly to the evaluation of theories of grammar .Chomsky made a distinction between grammars which achieved descriptive adequacy and those which went further and achieved explanatory adequacy .", "label": "", "metadata": {}, "score": "44.477158"}
{"text": "Consequently , the linguist can study an idealised version of language , greatly simplifying linguistic analysis ( see the \" Grammaticalness \" section below ) .The second idea related directly to the evaluation of theories of grammar .Chomsky made a distinction between grammars which achieved descriptive adequacy and those which went further and achieved explanatory adequacy .", "label": "", "metadata": {}, "score": "44.477158"}
{"text": "Consequently , the linguist can study an idealised version of language , greatly simplifying linguistic analysis ( see the \" Grammaticalness \" section below ) .The second idea related directly to the evaluation of theories of grammar .Chomsky made a distinction between grammars which achieved descriptive adequacy and those which went further and achieved explanatory adequacy .", "label": "", "metadata": {}, "score": "44.477158"}
{"text": "This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence .", "label": "", "metadata": {}, "score": "44.67887"}
{"text": "We show how the algorithms can be efficiently applied to exponential sized representations of parse trees , such as the \" all subtrees \" ( DOP ) representation described by ( Bod 98 ) , or a representation tracking all sub - fragments of a tagged sentence .", "label": "", "metadata": {}, "score": "44.78145"}
{"text": "In this paper , we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured .We also give an overview of the parsing approaches that participants took and the results that they achieved .", "label": "", "metadata": {}, "score": "45.10583"}
{"text": "Consequently , the linguist can study an idealised version of language , greatly simplifying linguistic analysis ( see the \" Grammaticality \" section below ) .The second idea related directly to the evaluation of theories of grammar .Chomsky distinguished between grammars that achieve descriptive adequacy and those that go further and achieved explanatory adequacy .", "label": "", "metadata": {}, "score": "45.174625"}
{"text": "For example , the resulting compressed syntactic / discourse tree could be supplied to a tree rewriter to convert it into another form , e.g. , to translate it into a target language .[ 0252 ] .[0252]FIG .20 shows a generalized process for training a decision - based summarizer .", "label": "", "metadata": {}, "score": "45.194984"}
{"text": "Although the subject matter has been described in language specific to structural features and/or methodological acts , it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described .", "label": "", "metadata": {}, "score": "45.570137"}
{"text": "[0117 ] Paraphrasing of classes 11 - 15 involves acquisition of semantically related lexicons .Both the atomic linguistic element and corresponding atomic paraphrasing element are patterns that may be learned .One embodiment proposes a unique mutual induction algorithm to learn atomic paraphrasing patterns and lexical relations of classes 11 - 15 .", "label": "", "metadata": {}, "score": "45.684704"}
{"text": "[ 0095 ] ( c ) learn transformation rules between dependency trees ; and .[ 0096 ] ( d ) learn the sentence generation model given a dependency tree .One embodiment implements the tree transformation rule learning algorithm and dependency tree based sentence generation algorithm described in Chris Quirk , Arul Menezes , and Colin Cherry , 2004 ( Dependency Tree Translation : Syntactically Informed Phrasal SMT , Microsoft Research Technical Report : MSR - TR-2004 - 113 ) .", "label": "", "metadata": {}, "score": "45.763718"}
{"text": "Chomsky argued that , even though linguists were still a long way from constructing descriptively adequate grammars , progress in terms of descriptive adequacy will only come if linguists hold explanatory adequacy as their goal .In other words , real insight into the structure of individual languages can only be gained through comparative study of a wide range of languages , on the assumption that they are all cut from the same cloth .", "label": "", "metadata": {}, "score": "45.908634"}
{"text": "If a sufficiently large number of texts were labeled manually , however , the clustering described above would be unnecessary .[ 0096 ] .In developing the discourse parser , one design parameter was to automatically derive rhetorical structures trees that were labeled with relation names that corresponded to the 17 clusters of rhetorical similarity .", "label": "", "metadata": {}, "score": "45.911156"}
{"text": "\" ...We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .", "label": "", "metadata": {}, "score": "45.99129"}
{"text": "Also , the entire text is parsed using the discourse parser .[0226 ] .Next , in step 1806 , the resulting parse tree pairs are compared - that is , the discourse or syntactic parse tree for a long segment is compared against the discourse or syntactic parse tree for its paired short segment - to identify similarities and differences between nodes of the tree pairs .", "label": "", "metadata": {}, "score": "46.239925"}
{"text": "Next , in step 2008 , the training process 2000 associates features ( e.g. , operational and original - tree - specific features ) with the learning cases to reflect the context in which the operations are to be performed .[ 0257 ] .", "label": "", "metadata": {}, "score": "46.25367"}
{"text": "The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .", "label": "", "metadata": {}, "score": "46.331703"}
{"text": "The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .", "label": "", "metadata": {}, "score": "46.331703"}
{"text": "The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .", "label": "", "metadata": {}, "score": "46.331703"}
{"text": "0007 ] .In previous work , such relationships have been referred to as paraphrases or variants ( Sparck Jones , K. and Tait , J. I. 1984 .Automatic Search Term Variant Generation .Journal of Documentation , 40(1):50 - 66 . ; Fabre , C. and Jacquemin , C. 2000 .", "label": "", "metadata": {}, "score": "46.76795"}
{"text": "[ 12 ] This idea was formalized by Chomsky in the Chomsky hierarchy .Chomsky argued that it is impossible to describe the structure of natural languages using context free grammars .[ 13 ] His general position regarding the non - context - freeness of natural language has held up since then , although his specific examples regarding the inadequacy of CFGs in terms of their weak generative capacity were later disproven .", "label": "", "metadata": {}, "score": "46.76908"}
{"text": "[ 12 ] This idea was formalized by Chomsky in the Chomsky hierarchy .Chomsky argued that it is impossible to describe the structure of natural languages using context free grammars .[ 13 ] His general position regarding the non - context - freeness of natural language has held up since then , although his specific examples regarding the inadequacy of CFGs in terms of their weak generative capacity were later disproven .", "label": "", "metadata": {}, "score": "46.76908"}
{"text": "[ 0091 ] Classes 2 - 4 of atomic paraphrasing transformation are active and passive exchange , reordering of sentence components , and realization in different syntactic categories , respectively .Paraphrasing of classes 2 - 4 mainly involves word re - ordering following a set of syntactic patterns .", "label": "", "metadata": {}, "score": "46.798523"}
{"text": "More generally , determining boundaries in the input text segment may include recognizing sentence boundaries , edu boundaries , parenthetical starts , and parenthetical ends .Examining each lexeme in the input text segment may include associating features with the lexeme based on surrounding context .", "label": "", "metadata": {}, "score": "46.8006"}
{"text": "In such a case , the discourse tree of a text will be taken to be the resulting tree of maximum probability .An advantage of such an approach is that it enables the creation of multiple trees , each one having associated a probability . [", "label": "", "metadata": {}, "score": "46.807533"}
{"text": "[ 15 ] This idea was formalized by Chomsky in the Chomsky hierarchy .Chomsky argued that it is impossible to describe the structure of natural languages using context - free grammars .[ 16 ] His general position regarding the non - context - freeness of natural language has held up since then , although his specific examples regarding the inadequacy of CFGs in terms of their weak generative capacity were later disproven .", "label": "", "metadata": {}, "score": "46.810497"}
{"text": "The resulting syntactic / discourse tree can be used for various purposes , for example , it can be rendered into a compressed text segment and output to a user ( e.g. , either a human end - user or a computer process ) .", "label": "", "metadata": {}, "score": "46.86325"}
{"text": "[ 0005 ] In some embodiments , a variety of atomic transformation types are identified to form atomic paraphrasing pairs .The atomic transformations and appropriate feature functions are acquired and trained to build atomic paraphrasing models which are used for selecting and evaluating candidate atomic paraphrasing pairs , and for scoring various combinations of candidate atomic paraphrasing pairs .", "label": "", "metadata": {}, "score": "46.893154"}
{"text": "Chomsky argued that , even though linguists were still a long way from constructing descriptively adequate grammars , progress in terms of descriptive adequacy would only come if linguists held explanatory adequacy as their goal .In other words , real insight into the structure of individual languages could only be gained through the comparative study of a wide range of languages , on the assumption that they are all cut from the same cloth .", "label": "", "metadata": {}, "score": "46.907158"}
{"text": "Chomsky argued that , even though linguists were still a long way from constructing descriptively adequate grammars , progress in terms of descriptive adequacy would only come if linguists held explanatory adequacy as their goal .In other words , real insight into the structure of individual languages could only be gained through the comparative study of a wide range of languages , on the assumption that they are all cut from the same cloth .", "label": "", "metadata": {}, "score": "46.907158"}
{"text": "Chomsky argued that , even though linguists were still a long way from constructing descriptively adequate grammars , progress in terms of descriptive adequacy would only come if linguists held explanatory adequacy as their goal .In other words , real insight into the structure of individual languages could only be gained through the comparative study of a wide range of languages , on the assumption that they are all cut from the same cloth .", "label": "", "metadata": {}, "score": "46.907158"}
{"text": "Accordingly , the following feature function is defined : .( 13 ) # # EQU00005 # # .[ 0100 ] Class 6 : The following describes exemplary lexicon acquisition for class 6 .[ 0101 ] The class 6 atomic paraphrasing transformation is pre - positional phrase attachment .", "label": "", "metadata": {}, "score": "46.982925"}
{"text": "A variety of atomic transformation types are identified to form atomic paraphrasing pairs .The candidate atomic paraphrasing pairs are evaluated using feature functions and a probability model .The principled approach scores a combination of multiple candidate atomic paraphrasing pairs using a score function which derives its value from the feature functions of the candidate atomic paraphrasing pairs .", "label": "", "metadata": {}, "score": "47.140778"}
{"text": "2 ] .[ 0154 ] .When this rule is applied in conjunction with the edts that correspond to the units marked in 5.3 , the resulting tree has the same shape as the tree shown in FIG .8C. [ 0155 ] .", "label": "", "metadata": {}, "score": "47.16458"}
{"text": "( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .", "label": "", "metadata": {}, "score": "47.180405"}
{"text": "( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .", "label": "", "metadata": {}, "score": "47.180405"}
{"text": "The systems and techniques described here result in a summarization system that can take virtually any longer text segment ( sentence , phrase , paragraph or treatise ) and compress it into a shorter version that is both grammatical and coherent .", "label": "", "metadata": {}, "score": "47.213596"}
{"text": "Semantic paraphrasing refers to non - decomposable combination of lexical substitution and syntactic variation .The present atomic paraphrasing method recognizes multiple major paraphrasing transformation classes in each of these levels .[ 0041 ] FIG .3 is a list of fifteen exemplary paraphrasing transformation classes .", "label": "", "metadata": {}, "score": "47.21999"}
{"text": "0008 ] .In conventional practice , discourse trees either have been generated by hand by trained personnel or have been pieced together in a semi - automated manner using manually generated instructions for a computer program .Development of the discourse parsing systems and techniques described below was based in part on the recognition that manually generating discourse trees in either of these fashions is time - consuming , expensive and prone to inconsistencies and error .", "label": "", "metadata": {}, "score": "47.227783"}
{"text": "132 - 139 .Seattle , Wash. ) may also be used to parse text .[ 0030 ] .In the dependency trees generated by Minipar , prepositions are represented by nodes .A transformation is applied for all the dependency trees .", "label": "", "metadata": {}, "score": "47.23928"}
{"text": "[0225 ] .Next , in step 1804 , the long - short text pairs are parsed to generate syntactic parse trees such as shown in FIG .11 , thereby resulting in corresponding long - short syntactic tree pairs .", "label": "", "metadata": {}, "score": "47.25535"}
{"text": "FIG .12 shows a few examples of sentence pairs extracted from the corpus .[ 0198 ] .This corpus was chosen because it is consistent with two desiderata specific to summarization work : ( i ) the human - written Abstract sentences are grammatical ; ( ii ) the Abstract sentences represent in a compressed form the salient points of the original newspaper Sentences .", "label": "", "metadata": {}, "score": "47.27948"}
{"text": "That is , one can specify that a certain substring may represent unimportant information without worrying that deleting the substring will result in an ungrammatical structure .That concern is left to the source model , which worries exclusively about well - formedness .", "label": "", "metadata": {}, "score": "47.323406"}
{"text": "This knowledge engineering task is extremely laborious .More importantly , building such a knowledge base is inherently difficult since humans are not good at generating a complete list of rules .[ 0009 ] .It is known in the art of information retrieval to identify phrasal terms from queries and generate variants for query expansion .", "label": "", "metadata": {}, "score": "47.32429"}
{"text": "[ 0148 ] .A binary representation of these features yields learning examples with 2789 features / example .[ 0149 ] .Examples of Rule Specific to the Action Identifier .[ 0150 ] .[ 0150]FIG .8A shows some of the rules that were learned by the C4.5 program using a binary representation of the features and learning cases extracted from the MUC corpus .", "label": "", "metadata": {}, "score": "47.399597"}
{"text": "( 2 )This book , which I have received from John , is the best book that I have read in a while .[ 0083 ] .The annotation process involved assigning edu and parenthetical unit boundaries , assembling edus and spans into discourse trees , and labeling the relations between edus and spans with rhetorical relation names from a taxonomy of 71 relations .", "label": "", "metadata": {}, "score": "47.40706"}
{"text": "17 .[ 0253 ] .Next , in step 2004 , the long - short text pairs are parsed to generate syntactic parse trees such as shown in FIG .11 , thereby resulting in corresponding long - short syntactic tree pairs .", "label": "", "metadata": {}, "score": "47.43465"}
{"text": "In the current embodiment , the final discourse tree is generated in a sequence of deterministic steps with no recursion or branching .Alternatively , it is possible to associate a probability with each individual step and build the discourse tree of a text by exploring multiple alternatives at the same time .", "label": "", "metadata": {}, "score": "47.518204"}
{"text": "In this regard , a hard - coded list of 250 potential abbreviations can be used .[ 0101 ] .The global context reflects features that pertain to the boundary identification process .These features specify whether there are any commas , closed parentheses , and dashes before the estimated end of the sentence , whether there are any verbs in the unit under consideration , and whether any discourse marker that introduces expectations was used in the sentence under consideration .", "label": "", "metadata": {}, "score": "47.583534"}
{"text": "[ 0119 ] .In step 606 , the C4.5 program was used in order to learn decision trees and rules that classify lexemes as boundaries of sentences , edus , or parenthetical units , or as non - boundaries .Learning was accomplished both from binary representations ( when possible ) and non - binary representations of the cases .", "label": "", "metadata": {}, "score": "47.70691"}
{"text": "( 2 ) Features that denote Wordnet - based measures of similarity between the bags of words in the promotion sets of the trees in focus .Fourteen Wordnet - based measures of similarity were used , one for each Wordnet relation ( Fellbaum , Wordnet : An Electronic Lexical Database , The MIT Press , 1998 ) .", "label": "", "metadata": {}, "score": "47.748146"}
{"text": "Using the collected monolingual parallel corpora , sentence alignment can be performed to extract monolingual parallel sentence pairs .Accordingly , three feature functions are defined as follows : .[ 0089 ] where common BP ( ph 1 , ph 2 ) refers to the common context words when ph 1 and ph 2 are aligned together in the monolingual parallel corpora .", "label": "", "metadata": {}, "score": "47.931812"}
{"text": "[0067 ] .Referring to FIG .6 , the steps carried out during an implementation of the invention are set forward .At step 101 , each sentence in the text ( corpus ) under consideration is parsed to identify paths formed by concatenated grammatical relationships between words in the text .", "label": "", "metadata": {}, "score": "48.01291"}
{"text": "[0106 ]The paraphrasing transformation of Class 7 involves change into different sentence types .In this class , both the atomic linguistic element and corresponding atomic paraphrasing element are patterns .Class 7 usually involves only close set of patterns , which can either be learned or handled easily by human - crafted rules .", "label": "", "metadata": {}, "score": "48.028282"}
{"text": "After the transformation , each link between two words in a dependency tree represents a direct semantic relationship .A path is used to represent indirect semantic relationships between two content words ( i.e. nouns , verbs , adjectives or adverbs ) .", "label": "", "metadata": {}, "score": "48.09635"}
{"text": "[ How to reference and link to summary or text ] .In the mid-1990s to mid-2000s , much research in transformational grammar was inspired by Chomsky 's Minimalist Program .[ 9 ] The \" Minimalist Program \" aims at the further development of ideas involving economy of derivation and economy of representation , which had started to become significant in the early 1990s , but were still rather peripheral aspects of Transformational - generative grammar theory .", "label": "", "metadata": {}, "score": "48.104874"}
{"text": "[ How to reference and link to summary or text ] .In the mid-1990s to mid-2000s , much research in transformational grammar was inspired by Chomsky 's Minimalist Program .[ 9 ] The \" Minimalist Program \" aims at the further development of ideas involving economy of derivation and economy of representation , which had started to become significant in the early 1990s , but were still rather peripheral aspects of Transformational - generative grammar theory .", "label": "", "metadata": {}, "score": "48.104874"}
{"text": "[ How to reference and link to summary or text ] .In the mid-1990s to mid-2000s , much research in transformational grammar was inspired by Chomsky 's Minimalist Program .[ 9 ] The \" Minimalist Program \" aims at the further development of ideas involving economy of derivation and economy of representation , which had started to become significant in the early 1990s , but were still rather peripheral aspects of Transformational - generative grammar theory .", "label": "", "metadata": {}, "score": "48.104874"}
{"text": "[ 1 ] .In 1957 , Noam Chomsky published Syntactic Structures , in which he developed the idea that each sentence in a language has two levels of representation - a deep structure and a surface structure .[ 2 ] [ 3 ] The deep structure represented the core semantic relations of a sentence , and was mapped on to the surface structure ( which followed the phonological form of the sentence very closely ) via transformations .", "label": "", "metadata": {}, "score": "48.138577"}
{"text": "[ 1 ] .In 1957 , Noam Chomsky published Syntactic Structures , in which he developed the idea that each sentence in a language has two levels of representation - a deep structure and a surface structure .[ 2 ] [ 3 ] The deep structure represented the core semantic relations of a sentence , and was mapped on to the surface structure ( which followed the phonological form of the sentence very closely ) via transformations .", "label": "", "metadata": {}, "score": "48.138577"}
{"text": "A grammar which achieves explanatory adequacy has the additional property that it gives an insight into the underlying linguistic structures in the human mind ; that is , it does not merely describe the grammar of a language , but makes predictions about how linguistic knowledge is mentally represented .", "label": "", "metadata": {}, "score": "48.389866"}
{"text": "A grammar which achieves explanatory adequacy has the additional property that it gives an insight into the underlying linguistic structures in the human mind ; that is , it does not merely describe the grammar of a language , but makes predictions about how linguistic knowledge is mentally represented .", "label": "", "metadata": {}, "score": "48.389866"}
{"text": "A grammar which achieves explanatory adequacy has the additional property that it gives an insight into the underlying linguistic structures in the human mind ; that is , it does not merely describe the grammar of a language , but makes predictions about how linguistic knowledge is mentally represented .", "label": "", "metadata": {}, "score": "48.389866"}
{"text": "A grammar that achieves explanatory adequacy has the additional property that it gives an insight into the underlying linguistic structures in the human mind ; that is , it does not merely describe the grammar of a language , but makes predictions about how linguistic knowledge is mentally represented .", "label": "", "metadata": {}, "score": "48.399994"}
{"text": "This paper describes a simple yet novel method for constructing sets of 50-best parses based on a co ... \" .Discriminative reranking is one method for constructing high - performance statistical parsers ( Collins , 2000 ) .A discriminative reranker requires a source of candidate parses for each sentence .", "label": "", "metadata": {}, "score": "48.424046"}
{"text": "At each step , the rewriting module applies an operation that is aimed at reconstructing the smaller tree s2 .In the context of the sentence - compression module , four types of operations are used : .[ 0231 ] .", "label": "", "metadata": {}, "score": "48.620125"}
{"text": "In the Extended Standard Theory and government and binding theory , GTs were abandoned in favor of recursive phrase structure rules .However , they are still present in tree - adjoining grammar as the Substitution and Adjunction operations and they have recently re - emerged in mainstream generative grammar in Minimalism as the operations Merge and Move . \"", "label": "", "metadata": {}, "score": "48.639366"}
{"text": "In the Extended Standard Theory and government and binding theory , GTs were abandoned in favor of recursive phrase structure rules .However , they are still present in tree - adjoining grammar as the Substitution and Adjunction operations and they have recently re - emerged in mainstream generative grammar in Minimalism as the operations Merge and Move . \"", "label": "", "metadata": {}, "score": "48.639366"}
{"text": "In the Extended Standard Theory and government and binding theory , GTs were abandoned in favor of recursive phrase structure rules .However , they are still present in tree - adjoining grammar as the Substitution and Adjunction operations and they have recently re - emerged in mainstream generative grammar in Minimalism as the operations Merge and Move . \"", "label": "", "metadata": {}, "score": "48.639366"}
{"text": "These actions assign POS tags to the words in the compressed sentence , which may be different from the POS tags in the original sentence .[ 0235 ] .The decision - based model is more flexible than the channel model because it enables the derivation of a tree whose skeleton can differ quite drastically from that of the tree given as input .", "label": "", "metadata": {}, "score": "48.66674"}
{"text": "Description : .BACKGROUND .[ 0001 ] Paraphrasing used in a computerized environment is a process of automatically generating a paraphrasing sentence from a reference sentence or an input sentence .The computer - generated paraphrases are alternative ways of conveying the same information .", "label": "", "metadata": {}, "score": "48.741005"}
{"text": "0065 ] .[ 0065]FIG .20 is a flowchart of a process for training a decision - based summarizer .DETAILED DESCRIPTION .[ 0066 ] .Discourse Parsing . [0067 ] .As described herein , a decision - based rhetorical parsing system ( equivalently , a discourse parsing system ) automatically derives the discourse structure of unrestricted texts and incrementally builds corresponding discourse trees based on a set of learned decision rules .", "label": "", "metadata": {}, "score": "48.746284"}
{"text": "Terms such as \" transformation \" can give the impression that theories of transformational generative grammar are intended as a model for the processes through which the human mind constructs and understands sentences .Chomsky is clear that this is not in fact the case : a generative grammar models only the knowledge that underlies the human ability to speak and understand .", "label": "", "metadata": {}, "score": "48.773777"}
{"text": "Terms such as \" transformation \" can give the impression that theories of transformational generative grammar are intended as a model for the processes through which the human mind constructs and understands sentences .Chomsky is clear that this is not in fact the case : a generative grammar models only the knowledge that underlies the human ability to speak and understand .", "label": "", "metadata": {}, "score": "48.773777"}
{"text": "Terms such as \" transformation \" can give the impression that theories of transformational generative grammar are intended as a model for the processes through which the human mind constructs and understands sentences .Chomsky is clear that this is not in fact the case : a generative grammar models only the knowledge that underlies the human ability to speak and understand .", "label": "", "metadata": {}, "score": "48.773777"}
{"text": "In one embodiment , the morphological variations are handled by the following exemplary procedure .[ 0110 ] ( b ) Provide a collection of sample parallel sentence pairs involving the above three sets of lexicon pairs .[ 0111 ]( c ) Perform word alignment between parallel sentence pairs .", "label": "", "metadata": {}, "score": "48.797203"}
{"text": "The first sentence in FIG .15 ( \" Beyond the basic level , the operations of the three products vary widely . \") was compressed in the same manner by humans and by both of the channel - based and decision - based algorithms ( the baseline algorithm chooses though not to compress this sentence ) .", "label": "", "metadata": {}, "score": "48.821198"}
{"text": "[ 0015 ] .[ 0016 ] .In another aspect , text parsing may include generating a set of one or more discourse segmenting decision rules based on a training set , and determining boundaries in an input text segment by applying the generated set of discourse segmenting decision rules to the input text segment .", "label": "", "metadata": {}, "score": "49.033535"}
{"text": "In another aspect , a computer - implemented summarization method may include generating a parse tree ( e.g. , a discourse tree or a syntactic tree ) for an input text segment , and iteratively reducing the generated parse tree by selectively eliminating portions of the parse tree .", "label": "", "metadata": {}, "score": "49.15265"}
{"text": "[ 12 ] The \" Minimalist Program \" aims at the further development of ideas involving economy of derivation and economy of representation , which had started to become significant in the early 1990s , but were still rather peripheral aspects of Transformational - generative grammar theory .", "label": "", "metadata": {}, "score": "49.16246"}
{"text": "In any event , the results of this comparison are \" events \" that are collected for each of the long / short pairs and stored in a database .In general , two different types of events are detected : \" joint events \" which represent a detected correspondence between a long and short segment pair and Context - Free Grammar ( CFG ) events , which relate only to characteristics of the short segment in each pair .", "label": "", "metadata": {}, "score": "49.219353"}
{"text": "[ 13 ] His general position regarding the non - context - freeness of natural language has held up since then , although his specific examples regarding the inadequacy of CFGs in terms of their weak generative capacity were later disproven .", "label": "", "metadata": {}, "score": "49.242332"}
{"text": "[ 0213 ] .An expansion - template probability can be assigned to each node in the forest .In reality , forests are produced that are much slimmer , as only methods of compressing a node that are locally grammatical according to the Penn Treebank are considered .", "label": "", "metadata": {}, "score": "49.321804"}
{"text": "One or more of the following advantages may be provided by discourse parsing systems and techniques as described herein .The systems and techniques described here result in a discourse parsing system that uses a set of learned decision rules to automatically determine the underlying discourse structure of any unrestricted text .", "label": "", "metadata": {}, "score": "49.322365"}
{"text": "Besides , an unsupervised context clustering has also been proposed to learn paraphrases based on dependency parsing results .[ 0003 ] The existing techniques for paraphrasing regard paraphrases as a whole set , and use unified machine learning frameworks to model the paraphrasing transformations .", "label": "", "metadata": {}, "score": "49.33715"}
{"text": "This can be done efficiently by storing for each word the set of slots it fills in .[0073 ] .This step effectively uses a simpler similarity formula to filter out some of the paths since computing mutual information is more costly than counting the number of features .", "label": "", "metadata": {}, "score": "49.51229"}
{"text": "FIG .5 illustrates how the statuses and promotion sets associated with the trees involved in the reduce operations are affected in each case .[ 0094 ] .Because the labeled data in the training corpus used was relatively sparse , the relations that shared some rhetorical meaning were grouped into clusters of rhetorical similarity .", "label": "", "metadata": {}, "score": "49.543392"}
{"text": "A second model then attempts to improve upon this i ... \" .This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .", "label": "", "metadata": {}, "score": "49.606544"}
{"text": "Chomsky emphasizes the importance of modern formal mathematical devices in the development of grammatical theory : .But the fundamental reason for [ the ] inadequacy of traditional grammars is a more technical one .Although it was well understood that linguistic processes are in some sense \" creative , \" the technical devices for expressing a system of recursive processes were simply not available until much more recently .", "label": "", "metadata": {}, "score": "49.77005"}
{"text": "Conventional attempts at automated summarization , in contrast , typically have focused on identifying relevant items of information in the text being summarized , extracting text segments ( e.g. , sentences , clauses or keywords ) corresponding to those identified items , and then concatenating together the extracted segments .", "label": "", "metadata": {}, "score": "49.82824"}
{"text": "Examples of operational features include the following : numberTreesInStack , wasPreviousOperationShift , syntacticLabelOfTreeAtTheTopOfStack .[ 0242 ] .Original - tree - specific features denote the syntactic constituents that start with the first unit in the input list .Examples of such features include inputListStartsWithA_CC and inputListStartsWithA_PP .", "label": "", "metadata": {}, "score": "49.901566"}
{"text": "The rewriting procedure simulates a bottom - up reconstruction of the smaller trees .[ 0239 ] .Overall , the 1067 pairs of long and short sentences yielded 46383 learning cases .Each case was labeled with one action name from a set of 210 possible actions : There are 37 different ASSIGNTYPE actions , one for each POS tag .", "label": "", "metadata": {}, "score": "49.974762"}
{"text": "Although few linguists in the 1950s actually took such an extreme position , Chomsky was at an opposite extreme , defining grammaticality in an unusually ( for the time ) mentalistic way .This ( according to Chomsky ) is entirely distinct from the question of whether a sentence is meaningful , or can be understood .", "label": "", "metadata": {}, "score": "50.08329"}
{"text": "Although few linguists in the 1950s actually took such an extreme position , Chomsky was at an opposite extreme , defining grammaticality in an unusually ( for the time ) mentalistic way .This ( according to Chomsky ) is entirely distinct from the question of whether a sentence is meaningful , or can be understood .", "label": "", "metadata": {}, "score": "50.08329"}
{"text": "In general , human annotators looked at text segments and for each lexeme ( word or punctuation mark ) determined whether an edu boundary existed at the lexeme under consideration and either marked it with a segment break or not , depending on whether an edu boundary existed .", "label": "", "metadata": {}, "score": "50.21305"}
{"text": "The candidate atomic paraphrasing pairs are evaluated using , for example , feature functions and a trained probability model .The principled approach scores a combination of multiple candidate atomic paraphrasing pairs using a score function which derives its value from the feature functions of the candidate atomic paraphrasing pairs .", "label": "", "metadata": {}, "score": "50.26763"}
{"text": "[ 0028 ] .The training set used to generate the decision rules may include pre - generated long / short tree pairs .Generating the set of summarization decision rules comprises iteratively performing one or more tree modification operations on a long tree until the paired short tree is realized .", "label": "", "metadata": {}, "score": "50.289204"}
{"text": "0097 ] .The Discourse Segmenter .[ 0098 ] .[ 0098]FIG .6 is a flowchart of a generalized process 600 for generating decision rules for the discourse segmenter .The first step in the process was to build , or otherwise obtain , the training corpus .", "label": "", "metadata": {}, "score": "50.319305"}
{"text": "A tree structure is summarized by generating a set of one or more summarization decision rules based on a training set , and compressing the tree structure by applying the generated set of summarization decision rules to the tree structure .A computer - implemented method of determining discourse structures , the method comprising : . generating a set of one or more discourse parsing decision rules based on a training set ; and . determining a discourse structure for an input text segment by applying the generated set of discourse parsing decision rules to the input text segment .", "label": "", "metadata": {}, "score": "50.367928"}
{"text": "A second parallel corpus , referred to as the Cmplg Corpus , was created by generating compressed grammatical versions of these sentences .Because some of the sentences in this corpus were extremely long , the baseline algorithm could not produce compressed versions in reasonable time .", "label": "", "metadata": {}, "score": "50.40207"}
{"text": "In addition , two constituency relations were marked that were ubiquitous in the corpus and that often subsumed complex rhetorical constituents .These relations were ATTRIBUTION , which was used to label the relation between a reporting and a reported clause , and APPOSITION .", "label": "", "metadata": {}, "score": "50.417683"}
{"text": "The atomic transformations for candidate atomic paraphrasing pairs may also be defined and recognized by the data source .[ 0019 ]At block 130 , the process obtains a probability value of each candidate atomic paraphrasing pair .In one embodiment , the process obtains the probability value by computing a value of an appropriate feature function describing a probability of the atomic paraphrasing pair .", "label": "", "metadata": {}, "score": "50.418987"}
{"text": "[ 0080 ] The acquisition of the paraphrasing patterns and the design of the feature functions for each class are described in the following .The detailed algorithm description below introduces several notations .[0081 ] Class 1 : Exemplary pattern learning and feature design for class 1 atomic paraphrasing transformation is described below .", "label": "", "metadata": {}, "score": "50.47938"}
{"text": "( 17 ) # # EQU00008 # # .[ 0107 ] Class 8 - 9 : The following describes exemplary methods for acquiring and learning class 8 - 9 atomic paraphrasing transformations .[ 0108 ] Both Classes ( 8) and ( 9 ) involve morphological variations .", "label": "", "metadata": {}, "score": "50.62683"}
{"text": "[ 0102 ] .The decision - based segmenter uses a total of twenty - five features , some of which can take as many as 400 values .When we represent these features in a binary format , we obtain learning examples with 2417 binary features / example .", "label": "", "metadata": {}, "score": "50.740295"}
{"text": "Tools . \" ...This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .", "label": "", "metadata": {}, "score": "50.81971"}
{"text": "Sign up to receive free email alerts when patent applications with chosen keywords are published SIGN UP .Abstract : .A principled approach to paraphrasing analyzes input text and paraphrases at atomic linguistic level , instead of analyzing the input text and paraphrases as a whole set at one time .", "label": "", "metadata": {}, "score": "50.896957"}
{"text": "[0004 ] This disclosure describes a principled approach to paraphrasing .The principled approach analyzes input text and constructs paraphrases at atomic linguistic level , instead of analyzing the input text and finding paraphrases as a whole set at one time .", "label": "", "metadata": {}, "score": "50.90583"}
{"text": "The rules generated by the system are interpretable by machines and used in other applications ( e.g. information extraction , information retrieval , and machine translation ) .A method of building a database from text , the method comprising the steps of : . parsing text to identify paths formed by concatenated relationships between words in the text ; and . associating , in a computer , paths with each other based on a similarity measure between the paths .", "label": "", "metadata": {}, "score": "50.98699"}
{"text": "The final paraphrasing model is used to decide if an atomic pattern is triggered given a specific context of the input text .[ 0015 ] With the final paraphrasing model built and trained , automatic paraphrasing can be performed using exemplary procedures described below .", "label": "", "metadata": {}, "score": "51.024246"}
{"text": "The discovery of inference rules is made by finding the most similar paths of a given path .The challenge here is that there are a large number of paths in the triple database ( step 104 of FIG .6 ) .", "label": "", "metadata": {}, "score": "51.043938"}
{"text": "0017 ] .In another aspect , generating discourse trees may include segmenting an input text segment into edus , and incrementally building a discourse tree for the input text segment by performing operations on the edus to selectively combine the edus into larger discourse tree units .", "label": "", "metadata": {}, "score": "51.208305"}
{"text": "Implementations of the disclosed summarization systems and techniques may include various combinations of the following features .[ 0026 ] .The tree structure to be compressed may be generated by parsing an input text segment such as a clause , a sentence , a paragraph , or a treatise .", "label": "", "metadata": {}, "score": "51.351097"}
{"text": "The performance results presented above suggest how well the discourse segmenter and the shift - reduce action identifier perform with respect to individual cases , but provide no information about the performance of a rhetorical parser that relies on these classifiers .", "label": "", "metadata": {}, "score": "51.370266"}
{"text": "We introduce a new method for the reranking task , based on the boosting approach to ranking problems described in Freund et al .( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .", "label": "", "metadata": {}, "score": "51.70495"}
{"text": "The expanded search string is used to carry out the search according to conventional search techniques .Apart from the manner of construction of the database , and the content of the database , the techniques for carrying out the search illustrated in FIG .", "label": "", "metadata": {}, "score": "51.737488"}
{"text": "Paths linking words in the dependency trees are identified .If two paths tend to link the same sets of words , their meanings are taken to be similar .An inference rule is generated for each pair of similar paths .", "label": "", "metadata": {}, "score": "51.809113"}
{"text": "Paths linking words in the dependency trees are identified .If two paths tend to link the same sets of words , their meanings are taken to be similar .An inference rule is generated for each pair of similar paths .", "label": "", "metadata": {}, "score": "51.809113"}
{"text": "0092 ] A number of sample paraphrasing instances of classes 2 - 4 may be provided to learn the dependency tree transformation rules and tree - based sentence generation model .An exemplary learning procedure is given as follows : .[ 0093 ] ( a ) perform word alignment between original and paraphrased sentences provided in the sample paraphrasing instances ; .", "label": "", "metadata": {}, "score": "51.82215"}
{"text": "[ 0017]FIG .1 shows an information retrieval system using an inference rule database created according to the invention ; .[ 0018 ] .[ 0018]FIGS .2A and 2B are each diagrams showing a dependency tree for use in a system of the invention ; . [ 0019 ] .", "label": "", "metadata": {}, "score": "51.83487"}
{"text": "Moreover , the summarized text segment may include sentences not present in a text segment from which the pre - compressed tree structure was generated .[ 0027 ] .Applying the generated set of summarization decision rules comprises performing a sequence of modification operations on the tree structure , for example , one or more of a shift operation , a reduce operation , and a drop operation .", "label": "", "metadata": {}, "score": "51.856865"}
{"text": "Wordnet - based similarities reflect the degree of synonymy , antonymy , meronymy , hyponymy , and the like between the textual segments subsumed by the trees in focus .The Wordnet - based similarities are computed over the tokens that are found in the promotion units associated with each segment . sim .", "label": "", "metadata": {}, "score": "51.914078"}
{"text": "[0222 ] .Next , the multiple possible solutions generated in step 1706 are ranked using pre - generated ranking statistics from a statistical model .For example , step 1706 may involve assigning an expansion - template probability to each node in the forest , as described above .", "label": "", "metadata": {}, "score": "51.972023"}
{"text": "This paper discusses the statistical theory underlying various parameter - estimation methods , and gives algorithms which depend on alternatives to ( smoothed ) maximumlikelihood estimation .We first give an overview of results from statistical learning theory .We then show how important concepts from the classification literature -- specifically , generalization results based on margins on training data -- can be derived for parsing models .", "label": "", "metadata": {}, "score": "52.1276"}
{"text": "[ 0139 ] .Operational Features . [0140 ] .Operational features includes features that specify what the last five parsing operations performed by the parser were .These features could be generated because , for learning , sequences of shift - reduce operations were used and not discourse trees .", "label": "", "metadata": {}, "score": "52.19361"}
{"text": "For example , the process may be incorporated in a word processor , in which the input text is generated by a user , and the paraphrasing text is output to the user as an alternative to the input text .This may be a useful add - on function in a word processor to assist the user 's writing by suggesting alternative ways of expressing a certain idea .", "label": "", "metadata": {}, "score": "52.25714"}
{"text": "For real time access , the server 16 may directly instruct the computer 10 to parse text and retrieve associated relations and then perform the search using the original search string as well as associated relations .[ 0026 ] .The database is created by a method of discovering inference rules from text .", "label": "", "metadata": {}, "score": "52.350212"}
{"text": "[ 0021 ] .The rhetorical parsing algorithm described herein implements robust lexical , syntactic and semantic knowledge sources .Moreover , the six reduce operations used by the parsing algorithm , along with the shift operation , are mathematically sufficient to derive the discourse structure of any input text .", "label": "", "metadata": {}, "score": "52.39793"}
{"text": "A description of a decision - based , history model of sentence compression follows .As in the noisy - channel approach , it is assumed that a parse tree t is given as input .The goal is to \" rewrite \" t into a smaller tree s , which corresponds to a compressed version of the original sentence subsumed by t. Assume the trees t and s2 in FIG .", "label": "", "metadata": {}, "score": "52.424408"}
{"text": "In parsing we would have training examples fs i ; t i g where e .. \" ...This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm .We show how the algorithms can be efficiently applied to exponential sized representations of parse trees , such as the \" all subtrees \" ( DOP ) representation described by ( Bod 9 ... \" .", "label": "", "metadata": {}, "score": "52.543"}
{"text": "\" [ 0007 ] .Rhetorical relations reflect semantic , intentional and/or textual relations that hold between text spans .Other types of rhetorical relations include EVIDENCE , BACKGROUND , JOINT , and CAUSE .In FIG .1 , the internal nodes of discourse tree 100 are labeled with their respective rhetorical relation names 108 .", "label": "", "metadata": {}, "score": "52.844967"}
{"text": "[ 0245 ] .Employing the Decision - Based Model .[ 0246 ] .To compress sentences , the shift - reduce - drop model is applied in a deterministic fashion .The sentence to be compressed is parsed and the input list is initialized with the words in the sentence and the syntactic constituents that \" begin \" at each word , as shown in FIG .", "label": "", "metadata": {}, "score": "52.879585"}
{"text": "Much current research in transformational grammar is inspired by Chomsky 's Minimalist Program .[ 1 ] .In 1957 , Noam Chomsky published Syntactic Structures , in which he developed the idea that each sentence in a language has two levels of representation - a deep structure and a surface structure .", "label": "", "metadata": {}, "score": "52.933815"}
{"text": "Much current research in transformational grammar is inspired by Chomsky 's Minimalist Program .[ 1 ] .In 1957 , Noam Chomsky published Syntactic Structures , in which he developed the idea that each sentence in a language has two levels of representation - a deep structure and a surface structure .", "label": "", "metadata": {}, "score": "52.933815"}
{"text": "[ 0132 ] .Structural features include the following : . [ 0133 ] .( 1 ) Features that reflect the number of trees in the stack and the number of edts in the input list .[ 0134 ] .", "label": "", "metadata": {}, "score": "52.961117"}
{"text": "We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .In addition , we introduce two novel smoothing techniques , one a variation of Jelinek - Mercer smoothing and one a very simple linear interpolation technique , both of which outperform existing methods . ... her applications as well , e.g. prepositional phrase attachment ( Collins & Brooks , 1995 ) , part - of - speech tagging ( Church , 1988 ) , and stochastic pars - S. Whenever data sparsity is an issue , smoothing can help performance , and data sparsity is almost always an issue in statistical modeling .", "label": "", "metadata": {}, "score": "52.96465"}
{"text": "[0007 ] The detailed description is described with reference to the accompanying figures .In the figures , the left - most digit(s ) of a reference number identifies the figure in which the reference number first appears .The use of the same reference numbers in different figures indicates similar or identical items .", "label": "", "metadata": {}, "score": "52.96873"}
{"text": "Both notions , as described here , are somewhat vague , and indeed the precise formulation of these principles is controversial .[ 10 ] [ 11 ] An additional aspect of minimalist thought is the idea that the derivation of syntactic structures should be uniform ; that is , rules should not be stipulated as applying at arbitrary points in a derivation , but instead apply throughout derivations .", "label": "", "metadata": {}, "score": "53.005363"}
{"text": "Both notions , as described here , are somewhat vague , and indeed the precise formulation of these principles is controversial .[ 10 ] [ 11 ] An additional aspect of minimalist thought is the idea that the derivation of syntactic structures should be uniform ; that is , rules should not be stipulated as applying at arbitrary points in a derivation , but instead apply throughout derivations .", "label": "", "metadata": {}, "score": "53.005363"}
{"text": "Both notions , as described here , are somewhat vague , and indeed the precise formulation of these principles is controversial .[ 10 ] [ 11 ] An additional aspect of minimalist thought is the idea that the derivation of syntactic structures should be uniform ; that is , rules should not be stipulated as applying at arbitrary points in a derivation , but instead apply throughout derivations .", "label": "", "metadata": {}, "score": "53.005363"}
{"text": "For example , medical text may be used to generate inference rules in a medical domain .The first step is to parse the text .Any of a variety of parsers may be used .The parser output identifies characteristics or attributes of the words ( noun , verb , etc ) , grammatical relationships between the words and paths formed by the grammatical relationships .", "label": "", "metadata": {}, "score": "53.020462"}
{"text": "[0264 ] .Similarly , noisy - channel modeling could be enhanced by taking into account subcategory and head - modifier statistics ( in addition to simple word - bigrams ) .For example , the subject of a sentence may be separated from the verb by intervening prepositional phrases .", "label": "", "metadata": {}, "score": "53.159225"}
{"text": "Finally , in step 808 , the process 800 applies a learning algorithm ( e.g. , C4.5 ) to generate decision rules 810 for the discourse parser .As noted above , the discourse parser will then be able to use these decision rules 810 to determine the rhetorical structure for any input text and , from it , generate a discourse tree as output .", "label": "", "metadata": {}, "score": "53.194897"}
{"text": "Both notions , as described here , are somewhat vague , and indeed the precise formulation of these principles is controversial .[ 13 ] [ 14 ] An additional aspect of minimalist thought is the idea that the derivation of syntactic structures should be uniform ; that is , rules should not be stipulated as applying at arbitrary points in a derivation , but instead apply throughout derivations .", "label": "", "metadata": {}, "score": "53.241623"}
{"text": "Rather , humans attempt to summarize by rewriting the longer text , for example , by constructing new sentences that are grammatical , that cohere with one another , and that capture the most salient items of information in the original document .", "label": "", "metadata": {}, "score": "53.259163"}
{"text": "The process may compute the composite paraphrasing score by computing a value of a score function .In one embodiment , the score function is a product of the appropriate feature functions of the candidate atomic paraphrasing pairs in the selected combination .", "label": "", "metadata": {}, "score": "53.26049"}
{"text": "In Proceedings of KDD -99 .pp .16 - 22 .San Diego , Calif. ) , identifying prototypical documents ( Rajman , M. and Besan\u00e7on , R. 1997 .Text Mining : Natural Language Techniques and Text Mining Applications .", "label": "", "metadata": {}, "score": "53.414036"}
{"text": "The earliest conceptions of transformations were that they were construction - specific devices .For example , there was a transformation that turned active sentences into passive ones .A different transformation raised embedded subjects into main clause subject position in sentences such as \" John seems to have gone \" ; and yet a third reordered arguments in the dative alternation .", "label": "", "metadata": {}, "score": "53.424725"}
{"text": "The earliest conceptions of transformations were that they were construction - specific devices .For example , there was a transformation that turned active sentences into passive ones .A different transformation raised embedded subjects into main clause subject position in sentences such as \" John seems to have gone \" ; and yet a third reordered arguments in the dative alternation .", "label": "", "metadata": {}, "score": "53.424725"}
{"text": "The earliest conceptions of transformations were that they were construction - specific devices .For example , there was a transformation that turned active sentences into passive ones .A different transformation raised embedded subjects into main clause subject position in sentences such as \" John seems to have gone \" ; and yet a third reordered arguments in the dative alternation .", "label": "", "metadata": {}, "score": "53.424725"}
{"text": "The earliest conceptions of transformations were that they were construction - specific devices .For example , there was a transformation that turned active sentences into passive ones .A different transformation raised embedded subjects into main clause subject position in sentences such as \" John seems to have gone \" ; and yet a third reordered arguments in the dative alternation .", "label": "", "metadata": {}, "score": "53.424725"}
{"text": "The rhetorical parsing algorithm implements robust lexical , syntactic and semantic knowledge sources .[0068 ] .In one embodiment , the resulting output of the discourse parsing system is a rhetorical tree .[ 0069 ] .[ 0069]FIG .", "label": "", "metadata": {}, "score": "53.514755"}
{"text": "Extracting Collocations from Text Corpora .Workshop on Computational Terminology .pp .57 - 63 .Montreal , Canada . ) takes this into account by computing the mutual information between a feature and a path .[0054 ] .", "label": "", "metadata": {}, "score": "53.553646"}
{"text": "The following three feature functions are defined accordingly .[ 0087 ] where common BP ( ph 1 , ph 2 ) refers to the common context words when phrases ph 1 and ph 2 are translated into the same phrases of a different language .", "label": "", "metadata": {}, "score": "53.57454"}
{"text": "[ 0005 ] .The example in FIG .1 shows the types of structures in a discourse tree 100 for a text fragment .The leaves 102 of the tree correspond to elementary discourse units ( \" edus \" ) and the internal nodes correspond to contiguous text spans .", "label": "", "metadata": {}, "score": "53.63988"}
{"text": "As will be described in detail herein , fifteen major atomic paraphrasing classes are identified based on data exploration and analysis .Different paraphrasing pattern acquisition schemes are designed for different paraphrasing classes .For each class of atomic paraphrasing transformation , paraphrasing patterns are acquired by either machine learning or hand - crafted rules .", "label": "", "metadata": {}, "score": "53.794006"}
{"text": "[ 0084 ] .[0085 ] .maintains logs of all tree - construction operations .As a result , in addition to the rhetorical structure of 90 texts , a corpus of logs was created that reflects the way that human judges determine edu and parenthetical unit boundaries .", "label": "", "metadata": {}, "score": "53.815205"}
{"text": "expanding the search by reference to associated paths in a database constructed according to the method of . claim 1 .The method of .claim 7 in which the search is initiated from a location remote from the location of the database .", "label": "", "metadata": {}, "score": "53.818153"}
{"text": "These and other aspects of the invention are described in the detailed description of the invention and claimed in the claims that follow .BRIEF DESCRIPTION OF THE DRAWINGS .[ 0016 ] .There will now be described preferred embodiments of the invention , with reference to the drawings , by way of illustration only and not with the intention of limiting the scope of the invention , in which like numerals denote like elements and in which : . [", "label": "", "metadata": {}, "score": "53.828587"}
{"text": "[ 0113 ]( e ) Learn a language generation model based on a given dependency tree .[0114 ]The steps ( b)-(e ) may only use a relatively small collection of human annotated sentence pairs ( e.g. , 1,000 pairs ) .", "label": "", "metadata": {}, "score": "53.845497"}
{"text": "[ 0074 ] .Further details of the discourse parser and the parsing process that it performs are provided with reference to FIGS .4 and 5 .Details on generating discourse segmenter decision rules and discourse parser decision rules appear below with reference to FIGS .", "label": "", "metadata": {}, "score": "53.962475"}
{"text": "BACKGROUND OF THE INVENTION .[0002 ] .Text is the most significant repository of human knowledge .Many algorithms have been proposed to mine textual data .Algorithms have been proposed that focus on document clustering ( Larsen , B. and Aone , C. 1999 .", "label": "", "metadata": {}, "score": "54.001335"}
{"text": "[ 0013 ] .Determining a discourse structure may include incrementally building a discourse tree for the input text segment , for example , by selectively combining elementary discourse trees ( edts ) into larger discourse tree units .Moreover , incrementally building a discourse tree for the input text segment may include performing operations on a stack and an input list of edts , one edt for each edu in a set of edus corresponding to the input text segment .", "label": "", "metadata": {}, "score": "54.099136"}
{"text": "1992 ) .They are important for a few reasons .Many systems applied to part - ofspeech tagging , speech recognition and other language or speech tasks also fall into this class of model .Second , a partic ... . \" ...", "label": "", "metadata": {}, "score": "54.121616"}
{"text": "The rhetorical structure assigned to each text is a ( possibly non - binary ) tree whose leaves correspond to elementary discourse units ( edu)s , and whose internal nodes correspond to contiguous text spans .Each internal node is characterized by a rhetorical relation , such as ELABORATION and CONTRAST .", "label": "", "metadata": {}, "score": "54.338367"}
{"text": "3 is a diagram illustrating a transformation used for modifying a dependency tree ; .[ 0020 ] .[ 0020]FIGS . 4 and 5 illustrate dependency trees for specific sentences ; . [ 0021 ] .[ 0021]FIG .6 is a flow diagram illustrating constructing a database of similar paths according to the invention and using the constructed database in an information retrieval system ; .", "label": "", "metadata": {}, "score": "54.4012"}
{"text": "[ 0043 ] .In the preferred embodiment , the algorithm makes use of an extension of the principle , known as the Distributional Hypothesis ( Harris , Z. 1985 .Distributional Structure .In : Katz , J. J. ( ed . )", "label": "", "metadata": {}, "score": "54.478485"}
{"text": "[ 0053]FIG .8E shows a result of applying Rule 4 in FIG .8A on the on the trees that subsume the two paragraphs in FIG .8D. [ 0054 ] .[ 0054]FIG .9 is a graph of a learning curve for a shift - reduce action identifier .", "label": "", "metadata": {}, "score": "54.49109"}
{"text": "Pages 1 - 52 .( See p. 49 fn .2 for comment on E - Language . )Linguistic Theory in America ( Second Edition ) , Academic Press .In linguistics , a transformational grammar , or transformational - generative grammar ( TGG ) , is a generative grammar , especially of a natural language , that has been developed in a Chomskyan tradition .", "label": "", "metadata": {}, "score": "54.49153"}
{"text": "[ 0255 ] .The output of step 2006 is a set of learning cases - one learning case for each long - short tree pair in the training set .In essence , each learning case is an ordered set of shift - reduce - drop operations that when applied to a long tree will generate the paired short tree .", "label": "", "metadata": {}, "score": "54.547302"}
{"text": "0115 ]Class 10 : The paraphrasing transformation of Class 10 involves only close set of patterns , and can be handled by human - crafted rules .The following feature function is defined for Class ( 10 ) : .( 20 ) # # EQU00010 # # .", "label": "", "metadata": {}, "score": "54.64953"}
{"text": "0115 ] .Rule 6 is an automatically derived rule that mirrors the manually derived rule specific to COMMA - like actions in the surface - based unit identification algorithm .Rule 6 will correctly insert an edu boundary after the comma marked in example 4.7 , because the marker \" While \" was used at the beginning of the sentence .", "label": "", "metadata": {}, "score": "54.768753"}
{"text": "This will result in both limited precision and high recall rate .Given the importance of automatic paraphrasing , especially in the context of natural language processing , it is desirable to discover new ways that may improve paraphrasing from various aspects .", "label": "", "metadata": {}, "score": "54.779915"}
{"text": "In Proceedings of ACL -93 . pp .112 - 120 .FIG .2A is a more compact form for the dependency tree of FIG .2B. The links in the diagram represent dependency relationships .The direction of a link is from the head to the modifier in the relationship .", "label": "", "metadata": {}, "score": "54.912083"}
{"text": "Table 4 also displays results obtained using a manual discourse segmenter , which identified correctly all edus .Table 4 displays these upper - bound figures as well .[0163 ] .The results in table 4 primarily show that errors in the discourse segmentation stage affect significantly the quality of the trees the parser builds .", "label": "", "metadata": {}, "score": "54.999012"}
{"text": "4 is a flowchart of an exemplary process of acquiring semantically related lexicons using the algorithm of mutual induction for paraphrasing patterns and lexical relations .[ 0119 ] At blocking 401 , for each of the above five paraphrasing classes 11 - 15 , an initial list of lexicon pairs is provided which trigger the following recursive learning procedure .", "label": "", "metadata": {}, "score": "55.060898"}
{"text": "This represents a 13 % decrease in error rate over the best single - parser results on this corpus [ 9].The major technical innova- tion is the use of a \" maximum - entropy - inspired \" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events .", "label": "", "metadata": {}, "score": "55.13874"}
{"text": "No .6,089,033 ) uses chains of relationships as features to compute similarities between words , the invention disclosed in this patent document uses words as features to compute the similarity between chains of relationships .[0010 ] .U.S. Pat .", "label": "", "metadata": {}, "score": "55.166313"}
{"text": "This algorithm is initiated with a list of pre - defined lexical pairs , and learns atomic paraphrasing patterns based on the lexical pair list .The learned patterns are then used to expand the lexical pair list which makes the learning a recursive procedure .", "label": "", "metadata": {}, "score": "55.176605"}
{"text": "What follows is a description of the training corpus that was used in generating decision rules for the discourse segmenter and for the discourse parser .[0075 ] .The Training Corpus .[ 0076 ] .The training corpus ( equivalently , the training set ) used was a body of manually built ( i.e. , by humans ) rhetorical structure trees .", "label": "", "metadata": {}, "score": "55.20266"}
{"text": "[ 0104 ] The above feature function F 14 is used to estimate the reliability of the paraphrasing transformation , while feature function F 15 is used to check if the transformation matches the context of the given input sentence .[", "label": "", "metadata": {}, "score": "55.217533"}
{"text": "The rules generated by the system are interpretable by machines and used in other applications ( e.g. information extraction , information retrieval , and machine translation ) .By contrast with Richardson ( U.S. Pat .No .6,098,033 ) , paths are used as features to compute the similarity of words .", "label": "", "metadata": {}, "score": "55.217705"}
{"text": "[ 0024 ] .Accordingly , as described in detail below , automated summarization systems and techniques were developed that can generate a coherent summary of an input text by generating new , grammatical sentences that capture the salient aspects of the input text .", "label": "", "metadata": {}, "score": "55.27181"}
{"text": "In step 606 , a learning algorithm such as the C4.5 algorithm as described in J. Ross Quinlan , \" C4.5 : Programs for Machine Learning , \" Morgan Kaufmann Publishers ( 1993 ) , to learn a set of decision rules from the learning cases .", "label": "", "metadata": {}, "score": "55.281635"}
{"text": "New York : Oxford University Press , pp .26 - 47 . )According to the Distributional Hypothesis words that tend to occur in the same contexts tend to have similar meanings .The present invention makes use of the principle in a different context and in a different way , using different formulae , from previous methods for computing the similarity between two sets of words .", "label": "", "metadata": {}, "score": "55.524765"}
{"text": "In general the binary representations yielded slightly better results than the non - binary representations and the tree classifiers were slightly better than the rule - based ones .[0120 ] .Table 1 shows accuracy results of non - binary , decision - tree classifiers .", "label": "", "metadata": {}, "score": "55.578697"}
{"text": "The program then returns to the next parsed sentence 105 and the process continues until all paths have been counted .The counts are then used as for example in the above noted equations to yield a similarity measure 106 , from which a ranked list of associated relations may be produced 107 as described below .", "label": "", "metadata": {}, "score": "55.634247"}
{"text": "[0017 ]At block 120 , for each atomic linguistic element , the process selects one or more atomic paraphrasing elements .The atomic linguistic element relates to the selected atomic paraphrasing element through an atomic transformation to form a candidate atomic paraphrasing pair .", "label": "", "metadata": {}, "score": "55.643997"}
{"text": "[ 0100 ] .To partition a text into edus and to detect parenthetical unit boundaries , features were relied on that model both the local and global contexts .The local context consists of a window of size 5 ( 1 + 2 + 2 ) that enumerates the Part - Of - Speech ( POS ) tags of the lexeme under scrutiny and the two lexemes found immediately before ( 2 ) and after it ( 2 ) .", "label": "", "metadata": {}, "score": "55.766613"}
{"text": "After an expansion template is chosen , then for each new child node introduced ( if any ) , a new subtree is grown rooted at that node - for example , ( PP ( P in ) ( NP Pittsburgh ) ) .", "label": "", "metadata": {}, "score": "55.78974"}
{"text": "Upon receiving the input text in step 202 , the process 200 breaks the text into elementary discourse units , or \" edus .\" Edus are defined functionally as clauses or clause - like units that are unequivocally the nucleus or satellite of a rhetorical relation that holds between two adjacent spans of text .", "label": "", "metadata": {}, "score": "55.83651"}
{"text": "^ Jackendoff , Ray ( 1974 ) .Semantic Interpretation in Generative Grammar .MIT Press .ISBN 0 - 262 - 10013 - 4 .^ May , Robert C. ( 1977 ) .The Grammar of Quantification .MIT Phd Dissertation .", "label": "", "metadata": {}, "score": "55.96573"}
{"text": "This algorithm is initiated with a list of pre - defined lexical pairs , and learns atomic paraphrasing patterns based on the lexical pair list .The learned patterns are then used to expand the lexical pair list , making the learning a recursive procedure .", "label": "", "metadata": {}, "score": "56.001457"}
{"text": "[ 0016 ] FIG .1 is a flowchart of an exemplary process of automatic paraphrasing using the atomic paraphrases .At block 110 , the process selects a plurality of atomic linguistic elements from an input text .The atomic linguistic elements may be extracted from the input text .", "label": "", "metadata": {}, "score": "56.016983"}
{"text": "This can be explained by the significant differences in style and discourse marker usage between the three corpora .When a perfect segmenter is used , the rhetorical parser determines hierarchical constituents and assigns them a nuclearity status at levels of performance that are not far from those of humans .", "label": "", "metadata": {}, "score": "56.19479"}
{"text": "[ 0009 ] .Implementations of the disclosed discourse parsing system and techniques may include various combinations of the following features .[0010 ] .[ 0011 ] .The training set may include a plurality of annotated text segments ( e.g. , built manually by human annotators ) and a plurality of elementary discourse units ( edus ) .", "label": "", "metadata": {}, "score": "56.23368"}
{"text": "In a departure from the above discussion and from previous work on statistical channel models , probabilities P tree ( s ) and P expand .[0177 ] .Good source strings are ones that have both ( 1 ) a normal - looking parse tree , and ( 2 ) normal - looking word pairs .", "label": "", "metadata": {}, "score": "56.24828"}
{"text": "There are 109 distinct REDUCE actions , one for each type of reduce operation that is applied during the reconstruction of the compressed sentence .And there is one SHIFT operation .Given a tree t and an arbitrary configuration of the stack and input list , the purpose of the decision - based classifier is to learn what action to choose from the set of 210 possible actions .", "label": "", "metadata": {}, "score": "56.344315"}
{"text": "[ 0077 ] .Better tools are necessary to tap into the vast amount of textual data that is growing at an astronomical pace .Knowledge about inference relationships in natural language expressions would be extremely useful for such tools .In the present invention , such knowledge is discovered automatically from a large corpus of text .", "label": "", "metadata": {}, "score": "56.367188"}
{"text": "The method of .claim 16 wherein determining boundaries in the input text segment comprises recognizing sentence boundaries , elementary discourse unit ( EDU ) boundaries , parenthetical starts , and parenthetical ends .A computer - implemented method of generating discourse trees , the method comprising : . segmenting an input text segment into elementary discourse units ( EDUs ) ; and . incrementally building a discourse tree for the input text segment by performing operations on the EDUs to selectively combine the EDUs into larger discourse tree units .", "label": "", "metadata": {}, "score": "56.378723"}
{"text": "[ 0132 ] The atomic paraphrasing techniques disclosed herein analyze and construct paraphrases using a principled approach based on a multiclass atomic paraphrasing model .The techniques potentially overcome some of the basic problems existing in the conventional paraphrasing techniques .It is , however , appreciated that the potential benefits and advantages discussed herein are not to be construed as a limitation or restriction to the scope of the appended claims .", "label": "", "metadata": {}, "score": "56.46878"}
{"text": "Paths are associated only when the similarity measure exceeds a threshold .[ 0013 ] .The search may be initiated from a location remote from the location of the database .[ 0014 ] .In another aspect of the invention , there is provided computer readable media containing instructions for carrying out the method steps .", "label": "", "metadata": {}, "score": "56.553337"}
{"text": "[0125 ] .In step 802 , the process receives as input the training corpus of discourse trees and , for each discourse tree , a set of edus from the discourse segmenter .Next , in step 804 , for each discourse tree / edu set , the process 800 determines a sequence of shift - reduce operations that reconstructs the discourse tree from the edus in that tree 's corresponding set .", "label": "", "metadata": {}, "score": "56.674973"}
{"text": "We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm , and we give experimental results on the ATIS corpus of parse trees . ... lems .The method is derived by the transformation from ranking problems to a margin - based classification problem in [ 8].", "label": "", "metadata": {}, "score": "56.743446"}
{"text": "0008 ] FIG .1 is a flowchart of an exemplary process of automatic paraphrasing using atomic paraphrases .[0009 ] FIG .2 shows an exemplary environment for implementing the atomic paraphrasing method of the present disclosure .[0010 ]", "label": "", "metadata": {}, "score": "57.177593"}
{"text": "In 1986 , Chomsky proposed a distinction between I - Language and E - Language , similar but not identical to the competence / performance distinction .E - Language encompasses all other notions of what a language is , for example that it is a body of knowledge or behavioural habits shared by a community .", "label": "", "metadata": {}, "score": "57.260986"}
{"text": "If the edt at position top-1 in the stack subsumes unit 1 in example 5.1 and the edt at position top subsumes unit 2 , this reduce action will correctly replace the two edts with a new rhetorical tree , that shown in FIG .", "label": "", "metadata": {}, "score": "57.463783"}
{"text": "The root of the dependency tree does not modify any word .It is also called the head of the sentence .[ 0028 ] .For example , FIGS .2A and 2B show the dependency tree for the sentence \" John found a solution to the problem \" , generated by a broad - coverage English parser called Minipar ( Lin , D. 1993 .", "label": "", "metadata": {}, "score": "57.48895"}
{"text": "This method generates 50-best lists that are of substantially higher quality than previously obtainable . ...m search , keeping some large number of possibilities to extend by adding the next word , and then re - pruning .", "label": "", "metadata": {}, "score": "57.519753"}
{"text": "Computing the similarity between every pair of paths is impractical with present computation devices .Given a path p , an exemplary algorithm for finding the most similar paths of p takes three steps : .[ 0072 ] . 1 )", "label": "", "metadata": {}, "score": "57.52009"}
{"text": "[ 0004 ] .Computational linguistics is the study of the applications of computers in processing and analyzing language , as in automatic machine translation ( \" MT \" ) and text analysis .In conjunction with MT research and related areas in computational linguistics , researchers have developed and frequently use various types of tree structures to graphically represent the structure of a text segment ( e.g. , clause , sentence , paragraph or entire treatise ) .", "label": "", "metadata": {}, "score": "57.572323"}
{"text": "In Proceedings of COLING -2000 .Sarrebr\u00fccken , Germany . )In this patent document , inference rules include relationships that are not exactly paraphrases , but are nonetheless related and are potentially useful to information retrieval systems .[0008 ] .", "label": "", "metadata": {}, "score": "57.60827"}
{"text": "A fundamental problem in statistical parsing is the choice of criteria and algorithms used to estimate the parameters in a model .The predominant approach in computational linguistics has been to use a parametric model with some variant of maximum - likelihood estimation .", "label": "", "metadata": {}, "score": "57.608925"}
{"text": "A fundamental problem in statistical parsing is the choice of criteria and algorithms used to estimate the parameters in a model .The predominant approach in computational linguistics has been to use a parametric model with some variant of maximum - likelihood estimation .", "label": "", "metadata": {}, "score": "57.608925"}
{"text": "Natural languages and context - free languages \" .Linguistics and Philosophy 4 ( 4 ) : 471 - 504 .doi : 10.1007/BF00360802 .^ Goldsmith , John A ( 1995 ) . \"Phonological Theory \" .In John A. Goldsmith .", "label": "", "metadata": {}, "score": "57.645744"}
{"text": "[0002 ] In the last decade , intensive research attention from computation linguistic community has been paid to the field of paraphrase acquisition , including paraphrasing at lexical level , syntactic level and semantic level .Especially , statistical machine translation techniques ( SMT ) have been used to model paraphrasing as a monolingual translation task .", "label": "", "metadata": {}, "score": "57.69681"}
{"text": "Valuable application of paraphrasing includes information retrieval , information extraction , question answering and machine translation .For example , in the automatic evaluation of machine translation , paraphrases may help to alleviate problems presented by the fact that there are often alternative and equally valid ways of translating a text .", "label": "", "metadata": {}, "score": "57.73471"}
{"text": "Innate linguistic knowledge .Terms such as \" transformation \" can give the impression that theories of transformational generative grammar are intended as a model for the processes through which the human mind constructs and understands sentences .Chomsky is clear that this is not in fact the case : a generative grammar models only the knowledge that underlies the human ability to speak and understand .", "label": "", "metadata": {}, "score": "57.75559"}
{"text": "Furthermore , \" text \" and \" corpus \" means \" any collection of natural language text \" .[ 0025 ] .Referring to FIG .1 , a method of building a database containing rules relating expressions in natural language text may be implemented in a general purpose computer 10 , which may be any one of many PCs or other computers available on the market .", "label": "", "metadata": {}, "score": "57.779015"}
{"text": "Accordingly , a feature function is defined as follows : .A transformation X i + nounnoun+Y i is then recognized as a valid paraphrasing transformation if E Z \u03b5Z(X i . sub . )max(freq(X i , Z)-C , 0)max(freq(X i , Z)-C , 0 ) is higher than a threshold ( where C is a constant ) .", "label": "", "metadata": {}, "score": "57.93072"}
{"text": "[ 0261 ] .It was also investigated how sensitive the channel - based and decision - based algorithms are with respect to the training data by carrying out the same experiments on sentences of a different genre , the scientific one .", "label": "", "metadata": {}, "score": "58.004066"}
{"text": "Since choosing only the action SHIFT never produces a discourse tree , column B4 presents the accuracy of a baseline classifier that chooses shift - reduce operations randomly , with probabilities that reflect the probability distribution of the operations in each corpus .", "label": "", "metadata": {}, "score": "58.05166"}
{"text": "Efficient Clustering of High - Dimensional Data Sets with Application to Reference Matching .In Proceedings of KDD -2000 .Boston , Mass. ) .[ 0074 ] .3 ) Compute the similarity between p and the candidates that passed the filter using equation ( 3 ) and output the paths in descending order of their similarity to p. .", "label": "", "metadata": {}, "score": "58.056366"}
{"text": "The Discourse Parsing Model .[ 0087 ] .The discourse parsing process is modeled as a sequence of shift - reduce operations .The input to the parser is an empty stack and an input list that contains a sequence of elementary discourse trees ( \" edts \" ) , one edt for each edu produced by the discourse segmenter .", "label": "", "metadata": {}, "score": "58.233665"}
{"text": "Chomsky argued that the notions \" grammatical \" and \" ungrammatical \" could be defined in a meaningful and useful way .Although few linguists in the 1950s actually took such an extreme position , Chomsky was at an opposite extreme , defining grammaticality in an unusually ( for the time ) mentalistic way .", "label": "", "metadata": {}, "score": "58.33139"}
{"text": "The process may also be incorporated in a search engine , in which the input text is generated by a user as a search query , and the paraphrasing text is used by the search engine as an alternative search query .", "label": "", "metadata": {}, "score": "58.453293"}
{"text": "In a typical more difficult case , a 25-word sentence may be optimally compressed by a 17-word version .Of course , if a user requires a shorter compression than that , another region of the curve may be selected and inspected for a local minimum .", "label": "", "metadata": {}, "score": "58.470894"}
{"text": "0126 ] .Additional details of training the discourse parser follow .[0127 ] .Shift - Reduce Action Identifier : Generation of Learning Examples .[ 0128 ] .When a derived sequence is applied as described above with respect to the parsing model , it produces a rhetorical tree that is a one - to - one copy of the original tree that was used to generate the sequence .", "label": "", "metadata": {}, "score": "58.49895"}
{"text": "p .Slot .X .w . and . to . denote .p .s .w .p .s .w .Following ( Lin , D. 1998 .Extracting Collocations from Text Corpora .Workshop on Computational Terminology .", "label": "", "metadata": {}, "score": "58.566437"}
{"text": "a plurality of operators for incrementally building the discourse tree for the input text segment by selectively combining the EDTs into a discourse tree segment according to the plurality of decision rules and moving the discourse tree segment onto the stack .", "label": "", "metadata": {}, "score": "58.571472"}
{"text": "0120 ] Block 410 extracts sentence pairs from a large monolingual corpus containing lexicon pairs .To be included in the extraction , the similarity of the sentence pairs should meet a preset condition , e.g. , a pre - defined threshold .", "label": "", "metadata": {}, "score": "58.65251"}
{"text": "Transformations actually come in two types : ( i ) the post - Deep structure kind mentioned above , which are string or structure changing , and ( ii ) Generalized Transformations ( GTs ) .Generalized transformations were originally proposed in the earliest forms of generative grammar ( e.g. , Chomsky 1957 ) .", "label": "", "metadata": {}, "score": "58.744278"}
{"text": "The effect of the operation is shown at the bottom of FIG .4 . [ 0092 ] .In order to enable a shift - reduce discourse parser to be able to derive any discourse tree , it is sufficient to implement one Shift operation and six types of Reduce operations , whose operational semantics are shown in FIG .", "label": "", "metadata": {}, "score": "58.78308"}
{"text": "T - tests showed no significant statistical differences between the two algorithms .Importance .[ 0263 ] .When applied to sentences of a different genre , the performance of the noisy - channel compression algorithm degrades smoothly , while the performance of the decision - based algorithm drops sharply .", "label": "", "metadata": {}, "score": "58.79308"}
{"text": "The judges were told that all outputs were generated automatically .The order of the outputs was scrambled randomly across test cases .[ 0260 ] .To avoid confounding , the judges participated in two experiments .In the first experiment , they were asked to determine on a scale from 1 to 5 how well the systems did with respect to selecting the most important words in the original sentence .", "label": "", "metadata": {}, "score": "58.80815"}
{"text": "( There are a few exceptions to this rule : some relations , such as SEQUENCE and CONTRAST , are multinuclear . )As noted above , the distinction between nuclei and satellites comes from the empirical observation that the nucleus expresses what is more essential to the writer 's purpose than the satellite .", "label": "", "metadata": {}, "score": "58.854866"}
{"text": "Chomsky goes so far as to suggest that a baby need not learn any actual rules specific to a particular language at all .Rather , all languages are presumed to follow the same set of rules , but the effects of these rules and the interactions between them can vary greatly depending on the values of certain universal linguistic parameters .", "label": "", "metadata": {}, "score": "58.877415"}
{"text": "Chomsky goes so far as to suggest that a baby need not learn any actual rules specific to a particular language at all .Rather , all languages are presumed to follow the same set of rules , but the effects of these rules and the interactions between them can vary greatly depending on the values of certain universal linguistic parameters .", "label": "", "metadata": {}, "score": "58.877415"}
{"text": "Chomsky goes so far as to suggest that a baby need not learn any actual rules specific to a particular language at all .Rather , all languages are presumed to follow the same set of rules , but the effects of these rules and the interactions between them can vary greatly depending on the values of certain universal linguistic parameters .", "label": "", "metadata": {}, "score": "58.877415"}
{"text": "The method of . claim 2 wherein generating the set of discourse parsing decision rules comprises iteratively performing one or more operations on a set of EDUs to incrementally build the annotated text segment associated with the set of EDUs .The method of .", "label": "", "metadata": {}, "score": "59.04773"}
{"text": "Treating paths as binary relations , the inventive system is able to generate inference rules by searching for similar paths .It is difficult for humans to recall a list of inference rules .However , given the output of the present invention , humans can easily identify the correct inference rules .", "label": "", "metadata": {}, "score": "59.116615"}
{"text": "In order to evaluate the rhetorical parser as a whole , each corpus was partitioned randomly into two sets of texts : 27 texts were used for training and the last 3 texts were used for testing .The evaluation employs \" labeled recall \" and \" labeled precision \" measures , which are extensively used to study the performance of syntactic parsers .", "label": "", "metadata": {}, "score": "59.22635"}
{"text": "The extractor selects the trees with the best combination of word - bigram and expansion template scores .It returns a list of such trees , one for each possible compression length .[ 0216 ] .Length Selection .[ 0217 ] .", "label": "", "metadata": {}, "score": "59.2854"}
{"text": "Prior to determining the discourse structure for the input text segment , the input text segment may be segmented into edus , which are inserted into the input list .Segmenting the input text segment into edus may be performed by applying a set of automatically learned discourse segmenting decision rules to the input text segment .", "label": "", "metadata": {}, "score": "59.430565"}
{"text": "6,076,088 issued Jun. 13 , 2000 , disclosed a method to extract Concept - Relation - Concept ( CRC ) triples from a text collection and to use the extracted CRC triples to answer queries about the text collection .The invention disclosed in this patent document differs from U.S. Pat .", "label": "", "metadata": {}, "score": "59.543945"}
{"text": "In linguistics , a transformational - generative grammar ( TGG ) , or transformational grammar is a generative grammar , especially of a natural language , that has been developed in a Chomskyan tradition .Additionally , transformational grammar is the Chomskyan tradition that gives rise to specific transformational grammars .", "label": "", "metadata": {}, "score": "59.580803"}
{"text": "[0158 ] .Evaluation of Shift - Reduce - Action Identifier .[ 0159 ] .Table 3 below displays the accuracy of the shift - reduce action identifiers , determined for each of the three corpora ( MUC , Brown , WSJ ) by means of a ten - fold cross - validation procedure .", "label": "", "metadata": {}, "score": "59.660713"}
{"text": "The method of . claim 58 wherein using a stochastic channel model algorithm comprises probabilistically choosing an expansion template .The method of .claim 55 wherein generating a plurality of potential solutions comprises identifying a forest of potential compressions for the parse tree .", "label": "", "metadata": {}, "score": "59.69706"}
{"text": "Particularly , the CRC triples represent information explicitly stated in the text collection whereas operation of the present invention results in inference rules that are typically not stated explicitly in a text collection .SUMMARY OF THE INVENTION .[ 0011 ] .", "label": "", "metadata": {}, "score": "59.820496"}
{"text": "[ 0172 ] .Channel model .[0173 ] .Decoder .[0174 ] .It is advantageous to break down the noisy channel problem this way , as it decouples the somewhat independent goals of creating a short text that ( 1 ) is grammatical and coherent , and ( 2 ) preserves important information .", "label": "", "metadata": {}, "score": "59.904533"}
{"text": "In general , the process selects those combinations that have the highest composite paraphrasing scores .One or more combinations may be selected .[ 0022 ]At block 160 , the process constructs a paraphrasing text using the atomic paraphrasing elements in the selected combination of candidate atomic paraphrasing pairs .", "label": "", "metadata": {}, "score": "59.931866"}
{"text": "[0031 ] The above - described atomic paraphrasing method uses an atomic paraphrasing model which can be built and trained before placed into the final application .The following describes the detail of building and training such an atomic paraphrasing model .", "label": "", "metadata": {}, "score": "60.035397"}
{"text": "[ 0077 ] Class 15 : Different semantic role realization [ 0078 ]a. He enjoyed the game .vs. The game pleased him .[0079 ] The above class 1 belongs to lexical level paraphrasing , classes 2 - 7 belong to syntactic level paraphrasing , and class 8 - 15 belong to semantic level paraphrasing .", "label": "", "metadata": {}, "score": "60.12239"}
{"text": "Additionally , transformational grammar is the Chomskyan tradition that gives rise to specific transformational grammars .Much current research in transformational grammar is inspired by Chomsky 's [ [ Minimalist Program]].The Minimalist Program .MIT Press .Additionally , transformational grammar is the Chomskyan tradition that gives rise to specific transformational grammars .", "label": "", "metadata": {}, "score": "60.189686"}
{"text": "In their original formulation ( Chomsky 1957 ) , these rules were stated as rules that held over strings of either terminals or constituent symbols or both .X NP AUX Y X AUX NP Y .In the 1970s , by the time of the Extended Standard Theory , following the work of Joseph Emonds on structure preservation , transformations came to be viewed as holding over trees .", "label": "", "metadata": {}, "score": "60.208427"}
{"text": "Grammatical theories .In the 1960s , Chomsky introduced two central ideas relevant to the construction and evaluation of grammatical theories .The first was the distinction between competence and performance .Chomsky noted the obvious fact that people , when speaking in the real world , often make linguistic errors ( e.g. starting a sentence and then abandoning it midway through ) .", "label": "", "metadata": {}, "score": "60.285675"}
{"text": "[ 0055]FIG .10 is a block diagram of an automated summarization system .[ 0056 ] .[ 0056]FIG .11 shows examples of parse ( or syntactic ) trees .[ 0057 ] .[ 0057]FIG .12 shows examples of text from a training corpus .", "label": "", "metadata": {}, "score": "60.31472"}
{"text": "[ 10 ] .Chomsky argued that the notions \" grammatical \" and \" ungrammatical \" could be defined in a meaningful and useful way .Although few linguists in the 1950s actually took such an extreme position , Chomsky was at an opposite extreme , defining grammaticality in an unusually mentalistic way ( for the time ) .", "label": "", "metadata": {}, "score": "60.359837"}
{"text": "In addition , sample parallel sentence pairs which may contain one or more atomic linguistic elements may also be stored in the system to further assist the application of the paraphrasing model .For example , a large number ( e.g. , in millions ) of monolingual parallel sentence pairs may be extracted from comparable news and multiple translations of the same novels .", "label": "", "metadata": {}, "score": "60.396656"}
{"text": "claim 7 in which , in the method steps of .claim 1 , the similarity measure is based on the frequency of occurrence of words in the paths .The method of .claim 9 , in which , in the method steps of .", "label": "", "metadata": {}, "score": "60.584793"}
{"text": "0075 ] .Table 3 lists the Top-50 most similar paths to \" X solves Y \" generated by an algorithm according to the invention .Most of the paths can be considered as paraphrases of the original expression .[ 0076 ] .", "label": "", "metadata": {}, "score": "60.620346"}
{"text": "15 ] .[ 0090 ] .19 . ] [ 0091 ] .[ 0091]FIG .4 shows the actions taken by a shift - reduce discourse parser starting with step i. At step i the parser decides , based on its predetermined decision rules , to perform a Shift operation .", "label": "", "metadata": {}, "score": "60.67783"}
{"text": "The proposition is assigned as the label of the direct relationship .FIG .3 gives an example for the phrase \" solution to the problem \" in which the two links in part ( a ) are replaced with a direct link shown in part ( b ) .", "label": "", "metadata": {}, "score": "60.897217"}
{"text": "In the decision - based summarizer model , the question presented is how may tree t be rewritten into s2 .One possible solution is to decompose the rewriting operation into a sequence of shift- reduce - drop actions that are specific to an extended shift - reduce parsing paradigm .", "label": "", "metadata": {}, "score": "60.913525"}
{"text": "The method of .claim 7 in which , in the method steps of .claim 1 , the step of associating paths with each other comprises the step of counting occurrences of words at the end points of specific paths .", "label": "", "metadata": {}, "score": "61.009842"}
{"text": "[ 0032 ] .Extracting one or more high - probability solutions may include selecting one or more trees based on a combination of each tree 's word - bigram and expansion - template score .For example , a list of trees may be selected , one for each possible compression length .", "label": "", "metadata": {}, "score": "61.03981"}
{"text": "[0079 ] .Paths may be extended with constraints on the inference rule 's variables .The \" where \" clause can be potentially discovered by generalizing the intersection of the SlotY fillers of the two relations .[ 0080 ] .", "label": "", "metadata": {}, "score": "61.12286"}
{"text": "These names are as follows : APPOSITION - PARENTHETICAL , ATTRIBUTION , CONTRAST , BACKGROUND - CIRCUMSTANCE , CAUSE - REASON - EXPLANATION , CONDITION , ELABORATION , EVALUATION - INTERPRETATION , EVIDENCE , EXAMPLE , MANNER - MEANS , ALTERNATIVE , PURPOSE , TEMPORAL , LIST , TEXTUAL , and OTHER .", "label": "", "metadata": {}, "score": "61.150063"}
{"text": "0070 ] .Software for implementing this invention may be readily prepared from the flow diagrams and descriptions set out here .It is preferable to use an existing parsing program for parsing .The software may be stored on computer readable media such as a hard disk or compact disk .", "label": "", "metadata": {}, "score": "61.171597"}
{"text": "[0070 ] .Next , in step 206 , the edus are put into an input list .In step 208 , the process 200 uses the input list , a stack , and a set of learned decision rules to perform the shift - reduce rhetorical parsing algorithm , which eventually yields the discourse structure of the text given as input .", "label": "", "metadata": {}, "score": "61.208565"}
{"text": "Many methods may be used to learn synonyms and synonymous phrases .The following are three exemplary methods used to learn synonyms .[ 0083 ] ( i ) Word clustering algorithm : Word similarity sim(w 1 , w 2 ) can be estimated between each pair of words , and any word pair with the similarity higher than a pre - defined threshold \u03b8 1 can be regarded as a synonym .", "label": "", "metadata": {}, "score": "61.27623"}
{"text": "claim 27 further comprising a discourse segmenter for partitioning the input text segment into EDUs and inserting the EDUs into the input list .A computer - implemented method comprising determining a discourse structure for an input text segment by applying a set of automatically learned discourse parsing decision rules to an input text segment .", "label": "", "metadata": {}, "score": "61.300316"}
{"text": "Afterwards , the learned classifier is asked in a stepwise manner what action to propose .Each action is then simulated , thus incrementally building a parse tree .The procedure ends when the input list is empty and when the stack contains only one tree .", "label": "", "metadata": {}, "score": "61.349606"}
{"text": "14 in a format that resembles the graphical representation of the trees in FIG .11 .[ 0237 ] .Learning the Parameters of ( Training ) the Decision - Based Model .[0238 ] .To train the decision - based model , each configuration of our shift - reduce - drop rewriting model is associated with a learning case .", "label": "", "metadata": {}, "score": "61.46192"}
{"text": "U.S. Pat .6,098,033 issued Aug. 1 , 2000 , disclosed a method for computing word similarity according to chains of semantic relationships between words .Examples of such semantic relationships include Cause , Domain , Hypernym , Location , Manner , Material , Means , Modifier , Part , and Possessor .", "label": "", "metadata": {}, "score": "61.532112"}
{"text": "In FIG .1 , nuclei 104 are represented by straight lines while satellites 106 are represented by arcs .[ 0006 ] .The distinction between nuclei and satellites comes from empirical observations that a nucleus expresses information that is more essential than a satellite to the writer 's intention , and that the nucleus of a rhetorical relation is comprehensible independent of the satellite but not vice versa .", "label": "", "metadata": {}, "score": "61.653854"}
{"text": "1998 .Extracting Classification Knowledge of Internet Documents with Mining Term Associations : A Semantic Approach .In Proceedings of SIGIR -98 .Melbourne , Australia . ) and hyponym relationships ( Hearst , M. 1992 .Automatic Acquisition of Hyponyms from Large Text Corpora .", "label": "", "metadata": {}, "score": "61.818882"}
{"text": "On the other hand , the decision - based summarizer is deterministic and thus provides a single solution and does so very quickly .Accordingly , depending on the objectives of the user , the decision - based summarizer may be advantageous both for its speed and for its deterministic approach .", "label": "", "metadata": {}, "score": "61.900135"}
{"text": "Moreover , the channel - based summarizer may be advantageous depending on a user 's objectives because its performance can be adjusted , or fine - tuned , to a particular application by replacing or adjusting its statistical model .Similarly , performance of the decision - based summarizer can be fine - tuned to a particular application by varying the training corpus used to learn decision rules .", "label": "", "metadata": {}, "score": "62.05812"}
{"text": "claim 52 wherein the drop operation deletes constituents from the tree structure .A computer - implemented summarization method comprising : . parsing an input text segment to generate a parse tree for the input segment ; . generating a plurality of potential solutions ; . applying a statistical model to determine a probability of correctness for each of potential solution ; . extracting one or more high - probability solutions based on the solutions ' respective determined probabilities of correctness .", "label": "", "metadata": {}, "score": "62.099045"}
{"text": "One embodiment implements dependency trees and the algorithm for learning tree transformation rules based sentence generation algorithm as disclosed in Chris Quirk , Arul Menezes , and Colin Cherry , 2004 ( Dependency Tree Translation : Syntactically Informed Phrasal SMT , Microsoft Research Technical Report : MSR - TR-2004 - 113 ) .", "label": "", "metadata": {}, "score": "62.12999"}
{"text": "[ 0104 ] .[ 0104]FIG .6A shows some of the rules that were learned by the C4.5 program using a binary representation of the features and learning cases extracted from the MUC corpus .Rule 1 can correctly identify the end of the parenthetic unit at the location marked with the symbol T in sentence ( 4.1 ) below . [", "label": "", "metadata": {}, "score": "62.207626"}
{"text": "For example , for each potential solution , a log - probability of correctness for the solution may be divided by a length of compression for the solution .[ 0033 ] .One or more of the following advantages may be provided by summarization systems and techniques as described herein .", "label": "", "metadata": {}, "score": "62.32578"}
{"text": "However , they are still present in tree - adjoining grammar as the Substitution and Adjunction operations , and they have recently re - emerged in mainstream generative grammar in Minimalism , as the operations Merge and Move .^ Chomsky , Noam ( 1965 ) .", "label": "", "metadata": {}, "score": "62.40613"}
{"text": "generate 2 n -1 new nodes , one for each non - empty subset of the children , and .[ 0211 ] .Pack those nodes so that they are referred to as a whole .[ 0212 ] .For example , consider the large tree t above .", "label": "", "metadata": {}, "score": "62.43088"}
{"text": "15 shows examples of text compression .[ 0061 ] .[ 0061]FIG .16 shows examples of summarizations of varying compression lengths . [0062 ] .[0062]FIG .17 is a flowchart of a channel - based summarization process .", "label": "", "metadata": {}, "score": "62.457054"}
{"text": "2 is a flowchart of generating a discourse tree for an input text .[ 0042 ] .[ 0042]FIG .3 is a block diagram of a discourse tree generating system .[ 0043 ] .[0043]FIG .4 shows an example of shift - reduce operations performed in discourse parsing a text .", "label": "", "metadata": {}, "score": "62.486214"}
{"text": "At step i+1 , the parser performs a \" Reduce - Apposition - NS \" operation , that combines edts 18 and 19 into a discourse tree whose nucleus is unit 18 and whose satellite is unit 19 .The rhetorical relation that holds between units 18 and 19 is APPOSITION .", "label": "", "metadata": {}, "score": "62.56624"}
{"text": "The similarity measure is based on the frequency of occurrences of words in the paths , where the words are at the end points of the paths .The step of associating paths with each other preferably comprises the step of counting occurrences of words at the end points of specific paths .", "label": "", "metadata": {}, "score": "62.62902"}
{"text": "[ 0047 ] .[ 0047]FIG .7 is a graph of a learning curve for a discourse segmenter .[0048 ] .[ 0048]FIG .8 is a flowchart of generating decision rules for a discourse segmenter .[ 0049 ] .", "label": "", "metadata": {}, "score": "62.66454"}
{"text": "8A shows examples of automatically derived shift - reduce rules .[ 0050 ] .[ 0050]FIG . 8B shows a result of applying Rule 1 in FIG .8A on the edts that correspond to the units in text example ( 5.1 ) .", "label": "", "metadata": {}, "score": "62.736565"}
{"text": "claim 44 wherein generating the set of decision rules comprises applying a learning algorithm to the plurality of learning cases .The method of .claim 44 further comprising associating one or more features with each of the learning cases to reflect context .", "label": "", "metadata": {}, "score": "62.82106"}
{"text": "As seen therein , Rule 1 enables the deletion of WH prepositional phrases in the context in which they follow other constituents that the program decided to delete .Rule 2 enables the deletion of WHNP constituents .Since this deletion is carried out only when the stack contains only one NP constituent , it follows that this rule is applied only in conjunction with complex nounphrases that occur at the beginning of sentences .", "label": "", "metadata": {}, "score": "62.888023"}
{"text": "The same goes for actual compression ( \" decoding \" in noisy - channel jargon)-one can re - use generic software packages to solve problems in all these application domains .[ 0175 ] .Statistical Models .[0176 ] .", "label": "", "metadata": {}, "score": "63.020958"}
{"text": "If the edt as position top-1 in the stack subsumes unit 1 in example 5.2 and the edt at position top subsumes unit 2 , rule 2 will correctly replace the two edts with the rhetorical tree shown in FIG .8C. [ 0153 ] .", "label": "", "metadata": {}, "score": "63.065094"}
{"text": "[ 0265 ] .Although only a few embodiments have been described in detail above , those having ordinary skill in the art will certainly understand that many modifications are possible in the preferred embodiment without departing from the teachings thereof .", "label": "", "metadata": {}, "score": "63.079586"}
{"text": "[ 0022]FIG .7 is a flow diagram illustrating the identification of paths in dependency trees according to the invention ; and .[ 0023 ] .[ 0023]FIG .8 is a flow diagram illustrating how paths are tested for compliance with constraints designed to identify valid paths .", "label": "", "metadata": {}, "score": "63.080956"}
{"text": "claim 30 further comprising generating the tree structure to be compressed by parsing an input text segment .The method of .claim 33 wherein the input text segment comprises a clause , a sentence , a paragraph , or a treatise .", "label": "", "metadata": {}, "score": "63.17248"}
{"text": "^ Newmeyer , Frederick J. ( 1986 ) .Linguistic Theory in America ( Second Edition ) .Academic Press .[ page needed ] .^ Chomsky , Noam ( 1995 ) .The Minimalist Program .MIT Press .ISBN 0 - 262 - 53128 - 3 .", "label": "", "metadata": {}, "score": "63.275967"}
{"text": "The method of .claim 4 in which the step of associating paths comprises the step of comparing counts of occurrences of words and associating paths based on the counts of occurrences of the words .The method of . claim 5 in which paths are associated only when the similarity measure exceeds a threshold .", "label": "", "metadata": {}, "score": "63.328312"}
{"text": "[ 0063]FIG .18 is a flowchart of a process for training a channel - based summarizer .[ 0064 ] .[ 0064]FIG .18A shows examples of rules that were learned automatically by the C4.5 program FIG .19 is a flowchart of a decision - based summarization process .", "label": "", "metadata": {}, "score": "63.35272"}
{"text": "Learning Atomic Paraphrasing Transformations and Designing Feature Functions : . [ 0040 ]Sentential paraphrasing may occur at three different levels , which are lexical level , syntactic level , and semantic level .Lexical level paraphrasing refers to synonym substitution , word deletion and insertion .", "label": "", "metadata": {}, "score": "63.470352"}
{"text": "Acknowledgement Many thanks to Amit Dubey and Yuval Krymolowski , the other two organizers of the shared task , for discussions , converting treebanks , writing software and helping with the papers . \" ...This paper presents a new approach to phrase - level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions .", "label": "", "metadata": {}, "score": "63.478424"}
{"text": "X NP AUX Y X AUX NP Y .In the 1970s , by the time of the Extended Standard Theory , following the work of Joseph Emonds on structure preservation , transformations came to be viewed as holding over trees .", "label": "", "metadata": {}, "score": "63.546528"}
{"text": "X NP AUX Y X AUX NP Y .In the 1970s , by the time of the Extended Standard Theory , following the work of Joseph Emonds on structure preservation , transformations came to be viewed as holding over trees .", "label": "", "metadata": {}, "score": "63.546528"}
{"text": "X NP AUX Y X AUX NP Y .In the 1970s , by the time of the Extended Standard Theory , following the work of Joseph Emonds on structure preservation , transformations came to be viewed as holding over trees .", "label": "", "metadata": {}, "score": "63.546528"}
{"text": "claim 42 wherein generating the set of summarization decision rules comprises iteratively performing one or more tree modification operations on a long tree until the paired short tree is realized .The method of . claim 43 wherein a plurality of long / short tree pairs are processed to generate a plurality of learning cases .", "label": "", "metadata": {}, "score": "63.87274"}
{"text": "2A and 2B , the relation between a and solution is excluded ; .[ 0039 ] .the frequency count of an internal relation must exceed a threshold ; and .[ 0040 ] .an internal relation must be between a verb and an object - noun or a small clause .", "label": "", "metadata": {}, "score": "63.975502"}
{"text": "Though transformations continue to be important in Chomsky 's current theories , he has now abandoned the original notion of Deep Structure and Surface Structure .To complicate the understanding of the development of Noam Chomsky 's theories , the precise meanings of Deep Structure and Surface Structure have changed over time - by the 1970s , the two were normally referred to simply as D - Structure and S - Structure by Chomskyan linguists .", "label": "", "metadata": {}, "score": "64.00694"}
{"text": "15 shows three sentences from the Test Corpus , together with the compressions produced by humans , the two compression algorithms described here ( channel - based and decision - based ) , and a baseline algorithm that produces compressions with highest word - bigram scores .", "label": "", "metadata": {}, "score": "64.041275"}
{"text": "[0012 ]The present disclosure proposes a principled approach to sentential paraphrasing .This approach is based on the following observation : there exist many different classes of atomic paraphrasing transformation , and a paraphrase may be created using a combination of atomic paraphrasing transformations .", "label": "", "metadata": {}, "score": "64.0811"}
{"text": "However , another rule that is derived automatically will insert a sentence break after the double quote that follows the T mark in example 4.4 .[0109 ] .( 4.3 )The meeting went far beyond Mr. Clinton 's normal weekly gathering of business leaders .", "label": "", "metadata": {}, "score": "64.085205"}
{"text": "claim 1 in which the similarity measure is based on the frequency of occurrence of words in the paths .The method of . claim 2 in which the words are at the end points of the paths .The method of .", "label": "", "metadata": {}, "score": "64.124466"}
{"text": "Extracting Collocations from Text Corpora .Workshop on Computational Terminology .pp .57 - 63 .Montreal , Canada . )Consider the words duty and responsibility .There are many contexts in which both of these words can fit .", "label": "", "metadata": {}, "score": "64.127884"}
{"text": "[ citation needed ] .In the 1960s , Chomsky introduced two central ideas relevant to the construction and evaluation of grammatical theories .The first was the distinction between competence and performance .Chomsky noted the obvious fact that people , when speaking in the real world , often make linguistic errors ( e.g. , starting a sentence and then abandoning it midway through ) .", "label": "", "metadata": {}, "score": "64.133575"}
{"text": "claim 12 wherein segmenting the input text segment into EDUs is performed by applying a set of automatically learned discourse segmenting decision rules to the input text segment .The method of . claim 13 further comprising generating the set of discourse segmenting decision rules by analyzing a training set .", "label": "", "metadata": {}, "score": "64.21136"}
{"text": "FIELD OF THE INVENTION .[ 0003 ] .The present application relates to computational linguistics and more particularly to techniques for parsing a text to determine its underlying rhetorical , or discourse , structure , and to techniques for summarizing , or compressing , text .", "label": "", "metadata": {}, "score": "64.35813"}
{"text": "[ 0035 ] .Moreover , the disclosed summarizer generates summaries automatically , e.g. , in a computer - implemented manner .Accordingly , the inconsistencies , errors , time and/or expense typically incurred with conventional approaches that require manual intervention are reduced dramatically .", "label": "", "metadata": {}, "score": "64.36987"}
{"text": "The method of .claim 1 wherein determining a discourse structure comprises incrementally building a discourse tree for the input text segment .The method of . claim 8 wherein incrementally building a discourse tree for the input text segment comprises selectively combining elementary discourse trees ( EDTs ) into larger discourse tree units .", "label": "", "metadata": {}, "score": "64.43224"}
{"text": "10 shows a block diagram of an automated summarization process .As shown therein , an input text 1000 is provided to a summarizer 1002 , which generates a summary 1004 of the input text 1000 .Ideally , whether produced manually or automatically , a summary will capture the most salient aspects of the longer text and present them in a coherent fashion .", "label": "", "metadata": {}, "score": "64.51489"}
{"text": "In the case of compression , the noise consists of optional text material that pads out the core signal .In summarizing the final article , the summarizer typically will not have access to the editor 's original version ( which may or may not exist ) , but the summarizer can guess at it - which is where probabilities come in .", "label": "", "metadata": {}, "score": "64.75645"}
{"text": "[ 0180 ] .[0181 ] .[0182 ] .[ 0183 ] .[ 0184 ] .The stochastic channel model performs minimal operations on a small tree s to create a larger tree t. For each internal node in s , an expansion template is chosen probabilistically based on the labels of the node and its children .", "label": "", "metadata": {}, "score": "64.8076"}
{"text": "Some edus may contain parenthetical units , i.e. , embedded units whose deletion does not affect the understanding of the edu to which they belong .For example , the unit shown in italics in text ( 2 ) , below , is parenthetic .", "label": "", "metadata": {}, "score": "64.88661"}
{"text": "[ 0051]FIG .8C shows a result of applying Rule 2 in FIG .8A on the edts that correspond to the units in text example ( 5.2 ) .[ 0052 ] .[ 0052]FIG .8D shows an example of a CONTRAST relation that holds between two paragraphs .", "label": "", "metadata": {}, "score": "64.93874"}
{"text": "Next , in step 2006 , for each long - short tree pair , the training process 2000 determines a sequence of shift - reduce - drop operations that will convert the long tree into the short tree .As discussed above , this step is performed based on the following four basic operations , referred to collectively as the \" shift - reduce - drop \" operations - shift , reduce , drop , and assignType .", "label": "", "metadata": {}, "score": "65.12216"}
{"text": "At each step , the parser applies a \" Shift \" or a \" Reduce \" operation .Shift operations transfer the first edt of the input list to the top of the stack .[ 0088 ] .Assume , for example , that the discourse segmenter partitions a text given as input as shown in text ( 3 ) below ( only the edus numbered from 12 to 19 are shown ) : .", "label": "", "metadata": {}, "score": "65.18166"}
{"text": "0044 ] .[ 0044]FIG .5 shows the operational semantics of six reduce operations .[ 0045 ] .[ 0045]FIG .6 is a flowchart of generating decision rules for a discourse segmenter .[ 0046 ] .[ 0046]FIG .", "label": "", "metadata": {}, "score": "65.26219"}
{"text": "Pages 1 - 52 .( See p. 49 fn .2 for comment on E - Language . )Linguistic Theory in America ( Second Edition ) , Academic Press .This article considers approaches which rerank the output of an existing probabilistic parser .", "label": "", "metadata": {}, "score": "65.34664"}
{"text": "Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation .", "label": "", "metadata": {}, "score": "65.400246"}
{"text": "claim 11 in which the step of associating paths comprises the step of comparing counts of occurrences of words and associating paths based on the counts of occurrences of the words .The method of .claim 12 in which paths are associated only when the similarity measure exceeds a threshold .", "label": "", "metadata": {}, "score": "65.430786"}
{"text": "E - Language encompasses all other notions of what a language is , for example that it is a body of knowledge or behavioural habits shared by a community .Competence , he argues , can only be studied if languages are treated as mental objects .", "label": "", "metadata": {}, "score": "65.570946"}
{"text": "E - Language encompasses all other notions of what a language is , for example that it is a body of knowledge or behavioural habits shared by a community .Competence , he argues , can only be studied if languages are treated as mental objects .", "label": "", "metadata": {}, "score": "65.570946"}
{"text": "E - Language encompasses all other notions of what a language is , for example that it is a body of knowledge or behavioural habits shared by a community .Competence , he argues , can only be studied if languages are treated as mental objects .", "label": "", "metadata": {}, "score": "65.570946"}
{"text": "[0197 ] .In order to train the channel - based summarizing system , the Ziff - Davis corpus - a collection of newspaper articles announcing computer products - was used .Many of the articles in the corpus are paired with human written abstracts .", "label": "", "metadata": {}, "score": "65.598724"}
{"text": "claim 30 further comprising converting the compressed tree structure into a summarized text segment .The method of .claim 35 wherein the summarized text segment is grammatical and coherent .The method of .claim 35 wherein the summarized text segment includes sentences not present in a text segment from which the pre - compressed tree structure was generated .", "label": "", "metadata": {}, "score": "65.60615"}
{"text": "iteratively reducing the generated parse tree by selectively eliminating portions of the parse tree .The method of .claim 47 wherein the generated parse tree comprises a discourse tree .The method of .claim 47 wherein the generated parse tree comprises a syntactic tree .", "label": "", "metadata": {}, "score": "65.629944"}
{"text": "The method of . claim 10 further comprising , prior to determining the discourse structure for the input text segment , segmenting the input text segment into EDUs and inserting the EDUs into the input list .The method of .claim 1 wherein determining the discourse structure for the input text segment further comprises : . segmenting the input text segment into elementary discourse units ( EDUs ) ; . incrementally building a discourse tree for the input text segment by performing operations on the EDUs to selectively combine the EDUs into larger discourse tree units ; and . repeating the incremental building of the discourse tree until all of the EDUs have been combined .", "label": "", "metadata": {}, "score": "65.734535"}
{"text": "The two different embodiments of the summarizer ( channel - based and decision - based ) both generate coherent , grammatical results but also potentially provide different advantages .On the one hand , the channel - based summarizer provides multiple different solutions at varying levels of compression .", "label": "", "metadata": {}, "score": "65.82113"}
{"text": "Information Retrieval , Data Structure and Algorithms .Prentice Hall . )The ranked associated paths 107 are stored in a database 108 .The database 108 may then be used in a search 109 . [0068 ] .Step 103 is expanded in FIG .", "label": "", "metadata": {}, "score": "65.930336"}
{"text": "Thus , if two paths tend to occur in similar contexts , the meanings of the paths tend to be similar .[0048 ] .As it can be seen from Table 2 , there are many overlaps between the corresponding slot fillers of the two paths , which indicates that the two paths have similar meaning .", "label": "", "metadata": {}, "score": "66.088104"}
{"text": "[ 0232 ] .REDUCE operations pop the k syntactic trees located at the top of the stack ; combine them into a new tree ; and push the new tree on the top of the stack .Reduce operations are used to derive the structure of the syntactic tree of the short sentence .", "label": "", "metadata": {}, "score": "66.14435"}
{"text": "Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . .", "label": "", "metadata": {}, "score": "66.296814"}
{"text": "Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . .", "label": "", "metadata": {}, "score": "66.296814"}
{"text": "In a dependency representation , every node in the tree structure is a surface word ( i.e. , there are no abstrac ... . by Paul Kingsbury , Martha Palmer - In Language Resources and Evaluation , 2002 . \" ...This paper describes our approach to the development of a Proposition Bank , which involves the addition of semantic information to the Penn English Treebank .", "label": "", "metadata": {}, "score": "66.40118"}
{"text": "The tenth CoNLL ( CoNLL - X ) saw a shared task on Multilingual Dependency Parsing .Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .", "label": "", "metadata": {}, "score": "66.404236"}
{"text": "15 , the noisy - channel model is again more conservative and decides not to drop any constituents .In contrast , the decision - based algorithm compresses the input substantially , but it fails to produce a grammatical output .[ 0259 ] .", "label": "", "metadata": {}, "score": "66.51321"}
{"text": "Computer readable media of .claim 18 in which , in the instructions for carrying out the method steps , paths are associated only when the similarity measure exceeds a threshold .Description .FIELD OF THE INVENTION .[ 0001 ] .", "label": "", "metadata": {}, "score": "66.68151"}
{"text": "[ 0078 ] .Care must be taken to account for polarity in inference relationships .High similarity values are often assigned to relations with opposite polarity .For example , \" X worsens Y \" has one of the highest similarity to \" X solves Y \" according to equation ( 2 ) .", "label": "", "metadata": {}, "score": "66.7104"}
{"text": "As shown , the tree t in FIG .11 spans the string abcde .Consider the parse tree for compression s1 , which is also shown in FIG .11 .[ 0187 ] .The factors P tree ( s1 ) and P expand .", "label": "", "metadata": {}, "score": "66.841446"}
{"text": "\" Labeled precision \" reflects the number of correctly labeled constituents identified by the rhetorical parser with respect to the total number of labeled constituents identified by the parser .[ 0162 ] .Labeled recall and precision figures were computed with respect to the ability of the discourse parser to identify elementary units , hierarchical text spans , text span nuclei and satellites , and rhetorical relations .", "label": "", "metadata": {}, "score": "66.86777"}
{"text": "[ 0039 ] .The above and other aspects will now be described in detail with reference to the accompanying drawings , wherein : . [ 0040 ] .[ 0040]FIG .1 shows and example of a discourse tree .[ 0041 ] .", "label": "", "metadata": {}, "score": "66.88803"}
{"text": "Though transformations continue to be important in Chomsky 's current theories , he has now abandoned the original notion of Deep Structure and Surface Structure .[ citation needed ] .To complicate the understanding of the development of Noam Chomsky 's theories , the precise meanings of Deep Structure and Surface Structure have changed over time - by the 1970s , the two were normally referred to simply as D - Structure and S - Structure by Chomskyan linguists .", "label": "", "metadata": {}, "score": "66.99054"}
{"text": "[ 0046 ] .Based on these common contexts , one can statistically determine that duty and responsibility have similar meanings .[ 0047 ] .In the algorithms for finding word similarity , dependency links are treated as contexts of words .", "label": "", "metadata": {}, "score": "67.00183"}
{"text": "In Table 1 , B1 corresponds to a majority - based baseline classifier that assigns the class \" none \" to all lexemes , and B2 to a baseline classifier that assigns a sentence boundary to every \" DOT \" ( that is , a period ( . ) , question mark ( ? ) , and/or exclamation point ( ! ) ) lexeme and a non - boundary to all other lexemes .", "label": "", "metadata": {}, "score": "67.01437"}
{"text": "The Shift - Reduce Action Identifier : Features used for Learning .[0130 ] .To make decisions with respect to parsing actions , the shift - reduce action identifier focuses on the three topmost trees in the stack and the first edt in the input list .", "label": "", "metadata": {}, "score": "67.038574"}
{"text": "In the 1960s , Chomsky introduced two central ideas relevant to the construction and evaluation of grammatical theories .The first was the distinction between competence and performance .Chomsky noted the obvious fact that people , when speaking in the real world , often make linguistic errors ( e.g. starting a sentence and then abandoning it midway through ) .", "label": "", "metadata": {}, "score": "67.052155"}
{"text": "In the 1960s , Chomsky introduced two central ideas relevant to the construction and evaluation of grammatical theories .The first was the distinction between competence and performance .Chomsky noted the obvious fact that people , when speaking in the real world , often make linguistic errors ( e.g. starting a sentence and then abandoning it midway through ) .", "label": "", "metadata": {}, "score": "67.052155"}
{"text": "[0099 ] Class 5 of atomic paraphrasing transformation is head omission .In this class , the atomic linguistic element may be a phrase X of Y ( as in \" group of students \" ) and a corresponding atomic paraphrasing element may be the word Y only ; or vice versa .", "label": "", "metadata": {}, "score": "67.124725"}
{"text": "As used in this patent document , a dependency relationship is a grammatical relationship , which is an asymmetric binary relationship between a word called head , and another word called modifier .The structure of a sentence is represented by a set of dependency relationships that form a tree .", "label": "", "metadata": {}, "score": "67.186325"}
{"text": "Summarization . [ 0167 ] .Various summarizing systems and techniques are described in detail below .In general , two different embodiments of a summarizer are described .First , a \" channel - based \" summarizer that uses a probabilistic approach for summarization ( equivalently , compression ) is described , and second , a \" decision - based \" summarizer that uses learned decision rules for summarization is described .", "label": "", "metadata": {}, "score": "67.210686"}
{"text": "The results of Table 5 show compression rate , and mean and standard deviation results across all judges , for each algorithm and corpus .The results show that the decision - based algorithm is the most aggressive : on average , it compresses sentences to about half of their original size .", "label": "", "metadata": {}, "score": "67.32446"}
{"text": "claim 64 wherein selecting one or more trees comprises selecting a list of trees , one for each possible compression length .The method of .claim 55 further comprising normalizing each potential solution based on compression length .The method of .", "label": "", "metadata": {}, "score": "67.34472"}
{"text": "[ 0208 ] .Decoding .[ 0209 ] .A vast number of potential compressions of a large tree t exist , but all of them can be packed efficiently into a shared - forest structure .For each node of t that has n children , the following operations are performed : .", "label": "", "metadata": {}, "score": "67.409706"}
{"text": "[ 0030 ] .Applying a statistical model may include using a stochastic channel model algorithm that , for example , performs minimal operations on a small tree to create a larger tree .Moreover , using a stochastic channel model algorithm may include probabilistically choosing an expansion template .", "label": "", "metadata": {}, "score": "67.4926"}
{"text": "Translating the paths into extraction templates , an IE system can be used to extract causal relations from medical text .The incorrect paths ( with asterisks ) in Table 4 exemplify a main source of error in the invention .The problem is that a cause in a causal event might be the effect of another causal event .", "label": "", "metadata": {}, "score": "67.61804"}
{"text": "[ 0160 ] .[ 0160]FIG .9 shows the learning curve that corresponds to the MUC corpus .As in the case of the discourse segmenter , this learning curve also suggests that more data can increase the accuracy of the shift - reduce action identifier .", "label": "", "metadata": {}, "score": "67.62792"}
{"text": "claim 61 wherein the generated parse tree has one or more nodes , and wherein identifying a forest of potential compressions comprises assigning an expansion - template probability to each node in the forest .The method of .claim 55 wherein extracting one or more high - probability solutions comprises selecting one or more trees based on a combination of each tree 's word - bigram and expansion - template score .", "label": "", "metadata": {}, "score": "67.77417"}
{"text": "[0156 ] .When the tree at the top of the stack subsumes a paragraph and starts with the marker \" but \" , the action to be applied is REDUCE - CONTRAST - NN . [0157 ] .The last rule in FIG .", "label": "", "metadata": {}, "score": "67.876816"}
{"text": "[0130 ]In practice , the multi - dimensional space defined by all available atomic paraphrasing pairs may result in an exceedingly large number of combinations of atomic paraphrasing pairs , making the computation prohibitively expensive .To overcome this problem , individual atomic paraphrasing pairs may be evaluated first using appropriate feature functions to filter out those paraphrasing pairs that score to low .", "label": "", "metadata": {}, "score": "67.98265"}
{"text": "MIT Press .ISBN 0 - 262 - 53007 - 4 .^ The Port - Royal Grammar of 1660 identified similar principles ; Chomsky , Noam ( 1972 ) .Language and Mind .Harcourt Brace Jovanovich .ISBN 0 - 15 - 147810 - 4 .", "label": "", "metadata": {}, "score": "68.08689"}
{"text": "For the second example in FIG .15 , the output of the Decision - based algorithm is grammatical , but the semantics are negatively affected .The noisy - channel algorithm deletes only the word \" break \" , which affects the correctness of the output less .", "label": "", "metadata": {}, "score": "68.13776"}
{"text": "0168 ] .Channel - Based Summarizer .[0169 ] .This section describes a probabilistic approach to the compression problem .In particular , a \" noisy channel \" framework is used .In this framework , a long text string is regarded as ( 1 ) originally being a short string , that ( 2 ) someone added some additional , optional text to it .", "label": "", "metadata": {}, "score": "68.140465"}
{"text": "claim 1 , the words are at the end points of the paths .Computer readable media of .claim 14 , in which the instructions for carrying out the step of associating paths with each other comprise instructions for carrying out the step of counting occurrences of words at the end points of specific paths .", "label": "", "metadata": {}, "score": "68.53641"}
{"text": "To complicate the understanding of the development of Noam Chomsky 's theories , the precise meanings of Deep Structure and Surface Structure have changed over time - by the 1970s , the two were normally referred to simply as D - Structure and S - Structure by Chomskyan linguists .", "label": "", "metadata": {}, "score": "68.63557"}
{"text": "To complicate the understanding of the development of Noam Chomsky 's theories , the precise meanings of Deep Structure and Surface Structure have changed over time - by the 1970s , the two were normally referred to simply as D - Structure and S - Structure by Chomskyan linguists .", "label": "", "metadata": {}, "score": "68.63557"}
{"text": "Iwould like toacknowledge the following people for their contribution to my education : I thank my advisor Mitch Marcus , who gave me the intellectual freedom to pursue what I believed to be the best way to approach natural language processing , and also gave me direction when necessary .", "label": "", "metadata": {}, "score": "68.69881"}
{"text": "However , for purposes of evaluation , the summarizing system was designed to be able to select a single compression .If log - probabilities as shown in FIG .16 are relied upon , then typically the shortest compression will be chosen .", "label": "", "metadata": {}, "score": "68.79704"}
{"text": "[ 0084 ] where common ws ( w 1 , w 2 ) refers to the common context words when estimating sim(w 1 , w 2 ) .An exemplary learning procedure is described as follows .It is noted that a word is a special case of a phrase .", "label": "", "metadata": {}, "score": "68.84475"}
{"text": "The Minimalist Program .MIT Press .In linguistics , a transformational - generative grammar ( TGG ) , or transformational grammar is a generative grammar , especially of a natural language , that has been developed in a Chomskyan tradition .", "label": "", "metadata": {}, "score": "68.87006"}
{"text": "We describe kernels for various natural ... \" .We describe the application of kernel methods to Natural Language Processing ( NLP ) problems .In many NLP tasks the objects being modeled are strings , trees , graphs or other discrete structures which require some mechanism to convert them into feature vectors .", "label": "", "metadata": {}, "score": "68.905975"}
{"text": "[ 0077 ] .The average number of words for each text was 405 in the MUC corpus , 2029 in the Brown corpus and 878 in the WSJ corpus .Each MUC text was tagged by three annotators ; each Brown and WSJ text was tagged by two annotators .", "label": "", "metadata": {}, "score": "68.93667"}
{"text": "To create a more reasonable competition , the log - probability is divided by the length of the compression , rewarding longer strings .This technique often is applied speech recognition .[ 0218 ] .If this normalized score is plotted against compression length , typically a ( bumpy ) U - shaped curve results , as illustrated in FIG .", "label": "", "metadata": {}, "score": "68.96153"}
{"text": "The performance is high with respect to recognizing sentence boundaries and ends of parenthetical units . none ( e ) .[ 0123 ] .Training the Discourse Parser .[0124 ] .[ 0124]FIG .8 shows a generalized flowchart for a process 800 for generating decision rules for the discourse parser .", "label": "", "metadata": {}, "score": "68.9769"}
{"text": "DROP operations are used to delete from the input list subsequences of words that correspond to syntactic constituents .A DROP X operations deletes from the input list all words that are spanned by constituent X in t. .[0234 ] .", "label": "", "metadata": {}, "score": "69.17697"}
{"text": "The major syntactic structures of English .Holt , Rinehart and Winston .ISBN 978 - 0 - 03 - 088042 - 1 .[ page needed ] .^ Emmon Bach , An Introduction to Transformational Grammars , Holt , Rinehart and Winston . Inc. , 1966 , pp .", "label": "", "metadata": {}, "score": "69.435616"}
{"text": "In fact , a real understanding of how a language can ( in Humboldt 's words ) \" make infinite use of finite means \" has developed only within the last thirty years , in the course of studies in the foundations of mathematics .", "label": "", "metadata": {}, "score": "69.48063"}
{"text": "In fact , a real understanding of how a language can ( in Humboldt 's words ) \" make infinite use of finite means \" has developed only within the last thirty years , in the course of studies in the foundations of mathematics .", "label": "", "metadata": {}, "score": "69.48063"}
{"text": "In other words , in algebraic terms , the I - Language is the actual function , whereas the E - Language is the extension of this function .In Michael Kenstowicz ( ed . )Ken Hale : A Life in Language .", "label": "", "metadata": {}, "score": "69.53685"}
{"text": "In other words , in algebraic terms , the I - Language is the actual function , whereas the E - Language is the extension of this function .In Michael Kenstowicz ( ed . )Ken Hale : A Life in Language .", "label": "", "metadata": {}, "score": "69.53685"}
{"text": "In other words , in algebraic terms , the I - Language is the actual function , whereas the E - Language is the extension of this function .In Michael Kenstowicz ( ed . )Ken Hale : A Life in Language .", "label": "", "metadata": {}, "score": "69.53685"}
{"text": "( VP . . . ) .[0203 ] .( PP . . . ) ) .[ 0204 ] .while the parse tree for its compressed version may begin .[ 0205 ] .( S ( NP . . . ) .", "label": "", "metadata": {}, "score": "69.5391"}
{"text": "14 ] [ 15 ] .Transformations .In TGG , Deep structures were generated by a set of phrase structure rules .For example a typical transformation in TG is the operation of subject - auxiliary inversion ( SAI ) .", "label": "", "metadata": {}, "score": "69.677925"}
{"text": "The cluster named \" evaluation - interpretation \" contained the rhetorical relations EVALUATION and INTERPRETATION .And the cluster named \" other \" contained rhetorical relations such as question - answer , proportion , restatement , and comparison , which were used very seldom in the corpus .", "label": "", "metadata": {}, "score": "69.840836"}
{"text": "[ 0028 ] ( c ) select a combination of candidate atomic paraphrasing pairs ; and .[ 0029 ] ( d ) construct a paraphrasing text of the input text using the atomic paraphrasing elements in the selected combination of candidate atomic paraphrasing pairs .", "label": "", "metadata": {}, "score": "69.87094"}
{"text": "0188 ] .The channel expansion - template factors and the channel PCFG ( new tree growth ) factors , which describe P expand .[0189 ] .A different compression will be scored with a different set of factors .", "label": "", "metadata": {}, "score": "69.94693"}
{"text": "claim 30 wherein applying the generated set of summarization decision rules comprises performing a sequence of modification operations on the tree structure .The method of .claim 38 wherein the sequence of modification operations comprises one or more of the following : a shift operation , a reduce operation , and a drop operation .", "label": "", "metadata": {}, "score": "69.98425"}
{"text": "Evaluation of the Summarizer Models To evaluate the compression algorithms , 32 sentence pairs were randomly selected from the parallel corpus .This random subset is referred to as the Test Corpus .The other 1035 sentence pairs were used for training as described above .", "label": "", "metadata": {}, "score": "70.01442"}
{"text": "The promotion sets of leaf nodes are the leaves themselves .The promotion sets of internal nodes are given by the union of the promotion sets of the immediate nuclei nodes .[0079 ] .As noted above , edus are defined functionally as clauses or clause - like units that are unequivocally the NUCLEUS or SATELLITE of a rhetorical relation that holds between two adjacent spans of text .", "label": "", "metadata": {}, "score": "70.321"}
{"text": "claim 1 wherein the training set comprises a plurality of annotated text segments and a plurality of elementary discourse units ( EDUs ) , each annotated text segment being associated with a set of EDUs that collectively represent the annotated text segment .", "label": "", "metadata": {}, "score": "70.32416"}
{"text": "In Proceedings of ACL -96 .pp .184 - 191 .Santa Cruz , Calif. ) ; Charniak , 2000 ( Charniak , E. 2000 .A Maximum - Entropy - Inspired Parser .In Proceedings of the North American Chapter of the Association for Computational Linguistics .", "label": "", "metadata": {}, "score": "70.4276"}
{"text": "In 1986 , Chomsky proposed a distinction between I - Language and E - Language , similar but not identical to the competence / performance distinction .[ 8 ] \" I - language \" refers to Internal language and is contrasted with External Language ( or E - language ) .", "label": "", "metadata": {}, "score": "70.72241"}
{"text": "The method as recited in claim 1 , wherein the method is incorporated in a search engine , the input text being generated by a user as a search query , and the paraphrasing text being used by the search engine as an alternative search query .", "label": "", "metadata": {}, "score": "70.797714"}
{"text": "0054 ]Class 7 : Change into different sentence types [ 0055 ] Who drew this picture ?vs. Tell me who drew this picture .[ 0056 ] Class 8 : Morphological derivation [ 0057 ] I was surprised that he destroyed the old house .", "label": "", "metadata": {}, "score": "70.82814"}
{"text": "0121 ] Block 420 learns among the similar sentences extracted above paraphrase patterns by replacing common words by a variable .The learned paraphrasing patterns are ranked based on their occurrence frequency which is denoted as supp ( AT ) .Preferably , only the patterns with top supp ( AT ) are kept .", "label": "", "metadata": {}, "score": "70.83017"}
{"text": "0127 ]Using the above - defined multiple atomic paraphrasing transformations , a paraphrasing model may be built which contains a large number of atomic linguistic elements and potential matching atomic paraphrasing elements .The information of the atomic linguistic elements and atomic paraphrasing elements , together with the statistical data of probabilities of the feature functions , can be stored in the system ( e.g. , stored as data 234 in memory 230 of FIG .", "label": "", "metadata": {}, "score": "70.91875"}
{"text": "In linguistics , a transformational grammar or transformational - generative grammar ( TG , TGG ) is a generative grammar , especially of a natural language , that involves the use of defined operations called transformations to produce new sentences from existing ones .", "label": "", "metadata": {}, "score": "71.02627"}
{"text": "0206 ] .( VP . . . ) ) .[ 0207 ] .Afterwards the events are normalized so that the probabilities add up to one .Not all nodes have corresponding partners ; some non - correspondences are due to incorrect parses , while others are due to legitimate reformulations that are beyond the scope of the simple channel model .", "label": "", "metadata": {}, "score": "71.18548"}
{"text": "claim 1 wherein the input text segment comprises a clause , a sentence , a paragraph or a treatise .A computer - implemented text parsing method comprising : . generating a set of one or more discourse segmenting decision rules based on a training set ; and .", "label": "", "metadata": {}, "score": "71.30612"}
{"text": "English for the Computer - The SUSANNE Corpus and Analytic Scheme .Clarendon Press .Oxford , England . ) shows that about 89 % of the dependency relationships in Minipar outputs are correct .Other broad - coverage English parsers such as those of Collins , 1996 ( Collins , M. J. 1996 .", "label": "", "metadata": {}, "score": "71.67241"}
{"text": "In that case , the source costs P tree ( t ) are : .[0190 ] .The channel costs P expand .[ 0191 ] .Now , the following values are compared - P expand .Note that P tree ( t ) and all the PCFG factors can be canceled out , as they appear in any potential compression .", "label": "", "metadata": {}, "score": "72.258736"}
{"text": "Automated summarization - that is , using a computer or other automated process to produce a summary - has many applications , for example , in information retrieval , abstracting , automatic test scoring , headline generation , television captioning , and audio scanning services for the blind .", "label": "", "metadata": {}, "score": "72.41743"}
{"text": "The method of . claim 21 wherein the incremental building of the discourse tree is based on predetermined decision rules .The method of .claim 23 wherein the predetermined decision rules comprise automatically learned decision rules .The method of .", "label": "", "metadata": {}, "score": "72.52054"}
{"text": "[ 0117 ] .Rule 7 specifies that no elementary or parenthetical unit boundary should be inserted immediately before a DOT .[ 0118 ] .As one can notice , the rules in FIG .6 a are more complex than typical manually derived rules .", "label": "", "metadata": {}, "score": "72.54265"}
{"text": "p .s . ) m .i .p .s .w . ) w .T .p .s . ) m .i .p .s .w . ) [ 0062 ] .Equation ( 2 ) measures the similarity between two slots .", "label": "", "metadata": {}, "score": "72.71892"}
{"text": "[ 0164 ] .[0164 ] 27 texts ( especially for the MUC and WSJ corpora , which have shorter texts that the Brown corpus ) , it has very low performance .Many of the intra - sentential edu boundaries are not identified , and as a consequence , the overall performance of the parser is low .", "label": "", "metadata": {}, "score": "72.821754"}
{"text": "compressing a tree structure by applying the generated set of summarization decision rules to the tree structure .The method of .claim 30 wherein the tree structure comprises a discourse tree .The method of .claim 30 wherein the tree structure comprises a syntactic tree .", "label": "", "metadata": {}, "score": "72.874176"}
{"text": "Claims : .The method as recited in claim 1 , wherein selecting the plurality of atomic linguistic elements comprises extracting atomic linguistic elements from the input text .The method as recited in claim 1 , wherein the at least one atomic linguistic element kind has multiple atomic linguistic elements .", "label": "", "metadata": {}, "score": "73.01398"}
{"text": "[ 0248 ] .[ 0248]FIGS .19 and 20 are respectively generalized flowcharts of the decision - based summarization and training processes described above .[0249 ] .As shown in FIG .19 , the first step 1902 in the decision - based summarization process 1900 is to receive the input text .", "label": "", "metadata": {}, "score": "73.1591"}
{"text": "Next , in step 1808 , the collected events are normalized to generate probabilities .These normalized events collectively represent the statistical learning model 1810 used by the channel - based summarizer .[0228 ] .Decision - based Summarizer .", "label": "", "metadata": {}, "score": "73.17511"}
{"text": "( 4.4 )The executives \" are here , just as I am , not because anyone agrees with every last line and jot and title of this economic program , \" Mr. Clinton acknowledged , but \" because it does far more good than harm .", "label": "", "metadata": {}, "score": "73.20461"}
{"text": "[ 0051 ] .[ 0052 ] .is shown in Table 2A. The first column of numbers in Table 2A represents the frequency counts of a word filling a slot of the path and the second column of numbers is the mutual information between a slot and a slot filler .", "label": "", "metadata": {}, "score": "73.30965"}
{"text": "A person searching for information inputs a search string to client computer 22 .The search string is forwarded to the server 16 , which parses the string and extracts paths from the parsed string .The paths are then forwarded to the computer 12 to retrieve associated paths from the database 24 .", "label": "", "metadata": {}, "score": "73.33554"}
{"text": "Rule 4 identifies an edu boundary before the occurrence of an \" and \" followed by a verb in the past tense ( VPT ) .This rule will correctly identify the marked edu boundary in sentence 4.5 .[ 0112 ] .", "label": "", "metadata": {}, "score": "73.58776"}
{"text": "In other words , the shift operation and the six reduce operations shown in FIG .5 are mathematically sufficient to derive the discourse tree of any unrestricted input text .[0093 ] .To create a binary tree whose immediate children are the trees at top and top-1 , an operation of type \" reduce - ns \" , \" reduce - sn \" , or \" reduce - nn \" is used .", "label": "", "metadata": {}, "score": "73.609184"}
{"text": "claim 39 wherein the reduce operation combines a plurality of trees into a larger tree .The method of .claim 39 wherein the drop operation deletes constituents from the tree structure .The method of .claim 30 wherein the training set comprises pre - generated long / short tree pairs .", "label": "", "metadata": {}, "score": "73.65831"}
{"text": "claim 55 wherein the generated parse tree comprises a discourse tree .The method of .claim 55 wherein the generated parse tree comprises a syntactic tree .The method of .claim 55 wherein applying a statistical model comprises using a stochastic channel model algorithm .", "label": "", "metadata": {}, "score": "73.83786"}
{"text": "The method of .claim 16 wherein determining boundaries comprises examining each lexeme in the input text segment in order .The method of . claim 17 further comprising assigning , for each lexeme , one of the following designations : sentence- break , EDU - break , start - parenthetical , end - parenthetical , and none .", "label": "", "metadata": {}, "score": "73.86269"}
{"text": "Pages 1 - 52 .( See p. 49 fn .2 for comment on E - Language . )Linguistic Theory in America ( Second Edition ) , Academic Press . 09826355 , 826355 , US 2003/0004915 A1 , US 2003/004915 A1 , US 20030004915 A1 , US 20030004915A1 , US 2003004915 A1 , US 2003004915A1 , US - A1 - 20030004915 , US - A1 - 2003004915 , US2003/0004915A1 , US2003/004915A1 , US20030004915 A1 , US20030004915A1 , US2003004915 A1 , US2003004915A1 .", "label": "", "metadata": {}, "score": "74.124245"}
{"text": "Then using the logarithm scale of the two frequency counts . log .p .S . w .P .P .S . log .p .S . w .S . p .S .S . w .", "label": "", "metadata": {}, "score": "74.19792"}
{"text": "The first method we discuss is based on a feature selection me ... . by Michael Collins , Nigel Duffy - Advances in Neural Information Processing Systems 14 , 2001 . \" ...We describe the application of kernel methods to Natural Language Processing ( NLP ) problems .", "label": "", "metadata": {}, "score": "74.25502"}
{"text": "The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .", "label": "", "metadata": {}, "score": "74.3672"}
{"text": "The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .", "label": "", "metadata": {}, "score": "74.3672"}
{"text": "[ 0031 ] .The generated parse tree may have one or more nodes , each node having N children ( wherein N is an integer ) .In that case , identifying a forest of potential compressions may include generating 2 N -1 new nodes , one node for each non - empty subset of the children , and packing the newly generated nodes into a whole .", "label": "", "metadata": {}, "score": "74.52274"}
{"text": "Table 1 lists a subset of the dependency relations in Minipar outputs .[ 0029 ] .The exemplary parser , Minipar , parses newspaper text at about 500 words per second on a Pentium - III(tm ) 700 Mhz with 500 MB memory .", "label": "", "metadata": {}, "score": "74.57215"}
{"text": "This paper describes our approach to the development of a Proposition Bank , which involves the addition of semantic information to the Penn English Treebank .Our primary goal is the labeling of syntactic nodes with specific argument labels that preserve the similarity of roles such as the window in John broke the window and the window broke .", "label": "", "metadata": {}, "score": "74.776955"}
{"text": "To save space , the SHIFT and ASSIGNTYPE operations are shown in FIG .14 on the same line .However , it should be understood that the SHIFT and ASSIGNTYPE operations correspond to two distinct actions .The ASSIGNTYPE K operation rewrites the POS tag of the word b ; the REDUCE operations modify the skeleton of the tree given as input .", "label": "", "metadata": {}, "score": "74.812775"}
{"text": "Numerous combinations may exist for a given set of atomic paraphrasing pairs .Each combination defines a set of atomic paraphrasing elements which together may be used to construct a paraphrasing text of the input text .The score function SC ( S OUT , S IN ) is used for computing a composite paraphrases score of each candidate combination .", "label": "", "metadata": {}, "score": "74.81308"}
{"text": "Ken Hale : A Life in Language .MIT Press .Pages 1 - 52 .( See p. 49 fn .2 for comment on E - Language . )^ Isac , Daniela , and Charles Reiss .I - language : An introduction to linguistics as cognitive science .", "label": "", "metadata": {}, "score": "74.848755"}
{"text": "( 3 ) . . .[ Close parallels between tests and practice tests are common , 12 ] [ some educators and researchers say .l3 ] [ Test preparation booklets , software and worksheets are a booming publishing subindustry .", "label": "", "metadata": {}, "score": "74.895744"}
{"text": "To each lexeme in a text , one learning case was associated using the features described below .The classes to be learned , which are associated with each lexeme , are \" sentence - break \" , \" edu - break \" , \" start - paren \" , and \" end - paren \" , and \" none \" .", "label": "", "metadata": {}, "score": "75.14491"}
{"text": "[ 0219]FIGS .17 and 18 are respectively generalized flowcharts of the channel - based summarization and training processes described above .[ 0220 ] .As shown in FIG .17 , the first step 1702 in the channel - based summarization process 1700 is to receive the input text .", "label": "", "metadata": {}, "score": "75.17477"}
{"text": "[0044 ] .duty can be modified by adjectives such as additional , administrative , assigned , assumed , collective , congressional , constitutional , . . ., so can responsibility ; .[ 0045 ] .duty can be the object of verbs such as accept , articulate , assert , assign , assume , attend to , avoid , become , breach , . . .", "label": "", "metadata": {}, "score": "75.18196"}
{"text": "The constraints are : . [ 0037 ] . slot fillers must be nouns because slots correspond to variables in inference rules and the variables are instantiated by entities ; . [ 0038 ] .any dependency relation that does not connect two content words is excluded from a path .", "label": "", "metadata": {}, "score": "75.18245"}
{"text": "( Supervised by Noam Chomsky , this dissertation introduced the idea of \" logical form . \" ) ^ Chomsky , Noam ( 2001 ) . \"Derivation by Phase . \"In other words , in algebraic terms , the I - Language is the actual function , whereas the E - Language is the extension of this function .", "label": "", "metadata": {}, "score": "75.31993"}
{"text": "Since the similarity between paths depends totally on the similarity of their slots , slots with the same kind of fillers are not distinguished in the present invention .It is easy to predict whether this type of error will happen in the outputs for a given path by computing the similarity between its SlotX and its SlotY. The higher this similarity is , the more likely the problem arises .", "label": "", "metadata": {}, "score": "75.332825"}
{"text": "[0121 ] .[ 0121]FIG .7 shows the learning curve that corresponds to the MUC corpus .It suggests that more data can increase the accuracy of the classifier .[0122 ] .The confusion matrix shown in Table 2 corresponds to a non - binary - based tree classifier that was trained on cases derived from 27 Brown texts and that was tested on cases derived from 3 different Brown texts , which were selected randomly .", "label": "", "metadata": {}, "score": "75.45382"}
{"text": "[ 0224]FIG .18 shows a generalized process for training a channel - based summarizer .As shown therein , the process 1800 starts in step 1802 with an input training set ( or corpus ) .As discussed above , this input training set comprises pairs of long - short text fragments , for example , long / short sentence pairs or treatise / abstract pairs .", "label": "", "metadata": {}, "score": "75.59438"}
{"text": "[0124 ] The above process may be repeated from block 510 for further learning and expansion .[0125 ]Accordingly , the following feature functions are defined for atomic paraphrasing transformation classes ( 11)-(15 ) : . [0126 ] where n lex ( AT ) is the iteration number in which the involved lexicon pair is learned .", "label": "", "metadata": {}, "score": "75.757866"}
{"text": "Computer readable media of .claim 14 in which , in the instructions for carrying out the method steps of .claim 1 , the similarity measure is based on the frequency of occurrence of words in the paths .Computer readable media of .", "label": "", "metadata": {}, "score": "75.772194"}
{"text": "claim 47 wherein the iterative reduction of the parse tree is performed based on a plurality of learned decision rules .The method of .claim 47 wherein iteratively reducing the parse tree comprises performing tree modification operations on the parse tree .", "label": "", "metadata": {}, "score": "75.85292"}
{"text": "0170 ] .In a noisy channel application , three problems must be solved : .[ 0171 ] .Source model .To every string s a probability s ) must be assigned .s ) represents the chance that s is generated as an \" original short string \" in the above hypothetical process .", "label": "", "metadata": {}, "score": "76.24623"}
{"text": "[0185 ] .Example of Using Statistical Models for Compression .[ 0186 ] .This example demonstrates how to tell whether one potential compression is more likely than another , according to the statistical models described above .FIG .", "label": "", "metadata": {}, "score": "76.56553"}
{"text": "[ 0050 ] .A database is used to store the frequency counts of the triples .We call this database a triple database .The triple database is organized as a collection of entries , where each entry consists of all triples with a common path .", "label": "", "metadata": {}, "score": "76.79308"}
{"text": "[ 0247 ] .Because the decision - based model is deterministic , it produces only one output .An advantage of this result is that compression using the decision - based model is very fast : it takes only a few milliseconds per sentence .", "label": "", "metadata": {}, "score": "77.016266"}
{"text": "[0073 ] .The discourse parser 304 takes in the edus from the segmenter 302 and applies the shift - reduce algorithm to incrementally build the discourse tree 305 .As indicated in FIG .3 , in this embodiment , each of the discourse segmenter 302 and the discourse parser 304 performs its operations based on a set of decision rules that were learned from analyzing a training set , as discussed in detail below .", "label": "", "metadata": {}, "score": "77.39004"}
{"text": "For example , in one embodiment , computer readable medium 230 has stored thereupon a plurality of instructions ( e.g. instructions in application programs 232 ) that , when executed by one or more processors 210 , causes the processor(s ) 210 to : . [ 0026 ] ( a ) select a plurality of atomic linguistic elements from an input text , wherein the plurality of atomic linguistic elements includes at least one atomic linguistic element kind selected from a word , a phrase , a pattern and a lexical dependency tree ; .", "label": "", "metadata": {}, "score": "77.89794"}
{"text": "0068 ] Most people died .vs. Few people survived .[ 0069 ] Class 12 : Verb nominalization [ 0070 ] He wrote the book .vs. He was the author of the book .[ 0071 ] Class 13 : Substitution using words with overlapping meanings [ 0072 ] He flew across the ocean . vs. He crossed the ocean by plane .", "label": "", "metadata": {}, "score": "77.921646"}
{"text": "A ten - fold cross - validation evaluation of the classifier yielded an accuracy of 98.16 % ( \u00b10.14 ) .A majority baseline classifier that chooses the action SHIFT has an accuracy of 28.72 % .[ 0244 ] .[ 0244]FIG .", "label": "", "metadata": {}, "score": "78.21539"}
{"text": "0058 ] .[ 0058]FIG .13 is a graph of adjusted log - probabilities for top scoring compressions at various compression lengths .[ 0059 ] .[ 0059]FIG .14 shows an example of incremental tree compression .[ 0060 ] .", "label": "", "metadata": {}, "score": "78.60761"}
{"text": "[ 0034 ] .the constraints significantly reduce the number of distinct paths and , consequently , the amount of computation required for computing similar paths ( described below ) ; and .[ 0035 ] .the constraints alleviate the sparse data problem because long paths tend to have very few occurrences .", "label": "", "metadata": {}, "score": "78.71113"}
{"text": "An example of an interpretable feature is the plural inflection on regular English nouns , e.g. , dog s .The word dogs can only be used to refer to several dogs , not a single dog , and so this inflection contributes to meaning , making it interpretable .", "label": "", "metadata": {}, "score": "78.74884"}
{"text": "As a result , the status of the tree [ 16 , 17 ] becomes \" nucleus \" and the status of the tree [ 18 , 19 ] becomes \" satellite .\" The rhetorical relation between the two trees is SMALL ATTRIBUTION .", "label": "", "metadata": {}, "score": "78.86598"}
{"text": "57 - 63 .Montreal , Canada . ) , the mutual information between a path slot and its filler can be computed by the formula : . m .i .p .Slot . w . ) log .p .", "label": "", "metadata": {}, "score": "79.04919"}
{"text": "However , since it is the SATELLITE of an EXPLANATION relation , it is treated as elementary .[ 0080 ] .( 1 ) [ Only the midday sun at tropical latitudes is warm enough ] [ to thaw ice on occasion , ] [ but any liquid water formed in this way would evaporate almost instantly ] [ because of the low atmospheric pressure . ]", "label": "", "metadata": {}, "score": "79.26997"}
{"text": "An example of an interpretable feature is the plural inflection on regular English nouns , e.g. dog s .The word dogs can only be used to refer to several dogs , not a single dog , and so this inflection contributes to meaning , making it interpretable .", "label": "", "metadata": {}, "score": "79.278244"}
{"text": "An example of an interpretable feature is the plural inflection on regular English nouns , e.g. dog s .The word dogs can only be used to refer to several dogs , not a single dog , and so this inflection contributes to meaning , making it interpretable .", "label": "", "metadata": {}, "score": "79.278244"}
{"text": "An example of an interpretable feature is the plural inflection on regular English nouns , e.g. dog s .The word dogs can only be used to refer to several dogs , not a single dog , and so this inflection contributes to meaning , making it interpretable .", "label": "", "metadata": {}, "score": "79.278244"}
{"text": "The method of . claim 5 wherein the reduce operations comprise one or more of the following six operations : reduce - ns , reduce - sn , reduce - nn , reduce - below - ns , reduce - below - sn , reduce- below - nn .", "label": "", "metadata": {}, "score": "79.44351"}
{"text": "The triple database records the fillers of SlotX and SlotY separately .Looking at the triple database , one would be unable to tell which SlotX filler occurred with which SlotY filler in the corpus .[ 0053 ] .Once the triple database is created , the similarity between two paths can be computed .", "label": "", "metadata": {}, "score": "79.59396"}
{"text": "The quantities that differ between the two proposed compressions are boxed above .Therefore , s1 will be preferred over t if and only if : . [0193 ] .[ 0194 ] .[ 0195 ] .[ 0196 ] .", "label": "", "metadata": {}, "score": "79.59515"}
{"text": "Each atomic linguistic element kind may be involved with multiple atomic linguistic elements .For example , among the plurality of atomic linguistic elements selected , one or more may be one atomic linguistic element kind , one or more may be another atomic linguistic element kind , and so on .", "label": "", "metadata": {}, "score": "80.848114"}
{"text": "The method of . claim 21 wherein the operations performed on the EDUs comprise one or more of the following : shift , reduce - ns , reduce - sn , reduce - nn , reduce - below - ns , reduce - below - sn , reduce - below - nn .", "label": "", "metadata": {}, "score": "80.92642"}
{"text": "claim 61 wherein the generated parse tree has one or more nodes , each node having N children ( wherein N is an integer ) , and wherein identifying a forest of potential compressions comprises : . generating 2 N -1 new nodes , one node for each non - empty subset of the children ; and . packing the newly generated nodes into a whole .", "label": "", "metadata": {}, "score": "81.01868"}
{"text": "That is , whenever a constituent with the same history is generated a second time , it is discarded if its probability is lower than the original version .I .. \" ...This article considers approaches which rerank the output of an existing probabilistic parser .", "label": "", "metadata": {}, "score": "81.26399"}
{"text": "It is not critical whether or not the \" original \" string is real or hypothetical .For example , in statistical machine translation , a French string could be regarded as originally being in English , but having noise added to it .", "label": "", "metadata": {}, "score": "81.36368"}
{"text": "claim 51 wherein the tree modification operations comprise one or more of the following : a shift operation , a reduce operation , and a drop operation .The method of .claim 52 wherein the reduce operation combines a plurality of trees into a larger tree .", "label": "", "metadata": {}, "score": "81.46922"}
{"text": "Rule 5 inserts edu boundaries before the occurrence of the word \" until \" , provided that \" until \" is followed not necessarily by a verb .This rule will correctly insert an edu boundary in example 4.6 .[0114 ] .", "label": "", "metadata": {}, "score": "81.55511"}
{"text": "In a path , dependency relations that are not slots are called internal relations .[ 0032 ] .Preferably , a set of constraints is imposed on the paths to be extracted from text for the following reasons : .[ 0033 ] .", "label": "", "metadata": {}, "score": "81.688065"}
{"text": "Rule 2 can correctly identify the beginning of the parenthetic unit 44 years old in sentence 4.2 because the unit is preceded by a comma and starts with a numeral ( CD ) followed by a plural noun ( NNS ) .", "label": "", "metadata": {}, "score": "82.13371"}
{"text": "[ 0024 ] The above - described process may be implemented with the help of a computing device , such as a server , a personal computer ( PC ) or a portable device having a computing unit .[ 0025 ] FIG .", "label": "", "metadata": {}, "score": "82.44966"}
{"text": "[ 0024 ] .In this patent document , \" comprising \" means \" including \" .In addition , a reference to an element by the indefinite article \" a \" does not exclude the possibility that more than one of the element is present .", "label": "", "metadata": {}, "score": "82.56056"}
{"text": "[ 0012 ] .The different types of reduce operations may include one or more of the following six operations : reduce - ns , reduce - sn , reduce - nn , reduce - below - ns , reduce - below - sn , reduce - below - nn .", "label": "", "metadata": {}, "score": "82.934074"}
{"text": "Slot .p .Slot .Slot . w .Equation ( 1 ) measures the strength of association between a slot of a path and a filler of that slot .The strength of the association between a slot of a path and a filler of that slot is measured by comparing the observed frequency of the filler - slot combination with the predicted frequency of the filler - slot combination assuming independence between the filler and the slot .", "label": "", "metadata": {}, "score": "83.02071"}
{"text": "Relation .W .W .w .W .w .W . wordnet .Relation .w .w .W .W .The Wordnet - based similarity function takes values in the interval [ 0,1 ] : the larger the value , the more similar with respect to a given Wordnet relation the two segments are .", "label": "", "metadata": {}, "score": "83.57292"}
{"text": "Description .RELATED APPLICATION .[ 0001 ] .This application claims the benefit of , and incorporates herein , U.S. Provisional Patent Application Ser .No .60/203,643 , filed May 11 , 2000 .ORIGIN OF INVENTION .[0002 ] The research and development described in this application were supported by the NSA under grant number MDA904 - 97 - 0262 and by DARPA / ITO under grant number MDA904 - 99-C-2535 .", "label": "", "metadata": {}, "score": "83.63288"}
{"text": "The root of both paths is find .A path begins and ends with two dependency relations , which are defined as the two slots of the path : SlotX on the left - hand side and SlotY on the right - hand side .", "label": "", "metadata": {}, "score": "84.55208"}
{"text": "The relationship between force and resign is an example of a verb - small clause relationship in the sentence shown in FIG .4 .[ 0041 ] .The paths extracted from the exemplary sentence shown in FIG .5 and their meanings are : .", "label": "", "metadata": {}, "score": "84.56784"}
{"text": "[ 0069 ] .To check that a path is valid 203 , the procedure set forward in FIG .8 is followed .For each relationship in the input path 301 , check that the path satisfies the constraints mentioned above .", "label": "", "metadata": {}, "score": "84.58534"}
{"text": "[ 0071 ] .[ 0071]FIG .3 shows a block diagram of a discourse tree generating system 300 that takes in input text 301 and produces discourse tree 305 .[ 0072 ] .The discourse segmenter 302 , which serves as a front - end to the discourse parser 304 , partitions the input text into edus .", "label": "", "metadata": {}, "score": "84.64696"}
{"text": "Blackwell Handbooks in Linguistics .Blackwell Publishers .p. 2 .ISBN 1 - 4051 - 5768 - 2 .", "label": "", "metadata": {}, "score": "84.68124"}
{"text": "The commonality of two slots is measured by the sum of association strength between the common fillers of the slots and the respective slots of the fillers .The totality of two slots is measured by the sum of association strength of the fillers of the first slot and the fillers of the second slot .", "label": "", "metadata": {}, "score": "84.6906"}
{"text": "However , not every feature is equally important .For example , the word he is much more frequent than the word sheriff .Two paths sharing the feature ( SlotX , he ) is less indicative of their similarity than if they shared the feature ( SlotX , sheriff ) .", "label": "", "metadata": {}, "score": "85.04143"}
{"text": "0223 ] .Finally , the best scoring candidate ( or candidates ) is ( are ) chosen as the final compression solutios ) in step 1710 .As described above , the best scoring candidate may be the one having the smallest log - probability / length of compression ratio .", "label": "", "metadata": {}, "score": "87.06499"}
{"text": "[ 0064 ] .The similarity between two slots ( p 1 , s ) and ( p 2 , s ) is then given by equation 2 .[0065 ] .The similarity between a pair of paths p 1 and p 2 is defined as the geometric average of the similarities of their SlotX and SlotY slots : . [ 0066 ] .", "label": "", "metadata": {}, "score": "87.12619"}
{"text": "In turn , this knowledge can be used to mine other types of knowledge from text .Consider the Top-50 paths similar to \" X causes Y \" generated by the invention in Table 4 .Suppose one wants to find out from the medical literature causal relationships involving the drug phentolamine .", "label": "", "metadata": {}, "score": "87.58644"}
{"text": "0122 ] Block 430 generalizes the learned paraphrasing patterns by replacing triggering lexicons by variables .The resulting generalized patterns are then used to extract more similar sentence pairs from the monolingual corpora .For example , the following two additional exemplary sentences are extracted because they fit the generalized pattern : Beethoven composed Symphonie No . 9 . vs. The composer of Symphonie No . 9 was Beethoven .", "label": "", "metadata": {}, "score": "87.78756"}
{"text": "The method as recited in claim 11 , wherein the probability value of each candidate atomic paraphrasing pair is obtained using an appropriate feature function of the atomic paraphrasing pair .The method as recited in claim 11 , further comprising : forming a plurality of combinations of candidate atomic paraphrasing pairs from a plurality of candidate atomic paraphrasing pairs ; andcomputing the composite paraphrasing score of each of the plurality of combinations of candidate atomic paraphrasing pairs .", "label": "", "metadata": {}, "score": "87.84578"}
{"text": "( 5.1 )[ If you refer to someone as a butt - head , 1 ] [ ordinarily speaking , no one is going to take that as any specific charge of any improper conduct or insinuation of any character trait . 2 ] .", "label": "", "metadata": {}, "score": "89.67316"}
{"text": "The input text may be generated by the local user on computing device 202 .The processor(s ) 210 presents the constructed paraphrasing text to the local user at computing device 202 .In other embodiments , the process may be implemented for network searches via network(s ) 290 , in which a user at computing device 202 searches data sources located on networked computing devices ( such as servers ) 241 , 242 and 243 .", "label": "", "metadata": {}, "score": "89.96094"}
{"text": "It is possible for a sentence to be both grammatical and meaningless , as in Chomsky 's famous example \" colorless green ideas sleep furiously .\" The use of such intuitive judgments permitted generative syntacticians to base their research on a methodology in which studying language through a corpus of observed speech became downplayed , since the grammatical properties of constructed sentences were considered to be appropriate data to build a grammatical model on .", "label": "", "metadata": {}, "score": "90.61471"}
{"text": "But such sentences manifest a linguistic problem distinct from that posed by meaningful but ungrammatical ( non)-sentences such as \" man the bit sandwich the \" , the meaning of which is fairly clear , but which no native speaker would accept as being well formed .", "label": "", "metadata": {}, "score": "91.08919"}
{"text": "But such sentences manifest a linguistic problem distinct from that posed by meaningful but ungrammatical ( non)-sentences such as \" man the bit sandwich the \" , the meaning of which is fairly clear , but which no native speaker would accept as being well formed .", "label": "", "metadata": {}, "score": "91.08919"}
{"text": "mi(p 1 , s , w)+mi(p 2 , s , w ) .[ 0063 ] .The totality of two slots ( p 1 , s ) and ( p 2 , s ) is given by : . 2 , s )", "label": "", "metadata": {}, "score": "91.50023"}
{"text": "[0058 ]He is a good teacher .vs. He teaches well . vs. He is good at teaching .[ 0059 ] The length of Long River is 6,000 kilometers .vs. Long River is as long as 6,000 kilometers .", "label": "", "metadata": {}, "score": "91.67784"}
{"text": "[ 0043 ] Class 2 : Active and passive exchange [ 0044 ] The gangster killed 3 innocent people .vs. 3 innocent people are killed by the gangster .[0045 ] Class 3 : Re - ordering of sentence components [ 0046 ] Tuesday they met .", "label": "", "metadata": {}, "score": "91.7607"}
{"text": "I thank all of my thesis committee members : John La erty from Carnegie Mellon University , Aravind Joshi , Lyle Ungar , and Mark Liberman , for their extremely valuable suggestions and comments about my thesis research .I thank Mike Collins , Jason Eisner , and Dan Melamed , with whom I 've had many stimulating and impromptu discussions in the LINC lab .", "label": "", "metadata": {}, "score": "92.54258"}
{"text": "[ 0047 ] Class 4 : Realization in different syntactic categories [ 0048 ] Palestinian leader Ararat vs. Ararat , Palestinian leader .[ 0049 ] Class 5 : Head omission [ 0050 ] group of students vs. students .[ 0051 ] Class 6 : Prepositional phrase attachment [ 0052 ] the Alabama plant vs. a plant in Alabama [ 0053 ] velvet dresses vs. dresses made of velvet .", "label": "", "metadata": {}, "score": "92.55498"}
{"text": "( 4.2 )Ms. Washington , 44 years old , would be the first woman and the first black to head the five - member commission that oversees the securities markets .[ 0108 ] .Rule 3 identifies the end of a sentence after the occurrence of a DOT ( period , questions mark , or exclamation mark ) that is not preceded or followed by another DOT and that is not followed by a DOUBLEQUOTE .", "label": "", "metadata": {}, "score": "93.51907"}
{"text": "The computers 10 and 12 are linked by a communication link 14 , which could be any suitable link and does not have to be permanent , including a network such as the Internet .The computer 12 may be accessed for the purpose of information retrieval by a server 16 , which in turn may be linked to the computer 12 by any suitable link 18 , such as the Internet .", "label": "", "metadata": {}, "score": "95.15993"}
{"text": "It is possible for a sentence to be both grammatical and meaningless , as in Chomsky 's famous example \" colorless green ideas sleep furiously \" .But such sentences manifest a linguistic problem distinct from that posed by meaningful but ungrammatical ( non)-sentences such as \" man the bit sandwich the \" , the meaning of which is fairly clear , but which no native speaker would accept as being well formed .", "label": "", "metadata": {}, "score": "96.41566"}
{"text": "sim . slot . slot .w .T .p .s . )T .p .s . ) m .i .p .s .w . ) m .i .p .s .w . ) w .", "label": "", "metadata": {}, "score": "97.464005"}
{"text": "( Y buys X sheep ) .subj : N .( Y sheep is bought from X ) .nn : N .( Y is bought from X ) .[ 0042 ] .A path is a binary relation between two entities .", "label": "", "metadata": {}, "score": "97.66399"}
{"text": "7 .If the program does not return no for any of the steps 302 , 304 , and 305 , then the program repeats this verification 306 for each relationship of the input path .If the program does not return no for steps 302 , 304 , and 305 for any relationship in the input path , then the program returns the indicator TRUE for that path in step 203 of FIG .", "label": "", "metadata": {}, "score": "97.7527"}
{"text": "If the program returns yes for this test , then the program checks if the relationship is an internal relation 303 .If the program returns yes for this test , then the program checks whether the relationship has a frequency greater than a threshold 304 and whether the relationship connects a verb with either an object clause or small clause 305 .", "label": "", "metadata": {}, "score": "97.951805"}
{"text": "17 ] [ 18 ] .In TGG , Deep structures were generated by a set of phrase structure rules .For example , a typical transformation in TG is the operation of subject - auxiliary inversion ( SAI ) .This rule takes as its input a declarative sentence with an auxiliary : \" John has eaten all the heirloom tomatoes . \" and transforms it into \" Has John eaten all the heirloom tomatoes ? \"", "label": "", "metadata": {}, "score": "101.8143"}
{"text": "0065 ] Class 11 : Converse word substitution [ 0066 ] John is Mary 's husband .vs. Mary is John 's wife .[0067 ] John sold the house to Mary .vs. Mary bought the house from John .", "label": "", "metadata": {}, "score": "105.159836"}
{"text": "Generalized transformations were originally proposed in the earliest forms of generative grammar ( e.g. Chomsky 1957 ) .They take small structures which are either atomic or generated by other rules , and combine them .For example , the generalized transformation of embedding would take the kernel \" Dave said X \" and the kernel \" Dan likes smoking \" and combine them into \" Dave said Dan likes smoking \" .", "label": "", "metadata": {}, "score": "105.368195"}
{"text": "Generalized transformations were originally proposed in the earliest forms of generative grammar ( e.g. Chomsky 1957 ) .They take small structures which are either atomic or generated by other rules , and combine them .For example , the generalized transformation of embedding would take the kernel \" Dave said X \" and the kernel \" Dan likes smoking \" and combine them into \" Dave said Dan likes smoking \" .", "label": "", "metadata": {}, "score": "105.368195"}
{"text": "Generalized transformations were originally proposed in the earliest forms of generative grammar ( e.g. Chomsky 1957 ) .They take small structures which are either atomic or generated by other rules , and combine them .For example , the generalized transformation of embedding would take the kernel \" Dave said X \" and the kernel \" Dan likes smoking \" and combine them into \" Dave said Dan likes smoking \" .", "label": "", "metadata": {}, "score": "105.368195"}
{"text": "14 ] [ 15 ] .In TGG , Deep structures were generated by a set of phrase structure rules .For example a typical transformation in TG is the operation of subject - auxiliary inversion ( SAI ) .This rule takes as its input a declarative sentence with an auxiliary : \" John has eaten all the heirloom tomatoes . \" and transforms it into \" Has John eaten all the heirloom tomatoes ? \"", "label": "", "metadata": {}, "score": "105.56259"}
{"text": "14 ] [ 15 ] .In TGG , Deep structures were generated by a set of phrase structure rules .For example a typical transformation in TG is the operation of subject - auxiliary inversion ( SAI ) .This rule takes as its input a declarative sentence with an auxiliary : \" John has eaten all the heirloom tomatoes . \" and transforms it into \" Has John eaten all the heirloom tomatoes ? \"", "label": "", "metadata": {}, "score": "105.56259"}
{"text": "Nantes , France . )Inference rules are extremely important in many fields such as natural language processing , information retrieval , and artificial intelligence in general .[ 0003 ] .For example , consider the query to an information retrieval system : \" Who is the author of the ' Star Spangled Banner ' ?", "label": "", "metadata": {}, "score": "106.82673"}
{"text": "0073 ] Bob excels at mathematics .vs. Bob studies mathematics well .[ 0074 ]He is a physicist .vs. He is a scientist trained in physics .[0075 ]Class 14 : Inference [ 0076 ] He was died of cancer .", "label": "", "metadata": {}, "score": "107.049225"}
{"text": "S .S . t .S .S . w .t . )S . w .t . )S . t .S . w .t . )S . t .S . w .t . )", "label": "", "metadata": {}, "score": "107.84296"}
{"text": "vs. The film made an impression on him . [0062 ]His machine operation is very good . vs. He operates the machine very well .[ 0063 ] Class 10 : Comparatives vs. superlatives [ 0064 ] He is smarter than everyone else . vs. He is the smartest one .", "label": "", "metadata": {}, "score": "111.49412"}
{"text": "For example , the generalized transformation of embedding would take the kernel \" Dave said X \" and the kernel \" Dan likes smoking \" and combine them into \" Dave said Dan likes smoking . \" GTs are thus structure building rather than structure changing .", "label": "", "metadata": {}, "score": "119.85472"}
{"text": "Computing system 201 is implemented with computing device 202 which includes processor(s ) 210 , I / O devices 220 , computer readable media ( e.g. , memory ) 230 , and network interface ( not shown ) .The computer readable media 230 stores application program modules 232 and data 234 ( such as paraphrasing data ) .", "label": "", "metadata": {}, "score": "123.080284"}
{"text": "[ 0004 ] . . . .Francis Scott Key wrote the \" Star Spangled Banner \" in 1814 .[ 0005 ] .higher than the sentence .[ 0006 ] . . . .comedian - actress Roseanne Barr sang her famous shrieking rendition of the \" Star Spangled Banner \" before a San Diego Padres - Cincinnati Reds game .", "label": "", "metadata": {}, "score": "130.07278"}
