{"text": "LDA creates a linear combination of the given independent features that yield the largest mean differences between the desired classes [ 2 ] .Given a set of . , for all the samples of all the . classes , the within - class scatter matrix .", "label": "", "metadata": {}, "score": "34.682934"}
{"text": "The generated PCA features do not have clear physical meanings .In contrast , LDA searches for those vectors in the underlying space that best discriminate among the classes rather than those that best describe the data .can be solved under different criteria such as the output variance maximization or MSE minimization .", "label": "", "metadata": {}, "score": "35.63478"}
{"text": "The overall performance of the two - stage approach is sensitive to the reduced dimension in the first stage .A generalization of LDA by using generalized SVD [ 153 ] can be used to solve the problem of singularity of .", "label": "", "metadata": {}, "score": "35.918556"}
{"text": "View at Google Scholar .T. Kohonen , \" Emergence of invariant - feature detectors in the adaptive - subspace self - organizing map , \" Biological Cybernetics , vol .75 , no .4 , pp .281 - 291 , 1996 .", "label": "", "metadata": {}, "score": "39.029823"}
{"text": "Each region is concatenated into a vector , and all the vectors constitute a training set .PCA is then applied to extract those prominent PCs , as such the image is compressed .Similar results for image compression have been reported in [ 11 , 142 ] by using a three - layer autoassociative network with BP learning .", "label": "", "metadata": {}, "score": "42.8497"}
{"text": "This is due to the fact that the physically meaningless features in Gram - Schmidt space can be linked back to the same number of variables of the measurement space , thus resulting in no dimensionality reduction .The GSO procedure starts with QR decomposition of the transpose of the full feature matrix , . , where .", "label": "", "metadata": {}, "score": "42.920906"}
{"text": "Other Generalizations of PCA .Simple neural models , described by differential equations , are derived in [ 104 , 105 ] to calculate the largest and smallest eigenvalues as well as their corresponding eigenvectors of any real symmetric matrix .Supervised PCA [ 106 , 107 ] is achieved by augmenting the input of the PCA with the class label of the data set .", "label": "", "metadata": {}, "score": "43.316135"}
{"text": "Based on a single - layer linear feedforward network , LDA algorithms are also given in [ 112 , 113 ] .References .G. H. Golub and C. F. van Loan , Matrix Computation , John Hopkins University Press , Baltimore , Md , USA , 2nd edition , 1989 .", "label": "", "metadata": {}, "score": "43.39872"}
{"text": "In [ 152 ] , a nonlinear discriminant analysis network with the MLP as the architecture and Fisher 's determinant ratio as the criterion function is obtained by combining the universal approximation properties of the MLP with the target - free nature of LDA .", "label": "", "metadata": {}, "score": "43.688393"}
{"text": "N. K. Nichols : Differential -- Algebraic Equations and .Control System Design / 208 \\\\ .G. W. Stewart : $ UTV$ Decompositions / 225 \\\\ .M. J. Todd : A Lower Bound on the Number of Iterations . of an Interior - Point Algorithm for Linear Programming / .", "label": "", "metadata": {}, "score": "43.90187"}
{"text": "See Golub and Van Loan ( 1996 ) for error bounds in this case , as well as for the underdetermined case .The solution of the overdetermined , full - rank problem may also be characterized as the solution of the linear system of equations .", "label": "", "metadata": {}, "score": "44.01333"}
{"text": "These neural network methods find wide applications in pattern recognition , blind source separation , adaptive signal processing , and information compression .Two methods that are strongly associated with PCA , namely , ICA and LDA , are described here in passing .", "label": "", "metadata": {}, "score": "44.02392"}
{"text": "The techniques for both problems are very similar .The following lemma characterizes the singular values and singular vectors of M .n . \" ...Every teacher of linear algebra should be familiar with the matrix singular value decomposition ( or SVD ) .", "label": "", "metadata": {}, "score": "44.334167"}
{"text": "All the scatter matrices are of size .The minimization of the MSE criterion is equivalent to the minimization of the trace of .or maximizing the trace of . . . .The objective for LDA is to maximize the between - class measure while minimizing the within - class measure after applying a . scatter matrices into . matrices .", "label": "", "metadata": {}, "score": "45.362545"}
{"text": "Any generalized eigenvector . is a stationary point of the criterion function .G .E .V .D .The LDA problem is a typical generalized EVD problem .The three - layer LDA network [ 111 ] is obtained by concatenating two Rubner - Tavan PCA subnetworks , each being trained by the Rubner - Tavan PCA algorithm [ 13 , 49 ] .", "label": "", "metadata": {}, "score": "45.47644"}
{"text": "The goal of the method is identical with that of the truncated singular value decomposition ( SVD ) , namely to preserve the quality of the resulting mat - vec product in the major singular directions of the matrix .The Lanczos - based approach achieves this goal by using a small number of Lanczos vectors , but it does not explicitly compute singular values / vectors of the matrix .", "label": "", "metadata": {}, "score": "45.519783"}
{"text": "then the stochastic system ( 1 ) can be transformed into a deterministic differential equation . to infinite magnitude , with a direction parallel to that of the eigenvector of .[ . .] corresponding to the largest eigenvalue [ 11 ] .", "label": "", "metadata": {}, "score": "45.900887"}
{"text": "In terms of reconstruction quality , we will compare our algorithm with CPPCA [ 11 ] by using the signal - to - noise ratio ( SNR ) .We note that Chen et al .[ 18 ] have recently provided an extensive study on the effects of linear projections on the performance of target detection and classification of hyperspectral imagery .", "label": "", "metadata": {}, "score": "46.100845"}
{"text": "This is done by using methods to obtain bounds .$ u$ is a given vector .Numerical examples are given for .the Gauss -- Seidel algorithm .Moreover , we show that .Dahlquist , Golub and Nash [ 1978 ] very good bounds of .", "label": "", "metadata": {}, "score": "46.15998"}
{"text": "For example , it is shown how the vectors .generated are algebraically related to ' controllable . space ' and ' observable space ' for a related linear .dynamical system .The algorithm described is .particularly appropriate for large sparse problems . \"", "label": "", "metadata": {}, "score": "46.444427"}
{"text": "nag_dtgexc ( f08yfc ) and nag_ztgexc ( f08ytc ) simply swap two diagonal elements or blocks , and may need to be called repeatedly to achieve a desired order .These functions can also compute the sensitivities of the cluster of eigenvalues and the deflating subspace .", "label": "", "metadata": {}, "score": "46.58736"}
{"text": "This advantage comes without sacrificing accuracy .The effectiveness of this approach is demonstrated on a few sample applications requiring dimension reduction , including information retrieval and face recognition .The proposed technique can be applied as a replacement to the truncated SVD technique whenever the problem can be formulated as a filtered mat - vec multiplication . \" ...", "label": "", "metadata": {}, "score": "46.789223"}
{"text": "We can see that the first three or four eigenvectors by CPPCA appear to be close to the true ones , while the rest are not .Hence if using more than four eigenvectors reconstructed by CPPCA , we observe a decrease in reconstruction quality or an increase in the norm error .", "label": "", "metadata": {}, "score": "46.88451"}
{"text": "book is completely up - to - date with the latest . developments on the Lanczos algorithm , .QR - factorizations , error propagation models , parameter .estimation problems , sparse systems , and .shape - preserving splines .", "label": "", "metadata": {}, "score": "47.10902"}
{"text": "It makes use of the implicit orthogonalization procedure that is built into it through an inflation technique .Localized Principal Component Analysis .The nonlinear PCA problem can be solved by partitioning the data space into a number of disjunctive regions and then estimating the principal subspace within each partition by linear PCA .", "label": "", "metadata": {}, "score": "47.30467"}
{"text": "In other cases Anderson et al .( 1999 ) gives code fragments to illustrate the computation of these estimates , and a number of the Chapter f08 example programs , for the driver functions , implement these code fragments .2.14.1 Least squares problems .", "label": "", "metadata": {}, "score": "47.379875"}
{"text": "More . importantly , under certain stationarity conditions , we .can show that the Lanczos - based methods are . asymptotically equivalent to the more costly SVD or . eigendecomposition based methods and that the . estimation of d is strongly consistent .", "label": "", "metadata": {}, "score": "47.465343"}
{"text": "Fast transforms and elliptic problems / T. Huckle / 393 .\\\\ .An interior - point method for minimizing the maximum .eigenvalue of a linear combination of matrices / F. .Jarre / 395 \\\\ .The lattice - ladder with generalized forgetting / J. .", "label": "", "metadata": {}, "score": "47.579308"}
{"text": "With the concepts of tensor , .-mode unfolding and matricization , an SVD - revision - based incremental learning method of bidirectional PCA [ 122 ] gives a close approximation to bidirectional PCA , but using less time .The uncorrelated multilinear PCA algorithm [ 123 ] is used for unsupervised subspace learning of tensorial data .", "label": "", "metadata": {}, "score": "47.68805"}
{"text": "SVD extracts between 50 to 300 dimensions or factors from all terms and aligns the documents in the new and much smaller term space .Syu et al .1996 have used LSI in combination with a two - layer ne ...", "label": "", "metadata": {}, "score": "47.702232"}
{"text": "Both the algorithms produce nearly orthonormal , but not exactly orthonormal , subspace basis or eigenvector estimates .If perfectly orthonormal eigenvector estimates are required , an orthonormalization procedure is necessary .Kalman - type RLS [ 31 ] combines the basic RLS algorithm with the GSO procedure .", "label": "", "metadata": {}, "score": "47.769897"}
{"text": "In its use of random projections , this technique can be considered to possess a certain duality with our approach to randomized SVD methods in HSI .However , CPPCA recovers coefficients of a known sparsity pattern in an unknown basis .", "label": "", "metadata": {}, "score": "47.87529"}
{"text": "in the order of descending eigenvalues .The performance is better than that of the generalized Hebbian algorithm ( GHA ) [ 8 ] .SLA has been extended in [ 25 ] so as to extract a noise robust projection .", "label": "", "metadata": {}, "score": "47.885643"}
{"text": "My primary goals in this article are to bring the topic to the attention of a broad audience , . ... ernative uses a rank 1 modification to split an SVD problem into two problems of lower dimension , the results of which can be used to find the SVD of the original problem .", "label": "", "metadata": {}, "score": "47.893402"}
{"text": "Here we introduce a variation of Algorithm 2 that only requires one pass over a large symmetric matrix .Now we define matrix . in rSVD is sufficient and can be applied to the whole dataset .Another restriction of CPPCA lies in the fact that the Rayleigh - Ritz method requires well - separated eigenvalues [ 22 ] , which might be true for the first few largest eigenvalues , but usually not true for the smaller eigenvalues .", "label": "", "metadata": {}, "score": "47.894623"}
{"text": "plane .Knowledge of the convex hull of the spectrum of .the matrix is required in order to choose parameters .upon which the iteration depends .Adaptive Chebyshev . algorithms , in which these parameters are determined by .using eigenvalue estimates computed by the power method .", "label": "", "metadata": {}, "score": "48.043415"}
{"text": "The much - smaller projected matrix is then factorized using a full - matrix decomposition such as SVD or PCA , after which the resulting singular vectors are backprojected to the original space .Compared to deterministic methods , probabilistic methods often offer the lower cost and more robustness in computation , while achieving high - accuracy results .", "label": "", "metadata": {}, "score": "48.22366"}
{"text": "The equivalence between LDA and such an application of CCA is proved .Two - dimensional CCA seeks linear correlation based on images directly .Motivated by locality - preserving CCA [ 138 ] and spectral clustering , a manifold learning method called local two - dimensional CCA [ 139 ] identifies the local correlation by weighting images differently according to their closeness .", "label": "", "metadata": {}, "score": "48.335396"}
{"text": "Finally , we draw some conclusions and identify some topics for future work in Section 4 .Review of Randomized Singular Value Decomposition .We start by defining terms and notations .The singular value decomposition ( SVD ) of a matrix . is a random matrix with independent and identically distributed ( i.i.d . ) entries .", "label": "", "metadata": {}, "score": "48.365852"}
{"text": "Experiments show that a high quality graph usually requires a small t which is close to one .A few of the practical details of the algorithms are as follows .First , the divide step uses an inexpensive Lanczos procedure to perform recursive spectral bisection .", "label": "", "metadata": {}, "score": "48.379745"}
{"text": "3.1.1 Driver functions . 3.1.1.1Linear least squares problems ( LLS ) .L .Q .factorization solve LLS using complete orthogonal factorization solve LLS using SVD solve LLS using divide - and - conquer SVD .Further functions are provided to compute all or part of the singular value decomposition of a real bidiagonal matrix ; the same functions can be used to compute the singular value decomposition of a real or complex matrix that has been reduced to bidiagonal form . to be stored conventionally ( see Section 3.3.1 in the f07 Chapter Introduction ) or in packed storage ( see Section 3.3.2 in the f07 Chapter Introduction ) ; or a band matrix to use band storage ( see Section 3.3.3 in the f07 Chapter Introduction ) .", "label": "", "metadata": {}, "score": "48.407646"}
{"text": "show that iterative methods based on incomplete LU . preconditioners have faster convergence rates than .block Jacobi relaxation methods .Numerical experiments . examine additional properties of the two classes of .methods , including the effects of direction of flow , . discretization , and grid ordering on performance .", "label": "", "metadata": {}, "score": "48.427303"}
{"text": "x . and .b .A .x . 2 . . .This chapter ( Chapter f08 ) contains driver functions to solve these problems with a single call , as well as computational functions that can be combined with functions in Chapter f07 to solve these linear least squares problems .", "label": "", "metadata": {}, "score": "48.63692"}
{"text": "Thus , . in the criterion ( 27 ) can be replaced by a transformed form so as to extract the next principal singular component .Using a deflation transformation , the two sets of neurons are trained with the cross - coupled Hebbian learning rules , which are given in [ 125 , 126 ] .", "label": "", "metadata": {}, "score": "48.69558"}
{"text": "T. Luk and S. Qiao / 241 \\\\ .The linear algebra of perfect reconstruction filtering ./ M. Stewart and G. Cybenko / 249 \\\\ .Determining rank in the presence of error / G. W. .Stewart / 275 \\\\ .", "label": "", "metadata": {}, "score": "48.76701"}
{"text": "It performs better than the global models implemented by PCA model and Kramer 's nonlinear PCA and is significantly faster than Kramer 's nonlinear PCA [ 65 ] .Adaptive combination of PCA and VQ is given in [ 89 ] , where an autoassociative network is used to perform PCA and simple competitive learning is used to perform VQ .", "label": "", "metadata": {}, "score": "48.86062"}
{"text": "Two - Dimensional PCA .Two - dimensional PCA [ 5 ] is especially designed for image representation .An image covariance matrix is constructed directly using the original image matrices instead of the transformed vectors , and its eigenvectors are derived for image feature extraction .", "label": "", "metadata": {}, "score": "48.915024"}
{"text": "The number of PCs to be extracted is not required to be known in advance .A hybrid hetero / autoassociative network [ 67 ] is constructed with a set of autoassociative outputs and a set of heteroassociative outputs .Both sets of output nodes are fully connected to the same bottleneck layer .", "label": "", "metadata": {}, "score": "48.97486"}
{"text": "Oja 's rule almost always converges exponentially to the unit eigenvector associated with the largest eigenvalue of ., starting from points in an invariant set [ 18 ] .A constant learning rate for fast convergence is suggested as .Principal Component Analysis .", "label": "", "metadata": {}, "score": "49.05555"}
{"text": "From some initial . operator A / sub 0/ with known eigenvalues and .If the eigenvectors of .H(t / sub 0/ ) are known , then they are used to determine .the eigenpairs of H(t / sub 0/+dt ) via the Rayleigh . quotient iteration , for some value of dt .", "label": "", "metadata": {}, "score": "49.14441"}
{"text": "Applications of an Element Model for Gaussian .Elimination / S. C. Eisenstat , M. H. Schultz , and 4 . H. .Sherman / 85 \\\\ .An Optimization Problem Arising from Tearing Methods ./ Alberto Sangiovanni - Vincentelli / 97 \\\\ .", "label": "", "metadata": {}, "score": "49.180397"}
{"text": "P. Howland and H. Park , \" Generalizing discriminant analysis using the generalized singular value decomposition , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .26 , no . 8 , pp .995 - 1006 , 2004 .", "label": "", "metadata": {}, "score": "49.259193"}
{"text": "Systems \" , .Quotient \" , .reconstruction algorithm \" , . applications to parametric image restoration and .resolution enhancement \" , . three - dimensional isopycnal flow \" , .three dimensions \" , .method for symmetric generalized eigenvalue problems \" , .", "label": "", "metadata": {}, "score": "49.300247"}
{"text": "We propose to populate a given ontology of categories and seed examples using matrix factorization with constraints , similar in spirit to singu ... \" .Matrix factorization methods are a well - scalable means of discovering generalizable information in noisy training data with many examples and many features .", "label": "", "metadata": {}, "score": "49.394485"}
{"text": "In the zero - mean case , this matrix becomes the covariance matrix .In the area of image coding , PCA is known as Karhunen - Loeve transform ( KLT ) [ 4 ] , which exploits correlation between neighboring pixels or groups of pixels for data compression .", "label": "", "metadata": {}, "score": "49.406918"}
{"text": "A new methodcal)x Frequency Domain Scoring ( FDS ) , which is based on the Fourier Transform is proposed . ... antic Indexing .This reduction of dimensionality reveals a latent structure of the documents that would not have been noticed otherwise .", "label": "", "metadata": {}, "score": "49.411087"}
{"text": "The . implementation is computationally more stable than .those usually given in the literature .A generalization . of Stiefel 's algorithm is developed which permits the . occasional exchange of two equations simultaneously .\" Approximations \" , .Decomposition \" , .", "label": "", "metadata": {}, "score": "49.480667"}
{"text": "original problem is found .A fundamental problem is the .selection of the step size dt .A simple criterion to . select dt is given .It is shown that the iterative . solver used to find the eigenvector at each step can be .", "label": "", "metadata": {}, "score": "49.503765"}
{"text": "Asymptotic and global error bounds can be obtained , which are generalizations of those given in Tables 2 and 3 .See Section 4.11 of Anderson et al .( 1999 ) for details .Functions are provided to compute estimates of reciprocal conditions numbers for eigenvalues and eigenspaces . 2.14.8", "label": "", "metadata": {}, "score": "49.512024"}
{"text": "Functions are provided to compute the eigenvalues and all or part of the Schur factorization of an upper Hessenberg matrix .Eigenvectors may be computed either from the upper Hessenberg form by inverse iteration , or from the Schur form by back - substitution ; these approaches are equally satisfactory for computing individual eigenvectors , but the latter may provide a more accurate basis for a subspace spanned by several eigenvectors .", "label": "", "metadata": {}, "score": "49.521835"}
{"text": "L. Trefethen and D. Bau , Numerical Linear Algebra , Society For Industrial Mathematics , 1997 .N. Halko , P. G. Martinsson , and J. A. Tropp , \" Finding structure with randomness : probabilistic algorithms for constructing approximate matrix decompositions , \" SIAM Review , vol .", "label": "", "metadata": {}, "score": "49.52903"}
{"text": "The algorithm is fully parallel in nature and evaluates singular values to tiny relative error if necessary .It ... \" .In this paper we propose an algorithm based on Laguerre 's iteration , rank two divide - and - conquer technique and a hybrid strategy for computing singular values of bidiagonal matrices .", "label": "", "metadata": {}, "score": "49.54693"}
{"text": "The localized PCA method is commonly used in image compression [ 8 ] .An image is often first transformation coded by PCA , and then the coefficients are quantized .VQ - PCA [ 65 ] is a locally linear model that uses VQ to define the Voronoi regions for localized PCA .", "label": "", "metadata": {}, "score": "49.73897"}
{"text": "The new algorithms are built on the polar decomposition and exploit the recently developed QR - based dynamically weighted Halley algorithm of Nakatsukasa , Bai , and Gygi , which computes the polar decomposition using a cubically convergent iteration based on the building blocks of QR factorization and matrix multiplication .", "label": "", "metadata": {}, "score": "49.763947"}
{"text": "Many SVD algorithms are reviewed in [ 129 ] .Tucker decomposition [ 130 ] decomposes a three - dimensional signal directly using three - dimensional PCA , which is a multilinear generalization of SVD to multidimensional data .For video frames , this higher - order SVD decomposes the dynamic texture as a multidimensional signal ( tensor ) without unfolding the video frames on column vectors .", "label": "", "metadata": {}, "score": "49.76395"}
{"text": "..e given matrix A is of approxi - rank about n/2 , the full SVD can be more efficient . \" ...Nearest neighbor graphs are widely used in data mining and machine learning .The brute - force method to compute the exact kNN graph takes \u0398(dn 2 ) time for n data points in the d dimensional Euclidean space .", "label": "", "metadata": {}, "score": "49.82453"}
{"text": "OOja , NOjia , and NOOjia require less computation load than the natural - gradient - based method [ 83 ] , self - stabilizing MCA [ 80 , 81 , 84 ] .By using the Rayleigh quotient as an energy function , invariant - norm MCA [ 85 ] is analytically proved to converge to the first MC of the input signals .", "label": "", "metadata": {}, "score": "49.929016"}
{"text": "The close connection between the SVD and the well ... \" .Every teacher of linear algebra should be familiar with the matrix singular value decomposition ( or SVD ) .It has interesting and attractive algebraic properties , and conveys important geometrical and theoretical insights about linear transformations .", "label": "", "metadata": {}, "score": "49.9782"}
{"text": "By calculating the eigenvectors of the covariance matrix of the input vector , PCA linearly transforms a high - dimensional input vector into a low - dimensional one whose components are uncorrelated .PCA is directly related to singular value decomposition ( SVD ) , and the most common way to perform PCA is via SVD of the data matrix .", "label": "", "metadata": {}, "score": "50.056274"}
{"text": "Hence it is highly suitable to use this low - dimensional representation for classification .Conclusions .As HSI data sets are growing increasingly massive , compression and dimensionality reduction for analytical purposes has become more and more critical .The randomized SVD algorithms proposed in this paper enable us to compress , reconstruct , and classify massive HSI datasets in an efficient way while maintaining high accuracy in comparison to exact SVD methods .", "label": "", "metadata": {}, "score": "50.106476"}
{"text": "\\\\ .A parallel image rendering algorithm and architecture . based on ray tracing and radiosity shading / E. F. .Deprettere and L.-S. Shen / 91 \\\\ .Reduction and approximation of linear computational .circuits / P. Dewilde and A.-J. van der Veen / 109 \\\\ .", "label": "", "metadata": {}, "score": "50.142254"}
{"text": "In addition , analytic results .show that iterative methods based on incomplete LU . preconditioners have faster convergence rates on . incomplete LU preconditioners have faster convergence . rates than block Jacobi relaxation methods .Numerical . experiments examine additional properties of the two . classes of methods , including the effects of flow , . discretization , and grid ordering on performance . \"", "label": "", "metadata": {}, "score": "50.215088"}
{"text": "The multifrontal method in a parallel environment / .I. S. Duff , N. I. M. Gould , M. Lescrenier , and J. K. .Reid / 93 \\\\ .A Schur - complement method for sparse quadratic .programming / Philip E. Gill , Walter Murray , Michael A. .", "label": "", "metadata": {}, "score": "50.32888"}
{"text": "For details the reader is referred to [ 2 ] and [ 7].07691850745534 0.22360679774998 0 .[ n. Phase one reduces a symmetric matrix A to the symmetric tridiagonal matrix T using MATLAB 's function hess.14571016336181 To this end we will deal with the symmetric eigenvalue problem .", "label": "", "metadata": {}, "score": "50.364777"}
{"text": "If eigenvectors are computed , the eigenvectors of the equivalent standard problem must be transformed back to those of the original generalized problem , as indicated in Section 2.8 ; functions from Chapter f16 may be used for this . 3.1.2.7 Nonsymmetric eigenvalue problems .", "label": "", "metadata": {}, "score": "50.38988"}
{"text": "2.14.9Other problems .Error bounds for other problems such as the generalized linear least squares problem and generalized singular value decomposition can be found in Anderson et al .( 1999 ) .2.15 Block Partitioned Algorithms .A number of the functions in this chapter use what is termed a block partitioned algorithm .", "label": "", "metadata": {}, "score": "50.477245"}
{"text": "In some problems only selected eigenvalues and associated eigenvectors are needed .ones(3 .for computing the dominant eigenpair of the matrix A.1 ) .end end Let ( [ 7].ones(n-1 .Housmvp and Rqi are used in the body of the function defl . v1 ) % % % % Deflated matrix B from the matrix A with a known eigenvector v1 of A.44721359549996 0.:))].", "label": "", "metadata": {}, "score": "50.561317"}
{"text": "( 1999 ) for more about block partitioned algorithms and the multishift strategy .The performance of a block partitioned algorithm varies to some extent with the block size - that is , the number of rows or columns per block .", "label": "", "metadata": {}, "score": "50.56826"}
{"text": "Consult a standard textbook for a more thorough discussion , for example Golub and Van Loan ( 1996 ) .2.1 Linear Least Squares Problems .x . is an .n .-element solution vector .In the most usual case .", "label": "", "metadata": {}, "score": "50.589466"}
{"text": "The singular value decomposition ( SVD ) is a fundamental matrix decomposition in linear algebra .It is widely applied in many modern techniques , for example , high- dimensional data visualization , dimension reduction , data mining , latent semantic analysis , and so forth .", "label": "", "metadata": {}, "score": "50.688904"}
{"text": "methods , such as the block Gauss -- Seidel and SOR .methods , and preconditioned generalized minimum . residual methods with incomplete LU preconditioners .New analysis extends convergence bounds for constant . coefficient problems to problems with separable .variable coefficients .", "label": "", "metadata": {}, "score": "50.76195"}
{"text": "5 , pp .1299 - 1319 , 1998 .View at Google Scholar \u00b7 View at Scopus .T. D. Sanger , \" An optimality principle for unsupervised learning , \" in Advances in Neural Information Processing Systems , D. S. Touretzky , Ed . , vol .", "label": "", "metadata": {}, "score": "50.796154"}
{"text": "( numerical methods ) ) ; 15A18 ( Eigenvalues of matrices , . etc . ) ; 15A60 ( Appl . of functional analysis to matrix . theory ) \" , .Zenios \" , . multistage generalizations \" , . common approach for generating web search results .", "label": "", "metadata": {}, "score": "50.856586"}
{"text": "In the coupled learning rules a first - order approximation of GSO is superior to the standard deflation procedure in terms of the orthonormality error and the quality of the eigenvectors and eigenvalues generated [ 47 ] .Anti - Hebbian Rule - Based Principal Component Analysis .", "label": "", "metadata": {}, "score": "50.900692"}
{"text": "135 \\\\ .Rational Krylov algorithms for nonsymmetric eigenvalue . problems / Axel Ruhe / 149 \\\\ .Highly parallel preconditioners for general sparse . matrices / Youcef Saad / 165 \\\\ .A two - stage iteration for solving nearly completely .", "label": "", "metadata": {}, "score": "51.01805"}
{"text": "approach lies in a specific choice of starting vectors .used in the block Lanczos algorithm so that the effect . of the perturbations is confined to lie in the first . diagonal block of the block tridiagonal matrix that is . produced by the block Lanczos algorithm .", "label": "", "metadata": {}, "score": "51.087875"}
{"text": "However , the quality of the restored image is very poor .This is because PCA assumes the Guassian statistics of the data set and a real picture usually does not satisfy this assumption .To improve the quality of the restored image , we employ nonoverlapping .", "label": "", "metadata": {}, "score": "51.193165"}
{"text": "The theoretical properties have been described , and close connections were revealed between the SV ... . ... es such as correspondence analysis and factor analysis .SVDPACKC has been developed to compute the SVD algorithm ( Berry et al . , 1993 ) . \" ...", "label": "", "metadata": {}, "score": "51.276962"}
{"text": "Golub \" , .Singular Value Decomposition \" , .P. O'Leary \" , .MathSciNet database \" , .Math .Appl .Conf .Ser .New Ser . \"Problems \" , . compute the norm of the error in iterative methods \" , . iterative methods ( Toulouse , 1995/1996 ) .", "label": "", "metadata": {}, "score": "51.46524"}
{"text": "The most popular methods for deterministic low - rank factorizations include the singular value decomposition ( SVD ) [ 9 ] and principal component analysis ( PCA ) [ 10 ] .This last advantage is especially suited for compression with the TSVD method , since the Frobenius norm of the residual matrix is the smallest among all rank- . representations of the original matrix , and hence we should expect a much lower entropy in its distributions - making it suitable for compressive coding schemes .", "label": "", "metadata": {}, "score": "51.49516"}
{"text": "M. T. Eismann , Hyperspectral Remote Sensing , SPIE Press , 2012 . H. F. Grahn and E. Paul Geladi , Techniques and Applications of Hyperspectral Image Analysis , Wiley , 2007 .J. M. Bioucas - Dias , A. Plaza , N. Dobigeon et al . , \" Hyperspectral unmixing overview : geometrical , statistical , and sparse regression - based approaches , \" IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing , vol .", "label": "", "metadata": {}, "score": "51.599823"}
{"text": "The matrix - matrix operations are performed by calls to the Level 3 BLAS ( see Chapter f16 ) , which are the key to achieving high performance on many modern computers .In the case of the .Q .R . algorithm for reducing an upper Hessenberg matrix to Schur form , a multishift strategy is used in order to improve performance .", "label": "", "metadata": {}, "score": "51.748188"}
{"text": "In this section we will show how to localize the eigenvalues of a matrix using celebrated Gershgorin 's Theorem .Function Gershg computes the centers and the radii of the Gershgorin circles of the matrix A and plots all Gershgorin circles .", "label": "", "metadata": {}, "score": "51.78162"}
{"text": "The unit centers are updated as in neural gas , while subspace learning is based on the RRLSA algorithm [ 33 ] .Similar to localized PCA , localized ICA is used to characterize nonlinear ICA .Clustering is first used for an overall coarse nonlinear representation of the underlying data and linear ICA is then applied in each cluster so as to describe local features of the data [ 92 ] .", "label": "", "metadata": {}, "score": "51.782845"}
{"text": "appropriately modified in order to generate a sequence . of bidiagonal matrices whose singular values .approximate those of the original sparse matrix .A . simple Lanczos recursion is proposed for determining .the corresponding left and right singular vectors .", "label": "", "metadata": {}, "score": "51.800556"}
{"text": "The Gauss - Seidel recursive PCA and Jacobi recursive PCA algorithms are derived in [ 36 ] .The LMSER algorithm is derived on the MSE criterion using the gradient - descent method [ 29 ] .LMSER reduces to Oja 's SLA algorithm when .", "label": "", "metadata": {}, "score": "51.98019"}
{"text": "The actual improvement depends to a large extent on the number of distinct eigenvalues and a good estimate thereof .However , at worst the algorithm behaves like a successive bandreduction approach to tridia ... . ... point operations .In addition , the need for data movement is reduced .", "label": "", "metadata": {}, "score": "51.99396"}
{"text": "Conventional methods of updating the . signal subspace rely on eigendecomposition or singular .value decomposition , which is computationally expensive . and difficult to implement in parallel .Recently , Xu . and Kailath proposed fast and parallelizable .Lanczos - based algorithms for estimating the signal . subspace based on the data matrices or the covariance . matrices .", "label": "", "metadata": {}, "score": "52.166443"}
{"text": "It is competitive with QR algorithm in serial mode in speed and advantageous in computing partial singular values .Error analysis and numerical results are presented . ...f this algorithm is to be reported in [ 9].2 The split - merge algorithm We briefly summarize the split - merge algorithm , developed by Li and Zeng [ 19 ] for the symmetric tridiagonal eigenvalue problem .", "label": "", "metadata": {}, "score": "52.171112"}
{"text": "C. Jutten and J. Herault , \" Blind separation of sources , part I : an adaptive algorithm based on neuromimetic architecture , \" Signal Processing , vol .24 , no . 1 , pp . 1 - 10 , 1991 .", "label": "", "metadata": {}, "score": "52.207767"}
{"text": "6 , pp .1318 - 1328 , 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. Cichocki and R. Unbehauen , Networks for Optimization and Signal Processing , John Wiley & Sons , New York , NY , USA , 1992 . A. Krogh and J. A. Hertz , \" Hebbian learning of principal components , \" in Parallel Processing in Neural Systems and Computers , R. Eckmiller , G. Hartmann , and G. Hauske , Eds . , pp .", "label": "", "metadata": {}, "score": "52.20852"}
{"text": "Reordering diagonal blocks in real Schur form / A. W. .Bojanczyk and P. Van Dooren / 351 \\\\ .Placing zeroes and the Kronecker canonical form / D. L. .Boley and P. Van Dooren / 353 \\\\ .Analysis of the recursive least squares lattice .", "label": "", "metadata": {}, "score": "52.21245"}
{"text": "( 1999 ) ) .They have been designed to be efficient on a wide range of high - performance computers , without compromising efficiency on conventional serial machines .It is not expected that you will need to read all of the following sections , but rather you will pick out those sections relevant to your particular problem . 2 Background to the Problems .", "label": "", "metadata": {}, "score": "52.230476"}
{"text": "By coding the residuals after subtracting the original matrix by its low - dimensional representation , one can compress the original data in a lossless manner , as in [ 8 ] .The success of lossless compression requires low entropy of the data distribution , and , as we shall see in the experiments section , generally the entropy of residuals for our method will be much lower than the entropy of the original data .", "label": "", "metadata": {}, "score": "52.26227"}
{"text": "The ability to form class . associations may be useful for building more refined .models of web traffic . \" applications \" , .ID 26914 , 3 \" , . different studies \" , . decomposition ( HOSVD ) in transforming a data tensor of . genes $ \\times $ ' ' x - settings , ' ' that is , different . settings of the experimental variable $ x \\times $ . ''", "label": "", "metadata": {}, "score": "52.350727"}
{"text": "Abstract .Spectral divide and conquer algorithms solve the eigenvalue problem for all the eigenvalues and eigenvectors by recursively computing an invariant subspace for a subset of the spectrum and using it to decouple the problem into two smaller subproblems .A number of such algorithms have been ... \" .", "label": "", "metadata": {}, "score": "52.41369"}
{"text": "Some measures of non - Gaussianity are kurtosis , differential entropy , negentropy , and mutual information , which can be derived from one another .Popular ICA algorithms include the Infomax , the natural - gradient , the equivariant adaptive separation via independence ( EASI ) , and the FastICA algorithms [ 10 ] .", "label": "", "metadata": {}, "score": "52.42296"}
{"text": "4 , pp .779 - 785 , 1994 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. Castrodad , Z. Xing , J. Greer , E. Bosch , L. Carin , and G. Sapiro , \" Learning discriminative sparse models for source separation and mapping of hyperspectral imagery , \" IEEE Transactions on Geoscience and Remote Sensing , vol .", "label": "", "metadata": {}, "score": "52.439613"}
{"text": "Some linear algebra issues in large - scale optimization ./ M. H. Wright / 315 \\\\ .Contributed Lectures / / 339 \\\\ .Direct and inverse unitary eigenproblems in signal .processing : an overview / G. S. Ammar , W. B. Gragg and .", "label": "", "metadata": {}, "score": "52.463486"}
{"text": "[ d2 .j ) .i ) ) .end .[ v1 .Function eigsen computes the condition number of all eigenvalues of a matrix .[ 3 ] ) .i))].[ d1 .[ n. Clearly Cond ( ) 1 .", "label": "", "metadata": {}, "score": "52.50586"}
{"text": "Finally functions are provided for reordering the Schur factorization , so that eigenvalues appear in any desired order on the diagonal of the Schur form .The functions nag_dtrexc ( f08qfc ) and nag_ztrexc ( f08qtc ) simply swap two diagonal elements or blocks , and may need to be called repeatedly to achieve a desired order .", "label": "", "metadata": {}, "score": "52.566124"}
{"text": "transform to iteratively obtain singular triplets , we obtained more than .an order of magnitude speed up ( we were doing SVDs on hundreds of .moderately sized sparse matrices ) .--Rob Tools . \" ... Abstract .A new rank revealing method is proposed .", "label": "", "metadata": {}, "score": "52.626614"}
{"text": "T. Kohonen , E. Oja , O. Simula , A. Visa , and J. Kangas , \" Engineering applications of the self - organizing map , \" Proceedings of the IEEE , vol .84 , no .10 , pp .", "label": "", "metadata": {}, "score": "52.63081"}
{"text": "[ 10 , 11 ] .Thus , Oja 's rule always converges to the principal component of .The Robbins - Monro conditions are not practical for implementation , especially for learning nonstationary data .Zufiria [ 17 ] has proposed to convert the stochastic discrete - time algorithms into their deterministic discrete - time formulations that characterize their average evolution from a conditional expectation perspective .", "label": "", "metadata": {}, "score": "52.71849"}
{"text": "However , RLS method may cause instability in certain cases .All these algorithms correspond to a three - layer .PCs in the descending order of the eigenvalues , where a GSO - like orthonormalization procedure is used . is sufficiently large , this term is negligible .", "label": "", "metadata": {}, "score": "52.79012"}
{"text": "Solving a least squares problem with boundary .constraints / L. Kaufman / 399 \\\\ .Estimating the extremal eigenvalues and condition .number by the Lanczos algorithm with a random start / .A generalized ADI iterative method / N. Levenberg and .", "label": "", "metadata": {}, "score": "52.813133"}
{"text": "C. Chatterjee , V. P. Roychowdhury , J. Ramos , and M. D. Zoltowski , \" Self - organizing algorithms for generalized eigen - decomposition , \" IEEE Transactions on Neural Networks , vol . 8 , no .6 , pp .", "label": "", "metadata": {}, "score": "52.90167"}
{"text": "The decision trees , at the end of Chapter f04 , direct you to the most appropriate functions in Chapters f04 or f08 .Chapters f04 and f08 contain Black Box ( or driver ) functions which enable standard linear least squares problems to be solved by a call to a single function .", "label": "", "metadata": {}, "score": "52.937527"}
{"text": "authors ' consideration , the block Lanczos scheme needs . to be applied to the original ( unperturbed ) matrix only . once and then the first diagonal block updated for each . perturbation so that for low - rank perturbations , the . algorithm presented in the paper results in significant .", "label": "", "metadata": {}, "score": "52.94879"}
{"text": "In order to increase the robustness of PCA against outliers , a robust version of the covariance matrix based on the .-estimator can be used .Several popular PCA algorithms have been generalized into robust versions by applying statistical physics approach [ 55 ] , where the defined objective function can be regarded as a soft generalization of an .", "label": "", "metadata": {}, "score": "53.009052"}
{"text": "Golub \" , . function for a set of document rank values is .iteratively solved with respect to a set of linked . documents until a first stability condition is . satisfied .After such condition is satisfied , some of .", "label": "", "metadata": {}, "score": "53.01846"}
{"text": "Algorithm 2 requires us to revisit the input matrix , while this may be not feasible for large matrices .For example , in ultraspectral imaging [ 20 ] , one could have thousands of spectral bands , and PCA on such datasets would require computing the eigenvectors and eigenvalues of a covariance matrix with a huge dimension .", "label": "", "metadata": {}, "score": "53.05017"}
{"text": "Each component of . is a stationary stochastic process , and only one of the components is allowed to be Gaussian distributed .The higher - order statistics of the original inputs are required for estimating ., rather than the second - order moment or covariance of the samples as used in PCA .", "label": "", "metadata": {}, "score": "53.077614"}
{"text": "View at Google Scholar .M. Jankovic and H. Ogawa , \" Time - oriented hierarchical method for computation of principal components using subspace learning algorithm , \" International Journal of Neural Systems , vol .14 , no .5 , pp .", "label": "", "metadata": {}, "score": "53.142097"}
{"text": "Two . classes of iterative methods are considered : block . stationary methods , such as the block Gauss -- Seidel and .SOR methods , and preconditioned generalized minimum . residual methods with incomplete LU preconditioners .New analysis extends convergence bounds for constant . coefficient problems to problems with separable .", "label": "", "metadata": {}, "score": "53.164993"}
{"text": "d .i . a .g .Typically , . and . are , respectively , the full covariance matrices of zero - mean stationary random signals .In this case , iterative generalized EVD algorithms can be obtained by using two PCA steps .", "label": "", "metadata": {}, "score": "53.22585"}
{"text": "The EVD formulates a . genes $ \\times $ genes network as a linear . superposition of genes $ \\times $ genes decorrelated . and decoupled rank-1 subnetworks , which can be . associated with functionally independent pathways .The . integrative pseudoinverse projection of a network .", "label": "", "metadata": {}, "score": "53.23932"}
{"text": "Quaternions and the symmetric eigenvalue problem / N. .Mackey / 405 \\\\ .Application of the Gauss - Seidel iteration to the RLS .\\\\ .Ranks of submatrices of a matrix and its inverse / C. . C. Paige and M. Wei / 409 \\\\ .", "label": "", "metadata": {}, "score": "53.286873"}
{"text": "The anti - Hebbian learning rule and its normalized version can be used for MCA [ 75 ] , but both may lead to infinite magnitudes of weights [ 76 ] .To avoid this , one can renormalize the weight vector at each iteration .", "label": "", "metadata": {}, "score": "53.460304"}
{"text": "In other words , LDA is a special case of CCA .CCA is equivalent to LDA for binary - class problems [ 134 ] , and it can be formulated as an LS problem for binary - class problems .CCA leads to a generalized EVD problem .", "label": "", "metadata": {}, "score": "53.51821"}
{"text": "In Section 2 , we introduce the Hebbian learning rule and Oja 's learning rule .Section 3 defines the PCA problem .Various PCA networks and algorithms are treated in Sections 4 - 7 .PCA algorithms based on the Hebbian rule are expanded in Section 4 .", "label": "", "metadata": {}, "score": "53.521233"}
{"text": "We will demonstrate that this fast SVD result is sufficiently accurate , and most importantly it can be derived immediately .Using this fast method , many infeasible modern techniques based on the SVD will become viable .Introduction .The singular value decomposition ( SVD ) and the principle component analysis ( PCA ) are fundamental in linear algebra and statistics .", "label": "", "metadata": {}, "score": "53.548695"}
{"text": "Singular Value Decomposition .Given two sets of random vectors with zero mean , . and . , the cross - correlation matrix is defined by .where . is the .th singular value , . and . are its corresponding left and right singular vectors , and .", "label": "", "metadata": {}, "score": "53.620373"}
{"text": "Dimensionality reduction is achieved by dropping the variables with insignificant variances .PCA is often used to select inputs , but it is not always useful , since the variance of a signal is not always related to the importance of the signal , for non - Gaussian signals .", "label": "", "metadata": {}, "score": "53.633015"}
{"text": "A probabilistic round - off error propagation model .Application to the eigenvalue problem / Francoise .Chatelin and Marie Christine Brunet / 139 \\\\ .Analysis of the Cholesky decomposition of a . semi - definite matrix / Nicholas J. Higham / 161 \\\\ .", "label": "", "metadata": {}, "score": "53.638496"}
{"text": "69 - 84 , 1985 .View at Google Scholar \u00b7 View at Scopus .T. D. Sanger , \" Optimal unsupervised learning in a single - layer linear feedforward neural network , \" Neural Networks , vol .2 , no .", "label": "", "metadata": {}, "score": "53.6991"}
{"text": "Other methods , such as Oja 's rule [ 14 ] , Yuille 's rule [ 15 ] , Linsker 's rule [ 16 ] , and Hassoun 's rule [ 11 ] , add a weight - decay term to the Hebbian rule to stabilize the algorithm .", "label": "", "metadata": {}, "score": "53.763977"}
{"text": "Hessenberg matrix , which directly produces the shift .vector without computing eigenvalues .This algorithm is . stable , more accurate , faster , and simpler than the . current alternative .It also allows for a consistent . shift strategy with dynamic adjustment of the number of . shifts . \"", "label": "", "metadata": {}, "score": "53.76899"}
{"text": "In a number of cases there are simple drivers , which just return the solution to the problem , as well as expert drivers , which return additional information , such as condition number estimates , and may offer additional facilities such as balancing .", "label": "", "metadata": {}, "score": "53.77275"}
{"text": "This is a nonlocal algorithm .During the training process , the outputs of the neurons are gradually uncorrelated and the lateral weights approach zero .The network should be trained until the lateral weights . are below a specified level .", "label": "", "metadata": {}, "score": "53.78818"}
{"text": "These storage schemes are compatible with those used in Chapters f07 and f16 , but different schemes for packed , band and tridiagonal storage are used in a few older functions in Chapters f01 , f02 , f03 and f04 .3.3.1 Conventional storage .", "label": "", "metadata": {}, "score": "53.803303"}
{"text": "2122 - 2130 , 1998 .View at Google Scholar \u00b7 View at Scopus .L. Xu , E. Oja , and C. Y. Suen , \" Modified Hebbian learning for curve and surface fitting , \" Neural Networks , vol .", "label": "", "metadata": {}, "score": "53.82168"}
{"text": "When computing eigenvalues of sym metric matrices and singular values of general matrices in finite precision arithmetic we in general only expect to compute them with an error bound pro - portional to the product of machine precision and the norm of the matrix .", "label": "", "metadata": {}, "score": "53.957912"}
{"text": "When computing eigenvalues of sym metric matrices and singular values of general matrices in finite precision arithmetic we in general only expect to compute them with an error bound pro - portional to the product of machine precision and the norm of the matrix .", "label": "", "metadata": {}, "score": "53.957912"}
{"text": "1042 - 1047 , 1996 .View at Google Scholar \u00b7 View at Scopus . Y. Miao and Y. Hua , \" Fast subspace tracking and neural network learning by a novel information criterion , \" IEEE Transactions on Signal Processing , vol .", "label": "", "metadata": {}, "score": "53.960888"}
{"text": "Assume that .Thus , when the estimated rank of the SCSVD is greater than the true rank , the accuracy of the SCSVD is pretty much the same as the SVD in the case of a small rank matrix .We would like to explore what happens if the estimated rank is smaller than the true rank .", "label": "", "metadata": {}, "score": "54.284554"}
{"text": "that of the original data without severely affecting performance of commonly used target detection and classification algorithms .The structure of the remainder of the paper is as follows .In Section 2 , we give a detailed overview of rSVD in Section 2.1 , the connections between this work and CPPCA in Section 2.2 , and the compression and reconstruction of HSI data in Section 2.3 .", "label": "", "metadata": {}, "score": "54.287643"}
{"text": "Thus , we can use the SCSVD in a huge data application to obtain a good approximated initial value .The updating algorithm of the SCMDS approach is discussed and compared with the general update approach .The performances of the SCMDS approach both in the computational time and error are worse than the general approach .", "label": "", "metadata": {}, "score": "54.29199"}
{"text": "The extracted features do not have any physical meaning .In contrast , feature selection decreases the size of the feature set or reduces the dimension of the features by discarding the raw information according to a criterion .Orthogonal decomposition is a well - known technique to eliminate ill - conditioning .", "label": "", "metadata": {}, "score": "54.307297"}
{"text": "131 - 137 , 2004 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . E. Oja and J. Karhunen , \" On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix , \" Journal of Mathematical Analysis and Applications , vol .", "label": "", "metadata": {}, "score": "54.377483"}
{"text": "369 - 383 , 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. D'Aspremont , F. Bach , and L. El Ghaoui , \" Optimal solutions for sparse principal component analysis , \" Journal of Machine Learning Research , vol .", "label": "", "metadata": {}, "score": "54.37992"}
{"text": "In the context of BSS , the higher - order statistics are necessary only for temporally uncorrelated stationary sources .Second - order statistics - based source separation exploits temporally correlated stationary sources and the nonstationarity of the sources [ 148 ] .", "label": "", "metadata": {}, "score": "54.487915"}
{"text": "Since all the order three complexities are restricted in the small number of data entries , we can therefore speed up MDS .Even in the case of big matrix with small rank , the traditional SVD method , for example the GR - SVD , can be implemented to be linear ( because of the dependency , many components of the matrix become zero in the GR - SVD algorithm ) .", "label": "", "metadata": {}, "score": "54.496773"}
{"text": "The stochastic approximation theory [ 6 ] , introduced by Robbins and Monro in 1951 , is an important tool for analyzing stochastic discrete - time systems including the classical gradient - descent method .Given a stochastic discrete - time system of the form .", "label": "", "metadata": {}, "score": "54.497948"}
{"text": "The rSVD algorithm as considered by [ 14 ] explores approximate matrix factorizations using random projections , separating the process into two stages .In the first stage , random sampling is used to obtain a reduced matrix whose range approximates the range of .", "label": "", "metadata": {}, "score": "54.530483"}
{"text": "/ K. V. Fernando and B. N. Parlett / 371 \\\\ .Orthogonal projection and total least squares / R. D. .Fierro and J. R. Bunch / 375 \\\\ .Gauss quadratures associated with the Arnoldi process .and the Lanczos algorithm / R. W. Freund and M. .", "label": "", "metadata": {}, "score": "54.593994"}
{"text": "Let A be a symmetric matrix .Recall that the SYISDA proceeds as follows : Scaling : Compute bounds on the spectrum ( A ) of A and use these boun ... . by Christian Bischof , Steven Huss - lederman , Xiaobai Sun , Anna Tsao , Thomas Turnbull , 1995 . \" ...", "label": "", "metadata": {}, "score": "54.611504"}
{"text": "( i ) ( ii ) Construct at least one matrix A for which function sol fails to compute a solution .b].Using functions Gausspre of Problem 5 .The second output parameter d stands for the determinant of A. 8 .", "label": "", "metadata": {}, "score": "54.6381"}
{"text": "The infomax principle [ 16 ] was first proposed by Linsker to describe a neural network algorithm .The principal subspace is derived by maximizing the mutual information criterion .The NIC algorithm [ 32 ] is obtained by applying the gradient - descent method to maximize the NIC , a cost function that is very similar to the mutual information criterion [ 16 , 45 ] but integrates a soft constraint on the weight orthogonalization .", "label": "", "metadata": {}, "score": "54.65351"}
{"text": "Many adaptive PCA algorithms actually optimize ( 17 ) by using the gradient - descent method [ 29 , 30 ] and the RLS method [ 30 - 34 ] .The gradient - descent or Hebbian rule - based algorithms are highly sensitive to .", "label": "", "metadata": {}, "score": "54.685005"}
{"text": "Higher - order statistics , defined by cumulants , are needed for a good characterization of non - Gaussian data .PCA can be generalized to distributions of the exponential family [ 53 ] .When the feature space is nonlinearly related to the input space , we need to use nonlinear PCA .", "label": "", "metadata": {}, "score": "54.686607"}
{"text": "C. Von Der Malsburg , \" Self organization of orientation sensitive cells in the striate cortex , \" Kybernetik , vol .14 , no . 2 , pp .85 - 100 , 1973 .View at Google Scholar \u00b7 View at Scopus . A. L. Yuille , D. M. Kammen , and D. S. Cohen , \" Quadrature and the development of orientation selective cortical cells by Hebb rules , \" Biological Cybernetics , vol .", "label": "", "metadata": {}, "score": "54.74022"}
{"text": "The gradient ascent - based WINC algorithm can be viewed be a weighted SLA [ 23 ] with an adaptive step size , leading to a much faster convergence speed .The RLS - based WINC algorithm not only provides fast convergence and high accuracy but also has low computational complexity .", "label": "", "metadata": {}, "score": "54.813248"}
{"text": "A heuristic complex extension of GHA [ 8 ] and APEX [ 35 ] is , respectively , given in [ 98 , 99 ] .The robust complex PCA algorithms have also been derived in [ 100 ] for hierarchically extracting PCs of complex - valued signals based on a robust statistics - based loss function .", "label": "", "metadata": {}, "score": "54.824883"}
{"text": "modified matrix over time .Applications to recursive .least squares computations using the covariance method .with sliding data windows are considered .Comparisons . are made with other adaptive and non - adaptive condition . estimators for recursive least squares problems .", "label": "", "metadata": {}, "score": "54.87449"}
{"text": "In practice , it usually grows just linearly ; .p .n .n . is often true , although generally only much weaker bounds can be actually proved .We normally describe .In addition to bounds for individual eigenvectors , bounds can be obtained for the spaces spanned by collections of eigenvectors .", "label": "", "metadata": {}, "score": "54.938576"}
{"text": "[127 ] .The algorithm can efficiently perform SVD of an ill - posed matrix .It can be used to solve the smallest singular component of the general matrix .and is especially useful for TLS problems .Some adaptive SVD algorithms for subspace tracking of a recursively updated data matrix have been surveyed and proposed in [ 128 ] .", "label": "", "metadata": {}, "score": "54.945446"}
{"text": "The ranking function is again modified and .process continues until complete . \"Conference , 2004 .CSB 2004 .expression data \" , .Golub \" , .eigenvalue problem solvers \" , .Numerical Mathematics \" , . definite systems \" , .", "label": "", "metadata": {}, "score": "54.960457"}
{"text": "4 , pp .558 - 569 , 1993 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .J. Karhunen and S. Malaroiu , \" Locally linear Independent Component Analysis , \" in Proceedings of the International Joint Conference on Neural Networks ( IJCNN ' 99 ) , pp .", "label": "", "metadata": {}, "score": "54.974014"}
{"text": "Dimensionality reduction techniques are generally regarded as lossy compression ; that is , the original data is not exactly represented or reconstructed by the lower - dimensional space .For lossless compression of HSI data , there have been efforts to exploit the correlation structure within HSI data plus coding the residuals after stripping off the correlated parts ; see , for example , [ 7 , 8 ] .", "label": "", "metadata": {}, "score": "54.978966"}
{"text": "In this section we discuss methods for computing the least squares solution .MATLAB 's function chol calculates the matrix H from A or generates an error message if A is not positive definite.g . such a system is inconsistent .[ 6 ] ) .", "label": "", "metadata": {}, "score": "55.070187"}
{"text": "Based on the APCA network , the principal singular component of . can be efficiently extracted by using a modification to the cross - coupled Hebbian rule with global asymptotic convergence [ 127 ] .This algorithm is extended for extracting the principal singular component of a general matrix . by replacing the cross - correlation matrix by a general nonsquare matrix [ 127 ] .", "label": "", "metadata": {}, "score": "55.073956"}
{"text": "The TLS technique achieves a better global optimal objective than the LS technique [ 1 ] .Both the TLS and LS problems can be solved by SVD .However , the TLS technique is computationally much more expensive than the least squares ( LSs ) technique [ 74 ] .", "label": "", "metadata": {}, "score": "55.090244"}
{"text": "R(t ) , slowly varying with time .It is convenient to .track the left singular vectors associated with the .largest singular values of the triangular factor , L(t ) , . of its Cholesky factorization .These algorithms are .", "label": "", "metadata": {}, "score": "55.15261"}
{"text": "Algorithm \" , .Wei - Pai Tang \" , .Equations \" , . subspace method \" , .An IMACS Journal \" , .Generalized Eigenvalue Problems \" , .Processing ( VECPAR 98 ) \" , . applications to preconditioning \" , .", "label": "", "metadata": {}, "score": "55.173866"}
{"text": "Some adaptive algorithms derived from the gradient descent , conjugate direction , and Newton - Raphson methods , whose simulation results are better than that of the gradient - descent method [ 29 ] , have also been proposed in [ 44 ] .", "label": "", "metadata": {}, "score": "55.178032"}
{"text": "namely the normal equations .e. etc . 22 .n ) that generates Vandermonde 's matrix V used in the polynomial least - squares fit .The degree of the approximating polynomial is n while the x - coordinates of the points to be fitted are stored in the vector t. 20 .", "label": "", "metadata": {}, "score": "55.187332"}
{"text": "319 - 329 , 1998 .View at Google Scholar \u00b7 View at Scopus . Y. Chauvin , \" Principal component analysis by gradient descent on a constrained linear Hebbian cell , \" in Proceedings of the International Joint Conference on Neural Networks ( IJCNN ' 89 ) , pp .", "label": "", "metadata": {}, "score": "55.221664"}
{"text": "problems / James M. Varah / 187 \\\\ .Rounding errors in algebraic process - in level - index .arithmetic / F. W J. Olver / 197 \\\\ .Experiments in tearing large sparse systems / Mario .Arioli and lain S. Duff / 207 \\\\ .", "label": "", "metadata": {}, "score": "55.263863"}
{"text": "Finally , a hash table is used to avoid repeating distance calculations during the divide and conquer process .The combination of these techniques is shown to yield quite effective algorithms for building kNN graphs . ... by Laurence A.F. Park , Marimuthu Palaniswami , Ramamohanarao Kotagiri - In Luc de Raedt and Arno Siebes , editors , Principles of Data Mining and Knowledge Discovery , number 2168 in Lecture Notes in Artificial Intelligence , 2001 . \" ...", "label": "", "metadata": {}, "score": "55.305885"}
{"text": "210 - 219 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .G. W. Cottrell , P. Munro , and D. Zipser , \" Learning internal representations from gray - scale images : an example of extensional programming , \" in Proceedings of the 9th Conference of tile Cognitive Science Society , pp .", "label": "", "metadata": {}, "score": "55.35076"}
{"text": "with the corresponding values of the mapping for . being .The first stage of canonical correlation is to choose . and . to maximize the correlation between the two vectors .m . a .x .c .o .", "label": "", "metadata": {}, "score": "55.353687"}
{"text": "Graph Theory and Gaussian Elimination / Robert Endre .Tarjan / 3 \\\\ .Partitioning Using PAQ / Thomas D. Howell / 23 \\\\ .Block Methods for Solving Sparse Linear Systems / .James R. Bunch / 39 \\\\ .A Recursive Analysis of Dissection Strategies / .", "label": "", "metadata": {}, "score": "55.38774"}
{"text": "You may wish to continue in this manner using larger values for the second and third parameters in the function repmat .In this exercise you are to experiment with the eigenvalues and eigenvectors of the partitioned matrices .what conjecture about the eigenvalues and eigenvectors of B can be formulated ?", "label": "", "metadata": {}, "score": "55.401722"}
{"text": "These methods are useful in adaptive signal processing , blind signal separation ( BSS ) , pattern recognition , and information compression .Introduction .In information processing such as pattern recognition , data compression and coding , image processing , high - resolution spectrum analysis , and adaptive beamforming , feature extraction or feature selection is necessary to deal with the large storage of raw data .", "label": "", "metadata": {}, "score": "55.411297"}
{"text": "A desirable number of neurons can be decided during the learning process .When the environment is changing over time , a new PC can be added to compensate for the change without affecting the previously computed PCs , and the network structure can be expanded if necessary .", "label": "", "metadata": {}, "score": "55.420418"}
{"text": "You do not normally need to be aware of what value is being used .Different block sizes may be used for different functions .Values in the range .to . are typical .On more conventional machines there is often no advantage from using a block partitioned algorithm , and then the functions use an unblocked algorithm ( effectively a block size of . , relying solely on calls to the Level 2 BLAS ( see Chapter f16 again ) .", "label": "", "metadata": {}, "score": "55.432945"}
{"text": "MATLAB 's function svd computes matrices of the SVD of A by invoking the command [ U. where U and V are unitary matrices of dimensions m and n. function [ U. .then only the first n columns of U are computed and S is an n - by - n matrix .", "label": "", "metadata": {}, "score": "55.435944"}
{"text": "Block implementations of the symmetric QR and Jacobi . algorithms / P. Arbenz and M. Oettli / 345 \\\\ .Linear algebra for large - scale information retrieval .applications / M. W. Berry / 347 \\\\ .Matched filter vs. least - squares approximation / L. H. .", "label": "", "metadata": {}, "score": "55.44483"}
{"text": "-dimensional subspace spanned by the first .PCs of the data .The vectors of weights leading to the hidden units form a basis set which spans the principal subspace , and data compression therefore occurs in the bottleneck layer .Many applications of the autoassociative MLP for PCA are available in the literature [ 60 - 63 ] .", "label": "", "metadata": {}, "score": "55.485394"}
{"text": "By using incremental learning ( which should be understood as learning data instance by data instance ) this problem can be avoided . A. The Base Algorithm The technique presented here is very sim ... . \" ...Soft Computing techniques like neural networks are well suited to process texts due to their vague processing capabilities .", "label": "", "metadata": {}, "score": "55.486744"}
{"text": "For example , for complex PCA , complex PCs can be extracted by minimizing the MSE function .where . is the .th input complex vector .In [ 96 ] , a complex - valued neural network model is developed for nonlinear complex PCA .", "label": "", "metadata": {}, "score": "55.495564"}
{"text": "Local two - dimensional CCA is formulated as solving generalized eigenvalue equations tuned by Laplacian matrices .CCArc [ 140 ] is a two - dimensional CCA that is based on representing the image as the sets of its rows and columns and implementation of CCA using these sets .", "label": "", "metadata": {}, "score": "55.496635"}
{"text": "can be solved applying successively Gauss transformations to the augmented matrix [ A. Given a square matrix A. Write MATLAB function [ L. A solution x then can be found using back substitution .Write a function dettri(A ) that computes the determinant of the matrix A. Suppose that L n x n is lower triangular and b n. 6 . j ) .", "label": "", "metadata": {}, "score": "55.557743"}
{"text": "12 , pp .3328 - 3333 , 2000 .View at Google Scholar \u00b7 View at Scopus .P. Foldiak , \" Adaptive network for optimal linear feature extraction , \" in Proceedings of the International Joint Conference on Neural Networks ( IJCNN ' 89 ) , pp .", "label": "", "metadata": {}, "score": "55.611313"}
{"text": "In coupled PCA / MCA algorithms [ 46 ] , both the eigenvalues and the eigenvectors are simultaneously adapted .The Newton method yields averaged systems with identical speed of convergence in all eigendirections .In order to extract multiple PCs , one has to apply an orthonormalization procedure , like GSO , or its first - order approximation as used in SLA [ 7 , 22 ] , or deflation as in GHA [ 8 ] .", "label": "", "metadata": {}, "score": "55.61225"}
{"text": "ASSOM [ 93 ] is another localized PCA for unsupervised extraction of invariant local features from the input data .ASSOM associates a subspace instead of a single weight vector to each node of the SOM .The subspaces in ASSOM can be formed by applying ICA [ 94 ] .", "label": "", "metadata": {}, "score": "55.66532"}
{"text": "It has . input and .output nodes .The third layer has .nodes .Nonlinear activation functions such as the sigmoidal functions are used in the second and fourth layers , while the nodes in the bottleneck and output layers usually have linear activation functions , although they can be nonlinear .", "label": "", "metadata": {}, "score": "55.67024"}
{"text": "An implementation of the QMR method based on coupled .two - term recurrences / R. W. Freund and N. M. Nachtigal ./ 381 \\\\ .Computationally efficient homotopies for the H 2 model .order reduction problem / Y. Ge , L. T. Watson , E. G. .", "label": "", "metadata": {}, "score": "55.68656"}
{"text": "Nachtigal \" , .Jahre Mathematische Gesellschaft in Hamburg , Teil 4 .( Hamburg , 1990 ) .Non - Self - Adjoint Linear Systems \" , .Problems \" , . vector x such that //Ax -b///sub 2/-min , subject to the .", "label": "", "metadata": {}, "score": "55.687904"}
{"text": "1269 - 1294 , 2008 .View at Google Scholar \u00b7 View at Scopus . S. Y. Kung , \" Constrained principal component analysis via an orthogonal learning network , \" in Proceedings of the IEEE International Symposium on Circuits and Systems ( ISCAS ' 90 ) , pp .", "label": "", "metadata": {}, "score": "55.70903"}
{"text": "the Lanczos and conjugate gradient algorithms .The book . bridges different mathematical areas to obtain . algorithms to estimate bilinear forms involving two . vectors and a function of the matrix .The first part of .the book provides the necessary mathematical background . and explains the theory .", "label": "", "metadata": {}, "score": "55.73538"}
{"text": "And we call this approach to obtaining .From the SCPCA to the SCSVD .The concepts of the SVD and the PCA are very similar .Since the PCA starts from decomposing the covariance matrix of a data set , it can be considered as adjusting the center of mass of a row vector to zero .", "label": "", "metadata": {}, "score": "55.77533"}
{"text": "Through successive variance maximization , uncorrelated multilinear PCA seeks a tensor - to - vector projection that captures most of the variation in the original tensorial input while producing uncorrelated features .It is the only multilinear extension of PCA that can produce uncorrelated features in a fashion similar to that of PCA , in contrast to other multilinear PCA extensions , such as two - dimensional PCA [ 5 ] and multilinear PCA ( MPCA ) [", "label": "", "metadata": {}, "score": "55.77974"}
{"text": "v ) % % % % % Power iteration with the Rayleigh quotient .Vector v is an approximation of the eigenvector associated with the dominant eigenvalue la of the matrix A. iter ) % % % % % % The Rayleigh quotient iteration.37019976472726i Note a dramatic change in the eigenvalues .", "label": "", "metadata": {}, "score": "55.78022"}
{"text": "The decision trees , at the end of Chapter f02 , direct you to the most appropriate functions in Chapters f02 or f08 .Chapters f02 and f08 contain Black Box ( or driver ) functions which enable standard types of problem to be solved by a call to a single function .", "label": "", "metadata": {}, "score": "55.88369"}
{"text": "CADCS and parallel computing / F. Dumortier , A. Van .Cauwenberghe and L. Boullart / 363 \\\\ .Eigenvalue roulette and random test matrices / A. .Edelman / 365 \\\\ .On numerical methods for unitary eigenvalue problems / .", "label": "", "metadata": {}, "score": "55.890957"}
{"text": "Low - rank perturbation ; Numerical solution ; Rayleigh . quotient iteration ; Schrodinger eigenvalue problem ; .Self - adjoint partial differential operators \" , . algebra ) ; C1140 G ( Monte Carlo methods ) ; C4140 ( Linear . algebra ) \" , .", "label": "", "metadata": {}, "score": "55.90123"}
{"text": "If the mean of the matrix rows is zero , the eigenvectors derived by the SVD are equal to the eigenvectors derived by the PCA .We are looking for a method which will give a fast approach to produce the SVD result without recomputing the eigenvectors of the whole data set , when the PCA result is given .", "label": "", "metadata": {}, "score": "55.936344"}
{"text": "View at Scopus . A. Valizadeh and M. Karimi , \" Fast subspace tracking algorithm based on the constrained projection approximation , \" Eurasip Journal on Advances in Signal Processing , vol .2009 , Article ID 576972 , 16 pages , 2009 .", "label": "", "metadata": {}, "score": "55.937004"}
{"text": "Applications of the BTSS method to the linear systems . of block two - by - two structures are discussed in detail .Numerical experiments further show the effectiveness of .our new methods . \"Eigenvalues ( numerical linear algebra ) 65F50", "label": "", "metadata": {}, "score": "55.984936"}
{"text": "Two - dimensional PCA [ 5 ] is designed for image feature extraction .In this paper , we give a state - of - the - art introduction to various neural network implementations and algorithms for PCA and its extensions .", "label": "", "metadata": {}, "score": "56.07923"}
{"text": "1208 - 1211 , 1997 .View at Google Scholar \u00b7 View at Scopus .T. M. Martinetz , S. G. Berkovich , and K. J. Schulten , \" ' Neural - gas ' network for vector quantization and its application to time - series prediction , \" IEEE Transactions on Neural Networks , vol .", "label": "", "metadata": {}, "score": "56.134438"}
{"text": "For example , the GR - SVD is a two - step method which performs Householder transformations to reduce the matrix to bidiagonal form then performs the QR iteration to obtain the singular values [ 3 , 4 ] .Since the off - diagonal regions are used to store the transform information , this approach is very efficient in saving the computational memory .", "label": "", "metadata": {}, "score": "56.152027"}
{"text": "For non - Gaussian input data , a nonlinear PCA permits the extraction of higher - order components and provides a sufficient representation .Kernel PCA [ 3 , 54 ] is a special , linear algebra - based nonlinear PCA , which introduces kernel functions into PCA .", "label": "", "metadata": {}, "score": "56.16604"}
{"text": "Block shift invariance and efficient system . identification algorithms / N. Kalouptsidis / 219 \\\\ .Computing the singular value decomposition on a . fat - tree architecture / T. J. Lee , F. T. Luk and D. L. .Boley / 231 \\\\ .", "label": "", "metadata": {}, "score": "56.184517"}
{"text": "View at Google Scholar \u00b7 View at Scopus . F. Peper and H. Noda , \" A symmetric linear neural network that learns principal components and their variances , \" IEEE Transactions on Neural Networks , vol .7 , no .", "label": "", "metadata": {}, "score": "56.216766"}
{"text": "For constant - coefficient problems , they present .analytic bounds on the spectral radii of the iteration . matrices in terms of cell Reynolds numbers that show .the methods to be rapidly - convergent .In addition , they . describe numerical experiments that supplement the . analysis and that indicate that the methods compare .", "label": "", "metadata": {}, "score": "56.23783"}
{"text": "a reliable stopping criterion . \" solution of ill - posed linear systems and linear least . squares problems .The choice of the regularization .parameter is a crucial step , and many methods have been .proposed for this purpose .", "label": "", "metadata": {}, "score": "56.25952"}
{"text": "All the algorithms of this class have the same order of convergence speed and are robust to implementation error .A rapidly convergent quasi - Newton method has been applied to extract multiple MCs in [ 88 ] .The proposed algorithm has a complexity of .", "label": "", "metadata": {}, "score": "56.30728"}
{"text": "Generally , such an operation is known as a spectral transform .The . reason one might wish to .do this is as follows .You wish to obtain enough singular value .triplets from the decomposition . of a large sparse matrix , M , to represent it to some particularly .", "label": "", "metadata": {}, "score": "56.321743"}
{"text": "The Hebbian and Oja rules are closely related to the RRLSA algorithm by suitable selection of the learning rates [ 33 ] .RRLSA [ 33 ] is also robust to the error accumulation from the previous components , which exists in the sequential PCA algorithms like Kalman - type RLS [ 31 ] and PASTd [ 30 ] .", "label": "", "metadata": {}, "score": "56.35264"}
{"text": "C. S. Cruz and J. R. Dorronsoro , \" A nonlinear discriminant algorithm for feature extraction and data classification , \" IEEE Transactions on Neural Networks , vol .9 , no .6 , pp .1370 - 1376 , 1998 .", "label": "", "metadata": {}, "score": "56.375572"}
{"text": "This is because the true computational cost of the SCSVD updating method is always more than the general updating method , even though they are of the same order .Thus , when we have to update the new data to the matrix on which the SVD will be performed , the general method is recommended .", "label": "", "metadata": {}, "score": "56.377205"}
{"text": "Equations / Paul Concus , Gene H. Golub , and Dianne P. .O'Leary / 309 \\\\ .Preconditioned Conjugate Gradient Iteration Applied . to Galerkin Methods for a Mildly - Nonlinear Dirichlet .Problem / Jim Douglas , Jr. and Todd Dupont / 333 \\\\ .", "label": "", "metadata": {}, "score": "56.41189"}
{"text": "All the methods are . based on modified moments .As applications the paper .presents Gaussian quadrature rules for integrals in . which the integrand has singularities close to the .interval of integration , and the generation of .orthogonal polynomials for the finite Hermite weight .", "label": "", "metadata": {}, "score": "56.418716"}
{"text": "( Linear algebra ) \" , .NC , USA \" , . estimator ; Adaptive Lanczos methods ; adaptive Lanczos .methods ; Covariance method ; covariance method ; least . squares approximations ; matrix algebra ; Matrix .condition number ; matrix condition number ; Matrix .", "label": "", "metadata": {}, "score": "56.439053"}
{"text": "The ranking function is .modified to take into account these converged ranks so . as to reduce the ranking function 's computation cost .The modified ranking function is then solved until a . second stability condition is satisfied .After such .", "label": "", "metadata": {}, "score": "56.46038"}
{"text": "In recent years , digital information has been proliferating and many analytic methods based on the PCA and the SVD are facing the challenge of their significant computational cost .Thus , it is crucial to develop a fast approach to compute the PCA and the SVD .", "label": "", "metadata": {}, "score": "56.47522"}
{"text": "Conclusion .We proposed the fast PCA and fast SVD methods derived from the technique of the SCMDS method .The new PCA and the SVD have the same accuracy as the traditional PCA and the SVD method when the rank of a matrix is much smaller than the matrix size .", "label": "", "metadata": {}, "score": "56.513206"}
{"text": "Taylor series , and the convergence can be accelerated .further by applying a block generalization of the . quadratically convergent Rayleigh quotient iteration .Numerical examples are presented to illustrate the . applicability of the method .( numerical linear algebra ) \" , . subspaces ; quadratic eigenvalue problems ; subspace .", "label": "", "metadata": {}, "score": "56.51513"}
{"text": "The role of coordinates in accurate computation of . discontinuous flow ( W. H. Hui)\\\\ .Regularized blind deconvolution using recursive inverse . filtering ( M. K. Ng and others)\\\\ .Solution of linear systems arising in nonlinear image . deblurring ( C. R. Vogel)\\\\ .", "label": "", "metadata": {}, "score": "56.552948"}
{"text": "The functions in this chapter ( Chapter f08 ) handle only dense , band , tridiagonal and Hessenberg matrices ( not matrices with more specialized structures , or general sparse matrices ) .The tables in Section 3 and the decision trees in Section 4 direct you to the most appropriate functions in Chapter f08 .", "label": "", "metadata": {}, "score": "56.584457"}
{"text": "ICASSP 2006 .Tensors \" , .Numerical Mathematics \" , . problems \" , . eigenvalue problems . microarray gene expression data : local least squares . imputation \" , . definite semiseparable matrices \" , . iteration methods for saddle - point problems \" , .", "label": "", "metadata": {}, "score": "56.584946"}
{"text": "Dalkowski \" , .Series on Mathematics ] \" , .eigenvalue problem of self - adjoint partial differential .operators \" , . self - adjoint partial differential operator , the . smallest few eigenvalues and eigenvectors of A / sub 1/ . are computed by the homotopy ( continuation ) method .", "label": "", "metadata": {}, "score": "56.604424"}
{"text": "By combining Oja 's rule and the GSO procedure , Sanger proposed GHA for extracting the first . for .GHA becomes a local algorithm by rewriting the summation term in ( 15 ) in a recursive form .th neuron converges to the .", "label": "", "metadata": {}, "score": "56.64547"}
{"text": "( f08flc ) is provided to compute the reciprocal condition numbers for the eigenvectors of a real symmetric or complex Hermitian matrix .A variety of functions are provided to compute eigenvalues and eigenvectors of the real symmetric tridiagonal matrix .T . , some computing all eigenvalues and eigenvectors , some computing selected eigenvalues and eigenvectors .", "label": "", "metadata": {}, "score": "56.702667"}
{"text": "Adaptive Chebyshev iteration based on modified moments ./ D. Calvetti , G. H. Golub and L. Reichel / 357 \\\\ .Continuous realization methods and their applications / .M. T. Chu / 359 \\\\ .Asymptotic behavior of orthogonal polynomials / T. Dehn .", "label": "", "metadata": {}, "score": "56.784603"}
{"text": ".. ing parallel computers .All the above contributions are aimed at general , nonsymmetric matrices .Very recently , Demmel , Dumitriu , and Holtz [ 18 ] have shown how to exploit randomization to allow standard QR factorization to be used throughout the alg ... .", "label": "", "metadata": {}, "score": "56.810234"}
{"text": "MN , USA \" , . dynamical system ; Matrix eigenvalue computation ; .Modified nonsymmetric Lanczos algorithm ; Observable . space \" , . controllability \" , . algorithm that does not require strict bi - orthogonality . among the generated vectors .", "label": "", "metadata": {}, "score": "56.84255"}
{"text": "( Time - varying systems ) ; C4140 ( Linear algebra ) ; C4170 .( Differential equations ) \" , .for Optical Engineering \" , .Periodic Schur decomposition ; QR algorithm ; Stable .algorithm ; Time - varying control systems ; Two - point .", "label": "", "metadata": {}, "score": "56.8767"}
{"text": "Then we . propose several numerical algorithms for computing the . canonical correlations of general matrix pairs ; . emphasis is placed on the case of large sparse or .structured matrices . \"Dooren \" , .Single - Output Systems \" , . single - input single - output system triple ( A , b , c ) .", "label": "", "metadata": {}, "score": "56.88824"}
{"text": "III .Linear algebra . \" problems \" , .with inner - outer iterations \" , . splittings \" , .MathSciNet database \" , . and scientific computation ( Kent , OH , 1999 ) . \" O'Leary \" , .", "label": "", "metadata": {}, "score": "56.910873"}
{"text": "617 - 624 , MIT Press , Cambridge , Mass , USA , 2002 .View at Google Scholar .B. Sch\u00f6lkopf , A. Smola , and K. R. M\u00fcller , \" Nonlinear component analysis as a kernel eigenvalue problem , \" Neural Computation , vol .", "label": "", "metadata": {}, "score": "56.936295"}
{"text": "It is much more complicated and may sometimes be caught more easily in local minima .PCA needs to deal with an eigenvalue problem of a . matrix , while kernel PCA needs to solve an eigenvalue problem of an . matrix .", "label": "", "metadata": {}, "score": "56.985916"}
{"text": "This paper shows how Singular Value Decomposition can be used to reduce the dimensionality of the data and how a standard neural network software package can implement a IR System . ... has proved to be an efficient method to reduce the dimensions of IR data ( cf .", "label": "", "metadata": {}, "score": "56.993347"}
{"text": "The ability to detect differences is a powerful tool for application specific network management , allowing for : optimal placement of routers , design of specialized protocols for vari - ous user populations and lending insight to gauging the energy / bandwidth needs of mobile devices . .", "label": "", "metadata": {}, "score": "56.998"}
{"text": "View at Google Scholar . E. Oja , H. Ogawa , and J. Wangviwattana , \" Principal component analysis by homogeneous neural networks , \" IEICE Transactions on Information and Systems , vol .E75-D , no . 3 , pp .", "label": "", "metadata": {}, "score": "57.03358"}
{"text": "Serra - Capizzano \" , . for the convection - diffusion equation \" , .Greif \" , . eigenstructure , sensitivity , and the derivative \" , . conjugate gradient for solving linear systems with .multiple right - hand sides \" , .", "label": "", "metadata": {}, "score": "57.057102"}
{"text": "4 , pp .863 - 872 , 2006 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . H. Lu , K. N. Plataniotis , and A. N. Venetsanopoulos , \" Uncorrelated multilinear principal component analysis for unsupervised multilinear subspace learning , \" IEEE Transactions on Neural Networks , vol .", "label": "", "metadata": {}, "score": "57.058"}
{"text": "Text Summarization is very effective in relevant assessment tasks .The Multiple Document Summarizer presents a novel approach to select sentences from documents according to several heuristic features .Summaries are generated modeling the set of documents as Semantic Vector Space Model ( SVSM ) and ap ... \" .", "label": "", "metadata": {}, "score": "57.071716"}
{"text": "The convergence speed of a system depends on the eigenvalues of its Jacobian .In PCA algorithms , the eigenmotion depends on the principal eigenvalue of the covariance matrix , while in MCA algorithms on all the eigenvalues [ 46 ] .", "label": "", "metadata": {}, "score": "57.090923"}
{"text": "Kramer 's nonlinear PCA fits a lower - dimensional surface through the training data .Usually , the data compression achieved in the bottleneck layer in such networks is somewhat better than that provided by the PCA solution [ 65 ] .", "label": "", "metadata": {}, "score": "57.1039"}
{"text": "SVDPACK / Michael W. Berry / 13 \\\\ .Gaussian quadrature applied to adaptive Chebyshev . iteration / D. Calvetti , G. H. Golub , and L. Reichel / .31 \\\\ .Ordering effects on relaxation methods applied to the .", "label": "", "metadata": {}, "score": "57.187702"}
{"text": "By introducing kernel into . , nonlinear discriminant analysis is obtained [ 3 , 154 ] .A multiple of the identity or the kernel matrix can be added to .or its reformulated matrix .after introducing the kernels to penalize . or . , respectively , [ 154 ] .", "label": "", "metadata": {}, "score": "57.225716"}
{"text": "It uses the architecture of Kramer 's nonlinear PCA network [ 62 ] , but with complex weights and biases .For a similar number of model parameters , the nonlinear complex PCA model captures more variance of a data set than the alternative real approach , where each complex variable is replaced by two real variables and is applied to Kramer 's nonlinear PCA .", "label": "", "metadata": {}, "score": "57.227135"}
{"text": "We have also illustrated weighted SLA , GHA , and APEX in [ 10 ] .We now provide an example to illustrate the application of PCA .Image compression is usually implemented by partitioning an image into many nonoverlapping .pixel blocks and then compressing them one by one .", "label": "", "metadata": {}, "score": "57.236298"}
{"text": "attractive feature for parallel computers .Comparisons . in efficiency and accuracy with an appropriate Lanczos . algorithm ( with selective re - orthogonalization ) are . presented on large sparse ( rectangular ) matrices .arising from applications such as information retrieval .", "label": "", "metadata": {}, "score": "57.266487"}
{"text": "96 - 104 , 1960 .Q. Zhang and Y. W. Leung , \" A class of learning algorithms for principal component analysis and minor component analysis , \" IEEE Transactions on Neural Networks , vol .11 , no . 1 , pp .", "label": "", "metadata": {}, "score": "57.27339"}
{"text": "The self - organizing map ( SOM ) [ 68 ] is a competitive learning - based neural network .It is capable of performing dimensionality reduction on the input .The SOM is inherently nonlinear and is viewed as a nonlinear PCA [ 69 ] .", "label": "", "metadata": {}, "score": "57.307327"}
{"text": "Section 11 extends all the methods to the complex - valued domain .Some other generalizations of PCA such as constrained PCA , generalized EVD , and two - dimensional PCA are described in Section 12 .In Section 13 , the cross - correlational PCA asymmetric network and SVD are described .", "label": "", "metadata": {}, "score": "57.32949"}
{"text": "Sparse Matrix Problems in a Finite Element Open .Ocean Model / Joel E. Hirsh and William L. Briggs / 391 .\\\\ .Calculation of Normal Modes of Oceans Using a .Lanczos Method / Alan K. Cline , Gene H. Golub , and .", "label": "", "metadata": {}, "score": "57.34768"}
{"text": "1155 - 1164 , 2004 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . Y. Liu , Z. You , and L. Cao , \" A simple functional neural network for computing the largest and smallest eigenvalues and corresponding eigenvectors of a real symmetric matrix , \" Neurocomputing , vol .", "label": "", "metadata": {}, "score": "57.386932"}
{"text": "Finite Element Approximations to the One- Dimensional .Stefan Problem / J. A. Nitsche / 119 \\\\ .The Hodie Method and Its Performance for Solving .Elliptic Partial Differential Equations / Robert E. .Lynch and John R. Rice / 143 \\\\ .", "label": "", "metadata": {}, "score": "57.400993"}
{"text": "The only possible bottleneck might be the residual coding , but the recent development in floating point coding has seen throughputs reaching as much as .Numerical Experiments .Accuracy of the rSVD Estimates . singular values and the associated singular vectors of a large matrix .", "label": "", "metadata": {}, "score": "57.403084"}
{"text": "21 , pp .8390 - 8394 , 1986 .View at Google Scholar \u00b7 View at Scopus .Z. Yi , M. Ye , J. C. Lv , and K. K. Tan , \" Convergence analysis of a deterministic discrete time system of Oja 's PCA learning algorithm , \" IEEE Transactions on Neural Networks , vol .", "label": "", "metadata": {}, "score": "57.413815"}
{"text": "However , the previously mentioned methods all require matrix multiplications for the SVD .One interesting problem is how do we compute the SVD for a matrix when the matrix size is huge and loading the whole matrix into the memory is not possible ?", "label": "", "metadata": {}, "score": "57.46682"}
{"text": "Given that PCA is an extremely useful tool in HSI data analysis , for example , for classification and target detection , it is essential to obtain a quality reconstruction of the eigenvectors .times , and , within each time , we compute the angles between eigenvectors by CPPCA and the true ones , and between eigenvectors by rSVD and the true ones .", "label": "", "metadata": {}, "score": "57.49267"}
{"text": "the associated least squares problem .Furthermore , it . is possible to decompose the problem in such a way that .the computation can be performed in parallel .The .author discusses this computation and points out its . connection to other large scale least squares .", "label": "", "metadata": {}, "score": "57.5069"}
{"text": "For example , in [ 11 ] Fowler proposed an approach that exploits the use of compressive projections in sensors that integrate dimensionality reduction and signal acquisition to effectively shift the computational burden of PCA from the encoder platform to the decoder site .", "label": "", "metadata": {}, "score": "57.52918"}
{"text": "]QR decomposition can be performed by using the Householder transform or the Givens rotation [ 1 ] , which is suitable for hardware implementation .The GSO transform can be used for feature subset selection ; it inherits the compactness of the orthogonal representation and at the same time provides features that retain their original meaning .", "label": "", "metadata": {}, "score": "57.529816"}
{"text": "a Large - scale nonlinear constrained optimization / A. .R. Conn , N. Gould and Ph .L. Toint / 21 \\\\ .Trading off parallelism and numerical stability / J. W. .Demmel / 49 \\\\ .Subband filtering : CORDIC modulation and systolic .", "label": "", "metadata": {}, "score": "57.53013"}
{"text": "In this paper , we present a randomized singular value decomposition ( rSVD ) method for the purposes of lossless compression , reconstruction , classification , and target detection .On a large HSI dataset we apply the rSVD method to demonstrate its efficiency and effectiveness of the proposed method .", "label": "", "metadata": {}, "score": "57.540787"}
{"text": "[ 13 ] , in which we have proved that when the data dimension is significantly smaller than the number of data entries , there is a fast linear approach for the classical MDS .The main idea of fast MDS is using statistical resampling to split data into overlapping subsets .", "label": "", "metadata": {}, "score": "57.553093"}
{"text": "The computational complexity of the new .methods is O(M / sup 2/d ) per update , where M is the size . of the data vectors and d is the dimension of the . signal subspace .Unlike most tracking methods that . assume d is fixed and/or known a priori , the new .", "label": "", "metadata": {}, "score": "57.55632"}
{"text": "Matrices via Modified Moments \" , . of the largest singular values of a large sparse . matrix .The method by Golub and Kent ( 1989 ) which uses .the method of modified moments for estimating the . eigenvalues of operators used in iterative methods for .", "label": "", "metadata": {}, "score": "57.62598"}
{"text": "The distribution is collectively modeled by a collection or a mixture of linear PCA models , each characterizing a partition .Most natural data sets have large eigenvalues in only a few eigendirections , while the variances in other eigendirections are so small as to be considered as noise .", "label": "", "metadata": {}, "score": "57.7319"}
{"text": "To illustrate functionality of this function we define a matrix A.1.3 4 9 .LineWidth ' .[ v.y ) . 'or ' ) .v1 . set(h1_line . length(t ) ) .j ) .It is well known that the eigenvalues are sensitive to small changes in the entries of the matrix ( see.g . v2 ( : .", "label": "", "metadata": {}, "score": "57.736862"}
{"text": "Generalized CCA consists of a generalization of CCA to more than two sets of variables [ 135 ] .onto that direction , .If we do the same for . by choosing direction ., we obtain a sample of the new mapping for .", "label": "", "metadata": {}, "score": "57.840904"}
{"text": "authors perform an analytic and experimental study of .line iterative methods for solving linear systems .arising from finite difference discretizations of .nonself - adjoint elliptic partial differential equations .on two - dimensional domains .The methods consist of .", "label": "", "metadata": {}, "score": "57.846977"}
{"text": "Elman and Michael P. Chernesky / 45 \\\\ .On the error computation for polynomial based iteration .methods / Bernd Fischer and Gene H. Golub / 59 \\\\ .Transpose - free quasi - minimal residual methods for .non - Hermitian linear systems / Roland W. Freund / 69 .", "label": "", "metadata": {}, "score": "57.855698"}
{"text": "We expand this method to handle an additional dimension of information for a defined neighborhood ancestry of node degree , exposing patterns when they exist .We test our methodology on node degree data from a simulated university cam - pus model ( Pedsims ) and real campus data .", "label": "", "metadata": {}, "score": "57.910507"}
{"text": "( paperback ) \" , .G65 1989 \" , . informatique ; na ; nla \" , .Vol .56 , No . 193 ( Jan. , 1991 ) , pp .380 - 381 . \"Algorithms : 1948 - -1976 \" , . solving linear systems of equations and eigenproblems .", "label": "", "metadata": {}, "score": "57.949844"}
{"text": "A PCA algorithm is obtained by adding a term to SLA [ 7 ] so as to rotate the basis vectors in the principal subspace toward the principal eigenvectors [ 24 ] .The adaptive learning algorithm ( ALA ) [ 20 ] is also a PCA algorithm that is based on SLA .", "label": "", "metadata": {}, "score": "58.000504"}
{"text": "The Multiple Document Summarizer presents a novel approach to select sentences from documents according to several heuristic features .Summaries are generated modeling the set of documents as Semantic Vector Space Model ( SVSM ) and applying Principal Component Analysis ( PCA ) to extract topic features .", "label": "", "metadata": {}, "score": "58.043728"}
{"text": "Vision Soc . ; British Machine Vision Assoc \" , .Golub \" , . sparse matrices via modified moments \" , . C4140 ( Linear algebra ) ; C4240P ( Parallel programming .and algorithm theory ) \" , .Sci . , Tennessee Univ . , Knoxville , TN , .", "label": "", "metadata": {}, "score": "58.047832"}
{"text": "Many information processing problems can be transformed into some form of eigenvalue or singular value problems .Eigenvalue decomposition ( EVD ) and singular value decomposition ( SVD ) are usually used for solving these problems .In this paper , we give an introduction to various neural network implementations and algorithms for principal component analysis ( PCA ) and its various extensions .", "label": "", "metadata": {}, "score": "58.096207"}
{"text": "In this paper , we give an overview of the Invariant Subspace Decomposition Algorithm for banded symmetric matrices and describe a sequential implementation of this algorithm .Our implementation uses a specialized routine for performing banded matrix multiplication together with successive band reduction , yielding a sequential algorithm that is competitive for large problems with the LAPACK QR code in computing all of the eigenvalues and eigenvectors of a dense symmetric matrix .", "label": "", "metadata": {}, "score": "58.1407"}
{"text": "Functions included here deal with this transformation .Entries c and s % are computed using numbers x1 and x2 .j ) % % % % Premultiplication of A by the Givens rotation which is represented by the 2-by-2 planar rotation J. Integers i and j describe position of the Givens parameters .", "label": "", "metadata": {}, "score": "58.145905"}
{"text": "becomes impossible , it is better to recompute the SVD result .In this case and with the condition that the true rank is much smaller than the matrix size , the SCSVD approach is recommended .Experimental Result .In this section , we show that our fast PCA and fast SVD methods work well for big - sized matrices with small ranks .", "label": "", "metadata": {}, "score": "58.163082"}
{"text": "This achieves a compression ratio of 1 : 16 .The Lena picture is thus transformed into a training set of 14400 vectors , and the kid picture is transformed into a validation set of 19200 vectors .The training set is so large that the APEX algorithm converges after only two epochs .", "label": "", "metadata": {}, "score": "58.235504"}
{"text": "Golub , R. Underwood \\\\ .The numerically stable reconstruction of a Jacobi .Michael Overton and Haesun Park and Michael Saunders . and James Varah \" , .Computational Mathematics \\ & Numerical Computing :A . conference celebrating the 50th anniversary George .", "label": "", "metadata": {}, "score": "58.24093"}
{"text": "PCA is often derived by optimizing some information criterion , such as the maximization of the variance of the projected data or the minimization of the reconstruction error .The objective of PCA is to extract . orthonormal directions . , in the input space that account for as much of the data 's variance as possible .", "label": "", "metadata": {}, "score": "58.279373"}
{"text": "441 - 457 , 1992 .View at Google Scholar \u00b7 View at Scopus . A. Taleb and G. Cirrincione , \" Against the convergence of the minor component analysis neurons , \" IEEE Transactions on Neural Networks , vol .10 , no . 1 , pp .", "label": "", "metadata": {}, "score": "58.310837"}
{"text": "10 , pp .2413 - 2422 , 1996 .View at Google Scholar \u00b7 View at Scopus . Y. N. Rao , J. C. Principe , and T. F. Wong , \" Fast RLS - like algorithm for generalized eigendecomposition and its applications , \" Journal of VLSI Signal Processing Systems for Signal , Image , and Video Technology , vol .", "label": "", "metadata": {}, "score": "58.321003"}
{"text": "Boolean functions of the discretized . subnetworks and couplings highlight differential , i.e. , . pathway - dependent , relations among genes .We illustrate .the EVD , pseudoinverse projection , and HOEVD of .genome - scale networks with analyses of yeast DNA . microarray data . \"", "label": "", "metadata": {}, "score": "58.329277"}
{"text": "from different studies , to a ' ' core tensor ' ' of . '' eigenarrays ' ' $ \\times $ ' ' x - eigengenes ' ' $ \\times $ . ''y - eigengenes . ' 'Reformulating this multilinear HOSVD .", "label": "", "metadata": {}, "score": "58.33998"}
{"text": "The objective is to compute a reliable . estimate of the array response vector given finite .sample estimates of the signal plus noise and noise .only array covariances .These covariances are computed .from data collected at the sensor array with a . calibration signal turned on and off .", "label": "", "metadata": {}, "score": "58.343502"}
{"text": "I. S. Duff : The Solution of Augmented Systems / 10 \\\\ .C. M. Elliott \\ & A. R. Gardiner : One Dimensional Phase .Field Computations / 56 \\\\ .J. de Frutos \\ & J. M. Sanz - Serna : Erring and being .", "label": "", "metadata": {}, "score": "58.38015"}
{"text": "n . and .rank .A .n . , so that .A . has full rank and in this case the solution to problem ( 1 ) is unique ; the problem is also referred to as finding a least squares solution to an overdetermined system of linear equations . and .", "label": "", "metadata": {}, "score": "58.418217"}
{"text": "Trefethen and D. 1989 .Philadelphia .Matrix Computations .MA.N. [ 2 ] J. Accuracy and Stability of Numerical Algorithms .Philadelphia . H. Second edition .[5 ] N. Johns Hopkins University Press .McGraw - Hill .B. Demmel .", "label": "", "metadata": {}, "score": "58.45017"}
{"text": "the SVD routine initially and requested 100 triplets .You can iterate .this procedure , requesting a small number of triplets with a shifted .spectrum each time until you achieve your desired precision .In addition to not having to compute singular triplets you do n't need , . this method usually has another benefit .", "label": "", "metadata": {}, "score": "58.454163"}
{"text": "View at Scopus .B. A. Pearlmutter , G. E. Hinton , and G. -maximization : , \" An unsupervised learning procedure for discovering regularities , \" in Proceedings of the Neural Networks for Computing , J. S. Denker , Ed . , vol .", "label": "", "metadata": {}, "score": "58.46634"}
{"text": "Spectral information is important in many fields such as environmental remote sensing , monitoring chemical / oil spills , and military target discrimination .For comprehensive discussions , please see , for example , [ 1 - 3 ] .Hyperspectral image data is often represented as a matrix .", "label": "", "metadata": {}, "score": "58.50081"}
{"text": "-dimensional space without losing essential intrinsic information .The vector . can be represented by being projected onto the .-dimensional subspace spanned by .using the inner products ., hence achieving dimensionality reduction .PCA finds those unit directions . , along which the projections of the input vectors , known as the principal components ( PCs ) , . , have the largest variance .", "label": "", "metadata": {}, "score": "58.51629"}
{"text": "4263 - 4281 , 2011 .View at Google Scholar .X. Tang and W. Pearlman , \" Three - dimensional wavelet - based compression of hyperspectral images , \" Hyperspectral Data Compression , pp .273 - 308 , 2006 .", "label": "", "metadata": {}, "score": "58.531105"}
{"text": "In this . paper , we present several algorithms which compute the . ellipse for which the sum of the squares of the . distances to the given points is minimal .These . algorithms are compared with classical simple and .", "label": "", "metadata": {}, "score": "58.545097"}
{"text": "PCA is based on the Gaussian assumption for data distribution , and the optimality of PCA results from taking into account only the second - order statistics .For non - Gaussian data distributions , PCA is not able to capture complex nonlinear correlations , and nonlinear processing of the data is usually more efficient .", "label": "", "metadata": {}, "score": "58.592102"}
{"text": "recursive least squares problems .Numerical experiments . are reported indicating that ale yields a very accurate .recursive condition estimator . \"NC , USA \" , . B0290H( Linear algebra ) ; B6140 ( Signal processing and . detection ) ; C1260 ( Information theory ) ; C4130 .", "label": "", "metadata": {}, "score": "58.613354"}
{"text": "The matrix eigenvalue problem .we will present MATLAB 's code for computing the dominant eigenvalue and the associated eigenvector of a matrix . 1 i. [ 1].The QR iteration for computing all eigenvalues of the symmetric matrices is also discussed . briefly discussed in Tutorial 3 .", "label": "", "metadata": {}, "score": "58.648556"}
{"text": "PageRank vector where the algorithm exploits the .lumpability of the underlying Markov chain .We make .three contributions .First , the algorithm speeds up the .PageRank calculation significantly .With web graphs .having millions of webpages , the speed - up is typically . in the two- to three - fold range .", "label": "", "metadata": {}, "score": "58.755653"}
{"text": "View at Scopus .T. Kohonen , Self - Organizing Maps , Springer , Berlin , Germany , 1997 . E. Oja and K. Valkealahti , \" Local independent component analysis by the self - organizing map , \" in Proceedings of the International Conference on Artificial Neural Networks , pp .", "label": "", "metadata": {}, "score": "58.756577"}
{"text": "We have also demonstrated the fast computation in compression and reconstruction of the proposed algorithms on a large HSI dataset in an urban setting .Overall , the rSVD provides a lower approximation error than some other recent methods and is particularly well suited for compression , reconstruction , classification , and target detection .", "label": "", "metadata": {}, "score": "58.768547"}
{"text": "algorithms have been further refined and have become a . basic tool for solving a wide variety of problems on a .wide variety of computer architectures .The conjugate . gradient algorithm has also been extended to solve .nonlinear systems of equations and optimization .", "label": "", "metadata": {}, "score": "58.776375"}
{"text": "Saunders : Solving Reduced KKT Systems in Barrier .Methods for Linear Programming / 89 \\\\ .G. H. Golub \\ & Gerard Meurant : Matrices , Moments , and . and Quadrature / 105 \\\\ .J. Groeneweg \\ & M. N. Spijker : On the Error due to the .", "label": "", "metadata": {}, "score": "58.780468"}
{"text": "( e - book ) \" , .Golub \" , . using the theory of moments \" , .Subspaces \" , .JSTOR database ; Parallel / par .lin.alg.bib ; .Theory / Matrix . bib \" , . algebra ) ; C1140 ( Probability and statistics ) ; C4140 .", "label": "", "metadata": {}, "score": "58.808754"}
{"text": "C .y .y . , this may then be solved using the functions described in the previous section .No special functions are needed to recover the eigenvectors .z . of the standard problem , because these computations are simple applications of Level 2 or Level 3 BLAS ( see Chapter f16 ) . 2.9 Packed Storage for Symmetric Matrices .", "label": "", "metadata": {}, "score": "58.85859"}
{"text": "Sci . , Stanford Univ . , CA , USA \" , . eigenvalues ; eigenvalues and eigenfunctions ; error ; . estimates ; Gauss quadrature ; integration ; iterative .methods ; linear equations ; linear systems ; lower . bounds ; theory of moments ; upper bounds \" , .", "label": "", "metadata": {}, "score": "58.944267"}
{"text": "All these algorithms first extract the principal generalized eigenvector and then estimate the minor generalized eigenvectors using a deflation procedure .A recurrent network with invariant .-norm [ 117 ] computes the largest or smallest generalized eigenvalue and the corresponding eigenvector of any symmetric positive pair , which can be simply extended to compute the second largest or smallest generalized eigenvalue and the corresponding eigenvector .", "label": "", "metadata": {}, "score": "58.991184"}
{"text": "A prototype is .the convection - diffusion equation .The methods consist . of applying one step of cyclic reduction , resulting in .a ' reduced system ' of half the order of the original .discrete problem , combined with a reordering and a .", "label": "", "metadata": {}, "score": "59.004368"}
{"text": "The second is scaling by a diagonal matrix .D .Scaling can make the matrix norm smaller with respect to the eigenvalues , and so possibly reduce the inaccuracy contributed by roundoff ( see Chapter 11 of Wilkinson and Reinsch ( 1971 ) ) .", "label": "", "metadata": {}, "score": "59.039352"}
{"text": "A hierarchical nonlinear PCA network composed of a number of independent subnetworks can extract ordered nonlinear PCs [66 ] .Each subnetwork extracts one PC and has at least five layers .The subnetworks can be selected as Kramer 's nonlinear PCA network and are hierarchically arranged and trained .", "label": "", "metadata": {}, "score": "59.064224"}
{"text": "the Markov matrix representing the Web link graph .The . algorithm presented here , called Quadratic .Extrapolation , accelerates the convergence of the Power .Method by periodically subtracting off estimates of the .nonprincipal eigenvectors from the current iterate of .", "label": "", "metadata": {}, "score": "59.08036"}
{"text": "Function wsft is used in the body of the function qrsft .[Q.32486912943335 Function eigv computes both the eigenvalues and the eigenvectors of a symmetric matrix provided the eigenvalues are distinct .pp .n-1 ) .Algorithm 8 .[ n.n-1)-mu)/2.1:j-1 ) .", "label": "", "metadata": {}, "score": "59.111893"}
{"text": "units , and one of its hidden units , known as the bottleneck or representation layer , have .units , .The network is trained to reproduce its input vectors themselves .This kind of networks is called the autoassociative network .", "label": "", "metadata": {}, "score": "59.138218"}
{"text": "Gradient Method \" , .Sacher \" , . complementarity problems : the block partitioned case \" , .Nash \" , . matrix from spectral data \" , .K. Samelson \" , .Nash \" , . stream function \" , .", "label": "", "metadata": {}, "score": "59.146152"}
{"text": "Eigenvalue Problems ./ / 111 \\\\ .A Bibliographical Tour of the Large , Sparse .Generalized Eigenvalue Problem / G. W. Stewart / 113 .\\\\ .How Far Should You Go With the Lanczos Process ? / W. .", "label": "", "metadata": {}, "score": "59.17614"}
{"text": "Color ' . 'Color ' .LineWidth'.25 .z(1 .y.1 ) 0 V(2 . \" Many properties of a matrix can be derived from its singular value decomposition ( SVD ) .[ 0 0 0 ] ) grid .", "label": "", "metadata": {}, "score": "59.20524"}
{"text": "We present a randomized singular value decomposition ( rSVD ) method for the purposes of lossless compression , reconstruction , classification , and target detection with hyperspectral ( HSI ) data .Recent work in low - rank matrix approximations obtained from random projections suggests that these approximations are well suited for randomized dimensionality reduction .", "label": "", "metadata": {}, "score": "59.210667"}
{"text": "Capizzano and Cristina Tablino Possio \" , . applications to the discrete convection - diffusion . equation \" , . preconditioning step to the Hermitian skew - Hermitian . splitting ( HSS ) method .The result suggests that the .technique is effective for handling non Hermitian , . positive definite and ill conditioned problems with an . optimal convergence rate .", "label": "", "metadata": {}, "score": "59.224106"}
{"text": "Sci . , Minnesota Univ . , Minneapolis , .MN , USA \" , . complexity ) ; C5230 ( Digital arithmetic methods ) ; C5470 .( Performance evaluation and testing ) \" , . error assertions ; Complexity ; Dense systems ; Error .", "label": "", "metadata": {}, "score": "59.229473"}
{"text": "The bigradient PSA algorithm [ 82 ] is a modification to SLA [ 7 ] and is obtained by introducing an additional bigradient term embodying the orthonormal constraints of the weights , and it can be used for MSA by reversing the sign of . . . .", "label": "", "metadata": {}, "score": "59.248978"}
{"text": "Wavelets , signal processing and textile surfaces ( H. .Kraus)\\\\ .Efficient image compression scheme using tree structure .wavelet transforms ( J. M. Li and J. S. Jin)\\\\ .A note on adaptive restarting procedure for pseudo . residual algorithms ( T. Nodera and T. Inadu)\\\\ .", "label": "", "metadata": {}, "score": "59.286995"}
{"text": "lower bounds for the error in the approximate solution . of linear equations , using essentially the same . information as that needed for the eigenvalue . calculations .The methods described depend strongly .upon the theory of moments and Gauss quadrature . \"", "label": "", "metadata": {}, "score": "59.341995"}
{"text": "In addition , several CCA extensions , including the sparse CCA formulation based on .-norm regularization , are proposed [ 136 ] .The LS formulation of CCA and its extensions can be solved efficiently .The LS formulation is extended to orthonormalized partial least squares by establishing the equivalence relationship between CCA and orthonormalized partial least squares [ 136 ] .", "label": "", "metadata": {}, "score": "59.394684"}
{"text": "uncertainties ; Response vector ; Robust numerical .approach ; Sensor array calibration \" , .Nonsymmetric Linear Systems Based on Modified Moments \" , .with a matrix whose eigenvalues lie in the right half .plane may be solved by an iterative method based on .", "label": "", "metadata": {}, "score": "59.432335"}
{"text": "Spectral divide and conquer algorithms solve the eigenvalue problem for all the eigenvalues and eigenvectors by recursively computing an invariant subspace for a subset of the spectrum and using it to decouple the problem into two smaller subproblems .A number of such algorithms have been developed over the last forty years , often motivated by parallel computing and , most recently , with the aim of achieving minimal communication costs .", "label": "", "metadata": {}, "score": "59.46429"}
{"text": "These are then used by Algorithm 4 to reconstruct the original data losslessly , and we can see it only involves a one - pass matrix - matrix multiplication and is without iterative algorithms .Compared to CPPCA , the number of bytes used for storing the .", "label": "", "metadata": {}, "score": "59.471886"}
{"text": "function of the problem size , but there exist faster .algorithms with linear complexity .Finally , in order to . make a choice among the large and fuzzy set of .available techniques , comparisons based on computer . simulations in a relevant signal processing context are . made . \" and detection ) ; C1260 ( Information theory ) ; C4100 .", "label": "", "metadata": {}, "score": "59.500664"}
{"text": "Although the matrix size remains unchanged in the Matthew 's method , we can still adopt the analysis of this paper for updating the SVD when the matrix size is changed .Multidimensional scaling ( MDS ) is a method of representing the high - dimensional data into the low - dimensional configuration [ 10 - 12 ] .", "label": "", "metadata": {}, "score": "59.510117"}
{"text": "Additional evidence of the significance of the SVD is its central role in a number of papers in recent years in Mathematics Magazine and The American Mathematical Monthly ( for example [ 2 , 3 , 17 , 23 ] ) .", "label": "", "metadata": {}, "score": "59.555817"}
{"text": "196 - 203 , 1999 .View at Google Scholar \u00b7 View at Scopus . H. Ritter , \" Self - organizing feature maps : kohonen maps , \" in The Handbook of Brain Theory and Neural Networks , M. A. Arbib , Ed . , pp .", "label": "", "metadata": {}, "score": "59.62118"}
{"text": "354 - 379 , 2012 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .J. C. Harsanyi and C. I. Chang , \" Hyperspectral image classification and dimensionality reduction : an orthogonal subspace projection approach , \" IEEE Transactions on Geoscience and Remote Sensing , vol .", "label": "", "metadata": {}, "score": "59.636894"}
{"text": "r . measures the closeness of the samples within the clusters , and .t .r . measures the separation between the clusters , where .t .r . denotes the trace operator .An optimal . should preserve the given cluster structure , and simultaneously maximize .", "label": "", "metadata": {}, "score": "59.646088"}
{"text": "Shampine / 177 \\\\ .Perturbation Theory for the Generalized Eigenvalue .Problem / G. W. Stewart / 193 \\\\ .Some Remarks on Good , Simple , and Optimal Quadrature .Formulas / H. F. Weinberger / 207 \\\\ .Linear Differential Equations and Kronecker 's Canonical .", "label": "", "metadata": {}, "score": "59.648212"}
{"text": "A simulation example of PCA is given in Section 15 .A brief summary is given in Section 16 , and independent component analysis ( ICA ) and linear discriminant analysis ( LDA ) are also mentioned in passing in this section .", "label": "", "metadata": {}, "score": "59.675617"}
{"text": "variants of these methods , and show that the methods . can be implemented efficiently on parallel . architectures . \"Sci . , Maryland Univ . , College Park , .MD , USA \" , . equations ) \" , . analysis ; Finite difference discretizations ; Line .", "label": "", "metadata": {}, "score": "59.678856"}
{"text": "M. Loeve , Probability Theory , Van Nostrand , New York , NY , USA , 3rd edition , 1963 .J. Yang , D. Zhang , A. F. Frangi , and J. Y. Yang , \" Two - dimensional PCA : a new approach to appearance - based face representation and recognition , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "59.680428"}
{"text": "\\\\ .9.4 Arnoldi and Unsymmetric Lanczos / 499 \\\\ .10 : Iterative Methods for Linear Systems / 508 \\\\ .10.1 The Standard Iterations / 509 \\\\ .10.2 The Conjugate Gradient Method / 520 \\\\ .10.3 Preconditioned Conjugate Gradients / 532 \\\\ .", "label": "", "metadata": {}, "score": "59.680733"}
{"text": "A(l.k+1:n ) ' .q ) .9 . k).0000 which is the same as the bidiagonal form obtained earlier.v].3 4 5 .V. A(p.0000 -0 . end end Let ( see [ 1].1 ) .l ) .Premultiplication and postmultiplication by a Givens matrix can be performed without computing a Givens matrix explicitly.2 5 6].3 Givens transformation Givens transformation ( synonym : Givens rotation ) is an orthogonal matrix used for zeroing a selected entry of the matrix .", "label": "", "metadata": {}, "score": "59.74131"}
{"text": ", one can initialize the algorithm with weights and biases of small magnitudes and use a weight penalty in the objective function .A complex - valued BP or quasi - Newton algorithm can be used for training .There are many other complex PCA algorithms .", "label": "", "metadata": {}, "score": "59.78105"}
{"text": "Before introducing our updating method , we review the general updating methods first . to the largest group of the subgroups .Because the dimension of this updated group will increase , we have to make some modifications in the combined approach .", "label": "", "metadata": {}, "score": "59.782173"}
{"text": "Matrices that generate the same Krylov residual spaces ./ Anne Greenbaum and Zdenek Strakos / 95 \\\\ .Incomplete block factorizations as preconditioners for . sparse SPD matrices / L. Yu .Kolotilina and A. Yu .Yeremin / 119 \\\\ .", "label": "", "metadata": {}, "score": "59.87224"}
{"text": "Size of covariance matrices in CCArc is equal to .m . a .x .Small - sample - size problem in CCArc does not occur , because we actually use . images of size . and . images of size .", "label": "", "metadata": {}, "score": "59.897038"}
{"text": "A more thorough filrough process can be performed on this information to sort out therelL ant documents .A new methodcal)x Frequency Domain Scoring ( FDS ) , which is based on the Fourier Transform is proposed .Most search engines return alW of unwanted information .", "label": "", "metadata": {}, "score": "59.90419"}
{"text": "We describe a divide - and - conquer tridiagonalization approach for matrices with repeated eigenvalues .Our algorithm hinges on the fact that , under easily constructively verifiable conditions , a symmetric matrix with bandwidth b and k distinct eigenvalues must be block diagonal with diagonal blocks ... \" .", "label": "", "metadata": {}, "score": "59.91944"}
{"text": "This algorithm only uses easily parallelizable linear algebra building blocks : matrix multiplication and QR decomposition , but not matrix inversion .Similar parallel algorithms for the nonsymmetric eigenproblem use the matrix sign function , which requires matrix inversion and is faster but can be less stable than the new algorithm .", "label": "", "metadata": {}, "score": "59.92463"}
{"text": "PCA usually obtains the best fixed - rank approximation to the data in the LS sense .On the other hand , constrained PCA allows specifying metric matrices that modulate the effects of rows and columns of a data matrix .This actually is the weighted LS estimation .", "label": "", "metadata": {}, "score": "59.94152"}
{"text": "H . have been computed by inverse iteration , in order to transform them to eigenvectors of .A . . .Functions are also provided to balance the matrix before reducing it to Hessenberg form , as described in Section 2.14.6 .", "label": "", "metadata": {}, "score": "59.956093"}
{"text": "Canonical Correlation Analysis .CCA [ 132 ] , proposed by Hotelling in 1936 , is a multivariate statistical technique .It makes use of two views of the same set of objects and projects them onto a lower - dimensional space in which they are maximally correlated .", "label": "", "metadata": {}, "score": "59.9575"}
{"text": "2.14 Error and Perturbation Bounds and Condition Numbers .In this section we discuss the effects of rounding errors in the solution process and the effects of uncertainties in the data , on the solution to the problem .A number of the functions in this chapter return information , such as condition numbers , that allow these effects to be assessed .", "label": "", "metadata": {}, "score": "59.96233"}
{"text": "Then we use the overlapping information to combine each configuration of subsets to recover the configuration of the whole data .Hence , we named this fast MDS method by the split - and - combine MDS ( SCMDS ) ., which is the computation time of the fast MDS method proposed by Morrison et al . , 2003 [ 14 ] .", "label": "", "metadata": {}, "score": "59.96672"}
{"text": "Perturbation subspaces for block .eigenvector matrices are used to reduce the modified . problem to a sequence of problems of smaller dimension .These perturbation subspaces are shown to be contained . in certain generalized Krylov subspaces of the .$ n$-dimensional space , where $ n$ is the undoubled . dimension of the matrices in the quadratic problem .", "label": "", "metadata": {}, "score": "59.993988"}
{"text": "LineWidth'.25.2)].25 ) set(h1_line(2).1 ) 0 U(1 .set(h1_line(1 ) .Function SVDdemo takes a 2-by-2 matrix and generates two graphs : the original circle together with two perpendicular vectors and their images under the transformation used . 'LineWidth ' .", "label": "", "metadata": {}, "score": "60.000374"}
{"text": "Jose Mourdo / 143 \\\\ .Quasiclassical Domains in a Quantum Universe / James B. .Hartle / 161 \\\\ .Gauge Invariant Energy - Momentum Tensor in Spinar .Electrodynamics / D. Petiot and Y. Takahashi / 173 \\\\ .$ \\gamma$-Ray Bursts and Neutron Star Mergers / Tsvi .", "label": "", "metadata": {}, "score": "60.008568"}
{"text": "Gaussv described in Section 4 . solving a linear system with an upper triangular matrix .Compute the relative error in x. Explain why it happened .where A is a square matrix .where M n x n is the Gauss transformation which is determined by the Gauss vector m and its column index k. i. Which of these methods is faster in general ?", "label": "", "metadata": {}, "score": "60.011642"}
{"text": "Applications addressed in the book include computing .elements of functions of matrices ; obtaining estimates . of the error norm in iterative methods for solving .linear systems and computing parameters in least . squares and total least squares ; and solving ill - posed . problems using Tikhonov regularization .", "label": "", "metadata": {}, "score": "60.123993"}
{"text": "expression for the desired coefficients .Several . examples , illustrating the numerical performance of the .various methods , are presented . \"fur Angewandte Math . , Hamburg Univ . , Germany \" , . C4130 ( Interpolation and function approximation ) \" , . moments ; Numerical performance ; Recursion coefficients ; .", "label": "", "metadata": {}, "score": "60.203896"}
{"text": "The network produces better results than by using the two algorithms successively .An online localized PCA algorithm [ 90 ] is developed by extending the neural gas method [ 91 ] .Instead of the Euclidean distance measure , a combination of a normalized Mahalanobis distance and the squared reconstruction error guides the competition between the units .", "label": "", "metadata": {}, "score": "60.242844"}
{"text": "We use the benchmark Lina picture of .pixels as the training set , and a kid picture of . , which has the similar statistics , is then used for generalization .We first use . blocks .For each square , only the first PC is significant , and all the other PCs can be ignored in terms of the quality of the restored picture .", "label": "", "metadata": {}, "score": "60.24592"}
{"text": "Application of Sparse Matrix Techniques to Reservoir .Simulation / P. T. Woo , S. C. Eisenstat , M. H. Schultz , . and A. H. Sherman / 427 \\\\ .On the Origins and Numerical Solution of Some Sparse . an Advanced Seminar ; Mathematics Research Center , the .", "label": "", "metadata": {}, "score": "60.246544"}
{"text": "The method . is especially well suited when the dimensions of A are . large and the matrix is sparse .It is also possible to .extend this technique to a constrained quadratic form : .For a symmetric matrix A it considers the minimization . of x / sup T / Ax-2b / sup T / x subject to the constraint .", "label": "", "metadata": {}, "score": "60.249016"}
{"text": "The problem is to find the .x . minimizing .A . x .b .Let .x .^ .be the solution computed using one of the methods described above .We discuss the most common case , where .", "label": "", "metadata": {}, "score": "60.256203"}
{"text": "The three - layer ( .linear autoassociative network can also be used as an ICA network , as long as the outputs of the hidden layer are independent .A well - known two - phase approach to ICA is to preprocess the data by PCA and then to estimate the necessary rotation matrix .", "label": "", "metadata": {}, "score": "60.268173"}
{"text": "m . a .x .A Simulation Example .The concept of subspace is involved in many information processing problems .This requires EVD of the autocorrelation matrix of a data set or SVD of the cross - correlation matrix of two data sets .", "label": "", "metadata": {}, "score": "60.281174"}
{"text": "Manteuffel ( 1978 ) .This paper presents an adaptive .Chebyshev iterative method , in which eigenvalue . estimates are computed from modified moments determined .during the iterations .The computation of eigenvalue .estimates from modified moments requires less computer .", "label": "", "metadata": {}, "score": "60.29467"}
{"text": "-1/b// , and estimates for the entries of the matrix .inverse A / sup -1/. All of these questions can be .formulated as a problem of finding an estimate or an . upper and lower bound on u / sup T / F(A)u , where . vector .", "label": "", "metadata": {}, "score": "60.295494"}
{"text": "7.1 Properties and Decompositions / 310 \\\\ . 7.2 Perturbation Theory / 320 \\\\ .7.3 Power Iterations / 330 \\\\ . 7.4 The Hessenberg and Real Schur Forms / 341 \\\\ . 7.5 The Practical $ Q R$ Algorithm / 352 \\\\ . 7.6 Invariant Subspace Computations / 362 \\\\ . 8 : The Symmetric Eigenvalue Problem / 391 \\\\ . 8.1 Properties and Decompositions / 393 \\\\ .", "label": "", "metadata": {}, "score": "60.302704"}
{"text": "LEAP can satisfactorily extract PCs even for ill - conditioned autocorrelation matrices [ 26 ] . , ordered arbitrarily .The algorithm induces the norms of the weight vectors towards the corresponding eigenvalues , that is , .The algorithm breaks the symmetry in its learning process by the difference in the norms of the weight vectors while keeping the symmetry in its structure .", "label": "", "metadata": {}, "score": "60.311317"}
{"text": "This is known as sparse PCA [ 108 ] .Constrained PCA , generalized EVD , and the two - dimensional PCA are three important generalizations to PCA .Constrained Principal Component Analysis .When certain subspaces are less preferred than others , this yields constrained PCA [ 109 ] .", "label": "", "metadata": {}, "score": "60.32763"}
{"text": "Previous algorithms can be unstable and compute the singular values and the singular vectors of A 0 in O \\Gamma ( m + n ) min 2 ( m;n ) \\Delta floating point operations .Introduction .The columns of U and V are the left singular vectors and the right singular vectors of A , respectively ; the diagonal entries of\\Omega are the singular values of A .... . ...", "label": "", "metadata": {}, "score": "60.381416"}
{"text": "A Fast , Stable Implementation of the Simplex Method .Using Bartels - Golub Updating / Michael A. Saunders / .213 \\\\ .Using the Steepest - edge Simplex Algorithm to Solve .Sparse Linear Programs / D. Goldfarb / 227 \\\\ .", "label": "", "metadata": {}, "score": "60.40663"}
{"text": "function approximation ) ; C4140 ( Linear algebra ) \" , . matrix ; Multishift QR iteration ; Shift vector ; Trailing . principal submatrix \" , . cyclically reduced non - symmetrizable linear systems \" , . associated with block relaxation methods for solving .", "label": "", "metadata": {}, "score": "60.434784"}
{"text": "y. respectively .Run function nrceig on several random matrices generated by the functions rand and randn .In this exercise you are to plot the error A .y. Based on your observations .Hint : You may wish to use the following MATLAB functions schur .", "label": "", "metadata": {}, "score": "60.436363"}
{"text": "We . hypothesize that the asymmetry in the distribution of .the peaks of the profiles is due to two competing .evolutionary forces .We show that the asymmetry in the . profiles of the genes might be due to a previously .", "label": "", "metadata": {}, "score": "60.478413"}
{"text": "method for iteratively solving the positive - definite . systems of linear equations .Theoretical analysis shows .that the PSS method converges unconditionally to the .exact solution of the linear system , with the upper . bound of its convergence factor dependent only on the .", "label": "", "metadata": {}, "score": "60.480927"}
{"text": "They may also be used to perform preliminary steps in the solution of eigenvalue or singular value problems , and are useful tools in the solution of a number of other problems .Table 1 Reduction of generalized symmetric - definite eigenproblems to standard problems .", "label": "", "metadata": {}, "score": "60.504807"}
{"text": "coefficients ; Unknown orthogonal polynomials ; Weight . function \" , .Nachtigal \" , . solving large linear systems are reviewed .The main .focus is on developments in the area of conjugate . gradient - type algorithms and Krylov subspace methods .", "label": "", "metadata": {}, "score": "60.53633"}
{"text": "The same bound applies to the right singular vector .v .^ .i .Since the relative gap may be much larger than the absolute gap , this error bound may be much smaller than the previous one .The relative gaps may be easily obtained from the computed singular values .", "label": "", "metadata": {}, "score": "60.57778"}
{"text": "1 Introduction Computation of eigenvalues and eigenvectors is an essential kernel in many applications , and several promising parallel algorithms have been investigated [ 8 , 11 , 7].The work presented in this paper is part of the PRISM ( Parallel Research on Invariant Subspace Methods )", "label": "", "metadata": {}, "score": "60.62211"}
{"text": "Least Mean Squared Error - Based Principal Component Analysis .Existing PCA algorithms including the Hebbian rule - based algorithms can be derived by optimizing an objective function using the gradient - descent method .The least mean squared error-( LMSE- ) based methods are derived from the modified MSE function .", "label": "", "metadata": {}, "score": "60.662376"}
{"text": "[57 ] .On the contrary , nonlinearities growing faster than linearly cause stability problems easily and are not recommended .These extensions are also introduced in [ 29 , 58 , 59 ] .The multilayer perceptron ( MLP ) can be used to perform nonlinear dimensionality reduction and hence nonlinear PCA .", "label": "", "metadata": {}, "score": "60.668095"}
{"text": "C7310 ( Mathematics ) \" , .Iterative methods ; Large scale least squares problems ; .Least squares methods ; Observations ; Parallel ; .Satellites ; Unknowns \" , . algorithms : 1948 - -1976 \" , .Eigenvalue Problem : Theoretical Considerations \" , .", "label": "", "metadata": {}, "score": "60.69949"}
{"text": "Computing , NUMERICAL ANALYSIS , General , Parallel . algorithms \\\\ G.1.6 Mathematics of Computing , NUMERICAL .data fitting applications \" , . non - self - adjoint elliptic problems \" , .JSTOR database \" , . eigenvalues of operators used in various iterative . methods for the solution of linear systems of . equations .", "label": "", "metadata": {}, "score": "60.74757"}
{"text": "Problems \" , .Solution \" , .Transformation \" , . to the paper by the same authors in the Numerical .Analysis department of this issue on pages 401 - -406 .This is the first instance of .this type of concurrent publication in Communications .", "label": "", "metadata": {}, "score": "60.77188"}
{"text": "It is shown how the classical QR . algorithm can be extended to provide a stable algorithm .for computing this generalized decomposition .The . decomposition is also applied to cyclic matrices and .two - point boundary value problems .", "label": "", "metadata": {}, "score": "60.79609"}
{"text": "Vision .ECCV ' 96 \" , .Center , Yorktown Heights , .NY , USA \" , . graphs ; discrete graph signals ; filter design ; Fourier . series ; low - pass filtering ; low - pass filters ; optimal . surface smoothing ; optimisation ; parameter estimation ; . polyhedral surfaces ; power spectrum estimation ; signal .", "label": "", "metadata": {}, "score": "60.82891"}
{"text": "Let A 2 R m\\Thetan be a matrix with known singular values and singular vectors , and let A 0 be the matrix obtained by appending a row to A. We present stable and fast algorithms for computing the singular values and the singular vectors of A 0 in O \\Gamma ( m + n ) min(m;n ) log 2 2 ffl \\De ... \" .", "label": "", "metadata": {}, "score": "60.83754"}
{"text": "The NIC algorithm is a PSA method .It can extract the principal eigenvectors when the deflation technique is incorporated .The NIC algorithm converges much faster than SLA [ 22 ] and LMSER [ 29 ] and is able to globally converge to the PSA solution from almost any weight initialization .", "label": "", "metadata": {}, "score": "60.83927"}
{"text": "At the same time , the SVD has fundamental importance in several different applications of linear algebra .Strang was aware of these facts when he introduced the SVD in his now classical text [ 22 , page 142 ] , observing ... it is not nearly as famous as it should be .", "label": "", "metadata": {}, "score": "60.841393"}
{"text": "There are at most .nonzero generalized eigenvalues and thus an upper bound on . is . ; at least . samples are needed to guarantee . to be nonsingular .This requirement on the number of samples may be severe for some problems like image processing .", "label": "", "metadata": {}, "score": "60.84244"}
{"text": "R . algorithm to find all the eigenvectors of .A . ; application of .Q . to another matrix is required after eigenvectors of .T . have been found by inverse iteration , in order to transform them to eigenvectors of .", "label": "", "metadata": {}, "score": "60.85087"}
{"text": "( Information theory ) \" , .Least squares polynomial fits ; Orthogonal polynomials ; .Similarity transformations ; Tridiagonal Jacobi . matrices ; Updating \" , . data fitting applications \" , . nonselfadjoint linear systems .JSTOR database \" , .", "label": "", "metadata": {}, "score": "60.874615"}
{"text": "Numerical Solution of Elliptic Partial Differential .Equations \" , . differential equations by a generalized conjugate . gradient method \" , .Nonlinear Least Squares Problems and Other Tales \" , .Department Technical Report STAN - CS-76 - 559 .", "label": "", "metadata": {}, "score": "60.881332"}
{"text": "Method \" , .Problem \" , . for Large Scale Geodetic Least Squares Problems \" , . nested dissection ; nla ; sparse ; theory ; verification \" , .Optimization , Least squares methods \\\\ J.2 Computer .Applications , PHYSICAL SCIENCES AND ENGINEERING , . nonlinear equations \" , .", "label": "", "metadata": {}, "score": "60.941406"}
{"text": "The APEX algorithm has been applied to recursively solve the constrained PCA problem [ 35 ] .The constrained PAST algorithm [ 110 ] is for tracking the signal subspace recursively .Based on an interpretation of the signal subspace as the solution of a constrained minimization task , it guarantees the orthonormality of the estimated signal subspace basis at each update , hence avoiding orthonormalization process .", "label": "", "metadata": {}, "score": "60.957825"}
{"text": ", the computational complexity becomes almost the same as the original PCA and SVD .Our approach has no advantage in the latter case .SVD for Continuously Growing Data .In this section , we look for the solution when the data is updated constantly and we need to compute the SVD continuously .", "label": "", "metadata": {}, "score": "60.971725"}
{"text": "22 , pp .4417 - 4435 , 2011 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . Y. Chen , N. M. Nasrabadi , and T. D. Tran , \" Effects of linear projections on the performance of target detection and classification in hyperspectral imagery , \" Journal of Applied Remote Sensing , vol .", "label": "", "metadata": {}, "score": "61.002693"}
{"text": "The inverse of the nonlinear mixing model can be modeled by using the three - layer MLP , the RBFN , the SOM , or the kernel - based nonlinear BSS method [ 10 ] .Nonnegativity is a natural condition for many real - world applications , for example , in the analysis of images , text , or air quality .", "label": "", "metadata": {}, "score": "61.00731"}
{"text": "By carrying out a small part of the . calculation in higher precision , it is demonstrated .that eigenvectors corresponding to clustered . eigenvalues can be computed to high accuracy .Some . numerical results for the Schrodinger eigenvalue .problem are given .", "label": "", "metadata": {}, "score": "61.03121"}
{"text": "In [ 86 ] , an MCA algorithm for extracting multiple MCs is described by using the idea of sequential addition ; a conversion method between MCA and PCA is also discussed .Based on a generalized differential equation for the generalized eigenvalue problem , a class of algorithms can be obtained for extracting the first PC or MC by selecting different parameters and functions [ 87 ] .", "label": "", "metadata": {}, "score": "61.106182"}
{"text": "z3950.loc.gov:7090/Voyager \" , . computing and linear algebra , relevant to image and . signal processing .For these applications , it is . important to consider ingredients such as : ( 1 ) .sophisticated mathematical models of the problems , . including a priori knowledge , ( 2 ) rigorous mathematical .", "label": "", "metadata": {}, "score": "61.138203"}
{"text": "The purpose of the second simulation experiment is to observe the approximation performance of applying the SCPCA to a big full rank matrix .We generate a random matrix with a fixed number of columns and rows , say 1000 .The square matrix is created by the form , . is a small coefficient for adjusting the influence to the previous matrix .", "label": "", "metadata": {}, "score": "61.156235"}
{"text": "Solution of Nonseparable Elliptic Equations \" , .JSTOR database \" , . H. Golub and F. S. Hillier and A. S. Manne and D. J. .Wilde and R. B. Wilson \" , .The original title .Mathematical Programming matches a great many library .", "label": "", "metadata": {}, "score": "61.209473"}
{"text": "1213 - 1218 , San Francisco , Calif , USA , 1993 .J. A. Catalan , J. S. Jin , and T. Gedeon , \" Reducing the dimensions of texture features for image retrieval using multilayer neural networks , \" Pattern Analysis and Applications , vol .", "label": "", "metadata": {}, "score": "61.20992"}
{"text": "v ) that computes the smallest . find .what conjecture can be formulated about the eigenvectors of A ? n ) .n ) that computes coefficients of the approximating polynomials .n ) be an n - by - n symmetric pentadiagonal matrix generated by function pent of Problem 22 .", "label": "", "metadata": {}, "score": "61.230465"}
{"text": "of ICA is not orthogonal , while in PCA the components of the weights are represented on an orthonormal basis .ICA provides in many cases a more meaningful representation of the data than PCA .ICA can be realized by adding nonlinearity to linear PCA networks such that they are able to improve the independence of their outputs .", "label": "", "metadata": {}, "score": "61.238796"}
{"text": "th neuron in Kalman - type RLSA takes a special value .In the one - unit case , both PAST and PASTd reduce to Oja 's learning rule [ 14 ] .Both PAST and PASTd provide much more robust estimates than eigenvalue decomposition ( EVD ) and converge much faster than SLA [ 14 ] .", "label": "", "metadata": {}, "score": "61.262733"}
{"text": "The differentiation of pseudo - inverses and non - linear .least squares problems whose variables separate , G. .Golub and V. Pereyra \\\\ .Generalized cross - validation as a method for choosing a .good ridge parameter , G. Golub , M. Heath , G. Wahba \\\\ .", "label": "", "metadata": {}, "score": "61.28443"}
{"text": "Iteration Polynomials / C. C. Paige / 83 \\\\ .Do We Fully Understand the Symmetric Lanczos Algorithm .Yet ? / Beresford N. Parlett / 93 \\\\ .On Generalized Band Matrices and Their Inverses / .Bevilacqua / 109 \\\\ .", "label": "", "metadata": {}, "score": "61.29668"}
{"text": "To compute the dominant eigenpair of A we use function Rqi [ l1 .Functions Housv .Here sp stands for the spectrum of a matrix .A(1 .the off diagonal entries approach zero fast .This is an iterative process .", "label": "", "metadata": {}, "score": "61.303158"}
{"text": "Diagonal PCA [ 119 ] improves two - dimensional PCA by defining the image scatter matrix as the covariances between the variations of the rows and those of the columns of the images and is shown to be more accurate than PCA and two - dimensional PCA .", "label": "", "metadata": {}, "score": "61.30863"}
{"text": "With the advent of . satellites , it is possible to collect large sets of .data which are frequently analysed through least . squares methods .These problems can have over a million . observations and several hundred thousand unknowns .", "label": "", "metadata": {}, "score": "61.324314"}
{"text": "with inner - outer iterations \" , .Ser .Math .Sci .Appl . \" Moments \" , . quadrature rules \" , .Speech , and Signal Processing , 1999 . superresolution \" , .Reichel \" , . quotient \" , .", "label": "", "metadata": {}, "score": "61.364376"}
{"text": "In addition to the feedforward weights .Figure 2 : Architecture of the PCA network with hierarchical lateral connections .The lateral weight matrix .The Rubner - Tavan PCA algorithm is based on the PCA network with hierarchical lateral connection topology [ 13 , 49 ] .", "label": "", "metadata": {}, "score": "61.429245"}
{"text": "317 - 320 , San Francisco , Calif , USA , 1992 . A. Cichocki , R. W. Swiniarski , and R. E. Bogner , \" Hierarchical neural network for robust PCA computation of complex valued signals , \" in Proceedings of the World Congress on Neural Networks , pp .", "label": "", "metadata": {}, "score": "61.458195"}
{"text": "Other Optimization - Based Principal Component Analysis .PCA can be derived by any optimization method based on a proper objective function .This leads to many other algorithms , including gradient - descent based algorithms [ 15 , 16 , 39 , 40 ] , the conjugate gradient ( CG ) method [ 41 ] , and the quasi - Newton method [ 42 , 43 ] .", "label": "", "metadata": {}, "score": "61.476833"}
{"text": "A .P .C .A . , as .The algorithm extracts the first .principal singular values in the descending order and their corresponding left and right singular vectors .Like APEX , the APCA algorithm incrementally adds nodes without retraining the learned nodes .", "label": "", "metadata": {}, "score": "61.493443"}
{"text": "11 - 19 , Morgan Kaufmann , San Mateo , Calif , USA , 1989 .View at Google Scholar .P. Baldi and K. Hornik , \" Neural networks and principal component analysis : learning from examples without local minima , \" Neural Networks , vol .", "label": "", "metadata": {}, "score": "61.501"}
{"text": "In HSI applications , the datasets can easily break into the million - pixel or even giga - pixel level , which renders this operation impossible on typical desktop computers .One solution is to apply probabilistic methods which give closely approximated singular vectors and singular values , while the complexity is at a much lower level .", "label": "", "metadata": {}, "score": "61.50895"}
{"text": ", whereas , for PCA , the size is .This results in considerable computational advantage in two - dimensional PCA .Two - dimensional PCA evaluates the covariance matrix more accurately over PCA .When used for face recognition , two - dimensional PCA results in a better recognition accuracy .", "label": "", "metadata": {}, "score": "61.527546"}
{"text": "[ . .] , as .The weighted SLA [ 22 ] performs well for extracting less dominant components .Other Hebbian Rule - Based Algorithms .The LEAP algorithm [ 26 ] is a local PCA algorithm for extracting all the .", "label": "", "metadata": {}, "score": "61.56539"}
{"text": "Vector space is enhanced semantically by modifying the weight of the word vector governed by Appearance and Disappearance ( Action class ) words .The knowledge base for Action words is maintained by classifying the words as Appearance or Disappearance with the help of Wordnet .", "label": "", "metadata": {}, "score": "61.586273"}
{"text": "multiplications per iteration , which is twice the complexity of the LMS [ 78 ] .An adaptive step - size learning algorithm [ 79 ] has been derived for extracting the MC by introducing information criterion .The algorithm globally converges asymptotically to the MC and its corresponding eigenvector .", "label": "", "metadata": {}, "score": "61.603065"}
{"text": "Function mytrid creates a sparse form of the tridiagonal matrix with constant entries along the diagonals .Its syntax is shown below spdiags(B.2 ) ( 5 . and c.b .[ 1 2 3 4]. main diagonal .n).6 ) .", "label": "", "metadata": {}, "score": "61.608112"}
{"text": "approximating the stable part of the column space of .\\$A\\$. \" determination , pert , variable selection \" , .If so , . which report number is correct ? ?\" If so , . which report number is correct ? ?", "label": "", "metadata": {}, "score": "61.609352"}
{"text": "318 - 362 , MIT Press , Cambridge , Mass , USA , 1986 .View at Google Scholar .N. Kambhatla and T. K. Leen , \" Fast non - linear dimension reduction , \" in Proceedings of the IEEE International Conference on Neural Networks , vol .", "label": "", "metadata": {}, "score": "61.611362"}
{"text": "Structured Problems , held in Minneapolis , Minnesota , .the IMA Workshop on Iterative Methods for Sparse and .Structured Problems , held in Minneapolis , Minnesota , .( Berlin ) \" , .R43 1994 \" , . methods / Dianne P. O'Leary / 1 / \\\\ .", "label": "", "metadata": {}, "score": "61.646732"}
{"text": "prepared for the American Mathematical Society Short .Course on Numerical Analysis , held in Atlanta , Georgia , .January 3 - -4 , 1978 . \" boundary conditions .Its Applications \" , .Overton \" , .Values and Corresponding Singular Vectors of a Matrix \" , . singular values , singular vectors , large sparse matrix , . singular - value decomposition , upper - triangular band .", "label": "", "metadata": {}, "score": "61.649532"}
{"text": "The adaptive invariant - norm MCA algorithm [ 85 ] has been generalized to the case for complex - valued input signal vector .For ICA algorithms , FastICA has been applied to complex signals [ 101 ] .The . -APEX algorithms and GHA are , respectively , extended to the complex - valued case [ 102 , 103 ] .", "label": "", "metadata": {}, "score": "61.690228"}
{"text": "To simulate large HSI datasets , we generate random test matrices . by rSVD as shown in Figure 3(b ) , where we clearly see that both sets are almost identical up to the fifteenth singular vector .To judge the accuracy of estimated singular values , we compute the relative absolute errors , . and remain in the same order even when the size of a matrix increases .", "label": "", "metadata": {}, "score": "61.76407"}
{"text": "solution of the resulting reduced system by line .relaxation .They augment previous analyses of one - line .methods , and derive a new convergence analysis for .two - line methods , showing that both classes of methods . are highly effective for solving the . convection - diffusion equation .", "label": "", "metadata": {}, "score": "61.826218"}
{"text": "Function spy creates a graph of the matrix .The above command generates a 50-by-50 random sparse matrix A with a density of about 25%.50 .Bau III .[ 3 ] G. Golub and Ch.40 - [ 1 ] B. Baltimore .", "label": "", "metadata": {}, "score": "61.85556"}
{"text": "In this case , computing .the entire decomposition is very wasteful .However , one does not . know , a priori , how many singular triplets .will be required .So you obtain a small number ( say , 50 ) of the highest .", "label": "", "metadata": {}, "score": "61.89114"}
{"text": "In order to implement IR Systems based on neural networks , the data needs to undergo appro ... \" .Soft Computing techniques like neural networks are well suited to process texts due to their vague processing capabilities .However , indexing in Information Retrieval ( IR ) produces large and sparse patterns .", "label": "", "metadata": {}, "score": "61.90264"}
{"text": "The first eight singular vectors , . , are folded back from the transformed data .Starting from the eighth , the rest of the singular vectors appear to be mostly noise .( notice the log scale on the y axis ) , which means that the entropy of residuals is significantly smaller than the entropy of the original .", "label": "", "metadata": {}, "score": "61.937317"}
{"text": "53 - 58 , 1989 .View at Google Scholar \u00b7 View at Scopus .M. A. Kramer , \" Nonlinear principal component analysis using autoassociative neural networks , \" AIChE Journal , vol .37 , no . 2 , pp .", "label": "", "metadata": {}, "score": "62.040245"}
{"text": "In the general case when we may have .rank .A . min .m .n . - in other words , .A . may be rank - deficient - we seek the minimum norm least squares solution .", "label": "", "metadata": {}, "score": "62.06431"}
{"text": "Variation - Based Image Restoration \" , .Sayed \" , .Errors - in - Variables Model \" , . with a Secant Update Formula \" , .M. Ram \" , .Inner - Outer Iteration \" , .Fourth Japan - China Joint Seminar on Numerical .", "label": "", "metadata": {}, "score": "62.065407"}
{"text": "Nonnegative PCA and nonnegative ICA algorithms are given in [ 150 ] , where the sources . must be nonnegative .Constrained ICA is a framework that incorporates additional requirements and prior information in form of constraints into the ICA contrast function [ 151 ] .", "label": "", "metadata": {}, "score": "62.107277"}
{"text": "333 - 344 , 2004 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .W. Zuo , D. Zhang , and K. Wang , \" Bidirectional PCA with assembled matrix distance metric for image recognition , \" IEEE Transactions on Systems , Man , and Cybernetics , Part B , vol .", "label": "", "metadata": {}, "score": "62.16796"}
{"text": "Global Homotopies and Newton Methods / Herbert B. .Keller / 73 \\\\ .Problems with Different Time Scales / Heinz - Otto Kreiss ./ 95 \\\\ .Accuracy and Resolution in the Computation of Solutions . of Linear and Nonlinear Equations / Peter D. Lax / 107 .", "label": "", "metadata": {}, "score": "62.274128"}
{"text": "P .T .F .F .F .F .F .F .F .P .B .P .T .G .G .G .G .G .G .G .This transformation usually improves the accuracy of computed generalized eigenvalues and eigenvectors .", "label": "", "metadata": {}, "score": "62.276966"}
{"text": "Algebra / 48 \\\\ . 2.2Vector Norms / 52 \\\\ . 2.3 Matrix Norms / 54 \\\\ . 2.4 Finite Precision Matrix Computations / 59 \\\\ . 2.5 Orthogonality and the SVD / 69 \\\\ . 2.6Projections and the $ CS$ Decomposition / 75 \\\\ .", "label": "", "metadata": {}, "score": "62.284515"}
{"text": "Lanczos process .Using this approach , we first recall .the exact arithmetic solution of the questions .formulated above and then analyze the effect of . rounding errors in the quadrature calculations .It is . proved that the basic relation between the accuracy of .", "label": "", "metadata": {}, "score": "62.292114"}
{"text": "1967 - 1979 , 1998 .View at Google Scholar \u00b7 View at Scopus . A. C. S. Leung , K. W. Wong , and A. C. Tsoi , \" Recursive algorithms for principal component extraction , \" Network , vol . 8 , no . 3 , pp .", "label": "", "metadata": {}, "score": "62.29596"}
{"text": "reliable methods for large scale problems are still .missing .In this paper approximation techniques based .on the Lanczos algorithm and the theory of Gauss .quadrature are proposed to reduce the computational .complexity for large scale . \" Burrage and S. L. Campbell and G. H. Golub and E. .", "label": "", "metadata": {}, "score": "62.373016"}
{"text": "There is one important difference between the general approach and the SCSVD approach .To obtain the SVD from the MDS result , we need the column mean vector of the matrix .The column mean vector of the original matrix must be computed , and then the column mean vector is updated by the simple weighted average when the new data comes in .", "label": "", "metadata": {}, "score": "62.452866"}
{"text": "grouping the vectors into clusters and enforcing the .bi - orthogonality property only between different . clusters , but relaxing the property within clusters .They show how this variant of the matrix Lanczos . algorithm applies directly to a problem of computing a . set of orthogonal polynomials and associated indefinite . weights with respect to an indefinite inner product , .", "label": "", "metadata": {}, "score": "62.45807"}
{"text": "Numerical Mathematics \" , .An . engineering \" , .Univ .Mex . , M\\'exico \" , . systems \" , . projection method and its applications \" , .An International Journal on the .Theory and Practice of Inverse Problems , Inverse .", "label": "", "metadata": {}, "score": "62.56018"}
{"text": "Thus , it is important to clarify the . connection between a finite Jacobi matrix and its .corresponding weight function(s ) .This leads to the .need for stable numerical processes that evaluate such . matrices .Three new O(n / sup 2/ ) methods are derived . that ' merge ' Jacobi matrices directly without using any .", "label": "", "metadata": {}, "score": "62.666176"}
{"text": "From the above analysis , we can have a fast PCA approach by computing the SCMDS first and then adapt the MDS result to obtain the PCA .We named this approach SCPCA .Similarly , the fast SVD approach , which computes the SCMDS first , then adapts the MDS result to obtain the PCA , and finally adapts the PCA result to the SVD , is called the SCSVD .", "label": "", "metadata": {}, "score": "62.685806"}
{"text": "The eigenvectors are also determined more accurately than for general matrices , and may be computed more accurately as well .This work extends results of Kahan and Demmel for bidiagonal and tridiagonal matrices . ... or sym metric positive definite tridiagonal matrices QR iteration ( suitably modified ) can be shown to attain high accuracy as well .", "label": "", "metadata": {}, "score": "62.715416"}
{"text": "Abstract .A new rank revealing method is proposed .When a row or column is inserted or deleted , algorithms for updating / downdating the approximate rank and null space are straightforward , stable and efficient .Numerical results exhibiting the advantages of our code over existing packages based on two - sided orthogonal rankrevealing decompositions are presented .", "label": "", "metadata": {}, "score": "62.716896"}
{"text": "approximation ; nla , signal processing , Lanczos . algorithm , rank determination ; Numerical methods ; .Signal processing ; Singular value decompositions ; .Square root algorithms ; Tracking ; Triangular factor \" , . eigenfunctions ; Matrix algebra ; Numerical methods ; . non - self - adjoint linear systems \" , . systems of the type arising from two - cyclic . discretizations of non - self - adjoint two - dimensional .", "label": "", "metadata": {}, "score": "62.74288"}
{"text": "We present a new algorithm hich computes all the singular values of a bidiagonal matrix to high relative accuracy independent of their magnitudes .In contrast , the standard algorithm for bidiagonal matrices may compute small singular values with no relative accuracy at all .", "label": "", "metadata": {}, "score": "62.744324"}
{"text": "Robust PCA can be defined so that the optimization criterion grows less than quadratically with the same constraint conditions as those for PCA , which are based on the quadratic criterion [ 57 ] .This usually leads to mildly nonlinear algorithms , in which the nonlinearities appear at selected places only and at least one neuron produces the linear response .", "label": "", "metadata": {}, "score": "62.747818"}
{"text": "In Quadratic Extrapolation , we take .advantage of the fact that the first eigenvalue of a .Markov matrix is known to be 1 to compute the .nonprincipal eigenvectors using successive iterates of .the Power Method .Empirically , we show that using .", "label": "", "metadata": {}, "score": "62.76051"}
{"text": "Modified moments ; nonsymmetric linear systems ; .Nonsymmetric linear systems ; polynomials \" , . eigenfunctions ; Iterative methods ; Polynomials \" , . iteration \" , .Law \" , . reanalysis problems \" , .Engineering \" , . of the shifts \" , . and Demmel ( 1989 ) requires the computation of a ' shift . vector ' defined by m shifts of the origin of the .", "label": "", "metadata": {}, "score": "62.79783"}
{"text": "major themes were on numerical linear algebra , signal . processing , and image processing . \"Kong ) \" , . processing \" , .( G. H. Golub and U. von Matt)\\\\ .Numerical stability of some fast algorithms for .", "label": "", "metadata": {}, "score": "62.88514"}
{"text": "Thus , it is important to make sure that the estimated rank is greater than the essential rank .In other words , when the estimated rank of the SCSVD is smaller than the essential rank , our SCSVD result can be used as the approximated solution of the SVD . . .", "label": "", "metadata": {}, "score": "62.897293"}
{"text": "Vectors \" , .Martin H. Gutknecht \" , . polynomials associated with indefinite weights \" , . matrix to tridiagonal by generating two sequences of . vectors which satisfy a mutual bi - orthogonality . property .The process can proceed as long as the two . vectors generated at each stage are not mutually . orthogonal , otherwise the process breaks down .", "label": "", "metadata": {}, "score": "62.96927"}
{"text": "Linear systems of equations ; Modified moments ; nla , . sparse , svd ; Parallel computers ; Seismic reflection .tomography ; Singular vectors \" , . applications \" , . algorithm that does not require strict bi - orthogonality . among the generated vectors .", "label": "", "metadata": {}, "score": "62.98258"}
{"text": "Sci .Numerical analysis ( Louvain - la - Neuve , 1995 ) \" , .Belg .Math .Soc .Simon Stevin \" , . approximations for advection diffusion problems \" , .( paperback ) \" , .G65 1996 \" , .", "label": "", "metadata": {}, "score": "62.98516"}
{"text": "A signal subspace rank estimator is employed to track the number of sources .Generalized Eigenvalue Decomposition .Generalized EVD is a statistical tool extremely useful in feature extraction , pattern recognition as well as signal estimation and detection .The generalized EVD problem involves the matrix equation .", "label": "", "metadata": {}, "score": "63.0365"}
{"text": "333 - 346 , 1998 .View at Google Scholar \u00b7 View at Scopus .L. Sun , S. Ji , and J. Ye , \" Canonical correlation analysis for multilabel classification : a least - squares formulation , extensions , and analysis , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "63.061882"}
{"text": "A fast algorithm for $ QR$ decomposition of Toeplitz . matrices / G. O. Glentis / 387 \\\\ .Unitary Hessenberg methods for Toeplitz approximations .and applications / Ch .He and A. Bunse - Gerstner / 389 .\\\\ .", "label": "", "metadata": {}, "score": "63.062115"}
{"text": "This paper takes an in - depth look at a technique for computing filtered matrix - vector ( mat - vec ) products which are required in many data analysis applications .In these applications the data matrix is multiplied by a vector and we wish to perform this product accurately in the space spanned by a few of the major singular vectors of the matrix .", "label": "", "metadata": {}, "score": "63.084587"}
{"text": "Systems \" , .Sci . , Stanford Univ . , CA , USA \" , . numerical methods ; error analysis ; error bound ; . experimentation ; inexactness ; iter ; iterative methods ; . linear algebra ; linear system ; linear systems ; . measurement ; nla ; nonsymmetric inexact Chebyshev . iteration ; numerical experiments ; performance ; . preconditioned iteration ; Richardson iterative methods ; . skew - symmetric iteration ; spectral radius ; theory ; . verification \" , .", "label": "", "metadata": {}, "score": "63.104847"}
{"text": "Math .Soc .Simon Stevin \" , . and parallel numerics ] \" , .Dieter Grigorieff and Hartmut Schwandt . \" introduction to numerical methods \" , .z3950.gbv.de:20011/gvk \" , . processing and detection ) ; C1160 ( Combinatorial .", "label": "", "metadata": {}, "score": "63.17093"}
{"text": "v2 ) of the matrix A is computed .[ l2.A ) .The eigenpair ( l2.44721359549996 0 .For details the reader is referred to [ 4].v2.rand(5 .i.[a .Houspre .127 - 128.2:n).44721359549996 and next apply function defl to compute another eigenpair of A .", "label": "", "metadata": {}, "score": "63.19751"}
{"text": "While .this approach works reasonably well , it can fail in .extreme cases involving aggressive personalization .We . prove that our algorithm is the generally correct way . of handling dangling nodes using probabilistic . arguments .We also discuss variants of our algorithm , . including a multistage extension for calculating a .", "label": "", "metadata": {}, "score": "63.226715"}
{"text": "View at Google Scholar \u00b7 View at Scopus . A. Weingessel , H. Bischof , K. Hornik , and F. Leisch , \" Adaptive combination of PCA and VQ networks , \" IEEE Transactions on Neural Networks , vol . 8 , no .", "label": "", "metadata": {}, "score": "63.30933"}
{"text": "Reorder Schur factorization .Reorder Schur factorization , find basis for invariant subspace and estimate sensitivities .Functions are also provided to balance a general matrix pair before reducing it to generalized Hessenberg form , as described in Section 2.14.8 .Companion functions are provided to transform vectors of the balanced pair to those of the original matrix pair .", "label": "", "metadata": {}, "score": "63.310677"}
{"text": "Here n stands for the degree of the polynomial .Write MATLAB function [ r. 16 .Repeat this experiment using random numbers in the band of the matrix A. 1 .Hint : To create the Vandermonde matrix needed in the body of the function lspol you may wish to use function Vandm of Problem 17 .", "label": "", "metadata": {}, "score": "63.31406"}
{"text": "\\\\ .Case studies of real - time processing in robotics / W. .M. Gentleman / 165 \\\\ .Adaptive signal processing with emphasis on QRD - least . squares lattice / S. Haykin / 183 \\\\ .A direct method for reordering eigenvalues in the .", "label": "", "metadata": {}, "score": "63.315296"}
{"text": "such that the components of . , which is the estimate of ., are statistically as independent as possible , . being a .demixing matrix .The statistical independence property implies that the joint probability density of the components of .", "label": "", "metadata": {}, "score": "63.324104"}
{"text": "View at Google Scholar \u00b7 View at Scopus . D. Xu , J. C. Principe , and H. C. Wu , \" Generalized eigendecomposition with an on - line local algorithm , \" IEEE Signal Processing Letters , vol .5 , no .", "label": "", "metadata": {}, "score": "63.37034"}
{"text": "The lateral connections can be in a symmetrical or hierarchical topology .The hierarchical lateral connection topology is illustrated in Figure 2 , based on which Rubner - Tavan PCA [ 13 , 49 ] and APEX [ 50 ] algorithms are proposed .", "label": "", "metadata": {}, "score": "63.403465"}
{"text": "View at Google Scholar \u00b7 View at Scopus . K. Gao , M. O. Ahmad , and M. N. S. Swamy , \" Constrained anti - Hebbian learning algorithm for total least - squares estimation with applications to adaptive FIR and IIR filtering , \" IEEE Transactions on Circuits and Systems II , vol .", "label": "", "metadata": {}, "score": "63.45237"}
{"text": "Mathematics of Computing , NUMERICAL ANALYSIS , Numerical .The authors show how to eliminate the linear .constraint .Three different methods are presented for .the solution of the resulting Lagrange equations . \" fur Inf . , ETH , Zurich , Switzerland \" , .", "label": "", "metadata": {}, "score": "63.479362"}
{"text": "The APEX algorithm is used to adaptively extract the PCs [ 50 ] .The algorithm is recursive and adaptive , namely , given .PCs , it can produce the .th PC iteratively .The hierarchical structure of lateral connections serves the purpose of weight orthogonalization and also allows the network to grow or shrink without retraining the old units .", "label": "", "metadata": {}, "score": "63.497807"}
{"text": "Here we apply the Hoffman coding due to its fast computation and show the compression ratios at various error rates , corresponding to the numbers of bits required to code the residuals .For example , a 16-bit coding would result in an error in the range of .", "label": "", "metadata": {}, "score": "63.509502"}
{"text": "antenna array signal processing / I. K. Proudler / 411 .\\\\ .On displacement structures for covariance matrices and .lossless functions / P. A. Regalia and F. Desbouvries / .413 \\\\ .Application of vector extrapolation and conjugate . gradient type methods to the semiconductor device .", "label": "", "metadata": {}, "score": "63.51818"}
{"text": "Note that the diag function takes a second optional argument .Which of the methods used seems to produce the least reliable numerical results ?Justify your answer .Repeat this experiment several times.4 . diag .and the y - coordinates of the points to be approximated .", "label": "", "metadata": {}, "score": "63.54414"}
{"text": "Least Squares Problems Whose Variables Separate \" , . database \" , . detecting spurosity in the general univariate linear . model \" , . models \" , . squares , which is available at .Reprinted in . algebra \" , .", "label": "", "metadata": {}, "score": "63.57644"}
{"text": "iterative methods ) \\\\ G.1.3 Mathematics of Computing , .NUMERICAL ANALYSIS , Numerical Linear Algebra , .Eigenvalues \\\\ G.1.3 Mathematics of Computing , .NUMERICAL ANALYSIS , Numerical Linear Algebra , Matrix .for a standardized nomenclature \" , . properties and applications \" , .", "label": "", "metadata": {}, "score": "63.625282"}
{"text": "ICA can extract the statistically independent components from the input data set .It is to estimate the mutual information between the signals by adjusting the estimated matrix to give outputs that are maximally independent .By applying ICA to estimate the independent input data from raw data , a statistical test can be derived to reduce the input dimension .", "label": "", "metadata": {}, "score": "63.629852"}
{"text": "Math . , Hamburg Univ . , Germany \" , . C4160 ( Numerical integration and differentiation ) ; . C4180 ( Integral equations ) \" , .Integrals ; Integrand ; Integration interval ; Known .orthogonal polynomials ; Modified moments ; Rational .", "label": "", "metadata": {}, "score": "63.681263"}
{"text": "Constant coefficient problems ; Convergence bounds ; .Convergence rates ; Cyclic reduction method ; Discrete . convection - diffusion equation ; Incomplete LU . preconditioners ; Line iterative methods ; Line . orderings ; Preconditioned generalized minimum residual .methods ; Separable variable coefficients ; SOR methods \" , .", "label": "", "metadata": {}, "score": "63.700943"}
{"text": "more complicated treatment .The understanding of these .degenerate situations makes it possible to construct a . stable approximate solution of an ill - conditioned . problem .The application to adaptive iterative methods .for linear systems of equations is anticipated .", "label": "", "metadata": {}, "score": "63.73468"}
{"text": "N .I .C . has a single global maximum , and all the other stationary points are unstable saddle points .At the global maximum , . yields an arbitrary orthonormal basis of the principal subspace .The NIC algorithm has a computational complexity of .", "label": "", "metadata": {}, "score": "63.768982"}
{"text": "These spaces are called invariant subspaces in the case of eigenvectors , because if .v . is any vector in the space , .A .v . is also in the space , where .A . is the matrix .", "label": "", "metadata": {}, "score": "63.780876"}
{"text": "exploit local similarity structures in the data as well . as least squares optimization process .Gene H. Golub \" , .Proper Singular Value Cluster in the Nonnormal Case \" , . bit - rate block coders \" , .Technology \" , .", "label": "", "metadata": {}, "score": "63.822945"}
{"text": "Numerical tests on real HSI data suggest that the method is promising and is particularly effective for HSI data interrogation .Introduction .Hyperspectral imagery ( HSI ) data are measurements of the electromagnetic radiation reflected from an object or scene ( i.e. , materials in the image ) at many narrow wavelength bands .", "label": "", "metadata": {}, "score": "63.833366"}
{"text": "Conventional algorithms , including those currently implemented in ( Sca)LAPACK , perform asymptotically more communication than these lower bounds require .In this paper we present parallel and sequential eigenvalue algorithms ( for pencils , nonsymmetric matrices , and symmetric matrices ) and SVD algorithms that do attain these lower bounds , and analyze their convergence and communication costs . ... rix it computes the Schur form .", "label": "", "metadata": {}, "score": "63.84713"}
{"text": "Oja 's minor subspace analysis ( MSA ) algorithm can be formulated by reversing the sign of the learning rate of the SLA .This algorithm requires the assumption that the smallest eigenvalue of the autocorrelation matrix .is less than unity .", "label": "", "metadata": {}, "score": "63.86172"}
{"text": "Problems \" , . a Good Ridge Parameter \" , . squares , which is available at .Reprinted in .Loan \" , . photogrammetry \" , . database \" , .Dissection and Orthogonal Decomposition \" , .S93 v.22 \" , .", "label": "", "metadata": {}, "score": "63.88931"}
{"text": "that is not of full rank .A definition of numerical .rank is given .It is shown that under certain .conditions when \\$A\\$ has numerical rank \\$r\\$ there is .a distinguished \\$r\\$ dimensional subspace of the . column space of \\$A\\$ that is insensitive to how it is . approximated by \\$r\\$ independent columns of \\$A\\$. The . consequences of this fact for the least squares problem . are examined .", "label": "", "metadata": {}, "score": "63.930794"}
{"text": "Multistep Methods / 157 \\\\ . D. J. Higham : The Dynamics of a Discretised Nonlinear .Delay Differential Equation / 167 \\\\ . A. R. Humphries , D. A. Jones \\ & A. M. Stuart : .Approximation of Dissipative Partial Differential .", "label": "", "metadata": {}, "score": "63.955566"}
{"text": "A(k+1:m . are of great importance in numerical linear algebra.n ) ' .Kahan method.7889 -1 .Function upbid works with square matrices only function [ A.0000 1 .This reduction is required in some algorithms for computing the singular value decomposition ( SVD ) of a matrix .", "label": "", "metadata": {}, "score": "63.98726"}
{"text": "Recursive condition estimation ; recursive condition .estimation ; Recursive least squares computations ; . recursive least squares computations ; signal .processing ; Sliding data windows ; sliding data . windows \" , .Numerical methods ; Signal processing \" , . several intervals \" , . recursion coefficients of orthogonal polynomials for a .", "label": "", "metadata": {}, "score": "64.01113"}
{"text": "47 , no . 9 , pp .1394 - 1397 , 2000 .View at Google Scholar \u00b7 View at Scopus .M. Collins , S. Dasgupta , and R. E. Schapire , \" A generalization of principal component analysis to the exponential family , \" in Advances in Neural Information Processing Systems , T. D. Dietterich , S. Becker , and Z. Ghahramani , Eds . , vol .", "label": "", "metadata": {}, "score": "64.01299"}
{"text": "data windows are considered .ale is fast for relatively .small n - parameter problems arising in RLS methods in . control and signal processing , and is adaptive over . time , i.e. , estimates at time t are used to produce . estimates at time t+1 .", "label": "", "metadata": {}, "score": "64.105606"}
{"text": "The weight function is assumed . to be the weighted sum of weight functions , each . supported on its own interval .Some of these intervals . may coincide , overlap or are contiguous .They discuss .three algorithms .", "label": "", "metadata": {}, "score": "64.108444"}
{"text": "Let m.3 .n ) % The k - th coordinate vector in the n - dimensional Euclidean space .On several occasions we will use function ek(k .The goal of this section is to discuss important matrix transformations that are used in numerical linear algebra .", "label": "", "metadata": {}, "score": "64.14729"}
{"text": "By switching the sign of .in given learning algorithms , both the NOja and the NOOja can be used for the estimation of minor and principal subspaces of a vector sequence .The above algorithms including Oja 's MSA [ 7 ] , the natural - gradient - based method [ 83 ] , self - stabilizing MCA [ 80 ] , OOja , NOja , and NOOja have a complexity of .", "label": "", "metadata": {}, "score": "64.15411"}
{"text": "Based on Singular Value Decomposition an incremental and iterative Matrix Factorization method for very sparse matrices is presented .Such matrices arise in Collaborative Filtering ( CF ) systems , like the Netflix system .This paper shows how such an incremental Matrix Factorization can be used to pre ... \" .", "label": "", "metadata": {}, "score": "64.1595"}
{"text": "minimize computational complexity .This method is recommended for solving linear systems with multiple right hand sides . ..4640 1.3837e-008 Number of decimal digits of accuracy in the computed solution x is defined as the negative decimal logarithm of the relative error ( see e. however it is not necessarily unique.4 .", "label": "", "metadata": {}, "score": "64.246544"}
{"text": "( Linear algebra ) \" , . 2 , Analog . and digital signal processing \" , . matrices ; Fast tracking ; Lanczos - based algorithms ; .Low - dimensional signal subspace ; Numerical properties ; .Real - time signal processing ; Signal scenarios \" , . detection ; Tracking \" , . algorithm \" , . assertions \" , . backward error analysis for error detection in . algorithms that solve dense systems of linear .", "label": "", "metadata": {}, "score": "64.30693"}
{"text": "Numerical Mathematics \" , . D. Ikramov \" , . volume 53 of this journal .Applications ' ' by G. H. Golub and G. Meurant , Princeton .Moments and Quadrature with Applications \" , .The Journal . of the Society for the Foundations of Computational .", "label": "", "metadata": {}, "score": "64.31644"}
{"text": "The objective function for extracting the first principal singular value of the covariance matrix is given by .A .P .C .A ._ ._ ._ ._ .It is an indefinite function .When . , it reduces to PCA [ 14 ] .", "label": "", "metadata": {}, "score": "64.32944"}
{"text": "generalized eigenvalue technique when the noise field . is near singular or has changed significantly over the .two covariance measurements .A robust numerical .approach is presented for determining the response . vector in these situations .Results of computer . simulations to verify the proposed technique are also . presented . \"", "label": "", "metadata": {}, "score": "64.33961"}
{"text": "The . complexity of verifying assertions is O(n / sup 2/ ) , .compared to the O(n / sup 3/ ) complexity of algorithms .methods , this assertion model does not require any . encoding of the matrix A. Experimental results under .", "label": "", "metadata": {}, "score": "64.34837"}
{"text": "extrapolation , the Gauss - Seidel method , or the .Biconjugate gradient stable method for an even greater .speed - up ; cumulative speed - up is as high as 7 to 14 . times .The second contribution relates to the handling . of dangling nodes .", "label": "", "metadata": {}, "score": "64.37584"}
{"text": "However , the convergence of the magnitudes of the weights can not be guaranteed either unless the initial weights take special values .The total least mean squares ( TLMS ) algorithm [ 74 ] is a random adaptive algorithm for extracting the MC , which has an equilibrium point under persistent excitation conditions .", "label": "", "metadata": {}, "score": "64.452324"}
{"text": "PASTd [ 30 ] is a well - known subspace tracking algorithm updating the signal eigenvectors and eigenvalues .PASTd is based on PAST .Both PAST and PASTd are derived for complex - valued signals .Both PAST and PASTd have linear computational complexity , that is , . operations every update , as in the cases of SLA [ 14 ] , GHA [ 8 ] , LMSER [ 29 ] , and novel information criterion ( NIC ) [ 32 ] .", "label": "", "metadata": {}, "score": "64.48337"}
{"text": "We discuss an inverse - free , highly parallel , spectral divide and conquer algorithm .This algorithm is based on earlier ones of Bulgakov , Godunov ... \" .We discuss an inverse - free , highly parallel , spectral divide and conquer algorithm .", "label": "", "metadata": {}, "score": "64.50638"}
{"text": "Signal processing ; Sliding data windows \" , . algebra ; Parameter estimation ; Signal processing ; .Birthday \" , .Theory and Numerical Linear Algebra , 30 March -- 1 .April , 1989 , Kent , Ohio . of orthogonal polynomials from the moments or from .", "label": "", "metadata": {}, "score": "64.56958"}
{"text": "A67 1999 \" , .G67 2007 \" , . influential papers in Matrix Computation authored by .Gene H. Golub , one of the founding fathers of the . field .The collection of 21 papers in divided into five . main areas : iterative methods for linear systems , . solution of least squares problems , matrix . factorizations and applications , orthogonal polynomials . and quadrature , and eigenvalue problems an commentaries .", "label": "", "metadata": {}, "score": "64.572876"}
{"text": "by 25 - -300\\% on a Web graph of 80 million nodes , with . minimal overhead .Our contribution is useful to the .PageRank community and the numerical linear algebra .community in general , as it is a fast method for .", "label": "", "metadata": {}, "score": "64.57829"}
{"text": "method is connected to the nonsymmetric Lanczos . procedure and also leads to canonical representations . of such triples .Sci . , Stanford Univ . , CA , USA \" , . methods ) \" , . nla , control , tridiagonal matrix , nonsymmetric matrix , .", "label": "", "metadata": {}, "score": "64.616104"}
{"text": "In the course of our work , we , like many other library developers , have been faced with many issues relating to portable programming .Previously , a notable obstacle to library development was the lack of standardization in message passing , from both a programming and a functional point of view .", "label": "", "metadata": {}, "score": "64.71589"}
{"text": "Singular values of A are stored . in the nonincreasing order . respectively .A common method used nowadays is the two - phase method .Function mysvd implements a method proposed in Problem 4 . on the main diagonal of the diagonal matrix S. The diagonal entries of S are the singular values of the matrix A. S. The left and the right singular vectors of A are stored in columns of matrices U and V. G(1 .", "label": "", "metadata": {}, "score": "64.77101"}
{"text": "The robust or nonlinear PCA algorithms are derived by using the gradient - descent method [ 57 ] .They can be treated as generalization of SLA [ 7 , 22 ] and the GHA [ 8 ] .Robust and nonlinear PCA algorithms have better stability properties than the corresponding PCA algorithms if the ( odd ) nonlinearity .", "label": "", "metadata": {}, "score": "64.80302"}
{"text": "298 - 301 , 1998 .View at Google Scholar \u00b7 View at Scopus .G. Mathew and V. U. Reddy , \" A quasi - newton adaptive algorithm for generalized symmetric eigenvalue problem , \" IEEE Transactions on Signal Processing , vol .", "label": "", "metadata": {}, "score": "64.890625"}
{"text": "Complex PCA is a generalization of PCA in complex - valued data sets [ 95 ] .Complex PCA has been widely applied to complex - valued data and two - dimensional vector fields .Complex PCA employs the same neural network architecture as that of PCA , but with complex weights .", "label": "", "metadata": {}, "score": "64.949684"}
{"text": ", but rigorous bounds are much more complicated and supply little extra information in the interesting case of small errors .These bounds are indicated by using the symbol .Thus , when these bounds are close to 1 or greater , they indicate that the computed answer may have no significant digits at all , but do not otherwise bound the error .", "label": "", "metadata": {}, "score": "65.00465"}
{"text": "As with the standard nonsymmetric eigenvalue problem , there are two preprocessing steps one may perform on a matrix pair .A .B . in order to make its eigenproblem easier ; permutation and scaling , which together are referred to as balancing , as indicated in the following two steps . to block upper triangular form by a similarity transformation : . P .", "label": "", "metadata": {}, "score": "65.05275"}
{"text": "An RLS version of the NIC algorithm is given in [ 32 ] .The PAST algorithm [ 30 ] is a special case of the NIC algorithm when . takes unity .The weighted information criterion ( WINC ) [ 34 ] is obtained by adding to the NIC a weight to break the symmetry in the NIC .", "label": "", "metadata": {}, "score": "65.067215"}
{"text": "pathways that are manifest in both experiments .We . define a comparative HOEVD that formulates a series of .networks as linear superpositions of decorrelated .rank-1 subnetworks and the rank-2 couplings among these .subnetworks , which can be associated with independent .", "label": "", "metadata": {}, "score": "65.22067"}
{"text": "View at Google Scholar \u00b7 View at Scopus .C. M. Bishop , Neural Networks For Pattern Recogonition , Oxford Press , New York , NY , USA , 1995 . D. E. Rumelhart , G. E. Hinton , and R. J. Williams , \" Learning internal representations by error propagation , \" in Parallel Distributed Processing : Explorations in the Microstructure of Cognition , D. E. Rumelhart and J. L. McClelland , Eds . , vol .", "label": "", "metadata": {}, "score": "65.25629"}
{"text": "333 - 338 , American Institute of Physics , Snowbird , Utah , USA , 1986 .Z. Kang , C. Chatterjee , and V. P. Roychowdhury , \" An adaptive quasi - newton algorithm for eigensubspace estimation , \" IEEE Transactions on Signal Processing , vol .", "label": "", "metadata": {}, "score": "65.32308"}
{"text": "Peyman Milanfar \" , .ICIP 2003 .Pseudoinverse Projection Predicts Novel Correlation .framework that formulates any number of genome - scale . molecular biological data sets in terms of one chosen . set of data samples , or of profiles extracted .", "label": "", "metadata": {}, "score": "65.39792"}
{"text": "View at Google Scholar \u00b7 View at Scopus . A. Weingessel and K. Hornik , \" A robust subspace algorithm for principal component analysis , \" International Journal of Neural Systems , vol .13 , no .5 , pp .", "label": "", "metadata": {}, "score": "65.44864"}
{"text": "Eigenvalue Problems \\\\ .Commentary , G. W. Stewart \\\\ .References \\\\ .Some modified matrix eigenvalue problems , G. Golub \\\\ .Ill - conditioned eigensystems and the computation of the .Jordan canonical form , G. Golub , J. Wilkinson \\\\ .", "label": "", "metadata": {}, "score": "65.454895"}
{"text": "Math . , Stevens Inst . of .Technol . , Hoboken , NJ , USA \" , .Chebyshev iterative method ; Chebyshev approximation ; .Chebyshev polynomials ; convex hull ; Convex hull ; . eigenvalue estimates ; Eigenvalue estimates ; . eigenvalues ; Eigenvalues ; eigenvalues and .", "label": "", "metadata": {}, "score": "65.46837"}
{"text": "b. Plot the graph of the computed errors using MATLAB 's function semilogy instead of the function plot .Let A be an n - by-3 random matrix generated by the MATLAB function rand .constant entries b along the subdiagonal .", "label": "", "metadata": {}, "score": "65.57057"}
{"text": "This procedure is . essentially motivated by the theory of moments and .Gauss quadrature . \"Sci . , Tennessee Univ . , Knoxville , TN , .USA \" , . B0290H( Linear algebra ) ; C4130 ( Interpolation and .", "label": "", "metadata": {}, "score": "65.61272"}
{"text": "r . and minimize .t .r .Assuming that . is a nonsingular matrix , conventionally , the following Fisher 's determinant ratio criterion is maximized for finding the projection directions [ 152 , 153 ] : .L .", "label": "", "metadata": {}, "score": "65.62337"}
{"text": "y].Color'.1 .vx.z(2 .[U.25 .vy ) .set(h1_line(2 ) .function SVDdemo(A ) % This illustrates a geometric effect of the application % of the 2-by-2 matrix A to the unit circle C.1.1 ) 0 U(2.1 . '", "label": "", "metadata": {}, "score": "65.713005"}
{"text": "When the data configuration is Euclidean , the MDS is similar to the PCA , in that both can remove inherent noise with its compact representation of data .The order three computational complexity makes it infeasible to apply to huge data , for example , when the sample size is more than one million .", "label": "", "metadata": {}, "score": "65.80614"}
{"text": "departments , J. G. Herriot and J. F. Traub , in the .March 1967 issue . \" system of equations \" , . determining a Chebyshev solution to an overdetermined . system of linear equations is presented , that uses .", "label": "", "metadata": {}, "score": "65.89005"}
{"text": "11 , pp .1820 - 1836 , 2009 .View at Google Scholar \u00b7 View at Scopus . D. Z. Feng , Z. Bao , and W. X. Shi , \" Cross - correlation neural network models for the smallest singular component of general matrix , \" Signal Processing , vol .", "label": "", "metadata": {}, "score": "65.899124"}
{"text": "M. A. O'Neil and M. Burtscher , \" Floating - point data compression at 75 Gb / s on a GPU , \" in Proceedings of the 4th Workshop on General Purpose Processing on Graphics Processing Units ( GPGPU ' 11 ) ,", "label": "", "metadata": {}, "score": "65.90489"}
{"text": "We can see that when the estimated rank decreases , the error arises rapidly .Lines in Figure 2 from the bottom to the top are the matrix size with . , respectively .The error increases slowly when the matrix size increases .", "label": "", "metadata": {}, "score": "65.916405"}
{"text": "It is derived from the optimization of the kurtosis or the negentropy measure by using Newton 's method .FastICA achieves a reliable and at least a quadratic convergence .FastICA with symmetric orthogonalization and tanh nonlinearity is concluded as the best trade - off for ICA [ 10 ] .", "label": "", "metadata": {}, "score": "65.93304"}
{"text": "Matrix \" , . database \" , . infinite stochastic matrix of special form \" , . equations \" , .Royal Irish Academy Conference on Numerical Analysis , . non - symmetric systems of linear equations \" , . equations \" , .", "label": "", "metadata": {}, "score": "65.99815"}
{"text": "Compression and Reconstruction of HSI Data by rSVD .The flight times of airplanes carrying hyperspectral scanning imagers are usually limited by the data capacity , since within 5 to 10 seconds hundreds of thousands of pixels of hyperspectral data are collected [ 1 ] .", "label": "", "metadata": {}, "score": "66.04678"}
{"text": "corresponding eigengene relative to all others .We show .that the eigengenes fit ' ' asymmetric Hermite .functions , ' ' a generalization of the eigenfunctions of .the quantum harmonic oscillator and the integral .transform which kernel is a generalized coherent state .", "label": "", "metadata": {}, "score": "66.059555"}
{"text": "Second Order Method \" , .Weighted Least Squares Regression Coefficients \" , .Matrix \" , .JSTOR database \" , .JSTOR database \" , .Transformations \" , . on the Unit Sphere \" , . database \" , . a Matrix \" , . database \" , .", "label": "", "metadata": {}, "score": "66.06618"}
{"text": "The anti - Hebbian rule can be used to remove correlations between units receiving correlated inputs [ 13 , 48 , 49 ] .The anti - Hebbian rule is inherently stable [ 13 , 49 ] .Anti - Hebbian rule - based PCA algorithms can be derived by using a .", "label": "", "metadata": {}, "score": "66.13451"}
{"text": "\"NC , USA \" , . and detection ) ; C1240 ( Adaptive system theory ) ; C1260 .( Information theory ) ; C4140 ( Linear algebra ) \" , .for Optical Engineering \" , .Modified matrix ; Recursive condition estimation ; .", "label": "", "metadata": {}, "score": "66.168304"}
{"text": "The authors discuss the .close relationship between the modified Lanczos . algorithm and the modified Chebyshev algorithm .They .further show the connection between this last problem .and checksum - based error correction schemes for .fault - tolerant computing . \"", "label": "", "metadata": {}, "score": "66.21782"}
{"text": "Its Applications \" , . and Recommendations \" , .computing sample variances \" , . procedure for solving systems of linear equations \" , .Conjugate - Gradient Algorithm \" , .this reference : The American Statistician \" , . sparse least squares \" , .", "label": "", "metadata": {}, "score": "66.32081"}
{"text": "Permuting has no effect on the condition numbers or their interpretation as described previously .Scaling , however , does change their interpretation and further details can be found in Anderson et al .( 1999 ) .2.14.7 The generalized nonsymmetric eigenvalue problem .", "label": "", "metadata": {}, "score": "66.32196"}
{"text": "Repeat Problem 2 with L being replaced by the upper triangular matrix U. 7 .The purpose of this problem is to test function sol of Problem 6 . and utri of Problem 3 .For comparison of a solution you found using function sol with an acceptable solution you may wish to use MATLAB 's backslash operator \\.3 .", "label": "", "metadata": {}, "score": "66.32722"}
{"text": "n+1 ) .Factors that should be considered include numerical stability of a method used and accuracy of the computed solution.1:n ) .to mention the most important ones .A choice of a numerical algorithm for solving a particular problem is often a complex task .", "label": "", "metadata": {}, "score": "66.346634"}
{"text": "The difference between PCA and MCA lies in the sign of the learning rate .The MCA algorithm proposed in [ 83 ] suffers from a marginal instability , and thus it requires intermittent normalization such that .[ 80 ] .", "label": "", "metadata": {}, "score": "66.4244"}
{"text": "A . common choice of shifts consists of the eigenvalues of .the trailing principal submatrix of order m , and . current practice includes the computation of these . eigenvalues in the determination of the shift vector .In this paper , the authors describe an algorithm based .", "label": "", "metadata": {}, "score": "66.43404"}
{"text": "194 - 200 , 2011 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .G. Kukharev and E. Kamenskaya , \" Application of two - dimensional canonical correlation analysis for face image processing and recognition , \" Pattern Recognition and Image Analysis , vol .", "label": "", "metadata": {}, "score": "66.54497"}
{"text": "Sci . , Maryland Univ . , College Park , .MD , USA \" , . elimination ; block Gaussian elimination ; Block .relaxation methods ; block relaxation methods ; . convection ; Convergence of line iterative methods ; . convergence of line iterative methods ; convergence of .", "label": "", "metadata": {}, "score": "66.679955"}
{"text": "n ) .A(j . k).42713228706575 -0.17214785894088 -0.42188474935756 0.02220446049250 0 -0 .z].73923873953922 0.1:k ) .end end end We will use this function to compute the eigenvalues and the eigenvectors of the matrix A of the last example [ la.11102230246252 0 . lu.2 ) ( 1 . inv .", "label": "", "metadata": {}, "score": "66.74724"}
{"text": "11 , pp .718 - 729 , 1994 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . D. Z. Feng , Z. Bao , and L. C. Jiao , \" Total least mean squares algorithm , \" IEEE Transactions on Signal Processing , vol .", "label": "", "metadata": {}, "score": "66.75935"}
{"text": "Some members in the class have better numerical performance and require less computational effort compared to that of both GHA and APEX .Most existing linear complexity methods including GHA [ 8 ] , SLA [ 7 ] , and PCA with the lateral connections [ 13 , 35 , 48 - 50 ] require a computational complexity of . per iteration .", "label": "", "metadata": {}, "score": "66.76784"}
{"text": "The latter is computed using function wsft .function [ la.22360679774998 0 . %Method used : the QR algorithm with Wilkinson 's shift .In practice .Function qrsft computes all eigenvalues of the symmetric matrix A.32 [ l2 .however . which theoretically is an infinite one .", "label": "", "metadata": {}, "score": "66.82265"}
{"text": "MATLAB has several built - in functions for computations with sparse matrices .A partial list of these functions is included here . \\. jordan .Computations with sparse matrices The following MATLAB functions work with sparse matrices : chol .qr .", "label": "", "metadata": {}, "score": "66.82765"}
{"text": "References to Chapter f08 functions in the manual normally include the LAPACK double precision names , for example nag_dgeqrf ( f08aec ) .The LAPACK routine names follow a simple scheme .Each name has the structure xyyzzz , where the components have the following meanings : . the initial letter x indicates the data type ( real or complex ) and precision : . -", "label": "", "metadata": {}, "score": "66.865"}
{"text": "Mathematical Software / / 241 \\\\ .Sparse Matrix Software / W. Morven Gentleman and .Alan George / 243 \\\\ .Considerations in the Design of Software for Sparse .Gaussian Elimination / S. C. Eisenstat , M. H. Schultz , . and A. H. Sherman / 263 \\\\ .", "label": "", "metadata": {}, "score": "66.8844"}
{"text": "The analysis applies to the reduced systems .derived when one step of block Gaussian elimination is . performed on red - black ordered two - cyclic . discretizations .We consider the case where centered .finite difference discretization is used and one cell .", "label": "", "metadata": {}, "score": "66.88734"}
{"text": "Nearest neighbor graphs are widely used in data mining and machine learning .The brute - force method to compute the exact kNN graph takes \u0398(dn 2 ) time for n data points in the d dimensional Euclidean space .We propose two divide and conquer methods for computing an approximate kNN graph in \u0398(dn t ) time for high dimensional data ( large d ) .", "label": "", "metadata": {}, "score": "66.89902"}
{"text": "Single - input single - output systems \" , .Introduction to Numerical Methods \" , .G62 1992 \" , .Methods in Numerical Linear Algebra \" , . iteration \" , .Los Angeles \" , . iteration methods \" , .", "label": "", "metadata": {}, "score": "66.91791"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .G. Martinsson , \" Randomized methods for computing the singular value decomposition of very large matrices , \" in Workshop on Algorithms for Modern Massive Data Sets , 2012 . A. D. Meigs , L. J. Otten , and T. Y. Cherezova , \" Ultraspectral imaging : a new contribution to global virtual presence , \" in Proceedings of the IEEE Aerospace Conference , vol .", "label": "", "metadata": {}, "score": "66.99475"}
{"text": "Our algorithm hinges on the fact that , under easily constructively verifiable conditions , a symmetric matrix with bandwidth b and k distinct eigenvalues must be block diagonal with diagonal blocks of size at most bk .A slight modification of the usual orthogonal band - reduction algorithm allows us to reveal this structure , which then leads to potential parallelism in the form of independent diagonal blocks .", "label": "", "metadata": {}, "score": "67.00012"}
{"text": "If the problem is the generalized symmetric definite eigenvalue problem .A .z .B .z .C . as defined in Section 2.8 is , in general , full .We can reduce the problem to a banded standard problem by modifying the definition of .", "label": "", "metadata": {}, "score": "67.005035"}
{"text": "32 , no . 1 - 2 , pp .29 - 43 , 2002 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Scopus . Y. Chen and C. Hou , \" High resolution adaptive bearing estimation using a complex - weighted neural network , \" in Proceedings of the IEEE International Conference on Acoustics , Speech , and Signal Processing ( ICASSP ' 92 ) , vol .", "label": "", "metadata": {}, "score": "67.100266"}
{"text": "Nonself - adjoint elliptic partial differential .equations ; Parallel architectures \" , .Plemmons \" , . estimation \" , . useful in many areas of scientific computing , . including : recursive least squares computations , . optimization , eigenanalysis , and general nonlinear .", "label": "", "metadata": {}, "score": "67.16345"}
{"text": "polynomials ; Tridiagonal Jacobi matrix ; Weight .functions \" , .Convection - Diffusion Problems \" , . methods for solving the discrete convection - diffusion .equation is performed .The methodology consists of .performing one step of the cyclic reduction method , . followed by iteration on the resulting reduced system . using line orderings of the reduced grid .", "label": "", "metadata": {}, "score": "67.18146"}
{"text": "the tridiagonal Jacobi matrix of coefficients of the . recurrence relation which they satisfy .Let J / sub 1/ .and J / sub 2/ be finite Jacobi matrices for the weight .functions w / sub 1/ and w / sub 2/ , resp .", "label": "", "metadata": {}, "score": "67.21968"}
{"text": "matrix modification techniques are used .The purpose of .this paper is to propose an adaptive Lanczos estimator . scheme , which the authors call ale , for tracking the .condition number of the modified matrix over time .Applications to recursive least squares ( RLS ) .", "label": "", "metadata": {}, "score": "67.23442"}
{"text": "View at Scopus .J. Rubner and K. Schulten , \" Development of feature detectors by self - organization , \" Biological Cybernetics , vol .62 , no . 3 , pp .193 - 199 , 1990 .View at Google Scholar \u00b7 View at Scopus .", "label": "", "metadata": {}, "score": "67.23685"}
{"text": "If you ca n't represent M to the desired precision , you call the svd . routine again , but this time passing the .a spectral shift closest to the smallest singular value from the . previous iteration .This returns 50 more singular . triplets which , when composed with the first 50 , constitute the first .", "label": "", "metadata": {}, "score": "67.24937"}
{"text": "data , where the mathematical variables and operations . may represent biological reality .The variables of SVD , . '' eigengenes ' ' and ' ' eigenarrays , ' ' and these of GSVD , . '' genelets ' ' and ' ' arraylets , ' ' appear to represent . independent processes and corresponding cellular states .", "label": "", "metadata": {}, "score": "67.25814"}
{"text": "III .Optimization , Least Squares and Linear Programming ./ / 145 \\\\ .Optimization for Sparse Systems / T. L. Magnanti / .147 \\\\ .Methods for Sparse Linear Least Squares Problems / .The Orthogonal Factorization of a Large Sparse .", "label": "", "metadata": {}, "score": "67.25868"}
{"text": "The drawback of the .eigenvalue decomposition ( EVD ) or the singular value . decompositions ( SVD ) is usually the volume of the . computations .Various numerical methods for carrying . out this task are surveyed , and it is shown why this . heavy computational burden is questionable in numerous . situations and should be revised .", "label": "", "metadata": {}, "score": "67.2875"}
{"text": "The interested reader is referred to [ 5 ] and [ 3].else [ Q. .The SVD is motivated by the following fact : the image of the unit sphere under the m - by - n matrix is a hyperellipse .", "label": "", "metadata": {}, "score": "67.36789"}
{"text": "459 - 473 , 1989 .View at Google Scholar \u00b7 View at Scopus . D. O. Hebb , The Organization of Behavior , John Wiley & Sons , New York , NY , USA , 1949 . K. L. Du and M. N. S. Swamy , Networks in a Softcomputing Framework , Springer , London , UK , 2006 .", "label": "", "metadata": {}, "score": "67.36862"}
{"text": "The algorithm described here is a promising variant of the Invariant Subspace Decomposition Algorithm for dense symmetric matrices ( SYISDA ) ... \" .We present an overview of the banded Invariant Subspace Decomposition Algorithm for symmetric matrices and describe a parallel implementation of this algorithm .", "label": "", "metadata": {}, "score": "67.387695"}
{"text": "The method under discussion is adequate when the condition number of A is small .This method however .-1].5011 . Gauss.5.5 .matrix of the system % b. requires more flops than the backslash method does . %Input : % A.the least - squares solution % dist .", "label": "", "metadata": {}, "score": "67.45909"}
{"text": "a power method and yields faster convergence for many . problems . \"Math . , Stevens Inst . of .Technol . , Hoboken , NJ , USA \" , . B0290H( Linear algebra ) ; C4130 ( Interpolation and .", "label": "", "metadata": {}, "score": "67.463615"}
{"text": "Accurate symmetric eigenreduction by a Jacobi method / .The order - recursive Chandrasekhar equations for fast . square - root Kalman filtering / D. T. M. Slock / 419 \\\\ .Hybrid iterative methods based on Faber polynomials / .G. Starke / 421 \\\\ .", "label": "", "metadata": {}, "score": "67.509766"}
{"text": "\" ...Computing the singular values of a bidiagonal matrix is the fin al phase of the standard algow rithm for the singular value decomposition of a general matrix .We present a new algorithm hich computes all the singular values of a bidiagonal matrix to high relative accuracy independent of their magni ... \" .", "label": "", "metadata": {}, "score": "67.5177"}
{"text": "Bartels and Gene H. Golub , Comm .ACM 11(6 ) 428 .JSTOR database \" , .Bases \" , .Decomposition for Linear - Programming Bases \" , .Murigande and Ph . L. Toint \" , .MathSciNet database \" , .", "label": "", "metadata": {}, "score": "67.52129"}
{"text": "r . m . a .x .After manipulation , we have .m . a .x . where the covariance matrix of . is defined by .Under a mild condition which tends to hold for high - dimensional data , CCA in the multilabel case can be formulated as an LS problem [ 136 ] .", "label": "", "metadata": {}, "score": "67.54314"}
{"text": "Stevin \" , .Bulletin of the Belgian Mathematical Society , Simon . H. Golub \\\\ .Convergence of rational interpolants / Herbert Stahl .\\\\ .Variations on Richardson 's method and acceleration / .Claude Brezinski \\\\ .Andr\\'e Louis Cholesky / Claude Brezinski \\\\ .", "label": "", "metadata": {}, "score": "67.635925"}
{"text": "The second purpose of this paper is to update the SVD when the matrix size is extended by new data updating .If the rank of matrix is much smaller than the matrix size , Matthew proposed a fast SVD updating method for the low - rank matrix in 2006 [ 9 ] .", "label": "", "metadata": {}, "score": "67.651764"}
{"text": "Precision Arithmetic / Anne Greenbaum / 49 \\\\ .The Lanczos Process and Pade Approximation / Martin H. .Gutknecht / 61 \\\\ .The Tau Method and the Numerical Solution of .Differential Equations : Past Research and Recent .Research / Eduardo L. Ortiz / 77 \\\\ .", "label": "", "metadata": {}, "score": "67.675705"}
{"text": "Unlike SLA [ 7 ] and GHA [ 8 ] , whose stability analysis is based on the stochastic approximation theory [ 6 ] , the stability analysis of LEAP is based on Lyapunov 's first theorem , and as such . can be selected as a small positive constant [ 26 ] .", "label": "", "metadata": {}, "score": "67.78468"}
{"text": "Series E , Applied sciences \" , . scale and real - time matrix computations ; these .computations arise in a variety of fields , such as . computer graphics , imaging , speech and image .processing , telecommunication , biomedical signal . processing , optimization and so on .", "label": "", "metadata": {}, "score": "67.784805"}
{"text": "Reprinted in .Subject to Linear Constraints \" , . database \" , .Solutions \" , .Subject to Linear Constraints \" , .Golub \" , . on Irregular Regions \" , . database \" , .Functions \" , . equations \" , .", "label": "", "metadata": {}, "score": "67.82132"}
{"text": "Other optimization - based PCA methods are described in Section 6 .PCA based on the anti - Hebbian rule is treated in Section 7 .Nonlinear PCA is addressed in Section 8 .Section 9 is dedicated to minor component analysis ( MCA ) .", "label": "", "metadata": {}, "score": "67.878044"}
{"text": "5 - 12 , March 1998 .View at Scopus .5806 of Proceedings of SPIE , pp .662 - 667 , 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .B. Parlett , The Symmetric Eigenvalue Problem , vol .", "label": "", "metadata": {}, "score": "67.87875"}
{"text": "The latter represents the cost of moving data , either between levels of a memory hierarchy , or between processors over a network .Communication often dominates arithmetic and represents a rapidly increasing proportion of the total cost , so we seek algorithms that minimize communication .", "label": "", "metadata": {}, "score": "67.89139"}
{"text": "the early 1950s .These methods came into wide use only .in the mid-1970s .Shortly thereafter , vector computers .and massive computer memories made it possible to use .these methods to solve problems which could not be . solved in any other way .", "label": "", "metadata": {}, "score": "67.98059"}
{"text": "AND PROBLEM COMPLEXITY , Numerical Algorithms and .Problems , Computations on matrices \\\\ I.1.1 Computing .Methodologies , ALGEBRAIC MANIPULATION , Expressions and .Their Representation , Simplification of expressions \\\\ .G.1.0 Mathematics of Computing , NUMERICAL ANALYSIS , .", "label": "", "metadata": {}, "score": "68.05739"}
{"text": "Rapidly - convergent ; Reordering ; Spectral radii \" , . nonselfadjoint elliptic problems \" , .Convection - Diffusion Problems \" , .iterative methods for solving the discrete . convection - diffusion equation .The methodology consists . of performing one step of the cyclic reduction method , . followed by iteration on the resulting cyclic reduction .", "label": "", "metadata": {}, "score": "68.10274"}
{"text": "A(:.7071 -4 .j ) % % % % Postmultiplication of A by the Givens rotation which is represented by the 2-by-2 planar rotation J.7071 -3.4142 0 2 .An important application of the Givens transformation is to compute the QR factorization of a matrix.j ) ) .", "label": "", "metadata": {}, "score": "68.15697"}
{"text": "HSI data can be collected over hundreds of wavelengths - creating truly massive data sets .The transmission , storing , and processing of these large data sets often present significant difficulties in practical situations [ 1 ] .Dimensionality reduction methods provide means to deal with the computational difficulties of the hyperspectral data .", "label": "", "metadata": {}, "score": "68.1732"}
{"text": "Effective missing value estimation methods are . needed since many algorithms for gene expression data . analysis require a complete matrix of gene array . values .In this paper , imputation methods based on the .least squares formulation are proposed to estimate .", "label": "", "metadata": {}, "score": "68.1942"}
{"text": "Both CPPCA and rSVD algorithms are applied to each simulated matrix , and results are compared in terms of their reconstruction quality and the computation time .Figure 12(a ) shows that the running time of rSVD increases linearly with .we compare their reconstruction qualities in terms of signal - to - noise ratio ( SNR ) in Figure 13 , and the computation time in Table 1 .", "label": "", "metadata": {}, "score": "68.208725"}
{"text": "sy .- symmetric .the last three letters zzz indicate the computation performed .For example , qrf is a .factorization of a real general matrix ; the corresponding function for a complex general matrix is nag_zgeqrf . 3.3 Matrix Storage Schemes .", "label": "", "metadata": {}, "score": "68.29578"}
{"text": "Paris , France , January 7 - -9 , 1987 \" , .Methods for Partial Differential Equations : Proceedings . of the First International Symposium on Domain .Decomposition Methods for Partial Differential .Paris , France , January 7 - -9 , 1987 \" , . I571 1987 \" , .", "label": "", "metadata": {}, "score": "68.34562"}
{"text": "In [ 137 ] , a strategy for reducing LDA to CCA is proposed .Within - class coupling CCA ( WCCCA ) is to apply CCA to pairs of data samples that are most likely to belong to the same class .", "label": "", "metadata": {}, "score": "68.4672"}
{"text": "Jun - Feng Yin \" , .Methods for Positive - Definite Linear Systems \" , . normal ) and skew - Hermitian splitting for a .non - Hermitian and positive - definite matrix , we . introduce a new splitting , called positive - definite and . skew - Hermitian splitting ( PSS ) , and then establish a .", "label": "", "metadata": {}, "score": "68.51379"}
{"text": "3 : General Linear Systems / 87 \\\\ . 3.1 Triangular Systems / 88 \\\\ .3.2 The $ L U$ Factorization / 94 \\\\ . 3.3 Roundoff Analysis of Gaussian Elimination / 104 \\\\ . 3.4 Pivoting / 109 \\\\ .", "label": "", "metadata": {}, "score": "68.515854"}
{"text": "The work presented in this paper is part of the PRISM ( Parallel Research on Invariant Subspace Methods )Project , which involves researchers from Argonne National Laboratory , the Supercomputing Research Center , the University of California at Berkeley , and the University of Kentucky .", "label": "", "metadata": {}, "score": "68.6196"}
{"text": "developed a fast multidimensional scaling method which turned the classical order three MDS method to be linear [ 13 ] .In this paper , we would like to implement SCMDS to the fast SVD approach , say SCSVD .The following subsections are reviews of the classical MDS and the SCSVD .", "label": "", "metadata": {}, "score": "68.620316"}
{"text": "12.3 Total Least Squares / 595 \\\\ . 12.4 Computing Subspaces with the SVD / 601 \\\\ .12.5 Updating Matrix Factorizations / 606 \\\\ .12.6 Modified / Structured Eigenproblems / 621 \\\\ .Bibliography / 637 \\\\ .Numerical analysis ( Louvain - la - Neuve , 1995 ) \" , .", "label": "", "metadata": {}, "score": "68.6377"}
{"text": "The . typical approach is to solve the quadratic eigenvalue . problem using a mathematically equivalent linearized .formulation , resulting in a doubled dimension and , in .many cases , a lack of backward stability .\\par .This paper introduces an approach to solving the . quadratic eigenvalue problem directly without .", "label": "", "metadata": {}, "score": "68.64337"}
{"text": "\\\\ G.1.6 Mathematics of Computing , NUMERICAL ANALYSIS , .Optimization , Least squares methods G Mathematics of .LeVeque \" , . and recommendations \" , . bib ; Distributed / QLD/1983 .and fixed knots \" , . C4160 ( Numerical integration and differentiation ) \" , .", "label": "", "metadata": {}, "score": "68.665726"}
{"text": "JSTOR database \" , .Dept . , Univ . of Minnesota , Minneapolis , .MN , USA \" , .numerically stable ; periodic Jacobi matrices ; spectral .data \" , . numerical solution of elliptic partial differential . equations \" , . forms \" , .", "label": "", "metadata": {}, "score": "68.67435"}
{"text": "C .A . where .Based on an analysis using the stochastic approximation theory [ 10 , 11 ] , when .The weighted SLA can perform PCA ; however , norms of the weight vectors are not equal to unity .", "label": "", "metadata": {}, "score": "68.687805"}
{"text": "183 - 194 , 1989 .View at Google Scholar \u00b7 View at Scopus .R. Linsker , \" From basic network principles to neural architecture : emergence of orientation - selective cells , \" Proceedings of the National Academy of Sciences of the United States of America , vol .", "label": "", "metadata": {}, "score": "68.72398"}
{"text": "Brigitte Verdonk \" , . in shape reconstruction \" , . for saddle point systems \" , .Eigenvalue Problem \" , .arise frequently in areas such as the vibration . analysis of structures , micro - electro - mechanical . systems ( MEMS ) simulation , and the solution of .", "label": "", "metadata": {}, "score": "68.821075"}
{"text": "On direct methods for solving Poisson 's equation , B. L. .Buzbee , G. Golub , C. W. Nielson \\\\ .Numerical methods for computing angles between linear .Methods for modifying matrix factorizations , P. Gill , .G. Golub , W. Murray , M. Saunders \\\\ .", "label": "", "metadata": {}, "score": "68.83364"}
{"text": "as do the eigenvalues of the integral transform which .kernel is a generalized coherent state .The . ''asymmetric generalized coherent state ' ' models the . measured data , where the profiles of mRNA abundance .levels of most genes as well as the distribution of the .", "label": "", "metadata": {}, "score": "68.938576"}
{"text": "Circles and ellipses may be .represented algebraically , i.e. , by an equation of the .If a point is on the curve , then its . coordinates x are a zero of the function F. .Alternatively , curves may be represented in parametric .", "label": "", "metadata": {}, "score": "68.97665"}
{"text": "Q ) .v].k : m ) ) .k+1:m ) .Function Hessred implements this method function [ A.i ) .Matrix A is overwritten with its upper Hessenberg form.1 ) .It is well known that any square matrix A can always be transformed to an upper Hessenberg matrix H by orthogonal similarity ( see [ 7 ] for more details ) .", "label": "", "metadata": {}, "score": "69.137726"}
{"text": "View at Google Scholar \u00b7 View at Scopus .B. Widrow and M. E. Hoff , \" Adaptive switching circuits , \" in Proceedings of the IRE Eastern Electronic Show & Convention Convention Record ( WESCON ' 60 ) , vol .", "label": "", "metadata": {}, "score": "69.13971"}
{"text": "Such matrices arise in Collaborative Filtering ( CF ) systems , like the Netflix system .This paper shows how such an incremental Matrix Factorization can be used to predict ratings in a CF system and therefore how to fill the empty fields of a rating matrix of a CF system .", "label": "", "metadata": {}, "score": "69.14844"}
{"text": "44 ( in magnitude ) eigenvalue of the nonsingular matrix A and the associated eigenvector x. Solve the matrix eigenvalue problem for the matrix B and compare the eigenvalues and eigenvectors of matrices A and B. 2 .The input parameter v is an estimate of the eigenvector of A that is associated with the largest ( in magnitude ) eigenvalue of A. 2 ) .", "label": "", "metadata": {}, "score": "69.163315"}
{"text": "We remind the reader that a sparse matrix is a matrix with more 0 values than non 0 values .We are using the term \" sparse \" , howev ... . \" ...This paper takes an in - depth look at a technique for computing filtered matrix - vector ( mat - vec ) products which are required in many data analysis applications .", "label": "", "metadata": {}, "score": "69.214455"}
{"text": "Function abs chol cond det diag diff eps eye fliplr flipud flops full funm hess hilb imag inv length lu max .min norm ones pascal pinv qr rand randn rank real repmat schur sign size sqrt sum svd tic toc trace tril triu zeros .", "label": "", "metadata": {}, "score": "69.2215"}
{"text": "53 , No . 188 ( Oct. , 1989 ) , p. 775 .Algorithm Design \" , .Algorithm Design \" , .H4831 1988 \" , .York ) \" , .N344 1989 \" , .University of California MELVYL catalog .", "label": "", "metadata": {}, "score": "69.33199"}
{"text": "data using polynomials orthogonal on all the data . points being used .Rather than fixing on one basis . throughout , the methods adaptively update and downdate .both the least squares fit and the polynomial basis .This is achieved by performing similarity . transformations on the tridiagonal Jacobi matrices . representing the basis . \"", "label": "", "metadata": {}, "score": "69.34594"}
{"text": "Matrix Pairs \" , .University \" , .Computing \" , .G64 1993 \" , . indexes . \" programming ; Vector processing ( Computer science ) \" , .Vol .60 , No . 202 ( Apr. , 1993 ) , p. 849 . \"", "label": "", "metadata": {}, "score": "69.41344"}
{"text": "Combined with first - order approximation of GSO , precise estimates of singular vectors and singular values with only small deviations from orthonormality are produced .Double deflation is clearly superior to standard deflation but inferior to first - order approximation of GSO , both with respect to orthonormality and diagonalization errors .", "label": "", "metadata": {}, "score": "69.45258"}
{"text": "217 - 288 , 2011 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .Q. Zhang , R. Plemmons , D. Kittle , D. Brady , and S. Prasad , \" Joint segmentation and reconstruction of hyperspectral data with compressed measurements , \" Applied Optics , vol .", "label": "", "metadata": {}, "score": "69.54881"}
{"text": "-norm based PCA .Bidirectional PCA [ 121 ] reduces the dimension in both column and row directions for image feature extraction .The feature dimension obtained is much less than that of two - dimensional PCA .Two - dimensional PCA can be regarded as a special bidirectional PCA .", "label": "", "metadata": {}, "score": "69.569855"}
{"text": "Commentary , Walter Gautschi \\\\ .References \\\\ .Calculation of Gauss quadrature rules , G. Golub , J. .Welsch \\\\ .Matrices , moments and quadrature , G. Golub , G. Meurant .\\\\ .Computation of Gauss -- Kronrod quadrature rules , D. .", "label": "", "metadata": {}, "score": "69.626564"}
{"text": "Eigenvalues and eigenfunctions ; Matrix algebra ; . and Franklin T. Luk \" , .Method \" , .Ahmed Sameh \" , . problems \" , . of slightly perturbed symmetric eigenvalue problems .The matrix B is . assumed to have full column rank .", "label": "", "metadata": {}, "score": "69.64667"}
{"text": "C .X .T .A .X . , where .X .U .Q . or \u200b .L .T .Q . can be regarded as an eigenvalue .In exact arithmetic a singular pencil will have . is small then the pencil is singular , or nearly singular , and no reliance can be placed on any of the computed eigenvalues .", "label": "", "metadata": {}, "score": "69.7667"}
{"text": "The BRD is the first step toward computing the singular value decomposition of a matrix which is one of the most important algorithms in numerical linear algebra due to its broad impact in computational science .Our primary focus is the BRD portion of the computation which can easily consume over 99 % of the time needed to obtain the singular values a .. \" ...", "label": "", "metadata": {}, "score": "69.94389"}
{"text": "( Jul. , 1986 ) , pp .376 - 377 , and review by T. J. Randall , .The Mathematical Gazette , Vol .69 , No . 448 ( Jun. , .1985 ) , p. 152 .linear systems ) \" , .", "label": "", "metadata": {}, "score": "70.02396"}
{"text": "In particular , performance results for different implementations of the broadcast operation are analyzed and compared on the Delta , Paragon , SP1 and CM5 . 1 Introduction For the past several years , members of the Parallel Research on Invariant Subspace Methods ( PRISM ) project have been investigating scalable parallel eigensolvers for distributed memory systems [ 1 , 3].", "label": "", "metadata": {}, "score": "70.101135"}
{"text": "A .d .e . t .d .e . t .d .e . t .d .e . t .where the column vectors . , of .are the first .principal eigenvectors of .", "label": "", "metadata": {}, "score": "70.134315"}
{"text": "non - symmetric systems of linear equations , P. Concus .and G. Golub \\\\ .A generalized conjugate gradient method for the .numerical solution of elliptic partial differential . equations , P. Concus , G. Golub , D. O'Leary \\\\ .", "label": "", "metadata": {}, "score": "70.137634"}
{"text": "system . \"Sci . , Maryland Univ . , College Park , .MD , USA \" , . equations ) \" , .Reynolds numbers ; Convection - diffusion equation ; .Cyclically reduced non - self - adjoint linear systems ; .", "label": "", "metadata": {}, "score": "70.142075"}
{"text": "MN , USA \" , .( Linear algebra ) \" , . computing ; Indefinite inner product ; Indefinite .weights ; Matrix Lanczos algorithm ; Modified Chebyshev .algorithm ; Modified Lanczos algorithm ; Mutual .bi - orthogonality property ; Nonsymmetric Lanczos . algorithm ; Orthogonal polynomials ; Tridiagonal matrix \" , .", "label": "", "metadata": {}, "score": "70.19729"}
{"text": "A . m . , there are an infinite number of solutions .x . which exactly satisfy .b .A . x .In this case it is often useful to find the unique solution .x . which minimizes .", "label": "", "metadata": {}, "score": "70.22453"}
{"text": "n . ; that is , the storage is almost halved .This storage format is referred to as packed storage ; it is described in Section 3.3.2 in the f07 Chapter Introduction .Functions designed for packed storage are usually less efficient , especially on high - performance computers , so there is a trade - off between storage and efficiency .", "label": "", "metadata": {}, "score": "70.235115"}
{"text": "Classification of the data samples according . to their reconstruction in the basis , rather than their .overall measured profiles , maps the cellular states of .the data onto those of the basis and gives a global .picture of the correlations and possibly also causal .", "label": "", "metadata": {}, "score": "70.32218"}
{"text": "In our experiment , we set the true rank of the simulated matrix to be .Figure 2 : The relationship between errors and estimated dimension of matrix size from 500 to 4000 with step size 500 .The true rank of these matrices is 50 .", "label": "", "metadata": {}, "score": "70.33274"}
{"text": "Q . , or to apply it to another matrix , as with the functions for orthogonal factorizations .Explicit generation of .Q . is required before using the .Q .R . algorithm on .H . to compute the Schur vectors ; application of .", "label": "", "metadata": {}, "score": "70.504196"}
{"text": "Let A be a tridiagonal matrix that is either diagonally dominant or positive definite .respectively .Does the error increase as n does ?Use format long to display the output to the screen .c. 23 .Run the modified function on the data of Problem 18 .", "label": "", "metadata": {}, "score": "70.62023"}
{"text": "problem by finite differences ) are presented .Finite difference methods ( BVP of PDE ) 65F35 Matrix . norms , etc .( numerical linear algebra ) 35J25 Second .order elliptic equations , boundary value problems \" , .Peyman Milanfar \" , .", "label": "", "metadata": {}, "score": "70.63534"}
{"text": "Nearest defective matrices and the geometry of .ill - conditioning / James Demmel / 35 \\\\ .Computational aspects of the Jordan canonical form / .Theo Beelen and Paul Van Dooren / 57 \\\\ .Some aspects of generalized QR factorizations / C. .", "label": "", "metadata": {}, "score": "70.64656"}
{"text": "methods ) ; C4140 ( Linear algebra ) ; C4160 ( Numerical . integration and differentiation ) \" , .Matrix inverse ; Quadratic formulas ; Quadrature .formulas ; Real symmetric positive definite matrix ; .Rounding errors \" , .", "label": "", "metadata": {}, "score": "70.74289"}
{"text": "AND PROBLEM COMPLEXITY , Numerical Algorithms and .Problems , Computations on matrices \\\\ G.2.1 Mathematics .Subject to Indefinite Low Rank Perturbations with .Applications \" , . equations as an efficient solution technique for . convection - diffusion equations \" , .", "label": "", "metadata": {}, "score": "70.84589"}
{"text": "Minor Component Analysis .MCA , as a variant of PCA , is to find the smallest eigenvalues and their corresponding eigenvectors of the autocorrelation matrix . of the signals .MCA is closely associated with the curve and surface fitting under the total least squares ( TLSs ) criterion [ 72 ] .", "label": "", "metadata": {}, "score": "70.874214"}
{"text": "In contrast , in PCA the dimensionality reduction is achieved by removing those dimensions that have a low variance .Let a .-vector . denote a linear mixture and a .-vector . , whose components have zero mean and are statistically mutually independent , denote the original source signals .", "label": "", "metadata": {}, "score": "70.91376"}
{"text": "Principal component analysis ( PCA ) is a well - known orthogonal transform that is used for dimensionality reduction .Another popular technique for feature extraction is linear discriminant analysis ( LDA ) , also known as Fisher 's discriminant analysis [ 2 , 3 ] .", "label": "", "metadata": {}, "score": "71.07805"}
{"text": "Assembly / Gary Hachtel / 349 \\\\ .A Capacitance Matrix Technique / B. L. Buzbee / 365 .\\\\ .M - Matrix Theory and Recent Results in Numerical .Linear Algebra / Richard S. Varga / 375 \\\\ .VI .", "label": "", "metadata": {}, "score": "71.17359"}
{"text": "August 1992 , gives an account of recent research . advances in numerical techniques used in large scale .and real - time computations and their implementation on . high performance computers .Invited Lectures / / 1 \\\\ .Large scale structural analysis on massively parallel .", "label": "", "metadata": {}, "score": "71.27165"}
{"text": "Sci . , Stanford Univ . , CA , USA \" , . methods ; degenerate situations ; Degenerate situations ; . ill - conditioned problem ; Ill - conditioned problem ; . indefinite weight functions ; Indefinite weight .functions ; iterative methods ; linear algebra ; linear . systems ; Linear systems ; orthogonal polynomials ; .", "label": "", "metadata": {}, "score": "71.30013"}
{"text": "Let us perturb the w20.00008448192546 0.39041284468158 1.20).24182386727359 0.32106082150033 3 .Its is an upper bidiagonal 20-by-20 matrix with diagonal entries 20.43013503466255i 7.07256664475500 5.39041284468158 -0.03697639135041 + + + + 2.37019976472684i 2.07256664475500 4.00978219090288 -0 .Functions powerit and Rqi implement the first and the third method.62654906220876i 7 .", "label": "", "metadata": {}, "score": "71.322075"}
{"text": "Applied Mathematics \" , . form ; large sparse matrix determinant ; large - scale .matrix computation problems ; matrix multiplication ; . matrix - vector multiplications ; Monte Carlo method ; .Monte Carlo methods ; numerical algorithm ; smooth .", "label": "", "metadata": {}, "score": "71.37753"}
{"text": "linear dynamical system .The algorithm described is .particularly appropriate for large sparse systems . \"Sci .Dept . , Minnesota Univ . , Minneapolis , MN , .USA \" , . and synthesis methods ) ; C1320 ( Stability ) \" , . dynamical system ; Nonsymmetric Lanczos algorithm ; .", "label": "", "metadata": {}, "score": "71.460495"}
{"text": "The picture that . emerges suggests that the conserved genes YKU70 , MRE11 , .AIF1 , and ZWF1 , and the processes of . retrotransposition , apoptosis , and the oxidative . pentose phosphate pathway that these genes are involved .", "label": "", "metadata": {}, "score": "71.51935"}
{"text": "Problems \" , .JSTOR database \" , . analysis ) ; C4170 ( Differential equations ) \" , . transforms ; Fourier Toeplitz method ; Fourier transform ; . numerical methods ; separable elliptic problems ; .Toeplitz factorizations \" , .", "label": "", "metadata": {}, "score": "71.676346"}
{"text": "Reorder generalized Schur factorization , find basis for deflating subspace and estimate sensitivites .3.2 NAG Names and LAPACK Names .As well as the NAG function short name ( beginning f08 ) , the tables in Section 3.1 show the LAPACK routine names in double precision .", "label": "", "metadata": {}, "score": "71.72159"}
{"text": "plane is a problem that arises in many application .areas , e.g. computer graphics , coordinate meteorology , .petroleum engineering , and statistics .In the past , . algorithms have been given which fit circles and . ellipses in some least - squares sense without minimizing .", "label": "", "metadata": {}, "score": "71.88784"}
{"text": "n )The resulting matrix is an m - by - n sparse matrix.1 ) ( 1 .All entries on the subdiagonal .and m and n are the row and column dimensions .b. [ 2 1 3 4].", "label": "", "metadata": {}, "score": "71.91202"}
{"text": "the other is greater than one .It is shown that line . ordered relaxation exhibits very fast rates of . convergence . \"Sci . , Maryland Univ . , College Park , .MD , USA \" , .( General theory , simulation and other computational .", "label": "", "metadata": {}, "score": "71.95035"}
{"text": "The eigengenes are the .eigenvectors of the arrays $ \\times $ arrays .correlation matrix , with the corresponding series of .eigenvalues proportional to the series of the . ''fractions of eigen abundance . '' Each fraction of .", "label": "", "metadata": {}, "score": "71.971275"}
{"text": "[ 4 ] M. .Pacific Grove .Numerical Linear Algebra .MD .Datta .Elementary Numerical Computing with Mathematica .McGraw - Hill .SIAM .PA .New York .NY .Van Loan .[ 6 ] R. Heath .", "label": "", "metadata": {}, "score": "71.973434"}
{"text": "The Symmetric $ Q R$ Algorithm / 414 \\\\ . 8.4 Jacobi Methods / 426 \\\\ . 8.5Tridiagonal Methods / 439 \\\\ .8.6 Computing the SVD / 448 \\\\ . 8.7 Some Generalized Eigenvalue Problems / 461 \\\\ .9 : Lanczos Methods / 470 \\\\ . 9.1 Derivation and Convergence Properties / 471 \\\\ .", "label": "", "metadata": {}, "score": "71.9895"}
{"text": "Gautschi , and G. W. ( Pete ) Stewart .Comments on each . paper are also provided by the original authors , . providing the reader with historical information on how .the paper came to be written and under what . circumstances the collaboration was undertaken .", "label": "", "metadata": {}, "score": "71.99277"}
{"text": "11 : Functions of Matrices / 555 \\\\ .11.1 Eigenvalue Methods / 556 \\\\ .11.2 Approximation Methods / 562 \\\\ .11.3 The Matrix Exponential / 572 \\\\ .12 : Special Topics / 579 \\\\ .12.1 Constrained Least Squares / 580 \\\\ .", "label": "", "metadata": {}, "score": "72.04172"}
{"text": "The Lena picture and its restored version are shown in Figure 4 .The trained network is then used to compress the kid picture .Both the kid picture and its restored version are shown in Figure 5 .Summary .In this paper , we have discussed various neural network implementations and algorithms for PCA and its various extensions , including PCA , MCA , generalized EVD , constrained PCA , two - dimensional methods , localized methods , complex - domain methods , and SVD .", "label": "", "metadata": {}, "score": "72.088554"}
{"text": "given .Sci . , Stanford Univ . , CA , USA \" , . C4140 ( Linear algebra ) \" , .Sci . , Stanford Univ . , CA , USA \" , . minimisation ; Minimization ; minimization ; nla ; nllsq ; . nlop ; Quadratically constrained least squares problems ; . quadratically constrained least squares problems ; .", "label": "", "metadata": {}, "score": "72.11003"}
{"text": "boundary conditions \" , .Solving Elliptic Difference Equations \" , . matrix from spectral data \" , .a good ridge parameter \" , .no .491 , University of Wisconsin , Madison , WI . computations \" , . computations \" , .", "label": "", "metadata": {}, "score": "72.13877"}
{"text": "Comput .Studies , .Maryland Univ . , College Park , MD , USA \" , . approximation and analysis ) ; A4725Q ( Convection and . heat transfer ) ; C4130 ( Interpolation and function .approximation ) ; C4170 ( Differential equations ) ; C7320 .", "label": "", "metadata": {}, "score": "72.21057"}
{"text": "Lanczos 's Early Contributions to Relativity and His .Relationship with Einstein / John Stachel / 201 \\\\ .Topological Roots of Black Hole Entropy / Claudio .Teitelboim / 223 \\\\ .Variational Principles , Local Symmetries , and Black .", "label": "", "metadata": {}, "score": "72.214424"}
{"text": "Least - squares fitting of circles and ellipses / Walter .Gander , Gene H. Golub , Rolf Strebel \\\\ .Entropy of Hermite polynomials with application to the .Howard ) Golub \" , . I835 1997 \" , .z3950.loc.gov:7090/Voyager \" , .", "label": "", "metadata": {}, "score": "72.32031"}
{"text": "Golub and C. Van Loan \\\\ .Matrix Factorizations and Applications \\\\ .Commentary , Nicholas Higham \\\\ .References \\\\ .Calculating the singular values and pseudo - inverse of a . matrix , G. Golub and W. Kahan \\\\ .", "label": "", "metadata": {}, "score": "72.374695"}
{"text": "Numerical Mathematics \" , . positive definite system ; symmetric and triangular ( ST ) .decomposition ; symmetric - positive - definite and .Problems \" , .Chang and G. H. Golub and C. C. Paige \" , .Least Squares Problems \" , . subsequent developments \" , . definite tridiagonal generalized eigenvalue problem \" , .", "label": "", "metadata": {}, "score": "72.39722"}
{"text": "Phase two computes the SVD of A using a variant of the QR factorization.15 in [ 4]. respectively and S is an m - by - n diagonal matrix with nonnegative diagonal entries stored in the nonincreasing order .This code works for the 2-by-2 real matrices only.respectively .", "label": "", "metadata": {}, "score": "72.67256"}
{"text": "Petroleum engineering ; Singular value decomposition ; .Statistics \" , . methods ; Least squares approximations ; Singular value . matrix pairs \" , .Kailath \" , . communication applications , it is often required to .track a low - dimensional signal subspace that slowly .", "label": "", "metadata": {}, "score": "72.71103"}
{"text": "Stewart / 423 \\\\ .Intermediate fill - in in sparse QR decomposition / M. .Shifting strategies for the parallel QR algorithm / D. .S. Watkins / 427 \\\\ .List of Participants / / 429 \\\\ . and Robert J. Plemmons \" , .", "label": "", "metadata": {}, "score": "72.73602"}
{"text": "Due to the large amount ... . by Hector D. Flores , Stephan Eidenbenz , Rudolf H. Riedi , Nicholas Hengartner - in Proc . of PE - WASUN'06 ( to appear , 2006 . \" ...Data collected in realistic mobility traces for mobile ad hoc networks ( MANETS ) is intrinsi - cally high dimensional .", "label": "", "metadata": {}, "score": "72.87958"}
{"text": "network \" , .determinant of symmetric positive matrices .The . B445 1997 \" , . by Gene Golub . \"Gu and Aki H. Sayed \" , . modeling errors \" , .Aki H. Sayed \" , . errors - in - variables model \" , .", "label": "", "metadata": {}, "score": "73.02377"}
{"text": "Equation ( 19 ) is the Hebbian part , and ( 20 ) the anti - Hebbian part . tends to be orthogonal to all the previous components due to the anti - Hebbian rule , also called orthogonalization rule .APEX can also be derived from the RLS method using the MSE criterion .", "label": "", "metadata": {}, "score": "73.19482"}
{"text": "Lanczos - Type Algorithms / Youcef Saad / 123 \\\\ .Lanczos and Linear Systems / G. W. Stewart / 135 \\\\ .\\\\ .Plenary Presentations : Theoretical Physics and .Astrophysics \\\\ .\\\\ .Integration on the Space of Connections Modulo Gauge .", "label": "", "metadata": {}, "score": "73.46261"}
{"text": "In this section we present two functions for computations with this transformation .Vector m used here is called the Gauss vector and I is the n - by - n identity matrix .For more information about this transformation the reader is referred to [ 3].", "label": "", "metadata": {}, "score": "73.47809"}
{"text": "[ n.18189894035459 0 .b. In the full version of the QR factorization the matrix Q is an m - by - m orthogonal matrix and R is an m - by - n matrix with an n - by - n upper triangular matrix stored in rows 1 through n and having zeros everywhere else .", "label": "", "metadata": {}, "score": "73.52582"}
{"text": "We will show that our method works well for this type of matrices .Figure 3 shows the error versus estimated rank , where the error is defined as ( 25 ) , which is a comparison of the orthogonality between .", "label": "", "metadata": {}, "score": "73.56997"}
{"text": "J. D. Horel , \" Complex principal component analysis : theory and examples , \" Journal of Climate and Applied Meteorology , vol .23 , no .12 , pp .1660 - 1673 , 1984 .View at Google Scholar \u00b7 View at Scopus .", "label": "", "metadata": {}, "score": "73.73021"}
{"text": "For practical purposes , an error rate in the order of 0.001 might be sufficient , and this would result in a compression ratio of 2.5 to 4 .For comparison purpose , the 3D - SPECK [ 7 ] on a small dataset of size .", "label": "", "metadata": {}, "score": "73.754715"}
{"text": "Christopher D. Manning and Gene H. Golub \" , .World Wide Web 2003 , Budapest , Hungary , May 20 - -24 , .Computations \" , . of PageRank , a hyperlink - based estimate of the . '' importance ' ' of Web pages .", "label": "", "metadata": {}, "score": "73.75626"}
{"text": "Chebyshev approximation ; Chebyshev semi - iterative .method ; deflation techniques ; dominant singular .triplets ; eigenvalues and eigenfunctions ; eigenvectors ; . equivalent eigensystems ; equivalent sparse .eigensystems ; large sparse matrices ; modified moments ; . parallel algorithms ; parallel implementations ; singular .", "label": "", "metadata": {}, "score": "74.05755"}
{"text": "Lanczos Algorithms for Large Scale Symmetric and .Nonsymmetric Matrix Eigenvalue Problems / Jane K. .Cullum / 11 \\\\ .The Look - Ahead Lanczos Process for Nonsymmetric .Matrices and its Applications / Roland W Freund / 33 .\\\\ .", "label": "", "metadata": {}, "score": "74.23443"}
{"text": "Figure 3 : Architecture of the cross - correlation APCA network .The APCA network is composed of two hierarchical PCA networks .The connections with solid arrows denote feedforward connections , and the connections with hollow arrows denote lateral connections .", "label": "", "metadata": {}, "score": "74.33044"}
{"text": "Commentary , Anne Greenbaum \\\\ .References \\\\ .Chebyshev semi - iterative methods , successive . over - relaxation iterative methods , and second - order .Richardson iterative methods , Parts I. and II , G. Golub . and R. S. Varga \\\\ .", "label": "", "metadata": {}, "score": "74.457596"}
{"text": "A band matrix is one whose elements are confined to a relatively small number of subdiagonals or superdiagonals on either side of the main diagonal .Algorithms can take advantage of bandedness to reduce the amount of work and storage required .", "label": "", "metadata": {}, "score": "74.47112"}
{"text": "This paper gives some of the . history of the conjugate gradient and Lanczos . algorithms and an annotated bibliography for the period .Conjugate Gradient Algorithm ; Lanczos Algorithm ; .Mathematical Techniques ; Optimization -- Theory ; Variable .Metric Algorithms \" , .", "label": "", "metadata": {}, "score": "74.5056"}
{"text": "Matrices % A. if A(1 . is ill - conditioned and rank deficient .Function lsqsvd should be used for illconditioned or rank deficient matrices.3333 .Singular or rectangular matrices always possess the pseudoinverse matrix .j n. Number is called the eigenvalue of the matrix A and x is the associated right eigenvector of A. The eigenvalues of the matrix A are also displayed .", "label": "", "metadata": {}, "score": "74.57413"}
{"text": "3.3.6 Representation of orthogonal or unitary matrices .Q . ) is often represented in the NAG C Library as a product of elementary reflectors - also referred to as elementary Householder matrices ( usually denoted Edward Neuman Department of Mathematics Southern Illinois University at Carbondale edneuman@siu.edu This tutorial is devoted to discussion of the computational methods used in numerical linear algebra .", "label": "", "metadata": {}, "score": "74.60479"}
{"text": "Analysis : Proceedings of a Symposium Conducted by the .Mathematics Research Center , the University of .U45 no .41 ; QA297 S994 1978 \" , .University of Wisconsin , Madison \" , .38 , No . 158 ( Apr. , 1982 ) , p. 654 .", "label": "", "metadata": {}, "score": "74.65665"}
{"text": "for either real - time or data - massive computations .Such . are the topics brought into focus by these proceedings . of the Workshop on Scientific Computing ( held in Hong .Kong on March 10 - -12 , 1997 , the sixth in such series of .", "label": "", "metadata": {}, "score": "74.67566"}
{"text": "45 , no .12 , pp .4187 - 4193 , 2007 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .G. H. Golub and C. F. V. Loan , Matrix Computations , The Johns Hopkins University Press , 3rd edition , 1996 .", "label": "", "metadata": {}, "score": "74.67589"}
{"text": "where . is an unknown constant full - rank . mixing matrix and .denotes the additive noise term , which is often omitted since it is usually impossible to separate noise from the sources .ICA takes one of three forms , namely , square ICA for . , overcomplete ICA for . , and undercomplete ICA for .", "label": "", "metadata": {}, "score": "74.72212"}
{"text": "Then [ Q. requires more flops than the previous one .however the latter is more stable .We will run function Givred on the overdetermined system introduced earlier in this chapter [ Q.b .Q(:.00000000000159% Reduced QR factorization of A Givens QR factorization Another method of computing the QR factorization of a matrix uses Givens rotations rather than the Householder reflectors.i ) .", "label": "", "metadata": {}, "score": "74.73778"}
{"text": "Numerical examples based on finite element .vibration analysis illustrate the advantages of this . approach .Sci . , Stanford Univ . , CA , USA \" , .( Classical mechanics of discrete systems : general . mathematical aspects ) ; A0340 K ( Waves and wave .", "label": "", "metadata": {}, "score": "74.795044"}
{"text": "View at Google Scholar \u00b7 View at Scopus .C. Chatterjee , V. P. Roychowdhury , and E. K. P. Chong , \" On relative convergence properties of principal component analysis algorithms , \" IEEE Transactions on Neural Networks , vol .", "label": "", "metadata": {}, "score": "75.16316"}
{"text": "A Quarterly on Numerical Analysis and Theory . electrophoresis band broadening \" , . yeast genome - scale mRNA lengths distribution data . measured by DNA microarrays .SVD uncovers in the mRNA .abundance levels data matrix of genes $ \\times $ . arrays , i.e. , electrophoretic gel migration lengths or .", "label": "", "metadata": {}, "score": "75.18245"}
{"text": "end end Running function mgs on our test system we obtain [ Q. See e.g. 0 ) Givred mgs Flop count 138 488 98 errorQ 2 . j ) .j ) .In all cases the least squares solution was found using function mylsq .", "label": "", "metadata": {}, "score": "75.19822"}
{"text": "The bounds usually contain the factor .p .n .( or .p .m .n . ) , which grows as a function of the matrix dimension .n .( or matrix dimensions .m .n . )", "label": "", "metadata": {}, "score": "75.41785"}
{"text": "Selected References / xv \\\\ .1 : Matrix Multiplication Problems \\\\ . 1.1 Basic Algorithms and Notation / 2 \\\\ .1.2 Exploiting Structure / 16 \\\\ . 1.3Block Matrices and Algorithms / 24 \\\\ . 1.4 Vectorization and Re - Use Issues / 34 \\\\ .", "label": "", "metadata": {}, "score": "75.42239"}
{"text": "The superdiagonal entries are all equal to 20.06389158525507 0 .We create this matrix using some MATLAB functions that are discussed in Section 4.9.69411856608888 0.28 In this example we will illustrate sensitivity of the eigenvalues of the celebrated Wilkinson 's matrix W.56521713930244 0.1)].", "label": "", "metadata": {}, "score": "75.505806"}
{"text": "/ Christian H. Reinsch / 267 \\\\ .Experiences with some software engineering . practices in numerical software / D. A. H. Jacobs and .G. Markham / 277 \\\\ .Evolution of numerical software for dense linear .algebra / Jack Dongarra and Sven Hammarling / 297 \\\\ .", "label": "", "metadata": {}, "score": "75.6252"}
{"text": "Algorithms , Analysis . and Applications \" , .Algorithms , .Analysis and Applications \" , .S93 1991 \" , .Eng .URI ; IEEE ; EURASIP ; SIAM \" , .( Berlin ) \" , .R43 1994 \" , .", "label": "", "metadata": {}, "score": "75.64972"}
{"text": "the previous signal subspace estimate is incorporated . to achieve better numerical properties for the current . signal subspace estimate .Numerical simulations for .some signal scenarios are also presented . \"and Comput .Eng . , Texas Univ . , .", "label": "", "metadata": {}, "score": "75.67062"}
{"text": "Danny Sorensen \" , .It consists of copies of . lecture slides from the ten symposium talks . 19thSymposium on the Interface \" , . 19thSymposium on the Interface \" , .Assoc \" , .Proceedings of the 1987 International Conference on .", "label": "", "metadata": {}, "score": "75.81556"}
{"text": "This order three computational cost makes many modern applications infeasible , especially when the scale of the data is huge and growing .Therefore , it is imperative to develop a fast SVD method in modern era .If the rank of matrix is much smaller than the matrix size , there are already some fast SVD approaches .", "label": "", "metadata": {}, "score": "75.85283"}
{"text": "Tang and D. M. Sloan)\\\\ .Image reconstruction from projections based on wavelet .polynomials : conference at the Mathematical Research .Institute Oberwolfach , Germany , March 22 - -28 , 1998 \" , . polynomials : conference at the Mathematical Research .", "label": "", "metadata": {}, "score": "75.85797"}
{"text": "Algorithms have two costs : arithmetic and communication .The latter represents the cost of moving data , either between levels of a memory hierarchy , or between processors over a network .Communication often dominates arithmetic and represents a rapidly increasing proportion of the total cost , so we ... \" .", "label": "", "metadata": {}, "score": "75.88638"}
{"text": "GB memory .From the computed singular values and vectors , we observed that the singular vectors after the ninth singular vector all appear to be noise , indicating that the data matrix does have a low - rank representation .Figure 8 shows the results for a small scene from the large dataset described above , consisting of part of the University of Southern Mississippi Campus , extracted from the large Gulfport MS dataset .", "label": "", "metadata": {}, "score": "76.30643"}
{"text": "A second MATLAB 's function that can be used for computing the least squares solution is the pinv command .For more information about the pseudoinverses . which is due to C.the right - hand sides % Output : % x.-1 . see Section 4 .", "label": "", "metadata": {}, "score": "76.33432"}
{"text": "( 1999 ) for further details .The generalized eigenvalue problem can be solved via the generalized Schur factorization of the pair . 2.13 The Sylvester Equation and the Generalized Sylvester Equation . matrices .The solution of a special case of this equation occurs in the computation of the condition number for an invariant subspace , but a combination of functions in this chapter allows the solution of the general Sylvester equation .", "label": "", "metadata": {}, "score": "76.43576"}
{"text": "4 : Special Linear Systems / 133 \\\\ .4.1 The $ L D M^T$ and $ L D L^T$ Factorizations / 135 .\\\\ . 4.2Positive Definite Systems / 140 \\\\ . 4.3Banded Systems / 152 \\\\ . 4.4 Symmetric Indefinite Systems / 161 \\\\ . 4.5 Block Systems / 174 \\\\ . 4.6", "label": "", "metadata": {}, "score": "76.534775"}
{"text": "N344 1989 \" , .Series F , Computer and systems . sciences \" , . I1991 \" , . and Parallel Algorithms \" , . and Parallel Algorithms \" , .( Berlin ) \" , .N39 1988 \" , .", "label": "", "metadata": {}, "score": "76.57674"}
{"text": "By using pseudoinverse projection , the . molecular biological profiles of the data samples are .least - squares - approximated as superpositions of the . basis profiles .Reconstruction of the data in the basis . simulates experimental observation of only the cellular . states manifest in the data that correspond to those of .", "label": "", "metadata": {}, "score": "76.75314"}
{"text": "Blind separation of the original signals in nonlinear mixtures has many difficulties such as the intrinsic indeterminacy , the unknown distribution of the sources as well as the mixing conditions , and the presence of noise .It is impossible to separate the original sources using only the source independence assumption of some unknown nonlinear transformations of the sources [ 149 ] .", "label": "", "metadata": {}, "score": "76.789505"}
{"text": "genome - scale signals by using matrix and tensor . computations \" , . decomposition ( EVD ) and pseudoinverse projection and a . tensor higher - order EVD ( HOEVD ) in reconstructing the .pathways that compose a cellular system from .", "label": "", "metadata": {}, "score": "76.894806"}
{"text": "Wavelets / / 411 \\\\ .\\\\ .Physics Minisymposia \\\\ .\\\\ .Computational Magnetohydrodynamics in Astrophysics / / .431 \\\\ .Numerical Simulations of Collisionless Space Plasmas / ./ 453 \\\\ .Detection of Gravitational Radiation from Astrophysical .", "label": "", "metadata": {}, "score": "77.00078"}
{"text": "P. Townsend \\ & M. F. Webster : Computational Analysis in .Rheological Flow Problems / 260 \\\\ .Engineering : The Sixth International Conference on .Domain Decomposition , June 15 - -19 , 1992 , Como , Italy \" , .", "label": "", "metadata": {}, "score": "77.15608"}
{"text": "Thus , it is often faster to request 50 . eigenvectors ( singular triplets ) twice than it is to request 100 . eigenvectors ( singular triplets ) once .The situation only gets worse .when you are requesting many hundreds or thousands of triplets .", "label": "", "metadata": {}, "score": "77.161255"}
{"text": "Analysis Conference ( 13th ) \" , .Symposium on Sparse Matrix Computations at Argonne .Symposium on Sparse Matrix Computations at Argonne .S989 1975 \" , .Preface / / xi \\\\ .I. Design and Analysis of Elimination Algorithms / / 1 .", "label": "", "metadata": {}, "score": "77.287796"}
{"text": "We propose a method for computing principal components u ... \" .Data collected in realistic mobility traces for mobile ad hoc networks ( MANETS ) is intrinsi - cally high dimensional .Principal Component Analysis ( PCA ) is a good tool for reducing the data dimemsion by extracting important features of the data .", "label": "", "metadata": {}, "score": "77.28851"}
{"text": "G65 1983 \" , . computer systems ; matrices --- data processing \" , .No . 2 ( Jul. , 1986 ) , pp .252 - 255 , review by David F. .Mayers , Mathematics of Computation , Vol .", "label": "", "metadata": {}, "score": "77.409065"}
{"text": "G67 2007 \" , . O'Leary . analysis \" , .Reichel and Daniel B. Szyld and Nick Trefethen and Paul .Van Dooren and Andy Wathen \" , .the occasion of his 75th birthday \" , .iterative bidiagonalization and revealing the noise .", "label": "", "metadata": {}, "score": "77.657104"}
{"text": "In this paper , we discuss the performance achieved by several implementations of the recently defined Message Passing Interface ( MPI ) standard .In particular , performance results for different implementations of the broadcast operation are analyzed and compared on the Delta , Paragon , SP1 and CM5 .", "label": "", "metadata": {}, "score": "77.76154"}
{"text": "Acknowledgement This research is supported by Microsoft ( Award # 024263 ) and Intel ( Award # 024894 ) funding and by matching funding by U.C. Discovery ( Award # DIG07 - 10227 ) .Additional support comes from Par Lab . ... rix it computes the Schur form .", "label": "", "metadata": {}, "score": "78.002625"}
{"text": "Method of Successive Overrelaxation \" , .Thesis in Mathematics \" , . at Urbana - Champaign \" , .Abstracted in Dissertation Abstracts , v. 20 ( 1959 ) , no .Overrelaxation Iterative Methods , and Second Order .Overrelaxation Iterative Methods , and Second Order .", "label": "", "metadata": {}, "score": "78.015076"}
{"text": "ICA [ 144 ] is a statistical model that extends PCA .ICA has been widely used for BSS , feature extraction , and signal detection .For BSS applications , the ICA model is required to have model identifiability and separability [ 144 ] .", "label": "", "metadata": {}, "score": "78.04"}
{"text": "Positive Functions and Some Applications to Stability .Questions for Numerical Methods / Germund Dahlquist / 1 .\\\\ .Constructive Polynomial Approximation in Sobolev Spaces ./ Todd Dupont and Ridgway Scott / 31 \\\\ .Questions of Numerical Condition Related to Polynomials .", "label": "", "metadata": {}, "score": "78.16919"}
{"text": "end end .then the product of A and B is an upper ( lower ) triangular matrix .For instance .\" [ m.3 In the following example a product of two random triangular matrices is computed using function prod2 t .", "label": "", "metadata": {}, "score": "78.23398"}
{"text": "The usual error analysis of the symmetric eigenproblem is as follows ( see Parlett ( 1998 ) ) .2.14.4 The generalized symmetric - definite eigenproblem . is positive definite .We consider each case in turn , assuming that functions in this chapter are used to transform the generalized problem to the standard symmetric problem , followed by the solution of the symmetric problem .", "label": "", "metadata": {}, "score": "78.3098"}
{"text": "Quadrature and Numerical Differentiation , Gaussian . quadrature \" , . 0 - 946536 - 00 - 7 , 0 - 946536 - 05 - 8 ( paperback ) \" , .( paperback ) , 978 - 0 - 946536 - 00 - 9 , 978 - 0 - 946536 - 05 - 4 .", "label": "", "metadata": {}, "score": "78.48103"}
{"text": "This paper is an extension of the high performance tridiagonal reduction implemented by the same authors ( Luszczek et al . , IPDPS 2011 ) to the BRD case .The BRD is the first step tow ... \" .Abstract - This paper presents a new high performance bidiagonal reduction ( BRD ) on homogeneous multicore architectures .", "label": "", "metadata": {}, "score": "78.5367"}
{"text": "( Linear algebra ) ; C4185 ( Finite element analysis ) \" , . journal \" , .Eigenvalues ; Eigenvectors ; Finite element vibration . analysis ; Numerical algorithms ; Perturbations ; .Perturbed eigenvalue problems ; Real symmetric matrices ; .", "label": "", "metadata": {}, "score": "78.56294"}
{"text": "th generalized eigenvalue and its corresponding generalized eigenvector .For real symmetric and positive definite matrices , all the generalized eigenvectors are real and the corresponding generalized eigenvalues are positive .Generalized EVD achieves simultaneous diagonalization of . and . where .", "label": "", "metadata": {}, "score": "78.67491"}
{"text": "two - cyclic discretisation ; red - black two - cyclic . discretisation \" , .Diffusion ; Finite difference methods ; Iterative . methods \" , .Saddle Point Problems \" , . condition \" , . iteration methods \" , .", "label": "", "metadata": {}, "score": "78.97705"}
{"text": "The authors wish to thank the referees and the project sponsors for providing very helpful comments and suggestions for improving the paper .Research by Jiani Zhang and Jennifer Erway was supported by NSF grant DMS-08 - 11106 .Research by Robert Plemmons and Qiang Zhang was supported by the U.S. Air Force Of- fice of Scientific Research ( AFOSR ) , award number FA9550 - 11 - 1 - 0194 , and by the U.S. National Geospatial - Intelligence Agency under Contract HM1582 - 10-C-0011 , public release number PA Case 12 - 433 .", "label": "", "metadata": {}, "score": "78.995605"}
{"text": "block - angular observation matrix / M. G. Cox / 227 \\\\ .An iterative method for solving linear inequalities ./ G. W Stewart / 241 \\\\ .Iterative refinement and reliable computing / Ake .Bjorck / 249 \\\\ .", "label": "", "metadata": {}, "score": "79.08292"}
{"text": "357 - 362 , Como , Italy , July 2000 .View at Scopus .Z. Yi , Y. Fu , and H. J. Tang , \" Neural networks based approach for computing eigenvectors and eigenvalues of symmetric matrix , \" Computers and Mathematics with Applications , vol .", "label": "", "metadata": {}, "score": "79.133255"}
{"text": "2.2 Orthogonal Factorizations and Least Squares Problems .A number of functions are provided for factorizing a general rectangular .m . by .n . matrix .A . , as the product of an orthogonal matrix ( unitary if complex ) and a triangular ( or possibly trapezoidal ) matrix .", "label": "", "metadata": {}, "score": "79.29105"}
{"text": "Lanczos $ H$-tensor / / 489 \\\\ .Cosmic Censorship / / 513 \\\\ .Cauchy Problem of General Relativity / / 527 \\\\ .Black Hole Evaporation and Thermodynamics / / 543 \\\\ .The Problem of Time in Quantum Gravity / / 555 \\\\ .", "label": "", "metadata": {}, "score": "79.323746"}
{"text": "iterative methods ) \" , .Approximation Theorem \" , .September 1923 - -13 .Computations \" , .ENGINEERING , Engineering \\\\ F.2.1 Theory of .Computation , ANALYSIS OF ALGORITHMS AND PROBLEM .COMPLEXITY , Numerical Algorithms and Problems , .", "label": "", "metadata": {}, "score": "79.39726"}
{"text": "Minor component analysis ( MCA ) is a variant of PCA , which is useful for solving total least squares ( TLSs ) problems .The algorithms are typical unsupervised learning methods .Some other neural network models for feature extraction , such as localized methods , complex - domain methods , generalized EVD , and SVD , are also described .", "label": "", "metadata": {}, "score": "79.5514"}
{"text": "There are some important classes of matrices where we can do much better , including bidiagonal matrices , scaled diagonally dominant matrices , and scaled diagonally dominant definite pencils .These classes include many graded matrices , and all sym metric positive definite matrices which can be consistently ordered ( and thus all symmetric positive definite tridiagonal matrices ) .", "label": "", "metadata": {}, "score": "79.84247"}
{"text": "LMSER [ 29 ] has been compared with the weighted SLA [ 22 ] and GHA [ 8 ] in [ 37 ] .LMSER [ 29 ] uses nearly twice as much computation as weighted SLA [ 22 ] and GHA [ 8 ] , for each update of the weight .", "label": "", "metadata": {}, "score": "79.99011"}
{"text": "We observe that our SCSVD method demonstrates significant improvement .Figure 1 shows the speed comparison between the economical SVD ( solid line ) and the SCSVD ( dashed line ) with the square matrix size from 500 to 4000 by fixed rank 50 .", "label": "", "metadata": {}, "score": "80.035324"}
{"text": "splitting matrix as well as the eigenvectors of all . matrices involved .When we specialize the PSS to block . triangular ( or triangular ) and skew - Hermitian splitting .( BTSS or TSS ) , the PSS method naturally leads to a BTSS .", "label": "", "metadata": {}, "score": "80.117645"}
{"text": "the 4 days June 29 - -July 2 , 1993 . \"Approximation of Degenerate Quasilinear Elliptic and .Parabolic Problems / 1 \\\\ .R. K. Beatson \\ & M. J. D. Powell : An Iterative Method .for Thin Plate Spline Interpolation that Employs .", "label": "", "metadata": {}, "score": "80.13227"}
{"text": "Summary thus generated provides more informative content as semantics of natural language has been taken into consideration .Split - and - Combine Singular Value Decomposition for Large - Scale Matrix .Department of Mathematical Sciences , National Chengchi University , No . 64 , Section 2 , ZhiNan Road , Wenshan District , Taipei City 11605 , Taiwan .", "label": "", "metadata": {}, "score": "80.284874"}
{"text": "else error('Index k is out of range ' ) end Let M be the Gauss transformation .x(k+1:n)/x(k)].In this section we give several functions for computations with this matrix . also called the Householder reflector . is a frequently used tool in many problems of numerical linear algebra .", "label": "", "metadata": {}, "score": "80.49391"}
{"text": "If either the upper or lower triangle is stored conventionally in the upper or lower triangle of a two - dimensional array , the remaining elements of the array can be used to store other useful data .However , that is not always convenient , and if it is important to economize on storage , the upper or lower triangle can be stored in a one - dimensional array of length .", "label": "", "metadata": {}, "score": "80.61677"}
{"text": "3.3.2 Packed storage .Please see Section 3.3.2 in the f07 Chapter Introduction for full details .3.3.3 Band storage .Please see Section 3.3.3 in the f07 Chapter Introduction for full details .3.3.4 Tridiagonal and bidiagonal matrices .A symmetric tridiagonal or bidiagonal matrix is stored in two one - dimensional arrays , one of length .", "label": "", "metadata": {}, "score": "80.698006"}
{"text": "Floating point fault tolerance ; Linear equations ; .Round - off errors ; Watchdog processor environment \" , . detection ; Fault tolerant computing ; Floating point .Golub \" , . matrix factorizations \" , . numerical computation \" , .", "label": "", "metadata": {}, "score": "80.86601"}
{"text": "PCs .The normalized Oja ( NOja ) is derived by optimizing the MSE criterion subject to an approximation to the orthonormal constraint [ 81 ] .This leads to the optimal learning rate .The normalized orthogonal Oja ( NOOja ) is an orthogonal version of the NOja such that the orthonormal constraint is perfectly satisfied [ 81 ] .", "label": "", "metadata": {}, "score": "80.88226"}
{"text": "interest researchers in numerical linear algebra and .matrix computations , as well as scientists and .engineers working on problems involving computation of .bilinear forms .\" % % % Part 2 ( of 2 ) : publications about Gene H. Golub and his works . % % % Bibliography entries , sorted by year , and then by citation label . % % % ( with ' ' bibsort -byyear ' ' ) .", "label": "", "metadata": {}, "score": "80.948204"}
{"text": "known orthogonal polynomials \" , . recursion coefficients of orthogonal polynomials for a .a given weight function w by a rational function r. .Algorithms for the construction of the orthogonal . polynomials for the new weight nu in terms of those for .", "label": "", "metadata": {}, "score": "80.97059"}
{"text": "non - Hermitian positive definite linear systems , Z.-Z. Bai , G. Golub , M. K. Ng \\\\ .Solution of Least Squares Problems \\\\ .References \\\\ .Numerical methods for solving linear least squares .problems , G. Golub \\\\ .", "label": "", "metadata": {}, "score": "81.15683"}
{"text": "n . containing the off - diagonal elements .( Older functions in Chapter f02 store the off - diagonal elements in elements .n . of a vector of length .n .3.3.5 Real diagonal elements of complex matrices .", "label": "", "metadata": {}, "score": "81.15888"}
{"text": "j).1667 3 . denoted by Q.8333 13 .where H is the Householder reflector % determined by the vector u and A is a matrix.3333 -17 .The product in question.0000 -5.0000 -0.0000 -10 .Let the Householder transformations are represented by their normalized reflection vectors stored in columns of the matrix V.0000 -0 . end", "label": "", "metadata": {}, "score": "81.2321"}
{"text": "If more sophisticated coding algorithms than Hoffman coding are applied here , we could see more improvements on the compression ratios .For computing the compression ratios , we have assumed 16-bit coding ( 2-byte ) for all the matrices , including .", "label": "", "metadata": {}, "score": "81.30337"}
{"text": "After removing the water - absorption and other noisy bands , we unfold the .Comparison between rSVD and CPPCA .In this section , we will compare rSVD with CPPCA from the aspects of accuracy and computation time , first on simulated data and then on a real HSI dataset .", "label": "", "metadata": {}, "score": "81.47115"}
{"text": "the squares of the distances . \" fur Wissenschaftliches Rechnen , Eidgenossische .Tech .Hochschule , Zurich , Switzerland \" , . C4130 ( Interpolation and function approximation ) ; C4260 .( Computational geometry ) \" , .Coordinate meteorology ; Coordinates ; Curve fitting ; .", "label": "", "metadata": {}, "score": "81.4902"}
{"text": "We . illustrate this HOSVD with an integration of .genome - scale mRNA expression data from three yeast cell .cycle time courses , two of which are under exposure to .either hydrogen peroxide or menadione .We find that . significant subtensors represent independent biological .", "label": "", "metadata": {}, "score": "81.706985"}
{"text": "linear systems ; cyclically reduced nonsymmetrizable .linear systems ; diffusion ; Discrete 2D . convection - diffusion equation ; discrete 2D . convection - diffusion equation ; Finite difference .discretization ; finite difference discretization ; . finite difference methods ; iterative methods ; One cell .", "label": "", "metadata": {}, "score": "81.90508"}
{"text": "The orthogonal Oja ( OOja ) algorithm consists of Oja 's MSA [ 7 ] plus an orthogonalization of .In this case , the above algorithms given in [ 7 , 80 , 83 ] are equivalent .The OOja is numerically very stable .", "label": "", "metadata": {}, "score": "82.10743"}
{"text": "Matrix A must be of a full column rank.00000000000026 Modified Gram - Schmidt orthogonalization The third method is a variant of the classical Gram - Schmidt orthogonalization .however .The flop count and the check of orthogonality of Q are contained in the following table .", "label": "", "metadata": {}, "score": "82.45488"}
{"text": "RRLSA [ 33 ] is more robust than PASTd [ 30 ] .RRLSA can be implemented in a sequential or parallel form .RRLSA has the flexibility as Kalman - type RLS [ 31 ] , PASTd [ 30 ] , APEX [ 35 ] in that increasing the number of neurons does not affect the previously extracted principal components .", "label": "", "metadata": {}, "score": "82.495125"}
{"text": "i .n .The cross - correlation asymmetric PCA / MCA networks can be used to extract the singular values of the cross - correlation matrix of two stochastic signal vectors or to implement SVD of a general matrix .Cross - correlation asymmetric PCA ( APCA ) network consists of two sets of neurons that are laterally hierarchically connected [ 125 ] .", "label": "", "metadata": {}, "score": "82.538086"}
{"text": "JSTOR database ; Theory / Matrix . bib \" , .Phys .Lab . , Teddington , UK \" , . matrix algebra ; matrix factorizations ; rank one . change \" , .Topics ( Univ .Colorado , Boulder , Colo. , 1972 ; dedicated .", "label": "", "metadata": {}, "score": "82.90001"}
{"text": "Acoustics , Speech , and Signal Processing , April 27 - -30 , .1993 , Minneapolis Convention Center , Minneapolis , .Minnesota \" , .IEEE Catalog No . 92CH3252 - 4 .Applications : Proceedings of the NATO Advanced Study .", "label": "", "metadata": {}, "score": "83.1797"}
{"text": "Q .explicitly ; additional functions are provided to generate .Q . , or to apply it to another matrix , as with the functions for orthogonal factorizations .Explicit generation of .Q . is required before using the .", "label": "", "metadata": {}, "score": "83.271194"}
{"text": "Software for Lanczos - based Algorithms / / 311 \\\\ .Tau Method / / 335 \\\\ .Chebyshev Polynomials / / 357 \\\\ .Lanczos Methods in Control and Signal Processing / / .375 \\\\ .Development of the FFT / / 393 \\\\ .", "label": "", "metadata": {}, "score": "83.62138"}
{"text": "Domain Decomposition , June 15 - -19 , 1992 , Como , Italy \" , . I55 1992 \" , .Computer Vision , Cambridge , UK , April 15 - -18 , 1996 : . proceedings \" , .1996 : proceedings \" , . 1 ) , 3 - 540 - 61123 - 1 ( vol . 1 ) , 978 - 3 - 540 - 61123 - 3 ( vol .", "label": "", "metadata": {}, "score": "83.72337"}
{"text": "ECCV ' 96 \" , .Center , Yorktown Heights , .NY , USA \" , .Vision Soc . ; British Machine Vision Assoc \" , . volume \" , . volume \" , . N827 1996 \" , . N825 1996 \" , .", "label": "", "metadata": {}, "score": "84.25949"}
{"text": "The . authors extend this theory and the basic algorithms to .the case of an indefinite weight function .While the .generic indefinite case is formally not much different .from the positive definite case , there exist nongeneric .", "label": "", "metadata": {}, "score": "84.76982"}
{"text": "The bi ... . by Christian Bischof , Xiaobai Sun , Anna Tsao , Thomas Turnbull - in Proceedings of the Fifth SIAM Conference on Applied Linear Algebra , 1994 . \" ...In this paper , we give an overview of the Invariant Subspace Decomposition Algorithm for banded symmetric matrices and describe a sequential implementation of this algorithm .", "label": "", "metadata": {}, "score": "84.94663"}
{"text": "and menadione on cell cycle progression .A genome - scale . correlation between DNA replication initiation and RNA . transcription , which is equivalent to a recently .discovered correlation and might be due to a previously .unknown mechanism of regulation , is independently .", "label": "", "metadata": {}, "score": "85.05113"}
{"text": "% % % checksum as the first value , followed by the . % % % equivalent of the standard UNIX wc ( word .% % % count ) utility output of lines , words , and . % % % characters .", "label": "", "metadata": {}, "score": "85.08876"}
{"text": "Stewart and D. F. McAllister / 201 \\\\ .Minimum residual modifications to Bi - CG and to the .Alistair ) Watson \" , .D85 1993 \" , . library.ox.ac.uk:210/ADVANCE\" , .( 15th : 1993 ) \" , .", "label": "", "metadata": {}, "score": "85.09124"}
{"text": "J. Skeel and J. SIAM .PA.W.D.Applied Numerical Linear Algebra .Scientific Computing : An Introductory Survey .[ 7 ] L. PA .Philadelphia .T. Higham .SIAM .Numerical Linear Algebra and Applications .F. 41 . . .", "label": "", "metadata": {}, "score": "85.17494"}
{"text": "This allows us to explain experimental .results observed in quadrature calculations and in .physical chemistry and solid state physics computations . which are based on continued fraction recurrences . \"Sci . , Stanford Univ . , CA , USA \" , .", "label": "", "metadata": {}, "score": "85.54692"}
{"text": "and J. M. SanzSerna and A. Stuart \" , .60th birthday \" , .MathSciNet database \" , .Math .Appl .Conf .Ser .New Ser . \"Radar Imaging \" , .Reichel \" , .Sayed \" , .", "label": "", "metadata": {}, "score": "86.051506"}
{"text": "p. [7].Once the matrix H is computed .the solution vector x. 4.0125 -0.4490 -2 .Here H is the upper triangular matrix with positive diagonal entries .Vector x always exists .Here pinv stands for the pseudoinverse matrix.b ) .", "label": "", "metadata": {}, "score": "86.10626"}
{"text": "3.1 Available Functions .The tables in the following sub - sections show the functions which are provided for performing different computations on different types of matrices .Each entry in the table gives the NAG function short name and the LAPACK routine name from which the NAG function long name is derived by prepending nag _ ( see Section 3.2 ) .", "label": "", "metadata": {}, "score": "86.196205"}
{"text": "\\\\ .Mathematics Minisymposia \\\\ .\\\\ .Eigenvalue Computations : Theory and Algorithms / / 241 .\\\\ .Eigenvalue Computations : Applications / / 249 \\\\ .Moments in Numerical Analysis / / 265 \\\\ .Iterative Methods for Linear Systems / / 277 \\\\ .", "label": "", "metadata": {}, "score": "86.364815"}
{"text": "Integers i and j describe position of the Givens parameters.0000 0.0000 0 .J.9497 6 .4738e-016 & ' .0000 0.0872 3.0000 -0.2236 -0.6708 -0 .and P is the permutation matrix .To this end .then one can easily design a computer code for computing the vector x. U is upper triangular .", "label": "", "metadata": {}, "score": "86.70327"}
{"text": "[ n. b. .A(j+1:n . and superdiagonal of A.42 10 . sin-cos ] .j ) .b. 15 .LUF .Here a.k ) .What is the Householder reflection vector u of H ? and c stand for the subdiagonal .", "label": "", "metadata": {}, "score": "86.70559"}
{"text": "5.6 Weighting and Iterative Improvement / 264 \\\\ .5.7 Square and Underdetermined Systems / 270 \\\\ .6 : Parallel Matrix Computations / 275 \\\\ .6.1 Basic Concepts / 276 \\\\ . 6.2 Matrix Multiplication / 292 \\\\ .6.3 Factorizations / 300 \\\\ .", "label": "", "metadata": {}, "score": "86.88492"}
{"text": "% % % by others above .From version 0.04 , \" bibsort -byyear \" will . % % % correctly position Book entries that contain booktitle information .M32423 2001 \" , . melvyl.cdlib.org:210/CDL90\" , .Scientists ( 2 - -9 June , 2002 ) .", "label": "", "metadata": {}, "score": "87.160736"}
{"text": "Factorizations \" , .Computations , Second Edition , Johns Hopkins University . on an Asynchronous Processor Array \" , .Birthday \" , .15th Birthday \" , .15th birthday \" , . fourth year , whence the curious title .", "label": "", "metadata": {}, "score": "87.17695"}
{"text": "Figure 16 shows the result of classification by .-means , where we can see the low - reflectance water and shadows in yellow , the foliage in red , the grass in dark red , the pavement in green , high - reflectance beach sand in dark blue , and dirt / sandy grass in blue and light blue .", "label": "", "metadata": {}, "score": "87.599365"}
{"text": "The red solid line is the time for the SCMDS and the blue line is that for general approach .We can see that the SCMDS approach is about 8 times of general approach .Hence the SCSVD update approach is not recommended for either saving time or controlling error .", "label": "", "metadata": {}, "score": "87.84934"}
{"text": "Citation tags were automatically .% % % generated by software developed for the . % % % BibNet Project .% % % In this bibliography , entries are sorted .% % % first by ascending year , and within each . % % % year , alphabetically by author or editor , . % % % and then , if necessary , by the 3-letter .", "label": "", "metadata": {}, "score": "87.881805"}
{"text": "Matrix / Fred Gustavson / 275 \\\\ .V. Matrix Methods for Partial Difference Equations / / .291 \\\\ .Marching Algorithms and Block Gaussian Elimination / .Randolph E. Bank / 293 \\\\ .A Generalized Conjugate Gradient Method for the .", "label": "", "metadata": {}, "score": "88.09783"}
{"text": "Science \" , . correlations of matrix pairs and their numerical . computation .We first develop a decomposition theorem .for matrix pairs having the same number of rows which .explicitly exhibits the canonical correlations .We then . present a perturbation analysis of the correlations .", "label": "", "metadata": {}, "score": "88.2278"}
{"text": "S .^ .and the true space .S .S .S .^ .acute angle between \u200b .S . and \u200b .S .^ .max .s .S . s . min .s .", "label": "", "metadata": {}, "score": "88.42085"}
{"text": "Middlesex , UK , 8th--10th July 1987 .respected pioneer in numerical analysis , this book . includes contributions from his colleagues and . collaborators , leading experts in their own right .The . breadth of Wilkinson 's research is reflected in the . topics covered , which include linear algebra , error . analysis and computer arithmetic algorithms , and . mathematical software .", "label": "", "metadata": {}, "score": "88.54718"}
{"text": "Cancer \" , .Cycle in Yeast is Predicted by Data - Driven Models \" , .( SVD ) ( 1 ) and generalized SVD ( GSVD ) ( 2 ) , applied to .genome - scale datasets of yeast RNA transcription during .", "label": "", "metadata": {}, "score": "88.605255"}
{"text": "Q .Q .H .Q .I .Orthogonal or unitary matrices have the important property that they leave the . is orthogonal or unitary .They usually help to maintain numerical stability because they do not amplify rounding errors .", "label": "", "metadata": {}, "score": "88.67354"}
{"text": "Reminiscences of Cornelius Lanczos / Jon Todd / lviii .\\\\ .Published Papers and Books of Cornelius Lanczos / / lx .\\\\ .\\\\ .Plenary Presentations : Computational Mathematics \\\\ .\\\\ .Lanczos and the FFT : A Discovery Before its Time / .", "label": "", "metadata": {}, "score": "88.84259"}
{"text": "\" of the art in numerical analysis .Organized by the .Institute of Mathematics and Its Applications and held .at York University in April 1996 . \" conference series : new series \" , .Luk and Robert James Plemmons \" , .", "label": "", "metadata": {}, "score": "89.08244"}
{"text": "Computational Mathematics \\ & Numerical Computing :A . conference celebrating the 50th anniversary George .Forsythe 's arrival at Stanford and Gene Golub 's 75th .[ SciPy - user ] sparse SVD .Does SVDLIBC offer such an option ?", "label": "", "metadata": {}, "score": "89.772964"}
{"text": "Figure 1 : Comparison of the elapsed time between economical SVD ( the solid line ) and SCSVD ( the dashed line ) .Note that when the estimated rank used in the SCSVD is greater than the real rank of data matrix , there is almost no error ( except rounding error ) between the economic SVD and the SCSVD .", "label": "", "metadata": {}, "score": "91.13023"}
{"text": "Decoherence and the Foundations of Quantum Mechanics / ./ 589 \\\\ .Open Questions in Particle Theory / / 603 \\\\ .Supercollider Physics / / 621 \\\\ .Switzerland \" , .Switzerland \" , .I82 1994 \" , .", "label": "", "metadata": {}, "score": "91.21518"}
{"text": "United States of America \" , . systems \" , .Chang and G. H. Golub and C. C. Paige \" , .Least Squares Problems \" , .Bernard Mourrain \" , .Cleve Moler and Keith Moore \" , .", "label": "", "metadata": {}, "score": "91.361496"}
{"text": "Tel : +1 801 581 5254 , .FAX : +1 801 581 4148 , . 2445Mission College Blvd . .Santa Clara , CA 95054 .USA .Center for High - Performance Computing , .University of Utah , .", "label": "", "metadata": {}, "score": "91.50779"}
{"text": "Minn. , Feb. 24 - -Mar . 1 , 1992 \" , .( Berlin ) \" , . A555 1993 \" , .Acoustics , Speech , and Signal Processing , April 27 - -30 , .1993 , Minneapolis Convention Center , Minneapolis , .", "label": "", "metadata": {}, "score": "91.789825"}
{"text": "If the matrix of the system has a special structure .then this fact should be utilized in the design of the algorithm .Matrix L is unit lower triangular .Since P is orthogonal.4 . among other things.1 Triangular systems If the matrix of the system is either a lower triangular or upper triangular .", "label": "", "metadata": {}, "score": "91.85199"}
{"text": "Tel : +1 801 581 3173 , .FAX : +1 801 585 5366 , . % % % Institutional abbreviations : . % % % Journal abbreviations : .Math .Austral .Math .Methods in Appl .Mech . systems .", "label": "", "metadata": {}, "score": "92.18288"}
{"text": "\" Applied Sciences and Engineering : Summaries : .Versailles , December , 15 - -19 , 1975 \" , .Applied Sciences and Engineering : Summaries : .Versailles , December , 15 - -19 , 1975 \" , .Systems , Vol .", "label": "", "metadata": {}, "score": "93.21779"}
{"text": "Kowloon , Hong Kong \" , . B0290H( Linear algebra ) ; B0290P ( Differential . equations ) ; C4130 ( Interpolation and function .approximation ) ; C4140 ( Linear algebra ) ; C4170 .( Differential equations ) \" , .", "label": "", "metadata": {}, "score": "93.330765"}
{"text": "We illustrate .this framework with an integration of yeast .genome - scale proteins ' DNA - binding data with cell cycle .mRNA expression time course data .Novel correlation .between DNA replication initiation and RNA . transcription during the yeast cell cycle , which might .", "label": "", "metadata": {}, "score": "93.64555"}
{"text": "Anal .International Society for Optical .Applied Mathematics : Series B , Numerical .Comput .Math . % % % Miscellaneous abbreviations : . % % % Publishers and their addresses : . Co. % % % Part 1 ( of 2 ) : publications by Gene H. Golub . % % % Bibliography entries , sorted by year , and then by citation label . % % % ( with ' ' bibsort -byyear ' ' ) : .", "label": "", "metadata": {}, "score": "93.67409"}
{"text": "% % % This bibliography was collected from . % % % multiple sources : . % % % collection on ftp.math.utah.edu in .% % % /pub / tex / bib ; . % % % bibliography collection on ftp.ira.uka.de .", "label": "", "metadata": {}, "score": "93.80776"}
{"text": "yeast protein binding , including nine cell cycle . transcription factors ( 4 ) , and four DNA replication . initiation proteins ( 5 ) , across 2,928 ORFs . \" semidefinite linear systems \" , . problems \" , . perspective \" , .", "label": "", "metadata": {}, "score": "93.97762"}
{"text": "matrix ; Vector ; vector \" , . properties and applications \" , .Computational aspects . and analysis \" , .Matrices \" , . applications \" , . sequence of matrices which they call the periodic Schur . decomposition .", "label": "", "metadata": {}, "score": "94.44644"}
{"text": "Minor components ( MCs ) can be extracted in ways similar to that for PCs .A simple idea is to reverse the sign of the PCA algorithms .This is because in many algorithms PCs correspond to the maximum of a cost function , while MCs correspond to the minimum of the same cost function .", "label": "", "metadata": {}, "score": "94.605225"}
{"text": "5 : Orthogonalization and Least Squares / 206 \\\\ .5.1 Householder and Givens Matrices / 208 \\\\ .5.2 The $ Q R$ Factorization / 223 \\\\ .5.3 The Full Rank LS Problem / 236 \\\\ .5.4 Other Orthogonal Factorizations / 248 \\\\ .", "label": "", "metadata": {}, "score": "95.1006"}
{"text": "United States of America \" , . splitting method for certain two - by - two block . matrices \" , .Numerical Mathematics \" , . point problems \" , . pencils \" , . and Gene H. Golub \" , .", "label": "", "metadata": {}, "score": "95.30824"}
{"text": "S .^ .s .^ .s .s .^ .or . max .s .^ .S .^ .s .^ .min .s .S . s .s .s .", "label": "", "metadata": {}, "score": "95.698"}
{"text": "original papers , this text will be of great interest to . students and researchers in numerical analysis and . scientific computation .Biography , Chen Greif \\\\ .Publications of Gene H. Golub \\\\ .Major Awards \\\\ .Students of Gene H. Golub \\\\ .", "label": "", "metadata": {}, "score": "98.22438"}
{"text": "Dublin \" , .Irish Academy Conference on Numerical Analysis , .Dublin \" , . R691 1972 \" , .Annual Symposium on the Interface , held Oct. 16 - -17 , .1972 , on the Berkeley campus of the University of .", "label": "", "metadata": {}, "score": "98.269455"}
{"text": "To copy otherwise , to republish , to post on servers or to redistribute to lists , requires pri ... \" .personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page .", "label": "", "metadata": {}, "score": "98.274414"}
{"text": "% % % At version 1.61 , the year coverage looked . % % % like this : . % % % Article : 275 .% % % Book : 30 . % % % InBook : 1 . % % % InCollection : 21 .", "label": "", "metadata": {}, "score": "98.3185"}
{"text": "University of Sydney , Australia \" , .Proceedings of the 1987 International Conference on .Computational Techniques and Applications , held at the .University of Sydney , Australia \" , . I561 1987 \" , .Methods for Partial Differential Equations : Proceedings . of the First International Symposium on Domain .", "label": "", "metadata": {}, "score": "98.51148"}
{"text": "Bauer , A. S. Householder , F. W. J. Olver , H. .Seminar \" , .Seminar \" , .University of Wisconsin \" , .Research Seminar Dalhousie University , Halifax , March .Research Seminar Dalhousie University , Halifax , March .", "label": "", "metadata": {}, "score": "98.938126"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus Neural Network Implementations for PCA and Its Extensions . 1 Enjoyor Labs , Enjoyor Inc. , Hangzhou 310030 , China 2 Faculty of Electromechanical Engineering , Guangdong University of Technology , Guangzhou 510006 , China 3 Department of Electrical and Computer Engineering , Concordia University , Montreal , QC , Canada H3 G 1M8 .", "label": "", "metadata": {}, "score": "99.22913"}
{"text": "regulators are overactive ) , respectively .Mathematical .reconstruction of gene and array expression in a subset . of eigengenes and eigenarrays , or that of genelets and . arraylets , appears to simulate experimental observation . of only the process and cellular state , respectively , . that these expression patterns represent .", "label": "", "metadata": {}, "score": "99.27493"}
{"text": "Randomized SVD Methods in Hyperspectral Imaging .Received 16 May 2012 ; Accepted 2 August 2012 .Academic Editor : Heesung Kwon .Copyright \u00a9 2012 Jiani Zhang et al .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "99.36159"}
{"text": "Q .H .Q .T .( or by a unitary transformation .A .Q .H .Q .H . if .A . is complex ) .These functions do not form the matrix .Q .", "label": "", "metadata": {}, "score": "99.411736"}
{"text": "A Photographic Essay / / xvii \\\\ .Cornelius Lanczos : A Biographical Essay / Barbara .Gellai / xxi \\\\ .Cornelius Lanczos ( 1893 - 1974 ) , and the Hungarian .Phenomenon in Science and Mathematics / Peter D. Lax / . xlix \\\\ .", "label": "", "metadata": {}, "score": "99.506355"}
{"text": "and vitality of this field , the volume is an essential .reference for all numerical analysts .Prologue .Reflections on Jim Wilkinson / Gene Golub / 1 .\\\\ .Misconvergence in the Lanczos algorithm / Beresford .Parlett / 7 \\\\ .", "label": "", "metadata": {}, "score": "100.91009"}
{"text": "Illinois , April 13 - -15 , 1977 \" , .Proceedings of the Symposium on High Speed Computer and .Algorithm Organization , held at the University of .Illinois , April 13 - -15 , 1977 \" , .Wisconsin , Madison , 1977 .", "label": "", "metadata": {}, "score": "100.94798"}
{"text": "Conference held at Dundee , June 28 - -July 1 , 1977 \" , .Conference held at Dundee , June 28 - -July 1 , 1977 \" , .L471 v.630 \" , .Computers Conference ; sponsored by the Army Mathematics .", "label": "", "metadata": {}, "score": "101.0807"}
{"text": "Missile Range , 14 - -16 February 1979 \" , .Computers Conference ; sponsored by the Army Mathematics .Army White Sands .Missile Range , 14 - -16 February 1979 \" , .Gasser and M. Rosenblatt \" , . of Workshop Held in Heidelberg , April 2 - -4 , 1979 \" , . of Workshop Held in Heidelberg , April 2 - -4 , 1979 \" , .", "label": "", "metadata": {}, "score": "101.133804"}
{"text": "Seventh Annual Symposium on the Interface , Ames , Iowa , .October 18 - -19 , 1973 \" , .Seventh Annual Symposium on the Interface , Ames , Iowa , .October 18 - -19 , 1973 \" , .", "label": "", "metadata": {}, "score": "103.41456"}
{"text": "19 - 23 , 1989''--T.p . verso . ''Published in cooperation .with NATO Scientific Affairs Division . ' ' Series F , Computer and systems . sciences \" , .Mathematics and Computing \" , .Mathematics and Computing \" , . R435 1990 \" , . late James Hardy Wilkinson ( died Sunday 5th October .", "label": "", "metadata": {}, "score": "103.4579"}
{"text": "Centenary Conference , Raleigh , North Carolina , December .C67 1993 \" , .z3950.loc.gov:7090/Voyager \" , .( 1993:Raleigh , NC ) \" , .Mathematics ; Lanczos , Cornelius ; Physicists ; Hungary ; .Biography ; Mathematicians \" , .", "label": "", "metadata": {}, "score": "103.74548"}
{"text": "Engineering , Versailles , France , December 1979 \" , . proceedings of the fourth International Symposium on .Computing Methods in Applied Sciences and Engineering , .Versailles , France , December 10 - -14 , 1979 \" , . proceedings of the fourth International Symposium on .", "label": "", "metadata": {}, "score": "104.03331"}
{"text": "Versailles , France , December 10 - -14 , 1979 \" , .I57 1979 \" , . C194 1982 \" , .Conference , held at Dundee , Scotland , June 23 - -26 , .Conference , held at Dundee , Scotland , June 23 - -26 , .", "label": "", "metadata": {}, "score": "104.80667"}
{"text": "Versailles , France , December 10 - -14 , 1979 \" , . proceedings of the fourth International Symposium on .Computing Methods in Applied Sciences and Engineering , .Versailles , France , December 10 - -14 , 1979 \" , .", "label": "", "metadata": {}, "score": "105.916664"}
{"text": "Department of Mathematics , .University of Utah , .Salt Lake City , UT 84112 , USA , .Tel : +1 801 581 6879 , .University of Utah , .Department of Mathematics , 110 LCB , .155 S 1400 E RM 233 , .", "label": "", "metadata": {}, "score": "106.222244"}
{"text": "Bad Honnef , August 30 - -September 1 , 1978 \" , .Bad Honnef , August 30 - -September 1 , 1978 \" , .S43 \" , .Symposium on Computing Methods in Applied Science and .Engineering , Versailles , France , December 1979 \" , .", "label": "", "metadata": {}, "score": "106.42186"}
{"text": "United States of America \" , .David Botstein \" , .D'Urso and T. Fletcher and F. Huijing and A. Marshall . and B. Pulverer and B. Renault and J. D. Rosenblatt and .J. M. Slingerland and W. J. Whelan \" , .", "label": "", "metadata": {}, "score": "106.88338"}
{"text": "Annual Symposium on the Interface , held Oct. 16 - -17 , .1972 , on the Berkeley campus of the University of .California \" , . , Venezuela \" , .Rech .D'Informatique et d'Automatique \" , . titles .", "label": "", "metadata": {}, "score": "107.505775"}
{"text": "Paul Saylor and James M. Boyle and Iain Duff and Jack .Dongarra \" , . extracts from messages that were sent over various . computer networks during the period October 5 , . 1986 - -February 13 , 1987 ' ' .", "label": "", "metadata": {}, "score": "107.97885"}
{"text": "Academic Editors : C. Kotropoulos and B. Schuller .Copyright \u00a9 2012 Jialin Qiu et al .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "110.484146"}
{"text": "% % % BibTeX citation tags are uniformly chosen . % % % as name : year : abbrev , where name is the . % % % family name of the first author or editor , . % % % year is a 4-digit number , and abbrev is a . % % % 3-letter condensation of important title .", "label": "", "metadata": {}, "score": "111.754364"}
{"text": "U45 no .25 \" , .University of Wisconsin , Madison \" , .Morris \" , .Dundee / Scotland , September 15 - -23 , 1970 \" , .Dundee / Scotland , September 15 - -23 , 1970 \" , .", "label": "", "metadata": {}, "score": "113.97766"}
{"text": "1983 , Versailles , France \" , .Applied Sciences and Engineering : December 12 - -16 , .1983 , Versailles , France \" , .Lions \" , . on Computing Methods in Applied Sciences and .Engineering , Versailles , France , December 12 - -16 , . on Computing Methods in Applied Sciences and .", "label": "", "metadata": {}, "score": "114.06224"}
{"text": "University of Wisconsin , Madison , October 8 - -10 , 1973 \" , .U45 no .32 \" , .University of Wisconsin , Madison \" , .Mathematicians ( Watervliet Arsenal , Watervliet , NY , .Proceedings of the Symposium on High Speed Computer and .", "label": "", "metadata": {}, "score": "115.25255"}
{"text": "Congress 65 \" , .I575 \" , .University of Wisconsin ) Statistical computation ; . proceedings \" , .University of Wisconsin ) Statistical computation ; . proceedings \" , .C6 1969 \" , .I57 \" , .", "label": "", "metadata": {}, "score": "116.512024"}
{"text": "Academic Editor : Nicola Mastronardi .Copyright \u00a9 2013 Jengnan Tzeng .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "116.973434"}
{"text": "Jim Wilkinson : some after - dinner sentiments / .TX , October 19 - -21 , 1988 ) \" , .I84 1990 \" , .Supercomputing ( 1989 : Trondheim , Norway ) \" , .Supercomputing ( 1989 : Trondheim , Norway ) \" , .", "label": "", "metadata": {}, "score": "117.04234"}
{"text": "University of Wisconsin , Madison \" , .( 1978 : Madison , Wis. ) .Recent Advances in Numerical .Analysis : Proceedings of a Symposium Conducted by the .Mathematics Research Center , the University of .( 1978 : Madison , Wis. ) .", "label": "", "metadata": {}, "score": "122.97518"}
{"text": "1979 , University of Waterloo , Waterloo , Ontario , .Canada \" , . 12th Annual Symposium on the Interface : May 10 - -11 , .1979 , University of Waterloo , Waterloo , Ontario , .Canada \" , . proceedings of the fourth International Symposium on .", "label": "", "metadata": {}, "score": "123.91057"}
{"text": "View at Scopus .View at Scopus % % % beebe at computer.org ( Internet ) \" , . % % % analysis , singular value decomposition , SVD \" , . % % % publications of the late Gene Howard Golub . % % % ( February 29 , 1932 - -November 16 , 2007 ) .", "label": "", "metadata": {}, "score": "125.62331"}
{"text": "Research Center , the University of Wisconsin , Madison , .March 28 - -30 , 1977 \" , .Wisconsin , Madison , 1977 .Proceedings of a Symposium Conducted by the Mathematics .Research Center , the University of Wisconsin , Madison , .", "label": "", "metadata": {}, "score": "128.72539"}
