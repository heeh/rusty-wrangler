{"text": "The constraints that are enforced can relate to the constituent boundaries and the semantic tags for the constituents .In another embodiment , the SLM operates in a left - to - right , bottom - up fashion and generates binary trees .", "label": "", "metadata": {}, "score": "29.91985"}
{"text": "These approaches assume access to a common input representation in the form of universal tags , which enables the model to connect patterns observed in the source language to th ... . \" ...In this paper , we study direct transfer methods for multilingual named entity recognition .", "label": "", "metadata": {}, "score": "31.385365"}
{"text": "We present an automatic method for mapping language - specific part - of - speech tags to a set of universal tags .This unified representation plays a crucial role in cross - lingual syntactic transfer of multilingual dependency parsers .Until now , however , such conversion schemes have been created manually ... \" .", "label": "", "metadata": {}, "score": "34.11943"}
{"text": "Optionally , said predictive model is a hand tuned decision tree based procedure .Preferably , said step of identifying an entity within said span of text further comprises : . dividing said span of text into a plurality of text elements ; . generating an entity feature vector for each of said text elements , said entity feature vector generated in part according to features relevant to said criteria , thereby generating an entity feature vector sequence for said span of text ; and .", "label": "", "metadata": {}, "score": "34.1251"}
{"text": "Optionally , said predictive model is a hand tuned decision tree based procedure .Preferably , said step of identifying an entity within said span of text further comprises : . dividing said span of text into a plurality of text elements ; . generating an entity feature vector for each of said text elements , said entity feature vector generated in part according to features relevant to said criteria , thereby generating an entity feature vector sequence for said span of text ; and .", "label": "", "metadata": {}, "score": "34.1251"}
{"text": "Tools . by Slav Petrov , Dipanjan Das , Ryan McDonald - IN ARXIV:1104.2086 , 2011 . \" ...To facilitate future research in unsupervised induction of syntactic structure and to standardize best - practices , we propose a tagset that consists of twelve universal part - of - speech categories .", "label": "", "metadata": {}, "score": "34.34484"}
{"text": "To facilitate future research in unsupervised induction of syntactic structure and to standardize best - practices , we propose a tagset that consists of twelve universal part - of - speech categories .In addition to the tagset , we develop a mapping from 25 different treebank tagsets to this universal set .", "label": "", "metadata": {}, "score": "34.49408"}
{"text": "We present a new family of models for unsupervised parsing , Dependency and Boundary models , that use cues at constituent boundaries to inform head - outward dependency tree generation .We build on three intuitions that are explicit in phrase - structure grammars but only implicit in standard dependency ... \" .", "label": "", "metadata": {}, "score": "35.176487"}
{"text": "Typically , not all the words in text are relevant to a particular frame .Assuming that the segments of text relevant to filling in the slots are non - overlapping , contiguous strings of words , one can represent the semantic frame as a simple semantic parse tree for the sentence to be processed .", "label": "", "metadata": {}, "score": "36.320747"}
{"text": "This \" site - based weighting \" approach yields substantially better performance from trained classifiers and extractors than uniform weighting schemes .This process of labeling is used at multiple stages throughout the extraction method to train the relevant classifier to classify for the relevant characteristic depending on which step of the extraction method is being performed .", "label": "", "metadata": {}, "score": "37.333652"}
{"text": "This \" site - based weighting \" approach yields substantially better performance from trained classifiers and extractors than uniform weighting schemes .This process of labeling is used at multiple stages throughout the extraction method to train the relevant classifier to classify for the relevant characteristic depending on which step of the extraction method is being performed .", "label": "", "metadata": {}, "score": "37.333652"}
{"text": "Preferably , said step of processing said span of text further comprises : . where a plurality of said entity are identified , associating said entities within said span of text , wherein said step of associating said entities includes linking related entities together for storage in a category of said structured record .", "label": "", "metadata": {}, "score": "37.663696"}
{"text": "Preferably , said step of processing said span of text further comprises : . where a plurality of said entity are identified , associating said entities within said span of text , wherein said step of associating said entities includes linking related entities together for storage in a category of said structured record .", "label": "", "metadata": {}, "score": "37.663696"}
{"text": "The XML markup also allows for the placements of the tags in other positions , as long as a span parameter is provided : .To obtain the syntactic annotation , you will likely use a third - party parser , which has its own idiosyncratic input and output format .", "label": "", "metadata": {}, "score": "37.81835"}
{"text": "Preferably , said step of normalizing said entities within said span of text further comprises : . selecting those associated entities sharing a predetermined number of features ; and normalizing these associated entities to refer to said same entity .In a second aspect the present invention accordingly provides a method for training a classifier to classify for text based elements in a collection of text based elements according to a characteristic , said method comprising the steps of : . forming a feature vector corresponding to each text based element ; . forming a sequence of said feature vectors corresponding to each of said text based elements in said collection of text based elements ; . labeling each text based element according to said characteristic thereby forming a sequence of labels corresponding to said sequence of feature vectors ; and .", "label": "", "metadata": {}, "score": "37.901184"}
{"text": "Preferably , said step of normalizing said entities within said span of text further comprises : . selecting those associated entities sharing a predetermined number of features ; and normalizing these associated entities to refer to said same entity .In a second aspect the present invention accordingly provides a method for training a classifier to classify for text based elements in a collection of text based elements according to a characteristic , said method comprising the steps of : . forming a feature vector corresponding to each text based element ; . forming a sequence of said feature vectors corresponding to each of said text based elements in said collection of text based elements ; . labeling each text based element according to said characteristic thereby forming a sequence of labels corresponding to said sequence of feature vectors ; and .", "label": "", "metadata": {}, "score": "37.901184"}
{"text": "The top down approach of the present invention also makes it directly applicable to a machine learning approach which automates the extraction process .Preferably , said step of processing said span of text further comprises : . identifying an entity within said span of text , said entity including at least one entity text element , wherein said entity is associated with at least one of said categories of said structured record .", "label": "", "metadata": {}, "score": "38.202045"}
{"text": "The top down approach of the present invention also makes it directly applicable to a machine learning approach which automates the extraction process .Preferably , said step of processing said span of text further comprises : . identifying an entity within said span of text , said entity including at least one entity text element , wherein said entity is associated with at least one of said categories of said structured record .", "label": "", "metadata": {}, "score": "38.202045"}
{"text": "In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level , we instead use features to transfer the behavior of words at a type level .In a discriminative dependency parsing framework , our approach produces gains across a range of target languages , using two different lowresource training methodologies ( one weakly supervised and one indirectly supervised ) and two different dictionary sources ( one manually constructed and one automatically constructed ) . ... me , correctly suggesting attachments to Verzicht and Gewerkschaften .", "label": "", "metadata": {}, "score": "38.47403"}
{"text": "Only nodes that result in a CCM level 2 assigned , either directly or inherited , will be analyzed .Typical monitoring dimensions such as dimensions 502 - 508 used by the multi - dimensional commodity model 104 may include performance tolerances , noise filters , oscillation thresholds or trends , consecutive trending , negative performance threshold , and any other dimensions desired .", "label": "", "metadata": {}, "score": "38.555046"}
{"text": "This unified representation plays a crucial role in cross - lingual syntactic transfer of multilingual dependency parsers .Until now , however , such conversion schemes have been created manually .Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across source and target languages .", "label": "", "metadata": {}, "score": "38.677383"}
{"text": "W . k .T . k . ) which ensures a proper probability over strings W , where S k is the set of all parses present in our stacks at the current stage k. .Each model component - word - predictor , tagger , parser - is initialized from a set of parsed sentences after undergoing headword percolation and binarization .", "label": "", "metadata": {}, "score": "38.83098"}
{"text": "The semantic schema for an application will thus contain a set of templates , or frames , that define an action to be taken by the computer .The frames or templates have one or more slots that are to be filled in from the input text in order to prompt the action to be performed .", "label": "", "metadata": {}, "score": "39.041317"}
{"text": "As mentioned above , the problem of information extraction can be viewed as the recovery of a two - level semantic parse for a given word sequence .In accordance with one embodiment of the present invention , a data driven approach to information extraction uses a SLM .", "label": "", "metadata": {}, "score": "39.655582"}
{"text": "Unsupervised Discovery of a Statistical Verb Lexicon .2006 Conference on Empirical Methods in Natural Language Processing ( EMNLP 2006 ) , pp . 1 - 8 .Dan Klein and Christopher D. Manning .Corpus - Based Induction of Syntactic Structure : Models of Dependency and Constituency .", "label": "", "metadata": {}, "score": "39.712776"}
{"text": "In this case , the training data used by the machine learning algorithms consists of one example for each pair of labeled entities ( of the appropriate types ) from each labeled span ( item 360 in FIG .9 ) .", "label": "", "metadata": {}, "score": "39.724934"}
{"text": "In this case , the training data used by the machine learning algorithms consists of one example for each pair of labeled entities ( of the appropriate types ) from each labeled span ( item 360 in FIG .9 ) .", "label": "", "metadata": {}, "score": "39.724934"}
{"text": "Further , the semantic schema is language independent , in the sense that it does not specify the linguistic expressions used to express a concept .Therefore , it is used not only for language - enabling applications , but also for integrating inputs from multi - modalities , such as mouse click events .", "label": "", "metadata": {}, "score": "40.379444"}
{"text": "Finally , the SLM is trained again using constrained parsing .This time , however , the constraint is not only to match the constituent spans or boundaries , but it is also constrained to match the annotated semantic labels and is thus referred to as enforcing the L - match ( for label - match ) constraint .", "label": "", "metadata": {}, "score": "40.657806"}
{"text": "Semantic schema is often used for many different purposes .For example , semantic schema serves as the specification for a language - enabled application .In other words , once a semantic schema is defined , grammar and application logic development can proceed simultaneously according to the semantic schema .", "label": "", "metadata": {}, "score": "40.72798"}
{"text": "For example , documents from the same site may share similar structure or biographies from the same site , may use common idioms peculiar to the site .Most machine learning algorithms can deal with \" weighted \" training examples in which the significance of each example is reflected by an assigned number between 0 and 1 .", "label": "", "metadata": {}, "score": "40.73223"}
{"text": "For example , documents from the same site may share similar structure or biographies from the same site , may use common idioms peculiar to the site .Most machine learning algorithms can deal with \" weighted \" training examples in which the significance of each example is reflected by an assigned number between 0 and 1 .", "label": "", "metadata": {}, "score": "40.73223"}
{"text": "These labels are typically obtained by parsing the target side of the training corpus .( However , it is also possible to use parses of the source side which has been projected onto the target side ( Ambati and Chen , 2007 ) ) .", "label": "", "metadata": {}, "score": "40.84985"}
{"text": "We consider the problem of using a bilingual dictionary to transfer lexico - syntactic information from a resource - rich source language to a resource - poor target language .In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level , we instead use features ... \" .", "label": "", "metadata": {}, "score": "40.88216"}
{"text": "The method for training a classifier to classify for text based elements in a collection of text based elements according to claim 22 , wherein said text based element is a span of text elements and said collection of text based elements is a document .", "label": "", "metadata": {}, "score": "41.02279"}
{"text": "The method for training a classifier to classify for text based elements in a collection of text based elements according to claim 22 , wherein said text based element is a span of text elements and said collection of text based elements is a document .", "label": "", "metadata": {}, "score": "41.02279"}
{"text": "COREFEREE .A referential phrase .This tag will also have an attribute which links it back to the ANTECEDENT to which the text refers .Reading List .This list has been extracted from the project proposal document .Sally Goldman and Yan Zhou .", "label": "", "metadata": {}, "score": "41.148453"}
{"text": "Specifically , the parser proposes a set of n syntactic binary parses for a given word string , all matching the constituent boundaries specified by the semantic parse .A parse T is said to match the semantic parse S denoted T .", "label": "", "metadata": {}, "score": "41.16181"}
{"text": "2 .Thus , the problem of information extraction can be viewed as the recovery of a two - level semantic parse for a given word sequence .In accordance with one embodiment of the present invention , a data driven approach to information extraction uses a structured language model ( SLM ) .", "label": "", "metadata": {}, "score": "41.266754"}
{"text": "Being largely languageuniversal , the selection component is learned in a supervised fashion from all the training languages .In contrast , the ordering decisions are only influenced by languages with similar properties .We systematically model this cross - lingual sharing using typological features .", "label": "", "metadata": {}, "score": "41.32573"}
{"text": "( iii ) Sentence - internal punctuation boundaries help with longer - distance dependencies , since punctuation correlates with constituent edges .Our models induce state - of - the - art dependency grammars for many languages without special knowledge of optimal input sentence lengths or biased , manually - tuned initializers . ...", "label": "", "metadata": {}, "score": "41.363407"}
{"text": "This is an active field of research .You may have read in the literature about hierarchical phrase - based , string - to - tree , tree - to - string , tree - to - tree , target - syntactified , syntax - augmented , syntax - directed , syntax - based , grammar - based , etc . , models in statistical machine translation .", "label": "", "metadata": {}, "score": "41.528587"}
{"text": "Data formats .The constituent data are in a bracketing format similar to the Penn treebank format , except that we deleted all the extra whitespace and newlines .So each sentence constitutes one line in the file .Sentences are separated by empty lines .", "label": "", "metadata": {}, "score": "41.648796"}
{"text": "It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure .While previous work has focused primarily on English , we extend these results to other languages along two dimensions .", "label": "", "metadata": {}, "score": "41.801075"}
{"text": "It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure .While previous work has focused primarily on English , we extend these results to other languages along two dimensions .", "label": "", "metadata": {}, "score": "41.801075"}
{"text": "These regions are generically referred to as \" text nodes \" , regardless of their method of construction .Each segmented text node is processed 720 to generate a vector of features .Such features would usually include indicators for each word in the text node , frequency information , membership of text node words in various lists such as first name , last name , jobtitle and so on .", "label": "", "metadata": {}, "score": "42.388966"}
{"text": "These regions are generically referred to as \" text nodes \" , regardless of their method of construction .Each segmented text node is processed 720 to generate a vector of features .Such features would usually include indicators for each word in the text node , frequency information , membership of text node words in various lists such as first name , last name , jobtitle and so on .", "label": "", "metadata": {}, "score": "42.388966"}
{"text": "We provide wrappers ( in scripts / training / wrapper ) for the following parsers .The use of syntactic annotation puts severe constraints on the number of rules that can be extracted , since each non - terminal has to correspond to an actual non - terminal in the syntax tree .", "label": "", "metadata": {}, "score": "42.430023"}
{"text": "Next , the syntactic labels in the syntactic parse are replaced with joint syntactic and semantic labels .This is indicated by block 232 in .FIG .8 .An example of this step is illustrated in .FIG .7B .", "label": "", "metadata": {}, "score": "42.612923"}
{"text": "Preferably , said step of associating said entities within said span of text further comprises : . forming pairs of entities to determine if they are to be associated ; . generating an entity pair feature vector for each pair of entities , said entity pair feature vector generated in part according to features relevant to associations between entity pairs ; . calculating an association label based on said entity pair feature vector to determine if a given pair of entities are linked , said association label calculated by a predictive algorithm adapted to generate said association label from an input entity pair feature vector .", "label": "", "metadata": {}, "score": "42.74105"}
{"text": "Preferably , said step of associating said entities within said span of text further comprises : . forming pairs of entities to determine if they are to be associated ; . generating an entity pair feature vector for each pair of entities , said entity pair feature vector generated in part according to features relevant to associations between entity pairs ; . calculating an association label based on said entity pair feature vector to determine if a given pair of entities are linked , said association label calculated by a predictive algorithm adapted to generate said association label from an input entity pair feature vector .", "label": "", "metadata": {}, "score": "42.74105"}
{"text": "We build on three intuitions that are explicit in phrase - structure grammars but only implicit in standard dependency formulations : ( i ) Distributions of words that occur at sentence boundaries - such as English determiners - resemble constituent edges .", "label": "", "metadata": {}, "score": "42.74443"}
{"text": "The correct pattern will depend on the labels assigned to the training data on which the model was trained .As described previously , it is important that the label sequence be able to distinguish the boundaries between adjacent entities of interest .", "label": "", "metadata": {}, "score": "42.858185"}
{"text": "The correct pattern will depend on the labels assigned to the training data on which the model was trained .As described previously , it is important that the label sequence be able to distinguish the boundaries between adjacent entities of interest .", "label": "", "metadata": {}, "score": "42.858185"}
{"text": "The two constrained parsing steps illustrated by blocks 230 and 240 in .FIG .8 ensure that the constituents proposed by the SLM do not cross semantic constituent boundaries and that the labels proposed are the desired ones .where l is the left boundary of the constraint , r is the right boundary of the constraint and Q is the set of allowable non - terminal ( semantic ) tags for the constraint .", "label": "", "metadata": {}, "score": "43.106274"}
{"text": "The tags annotating the nodes of the tree are purely syntactic during the training step 230 and are syntactic and semantic during the training step 240 .For a given word - parse k - prefix W k T k accept an adjoin transition if an only if : . Q. .", "label": "", "metadata": {}, "score": "43.15869"}
{"text": "It is an iterative and time consuming process that requires grammars to be written using a combination of knowledge and data , and then tested and refined using test data .Thus , the current approaches can tend to be not only time consuming , but quite costly .", "label": "", "metadata": {}, "score": "43.610657"}
{"text": "However , deciding which sequences are units at all requires telling apart the red and blue clusters on the left -- which is much harder .However , using a constituent - context model , which essentially allows distributional clustering in the presence of no - overlap constraints , we can successfully recover a substantial amount of hierarchical structure , even with just a few thousand training sentences .", "label": "", "metadata": {}, "score": "43.845016"}
{"text": "In practice , however , phrase - based models use a reordering limit , which leads to linear decoding time .For tree - based models , decoding is not linear with respect to sentence length , unless reordering limits are used .", "label": "", "metadata": {}, "score": "43.896088"}
{"text": "Preferably , said step of identifying a span of text further comprises : . dividing said document into a plurality of text nodes , said text nodes each including at least one text element ; . generating a text node feature vector for each of said text nodes , said text node feature vector generated in part according to features relevant to said criteria , thereby generating a text node feature vector sequence for said document ; and .", "label": "", "metadata": {}, "score": "43.896484"}
{"text": "Preferably , said step of identifying a span of text further comprises : . dividing said document into a plurality of text nodes , said text nodes each including at least one text element ; . generating a text node feature vector for each of said text nodes , said text node feature vector generated in part according to features relevant to said criteria , thereby generating a text node feature vector sequence for said document ; and .", "label": "", "metadata": {}, "score": "43.896484"}
{"text": "In some cases many iterations may be required before adequate performance from the trained classifiers and extractors is achieved .Two of the primary determinants of trained classifier and extractor performance are the number of independent labeled training examples and the extent to which spurious or irrelevant features can be pruned from the training data .", "label": "", "metadata": {}, "score": "44.02977"}
{"text": "In some cases many iterations may be required before adequate performance from the trained classifiers and extractors is achieved .Two of the primary determinants of trained classifier and extractor performance are the number of independent labeled training examples and the extent to which spurious or irrelevant features can be pruned from the training data .", "label": "", "metadata": {}, "score": "44.02977"}
{"text": "For example , features that occur on too few training examples can be pruned .In a similar fashion , the labeled training examples can be weighted so that each site 's examples contributes the same amount to the statistics upon which pruning is based .", "label": "", "metadata": {}, "score": "44.135017"}
{"text": "For example , features that occur on too few training examples can be pruned .In a similar fashion , the labeled training examples can be weighted so that each site 's examples contributes the same amount to the statistics upon which pruning is based .", "label": "", "metadata": {}, "score": "44.135017"}
{"text": "A complementary approach to induction is to focus on relationships between word pairs , or dependencies .Previous work using this approach was quite unsuccessful because it used too simple a dependency model .Our model , borrowing ideas of word classes and valence from supervised parsing dependency models , performs well above baseline .", "label": "", "metadata": {}, "score": "44.225365"}
{"text": "The model is then trained by generating parses on the semantically annotated training data enforcing the semantic tags ( or labels ) as well as the annotated constituent boundaries found in the training data .In one embodiment , the structured language model operates with binary trees in a left - to - right , bottom - up fashion .", "label": "", "metadata": {}, "score": "44.23635"}
{"text": "The correct pattern will depend on the labels assigned to the training data on which the model was trained .The locations of all such entities within a document and their category ( name , organization , jobtitle , etc ) are output 1020 .", "label": "", "metadata": {}, "score": "44.27807"}
{"text": "The correct pattern will depend on the labels assigned to the training data on which the model was trained .The locations of all such entities within a document and their category ( name , organization , jobtitle , etc ) are output 1020 .", "label": "", "metadata": {}, "score": "44.27807"}
{"text": "A set of attributes that defines the dimensions to analyze , along with the associated trends and patterns of interest , for any particular node in any commodity tree .FIG .1 is a block diagram describing the multi - dimensional commodity model in the context of a quality management system ( QMS ) environment .", "label": "", "metadata": {}, "score": "44.46826"}
{"text": "Mare\u010dek and Zabokrtsk\u00b4y , 2011 , inter alia ) . \" ...We present a novel algorithm for multilingual dependency parsing that uses annotations from a diverse set of source languages to parse a new unannotated language .Our motivation is to broaden the advantages of multilingual learning to languages that exhibit significant differences from existing reso ... \" .", "label": "", "metadata": {}, "score": "44.4859"}
{"text": "In contrast to annotation projection approaches ( Yarowsky et al . , 2001 ; Hwa et al . , 2005 ; Ganchev et al ., 2009 ; Spreyer and Kuhn , 2009 ) , delexicalized transfer methods do not rely on ... . \" ...", "label": "", "metadata": {}, "score": "44.563786"}
{"text": "Toward conditional models of identity uncertainty with application to proper noun coreference .In IJCAI Workshop on Information Integration on the Web , 2003 .Christoph Mueller , Stefan Rapp , and Michael Strube .Applying cotraining to reference resolution .In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics ( ACL ) , July 2002 .", "label": "", "metadata": {}, "score": "44.759575"}
{"text": "Reordering is limited in phrase - based models , and non - syntactic tree - based models ( better known as hierarchical phrase - based models ) and should limit reordering for the same reason : they are just not very good at long - distance reordering anyway .", "label": "", "metadata": {}, "score": "44.787865"}
{"text": "The output 1550 of all these algorithms is a trained classifier that assigns the appropriate label to the feature vector of an association .Apply the trained association classifier to the feature - vectors and output the positively classified associations .Once all extraction steps have been performed on a document , the extracted spans , entities , associations and classification are assembled 190 into a structured record such as the XML document referred to above .", "label": "", "metadata": {}, "score": "44.900505"}
{"text": "The output 1550 of all these algorithms is a trained classifier that assigns the appropriate label to the feature vector of an association .Apply the trained association classifier to the feature - vectors and output the positively classified associations .Once all extraction steps have been performed on a document , the extracted spans , entities , associations and classification are assembled 190 into a structured record such as the XML document referred to above .", "label": "", "metadata": {}, "score": "44.900505"}
{"text": "Referring now to FIG .9 , there is shown the next step in the labeling process wherein the spans of interest within the previously labeled web - pages of interest are labeled .The locations of the token boundaries of each span in each document are then stored 360 .", "label": "", "metadata": {}, "score": "44.91378"}
{"text": "Referring now to FIG .9 , there is shown the next step in the labeling process wherein the spans of interest within the previously labeled web - pages of interest are labeled .The locations of the token boundaries of each span in each document are then stored 360 .", "label": "", "metadata": {}, "score": "44.91378"}
{"text": "The labels for each association are read from the classified associations store 500 ( generated by the labeling process of FIG .14 ) and assigned 1530 to the feature vectors of the corresponding associations .The feature vectors for each association and their corresponding labels are then used 1540 as training data to train a classifier to distinguish associations of different categories .", "label": "", "metadata": {}, "score": "44.931343"}
{"text": "The labels for each association are read from the classified associations store 500 ( generated by the labeling process of FIG .14 ) and assigned 1530 to the feature vectors of the corresponding associations .The feature vectors for each association and their corresponding labels are then used 1540 as training data to train a classifier to distinguish associations of different categories .", "label": "", "metadata": {}, "score": "44.931343"}
{"text": "It is an object of the present invention to provide a method that is capable of extracting a structured record from a document relevant to a given query type that is substantially independent of the domain of interest of that query .", "label": "", "metadata": {}, "score": "44.955498"}
{"text": "It is an object of the present invention to provide a method that is capable of extracting a structured record from a document relevant to a given query type that is substantially independent of the domain of interest of that query .", "label": "", "metadata": {}, "score": "44.955498"}
{"text": "FIG . 8 .During this process , the SLM is allowed to explore ( or generate parses for ) only the semantic parses found in the training data .Thus , the semantic constituent labels are taken into account .This means that a parse P -containing both syntactic and semantic information -is said to L - match S if and only if the set of labeled semantic constituents that defines S is identical to the set of semantic constituents that defines P. In the present embodiment , the semantic tree S has a two - level structure .", "label": "", "metadata": {}, "score": "45.044838"}
{"text": "The method for training a classifier to classify for text based elements in a collection of text based elements according to claim 22 , wherein said text based element is a sub - entity comprising at least one text element and said collection of text based elements is an entity .", "label": "", "metadata": {}, "score": "45.091427"}
{"text": "The method for training a classifier to classify for text based elements in a collection of text based elements according to claim 22 , wherein said text based element is a sub - entity comprising at least one text element and said collection of text based elements is an entity .", "label": "", "metadata": {}, "score": "45.091427"}
{"text": "Although all dimensions are assigned at different levels within the commodity tree , it is important to note that all dimensions are derived from the detailed constituents .Since the CCM L 1 attributes established uniform characteristics across all nodes under the commodity , these analyses and analytics are derived from the bottom up from the nodes that inherited its properties as illustrated in .", "label": "", "metadata": {}, "score": "45.260292"}
{"text": "Our motivation is to broaden the advantages of multilingual learning to languages that exhibit significant differences from existing resource - rich languages .The algorithm learns which aspects of the source languages are relevant for the target language and ties model parameters accordingly .", "label": "", "metadata": {}, "score": "45.540726"}
{"text": "( 2012 ) , which is based on cross - lingual word cluster features .First , we show that by using multiple source languages , combined with sel ... \" .In this paper , we study direct transfer methods for multilingual named entity recognition .", "label": "", "metadata": {}, "score": "45.553917"}
{"text": "A machine learning based approach is the preferred method for training association classifiers , although other direct ( not - trained ) methods are also applicable .In this case , the training data used by the machine learning algorithms consists of one example for each labeled association ( of the appropriate type ) ( item 500 at FIG .", "label": "", "metadata": {}, "score": "45.579357"}
{"text": "A machine learning based approach is the preferred method for training association classifiers , although other direct ( not - trained ) methods are also applicable .In this case , the training data used by the machine learning algorithms consists of one example for each labeled association ( of the appropriate type ) ( item 500 at FIG .", "label": "", "metadata": {}, "score": "45.579357"}
{"text": "P .p .i . k .W . k .T . k . )P .p .i . k . h .h .It is worth noting that if the binary branching structure developed by the parser were always right - branching and the POStag and non - terminal label vocabularies were mapped to a single type then the model would be equivalent to a trigram language model .", "label": "", "metadata": {}, "score": "45.70271"}
{"text": ", f n ] where n is the number of tokens in the span .The label assigned to each token will depend upon the entity containing the token .For example , assuming that job titles , person names , and organization names are labeled as distinct entities during the entity labeling process of FIG .", "label": "", "metadata": {}, "score": "45.763702"}
{"text": ", f n ] where n is the number of tokens in the span .The label assigned to each token will depend upon the entity containing the token .For example , assuming that job titles , person names , and organization names are labeled as distinct entities during the entity labeling process of FIG .", "label": "", "metadata": {}, "score": "45.763702"}
{"text": "This is a special case of association in which the entities being associated shared the same label ( \" name \" in this case ) , hence the entire association procedure described above applies .Feature extraction for normalization may be facilitated by performing sub - entity extraction first .", "label": "", "metadata": {}, "score": "45.76997"}
{"text": "This is a special case of association in which the entities being associated shared the same label ( \" name \" in this case ) , hence the entire association procedure described above applies .Feature extraction for normalization may be facilitated by performing sub - entity extraction first .", "label": "", "metadata": {}, "score": "45.76997"}
{"text": "3 ) and for which a frame label 222 and slot labels 224 and 226 have been added by annotation .As discussed with respect to frame 200 above , frame label 222 indicates the overall action being referred to by the input sentence .", "label": "", "metadata": {}, "score": "45.802456"}
{"text": "The SLM is now described in a bit greater detail for the sake of completeness .The model assigns a probability P(W , T ) to every sentence W and its every possible binary parse T. The terminals of T are the words of W with part of speech tags ( POStags ) , and the nodes of T are annotated with phrase headwords and non - terminal labels .", "label": "", "metadata": {}, "score": "45.81392"}
{"text": "This should be done in all data or tables that mention such syntactic categories .If you roll your own rule tables ( or use an unknown - lhs file ) , you should make sure they are properly escaped .Most SCFG - based machine translation decoders at the current time are designed to uses hierarchical phrase - based grammar ( Chiang , 2005 ) or syntactic grammar .", "label": "", "metadata": {}, "score": "46.09986"}
{"text": "Referring back again to FIG .5 , entity extraction step 140 requires the extraction of entities of interest from the spans identified at step 130 .As shown in FIG .7 , each individual entity must be automatically identified and segmented from the text of the surrounding span .", "label": "", "metadata": {}, "score": "46.110237"}
{"text": "Referring back again to FIG .5 , entity extraction step 140 requires the extraction of entities of interest from the spans identified at step 130 .As shown in FIG .7 , each individual entity must be automatically identified and segmented from the text of the surrounding span .", "label": "", "metadata": {}, "score": "46.110237"}
{"text": "In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ( ACL ) , 2005 .Vincent Ng and Claire Cardie .Bootstrapping coreference classifiers with multiple machine learning algorithms .In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing ( EMNLP03 ) , 2003 .", "label": "", "metadata": {}, "score": "46.12159"}
{"text": "FIG .6 , the commodity constituent mapping is a simple tree consisting of two sub - commodities with three constituents categorized or related to sub - commodity 1 and one constituent related to sub - commodity 2 .The quality engineer or other professional has assigned four specific dimensions to be analyzed directly .", "label": "", "metadata": {}, "score": "46.126102"}
{"text": "The associated entities and their type ( label ) are stored 450 .Normalization labeling is similar to association labeling in that it involves grouping multiple labeled entities together , however unlike association labeling it involves grouping entities of the same type together .", "label": "", "metadata": {}, "score": "46.27351"}
{"text": "The associated entities and their type ( label ) are stored 450 .Normalization labeling is similar to association labeling in that it involves grouping multiple labeled entities together , however unlike association labeling it involves grouping entities of the same type together .", "label": "", "metadata": {}, "score": "46.27351"}
{"text": "Some users have reported that category symbols were mangled ( by splitting them at the square brackets ) after converting to an on - disk representation ( and potentially in other scenarios -- this is currently an open issue ) .A way to side - step this issue is to escape square brackets with a symbol that is not part of the meta - language of the grammar files , e.g. using the underscore symbol : . and .", "label": "", "metadata": {}, "score": "46.306107"}
{"text": "A method for extracting a structured record ( 190 ) from a document ( 100 ) is described where the the structured record includes information related to a predetermined subject matter ( 120 ) , with this information being organized into categories within the structured record .", "label": "", "metadata": {}, "score": "46.350754"}
{"text": "p .i . k . ) . ] where : .W k is the word predicted by a word - predictor component ; .t k is the tag assigned to wk by a POS tagger component ; .N k -1 is the number of operations the parser executes at sentence position k before passing control to the word - predictor ( the N k -th operation at position k is the null transition ) N k is a function of T ; .", "label": "", "metadata": {}, "score": "46.399776"}
{"text": "i P(P i , W ) ) .This is indicated by blocks 320 and 322 in .FIG .9 .Also , however , ranking component 316 can sum the probability of a semantic parse over all of the parses P that yield the same semantic parse S , and then choose the top N semantic parses with the highest associated probabilities .", "label": "", "metadata": {}, "score": "46.4377"}
{"text": "2 illustrates one simplified embodiment of a template or frame that may be found in an application schema .FIG .3 illustrates a parse generated by a structured language model .FIG .4 illustrates a word - parse k - prefix .", "label": "", "metadata": {}, "score": "46.663662"}
{"text": "These records would then be queried as if one were simply querying a database , with the results being returned as lists of structured records rather than web pages .There have been a number of attempts to provide this type of searching functionality .", "label": "", "metadata": {}, "score": "46.69995"}
{"text": "These records would then be queried as if one were simply querying a database , with the results being returned as lists of structured records rather than web pages .There have been a number of attempts to provide this type of searching functionality .", "label": "", "metadata": {}, "score": "46.69995"}
{"text": "This modeling approach also provides the ability to change the time history of the data being analyzed , providing additional flexibility in performance monitoring across the dimension of time .Integration of this data modeling approach into an existing quality management system produces a dynamic ability to facilitate real - time quality performance monitoring and control .", "label": "", "metadata": {}, "score": "46.90408"}
{"text": "The attributes assigned during this phase are only used when CCM L 2 selection is made on a particular node in the tree .It is important to note that all analytics are derived from the detailed constituents where the actual performance occurs .", "label": "", "metadata": {}, "score": "46.925014"}
{"text": "For instance left - binarization adds two additional nodes and converts the subtree into : .The additional node with the label ^NP allows for the straight - forward extraction of a translation rule ( of course , unless the word alignment does not provide a consistent alignment ) .", "label": "", "metadata": {}, "score": "47.01887"}
{"text": "Dan Klein and Christopher D. Manning .A Generative Constituent - Context Model for Improved Grammar Induction .Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pp .128 - 135 .Dan Klein and Christopher D. Manning .", "label": "", "metadata": {}, "score": "47.075043"}
{"text": "Tree - based decoding builds a chart , which consists of partial translation for all possible spans over the input sentence .Currently Moses implements a CKY+ algorithm for arbitrary number of non - terminals per rule and an arbitrary number of types of non - terminals in the grammar .", "label": "", "metadata": {}, "score": "47.192425"}
{"text": "The training data used by the machine learning algorithms consists of one example for each labeled entity from the positively labeled training documents .The training procedure is similar to that used to extract entities from within spans , and with some simplification may be described as the same process with \" span \" replaced by \" entity \" and \" entity \" replaced by \" sub - entity \" .", "label": "", "metadata": {}, "score": "47.200966"}
{"text": "The training data used by the machine learning algorithms consists of one example for each labeled entity from the positively labeled training documents .The training procedure is similar to that used to extract entities from within spans , and with some simplification may be described as the same process with \" span \" replaced by \" entity \" and \" entity \" replaced by \" sub - entity \" .", "label": "", "metadata": {}, "score": "47.200966"}
{"text": "In this manner , the algorithms obtain the benefit of the domain knowledge contained in the rules but can also use the labeled data to find the appropriate weighting to assign to these rules .As is known in the art , the application of machine learning algorithms requires hand - labeling example data of interest , extracting features from the labeled data , and then training classifiers and extractors based on these features and labels .", "label": "", "metadata": {}, "score": "47.32714"}
{"text": "In this manner , the algorithms obtain the benefit of the domain knowledge contained in the rules but can also use the labeled data to find the appropriate weighting to assign to these rules .As is known in the art , the application of machine learning algorithms requires hand - labeling example data of interest , extracting features from the labeled data , and then training classifiers and extractors based on these features and labels .", "label": "", "metadata": {}, "score": "47.32714"}
{"text": "In the case of a trained Markov model , a label sequence is assigned or computed for an input feature - vector - sequence by choosing the most probable sequence using Viterbi decoding .However , the label sequence may not distinguish the boundaries between adjacent entities of interest .", "label": "", "metadata": {}, "score": "47.32947"}
{"text": "In the case of a trained Markov model , a label sequence is assigned or computed for an input feature - vector - sequence by choosing the most probable sequence using Viterbi decoding .However , the label sequence may not distinguish the boundaries between adjacent entities of interest .", "label": "", "metadata": {}, "score": "47.32947"}
{"text": "If , on the other hand , all the attributes have been assigned , the second phase begins .The second phase establishes dimensions for analyzing each commodity .At step 310 , the commodity tree created by steps 302 - 308 is examined to determine which nodes or level in the commodity tree are to be analyzed .", "label": "", "metadata": {}, "score": "47.37964"}
{"text": "However , this may be even more confusing so we will stick with our convention for now . )RECAP - Grammar rules in Moses have 2 labels for each non - terminals ; one to constrain the non - terminal to the input parse tree , the other is used in parsing .", "label": "", "metadata": {}, "score": "47.446796"}
{"text": "However , as would also be apparent to those skilled in the art , the method of extracting structural records according to the present invention is equally applicable to generating structural records from any text based source .Accordingly , the goal of the extraction process is to process the web pages in a corporate web site ; locate the biographical pages such as the one shown in FIG .", "label": "", "metadata": {}, "score": "48.011467"}
{"text": "However , as would also be apparent to those skilled in the art , the method of extracting structural records according to the present invention is equally applicable to generating structural records from any text based source .Accordingly , the goal of the extraction process is to process the web pages in a corporate web site ; locate the biographical pages such as the one shown in FIG .", "label": "", "metadata": {}, "score": "48.011467"}
{"text": "Most of the data I 'll be using will be stored as \" Tag \" objects ( or annotations ) inside an ADB instance .Each Tag object has a \" type \" , as well as \" source \" which help identify what it is and where it came from .", "label": "", "metadata": {}, "score": "48.021797"}
{"text": "Note that at this point , the non - terminal X , which covers the input span over schnell is replaced by a known translation quickly .Finally , the glue rule ( 4 ) X 1 X 2 to X 1 X 2 combines the two fragments into a complete sentence .", "label": "", "metadata": {}, "score": "48.03634"}
{"text": "In addition , the system can access the schema associated with the application program .During run time , the system can discard parsed hypotheses ( as they are being generated ) if they are found to violate the structure imposed by the schema .", "label": "", "metadata": {}, "score": "48.113194"}
{"text": "SUMMARY OF THE INVENTION . identifying a span of text in said document according to criteria associated with said predetermined subject matter ; and . processing said span of text to extract at least one text element associated with at least one of said categories of said structured record from said document .", "label": "", "metadata": {}, "score": "48.162193"}
{"text": "SUMMARY OF THE INVENTION . identifying a span of text in said document according to criteria associated with said predetermined subject matter ; and . processing said span of text to extract at least one text element associated with at least one of said categories of said structured record from said document .", "label": "", "metadata": {}, "score": "48.162193"}
{"text": "( 2012 ) , which is based on cross - lingual word cluster features .First , we show that by using multiple source languages , combined with self - training for target language adaptation , we can achieve significant improvements compared to using only single source direct transfer .", "label": "", "metadata": {}, "score": "48.2098"}
{"text": "FIG .3 is one illustrative syntactic parse of the example input sentence .The vertical line in the input sentence illustrates the place at which processing is to commence .The SLM percolates a headword up to each node in the syntactic parse , wherein the headword is a word that most closely defines that constituent of the sentence .", "label": "", "metadata": {}, "score": "48.395584"}
{"text": "A sub - commodity refers to a more granular grouping of a commodity used to detail the sub - elements of a commodity .A sub - commodity can have multiple sub - commodities identified within it ( i.e. , nested sub - commodities ) to define things such as classification , reliability , physical characteristics , naming convention , supplier or marketing preferences or any other suitable attributes .", "label": "", "metadata": {}, "score": "48.51805"}
{"text": "10 is a data flow diagram also illustrating the operation of the present system during run - time or test - time , in accordance with one embodiment of the present invention .FIG .11 is a block diagram of a speech recognition system employing a structured language model in accordance with one embodiment of the present invention .", "label": "", "metadata": {}, "score": "48.54308"}
{"text": "] ) , .rule application probability ( 2.526 ) , and .prior hypotheses , i.e. the children nodes in the tree , that this hypothesis is built on ( 20 ) .As you can see , the model used here is a target - syntax model , It uses linguistic syntactic annotation on the target side , but on the input side everything is labeled X .", "label": "", "metadata": {}, "score": "48.878685"}
{"text": "8) , or from a statistical sample thereof .The extracted features and associated labels are stored in a training index .Once these features are extracted , many existing methods for training document classifiers may be applied , including decision trees , and various forms of linear classifier , including maximum entropy .", "label": "", "metadata": {}, "score": "49.20089"}
{"text": "8) , or from a statistical sample thereof .The extracted features and associated labels are stored in a training index .Once these features are extracted , many existing methods for training document classifiers may be applied , including decision trees , and various forms of linear classifier , including maximum entropy .", "label": "", "metadata": {}, "score": "49.20089"}
{"text": "Right - binarization creates a right - branching tree .--SAMT 1 : Combines pairs of neighboring children nodes into tags , such as DET+ADJ .Also nodes for everything except the first child ( NP\\\\DET ) and everything except the last child ( NP / NN ) are added .", "label": "", "metadata": {}, "score": "49.239723"}
{"text": "The Hiero / Joshua / cdec file format is sufficient for hierarchical models , but not for the various syntax models supported by Moses .Bracketing induction is the unsupervised learning of hierarchical constituents without labeling their syntactic categories such as verb phrase ( VP ) from natural raw sentences .", "label": "", "metadata": {}, "score": "49.287956"}
{"text": "The nodes of the commodity tree are assigned uniform attributes and dimensional attributes .These attributes are then inherited down the commodity tree to all applicable dependent nodes .Commodity constituent model Level 1 ( CCM L 1 ) .A set of attributes that defines the uniform characteristics of interest associated with nodes for a particular commodity and for which analytics are to be performed .", "label": "", "metadata": {}, "score": "49.292152"}
{"text": "The method of .claim 1 wherein identifying comprises : . generating a probability that generated overall parses occur given a word sequence ; . summing the probability over all parses having a common semantic parse ; and . selecting the semantic parse based on the summed probability .", "label": "", "metadata": {}, "score": "49.346767"}
{"text": "6 .The data modeling approach described above delivers multi - dimensional flexibility in quality data analysis without the need for extensive program hard - coding or rewrites .This invention is readily integrated into existing quality management systems .As a result , this invention becomes an enabler for dynamic queries of quality data in near real - time .", "label": "", "metadata": {}, "score": "49.351753"}
{"text": "a second commodity constituent model created by selectively assigning at least one dimensional attribute to a node in the hierarchy that is used in the analytics performed by the analytic engine ; . wherein dependent nodes inherit dimensional attributes assigned to corresponding upper level nodes ; and .", "label": "", "metadata": {}, "score": "49.440884"}
{"text": "In order to initialize the SLM , all that is needed is the syntactic portion of the annotated training data .In that case , a general purpose parser can be used to generate a syntactic tree bank from which the SLM parameters can be initialized .", "label": "", "metadata": {}, "score": "49.458534"}
{"text": "A feature vector is extracted 1320 from each entity pair .Any feature of an entity pair that will help distinguish associated entities from non - associated entities and can be automatically computed should be considered .All association pairs that are not positively labeled are assigned the \" not - associated \" or \" other \" label .", "label": "", "metadata": {}, "score": "49.502205"}
{"text": "A feature vector is extracted 1320 from each entity pair .Any feature of an entity pair that will help distinguish associated entities from non - associated entities and can be automatically computed should be considered .All association pairs that are not positively labeled are assigned the \" not - associated \" or \" other \" label .", "label": "", "metadata": {}, "score": "49.502205"}
{"text": "In this tutorial , we refer to un - annotated trees as trees , and to trees with syntactic annotation as syntax .So a so - called string - to - tree model is here called a target - syntax model .", "label": "", "metadata": {}, "score": "49.621128"}
{"text": "Once all of the attributes have been established for each commodity , the resulting commodity tree is created at step 316 and may be immediately utilized by analytic engine 106 to search , analyze and indicate the results of the intended mapping at step 318 .", "label": "", "metadata": {}, "score": "49.82994"}
{"text": ".. rget languages in which no or few such resources are available ( Hwa et al . , 2005 ) .Figure 1 : Cross - lingual word cluster features for parsing .Top - left : Cross - lingual ( EN - ES ) word clustering model .", "label": "", "metadata": {}, "score": "49.840736"}
{"text": "The task is restricted to parsing only .We will not evaluate POS tags for parsers that assign them automatically as part of the parsing process .An exemplary embodiment of the invention relates to a method , system , and storage medium for providing a dynamic multi - dimensional commodity modeling process .", "label": "", "metadata": {}, "score": "49.96771"}
{"text": "Any classifier training algorithm will do , including hand - building rule - based algorithms although automated methods usually perform better .The output 1350 of all these algorithms is a trained classifier that assigns either the \" associated \" or \" not - associated \" label to a feature vector from an entity pair .", "label": "", "metadata": {}, "score": "50.224503"}
{"text": "Any classifier training algorithm will do , including hand - building rule - based algorithms although automated methods usually perform better .The output 1350 of all these algorithms is a trained classifier that assigns either the \" associated \" or \" not - associated \" label to a feature vector from an entity pair .", "label": "", "metadata": {}, "score": "50.224503"}
{"text": "The feature extraction module 408 produces a stream of feature vectors that are each associated with a frame of the speech signal .If the input signal is a training signal , this series of feature vectors is provided to a trainer 410 , which uses the feature vectors and a training text 412 to train an acoustic model 414 .", "label": "", "metadata": {}, "score": "50.267227"}
{"text": "FIGS .5 - 6 .FIG .5 shows the result of an adjoin - left operation and .FIG .6 shows the result of an adjoin - right operation .These operations ensure that all possible binary branching parses with all possible head - word and non - terminal label assignments for the w 1 . . .", "label": "", "metadata": {}, "score": "50.296043"}
{"text": "( Some papers by people at ISI inverted this naming convention due to their adherance to the noisy - channel framework ) .The implementation of string - to - tree models is fairly standard and similar across different open - source decoders such as Moses , Joshua , cdec and Jane .", "label": "", "metadata": {}, "score": "50.39065"}
{"text": "The method of .claim 1 wherein identifying comprises : . generating a probability that generated overall parses occur given a word sequence ; . selecting an overall parse generated during parsing that has a highest probability of occurring ; and .", "label": "", "metadata": {}, "score": "50.41238"}
{"text": "Process 116 takes the analytics output by process 114 and performs a secondary analysis that looks for patterns and trends , which are also defined by the commodity constituent model , identifying those constituents that match the patterns and trends ( referred to as nonconformances ) .", "label": "", "metadata": {}, "score": "50.414413"}
{"text": "As a simple example of this shift process , consider the following portion of a tokenized biographical span : .Assuming that \" Jonathan \" is present in a first - name list and that the first occurrence of Jonathan in the span portion is also the first occurrence of \" Jonathan \" within the surrounding document , the feature - vector for the first \" Jonathan \" token would be : .", "label": "", "metadata": {}, "score": "50.48679"}
{"text": "As a simple example of this shift process , consider the following portion of a tokenized biographical span : .Assuming that \" Jonathan \" is present in a first - name list and that the first occurrence of Jonathan in the span portion is also the first occurrence of \" Jonathan \" within the surrounding document , the feature - vector for the first \" Jonathan \" token would be : .", "label": "", "metadata": {}, "score": "50.48679"}
{"text": "i . s .t .SEM .P .i . )S .P .P .i .W . ) . . .This is indicated by block 324 in .FIG .9 .One additional advantage of the present invention is that the SLM is not simply a parser , but is actually a language model that can be used in a speech recognition system .", "label": "", "metadata": {}, "score": "50.53128"}
{"text": "The system also includes a closed loop / corrective action component operable for resolving nonconformance issues resulting from analysis of the raw data and commodity tree , and an analytic engine in communication with the data collection component , the multi - dimensional commodity model component , and the closed loop / corrective action component .", "label": "", "metadata": {}, "score": "50.581753"}
{"text": "10 , the next step in the labeling process is to label the entities of interest within each previously labeled span of interest .The locations of the boundaries of each entity within each span , and the category ( label ) of each entity ( name , jobtitle , organization , etc ) are then further stored 390 .", "label": "", "metadata": {}, "score": "50.59411"}
{"text": "10 , the next step in the labeling process is to label the entities of interest within each previously labeled span of interest .The locations of the boundaries of each entity within each span , and the category ( label ) of each entity ( name , jobtitle , organization , etc ) are then further stored 390 .", "label": "", "metadata": {}, "score": "50.59411"}
{"text": "Second , and more interestingly , we provide an algorithm for inducing cross - lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross - lingual structure prediction .Specifically , we show that by augmenting direct - transfer systems with cross - lingual cluster features , the relative error of delexicalized dependency parsers , trained on English treebanks and transferred to foreign languages , can be reduced by up to 13 % .", "label": "", "metadata": {}, "score": "50.60649"}
{"text": "Although the present invention has been described with reference to particular embodiments , workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention .A method for extracting a structured record ( 190 ) from a document ( 100 ) is described where the the structured record includes information related to a predetermined subject matter ( 120 ) , with this information being organized into categories within the structured record .", "label": "", "metadata": {}, "score": "50.81424"}
{"text": "Our primary focus is on grammar induction , which aims to find the hierarchical structure of natural language .Grammar search methods have met with little success , and simple distributional approaches that work for part - of - speech induction do not directly apply .", "label": "", "metadata": {}, "score": "50.825573"}
{"text": "Additionally the performance data for a supplier sourced assembly may include the details of the assembly or could be a ' pass / fail ' of the assembly itself .Multi - dimensional commodity model component 104 provides an efficient method to dynamically select what data ( e.g. , commodities , sub - commodities , constituents ) to analyze , how to analyze it , and what patterns and trends to look for .", "label": "", "metadata": {}, "score": "50.879295"}
{"text": "Abstract .One feature of the present invention uses the parsing capabilities of a structured language model in the information extraction process .During training , the structured language model is first initialized with syntactically annotated training data .The model is then trained by generating parses on semantically annotated training data enforcing annotated constituent boundaries .", "label": "", "metadata": {}, "score": "50.944508"}
{"text": "Confidence measure module 422 then provides the sequence of hypothesis words to an output module 424 along with identifiers indicating which words may have been improperly identified .Those skilled in the art will recognize that confidence measure module 422 is not necessary for the practice of the present invention .", "label": "", "metadata": {}, "score": "51.090282"}
{"text": "This feature vector sequence can be associated with a label sequence and in combination these two sequences may be used to train predictive algorithms which may then be applied accordingly to other documents .Referring once again to FIG .5 , entity association step 170 requires the automatic association of entities identified at step 150 .", "label": "", "metadata": {}, "score": "51.144894"}
{"text": "This feature vector sequence can be associated with a label sequence and in combination these two sequences may be used to train predictive algorithms which may then be applied accordingly to other documents .Referring once again to FIG .5 , entity association step 170 requires the automatic association of entities identified at step 150 .", "label": "", "metadata": {}, "score": "51.144894"}
{"text": "( Using this notation is probably wrong as the source sentence is not properly parsed - see next section .It may be better to express the Moses tree - to - string grammar as a hierarchical grammar , with added constraints .", "label": "", "metadata": {}, "score": "51.228355"}
{"text": "The particular method used for decoding is not important to the present invention and any of several known methods for decoding may be used .The most probable sequence of hypothesis words is illustratively provided to a confidence measure module 422 .", "label": "", "metadata": {}, "score": "51.462852"}
{"text": "perl , you will have to specify additional parameters , e.g. -hierarchical and -glue - grammar .You typically will also reduce the number of lexical items in the grammar with -max - phrase - length 5 .There are a number of additional decisions about the type of rules you may want to include in your model .", "label": "", "metadata": {}, "score": "51.552624"}
{"text": "The locations of the boundaries of each sub - entity within each entity , and the sub - entity category ( label ) are stored 420 .Association labeling involves grouping multiple labeled entities of different types together , for example jobtitle with organization , or degree with school .", "label": "", "metadata": {}, "score": "51.574883"}
{"text": "The locations of the boundaries of each sub - entity within each entity , and the sub - entity category ( label ) are stored 420 .Association labeling involves grouping multiple labeled entities of different types together , for example jobtitle with organization , or degree with school .", "label": "", "metadata": {}, "score": "51.574883"}
{"text": "An individual accesses the multi - dimensional commodity model 104 of QMS 100 at step 302 .Initial assignment of commodities and sub - commodities to constituents is performed at step 304 .All attributes that provide uniform characteristics during the CCM L 1 phase to each commodity tree are established at step 306 .", "label": "", "metadata": {}, "score": "51.608246"}
{"text": "In Proceedings of the 17th International Conference on Machine Learning ( ICML ) , pages 327 - -334 , 2000 .Sanda Harabagiu , Razvan Bunescu , and Steven Maiorano .Text and knowledge mining for coreference resolution .In Proceedings of the 2nd Meeting of the North American Chapter of the Association of Computational Linguistics ( NAACL2001 ) , pages 55 - -62 , 2001 .", "label": "", "metadata": {}, "score": "51.709663"}
{"text": "frequency counts of source & target phrase ( for debugging purposes ; not used during decoding ) .The format is slightly different from the Hiero format .For example , the Hiero rule .It completes a translation with of a sentence span ( S ) .", "label": "", "metadata": {}, "score": "51.749973"}
{"text": "5 , the step of span extraction 130 , requires the automatic extraction of spans of interest from classified positive documents .With reference to FIGS . 2 and 6 , the text of each individual biography is automatically identified and segmented from the surrounding text .", "label": "", "metadata": {}, "score": "51.929657"}
{"text": "5 , the step of span extraction 130 , requires the automatic extraction of spans of interest from classified positive documents .With reference to FIGS . 2 and 6 , the text of each individual biography is automatically identified and segmented from the surrounding text .", "label": "", "metadata": {}, "score": "51.929657"}
{"text": "For example , when checking whether the following rule can be applied .the decoder will check whether the RHS non - terminal , and the whole rule , spans an input parse constituent X. Therefore , even when decoding with a string - to - string or string - to - tree grammar , it is necessary to add the X non - terminal to every input span .", "label": "", "metadata": {}, "score": "51.948364"}
{"text": "One feature of the present invention uses the parsing capabilities of a structured language model in the information extraction process .During training , the structured language model is first initialized with syntactically annotated training data .The model is then trained by generating parses on semantically annotated training data enforcing annotated constituent boundaries .", "label": "", "metadata": {}, "score": "51.994377"}
{"text": "The structured language model is set out in greater detail in Ciprian Chelba and Frederick Jelinek , Structured Language Modeling , Computer Speech and Language , 14(4):283 - 332 , October 2000 ; and Chelba , Exploiting Syntactic Structure for Natural Language Modeling , Ph.D. Dissertation , Johns Hopkins University , 2000 .", "label": "", "metadata": {}, "score": "52.027554"}
{"text": "We then show how the parser can be relexicalized and adapted using unlabeled target language data and a learning method that can incorporate diverse knowledge sources through ambiguous labelings .In the latter scenario , we exploit two sources of knowledge : arc marginals derived from the base parser in a self - training algorithm , and arc predictions from multiple transfer parsers in an ensemble - training algorithm .", "label": "", "metadata": {}, "score": "52.039917"}
{"text": "In an alternative embodiment , a user may access the multi - dimensional commodity model 104 at step 320 and bypass the CCM L 1 phase definitions if desired .This may be desirable where attributes for CCM L 1 have already been established and it is not necessary to access these features .", "label": "", "metadata": {}, "score": "52.06016"}
{"text": "20 is a flowchart illustrating the steps involved in running a trained sub - entity extractor according to a preferred embodiment of the present invention ; .FIG .21 is a flowchart illustrating the steps involved in training an associator to associate entities from labeled documents according to a preferred embodiment of the present invention ; .", "label": "", "metadata": {}, "score": "52.18901"}
{"text": "20 is a flowchart illustrating the steps involved in running a trained sub - entity extractor according to a preferred embodiment of the present invention ; .FIG .21 is a flowchart illustrating the steps involved in training an associator to associate entities from labeled documents according to a preferred embodiment of the present invention ; .", "label": "", "metadata": {}, "score": "52.18901"}
{"text": "The largest improvement is achieved on the non Indo - European languages yielding a gain of 14.4 % . ... parser trained using parallel data .The underlying parsing model is the dependency model with valance ( DMV ) ( Klein and Manning , 2004 ) .", "label": "", "metadata": {}, "score": "52.198708"}
{"text": "A node refers to any level of grouping of commodities , sub - commodities , or constituents in a commodity tree for the purpose of performing an analysis .A commodity tree refers to a hierarchical data structure for a commodity including associated sub - commodities and constituents and is created by the multi - dimensional commodity model process .", "label": "", "metadata": {}, "score": "52.219467"}
{"text": "Moses supports models that have become known as hierarchical phrase - based models and syntax - based models .These models use a grammar consisting of SCFG ( Synchronous Context - Free Grammar ) rules .In the following , we refer to these models as tree - based models .", "label": "", "metadata": {}, "score": "52.23836"}
{"text": "Or if more than one person is mentioned in the biography , each normalized person may need to be classified as to whether they are the subject of the biography or not .These three classification tasks may be grouped together because they all possess a similar structure .", "label": "", "metadata": {}, "score": "52.243225"}
{"text": "Or if more than one person is mentioned in the biography , each normalized person may need to be classified as to whether they are the subject of the biography or not .These three classification tasks may be grouped together because they all possess a similar structure .", "label": "", "metadata": {}, "score": "52.243225"}
{"text": "--NoFractionalCounting: For any given source span , any number of rules can be generated .By default , fractional counts are assigned , so probability of these rules adds up to one .This option leads to the count of one for each rule .", "label": "", "metadata": {}, "score": "52.2481"}
{"text": "So , when we build a data structure called a tree ( as Computer Scientist call it ) , do we mean that we build a linguistic syntax tree ( as Linguists call it ) ?Not always , and hence the confusion .", "label": "", "metadata": {}, "score": "52.270515"}
{"text": "The chart decoder has an implementation of CKY decoding using cube pruning .The latter means that only a fixed number of hypotheses are generated for each span .This number can be changed with the option cube - pruning - pop - limit ( or short cbp ) .", "label": "", "metadata": {}, "score": "52.33689"}
{"text": "The inheritance properties of these models , along with their two - level design , provide virtually unlimited capability to analyze a variety of attributes and dimensions , to change these attributes and dimensions at will , while maintaining uniform characteristics for each commodity .", "label": "", "metadata": {}, "score": "52.36691"}
{"text": "Apart from these deviations , the method of training a sub - entity extractor parallels that for training an entity extractor .Similarly , the procedure for applying the trained sub - entity extractor to extract sub - entities as illustrated in FIG .", "label": "", "metadata": {}, "score": "52.38748"}
{"text": "Apart from these deviations , the method of training a sub - entity extractor parallels that for training an entity extractor .Similarly , the procedure for applying the trained sub - entity extractor to extract sub - entities as illustrated in FIG .", "label": "", "metadata": {}, "score": "52.38748"}
{"text": "Bigger rule tables have a negative impact on memory use and speed of the decoder .There are two parts to create a rule table : the extraction of rules and the scoring of rules .perl .--MaxSpan SIZE : maximum span size of the rule .", "label": "", "metadata": {}, "score": "52.5155"}
{"text": "3 .The first phase of the process is to establish the base commodity hierarchy which defines the relationships between commodities , sub - commodities ( as well as nested sub - commodities , if desired ) , and the constituents that are assigned to those sub - commodities .", "label": "", "metadata": {}, "score": "52.529552"}
{"text": "Referring to FIG .5 , the extracted records are then stored 210 in a database and indexed 220 for search , so that records may be retrieved by querying on different extracted fields such as name , job title , etc . .", "label": "", "metadata": {}, "score": "52.604256"}
{"text": "Referring to FIG .5 , the extracted records are then stored 210 in a database and indexed 220 for search , so that records may be retrieved by querying on different extracted fields such as name , job title , etc . .", "label": "", "metadata": {}, "score": "52.604256"}
{"text": "During test or run time , one embodiment of the present invention constrains the parser with the semantic schema such that the parser only considers parses having structures which do not violate the structures in the semantic schema .In addition , the output of one embodiment of the present invention is a desired number of semantic parse trees which are summed over all parse trees that have the same semantic annotation .", "label": "", "metadata": {}, "score": "52.660294"}
{"text": "Communication media typically embodies computer readable instructions , data structures , program modules or other data in a modulated data signal such as a carrier WAV or other transport mechanism and includes any information delivery media .The term \" modulated data signal \" means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal .", "label": "", "metadata": {}, "score": "52.86535"}
{"text": "Building A Large Annotated Corpus of English : The Penn Tree Bank , Computational Linguistics , 19(2):313 - 330(1993 ) ) .The next step in training is to train the model by constraining parses generated during the training to enforce the annotated constituent boundaries .", "label": "", "metadata": {}, "score": "52.952618"}
{"text": "In the Penn Treebank , transitions between upper- and lowercase tokens tend to align with the boundaries of base ( English ) noun phrases .Such signals can be used as partial bracketing constraints to train a grammar inducer : in our experiments , directed dependency accuracy increased by 2.2 % ( average over 14 languages having case information ) .", "label": "", "metadata": {}, "score": "52.960274"}
{"text": "FIG .1 .However , the present invention can be carried out on a server , a computer devoted to message handling , or on a distributed system in which different portions of the present invention are carried out on different parts of the distributed computing system .", "label": "", "metadata": {}, "score": "52.99201"}
{"text": "To make the conversion efficient , the text rule table must have the following properties : .For every rule , the sequence of terminals and non - terminals in the first column ( the ' source ' column ) should match the lookup sequence that the decoder will perform .", "label": "", "metadata": {}, "score": "53.150585"}
{"text": "Overview .Data produced by this module will be somewhat low - level ; Annotations sitting directly on top of documents stored in an Annotations Database ( ADB ) instance will encode referential phrases and their antecedent phrases .No attempt will be made to link these phrases to higher level entities in some knowledge base or concepts in some ontology .", "label": "", "metadata": {}, "score": "53.235477"}
{"text": "Thus in the example queries outlined above which relate to different domains or areas of interests such as employment , corporate information or even obituaries , the extraction systems must be customised according to the expected query .Clearly , this has a number of disadvantages as extraction systems of this type must each be developed and tuned separately depending on the expected query type .", "label": "", "metadata": {}, "score": "53.32306"}
{"text": "Thus in the example queries outlined above which relate to different domains or areas of interests such as employment , corporate information or even obituaries , the extraction systems must be customised according to the expected query .Clearly , this has a number of disadvantages as extraction systems of this type must each be developed and tuned separately depending on the expected query type .", "label": "", "metadata": {}, "score": "53.32306"}
{"text": "Another setting that directly affects speed is the number of rules that are considered for each input left hand side .It can be set with ttable - limit .The number of spans that are filled during chart decoding is quadratic with respect to sentence length .", "label": "", "metadata": {}, "score": "53.345757"}
{"text": "20 .The main deviation points from applying an entity extractor are : .the model operates over feature - vector sequences 1130 constructed from the tokens in each entity , not the tokens from the entire span ; . feature extraction 1120 for the tokens within each entity will be the same as that used when generating the training features for subentity extraction ; .", "label": "", "metadata": {}, "score": "53.37925"}
{"text": "20 .The main deviation points from applying an entity extractor are : .the model operates over feature - vector sequences 1130 constructed from the tokens in each entity , not the tokens from the entire span ; . feature extraction 1120 for the tokens within each entity will be the same as that used when generating the training features for subentity extraction ; .", "label": "", "metadata": {}, "score": "53.37925"}
{"text": "It is important to note in this example that it is highly unusual for there to be no boundaries between unrelated text .In particular , it would almost never be the case that a single text node contained more than one biography , or obituary , or job , etc . .", "label": "", "metadata": {}, "score": "53.44255"}
{"text": "It is important to note in this example that it is highly unusual for there to be no boundaries between unrelated text .In particular , it would almost never be the case that a single text node contained more than one biography , or obituary , or job , etc . .", "label": "", "metadata": {}, "score": "53.44255"}
{"text": "Their principal advantage is that they require less explicit domain knowledge .Machine learning algorithms essentially infer domain knowledge from the labeled examples .In contrast , the use of purely hand - coded rules requires an engineer or scientist to explicitly identify and hand - code prior domain knowledge , thereby adding to the expense and development time of extraction tools based on these methods .", "label": "", "metadata": {}, "score": "53.450222"}
{"text": "Their principal advantage is that they require less explicit domain knowledge .Machine learning algorithms essentially infer domain knowledge from the labeled examples .In contrast , the use of purely hand - coded rules requires an engineer or scientist to explicitly identify and hand - code prior domain knowledge , thereby adding to the expense and development time of extraction tools based on these methods .", "label": "", "metadata": {}, "score": "53.450222"}
{"text": "Referring once again to FIG .5 , classification of \" Entities / Associated Entities / Normalized Entities \" at step 180 requires the automatic classification of entities , associated entities , and normalized entities identified at steps 150 , 170 and 175 respectively .", "label": "", "metadata": {}, "score": "53.589867"}
{"text": "Referring once again to FIG .5 , classification of \" Entities / Associated Entities / Normalized Entities \" at step 180 requires the automatic classification of entities , associated entities , and normalized entities identified at steps 150 , 170 and 175 respectively .", "label": "", "metadata": {}, "score": "53.589867"}
{"text": "Learning noun phrase anaphoricity to improve coreference resolution : Issues in representa- tion and optimization .In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ( ACL04 ) , pages 152 - -159 , 2004 .Vincent Ng .", "label": "", "metadata": {}, "score": "53.640076"}
{"text": "Information extraction from text can be characterized as a template filling process .In other words , a given template or frame contains a certain number of slots that need to be filled in with segments of text .The label of the frame corresponds to a high level of understanding , such as the particular action being requested by the user .", "label": "", "metadata": {}, "score": "53.682236"}
{"text": "This process includes examining the entire commodity tree and selecting which nodes ( e.g. , level in the tree or model ) and what patterns or trends at that level within the tree will be assigned dimensional attributes for analysis .The level 2 process updates tertiary trees created in the first part of the CCM process ( Level 1 ) 202 with the dimensions and trends specified for each node within the tree as demonstrated in .", "label": "", "metadata": {}, "score": "53.73156"}
{"text": "A feature vector is extracted 920 from each token in the span .Any feature of a token that will help distinguish entities from the surrounding text and can be automatically computed should be considered .Some other examples of features that are particularly suited for biographical span and entity extraction include : . features indicating that a text node contains a first name or surname , computed by looking all the text node tokens up in a list of first - names or surnames ; . features indicating that a text node contains only a first name or surname and possibly punctuation ; . features indicating that a text node contains a first name or surname that is not also a standard dictionary word ; . features indicating that a text node contains a first name or surname that is the first occurrence of that first name or surname on any text node within the document ( particularly indicative of a text node commencing a biographical span ) ; .", "label": "", "metadata": {}, "score": "53.875126"}
{"text": "A feature vector is extracted 920 from each token in the span .Any feature of a token that will help distinguish entities from the surrounding text and can be automatically computed should be considered .Some other examples of features that are particularly suited for biographical span and entity extraction include : . features indicating that a text node contains a first name or surname , computed by looking all the text node tokens up in a list of first - names or surnames ; . features indicating that a text node contains only a first name or surname and possibly punctuation ; . features indicating that a text node contains a first name or surname that is not also a standard dictionary word ; . features indicating that a text node contains a first name or surname that is the first occurrence of that first name or surname on any text node within the document ( particularly indicative of a text node commencing a biographical span ) ; .", "label": "", "metadata": {}, "score": "53.875126"}
{"text": "For Moses , the parse tree should be formatted using XML .The decoder converts the parse tree into an annotated string ( a chart ? )Each span in the chart is labelled with the non - terminal from the parse tree .", "label": "", "metadata": {}, "score": "54.012177"}
{"text": "Thus , SLM 400 can be used in this embodiment not only to assist in the recognition of speech , but also to perform information extraction from speech .It can thus be seen that the present invention employs a structured language model in information extraction .", "label": "", "metadata": {}, "score": "54.016014"}
{"text": "22 , once the associator has been trained , it can be applied to classify entity pairs within each extracted span as follows : .Generate 1320 the feature - vector for each candidate entity pair using the same feature extraction process used to generate the training feature vectors ( item 1320 at FIG .", "label": "", "metadata": {}, "score": "54.07253"}
{"text": "22 , once the associator has been trained , it can be applied to classify entity pairs within each extracted span as follows : .Generate 1320 the feature - vector for each candidate entity pair using the same feature extraction process used to generate the training feature vectors ( item 1320 at FIG .", "label": "", "metadata": {}, "score": "54.07253"}
{"text": "As alluded to earlier , this problem may be addressed by augmenting the label set with a \" bio_start \" label , and then assigning that label to the first text node of each biography in the training data .The actual biographies may then be extracted correctly as all contiguous sequences of text nodes beginning with a \" bio_span_start \" node , followed by zero or more \" bio_span \" nodes .", "label": "", "metadata": {}, "score": "54.11198"}
{"text": "As alluded to earlier , this problem may be addressed by augmenting the label set with a \" bio_start \" label , and then assigning that label to the first text node of each biography in the training data .The actual biographies may then be extracted correctly as all contiguous sequences of text nodes beginning with a \" bio_span_start \" node , followed by zero or more \" bio_span \" nodes .", "label": "", "metadata": {}, "score": "54.11198"}
{"text": "11 shows a speech recognition system in which one or more of the information extraction techniques of the present invention can be used to extract information ( frame and slots ) from a natural language speech input signal .In .FIG .", "label": "", "metadata": {}, "score": "54.13462"}
{"text": "All of the above .The avalanche of terminology stems partly from the need of researchers to carve out their own niche , partly from the fact that work in this area has not yet fully settled on a agreed framework , but also from a fundamental difference .", "label": "", "metadata": {}, "score": "54.1477"}
{"text": "We first show how recent insights on selective parameter sharing , based on typological and language - family fe ... \" .We study multi - source transfer parsing for resource - poor target languages ; specifically methods for target language adaptation of delexicalized discriminative graph - based dependency parsers .", "label": "", "metadata": {}, "score": "54.165062"}
{"text": "If the model uses real syntax , there has to be a syntactic justification for the reordering .The most important consideration in decoding is a speed / quality trade - off .If you want to win competitions , you want the best quality possible , even if it takes a week to translate 2000 sentences .", "label": "", "metadata": {}, "score": "54.313236"}
{"text": "training a predictive algorithm based on said sequence of labels and said corresponding sequence of said feature vectors , said algorithm trained to generate new label sequences from an input sequence of feature vectors thereby classifying text based elements that form said input sequence of feature vectors .", "label": "", "metadata": {}, "score": "54.524323"}
{"text": "training a predictive algorithm based on said sequence of labels and said corresponding sequence of said feature vectors , said algorithm trained to generate new label sequences from an input sequence of feature vectors thereby classifying text based elements that form said input sequence of feature vectors .", "label": "", "metadata": {}, "score": "54.524323"}
{"text": "All the extracted information is formed into a structured record 190 ; .The structured record is stored in a database 210 and indexed for searching 200 .Each step in the process , from classification 110 ( step 2 ) through to normalization 175 ( step 7 ) , can be performed using hand - coded rules or in this preferred embodiment with the use of classifiers and extractors trained using machine learning algorithms .", "label": "", "metadata": {}, "score": "54.602974"}
{"text": "All the extracted information is formed into a structured record 190 ; .The structured record is stored in a database 210 and indexed for searching 200 .Each step in the process , from classification 110 ( step 2 ) through to normalization 175 ( step 7 ) , can be performed using hand - coded rules or in this preferred embodiment with the use of classifiers and extractors trained using machine learning algorithms .", "label": "", "metadata": {}, "score": "54.602974"}
{"text": "This ' mixed - syntax ' model was explored in ( Hoang and Koehn , 2010 ) and in Hieu Hoang 's thesis .The source non - terminals in translation rules are used just to constrain against the input parse tree , not for parsing .", "label": "", "metadata": {}, "score": "54.63224"}
{"text": "Entities , normalized entities , or associated entities may also require further classification such as jobtitles / organizations being classified into either former or current .Referring now to FIG .11 , positively labeled documents , the locations of their spans , and the locations of the entities within the spans 400 are retrieved from the labeled document store 330 , the labeled span store 360 , and the labeled entities store 390 .", "label": "", "metadata": {}, "score": "54.637497"}
{"text": "Entities , normalized entities , or associated entities may also require further classification such as jobtitles / organizations being classified into either former or current .Referring now to FIG .11 , positively labeled documents , the locations of their spans , and the locations of the entities within the spans 400 are retrieved from the labeled document store 330 , the labeled span store 360 , and the labeled entities store 390 .", "label": "", "metadata": {}, "score": "54.637497"}
{"text": "The decoder needs to look up the target non - terminals on the right - hand - side of each rule so the first column consists of source terminals and non - terminal , and target non - terminals from the right - hand - side .", "label": "", "metadata": {}, "score": "54.6398"}
{"text": "Here we are relying on the assumption that breaks between biographies do not occur within text nodes .One technique is to assign a special \" bio_span_start \" label to the first text node in a biography .In cases where the data exhibits particularly uniform structure one could further categorize the text nodes and label as such .", "label": "", "metadata": {}, "score": "54.643234"}
{"text": "Here we are relying on the assumption that breaks between biographies do not occur within text nodes .One technique is to assign a special \" bio_span_start \" label to the first text node in a biography .In cases where the data exhibits particularly uniform structure one could further categorize the text nodes and label as such .", "label": "", "metadata": {}, "score": "54.643234"}
{"text": "Distributional Phrase Structure Induction .Proceedings of the Fifth Conference on Natural Language Learning ( CoNLL-2001 ) , pp .113 - 120 .Background .Syntax Tutorial .24 And the people murmured against Moses , saying , What shall we drink ? 25 And he cried unto the Lord ; and the Lord showed him a tree , which when he had cast into the waters , the waters were made sweet .", "label": "", "metadata": {}, "score": "54.654053"}
{"text": "End tokens can also be qualified in the same way \" name_end \" or \" organization_end \" .Assuming the use of qualifying start labels , the label sequence set out above would become : .Referring now to FIG .18 , once the entity extraction model has been trained , it can be applied to generate entities from each extracted span as follows : .", "label": "", "metadata": {}, "score": "54.804585"}
{"text": "End tokens can also be qualified in the same way \" name_end \" or \" organization_end \" .Assuming the use of qualifying start labels , the label sequence set out above would become : .Referring now to FIG .18 , once the entity extraction model has been trained , it can be applied to generate entities from each extracted span as follows : .", "label": "", "metadata": {}, "score": "54.804585"}
{"text": "The output is a string .However , the CFG - tree derivation of the output ( target ) can also be obtained ( in Moses by using the -T argument ) , the non - terminals in this tree will be labelled with the linguistically - motivated labels .", "label": "", "metadata": {}, "score": "54.815308"}
{"text": "Constituent .A constituent refers to the detailed component or basic foundational unit or element of a commodity from which measurements are performed .For example , under the sub - commodity processors , a specific constituent might be a part number ' 99X9999 ' .", "label": "", "metadata": {}, "score": "54.875504"}
{"text": "The attributes defined for a manufacturing industry may include sampling criteria , period definition , history definition , type of measure / analytic or any other attribute desired .At step 308 , it is determined whether additional attributes are to be defined .", "label": "", "metadata": {}, "score": "54.939636"}
{"text": "4 .Clearly , as would be apparent to those skilled in the art , the corpus of documents could be further generalised to include all web pages located on servers originating from a given country domain name or alternatively all web pages that have been updated in the last year .", "label": "", "metadata": {}, "score": "54.980705"}
{"text": "4 .Clearly , as would be apparent to those skilled in the art , the corpus of documents could be further generalised to include all web pages located on servers originating from a given country domain name or alternatively all web pages that have been updated in the last year .", "label": "", "metadata": {}, "score": "54.980705"}
{"text": "The program relax - parse ( in training / phrase - extract ) implements two kinds of parse relaxations : binarization and a method proposed under the label of syntax - augmented machine translation ( SAMT ) by Zollmann and Venugopal .", "label": "", "metadata": {}, "score": "54.99772"}
{"text": "FIG .11 , it will be appreciated that structured language model 420 can perform its speech recognition language model duties in order to recognize the speech input by speaker 400 and then perform feature extraction parsing on the recognized speech , as discussed above .", "label": "", "metadata": {}, "score": "55.03713"}
{"text": "14 is a flowchart of the entity / association / normalization classification labeling method according to a preferred embodiment of the present invention ; .FIG .15 is a flowchart illustrating the steps involved in training a span extractor to extract spans from labeled documents according to a preferred embodiment of the present invention ; .", "label": "", "metadata": {}, "score": "55.08937"}
{"text": "14 is a flowchart of the entity / association / normalization classification labeling method according to a preferred embodiment of the present invention ; .FIG .15 is a flowchart illustrating the steps involved in training a span extractor to extract spans from labeled documents according to a preferred embodiment of the present invention ; .", "label": "", "metadata": {}, "score": "55.08937"}
{"text": "The language model probability assignment for the word at position k+1 in the input sentence is made using : .P .w . k .W . k . )T . k .S . k .P .w . k .", "label": "", "metadata": {}, "score": "55.153053"}
{"text": "According to a preferred embodiment of the present invention an automated method of associating extracted jobtitles with their corresponding organization is provided .A machine learning - based method is employed by the present invention to train entity associators , although other direct ( not trained ) methods are also applicable .", "label": "", "metadata": {}, "score": "55.3117"}
{"text": "According to a preferred embodiment of the present invention an automated method of associating extracted jobtitles with their corresponding organization is provided .A machine learning - based method is employed by the present invention to train entity associators , although other direct ( not trained ) methods are also applicable .", "label": "", "metadata": {}, "score": "55.3117"}
{"text": "Any desire to change the analytics or product dimensions to be monitored results in the need for extensive hard - code changes to the computing application or query .Attempts to rewrite software queries that will measure atypical characteristics take time , such as a few hours to several days .", "label": "", "metadata": {}, "score": "55.33914"}
{"text": "Referring now to the drawings wherein like elements are numbered alike in the several FIGURES : .FIG .1 is a block diagram illustrating a quality management system upon which the multi - dimensional commodity modeling process is implemented in an exemplary embodiment ; .", "label": "", "metadata": {}, "score": "55.346355"}
{"text": "These trends or patterns may include elements such as performance tolerances , noise filters , oscillation thresholds or trends , consecutive trending , and negative performance threshold .At step 314 , it is determined whether there are additional nodes or levels to be analyzed .", "label": "", "metadata": {}, "score": "55.417862"}
{"text": "7B illustrates joint syntactic and semantic labels .FIG .8 is a flow diagram illustrating one embodiment of the process of training a structured language model in accordance with the present invention .FIG .9 is a flow diagram illustrating one embodiment of the operation of the structured language model during run - time , or test - time , in accordance with one embodiment of the present invention .", "label": "", "metadata": {}, "score": "55.64899"}
{"text": "Extracted entities may be further associated 170 into related groups for example jobtitles associated with the correct organization ; .Extracted entities may also be normalized 175 , for example multiple variants of the same person name may be combined together ; .", "label": "", "metadata": {}, "score": "55.753098"}
{"text": "Extracted entities may be further associated 170 into related groups for example jobtitles associated with the correct organization ; .Extracted entities may also be normalized 175 , for example multiple variants of the same person name may be combined together ; .", "label": "", "metadata": {}, "score": "55.753098"}
{"text": "The model is then trained by generating parses on the semantically annotated training data enforcing the semantic tags or labels found in the training data .The trained model can then be used to extract information from test data using the parses generated by the model .", "label": "", "metadata": {}, "score": "55.903538"}
{"text": "Here are some lines as example : .Each line in the rule table describes one translation rule .It consists of five components separated by three bars : . score(s ) : here only one , but typically multiple scores are used , .", "label": "", "metadata": {}, "score": "55.93454"}
{"text": "We present LLCCM , a log - linear variant of the constituent context model ( CCM ) of grammar induction .LLCCM retains the simplicity of the original CCM but extends robustly to long sentences .On sentences of up to length 40 , LLCCM outperforms CCM by 13.9 % bracketing F1 and outperforms a right - branchin ... \" .", "label": "", "metadata": {}, "score": "56.07951"}
{"text": "In a third aspect the present invention accordingly provides an apparatus adapted for extracting a structured record from a document , said structured record including information related to a predetermined subject matter , said information to be organized into categories within said structured record , said apparatus comprising : . processor means adapted to operate in accordance with a predetermined instruction set ; . said apparatus in conjunction with said instruction set , being adapted to perform the method of : . identifying a span of text in said document according to criteria associated with said predetermined subject matter ; and . processing said span of text to extract at least one text element associated with at least one of said categories of said structured record from said document .", "label": "", "metadata": {}, "score": "56.113514"}
{"text": "In a third aspect the present invention accordingly provides an apparatus adapted for extracting a structured record from a document , said structured record including information related to a predetermined subject matter , said information to be organized into categories within said structured record , said apparatus comprising : . processor means adapted to operate in accordance with a predetermined instruction set ; . said apparatus in conjunction with said instruction set , being adapted to perform the method of : . identifying a span of text in said document according to criteria associated with said predetermined subject matter ; and . processing said span of text to extract at least one text element associated with at least one of said categories of said structured record from said document .", "label": "", "metadata": {}, "score": "56.113514"}
{"text": "Any other algorithms for predicting label - sequences from feature - vector - sequences could also be used , including hand - tuning of rule - based procedures .In the case of Markov models , several different types may be used .", "label": "", "metadata": {}, "score": "56.131493"}
{"text": "Any other algorithms for predicting label - sequences from feature - vector - sequences could also be used , including hand - tuning of rule - based procedures .In the case of Markov models , several different types may be used .", "label": "", "metadata": {}, "score": "56.131493"}
{"text": "6 is a diagram of a sample commodity tree created by the multi - dimensional commodity modeling component in an exemplary embodiment .DETAILED DESCRIPTION .Embodiments of the invention include a multi - dimensional commodity model process , which as implemented , facilitates an on - demand quality monitoring and control process that is tailorable to meet the needs of any quality management system as well as for use in any industry .", "label": "", "metadata": {}, "score": "56.193954"}
{"text": "FIG .7A illustrates one example sentence of annotated training data that can be used to train the system , and .FIG .8 is a flow diagram illustrating the training procedure .In order to train the system , the SLM is first initialized with syntactic knowledge using annotated training data .", "label": "", "metadata": {}, "score": "56.199215"}
{"text": "Since dimension A 502 was assigned at the highest level - the commodity , every node in the tree that is either identified to be analyzed by another assignment of a dimension or every constituent under commodity 1 inherits dimension A 502 attributes .", "label": "", "metadata": {}, "score": "56.223976"}
{"text": "16 ) and the document token sequence 345 generated from the positively labeled documents ( item 120 in FIG .5 ) and generate 900 the token subsequence for each span .Generate 920 a feature - vector for each token in the span token subsequence with the same feature extraction process used to generate the training sequences ( item 920 in FIG .", "label": "", "metadata": {}, "score": "56.47847"}
{"text": "16 ) and the document token sequence 345 generated from the positively labeled documents ( item 120 in FIG .5 ) and generate 900 the token subsequence for each span .Generate 920 a feature - vector for each token in the span token subsequence with the same feature extraction process used to generate the training sequences ( item 920 in FIG .", "label": "", "metadata": {}, "score": "56.47847"}
{"text": "However , every constituent 406 is directly assigned to one and only one dependent node within a commodity tree .Other commodity trees can use the same constituents but not within the same tree .CCM L 1 criteria is assigned at the commodity level and inherited to every node in the tree in order to ensure that all nodes have all uniform characteristics assigned .", "label": "", "metadata": {}, "score": "56.485634"}
{"text": "A feature which is currently unique to the Moses decoder is the ability to separate out these two roles .Each non - terminal in all translation rules is represented by two labels : .The source non - terminal which constrains rules to the input parse tree .", "label": "", "metadata": {}, "score": "56.529007"}
{"text": "Referring once again to FIG .5 , document classification step 110 according to a preferred embodiment of the present invention requires classification of text documents into preassigned categories such as \" biographical page \" versus \" non - biographical page \" .", "label": "", "metadata": {}, "score": "56.732643"}
{"text": "Referring once again to FIG .5 , document classification step 110 according to a preferred embodiment of the present invention requires classification of text documents into preassigned categories such as \" biographical page \" versus \" non - biographical page \" .", "label": "", "metadata": {}, "score": "56.732643"}
{"text": "Various kinds of HSS are proposed in this paper .One feature of the present invention uses the parsing capabilities of a structured language model in the information extraction process .During training , the structured language model is first initialized with syntactically annotated training data .", "label": "", "metadata": {}, "score": "56.7627"}
{"text": "These digital values are provided to a frame constructor 406 , which , in one embodiment , groups the values into 25 millisecond frames that start 10 milliseconds apart .These \" frames \" are not the same as the frames or templates used in information extraction , but are just portions of the digitized speech signal .", "label": "", "metadata": {}, "score": "56.798676"}
{"text": "15 , there is shown a flowchart illustrating this segmentation process : .Positively labeled Documents 340 from the labeled document corpus 330 are tokenized 345 into their constituent tokens or text elements .Text documents can be automatically split into \" natural \" contiguous regions .", "label": "", "metadata": {}, "score": "56.84346"}
{"text": "15 , there is shown a flowchart illustrating this segmentation process : .Positively labeled Documents 340 from the labeled document corpus 330 are tokenized 345 into their constituent tokens or text elements .Text documents can be automatically split into \" natural \" contiguous regions .", "label": "", "metadata": {}, "score": "56.84346"}
{"text": "For example , the following rules can be applied to the above sentence .However , these rules ca n't as they do n't match one or more non - terminals .Therefore , non - terminal in the translation rules in a tree - to - string model acts as constraints on which rules can be applied .", "label": "", "metadata": {}, "score": "56.91466"}
{"text": "Incorporation of the dependency model substantially improves performance , closing in on the performance achieved by supervised parsers ( 92.8 % for English ) .However , interesting issue remain ranging from dealing better with languages like Chinese that largely lack morphology and function words , to making substantive use of morphology in languages with rich morphology .", "label": "", "metadata": {}, "score": "57.05458"}
{"text": "The dynamic need to change analytics across multiple dimensions presents significant problems with existing processes and technology .This problem is most evident in manufacturing operations where complex products are produced in a \" build - to - order \" environment with a high degree of featurability .", "label": "", "metadata": {}, "score": "57.136616"}
{"text": "Therefore , the novel concepts of the present invention can easily be practiced regardless of whether the input is actually text , or speech .BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1 is a block diagram of one embodiment of a computer environment in which the present invention can be practiced .", "label": "", "metadata": {}, "score": "57.358612"}
{"text": "Performance tolerances define standard deviations from the mean .Noise filters define what is statistically significant sample size per period .Oscillation thresholds or trends define unwanted change oscillating around the mean within limits .Consecutive trending defines significant trending ( negative or positive ) .", "label": "", "metadata": {}, "score": "57.47501"}
{"text": "The method for extracting a structured record from a document as claimed in claim 10 , wherein said predictive model is a hand tuned decision tree based procedure .The method for extracting a structured record from a document as claimed in claim 14 , wherein said predictive model is a classifier based on a Markov model trained on labeled sub - entity feature vector sequences .", "label": "", "metadata": {}, "score": "57.51165"}
{"text": "The method for extracting a structured record from a document as claimed in claim 10 , wherein said predictive model is a hand tuned decision tree based procedure .The method for extracting a structured record from a document as claimed in claim 14 , wherein said predictive model is a classifier based on a Markov model trained on labeled sub - entity feature vector sequences .", "label": "", "metadata": {}, "score": "57.51165"}
{"text": "Here an example , what parse relaxation does to the number of rules extracted ( English - German News Commentary , using Bitpar for German , no English syntax ) : .Often a syntactic formalism will use symbols that are part of the meta - symbols that denote non - terminal boundaries in the SCFG rule table , and glue grammar .", "label": "", "metadata": {}, "score": "57.515137"}
{"text": "To avoid the above and ensure that source spans are always consistently labelled , simply project the non - terminal label to both source and target .For example , change the rule .The text rule table should be easy to convert to a binary , on - disk format .", "label": "", "metadata": {}, "score": "57.620262"}
{"text": "To distinguish models that use proper linguistic syntax on the input side , on the output side , on both , or on neither all this terminology has been invented .Let 's decipher common terms found in the literature : . syntax - directed : linguistic syntax only in input language , . syntax - based : unclear , we use it for models that have any linguistic syntax , and .", "label": "", "metadata": {}, "score": "57.710503"}
{"text": "The idea is the following : If the training data contains a subtree such as .then it is not possible to extract translation rules for Ariel Sharon without additional syntactic context .Recall that each rule has to match a syntactic constituent .", "label": "", "metadata": {}, "score": "57.867794"}
{"text": "Generally , program modules include routines , programs , objects , components , data structures , etc . that perform particular tasks or implement particular abstract data types .The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network .", "label": "", "metadata": {}, "score": "58.100433"}
{"text": "The pieces of information in each line ( with the first line as example ) are : . the hypothesis number , a sequential identifier ( 41 ) , .the input non - terminal ( X ) , .the output non - termial ( S ) , . alignment information between input and output non - terminals in the rule ( ( 1,1 ) ) , .", "label": "", "metadata": {}, "score": "58.116398"}
{"text": "FIG .26 depicts the cached page from which the record in FIG .25 was extracted .In the following description , like reference characters designate like or corresponding parts or steps throughout the several views of the drawings .DETAILED DESCRIPTION OF THE INVENTION .", "label": "", "metadata": {}, "score": "58.2016"}
{"text": "FIG .26 depicts the cached page from which the record in FIG .25 was extracted .In the following description , like reference characters designate like or corresponding parts or steps throughout the several views of the drawings .DETAILED DESCRIPTION OF THE INVENTION .", "label": "", "metadata": {}, "score": "58.2016"}
{"text": "In this embodiment directed to the extraction of biographical spans , the applicant has found a simpler stateless model to be the most effective .p .l .t .l .l .t .l .f .t .", "label": "", "metadata": {}, "score": "58.333683"}
{"text": "In this embodiment directed to the extraction of biographical spans , the applicant has found a simpler stateless model to be the most effective .p .l .t .l .l .t .l .f .t .", "label": "", "metadata": {}, "score": "58.333683"}
{"text": "The main hurdle is to get the annotation .This requires a syntactic parser .Syntactic annotation is provided by annotating all the training data ( input or output side , or both ) with syntactic labels .The format that is used for this uses XML markup .", "label": "", "metadata": {}, "score": "58.41531"}
{"text": "Tree - based models operate on so - called grammar rules , which include variables in the mapping rules : .The variables in these grammar rules are called non - terminals , since their occurrence indicates that the process has not yet terminated to produce the final words ( the terminals ) .", "label": "", "metadata": {}, "score": "58.427803"}
{"text": "Claims : .The method for extracting a structured record from a document as claimed in claim 6 , wherein said predictive model is a classifier based on a Markov model trained on labeled text node feature vector sequences .The method for extracting a structured record from a document as claimed in claim 6 , wherein said predictive model is a hand tuned decision tree based procedure .", "label": "", "metadata": {}, "score": "58.530617"}
{"text": "Claims : .The method for extracting a structured record from a document as claimed in claim 6 , wherein said predictive model is a classifier based on a Markov model trained on labeled text node feature vector sequences .The method for extracting a structured record from a document as claimed in claim 6 , wherein said predictive model is a hand tuned decision tree based procedure .", "label": "", "metadata": {}, "score": "58.530617"}
{"text": "In order to better understand the kinds of data I 'm using , I 'll outline all the ADB tag types , their sources , and possible attributes here .Please note this list is subject to change , especially if you have some useful input on how your project might be doing things differently !", "label": "", "metadata": {}, "score": "58.55036"}
{"text": "In Proceedings of the EACL Workshop on the Computational Treatment of Anaphora , pages 23 - -30 , 2003 .Katja Markert , Natalia Modjeska , and Malvina Nissim .Using the Web for nominal anaphora resolution .In Proceedings of the EACL Workshop on the Computational Treatment of Anaphora , 2003 .", "label": "", "metadata": {}, "score": "58.69481"}
{"text": "In this case the structured record might include the full obituary text , deceased name , age at death , date of birth and other fields such as next - of - kin .Referring now to FIG .2 , there is shown a web page in which the spans of interest are executive biographies .", "label": "", "metadata": {}, "score": "58.773552"}
{"text": "In this case the structured record might include the full obituary text , deceased name , age at death , date of birth and other fields such as next - of - kin .Referring now to FIG .2 , there is shown a web page in which the spans of interest are executive biographies .", "label": "", "metadata": {}, "score": "58.773552"}
{"text": "Given the exponential size of the mapping space , we propose a novel method for optimizing over soft mappings , and use entropy regularization to drive those towards hard mappings .Our results demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the context of multilingual parsing . ...", "label": "", "metadata": {}, "score": "58.81448"}
{"text": "18 is a flowchart illustrating the steps involved in running trained entity extractor according to a preferred embodiment of the present invention ; .FIG .19 is a flowchart illustrating the steps involved in training a sub - entity extractor to extract sub - entities from labeled documents according to a preferred embodiment of the present invention ; .", "label": "", "metadata": {}, "score": "58.889343"}
{"text": "18 is a flowchart illustrating the steps involved in running trained entity extractor according to a preferred embodiment of the present invention ; .FIG .19 is a flowchart illustrating the steps involved in training a sub - entity extractor to extract sub - entities from labeled documents according to a preferred embodiment of the present invention ; .", "label": "", "metadata": {}, "score": "58.889343"}
{"text": "The training data used by the machine learning algorithms consists of one example for each labeled span from the positively labeled training documents .Referring now to FIG .17 there is shown a flowchart illustrating this process : .Positively labeled Documents 340 from the labeled document corpus 330 are tokenized 345 into their constituent tokens or text elements .", "label": "", "metadata": {}, "score": "58.92433"}
{"text": "The training data used by the machine learning algorithms consists of one example for each labeled span from the positively labeled training documents .Referring now to FIG .17 there is shown a flowchart illustrating this process : .Positively labeled Documents 340 from the labeled document corpus 330 are tokenized 345 into their constituent tokens or text elements .", "label": "", "metadata": {}, "score": "58.92433"}
{"text": "In the Penn Treebank , transitions between upper- and lowercase tokens tend to align with the boundaries of base ( English ) noun phrases .Such signals can be used as partial bracketing constraints to train a grammar inducer : in ou ... \" .", "label": "", "metadata": {}, "score": "58.954296"}
{"text": "4 illustrates a level 1 commodity constituent model 202 produced by the multi - dimensional commodity model 104 .The modeling of the commodity hierarchy and CCM L 1 attributes build the base commodity hierarchy for each commodity and provides uniform characteristics to each commodity tree .", "label": "", "metadata": {}, "score": "59.02117"}
{"text": "Referring now to FIG .23 : .Positively labeled Documents 340 from the labeled document corpus 330 are tokenized 345 into their constituent tokens .Each associated entity pair of the appropriate type generates a distinct training example 1510 .A feature vector is extracted 1520 from each associated entity pair .", "label": "", "metadata": {}, "score": "59.459595"}
{"text": "Referring now to FIG .23 : .Positively labeled Documents 340 from the labeled document corpus 330 are tokenized 345 into their constituent tokens .Each associated entity pair of the appropriate type generates a distinct training example 1510 .A feature vector is extracted 1520 from each associated entity pair .", "label": "", "metadata": {}, "score": "59.459595"}
{"text": "Description .CROSS REFERENCE TO RELATED APPLICATION .This application is a divisional application of U.S. Ser .No .10/652,017 , filed Aug. 29 , 2003 , the contents of which are incorporated by reference herein in their entirety .BACKGROUND .", "label": "", "metadata": {}, "score": "59.55409"}
{"text": "[ Jonathan , Baxter , Jonathan , Baxter , is , the , CEO , of , Panscient , Technologies , . ]In order to distinguish a single long entity from two entities that run together ( with no intervening token , such as the adjacent occurrences of \" Jonathan Baxter \" above ) , additional labels must be assigned 950 to distinguish boundary tokens within entities .", "label": "", "metadata": {}, "score": "59.76389"}
{"text": "[ Jonathan , Baxter , Jonathan , Baxter , is , the , CEO , of , Panscient , Technologies , . ]In order to distinguish a single long entity from two entities that run together ( with no intervening token , such as the adjacent occurrences of \" Jonathan Baxter \" above ) , additional labels must be assigned 950 to distinguish boundary tokens within entities .", "label": "", "metadata": {}, "score": "59.76389"}
{"text": "The joint probability P(W , T ) of a word sequence W and a complete parse T can be broken into : .P .W .T . ) k .n .[ .P .w . k .", "label": "", "metadata": {}, "score": "59.79496"}
{"text": "While preferred embodiments have been shown and described , various modifications and substitutions may be made thereto without departing from the spirit and scope of the invention .Accordingly , it is to be understood that the present invention has been described by way of illustration and not limitation .", "label": "", "metadata": {}, "score": "59.910034"}
{"text": "5 shows the result of an adjoin - left operation during parsing .FIG .6 shows the result of an adjoin - right operation during parsing .FIG .7A illustrates one embodiment of a sentence found in the annotated training data .", "label": "", "metadata": {}, "score": "59.921654"}
{"text": "FIG .2 illustrates a template or frame 200 in accordance with one example .Frame 200 defines an action \" schedule meeting \" and that is the label 202 of frame 200 .Frame 200 also includes a plurality of slots .", "label": "", "metadata": {}, "score": "59.96286"}
{"text": "Once rules are collected , the file of rules and their counts have to be converted into a probabilistic model .This is called rule scoring , and there are also some additional options : . --OnlyDirect : only estimates direct conditional probabilities .", "label": "", "metadata": {}, "score": "59.965073"}
{"text": "Data collection component 102 provides performance and parametric data to analytic engine 106 by collecting all relevant data spanning a range of activities from the procurement of raw materials to customer installation and return of products and components .The process performed by data collection component 102 is highly dependent on the product characteristics from commodity to commodity .", "label": "", "metadata": {}, "score": "59.974785"}
{"text": "This problem becomes significant in large manufacturing operations where thousands , or tens of thousands , of component parts are utilized for production , especially when many of these parts have common characteristics ( e.g. , same supplier , same function , same size , etc . ) .", "label": "", "metadata": {}, "score": "60.01908"}
{"text": "The logical connections depicted in .FIG .1 include a local area network ( LAN ) 171 and a wide area network ( WAN ) 173 , but may also include other networks .Such networking environments are commonplace in offices , enterprise - wide computer networks , intranets and the Internet .", "label": "", "metadata": {}, "score": "60.03907"}
{"text": "The number of spans that are combined into a span grows linear with sentence length for binary rules , quadratic for trinary rules , and so on .In short , long sentences become a problem .A drastic solution is the size of internal spans to a maximum number .", "label": "", "metadata": {}, "score": "60.052383"}
{"text": "As used herein , a commodity refers to a high - level ( e.g. , top level ) grouping of elements that are arranged according to relationship factors .For example , in the electronics industry , commodities might include power supplies , mechanical parts , cables , electronics and decorative covers .", "label": "", "metadata": {}, "score": "60.0887"}
{"text": "19 , there is shown a flowchart illustrating the steps involved in training a sub - entity extractor .The main deviation points from the entity extractor training as illustrated in FIG .17 are : .there is one training example per labeled entity 1110 , rather than one training example per labeled span ( item 910 in FIG .", "label": "", "metadata": {}, "score": "60.160393"}
{"text": "19 , there is shown a flowchart illustrating the steps involved in training a sub - entity extractor .The main deviation points from the entity extractor training as illustrated in FIG .17 are : .there is one training example per labeled entity 1110 , rather than one training example per labeled span ( item 910 in FIG .", "label": "", "metadata": {}, "score": "60.160393"}
{"text": "Such method causes severe data sparse problem because long constituents are more unlikely to appear in test set .To overcome the data sparse problem , this paper proposes to define a non - parametric Bayesian prior distribution , namely the Pitman - Yor Process ( PYP ) prior , over constituents for constituent smoothing .", "label": "", "metadata": {}, "score": "60.33164"}
{"text": "An entity name expression ( or proper name ) , e.g. a person name , location , or organization .This does not include anaphoric mentions of named entities such as pronominal phrases such as \" he \" .The types supported are PERSON , ORGANIZATION , LOCATION .", "label": "", "metadata": {}, "score": "60.541954"}
{"text": "Dimension C 506 was assigned to constituent 1 and dimension D 508 was assigned to constituent 2 which are the lowest levels in the commodity model .All dimensions assigned at the constituent level are not inherited by any other nodes and results in a pure analysis assigned only to this constituent in this commodity tree .", "label": "", "metadata": {}, "score": "60.551052"}
{"text": "With the option unknown - lhs you can specify a file that contains pairs of non - terminal labels and their probability per line .Optionally , we can also model the choice of non - terminal for unknown words through sparse features , and optimize their cost through MIRA or PRO .", "label": "", "metadata": {}, "score": "60.58245"}
{"text": "21 : .Positively labeled Documents 340 from the labeled document corpus 330 are tokenized 345 into their constituent tokens .The token boundaries of each labeled span within each document are read from the labeled span store 360 , and the locations of the entities to be associated are read from the labeled entity store 390 .", "label": "", "metadata": {}, "score": "60.938763"}
{"text": "21 : .Positively labeled Documents 340 from the labeled document corpus 330 are tokenized 345 into their constituent tokens .The token boundaries of each labeled span within each document are read from the labeled span store 360 , and the locations of the entities to be associated are read from the labeled entity store 390 .", "label": "", "metadata": {}, "score": "60.938763"}
{"text": "Closed loop / corrective action process 108 drives problems , issues , or nonconformance items to closure .Multi - dimensional commodity model 104 comprises a commodity hierarchy data structure and level 1 attributes referred to as commodity constituent model level 1 ( CCM L 1 ) 202 as shown in .", "label": "", "metadata": {}, "score": "61.00106"}
{"text": "Thus , the examples provided with respect to the electronics industry are intended for purposes of illustration and are not to be construed as limiting in scope .The following definitions are supplied in reference to the components of the multi - dimensional commodity model .", "label": "", "metadata": {}, "score": "61.0754"}
{"text": "srilm.gz ) , and the rule table file rule - table .The configuration file moses.ini has a fairly familiar format .It is mostly identical to the configuration file for phrase - based models .We will describe further below in detail the new parameters of the chart decoder .", "label": "", "metadata": {}, "score": "61.088493"}
{"text": "A document that is \" marked up \" ( such as an HTML document ) can be broken into contiguous text node regions .For example , the HTML document : . would naturally split into 5 \" text nodes \" : [ Jonathan Baxter ] , [ CEO ] , [ Jonathan co - founded Panscient Technologies in 2002 . . .", "label": "", "metadata": {}, "score": "61.112488"}
{"text": "A document that is \" marked up \" ( such as an HTML document ) can be broken into contiguous text node regions .For example , the HTML document : . would naturally split into 5 \" text nodes \" : [ Jonathan Baxter ] , [ CEO ] , [ Jonathan co - founded Panscient Technologies in 2002 . . .", "label": "", "metadata": {}, "score": "61.112488"}
{"text": "The issue of combining anaphoricity determination and antecedent identification in anaphora resolution .In IEEE International Conference on Natural Language Processing and Knowledge Engineering ( IEEE NLPKE ) , pages 244 - -249 , 2005 .Ryu Iida , Kentaro Inui , Hiroya Takamura , and Yuji Matsumoto .", "label": "", "metadata": {}, "score": "61.186943"}
{"text": "What is needed , therefore , is a way to provide flexible , commodity data modeling that allows for analysis criteria to be alterable in a near real - time environment .SUMMARY .An exemplary embodiment of the invention relates to a method , system , and storage medium for providing a dynamic multi - dimensional commodity modeling process .", "label": "", "metadata": {}, "score": "61.317856"}
{"text": "5 .CCM Level 2 is applied to the commodity tree by assigning \" none \" , \" any \" , or \" all \" nodes in the tree at each node level .These assignments are inherited down the tree to every dependent node in the tree that has a CCM L 2 .", "label": "", "metadata": {}, "score": "61.32005"}
{"text": "LLCCM retains the simplicity of the original CCM but extends robustly to long sentences .On sentences of up to length 40 , LLCCM outperforms CCM by 13.9 % bracketing F1 and outperforms a right - branching baseline in regimes where CCM does not . ... ependency grammar induction .", "label": "", "metadata": {}, "score": "61.41886"}
{"text": "Referring to FIG .3 , there is shown a web page in which the spans of interest are open job positions .As for biographies , the corpus is the collection of all web pages on the company 's website ; the documents of interest are the job pages , and the job records might include title , full or part - time , location , contact information , description , etc .", "label": "", "metadata": {}, "score": "61.787167"}
{"text": "Referring to FIG .3 , there is shown a web page in which the spans of interest are open job positions .As for biographies , the corpus is the collection of all web pages on the company 's website ; the documents of interest are the job pages , and the job records might include title , full or part - time , location , contact information , description , etc .", "label": "", "metadata": {}, "score": "61.787167"}
{"text": "However , with a large maximum phrase length , too many rule tables and no rule table limit , this may explode .The number of rules considered can be limited with rule - limit .Default is 5000 .In short , training uses the identical training script as phrase - based models .", "label": "", "metadata": {}, "score": "61.843616"}
{"text": "Referring now to FIG .8 , there is shown a flowchart illustrating the process for initially labeling documents of interest from the unlabeled corpus of documents 300 .Documents are retrieved 310 from the unlabeled corpus 300 and human - labeled 320 according to the characteristic of interest ( for example \" biographical page \" or \" non - biographical page \" ) .", "label": "", "metadata": {}, "score": "62.087624"}
{"text": "Referring now to FIG .8 , there is shown a flowchart illustrating the process for initially labeling documents of interest from the unlabeled corpus of documents 300 .Documents are retrieved 310 from the unlabeled corpus 300 and human - labeled 320 according to the characteristic of interest ( for example \" biographical page \" or \" non - biographical page \" ) .", "label": "", "metadata": {}, "score": "62.087624"}
{"text": "the cat --X-- --X-- -----X----- . to allow the string to be decoded with a string - to - string or string - to - tree grammar .There is no difference between a linguistically derived non - terminal label , such as NP , VP etc , and the non - linguistically motivated X label .", "label": "", "metadata": {}, "score": "62.193844"}
{"text": "ASSERT , KANTOO .A semantic argument or \" target \" action .Following the Propbank tagset ( details in Propbank distribution README ) , the types of primary semantic tags are ARG0 , ARG1 , ARG2 , ARG3 , ARG4 , and TARGET .", "label": "", "metadata": {}, "score": "62.2389"}
{"text": "Each one of these documents may include one or more \" spans \" of interest .Referring to FIG .1 , there is shown a web page from an online newspaper that contains several obituaries ( the first is highlighted ) .", "label": "", "metadata": {}, "score": "62.344284"}
{"text": "Each one of these documents may include one or more \" spans \" of interest .Referring to FIG .1 , there is shown a web page from an online newspaper that contains several obituaries ( the first is highlighted ) .", "label": "", "metadata": {}, "score": "62.344284"}
{"text": "The hierarchical phrase - based grammar is well described elsewhere so we will not go into details here .Briefly , the non - terminals are not labelled with any linguistically - motivated labels .By convention , non - terminals have been simply labelled as X , e.g. .", "label": "", "metadata": {}, "score": "62.41432"}
{"text": "Pty Ltd as well as Merchandising and Stores Director and a Director of Grace Bros.He was appointed an Executive Director in 1990 .He is Chairman of the Strategy Committee .Candidate pages are generated by a directed crawl from the home page or collection of pages from the corporate web site ; .", "label": "", "metadata": {}, "score": "62.6446"}
{"text": "Pty Ltd as well as Merchandising and Stores Director and a Director of Grace Bros.He was appointed an Executive Director in 1990 .He is Chairman of the Strategy Committee .Candidate pages are generated by a directed crawl from the home page or collection of pages from the corporate web site ; .", "label": "", "metadata": {}, "score": "62.6446"}
{"text": "Abstract .An exemplary embodiment of the invention relates to a method , system , and storage medium for providing a dynamic multi - dimensional commodity modeling process .The system includes a data collection component operable for collecting raw data , a dynamic multi - dimensional commodity model component , and a commodity tree generated by the dynamic multi - dimensional commodity model component .", "label": "", "metadata": {}, "score": "62.844707"}
{"text": "12 , positively labeled documents , the locations of their spans , and the locations of the entities within the spans 430 are retrieved from the labeled document store 330 , the labeled span store 360 , and the labeled entities store 390 .", "label": "", "metadata": {}, "score": "62.902615"}
{"text": "12 , positively labeled documents , the locations of their spans , and the locations of the entities within the spans 430 are retrieved from the labeled document store 330 , the labeled span store 360 , and the labeled entities store 390 .", "label": "", "metadata": {}, "score": "62.902615"}
{"text": "f .l .w . ll .f .The log - probability of the entire label sequence is then the sum of the log transition probabilities : . log .p .l .l .t .f .", "label": "", "metadata": {}, "score": "62.93442"}
{"text": "f .l .w . ll .f .The log - probability of the entire label sequence is then the sum of the log transition probabilities : . log .p .l .l .t .f .", "label": "", "metadata": {}, "score": "62.93442"}
{"text": "The process performed by the multi - dimensional commodity model 104 results in a set of commodity constituent models and is described further herein .Analytic engine 106 applies the commodity constituent models created by multi - dimensional commodity model 104 .", "label": "", "metadata": {}, "score": "63.01429"}
{"text": "The .p . k .p .N . k . k .The model is based on three probabilities , each illustratively estimated using deleted interpolation and parameterized ( approximated ) as follows : .P .w . k .", "label": "", "metadata": {}, "score": "63.284546"}
{"text": "For each treebank , we provide a training and a development set .Those of you who are familiar with TIGER will notice that we have restricted the training set for Tiger to match the size of the TueBa set .The sets contain sentences up to length 40 .", "label": "", "metadata": {}, "score": "63.28578"}
{"text": "The shared task is to parse either the constituent versions or the dependency versions ( or a combination of both ) .We will provide the test data sets on March 5 and expect the parsed data on March 10 .We will announce the results of the evaluation to the participants on March 12 .", "label": "", "metadata": {}, "score": "63.303223"}
{"text": "--MaxSymbolsSource SIZE and --MaxSymbolsTarget SIZE : While a rule may be extracted from a large span , much of it may be knocked out by sub - phrases that are substituted by non - terminals .So , fewer actual symbols ( non - terminals and words remain ) .", "label": "", "metadata": {}, "score": "63.308548"}
{"text": "2 is a diagram of the features of the multi - dimensional commodity model component of a quality management system in an exemplary embodiment ; .FIG .3 is a flowchart describing a two - phase process of the multi - dimensional commodity model component for building a commodity tree structure in an exemplary embodiment ; .", "label": "", "metadata": {}, "score": "63.4912"}
{"text": "4 .Commodity 402 may have sub - commodities 404 and nested sub - commodities 405 that are 0 . . .n levels deep to provide granular sub - groupings .These nodes are only analyzed when CCM L 2 selections are made at these particular levels within the commodity tree .", "label": "", "metadata": {}, "score": "63.898064"}
{"text": "When we need to differentiate source and target non - terminals , the translation rules are instead written like this : .This rule indicates that the non - terminal should span a NN constituent in the input text , and that the whole rule should span an NP constituent .", "label": "", "metadata": {}, "score": "63.99073"}
{"text": "In Thomas G. Dietterich , Suzanna Becker , and Zoubin Ghahramani ( eds ) , Advances in Neural Information Processing Systems 14 ( NIPS 2001 ) .Cambridge , MA : MIT Press , vol .1 , pp .35 - 42 .", "label": "", "metadata": {}, "score": "64.26594"}
{"text": "Sampling criteria defines what data to sample such as product types , operations , steps , sources , etc .Period definition defines the unit of time to apply the specified analytic such as hour , day , week , or month .", "label": "", "metadata": {}, "score": "64.27069"}
{"text": "16 is flowchart illustrating the steps involved in running a trained span extractor according to a preferred embodiment of the present invention ; .FIG .17 is a flowchart illustrating the steps involved in training an entity extractor to extract entities from labeled documents according to a preferred embodiment of the present invention ; .", "label": "", "metadata": {}, "score": "64.29495"}
{"text": "16 is flowchart illustrating the steps involved in running a trained span extractor according to a preferred embodiment of the present invention ; .FIG .17 is a flowchart illustrating the steps involved in training an entity extractor to extract entities from labeled documents according to a preferred embodiment of the present invention ; .", "label": "", "metadata": {}, "score": "64.29495"}
{"text": "Computational Linguistics , 27(4):521 - -544 , 2001 .Mark Steedman , Miles Osborne , Anoop Sarkar , Stephen Clark , Rebecca Hwa , Julia Hockenmaier , Paul Ruhlen , Steven Baker , and Jeremiah Crim .Bootstrapping statistical parsers from small datasets .", "label": "", "metadata": {}, "score": "64.569305"}
{"text": "Pages that are positively classified 120 are processed 130 to identify the spans ( contiguous biographies ) of interest ; .Spans are further processed 150 to identify entities of interest , such as people and organization names , jobtitles , degrees ; .", "label": "", "metadata": {}, "score": "64.66848"}
{"text": "Pages that are positively classified 120 are processed 130 to identify the spans ( contiguous biographies ) of interest ; .Spans are further processed 150 to identify entities of interest , such as people and organization names , jobtitles , degrees ; .", "label": "", "metadata": {}, "score": "64.66848"}
{"text": "By convention , the non - terminals for glue rules are labelled as S , e.g. .In a syntactic model , non - terminals are labelled with linguistically - motivated labels such as ' NOUN ' , ' VERB ' etc .", "label": "", "metadata": {}, "score": "64.77072"}
{"text": "t . ) t .t .p .l .t .l .t .f .t .Accordingly , the parameters w ll , may be trained by computing the gradient with respect to the parameters of the sum of the log - probabilities of a sufficiently large number of training sequences .", "label": "", "metadata": {}, "score": "65.17312"}
{"text": "t . ) t .t .p .l .t .l .t .f .t .Accordingly , the parameters w ll , may be trained by computing the gradient with respect to the parameters of the sum of the log - probabilities of a sufficiently large number of training sequences .", "label": "", "metadata": {}, "score": "65.17312"}
{"text": "We call these models tree - based , because during the translation a data structure is created that is a called a tree .To fully make this point , consider the following input and translation rules : .First the simple phrase mappings ( 1 ) Das Tor to The door and ( 2 ) schnell to quickly are carried out .", "label": "", "metadata": {}, "score": "65.19978"}
{"text": "As the web continues to expand at an exponential rate , the primary mechansim for finding web pages of interest is through the use of search engines such as Google \u2122 .Search engines of this type use sophisticated ranking technology to determine lists of web pages that attempt to match a given query .", "label": "", "metadata": {}, "score": "65.25178"}
{"text": "As the web continues to expand at an exponential rate , the primary mechansim for finding web pages of interest is through the use of search engines such as Google \u2122 .Search engines of this type use sophisticated ranking technology to determine lists of web pages that attempt to match a given query .", "label": "", "metadata": {}, "score": "65.25178"}
{"text": "Referring now to FIG .13 , positively labeled documents , the locations of their spans , and the locations of the entities within the spans 430 are retrieved from the labeled document store 330 , the labeled span store 360 , and the labeled entities store 390 .", "label": "", "metadata": {}, "score": "65.31015"}
{"text": "Referring now to FIG .13 , positively labeled documents , the locations of their spans , and the locations of the entities within the spans 430 are retrieved from the labeled document store 330 , the labeled span store 360 , and the labeled entities store 390 .", "label": "", "metadata": {}, "score": "65.31015"}
{"text": "FIELD OF THE INVENTION .The present invention relates to a machine learning system for extracting structured records from documents in a corpus .In one particular form the present invention relates to a system for extracting structured records from a web site .", "label": "", "metadata": {}, "score": "65.3225"}
{"text": "FIELD OF THE INVENTION .The present invention relates to a machine learning system for extracting structured records from documents in a corpus .In one particular form the present invention relates to a system for extracting structured records from a web site .", "label": "", "metadata": {}, "score": "65.3225"}
{"text": "an analytic engine in communication with the data collection component , the multi - dimensional commodity model component , and the closed loop / corrective action component , the analytic engine performing ; . receiving the raw data from the data collection component ; . receiving the commodity tree ; . performing analytics on the raw data according to rules defined by the commodity tree ; . transmitting any nonconformance data resulting from performing the analytics to the closed loop / correction action component ; and . resolving any identified nonconformances ; . wherein generating the commodity tree comprises : . creating a commodity hierarchy data structure comprising : . at least one top level node ; and .", "label": "", "metadata": {}, "score": "65.48231"}
{"text": "The final example illustrates how reordering works in a tree - based model : .The reordering in the sentence happens when hypothesis 18 is generated .Not any arbitrary reordering is allowed --- as this can be the case in phrase models .", "label": "", "metadata": {}, "score": "65.56073"}
{"text": "A time expression .Types supported are DATE , TIME .IDENTIFINDER .A numerical expression .Types supported are MONEY , PERCENT .ANTECEDENT .COREFEREE .An antecedent phrase .This tag is a marker which signals that some other REFERENT tag links back to this ANTECEDENT tag .", "label": "", "metadata": {}, "score": "65.61553"}
{"text": "--MinWords SIZE : minimum number of words in a rule .Default is 1 , meaning that each rule has to have at least one word in it .If you want to allow non - lexical rules set this to zero .", "label": "", "metadata": {}, "score": "65.72905"}
{"text": "These attributes and dimensions are dynamically alterable via the multi - dimensional commodity model component 104 during instantiation of the analytic process .Commodity tree 206 combines the elements provided in CCM L 1 202 and CCM L 2 204 .A sample commodity tree is shown in greater detail in .", "label": "", "metadata": {}, "score": "65.870735"}
{"text": "BACKGROUND OF THE INVENTION .The present invention relates to machine understanding of textual or speech inputs .More specifically , the present invention relates to the task of information extraction in the machine understanding process .Natural language user interfaces to computers attempt to allow the user to operate a computer simply by inputting commands or directions to the computer in a natural language .", "label": "", "metadata": {}, "score": "65.89527"}
{"text": "A quality management system for utilizing dynamic multi - dimensional commodity modeling , comprising : . a data collection component operable for collecting raw data ; . a dynamic multi - dimensional commodity model component , comprising a plurality of dimensions and attributes , the dimensions and attributes are dynamically alterable via the dynamic multi - dimensional commodity model component ; .", "label": "", "metadata": {}, "score": "65.97562"}
{"text": "STANFORD .A syntactic constituent .There are many types of constituent tags , but each may have a \" parent \" tag which represents some higher level sentential constituent .Currently , the tagset includes NP , VP , PP , ADJP , ADVP , S , SBAR , SBARQ , SQ , WHADVP , WHNP , WHPP .", "label": "", "metadata": {}, "score": "66.261345"}
{"text": "22 is a flowchart illustrating the steps involved in running a trained associator according to a preferred embodiment of the present invention ; .FIG .23 is a flowchart illustrating the steps involved in training an associator from labeled documents according to a preferred embodiment of the present invention ; .", "label": "", "metadata": {}, "score": "66.331345"}
{"text": "22 is a flowchart illustrating the steps involved in running a trained associator according to a preferred embodiment of the present invention ; .FIG .23 is a flowchart illustrating the steps involved in training an associator from labeled documents according to a preferred embodiment of the present invention ; .", "label": "", "metadata": {}, "score": "66.331345"}
{"text": "The four dimensions 502 - 508 were assigned at different levels within the commodity tree as shown in .FIG .5 .Dimension A 502 was assigned to the entire Commodity 1 .Dimension B 504 was assigned to the entire sub - commodity 2 , dimension C 506 was assigned for the specific constituent 1 , and dimension D 508 was assigned for the specific constituent 2 .", "label": "", "metadata": {}, "score": "66.41561"}
{"text": "This has an effect on the arity of rules , in terms of non - terminals .Default is to generate only binary rules , so the setting is 2 .--MinHoleSource SIZE and --MinHoleTarget SIZE : When sub - phrases are replaced by non - terminals , we may require a minimum size for these sub - phrases .", "label": "", "metadata": {}, "score": "66.47619"}
{"text": "The present invention is an information extraction system that utilizes a structured language model .The system can be implemented on a computing device and as a method .FIG .1 illustrates an example of a suitable computing system environment 100 on which the invention may be implemented .", "label": "", "metadata": {}, "score": "66.62337"}
{"text": "Shared Task Instructions .Getting the data .Sandra will then send you the username and the password for the web page from which you can download the data .Right now , you can download the training data for the shared task from the following web directory : .", "label": "", "metadata": {}, "score": "67.08806"}
{"text": "Weakly supervised natural language learning without redundant views .In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics ( HLTNAACL ) , 2003 .WeeMeng Soon , Hwee Tou Ng , and Daniel Chung Yong Lim .", "label": "", "metadata": {}, "score": "67.10147"}
{"text": "These regions are \" natural \" in the sense that their text refers to a particular named entity or are related in some other fashion .In the above example , the first text node contains the subject of the first biography \" Jonathan Baxter \" , the second contains his jobtitle \" CEO \" , while the third contains the first paragraph of Jonathan 's biography .", "label": "", "metadata": {}, "score": "67.125824"}
{"text": "These regions are \" natural \" in the sense that their text refers to a particular named entity or are related in some other fashion .In the above example , the first text node contains the subject of the first biography \" Jonathan Baxter \" , the second contains his jobtitle \" CEO \" , while the third contains the first paragraph of Jonathan 's biography .", "label": "", "metadata": {}, "score": "67.125824"}
{"text": "FIG .1 is described herein for illustration and is not intended to be limiting in scope .The QMS 100 may comprise a commercial quality management application or may be IBM 's Product Quality Management System \u2122 .Further , while the invention is described with respect to a specific manufacturing industry , i.e. , electronics , it will be understood that the modeling features of the multi - dimensional commodity model may be applied to any industry that may benefit from utilizing a quality management system .", "label": "", "metadata": {}, "score": "67.174034"}
{"text": "Analytic engine 106 also runs secondary analysis on the analytic output to identify nonconforming trends and patterns as defined by the commodity constituent models and automatically notifies all nonconforming trends to the closed loop / corrective action process 108 .The process 110 of analytic engine 106 represents an instantiation of a set of commodity constituent models from 104 .", "label": "", "metadata": {}, "score": "67.270775"}
{"text": "FIG .6 illustrates a sample commodity tree 206 produced by the multi - dimensional commodity model and describes its dimensions and inheritance properties .The sample commodity tree 206 demonstrates how dimensions are assigned and inherited and result in analyzed dimensions of a commodity tree .", "label": "", "metadata": {}, "score": "67.40837"}
{"text": "Finally , we show that we can significantly improve target language performance , even after annotating up to 64,000 tokens in the target language , by simply concatenating source and target language annotations .Here we perform a set of experiments where we investigate the potential of multi - source transfer for NER , in German ( DE ) , English ( EN ) , Spanish ( ES ) and Dutch ( NL ) , using cro ... . \" ...", "label": "", "metadata": {}, "score": "67.73091"}
{"text": "The system of .claim 4 and further comprising : . a ranking component ranking the candidate semantic parse trees generated by the structured language model .The system of . claim 5 wherein the ranking component ranks each candidate semantic parse tree by summing over all generated candidate semantic parse trees .", "label": "", "metadata": {}, "score": "67.758804"}
{"text": "Type of measure / analytic defines the type of analytic to be applied such as standard Shewhart Control Charts ( i.e. , p - chart , np - chart , u - chart , or other similar charts .FIG .5 illustrates a level 2 commodity constituent model 204 .", "label": "", "metadata": {}, "score": "67.96074"}
{"text": "Apply 1400 the trained associator ( item 1350 at FIG .21 ) to the feature - vector .Referring once again to FIG .5 , entity normalization step 175 requires the automatic normalization of entities identified at step 150 .", "label": "", "metadata": {}, "score": "68.28719"}
{"text": "Apply 1400 the trained associator ( item 1350 at FIG .21 ) to the feature - vector .Referring once again to FIG .5 , entity normalization step 175 requires the automatic normalization of entities identified at step 150 .", "label": "", "metadata": {}, "score": "68.28719"}
{"text": "Apply 1000 the trained entity extraction model ( item 970 in FIG .17 ) to the feature - vector sequence to generate the most likely label sequence [ l 1 , l 2 , . . ., l n ] .", "label": "", "metadata": {}, "score": "68.38454"}
{"text": "Apply 1000 the trained entity extraction model ( item 970 in FIG .17 ) to the feature - vector sequence to generate the most likely label sequence [ l 1 , l 2 , . . ., l n ] .", "label": "", "metadata": {}, "score": "68.38454"}
{"text": "The annotation has been converted into a true tree format , all trees are dominated by a VROOT node .The dependency data follow the CoNLL format .The conversion has been carried out by Yannick Versley ( thanks a lot , Yannick ) , the resulting dependency annotation is similar to the Hamburg dependency format .", "label": "", "metadata": {}, "score": "68.401596"}
{"text": "The third is illustrated by numeral 238 and is the S_schedule_ScheduleMeeting label .This step thus enriches the non - terminal and pre - terminal labels of the resulting parses with the semantic tags ( both frame and slot ) present in the semantic parse .", "label": "", "metadata": {}, "score": "68.50013"}
{"text": "2 .As described above , CCM L 1 202 refers to a set of attributes that defines the uniform characteristics of interest associated with a particular commodity for all analytics to be performed .Multi - dimensional commodity model 104 also comprises a number of dimensions and level 2 dimensional attributes referred to as commodity constituent model level 2 ( CCM L 2 ) 204 .", "label": "", "metadata": {}, "score": "68.65892"}
{"text": "The frame ( or root ) level and the slot ( or leaf ) level .During parsing , SLM 310 discards any hypothesis parses which violate this structure .This is indicated by block 304 .When parsing is complete , SLM 310 will illustratively have parsed a desired number of parse trees 314 .", "label": "", "metadata": {}, "score": "68.697205"}
{"text": "--NoLex : only includes rule - level conditional probabilities , not lexical scores .--GoodTuring : Uses Good Turing discounting to reduce actual accounts .This is a good thing , use it .Training hierarchical phrase models , i.e. , tree - based models without syntactic annotation , is pretty straight - forward .", "label": "", "metadata": {}, "score": "68.84795"}
{"text": "On the contrary , since sub - commodity 2 has been identified as an analyzed dimension ( by assignment of dimension B 504 ) , sub - commodity 2 also inherits dimension A 502 which implies sub - commodity 2 will be analyzed with dimension A 502 and B 504 attributes .", "label": "", "metadata": {}, "score": "69.04088"}
{"text": "This is indicated by block 300 in .FIG .9 .Next , the trained SLM 310 parses the input data as indicated by block 302 .During parsing , SLM 310 accesses the semantic application schema 312 .In doing so , SLM 310 enforces the template structure in schema 312 during the parsing operation .", "label": "", "metadata": {}, "score": "69.11807"}
{"text": "( VP ( NP ( PRO he ) ) ( VB goes ) ) .and tree - to - string rules are : .This will create a valid translation .However , the span over the word ' he ' will be labelled as PRO by the first rule , and NP by the 3rd rule .", "label": "", "metadata": {}, "score": "69.189"}
{"text": "The process of attempting to understand what the user has expressed is commonly referred to as natural language understanding ( NLU ) or , if the input modality being used by the user is speech , the process is referred to as spoken language understanding ( SLU ) .", "label": "", "metadata": {}, "score": "69.52844"}
{"text": "By way of example only , .The drives and their associated computer storage media discussed above and illustrated in .FIG .1 , provide storage of computer readable instructions , data structures , program modules and other data for the computer 110 .", "label": "", "metadata": {}, "score": "70.033005"}
{"text": "FIG .8 .The constituent boundaries for the \" attendee \" semantic label 224 are the words \" John \" and \" Smith \" .Therefore , during the training step 230 , any parses generated by the SLM which do not combine \" John \" and \" Smith \" together as a single constituent will not be considered .", "label": "", "metadata": {}, "score": "70.23121"}
{"text": "Conditional Markov Models model the likelihood of a sequence of labels l 1 , . . .l t assigned to a sequence of text node feature - vectors f 1 , . . ., f t as a linear function of the individual features of each text node .", "label": "", "metadata": {}, "score": "70.27283"}
{"text": "Conditional Markov Models model the likelihood of a sequence of labels l 1 , . . .l t assigned to a sequence of text node feature - vectors f 1 , . . ., f t as a linear function of the individual features of each text node .", "label": "", "metadata": {}, "score": "70.27283"}
{"text": "In this example , the assignment of dimension A 502 resulted in five inherited dimensions to sub - commodity 2 and constituents 1 - 4 406 .Since dimension B 504 was assigned for sub - commodity 2 , not only is sub - commodity 2 analyzed to this dimension , but also inherited to constituent 4 .", "label": "", "metadata": {}, "score": "70.54884"}
{"text": "MXTERMINATOR .A sentence tag which bounds some span of text which represents a sentence .STANFORD .A part - of - speech ( POS ) tag .There are many types of POS tags , such as NN , NNS , VB , VBD , etc .", "label": "", "metadata": {}, "score": "71.3653"}
{"text": "--DisallowNonTermConsecTarget and --NonTermConsecSource .We may want to restrict if there can be neighboring non - terminals in rules .In hierarchical models there is a bad effect on decoding to allow neighboring non - terminals on the source side .The default is to disallow this -- it is allowed on the target side .", "label": "", "metadata": {}, "score": "71.41347"}
{"text": "4 is a diagram of a level 1 commodity constituent model that is associated with the CCM level 1 phase in an exemplary embodiment ; .FIG .5 is a diagram of a level 2 commodity constituent model that is associated with the CCM level 2 phase in an exemplary embodiment ; and .", "label": "", "metadata": {}, "score": "71.68807"}
{"text": "The Unsupervised Language Learning Project Page .Humans are able to acquire linguistic knowledge in a more or less unsupervised manner .Although machines lack the contextual situation of a human learner , as well as whatever innate knowledge humans might have , much of the structure of natural language is distributionally detectable .", "label": "", "metadata": {}, "score": "71.80652"}
{"text": "5 requires the automatic extraction of sub - entities of interest from the entities identified at step 150 .Not all entities will necessarily require sub - entity extraction , the prototypical example is extraction of name parts ( for example title , first , middle / maiden / nick , last , suffix ) from full - name entities .", "label": "", "metadata": {}, "score": "72.20516"}
{"text": "5 requires the automatic extraction of sub - entities of interest from the entities identified at step 150 .Not all entities will necessarily require sub - entity extraction , the prototypical example is extraction of name parts ( for example title , first , middle / maiden / nick , last , suffix ) from full - name entities .", "label": "", "metadata": {}, "score": "72.20516"}
{"text": "24 is an example search application according to a preferred embodiment of the present invention over corporate biographical data extracted from the Australian web .Summary hits from a query on \" patent attorney \" are shown ; .FIG .25 is the full extracted record from the first hit in FIG .", "label": "", "metadata": {}, "score": "72.45859"}
{"text": "24 is an example search application according to a preferred embodiment of the present invention over corporate biographical data extracted from the Australian web .Summary hits from a query on \" patent attorney \" are shown ; .FIG .25 is the full extracted record from the first hit in FIG .", "label": "", "metadata": {}, "score": "72.45859"}
{"text": "It generates opens NP to cover the input span [ 2 . .5] by using hypothesis 10 , which coveres the span [ 3 . .4 ] .Note that this rule allows us to do something that is not possible with a simple phrase - based model .", "label": "", "metadata": {}, "score": "72.546906"}
{"text": "Such interfaces ( such as spoken language interfaces ) are sometimes one of the only interfaces practicable as opposed to other traditional methods of input , such as keyboards and mice .In natural language interfaces , the user speaks or otherwise interacts with the computer ( which can be a PDA , a desktop computer , a telephone , etc . ) and asks the computer to carryout certain actions .", "label": "", "metadata": {}, "score": "72.753105"}
{"text": "The normalized entities are stored 470 .Entities , normalized entities , or associated entities may also require further classification such as jobtitles / organizations being classified into either former or current .The entities / associated entities / normalized entities of interest within each span are classified 490 by a human .", "label": "", "metadata": {}, "score": "73.61491"}
{"text": "The normalized entities are stored 470 .Entities , normalized entities , or associated entities may also require further classification such as jobtitles / organizations being classified into either former or current .The entities / associated entities / normalized entities of interest within each span are classified 490 by a human .", "label": "", "metadata": {}, "score": "73.61491"}
{"text": "FIG .7A , frame label 222 identifies the \" schedule meeting \" action .Semantic slot labels 224 and 226 correspond to slots for the frame .Slot label 224 is the \" attendee \" slot and , as annotated , corresponds to the input word sequence ( or constituent ) \" John Smith \" .", "label": "", "metadata": {}, "score": "73.67232"}
{"text": "Coreference Resolution .Andy Schlaikjer ( hazen+ ) Read the Web , Spring 2006 School of Computer Science Carnegie Mellon University This is my project page for the coreference resolution work I 'm doing for the 10709 Read the Web course .", "label": "", "metadata": {}, "score": "73.83873"}
{"text": "--AllowOnlyUnalignedWords : This is related to the above .A rule may have words in it , but these may be unaligned words that are not connected .By default , at least one aligned word is required .Using this option , this requirement is dropped .", "label": "", "metadata": {}, "score": "74.110596"}
{"text": "24 , 25 , and 26 .FIG .24 shows summary hits from the query \" patent attorney \" over the extracted biographical data .FIG .25 shows the full record of the first hit , and FIG .26 shows the cached page from which the biographical information was automatically extracted .", "label": "", "metadata": {}, "score": "74.98703"}
{"text": "24 , 25 , and 26 .FIG .24 shows summary hits from the query \" patent attorney \" over the extracted biographical data .FIG .25 shows the full record of the first hit , and FIG .26 shows the cached page from which the biographical information was automatically extracted .", "label": "", "metadata": {}, "score": "74.98703"}
{"text": "6 .Once created , commodity tree 206 is ready to be processed by analytic engine 106 as described above in .FIG .1 .A commodity tree is created by the multi - dimensional commodity model 104 utilizing a two - phase process as described in .", "label": "", "metadata": {}, "score": "75.42332"}
{"text": "To activate this feature : . perl ) .The parameter non - terminals is used to specify privileged non - terminals .These are used for unknown words ( unless there is a unknown word label file ) and to define the non - terminal label on the input side , when this is not specified .", "label": "", "metadata": {}, "score": "75.97974"}
{"text": "The output of process 112 is the pruned data of constituents identified by the commodity constituent models having attributes and dimensions that meet the criteria provided in the model .The output of process 112 is used by process 114 .Process 114 performs the analysis defined by the commodity constituent models on the pruned data .", "label": "", "metadata": {}, "score": "76.51283"}
{"text": "In fact its default is 10 , which is not a useful setting for syntax models .In a target - syntax model , unknown words that just copied verbatim into the output need to get a non - terminal label .", "label": "", "metadata": {}, "score": "76.92078"}
{"text": "The audio signals detected by microphone 402 are converted into electrical signals that are provided to analog - to - digital converter 404 .A - to - D converter 404 converts the analog signal from microphone 402 into a series of digital values .", "label": "", "metadata": {}, "score": "76.97731"}
{"text": "the cat .the cat -DET- -NN-- ----NP----- .To support easier glue rules , the non - terminal ' X ' is also added for every span in the annotated string .Therefore , the input above is actually converted to : .", "label": "", "metadata": {}, "score": "77.183655"}
{"text": "Neither should the computing environment 100 be interpreted as having any dependency or requirement relating to any one or combination of components illustrated in the exemplary operating environment 100 .The invention is operational with numerous other general purpose or special purpose computing system environments or configurations .", "label": "", "metadata": {}, "score": "77.76991"}
{"text": "The information extraction process will also desirably associate the phrase \" John Smith \" with the concept of \" meeting attendee \" and the word \" Saturday \" with the concept of \" meeting day \" .Current approaches used for information extraction require handwritten grammars , usually context free grammars ( CFGs ) .", "label": "", "metadata": {}, "score": "77.856384"}
{"text": "Manufacturing operations typically involve some degree of monitoring production quality performance and provide quality control capability by monitoring and analyzing quality data .While various software applications exist for facilitating these activities , they are generally limited to a fixed set of programs that analyze and monitor quality data across static product characteristics .", "label": "", "metadata": {}, "score": "77.85886"}
{"text": "With reference to .FIG .1 , an exemplary system for implementing the invention includes a general purpose computing device in the form of a computer 110 .Components of computer 110 may include , but are not limited to , a processing unit 120 , a system memory 130 , and a system bus 121 that couples various system components including the system memory to the processing unit 120 .", "label": "", "metadata": {}, "score": "79.298836"}
{"text": "--SAMT 3 : not implemented .--SAMT 4 : As above , but in addition each previously unlabeled node is labeled as FAIL , so no syntactic constraint on grammar constraint remains .Note that you can also use both --LeftBinarize and --RightBinarize .", "label": "", "metadata": {}, "score": "82.34208"}
{"text": "For example , the feature vector f corresponding to the text node \" Jonathan Baxter \" might look like : .The feature vectors from the text nodes in a single document are concatenated 730 to form a feature vector sequence for that document : [ f 1 , f 2 , . . .", "label": "", "metadata": {}, "score": "82.34957"}
{"text": "For example , the feature vector f corresponding to the text node \" Jonathan Baxter \" might look like : .The feature vectors from the text nodes in a single document are concatenated 730 to form a feature vector sequence for that document : [ f 1 , f 2 , . . .", "label": "", "metadata": {}, "score": "82.34957"}
{"text": "9 illustrates the operation of the SLM parser during test or run time .FIG .10 is a data flow diagram illustrating a SLM 310 and a ranking component 316 .FIGS . 9 and 10 will be described in conjunction with one another .", "label": "", "metadata": {}, "score": "82.55809"}
{"text": "FIG .1 , for example , hard disk drive 141 is illustrated as storing operating system 144 , application programs 145 , other program modules 146 , and program data 147 .Note that these components can either be the same as or different from operating system 134 , application programs 135 , other program modules 136 , and program data 137 .", "label": "", "metadata": {}, "score": "82.692"}
{"text": "FIG .7A have been annotated with semantic labels , those labels are added to the syntactic labels to make a joint syntactic and semantic label .The first such label is indicated by numeral 234 and is the NP_Smith_attendee label .", "label": "", "metadata": {}, "score": "83.440094"}
{"text": "Of course , it should be noted that ranking component 316 can be integrally formed with SLM component 310 , or it can be a separate component .In ranking parse trees 314 , ranking component 316 can rank in a number of different ways .", "label": "", "metadata": {}, "score": "83.68347"}
{"text": "The quality control and monitoring process enabled by this invention delivers the platform for time relevant quality data analysis , analysis of patterns and trends , predictive indicators , and uncovering the quality ' needle in the haystack ' .As described above , the present invention can be embodied in the form of computer - implemented processes and apparatuses for practicing those processes .", "label": "", "metadata": {}, "score": "84.80704"}
{"text": "Computer 110 typically includes a variety of computer readable media .Computer readable media can be any available media that can be accessed by computer 110 and includes both volatile and nonvolatile media , removable and non - removable media .By way of example , and not limitation , computer readable media may comprise computer storage media and communication media .", "label": "", "metadata": {}, "score": "87.40484"}
{"text": "As an illustrative example the structured record could be generated in XML format as follows : .CEO and Group Managing Director Mr Corbett was appointed Chief Executive Officer and Group Managing Director in January 1999 , having been Chief Operating Officer since July 1998 , Managing Director Retail since July 1997 and Managing Director BIG W since May 1990 .", "label": "", "metadata": {}, "score": "87.69777"}
{"text": "As an illustrative example the structured record could be generated in XML format as follows : .CEO and Group Managing Director Mr Corbett was appointed Chief Executive Officer and Group Managing Director in January 1999 , having been Chief Operating Officer since July 1998 , Managing Director Retail since July 1997 and Managing Director BIG W since May 1990 .", "label": "", "metadata": {}, "score": "87.69777"}
{"text": "8) .Assuming a prespecified list of animal names , the feature vector for this document would then be : . , frequency - 3_fox , leadcap_fox , leadcap_jumping , leadcap_the , leadcap_what , title_fox , title_jumping , heading_what , heading_the , heading_fox , heading_did , emphasis_lazy , emphasis_quick , list_animal_fox , list_animal_dog].", "label": "", "metadata": {}, "score": "87.74333"}
{"text": "8) .Assuming a prespecified list of animal names , the feature vector for this document would then be : . , frequency - 3_fox , leadcap_fox , leadcap_jumping , leadcap_the , leadcap_what , title_fox , title_jumping , heading_what , heading_the , heading_fox , heading_did , emphasis_lazy , emphasis_quick , list_animal_fox , list_animal_dog].", "label": "", "metadata": {}, "score": "87.74333"}
{"text": "RAM 132 typically contains data and/or program modules that are immediately accessible to and/or presently being operated on by processing unit 120 .By way of example , and not limitation , .FIG .1 illustrates operating system 134 , application programs 135 , other program modules 136 , and program data 137 .", "label": "", "metadata": {}, "score": "90.20885"}
{"text": "Combinations of any of the above should also be included within the scope of computer readable media .The system memory 130 includes computer storage media in the form of volatile and/or nonvolatile memory such as read only memory ( ROM ) 131 and random access memory ( RAM ) 132 .", "label": "", "metadata": {}, "score": "90.857956"}
{"text": "In the embodiment shown in .FIG .2 , the slots for frame 200 include an \" attendee \" slot 204 , a \" date \" slot 206 and a \" location \" slot 208 .The job of the information extraction component is to identify frame 202 , and fill in the appropriate slots from a natural language user input such as the input sentence \" Schedule a meeting with John Smith on Saturday . \" In accordance with one embodiment of the present invention , frames 200 for applications correspond to a two - level structure such as that shown in .", "label": "", "metadata": {}, "score": "91.36723"}
{"text": "In addition to the monitor , computers may also include other peripheral output devices such as speakers 197 and printer 196 , which may be connected through an output peripheral interface 190 .The computer 110 may operate in a networked environment using logical connections to one or more remote computers , such as a remote computer 180 .", "label": "", "metadata": {}, "score": "91.68979"}
{"text": "FIG .8 .One example of annotated training data is that shown in .FIG .7A which contains the example sentence \" Schedule a meeting with John Smith on Saturday . \" for which a parse tree has been built ( as shown in .", "label": "", "metadata": {}, "score": "94.768555"}
{"text": "T . k .P .t . k .W . k .T . k . w . k . )i .N . k .P .p .i . k .W . k .T . k . w . k . t . k .", "label": "", "metadata": {}, "score": "94.96663"}
{"text": "For example , after successful entity extraction from the following ( truncated ) biography : . . . . .Dr Jonathan Baxter .Jonathan is the CEO of Panscient Technologies . . . . . .the system should have identified \" Dr Jonathan Baxter \" and \" Jonathan \" as separate names .", "label": "", "metadata": {}, "score": "94.97946"}
{"text": "For example , after successful entity extraction from the following ( truncated ) biography : . . . . .Dr Jonathan Baxter .Jonathan is the CEO of Panscient Technologies . . . . . .the system should have identified \" Dr Jonathan Baxter \" and \" Jonathan \" as separate names .", "label": "", "metadata": {}, "score": "94.97946"}
{"text": "T . k . )W . k .T . k . )W . k .T . k . )P .W . k .T . k . )T . k .S . k .", "label": "", "metadata": {}, "score": "95.708954"}
{"text": "This step in the process is commonly referred to as information extraction .Take as an example a user input sentence ( where the user says or types or handwrites ) \" Schedule a meeting with John Smith on Saturday \" .", "label": "", "metadata": {}, "score": "96.5202"}
{"text": "When used in a WAN networking environment , the computer 110 typically includes a modem 172 or other means for establishing communications over the WAN 173 , such as the Internet .The modem 172 , which may be internal or external , may be connected to the system bus 121 via the user input interface 160 , or other appropriate mechanism .", "label": "", "metadata": {}, "score": "96.751755"}
{"text": "It can be seen from .FIG .3 that the headwords that immediately precede the word \" on \" are \" schedule meeting \" .Thus , it can be appreciated that the probability of seeing the word \" on \" after the words \" schedule meeting \" is much greater than the probability of seeing the word \" on \" after the words \" John Smith \" .", "label": "", "metadata": {}, "score": "100.491196"}
{"text": "A user may enter commands and information into the computer 110 through input devices such as a keyboard 162 , a microphone 163 , and a pointing device 161 , such as a mouse , trackball or touch pad .Other input devices ( not shown ) may include a joystick , game pad , satellite dish , scanner , or the like .", "label": "", "metadata": {}, "score": "100.58127"}
{"text": "By way of example , and not limitation , .FIG .1 illustrates remote application programs 185 as residing on remote computer 180 .It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used .", "label": "", "metadata": {}, "score": "101.64808"}
{"text": "For example , assume that an input sentence is the same as discussed above \" Schedule a meeting with John Smith on Saturday . \"Assume also that the last recognized word is \" Smith \" such that the next word to be recognized will be \" on \" .", "label": "", "metadata": {}, "score": "102.08707"}
{"text": "T . k .P .w . k . h .h .P .t . k . w . k .W . k .T . k .P .t . k . w . k . h .", "label": "", "metadata": {}, "score": "102.389854"}
