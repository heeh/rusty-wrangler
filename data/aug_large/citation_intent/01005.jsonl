{"text": "Further reading .For a detailed comparison of the log - likelihood and chi - squared statistics , see Rayson P. , Berridge D. and Francis B. ( 2004 ) .Extending the Cochran rule for the comparison of word frequencies between corpora .", "label": "", "metadata": {}, "score": "29.35299"}
{"text": "Further reading .For a detailed comparison of the log - likelihood and chi - squared statistics , see Rayson P. , Berridge D. and Francis B. ( 2004 ) .Extending the Cochran rule for the comparison of word frequencies between corpora .", "label": "", "metadata": {}, "score": "29.35299"}
{"text": "ISBN 2 - 930344 - 50 - 4 .The log - likelihood test can be used for corpus comparison .See Rayson , P. and Garside , R. ( 2000 ) .Comparing corpora using frequency profiling .In proceedings of the workshop on Comparing Corpora , held in conjunction with the 38th annual meeting of the Association for Computational Linguistics ( ACL 2000 ) .", "label": "", "metadata": {}, "score": "30.646772"}
{"text": "ISBN 2 - 930344 - 50 - 4 .The log - likelihood test can be used for corpus comparison .See Rayson , P. and Garside , R. ( 2000 ) .Comparing corpora using frequency profiling .In proceedings of the workshop on Comparing Corpora , held in conjunction with the 38th annual meeting of the Association for Computational Linguistics ( ACL 2000 ) .", "label": "", "metadata": {}, "score": "30.646772"}
{"text": "( pdf ) .Andrew Hardie has created a significance test system which calculates Chi - squared , log - likelihood and the Fisher Exact Test for contingency tables using R. .There is an increasing movement in corpus linguistics and other fields ( e.g. Psychology ) to move away from null hypothesis testing and p - values , and to calculate effect size measures as well as significance values .", "label": "", "metadata": {}, "score": "30.758675"}
{"text": "( pdf ) .Andrew Hardie has created a significance test system which calculates Chi - squared , log - likelihood and the Fisher Exact Test for contingency tables using R. .There is an increasing movement in corpus linguistics and other fields ( e.g. Psychology ) to move away from null hypothesis testing and p - values , and to calculate effect size measures as well as significance values .", "label": "", "metadata": {}, "score": "30.758675"}
{"text": "For the algorithmically minded : this post should act as a somewhat idiosyncratic approach to Dunning 's Log - likelihood statistic .For the hermeneutically minded : this post should explain why you might need _ any _ log - likelihood statistic .", "label": "", "metadata": {}, "score": "31.227417"}
{"text": "For the algorithmically minded : this post should act as a somewhat idiosyncratic approach to Dunning 's Log - likelihood statistic .For the hermeneutically minded : this post should explain why you might need _ any _ log - likelihood statistic .", "label": "", "metadata": {}, "score": "31.227417"}
{"text": "The calculation for the expected values takes account of the size of the two corpora , so we do not need to normalize the figures before applying the formula .We can then calculate the log - likelihood value according to this formula : .", "label": "", "metadata": {}, "score": "33.88731"}
{"text": "The calculation for the expected values takes account of the size of the two corpora , so we do not need to normalize the figures before applying the formula .We can then calculate the log - likelihood value according to this formula : .", "label": "", "metadata": {}, "score": "33.88731"}
{"text": "Similarly , the result of Bayesian inference applied to a choice of single multinomial distribution for all rows of the contingency table taken together versus the more general alternative of a separate multinomial per row produces results very similar to the G statistic .", "label": "", "metadata": {}, "score": "36.065556"}
{"text": "247 - 48 ) .On the other hand , Paul Rayson has argued that by reducing frequency to a rank measure , this approach discards \" most of the evidence we have about the distribution of words \" ( 2 ) .", "label": "", "metadata": {}, "score": "36.37224"}
{"text": "For a more complete account of how this is calculated , see Wordhoard .But there 's a problem with this measure , as Adam Kilgarriff has pointed out ( 1 , pp .237 - 38 , 247 - 48 ) .", "label": "", "metadata": {}, "score": "37.496674"}
{"text": "However , my script compares relative frequencies between the two corpora in order to insert an indicator for ' + ' overuse and ' - ' underuse of corpus 1 relative to corpus 2 .How to calculate log likelihood .Note that the value ' c ' corresponds to the number of words in corpus one , and 'd ' corresponds to the number of words in corpus two ( N values ) .", "label": "", "metadata": {}, "score": "37.81755"}
{"text": "However , my script compares relative frequencies between the two corpora in order to insert an indicator for ' + ' overuse and ' - ' underuse of corpus 1 relative to corpus 2 .How to calculate log likelihood .Note that the value ' c ' corresponds to the number of words in corpus one , and 'd ' corresponds to the number of words in corpus two ( N values ) .", "label": "", "metadata": {}, "score": "37.81755"}
{"text": "For samples of a reasonable size , the G -test and the chi - squared test will lead to the same conclusions .For very small samples the multinomial test for goodness of fit , and Fisher 's exact test for contingency tables , or even Bayesian hypothesis selection are preferable to either the chi - squared test or the G -test .", "label": "", "metadata": {}, "score": "38.130344"}
{"text": "Thus the probability distribution log2Probability(int [ ] ) is over an array of counts for the dimensions of the underlying multivariate distribution .This class also contains a static method log2MultinomialCoefficient(int [ ] ) to compute multinomial coefficients .The method chiSquared(int [ ] ) returns the chi - squared statistic for a sample of outcome counts represented by an array of integers .", "label": "", "metadata": {}, "score": "38.614246"}
{"text": "In the language modeling task , a similarity - based model is used to improve probability estimates for unseen bigrams in a back - off language model .The similaritybased method yields a 20 % perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech - recognition error .", "label": "", "metadata": {}, "score": "38.78109"}
{"text": "That 's what I hope to address in my next post .I think my urge to multiply it by Dunning 's log - likelihood may have been the needless caution of someone who 's using an unfamiliar metric and is n't sure yet whether it will work unassisted .", "label": "", "metadata": {}, "score": "39.561104"}
{"text": "The higher the G2 value , the more significant is the difference between two frequency scores .Effect Size for Log Likelihood ( ELL ) - see Johnston et al ( 2006 ) ELL varies between 0 and 1 ( inclusive ) .", "label": "", "metadata": {}, "score": "40.501297"}
{"text": "The higher the G2 value , the more significant is the difference between two frequency scores .Effect Size for Log Likelihood ( ELL ) - see Johnston et al ( 2006 ) ELL varies between 0 and 1 ( inclusive ) .", "label": "", "metadata": {}, "score": "40.501297"}
{"text": "Despite the persistence of this theory , however , there is widespread agreement about its empirical shortcomings ( McCawley , 1968 ; Fodor , 1977 ) .As an alternative , some critics of the Katz - Fodor theory ( e.g. ( Johnson - Laird , 1983 ) ) have abandoned the treatment of selectional constraints as semantic , instead treating them as indistinguishable from inferences made on the basis of factual knowledge .", "label": "", "metadata": {}, "score": "40.53888"}
{"text": "Statistical NLP methods determine the likelihood of a word combination from its frequency in a training corpus .However , the nature of language is such that many word combinations are infrequent and do not occur in any given corpus .In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on \" most similar \" words .", "label": "", "metadata": {}, "score": "40.699127"}
{"text": "Gries )The form of the log - likelihood calculation that I use comes from the Read and Cressie research cited in Rayson and Garside ( 2000 ) rather than the form derived in Dunning ( 1993 ) .For strictly positive x it is easy to compute these terms , while if x is zero ln(x / E ) will be negative infinity .", "label": "", "metadata": {}, "score": "40.782433"}
{"text": "Gries )The form of the log - likelihood calculation that I use comes from the Read and Cressie research cited in Rayson and Garside ( 2000 ) rather than the form derived in Dunning ( 1993 ) .For strictly positive x it is easy to compute these terms , while if x is zero ln(x / E ) will be negative infinity .", "label": "", "metadata": {}, "score": "40.782433"}
{"text": "Specifically , the proposed measure is a combined approach that inherits the edge - based approach of the edge counting scheme , which is then enhanced by the node - based approach of the information content calculation .When tested on a common data set of word pair similarity ratings , the proposed approach outperforms other computational models .", "label": "", "metadata": {}, "score": "41.627907"}
{"text": "The statistic used is Chi^2 .Dunning discussed the use of this statistic in his 1993 .Dunning proposed the log - likelihood ratio as a better statistic for word frequency analysis .He suggested that exact tests may even be better .", "label": "", "metadata": {}, "score": "42.086266"}
{"text": "Deterministic annealing is used to find lowest distortion sets of clusters : as the an-nealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchi- cal \" soft \" clustering of the data .Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held - out test data . .", "label": "", "metadata": {}, "score": "42.35794"}
{"text": "It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantifie ... \" .This paper presents a new approach for measuring semantic similarity / distance between words and concepts .", "label": "", "metadata": {}, "score": "42.37515"}
{"text": "Returns the log ( base 2 ) probability of the distribution of outcomes specified in the argument . int .numDimensions ( ) Returns the number of dimensions in this multinomial .MultinomialDistribution .Construct a multinomial distribution based on the specified multivariate distribution .", "label": "", "metadata": {}, "score": "42.4216"}
{"text": "The solution Kilgarriff offers is to instead use a Mann - Whitney ranks test .This allows us to assess how consistently a given term is more common in one corpus than in another .For instance , suppose I have eight text samples of equal length .", "label": "", "metadata": {}, "score": "42.53492"}
{"text": "Thus Log Ratio .But what is Log Ratio ?Log Ratio is my attempt to suggest a better statistic for keywords / key tags than log - likelihood , which is the statistic normally used .The problem with this accepted procedure is that log - likelihood is a statistical significance measure - it tells us how much evidence we have for a difference between two corpora .", "label": "", "metadata": {}, "score": "42.637154"}
{"text": "We could use the ratio of relative frequencies as a keyness statistic but , in my view , it is useful to convert it into a logarithm ( \" log \" for short ) first - specifically , the logarithm to base 2 or binary logarithm .", "label": "", "metadata": {}, "score": "42.907356"}
{"text": "The outcome is a collocation measure very similar to Mutual Information .Another advantage of Log Ratio is that it can be used for lockwords as well as keywords , which log - likelihood ca n't .A Log Ratio of zero or nearly zero indicates a word that is \" locked \" between Corpus A and Corpus B. In consequence the new version of CQPweb allows you to look at lockwords - to my knowledge , the first general corpus tool that makes this possible .", "label": "", "metadata": {}, "score": "43.015945"}
{"text": "Notes : 1 .Please enter plain numbers without commas ( or other non - numeric characters ) as they will confuse the calculator !The LL wizard shows a plus or minus symbol before the log - likelihood value to indicate overuse or underuse respectively in corpus 1 relative to corpus 2 .", "label": "", "metadata": {}, "score": "43.273605"}
{"text": "Notes : 1 .Please enter plain numbers without commas ( or other non - numeric characters ) as they will confuse the calculator !The LL wizard shows a plus or minus symbol before the log - likelihood value to indicate overuse or underuse respectively in corpus 1 relative to corpus 2 .", "label": "", "metadata": {}, "score": "43.273605"}
{"text": "We first describe an algorithm for converting the hierarchical structure of WordNet [ 13 ] ... \" .We discuss a method for augmenting and rearranging a structured lexicon in order to make it more suitable for a topic labeling task , by making use of lexical association information from a large text corpus .", "label": "", "metadata": {}, "score": "43.6281"}
{"text": "Cumming , G. ( 2014 )The New Statistics : Why and How .Psychological Science .25(1 ) , pp .7 - 29 .DOI : 10.1177/0956797613504966 [ Mentions that Cohen 's D is widely used but has pitfalls . ]", "label": "", "metadata": {}, "score": "43.733807"}
{"text": "Cumming , G. ( 2014 )The New Statistics : Why and How .Psychological Science .25(1 ) , pp .7 - 29 .DOI : 10.1177/0956797613504966 [ Mentions that Cohen 's D is widely used but has pitfalls . ]", "label": "", "metadata": {}, "score": "43.733807"}
{"text": "We describe and evaluate experimentally a method for clustering words according to their dis- tribution in particular syntactic contexts .Words are represented by the relative frequency distributions of contexts in which they appear , and relative entropy between those distributions is used as the si ... \" .", "label": "", "metadata": {}, "score": "44.440186"}
{"text": "But for a shameless literary hack like myself , it 's no trouble to cut the Gordian knot with an improvised algorithm that combines both measures .For instance , one could multiply rho by the log of Dunning 's log likelihood ( represented here as G - squared ) ...", "label": "", "metadata": {}, "score": "44.639427"}
{"text": "Returns the chi - squared statistic for rejecting the null hypothesis that the specified samples were generated by this distribution .static double .log2MultinomialCoefficient ( int [ ] sampleCounts )Returns the log ( base 2 ) multinomial coefficient for the specified counts . double .", "label": "", "metadata": {}, "score": "44.92034"}
{"text": "We often then multiply by a normalisation factor - 1,000 or 1,000,000 being the most usual factors - but this is , strictly speaking , optional and merely for presentation purposes .Once we have made a frequency into a relative frequency by dividing it by the corpus size , we can compare it to the relative frequency of the same item in a different corpus .", "label": "", "metadata": {}, "score": "45.208633"}
{"text": "But the second problem is that it does n't penalize words that are extremely common in a very restricted context - character names and the like .I imagine multiplying by IDF will address the first of these problems , but actually aggravate the second one , inasmuch as it favors terms with a restricted distribution .", "label": "", "metadata": {}, "score": "46.461407"}
{"text": "We can make use of the Dunning data here to solve the first problem though not the second .Unlike in a normal Wordle , where size is frequency , here size is Dunning score : and the word clouds are paired , so each one represents two ends of a comparison .", "label": "", "metadata": {}, "score": "47.212936"}
{"text": "For a more detailed review of various statistics , see : Rayson , P. ( 2003 ) .Matrix : A statistical method and software tool for linguistic analysis through corpus comparison .Ph.D. thesis , Lancaster University .And to read more about the use of log - likelihood with tag - level comparisons , see : Rayson , P. ( 2008 ) .", "label": "", "metadata": {}, "score": "47.7644"}
{"text": "For a more detailed review of various statistics , see : Rayson , P. ( 2003 ) .Matrix : A statistical method and software tool for linguistic analysis through corpus comparison .Ph.D. thesis , Lancaster University .And to read more about the use of log - likelihood with tag - level comparisons , see : Rayson , P. ( 2008 ) .", "label": "", "metadata": {}, "score": "47.7644"}
{"text": "So this is reasonably good at giving us a sense of the differences between corpuses as objectively defined .So far , so good .But these lists are a ) not engaging , and b ) do n't use frequency data .", "label": "", "metadata": {}, "score": "47.92356"}
{"text": "We then use lexical cooccurrence statistics in combination with these categories to classify proper names , assign more specific senses to broadly defined terms , and classify new words into existing categories .We also describe how to use these statistics to assign schema - like information to the categories and show how the new categories improve a text - labeling algorithm .", "label": "", "metadata": {}, "score": "48.027527"}
{"text": "The multinomial coefficient is often written using a notation similar to that used for the factorial as ( sampleCounts[0], ... ,sampleCounts[n-1 ] ) !Tools . by Jay J. Jiang , David W. Conrath - Proc of 10th International Conference on Research in Computational Linguistics , ROCLING'97 , 1997 . \" ...", "label": "", "metadata": {}, "score": "48.361015"}
{"text": "The similaritybased methods perform up to 40 % better on this particular task . ...Sections 2.3.1 and 2.3.2 discuss two related information - theoretic functions , the KL divergence and the Jensen - Shannon divergence .Section 2.3.3 describes the L 1 norm , ... . by Ido Dagan , Lillian Lee , Fernando Pereira - In Proceedings of the Association for Computational Linguistics , 1997 . \" ...", "label": "", "metadata": {}, "score": "49.060364"}
{"text": "We also conclude that events that occur only once in the training set have major impact on similarity - based estimates . by Marti A. Hearst , Hinrich Sch\u00fctze - Proc . of the Workshop on Extracting Lexical Knowledge , 1996 . \" ...", "label": "", "metadata": {}, "score": "49.40089"}
{"text": "This approximation was developed by Karl Pearson because at the time it was unduly laborious to calculate log - likelihood ratios .With the advent of electronic calculators and personal computers , this is no longer a problem .G -tests are coming into increasing use , particularly since they were recommended in the 1994 edition of the popular statistics text book by Sokal and Rohlf [ 1 ] .", "label": "", "metadata": {}, "score": "49.525505"}
{"text": "Given a multidimensional space upon which a node represents a 2unique concept consisting of a certain amount of information , and an edge represents a direct association between two concepts , ... .by Fernando Pereira , Naftali Tishby , Lillian Lee - In Proceedings of the 31st", "label": "", "metadata": {}, "score": "49.83947"}
{"text": "The similarity - based methods perform up to 40 % better on this particular task .We al ... \" .We compare four similarity - based estimation methods against back - off and maximum - likelihood estimation methods on a pseudo - word sense disambiguation task in which we controlled for both unigram and bigram frequency .", "label": "", "metadata": {}, "score": "49.99418"}
{"text": "In Dylan Glynn & Justyna Robinson ( eds . ) , Corpus methods for semantics : quantitative studies in polysemy and synonymy , 365 - 389 .Amsterdam & Philadelphia : John Benjamins .[ In this paper , Stefan uses effect size measures Phi , Odds Ratio for 2 x 2 tables and Cramer 's V for larger r - by - c tables . ]", "label": "", "metadata": {}, "score": "50.347107"}
{"text": "In Dylan Glynn & Justyna Robinson ( eds . ) , Corpus methods for semantics : quantitative studies in polysemy and synonymy , 365 - 389 .Amsterdam & Philadelphia : John Benjamins .[ In this paper , Stefan uses effect size measures Phi , Odds Ratio for 2 x 2 tables and Cramer 's V for larger r - by - c tables . ]", "label": "", "metadata": {}, "score": "50.347107"}
{"text": "The outcome of our research showed some unexpected but interesting phenomena .In the end , we could explain ( and predict ) these phenomena by the statistics we used ( our paper is in prep . )The previous paragraph may be a bit cryptic , so here is an example .", "label": "", "metadata": {}, "score": "50.36416"}
{"text": "Both from the user end ( no one will wait that long for data to load ) and from the server end ( we ca n't handle too many concurrent queries , and longer queries means more concurrent ones ) .But Wordle clouds and UI issues aside , the base idea has all sorts of applications I 'll get into more later .", "label": "", "metadata": {}, "score": "50.65351"}
{"text": "More specifically : in what way do corpus linguists understand the mathematical / statistical aspects of corpus linguistics .At the risk of being off - topic to this thread , I 'll explain myself .In my own research , I started with the application of some ' standard ' corpus frequency analysis techniques .", "label": "", "metadata": {}, "score": "50.832474"}
{"text": "In this dissertation , I suggest that an answer to this question lies in the representation of conceptual . \" ... Introduction An impressive array of statistical methods have been developed for word sense identification .They range from dictionary - based approaches that rely on definitions ( Vronis and Ide 1990 ; Wilks et al .", "label": "", "metadata": {}, "score": "50.917503"}
{"text": "All outcomes with values of more than zero must have a non - zero probability .Note that the probability returned is normalized for all sets of the same number of samples .The definition of the probability value for multinomials is : . chiSquared . public double chiSquared ( int [ ] sampleCounts ) .", "label": "", "metadata": {}, "score": "51.087"}
{"text": "The full descriptions could tell us : but as a test case , it should be informative to use only the texts themselves to see the difference .That leads a tricky question .Just what does it mean to compare usage frequencies across two corpuses ?", "label": "", "metadata": {}, "score": "51.325752"}
{"text": "The full descriptions could tell us : but as a test case , it should be informative to use only the texts themselves to see the difference .That leads a tricky question .Just what does it mean to compare usage frequencies across two corpuses ?", "label": "", "metadata": {}, "score": "51.325752"}
{"text": "The general formula for Pearson 's chi - squared test statistic is .i .O .i .E .i . )E .i . where ln denotes the natural logarithm ( log to the base e ) and the sum is again taken over all cells .", "label": "", "metadata": {}, "score": "51.50152"}
{"text": "Introduction An impressive array of statistical methods have been developed for word sense identification .They range from dictionary - based approaches that rely on definitions ( Vronis and Ide 1990 ; Wilks et al .1993 ) to corpus - based approaches that use only word cooccurrence frequencies extracted from large textual corpora ( Schfitze 1995 ; Dagan and Itai 1994 ) .", "label": "", "metadata": {}, "score": "51.610283"}
{"text": "Effect sizes in corpus linguistics : keywords , collocations and diachronic comparison .Presented at the ICAME 2014 conference , University of Nottingham .[ Vaclav uses Cohen 's D as an effect size measure . ] Gries , Stefan Th .", "label": "", "metadata": {}, "score": "51.683556"}
{"text": "Effect sizes in corpus linguistics : keywords , collocations and diachronic comparison .Presented at the ICAME 2014 conference , University of Nottingham .[ Vaclav uses Cohen 's D as an effect size measure . ] Gries , Stefan Th .", "label": "", "metadata": {}, "score": "51.683556"}
{"text": "Is there any way to find words that are interesting on _ both _ counts ?I find it helpful to do this visually .Suppose we make a graph .We 'll put the addition score on the X axis , and the multiplication one on the Y axis , and make them both on a logarithmic scale .", "label": "", "metadata": {}, "score": "51.89161"}
{"text": "Since the number of documents in each corpus is also going to vary , it 's useful to replace the rank - sum ( U ) with a statistic \u03c1 ( Mann - Whitney rho ) that is U , divided by the product of the sizes of the two corpora .", "label": "", "metadata": {}, "score": "51.95359"}
{"text": "I should maybe dwell on this longer , because it really matters .Dunning 's is the method that seems to be most frequently used by digital humanities types , but the innards are n't exactly what you might think .In MONK , for example , the words with the highest Dunning scores are represented as bigger , which may lead users to think Dunning gives a simple frequency count .", "label": "", "metadata": {}, "score": "52.40079"}
{"text": "There 's no reason this should be so -- local histories are often the most intensely insular .Is there a historical pattern in the second - person - plural ?Bookworm says yes , emphatically --in a quite interesting way .", "label": "", "metadata": {}, "score": "52.50082"}
{"text": "This is brilliant stuff .I 've been taking the value of Dunning 's more or less on faith .By inspecting the formula I could see that it struck some kind of compromise between the measures of difference you 're characterizing as \" multiplicative \" and \" additive , \" but graphing that as a log scatterplot is a fabulous idea .", "label": "", "metadata": {}, "score": "52.831963"}
{"text": "Returns the log ( base 2 ) multinomial coefficient for the specified counts .The multinomial coefficient counts the number of ways the set of outcomes represented by the array of individual outcome counts can be linearly ordered .The result is : . / ( \u03a0 i sampleCounts[i ] ! )", "label": "", "metadata": {}, "score": "53.251812"}
{"text": "( I.e. , any term - document matrix that 's hundreds of thousands of elements in both dimensions ) .Am I wrong about this - ie , are your compute times higher for one or the other ?Anyhow , that 's not really an objection , just an explanation for why I might post my thing anyway without taking all this into account .", "label": "", "metadata": {}, "score": "53.36196"}
{"text": "International Journal of Corpus Linguistics .13:4 pp .519 - 549 .DOI : 10.1075/ijcl.13.4.06ray .Also see Dunning , Ted .Accurate Methods for the Statistics of Surprise and Coincidence .Computational Linguistics , Volume 19 , number 1 , pp .", "label": "", "metadata": {}, "score": "53.74186"}
{"text": "International Journal of Corpus Linguistics .13:4 pp .519 - 549 .DOI : 10.1075/ijcl.13.4.06ray .Also see Dunning , Ted .Accurate Methods for the Statistics of Surprise and Coincidence .Computational Linguistics , Volume 19 , number 1 , pp .", "label": "", "metadata": {}, "score": "53.74186"}
{"text": "Words are represented by the relative frequency distributions of contexts in which they appear , and relative entropy between those distributions is used as the similarity measure for clustering .Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership .", "label": "", "metadata": {}, "score": "53.99463"}
{"text": "So now we 've arrived at our measure - the binary log of the ratio of relative frequencies , or Log Ratio for short .If you followed the explanation above , then you know everything you need to know in order to interpret Log Ratio scores .", "label": "", "metadata": {}, "score": "54.561607"}
{"text": "Well , here 's how taking the log of the ratio works : .A word has the same relative frequency in A and B - the binary log of the ratio is 0 .A word is 2 times more common in A than in B - the binary log of the ratio is 1 .", "label": "", "metadata": {}, "score": "55.103622"}
{"text": "( 2 ) Paul Rayson , Matrix : A Statistical Method and Software Tool for Linguistic Analysis through Corpus Comparison .Unpublished Ph .D thesis , Lancaster University , 2003 , p. 47 .( 3 )The corpora used in this post were selected by Jordan Sellers , mostly from texts available in the Internet Archive , and corrected with a Python script described in this post .", "label": "", "metadata": {}, "score": "55.310364"}
{"text": "The Log Ratio statistic is an \" effect - size \" statistic , not a significance statistic : it does represent how big the difference between two corpora are for a particular keyword .It 's also a very transparent statistic in that it is easy to understand how it is calculated and why it represents the size of the difference .", "label": "", "metadata": {}, "score": "55.324074"}
{"text": "Maybe \" that \" actually _ is _ more distinctive than daimyo , or vice - versa .So by random chance , we 'd expect to have more outliers on the top of the graph than on the bottom .By using Bookworm to explore the actual texts , I can see that \" daimyo \" appears so often in large part because Open Library does n't recognize these two books are the same work .", "label": "", "metadata": {}, "score": "55.325634"}
{"text": "I might be a tiny bit tempted to tinker with the Dunning 's algorithm by varying the base that gets used for the log function .It 's supposed to be log base e , but by fiddling with the base I bet you could stretch the \" green \" region a bit toward the y axis ...", "label": "", "metadata": {}, "score": "55.374756"}
{"text": "Clearly , neither of these is working all that well .Basically , the first group are so rare they do n't tell us much : and the second group , with the intriguing addition of \" general \" , are so common as to be uninformative .", "label": "", "metadata": {}, "score": "55.504936"}
{"text": "First , I would like to thank Oliver and Ylva for starting this very interesting thread .I think one of the discussion items is whether linguists should implement the earlier mentioned ' bag of tricks ' themselves , for instance , how to put corpus ( frequency ) data into SPSS readable format .", "label": "", "metadata": {}, "score": "55.97509"}
{"text": "I know that the statistical aspects are not every linguist 's cup of tea , but a mere application of supposed standards will not do anymore : the researcher needs to really understand the techniques he / she uses .And because there are not many well established standards in Corpus Linguistics , as pointed out by Oliver Mason , the researcher should know the background and should , ideally , be capable of implementing new insights him / herself .", "label": "", "metadata": {}, "score": "55.975708"}
{"text": "This ties back to my point a few months ago that stopwords carry a lot of meaning in the aggregate .If I did n't actually really find the stopwords useful , I 'd be more inclined to put some serious effort into building my own log - difference comparison like the straight line above ; as it is , I 'm curious if anyone knows of some good ones .", "label": "", "metadata": {}, "score": "56.24688"}
{"text": "The two traditions complement each other .Corpus - based approaches have the advantage of being generally applicable to new texts , domains , and corpora without needing costly and perhaps error - prone parsing or semantic analysis .They require only training corpora in which the sense distinctions have been marked , but therein lies their weakness .", "label": "", "metadata": {}, "score": "56.269096"}
{"text": "That is , once we take a binary log , every point represents a doubling of the ratio .This is very useful to help us focus on the overall magnitude of the difference ( 4 vs. 8 vs. 16 ) rather than differences that are pretty close together ( e.g. 4 vs. 5 vs. 6 ) .", "label": "", "metadata": {}, "score": "56.581875"}
{"text": "Parameters : . distribution - Underlying multivariate distribution defining the constructed multinomial .log2Probability . public double log2Probability ( int [ ] sampleCounts ) .Returns the log ( base 2 ) probability of the distribution of outcomes specified in the argument .", "label": "", "metadata": {}, "score": "56.8973"}
{"text": "A word is 8 times more common in A than in B - the binary log of the ratio is 3 .A word is 16 times more common in A than in B - the binary log of the ratio is 4 .", "label": "", "metadata": {}, "score": "57.636658"}
{"text": "Post navigation .Log Ratio - an informal introduction .In the latest version of CQPweb ( v 3.1.7 ) a new statistic for keywords , collocations and lockwords is introduced , called Log Ratio . \" Log Ratio \" is actually my own made - up abbreviated title for something which is more precisely defined as either the binary log of the ratio of relative frequencies or the binary log of the relative risk .", "label": "", "metadata": {}, "score": "57.67196"}
{"text": "n tendencies into associations of words to certain hidden senses classes and associations between the classes themselves .More specifically , we model senses as probabilistic concepts or clusters c with corresponding clus ... . \" ...Selectional constraints are limitations on the applicability of predicates to arguments .", "label": "", "metadata": {}, "score": "58.233418"}
{"text": "Calculation time is n't a big issue , because it 's built into R as wilcox.test ( ) .But I 'm not sure how reliable it is with small corpora , and obviously it would become useless with individual texts .", "label": "", "metadata": {}, "score": "58.40933"}
{"text": "So what about addition ?Compensating for different corpus sizes , it 's also pretty easy to find out the number of more occurrences than we 'd expect based on the previous corpus .( For example , \" not \" appears about 1.4 million more times than we 'd expect in E given the number of times it appears in F and the total number of words in E. ) .", "label": "", "metadata": {}, "score": "58.616188"}
{"text": "With small documents , and rare words , you could get into a situation where there are a whole lot of zero - to - zero ties in the ranking list , and I 'm not sure yet what effect that would have on reliability .", "label": "", "metadata": {}, "score": "58.892906"}
{"text": "As promised , some quick thoughts broken off my post on Dunning Log - likelihood .There , I looked at _ big _ corpuses -- two history classes of about 20,000 books each .But I also wonder how we can use algorithmic comparison on a much smaller scale : particularly , at the level of individual authors or works .", "label": "", "metadata": {}, "score": "59.048065"}
{"text": "( Feel free to skip down to Dunning if you just want the best answer I 've got . )I 'm comparing E and F : suppose I say my goal to answer this question : .What words appear the most times more in E than in F , and vice versa ?", "label": "", "metadata": {}, "score": "60.064312"}
{"text": "( Feel free to skip down to Dunning if you just want the best answer I 've got . )I 'm comparing E and F : suppose I say my goal to answer this question : .What words appear the most times more in E than in F , and vice versa ?", "label": "", "metadata": {}, "score": "60.064312"}
{"text": "Sense disambiguation is an \" intermediate task \" ( Wilks and Stevenson , 1996 ) which is not an end in itself , but rather is necessary at one level or another to accomplish most natural language processing tasks .It is . by Lillian Lee , Fernando C. N. Pereira , Claire Cardie , Raymond Mooney - Machine Learning , 1999 . \" ... Abstract .", "label": "", "metadata": {}, "score": "60.619965"}
{"text": "Computing P - Values .As of LingPipe 3.2.0 , the dependency on Jakarta Commons Math was removed .As a result , we removed the two methods that computed p - values .Here 's their implementation in case you need the functionality ( you may need to increas the text size ) : . basisDistribution ( ) Returns the multivariate distribution that forms the basis of this multinomial distribution . double .", "label": "", "metadata": {}, "score": "60.734688"}
{"text": "But we very often want to know how big a difference is !For instance , if we look at the top 200 keywords in a list , we want to look at the \" most key \" words , i.e. the words where the difference in frequency is greatest .", "label": "", "metadata": {}, "score": "61.503693"}
{"text": "w .c .o .l . )H .i .j . )H .i . . . ) .H .\u03c0 . . .j . ) is the \" Mutual Information between the row vector and the column vector \" of the contingency table .", "label": "", "metadata": {}, "score": "61.552155"}
{"text": "The automatic disambiguation of word senses has been an interest and concern since the earliest days of computer treatment of language in the 1950 's .Sense disambiguation is an \" intermediate task \" ( Wilks and Stevenson , 1996 ) which is not an end in itself , but rather is necessary at one level o ... \" .", "label": "", "metadata": {}, "score": "61.705826"}
{"text": "One possible solution would be to simply draw a line between \" daimyo \" and \" that \" , and assume that words are interesting to the degree that they stick out beyond that line .That gives us the following word list , placed on that same chart : . ... which is a lot better .", "label": "", "metadata": {}, "score": "61.858307"}
{"text": "I 'll be interested to read it .We should be tossing around a bunch of different possible solutions to any given problem .That was exactly what I liked about your post on Dunnings : it insisted on looking inside the black box and asking what a widely - used algorithm is actually doing for us as humanists .", "label": "", "metadata": {}, "score": "62.420048"}
{"text": "Biometry : the principles and practice of statistics in biological research ., 3rd edition .New York : Freeman .ISBN 0 - 7167 - 2411 - 1 .Dunning , Ted ( 1993 ) .Accurate Methods for the Statistics of Surprise and Coincidence . , Computational Linguistics , Volume 19 , issue 1 ( March , 1993 ) .", "label": "", "metadata": {}, "score": "62.58875"}
{"text": "The number of degrees of freedom is the number of outcomes minus one .The lower the return value , the more likely the sample was derived from this distribution .The definition for the chi - square value is the sum of square differences between sample counts and expected counts , normalized by expected count : . where the expected counts are computed based on the underlying multivariate distribution and the total sample count : . where totalCount is the sum of all of the sample counts .", "label": "", "metadata": {}, "score": "62.869286"}
{"text": "We are n't restricted to questions where the genres are predefined by the Library of Congress .There is a _ lot _ to do with cross corpus comparisons in a library as large as the Internet Archive collection .We can compare authors , for example : I 'll post that bit tomorrow .", "label": "", "metadata": {}, "score": "62.98349"}
{"text": "If you compare ratios ( dividing word frequencies in the genre A that interests you by the frequencies in a corpus B used as a point of comparison ) , you 'll get a list of very rare words .But if you compare the absolute magnitude of the difference between frequencies ( subtracting B from A ) , you 'll get a list of very common words .", "label": "", "metadata": {}, "score": "63.349556"}
{"text": "But a lot of interesting thoughts can come from the unlikely events in here .For example , ' our ' and ' we ' are both substantially overrepresented in the national histories as opposed to the local histories .( BTW , I should note somewhere that both E and F include a fair number of historical _ documents _ , speeches , etc . , as well as histories themselves .", "label": "", "metadata": {}, "score": "63.864815"}
{"text": "I want to know whether \" lamb \" is significantly more common in the poetry corpus than in prose .But a log - likelihood test would have identified this word as more common in prose .In reality , one never has \" equal - sized \" documents , but the test is not significantly distorted if one simply replaces absolute frequency with relative frequency ( normalized for document size ) .", "label": "", "metadata": {}, "score": "64.85288"}
{"text": "I wo n't explain the details , except to say that like our charts it uses logarithms and that it is much more closely to our addition measure than to the multiplication one .On our E vs F comparison , it turns up the following word - positions ( in green ) as the 100 most significantly higher in E than F : .", "label": "", "metadata": {}, "score": "65.07251"}
{"text": "Can they explain my Howells fixation ?I 'll present the results in faux - wordle form as discussed last time .What does that look like ?Historians often hope that digitized texts will enable better , faster comparisons of groups of texts .", "label": "", "metadata": {}, "score": "65.14082"}
{"text": "According to the influential theo ... \" .Selectional constraints are limitations on the applicability of predicates to arguments .For example , the statement \" The number two is blue \" may be syntactically well formed , but at some level it is anomalous - BLUE is not a predicate that can be applied to numbers .", "label": "", "metadata": {}, "score": "65.5384"}
{"text": "We can represent it like it has to do with frequency , but it 's important to remember that it 's not .( Whence the curve on our plot ) .Ultimately , what 's useful is defined by results .", "label": "", "metadata": {}, "score": "66.27758"}
{"text": "In our example set , here are the top words that distinguish E from F by multiplication , by occurences in E divided by occurrences in F. For example , \" gradualism \" appears 61x more often in E than in F. .", "label": "", "metadata": {}, "score": "66.361435"}
{"text": "All of the common words from our initial sets of 12 additive words , and none of the rare ones , are included .It includes about half of the words my naive straight - line method produced : even \" skirmisher \" , which seemed to clump with the more common words , is n't frequent enough for Dunning to privilege it over a blander word like \" movement \" .", "label": "", "metadata": {}, "score": "67.28673"}
{"text": "If this is a problem with 20,000 books in each set , it will be far worse when we 're using smaller sets .That would suggest we want a method that takes into account the possibility of random fluctuations for rarer ones .", "label": "", "metadata": {}, "score": "69.11371"}
{"text": "\"Well , shoot - I was just finally getting around to writing something in response to your comments along these lines , and now you scuttle that with a better method .In terms of indiscriminately multiplying unrelated terms together , I also wonder what happens if you throw IDF into the mix - would Dunning - IDF scores be better than pure Dunning scores for revealing meaningful diction , or maybe worse ?", "label": "", "metadata": {}, "score": "69.53145"}
{"text": "I 'm not sure it 's better , but it could certainly go in the mix .( It 's possible what we really want is a tool like Gary King 's topic browser for choosing among different word relevance schemes based on our interests .", "label": "", "metadata": {}, "score": "69.66416"}
{"text": "Identifying diction that characterizes an author or genre : why Dunning 's may not be the best method .The basic question is just this : if I want to know what words or phrases characterize an author or genre , how do I find out ?", "label": "", "metadata": {}, "score": "70.42131"}
{"text": "Returns : .The chi - square estimate of the confidence that the specified samples were generated by this distribution .Throws : . IllegalArgumentException - If the number of outcome counts is not the same as the number of dimensions of this multinomial .", "label": "", "metadata": {}, "score": "70.55246"}
{"text": "I never thought I 'd say this , but : let 's wordle !Wordle in general is a heavily overrated form of text analysis ; Drew Conway has a nice post from a few months ago criticizing it because it does n't use a meaningful baseline of comparison , and uses spatial arrangement arbitrarily .", "label": "", "metadata": {}, "score": "70.591194"}
{"text": "In plain English , this can mean two completely different things .Say E and F are exactly the same overall length ( eg , each have 10,000 books of 100,000 words ) .Comparing Corpuses by Word Use .Historians often hope that digitized texts will enable better , faster comparisons of groups of texts .", "label": "", "metadata": {}, "score": "70.95561"}
{"text": "To wrap things up : I think it is very important for a corpus linguist to really understand the bag of tricks .And because corpus linguistics has no ( not yet ? ) bag of standard tricks , corpus linguists should have programming skills .", "label": "", "metadata": {}, "score": "71.61577"}
{"text": "For example , a speech recognizer may need to determine which of the two word combinations \" eat a peach \" and \" eat a beach \" is more likely .Statistical NLP met ... \" .Abstract .In many applications of natural language processing ( NLP ) it is necessary to determine the likelihood of a given word combination .", "label": "", "metadata": {}, "score": "72.96326"}
{"text": "It demotes oddities like \" canto , \" but also slightly demotes pronouns like \" thou \" and \" his , \" which may be very common in some works of poetry but not others .In general , it gives less weight to raw frequency , and more weight to the relative ubiquity of a term in different corpora .", "label": "", "metadata": {}, "score": "73.05731"}
{"text": "Le poids des mots : Proceedings of the 7th International Conference on Statistical analysis of textual data ( JADT 2004 ) , Louvain - la - Neuve , Belgium , March 10 - 12 , 2004 , Presses universitaires de Louvain , pp .", "label": "", "metadata": {}, "score": "73.43372"}
{"text": "Le poids des mots : Proceedings of the 7th International Conference on Statistical analysis of textual data ( JADT 2004 ) , Louvain - la - Neuve , Belgium , March 10 - 12 , 2004 , Presses universitaires de Louvain , pp .", "label": "", "metadata": {}, "score": "73.43372"}
{"text": "j .i .j .k . i .j . and .\u03c0 . . .j .i . k . i .j .i .j .k . i .j .I . r .", "label": "", "metadata": {}, "score": "74.35413"}
{"text": "( We could also put them together and color - code like MONK does , but I think it 's easier to get the categories straight by splitting them apart like this ) .One nice thing about this is that the statistical overrepresentation of ' county ' in class F really comes through .", "label": "", "metadata": {}, "score": "74.66132"}
{"text": "( BWT , I simply omit the hundreds of words that appear in E but never appear in F ; and I do n't use capitalized words because they tend to _ very _ highly concentrated and in fictional works in particular can cause very strange results .", "label": "", "metadata": {}, "score": "74.919846"}
{"text": "It simply takes too long .The database driving Bookworm can add up the counts for any individual word in about half a second ; it takes more like two minutes to add up all the words in a given set of books .", "label": "", "metadata": {}, "score": "74.97478"}
{"text": "i . )i . q .i .l .o .g . q .i . and .i .j .k . i .j .i .j .k . i .j .j .", "label": "", "metadata": {}, "score": "75.63276"}
{"text": "A journal article will follow in due course .The Centre for Corpus Approaches to Social Science is an ESRC - funded research centre ( grant reference : ES / K002155/1 ) located at Lancaster University and operating in partnership with the University Centre for Computer Corpus Research on Language ( UCREL ) and the Academy of Social Sciences .", "label": "", "metadata": {}, "score": "75.914635"}
{"text": "21 ] \" year \" \" has \" \" family \" \" father \" .[ 25 ] \" located \" \" parents \" \" land \" \" native \" .[29 ] \" built \" \" mill \" \" city \" \" member \" .", "label": "", "metadata": {}, "score": "75.9647"}
{"text": "G . [ .i .j .k . i .j . ][ .H .i .j . )H .i . . . ) .H .\u03c0 . . .j . ) . ]", "label": "", "metadata": {}, "score": "75.99313"}
{"text": "Where do the words we 're talking about fall ?This nicely captures our dilemma .The two groups are in opposite corners , and words do n't ever score highly on both .( BTW , log - scatter plots are fun .", "label": "", "metadata": {}, "score": "76.85045"}
{"text": "[29 ] \" wounded \" \" artillery \" \" division \" \" government \" .Significantly overrepresented in F , in order : .[ 1 ] \" county \" \" born \" \" married \" \" township \" .[", "label": "", "metadata": {}, "score": "77.05752"}
{"text": "We have about 150 books by each ( they 're among the most represented authors in the Open Library , which is why I choose it ) , which means lots of duplicate copies published in different years , perhaps some miscategorizations , certainly some OCR errors .", "label": "", "metadata": {}, "score": "78.21381"}
{"text": "Keyness : Appropriate metrics and practical issues .CADS International Conference 2012 .Corpus - assisted Discourse Studies : More than the sum of Discourse Analysis and computing ? , 13 - 14 September , University of Bologna , Italy .[ Presents the % DIFF effect size measure which Costas and Anna argue should be applied to pairwise corpus comparisons to calculate keyness . ]", "label": "", "metadata": {}, "score": "79.07086"}
{"text": "Keyness : Appropriate metrics and practical issues .CADS International Conference 2012 .Corpus - assisted Discourse Studies : More than the sum of Discourse Analysis and computing ? , 13 - 14 September , University of Bologna , Italy .[ Presents the % DIFF effect size measure which Costas and Anna argue should be applied to pairwise corpus comparisons to calculate keyness . ]", "label": "", "metadata": {}, "score": "79.07086"}
{"text": "A lot of what we 'll be interested in historically are subtle differences between closely related sets , so a good start might be the two Library of Congress subject classifications called \" History of the Americas , \" letters E and F. The Bookworm database has over 20,000 books from each group .", "label": "", "metadata": {}, "score": "79.15471"}
{"text": "A lot of what we 'll be interested in historically are subtle differences between closely related sets , so a good start might be the two Library of Congress subject classifications called \" History of the Americas , \" letters E and F. The Bookworm database has over 20,000 books from each group .", "label": "", "metadata": {}, "score": "79.15471"}
{"text": "[ 9 ] \" daughter \" \" son \" \" acres \" \" farm \" .[ 13 ] \" business \" \" in \" \" school \" \" is \" .[17 ] \" and \" \" building \" \" he \" \" died \" .", "label": "", "metadata": {}, "score": "79.214325"}
{"text": "One thing this does with fiction is pull out characteristic names of fictional characters .( \" Jack\",\"Dick\",\"etc \" ) .I 'm not sure if that 's good or bad behavior .Dunning scores multiplied by IDF to deprecate extremely common words .", "label": "", "metadata": {}, "score": "79.29608"}
{"text": "( That 's a historian 's category , of course , not a librarian 's one ) .If we were to generalize that , it might suggest the rise of several forms of authorial identification with national communities ( class , race , international , industrial ) in the late nineteenth century , and a corresponding tendency to not necessarily see local history as first - person history .", "label": "", "metadata": {}, "score": "79.46885"}
{"text": "In plain English , this can mean two completely different things .Say E and F are exactly the same overall length ( eg , each have 10,000 books of 100,000 words ) .It turns out that neither of these simple operations works all that well .", "label": "", "metadata": {}, "score": "80.49852"}
{"text": "The national words might not have turned up by my straight - line test , which seemed intent on finding all sorts of rarer military words ( \" skirmishers \" , for example ) .Looking at the official LC classification definition ( pdf ) , that turns out to be mostly be the case .", "label": "", "metadata": {}, "score": "83.09013"}
{"text": "Maybe it 's just about memoirs switching from F to E , say .But there might be : we could use this sort of data as a jumping off point for some explorations of nation - building and sectionalism .For example , clicking on the E results around 1900 gives books that use the words ' we ' and ' our ' the most .", "label": "", "metadata": {}, "score": "86.75015"}
{"text": "Still , though , the less frequent words seem less helpful .Are \" sherd \" and \" peyote \" and \" daimyo \" up there because they really characterize the difference between E and F , or because a few authors just happened to use them a lot ?", "label": "", "metadata": {}, "score": "87.173775"}
{"text": "( Sidenote-- interesting post by Ted Underwood today on the mechanics of creating a middle group between these two poles ) .As an example , let 's compare all the books in my library by Charles Dickens and William Dean Howells , respectively .", "label": "", "metadata": {}, "score": "106.73341"}
{"text": "It happens to be very common in a few works of poetry that are divided into cantos ( works for instance by Lord Byron and Walter Scott ) .So when everything is added up , yes , it 's more common in poetry - but it does n't broadly characterize the corpus .", "label": "", "metadata": {}, "score": "108.61612"}
