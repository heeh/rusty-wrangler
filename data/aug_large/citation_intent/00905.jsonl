{"text": "One sense per collocation .In Proceedings of the 5th DARPA speech and natural language workshop .Yarowsky , D. , Cucerzan , S. , Florian , R. , Schafer , C. , & Wicentowski , R. ( 2001 ) .The Johns Hopkins SENSEVAL2 system description .", "label": "", "metadata": {}, "score": "26.160486"}
{"text": "In this paper we show that for certain definitions of collocation , a polysemous word exhibits essentially only one sense per collocation .We test this empirical hypothesis ... \" .Previous work [ Gale , Church and Yarowsky , 1992 ] showed that with high probability a polysemous word has one sense per discourse .", "label": "", "metadata": {}, "score": "29.338158"}
{"text": "We test this empirical hypothesis for several definitions of sense and collocation , and discover that it holds with 90 - 99 % accuracy for binary ambiguities .We utilize this property in a disambiguation algorithm that achieves precision of 92 % using combined models of very local context .", "label": "", "metadata": {}, "score": "35.119934"}
{"text": "Links ] .[ 16 ] H. Schutze , \" Automatic word sense discrimination , \" Computational Linguistics , vol .[Links ] .[Links ] .[ 18 ] N. Ide , \" Parallel translations as sense discriminators , \" in SIGLEX Workshop On Standardizing Lexical Resources , 1999 .", "label": "", "metadata": {}, "score": "36.41029"}
{"text": "The algorithm is based on two powerful constraints -- that words tend to have one sense per discourse and one sense per collocation -- exploited in an iterative bootstrapping procedure .Tested accuracy exceeds 96 % . ... training sets .", "label": "", "metadata": {}, "score": "37.68317"}
{"text": "At the end of each iteration , the ' One sense per discourse ' property can be used to help preventing initially mistagged collocates and hence improving the purity of the seed sets .In order to avoid strong collocates becoming indicators for the wrong class , the class - inclusion threshold needs to be randomly altered .", "label": "", "metadata": {}, "score": "38.59841"}
{"text": "Thus , many word sense disambiguation algorithms use semi - supervised learning , which allows both labeled and unlabeled data .The Yarowsky algorithm was an early example of such an algorithm .Yarowsky 's unsupervised algorithm uses the ' One sense per collocation ' and the ' One sense per discourse ' properties of human languages for word sense disambiguation .", "label": "", "metadata": {}, "score": "38.73804"}
{"text": "In Proceedings of the 43rd annual meeting of the association for computational linguistics , Ann Arbor , MI .Dang , H. T. , & Palmer , M. ( 2002 ) .Combining contextual features for word sense disambiguation .In Proceedings of the SIGLEX / SENSEVAL workshop on WSD : Recent successes and future directions , in conjunction with ACL-02 , Philadelphia .", "label": "", "metadata": {}, "score": "41.90131"}
{"text": "Yarowsky , D. , & Florian , R. ( 2002 ) .Evaluating sense disambiguation across diverse parameter spaces .Journal of Natural Language Engineering , 8 ( 4 ) , 293 - 310 .CrossRef .Yi , S.-t . , Loper , E. , & Palmer , M. ( 2007 , April ) .", "label": "", "metadata": {}, "score": "43.32817"}
{"text": "The algorithm will continue to iterate until no more reliable collocations are found .The ' One sense per discourse ' property can be used here for error correction .For a target word that has a binary sense partition , if the occurrences of the majority sense A exceed that of the minor sense B by a certain threshold , the minority ones will be relabeled as A. According to Yarowsky , for any sense to be clearly dominant , the occurrences of the target word should not be less than 4 .", "label": "", "metadata": {}, "score": "44.118267"}
{"text": "Our results show that the multilingual approach outperforms the classification experiments where no additional evidence from other languages is used .These results confirm our initial hypothesis that each language adds evidence to further refine the senses of a given word .", "label": "", "metadata": {}, "score": "44.49279"}
{"text": "Links ] .[ 6 ] P. Resnik and D. Yarowsky , \" Distinguishing systems and distinguishing senses : New evaluation methods for word sense disambiguation , \" Natural Language Engineering , vol .[Links ] .[Links ] .", "label": "", "metadata": {}, "score": "44.654823"}
{"text": "This will also give us an opportunity to see a wide range of approaches : hand - coded resources and methods involving supervised , semi - supervised , and unsupervised training .Terminology [ J&M 19.1 , 2 ] .Word Sense Disambiguation [ J&M 20.1 ] .", "label": "", "metadata": {}, "score": "44.70791"}
{"text": "One solution some researchers have used is to choose a particular dictionary , and just use its set of senses .Generally , however , research results using broad distinctions in senses have been much better than those using narrow , so most researchers ignore the fine - grained distinctions in their work .", "label": "", "metadata": {}, "score": "44.971474"}
{"text": "Yarowsky 's method uses neither word trigrams nor part - of - speech trigrams , and is thus immune from both problems mentioned earlier , i.e. , sparse data , and the inability to discriminate among words with the same part of speech .", "label": "", "metadata": {}, "score": "45.02664"}
{"text": "[Links ] .[Links ] .[Links ] .[Links ] .[ 12 ] Y. Chan and H. Ng , \" Scaling up word sense disambiguation via parallel texts , \" in AAAI'05 : Proceedings of the 20th national conference on Artificial intelligence .", "label": "", "metadata": {}, "score": "45.378014"}
{"text": "Keywords .Word sense disambiguation Sense granularity Maximum entropy Linguistically motivated features Linear regression .Share .References .Agirre , E. , & Edmonds , P. ( 2007 ) .Word sense disambiguation : Algorithms and applications .Text , Speech and Language Technology Series ( Vol .", "label": "", "metadata": {}, "score": "45.41622"}
{"text": "Lee , Y. K. , Ng , H. T. , & Chia , T. K. ( 2004 ) .Supervised word sense disambiguation with support vector machines and multiple knowledge sources .In Proceedings of SENSEVAL-3 : Third international workshop on the evaluation of systems for the semantic analysis of text , Barcelona , Spain ( pp .", "label": "", "metadata": {}, "score": "45.530617"}
{"text": "This paper presents an unsupervised learning algorithm for sense disambiguation that , when trained on unannotated English text , rivals the performance of supervised techniques that require time - consuming hand annotations .The algorithm is based on two powerful constraints -- that words tend to have ... \" .", "label": "", "metadata": {}, "score": "45.558434"}
{"text": "An important area of NLP is the study of Word Sense Disambiguation ( WSD ) , which may assign a unique word sense to a word .Many experiments in recent years of both supervised ( Leacock 1993 ) and unsupervised ( Yarowsky , 1993 ) WSD algorithms have accomplished promising performance with a high precision rate .", "label": "", "metadata": {}, "score": "45.683617"}
{"text": "249 - 252 ) .Carpuat , M. , & Wu , D. ( 2007 ) .Improving statistical machine translation using word sense disambiguation .In Proceedings of the 2007 joint conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ( EMNLP - CoNLL ) ( pp .", "label": "", "metadata": {}, "score": "46.387245"}
{"text": "Links ] .[19 ] F. Smadja , \" Retrieving collocations from text : Xtract , \" Computational Linguistics , 19(1 ) , 1993 .[Links ] .[20 ] M. Stubbs , \" Two quantitative methods of studying phraseology in English , \" Corpus Linguistics 7(2 ) , 2002 .", "label": "", "metadata": {}, "score": "46.67336"}
{"text": "Current accuracy exceeds 99 % on the full task , and typically is over 90 % for even the most difficult ambiguities .In computational linguistics , word sense disambiguation ( WSD ) is the process of identifying which sense of a word is used in any given sentence , when the word has a number of distinct senses .", "label": "", "metadata": {}, "score": "48.630867"}
{"text": "The interdependence between the collocations is represented by the parts of their patterns that overlap .If one feature is a context word , and the other is a collocation , then they are egregiously interdependent if and only if the context word appears explicitly in the pattern of the collocation .", "label": "", "metadata": {}, "score": "48.712593"}
{"text": "In addition , words that occur near the target word in great frequency can be selected as seed collocations representative .This approach is not fully automatic , a human judge must decide which word will be selected for each target word 's sense , the outputs will be reliable indicators of the senses .", "label": "", "metadata": {}, "score": "48.84279"}
{"text": "Chan , Y. S. , Ng , H. T. , & Chiang , D. ( 2007 , June ) .Word sense disambiguation improves statistical machine translation .In Proceedings of the 45th annual meeting of the association of computational linguistics , Prague , Czech Republic .", "label": "", "metadata": {}, "score": "49.112698"}
{"text": "The residual examples ( 85 % - 98 % according to Yarowsky ) remain untagged .The algorithm should initially choose seed collocations representative that will distinguish sense A and B accurately and productively .This can be done by selecting seed words from a dictionary 's entry for that sense .", "label": "", "metadata": {}, "score": "49.422264"}
{"text": "Criteria for the manual grouping of verb senses .In Linguistics annotation workshop , held in conjunction with ACL-2007 , Prague , The Czech Republic .Edmonds , P. , & Cotton , S. ( 2001 ) .SENSEVAL-2 : Overview .", "label": "", "metadata": {}, "score": "49.881798"}
{"text": "This paper describes a program that disambiguates English word senses in unrestricted text using statistical models of the major Roget 's Thesaurus categories .Roget 's categories serve as approximations of conceptual classes .The categories listed for a word in Roget 's index tend to correspond to sense distinctions ; thus selecting the most likely category provides a useful level of sense disambiguation .", "label": "", "metadata": {}, "score": "49.961136"}
{"text": "In Proceedings of SemEval , held in conjunction with ACL 2007 , Prague , Czech Republic .Palmer , M. , Babko - Malaya , O. , & Dang , H. T. ( 2004 ) .Different sense granularities for different applications .", "label": "", "metadata": {}, "score": "50.21765"}
{"text": "Meaningful clustering of senses helps boost word sense disambiguation performance .In Proceedings of the 21st international conference on computational linguistics and the 44th annual meeting of the ACL , Sydney , Australia ( pp .105 - 112 ) .Navigli , R. , Litkowski , K. C. , & Hargraves , O. ( 2007 , June ) .", "label": "", "metadata": {}, "score": "50.282787"}
{"text": "OntoNotes: The 90 % solution .In Proceedings of HLT - NAACL06 , New York .Ide , N. , & Veronis , J. ( 1998 ) .Introduction to the special issue on word sense disambiguation : The state of the art .", "label": "", "metadata": {}, "score": "50.701176"}
{"text": "But over the last few years , there has n't been any major improvement in performance of any of these methods .It is instructive to compare the word sense disambiguation problem with the problem of part - of - speech tagging .", "label": "", "metadata": {}, "score": "50.864937"}
{"text": "Maximum entropy models for natural language ambiguity resolution .Ph.D. Thesis , University of Pennsylvania .Sanderson , M. ( 1994 ) .Word sense disambiguation and information retrieval .In Proceedings of the 17th International ACM SIGIR , Dublin , Ireland .", "label": "", "metadata": {}, "score": "50.924934"}
{"text": "This attempt used as data a punched - card version of Roget 's Thesaurus and its numbered \" heads \" , as an indicator of topics and looked for repetitions in text , using a set intersection algorithm .It was not very successful , as is described in some detail in ( Wilks , Y. et al .", "label": "", "metadata": {}, "score": "51.12986"}
{"text": "This is really problematic for a semantic representation : one word may have several meanings ( polysemy ) and several words may have the same or nearly the same meaning ( synonymy ) .Both of these can cause problems for NLP applications , including infomation extraction .", "label": "", "metadata": {}, "score": "51.22779"}
{"text": "In G. Barnbrook , P. Danielsson , & M. Mahlberg ( Eds . ) , Meaningful texts : The extraction of semantic information from monolingual and multilingual corpora ( pp .31 - 38 ) .Birmingham , UK : Birmingham University Press .", "label": "", "metadata": {}, "score": "51.354668"}
{"text": "33 - 40 ) .Chen , J. ( 2006 ) .Towards high - performance word sense disambiguation by combining rich linguistic knowledge and machine learning approaches .PhD Thesis , University of Pennsylvania .Chen , J. , Dligach , D. , & Palmer , M. ( 2007 ) .", "label": "", "metadata": {}, "score": "51.48058"}
{"text": "The use of dictionary ... . \" ...This paper describes a program that disambiguates English word senses in unrestricted text using statistical models of the major Roget 's Thesaurus categories .Roget 's categories serve as approximations of conceptual classes .", "label": "", "metadata": {}, "score": "51.575027"}
{"text": "Abstract .A system is provided for spelling correction in which the context of a wordn a sentence is utilized to determine which of several alternative or possible words was intended .The system successfully combines multiple types of features via Bayesian analysis through means for resolving egregious interdependencies among features .", "label": "", "metadata": {}, "score": "51.869774"}
{"text": "This paper presents a statistical decision procedure for lexical ambiguity resolution .The algorithm exploits both local syntactic patterns and more distant collocational evidence , generating an efficient , effective , and highly perspicuous recipe for resolving a given ambiguity .By identifying and utilizing only the single best disambiguating evidence in a target context , the algorithm avoids the problematic complex modeling of statistical dependencies .", "label": "", "metadata": {}, "score": "51.968533"}
{"text": "Thus the two sentences are essentially indistinguishable to the method of Schabes et al . , which analyzes the sentences at the level of their part - of - speech sequences .In general , the method of Schabes et al . is ineffective at correcting context - sensitive spelling errors whenever the offending word and the intended word have the same part of speech .", "label": "", "metadata": {}, "score": "52.146515"}
{"text": "[Links ] .[Links ] .[17 ] I.S.P. Nation , Learning Vocabulary in Another Language .Cambridge : Cambridge Press , 2001 .[Links ] .[ 18 ] N. Nesselhauf , \" The use of collocations by advanced learners of English and some implications for teaching , \" in Applied Linguistics , 24 ( 3 ) , 2003 .", "label": "", "metadata": {}, "score": "52.207718"}
{"text": "In Proceedings of the COLING / ACL'98 workshop on usage of WordNet for NLP , Montreal , Canada .Hanks , P. ( 1996 ) .Contextual dependencies and lexical sets .The International Journal of Corpus Linguistics , 1 , 1 .", "label": "", "metadata": {}, "score": "52.227272"}
{"text": "Another aspect of word sense disambiguation that differentiates it from part - of - speech tagging is the availability of training data .While it is relatively easy to assign parts of speech to text , training people to tag senses is far more difficult .", "label": "", "metadata": {}, "score": "52.252518"}
{"text": "Boston , MA .Palmer , M. , Dang , H. , & Fellbaum , C. ( 2007 , June ) .Making fine - grained and coarse - grained sense distinctions , both manually and automatically .Journal of Natural Language Engineering , 13 ( 2 ) , 137 - 163 .", "label": "", "metadata": {}, "score": "52.254692"}
{"text": "Levin , B. ( 1993 ) .English verb classes and alternations : A preliminary investigation .Chicago : University of Chicago Press .Lucke , J. F. , & Embretson , S. ( 1984 ) .The biases and mean squared errors of estimators of multinormal squared multiple correlation .", "label": "", "metadata": {}, "score": "52.40683"}
{"text": "This drastically reduces the size of the training corpus that is needed , thereby solving the aforementioned sparse - data problem .The method of Schabes et al . introduces a new problem , however .Because it analyzes sentences in terms of their part - of - speech sequences , it has trouble with errors in which the offending word has the same part of speech as the intended word .", "label": "", "metadata": {}, "score": "52.900627"}
{"text": "\" Yarowsky 's method combines these two types of features , context words and collocations , via the method of decision lists .A decision list is an ordered list of features that are used to make a decision in favor of one option or another .", "label": "", "metadata": {}, "score": "53.1781"}
{"text": "The system of claim 6 , and further including means for ordering the uneliminated remaining features in order of decreasing strength .Description .BACKGROUND OF THE INVENTION .Conventional spell checkers work by looking up each word in the target document in a dictionary .", "label": "", "metadata": {}, "score": "53.265495"}
{"text": "In Proceedings of NAACL 2007 , Rochester , NY .Zhong , Z. , Tou Ng , H. , & Chan , Y. S. ( 2008 , October ) .Word sense disambiguation using OntoNotes : An empirical study .In Proceedings of EMNLP 2008 , Waikiki , Honolulu , HI .", "label": "", "metadata": {}, "score": "53.57817"}
{"text": "Other . \" ...We present a syntax - based statistical translation model .Our model transforms a source - language parse tree into a target - language string by applying stochastic operations at each node .These operations capture linguistic differences such as word order and case marking .", "label": "", "metadata": {}, "score": "53.779522"}
{"text": "Links ] .[ 22 ] D. Wible and N.L. Tsao , \" StringNet as a computational resource for discovering and investigating linguistic constructions , \" in Proceedings of NAACL , 2010 .[Links ] .[ 23 ] D. Yarowsky , \" Unsupervised word sense disambiguation rivaling supervised methods , \" in Proceedings of the Annual Meeting of the ACL , 1995 .", "label": "", "metadata": {}, "score": "53.780907"}
{"text": "This could give the word different rankings or even different classifications .Alternatively , it can be done by identifying a single defining collocate for each class , and using for seeds only those contexts containing one of these defining words .", "label": "", "metadata": {}, "score": "53.847767"}
{"text": "[ 1 ] E. Agirre and P. Edmonds , Eds . , Word Sense Disambiguation , ser .Text , Speech and Language Technology .Dordrecht : Springer , 2006 .[Links ] .[ 2 ] R. Navigli , \" Word sense disambiguation : a survey , \" in ACM Computing Surveys , 2009 , vol .", "label": "", "metadata": {}, "score": "54.125504"}
{"text": "This problem has led others to develop alternative methods of context - sensitive spelling correction .Schabes et al . , in U.S. patent application Ser .However , Schabes et al .use part - of - speech trigrams , rather than word trigrams .", "label": "", "metadata": {}, "score": "54.182083"}
{"text": "A context - word feature tests for the presence of a particular word within \u00b1k words of the target word .A collocation feature tests for the presence of a particular pattern of words and part - of - speech tags around the target word .", "label": "", "metadata": {}, "score": "54.28675"}
{"text": "We present a syntax - based statistical translation model .Our model transforms a source - language parse tree into a target - language string by applying stochastic operations at each node .These operations capture linguistic differences such as word order and case marking .", "label": "", "metadata": {}, "score": "54.30272"}
{"text": "Bayes ' rule permits ascertaining the probability of each word in the confusion set , but it requires estimating the joint probability of all features of interest being simultaneously present for each word in the confusion set .In general , it would require a massive training corpus to be able to estimate these joint probabilities accurately .", "label": "", "metadata": {}, "score": "54.3369"}
{"text": "Stokoe , C. , Oakes , M. P. , & Tait , J. ( 2003 ) .Word sense disambiguation and information retrieval revisited .In Proceedings of the 26th annual international ACM SIGIR conference on research and development in information retrieval , Toronto , Canada .", "label": "", "metadata": {}, "score": "54.35572"}
{"text": "Retrieving with good sense .Information Retrieval , 2 ( 1 ) , 49 - 69 .CrossRef .Snyder , B. , & Palmer , M. ( 2004 , July ) .The English all - words task .In Proceedings of Senseval-3 : The third international workshop on the evaluation of systems for the semantic analysis of text .", "label": "", "metadata": {}, "score": "54.39415"}
{"text": "In Proceedings of NAACL - HLT 2006 , NY , 2006 .Dang , H. T. ( 2004 ) .Investigations into the role of lexical semantics in word sense disambiguation .PhD Thesis , University of Pennsylvania .Dang , H. T. , & Palmer , M. ( 2005 , June 26 - 28 ) .", "label": "", "metadata": {}, "score": "54.545418"}
{"text": "If one feature is a collocation feature , and the other is a context - word feature , then the two features are said to conflict if and only if the collocation feature explicitly tests for the presence of the context word .", "label": "", "metadata": {}, "score": "54.582775"}
{"text": "In the case of context - sensitive spelling correction , the hypotheses suggest various words that could have been intended when the target word was typed .The body of evidence that is used to decide among these hypotheses consists of two types of features about the context of the target word : context - word features and collocation features .", "label": "", "metadata": {}, "score": "54.60853"}
{"text": "WSD systems are normally tested by having their results on a task compared against those of a human .However , humans do not agree on the task at hand - give a list of senses and sentences , and humans will not always agree on which word belongs in which sense .", "label": "", "metadata": {}, "score": "54.765842"}
{"text": "Mihalcea , R. , Chklovski , T. , & Kilgarriff , A. ( 2004 , July ) .The Senseval-3 English lexical sample task .In Proceedings of Senseval-3 : The third international workshop on the evaluation of systems for the semantic analysis of text , Barcelona , Spain .", "label": "", "metadata": {}, "score": "55.40424"}
{"text": "The determination of which word was intended for the target word is therefore based on the strongest non - conflicting evidence available .This is the reason for sorting the features in order of decreasing strength in the training phase .An egregious interdependency between two features is defined as follows : if both features are context - word features , then the two features are said not to be egregiously interdependent .", "label": "", "metadata": {}, "score": "55.496094"}
{"text": "OntoNotes data is designed to provide clear sense distinctions , based on using explicit syntactic and semantic criteria to group WordNet senses , with sufficient examples to constitute high quality , broad coverage training data .Using similar syntactic and semantic features for WSD , we achieve performance comparable to that of human taggers , and competitive with the top results for the SemEval-2007 task .", "label": "", "metadata": {}, "score": "55.988678"}
{"text": "The most reliable collocations are at the top of the new list instead of the original seed words .The original untagged corpus is then tagged with sense labels and probabilities .The final decision list may now be applied to new data , the collocation with the highest rank in the list is used to classify the new data .", "label": "", "metadata": {}, "score": "55.994766"}
{"text": "[Links ] .[ 10 ] J.Y. Jian , Y.C. Chang , and J.S. Chang , \" TANGO : Bilingual collocational concordance \" in Proceedings of ACL , 2004 .[Links ] .[11 ] J.H. Johnson , J. Martin , G. Foster , and R. Kuhn , \" Improving translation quality by discarding most of the phrasetable , \" in Proceedings of EMNLP , 2007 .", "label": "", "metadata": {}, "score": "56.495663"}
{"text": "However , if such knowledge did exist , then deep approaches would be much more accurate than the shallow approaches .Also , there is a long tradition in computational linguistics , of trying such approaches in terms of coded knowledge and in some cases , it is hard to say clearly whether the knowledge involved is linguistic or world knowledge .", "label": "", "metadata": {}, "score": "56.52259"}
{"text": "[Links ] .[ 8 ] J.R. Firth , \" Modes of meaning , \" Papers in linguistics .Oxford : Oxford University Press , 1957 .[Links ] .[ 9 ] M. Gamon , C. Leacock , C. Brockett , W.B. Dolan .", "label": "", "metadata": {}, "score": "56.610138"}
{"text": "Covers the entire field with chapters contributed by leading researchers .CSCI - GA.2590 - Natural Language Processing - Spring 2013 Prof. Grishman .Lecture 8 Outline .Lexical Semantics [ J&M Chap 19 and 20 ] .Until now we have focussed primarily on syntactic issues : part of speech , names , noun and verb groups , and some larger structures .", "label": "", "metadata": {}, "score": "56.6309"}
{"text": "5 ] P. Durrant , \" Investigating the viability of a collocation list for students of English for academic purposes , \" English for Specific Purposes , 28 ( 3 ) , 2009 .[Links ] .[Links ] .", "label": "", "metadata": {}, "score": "57.129654"}
{"text": "We present a detailed case study of this learni ... \" .this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance .", "label": "", "metadata": {}, "score": "57.50952"}
{"text": "A system for spelling correction in which the context of a target word in a sentence is utilized to determine which of several possible words was intended , comprising : . a training corpus containing a set of sentences ; . a confusion set including a list of possible words that could have been intended for said target word ; . an ordered list of features usable to discriminate among words in said confusion set to correct instances in which one word in said confusion set has been incorrectly substituted for another ; and , .", "label": "", "metadata": {}, "score": "57.928"}
{"text": "Computational Linguistics , 20 ( 4 ) , 535 - 561 .Lee , Y. K. , & Ng , H. T. ( 2002 ) .An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation .In Proceedings of the conference on Empirical Methods in Natural Language Processing ( EMNLP ) ( pp .", "label": "", "metadata": {}, "score": "58.17381"}
{"text": "Semi - supervised WSD algorithm [ J&M 20.5 ] .Based on Gale / Yarowsky 's \" one sense per collocation \" and \" one sense per discourse \" observation ( generally true for coarse word senses ) Allows bootstrapping ( semi - supervised learning ) from a small set of sense - annotated seeds Basic idea of bootstrapping : . start with a small set of labeled seeds L and a large set of unlabeled examples U repeat . train classifier C on L apply C to U identify examples with most confident labels ; remove them from U and add them ( with labels ) to L .", "label": "", "metadata": {}, "score": "58.485992"}
{"text": "Manual and automatic semantic annotation with WordNet .In SIGLEX workshop on WordNet and other lexical resources ( NAACL-01 ) , Invited talk , Pittsburgh , PA . .Gonzalo , J. , Verdejo , F. , Chugur , I. , & Cigarran , J. ( 1998 ) .", "label": "", "metadata": {}, "score": "58.636887"}
{"text": "Contents .Difficulties .One problem with word sense disambiguation is deciding what the senses are .In cases like the word bass above , at least some senses are obviously different .In other cases , however , the different senses can be closely related ( one meaning being a metaphorical or metonymic extension of another ) , and in such cases division of words into senses becomes much more difficult .", "label": "", "metadata": {}, "score": "58.72805"}
{"text": "Identifying similar words .Distance metric for Wordnet [ J&M 20.6 ] .Similarity metric from corpora [ J&M 20.7 ] .See the Thesaurus demo by Patrick Pantel .By applying clustering methods we have an unsupervised way of creating semantic word classes .", "label": "", "metadata": {}, "score": "58.769592"}
{"text": "If the new feature f j is not found to be egregiously interdependent on any previously - accepted feature , then it is used as one of the features f 1 , . . ., f h in the calculation .", "label": "", "metadata": {}, "score": "58.880234"}
{"text": "If the association is not found to be significant at the 0.05 level , then the feature is pruned .Note that uninformative features would only introduce noise into the decision - making process .In summary , a system is provided for spelling correction in which the context of a word in a sentence is utilized to determine which of several alternative or possible words was intended .", "label": "", "metadata": {}, "score": "59.06002"}
{"text": "Several methods have been developed for detecting and correcting context - sensitive spelling errors .It then determines the probability that each such sentence was in fact the one that was intended .It selects as its answer the sentence with the highest probability of being intended .", "label": "", "metadata": {}, "score": "59.198067"}
{"text": "Links ] .[ 3 ] C. Fellbaum , WordNet : An Electronic Lexical Database .MIT Press , 1998 .[Links ] .[Links ] .[5 ] P. Koehn , \" Europarl : A parallel corpus for statistical machine translation , \" in Proceedings of the MT Summit , 2005 .", "label": "", "metadata": {}, "score": "59.860275"}
{"text": "Add those examples in the residual that are tagged as A or B with probability above a reasonable threshold to the seed sets .Apply the decision - list algorithm and the above adding step iteratively .As more newly - learned collocations are added to the seed sets , the sense A or sense B set will grow , and the original residual will shrink .", "label": "", "metadata": {}, "score": "59.947266"}
{"text": "What makes possible the above is the utilization of only features which have previously been determined to not be egregiously interdependent , as determined by subsystem 48 .It will be appreciated that subsystem 46 improves the robustness of the decisions made by ensuring that each feature is supported by an adequate number of examples in the training corpus .", "label": "", "metadata": {}, "score": "60.216324"}
{"text": "Bayes ' rule allows the system to calculate the probability of each word from multiple pieces of evidence , namely the various features which are finally utilized .Having done feature matching and conflict resolution , the system has selected a list of features with no egregious interdependencies among any of the features .", "label": "", "metadata": {}, "score": "60.344566"}
{"text": "Human performance , however , is much better on coarse - grained than fine - grained distinctions , so this again is why research on coarse - grained distinctions is most useful .Approaches .Deep approaches presume access to a comprehensive body of world knowledge .", "label": "", "metadata": {}, "score": "60.440887"}
{"text": "Thus , we draw on work done at AT&T Bell Laboratories by Gale and Church ( 1991a ; 1991b ... . \" ...This paper presents a statistical decision procedure for lexical ambiguity resolution .The algorithm exploits both local syntactic patterns and more distant collocational evidence , generating an efficient , effective , and highly perspicuous recipe for resolving a given ambiguity .", "label": "", "metadata": {}, "score": "60.829628"}
{"text": "Chapter 4 Transformation - Based Error - Driven Learning Applied to Natural Language 4.1 Introduction In this section , we describe a framew ... . by David Yarowsky - In Proceedings of the ARPA Human Language Technology Workshop , 1993 . \" ...", "label": "", "metadata": {}, "score": "61.014008"}
{"text": "This is disadvantageous when either the method is mistaken in its evaluation of which piece of evidence is strongest , or the strongest piece of evidence is outweighed by several weaker pieces of evidence that together suggest an alternative decision .What is necessary is a new method for context - sensitive spelling correction that bases its decisions not on the single strongest piece of evidence , but on all of the available evidence , thereby avoiding the abovementioned disadvantages of decision lists .", "label": "", "metadata": {}, "score": "61.650887"}
{"text": "In this way , egregious conflicts are resolved by retaining the strongest feature involved in the conflict , and deleting the others , thereby preserving the strongest set of non - conflicting features as the set on which decisions will be based .", "label": "", "metadata": {}, "score": "61.725605"}
{"text": "CORPUS REPRESENTATIVENESS FOR SYNTACTIC INFORMATION ACQUISITION N\u00faria Bel .FINDING ANCHOR VERBS FOR BIOMEDICAL IE USING PREDICATE - ARGUMENT STRUCTURES Akane YAKUSHIJI , Yuka TATEISI , Yusuke MIYAO , Jun'ichi TSUJII .DYNA :A LANGUAGE FOR WEIGHTED DYNAMIC PROGRAMMING Jason Eisner , Eric Goldlust , Noah A. Smith .", "label": "", "metadata": {}, "score": "61.83357"}
{"text": "Machine Learning , 34(1 - 3 ) .Special Issue on Natural Language Learning .Cai , J. F. , Lee , W. S. , & Teh , Y. W. ( 2007 ) .NUS - ML : Improving word sense disambiguation using topic features .", "label": "", "metadata": {}, "score": "61.90676"}
{"text": "IS CONECPTUAL COMBINATION INFLUENCED BY WORD ORDER ?Phil Maguire , Edward Loper .NLTK : THE NATURAL LANGUAGE TOOLKIT Steven Bird , Edward Loper .SUBSENTENTIAL TRANSLATION MEMORY FOR COMPUTER ASSISTED WRITING AND TRANSLATION Jian - Cheng Wu , Thomas C. Chuang , Wen - Chi Shei , Jason S. Chang .", "label": "", "metadata": {}, "score": "62.073105"}
{"text": "According to the criteria given in Yarowsky ( 1993 ) , seed words that appear in the most reliable collocational relationships with the target word will be selected .The effect is much stronger for words in a predicate - argument relationship than for arbitrary associations at the same distance to the target word , and is much stronger for collocations with content words than with function words .", "label": "", "metadata": {}, "score": "62.10312"}
{"text": "The success rate for part - of - speech tagging algorithms is at present much higher than that for WSD , state - of - the art being around 95 % accuracy or better , as compared to less than 75 % accuracy in word sense disambiguation with supervised learning .", "label": "", "metadata": {}, "score": "62.12444"}
{"text": "In this study , we will use WSD as part of a method to create innovative features to represent the documents for classification task .With the help of WSD , a set of specially selected ambiguous words can be further distinguished by word sense clustering , in order to achieve better document classification .", "label": "", "metadata": {}, "score": "62.212875"}
{"text": "A dictionary 18 and an ordered list of features 12 are then utilized to determine the intended spelling of the target word through the utilization of Bayes ' rule as will be seen hereinafter .The result of the analysis is the indication of the intended spelling of the target word as illustrated at 26 .", "label": "", "metadata": {}, "score": "62.26103"}
{"text": "Additionally , a dictionary 18 is provided to look up the set of possible part - of - speech tags of words in the training corpus and in the test sentence , herein referred to as the target sentence .It will be noted that each target sentence has a target word the spelling of which is to be verified .", "label": "", "metadata": {}, "score": "62.293617"}
{"text": "A confusion set 16 is also provided , with the confusion set constituting a list of possible words that could have been intended for the target word .In one embodiment , the confusion sets were derived from a listing of \" Words Commonly Confused \" appearing in Flexner , ed .", "label": "", "metadata": {}, "score": "62.4719"}
{"text": "By \" conflicting \" is meant there is an egregious interdependency between the present feature and a previously accepted feature .An egregious interdependency is defined as follows : if both features at issue are context - word features , then the two features are said not to conflict .", "label": "", "metadata": {}, "score": "62.76793"}
{"text": "ACL - SIGLEX , Toulouse France .Fellbaum , C. ( 1998 ) .WordNet - an electronic lexical database .Cambridge , MA / London : The MIT Press .Fellbaum , C. , Delfs , L. , Wolff , S. , & Palmer , M. ( 2005 ) .", "label": "", "metadata": {}, "score": "63.203583"}
{"text": "To decide which was the intended spelling of the word , e.g. , \" terminara \" or \" terminara \" , Yarowsky 's method analyzes the context in which the word occurred .In particular , it tests two kinds of features of the context : context - word features , and collocation features .", "label": "", "metadata": {}, "score": "63.367287"}
{"text": "The suggested word therefore represents the likely intended spelling of the target word as illustrated at 54 .More particularly , as to the top level of the subject algorithm , FIG .1 shows the overall operation of the subject method .", "label": "", "metadata": {}, "score": "63.471027"}
{"text": "Two features are said to be egregiously interdependent if the presence of one feature has a very strong influence on the presence of the other feature .Secondly , means are provided for deleting features from the set of features that are used for making a decision , such that egregious interdependencies are avoided .", "label": "", "metadata": {}, "score": "63.92501"}
{"text": "Bikel , D. M. ( 2002 ) .Design of a multi - lingual , parallel - processing statistical parsing engine .In Proceedings of HLT 2002 , San Diego , CA .Bikel , D. M. , Schwartz , R. , & Weischedel , R. M. ( 1999 ) .", "label": "", "metadata": {}, "score": "63.967323"}
{"text": "INTERACTIVE GRAMMAR DEVELOPMENT WITH WCDG Kilian A. Foth , Michael Daum , Wolfgang Menzel .WIDE COVERAGE SYMBOLIC SURFACE REALIZATION Charles Callaway .HIERARCHY EXTRACTION BASED ON INCLUSION OF APPEARANCE Eiko Yamamoto , Kyoko Kanzaki , and Hitoshi Isahara .MULTIMODAL DATABASE ACCESS ON HANDHELD DEVIDES Elsa Pecourt , Norbert Reithinger .", "label": "", "metadata": {}, "score": "63.97302"}
{"text": "Referring now to FIG .1b , during the run - time phase , a system 20 is utilized to determine the intended spelling of the target word from context .The system is provided with a target word in a sentence , as illustrated at 22 , as well as a confusion set for the target word , as illustrated at 24 .", "label": "", "metadata": {}, "score": "64.19606"}
{"text": "AUTOMATIC CLUSTERING OF COLLOCATION FOR DETECTING PRACTICAL SENSE BOUNDARY Saim Shin , Key - Sun Choi . CO - TRAINING FOR PREDICTING EMOTIONS WITH SPOKEN DIALOGUE DATA Beatriz Maeireizo , Diane Litman , Rebecca Hwa .FRAGMENTS AND TEXT CATEGORIZATION Jan Blat\u00e1k , Eva Mr\u00e1kov\u00e1 , Lubos Popel\u00ednsky .", "label": "", "metadata": {}, "score": "64.22143"}
{"text": ": : Programme Committee : . : : List of accepted papers : .INCORPORATING TOPIC INFORMATION INTO SEMANTIC ANALYSIS MODELS Tony Mullen , Nigel Collier .A PRACTICAL SOLUTION TO THE PROBLEM OF AUTOMATIC WORD SENSE INDUCTION Reinhard Rapp .TRANSTYPE2 - AN INNOVATIVE COMPUTER - ASSISTED TRANSLATION SYSTEM Jos\u00e9 Esteban , Jos\u00e9 Lorenzo , Antonio S. Valderr\u00e1banos and Guy Lapalme .", "label": "", "metadata": {}, "score": "64.549774"}
{"text": "In the subject invention , a new method for context - sensitive spelling correction uses Bayes ' rule to combine evidence from multiple types of features ; in particular , from context - word features and collocation features .The subject method differs from Yarowsky 's method involving decision lists in that the subject method makes use of all available evidence when making a decision , whereas Yarowsky 's method uses only the single strongest piece of evidence .", "label": "", "metadata": {}, "score": "64.65329"}
{"text": "This approach , while theoretically not as powerful as deep approaches , gives superior results in practice , due to the computer 's limited world knowledge .Two shallow approaches used to train and then disambiguate are Na\u00efve Bayes classifiers and decision trees .", "label": "", "metadata": {}, "score": "64.74334"}
{"text": "Many experiments in recent years of both supervised ( Leacock 1993 ) and unsupervised ( Yarowsky , 1993 ) WSD algorithms have accomplished promising performance with a high precision rate .Another important area in the field of text mining ( Lewis & Spark Jones papers ) is document classification , which identifies one or more of several topic labels for a text document .", "label": "", "metadata": {}, "score": "65.15839"}
{"text": "2 , system 10 is comprised of a number of subsystems , with the first of which , subsystem 30 , proposing all possible features as candidate features to be utilized in the context - sensitive spelling correction .These features are proposed by scanning the training corpus for instances of any word in the confusion set , and proposing a context - word or collocation feature whenever it occurs for one or more such instances .", "label": "", "metadata": {}, "score": "65.177795"}
{"text": "Let the set of features that match the context of the target word be denoted by f 1 , . . ., f h .The calculation is done using Bayes ' rule : # # EQU4 # # By choosing the f j judiciously , it can be guaranteed that there are no egregious interdependencies among features .", "label": "", "metadata": {}, "score": "65.70845"}
{"text": "The model produces word alignments that are better than those produced by IBM Model 5 .Developing a better TM is a fundamental issue for those applications .Researchers at IBM first described such a statistical TM in ( Brown et al . , 1988 ) .", "label": "", "metadata": {}, "score": "65.72684"}
{"text": "On the other hand , a different context - word feature , the presence of the word \" sand \" within \u00b120 words , would tend to suggest that \" desert \" was intended .The second type of feature used by Yarowsky 's method is collocation features .", "label": "", "metadata": {}, "score": "65.78537"}
{"text": "In one embodiment , the set of all possible features includes two types : context - word features and collocation features .A feature is proposed if and only if it occurs for at least one example in the training corpus .", "label": "", "metadata": {}, "score": "66.130844"}
{"text": "The system first recognizes the interdependencies , and then resolves them by deleting all but the strongest feature involved in each interdependency , thereby allowing it to make its decisions based on the strongest non - conflicting set of features .BRIEF DESCRIPTION OF THE DRAWING .", "label": "", "metadata": {}, "score": "66.40105"}
{"text": "If it does not match , another feature is obtained from the ordered list of features .If it does match , as illustrated at 48 , a determination is made as to whether the feature conflicts with a previously accepted feature .", "label": "", "metadata": {}, "score": "66.59212"}
{"text": "If so , control proceeds to the next feature , as the inclusion of f would violate the assumption of independence among the f h .Note that this strategy eliminates egregiously interdependent features by accepting the stronger of two interdependent features , and rejecting the weaker of the two .", "label": "", "metadata": {}, "score": "66.70245"}
{"text": "The system of claim 3 , wherein said means for updating said running probability includes means utilizing Bayes ' rule for estimating the joint probability of all matched features .The system of claim 1 , and further including means for providing said ordered list including means for providing a pruned list of features .", "label": "", "metadata": {}, "score": "66.96839"}
{"text": "Referring now to FIG .1a , in the subject system , a system 10 is provided which learns the usage of each word in a confusion set and provides an ordered list of features 12 .In order to provide the ordered list of features which will be utilized to analyze words in context for purposes of spelling correction , a training corpus 14 is provided , which in one embodiment constitutes a raw or unannotated set of correct sentences of English .", "label": "", "metadata": {}, "score": "67.21042"}
{"text": "ISBN : 978 - 1 - 4020 - 6870 - 6 .Berger , A. L. , Della Piertra , S. A. , & Della Pietra , V. J. ( 1996 ) .A maximum entropy approach to natural language processing .", "label": "", "metadata": {}, "score": "67.396225"}
{"text": "In the former case , there are insufficient data to measure its presence ; in the later , its absence .A feature f is also pruned , as illustrated at 36 , if it is not informative at discriminating among the words in the confusion set .", "label": "", "metadata": {}, "score": "67.49104"}
{"text": "RESOURCE ANALYSIS FOR QUESTION ANSWERING Lucian Vlad Lita , Warren A. Hunt , Eric Nyberg .TANGO : BILINGUAL COLLOCATIONAL CONCORDANCER Jia - Yan Jian , Yu - Chia Chang , Jason S. Chang .A NEW FEATURE SELECTION SCORE FOR MULTINOMIAL NAIVE BAYES TEXT CLASSIFICATION BASED ON KL - DIVERGENCE Karl - Michael Schneider .", "label": "", "metadata": {}, "score": "67.864075"}
{"text": "The method of Mays et al . needs an enormous corpus of training sentences in order to learn these trigram probabilities .To measure each trigram probability reliably , it needs enough sentences to have seen every triple of words that can occur in the English language a statistically significant number of times .", "label": "", "metadata": {}, "score": "67.86994"}
{"text": "The system operates to update the running probability for each word through the use of Bayes ' rule .This is accomplished by obtaining the next feature from the ordered list of features as illustrated at 42 .As illustrated at 44 , it is determined whether the end of the list has been reached .", "label": "", "metadata": {}, "score": "67.93936"}
{"text": "Links ] Abstract .This paper presents a high - performance broad - coverage supervised word sense disambiguation ( WSD ) system for English verbs that uses linguistically motivated features and a smoothed maximum entropy machine learning model .We describe three specific enhancements to our system 's treatment of linguistically motivated features which resulted in the best published results on SENSEVAL-2 verbs .", "label": "", "metadata": {}, "score": "68.38638"}
{"text": "[Links ] .[ 2 ] M. Benson , E. Benson and R. Ilson , The BBI Combinatory Dictionary of English .A Guide to Word Combinations , 1986 .[Links ] .[Links ] .[Links ] .", "label": "", "metadata": {}, "score": "68.449974"}
{"text": "In Proceedings of the international conference on semantic computing ( ICSC 2007 ) .Irvine , CA .Chen , J. , & Palmer , M. ( 2005 , October 11 - 13 ) .Towards robust high performance word sense disambiguation of English verbs using rich linguistic features .", "label": "", "metadata": {}, "score": "68.46387"}
{"text": "AN AUTOMATIC FILTER FOR NON - PARALLEL TEXTS Christopher Pike , I. Dan Melamed .GRAPH - BASED RANKING ALGORITHMS FOR SENTENCE EXTRACTION , APPLIED TO TEXT SUMMARIXATION Rada Mihalcea .EXPLOITING UNANNOTATED CORPORA FOR TAGGING AND CHUNKING Rie Kubota Ando .", "label": "", "metadata": {}, "score": "68.559296"}
{"text": "Chen , S. F. , & Rosenfeld , R. ( 1999 ) .A Gaussian prior for smoothing maximum entropy models .Technical Report CMU - CS-99 - 108 , CMU .Chen , J. , Schein , A. , Ungar , L. , & Palmer , M. ( 2006 ) .", "label": "", "metadata": {}, "score": "68.58403"}
{"text": "Links ] .[Links ] .[14 ] F. Och and H. Ney , \" A systematic comparison of various statistical alignment models , \" Computational Linguistics , vol .[Links ] .Cambridge University Press , 2005 .", "label": "", "metadata": {}, "score": "68.7348"}
{"text": "Links ] .[Links ] .[Links ] .[21 ] J. Quinlan , C4.5 : Programs for machine learning .Morgan Kaufmann , San Mateo , CA , 1993 .[Links ] .[Links ] Description .", "label": "", "metadata": {}, "score": "69.20668"}
{"text": "Links ] .[ 12 ] A. Kilgarriff , P. Rychly , P. Smrz , and D. Tugwell , \" The sketch engine , \" in Proceedings of EURALEX , 2004 .[Links ] .[Links ] .[Links ] .", "label": "", "metadata": {}, "score": "69.45091"}
{"text": "Title .Improving English verb sense disambiguation performance with linguistically motivated features and clear sense distinction boundaries A system is provided for spelling correction in which the context of a wordn a sentence is utilized to determine which of several alternative or possible words was intended .", "label": "", "metadata": {}, "score": "69.476234"}
{"text": "In one embodiment , the \" minimum occurrences \" threshold is set at 10 .As illustrated at 36 , a subsystem prunes features that are uninformative at discriminating among the words in the confusion set .A chi - square test is run to ascertain the degree of association between the presence of the feature and the choice of word in the confusion set .", "label": "", "metadata": {}, "score": "69.82635"}
{"text": "The list is ordered in the sense of decreasing strength to facilitate the extraction of the strongest non - conflicting set of features for purposes of deriving the intended spelling of the target word .System 20 incorporates a subsystem 40 which initializes each probability p(w i ) to the prior probability of word w i .", "label": "", "metadata": {}, "score": "69.90755"}
{"text": "English tasks : All - words and verb lexical sample .In Proceedings of SENSEVAL-2 : Second international workshop on evaluating word sense disambiguation systems .Toulouse , France .Palmer , M. , Gildea , D. , & Kingsbury , P. ( 2005 ) .", "label": "", "metadata": {}, "score": "70.01074"}
{"text": "A smoothing algorithm will then be used to avoid 0 values .The decision - list algorithm resolves many problems in a large set of non - independent evidence source by using only the most reliable piece of evidence rather than the whole matching collocation set .", "label": "", "metadata": {}, "score": "70.0723"}
{"text": "The difficulty of utilizing multiple types of features lies in combining the probabilities associated with the features .If the features can be assumed independent , then the probabilities can simply be multiplied .However , the features are generally not independent when there are multiple types of features .", "label": "", "metadata": {}, "score": "70.46651"}
{"text": "In one embodiment , the significance level is set to 0.05 .As illustrated at 38 , a sort of the remaining features in order of decreasing strength is performed so as to provide ordered list of features 12 .The strength of a feature reflects the feature 's reliability for decision - making .", "label": "", "metadata": {}, "score": "71.17615"}
{"text": "These a priori probabilities are calculated using a word trigram model .The model estimates the a priori probability of a sentence in terms of the probability of each consecutive 3-word sequence , or word trigram , in the sentence .", "label": "", "metadata": {}, "score": "71.205154"}
{"text": "In general , system 10 learns the usage of a word by learning a set of context - word and collocation features that characterize the contexts in which that word tends to occur , and that thereby discriminate that word from the other words in the confusion set .", "label": "", "metadata": {}, "score": "71.267456"}
{"text": "In the process of testing for features , it may use the dictionary to look up the set of possible tags for a word .As to the training phase , FIG .2 shows this phase in more detail .The overall purpose of the training phase is to learn an ordered list of features to be used later , by the run - time module , to discriminate among the words in the confusion set .", "label": "", "metadata": {}, "score": "71.353615"}
{"text": "The run - time module takes as input a target word in a sentence and a confusion set .The target word must be one of the words in the confusion set .The run - time module then uses the context of the target word to decide which word in the confusion set was intended .", "label": "", "metadata": {}, "score": "71.433945"}
{"text": "\" The probability of a word trigram ( w 1 , w 2 , w 3 ) is the probability that , given that words w 1 and w 2 occur consecutively in a sentence , the next word in the sentence will be w 3 .", "label": "", "metadata": {}, "score": "71.62584"}
{"text": "It will be appreciated that this count provides statistical data which will be used in following steps to assist in the pruning subsystem and to perform the probability updates as required by Bayes ' rule .As illustrated at 34 , a subsystem prunes features that have insufficient data .", "label": "", "metadata": {}, "score": "71.78625"}
{"text": "The training - phase module learns the correct usage of each word in a given confusion set from a training corpus .The confusion set is a set of words that the user may confuse with each other ; that is , he may type one word in the confusion set when he intended to type another .", "label": "", "metadata": {}, "score": "72.111885"}
{"text": "Links relacionados .Compartir .Polibits no.43 M\u00e9xico ene . /jun .2011 .Manuscript received November 6 , 2010 .Manuscript accepted for publication January 12 , 2011 .Abstract .To train and test the classifier , we used English as an input language and we incorporated the translations of our target words in five languages ( viz .", "label": "", "metadata": {}, "score": "72.31504"}
{"text": "In particular , let f be a feature in the training corpus .As illustrated at 34 , features that have insufficient data are pruned .This is done to avoid making unjustified conclusions about features that occur very few times in the training corpus .", "label": "", "metadata": {}, "score": "72.44308"}
{"text": "The strength of a feature f is calculated as : # # EQU3 # # The strength of f is essentially the extent to which its presence is unambiguously correlated with one particular w i .For instance , if f occurs only in the presence of w 1 , but never in the presence of any other w i , then the strength of f will be 1.0 , the highest possible strength value .", "label": "", "metadata": {}, "score": "73.24637"}
{"text": "If the observed association is not judged to be significant , then the feature is pruned .In one embodiment , the significance level is set to 0.05 .As illustrated at 38 , the set of features that remain , after the preceding pruning steps , are sorted in order of decreasing strength .", "label": "", "metadata": {}, "score": "74.15347"}
{"text": "This task is to take a word that has been stripped of any accent , such as \" terminara \" in Spanish , and to decide whether the intended word is the accented version \" terminara \" or the unaccented version \" terminara \" .", "label": "", "metadata": {}, "score": "74.1655"}
{"text": "The run - time phase works by calculating , for each w i in the confusion set , the probability that that w i was the word that was intended for the target word .This is done by first looking for features in the context of the target word that are indicative of one word or another in the confusion set .", "label": "", "metadata": {}, "score": "74.24032"}
{"text": "WYSIWYM WITH WIDER COVERAGE Richard Power , Roger Evans .COMPILING BOSSTEXTER RULES INTO A FINITE - STATE TRANSDUCER Srinivas Bangalore .COMBINING LEXICAL , SYNTACTIC , AND SEMANTIC FEATURES WITH MAXIMUM ENTROPY MODELS FOR INFORMATION EXTRACTION Nanda Kambhatla .PART - OF - SPEECH TAGGING CONSIDERING SURFACE FORM FOR AN AGGLUTINATIVE LANGUAGE Do - Gil Lee , Hae - Chang Rim .", "label": "", "metadata": {}, "score": "74.36705"}
{"text": "Kipper , K. , Korhonen , A. , Ryant , N. , & Palmer , M. ( 2006 ) .Extensive classifications of English verbs .In Proceedings of the 12th EURALEX international congress , Turin , Italy .Lappin , S. , & Leass , H. ( 1994 ) .", "label": "", "metadata": {}, "score": "74.42256"}
{"text": "In particular , feature is pruned if it is found to have an insufficient number of examples in the training corpus , as it therefore provides unreliable evidence .A feature is also pruned if it is found to be uninformative in discriminating among the different possible intended spellings of the target word .", "label": "", "metadata": {}, "score": "74.85405"}
{"text": "means responsive to said training corpus , said confusion set , and said dictionary for proposing all possible features as candidate features ; . means for providing a count of the occurrences of each candidate feature in said training corpus ; . means responsive to said count for enumerating features having a count below a predetermined threshold ; and , . means for eliminating features that are not informative at discriminating among the words in said confusion set .", "label": "", "metadata": {}, "score": "74.893875"}
{"text": "For any given target problem , it applies the single strongest piece of evidence , whichever type that happens to be .This is implemented by applying the first feature that matches , where \" first \" corresponds to \" strongest \" because the features have been sorted in order of decreasing reliability .", "label": "", "metadata": {}, "score": "76.08819"}
{"text": "It tries to match feature ( 2 ) , which succeeds , since \" in \" is a PREP and the word \" the \" appears before \" desert \" .Because feature ( 2 ) suggests \" desert \" , the method accepts the given sentence as correct .", "label": "", "metadata": {}, "score": "77.149734"}
{"text": "The training corpus is a set of sentences including , crucially , sentences that illustrate the correct usage of the words in the confusion set .From these examples of correct usage , the training - phase module infers the contexts in which each word in the confusion set tends to occur .", "label": "", "metadata": {}, "score": "77.54898"}
{"text": "A significant body of research has improved the results of document classification , with innovations in identifying document features as well as improving algorithms .In this study , we will use WSD as part of a method to create innovative features to represent the documents for classification task .", "label": "", "metadata": {}, "score": "78.32674"}
{"text": "Computational Linguistics , 31 , 1 . doi : 10 .CrossRef .Philpot , A. , Hovy , E. , & Pantel , P. ( 2005 ) .The omega ontology .In Proceedings of the ONTOLEX workshop at the International Conference on Natural Language Processing ( IJCNLP05 ) .", "label": "", "metadata": {}, "score": "78.47576"}
{"text": "If the total number of occurrences of the feature is below the threshold value , it is said that there are insufficient data to measure the feature 's presence .In addition , if the total number of instances of all words in the confusion set minus the total number of occurrences of the feature is below the threshold value , it is said that there are insufficient data to measure the feature 's absence .", "label": "", "metadata": {}, "score": "78.543884"}
{"text": "doi : 10 .CrossRef .Marcus , M. , Kim , G. , Marcinkiewicz , M. A. , MacIntyre , R. , Ferguson , M. , Katz , K. , et al .( 1994 ) .The Penn Treebank : Annotating predicate argument structure .", "label": "", "metadata": {}, "score": "78.928925"}
{"text": "Pradhan , S. , Loper , E. , Dligach , D. , & Palmer , M. ( 2007 , June ) .SemEval-2007 task-17 : English lexical sample , SRL and all words .In Proceedings of SemEval , held in conjunction with ACL 2007 , Prague , Czech Republic .", "label": "", "metadata": {}, "score": "79.058105"}
{"text": "The corpus is initially untagged .The algorithm starts with a large corpus , in which it identifies examples of the given polysemous word , and stores all the relevant sentences as lines .For instance , Yarowsky uses the word ' plant ' in his 1995 paper to demonstrate the algorithm .", "label": "", "metadata": {}, "score": "79.19059"}
{"text": "FIG .3 shows how the procedure just described is embodied in the run - time phase of the method .As illustrated at 42 , the algorithm proceeds to loop through each feature in the list that was learned in the training phase .", "label": "", "metadata": {}, "score": "80.61705"}
{"text": "Shallow approaches do n't try to understand the text .They just consider the surrounding words , using information such as \" if bass has words sea or fishing nearby , it probably is in the fish sense ; if bass has the words music or song nearby , it is probably in the music sense . \"", "label": "", "metadata": {}, "score": "81.78208"}
{"text": "This step is therefore omitted .If there is another feature f in the list to process , control proceeds to 46 , where a test is performed to see whether f matches the context of the target word .If it does not match , then f does not furnish any information about the identity of the target word ; hence control proceeds to the next feature .", "label": "", "metadata": {}, "score": "82.44252"}
{"text": "The last input to the training phase is a dictionary .The dictionary is used only to look up the set of possible part - of - speech tags of a word .The training phase returns as its output an ordered list of features that can then be used to discriminate among the words in the confusion set .", "label": "", "metadata": {}, "score": "85.20894"}
{"text": "Poster / Demo presentations present original work in progress , on - going research projects with novel ideas / applications , or late - breaking results that are best communicated in an interactive format .Interactive Poster / Demo sessions provide a forum of academic and technical exchanges .", "label": "", "metadata": {}, "score": "86.25838"}
{"text": "One example of the program , written in C , follows , which describes the major operation of the system without the utilities , but including the top - level routines for the training phase and the run - time phase : # # SPC1 # # .", "label": "", "metadata": {}, "score": "87.8376"}
{"text": "For example , the pattern \" for -- \" specifies that the word \" for \" occurs directly before the target word , whose position is symbolized by an underscore .The presence of this pattern would tend to suggest that \" dessert \" was intended , as in the sentence above .", "label": "", "metadata": {}, "score": "89.47274"}
{"text": "It is accordingly intended to define the scope of the invention only as indicated in the following claims : Co - Chairs : Philippe Blache , CNRS & Universit\u00e9 de Provence Co - Chairs : Horacio Rodr\u00edguez , Universitat Polit\u00e8cnica de Catalunya .", "label": "", "metadata": {}, "score": "90.041504"}
{"text": "would use the word trigram ( \" the \" , \" chocolate \" , \" cake \" ) , Schabes et al .would use the corresponding part - of - speech trigram ( ARTICLE , ADJ , NOUN ) .Instead of needing sentences illustrating every triple of words that can occur in English , Schabes et al .", "label": "", "metadata": {}, "score": "90.56969"}
{"text": "\" ; \" I would like the chocolate cake for dessert .\" ; and \" I would like the chocolate cake for desert . \"Note that the last sentence is the same as the original sentence , and thus represents the possibility that the original sentence , as typed , was the one that was intended .", "label": "", "metadata": {}, "score": "90.893005"}
{"text": "and the sentences : .I went fishing for some sea bass .The bass line of the song is very moving .To a human , it is obvious that the first sentence is using the word bass , as in the former sense above and in the second sentence , the word bass is being used as in the latter sense below .", "label": "", "metadata": {}, "score": "92.11368"}
{"text": "For instance , suppose Yarowsky 's method is used to decide which word was intended , \" desert \" or \" dessert \" , in the sentence : \" I would like the chocolate cake for desert .\" One possible context - word feature would be the presence of the word \" chocolate \" within \u00b120 words of \" desert \" .", "label": "", "metadata": {}, "score": "92.507484"}
{"text": "Manuscript accepted for publication January 5 , 2011 .We introduce a method for learning to grammatically categorize and organize the contexts of a given query .In our approach , grammatical descriptions , from general word groups to specific lexical phrases , are imposed on the query 's contexts aimed at accelerating lexicographers ' and language learners ' navigation through and GRASP upon the word usages .", "label": "", "metadata": {}, "score": "94.42884"}
{"text": "Consider , for example , the sentence : \" I would like the chocolate cake for desert .\" In this sentence , the word \" dessert \" was intended , but \" desert \" was typed .Because \" desert \" can be found in the dictionary , this error will go undetected by conventional spell checkers .", "label": "", "metadata": {}, "score": "96.09024"}
{"text": "For example , consider the sentence : \" I would like teh chocolate cake for dessert . \"A conventional spell checker would notice that \" teh \" is not in the dictionary , and thus would flag it as an error .", "label": "", "metadata": {}, "score": "96.86601"}
{"text": "For example , suppose Yarowsky 's method is used to decide which word was intended , \" desert \" or \" dessert \" .This decision list is used by testing whether each feature in the list in turn matches the target context .", "label": "", "metadata": {}, "score": "97.26579"}
{"text": "aining algorithm still converges .We expect to use it in models that we develop beyond Model 5 . 293sComputational Linguistics Volume 19 , Number 2 6.3 Multi - Word Cepts In Models 1 - 5 , we restrict our attention to alignments w .. \" ... this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .", "label": "", "metadata": {}, "score": "97.71762"}
{"text": "\" , as the target sentence to correct .It generates a large number of possible intended sentences by inserting up to one typo in each word of the given sentence .Its resulting set of possible sentences includes , among others : \" A would like the chocolate cake for desert .", "label": "", "metadata": {}, "score": "98.602936"}
{"text": "Schabes et al .analyze these sentences in terms of their part - of - speech sequences , namely : PRONOUN MODAL VERB ARTICLE ADJ NOUN PREP NOUN PUNC and PRONOUN MODAL VERB ARTICLE ADJ NOUN PREP NOUN PUNC .Here the intended word , \" dessert \" , and the offending word , \" desert \" , have the same part of speech , i.e. , NOUN .", "label": "", "metadata": {}, "score": "100.606766"}
{"text": "Consider the context - word feature : \" then \" within \u00b120 .This feature tests whether the word \" then \" occurs within 20 words of the target word .However , the presence of \" then \" within 20 words of the target word is unlikely to provide any useful indication of whether the target word should be \" desert \" or \" dessert \" .", "label": "", "metadata": {}, "score": "102.79378"}
{"text": "Consider , for example , the application of this procedure to the sentence : \" I would like the chocolate cake for desert .\" The method first tests whether feature ( 1 ) matches the context around the target word \" desert \" .", "label": "", "metadata": {}, "score": "103.02908"}
{"text": "The test succeeds , and so the method suggests that the target word should be changed to \" dessert \" .Now consider the application of the method to \" desert \" in the sentence : \" He wandered aimlessly in the desert .", "label": "", "metadata": {}, "score": "122.97034"}
