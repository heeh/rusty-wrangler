{"text": "The Maximum Entropy ( ME ) principle has recently been demonstrated as a powerful tool for combining statistical estimates from diverse sources[l , 2 , 3].The ME principle ( [ 4 , 5 ] ) proposes the following : 1 .", "label": "", "metadata": {}, "score": "27.688234"}
{"text": "A note on the prior distributions of Weibull parameters .SCIMA 19 : 5 - 13 MATH .Zellner A ( 1991 )Bayesian methods and entropy in economics and econometrics .In : Grandy WT Jr , Schick LH(eds ) Maximum entropy and Bayesian methods .", "label": "", "metadata": {}, "score": "30.43843"}
{"text": "Adam Berger , Stephen Della Pietra , and Vincent Della Pietra Computational Linguistics , ( 22 - 1 ) , March 1996 ; .The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .", "label": "", "metadata": {}, "score": "30.472496"}
{"text": "The concept of Maximum Entropy can be traced back along multiple threads to Biblical times .However , not until the late of 21st century has computer become powerful enough to handle complex problems with statistical modeling technique like Maxent .Maximum Entropy was first introduced to NLP area by Berger , et al ( 1996 ) and Della Pietra , et al .", "label": "", "metadata": {}, "score": "30.74532"}
{"text": "Tools . by Adam L. Berger , Stephen A. Della Pietra , Vincent J. Della Pietra - COMPUTATIONAL LINGUISTICS , 1996 . \" ...The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .", "label": "", "metadata": {}, "score": "31.325043"}
{"text": "That is to say , when characterizing some unknown events with a statistical model , we should always choose the one that has Maximum Entropy .Maximum Entropy Modeling has been successfully applied to Computer Vision , Spatial Physics , Natural Language Processing and many other fields .", "label": "", "metadata": {}, "score": "32.72831"}
{"text": "This link is to the Maximum Entropy Modeling Toolkit , for parameter estimation and prediction for maximum entropy models in discrete domains .The software comes with documentation , and was used as the basis of the 1996 Johns Hopkins workshop on language modelling .", "label": "", "metadata": {}, "score": "32.917953"}
{"text": "Instead , we apply the principle of Maximum Entropy ( ME ) .Each information source gives rise to a set of constraints , to be imposed on the combined estimate .The intersection of these constraints is the set of probability functions which are consistent with all the information sources .", "label": "", "metadata": {}, "score": "36.35164"}
{"text": "What is Maximum Entropy Modeling .In his famous 1957 paper , Ed .T. Jaynes wrote : Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge , and leads to a type of statistical inference which is called the maximum entropy estimate .", "label": "", "metadata": {}, "score": "37.4532"}
{"text": "The Maximum Entropy principle ( ME ) is an ap- propriate framework for combining information of a diverse nature from several sources into the same language model .In order to incorporate long - distance information into the ME framework in a language model , a Whole Sentence Maximum Entropy Language Model ( WSME ) could be used .", "label": "", "metadata": {}, "score": "37.588844"}
{"text": "The Maximum Entropy principle ( ME ) is an ap- propriate framework for combining information of a diverse nature from several sources into the same language model .In order to incorporate long - distance information into the ME framework in a language model , a Whole Sentence Maximum Entropy Language Model ( WSME ) could be used .", "label": "", "metadata": {}, "score": "37.588844"}
{"text": "The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .", "label": "", "metadata": {}, "score": "37.78592"}
{"text": "The point of this document is twofold : first , to motivate the improved iterative scaling algorithm for conditional models , and second , to do so in a way that is minimizes the mathematical burden on the reader .This tutorial explains how to build maximum entropy models for natural language applications such as information retrieval and speech recognition .", "label": "", "metadata": {}, "score": "40.175617"}
{"text": "The estimation processes of the models are described in detail .Finally , experiments on the Wall Street Journal corpus are reported . \" ...The Maximum Entropy principle ( ME ) is an ap- propriate framework for combining information of a diverse nature from several sources into the same language model .", "label": "", "metadata": {}, "score": "40.34398"}
{"text": "The estimation processes of the models are described in detail .Finally , experiments on the Wall Street Journal corpus are reported . \" ...The Maximum Entropy principle ( ME ) is an ap- propriate framework for combining information of a diverse nature from several sources into the same language model .", "label": "", "metadata": {}, "score": "40.34398"}
{"text": "At the heart of our approach is a Maximum Entropy ( ME ) model which inc.orlxnates many knowledge sources in a consistent manner .The other components are a selective unigram cache , a conditional bigram cache , and a conventionalstatic trigram .", "label": "", "metadata": {}, "score": "41.221146"}
{"text": "Unlike the linear interpo ... \" . application .Furthermore , other structure knowledge such as context free grammar and link grammar [ 10 , 11 ] is introduced to language models for improving their performances .To combine statistical information from multiple sources , maximum entropy ( ME ) LM is presented [ 12].", "label": "", "metadata": {}, "score": "42.51835"}
{"text": "Unlike the linear interpo ... \" . application .Furthermore , other structure knowledge such as context free grammar and link grammar [ 10 , 11 ] is introduced to language models for improving their performances .To combine statistical information from multiple sources , maximum entropy ( ME ) LM is presented [ 12].", "label": "", "metadata": {}, "score": "42.518356"}
{"text": "Zhu , S.C. , Wu , Y.N. , Mumford , D. : Filters , Random Fields , and Maximum Entropy ( FRAME ) : Towards a Unified Theory for Texture Modeling .IJCV 27(2 ) , 1 - 20 ( 1998 ) CrossRef .", "label": "", "metadata": {}, "score": "45.183937"}
{"text": "Wiley , London MATH .Skilling J ( 1989 ) Maximum entropy and Bayesian methods .Kluwer , Dordrecht MATH .Smith , CR , Erickson , G , Neudorfer , PO ( eds ) ( 1992 ) Maximum entropy and Bayesian methods ( fundamental theories of physics ) .", "label": "", "metadata": {}, "score": "45.481525"}
{"text": "In this paper , we propose a machine learning algorithm for shallow semantic parsing , extending the work of Gildea and Jurafsky ( 2002 ) , Surdeanu et al .( 2003 ) and others .Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers .", "label": "", "metadata": {}, "score": "45.737396"}
{"text": "In this paper , we propose a machine learning algorithm for shallow semantic parsing , extending the work of Gildea and Jurafsky ( 2002 ) , Surdeanu et al .( 2003 ) and others .Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers .", "label": "", "metadata": {}, "score": "45.737396"}
{"text": "Jaynes ET ( 1957 )Information theory and statistical mechanics .Phys Rev 106 , 620 - 630 , and 108 , 171 - 190 .Jaynes ET ( 1982 )On the rationale of maximum entropy methods .Proc IEEE 70 : 939 - 952 CrossRef .", "label": "", "metadata": {}, "score": "46.217163"}
{"text": "Linnemann JT ( 2000 ) Upper limits and priors .FNAL CL Worshop ( with notes added in January 2003 ) .Luttrell SP ( 1985 )The use of transinformation in the design of data sampling schemes for inverse problems .", "label": "", "metadata": {}, "score": "46.22689"}
{"text": "Kass RE , Wasserman L ( 1996 )The selection of prior distributions by formal rules .J Am Stat Assoc 91 : 1343 - 1370 MATH CrossRef .Lannoy A , Procaccia H ( 2003 ) L'utilisation du jugement d'expert en s\u00fbret\u00e9 de fonctionnement .", "label": "", "metadata": {}, "score": "46.79003"}
{"text": "The ME principle ( [ 4 , 5 ] ) proposes the following : 1 .Reformulate the different estimates as constraints on the expectation of various functions , to be satisfied by the target ( combined ) estimate .Among all probability distributions that satisfy these con - straints , choose the one that has the highest entropy .", "label": "", "metadata": {}, "score": "46.861492"}
{"text": "Title .A Discriminative Framework for Texture and Object Recognition Using Local Image Features BibTeX .Share .OpenURL .Abstract .We desert'be our latest attempt at adaptive language modeling .At the heart of our approach is a Maximum Entropy ( ME ) model which inc.orlxnates many knowledge sources in a consistent manner .", "label": "", "metadata": {}, "score": "47.82164"}
{"text": "Four iterative parameter estimation algorithms are compared on several NLP tasks .L - BFGS is observed to be the most effective parameter estimation method for Maximum Entropy model , much better than IIS and GIS .( Wallach 02 ) reported similar results on parameter estimation of Conditional Random Fields .", "label": "", "metadata": {}, "score": "48.121197"}
{"text": "The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non - Markovian and have a large number of parameters that must be estimated .", "label": "", "metadata": {}, "score": "48.278313"}
{"text": "Lawless JF ( 2003 ) Statistical models and methods for lifetime data , 2nd edn .Wiley , London MATH .Le Besnerais G , Bercher J - F , Demoment G ( 1999 )A new look at entropy for solving linear inverse problems .", "label": "", "metadata": {}, "score": "48.37848"}
{"text": "Well , it 's time to have a look at this one .Edwin Thompson Jaynes presented some insightful results of maximum entropy principle in this 1957 paper published in Physics Reviews .This is also his first paper in information theory .", "label": "", "metadata": {}, "score": "48.45558"}
{"text": "It could play a key role in NLP tasks like Information Extraction , Question Answering and Summarization .We propose a machine learning algorithm for semantic role parsing , extending the work of Gildea and Jurafsky ( 2002 ) , Surdeanu et al .", "label": "", "metadata": {}, "score": "49.58497"}
{"text": "Miller DJ , Yan L ( 2000 )Approximate maximum entropy joint feature inference consistent with arbitrary lower order probability constraints : application to statistical classification .Neural Comput 12 : 2175 - 2208 CrossRef .Natarajan R , McCulloch CE ( 1998 )", "label": "", "metadata": {}, "score": "49.936607"}
{"text": "This paper presents a new Markovian sequence model , closely related to HMMs , that allows observations to be represented as arbitrary overlapping features ( such as word , capitalization , formatting , part - of - speech ) , and defines the conditional probability of state sequences given observation sequences .", "label": "", "metadata": {}, "score": "50.39607"}
{"text": "We present a maximum - likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently , using as examples several problems in natural language processing .Stephen Della Pietra , Vincent Della Pietra , and John Lafferty IEEE Transactions on Pattern Analysis and Machine Intelligence 19:4 , pp.380 - -393 , April , 1997 .", "label": "", "metadata": {}, "score": "50.409214"}
{"text": "The intersection of these constraints is the set of probability functions which are consistent with all the information sources .The function with the highest entropy within that set is the ME solution .ME takes all the previous words as possible features , so training ME model is computational challenging and sometimes almost infeasible .", "label": "", "metadata": {}, "score": "50.53531"}
{"text": "The intersection of these constraints is the set of probability functions which are consistent with all the information sources .The function with the highest entropy within that set is the ME solution .ME takes all the previous words as possible features , so training ME model is computational challenging and sometimes almost infeasible .", "label": "", "metadata": {}, "score": "50.535316"}
{"text": "Lazebnik , S. , Schmid , C. , Ponce , J. : A Sparse Texture Representation Using Local Affine Regions .IEEE Trans .PAMI 27(8 ) , 1265 - 1278 ( 2005 ) .Lazebnik , S. , Schmid , C. , Ponce , J. : A Maximum Entropy Framework for Part - Based Texture and Object Recognition .", "label": "", "metadata": {}, "score": "50.92262"}
{"text": "Notes .In ' 'A maximum entropy approach to natural language processing ' ' ( Computational Linguistics 22:1 , March 1996 ) , the appendix describes an approach to computing the gain of a single feature f .This note elaborates on the equations presented there ; in particular , we show how to derive equations ( 37 ) and ( 38 ) .", "label": "", "metadata": {}, "score": "50.973846"}
{"text": "We present a maximum - likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently , using as examples several problems in natural language processing .Abstract .This chapter presents an approach for texture and object recognition that uses scale- or affine - invariant local image features in combination with a discriminative classifier .", "label": "", "metadata": {}, "score": "51.09906"}
{"text": "The Relation of bayesian and maximum entropy methods ( 510Kb ) .In : Erickson GJ , Smith CR(eds ) Maximum - entropy and Bayesian methods in science and engineering , vol 1 .Kluwer , Dordrecht , pp 25 - 29 .", "label": "", "metadata": {}, "score": "51.663376"}
{"text": "Later , Rosenfeld and his group proposed a Whole Sentence Exponential Model that overcome the computation bottleneck of conditional ME model .You can find more on my SLM page .This dissertation discusses the application of maxent model to various Natural Language Dis - ambiguity tasks in detail .", "label": "", "metadata": {}, "score": "52.77531"}
{"text": "However , when the prior knowledge remains fuzzy or dubious , they often suffer from impropriety which can make them uncomfortable to use .In this article we suggest the formal elicitation of an encompassing family for the standard maximal entropy ( ME ) priors and the maximal data information ( MDI ) priors , which can lead to obtain proper families .", "label": "", "metadata": {}, "score": "53.19156"}
{"text": "The second approach uses the voted perceptron algorithm .Both algorithms give comparable , significant improvements over the maximum - entropy baseline .The voted perceptron algorithm can be considerably more efficient to train , at some cost in computation on test examples .", "label": "", "metadata": {}, "score": "53.733902"}
{"text": "References .Andrieu C , Doucet A , Fitzgerald WJ , P\u00e9rez JM ( 2001 )Bayesian computational approaches to model selection .In : Smith RL , Young PC , Walkden A(eds ) Nonlinear and non - Gaussian signal processing .", "label": "", "metadata": {}, "score": "54.687733"}
{"text": "MaxEnt and Exponential Models .This page contains pedagogically - oriented material on maximum entropy and exponential models .The emphasis is towards modelling of discrete - valued stochastic processes which arise in human language applications , such as language modelling .", "label": "", "metadata": {}, "score": "54.726196"}
{"text": "A note on noninformative priors for Weibull distributions .J Stat Plan Inference 61 : 319 - 338 MATH CrossRef .van Noortwijk JM , Dekker R , Cooke RM , Mazzuchi TA ( 1992 ) Expert judgment in maintenance optimization .", "label": "", "metadata": {}, "score": "54.903595"}
{"text": "( eds . )ECCV 2002 .LNCS , vol .2352 , pp .776 - 790 .Springer , Heidelberg ( 2002 ) CrossRef .McCallum , A. , Nigam , K. : A Comparison of Event Models for Naive Bayes Text Classification .", "label": "", "metadata": {}, "score": "55.78875"}
{"text": "Cambridge University Press , London MATH .Kadane JB , Wolfson J ( 1998 )A experiences in elicitation .The Statistician 47 : 3 - 19 .Kaminskiy MP , Krivtsov VV ( 2005 )A simple procedure for Bayesian estimation of the Weibull distribution .", "label": "", "metadata": {}, "score": "55.899185"}
{"text": "Constructing partial prior specifications for models of complex physical systems .The Statistician 47 : 37 - 53 .Forrester Y ( 2005 )The quality of expert judgement : an interdisciplinary investigation .Ph.D. thesis , University of Maryland .Friedlander MP , Gupta MR ( 2006 )", "label": "", "metadata": {}, "score": "55.909668"}
{"text": "Given consistent statistical evidence , a unique ME solution is guaranteed to exist , and an iterative algorithm exists which is guaranteed to converge to it .The ME framework is extremely general : any phenomenon that can be described in terms of statistics of the text can be readily incorporated .", "label": "", "metadata": {}, "score": "56.413"}
{"text": "IEEE Trans Inform Theory 52 : 238 - 245 CrossRef MathSciNet .Dai Y - S , Xie M , Long Q , Ng S - H ( 2007 ) Uncertainty analysis in software reliability modeling by bayesian analysis with maximum - entropy principle .", "label": "", "metadata": {}, "score": "56.479218"}
{"text": "We describe the knowledge sources used to build such a model with ARPA 's official WSJ corpus , and report on perplexity and word error rate results obtained with it .Then , three different adaptation paradigms are discussed , and an additional experiment , based on AP wire data , is used to compare them .", "label": "", "metadata": {}, "score": "56.491714"}
{"text": "Clarke B , Barron AR ( 1994 ) Jeffreys ' prior is asymptotically least favorable under entropy risk .J Stat Plan Inference 41 : 37 - 60 MATH CrossRef MathSciNet .Clarke B ( 2007 )Information optimality and Bayesian modelling .", "label": "", "metadata": {}, "score": "56.845818"}
{"text": "Venegaz - Martinez F ( 2004 )On information measures and prior distributions : a synthesis .Morfismos 8 : 27 - 50 .Zellner A ( 1977 ) Maximal data information prior distributions .In : Aykae A , Brumat C ( eds ) New developments in the applications of Bayesian methods , Amsterdam .", "label": "", "metadata": {}, "score": "57.392258"}
{"text": "Guida M , Calabria R , Pulcini G ( 1989 ) Bayes inference for a non - homogeneous Poisson process with power intensity law reliability .IEEE Trans Inform Theory 5 : 603 - 609 .Hill SD , Spall JC ( 1994 ) Sensitivity of a Bayesian analysis to the prior distribution .", "label": "", "metadata": {}, "score": "57.603374"}
{"text": "However , supervised training of accurate entity and relation extractors is costly , requiring a substantial number of labeled training examples for each type of entity and relation to be extracted .B .. by William W. Cohen , Sunita Sarawagi - In Proceedings of the ACM SIGKDD Conference , 2004 . \" ...", "label": "", "metadata": {}, "score": "58.296936"}
{"text": "To extract information from further back in the document 's history , we propose and use trigger pairs as the basic information bearing elements .This allows the model to adapt its expectations to the topic of discourse .Next , statistical evidence from multiple sources must be combined .", "label": "", "metadata": {}, "score": "58.325165"}
{"text": "To that end we put forward a unifying and multimodal framework , which views a video document from the perspective of its author .This framework forms the guiding principle for identifying index types , for which automatic methods are found in literature .", "label": "", "metadata": {}, "score": "58.353714"}
{"text": "Since then , Maximum Entropy technique ( and the more general framework Random Fields ) has enjoyed intensive research in NLP community .YASMET --Yet Another Simple Maximum Entropy Toolkit with Feature Selection .YASMET(2 ) --Yet Another Small MaxEnt Toolkit .", "label": "", "metadata": {}, "score": "58.48594"}
{"text": "The focus in this tutorial is on the foundation common to the two algorithms : convex functions and their convenient properties .Where examples are called for , we draw from applications in human language technology .This note concerns the improved iterative scaling algorithm for computing maximum - likelihood estimates of the parameters of exponential models .", "label": "", "metadata": {}, "score": "58.98286"}
{"text": "Shannon CE ( 1948 )A mathematical theory of communication .Bell Syst Technol J 27:379 - 423 , 623 - 656 .Shulman N , Feder M ( 2004 )The uniform distribution as a universal prior .IEEE Trans Inform Theory 50 : 1356 - 1362 CrossRef MathSciNet .", "label": "", "metadata": {}, "score": "59.228165"}
{"text": "The learning paradigm builds increasingly complex fields by allowing potential functions , or features , that are supported by increasingly large subgraphs .Each feature has a weight that is trained by minimizing the Kullback - Leibler divergence between the model and the empirical distribution of the training data .", "label": "", "metadata": {}, "score": "59.52428"}
{"text": "Efficient and effective handling of video documents depends on the availability of indexes .Manual indexing is unfeasible for large video collections .In this paper we survey several methods aiming at automating this time and resource consuming process .Good reviews on single modality based video in ... \" .", "label": "", "metadata": {}, "score": "59.564323"}
{"text": "J Comput Graph Stat 7 : 267 - 277 CrossRef .Oakley JE , O'Hagan A ( 2007 ) Uncertainty in prior elicitations : a nonparametric approach .Biometrika 94 : 427 - 441 MATH CrossRef MathSciNet .O'Hagan A ( 1998 )", "label": "", "metadata": {}, "score": "59.998707"}
{"text": "We reformulate the task as a combined chunking and classification problem , thus allowing our algorithm to be applied to new languages or genres of text for which statistical syntactic parsers may not be available . ... t of locative or temporal cues .", "label": "", "metadata": {}, "score": "60.159023"}
{"text": "To correct this mismatch we formalize a semi - Markov extraction process which relaxes the usual Markov assumptions .This process is based on sequentially classifying segments of several adjacent words , rather than single words .In addition to allowing a natural way of coupling NER and high - performance record linkage methods , this formalism also allows the direct use of other useful entity - level features , and provides a more natural formulation of the NER problem than sequential word classification .", "label": "", "metadata": {}, "score": "60.273827"}
{"text": "As a demonstration of the method , we describe its application to the problem of automatic word classification in natural language processing .An adaptive statistical language model is described , which successfullyintegrates long distance linguistic information with other knowledge sources .", "label": "", "metadata": {}, "score": "60.60087"}
{"text": "In this paper , we propose the application of another sampling technique : the Perfect Sampling ( PS ) .The experiment has shown a reduction of 30 % in the perplexity of the WSME model over the trigram model and a reduc- tion of 2 % over the WSME model trained with MCMC . by Diego Linares , Jos\u00e9 - miguel Bened\u00ed , Joan - andreu S\u00e1nchez , Javeriana Cali . \" ... Abstract .", "label": "", "metadata": {}, "score": "60.94706"}
{"text": "In this paper , we propose the application of another sampling technique : the Perfect Sampling ( PS ) .The experiment has shown a reduction of 30 % in the perplexity of the WSME model over the trigram model and a reduc- tion of 2 % over the WSME model trained with MCMC . by Diego Linares , Jos\u00e9 - miguel Bened\u00ed , Joan - andreu S\u00e1nchez , Javeriana Cali . \" ... Abstract .", "label": "", "metadata": {}, "score": "60.94706"}
{"text": "Another must read paper on maxent .It deals with a more general frame work : Random Fields and proposes an Improved Iterative Scaling algorithm for estimating parameters of Random Fields .This paper gives theoretical background to Random Fields ( and hence Maxent model ) .", "label": "", "metadata": {}, "score": "60.964767"}
{"text": "In : IJCAI Workshop on Machine Learning for Information Filtering , pp .61 - 67 ( 1999 ) .Rothganger , F. , Lazebnik , S. , Schmid , C. , Ponce , J. : 3D object modeling and recognition using local affine - invariant image descriptors and multi - view spatial constraints .", "label": "", "metadata": {}, "score": "61.20943"}
{"text": "Among all probability distributions that satisfy these con - straints , choose the one that has the highest entropy .More specifically , for estimating a probability function P(x ) , each constraint i is associated with a constraintfunctionfi(x ) and a desired expectation ci .", "label": "", "metadata": {}, "score": "61.492214"}
{"text": "Whole Sentence Language Model ) with sampling based training .Now seems to be part of scipy .Stanford Classifer is another open source implementation of Maximum Entropy Model in java , suitable for NLP tagging and parsing tasks .NLTK includes a maxent classifier written entirely in Python .", "label": "", "metadata": {}, "score": "61.57213"}
{"text": "Soofi ES ( 1992 )Information theory and Bayesian statistics .In : Berry DA , Chaloner KM , Geweke JK(eds )Bayesian analysis in statistics and econometrics in honor of Arnold Zellner .Wiley , New York , pp 179 - 189 .", "label": "", "metadata": {}, "score": "61.610542"}
{"text": "Bacha M , Celeux G , Id\u00e9e E , Lannoy A , Vasseur D ( 1998 ) Estimation de mod\u00e8les de dur\u00e9es de vie fortement censur\u00e9es .Eyrolles .Beirlant J , Dudewicz E , Gyorfi L , van der Meulen E ( 1997 )", "label": "", "metadata": {}, "score": "61.85627"}
{"text": "The first approach uses a boosting algorithm for ranking problems .The second approach uses the voted perceptron algorithm .Both algorithms g ... \" .We describe algorithms that rerank the top N hypotheses from a maximum - entropy tagger , the application being named - entity recognition in a corpus of web data .", "label": "", "metadata": {}, "score": "61.94007"}
{"text": "Berger JO , Bernardo JM ( 1992 )On the development of reference priors ( with discussion ) .In : Bernardo JM , Berger JO , Dawid AP , Smith AFM(eds ) Bayesian statistics , vol 4 .Oxford University Press , USA , pp 35 - 60 .", "label": "", "metadata": {}, "score": "62.05031"}
{"text": "Hidden Markov models ( HMMs ) are a powerful probabilistic tool for modeling sequential data , and have been applied with success to many text - related tasks , such as part - of - speech tagging , text segmentation and information extraction .", "label": "", "metadata": {}, "score": "62.149784"}
{"text": "In : Vernon , D. ( ed . )ECCV 2000 .LNCS , vol .1842 , pp .18 - 32 .Springer , Heidelberg ( 2000 ) CrossRef .Willamowski , J. , Arregui , D. , Csurka , G. , Dance , C.R. , Fan , L. : Categorizing nine visual classes using local appearance descriptors .", "label": "", "metadata": {}, "score": "62.65076"}
{"text": "Cooke R ( 1991 )Experts in uncertainty : opinion and subjective probability in science .Oxford University Press , New York .Cover TM , Thomas JA ( 1991 ) Elements of information theory .Wiley , New York MATH CrossRef .", "label": "", "metadata": {}, "score": "62.888824"}
{"text": "41 - 48 ( 1998 ) .Mikolajczyk , K. , Schmid , C. : A performance evaluation of local descriptors .In : Proc .CVPR 2003 , vol .2 , pp .257 - 263 ( 2003 ) .", "label": "", "metadata": {}, "score": "63.83088"}
{"text": "Lowe , D. : Distinctive image features from scale - invariant keypoints .IJCV 60(2 ) , 91 - 110 ( 2004 ) CrossRef .Mahamud , S. , Hebert , M. , Lafferty , J. : Combining Simple Discriminators for Object Discrimination .", "label": "", "metadata": {}, "score": "63.842422"}
{"text": "We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum - entropy models . \" ...Hidden Markov models ( HMMs ) are a powerful probabilistic tool for modeling sequential data , and have been applied with success to many text - related tasks , such as part - of - speech tagging , text segmentation and information extraction .", "label": "", "metadata": {}, "score": "64.03807"}
{"text": "A hybrid language model is defined as a combination of a word - based n - gram , which is used to capture the local relation ... \" .Abstract .This paper explores the use of initial Stochastic Context - Free Grammars ( SCFG ) obtained from a treebank corpus for the learning of SCFG by means of estimation algorithms .", "label": "", "metadata": {}, "score": "64.48167"}
{"text": "A hybrid language model is defined as a combination of a word - based n - gram , which is used to capture the local relation ... \" .Abstract .This paper explores the use of initial Stochastic Context - Free Grammars ( SCFG ) obtained from a treebank corpus for the learning of SCFG by means of estimation algorithms .", "label": "", "metadata": {}, "score": "64.48167"}
{"text": "This paper pursues the thesis that much greater accuracy can be achieved by further constraining the learning task , by coupling the semi - supervised training of many extractors for different categories and relations .We characterize several ways in which the training of category and relation extractors can be coupled , and present experimental results demonstrating significantly improved accuracy as a result .", "label": "", "metadata": {}, "score": "64.54652"}
{"text": "Tools . \" ...Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position .Among sequence labeling tasks in language processing , shallow parsing has received much attention , with the development of standard evaluati ... \" .", "label": "", "metadata": {}, "score": "64.96057"}
{"text": "Then , three different adaptation paradigms are discussed , and an additional experiment , based on AP wire data , is used to compare them .OVERVIEW OF ME FRAMEWORK Using several different probability estimates to arrive at one combined estimate is a general problem that arises in many tasks .", "label": "", "metadata": {}, "score": "64.96731"}
{"text": "Bayesian analysis for the poly - Weibull distribution .J Am Stat Assoc 88 : 1412 - 1418 MATH CrossRef MathSciNet .Berger JO , Sun D ( 1994 )Bayesian sequential reliability for Weibull and related distributions .Ann Inst Stat Math 46 : 221 - 249 MATH CrossRef MathSciNet .", "label": "", "metadata": {}, "score": "65.57876"}
{"text": "Manual indexing is unfeasible for large video collections .In this paper we survey several methods aiming at automating this time and resource consuming process .Good reviews on single modality based video indexing have appeared in literature .Effective indexing , however , requires a multimodal approach in which either the most appropriate modality is selected or the different modalities are used in collaborative fashion .", "label": "", "metadata": {}, "score": "65.62865"}
{"text": "A Bayesian analysis of industrial lifetime data with Weibull distributions , HAL - INRIA research report RR-6025 .Bousquet N ( 2006b )Analyse bay\u00e9sienne de la dur\u00e9e de vie de composants industriels ( Elements of Bayesian analysis for the prediction of the lifetime of industrial components ) .", "label": "", "metadata": {}, "score": "66.08025"}
{"text": "Object classes are represented using a dictionary of composite semi - local parts , or groups of nearby features with stable and distinctive appearance and geometric layout .A discriminative maximum entropy framework is used to learn the posterior distribution of the class label given the occurrences of parts from the dictionary in the training set .", "label": "", "metadata": {}, "score": "66.155594"}
{"text": "In a subjective framework , the performance of the method is shown in a reliability context when flat but proper priors are elicited for the Weibull lifetime distributions .Such priors appear as practical tools for sensitivity studies .Keywords .Bayesian inference Expert opinion Kullback - Leibler distance Shannon 's entropy Noninformative priors Channel coding Sensitivity study Weibull .", "label": "", "metadata": {}, "score": "66.97899"}
{"text": "The Statistician 47(1 ) : 21 - 35 MathSciNet .O'Hagan A ( 2006 )Research in elicitation .In : Upadhyay SK , Singh U , Dey DK(eds )Bayesian statistics and its applications .Anamaya , New Delhi , pp 375 - 382 . O'Hagan A , Buck CE , Daneshkhah A , Eiser JR , Garthwaite PH , Jenkinson DJ , Oakley JE , Rakow T ( 2006 )", "label": "", "metadata": {}, "score": "67.264984"}
{"text": "ICCV 2005 ( to appear , 2005 ) .Varma , M. , Zisserman , A. : Texture Classification : Are Filter Banks Necessary ?In : Proc .CVPR 2003 , vol .2 , pp .691 - 698 ( 2003 ) .", "label": "", "metadata": {}, "score": "67.27404"}
{"text": "ICCV 2005 ( to appear , 2005 ) .Lazebnik , S. , Schmid , C. , Ponce , J. : Semi - local Affine Parts for Object Recognition .In : Proc .BMVC 2004 ( 2004 ) .Lindeberg , T. : Feature Detection with Automatic Scale Selection .", "label": "", "metadata": {}, "score": "68.16882"}
{"text": "We evaluate our approach on common data sets ( namely , the MUC-6 and MUC-7 coreference corpora ) and obtain encouraging results , indicating that on the general noun phrase coreference task , the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches .", "label": "", "metadata": {}, "score": "69.78158"}
{"text": "Each of these features is true if the respective named entity is contained in the constituent .We found that adding this feature to the Null vs non - Null classifi ... . by Andrew Carlson , Estevam R. Hruschka , Justin Betteridge , Tom M. Mitchell , Richard C. Wang .", "label": "", "metadata": {}, "score": "70.21014"}
{"text": "This is a high - level tutorial on how to use MaxEnt for modelling discrete stochastic processes .The motivating example is the task of determining the most appropriate translation of a French word in context .The tutorial discusses the process of growing an exponential model by automatic feature selection ( \" inductive learning , \" if you will ) and also the task of estimating maximum - likelihood parameters for a model containing a fixed set of features .", "label": "", "metadata": {}, "score": "70.2361"}
{"text": "Int J Math Stat Sci 6 : 17 - 39 MATH MathSciNet .Berger JO ( 1985 ) Statistical decision theory and Bayesian analysis .Springer , New York MATH .Berger J ( 2006 )The Case for objective bayesian analysis .", "label": "", "metadata": {}, "score": "70.25281"}
{"text": "Noninformative priors do not exist : a discussion .J Stat Plan Inference 65 : 159 - 189 CrossRef MathSciNet .Billy F , Bousquet N , Celeux G ( 2006 )Modelling and eliciting expert knowledge with fictitious data .In : Proceedings of the workshop on the use of expert judgement for decision - making , CEA Cadarache .", "label": "", "metadata": {}, "score": "70.39955"}
{"text": "J Am Stat Assoc 90 : 598 - 604 MATH CrossRef MathSciNet .Goldstein M ( 2006 ) Subjective Bayesian analysis : principles and practice .Bayesian Anal 1 : 403 - 420 CrossRef MathSciNet .Goossens LHJ , Cooke RM ( 2006 ) Expert judgement - calibration and combination .", "label": "", "metadata": {}, "score": "70.81929"}
{"text": "Capturing the intangible concept of information .J Am Stat Assoc 89 : 1243 - 1254 MATH CrossRef MathSciNet .Soofi ES ( 2000 ) Principal information theoretic approaches .J Am Stat Assoc 95 : 1349 - 1353 MATH CrossRef MathSciNet .", "label": "", "metadata": {}, "score": "70.977264"}
{"text": "Claude Elwood Shannon 's influential 1948 paper that laid the foundation of information theory and changed the whole world since then .I see no reason who has read the above papers does not want to read this one .Information Theory and Statistical Mechanics ( Jaynes , E. T. , 1957 )", "label": "", "metadata": {}, "score": "71.04849"}
{"text": "Comparison with other machine learning technique ( Naive Bayes , Transform Based Learning , Decision Tree etc . ) was given .Ratnaparkhi also had a short introduction paper on ME .Abney applies Improved Iterative Scaling algorithm to parameters estimation of Attribute - Value grammars , which can not be corrected calculated by ERF method ( though it works on PCFG ) .", "label": "", "metadata": {}, "score": "71.33261"}
{"text": "Bousquet N , Celeux G ( 2006 )Bayesian agreement between prior and data .In : Proceedings of the ISBA congress , Benidorm , Spain .Celeux G , Marin J - M , Robert CP ( 2006 ) Iterated importance sampling in missing data problems .", "label": "", "metadata": {}, "score": "71.84071"}
{"text": "Experiments on the UPenn Treebank corpus are reported .These experiments have been carried out in terms of the test set perplexity and the word error rate in a speech recognition experiment . \" ... application .Furthermore , other structure knowledge such as context free grammar and link grammar [ 10 , 11 ] is introduced to language models for improving their performances .", "label": "", "metadata": {}, "score": "72.34429"}
{"text": "Experiments on the UPenn Treebank corpus are reported .These experiments have been carried out in terms of the test set perplexity and the word error rate in a speech recognition experiment . \" ... application .Furthermore , other structure knowledge such as context free grammar and link grammar [ 10 , 11 ] is introduced to language models for improving their performances .", "label": "", "metadata": {}, "score": "72.34429"}
{"text": "It also does not rest ... \" .this paper , we present a learning approach to coreference resolution of noun phrases in unrestricted text .The approach learns from a small , annotated corpus and the task includes resolving not just a certain type of noun phrase ( e.g. , pronouns ) but rather general noun phrases .", "label": "", "metadata": {}, "score": "72.467514"}
{"text": "Garthwaite PH , Kadane JB , O'Hagan A ( 2005 ) Statistical methods for eliciting probability distributions .J Am Stat Assoc 100 : 680 - 701 MATH CrossRef MathSciNet .Gelfand AE , Mallick BK , Dey DK ( 1995 )", "label": "", "metadata": {}, "score": "72.630066"}
{"text": "An word morphology application for English was developed . longer version .This paper applies ME technique to statistical language modeling task .More specifically , it builds a conditional Maximum Entropy model that incorporates traditional N - gram , distant N - gram and trigger pair features .", "label": "", "metadata": {}, "score": "72.69609"}
{"text": "Our algorithm is based on Support Vector Machines which we show give large improvement in performance over earlier classifiers .We show performance improvements through a number of new features designed to improve generalization to unseen data , such as automatic clustering of verbs .", "label": "", "metadata": {}, "score": "72.951416"}
{"text": "On the task of assigning semantic labels to the PropBank ( Kingsbury , Palmer , & Marcus , 2002 ) corpus , our final system has a precision of 84 % and a recall of 75 % , which are the best results currently reported for this task .", "label": "", "metadata": {}, "score": "73.253525"}
{"text": "The natural language processing community has recently experienced a growth of interest in domain independent shallow semantic parsing - the process of assigning a WHO did WHAT to WHOM , WHEN , WHERE , WHY , HOW etc . structure to plain text .", "label": "", "metadata": {}, "score": "73.30333"}
{"text": "The natural language processing community has recently experienced a growth of interest in domain independent shallow semantic parsing - the process of assigning a WHO did WHAT to WHOM , WHEN , WHERE , WHY , HOW etc . structure to plain text .", "label": "", "metadata": {}, "score": "73.30333"}
{"text": "Probability functions of the form ( 2 ) are called log - linear , and the family of functions defined by holding thefi 's fixed and varying the pi 's is called an exponential family . \" ...Lexical decoding is the obtaining of the most probable sequence of categories associated to a sequence of words .", "label": "", "metadata": {}, "score": "73.351685"}
{"text": "We present positive experimental results on the segmentation of FAQ 's . by Wee Meng Soon , Daniel Chung , Daniel Chung Yong Lim , Yong Lim , Hwee Tou Ng , 2001 . \" ... this paper , we present a learning approach to coreference resolution of noun phrases in unrestricted text .", "label": "", "metadata": {}, "score": "73.864075"}
{"text": "Sivic , J. , Zisserman , A. : Video Google : A Text Retrieval Approach to Object Matching in Videos .In : Proc .ICCV 2003 , pp .1470 - 1477 ( 2003 ) .Sivic , J. , Russell , B. , Efros , A. , Zisserman , A. , Freeman , W. : Discovering objects and their location in images .", "label": "", "metadata": {}, "score": "74.30505"}
{"text": "Suitable for text categorization and related NLP tasks .Here is another small maxent package in C++ with a BSD - like license , written by Dekang Lin .A must read paper on applying maxent technique to Natural Language Processing .", "label": "", "metadata": {}, "score": "74.42204"}
{"text": "When interfaced to SPHINX - II , Carnegie Mellon 's speech recognizer , it reduced its error rate by 10%--14 % .This thus illustrates the feasibility of incorporating many diverse knowledge sources in a single , unified statistical framework .Abstract .", "label": "", "metadata": {}, "score": "74.47277"}
{"text": "..Typically this data is in the form of annotated documents , which can be readily converted to ( x , y ) pairs .Most methods for learning taggers exploit , in some way , the sequential nature of the classif ... . \" ...", "label": "", "metadata": {}, "score": "74.572464"}
{"text": "You need GCC 2.9x to compile the source .link2 .MEGA Model Optimization Package .A recently appeared ME implementation by Hal Daum\u00e9 III .The software features CG and LM - BFGS Optimization and is written in OCaml .Although I no longer use OCaml , I 'd say that 's a great language , and is worth learning .", "label": "", "metadata": {}, "score": "75.25166"}
{"text": "Wiley , Chichester MATH CrossRef .Press SJ ( 2003 ) Subjective and objective Bayesian statistics , 2nd edn .Wiley , New York MATH .Robert CP ( 2001 )The Bayesian choice .a decision - theoretic motivation , 2nd edn .", "label": "", "metadata": {}, "score": "78.27849"}
{"text": "The proposed model combines trigram and structure knowledge of base phrase in which trigram is used to capture the local relation between words , while structure knowledge", "label": "", "metadata": {}, "score": "78.97508"}
{"text": "The proposed model combines trigram and structure knowledge of base phrase in which trigram is used to capture the local relation between words , while structure knowledge \" ...Lexical decoding is the obtaining of the most probable sequence of categories associated to a sequence of words .", "label": "", "metadata": {}, "score": "79.10239"}
{"text": "This is d ... \" .We consider the problem of improving named entity recognition ( NER ) systems by using external dictionaries - more specifically , the problem of extending state - of - the - art NER systems by incorporating information about the similarity of extracted entities to entities in an external dictionary .", "label": "", "metadata": {}, "score": "79.64075"}
{"text": "Among sequence labeling tasks in language processing , shallow parsing has received much attention , with the development of standard evaluation datasets and extensive comparison among methods .We show here how to train a conditional random field to achieve performance as good as any reported base noun - phrase chunking method on the CoNLL task , and better than any reported single model .", "label": "", "metadata": {}, "score": "79.86339"}
{"text": "In Chinese , shallow parsing has gotten some promising results [ 13 , 14].However , due to the lacks of the fine - annotated corpus ( such as Treebanks ) and competitive syntactic parser , it is infeasible to build a language model that depends on the complete parsing technique such as Chelba [ 9].", "label": "", "metadata": {}, "score": "81.838844"}
{"text": "In Chinese , shallow parsing has gotten some promising results [ 13 , 14].However , due to the lacks of the fine - annotated corpus ( such as Treebanks ) and competitive syntactic parser , it is infeasible to build a language model that depends on the complete parsing technique such as Chelba [ 9].", "label": "", "metadata": {}, "score": "81.83885"}
{"text": "Lexical decoding is the obtaining of the most probable sequence of categories associated to a sequence of words .This paper describes two lexical decoding combined models which are based on a stochastic category - based model and a probabilistic model of word distribution into linguistic categories .", "label": "", "metadata": {}, "score": "85.20713"}
{"text": "Lexical decoding is the obtaining of the most probable sequence of categories associated to a sequence of words .This paper describes two lexical decoding combined models which are based on a stochastic category - based model and a probabilistic model of word distribution into linguistic categories .", "label": "", "metadata": {}, "score": "85.20713"}
