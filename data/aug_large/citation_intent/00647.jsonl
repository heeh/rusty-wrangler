{"text": "In this paper , we propose a machine learning algorithm for shallow semantic parsing , extending the work of Gildea and Jurafsky ( 2002 ) , Surdeanu et al .( 2003 ) and others .Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers .", "label": "", "metadata": {}, "score": "23.600372"}
{"text": "In this paper , we propose a machine learning algorithm for shallow semantic parsing , extending the work of Gildea and Jurafsky ( 2002 ) , Surdeanu et al .( 2003 ) and others .Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers .", "label": "", "metadata": {}, "score": "23.600372"}
{"text": "The resulting word vectors are shown to be of higher quality in standard benchmark evaluations than vectors learned from monolingual text alone .Bamman et al .( 2014 ) introduced a model for incorporating geographical contextual information in learning vector - space representations of situated language .", "label": "", "metadata": {}, "score": "27.302326"}
{"text": "Moreover , in ( Le et al ., 2012 ) , a continuous space translation model was introduced and its use in a large scale machine translation system yielded promising results in the last WMT evaluation .This prevents them from gaining a deeper understanding of the semantics of longer phrases or sentences .", "label": "", "metadata": {}, "score": "27.823946"}
{"text": "We evaluate these models on two cross - lingual document classification tasks , outperforming the prior state of the art .Through qualitative analysis and the study of pivoting effects we demonstrate that our representations are semantically plausible and can capture semantic rela - tionships across languages without paral - lel data . ... al language models are another popular approach for inducing distributed word representations ( Bengio et al . , 2003 ) .", "label": "", "metadata": {}, "score": "28.26749"}
{"text": "2011 ) , dependency parsing ( Dhillon et al .2012 ) and probabilistic context - free grammars ( Cohen et al .2012 ) .In this workshop , we will bring together researchers who are interested in how to learn continuous vector space models , their compositionality and how to use this new kind of representation in NLP applications .", "label": "", "metadata": {}, "score": "28.872046"}
{"text": "2003 ) as a mapping of documents or words into a continuous lower dimensional topic - space .Another example , continuous word vector - space models ( Sahlgren 2006 , Reisinger 2012 , Turian et al . , 2010 , Huang et al .", "label": "", "metadata": {}, "score": "30.338703"}
{"text": "This paper presents an experimental comparison of all these approaches on a large statistical machine translation task .We also describe an open - source implementation to train and use continuous space language models ( CSLM ) for such large tasks .", "label": "", "metadata": {}, "score": "30.464806"}
{"text": "( 2015 ) proposed methods that transform word vectors into sparse ( and optionally binary ) vectors .The resulting representations are more similar to the interpretable features typically used in NLP , though they are discovered automatically from raw corpora .", "label": "", "metadata": {}, "score": "30.831741"}
{"text": "On the contrary , one influential proposal that uses the idea of continuous vector spaces for language modeling is that of neural language models ( Bengio et al . , 2003 , Mikolov 2012 ) .In these approaches , n - gram probabilities are estimated using a continuous representation of words in lieu of standard discrete representations , using a neural network that performs both the projection and the probability estimate .", "label": "", "metadata": {}, "score": "31.246117"}
{"text": "We find the new vectors show substantial improvement on benchmark tasks .Yogatama et al .( 2015 ) introduced a new technique for learning word representations using hierarchical regularization in sparse coding .This required significant algorithmic development .The resulting vectors were found to perform competitively on standard evaluation tasks , and the sparsity patterns are suggestive that greater interpretability in these representations may be possible .", "label": "", "metadata": {}, "score": "33.095367"}
{"text": "The multilingual vector representations produced will be released to the research community and will be used in undergraduate class projects .The project supports the education of two graduate students in a dynamic research environment .Summary of Research Findings .Faruqui and Dyer ( 2014 ) introduced a technique based on canonical correlation analysis that incorporates multilingual evidence into word vectors .", "label": "", "metadata": {}, "score": "34.29198"}
{"text": "2012 compares several of these approaches on supervised tasks and for phrases of arbitrary type and length .Another different trend of research on continuous vector space models belongs to the family of spectral methods .The motivation in that context is that working in a continuous space allows for the design of algorithms that are not plagued with the local minima issues that discrete latent space models ( e.g. HMM trained with EM ) tend to suffer from ( Hsu et al .", "label": "", "metadata": {}, "score": "34.76219"}
{"text": "On the fundamental task of language modeling , many hard clustering approaches have been proposed such as Brown clustering ( Brown et al.,1992 ) or exchange clustering ( Martin et al.,1998 ) .These algorithms can provide desparsification and can be seen as examples of unsupervised pre - training .", "label": "", "metadata": {}, "score": "34.83886"}
{"text": "We present a novel technique for learn - ing semantic representations , which ex - tends the distributional hypothesis to mul - tilingual data and joint - space embeddings .Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences , while maintaining suf ... \" .", "label": "", "metadata": {}, "score": "34.979465"}
{"text": "Getting the Meaning Right , ISWC 2011 .Freitas et al .Distributional Relational Networks , AAAI Fall Symposium 2013 .A language model is a function , or an algorithm for learning such a function , that captures the salient statistical characteristics of the distribution of sequences of words in a natural language , typically allowing one to make probabilistic predictions of the next word given preceding ones .", "label": "", "metadata": {}, "score": "35.47891"}
{"text": "However , due to lexical ambiguity , encoding word meaning with a single vector is problematic .This paper presents a method that uses clustering to produce multiple \" sense - specific&rdquo vectors for each word .This approach provides a context - dependent vector representation of word meaning that naturally accommodates homonymy and polysemy .", "label": "", "metadata": {}, "score": "35.579224"}
{"text": "We developed a new quantitative evaluation that judges geographically informed semantic similarity , and used it to demonstrated the ability of the new method to make reasonable inferences about word meaning .Faruqui et al .( 2015 ) introduced a technique , \" retrofitting , \" that uses semantic lexicons such as WordNet , FrameNet , and the Paraphrase Database to improve word vectors .", "label": "", "metadata": {}, "score": "36.244713"}
{"text": "Our models leverage parallel data and learn to strongly align the embeddings of semantically equivalent sentences , while maintaining sufficient distance between those of dissimilar sentences .The mod - els do not rely on word alignments or any syntactic information and are success - fully applied to a number of diverse lan - guages .", "label": "", "metadata": {}, "score": "36.463707"}
{"text": "The algorithm there is specialized to word prediction and is substantially s .. \" ...Continuous space language models have recently demonstrated outstanding results across a variety of tasks .In this paper , we examine the vector - space word representations that are implicitly learned by the input - layer weights .", "label": "", "metadata": {}, "score": "36.576817"}
{"text": "An early discussion can also be found in the Parallel Distributed Processing book ( 1986 ) , a landmark of the connectionist approach .For example , with \\(m\\ ) binary features , one can describe up to \\(2^m\\ ) different objects .", "label": "", "metadata": {}, "score": "36.631905"}
{"text": "Taken together , these additional features give a broader definition of a word 's context and lead to a more unified approach to the distributional approach to modeling human language , moving in the direction of a language - independent semantics .", "label": "", "metadata": {}, "score": "36.856873"}
{"text": "We find that this hypothesis only holds when it is applied to relevant dimensions .We propose a robust supervised approach that achieves accuracies of .84 and .85 on two existing datasets and that can be interpreted as selecting the dimensions that are relevant for distributional inclusion .", "label": "", "metadata": {}, "score": "37.029976"}
{"text": "First , this project incorporates translation contexts , i.e. , words readily available in multilingual parallel corpora , alongside traditional monolingual corpora .This allows evidence - sharing across languages , most importantly from resource - rich languages with large corpora to more resource - poor languages .", "label": "", "metadata": {}, "score": "37.27545"}
{"text": "First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar without a treebank .In our experiments , hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy .", "label": "", "metadata": {}, "score": "37.382893"}
{"text": "We present several improvements to unlexicalized parsing with hierarchically state - split PCFGs .First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar wi ... \" .", "label": "", "metadata": {}, "score": "37.524723"}
{"text": "This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm .We show how the algorithms can be efficiently applied to exponential sized representations of parse trees , such as the \" all subtrees \" ( DOP ) representation described by ( Bod 9 ... \" .", "label": "", "metadata": {}, "score": "37.585175"}
{"text": "Because many different combinations of feature values are possible , a very large set of possible meanings can be represented compactly , allowing a model with a comparatively small number of parameters to fit a large training set .The dominant methodology for probabilistic language modeling since the 1980 's has been based on n - gram models ( Jelinek and Mercer , 1980;Katz 1987 ) .", "label": "", "metadata": {}, "score": "38.185432"}
{"text": "We present a novel discriminative approach to parsing inspired by the large - margin criterion underlying support vector machines .Our formulation uses a factorization analogous to the standard dynamic programs for parsing .In particular , it allows one to efficiently learn a model which discriminates ... \" .", "label": "", "metadata": {}, "score": "38.259384"}
{"text": "Previous work in computational linguistics on extracting lexical semantic information from unannotated corpora does not provide adequate representational flexibility and hence fails to capture the full extent of human conceptual knowledge .In this thesis I outline a family of probabilistic models capable of capturing important aspects of the rich organizational structure found in human language that can predict contextual variation , selectional preference and feature - saliency norms to a much higher degree of accuracy than previous approaches .", "label": "", "metadata": {}, "score": "38.261528"}
{"text": "A key emphasis is scaling up algorithms for inferring distributional representations to web - scale corpora and dealing with much larger contextual vectors representing the expanded notion of context .The approach also leverages noisy syntactic processing to enable syntactic information , rather than just information about neighboring words , to be considered when defining context .", "label": "", "metadata": {}, "score": "38.444027"}
{"text": "References .Jelinek , F. and Mercer , R.L. ( 1980 ) Interpolated Estimation of Markov Source Parameters from Sparse Data .Pattern Recognition in Practice , Gelsema E.S. and Kanal L.N. eds , North - Holland .pp .381 - 397 .", "label": "", "metadata": {}, "score": "38.896076"}
{"text": "In fact , this motivation strikes with the conventional justification behind vector space models from the neural network literature , which are usually motivated as a way of tackling data sparsity issues .This apparent dichotomy is interesting and has not been investigated yet .", "label": "", "metadata": {}, "score": "38.926727"}
{"text": "Optimizing the latter remains a difficult challenge .In addition , it could be argued that using a huge training set ( e.g. , all the text in the Web ) , one could get n - gram based language models that appear to capture semantics correctly .", "label": "", "metadata": {}, "score": "39.165768"}
{"text": "We present a new neural network architecture which 1 ) learns word embeddings that better capture the semantics of words by incorporating both local and global document context , and 2 ) accounts for homonymy and polysemy by learning multiple embeddings per word .", "label": "", "metadata": {}, "score": "39.40055"}
{"text": "We propose an unbounded - depth , hierarchical , Bayesian nonparametric model for discrete sequence data .This model can be estimated from a single training sequence , yet shares statistical strength between subsequent symbol predictive distributions in such a way that predictive performance generalizes ... \" .", "label": "", "metadata": {}, "score": "39.693344"}
{"text": "Our formulation uses a factorization analogous to the standard dynamic programs for parsing .In particular , it allows one to efficiently learn a model which discriminates among the entire space of parse trees , as opposed to reranking the top few candidates .", "label": "", "metadata": {}, "score": "39.9349"}
{"text": "We evaluate Brown clusters , Collobert and Weston ( 2008 ) embeddings , and HLBL ( Mnih & Hinton , 2009 ) embeddings of words on both NER and ch ... \" .If we take an existing supervised NLP system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features .", "label": "", "metadata": {}, "score": "40.09424"}
{"text": "We show how the algorithms can be efficiently applied to exponential sized representations of parse trees , such as the \" all subtrees \" ( DOP ) representation described by ( Bod 98 ) , or a representation tracking all sub - fragments of a tagged sentence .", "label": "", "metadata": {}, "score": "40.27831"}
{"text": "In this proposal I will outline a family of probabilistic models capable of accounting for the rich organizational structure found in human language that can predict contextual variation , selectional preference and feature - saliency norms to a much higher degree of accuracy than previous approaches .", "label": "", "metadata": {}, "score": "40.69485"}
{"text": "Another idea is to decompose the probability computation hierarchically , using a tree of binary probabilistic decisions , so as to replace \\(O(N)\\ ) computations by \\(O(\\log N)\\ ) computations ( Morin and Bengio 2005 ) .Yet another idea is to replace the exact gradient by a stochastic estimator obtained using a Monte - Carlo sampling technique ( Bengio and Senecal 2008 ) .", "label": "", "metadata": {}, "score": "41.178574"}
{"text": "On the statistical side , I work with vector space models of real text .On the logical side , my work is based on type - logic - based grammars such as CCG , Lambek calculus , and pregroup grammars .", "label": "", "metadata": {}, "score": "41.24204"}
{"text": "( 3 )We provide two novel ways to extend the bimodal models to support three or more modalities .We find that the three- , four- , and five - dimensional models significantly outperform models using only one or two modalities , and that nontextual modalities each provide separate , disjoint knowledge that can not be forced into a shared , latent structure .", "label": "", "metadata": {}, "score": "41.654278"}
{"text": "This model can be estimated from a single training sequence , yet shares statistical strength between subsequent symbol predictive distributions in such a way that predictive performance generalizes well .The model builds on a specific parameterization of an unbounded - depth hierarchical Pitman - Yor process .", "label": "", "metadata": {}, "score": "41.95923"}
{"text": "However , we plan to extend the system to improve parse coverage , depth and accuracy . ... realistic texts .Evaluation of such systems has been primarily in terms of the PARSEVAL scheme tree similarity measures of ( labelled ) precision and recall and crossing bracket rate .", "label": "", "metadata": {}, "score": "42.009163"}
{"text": "Continuous space language models have recently demonstrated outstanding results across a variety of tasks .In this paper , we examine the vector - space word representations that are implicitly learned by the input - layer weights .We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language , and that each relationship is characterized by a relation - specific vector offset .", "label": "", "metadata": {}, "score": "42.055832"}
{"text": "Lambek J. ( 2008 )From word to sentence .Polimetrica , Milano .Preller , A. ( 2005 ) .Category theoretical semantics for pregroup grammars .In P. Blache , E. Stabler ( Eds . ) , Logical aspects of computational linguistics ( vol .", "label": "", "metadata": {}, "score": "42.22039"}
{"text": "CL ] , 2015 .NLP tasks differ in the semantic information they require , and at this time no single semantic representation fulfills all requirements .Logic - based representations characterize sentence structure , but do not capture the graded aspect of meaning .", "label": "", "metadata": {}, "score": "42.292534"}
{"text": "We present a novel generative model for natural language tree structures in which semantic ( lexical dependency ) and syntactic ( PCFG ) structures are scored with separate models .This factorization provides conceptual simplicity , straightforward opportunities for separately improving the component models , and a level of performance comparable to similar , non - factored models . ... known to be very effective .", "label": "", "metadata": {}, "score": "42.451653"}
{"text": "Multi - Prototype Vector - Space Models of Word Meaning [ Details ] [ PDF ] [ Slides ] Joseph Reisinger , Raymond J. Mooney In Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics ( NAACL-2010 ) , 109 - 117 , 2010 .", "label": "", "metadata": {}, "score": "42.61567"}
{"text": "( 2011 ) further popularised using neural network architectures for learning wo ... \" ...We present a novel generative model for natural language tree structures in which semantic ( lexical dependency ) and syntactic ( PCFG ) structures are scored with separate models .", "label": "", "metadata": {}, "score": "42.628464"}
{"text": "The generalizations of these models to high level categories have been applied to quantum protocols .Previously , I worked on algebraic and categorical models of multi - agent information flow , resulting in a sound and complete algebraic semantics and a cut - free sequent calculus with adjoint modalities for a positive and an intuitionistic fragment of dynamic epistemic logic .", "label": "", "metadata": {}, "score": "42.676575"}
{"text": "Finally , we present multilingual experiments which show that parsing with hierarchical state - splitting is fast and accurate in multiple languages and domains , even without any language - specific tuning . ... state splitting ( Matsuzaki et al . , 2005 ; Petrov et al . , 2006 ) .", "label": "", "metadata": {}, "score": "42.735992"}
{"text": "Figure 1 : Example of 2-dimensional distributed representation for words obtained in ( Blitzer et al 2005 ) .In ( Bengio et al 2001 , Bengio et al 2003 ) , it was demonstrated how distributed representations for symbols could be combined with neural network probability predictions in order to surpass standard n - gram models on statistical language modeling tasks .", "label": "", "metadata": {}, "score": "42.908463"}
{"text": "254 - 270 ) .of Lecture Notes in Artificial Intelligence .Preller A. ( 2007 ) Toward discourse representation via pregroup grammars .JoLLI 16 : 173 - 194 CrossRef .Preller , A. , & Prince , V. ( 2010 ) .", "label": "", "metadata": {}, "score": "42.925034"}
{"text": "A fundamental obstacle to progress in this direction has to do with the diffusion of gradients through long chains of non - linear transformations , making it difficult to learn long - term dependencies ( Bengio et al 1994 ) in sequential data .", "label": "", "metadata": {}, "score": "42.958176"}
{"text": "Concrete compositional sentence spaces .In Compositionality and distributional semantic models , ESSLLI 2010 .Kracht M. ( 2007 )The emergence of syntactical structure .Linguistics and Philosophy 30 : 47 - 95 CrossRef .Lambek J. ( 1958 )The mathematics of sentence structure .", "label": "", "metadata": {}, "score": "43.3441"}
{"text": "In this work , we present 1 . an effective method for pruning in split PCFGs 2 . a comparison of ... . \" ...We describe a robust accurate domain - independent approach to statistical parsing incorporated into the new release of the ANLT toolkit , and publicly available as a research tool .", "label": "", "metadata": {}, "score": "43.350105"}
{"text": "This is quite different from representing them in standard first - order logic . 2 ) knowledge base construction in the form of weighted inference rules from different sources like WordNet , paraphrase collections , and lexical and phrasal distributional rules generated on the fly .", "label": "", "metadata": {}, "score": "43.48385"}
{"text": "However , naive implementations of the above equations yield predictors that are too slow for large scale natural language applications .Schwenk and Gauvain ( 2004 ) were able to build systems in which the neural network component took less than 5 % of real - time ( the duration of the speech being analyzed ) .", "label": "", "metadata": {}, "score": "43.99166"}
{"text": "In addition to the computational challenges briefly described above , several weaknesses of the neural network language model are being worked on by researchers in the field .One of them is the representation of a fixed - size context .To represent longer - term context , one may employ a recurrent network formulation , which learns a representation of context that summarizes the past word sequence in a way that preserves information predictive of the future .", "label": "", "metadata": {}, "score": "44.015385"}
{"text": "We outperform text - only models in two different evaluations , and demonstrate that low - level visual features are directly compatible with the existing model .( 2 ) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images .", "label": "", "metadata": {}, "score": "44.14691"}
{"text": "Towards Dynamic Word Sense Discrimination with Random Indexing .Hans Moen , Erwin Marsi and Bjo\u0308rn Gamba\u0308ck .In recent years , there has been a growing interest in algorithms that learn a continuous representation for words , phrases , or documents .", "label": "", "metadata": {}, "score": "44.275543"}
{"text": "For a discussion of shallow vs deep architectures , see ( Bengio and LeCun 2007 ) .Whereas current models have two or three layers , theoretical research on deep architectures suggests that representing high - level semantic abstractions efficiently may require deeper networks .", "label": "", "metadata": {}, "score": "44.395615"}
{"text": "The Geometry of Information Retrieval .Cambridge University Press , Cambridge CrossRef .Sadrzadeh , M. ( 2007 ) .High - level quantum structures in linguistics and multi - agent systems .In AAAI spring symposium on quantum interactions .Publications : Lexical Semantics .", "label": "", "metadata": {}, "score": "44.625366"}
{"text": "This is much more than the number of operations typically involved in computing probability predictions for n - gram models .Several researchers have developed techniques to speed - up either probability prediction ( when using the model ) or estimating gradients ( when training the model ) .", "label": "", "metadata": {}, "score": "44.82568"}
{"text": "ML ID : 305 .Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views .In this work , we improve a two - dimensional multimodal version of Latent Dirichlet Allocation ( Andrews et al . , 2009 ) in various ways .", "label": "", "metadata": {}, "score": "45.039597"}
{"text": "Our work in the area has focused on learning word meanings for use in semantic parsing and , more recently , improved distributional ( vector space ) models of word meaning .Lexical semantics is part of our research on natural language learning .", "label": "", "metadata": {}, "score": "45.06735"}
{"text": "Our system handles overall sentence structure and phenomena like negation in the logic , then uses our Robinson resolution variant to query distributional systems about words and short phrases .Therefor , we use our system to evaluate distributional lexical entailment approaches .", "label": "", "metadata": {}, "score": "45.11763"}
{"text": "Furthermore , MVM uses soft feature assignment , hence the contribution of each data point to each clustering view is variable , isolating the impact of data only to views where they assign the most features .Through a series of experiments , we demonstrate the utility of MVM as an inductive bias for capturing relations between words that are intuitive to humans , outperforming related models such as Latent Dirichlet Allocation .", "label": "", "metadata": {}, "score": "45.33375"}
{"text": "Katz , S.M. ( 1987 )Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer .IEEE Transactions on Acoustics , Speech and Signal Processing 3:400 - 401 .Hinton , G.E. ( 1989 )Connectionist Learning Procedures .", "label": "", "metadata": {}, "score": "45.473206"}
{"text": "Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions .Abstract .We show that vector space semantics and functional semantics in two - sorted first order logic are equivalent for pregroup grammars .", "label": "", "metadata": {}, "score": "45.51712"}
{"text": "Lambek , J. ( 1993 ) .From categorial grammar to bilinear logic .In Substructural logics ( pp .207 - 237 ) .Oxford : Oxford University Press .Lambek J. ( 1999 ) Type grammar revisited .In : Lecomte et al . A. ( eds ) Logical Aspects of Computational Linguistics , Vol 1582 of LNAI .", "label": "", "metadata": {}, "score": "45.632137"}
{"text": "Context - dependent word similarity can be measured over multiple cross - cutting dimensions .Both of these notions of similarity play a role in determining word meaning , and hence lexical semantic models must take them both into account .Towards this end , we develop a novel model , Multi - View Mixture ( MVM ) , that represents words as multiple overlapping clusterings .", "label": "", "metadata": {}, "score": "45.925034"}
{"text": "We also describe the multiplicative combination of this dependency model with a model of linear constituency .The product model outperforms both components on their respective evaluation metrics , giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing .", "label": "", "metadata": {}, "score": "46.208187"}
{"text": "We show how to perform inference in such a model without truncation approximation and introduce fragmentation operators necessary to do predictive inference .We demonstrate the sequence memoizer by using it as a language model , achieving state - of - the - art results .", "label": "", "metadata": {}, "score": "46.211082"}
{"text": "Comparative Experiments on Disambiguating Word Senses : An Illustration of the Role of Bias in Machine Learning [ Details ] [ PDF ] Raymond J. Mooney In Proceedings of the Conference on Empirical Methods in Natural Language Processing ( EMNLP-96 ) , 82 - 91 , Philadelphia , PA , 1996 .", "label": "", "metadata": {}, "score": "46.27632"}
{"text": "Similar head - finding rules were used for Chinese experiments .The ... . \" ...We discuss the relevance of k - best parsing to recent applications in natural language processing , and develop efficient algorithms for k - best trees in the framework of hypergraph parsing .", "label": "", "metadata": {}, "score": "46.30403"}
{"text": "Remarkably , this method outperforms the best previous systems . ... performance in terms of word - prediction , but also the need for more computationally efficient models .Also of note , the use of distributed topic representations has been studied in ( Hinton and Salakhutdinov , 2006 ; Hinton and Salakhutdin ... . by Holger Schwenk , Anthony Rousseau , Mohammed Attik - In Proceedings of NAACL - HLT 2012 Workshop : Will We Ever Really Replace the N - gram Model ?", "label": "", "metadata": {}, "score": "46.385437"}
{"text": "A large literature on techniques to smooth frequency counts of subsequences has given rise to a number of algorithms and variants .Distributed representations .The idea of distributed representation has been at the core of the revival of artificial neural network research in the early 1980 's , best represented by the connectionist bringing together computer scientists , cognitive psychologists , physicists , neuroscientists , and others .", "label": "", "metadata": {}, "score": "46.40899"}
{"text": "U.K. : University of Sussex .Clark S.B.C. , Sadrzadeh M. ( 2008 )A compositional distributional model of meaning .In : Bruza W.L.P. , van Rijsbergen J. ( eds ) Proceedings of conference on quantum interactions .University of Oxford , College Publications .", "label": "", "metadata": {}, "score": "46.789043"}
{"text": "Figure 2 : Architecture of neural net language model introduced in ( Bengio et al 2001 ) .Note that the gradient on most of \\(C\\ ) is zero ( and need not be computed or used ) for most of the columns of \\(C\\ : \\ ) only those corresponding to words in the input subsequence have a non - zero gradient .", "label": "", "metadata": {}, "score": "46.814316"}
{"text": "the notion that humans make use of different categorization systems for different kinds of generalization tasks - and can be applied to Web - scale corpora .Using these models , natural language systems will be able to infer a more comprehensive semantic relations , in turn improving question answering , text classification , machine translation , and information retrieval .", "label": "", "metadata": {}, "score": "46.965794"}
{"text": "Frege in Space Formal World Real World .Based on vector space models .Practical way to automatically harvest word \" meanings \" on a large - scale .Focus on semantic approximation .Applications - Semantic search . -Approximate semantic inference . -", "label": "", "metadata": {}, "score": "47.346092"}
{"text": "Computational issues and applications .The experiments have been mostly on small corpora , where training a neural network language model is easier , and show important improvements on both log - likelihood and speech recognition accuracy .Resampling techniques may be used to train the neural network language model on corpora of several hundreds of millions of words ( Schwenk and Gauvain 2004 ) .", "label": "", "metadata": {}, "score": "47.457943"}
{"text": "This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance .We describe a hierarchy of loss functions that incorporate different levels of l ... \" .We present Minimum Bayes - Risk ( MBR ) decoding for statistical machine translation .", "label": "", "metadata": {}, "score": "47.514687"}
{"text": "Language models play an important role in large vocabulary speech recognition and statistical machine translation systems .The dominant approach since several decades are back - off language models .Some years ago , there was a clear tendency to build huge language models trained on hundreds of billion ... \" .", "label": "", "metadata": {}, "score": "48.23529"}
{"text": "Topics of Interest Compositional Distributional Semantics Category Theoretical Models of Natural Language Pregroup Grammars , Vector Space Models Dynamic Epistemic Logic , Algebra , Coalgebra and Proof Theory .PC work : Intl .I frequently review for Logic in CS and Computational Linguistics journals such as TCS , MSCS , JoLLI , Entropy , JLC , CL and ACL conferences .", "label": "", "metadata": {}, "score": "48.388283"}
{"text": "References Querying Freitas & Curry , Natural Language Queries over Heterogeneous Linked Data Graphs : A Distributional - Compositional Semantics Approach , IUI 2014 .Reasoning Pereira da Silva & Freitas , Towards An Approximative OntologyAgnostic Approach for Logic Programs , In FoIKS 2014 .", "label": "", "metadata": {}, "score": "49.245323"}
{"text": "We provide an efficient algorithm for learning such models and show experimental evidence of the model 's improved performance over a natural baseline model and a lexicalized probabilistic context - free grammar . by Shankar Kumar , William Byrne , Speech Processing - In Proceedings of HLT - NAACL , 2004 . \" ...", "label": "", "metadata": {}, "score": "49.270485"}
{"text": "We use near state - of - the - art supervised baselines , and find that each of the three word representations improves the accuracy of these baselines .We find further improvements by combining different word representations .You can download our word features , for off - the - shelf use in existing NLP systems , as well as our code , here : . ... system , a simple and general way to improve accuracy is to use unsupervised word representations as extra word features .", "label": "", "metadata": {}, "score": "49.385887"}
{"text": "Experimental results show that the number of examples needed to reach a given level of performance can be significantly reduced with this method .ML ID : 90 .Semantic Lexicon Acquisition for Learning Natural Language Interfaces [ Details ] [ PDF ] Cynthia A. Thompson and Raymond J. Mooney In Proceedings of the Sixth Workshop on Very Large Corpora , Montreal , Quebec , Canada , August 1998 .", "label": "", "metadata": {}, "score": "49.546234"}
{"text": "We introduce tiered clustering , a mixture model capable of accounting for varying degrees of shared ( context - independent ) feature structure , and demonstrate its applicability to inferring distributed representations of word meaning .Tiered clustering can also be viewed as a form of soft feature selection , where features that do not contribute meaningfully to the clustering can be excluded .", "label": "", "metadata": {}, "score": "49.551132"}
{"text": "We discuss the relevance of k - best parsing to recent applications in natural language processing , and develop efficient algorithms for k - best trees in the framework of hypergraph parsing .To demonstrate the efficiency , scalability and accuracy of these algorithms , we present experiments on Bikel 's implementation of Collins ' lexicalized PCFG model , and on Chiang 's CFG - based decoder for hierarchical phrase - based translation .", "label": "", "metadata": {}, "score": "49.587624"}
{"text": "Finally , we test the system on an alternate sentence representation , and on a set of large , artificial corpora with varying levels of ambiguity and synonymy .One difficulty in using machine learning methods for building natural language interfaces is building the required annotated corpus .", "label": "", "metadata": {}, "score": "49.82444"}
{"text": "This in turn will benefit the design of cognitive robots capable of learning ... \" .This position paper proposes that the study of embodied cognitive agents , such as humanoid robots , can advance our understanding of the cognitive development of complex sensorimotor , linguistic and social learning skills .", "label": "", "metadata": {}, "score": "50.13508"}
{"text": "234 .Introduction to Distributional Semantics Andr\u00e9 Freitas Insight Centre for Data Analytics Insight Workshop on Distributional Semantics Galway , 2014 Based on the Great ESSLLI Tutorial from Evert & Lenci .Shift in the Semantics Landscape Corroboration PraxisScientific / FormalPhilosophical Semantics as a complex phenomena .", "label": "", "metadata": {}, "score": "50.162086"}
{"text": "The usefulness is evaluated by examining the effect of using the lexicon learned by WOLFIE to assist a parser acquisition system , where previously this lexicon had to be hand - built .Future work in the form of extensions to the algorithm , further evaluation , and possible applications is discussed .", "label": "", "metadata": {}, "score": "50.30071"}
{"text": "The Frobenius anatomy of word meanings II : possessive relative pronouns , Mehrnoosh Sadrzadeh , Stephen Clark , Bob Coecke , Journal of Logic and Computation , Essays dedicated to Roy Dyckhoff on the occasion of his retirement , S. Graham - Lengrand and D. Galmiche ( eds . ) , doi : 10.1093/logcom / exu027 , 2014 .", "label": "", "metadata": {}, "score": "50.350006"}
{"text": "We present a generative model for the unsupervised learning of dependency structures .We also describe the multiplicative combination of this dependency model with a model of linear constituency .The product model outperforms both components on their respective evaluation metrics , giving the best pu ... \" .", "label": "", "metadata": {}, "score": "50.58235"}
{"text": "More sources can easily be added by mapping them to logical rules ; our system learns a resource - specific weight that counteract scaling differences between resources .3 ) inference , where we show how to solve the inference problems efficiently .", "label": "", "metadata": {}, "score": "50.745888"}
{"text": "The neural network learns to map that sequence of feature vectors to a prediction of interest , such as the probability distribution over the next word in the sequence .The advantage of this distributed representation approach is that it allows the model to generalize well to sequences that are not in the set of training word sequences , but that are similar in terms of their features , i.e. , their distributed representation .", "label": "", "metadata": {}, "score": "51.120636"}
{"text": "415 - 425 .Benthem , J.v . , & Doets , K. ( 1983 ) .Higher - order logic .In Handbook of Philosophical Logic ( pp .275 - 329 ) .Dordrecht : Reidel Publishing Company .Clarke , D. ( 2007 ) .", "label": "", "metadata": {}, "score": "51.210655"}
{"text": "The dominant approach since several decades are back - off language models .Some years ago , there was a clear tendency to build huge language models trained on hundreds of billions of words .Lately , this tendency has changed and recent works concentrate on data selection .", "label": "", "metadata": {}, "score": "51.28791"}
{"text": "arXiv:1405.2874 .Semantic Unification : a sheaf theoretic approach to natural language , Samson Abramsky and Mehrnoosh Sadrzadeh , In : Catgories and Types in Logic , Language , and Physics , Essays dedicated to Jim Lambek on the occasion of his 90th birthday , Casadio et al ( eds . ) , LNCS 8222 , pages 1- 13 , 2014 . arXiv:1403.3351 .", "label": "", "metadata": {}, "score": "51.296238"}
{"text": "Learning Distributed Representations of Concepts .Proceedings of the Eighth Annual Conference of the Cognitive Science Society:1 - 12 .Rumelhart , D. E. and McClelland , J. L ( 1986 ) Parallel Distributed Processing : Explorations in the Microstructure of Cognition .", "label": "", "metadata": {}, "score": "51.462597"}
{"text": "Edward Grefenstette ( graduated , Oxford , with Profs Coecke and Pulman ) Very - large and dynamic \" schemas \" .Knowledge circa 2013 circa 2000 10s-100s attributes 1,000s-1,000,000s attributes .Multiple perspectives ( conceptualizations ) of the reality .Ambiguity , vagueness , inconsistency .", "label": "", "metadata": {}, "score": "51.521935"}
{"text": "Transducing Sentences to Syntactic Feature Vectors : an Alternative Way to \" Parse \" ?Fabio Massimo Zanzotto and Lorenzo Dell'Arciprete .General estimation and evaluation of compositional distributional semantic models .Georgiana Dinu , Nghia The Pham and Marco Baroni .", "label": "", "metadata": {}, "score": "51.603302"}
{"text": "Current distributed representations of words show little resemblance to theories of lexical semantics .The former are dense and uninterpretable , the latter largely based on familiar , discrete classes ( e.g. , supersenses ) and relations ( e.g. , synonymy and hypernymy ) .", "label": "", "metadata": {}, "score": "51.84251"}
{"text": "In Compositionality and distributional semantic models , ESSLLI 2010 .Preller , A. , & Sadrzadeh , M. ( 2009 ) .Bell states and negated sentences in the distributed model of meaning .In 6th Workshop on quantum physics and logic .", "label": "", "metadata": {}, "score": "51.923306"}
{"text": "We describe a robust accurate domain - independent approach to statistical parsing incorporated into the new release of the ANLT toolkit , and publicly available as a research tool .The system has been used to parse many well known corpora in order to produce data for lexical acquisition efforts ; it has also been used as a component in an open - domain question answering project .", "label": "", "metadata": {}, "score": "51.938957"}
{"text": "Latent Variable Models of Distributional Lexical Semantics [ Details ] [ PDF ] Joseph Reisinger PhD Thesis , Department of Computer Science , University of Texas at Austin , May 2012 .In order to respond to increasing demand for natural language interfaces - and provide meaningful insight into user query intent - fast , scalable lexical semantic models with flexible representations are needed .", "label": "", "metadata": {}, "score": "51.947495"}
{"text": "Identifying Phrasal Verbs Using Many Bilingual Corpora [ Details ] [ PDF ][Poster ] Karl Pichotta and John DeNero In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing ( EMNLP 2013 ) , 636 - -646 , Seattle , WA , October 2013 .", "label": "", "metadata": {}, "score": "52.065384"}
{"text": "Poster session .A Structured Distributional Semantic Model : Integrating Structure with Semantics .Kartik Goyal , Sujay Kumar Jauhar , Huiying Li , Mrinmaya Sachan , Shashank Sri- vastava and Eduard Hovy .Letter N - Gram - based Input Encoding for Continuous Space Language Model .", "label": "", "metadata": {}, "score": "52.327217"}
{"text": "However , novel natural language text or discourse often presents both unknown concepts and words which refer to these concepts .Also , developmental data suggests that the learning of words and their concepts frequently occurs concurrently instead of concept learning proceeding word learning .", "label": "", "metadata": {}, "score": "52.60324"}
{"text": "We also discuss the role of bias in machine learning and its importance in explaining performance differences observed on specific problems .ML ID : 62 .Corpus - Based Lexical Acquisition For Semantic Parsing [ Details ] [ PDF ] Cynthia Thompson February 1996 .", "label": "", "metadata": {}, "score": "52.79052"}
{"text": "Semantic anomaly detection .Addressing the Vocabulary Problem for Databases ( with Distributional Semantics ) .Avg .query execution time : 8.53 s. Dataset ( DBpedia 3.7 + YAGO ) : 45,767 predicates , 5,556,492 classes and 9,434,677 instances .Compositional - Distributional Model : Distributional Relational Networks ( DRNs ) .", "label": "", "metadata": {}, "score": "52.912704"}
{"text": "\\ )Vector \\(C_k\\ ) contains the learned features for word \\(k\\ .Let us denote \\(\\theta\\ ) for the concatenation of all the parameters .The capacity of the model is controlled by the number of hidden units \\(h\\ ) and by the number of learned word features \\(d\\ .", "label": "", "metadata": {}, "score": "52.988262"}
{"text": "Experimental results are presented demonstrating WOLFIE 's ability to learn useful lexicons for a database interface in four different natural languages .The lexicons learned by WOLFIE are compared to those acquired by a competing system developed by Siskind ( 1996 ) .", "label": "", "metadata": {}, "score": "53.0884"}
{"text": "However , hierarchical architectures are notoriously hard to train and may therefore not reach up to their full potential .Hinton et al .proposed a novel learning method for deep belief networks , whi ... . \" ...Recurrent Neural Networks ( RNNs ) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly .", "label": "", "metadata": {}, "score": "53.45266"}
{"text": "After training the multiplicative RNN with the HF optimizer for five days on 8 high - end Graphics Processing Units , we were able to surpass the performance of the best previous single method for characterlevel language modeling - a hierarchical nonparametric sequence model .", "label": "", "metadata": {}, "score": "53.539757"}
{"text": "Each word corresponds to a point in a feature space .One can imagine that each dimension of that space corresponds to a semantic or grammatical characteristic of words .The hope is that functionally similar words get to be closer to each other in that space , at least along some directions .", "label": "", "metadata": {}, "score": "53.60413"}
{"text": "This is problematic because words are often polysemous and ... \" .Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems .However , most of these models are built with only local context and one representation per word .", "label": "", "metadata": {}, "score": "53.66747"}
{"text": "Most importantly , we find that they outperform the original vectors on benchmark tasks .Dr Mehrnoosh Sadrzadeh .Research Interests .I am interested in logical and mathematical systems and tools for modelling and reasoning about artificial intelligence .In particular , I am interested in understanding how language works , so we may enable computers to use it in ways similar to humans .", "label": "", "metadata": {}, "score": "54.116783"}
{"text": "Resolving Lexical Ambiguity in Tensor Regression Models of Meaning , Dimitri Kartsaklis , Nal Kalchbrenner and Mehrnoosh Sadrzadeh , In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics ( ACL ) : Short Papers .Baltimore ' USA , June , 2014 .", "label": "", "metadata": {}, "score": "54.16766"}
{"text": "ML ID : 252 .Cross - cutting Models of Distributional Lexical Semantics [ Details ] [ PDF ] [ Slides ] Joseph S. Reisinger June 2010 .Ph.D. proposal , Department of Computer Sciences , University of Texas at Austin .", "label": "", "metadata": {}, "score": "54.30126"}
{"text": "We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings , word - to - word alignments from an MT system , and syntactic structure from parse - trees of source and target language sentences .", "label": "", "metadata": {}, "score": "54.96454"}
{"text": "Kiela & Clark : A Systematic Study of Semantic Vector Space Model Parameters , 2014 .Tools . by Joseph Turian , D\u00e9partement D'informatique Et , Recherche Op\u00e9rationnelle ( diro , Universit\u00e9 De Montr\u00e9al , Lev Ratinov , Yoshua Bengio - In ACL , 2010 . \" ...", "label": "", "metadata": {}, "score": "55.37628"}
{"text": "\" If we can equate meaning with context , we can simply record the contexts in which a word occurs in a collection of texts ( a corpus ) .This can then be used as a surrogate of its semantic representation .", "label": "", "metadata": {}, "score": "56.1337"}
{"text": "Keywords .Compositional semantics Quantum logic Pregroup grammars Semantic vector models Symmetric compact closed categories Two - sorted functional first order logic .Share .References .Abramsky , S. , & Coecke , B. ( 2004 ) .A categorical semantics of quantum protocols .", "label": "", "metadata": {}, "score": "56.445824"}
{"text": "Lexical Acquisition : A Novel Machine Learning Problem [ Details ] [ PDF ] Cynthia A. Thompson and Raymond J. Mooney Technical Report , Artificial Intelligence Lab , University of Texas at Austin , January 1996 .This paper defines a new machine learning problem to which standard machine learning algorithms can not easily be applied .", "label": "", "metadata": {}, "score": "56.460648"}
{"text": "Marton Makrai , David Mark Nemeskey and Andras Kornai .Determining Compositionality of Expresssions Using Various Word Space Models and Methods .Lubomi\u0301r Krcma\u0301r , Karel Jez\u030cek and Pavel Pecina . \"Not not bad \" is not \" bad \" : A distributional account of negation .", "label": "", "metadata": {}, "score": "56.596344"}
{"text": "Core Operations Query DRN Core Operations Query & Reasoning Algorithms .Address the simplest matchings first ( heuristics ) .Semantic Relatedness as a primitive operation .Distributional semantics as commonsense knowledge .Specificity Ordering Use specificity ( grammatical class + ( IDF ) ) as a heuristc measure Ambiguity , vagueness , synonimy Low High .", "label": "", "metadata": {}, "score": "56.62819"}
{"text": "Building natural language parsing systems by hand is a tedious , error - prone undertaking .We build on previous research in automating the construction of such systems using machine learning techniques .The result is a combined system that learns semantic lexicons and semantic parsers from one common set of training examples .", "label": "", "metadata": {}, "score": "57.19344"}
{"text": "In natural language acquisition , it is difficult to gather the annotated data needed for supervised learning ; however , unannotated data is fairly plentiful .Active learning methods ( Cohn , Atlas , & Ladner , 1994 ) attempt to select for annotation and training only the most informative examples , and therefore are potentially very useful in natural language applications .", "label": "", "metadata": {}, "score": "57.273964"}
{"text": "The AP training corpus ( with 1 million word validation set folded in ) consisted of a total of 15 million words while t .. by Alina Beygelzimer , John Langford , Yuri Lifshits , Gregory Sorkin , Alex Strehl . \" ... 1.1 Main Results We consider the problem of estimating the conditional probability of a label in time O(log n ) , where n is the number of possible labels .", "label": "", "metadata": {}, "score": "57.282246"}
{"text": "We represent natural language semantics by combining logical and distributional information in probabilistic logic .We use Markov Logic Networks ( MLN ) for the RTE task , and Probabilistic Soft Logic ( PSL ) for the STS task .The system is evaluated on the SICK dataset .", "label": "", "metadata": {}, "score": "57.351772"}
{"text": "The review of specific issues and progress in these areas is then translated into a practical roadmap based on a series of milestones .These milestones provide a possible set of cognitive robotics goals and test - scenarios , thus acting as a research roadmap for future work on cognitive developmental robotics . .", "label": "", "metadata": {}, "score": "57.496536"}
{"text": "We fin ... .by Eric H. Huang , Richard Socher , Christopher D. Manning , Andrew Y. Ng - In Proc . of the Annual Meeting of the Association for Computational Linguistics ( ACL , 2012 . \" ...Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems .", "label": "", "metadata": {}, "score": "58.366173"}
{"text": "ML ID : 56 .Acquisition of a Lexicon from Semantic Representations of Sentences [ Details ] [ PDF ] Cynthia A. Thompson In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics ( ACL-95 ) , 335 - 337 , Cambridge , MA , 1995 .", "label": "", "metadata": {}, "score": "58.397354"}
{"text": "Big Multilinguality for Data - Driven Lexical Semantics .Last updated : August 4 , 2015 .A key challenge in natural language processing is defining the computational representation of words .Data - driven distributional approaches use corpora to induce vector - space representations for words , based on the contexts they occur in .", "label": "", "metadata": {}, "score": "58.52417"}
{"text": "These non - parametric learning algorithms are based on storing and combining frequency counts of word subsequences of different lengths , e.g. , 1 , 2 and 3 for 3-grams .\\ )Furthermore , a new observed sequence typically will have occurred rarely or not at all in the training set .", "label": "", "metadata": {}, "score": "58.933334"}
{"text": "The lexicons learned by Wolfie are compared to those acquired by a competing system developed by Siskind .ML ID : 95 .Semantic Lexicon Acquisition for Learning Natural Language Interfaces [ Details ] [ PDF ] Cynthia Ann Thompson PhD Thesis , Department of Computer Sciences , University of Texas at Austin , Austin , TX , December 1998 .", "label": "", "metadata": {}, "score": "59.11116"}
{"text": "The algorithms tested include statistical , neural - network , decision - tree , rule - based , and case - based classification techniques .The specific problem tested involves disambiguating six senses of the word ' ' line ' ' using the words in the current and proceeding sentence as context .", "label": "", "metadata": {}, "score": "59.179665"}
{"text": "This paper describes a system , WOLFIE ( WOrd Learning From Interpreted Examples ) , that acquires a semantic lexicon from a corpus of sentences paired with representations of their meaning .The lexicon learned consists of words paired with meaning representations .", "label": "", "metadata": {}, "score": "60.080788"}
{"text": "Dimitri Kartsaklis , Mehrnoosh Sadrzadeh . 14thMeeting on the Mathematics of Language ( MoL ) , 2015 .Open System Categorical Quantum Semantics in Natural Language Processing .Robin Piedeleu , Dimitri Kartsaklis , Bob Coecke , Mehrnoosh Sadrzadeh , Conference on Algebra and Coalgebra in Computer Science ( CALCO ) , 2015 .", "label": "", "metadata": {}, "score": "60.40445"}
{"text": "Also appears as Technical Report AI 99 - 278 , Artificial Intelligence Lab , University of Texas at Austin .A long - standing goal for the field of artificial intelligence is to enable computer understanding of human languages .A core requirement in reaching this goal is the ability to transform individual sentences into a form better suited for computer manipulation .", "label": "", "metadata": {}, "score": "60.960052"}
{"text": "The semantics is compositional , variable free and invariant under change of order or multiplicity .It includes the semantic vector models of Information Retrieval Systems and has an interior logic admitting a comprehension schema .A sentence is true in the interior logic if and only if the ' usual ' first order formula translating the sentence holds .", "label": "", "metadata": {}, "score": "61.12901"}
{"text": "Human concept organization is a rich epiphenomenon that has yet to be accounted for by a single coherent psychological framework : Concept generalization is captured by a mixture of prototype and exemplar models , and local taxonomic information is available through multiple overlapping organizational systems .", "label": "", "metadata": {}, "score": "61.26088"}
{"text": "Word Order Alternations in Sanskrit via Precyclicity in Pregroup Grammars , Claudia Casadio and Mehrnoosh Sadrzadeh , In Horizons of the Mind , Essays dedicated to Prakash Panangaden on the occasion of his 60th birthday , LNCS 8464 , May 2014 .", "label": "", "metadata": {}, "score": "61.29702"}
{"text": "ML ID : 241 .Acquiring Word - Meaning Mappings for Natural Language Interfaces [ Details ] [ PDF ] Cynthia A. Thompson and Raymond J. Mooney Journal of Artificial Intelligence Research , 18:1 - 44 , 2003 .This paper focuses on a system , Wolfie ( WOrd Learning From Interpreted Examples ) , that acquires a semantic lexicon from a corpus of sentences paired with semantic representations .", "label": "", "metadata": {}, "score": "61.48627"}
{"text": "Wolfie is part of an integrated system that learns to parse representations such as logical database queries .Experimental results are presented demonstrating Wolfie 's ability to learn useful lexicons for a database interface in four different natural languages .The usefulness of the lexicons learned by Wolfie are compared to those acquired by a similar system developed by Siskind ( 1996 ) , with results favorable to Wolfie .", "label": "", "metadata": {}, "score": "61.86879"}
{"text": "We analyze a natural reduction of this problem to a set of binary regression problems organized in a tree structure , proving a regret bound that scales with the depth of the tree .Motivated by this analysis , we propose the first online algorithm which provably constructs a logarithmic depth tree on the set of labels to solve this problem .", "label": "", "metadata": {}, "score": "62.343365"}
{"text": "If a human were to choose the features of a word , he might pick grammatical features like gender or plurality , as well as semantic features like animate \" or invisible .With a neural network language model , one relies on the learning algorithm to discover these features , and the features are continuous - valued ( making the optimization problem involved in learning much simpler ) .", "label": "", "metadata": {}, "score": "62.428154"}
{"text": "This paper describes a system , Wolfie ( WOrd Learning From Interpreted Examples ) , that acquires a semantic lexicon from a corpus of sentences paired with semantic representations .The lexicon learned consists of words paired with meaning representations .Wolfie is part of an integrated system that learns to parse novel sentences into semantic representations , such as logical database queries .", "label": "", "metadata": {}, "score": "62.475765"}
{"text": "10:00 Contributed talk : Jayant Krishnamurthy and Tom Mitchell Vector Space Semantic Parsing : A Framework for Compositional Vector Space Models .10:20 Contributed talk : Phong Le , Willem Zuidema and Remko Scha Learning from errors : Using vector - based compositional semantics for parse reranking .", "label": "", "metadata": {}, "score": "62.699455"}
{"text": "ML ID : 316 .Inclusive yet Selective : Supervised Distributional Hypernymy Detection [ Details ] [ PDF ] Stephen Roller and Katrin Erk and Gemma Boleda In Proceedings of the 25th International Conference on Computational Linguistics ( COLING 2014 ) , 1025 - -1036 , Dublin , Ireland , August 2014 .", "label": "", "metadata": {}, "score": "62.930542"}
{"text": "The ambiguous and synonymous nature of words causes the difficulty of using standard induction techniques to learn a lexicon .Additionally , negative examples are typically unavailable or difficult to construct in this domain .One approach to solve the lexical acquisition problem is presented , along with preliminary experimental results on an artificial corpus .", "label": "", "metadata": {}, "score": "63.021847"}
{"text": "By these means , we are able to train an CSLM on more than 500 million words in 20 hours .This CSLM provides an improvement of up to 1.8 BLEU points with respect to the best back - off language model that we were able to build .", "label": "", "metadata": {}, "score": "63.23101"}
{"text": "So it has been argued that the two are complementary .In this paper , we adopt a hybrid approach that combines logic - based and distributional semantics through probabilistic logic inference in Markov Logic Networks ( MLNs ) .We focus on textual entailment ( RTE ) , a task that can utilize the strengths of both representations .", "label": "", "metadata": {}, "score": "63.305397"}
{"text": "Associated large - scale commonsense knowledge base ( KB ) with no manual construction effort .The compositional - distributional model supports schema - agnostic QA system over a database : - Better recall and query coverage compared to baseline systems .", "label": "", "metadata": {}, "score": "63.468098"}
{"text": "3rd order tensor vector vector ( CHASE \u00d7 cats ) Baroni et al ., 2012 .( many slides were taken or adapted from this great tutorial ) .Turney & Pantel , From Frequency to Meaning : Vector Space Models of Semantics , 2010 .", "label": "", "metadata": {}, "score": "64.2973"}
{"text": "A new system , Wolfie , learns semantic lexicons to be used as background knowledge by a previously developed parser acquisition system , Chill .The combined system is tested on a real world domain of answering database queries .We also compare this combination to a combination of Chill with a previously developed lexicon learner , demonstrating superior performance with our system .", "label": "", "metadata": {}, "score": "65.05098"}
{"text": "ML ID : 45 .Integrated Learning of Words and their Underlying Concepts [ Details ] [ PDF ] Raymond J. Mooney In Proceedings of the Ninth Annual Conference of the Cognitive Science Society , 947 - 978 , Seattle , WA , July 1987 .", "label": "", "metadata": {}, "score": "65.71683"}
{"text": "Tree least general generalizations ( TLGGs ) of the representations of input sentences are performed to assist in determining the representations of individual words in the sentences .The best guess for a meaning of a word is the TLGG which overlaps with the highest percentage of sentence representations in which that word appears .", "label": "", "metadata": {}, "score": "65.8147"}
{"text": "For example , the male / female relationship is automatically learned , and with the induced vector representations , \" King - Man + Woman \" results in a vector very close to \" Queen .\" We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions ( provided with this paper ) , and are able to correctly answer almost 40 % of the questions .", "label": "", "metadata": {}, "score": "66.00125"}
{"text": "f 7.25 , while ( cup , substance ) received an average score of 1.92 .We downloaded these embeddings from Turian et al .( 2010 ) .These embeddings were trained on the smaller corpus RCV1 that contains one year of Reuters E .. \" ...", "label": "", "metadata": {}, "score": "66.40732"}
{"text": "Our polyglot ranking approach integrates frequency statistics from translated corpora in 50 different languages .Our experimental evaluation demonstrates that combining statistical evidence from many parallel corpora using a novel ranking - oriented boosting algorithm produces a comprehensive set of English phrasal verbs , achieving performance comparable to a human - curated set .", "label": "", "metadata": {}, "score": "67.27153"}
{"text": "This proposal describes a system , WOLFIE ( WOrd Learning From Interpreted Examples ) , that learns a lexicon from input consisting of sentences paired with representations of their meanings .Preliminary experimental results show that this system can learn correct and useful mappings .", "label": "", "metadata": {}, "score": "67.36508"}
{"text": "Recurrent Neural Networks ( RNNs ) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly .Fortunately , recent advances in Hessian - free optimization have been able to overcome the difficulties associated with training RNNs , making it possible to apply them successfully to challenging sequence problems .", "label": "", "metadata": {}, "score": "68.57"}
{"text": "A distributed representation is opposed to a local representation , in which only one neuron ( or very few ) is active at each time , i.e. , as with grandmother cells .One can view n - gram models as a mostly local representation : only the units associated with the specific subsequences of the input sequence are turned on .", "label": "", "metadata": {}, "score": "69.501465"}
{"text": "The best parameter configuration depends on the task .Systematic exploration of the parameters .I find it a little strange to now obliging the Commission to a motion for a resolution and to ask him at the same time to draw up a Green Paper on the current state of voluntary insurance and supplementary sickness insurance .", "label": "", "metadata": {}, "score": "70.8042"}
{"text": "Building accurate and efficient natural language processing ( NLP ) systems is an important and difficult problem .There has been increasing interest in automating this process .The lexicon , or the mapping from words to meanings , is one component that is typically difficult to update and that changes from one domain to the next .", "label": "", "metadata": {}, "score": "71.98219"}
{"text": "Compositional Semantics Words in which the meaning is directly determined by their distributional behaviour ( e.g. , nouns ) .Words that act as functions transforming the distributional profile of other words ( e.g. , verbs , adjectives , ... ) .", "label": "", "metadata": {}, "score": "72.04135"}
{"text": "This model is implemented as a word learning component added to the GENESIS explanation - based learning schema acquisition system for narrative understanding .A detailed example is described in which GENESIS learns provisional definitions for the words \" kidnap \" , \" kidnapper \" , and \" ransom \" as well as a kidnapping schema from a single narrative .", "label": "", "metadata": {}, "score": "72.26811"}
{"text": "Okham 's Razor Applied to Reasoning about Information Flow ' , Workshop on the Philosophy of Information and Logic , University of Oxford , Nov 2007 .I R meets NLP : On the Semantic Similarity between Subject - Verb - Object Phrases , Dmitrijs Milajevs , Mehrnoosh Sadrzadeh , Thomas Roelleke , ACM SIGIR International Conference on the Theory of Information Retrieval ( ICTIR ) , USA , 2015 .", "label": "", "metadata": {}, "score": "73.54346"}
{"text": "When the number of input variables increases , the number of required examples can grow exponentially .The curse of dimensionality arises when a huge number of different combinations of values of the input variables must be discriminated from each other , and the learning algorithm needs at least one example per relevant combination of values .", "label": "", "metadata": {}, "score": "75.82823"}
{"text": "Approach Overview Querying & Reasoning Core semantic approximation & composition operations Distributional Relational Network ( DRN )Structured / logic Knowledge Bases Distributional semantics Large - scale unstructured data Commonsense knowledge .Approach Overview Querying & Reasoning Core semantic approximation & composition operations Distributional Relational Network ( DRN ) Graph Data ( RDF ) Explicit Semantic Analysis ( ESA ) Wikipedia Commonsense knowledge .", "label": "", "metadata": {}, "score": "81.81657"}
{"text": ", 2012 .Strong DH : - A cognitive hypothesis about the form and origin of semantic representations .For our purposes ... - Context is equated with linguistic context .Distributional Semantic Models ( DSMs ) \" The dog barked in the park .", "label": "", "metadata": {}, "score": "83.0086"}
{"text": "To reduce annotation effort while maintaining accuracy , we apply active learning to semantic lexicons .We show that active learning can significantly reduce the number of annotated examples required to achieve a given level of performance .ML ID : 121 .", "label": "", "metadata": {}, "score": "84.22258"}
{"text": "Using these models , natural language systems will be able to infer a more comprehensive semantic relations , which in turn may yield improved systems for question answering , text classification , machine translation , and information retrieval .ML ID : 309 .", "label": "", "metadata": {}, "score": "88.28313"}
{"text": "Distributional Semantic Models ( DSMs ) \" The dog barked in the park .The owner of the dog put him on the leash since he barked . \"Distributional Semantic Models ( DSMs ) \" The dog barked in the park .", "label": "", "metadata": {}, "score": "88.72456"}
{"text": "Semantic Similarity & Relatedness \u03b8 car dog cat bark run leash .DSMs as Commonsense Reasoning Commonsense is here \u03b8 car dog cat bark run leash .Association measures ( Evert 2005 ) : are used to give more weight to contexts that are more significantly associated with a target word .", "label": "", "metadata": {}, "score": "93.353714"}
{"text": "Vocabulary Problem for Databases Query : Who is the daughter of Bill Clinton married to ?\" If these idealizations are removed it is not clear at all that modern semantics can give a full account of all but the simplest sentences . \"", "label": "", "metadata": {}, "score": "116.9415"}
{"text": "Question Analysis Transform natural language queries into triple patterns \" Who is the daughter of Bill Clinton married to ? \" Bill Clinton daughter married to PODS ( INSTANCE ) ( PREDICATE ) ( PREDICATE )Query Features .Query Plan Map query features into a query plan .", "label": "", "metadata": {}, "score": "118.20649"}
{"text": "Predicate Search Query : Bill Clinton daughter married to Which properties are semantically related to \" daughter \" ?Navigate Query : Linked Data : Bill Clinton daughter : child : Bill_Clinton : Chelsea_Clinton married to .Navigate Query : Linked Data : Bill Clinton daughter : child : Bill_Clinton : Chelsea_Clinton ( PIVOT ENTITY ) married to .", "label": "", "metadata": {}, "score": "128.60611"}
{"text": "Instance Search Query : Bill Clinton daughter Instance Search Linked Data : : Bill_Clinton married to .Predicate Search Query : Linked Data : Bill Clinton daughter married to : child : Bill_Clinton : Chelsea_Clinton : religion : Baptists : almaMater ...", "label": "", "metadata": {}, "score": "129.70941"}
