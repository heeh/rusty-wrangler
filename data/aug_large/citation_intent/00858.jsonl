{"text": "This is a particular type of HMM whose emission probabilities in each state are mixtures of shared Gaussians .This crucial constraint provides two major benefits .First , the a priori information contained in the common set of Gaussians leads to a more accurate estimate of the HMM parameters .", "label": "", "metadata": {}, "score": "27.724586"}
{"text": "It can be trained by the EM or GEM algorithms , considering state trajectories as missing data , which decouples temporal credit assignment and actual parameter estimation .The model presents similarities to hidden Markov models ( HMMs ) , but allows us to map input se - quences to output sequences , using the same processing style as recurrent neural networks .", "label": "", "metadata": {}, "score": "28.028027"}
{"text": "The HMM appecvs to have a slight advantage over PROFILESEARCH in terms of lower rates of false . \" ...Factor analysis , principal component analysis , mixtures of gaussian clusters , vector quantization , Kalman filter models , and hidden Markov models can all be unified as variations of unsupervised learning under a single basic generative model .", "label": "", "metadata": {}, "score": "28.120293"}
{"text": "The HMM appecvs to have a slight advantage over PROFILESEARCH in terms of lower rates of false . \" ...Factor analysis , principal component analysis , mixtures of gaussian clusters , vector quantization , Kalman filter models , and hidden Markov models can all be unified as variations of unsupervised learning under a single basic generative model .", "label": "", "metadata": {}, "score": "28.120293"}
{"text": "This paper proposes a novel similarity measure between vector sequences .We work in the framework of model - based approaches , where each sequence is first mapped to a Hidden Markov Model ( HMM ) and then a measure of similarity is computed between the HMMs .", "label": "", "metadata": {}, "score": "28.288956"}
{"text": "State space models are based on a continuous state vector evolving through time according to a state evo- .... mance , mathematically this technique conflicts with the independence assumption .This independence assumption is widely thought to be the major drawback of the use of HMMs for speech recognition ( eg .", "label": "", "metadata": {}, "score": "28.59148"}
{"text": "This report describes an attempt at capturing segmental transition information for speech recognition tasks .The slowly varying dynamics of spectral trajectories carries much discriminant information that is very crudely modelled by traditional approaches such as HMMs .In approaches such as recurrent neural networks there is the hope , but not the convincing demonstration , that such transitional information could be captured .", "label": "", "metadata": {}, "score": "29.028933"}
{"text": "Statistical techniques based on hidden Markov Models ( HMMs ) with Gaussian emission densities have dominated signal processing and pattern recognition literature for the past 20 years .However , HMMs trained using maximum likelihood techniques suffer from an inability to learn discriminative information and are prone to overfitting and over - parameterization .", "label": "", "metadata": {}, "score": "29.160465"}
{"text": "The posteriors are then converted into scaled likelihoods and used as the observation probabilities within a conventional decoding paradigm ( e.g. , Viterbi decoding ) .The advantages of using recurrent networks are that they require a small number of parameters and provide a fast decoding capability ( relative 3 to conventional , large - vocabulary , HMM systems ) .", "label": "", "metadata": {}, "score": "29.268784"}
{"text": "We show that factor analysis and mixtures of gaussians can be implemented in autoencoder neural networks and learned using squared error plus the same regularization term .We introduce a new model for static data , known as sensible principal component analysis , as well as a novel concept of spatially adaptive observation noise .", "label": "", "metadata": {}, "score": "29.553099"}
{"text": "We show that factor analysis and mixtures of gaussians can be implemented in autoencoder neural networks and learned using squared error plus the same regularization term .We introduce a new model for static data , known as sensible principal component analysis , as well as a novel concept of spatially adaptive observation noise .", "label": "", "metadata": {}, "score": "29.553099"}
{"text": "The observation process can be represented as a factor analysis model or a linear discriminant analysis model .General HMMs and schemes proposed to improve their performance such as STC can be regarded as special cases in this framework . \" ...", "label": "", "metadata": {}, "score": "30.034916"}
{"text": "i . ... aches have been proposed for extending the range of dependencies modelled by both generative and discriminative statistical models .For discriminative models , latent - variable extensions such as ... . \" ...Currently , most approaches to speech recognition are frame - based in that they represent speech as a temporal sequence of feature vectors .", "label": "", "metadata": {}, "score": "30.644855"}
{"text": "Successively more general models ... \" .This report describes a new technique for inducing the structure of Hidden Markov Models from data which is based on the general ' model merging ' strategy ( Omohundro 1992 ) .The process begins with a maximum likelihood HMM that directly encodes the training data .", "label": "", "metadata": {}, "score": "30.795227"}
{"text": "Linear Gaussian models ( LGM ) are popular as many forms may be trained efficiently using the expectation maximisation algorithm .In this paper , several LGMs and generalised LGMs are reviewed .The models can be roughly categorised into four combinations according to two different state evolution and two different observation processes .", "label": "", "metadata": {}, "score": "31.202396"}
{"text": "As was mentioned above , the probability that an input utterance corresponds to a given HMM can be computed by the Viterbi algorithm , which finds the sequence of model states which maximizes this probability .This optimization can be viewed as asimultaneous probability computation and alignment of the input utterance and the model .", "label": "", "metadata": {}, "score": "31.681824"}
{"text": "We present a tree - structured architecture for supervised learning .The statistical model underlying the architecture is a hi - erarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models ( GLIM 's ) .", "label": "", "metadata": {}, "score": "32.61885"}
{"text": "We present a tree - structured architecture for supervised learning .The statistical model underlying the architecture is a hi - erarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models ( GLIM 's ) .", "label": "", "metadata": {}, "score": "32.61885"}
{"text": "We present a tree - structured architecture for supervised learning .The statistical model underlying the architecture is a hi - erarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models ( GLIM 's ) .", "label": "", "metadata": {}, "score": "32.61885"}
{"text": "The Viterbi algorithm can be viewed as simultaneously performing an alignment of the input utterance and the model and computing the probability ofthat alignment .HMMs can be created to model entire words , or alternatively , a variety of sub - word linguistic units , such as phonemes or syllables .", "label": "", "metadata": {}, "score": "33.32531"}
{"text": "Factor analysis , principal component analysis , mixtures of gaussian clusters , vector quantization , Kalman filter models , and hidden Markov models can all be unified as variations of unsupervised learning under a single basic generative model .This is achieved by collecting together disparate observations and derivations made by many previous authors and introducing a new way of linking discrete and continuous state models using a simple nonlinearity .", "label": "", "metadata": {}, "score": "33.501167"}
{"text": "Factor analysis , principal component analysis , mixtures of gaussian clusters , vector quantization , Kalman filter models , and hidden Markov models can all be unified as variations of unsupervised learning under a single basic generative model .This is achieved by collecting together disparate observations and derivations made by many previous authors and introducing a new way of linking discrete and continuous state models using a simple nonlinearity .", "label": "", "metadata": {}, "score": "33.501167"}
{"text": "Currently , most approaches to speech recognition are frame - based in that they represent speech as a temporal sequence of feature vectors .Although these approaches have been successful , they can not easily incorporate complex modeling strategies that may further improve speech recognition performance .", "label": "", "metadata": {}, "score": "33.65689"}
{"text": "A Bayesian posterior probability criterion is used to determine which states to merge and when to stop generalizing .The procedure may be considered a heuristic search for the HMM structure with the highest posterior probability .We discuss a variety of possible priors for HMMs , as well as a number of approximations which improve the computational efficiency of the algorithm .", "label": "", "metadata": {}, "score": "34.272648"}
{"text": "Furthermore , in the last few years , many new and promising probabilistic models related to HMMs have been proposed .We firs ... \" .Hidden Markov Models ( HMMs ) are statistical models of sequential data that have been used successfully in many machine learning applications , especially for speech recognition .", "label": "", "metadata": {}, "score": "34.356125"}
{"text": "For each subsequent frame i.e. , as speech continues to be processed , the most probable sequence of HMM states is updated .A variety of methods of varying computational complexity are known to persons of ordinary skill in the ASR art for finding the most probable sequence of HMM states .", "label": "", "metadata": {}, "score": "34.629143"}
{"text": "Notwithstanding their higher accuracy , more complex approaches tend to obscure the relationship between secondary structure and chemical shift and often involve many parameters that need to be trained .We present hidden Markov models ( HMM ) with Gaussian emission probabilities to model the dependence between protein chemical shifts and secondary structure .", "label": "", "metadata": {}, "score": "34.64416"}
{"text": "A wide variety of models have been developed but they all share the property that they describe the temporal characteristics of spectra typical to particularwords or sub- word segments .The sequence of spectral vectors arising from an input utterance is compared with the models and the success with which models of different words predict the behavior of the input frames , determines the putative identity ofthe utterance .", "label": "", "metadata": {}, "score": "34.66641"}
{"text": "\" Segmental hidden Markov models \" ( SHMMs ) are intended to overcome important speech - modelling limitations of the conventional - HMM approach by representing sequences ( or segments ) of features and incorporating the concept of trajectories to describe how features change over time .", "label": "", "metadata": {}, "score": "34.92282"}
{"text": "This paper addresses the time - series modelling of high dimensional data .Currently , the hidden Markov model ( HMM ) is the most popular and successful model especially in speech recognition .However , there are well known shortcomings in HMMs particularly in the modelling of the correlation between successive observation vectors ; that is , inter - frame correlation .", "label": "", "metadata": {}, "score": "35.015915"}
{"text": "We describe a pre - trained deep neural network hidden Markov model ( DNN - HMM ) hybrid architecture that trains the DNN to produce a distribution over senones ( tied triphone states ) as its output .The deep belief network pre - training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error .", "label": "", "metadata": {}, "score": "35.18962"}
{"text": "Previous attempts to construct acoustic models that remove this assumption have suffered from a significant increase in the number of parameters to train .Another weakness ... \" .The HMM assumption of conditional independence of observations causes a variety of problems for speech - recognition applications .", "label": "", "metadata": {}, "score": "35.266384"}
{"text": "We would like to emphasize that , even though we use the UBM as the quantizer in this paper , the occurrence counts of the discrete events could come from a phone recognizer , a prosodic feature extractor or a speech recognition system as shown in Fig .", "label": "", "metadata": {}, "score": "35.437176"}
{"text": "As used heretofore , discriminative training has been applied most successfully to small - vocabulary tasks .In addition , it presents a number of new problems , such as how to appropriately smooth the discriminatively - trained pdfs and how to adaptthese systems to a new user with a relatively small amount of training data .", "label": "", "metadata": {}, "score": "35.574356"}
{"text": "Using these distributions as outputs of _ rst and second order HMMs we achieve a prediction accuracy of 82.3 % , which is competitive with existing methods for predicting secondary structure from protein chemical shifts .Incorporation of sequence - based secondary structure prediction into our HMM improves the prediction accuracy to 84.0 % .", "label": "", "metadata": {}, "score": "35.57482"}
{"text": "Currently the most popular acoustic model for speech recognition is the hidden Markov model ( HMM ) .However , HMMs are based on a series of assumptions some of which are known to be poor .In particular , the assumption that successive speech frames are conditionally independent given the discrete state that generated them is not a good assumption for speech recognition .", "label": "", "metadata": {}, "score": "35.67439"}
{"text": "When multiple sets of models are used sequentially during the search , a separate simultaneous alignment and pdf evaluation is essentiallycarried out for each set .In other prior art approaches , computational speedups are applied to the evaluation of the high - resolution pdfs .", "label": "", "metadata": {}, "score": "35.755356"}
{"text": "In order to accommodate the variety of accents and other variations in the way words are pronounced , spoken messages to be identified using a HMM ASR system are processed in such a manner as to extract feature vectors that characterize successive periods of the spoken message .", "label": "", "metadata": {}, "score": "35.86204"}
{"text": "State space models are based on a hidden continuous state evolution process and an observation process wh ... . \" ...This report describes an attempt at capturing segmental transition information for speech recognition tasks .The slowly varying dynamics of spectral trajectories carries much discriminant information that is very crudely modelled by traditional approaches such as HMMs .", "label": "", "metadata": {}, "score": "36.025772"}
{"text": "One of the advantages of this approach isthat it makes no assumptions about the nature of such pdfs , but this is offset by the information loss incurred in the quantization stage .The use of continuous pdfs eliminates the quantization step , and the probability vectors are replaced by parametric functions which specify the probability of any arbitrary input spectral vector given a state .", "label": "", "metadata": {}, "score": "36.06019"}
{"text": "This paper first provides a brief overview of graphical models and their uses as statistical models .Moreover , this paper shows that many advanced models for speech recognition and language processing can also be simply described by a graph , including many at the acoustic- , pronunciation- , and language - modeling levels .", "label": "", "metadata": {}, "score": "36.08263"}
{"text": "We approach this by introducing a temporal constraint into the well known technique of Principal Component Analysis .On this subspace , we attempt a parametric modelling of the trajectory , and compute a distance metric to perform classification of diphones .", "label": "", "metadata": {}, "score": "36.193916"}
{"text": "The evolution of the state vector may be viewed as a ...Support vector machines ( SVMs ) , and kernel classifiers in general , rely on the kernel functions to measure the pairwise similarity between inputs .This paper advocates the use of discrete representation of speech signals in terms of the probabilities of discrete events as feature for speaker verification and proposes the use of Bhattacharyya coefficient as the similarity measure for this type of inputs to SVM .", "label": "", "metadata": {}, "score": "36.24775"}
{"text": "Thisrequirement constrains this method to use approximate and detailed models which are fairly closely related and thus generate probabilities of comparable magnitude .It should also be noted that in this method there is no guarantee that all of theindividual state probabilities that make up the final alignment probability come from detailed models .", "label": "", "metadata": {}, "score": "36.556084"}
{"text": "Tools . \" ...We present a tree - structured architecture for supervised learning .The statistical model underlying the architecture is a hi - erarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models ( GLIM 's ) .", "label": "", "metadata": {}, "score": "36.55876"}
{"text": "The extra - segmental component of the model is represented in terms of variability in the trajectory parameters , and these models are therefore referred to as \" probabilistic - trajectory segmental HMMs \" ( PTSHMMs ) .This paper presents the theory of PTSHMMs using a linear trajectory description characterized by slope and mid - point parameters , and presents theoretical and experimental comparisons between different types of PTSHMMs , simpler SHMMs and conventional HMMs .", "label": "", "metadata": {}, "score": "36.592293"}
{"text": "In traditional speech recognition using Hidden Markov Models ( HMMs ) , each state represents an acoustic portion of a phoneme .We explore the concept of an articulator based HMM , where each state represents a particular articulatory configuration [ Erler 1996].", "label": "", "metadata": {}, "score": "36.744244"}
{"text": "Finally , we discuss some of the challenges of future research in this very active area .1 Introduction Hidden Markov Models ( HMMs ) are statistical models of sequential data that have been used successfully in many applications in artificial intelligence , pattern recognition , speech recognition , and modeling of biological ... . ... ognition , and speech recognition .", "label": "", "metadata": {}, "score": "36.76654"}
{"text": "Additionally , the development of CDG grammars using our grammar tools and parser is discussed . ... s ( e.g. , [ 21 , 40 ] ) ) have reduced recognition errors by incorporating some language model into their system to reduce perplexity .", "label": "", "metadata": {}, "score": "36.88824"}
{"text": "A thorough exploration of this space should yield techniques that ultimately will supersede the hidden Markov model . ... ls , Pattern Recognition , Delta Features , Time - Derivative Features , Structural Discriminability , Language Modeling 1 .Introduction . \" ... \" Segmental hidden Markov models \" ( SHMMs ) are intended to overcome important speech - modelling limitations of the conventional - HMM approach by representing sequences ( or segments ) of features and incorporating the concept of trajectories to describe how features change over time .", "label": "", "metadata": {}, "score": "37.06064"}
{"text": "Given a set of points drawn from a smooth manifold in an abstract feature space , the technique is capable of determining the structure of the surface and offinding the closest ... \" .A technique for representing and learning smooth nonlinear manifolds is presented and applied to several lip reading tasks .", "label": "", "metadata": {}, "score": "37.58883"}
{"text": "These lattices can be used to constrain further decoding , allowing efficient use of complex acoustic and language models .The effectiveness of these techniques has been assessed on a variety of large vocabulary continuous speech recognition tasks and results are presented which analyse performance in terms of computational complexity and recognition accuracy .", "label": "", "metadata": {}, "score": "37.717773"}
{"text": "f ) taking the log of the magnitude of each of the bandpass filtered outputs to obtain a plurality of log magnitude band pass filtered outputs ; and .g ) transforming the plurality of log magnitude bandpass filtered outputs to a time domain to obtain at least a subset of the first set of numbers .", "label": "", "metadata": {}, "score": "37.78203"}
{"text": "The search algorithm is based on stack decoding and uses both likelihood- and posterior - based pruning .The use of the posterior - based phone deactivation pruning techniques is well - suited to hybrid connectionist / HMM systems because posterior phone probabilities are directly computed by the connectionist acoustic model .", "label": "", "metadata": {}, "score": "37.79583"}
{"text": "f ) taking the log of the magnitude of each of the bandpass filtered outputs to obtain a plurality of log magnitude bandpass filtered outputs ; and .g ) transforming the plurality of log magnitude bandpass filtered outputs to a time domain to obtain at least a subset of the first set of numbers .", "label": "", "metadata": {}, "score": "37.962776"}
{"text": "This paper describes a new search technique for large vocabulary speech recognition based on a stack decoder .Considerable memory savings are achieved with the combination of a tree based lexicon and a new search technique .The search proceeds time - first , that is partial path hypotheses are extended ... \" .", "label": "", "metadata": {}, "score": "38.044056"}
{"text": "For these reasons it is sufficient to rely on conventional Maximum - Likelihood training for the discrete - density component , and apply a discriminative criterion to the training of the continuous density component only .The continuous pdf traininghowever , users alignment paths established on the basis of the discrete pdfs .", "label": "", "metadata": {}, "score": "38.23829"}
{"text": "FIG .2 illustrates the relationship between model states and the two sets of log - pdfs .The vector - quantized input utterances V are matched against reference models Y.sub.i by the Viterbi algorithm described in greater detail hereinafter using the discrete - pdf section of the DTMR models .", "label": "", "metadata": {}, "score": "38.434143"}
{"text": "Several other time - series models have been proposed recently especially in the segment model framework to address the inter - frame correlation problem such as Gauss - Markov and dynamical system segment models .The lack of intra - frame correlation has been compensated for with transform schemes such as semi - tied full covariance matrices ( STC ) .", "label": "", "metadata": {}, "score": "38.442436"}
{"text": "In future , it would be interesting to compare how much we would benefit by using the proposed method with a different front - end quantizer such as a phone recognizer or a prosodic feature extractor .We also expect the method to be readily applicable for spoken language recognition , and applications beyond speech technology that operate on discrete symbols , such as natural language processing ( NLP ) and bioinformatics .", "label": "", "metadata": {}, "score": "38.47991"}
{"text": "These states are assumed to be hidden and only output based on the states , i.e. speech is observed .According to the model , transitions between these states are governed by a matrix of transition probabilities .For each state there is an output function , specifically a probability density function that determines an a posteriori probability that the HMM was in the state , given measured features of an acoustic signal .", "label": "", "metadata": {}, "score": "38.621284"}
{"text": "b. comparing the sequence of representative vectors with a plurality of word model state sequences and using the continuous PDFs to score each word model state sequence for a likelihood that such state sequence represents the sequence ofrepresentative vectors ; .c. selecting the word model state sequence having the best score as a recognition result for output to a user ; .", "label": "", "metadata": {}, "score": "38.651756"}
{"text": "In order to achieve real - timerecognition , a variety of speedup techniques are usually used .In one typical approach , the vocabulary search is performed in multiple stages or passes , where each successive pass makes use of increasingly detailed and expensive models , applied to increasingly small lists of candidate models .", "label": "", "metadata": {}, "score": "38.69045"}
{"text": "IEEE Intl .Conf . on Acoustics , Speech , and Signal Processing , 2000 . \" ...In traditional speech recognition using Hidden Markov Models ( HMMs ) , each state represents an acoustic portion of a phoneme .We explore the concept of an articulator based HMM , where each state represents a particular articulatory configuration [ Erler 1996].", "label": "", "metadata": {}, "score": "38.727203"}
{"text": "In addition , we use diphone modeling which allows context dependent training of transition probabilities .Our goal is to confirm that articulatory knowledge can assist speech recognition .We demonstrate this by showing that our mapping of articulatory configurations to phonemes performs better than random mappings .", "label": "", "metadata": {}, "score": "38.89941"}
{"text": "mu.s.sub.r , k .This can be done by training a conventional Maximum Likelihood Gaussian Mixture pdf for each model state from the inpututterance frames aligned with that state using the discrete - pdf component .The total number of mean vectors can be set to reflect the variance of the data frames aligned with each state during the iterative training .", "label": "", "metadata": {}, "score": "39.01242"}
{"text": "In this paper , we attempt to systematically review the use of dynamic programming search strategies for small - vocabulary and large - vocabulary continuous speech recognition .The following methods are described in detail : search using a linear lexicon , search using a lexical tree , language - model look - ahead and word graph generation . \" ...", "label": "", "metadata": {}, "score": "39.139145"}
{"text": "This approach is particularly well - suited to hybrid connectionist / hidden Markov model systems because posterior phone probabilities are directly computed by the acoustic model .On large vocabulary tasks , using a trigram language model , this increased the search speed by an order of magnitude , with 2 % or less relative search error .", "label": "", "metadata": {}, "score": "39.381958"}
{"text": "These results motivate us to use larger UBM , and therefore larger event set , for discrete probabilities modeling in the next and subsequent sections .C. Computational Speed Up : Gaussian Selection For a large UBM ( large as compared to the dimensionality of the feature vectors ) an input vector will be close only to a few Gaussian components .", "label": "", "metadata": {}, "score": "39.463333"}
{"text": "Considerable memory savings are achieved with the combination of a tree based lexicon and a new search technique .The search proceeds time - first , that is partial path hypotheses are extended into the future in the inner loop and a tree walk over the lexicon is performed as an outer loop .", "label": "", "metadata": {}, "score": "39.471203"}
{"text": "Given the new , composite set of probabilities a new Viterbi search is performed to determine the optimal alignment and overall probability .In this method , the alignment has to be repeated , and in addition , the approximate and detailed probabilitiesmust be similar , compatible quantities .", "label": "", "metadata": {}, "score": "39.59175"}
{"text": "Such models consist of sequences of states connected by arcs , and a probability density function ( pdf ) associated with each state describesthe likelihood of observing any given spectral vector at that state .A separate set of probabilities may be provided which determine transitions between states .", "label": "", "metadata": {}, "score": "39.645767"}
{"text": "Alternatively , the process is also applied to refine the Gaussian mixtures associated with other emitting states of the Hidden Markov Model .A method of performing automatic speech recognition in a variable background noise environment , the method comprising the steps of : . processing a first portion of an inter - sentence pause to obtain a first characterization of the first portion of the inter - sentence pause ; . comparing the first characterization to a set of non - speech audio characterizations to determine a particular non - speech audio characterization among the set of non - speech audio characterizations that most closely matches the first characterization ; . generating an updated set of non - speech characterizations by updating the particular non - speech audio characterization so that the particular non - speech audio characterization more closely resembles the first characterization .", "label": "", "metadata": {}, "score": "40.29438"}
{"text": "Graphical models provide a promising paradigm to study both existing and novel techniques for automatic speech recognition .This paper first provides a brief overview of graphical models and their uses as statistical models .It is then shown that the statistical assumptions behind many pattern recog ... \" .", "label": "", "metadata": {}, "score": "40.40164"}
{"text": "Discrete probabilities are also useful in modeling prosodic feature sequences [ 11].In [ 7 , 8 ] , we investigated the use of discrete acoustic events derived using the UBM .In this paper , we show that various discrete representations mentioned above can be summarized under the maximum a posteriori ( MAP ) parameter estimation framework [ 12].", "label": "", "metadata": {}, "score": "40.8362"}
{"text": "the model comprises a hidden marcov model that includes a plurality of emitting states and multi component Gaussian mixtures that give the a posteriori probability that a given feature vector is attributable to a given emitting state ; .the detector detects the absence of speech sounds by comparing a function of one or more cepstral coefficients to a threshold : and .", "label": "", "metadata": {}, "score": "40.874466"}
{"text": "Performance benefits have been demonstrated from incorporating a linear trajectory description and additionally from modelling variability in the mid - point parameter . \" ...This paper addresses the time - series modelling of high dimensional data .Currently , the hidden Markov model ( HMM ) is the most popular and successful model especially in speech recognition .", "label": "", "metadata": {}, "score": "41.15989"}
{"text": "The purposeof the analysis is to transform the spectral frames so as to enhance the discriminability of different phonetic events .While the raw vectors are subsequently quantized for use in alignment and initial scoring , the data comprising the raw vectors ispreserved for use in more precise final scoring using continuous pdfs as described hereinafter .", "label": "", "metadata": {}, "score": "41.16183"}
{"text": "Since these filters can be shown to form the eigenvectors of arbitrary images containing both natural and man - made structures , they are well - suited for indexing in disparate domains .The indexing algorithm uses an active vision system in conjunction with a modified form of Kanerva 's sparse distributed memory which facilitates interpolation between views and provides a convenient platform for learning the association between an object 's appearance and its identity .", "label": "", "metadata": {}, "score": "41.287247"}
{"text": "Since these filters can be shown to form the eigenvectors of arbitrary images containing both natural and man - made structures , they are well - suited for indexing in disparate domains .The indexing algorithm uses an active vision system in conjunction with a modified form of Kanerva 's sparse distributed memory which facilitates interpolation between views and provides a convenient platform for learning the association between an object 's appearance and its identity .", "label": "", "metadata": {}, "score": "41.287247"}
{"text": "We investigate the issues that are involved in trade - offs between trajectory and mixture modeling in segment - based word re ... \" .This paper presents a mechanism for implementing mixtures at a phone - subsegment ( microsegment ) level for continuous word recognition based on the Stochastic Segment Model ( SSM ) .", "label": "", "metadata": {}, "score": "41.802277"}
{"text": "Recall that the Fisher mapping in ( 12 ) was obtained by taking the derivative of the log - likelihood function with respect to the weights of the UBM .In addition to the weights , taking the derivative with respect to the mean vectors and covariance matrices , as originally proposed in [ 18 ] , increases the dimensionality of the supervector .", "label": "", "metadata": {}, "score": "41.819"}
{"text": "The next step consists of the discriminative training of the mean vectors .This is accomplished by defining an appropriate training objective function which reflects recognition error - rate and optimizing the mean parameters so as to minimizethis function .One common technique applicable to the minimization of the objective function is gradient descent optimization .", "label": "", "metadata": {}, "score": "41.8235"}
{"text": "c ) applying a window function to successive subsets of the filtered sequence of samples to obtain a sequence of frames of windowed filtered samples ; .d ) transforming each of the frames of windowed filtered samples to a frequency domain to obtain a plurality of frequency components ; .", "label": "", "metadata": {}, "score": "41.990776"}
{"text": "c ) applying a window function to successive subsets of the filtered sequence of samples to obtain a sequence of frames of windowed filtered samples ; .d ) transforming each of the frames of windowed filtered samples to a frequency domain to obtain a plurality of frequency components ; .", "label": "", "metadata": {}, "score": "41.990776"}
{"text": "These results suggest that the Bhattacharyya measure is a strong candidate for measuring the similarity between discrete distributions with SVM classifier .The proposed performance to the state - of - the - art GMM supervector approach .Their fusion gave 5.35 % relative improvement in EER , even though the improvement in MinDCF was marginal .", "label": "", "metadata": {}, "score": "42.008453"}
{"text": "Similar problem happens for the Fisher kernel and TFLLR scaling .This has profound impact on the performance of the SVM , as shown in the next section .i \u03c9 ? , when treated independently , follows a beta iB have V. EXPERIMENTS AND RESULTS A. Experimental Setup The experiments were carried out on the NIST 2006 speaker recognition evaluation ( SRE ) task [ 23].", "label": "", "metadata": {}, "score": "42.054394"}
{"text": "Another approach speeds up the evaluation of Gaussian - mixture models by exploiting a geometric approximation of the computation .However , even with speedups the evaluation can be slow enough that only a small number can be carried out .In another scheme , approximate models are first used to compute the state probabilities given the input speech .", "label": "", "metadata": {}, "score": "42.08772"}
{"text": "Experimental results are reported on DARPA 's speakerindependent Resource Management corpus .INTRODUCTION In earlier work , the Stochastic Segment Model ( SSM ) [ 1 , 2 ] has been shown to be a viable alternative to the Hidden Markov Model ( HMM ) for representing variable - duration phones .", "label": "", "metadata": {}, "score": "42.180862"}
{"text": "i .D .Y .i .P . ijn . ) ij .where , \u03bc ijn is a mean of an ith parameter ( corresponding to an ith elements of the feature vectors ) , of the nth mixture component of the jth acoustic state 132 ( for a phoneme or for background sounds ) of the HMM model .", "label": "", "metadata": {}, "score": "42.20307"}
{"text": "Transitions between the states in each phoneme model are also governed by a transition probability matrix .The acoustic layer also includes an HMM model 156 for the absence of speech sounds that occur between speech sounds ( e.g. , between words , and between sentences ) .", "label": "", "metadata": {}, "score": "42.223164"}
{"text": "The fundamental difference between connectionist systems and more conventional mixture - of - Gaussian systems is that connectionist models directly estimate posterior probabilities as opposed to likelihoods .Access to post ... \" .This paper describes connectionist techniques for recognition of Broadcast News .", "label": "", "metadata": {}, "score": "42.33025"}
{"text": "Classifying variable - length speech sequences is thereby translated into a simpler task of classifying the supervectors .For instance , in [ 3 ] speech vectors are mapped to a high - dimensional space via time - averaged polynomial expansion .", "label": "", "metadata": {}, "score": "42.36171"}
{"text": "The use of cross word context dependent models presents problems for conventional decoders .The second part of the thesis therefore presents a new decoder design which is capable of using these models efficiently .The decoder is suitable for use with very large vocabularies and long span language models .", "label": "", "metadata": {}, "score": "42.399223"}
{"text": "In addition , continuous pdf models are computationally far more expensive than discretepdf models , since following vector quantization the computation of a discrete probability involves no more than a single table lookup .The probability values in the discrete pdf case and the parameter values of the continuous pdf are most commonly trained using the Maximum Likelihood method .", "label": "", "metadata": {}, "score": "42.45763"}
{"text": "The performance of the proposed method is close to that of the popular GMM supervector with a small margin .Full - text .Abstract - Support vector machines ( SVMs ) , and kernel classifiers in general , rely on the kernel functions to measure the pairwise similarity between inputs .", "label": "", "metadata": {}, "score": "42.715015"}
{"text": "The first compares the merging algorithm with the standard Baum - Welch approach in inducing simple finitestate languages from small , positive - only training samples .We found that the merging procedure is more robust and accurate , part ... . \" ...", "label": "", "metadata": {}, "score": "42.78534"}
{"text": "M .c .j .n .b .j .n .Y .P . ) where , b j ( Y P ) is the a posteriori probability that the HMM model 132 was in a jth state during frame P given the fact that the audio signals during frame P was characterized by a feature vector Y P ; .", "label": "", "metadata": {}, "score": "43.03257"}
{"text": "The stack maintains information about groups of hypotheses and whole groups are extended by one word to form new stack entries .An implementation is described of a one - pass decoder employing a 65,000 word lexicon and a disk - based trigram language model .", "label": "", "metadata": {}, "score": "43.070133"}
{"text": "The continuous pdfs are parametric models and thus the probabilities can not be precomputed .Rather than storing pre - computed probabilities as is the case for the discrete pdfs , we store the pdf parameters themselves and use them to compute thelog - probabilities for specific input frames .", "label": "", "metadata": {}, "score": "43.1095"}
{"text": "These mixtures are designed to be combined with observed amino acid frequencies , to form estimates of expected amino acid probabilities at each position in a profile , hidden Markov model , or other statistical model .These estimates give a statistical model greater generalization capacity , such that remotely related family members can be more reliably recognized by the model .", "label": "", "metadata": {}, "score": "43.237003"}
{"text": "These mixtures are designed to be combined with observed amino acid frequencies , to form estimates of expected amino acid probabilities at each position in a profile , hidden Markov model , or other statistical model .These estimates give a statistical model greater generalization capacity , such that remotely related family members can be more reliably recognized by the model .", "label": "", "metadata": {}, "score": "43.237003"}
{"text": "In practice , the Fisher information estimated by replacing the expectation with sample average computed from a large background corpus .The Fisher information normalizes individual dimensions of the supervector to the same scale corresponding to the mean- square value of the discrete probabilities estimated from the background samples .", "label": "", "metadata": {}, "score": "43.342525"}
{"text": "A discrete - pdf system is used to establish alignment paths of an input utterance and a reference model , while the final probability metric is obtained by post - processing frame - state pairs with more powerful , discriminatively trained continuous - density pdfs , but using the same alignment path .", "label": "", "metadata": {}, "score": "43.421616"}
{"text": "This is especially advantageous for users that are frequently engaged in other critical activities ( e.g. , driving ) while operating their wireless phones .The most widely used algorithms for performing automated speech recognition ( ASR ) are based on Hidden Markov Models ( HMM ) .", "label": "", "metadata": {}, "score": "43.438972"}
{"text": "After the HMM is built , it is used to obtain a multiple alignment of all the training sequences .It is also used to search the .SWISS - PROT 22 database for other sequences .that are members of the given protein family , or contain the given domain .", "label": "", "metadata": {}, "score": "43.440334"}
{"text": "After the HMM is built , it is used to obtain a multiple alignment of all the training sequences .It is also used to search the .SWISS - PROT 22 database for other sequences .that are members of the given protein family , or contain the given domain .", "label": "", "metadata": {}, "score": "43.440334"}
{"text": "We present a method for condensing the information in a protein dat ... \" .This paper presents the mathematical foundations of Dirichlet mixtures , which have been used to improve database search results for homologous sequences , when a variable number of sequences from a protein family or domain are known .", "label": "", "metadata": {}, "score": "43.472855"}
{"text": "We present a method for condensing the information in a protein dat ... \" .This paper presents the mathematical foundations of Dirichlet mixtures , which have been used to improve database search results for homologous sequences , when a variable number of sequences from a protein family or domain are known .", "label": "", "metadata": {}, "score": "43.472855"}
{"text": "The means \u03bc ijn serve as reference characterizations of a sound modeled by the a posteriori probability .In the operation a seach engine 164 searches the HMM 132 , for one or more sequences of states that are characterized by high probabilities , and outputs one or more sequences of words that correspond to the high probability sequences of states .", "label": "", "metadata": {}, "score": "43.474888"}
{"text": "The model has a statistical interpretation ... \" .We consider problems of sequence processing and propose a solution based on a discrete state model in order to represent past context .Weintroduce a recurrent connectionist architecture having a modular structure that associates a subnetwork to each state .", "label": "", "metadata": {}, "score": "43.492218"}
{"text": "A text - based and spoken language processing framework based on the Constraint Dependency Grammar ( CDG ) developed by Maruyama [ 24 , 25 ] is discussed .The scope of CDG is expanded to allow for the analysis of sentences containing lexically ambiguous words , to allow feature analysis in constraints , and to efficiently process multiple sentence candidates that are likely to arise in spoken language processing .", "label": "", "metadata": {}, "score": "43.53354"}
{"text": "We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters .These networks are first pre - trained as a multi - layer generative model of a window of spectral feature vectors without making use of any discriminative information .", "label": "", "metadata": {}, "score": "43.744766"}
{"text": "The supervector is then formed by concatenating the mean vectors of the adapted GMM .In [ 5 ] , supervectors are formed by stacking the likelihood scores with respect to a cohort of anchor models on a per - utterance basis .", "label": "", "metadata": {}, "score": "44.01687"}
{"text": "A method for a speech recognition system to convert an input utterance into a representative word sequence text , the method comprising : .a. converting the input utterance into a sequence of representative vectors ; .b. quantizing the sequence of representative vectors into a sequence of standard prototype vectors ; .", "label": "", "metadata": {}, "score": "44.14585"}
{"text": "The continuous density pdfs used in this work are a simplified form of Gaussian Mixtures .Experimental evidence revealed that with the use of discriminative training there was no advantage to using the full mixture models over the simplifiedversion .In addition , reducing the number of free parameters in the model significantly improves their trainability with limited quantities of data .", "label": "", "metadata": {}, "score": "44.162033"}
{"text": "Spoken Language Processing , 2006 .[ 6 ] C. Bahlmann and H. Burkhardt , \" Measuring HMM Similarity with the Bayes Probability of Error and Its Application to Online Handwriting Recognition , \" Proc .Sixth Int'l Conf .Document Analysis and Recognition , 2001 .", "label": "", "metadata": {}, "score": "44.223156"}
{"text": "The discriminant function of an SVM [ 19 ] can be expressed in terms of the supervector as follows .Table II shows the results with and without feature normalization , channel compensation and score normalization .Notably , 15.40 % relative improvement in EER and 12.31 % relative improvement in MinDCF are obtained by applying the Bhattacharyya measure on the raw discrete probabilities .", "label": "", "metadata": {}, "score": "44.588295"}
{"text": "However , difficulties in segmentbased recognition have impeded the realization of potential advantages in modeling .This thesis . \" ...Currently the most popular acoustic model for speech recognition is the hidden Markov model ( HMM ) .However , HMMs are based on a series of assumptions some of which are known to be poor .", "label": "", "metadata": {}, "score": "44.62841"}
{"text": "Because today 's state - of - the - art recognizers are not designed to be situated naturally in an error feedback loop , they are ill - positioned for inclusion in multi- ... \" .This thesis is about modeling , analyzing , and predicting errorful behavior in large vocabulary continuous speech recognition systems .", "label": "", "metadata": {}, "score": "44.699024"}
{"text": "The MAP estimate of discrete probabilities in ( 5 ) is given by the sum of the observed statistics of the prior distribution .In ( 2 ) , speech feature vectors are quantized as discrete events or symbols on a frame - by- frame basis .", "label": "", "metadata": {}, "score": "44.84739"}
{"text": "An object is represented by a set of high - dimensional iconic feature vectors com ... \" .A general - purpose object indexing technique is described that combines the virtues of principal component analysis with the favorable matching properties of high - dimensional spaces to achieve high precision recognition .", "label": "", "metadata": {}, "score": "44.91342"}
{"text": "An object is represented by a set of high - dimensional iconic feature vectors com ... \" .A general - purpose object indexing technique is described that combines the virtues of principal component analysis with the favorable matching properties of high - dimensional spaces to achieve high precision recognition .", "label": "", "metadata": {}, "score": "44.91342"}
{"text": "The transformed acoustic frames are vector quantized with a codebook of 1024 standard vector prototypes and each original spectral frame x.sub.t ( omitting the subscript u ) is assigned a corresponding vector quantizer ( VQ ) label v.sub.t .sub.i ) , where M.sub.i is the length of a model and i is the model index .", "label": "", "metadata": {}, "score": "44.916138"}
{"text": "The probabilities of the components in the shortlist of the top- scoring component are then computed .Let H be the size of the hash model and Q be the length of the shortlists .Gaussian selection results in M/(H + Q ) times faster computation , where M is the order of the UBM .", "label": "", "metadata": {}, "score": "44.91968"}
{"text": "This chapter describes a use of recurrent neural networks ( i.e. , feedback is incorporated in the computation ) as an acoustic model for continuous speech recognition .The form of the recurrent neural network is described along with an appropriate parameter estimation procedure .", "label": "", "metadata": {}, "score": "44.98378"}
{"text": "This chapter describes a use of recurrent neural networks ( i.e. , feedback is incorporated in the computation ) as an acoustic model for continuous speech recognition .The form of the recurrent neural network is described along with an appropriate parameter estimation procedure .", "label": "", "metadata": {}, "score": "44.98378"}
{"text": "Furthermore , since the goal of these systems is re ... . \" ...The search problem in large vocabulary continuous speech recognition ( LVCSR ) is to locate the most probable string of words for a spoken utterance given the acoustic signal and a set of sentence models .", "label": "", "metadata": {}, "score": "45.122253"}
{"text": "Eachlist is sorted by the score Di , and an augmented alignment path structure is retained for each reference model in the list .The additional stored path information is as follows : . b.sub.i is used to store the index of the best mean vector at a particular path point .", "label": "", "metadata": {}, "score": "45.25231"}
{"text": "The Bhattacharyya measure performs consistently better than the rank normalization in terms of EER and MinDCF .The effectiveness of the rank normalization depends on the extent the supervectors matches the background distribution .It is also possible to use bigram ( i.e. , subsequences of two Gaussian indexes ) probabilities to construct supervectors and to compare the performance of various normalization methods .", "label": "", "metadata": {}, "score": "45.257317"}
{"text": "Abstract - We propose a novel context - dependent ( CD ) model for large vocabulary speech recognition ( LVSR ) that leverages recent advances in using deep belief networks for phone recognition .We describe a pre - trained deep neural network hidden Markov model ( DNN - HMM ) hybrid architecture that trains the ... \" .", "label": "", "metadata": {}, "score": "45.354168"}
{"text": "The proposed method benefits from the use of a continuous density model and a discriminative training criterionwhich leads to a high recognition performance on a large vocabulary task at the cost of only a marginal increase of computation over a simple discrete pdf system .", "label": "", "metadata": {}, "score": "45.40564"}
{"text": "Evaluation of our maxent model on a simple problem cuts an already - low error rate in half compared to an equivalent HMM with the same number of parameters . ... ry itself may be non - parametric [ 17 ] , [ 16 ] , [ 18 ] or a low - order polynomial [ 19 ] , [ 20 ] , [ 23 ] , [ 15 ] , with constant and linear trajectories having received the most attention .", "label": "", "metadata": {}, "score": "45.41617"}
{"text": "This paper corrects a previously p .. ation in the databases into the form of a mixture of densities .These densities assign a probability to every possible distribution of the amino acids .Often , these densities capture some prototypical distribution ... . by Randall C. O'Reilly , Jerry W. Rudy - PSYCHOLOGICAL REVIEW , 2001 . \" ...", "label": "", "metadata": {}, "score": "45.449886"}
{"text": "This paper corrects a previously p .. ation in the databases into the form of a mixture of densities .These densities assign a probability to every possible distribution of the amino acids .Often , these densities capture some prototypical distribution ... . by Randall C. O'Reilly , Jerry W. Rudy - PSYCHOLOGICAL REVIEW , 2001 . \" ...", "label": "", "metadata": {}, "score": "45.449886"}
{"text": "We analyze the effectiveness of the Bhattacharyya measure from the perspective of feature normalization and distribution warping in the SVM feature space .Experiments conducted on the NIST 2006 speaker verification task indicate that the Bhattacharyya measure outperforms the Fisher kernel , term frequency log - likelihood ratio ( TFLLR ) scaling , and rank normalization reported earlier in literature .", "label": "", "metadata": {}, "score": "45.524414"}
{"text": "The verification process typically consists of extracting a sequence of short - term spectral vectors from the given speech signal , matching the sequence of vectors against the claimed speaker 's model , and finally comparing the matched score against a verification threshold .", "label": "", "metadata": {}, "score": "45.721664"}
{"text": "For each raw vector the system identifies that one of a preselected plurality of quantized vectors whichbest matches the raw vector .The raw vector information is , however , retained for subsequent utilization .Each word model is represented by a sequence of states , the states being selected from a preselected group of states .", "label": "", "metadata": {}, "score": "45.72367"}
{"text": "The new score for a path corresponding to an alignment of input utterance with reference model i is obtained as # # EQU3 # # .The rescored models are then re - sorted according to their new scores .It is , however , not called upon to provide fine discrimination between highly confusable models .", "label": "", "metadata": {}, "score": "45.946365"}
{"text": "SIGMA . ) denotes the probability of observing x(t ) given a multivariate Gaussian with mean . mu . and covariance .SIGMA . N(s.sub.r ) is the number of mixturecomponents .The discriminatively trained continuous density log - pdf ( CDLP ) used in this work is as follows : # # EQU2 # # .", "label": "", "metadata": {}, "score": "46.00103"}
{"text": "There are two major approaches : the discretepdf and the continuous pdf .In the former , the spectral vectors corresponding to the input speech are first quantized with a vector quantizer which assigns each input frame an index corresponding to the closest vector from a codebook of prototypes .", "label": "", "metadata": {}, "score": "46.135532"}
{"text": "For this task , our single - pass decodertook around 15\u00d7 realtime on an HP73 ... . ... ity estimates at each frame without much additional computational cost .In this paper we describe a new search algorithm that we have developed .", "label": "", "metadata": {}, "score": "46.163044"}
{"text": "We use this technique to learn the \" space of lips \" in a visual speech recognition task .The learned manifold is used for tracking and extracting the lips , for interpolating between frames in an image sequence and for providing features for recognition .", "label": "", "metadata": {}, "score": "46.174427"}
{"text": "i. automatically performing an adjustment to the descriptive parameters modified in step ( f ) that substantially undoes the discriminative adjustment performed in step ( f ) , and . ii . automatically performing a discriminative adjustment to the descriptive parameters of the word model state sequences for the words in the user corrected word sequence and the descriptive parameters of at least one other word model statesequence .", "label": "", "metadata": {}, "score": "46.505417"}
{"text": "Access to posterior probabilities has enabled us to develop a number of novel approaches to confidence estimation , pronunciation modelling and search .In addition we have investigated a new feature extraction technique based on the modulation - filtered spectrogram ( MSG ) , and methods for combining multiple information sources .", "label": "", "metadata": {}, "score": "46.613823"}
{"text": "The first approach is known as the Forward - Backward algorithm , and usesan efficient recursion to compute the match probability as the sum of the probabilities of all possible alignments of the input sequence and the model states permitted by the model topology .", "label": "", "metadata": {}, "score": "46.643707"}
{"text": "Hence , it can be used as a legitimate kernel function [ 19 ] in SVM .From feature normalization perspective , the square - root operator has an effect in normalizing the contribution of individual dimensions to the inner product .", "label": "", "metadata": {}, "score": "46.723206"}
{"text": "15 , pp .69 - 90 , 2001 .[ 8 ] A. Vinciarelli , S. Bengio , and H. Bunke , \" Offline Recognition of Unconstrained Handwritten Texts Using HMMs and Statistical Language Models , \" IEEE Trans .Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "46.855423"}
{"text": "On Speaker - Independent , Speaker - Dependent , and Spekaer - Adaptive Speech Recognition .IEEE Transactions on Speech and Audio Processing , Apr. 1993 . Y. Zhao , \" A Speaker - Independent Continuous Speech Recognition System Using Continuous Mixture Gaussian Density HMM Of Phoneme - Sized Units\"--IEEE Transactions On Speech And Audio Processing , vol . 1 , No . 3 , Jul. 1993 , pp .", "label": "", "metadata": {}, "score": "46.97769"}
{"text": "[ Eq .( 22 ) ] MAP estimation [ Eq .( 1 ) , ( 5 ) ] UBM [ Eq .( 5 ) , ( 10 ) ] and stacked to form a supervector [ Eq .( 17 ) ] Speaker model Number of occurrences of discrete events Speech utteranceRaw supervector Normalized supervector Normalized and compensated supervector .", "label": "", "metadata": {}, "score": "47.11599"}
{"text": "We introduce the MAP framework for the estimation of discrete probabilities in Section II .Using the UBM as a soft quantizer , we describe the process of constructing supervectors using discrete probabilities and show its relevance to the Fisher kernel in Section III .", "label": "", "metadata": {}, "score": "47.246"}
{"text": "The best alignment path is recovered by using the predecessor arrays Pred.sub.t(t , m ) and Pred.sub.m ( t , m ) in the following backtracking recursion : . gtoreq . 1 and m . gtoreq .The original acoustic vector at a particular path point p can thus be identified as x(f.sub.i , p ) while the state index at path position p is directly given by q.sub.i,p . .", "label": "", "metadata": {}, "score": "47.3211"}
{"text": "F. Comparison and Fusion of Supervectors Finally , we evaluate the performance of the supervector of discrete probabilities ( with the Bhattacharyya measure ) in comparison with the GLDS kernel [ 3 ] and GMM supervector [ 4].For the GLDS kernel , we used all monomials up to the third order .", "label": "", "metadata": {}, "score": "47.36573"}
{"text": "The gradient is averaged over all utterances and correct - incorrect pairs : # # EQU9 # # . where N.sub .C , I , u is the number of correct - incorrect model pairs for utterance u. The mean components are modified by the addition of the scaled gradient : . where w is a weight which determines the magnitude of the change to the parameter set in one iteration .", "label": "", "metadata": {}, "score": "47.429966"}
{"text": "INTRODUCTION Hidden Markov Models ( HMMs ) are a popular approach for speech recognition .Commonly , a left - to - r ... . by Christoph Bregler , Stephen M. Omohundro - Proceedings of the Fifth International Conference on Computer Vision , 1995 . \" ...", "label": "", "metadata": {}, "score": "47.439957"}
{"text": "[ 30 ] J.A. Rodr\u00edguez - Serrano , F. Perronnin , J. Llad\u00f3s , and G. S\u00e1nchez , \" A Similarity Measure between Vector Sequences with Application to Handwritten Word Image Retrieval , \" Proc .IEEE Conf .Computer Vision and Pattern Recognition , 2009 .", "label": "", "metadata": {}, "score": "47.704346"}
{"text": "We demonstrate that IOHMMs are well suited for solving grammatical inference problems on a benchmark problem .Experimental results are presented for the seven Tomita grammars , showing that these adaptive models can attain excellent generalization . \" ...This report describes a new technique for inducing the structure of Hidden Markov Models from data which is based on the general ' model merging ' strategy ( Omohundro 1992 ) .", "label": "", "metadata": {}, "score": "47.70477"}
{"text": "Accepted for IEEE T - ASL 2 of discrete distribution is simple and any arbitrarily shaped distribution is possible since it is non - parametric .Another challenge concerning the use of supervectors with SVM is feature normalization - the process where the elements of a feature vector are scaled or warped prior to SVM modeling .", "label": "", "metadata": {}, "score": "47.78926"}
{"text": "a microprocessor coupled to the analog to digital converter for receiving the discretized audio signal and executing a program for performing automated speech recognition , the program comprising programming instructions for : . detecting an inter - sentence pause of an audio signal ; . processing a first portion of the inter - sentence pause to obtain a first characterization of the first portion of the inter - sentence pause ; . comparing the first characterization to a set of non - speech audio characterization to determine a particular non - speech audio characterization among the set of non - speech audio characterizations that most closely matches the first characterization ; and .", "label": "", "metadata": {}, "score": "47.8854"}
{"text": "For the score normalization [ 27 ] , t - norm cohorts were selected from NIST 2005 SRE dataset .We use the same configuration for subsequent experiments .The overall process from supervector construction to SVM training is summarized and illustrated in Fig .", "label": "", "metadata": {}, "score": "48.1055"}
{"text": "FIG .3 is a preferred form of block 202 of FIG .2 .In process block 302 for each successive increment of time ( frame ) a feature vector that characterizes an audio signal is extracted .In process block 304 for each successive increment of time , the feature vector is used to evaluate Gaussian mixtures that give the a posteriori probabilities that various states of the HMM result in audio signal characterized by the feature vector .", "label": "", "metadata": {}, "score": "48.173813"}
{"text": "Another weakness of current acoustic models is that they do not account for the origin of derived features ( estimated derivatives ) .We show how to both remove the independence assumption and properly account for derived features , with little or no increase in the number of parameters to train , by applying the principle of maximum entropy .", "label": "", "metadata": {}, "score": "48.18614"}
{"text": "In this paper , we conduct a comparative study between GMM - SVM with adaptive relevance factor and JFA / i - vector under the framework of Speaker Recognition Evaluation ( SRE ) formulated by the National Institute of Standards and Technology ( NIST ) .", "label": "", "metadata": {}, "score": "48.31225"}
{"text": "The parameter . beta . controls the amount of influence \" near - errors \" will have on the training .The score D.sub.i between the training utterance and the target model i is obtained by rescoring the alignment path as shown in equation ( 6 ) .", "label": "", "metadata": {}, "score": "48.41291"}
{"text": "We anticipate that , had we chosen such a completely different front - end , we would likely observe higher fusion gain .This is a point for future research .VI .CONCLUSIONS Speech signals can be represented in terms of the probability distribution of acoustic , idiolect , phonotactic or some high - level discrete events .", "label": "", "metadata": {}, "score": "48.452633"}
{"text": "Define Sum(t , m ) as the accumulated negative log - probabilities .The alignment algorithm used in this work can then be summarized as follows .nu .This basic recursion is also illustrated in FIG .3 of the drawings .", "label": "", "metadata": {}, "score": "48.516655"}
{"text": "When employed in discrimination tests ( by examining how closely the sequences in a database fit the globin , kinase and EF - hand HMMs ) , the ' \\ HMM is able to distinguish members of these families from non - members with a high degree of accuracy .", "label": "", "metadata": {}, "score": "48.5755"}
{"text": "When employed in discrimination tests ( by examining how closely the sequences in a database fit the globin , kinase and EF - hand HMMs ) , the ' \\ HMM is able to distinguish members of these families from non - members with a high degree of accuracy .", "label": "", "metadata": {}, "score": "48.5755"}
{"text": "A computer readable medium storing programming instructions for performing automatic speech recognition in a variable background noise environment , including programming instructions for : . detecting a plurality of inter - sentence pauses of an audio signal ; . processing a first portion of a first inter - sentence pause to obtain a first characterization of the first portion of the first inter - sentence pause ; . comparing the first characterization to a set of non - speech audio characterizations to determine a particular non - speech audio characterization among the set of non - speech audio characterizations that most closely matches the first characterization ; and .", "label": "", "metadata": {}, "score": "48.636242"}
{"text": "Once the characteristic feature vector has been obtained , a mean vector , from among a plurality mean vectors of one or more emitting states of the background sound model , that is closest to the characteristic feature vector is determined .", "label": "", "metadata": {}, "score": "48.745487"}
{"text": "We further proposed and analyzed the use of Bhattacharyya coefficient as the similarity measure between supervectors constructed from the discrete probabilities .From the perspective of feature normalization in the supervector space , the Bhattacharyya measure warps the distribution of each dimension with a square - root function , a much simpler and data - independent operation , yet leading to higher accuracy compared to the Fisher kernel , TFLLR scaling , and rank normalization .", "label": "", "metadata": {}, "score": "48.773582"}
{"text": "A stored table is provided which contains distance metric values for each combination of aquantized input vector with model state as characterized by the discrete pdfs .Word models are aligned with an input utterance using the respective discrete PDFs and initial match scores are generated using the stored table .", "label": "", "metadata": {}, "score": "48.789497"}
{"text": "This is different from the normalization scheme in the Fisher kernel , where constant scaling is applied to individual dimension based on the Fisher information estimated from a background corpus . A. Term Frequency Log - Likelihood Ratio ( TFLLR ) Term frequency log - likelihood ratio ( TFLLR ) was introduced in [ 10 ] for the scaling of n - gram probabilities .", "label": "", "metadata": {}, "score": "48.809006"}
{"text": "Discrete events arise naturally in modeling many types of data , for example , letters , words , and DNA sequences .Speech signals can also be represented as sequences of discrete symbols by using a quantizer .Notably , high - level feature extraction ( e.g. , idiolect , phonotactic , prosody ) usually produces discrete symbols .", "label": "", "metadata": {}, "score": "48.936985"}
{"text": "The search problem in large vocabulary continuous speech recognition ( LVCSR ) is to locate the most probable string of words for a spoken utterance given the acoustic signal and a set of sentence models .Searching the space of possible utterances is difficult because of the large vocabulary size and the complexity imposed when long - span language models are used .", "label": "", "metadata": {}, "score": "48.95965"}
{"text": "[19 ] V. Kecman , Learning and Soft Computing : Support Vector Machines , Neural Networks , and Fuzzy Logic Models .MA : MIT Press , 2001 .[20 ] \u00c1. de la Torre , A. M. Peinado , J. C. Segura , J. L. P\u00e9rez - C\u00e9rdoba , M. C. Ben\u00edtez , and A. J. Rubio , \" Histogram equalization of speech representation for robust speech recognition , \" IEEE Trans .", "label": "", "metadata": {}, "score": "49.153328"}
{"text": "In practice , however , the delayed correction has proved to be as effective as supervised adaptation , i.e. where the correct answer is identified before gradient computation .The delayed correction algorithm is as follows : . mu .. sub.u-1 Identify subsets I.sub.top - choices ( assumed incorrect models ) and C.sub.top - choice ( assumed correct models ) .", "label": "", "metadata": {}, "score": "49.239212"}
{"text": "Expressed mathematically the probability of a sequence of states S 1 . . .T given the fact that a sequence of feature vectors Y 1 . . .T was extracted from the audio signal is given by : . P .", "label": "", "metadata": {}, "score": "49.388885"}
{"text": "T .Y . . . .T . s .b .s .Y .t .T . a .S . t .S . t .b .s .t .Y .t . ) where \u0398 specifies the underlying HMM model ; .", "label": "", "metadata": {}, "score": "49.430126"}
{"text": "ML estimate is used when we have in X for , M ... represent the counts III .CONSTRUCTING SUPERVECTOR USING DISCRETE PROBABILITIES A. UBM as Soft Quantizer A universal background model , or UBM , is a GMM trained to represent a speaker - independent distribution [ 17].", "label": "", "metadata": {}, "score": "49.491985"}
{"text": "e. if the user corrects the recognition result by selecting a different word sequence , .i. automatically performing an adjustment to the descriptive parameters modified in step ( d ) that substantially undoes the discriminative adjustment performed in step ( d ) , and . ii . automatically performing a discriminative adjustment to the descriptive parameters of the word model state sequences for the words in the user corrected word sequence and the descriptive parameters of at least one other word model statesequence .", "label": "", "metadata": {}, "score": "49.595882"}
{"text": "Error feedback enables the construction of statistical models that map measurements of the recognizer 's internal states and behaviors to externally de ned error conditions . ...e major differences occur .All of the experimental techniques described in this thesis have been designed to be portable to all such systems .", "label": "", "metadata": {}, "score": "49.654243"}
{"text": "In recent decades , we have made tremendous progress in spoken language recognition , which benefited from technological breakthroughs in related areas , such as signal processing , pattern recognition , cognitive science , and machine learning .In this paper , we attempt to provide an introductory tutorial on the fundamentals of the theory and the state - of - the - art solutions , from both phonological and computational aspects .", "label": "", "metadata": {}, "score": "50.18328"}
{"text": "Index Terms - Speech recognition , deep belief network , context - dependent phone , LVSR , DNN - HMM , ANN - HMM I. . ... estimated from the training set , and p(xt ) is independent of the word sequence and thus can be ignored .", "label": "", "metadata": {}, "score": "50.334873"}
{"text": "In this regard , the event set S consists of all unique n - grams .The TFLLR scales individual dimensions of the supervector ( i.e. , the n - gram probabilities ) in proportion to the square root of the inverse n - gram probabilities computed from a large background corpus .", "label": "", "metadata": {}, "score": "50.428062"}
{"text": "3 is a high level flow chart of a process of performing automated speech recognition using a Hidden Markov Model .FIG .4 is a first part of flow chart of a process for extracting feature vectors from an audio signal according to the preferred embodiment of the invention .", "label": "", "metadata": {}, "score": "50.501698"}
{"text": "Preferably , DCT components and the first two orders of differences are used in the feature vectors .where the first k vector elements are DCT components , and the ( k+1)th through Dth vector elements are discrete differences of the DCT components .", "label": "", "metadata": {}, "score": "50.576893"}
{"text": "An automated speech recognition system comprising : . an audio signal input for inputting an audio signal that includes speech and background sounds ; . a feature extractor coupled to the audio signal input for receiving the audio signal and outputting characterizations of a sequence of segments of the audio signal ; . a model coupled to the feature extractor , wherein the model includes a plurality of states to which characterization of the sequence of segments are applied for evaluating a posteriori probabilities that one or more of the plurality of states occurred ; . a search engine coupled to model for finding one or more high probability sequences of the plurality of states of the model ; . a detector for detecting an absence of speech sounds of the audio signal and outputting a predetermined signal when the absence of speech sounds is detected ; and .", "label": "", "metadata": {}, "score": "50.68453"}
{"text": "Since the Gaussian components may have different covariance matrices , their average is used in computing the Mahalanobis distance .A shortlist is then generated for each component of the hash model .The shortlist contains indices of those components of the UBM having the closest distance to a particular component of the hash model .", "label": "", "metadata": {}, "score": "50.778748"}
{"text": "For example if an a posterior probability of the form shown above is used then the mixture component weights , the means \u03bc ijn and the variances \u03c3 ijn that characterize background sound must be set during training .As discussed in the background section characteristics of the background sound are not fixed .", "label": "", "metadata": {}, "score": "50.883926"}
{"text": "2108 - 2120 , Nov. 2012 , doi:10.1109/TPAMI.2012.25 .[ 2 ] Q. Huo and W. Li , \" A DTW - Based Dissimilarity Measure for Left - to - Right Hidden Markov Models and Its Application to Word Confusability Analysis , \" Proc .", "label": "", "metadata": {}, "score": "51.06434"}
{"text": "When the background sound in use differs from that present during training , the HMM ASR is more likely to make errors .According to the present invention a model used in the ASR , preferably the model of non - speech background sounds is updated frequently while the ASR is in regular use .", "label": "", "metadata": {}, "score": "51.11126"}
{"text": "There are two reasons why this has occurred : First , the dynamic programming strategy can be combined with avery efficient and practical pruning strategy so that very large search spaces can be handled .Second , the dynamic programming strategy has turned out to be extremely flexible in adapting to new requirements .", "label": "", "metadata": {}, "score": "51.11515"}
{"text": "4 .The speech utterance , X , is mapped to a supervector of discrete probabilities p , normalized in accordance with the Bhattacharya measure , and channel compensated prior to SVM modeling .Similar mapping operation is performed on the training utterance of the target speaker , all the utterances in the background corpus ( as indicated by the dotted lines ) , and test utterances ( not shown in the figure ) .", "label": "", "metadata": {}, "score": "51.186916"}
{"text": "Various methods are know to persons of ordinary skill in the ASR art for finding a likely sequence of states without having to exhaustively evaluate the above equation for each possible sequence of states .One known method is the Viterbi search method .", "label": "", "metadata": {}, "score": "51.29307"}
{"text": "Tools . by Yoshua Bengio , Paolo Frasconi - IEEE Transactions on Neural Networks , 1996 . \" ...We consider problems of sequence processing and propose a solution based on a discrete state model in order to represent past context .", "label": "", "metadata": {}, "score": "51.392937"}
{"text": "( 18 )For discrete probabilities derived from the UBM quantizer , the background probabilities weights of the UBM since the weights are estimated from a large background corpus .The TFLLR de - emphasizes frequent events and emphasizes rare events .", "label": "", "metadata": {}, "score": "51.476616"}
{"text": "More sophisticated versions reflect the fact that contextual effects can cause large variations in the way different phones are realized .Such models are known as allophonic or context - dependent .A common approach is to initiate the search with relatively inexpensive context - independent models and re - evaluate a small number of promising candidates with context - dependent phonetic models .", "label": "", "metadata": {}, "score": "51.53472"}
{"text": "Experiments are carried out on a handwritten word retrieval task in three different datasets - an in - house dataset of real handwritten letters , the George Washington dataset , and the IFN / ENIT dataset of Arabic handwritten words .These experiments show that the proposed similarity outperforms the traditional DTW between the original sequences , and the model - based approach which uses ordinary continuous HMMs .", "label": "", "metadata": {}, "score": "51.587433"}
{"text": "The high - resolution pdfs are trained using alignments of models and speech data obtained using the low - resolution pdfs , and thus the discriminative training incorporates knowledge of the characteristics of thediscrete pdf system .BRIEF SUMMARY OF THE INVENTION .", "label": "", "metadata": {}, "score": "51.67887"}
{"text": "For the GMM supervector , the UBM consists of 512 mixtures leading to supervectors of dimensionality 18432 .The datasets used for UBM training , SVM background data , NAP and t - norm are the same for all systems .Table IV shows the EER and MinDCF .", "label": "", "metadata": {}, "score": "51.73819"}
{"text": "Also included in the figure are references to equations used at each stage .Fig .5 shows the detection error trade - off ( DET ) curves .It can be seen that the Fisher kernel and TFLLR scaling perform better than just using the raw discrete probabilities , which indicates that kernel normalization is important .", "label": "", "metadata": {}, "score": "51.78038"}
{"text": "Hence , TFLLR scaling falls into the same category as the Fisher kernel from the perspective of feature normalization in the supervector space . i\u03bb correspond directly to the B. Rank Normalization In [ 15 ] , rank normalization was proposed for normalizing the supervectors of n - gram probabilities .", "label": "", "metadata": {}, "score": "51.791584"}
{"text": "In the embodiment beingdescribed , acoustic vectors ( X.sub.u ) are generated at a rate of one every 10 ms , and have 14 output dimensions .Preferably , the raw vectors are subjected to a gender - normalizing linear discriminant analysis , as described in my co - pending , coassigned application Ser .", "label": "", "metadata": {}, "score": "51.9602"}
{"text": "In this paper we present a novel , efficient search strategy for large vocabulary continuous speech recognition ( LVCSR ) .The search algorithm , based on stack decoding , uses posterior phone probability estimates to substantially increase its efficiency with minimal effect on accuracy .", "label": "", "metadata": {}, "score": "52.132336"}
{"text": "In this paper we present a novel , efficient search strategy for large vocabulary continuous speech recognition ( LVCSR ) .The search algorithm , based on stack decoding , uses posterior phone probability estimates to substantially increase its efficiency with minimal effect on accuracy .", "label": "", "metadata": {}, "score": "52.132336"}
{"text": "In this paper , we show that SVMs provide a significant improvement in performance on a static pattern classification task based on the Deterding vowel data .We also describe an application of SVMs to large vocabulary speech recognition , and demonstrate an improvement in error rate on a continuous alphadigit task ( OGI Aphadigits ) and a large vocabulary conversational speech task ( Switchboard ) .", "label": "", "metadata": {}, "score": "52.29601"}
{"text": "mu .Due to the lack of normalizing terms in equation ( 4 ) , CDLP is not a true log - probability , and thus is not interchangeable with thediscrete log - probabilities VQLP .This incompatibility is not an issue , however , because once the alignment paths are established the discrete log - probabilities are no longer used .", "label": "", "metadata": {}, "score": "52.333996"}
{"text": "II-692 - -II-695 .A speech recognition system has vocabulary word models having for each word model state both a discrete probability distribution function and a continuous probability distribution function .Word models are initially aligned with an input utterance using the discrete probability distribution functions , and an initial matching performed .", "label": "", "metadata": {}, "score": "52.3651"}
{"text": "[45 ] Z. Kim , G. Gomes , R. Hranac , and A. Skabardonis , \" A Machine Vision System for Generating Vehicle Trajectories over Extended Freeway Segments , \" Proc .12th World Congress Intelligent Transportation Systems , 2005 .", "label": "", "metadata": {}, "score": "52.54859"}
{"text": "We also develop an on - line learning algorithm in which the pa - rameters are updated incrementally .Com - parative simulation results are presented in the robot dynamics domain . ... sively to unsupervised learning problems . by Anders Krogh , Michael Brown , I. Saira Mian , Kimmen Sj\u00f6lander , David Haussler - JOURNAL OF MOLECULAR BIOLOGY , 1994 . \" ...", "label": "", "metadata": {}, "score": "52.6662"}
{"text": "We also develop an on - line learning algorithm in which the pa - rameters are updated incrementally .Com - parative simulation results are presented in the robot dynamics domain . ... sively to unsupervised learning problems . by Anders Krogh , Michael Brown , I. Saira Mian , Kimmen Sj\u00f6lander , David Haussler - JOURNAL OF MOLECULAR BIOLOGY , 1994 . \" ...", "label": "", "metadata": {}, "score": "52.6662"}
{"text": "The size of the UBM ( i.e. , the cardinality of the discrete event set ) has a great impact on the performance as shown in Fig .3(a ) .It can be seen that the EER reduces as the size of the UBM increases .", "label": "", "metadata": {}, "score": "52.781837"}
{"text": "In this study , since the supervectors represent the probability distributions of discrete events , we propose using Bhattacharyya coefficient [ 13 ] as the similarity measure .The Bhattacharyya measure is symmetric as opposed to other probabilistic measures such as Kullback - Leibler ( KL ) divergence [ 14 ] , which is non - symmetric and has to be simplified and approximated substantially to arrive at a symmetric kernel .", "label": "", "metadata": {}, "score": "52.87635"}
{"text": "As indicated , a relatively low weighting factor is used in this adjustment since it is based on a single example rather than a batch of examples as was the case of the adjustments made during the initial or batch training illustrated in FIG .", "label": "", "metadata": {}, "score": "52.877808"}
{"text": "We also present preliminary results on a purely visual lip reader . by Aravind Ganapathiraju , Jonathan Hamaker , Joseph Picone - Proceedings of the International Conference on Spoken Language Processing , 1998 . \" ...Statistical techniques based on hidden Markov Models ( HMMs ) with Gaussian emission densities have dominated signal processing and pattern recognition literature for the past 20 years .", "label": "", "metadata": {}, "score": "52.918587"}
{"text": "28 ] R. Auckenthaler and J. S. Mason , \" Gaussian selection applied to text- independent speaker verification , \" in Proc .Odyssey , 2001 .[29 ] A. Solomonoff , W. M. Campbell , and C. Quillen , \" Channel compensation for SVM speaker recognition , \" in Proc .", "label": "", "metadata": {}, "score": "52.928474"}
{"text": "5 is a flow chart illustrating initial , batch training of word models ; and .FIG .6 is a flow chart illustrating on - line adaptive training of word models .Corresponding reference characters indicate corresponding elements throughout the several views of the drawings .", "label": "", "metadata": {}, "score": "52.98849"}
{"text": "A new gradient which reflects the modified parameters is computed and the parameters are adjusted further .The iteration is continued until convergence is attained , usually determined by monitoring theperformance on evaluation data independent from the training data .A training database is preprocessed by obtaining for each training utterance a short list of candidate recognition models .", "label": "", "metadata": {}, "score": "53.180363"}
{"text": "Gaussian selection technique [ 28 ] as described below can then be used to speed up the probability computation in ( 8) .A smaller GMM , referred to as the hash model [ 28 ] , is trained with the same training data as the UBM .", "label": "", "metadata": {}, "score": "53.31904"}
{"text": "Page 6 .Accepted for IEEE T - ASL 6 experiments Bhattacharyya measure was used for normalizing the supervectors and t - norm [ 27 ] was performed at the score level ( see Section V.D for more details about t - norm ) .", "label": "", "metadata": {}, "score": "53.388123"}
{"text": "Index Terms - Acoustic modeling , deep belief networks ( DBNs ) , neural networks , phone recognition .I. .4 We can repeat the process of freezing and untying the lowest copy of the currently tied weight matrices as many times as we like , so we can learn as many layers of features as we desire .", "label": "", "metadata": {}, "score": "53.395695"}
{"text": "When the user specifies the correct answer , the candidate alignment paths are regenerated and the utterance gradient term is recomputed .The weighted gradient is subtracted from the affected model parameters .A new gradient term , reflecting the correct target model is calculated and applied tothe DTMR parameters .", "label": "", "metadata": {}, "score": "53.45003"}
{"text": "( 9 )The UBM quantizes the input vectors into discrete symbols , much similar to the VQ codebook except that the codewords are now modeled as Gaussian densities .Since the Gaussian densities can be overlapped , rather than partitioned , soft membership can be computed based on the Bayes rule as given in ( 8) .", "label": "", "metadata": {}, "score": "53.63778"}
{"text": "An important potential source of errors arises from novel input data , that is input data which differ significantly from the data used to train the network .In this paper we investigate the relationship between the degree of novelty of input data and the corresponding reliability of the outputs from the network .", "label": "", "metadata": {}, "score": "53.657475"}
{"text": "An important potential source of errors arises from novel input data , that is input data which differ significantly from the data used to train the network .In this paper we investigate the relationship between the degree of novelty of input data and the corresponding reliability of the outputs from the network .", "label": "", "metadata": {}, "score": "53.657475"}
{"text": "Fig .5 .DET curves showing a comparison of various normalization methods ( or kernels ) on the supervector of discrete probabilities .Page 9 .Accepted for IEEE T - ASL 9 differences in EER and MinDCF are significant .", "label": "", "metadata": {}, "score": "53.716858"}
{"text": "Tools . \" ... decide which contexts are similar and can share parameters .A key feature of this approach is that it allows the construction of models which are dependent upon contextual effects occurring across word boundaries .The use of cross word context dependent models presents problems for conventional dec ... \" . decide which contexts are similar and can share parameters .", "label": "", "metadata": {}, "score": "53.73737"}
{"text": "It can be observed that EER increases drastically for \u03c4 greater than 10 .Larger value of \u03c4 pushes the MAP estimate toward the prior weights .This weakens the effect of the observed statistics , which contain speaker characteristics .In particular , using a very large \u03c4 in ( 10 ) and ( 5 ) would cause the MAP estimation to give the prior weights as the probabilities estimate , and thus losing all speaker- related information .", "label": "", "metadata": {}, "score": "53.821823"}
{"text": "Some improvement can also be observed for the fusion with the GMM supervector , which amounts to 5.35 % and 0.95 % relative reduction in EER and MinDCF , respectively , over the best single system .A McNemar statistical test [ 14 , 30 ] was conducted to see if the TABLE IV COMPARISON OF EER AND MINDCF FOR DIFFERENT SUPERVECTORS .", "label": "", "metadata": {}, "score": "53.907207"}
{"text": "Abstract / OtherAbstract : .Protein chemical shifts encode detailed structural information that is di_cult and computationally costly to describe at a fundamental level .Statistical and machine learning approaches have been used to infer correlations between chemical shifts and secondary structure from experimental chemical shifts .", "label": "", "metadata": {}, "score": "54.08515"}
{"text": "We implement this framework in a computational neural network model , and show that it can account for a wide range of data in animal learning , thus validating our theoretical ideas , and providing a number of insights and predictions about these learning phenomena . by Kenneth A. Norman , Randall C. O'Reilly - PSYCHOLOGICAL REVIEW , 2003 . \" ...", "label": "", "metadata": {}, "score": "54.115707"}
{"text": "We implement this framework in a computational neural network model , and show that it can account for a wide range of data in animal learning , thus validating our theoretical ideas , and providing a number of insights and predictions about these learning phenomena . by Kenneth A. Norman , Randall C. O'Reilly - PSYCHOLOGICAL REVIEW , 2003 . \" ...", "label": "", "metadata": {}, "score": "54.115707"}
{"text": "the plurality of non - speech audio sets of numbers are means of components of Gaussian mixtures that characterize the probability of an underlying state of a hidden Markov model of the audio signal , given the first set of numbers .", "label": "", "metadata": {}, "score": "54.20926"}
{"text": "13 , no . 3 , pp .355 - 366 , May. [21 ] J. Pelecanos and S. Sridharan , \" Feature warping for robust speaker verification , \" in Proc .Odyssey , 2001 .[ 22 ] R. O. Duda , P. E. Hart , and D. G. Stork , Pattern Classification , NY : Wiley , 2001 .", "label": "", "metadata": {}, "score": "54.343582"}
{"text": "One of the key factors limiting the use of neural networks in many industrial applications has been the difficulty of demonstrating that a trained network will continue to generate reliable outputs once it is in routine use .An important potential source of errors arises from novel input data , that ... \" .", "label": "", "metadata": {}, "score": "54.436703"}
{"text": "One of the key factors limiting the use of neural networks in many industrial applications has been the difficulty of demonstrating that a trained network will continue to generate reliable outputs once it is in routine use .An important potential source of errors arises from novel input data , that ... \" .", "label": "", "metadata": {}, "score": "54.436703"}
{"text": "( 12 )We deliberately write ( 12 ) in terms of in ( 8) , to establish the connection to our earlier discussion .Notice that .Hence , Fisher mapping essentially boils down to the ML estimate of discrete distribution , with additional normalization factors depending on the Fisher information .", "label": "", "metadata": {}, "score": "54.470608"}
{"text": "A method as in claim 5 , further comprising : .f. automatically performing a discriminative adjustment to descriptive parameters of the best rescored word model state sequence and the descriptive parameters of an inferior scoring word model state sequence ; and .", "label": "", "metadata": {}, "score": "54.626434"}
{"text": "P. Beyerlein & M. Ullrich \" Hamming Distance Approximation For A Fast Log - Likelihood Computation For Mixture Densities \" ESCA , Eurospeech ' 95,4th European Conference on Speech Communication and Technology , Madrid , Sep. 1995 .ISSN 1018 - 4074 , pp.1083 - 1086 . Y. Komori et al . , \" An Efficient Output Probability Computation For Continuous HMM Using Rough And Detail Models\"--ESCA .", "label": "", "metadata": {}, "score": "54.72472"}
{"text": "( about 200 to 480 samples ) .Preferably , there is about a 15 - 20 ms overlaps between the two successive blocks .Each filtered discretized sample in each frame is multiplied by a specific coefficient of the window function that is determined by the position of the filtered discretized sample in the window .", "label": "", "metadata": {}, "score": "54.725784"}
{"text": "The earliest example of sequence kernel can be traced back to [ 18 ] in which the Fisher kernel was proposed .The Fisher kernel maps a sequence into a supervector by taking the derivatives of the log - likelihood function with respect to the parameters of the model .", "label": "", "metadata": {}, "score": "54.74911"}
{"text": "Feature normalization is essential for effective SVM modeling .The reason is that SVMs are not invariant to linear transformations , i.e. , any form of scaling would cause some of the dimensions to dominate the overall decision .For the MAP estimation , the parameter \u03c4 in ( 10 ) is set to 0.1 .", "label": "", "metadata": {}, "score": "54.8261"}
{"text": "The solution to this problem provided by the present invention is to begin by assuming that the top - choice recognition candidate is in fact the correct answer and to update the models immediately .However , if the user makes a correction at somesubsequent time , the original misadaptation will be undone and a new modification of the model parameters will be performed based on the corrected information .", "label": "", "metadata": {}, "score": "54.836174"}
{"text": "153 - 168 , 2000 .[26 ] J. Goldberger , S. Gordon , and H. Greenspan , \" An Efficient Image Similarity Measure Based on Approximations of KL - Divergence between Two Gaussian Mixtures , \" Proc .IEEE Int'l Conf .", "label": "", "metadata": {}, "score": "54.869232"}
{"text": "INDEX TERMS .Hidden Markov models , Vectors , Computational modeling , Visualization , Training , Feature extraction , Handwriting recognition , hidden Markov model , Handwriting recognition , word spotting , image retrieval .CITATION .Jose\u0301 A. Rodri\u0301guez - Serrano , F. Perronnin , \" A Model - Based Sequence Similarity with Application to Handwritten Word Spotting \" , IEEE Transactions on Pattern Analysis & Machine Intelligence , vol.34 , no .", "label": "", "metadata": {}, "score": "54.911964"}
{"text": "As indicated at block 115 , a gradient is accumulated for each pair trace backed along the correct and incorrect paths .An accumulated gradient is applied to the continuous density pdf parameters as indicated at block 119 .A test for convergenceis applied as indicated at block 121 and the procedure beginning at block 117 is repeated until the models have converged .", "label": "", "metadata": {}, "score": "54.9382"}
{"text": "It should be mentioned that the term \" supervector \" was originally used in [ 4 , 9 ] to refer to the GMM supervector .Here , we use similar term in a broader sense referring to any fixed - dimensional vector that represents a speech sequence as a single point in the vector space , having a much higher dimensionality than the original input space .", "label": "", "metadata": {}, "score": "55.057507"}
{"text": "294 - 297 , 2007 .[ 8 ] K. A. Lee , C. You , H. Li , T. Kinnunen , and D. Zhu , \" Characterizing speech utterances for speaker verification with sequence kernel SVM , \" in Proc .", "label": "", "metadata": {}, "score": "55.282772"}
{"text": "210- 229 , 2006 .[ 4 ] W. M. Campbell , D. E. Sturim , and D. A. Reynolds , \" Support vector machines using GMM supervectors for speaker recognition , \" IEEE Signal Processing Lett . , vol .13 , no .", "label": "", "metadata": {}, "score": "55.408615"}
{"text": "Experiments conducted on the NIST 2006 speaker verification task indicate that the Bhattacharyya measure outperforms the Fisher kernel , term frequency log - likelihood ratio ( TFLLR ) scaling , and rank normalization reported earlier in literature .Moreover , the Bhattacharyya measure is computed using a data - independent square - root operation instead of data - driven normalization , which simplifies the implementation .", "label": "", "metadata": {}, "score": "55.54611"}
{"text": "Denoting the rank of supervector is now given by i \u03c9 ? as ir , the .i \u03c9 ? is given by the number of elements in the i \u03c9 ? follow a Dirichlet distribution .This assumption implies that each element distribution [ 22 ] , which is warped to a uniform distribution via rank normalization .", "label": "", "metadata": {}, "score": "55.553406"}
{"text": "1 , higher gain is applied to rare events , i.e. , those events with lower probabilities .The gain reduces gradually ( so as the slope of the curve ) when the input approaches unity .By so doing , the situation where rare events are outweighed by those with higher probabilities is avoided .", "label": "", "metadata": {}, "score": "55.579826"}
{"text": "The a posteriori probability for each emitting state ( including the emitting state 146 in the background sound model 150 ) is preferably a multi component Gaussian mixture of the form : .b .j .Y .P . )", "label": "", "metadata": {}, "score": "55.587757"}
{"text": "Alternatively a weighted sum of feature vectors from the inter sentence pause is used .Weights used in the weighted sum may be coefficients of a FIR low pass filter .According to another alternative embodiment of the invention the weighted sum may sum feature vectors extracted from multiple inter sentence pauses ( excluding speech sounds between them ) .", "label": "", "metadata": {}, "score": "55.70157"}
{"text": "FIGS . 2 - 5 and outputs a stream of recognized sentences through the transcribed language output 614 .Alternatively the recognized words or sentences are used to control the operation of other programs executed by the processor .For example the system 100 may comprise other peripheral devices such as wireless phone transceiver ( not shown ) , in which case the recognized words may be used to select a telephone number to be dialed automatically .", "label": "", "metadata": {}, "score": "55.790955"}
{"text": "Typically , the model assumes that segme ... . ... istic mapping specifies which region corresponds to each observation vector .Initial experiments with contextindependent ( CI ) phone classification suggested that microsegment models provided a significant gain over the standard SSM ... . \" ...", "label": "", "metadata": {}, "score": "55.84112"}
{"text": "Audio , Speech , and Lang .Process . , vol .15 , pp .1987 - 1998 , Sep. 2007 .[ 7 ] K. A. Lee , C. You , H. Li , and T. Kinnunen , \" A GMM - based probabilistic sequence kernel for speaker recognition , \" in Proc .", "label": "", "metadata": {}, "score": "56.002945"}
{"text": "[ 18 ] T. S. Jaakkola and D. Haussler , \" Exploiting generative models in discriminative classifisers , \" in Advances in Nueral Information Processing Systems 11 , M. S. Kearns , S. A. Solla , and D. A. Cohn , Eds .", "label": "", "metadata": {}, "score": "56.55201"}
{"text": "Thus : # # EQU6 # # . which can be rewritten as # # EQU7 # # .A similar expression can be written for D.sub.j .Differentiating the error function with respect to a particular component of the mean vector . mu .", "label": "", "metadata": {}, "score": "56.870476"}
{"text": "The word layer transition matrix includes a probability for each possible transition between word states .Some transition probabilities may be zero .The phoneme layer 136 includes a word HMM for each word in the word layer 138 .Each word HMM includes a sequence of states corresponding to a sequence of phonemes that comprise the word .", "label": "", "metadata": {}, "score": "56.926224"}
{"text": "During the test phase , a test utterance is compared to each of the language - dependent models after going through the same preprocessing and feature extraction step .\" [ Show abstract ] [ Hide abstract ] ABSTRACT : Spoken language recognition refers to the automatic process through which we determine or verify the identity of the language spoken in a speech sample .", "label": "", "metadata": {}, "score": "57.193844"}
{"text": "By taking the magnitude of each MEL scale frequency component , phase information , which does not encode speech information , is discarded .By discarding phase information , the dimensionality of acoustic signal information is further reduced .By taking the log of the resulting magnitude the magnitudes of the MEL scale frequency components are put on a scale which more accurately models the response of the human hearing to changes in sound intensity .", "label": "", "metadata": {}, "score": "57.36286"}
{"text": "Claim : .What is claimed is : .A method for a speech recognition system with word models having descriptive parameters and associated continuous probability density functions ( PDFs ) to dynamically adjustthe word model descriptive parameters , the method comprising : .", "label": "", "metadata": {}, "score": "57.433723"}
{"text": "Performance comparison between ML and MAP in terms of EER .( a )The value of \u03c4 was increased from 0 to 1.0 with a step size of 0.1 for UBM with various sizes .( b )The EER was evaluated as a function of \u03c4 with value increases from 0.1 to 100 .", "label": "", "metadata": {}, "score": "57.644684"}
{"text": "In step 414 the log of each MEL frequency component magnitude is taken to obtain a plurality of log magnitude MEL scale frequency components .Referring to .FIG .5 which is a second part of the flow chart begun in .", "label": "", "metadata": {}, "score": "57.701355"}
{"text": "30 , no .11 , pp .1945 - 1957 , Nov. 2008 .[ 23 ] A. Kolcz , J. Alspector , M. Augusteijn , R. Carlson , and G.V. Popescu , \" A Line - Oriented Approach to Word Spotting in Handwritten Documents , \" Pattern Analysis and Applications , vol .", "label": "", "metadata": {}, "score": "57.95721"}
{"text": "A McNemar 's statistical test [ 14 , 30 ] was conducted to see if the EER and MinDCF of the Bhattacharyya measure are significantly better than other normalization methods .The p - values obtained were all less than 0.05 , which means that the improvements are significant with a confidence level of 95 % .", "label": "", "metadata": {}, "score": "58.07234"}
{"text": "52 , no . 1 , pp .12 - 40 , Jan. 2010 .[ 2 ] V. Wan and S. Renals , \" Speaker verification using sequence discriminant support vector machines , \" IEEE Trans .Speech Audio Process . , vol .", "label": "", "metadata": {}, "score": "58.15661"}
{"text": "Such transitions often occur at the end of postulated words .Thus , in order to be able to determine the ending of words , and in order to be able to discriminate between short words that sound like the beginning of longer words and the longer words , it is important to be able to recognize background sounds .", "label": "", "metadata": {}, "score": "58.184765"}
{"text": "...... , ( 11 ) where the superscript T denotes transposition .The vector p has a fixed dimensionality , M , equivalent to the cardinality of the event set S. It represents the speech segment X in terms of the distribution of discrete events observed in X .", "label": "", "metadata": {}, "score": "58.268616"}
{"text": "The computer readable medium according to .claim 16 wherein the programming instructions for processing the first portion of the first inter - sentence pause to obtain the first characterization of the first portion of the first inter - sentence pause comprise the programming instruction , for : . a ) time domain sampling the first inter - sentence pause to obtain a discretized representation of the audio signal that includes a sequence of samples ; .", "label": "", "metadata": {}, "score": "58.31571"}
{"text": "Initially introduced in the late 1960s and early 1970s , dynamic programming algorithms have become increasingly popular in automatic speech recognition .There are two reasons why this has occurred : First , the dynamic programming strategy can be combined with avery efficient and practical pruning str ... \" .", "label": "", "metadata": {}, "score": "58.485447"}
{"text": "MTLC can not support recall , but it is possible to extract a scalar familiarity signal from MTLC that tracks how well the test item matches studied items .We present simulations that establish key qualitative differences in the operating characteristics of the hippocampal recall and MTLC familiarity signals , and we identify several manipulations ( e.g. , target - lure similarity , interference ) that differentially affect the two signals .", "label": "", "metadata": {}, "score": "58.784607"}
{"text": "MTLC can not support recall , but it is possible to extract a scalar familiarity signal from MTLC that tracks how well the test item matches studied items .We present simulations that establish key qualitative differences in the operating characteristics of the hippocampal recall and MTLC familiarity signals , and we identify several manipulations ( e.g. , target - lure similarity , interference ) that differentially affect the two signals .", "label": "", "metadata": {}, "score": "58.784607"}
{"text": "\u03b1 is a weighting parameter that is preferably at least about 0.7 and more preferably at least about 0.9 ; and CRV is the characteristic feature vector for non speech background sounds as measured during the inter sentence pause .Thus the ASR system 100 will be better able to identify background noise , and the likelihood of the ASR system 100 construing background noise 100 as a speech phoneme will be reduced .", "label": "", "metadata": {}, "score": "58.8257"}
{"text": "[ 24 ] H. Hermansky and N. Morgan , \" RASTA processing of speech , \" IEEE Trans .Speech Audio Process . , vol .2 , no .4 , pp .578 - 589 , Oct. 1994 .Page 10 .", "label": "", "metadata": {}, "score": "58.827324"}
{"text": "A method as in claim 6 , wherein in step ( g)(ii ) the at least one other word model state sequence is the word model state sequence having the next best score to the word model state sequence of the user corrected word sequence .", "label": "", "metadata": {}, "score": "58.84499"}
{"text": "After each utterance , the preselected parameters are adjusted to increase , by a small proportion , the difference in scoring between the top and next ranking models .BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1 is a block diagram of a speech recognition system in accordance with the present invention ; .", "label": "", "metadata": {}, "score": "58.880234"}
{"text": "203 - 210 , Mar. 2005 .[ 3 ] W. M. Campbell , J. P. Campbell , D. A. Reynolds , E. Singer , and P. A. Torres - Carrasquillo , \" Support vector machines for speaker and language recognition , \" Computer Speech and Language , vol .", "label": "", "metadata": {}, "score": "59.073013"}
{"text": "[ Show abstract ] [ Hide abstract ] ABSTRACT :Recently , joint factor analysis ( JFA ) and identity - vector ( i - vector ) represent the dominant techniques used for speaker recognition due to their superior performance .Developed relatively earlier , the Gaussian mixture model - support vector machine ( GMM - SVM ) with nuisance attribute projection ( NAP ) has gradually become less popular .", "label": "", "metadata": {}, "score": "59.208603"}
{"text": "Another practical virtue of discrete representation is that the estimation Using Discrete Probabilities with Bhattacharyya Measure for SVM - based Speaker Verification Kong Aik Lee , Chang Huai You , Haizhou Li , Tomi Kinnunen , and Khe Chai Sim S .", "label": "", "metadata": {}, "score": "59.258987"}
{"text": "The computer readable medium according to . claim 13 wherein the programming instructions for updating the non - speech audio characterization comprising programming instructions for : . replacing each number in the particular set of numbers with a weighted average of the number and a corresponding number in the first set of numbers .", "label": "", "metadata": {}, "score": "59.317337"}
{"text": "There are 3616 genuine and 47452 imposter trials , where test utterances are scored against target speakers of the same gender .All speech utterances were first pre - processed to remove silence and converted into sequences of 36-dimensional feature vectors , each consisting of 12 Mel frequency cepstral coefficients ( MFCCs ) appended with deltas and double deltas .", "label": "", "metadata": {}, "score": "59.36982"}
{"text": "The warping functions are shown in Fig .2 for individual dimensions and their ensemble average .Comparing Fig .2 to Fig .1 , it can be seen that the warping function closely resembles the square - root curve in the sense that the input axis is shrunk for values closer to the origin and stretched at the other end .", "label": "", "metadata": {}, "score": "59.397194"}
{"text": "Recently as the processing power of portable electronic devices has increased there has been an increased interest in adding speech recognition capabilities to such devices .Wireless telephones that are capable of operating under the control of voice commands have been introduced into the market .", "label": "", "metadata": {}, "score": "59.413124"}
{"text": "Markov Models ( HMMs ) are applied t.0 the problems of statistical modeling , database searching and multiple sequence alignment of protein families and protein domains .These methods are demonstrated the on globin family , the protein kinase catalytic domain , and the EF - hand calcium binding motif .", "label": "", "metadata": {}, "score": "59.429443"}
{"text": "Markov Models ( HMMs ) are applied t.0 the problems of statistical modeling , database searching and multiple sequence alignment of protein families and protein domains .These methods are demonstrated the on globin family , the protein kinase catalytic domain , and the EF - hand calcium binding motif .", "label": "", "metadata": {}, "score": "59.429443"}
{"text": "A UBM , denoted by \u0398 , with M mixture components is characterized by the following probability density function : .Page 3 .Accepted for IEEE T - ASL 3 .The mixture weights satisfy the constraint covariance matrices are assumed to be diagonal in this paper .", "label": "", "metadata": {}, "score": "59.4601"}
{"text": "The discrete events may also correspond to abstract linguistic units such as phonemes , syllables , words , or subsequences of n symbols ( i.e. , n - grams ) .For instance , in spoken language recognition [ 16 ] and speaker recognition utilizing high - level features [ 10 , 11 ] , the events represent n - grams of phones , words or some prosodic features .", "label": "", "metadata": {}, "score": "59.464172"}
{"text": "This is known as the \u03c4 - initialization method in [ 12].Feasible values for \u03c4 range from 0 to 1 , which we have found effective for this application .Equation ( 10 ) controls the broadness of the prior density ( ) g \u03a9 in ( 4 ) with the parameter \u03c4 .", "label": "", "metadata": {}, "score": "59.47235"}
{"text": "Background noise , which predominates during pauses in speech , is also modeled by one or more states of the HMM model so that the ASR will properly identify pauses and not try to construe background noise as speech .One problem for ASR systems , particularly those used in portable devices , is that the characteristics of the background noise in the environment of the ASR system is not fixed .", "label": "", "metadata": {}, "score": "59.514412"}
{"text": "The weighting used in the on - line adaptation ( . omega . ' ) is set much smaller than the weighting used in the batch - mode training since the reliability of the change estimated from a single utterance is considerably lower than the estimate from acomplete training set .", "label": "", "metadata": {}, "score": "59.514717"}
{"text": "He is currently a Senior Research Fellow with Human Language Institute for Infocomm Research ( I2R ) , Singapore .His research focuses on statistical methods for speaker and spoken language recognition , adaptive echo and noise control , and subband adaptive filtering .", "label": "", "metadata": {}, "score": "59.537254"}
{"text": "In view of the foregoing it may be seen that several objects of the present invention are achieved and other advantageous results have been attained .As various changes could be made in the above constructions without departing from the scope of the invention , it should be understood that all matter contained in the above description or shown in the accompanying drawings shall be interpretedas illustrative and not in a limiting sense .", "label": "", "metadata": {}, "score": "59.6097"}
{"text": "where the index n now denotes position within a frame ; .the index F denotes a frame number ; .X n F is a nth windowed filtered sample ; and .W n is a window coefficient corresponding to the nth position within each frame .", "label": "", "metadata": {}, "score": "59.744232"}
{"text": "y .P . k . ) m .M . log .Z .m . ) cos . k . m .M . ) where y P ( k ) is a kth order DCT component output by the DCT 116 for a pth frame ; and .", "label": "", "metadata": {}, "score": "59.764114"}
{"text": "I make improvements to the current approach to predicting and analyzing error behaviors , which is currently based only on the measurement ofword error rate .The speech recognizer 's functionality is extended to include con dence annotations , which are \\meta - level \" markings that indicate how certain the recognizer is that it has decoded its input correctly .", "label": "", "metadata": {}, "score": "59.83745"}
{"text": "A method as in claim 1 , wherein in step ( e)(ii ) the at least one other word model state sequence is the word model state sequence having the next best score to the word model state sequence of the user corrected word sequence .", "label": "", "metadata": {}, "score": "59.837944"}
{"text": "Although all approaches try to circumvent the frame independence assumption within a segment and repo ... . by Ashvin Kannan , Mari Ostendorf - in Proc .Int&apos;l .Conf . on Acoust . , Speech and Signal Proc , 1993 . \" ...", "label": "", "metadata": {}, "score": "59.84552"}
{"text": "[17 ] D. A. Reynolds , T. F. Quatieri , and R. B. Dunn , \" Speaker verification using adapted Gaussian mixture models , \" Digital Signal Processing , vol .10 , no . 1 - 3 , pp .", "label": "", "metadata": {}, "score": "59.851265"}
{"text": "The invention is not limited to any particular way of determining the closeness of the characteristic feature vector to the mean vectors \u03bc jn of the Gaussian mixture components .Once the closest mean vector is identified , the mixture component with which it is associated is altered so that it yields a higher a posteriori probability when evaluated with the characteristic feature vector .", "label": "", "metadata": {}, "score": "59.933254"}
{"text": "INTRODUCTION Search is an interesting problem in the field of large vocabulary speech recognition .Typically the acoustic vectors correspondi ... . by Mary P. Harper , Randall A. Helzerman - COMPUTER SPEECH AND LANGUAGE , 1995 . \" ...A text - based and spoken language processing framework based on the Constraint Dependency Grammar ( CDG ) developed by Maruyama [ 24 , 25 ] is discussed .", "label": "", "metadata": {}, "score": "59.991756"}
{"text": "455 - 472 , Jul. 2005 .[ 12 ] C. H. Lee and Q. Huo , \" On adaptive decision rules and decision parameter adaptation for sutomatic speech recognition , \" Proceedings of the IEEE , vol . 88 , no . 8 , pp .", "label": "", "metadata": {}, "score": "60.040306"}
{"text": "Additionally , this paper includes a novel graphical analysis regarding why derivative ( or delta ) features improve hidden Markov model - based speech recognition by improving structural discriminability .It also includes an example where a graph can be used to represent language model smoothing constraints .", "label": "", "metadata": {}, "score": "60.15079"}
{"text": "14 ] X. Huang , A. Acero , and H. W. Hon , Spoken Language Processing : a Guide to Theory , Algorithm , and System Development .NJ : Prentice-Hall , 2001 .[ 15 ] A. Stolcke , S. Kajarekar , and L. Ferrer , \" Nonparametric feature normalization for SVM - based speaker verification , \" in Proc .", "label": "", "metadata": {}, "score": "60.202084"}
{"text": "The method according to .claim 3 wherein the step of updating the non - speech audio characterization comprises the sub - steps of : . replacing each number in the particular set of numbers with a weighted average of the number and a corresponding number in the first set of numbers .", "label": "", "metadata": {}, "score": "60.357933"}
{"text": "..Later we introduce extensions to the model which remove this restriction .As in the continuous state case , we can consider the limit as the ob ... .by Kimmen Sj\u00f6lander , Kevin Karplus , Michael Brown , Richard Hughey , Anders Krogh , I. Saira Mian , David Haussler , 1996 . \" ...", "label": "", "metadata": {}, "score": "60.46907"}
{"text": "..Later we introduce extensions to the model which remove this restriction .As in the continuous state case , we can consider the limit as the ob ... .by Kimmen Sj\u00f6lander , Kevin Karplus , Michael Brown , Richard Hughey , Anders Krogh , I. Saira Mian , David Haussler , 1996 . \" ...", "label": "", "metadata": {}, "score": "60.46907"}
{"text": "Description : .BACKGROUND OF THE INVENTION .The function of automatic speech recognition ( ASR ) systems is to determine the lexical identity of spoken utterances .The recognition process , also referred to as classification , typically begins with the conversion of an analog acousticalsignal into a stream of digitally represented spectral vectors or frames which describe important characteristics of the signal at successive time intervals .", "label": "", "metadata": {}, "score": "60.60047"}
{"text": "If a long pause is detected , the process continues with process block 206 in which a characteristic feature vector that characterizes the audio signal during the long pause ( i.e. , characterizes the background sound ) is extracted from the audio signal .", "label": "", "metadata": {}, "score": "60.697098"}
{"text": "The computer readable medium according to .claim 12 wherein : . the programming instructions for processing the first portion of the first inter - sentence pause to obtain a first characterization include programming instructions for : . processing the first portion of the first inter - sentence pause to obtain a first set of numbers that characterize the first portion of the first inter - sentence pause ; and .", "label": "", "metadata": {}, "score": "60.726513"}
{"text": "Markov Models ( HMMs ) are applied t.0 the problems of statistical modeling , database searching and multiple sequence alignment of protein families and protein domains .These methods are demonstrated the on globin family , the protein kinase catalytic domain , and the EF - hand calcium binding moti ... \" .", "label": "", "metadata": {}, "score": "60.736977"}
{"text": "Markov Models ( HMMs ) are applied t.0 the problems of statistical modeling , database searching and multiple sequence alignment of protein families and protein domains .These methods are demonstrated the on globin family , the protein kinase catalytic domain , and the EF - hand calcium binding moti ... \" .", "label": "", "metadata": {}, "score": "60.736977"}
{"text": "As indicated previously , the present invention is particularly concerned with the provision of discriminatively trained multi - resolution vocabulary models which increase accuracy and reduce computational load in an automatic speech recognition(ASR ) system .At the outset , however , it is appropriate to describe in general terms the type of speech recognition system to which the present invention is applicable .", "label": "", "metadata": {}, "score": "60.86"}
{"text": "308 - 311 , May 2006 .[5 ] Y. Mami and D. Charlet , \" Speaker recognition by location in the space of reference speakers , \" Speech communication , vol .48 , no . 2 , pp . 127- 141 , 2006 .", "label": "", "metadata": {}, "score": "60.95272"}
{"text": "As mentioned above a difficulty arises in ASR due to the fact that the background noise varies .Feature vectors that characterizes the audio signal that are output by the feature extractor 124 are input into the HMM 132 and used within the acoustic layer 134 .", "label": "", "metadata": {}, "score": "60.998863"}
{"text": "Audio , Speech , and Language Process . , vol .15 , no . 7 , pp .2085 - 2094 , Sep. 2007 .[ 11 ] E. Shriberg , L. Ferrer , S. Kajarekar , A. Venkataraman and A. Stolcke , \" Modeling prosodic feature sequences for speaker recognition , \" Speech Communication , vol .", "label": "", "metadata": {}, "score": "61.041412"}
{"text": "For discrete distributions , the Bhattacharyya coefficient is given by .The coefficient \u03c1 lies between zero and unity , where distributions are fully overlapped , while case of non - overlapping distributions . in ( 15 ) , the . , ( 16 ) where the supervector is now given by .", "label": "", "metadata": {}, "score": "61.10151"}
{"text": "The DCT 116 transforms the rescaled magnitudes to the time domain .The output of the DCT 116 comprises a set of DCT components values ( cepstral coefficients ) for each frame .The zero order component output by the DCT is proportional to the log energy of the acoustic signal during the frame from which the component was generated .", "label": "", "metadata": {}, "score": "61.104446"}
{"text": "We evaluate the accuracy by comparing the probability estimates obtained with and without Gaussian selection in terms of the Bhattacharyya measure ( 16 ) averaged over 100 random samples , which were selected from our development data .Formally , . , , 11 1 K \u02c6 Average accuracy KM i k i k ki \u03c9 ?", "label": "", "metadata": {}, "score": "61.265694"}
{"text": "The emitting states 140 B and 140 C of the first phoneme HMM have associated probability density functions 144 and 162 respectively .Likewise , the emitting state 146 of the background sound model 156 has a background sound PDF 148 .", "label": "", "metadata": {}, "score": "61.35699"}
{"text": "Audio , Speech , Lang .Process , 2012 . \" ...Abstract - Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition .We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that co ... \" .", "label": "", "metadata": {}, "score": "61.42252"}
{"text": "Page 5 .Accepted for IEEE T - ASL 5 supervector are processed separately where a warping function is used for mapping each dimension to a uniform distribution over the interval from zero to unity .The warping function is non - parametric and is derived from the concept of cumulative density function ( CDF ) matching , similar to that used in histogram equalization ( HEQ ) [ 20 ] and feature warping [ 21].", "label": "", "metadata": {}, "score": "61.781895"}
{"text": "The FIR time domain filter 106 processes the discretized audio signal and outputs a sequence of filtered discretized samples at the sampling rate .The each nth filter output may be expressed as : .X .n .l . k .", "label": "", "metadata": {}, "score": "61.945713"}
{"text": "The first buffer 118 , and the differencer 120 are coupled to a second buffer 122 .The feature vectors are assembled and stored in the second buffer 122 .The function of the feature extractor 124 is to eliminate extraneous , and redundant information from audio signals that include speech sounds , and produce feature vectors each of which is highly correlated to a particular sound that is one variation of a component of spoken language .", "label": "", "metadata": {}, "score": "61.962097"}
{"text": "Different background noise that is unfamiliar to the ASR system may be construed as parts of speech .What is needed is a ASR system that can achieve high rates of speech recognition when operated in environments with different types of background noise .", "label": "", "metadata": {}, "score": "62.018105"}
{"text": "A novel aspect of this algorithm is a new pruning strategy , phone deactivation pruni ... . by A. J. Robinson , G. D. Cook , D. P. W. Ellis , E. Fosler - Lussier , S. J. Renals , D. A. G. Williams , 2002 . \" ...", "label": "", "metadata": {}, "score": "62.12653"}
{"text": "NAP AND T - NORM WERE APPLIED USING EXACTLY THE SAME DATASET .Page 8 .Accepted for IEEE T - ASL 8 quantizers , e.g. , UBM and VQ codebook , the event set that contains only the unigrams can be made sufficiently large .", "label": "", "metadata": {}, "score": "62.192398"}
{"text": "The method according to . claim 1 wherein : . the step of processing the first portion of the inter - sentence pause to obtain a first characterization includes a sub - step of : . processing the first portion of the inter - sentence pause to obtain a first set of numbers that characterize the first portion of the inter - sentence pause ; and .", "label": "", "metadata": {}, "score": "62.27559"}
{"text": "There may be more than one word HMM for each word in the word layer 138 .Finally , the acoustic layer 134 includes a phoneme HMM model of each phoneme in the language that the HMM 132 is capable of recognizing .", "label": "", "metadata": {}, "score": "62.328365"}
{"text": "1 is a functional block diagram of a system for performing automated speech recognition according to the preferred embodiment of the invention .FIG .2 is a flow chart of a process for updating a model of background noise according to the preferred embodiment of the invention .", "label": "", "metadata": {}, "score": "62.354416"}
{"text": "2 illustrates vocabulary word models used in the speech recognition system of the present invention ; .FIG .3 illustrates a recursion procedure used in the speech recognition system of the present invention ; .FIG .4 illustrates a training data structure set used in training word models ; .", "label": "", "metadata": {}, "score": "62.35733"}
{"text": "Digital signal processors have instruction sets and architectures that are suitable for processing audio signal .As will be apparent to those of ordinary skill in the pertinent arts , the invention may be implemented in hardware or software or a combination thereof .", "label": "", "metadata": {}, "score": "62.427666"}
{"text": "taking the difference between corresponding numbers in the two sets of numbers to obtain at least a subset of the first set of numbers .Description .FIELD OF THE INVENTION .This invention pertains to automated speech recognition .More particularly this invention pertains to speaker independent speech recognition suitable for varied background noise environments .", "label": "", "metadata": {}, "score": "62.48526"}
{"text": "B. Constructing Supervector The discrete probabilities \u03a9 ?That is , the function probability mass function ( PMF ) [ 14]. , M \u03c9 ?P h , where ... can be ( ) ( ) P h is the .", "label": "", "metadata": {}, "score": "62.48771"}
{"text": "3(a ) .This observation is consistent across different UBM sizes .To further investigate the influence of \u03c4 on MAP estimation , we gradually increased the value from 0.1 to 100 .Fig .3(b ) shows the EER as a function of \u03c4 .", "label": "", "metadata": {}, "score": "62.48956"}
{"text": "1995 Internation Conference on Acoustics , Speech and SignalProcessing . vol .1 pp .301 - 304 , May 1995 .Lee et al .A study on Speak Adaptation of the Parameters of Continuous Density HMMs .IEEE Transactions on Signal Processing , Apr. 1991 .", "label": "", "metadata": {}, "score": "62.5277"}
{"text": "[ 27 ] R. Auckenthaler , M. Carey , and H. Lloyd - Thomas , \" Score - normalization for text - independent speaker verification , \" Digital Signal Process . , vol .10 , pp .42 - 54 , Jan. 2000 .", "label": "", "metadata": {}, "score": "62.56246"}
{"text": "Network circuits may also serve temporarily as computer readable media from which programs taught by the present invention are read .While the preferred and other embodiments of the invention have been illustrated and described , it will be clear that the invention is not so limited .", "label": "", "metadata": {}, "score": "62.651825"}
{"text": "The HMM 132 models spoken language .The HMM 132 comprises a hierarchy of three interconnected layers of states including an acoustic layer 134 , a phoneme layer 136 , and a word layer 138 .The word layers 138 includes a plurality of states corresponding to a plurality of words in a vocabulary of the HMM .", "label": "", "metadata": {}, "score": "62.664852"}
{"text": "[ 13 ] T. Kailath , \" The divergence and Bhattacharyya distance measures in signal detection , \" IEEE Trans .Commun .Technol . , vol .COM-15 , no . 1 , pp .52 - 60 , Feb. 1967 .", "label": "", "metadata": {}, "score": "62.763424"}
{"text": "An inter sentence pause detector 152 is coupled to the DCT 116 for receiving one or more of the coefficients output by the DCT for each frame .Preferably , the inter - sentence pause detector receives the zero order DCT coefficient ( log energy value ) for each frame .", "label": "", "metadata": {}, "score": "62.782555"}
{"text": "Between the beginning and ending states of each phoneme HMM are a number of acoustic emitting states ( e.g. , 140 B , 140 C ) .Although two are shown for the purpose of illustration , in practice there may be more than two emitting states in each phoneme model .", "label": "", "metadata": {}, "score": "63.071854"}
{"text": "The effectiveness of the Bhattacharyya measure becomes more apparent when channel compensation is applied at the model and score levels .The performance of the proposed method is close to that of the popular GMM supervector with a small margin .Index Terms- Bhattacharyya coefficient , speaker verification , support vector machine , supervector .", "label": "", "metadata": {}, "score": "63.08114"}
{"text": "For the case of UBM , the relation between \u03a9 ? and X is given by ( 5 ) and ( 9 ) , and the dimensionality of the supervector is determined by the number of Gaussian densities in the UBM .", "label": "", "metadata": {}, "score": "63.12548"}
{"text": "FIG .4 illustrates the training structure set for an input utterance .An error function . epsilon .sub.u for a particular training utterance u is computed from the pairwise error functions O.sub.i , j : # # EQU5 # # . beta . is a scaler multiplier , D.sub.i , i.epsilon.", "label": "", "metadata": {}, "score": "63.19937"}
{"text": "The summation on the left hand side of the above equation effects the DCT transformation .The DCT components are also termed cepstrum coefficients .The windower 108 , FFT 110 , MEL scale filter bank 112 , log - magnitude evaluator 114 , and DCT 116 operate in synchronism .", "label": "", "metadata": {}, "score": "63.23363"}
{"text": "Retrieve candidate list and alignment paths for .nu .Identify subsets I.sub.top - choice and C.sub.top - choice Compute .DELTA . sub .nu . mu .( s , k , l ) .sub.u for all s , k , l specified byalignment paths for all pairs in I.sub.top - choice and C.sub.top - choice .", "label": "", "metadata": {}, "score": "63.64042"}
{"text": "As indicated at block 109 , the models are then re - sorted based on the scores obtained with the continuous density pdfs .Correct and incorrect models are identified as indicated at block 111 and for each pair of correct and incorrect models anerror function is computed as indicated at block 113 .", "label": "", "metadata": {}, "score": "63.80033"}
{"text": "f .N .N .C .N .where , P(0 ) is a zero order power Fourier frequency component ( equal to an average of power of a frame ) ; .P(f l ) is an lth power Fourier frequency component of the frame ; .", "label": "", "metadata": {}, "score": "64.461266"}
{"text": "The performance evaluation is reported in Section V. Finally , Section VI concludes the paper .II .Given a speech segment ie , i.e. , .\u03a9\u03a9 X , ( 1 ) where ( ) g \u03a9 is the prior distribution of the parameters \u03a9. The MAP estimate .", "label": "", "metadata": {}, "score": "64.48715"}
{"text": "Nuisance attribute projection ( NAP ) [ 29 ] removes the unwanted variability from a supervector via a projection to the subspace complementary to E , as follows ( p I EE . )The columns of E are the eigenvectors of the within - speaker covariance matrix TABLE I ACCURACY OVER 100 SAMPLES FOR DIFFERENT SIZES OF UBM AND HASH MODEL .", "label": "", "metadata": {}, "score": "64.56888"}
{"text": "Some of the work has been published previously in conference proceedings [ 66,69 ] , two journal articles [ 36,68 ] , two workshop papers [ 35,67 ] and a tech - nical report [ 65].The length of this thesis including appendices , bibliography , footnotes , tables and equations is approximately 60,000 words .", "label": "", "metadata": {}, "score": "64.6147"}
{"text": "Dr Li 's current research interests include automatic speech recognition , speaker and language recognition , and natural language processing .He has published over 150 technical papers in international journals and conferences .He holds five international patents .Dr Li now serves as an Associate Editor of IEEE Transactions on Audio , Speech and Language Processing , and Springer International Journal of Social Robotics .", "label": "", "metadata": {}, "score": "64.83708"}
{"text": "After a candidate list is obtained as indicated at block 151 , correct ( C ) and incorrect ( I ) subsets areidentified as indicated at block 153 .Corrections to model parameters are computed for all pairs C and I as indicated at block 155 and the corrections are added to the then current model parameters , as indicated at block 157 , using a relatively lowweight .", "label": "", "metadata": {}, "score": "65.04905"}
{"text": "The improvement in MinDCF for the first fusion is also significant with 95 % confidence , which , however does not hold for the second fusion .The fusion of Bhattacharyya and GMM supervector is less successful because the same datasets were used for UBM training , SVM background data , NAP and t - norm .", "label": "", "metadata": {}, "score": "65.12141"}
{"text": "Closeness is preferably judged by determining which mixture component assumes the highest value when evaluated using the characteristic feature vector .Alternatively , closeness is judged by determining which mean vector \u03bc jn yields the highest dot product with the characteristic feature vector .", "label": "", "metadata": {}, "score": "65.13977"}
{"text": "5 is a second part of the flow chart begun in FIG .4 .FIG .6 is a hardware block diagram of the system for performing automated speech recognition according to the preferred embodiment of the invention .DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT .", "label": "", "metadata": {}, "score": "65.451706"}
{"text": "A Hamming window function is preferred .The FFT 110 is coupled to the windower 108 for receiving the successive frames of windowed filtered samples .The FFT projects successive frames of windowed filtered discretized audio signal samples onto a Fourier frequency domain basis to obtain and outputs a plurality of audio signal Fourier frequency components , and processes the Fourier frequency components to determine a set of power Fourier frequency component for each frame .", "label": "", "metadata": {}, "score": "65.46308"}
{"text": "13 , no . 3 , pp .314 - 325 , Mar. 2004 .[14 ] T. Van der Zant , L. Schomaker , and K. Haak , \" Handwritten - Word Spotting Using Biologically Inspired Features , \" IEEE Trans .", "label": "", "metadata": {}, "score": "65.497"}
{"text": "4 , in step 502 a DCT is applied to the log magnitude MEL scale frequency components for each frame to obtain a cepstral coefficient vector for each frame .In step 504 first or higher order differences are taken between corresponding cepstral coefficients for two or more frames to obtain at least first order inter frame cepstral coefficient differences ( deltas ) .", "label": "", "metadata": {}, "score": "65.54523"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .The features of the invention believed to be novel are set forth in the claims .The invention itself , however , may be best understood by reference to the following detailed description of certain exemplary embodiments of the invention , taken in conjunction with the accompanying drawings in which : .", "label": "", "metadata": {}, "score": "65.65068"}
{"text": "Accepted for IEEE T - ASL 4 the focus of the paper .A full account of using this form of supervector for speaker verification can be found in [ 2].IV .THE BHATTACHARYYA MEASURE The Bhattacharyya coefficient [ 13 ] is commonly used in statistics to measure the similarity of two probability distributions .", "label": "", "metadata": {}, "score": "65.69905"}
{"text": "This is particularly useful for forwardingstructured medical reports as described in co - assigned U.S. Pat .No .5,168,548 .To facilitate the use of the computer system for speech recognition , a digital signal processor is provided as indicated by reference character 16 , typically this processor being configured as an add - in circuit card coupled to the system bus 11 .", "label": "", "metadata": {}, "score": "65.84679"}
{"text": "B. ML vs. MAP Estimation This section investigates the difference between ML and MAP estimation and the influence of the parameter \u03c4 on the system performance .We increased the value of \u03c4 in ( 10 ) from 0 to 1.0 with a step size of 0.1 .", "label": "", "metadata": {}, "score": "66.20591"}
{"text": "By requiring that the zero order DCT coefficient remain below the threshold it is possible to distinguish longer inter sentence breaks in speech sound from shorter intra sentence breaks .According to an alternative embodiment of the invention an absence of speech sounds is detected by comparing a weighted sum of DCT coefficients to a threshold value .", "label": "", "metadata": {}, "score": "66.31526"}
{"text": "FIG .2 , in process block 202 an HMM ASR process is run on an audio signal that includes speech and non speech background sounds .Block 202 is decision block that depends on whether a long pause in the speech component of the audio signal is detected .", "label": "", "metadata": {}, "score": "66.35435"}
{"text": "According to the present invention , the background sound is preferably measured in the absence of speech sounds , e.g. , between words or sentences .According to the preferred embodiment of the invention the updating takes place during breaks of at least 600 milliseconds , e.g. breaks that occur between sentences .", "label": "", "metadata": {}, "score": "66.54049"}
{"text": "In 2006 , he was appointed as a Senior Research Fellow with I2R. Since 2007 , he has been a Research Scientist with Human Language Technology department of I2R. His research interests include speaker recognition , language recognition , speech enhancement , speech recognition , acoustic noise reduction , array signal processing , audio signal processing , and image processing .", "label": "", "metadata": {}, "score": "66.55287"}
{"text": "5 .Initially , candidate models are selected using the discrete density pdfs as indicated in step 101 .Again using the discrete pdfs , the input utterances aligned with the best models using the Viterbi algorithm and the traceback information isstored as indicated at block 103 .", "label": "", "metadata": {}, "score": "66.605316"}
{"text": "We use two well - known metrics in evaluating the performance of the speaker verification systems - equal error rate ( EER ) and the minimum detection cost function ( MinDCF ) [ 23].The EER corresponds to the decision that gives equal false acceptance rate ( FAR ) and false rejection rate ( FRR ) .", "label": "", "metadata": {}, "score": "66.76639"}
{"text": "The audio signal sampler 104 preferably samples the audio signal at a sampling rate of about 8,000 to 16,000 samples per second and at 8 to 16 bit resolution and outputs a representation of the input audio signal that is discretized in time and amplitude .", "label": "", "metadata": {}, "score": "67.0063"}
{"text": "C . k .X .n . k . where X n l is an nth time domain filtered output , .C k is a kth FIR time domain filter coefficient , .M is one less than the number of FIR time domain coefficients ; and .", "label": "", "metadata": {}, "score": "67.02969"}
{"text": "The speaker verification system was designed to be gender- dependent1 .Two gender - dependent UBMs were trained using data drawn from the NIST 2004 dataset .The same dataset was used to form the background data for SVM training .The commonly available libSVM toolkit [ 25 ] was used for this purpose .", "label": "", "metadata": {}, "score": "67.06328"}
{"text": "the index n ranges up to a limit N determined by the length of the audio signal .A Finite Impulse Response ( FIR ) time domain filter 106 is coupled to the audio signal sampler 104 for receiving the discretized audio signal .", "label": "", "metadata": {}, "score": "67.09576"}
{"text": "Since rank normalization is non - parametric , the background sets to be stored and therefore computation of ( 20 ) is far more expensive than the square - root operation .Recall that the Bhattacharyya coefficient has a dynamic range bounded between zero and unity .", "label": "", "metadata": {}, "score": "67.160324"}
{"text": "Similar procedure was repeated for various sizes of UBM from 128 to 4096 .In all the . 1 Gender information is provided and there is no cross - gender trial in NIST SREs .Gender - dependent systems have shown better result than gender- independent systems in past evaluations . 0.1 1.010.0 100.0 10 12 14 16 18 MAP controlled parameter , \u03c4 ( b ) Equal Error Rate ( EER ) Fig .", "label": "", "metadata": {}, "score": "67.55798"}
{"text": "In the description below , like reference numbers are used to describe the same , similar , or corresponding parts in the several views of the drawings .FIG .1 is a functional block diagram of a system 100 for performing automated speech recognition according to the preferred embodiment of the invention .", "label": "", "metadata": {}, "score": "67.59954"}
{"text": "In step 408 a FFT is applied to successive frames of samples to obtain a plurality of frequency components .In step 410 the plurality of frequency components are run through a MEL scale filter bank to obtain a plurality of MEL scale frequency components .", "label": "", "metadata": {}, "score": "67.72381"}
{"text": "6 , pp .709 - 720 , June 2004 .[ 12 ] E. Saykol , A. Sinop , U. Gudukbay , O. Ulusoy , and A. Cetin , \" Content - Based Retrieval of Historical Ottoman Documents Stored as Textual Images , \" IEEE Trans .", "label": "", "metadata": {}, "score": "67.88678"}
{"text": "European Conference on Speech Communication and Technology , Madrid , Sep. 1995 , ISSN 1018 - 4074 , pp .1087 - 1090 .M. Bates et al . , \" The BBN / HARC Spoken Language Understanding System\"--1993 IEEE , pp .", "label": "", "metadata": {}, "score": "68.145935"}
{"text": "The corresponding correction factor is subtracted from the then extant model parameters without attempting to undue all intervening corrections which may have been applied .The subset for the C(after correction ) and I ( after correction ) are identified as indicated at block 183 and correction terms are computed for all pairs in I and C as indicated at block 183 .", "label": "", "metadata": {}, "score": "68.22243"}
{"text": "It is worth noting that bigram supervector gives better result when the same UBM is used for deriving the unigram supervector .Clearly , bigram probabilities are useful but not as effective as simply increasing the UBM size to obtain unigram supervector having the same dimensionality .", "label": "", "metadata": {}, "score": "68.6043"}
{"text": "b j n ( Y P ) is an nth mixture component for the jth state that is given by : .b .j .n .Y .P . )D .i .D .i .j .", "label": "", "metadata": {}, "score": "68.89297"}
{"text": "A -valuep ARE OBSERVING A SIGNIFICANT DIFFERENCE IN THE PERFORMANCE AT A CONFIDENCE LEVEL OF 95 % .p OF MCNEMAR 'S TESTS ON THE DIFFERENCES IN THE PERFORMANCE LESS THAN 0.05 MEANS THAT WE .Fig .6 .DET curves showing a comparison of the performance of the GLDS kernel , GMM supervector , and the fusion with the proposed Bhattacharyya system . 0.2 0.5 1 2 5 10 20 40 0.2 0.5 1 2 5 10 20 40 False acceptance rate ( % )", "label": "", "metadata": {}, "score": "68.89352"}
{"text": "This idea appears problematic , however , because it is contradicted by the fact that hippocampally lesioned rats can learn nonlinear discrimination problems that require conjunctive representations .Our framework accommodates this finding by establishing a principled division of labor between the cortex and hippocampus , where the cortex is responsible for slow learning that integrates over multiple experiences to extract generalities , while the hippocampus performs rapid learning of the arbitrary contents of individual experiences .", "label": "", "metadata": {}, "score": "69.207855"}
{"text": "This idea appears problematic , however , because it is contradicted by the fact that hippocampally lesioned rats can learn nonlinear discrimination problems that require conjunctive representations .Our framework accommodates this finding by establishing a principled division of labor between the cortex and hippocampus , where the cortex is responsible for slow learning that integrates over multiple experiences to extract generalities , while the hippocampus performs rapid learning of the arbitrary contents of individual experiences .", "label": "", "metadata": {}, "score": "69.207855"}
{"text": "4 is a first part of flow chart of a process 400 for extracting feature vectors from an audio signal according to the preferred embodiment of the invention .FIGS .4 and 5 show a preferred form of block 302 of FIG .", "label": "", "metadata": {}, "score": "69.70826"}
{"text": "1577 - 1580 , 2008 .[16 ] M. A. Zissman , \" Comparison of four approaches to automatic language identification of telephone speech , \" IEEE Trans .Speech Audio Process . , vol .4 , no . 1 , pp .", "label": "", "metadata": {}, "score": "69.83157"}
{"text": "The DCT component values output for each frame by the DCT 116 , along with discrete differences of one or more orders serve to characterize the audio signal during each frame .( The DCT component values and the discrete differences are numbers . )", "label": "", "metadata": {}, "score": "70.188705"}
{"text": "A comparer and updater 154 is coupled to the inter - sentence pause detector for receiving the trigger signal .The comparer and updater 154 also coupled to the second buffer 122 for receiving feature vectors .In response to receiving the trigger signal the comparer and updater 154 reads one or more feature vectors that were extracted from the end of the inter sentence pause from the second buffer 122 .", "label": "", "metadata": {}, "score": "70.962006"}
{"text": "Theory and Implementation ( Wiley , 2009 ) .Chang Huai You received the B.Sc . degree in physics and wireless from Xiamen University , China , in communication and electronics engineering from Shanghai University of Science and Technology , China , in 1989 , and the Ph.D. degree in electrical and electronic Technological University ( NTU ) , Singapore , in 2006 .", "label": "", "metadata": {}, "score": "70.98922"}
{"text": "6 shows the DET curves .The Bhattacharyya system exhibits competitive performance compared to the other two systems , with the GMM supervector being the best .We fused the Bhattacharyya system with the other two at the score level using equal weights summation .", "label": "", "metadata": {}, "score": "71.05448"}
{"text": "57- 62 , 2004 .[ 30 ] L. Gillick and S. J. Cox , \" Some statistical issues in the comparison of speech recognition algorithms , \" in Proc .ICASSP , pp .532 - 535 , 1989 .Kong Aik Lee received the B.Eng .", "label": "", "metadata": {}, "score": "71.056595"}
{"text": "His research interests include statistical pattern classification , automatic speech recognition , speaker recognition and spoken language recognition .He has also worked on the DARPA funded EARS project from 2002 to 2005 and the GALE project from 2005 to 2006 .", "label": "", "metadata": {}, "score": "71.323006"}
{"text": "It is however also possible to train the models with an on - lineadaptive algorithm , where the models are updated after each training utterance has been processed .The notation .DELTA.u means that the utterance u is used to compute the gradient , and the operation is performed on the current model . mu .", "label": "", "metadata": {}, "score": "72.08815"}
{"text": "As a technologist , he was appointed as Research Manager in Apple - ISS Research Centre ( 1996 - 1998 ) , Research Director in Lernout & Hauspie Asia Pacific ( 1999 - 2001 ) , and Vice President in InfoTalk Corp.", "label": "", "metadata": {}, "score": "72.091705"}
{"text": "1397 - 1400 , Sep. 2008 .[ 9 ] P. Kenny , M. Mihoubi , and P. Dumouchel , \" New MAP estimates for speaker recognition , \" in Proc .EUROSPEECH , pp .2964 - 2967 , 2003 .", "label": "", "metadata": {}, "score": "72.401726"}
{"text": "omega .A complicating factor in on - line adaptation is that the identity of the input utterances is not known with certainty .Relying on the recognition system to identify the input utterances will inevitably lead to errors and misadaptations of themodels .", "label": "", "metadata": {}, "score": "72.67646"}
{"text": "The ASR system 100 may be implemented in hardware or software or a combination of the two .FIG .2 is a flow chart of a process 200 for updating a model of background noise according to the preferred embodiment of the invention .", "label": "", "metadata": {}, "score": "73.214554"}
{"text": "If the user does not make a correction , the utterance path is incremented , as indicated at block 163 , and , if there are no pending utterances , as tested at block 165,the procedure returns to the initial point to await a new utterance .", "label": "", "metadata": {}, "score": "73.4962"}
{"text": "It has not been submitted in whole or in part for a degree at any other university .Some of the work has been published previously in conference proceedings [ 66,69 ] , two ... \" .Declaration This dissertation is the result of my own work and includes nothing that is the outcome of work done in collaboration .", "label": "", "metadata": {}, "score": "73.70552"}
{"text": "Dr. You was the recipient of Silver Prize of EEE Technology Exhibition at NTU for his \" Intelligent Karaoke Project \" as a major designer and project leader in 2001 .Haizhou Li ( M'91-SM'01 ) is currently the Principal Scientist and Department Head of Human Language Technology at the Institute for Infocomm Research .", "label": "", "metadata": {}, "score": "74.04585"}
{"text": "The hippocampal component of the model contributes to recognition by recalling specific ... \" .We present a computational neural network model of recognition memory based on the biological structures of the hippocampus and medial temporal lobe cortex ( MTLC ) , which perform complementary learning functions .", "label": "", "metadata": {}, "score": "74.38731"}
{"text": "The hippocampal component of the model contributes to recognition by recalling specific ... \" .We present a computational neural network model of recognition memory based on the biological structures of the hippocampus and medial temporal lobe cortex ( MTLC ) , which perform complementary learning functions .", "label": "", "metadata": {}, "score": "74.38731"}
{"text": "The MEL scale bands are chosen in view of understood characteristics of human acoustic perception .There are preferably about 10 evenly spaced MEL scale bandpass filters below 1 KHz .Beyond 1 KHz the bandwidth of successive MEL frequency bandpass filters preferably increase by a factor of about 1.2 .", "label": "", "metadata": {}, "score": "74.71356"}
{"text": "C . k .n .N .X .n .F .i .n . k .N . k .N .where C K is a kth Fourier frequency component ; .i is the square root of negative one ; .", "label": "", "metadata": {}, "score": "74.71802"}
{"text": "Likewise , the candidate set and alignment paths for the utterance to be corrected areretrieved as indicated at block 173 .The correct and incorrect subsets are identified as indicated at block 175 and the correction term is computed for all pairs in I and C as indicated at block 179 .", "label": "", "metadata": {}, "score": "75.46587"}
{"text": "I is the score of the token and an incorrect model j. The sizes of the sets C and I can be controlled todetermine how many correct models and incorrect or potential intruder models are used in the training . O.sub.i , j takes on values near 1 when the correct model score D.sub.i is much greater ( i.e. , worse ) than the intruder score Dj , and near 0 when the converse is true .", "label": "", "metadata": {}, "score": "75.68782"}
{"text": "DELTA . sub.u.mu .( s , k , l ) .sub.u-1 for all s , k , l specifiedby alignment paths for all pairs in I.sub.top - choice and C.sub.top - choice .Update . mu .( s , k , l ) .", "label": "", "metadata": {}, "score": "76.817696"}
{"text": "claim 4 wherein the step of comparing the first characterization to a set of non - speech audio characterizations comprises the sub - steps of : . taking a dot product between the first set of numbers and each of the plurality of non - speech audio sets of numbers .", "label": "", "metadata": {}, "score": "76.82238"}
{"text": "FIG .6 is a hardware block diagram of the system 100 for performing automated speech recognition according to the preferred embodiment of the invention .As illustrated in .FIG .6 , the system 100 is a processor 602 based system that executes programs 200 , 300 , 400 that are stored in a program memory 606 .", "label": "", "metadata": {}, "score": "77.10187"}
{"text": "He works currently as a post - doctoral researcher in the University of Eastern Finland , Joensuu , Finland , and his research is funded by the Academy of Finland .His research areas cover speaker recognition and speech signal processing .", "label": "", "metadata": {}, "score": "78.23514"}
{"text": "claim 14 wherein the programming instructions for comparing the first characterization to a set of non - speech audio characterizations comprise programming instructions for : . taking a dot product between the first set of numbers and each of the plurality of non - speech audio sets of numbers .", "label": "", "metadata": {}, "score": "78.71344"}
{"text": "It can be seen that the bigram supervector gives poorer accuracy compared with the unigram supervector .This is likely due to the fact that we have significantly reduced the size of the UBM to the resulting bigram supervector has the same dimensionality as the unigram supervector .", "label": "", "metadata": {}, "score": "79.01781"}
{"text": "The power FFT components are given by the following relations : .P .N .C .P .f . k . )N .[ .C . k .C .N . k . 2 . ]", "label": "", "metadata": {}, "score": "79.11427"}
{"text": "He then received the M.Phil degree in Computer Speech , Text and Internet Technology from the same university in 2002 before joining the Machine Intelligence laboratory , Cambridge University Engineering Department in the same year as a research student .Upon completion of his Ph.D. degree in 2006 , he joined the Institute for Infocomm Research , Singapore as a research engineer .", "label": "", "metadata": {}, "score": "79.196556"}
{"text": "In step 402 an audio signal is sampled in the time domain to obtain a discretized representation of the audio signal that includes a sequence of samples .In step 404 a FIR filter is applied to the sequence of samples to emphasize high frequency components .", "label": "", "metadata": {}, "score": "80.31187"}
{"text": "Other suitable filter functions may be used for pre - emphasizing high frequency components of the discretized audio signal .A windower 108 is coupled to the FIR filter 106 for receiving the filtered discretized samples .The windower 108 multiplies successive subsets of filtered discretized samples by a discretized representation of a window function .", "label": "", "metadata": {}, "score": "80.50035"}
{"text": "1 Introduction Neural networks have been shown to have a useful degree of performance in a wide range of industrial and medical applications .However , a key factor limiting the widespread implementation of neural network solutions in many areas has been the difficulty of demonstr ... . \" ...", "label": "", "metadata": {}, "score": "80.537445"}
{"text": "1 Introduction Neural networks have been shown to have a useful degree of performance in a wide range of industrial and medical applications .However , a key factor limiting the widespread implementation of neural network solutions in many areas has been the difficulty of demonstr ... . \" ...", "label": "", "metadata": {}, "score": "80.537445"}
{"text": "The MEL scale filter bank 112 outputs a plurality of MEL scale frequency components .An mth MEL scale frequency component of the MEL scale filter bank 112 corresponding to an mth MEL bandpass filter is denoted Z(m ) .A log - magnitude evaluator 114 is coupled to the MEL scale frequency filter bank 112 for applying a composite function to each MEL scale frequency component .", "label": "", "metadata": {}, "score": "80.63925"}
{"text": "He was named one the two Nokia Visiting Professors 2009 by Nokia Foundation in recognition of his contribution to speaker and language recognition technologies .Tomi Kinnunen received the M.Sc . and Ph.D. degrees in computer science from the University of Joensuu , Finland , in 1999 and 2005 , respectively .", "label": "", "metadata": {}, "score": "81.632355"}
{"text": "The MEL scale band pass filters 112 A- 112 D preferably have a triangular profile in the frequency domain .Alternatively , the MEL scale bandpass filters 112 A- 112 D have Hamming or Hanning frequency domain profile .Each MEL bandpass filter 112 A- 112 D preferably integrates a plurality of power Fourier frequency components into a MEL scale frequency component .", "label": "", "metadata": {}, "score": "81.98825"}
{"text": "( s , k , l ) .sub.u + .omega . 'DELTA . sub .nu . mu .( s , k , l ) .sub.u end end .This procedure is illustrated in the flowchart of FIG .", "label": "", "metadata": {}, "score": "82.388016"}
{"text": "Khe Chai Sim is with the School of Computing , National University of Singapore , Singapore ( e - mail : simkc@comp.nus.edu.sg ) . speaker modeling .One reason for the popularity of SVM is its good generalization performance .The key issue in using SVM for classifying speech signals , which have a varying number of spectral vectors , is how to represent them in a suitable form as SVM can only use input of a fixed dimensionality .", "label": "", "metadata": {}, "score": "82.70439"}
{"text": "The computer system utilizes a microprocessor , designated by reference character 13 , which may , for example , be an IntelPentium type processor .The system is also provided with an appropriate amount of local or random access memory , e.g. , 32 megabytes , designated by reference character 15 .", "label": "", "metadata": {}, "score": "82.8633"}
{"text": "FOR ALL COMBINATIONS .100 % \u03c1 \u00d7 OF GAUSSIAN SELECTION TECHNIQUE AVERAGED Size of the UBM , M Size of the hash model , H Average accuracy ( \u00d7100 % ) Female 98.59 99.19 99.48 99.62 99.68 Male 98.19 99.13 99.46 99.58 99.68 1024 2048 4096 8192 16384 32 64 128 256 512 .", "label": "", "metadata": {}, "score": "82.890305"}
{"text": "claim 6 wherein the step of processing the first portion of the inter - sentence pause to obtain the first characterization of the first portion of the inter - sentence pause comprises the sub - steps of : . a ) time domain sampling the inter - sentence pause to obtain a discretized representation of the inter - sentence pause that includes a sequence of samples ; .", "label": "", "metadata": {}, "score": "83.0689"}
{"text": "A first buffer 118 is coupled to the DCT 116 for receiving successive sets of DCT component values .A differencer 120 is coupled to the first buffer 118 for receiving successive sets of DCT component values .The differencer 120 operates on two or more successive sets of component values by taking the difference between corresponding DCT component values from different sets and outputting sets of discrete differences ( including one difference for each DCT component ) of first and/or higher order , for each frame .", "label": "", "metadata": {}, "score": "84.03662"}
{"text": "Supervector Raw , p + NAP + NAP + t - norm Bhattacharyya , + NAP + NAP + t - norm % EER 8.57 7.05 6.60 7.25 5.53 4.98 .Page 7 .Accepted for IEEE T - ASL 7 estimated from a development dataset with a large number of speakers , each having several training sessions .", "label": "", "metadata": {}, "score": "84.21711"}
{"text": "This framework incorporates a theme found in many theories of hippocampal function , that the hippocampus is responsible for developing conjunctive representations binding together ... \" .We present a theoretical framework for understanding the roles of the hippocampus and neocortex in learning and memory .", "label": "", "metadata": {}, "score": "85.2566"}
{"text": "This framework incorporates a theme found in many theories of hippocampal function , that the hippocampus is responsible for developing conjunctive representations binding together ... \" .We present a theoretical framework for understanding the roles of the hippocampus and neocortex in learning and memory .", "label": "", "metadata": {}, "score": "85.2566"}
{"text": "User input to the computer system is conventionally provided by means of keyboard 25 and feedback to the user is provided by means of a CRT or other video display 27 operating from the bus through a video controller 29 .External communicationsmay be provided through an I / O system designated by reference character 31 which supports a serial port 33 and a printer 35 .", "label": "", "metadata": {}, "score": "85.62869"}
{"text": "claim 7 wherein the step of processing the first portion of the inter - sentence pause to obtain the first characterization of the first portion of the inter - sentence pause further comprises the sub - steps of : . repeating sub - steps ( a ) through ( g ) for two portions obtained from at least one inter - sentence pause to obtain two sets of numbers ; and .", "label": "", "metadata": {}, "score": "85.64041"}
{"text": "A first phoneme HMM model 140 and second phoneme HMM model 142 are illustrated .In actuality , there are many phoneme HMM models in the acoustic layer 134 .The details of phoneme HMM models will be discussed with reference to the first phoneme HMM model 140 .", "label": "", "metadata": {}, "score": "86.08148"}
{"text": "THE FUSION RESULTS WERE OBTAINED VIA LINEAR COMBINATION WITH EQUAL WEIGHTS AT THE SCORE LEVEL .Supervector Bhattacharyya ( proposed ) GLDS GMM supervector % EER 4.98 5.39 4.30 MinDCF ( \u00d7100 ) 2.46 2.67 2.10 Bhattacharyya + GLDS Bhattacharyya + GMM supervector 4.23 2.26 4.07 2.08 .", "label": "", "metadata": {}, "score": "88.02514"}
{"text": "The trigger signal is output at the end of long ( inter sentence ) pauses .According to the preferred embodiment of the invention adjustment of the non speech sound model is based on background sounds that occur near the end of inter sentence breaks in speech sound .", "label": "", "metadata": {}, "score": "88.145424"}
{"text": "A MEL scale filter bank 112 is coupled to the FFT 110 for receiving the power Fourier frequency components .The MEL scale filter bank includes a plurality of MEL scale band pass filters 112 A , 112 B , 112 C , 112 D ( four of which are shown ) .", "label": "", "metadata": {}, "score": "88.18805"}
{"text": "The A / D 612 is coupled to the audio signal input 102 that preferably comprises a microphone .In operation the audio signal is input at the audio signal input 102 converted to the above mentioned discretized representation of the audio signal by the A / D 612 which operates under the control of the processor 602 .", "label": "", "metadata": {}, "score": "88.294"}
{"text": "The background sound model 156 includes a first state 158 that is non - emitting , and a final state 160 that is non - emitting .An emitting state 146 is located between the first 158 and final 160 states .", "label": "", "metadata": {}, "score": "88.50855"}
{"text": "Nlm Unique ID : 8700181 Medline TA : Proteins Country : - .Other Details : .Languages : ENG Pagination : - Citation Subset : - .Affiliation : .Department of Protein Evolution , Max Planck Institute for Developmental Biology , Spemannstr .", "label": "", "metadata": {}, "score": "91.490005"}
{"text": "1 , the computer system illustrated there is of the type generally referred to as a personal computer .The computer runs under the MS DOS or WINDOWS.RTM . operating system and is organized around a system bus , designatedgenerally by reference character 11 .", "label": "", "metadata": {}, "score": "92.68826"}
{"text": "Input 0.64 0.811.0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Output Fig . 1 .The horizontal axis is warped according to a square - root function .00.050.100.150.200.250.300.350.40 0.450.50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Input ( \u00d710 - 2 ) ( a ) Output 0 0.050.10 0.150.200.25 0.300.350.400.45 0.50 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Input ( \u00d710 - 2 ) ( b ) Output Fig . 2 .", "label": "", "metadata": {}, "score": "92.85481"}
{"text": "( s , k , l ) .sub.u-1 + .omega . 'DELTA . sub.u.mu .( s , k , l ) .sub.u-1 Save candidate list ( including alignment paths ) for u if user corrects result forutterance .", "label": "", "metadata": {}, "score": "93.054276"}
{"text": "nu . mu .( s , k , l ) .sub.u for all s , k , l specified by alignment paths for all pairs in I.sub.corrected and C.sub.corrected .Update . mu .( s , k , l ) .", "label": "", "metadata": {}, "score": "93.24009"}
{"text": "In process block 210 the particular mean found in process block 208 is updated so that it is closer to the characteristic feature vector extracted in block 206 .From block 210 the process 200 loops back to block 202 .FIG .", "label": "", "metadata": {}, "score": "93.97492"}
{"text": "From 1992 to 1998 , he was an Engineering Specialist with Seagate International , Singapore .He joined the Centre for Signal Processing , NTU , as a Research Engineer in 1998 and became a Senior Research Engineer in 2001 .In 2002 , he joined the Agency for Science , Technology , and Research , Singapore , and was appointed as a Member of Associate Research Staff .", "label": "", "metadata": {}, "score": "94.01999"}
{"text": "Proteins 2012 .\u00a9 2012 Wiley Periodicals , Inc. .Title : Proteins Volume : - ISSN : 1097 - 0134 ISO Abbreviation : Proteins Publication Date : 2013 Jan .Date Detail : .Created Date : 2013 - 1 - 7 Completed Date : - Revised Date : - .", "label": "", "metadata": {}, "score": "94.92165"}
{"text": "( s , k , l ) .mu .( s , k , l ) .sub.u - . omega . 'DELTA . sub .nu . mu .( s , k , l ) .sub.u Identify subsets I.sub.corrected and C.sub.corrected Compute .", "label": "", "metadata": {}, "score": "95.52078"}
{"text": "He taught in the University of Hong Kong ( 1988- 1990 ) , South China University of Technology ( 1990 - 1994 ) , Technology department , 1986 , the M.Eng . degree in engineering from Nanyang and Nanyang Technological University ( 2006-present ) .", "label": "", "metadata": {}, "score": "96.53275"}
{"text": "( e - mail : kalee@i2r.a-star.edu.sg ; echyou@i2r.a- star.edu.sg ; hli@i2r.a-star.edu.sg ) .The work of Haizhou Li was partially supported by Nokia Foundation .Tomi Kinnunen is with the School of Computing , University of Eastern Finland , Finland ( e - mail : tkinnu@cs.joensuu.fi ) .", "label": "", "metadata": {}, "score": "96.74127"}
{"text": "The processor 602 , program memory 606 , a workspace memory 604 , e.g. Random Access Memory ( RAM ) , and input / output ( I / O ) interface 610 are coupled together through a digital signal bus 608 .", "label": "", "metadata": {}, "score": "99.38743"}
{"text": "Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .Publisher conditions are provided by RoMEO .Differing provisions from the publisher 's actual policy or licence agreement may be applicable .", "label": "", "metadata": {}, "score": "107.802704"}
{"text": "Copyright ( c ) 2010 IEEE .Personal use of this material is permitted .However , permission to use this material for any other purposes must be obtained from the IEEE by sending a request to pubs-permissions@ieee.org .Manuscript received December 11 , 2009 .", "label": "", "metadata": {}, "score": "120.27396"}
