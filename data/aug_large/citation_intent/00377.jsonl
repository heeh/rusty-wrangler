{"text": "204 - 208 .Haas , P.J. , Naughton , J.F. , Seshadri , S. , and Stokes , L. 1995 .Sampling - based estimation of the number of distinct values of an attribute .In Proceedings of the Eighth International Conference on Very Large Databases ( VLDB ) .", "label": "", "metadata": {}, "score": "34.127937"}
{"text": "Kohavi , R. ( 1995a ) .A study of cross - validation and bootstrap for accuracy estimation and model selection .In C.S. Mellish ( Ed . ) , Proceedings of the 14th International Joint Conference on Artificial Intelligence ( pp .", "label": "", "metadata": {}, "score": "34.37857"}
{"text": "Dietterich and Bakiri introduced ECOC to be used within the ensemble setting ( Dietterich 1995 ) .The idea is to use a different class encoding for each member of the ensemble .The encodings constitute a binary C by T code matrix , where C and T are the number of classes and ensemble size , respectively , combined by the minimum Hamming distance rule .", "label": "", "metadata": {}, "score": "34.71483"}
{"text": "6 , pp .153 - 180 .Cheeseman , P. , Kelly , J. , Self , M. , Stutz , J. , Taylor , W. , and Freeman , D. 1988 .Autoclass : A bayesian classification system .In Proceedings of the Fifth International Conference on Machine Learning .", "label": "", "metadata": {}, "score": "34.843063"}
{"text": "In this case , the classifier combination involves merging the individual ( usually weaker and/or diverse ) classifiers to obtain a single ( stronger ) expert of superior performance .Examples of this approach include bagging predictors ( Breiman 1996 ) , boosting ( Schapire 1990 ) , AdaBoost ( Freund 2001 ) and their many variations .", "label": "", "metadata": {}, "score": "35.44199"}
{"text": "For example , a series of multilayer perceptron ( MLP ) neural networks can be trained by using different weight initializations , number of layers / nodes , error goals , etc .Adjusting such parameters allows one to control the instability of the individual classifiers , and hence contribute to their diversity .", "label": "", "metadata": {}, "score": "35.755272"}
{"text": "A simple pattern - matching algorithm for recovering empty nodes and their antecedents .In ACL 2002 : Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 136 - 143 .Johnson , M. , Geman , S. , Canon , S. , Chi , Z. , and Riezler , S. ( 1999 ) .", "label": "", "metadata": {}, "score": "35.790817"}
{"text": "Log - Linear Models and Logistic Regression , 2nd ed .Springer .Corruble , V. , Brown , D.E. , and Pittard , C.L. 1993 .A comparison of decision classifiers with backpropagation neural networks for multimodal classification problems .Pattern Recognition , 26:953 - 961 .", "label": "", "metadata": {}, "score": "35.97438"}
{"text": "430 - 437 ) .Morgan Kaufmann .Pazzani , M. , Merz , C. , Murphy , P. , Ali , K. , Hume , T. , & Brunk , C. ( 1994 ) .Reducing misclassification costs .Machine Learning : Proceedings of the Eleventh International Conference .", "label": "", "metadata": {}, "score": "36.255684"}
{"text": "Ibarra , O.H. and Kim , C.E. 1975 .Fast approximation algorithms for the knapsack and sum of subsets problem .Journal of the ACM , 22:463 - 468 .CrossRef MathSciNet .Inman , W.H. 1996 .The data warehouse and data mining .", "label": "", "metadata": {}, "score": "36.308258"}
{"text": "In : Proceedings of fourteenth international conference machine learning , pp 211 - 218 .Melville P , Mooney RJ ( 2003 )Constructing diverse classifier ensembles using artificial training examples .IJCAI 505 - 512 .Merler S , Caprile B , Furlanello C ( 2007 ) Parallelizing AdaBoost by weights dynamics .", "label": "", "metadata": {}, "score": "36.59604"}
{"text": "Maxwell , J. T. and Kaplan , R. M. ( 1996 ) .Unification - based parsers that automati- cally take advantage of context freeness .In LFG 1996 : Proceedings of the Lexical Functional Grammar Conference .McCallum , A. , Freitag , D. , and Pereira , F. ( 2000 ) .", "label": "", "metadata": {}, "score": "36.76595"}
{"text": "Curram , S.P. and Mingers , J. 1994 .Neural networks , decision tree induction and discriminant analysis : An empirical comparison .Journal of the Operational Research Society , 45:440 - 450 .Dougherty , J. , Kahove , R. , and Sahami , M. 1995 .", "label": "", "metadata": {}, "score": "36.935726"}
{"text": "In his 2000 review article , Dietterich lists three primary reasons for using an ensemble based system : i ) statistical ; ii ) computational ; and iii ) representational ( Dietterich 2000 ) .Note that these reasons are similar to those listed above .", "label": "", "metadata": {}, "score": "36.936035"}
{"text": "Kohavi , R. , & Sahami , M. ( 1996 ) .Error - based and entropy - based discretization of continuous features .Proceedings of the Second International Conference on Knowledge Discovery and Data Mining ( pp .114 - 119 ) .", "label": "", "metadata": {}, "score": "36.990074"}
{"text": "We introduce a nearest neighbor algorithm for learning in domains with symbolic features .Our algorithm calculates distance tables that allow it to produce real - valued distances between instances , and attaches weights to the instances to further modify the structure of feature space .", "label": "", "metadata": {}, "score": "37.168854"}
{"text": "Z. .SCHAPIRE , R. 1990 .The strength of weak learnability .Machine Learning 5 197 227 .Z. .SIMARD , P. , LE CUN , Y. and DENKER , J. 1993 .Efficient pattern recognition using a new transformation distance .", "label": "", "metadata": {}, "score": "37.315582"}
{"text": "Elsevier Science Publishers .Langley , P. , Iba , W. , & Thompson , K. ( 1992 ) .An analysis of Bayesian classifiers .Proceedings of the Tenth National Conference on Artificial Intelligence ( pp .223 - 228 ) .", "label": "", "metadata": {}, "score": "37.345947"}
{"text": "The outputs of these classifiers on their pseudo - training blocks , along with the actual correct labels for those blocks constitute the training dataset for the Tier 2 classifier ( see Figure 7 ) .Jordan and Jacobs ' mixture of experts ( Jacobs 1991 ) generates several experts ( classifiers ) whose outputs are combined through a ( generalized ) linear rule .", "label": "", "metadata": {}, "score": "37.38034"}
{"text": "The analysis is done by interpreting a protocol as a .. \" ...This article describes an approach to combining symbolic and connectionist approaches to machine learning .A three - stage framework is presented and the research of several groups is reviewed with respect to this framework .", "label": "", "metadata": {}, "score": "37.465195"}
{"text": "In this paper , C4.5 decision tree classification method is used to build an effective decision tree for intrusion detection , then convert the decision tree into rules and save them into the knowledge base of intrusion detection system .These rules are used to judge whether the new network behavior is normal or abnormal .", "label": "", "metadata": {}, "score": "37.965893"}
{"text": "Finally , by studying neural networks in addition to decision trees we can examine how Bagging and Boosti ... . by Paul E. Utgoff , Neil C. Berkman , Jeffery A. Clouse , Doug Fisher - Machine Learning , 1996 .The ability to restructure a decision tree efficiently enables a variety of approaches to decision tree induction that would otherwise be prohibitively expensive .", "label": "", "metadata": {}, "score": "38.416645"}
{"text": "For a detailed overview of these and other combination rules , see ( Kuncheva 2005 ) .Other applications of ensemble systems .Ensemble based systems can be used in problem domains other than improving the generalization performance of a classifier .", "label": "", "metadata": {}, "score": "38.479027"}
{"text": "James , M. 1985 .Classification Algorithms .Wiley .Kerber , R. 1991 .Chimerge discretization of numeric attributes .In Proceedings of the 10th International Conference on Artificial Intelligence , pp .123 - 128 .Kohavi , R. 1995 .", "label": "", "metadata": {}, "score": "38.5383"}
{"text": "Computer Systems that Learn : Classification and Prediction Methods from Statistics , Neural Nets , Machine Learning , and Expert Systems .Morgan Kaufman .Zighed , D.A. , Rakotomalala , R. , and Feschet , F. 1997 .Optimal multiple intervals discretization of continous attributes for supervised learning .", "label": "", "metadata": {}, "score": "38.607925"}
{"text": "Kolen JF , Pollack JB ( 1991 )Back propagation is sesitive to initial conditions .In : Advances in neural information processing systems , vol 3 .Morgan Kaufmann , San Francisco , pp 860 - 867 .Krogh A , Vedelsby J ( 1995 ) Neural network ensembles , cross validation and active learning .", "label": "", "metadata": {}, "score": "38.674896"}
{"text": "Inf Fusion 6(1 ) : 5 - 20 CrossRef .Buchanan BG , Shortliffe EH ( 1984 ) Rule based expert systems .Addison - Wesley , Reading 272 - 292 .Buntine W ( 1990 )A theory of learning classification rules .", "label": "", "metadata": {}, "score": "38.728394"}
{"text": "Kohavi , R. , & Wolpert , D.H. ( 1996 ) .Bias plus variance decomposition for zero - one loss functions .In L. Saitta ( Ed . ) , Machine Learning : Proceedings of the Thirteenth International Conference ( pp .", "label": "", "metadata": {}, "score": "38.84328"}
{"text": "This method is based on Principal Component Analysis ( PCA ) .The work present two well known algorithms , decision trees and nearest neighbor , and the authors show the contribution of their approach to alleviate the decision process .[ 4 ] .", "label": "", "metadata": {}, "score": "38.86953"}
{"text": "Brachman , R.J. , Khabaza , T. , Kloesgen , W. , Shapiro , G.P. , and Simoudis , E. 1996 .Mining business databases .Communications of the ACM , 39(11):42 - 48 .CrossRef .Bishop , C.M. 1995 .", "label": "", "metadata": {}, "score": "38.96884"}
{"text": "Cheeseman , P. and Stutz , J. 1996 .Bayesian classification ( autoclass ) : Theory and results .In Advances in Knowledge Discovery and Data Mining , U.M. Fayyad , G.P. Shapiro , P. Smyth , and R. Uthurusamy ( Eds . )", "label": "", "metadata": {}, "score": "38.986614"}
{"text": "For n training instances held in memory , the best - known SVM implementations take time proportional to na , where a is typically between 1.8 and 2.1 .Na\u00c3\u00afve Bayes .These classifiers work a generative framework in which each document is generated by a parametric distribution governed by a set of hidden parameters .", "label": "", "metadata": {}, "score": "39.012573"}
{"text": "Advances in neural information processing systems 8 ' ( pp .479 - 485 ) .Duda , R. , & Hart , P. ( 1973 ) .Pattern classification and scene analysis .Wiley .Efron , B. , & Tibshirani , R. ( 1993 ) .", "label": "", "metadata": {}, "score": "39.015812"}
{"text": "This article describes an approach to combining symbolic and connectionist approaches to machine learning .A three - stage framework is presented and the research of several groups is reviewed with respect to this framework .The first stage involves the insertion of symbolic knowledge into neural networks , the second addresses the refinement of this prior knowledge in its neural representation , while the third concerns the extraction of the refined symbolic knowledge .", "label": "", "metadata": {}, "score": "39.079967"}
{"text": "945 - 954 , 1995 .K. Woods , W. P. J. Kegelmeyer , and K. Bowyer , \" Combination of multiple classifiers using local accuracy estimates , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .19 , no .", "label": "", "metadata": {}, "score": "39.13339"}
{"text": "The algorithm is independent of specific applications so that many ideas and solutions can be transferred to other classifier paradigms .[ 13 ] .Neural Networks .Rule Based Classification .The Association Rules are canonical data mining taking aim at discovering relationships between items in the dataset .", "label": "", "metadata": {}, "score": "39.198505"}
{"text": "Mining scientific data .Communications of the ACM , 39(11 ) .Fayyad , U.M. and Irani , K. 1993 .Multi - interval discretization of continous - valued attributes for classification learning .In Proceedings of the 13th International Joint Conference on Artificial Intelligence .", "label": "", "metadata": {}, "score": "39.322487"}
{"text": "PAMI 19 1300 1306 .5 BREIMAN , L. , FRIEDMAN , J. , OLSHEN , R. and STONE , C. 1984 .Classification and Regression Trees .Wadsworth , Belmont , CA . 7 FRIEDMAN , J. H. 1973 .A recursive partitioning decision rule for nonparametric classification .", "label": "", "metadata": {}, "score": "39.479324"}
{"text": "AAAI Press .Domingos , P. , & Pazzani , M. ( 1997 ) .Beyond independence : Conditions for the optimality of the simple Bayesian classifier .Machine Learning , 29 ( 2/3 ) , 103 - 130 .Drucker , H. , & Cortes , C. ( 1996 ) .", "label": "", "metadata": {}, "score": "39.527298"}
{"text": "6 , no.3 , pp .21 - 45 , 2006 .Abstract .Classification of large datasets is an important data mining problem .Many classification algorithms have been proposed in the literature , but studies have shown that so far no algorithm uniformly outperforms all other algorithms in terms of quality .", "label": "", "metadata": {}, "score": "39.637657"}
{"text": "The code vectors in a region are trained and test instances are approximated against the code vectors .MLP .RBF .Markov Modeling .Within this category , we may distinguish two main approaches : Markov chains and hidden Markov models .", "label": "", "metadata": {}, "score": "39.77217"}
{"text": "The algorithms described above have their built in combination rules , such as simple majority voting for bagging , weighted majority voting for AdaBoost , a separate classifier for stacking , etc .However , an ensemble of classifiers can be trained simply on different subsets of the training data , different parameters of the classifiers , or even with different subsets of features as in random subspace models .", "label": "", "metadata": {}, "score": "39.787903"}
{"text": "Direct experimental comparisons with the other learning algorithms show that our nearest neighbor algorithm is comparable or superior ... . by David Opitz , Richard Maclin - Journal of Artificial Intelligence Research , 1999 . \" ...An ensemble consists of a set of individually trained classifiers ( such as neural networks or decision trees ) whose predictions are combined when classifying novel instances .", "label": "", "metadata": {}, "score": "39.804874"}
{"text": "D\u017eeroski S , \u017denko B ( 2004 )Is combining classifiers with stacking better than selecting the best one ?Mach Learn 54(3 ) : 255 - 273 MATH CrossRef .Dietterich TG , Bakiri G ( 1995 ) Solving multiclass learning problems via error - correcting output codes .", "label": "", "metadata": {}, "score": "39.97168"}
{"text": "4 , pp .497 - 508 , 2001 .R. Polikar , \" Bootstrap inspired techniques in computational intelligence : ensemble of classifiers , incremental learning , data fusion and missing features , IEEE Signal Processing Magazine , v. 24 , no .", "label": "", "metadata": {}, "score": "40.157192"}
{"text": "Finally , diversity can also be achieved by using different features , or different subsets of existing features .In fact , generating different classifiers using random feature subsets is known as the random subspace method ( Ho 1998 ) , as described later in this article .", "label": "", "metadata": {}, "score": "40.29869"}
{"text": "MICHIE , D. , SPIEGELHALTER , D. and TAy LOR , C. 1994 .Machine Learning , Neural and Statistical Classification .Ellis Horwood , London .Z. .QUINLAN , J. R. 1996 .Bagging , Boosting , and C4.5 .", "label": "", "metadata": {}, "score": "40.308044"}
{"text": "In this paper , after analyzing the problem and reviewing our risk estimation method , we create a decision tree to distinguish between high risk and normal situations .To evaluate our methodology , test and training datasets were collected during a large mobile - phone field study for a location - aware application .", "label": "", "metadata": {}, "score": "40.441143"}
{"text": "In Proceedings of the IEEE Tools on AI .Loh , W.-Y. and Shih , Y.-S. Split selection methods for classification trees .Statistica Sinica , 7(4):815 - 840 .MathSciNet .Loh , W.-Y. and Vanichsetakul , N. 1988 .Tree - structured classification via generalized disriminant analysis ( with discussion ) .", "label": "", "metadata": {}, "score": "40.53685"}
{"text": "405 - 410 , 1997 .S. B. Cho and J. H. Kim , \" Combining multiple neural networks by fuzzy integral for robust classification , \" IEEE Transactions on Systems , Man and Cybernetics , vol .25 , no . 2 , pp .", "label": "", "metadata": {}, "score": "40.60211"}
{"text": "T.K. Ho , \" Complexity of classification problems and comparative advantages of combined classifiers , \" Int .Workshop on Multiple Classifier Systems , lecture Notes on Computer Science , Vol .1857 , pp .97 - 106 , 2000 , Springer- Verlag . F. Roli , G. Giacinto , \" Design of Multiple Classifier Systems , \" in H. Bunke and A. Kandel ( Eds . )", "label": "", "metadata": {}, "score": "40.625694"}
{"text": "Sha , F. and Pereira , F. ( 2003 ) .Shallow parsing with Conditional Random Fields .In NAACL 2003 : Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology , pages 134 - 141 .", "label": "", "metadata": {}, "score": "40.701195"}
{"text": "NEAL , R. 1993 .Probabilistic inference using Markov chain Monte Carlo methods .Technical Report CRG - TR-93 - 1 , Dept .Computer Science , Univ .Toronto . Z. .PERRONE , M. P. and COOPER , L. N. 1993 .", "label": "", "metadata": {}, "score": "40.8902"}
{"text": "PCA is a common statistical method used in multivariate optimization problems in order to reduce the dimensionality of data while retaining a large fraction of the data characteristic .First , PCA is used to project the training set onto eigenspace vectors representing the mean of the data .", "label": "", "metadata": {}, "score": "40.964695"}
{"text": "10 , pp .993 - 1001 , 1990 .R. E. Schapire , \" The Strength of Weak Learnability , \" Machine Learning , vol .5 , no . 2 , pp .197 - 227 , 1990 .R. A. Jacobs , M. I. Jordan , S. J. Nowlan , and G. E. Hinton , \" Adaptive mixtures of local ex - perts , \" Neural Computation , vol .", "label": "", "metadata": {}, "score": "40.9713"}
{"text": "MDL - based decision tree pruning .In Proc . of the 1st Int'l Conference on Knowledge Discovery in Databases and Data Mining , Montreal , Canada .Michie , D. , Spiegelhalter , D.J. , and Taylor , C.C. 1994a .", "label": "", "metadata": {}, "score": "41.057632"}
{"text": "Approximate statistical tests for comparing supervised classification learning algorithms .Neural Computation , 10 ( 7 ) .Dietterich , T.G. , & Bakiri , G. ( 1991 ) .Error - correcting output codes : A general method for improving multiclass inductive learning programs .", "label": "", "metadata": {}, "score": "41.05775"}
{"text": "79 - 87 , 1991 .M. J. Jordan and R. A. Jacobs , \" Hierarchical mixtures of experts and the EM algorithm , \" Neural Computation , vol .6 , no . 2 , pp .181 - 214 , 1994 . D. H. Wolpert , \" Stacked generalization , \" Neural Networks , vol .", "label": "", "metadata": {}, "score": "41.10727"}
{"text": "Morgan Kaufmann .Kong , E.B. , & Dietterich , T.G. ( 1995 ) .Error - correcting output coding corrects bias and variance .In A. Prieditis & S. Russell ( Eds . ) , Machine Learning : Proceedings of the Twelfth International Conference ( pp .", "label": "", "metadata": {}, "score": "41.18049"}
{"text": "418 - 435 , 1992 . E. Allwein , R. E. Schapire , and Y. Singer , \" Reducing Multiclass to Binary : A Unifying Approach for Margin Classifiers , \" Journal of Machine Learning Research , vol .1 , pp .", "label": "", "metadata": {}, "score": "41.19612"}
{"text": "Experiments comparing the ID3 symbolic learning algorithm with ... \" .Abstract Despite the fact that many symbolic and neural network ( connectionist ) learning algorithms address the same problem of learning from classified examples , very little is known regarding their comparative strengths and weaknesses .", "label": "", "metadata": {}, "score": "41.24036"}
{"text": "Thus , we conclude that estimating the information entropy and the relevant inference risk using a pre - processor can yield a simpler and more accurate classification tree . ... w inferences as threats to information security or database confidentiality [ 7 , 8].", "label": "", "metadata": {}, "score": "41.270317"}
{"text": "An empirical study of building compact ensembles .WAIM 622 - 627 .Long C ( 2003 )Bi - decomposition of function sets using multi - valued logic , Eng Doc Dissertation , Technischen Universitat Bergakademie Freiberg .Maimon O , Rokach L ( 2005 ) Decomposition methodology for knowledge discovery and data mining : theory and applications .", "label": "", "metadata": {}, "score": "41.290253"}
{"text": "In this paper we evaluate these methods on 23 data sets using both neural networks and decision trees as our classification algorithm .Our results clearly indicate a number of conclusions .First , while Bagging is almost always more accurate than a single classifier , it is sometimes much less accurate than Boosting .", "label": "", "metadata": {}, "score": "41.30724"}
{"text": "These heterogeneous features can not be used all together to train a single classifier ( and even if they could - by converting all features into a vector of scalar values - such a training is unlikely to be successful ) .", "label": "", "metadata": {}, "score": "41.493973"}
{"text": "Such a set of classifiers is said to be diverse .Classifier diversity can be achieved in several ways .Preferably , the classifier outputs should be class - conditionally independent , or better yet negatively correlated .The most popular method is to use different training datasets to train individual classifiers .", "label": "", "metadata": {}, "score": "41.528225"}
{"text": "Papers [ 11 ] and[10 ] were concernedwith reducing false alerts by correlating intrusion alerts and limiting the sc ...Ensemble learning is the process by which multiple models , such as classifiers or experts , are strategically generated and combined to solve a particular computational intelligence problem .", "label": "", "metadata": {}, "score": "41.53953"}
{"text": "Machine Learning , 11 , 63 - 90 .Iba , W. , & Langley , P. ( 1992 ) .Induction of one - level decision trees .Proceedings of the Ninth International Conference on Machine Learning ( pp .233 - 240 ) .", "label": "", "metadata": {}, "score": "41.66321"}
{"text": "Symbolic and neural learning algorithms : An empirical comparison .Machine Learning , 6:111 - 144 .Sonquist , J.A. , Baker , E.L. , and Morgan , J.N. 1971 .Searching for structure .Technical Report , Institute for Social Research , University of Michigan , Ann Arbor , Michigan .", "label": "", "metadata": {}, "score": "41.68257"}
{"text": "Z. .TIBSHIRANI , R. 1996 .Bias , variance , and prediction error for classification rules .Technical Report , Dept .Statistics , Univ .Toronto . Z. .VAPNIK , V. 1995 .The Nature of Statistical Learning Theory .", "label": "", "metadata": {}, "score": "41.724373"}
{"text": "Ridgeway , G. , Madigan , D. , & Richardson , T. ( 1998 ) .Interpretable boosted naive bayes classification .Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining .Schaffer , C. ( 1994 ) .", "label": "", "metadata": {}, "score": "41.75077"}
{"text": "Although this maximum gain incurs more degradation of false alarm rate performance , the resulting performance is still reasonable .This essay is an example of a student 's work .Disclaimer .Another Markov Model i.e. Service Specification and Stochastic Markovian modeling \" ( S3 M ) is also applied to IDS [ 2].", "label": "", "metadata": {}, "score": "41.76294"}
{"text": "Memory - based morphological analy- sis .In ACL 1999 : Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics , pages 285 - 292 .van der Beek , L. , Bouma , G. , Malouf , R. , and van Noord , G. ( 2002 ) .", "label": "", "metadata": {}, "score": "41.89288"}
{"text": "119 - 139 , 1997 .J. Kittler , M. Hatef , R. P. W. Duin , and J. Mates , \" On combining classifiers , \" IEEE Trans . on Pattern Analysis and Machine Intelligence , vol .20 , no . 3 , pp .", "label": "", "metadata": {}, "score": "41.939903"}
{"text": "Mitchell T ( 1980 )The need for biases in learning generalizations .Technical Report CBM - TR-117 , Rutgers University , Department of Computer Science , New Brunswick .Nowlan SJ , Hinton GE ( 1991 ) Evaluation of adaptive mixtures of competing experts .", "label": "", "metadata": {}, "score": "41.94224"}
{"text": "Prodromidis AL , Stolfo SJ ( 2001 )Cost complexity - based pruning of ensemble classifiers .Knowl Inf Syst 3(4 ) : 449 - 469 MATH CrossRef .Provost FJ , Kolluri V ( 1997 )A survey of methods for scaling up inductive learning algorithms .", "label": "", "metadata": {}, "score": "41.94651"}
{"text": "DIETTERICH , T. G. and BAKIRI , G. 1995 .Solving multiclass learning problems via error - correcting output codes .J. Artificial Intelligence Res .Z. .DIETTERICH , T. G. and KONG , E. B. 1995 .Machine learning bias , statistical bias , and statistical variance of decision tree algorithms .", "label": "", "metadata": {}, "score": "41.996284"}
{"text": "Edinburgh University Press : Edinburgh , UK .Quinlan , J.R. 1983 .Learning efficient classification procedures .In Machine Learning : An Artificial Intelligence Approach , T.M. Mitchell , R.S. Michalski , and J.G. Carbonell ( Eds . )Palo Alto , CA : Tioga Press .", "label": "", "metadata": {}, "score": "42.051388"}
{"text": "Using output codes to boost multiclass learning problems .In Proceedings of the Fourteenth International Conference on Machine Learning 313 321 .Morgan Kaufmann , San Francisco .CORVALLIS , OREGON 97331 - 3202 E - MAIL : tgd@cs.orst.edu .AMIT , Y. and GEMAN , D. 1997 .", "label": "", "metadata": {}, "score": "42.37259"}
{"text": "MIT Press .Tsochantaridis , I. , Joachims , T. , Hofmann , T. , and Altun , Y. ( 2005 ) .Large margin methods for structured and interdependent output variables .Journal of Machine Learning Research , 6:1453 - 1484 .", "label": "", "metadata": {}, "score": "42.37684"}
{"text": "Other applications of ensemble learning include assigning a confidence to the decision made by the model , selecting optimal ( or near optimal ) features , data fusion , incremental learning , nonstationary learning and error - correcting .This article focuses on classification related applications of ensemble learning , however , all principle ideas described below can be easily generalized to function approximation or prediction type problems as well .", "label": "", "metadata": {}, "score": "42.396183"}
{"text": "Brodley CE ( 1995 ) Recursive automatic bias selection for classifier construction .Mach Learn 20 : 63 - 94 .Bryll R , Gutierrez - Osuna R , Quek F ( 2003 )Bagging : improving accuracy of classifier ensembles by using random feature subsets .", "label": "", "metadata": {}, "score": "42.402615"}
{"text": "Our results show that the achieved true classification rates are significantly better than approaches that employ other available features for the internal nodes of the trees .The results also suggest that common classification tools can not accurately capture the information entropy for social applications .", "label": "", "metadata": {}, "score": "42.418556"}
{"text": "L. I. Kuncheva , J. C. Bezdek , and R. Duin , \" Decision templates for multiple classifier fu - sion : an experimental comparison , \" Pattern Recognition , vol .34 , no . 2 , pp .299 - 314 , 2001 .", "label": "", "metadata": {}, "score": "42.428658"}
{"text": "Quinlan , J.R. ( 1993 ) .C4.5 : programs for machine learning .San Mateo , California : Morgan Kaufmann .Quinlan , J.R. ( 1994 ) .Comparing connectionist and symbolic learning methods .In S.J. Hanson , G.A. Drastal , & R.L. Rivest ( Eds . ) , Computational learning theory and natural learning systems ( Vol .", "label": "", "metadata": {}, "score": "42.444954"}
{"text": "IEEE Transactions on Computers , 26:404 - 408 .MATH .Fukuda , T. , Morimoto , Y. , and Morishita , S. 1996 .Constructing efficient decision trees by using optimized numeric association rules .In Proceedings of the 22nd VLDB Conference .", "label": "", "metadata": {}, "score": "42.45011"}
{"text": "L. I. Kuncheva , \" A theoretical study on six classifier fusion strategies , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .24 , no . 2 , pp .281 - 286 , 2002 .S. B. Cho and J. H. Kim , \" Multiple network fusion using fuzzy logic , \" IEEE Transactions on Neural Networks , vol .", "label": "", "metadata": {}, "score": "42.47753"}
{"text": "Mehta , M. , Agrawal , R. , and Rissanen , J. 1996 .SLIQ :A fast scalable classifier for data mining .In Proc . of the Fifth Int'l Conference on Extending Database Technology ( EDBT ) , Avignon , France .", "label": "", "metadata": {}, "score": "42.480713"}
{"text": "However , there is much to be gained by utilizing the capability to reason nearly correctly .In the presented EBL - ANN algorithm , a \" roughly - correct \" explanatory capability leads to the acquisition of a classification rule that is almost correct .", "label": "", "metadata": {}, "score": "42.50994"}
{"text": "Their problem is high variance .Combining multiple versions either through bagging or arcing reduces variance significantly .DRUCKER , H. and CORTES , C. 1996 .Boosting decision trees .Advances in Neural Information Processing Sy stems 8 479 485 .", "label": "", "metadata": {}, "score": "42.538925"}
{"text": "School of Computing Science , University of Technology .Sydney .Australia .Buntine W ( 1996 )Graphical models for discovering knowledge .In : Fayyad U , Piatetsky - Shapiro G , Smyth P , Uthurusamy R ( eds ) Advances in knowledge discovery and data mining .", "label": "", "metadata": {}, "score": "42.701912"}
{"text": "23 - 37 ) .Springer - Verlag , To appear in Journal of Computer and System Sciences .Freund , Y. , & Schapire , R.E. ( 1996 ) .Experiments with a new boosting algorithm .In L. Saitta ( Ed . ) , Machine Learning : Proceedings of the Thirteenth National Conference ( pp .", "label": "", "metadata": {}, "score": "42.756924"}
{"text": "[ 12 ] .Nearest Neighbor .The k - nearest neighbors algorithm ( k - NN ) is a method for classifying objects based on closest training examples in the feature space .k - NN is a type of instance - based learning , or lazy learning where the function is only approximated locally and all computation is deferred until classification .", "label": "", "metadata": {}, "score": "42.779175"}
{"text": "24 , no . 2 , pp .123 - 140 , 1996 .Y. Freund and R. E. Schapire , \" Decision - theoretic generalization of on - line learning and an application to boosting , \" Journal of Computer and System Sciences , vol .", "label": "", "metadata": {}, "score": "42.852066"}
{"text": "The gold - standard Cast3LB function labels are shown in the first row , the predicted tags in the first column .So e.g. suj was mistagged as cd in 26 cases .Page 168 . based learning .In ACL 2004 : Proceedings of the 42nd Annual Meeting of the Asso- ciation for Computational Linguistics , pages 311 - 318 .", "label": "", "metadata": {}, "score": "42.91809"}
{"text": "Morgan Kaufmann .Kohavi , R. ( 1995b ) .Wrappers for performance enhancement and oblivious decision graphs .Ph.D. thesis , Stanford University , Computer Science department .STAN - CS - TR-95 - 1560 .Kohavi , R. , Becker , B. , & Sommerfield , D. ( 1997 ) .", "label": "", "metadata": {}, "score": "43.015587"}
{"text": "Nor an improvement on the ensemble 's average performance can be guaranteed except for certain special cases ( Fumera 2005 ) .Hence combining classifiers may not necessarily beat the performance of the best classifier in the ensemble , but it certainly reduces the overall risk of making a particularly poor selection .", "label": "", "metadata": {}, "score": "43.06611"}
{"text": "ACM Press , New York .Z. .KOHAVI , R. and WOLPERT , D. H. 1996 .Bias plus variance decomposition for zero - one loss functions .In Machine Learning : Proceedings of the Thirteenth International ConferZ. ence .L. Saitta , ed .", "label": "", "metadata": {}, "score": "43.154423"}
{"text": "Feature selection for ensembles .In : Proceedings of 16th National Conference on Artificial Intelligence , AAAI .pp 379 - 384 .Opitz D , Shavlik J ( 1996 )Generating accurate and diverse members of a neural\u00e11network ensemble .In : Touretzky DS , Mozer MC , Hasselmo ME ( eds ) Advances in neural information processing systems , vol 8 .", "label": "", "metadata": {}, "score": "43.19052"}
{"text": "Learn + + primarily for incremental learning problems that do not introduce new classes ( Polikar2001 ) , and Learn + + .NC for those that introduce new classes with additional datasets ( Muhlbaier 2008 ) are two examples of ensemble based incremental learning algorithms .", "label": "", "metadata": {}, "score": "43.266624"}
{"text": "Morgan Kaufmann .Kwok , S.W. , & Carter , C. ( 1990 ) .Multiple decision trees .In R.D. Schachter , T.S. Levitt , L.N. Kanal , & J.F. Lemmer ( Eds . ) , Uncertainty in Artificial Intelligence ( pp .", "label": "", "metadata": {}, "score": "43.350792"}
{"text": "Catlett , J. 1991b .Megainduction : Machine learning on very large databases .PhD Thesis , University of Sydney .Chan , P.K. and Stolfo , S.J. 1993a .Experiments on multistrategy learning by meta - learning .In Proc .", "label": "", "metadata": {}, "score": "43.416504"}
{"text": "On the accuracy of meta - learning for scalable data mining .J Intell Inf Syst 8 : 5 - 28 CrossRef .Charnes A , Cooper WW , Rhodes E ( 1978 ) Measuring the efficiency of decision making units .", "label": "", "metadata": {}, "score": "43.43122"}
{"text": "AAAI Press , Cambridge , pp 29 - 34 .Frelicot C , Mascarilla L ( 2001 ) Reject strategies driver combination of pattern classifiers .Freund S ( 1995 ) Boosting a weak learning algorithm by majority .Inf Comput 121(2 ) : 256 - 285 MATH CrossRef MathSciNet .", "label": "", "metadata": {}, "score": "43.460815"}
{"text": "In : Kargupta H , Chan P ( eds ) Advances in distributed and parallel knowledge discovery .AAAI / MIT Press , Cambridge , pp 185 - 210 .Wang W , Jones P , Partridge D ( 2000 ) Diversity between neural networks and decision trees for building multiple classifier systems , In : Proceeding of international workshop on multiple classifier systems ( LNCS 1857 ) , Springer , Calgiari , pp 240 - 249 .", "label": "", "metadata": {}, "score": "43.525135"}
{"text": "Maximum Entropy Estimation for Feature Forests .In HLT 2002 : Proceedings of Human Language Technology Conference , pages 292 - 297 .Miyao , Y. and Tsujii , J. ( 2004 ) .Deep linguistic analysis for the accurate identifica- tion of predicate - argument relations .", "label": "", "metadata": {}, "score": "43.54207"}
{"text": "Then we implement the SIA system and test the efficiency of it in the managed networks .Thus we confirm that the SIA system enables security managers to deal with security threats efficiently . ... to detect potential information andprotect the system , but also requires a lot of manpower , skilledin such security issues .", "label": "", "metadata": {}, "score": "43.54474"}
{"text": "Z. .KONG , E. B. and DIETTERICH , T. G. 1995 .Error - correcting output coding corrects bias and variance .In Proceedings of the Twelfth International Conference on Machine Learning Z. A. Prieditis and S. Russell , eds .", "label": "", "metadata": {}, "score": "43.555508"}
{"text": "Many solutions for intrusion detection based on machine learning techniques have been proposed , but most of them introduce significant computational overhead , which makes them time - consuming and thus increases their period of adapting to the environmental changing .In the first step , features are extracted in order to reduce the amount of data that the system needs to process .", "label": "", "metadata": {}, "score": "43.60811"}
{"text": "An Empirical comparison of pruning methods for ensemble classifiers , IDA2001 , LNCS 2189 , pp 208 - 217 .Woods K , Kegelmeyer W , Bowyer K ( 1997 ) Combination of multiple classifiers using local accuracy estimates .IEEE Trans Pattern Anal Mach Intell 19 : 405 - 410 CrossRef .", "label": "", "metadata": {}, "score": "43.614693"}
{"text": "Stacked Generalization .In Wolpert 's stacked generalization ( or stacking ) , an ensemble of classifiers is first trained using bootstrapped samples of the training data , creating Tier 1 classifiers , whose outputs are then used to train a Tier 2 classifier ( meta - classifier ) ( Wolpert 1992 ) .", "label": "", "metadata": {}, "score": "43.61624"}
{"text": "In Proceedings of the Thirteenth National Conference on Artificial Intelligence 725 730 .11 QUINLAN , J. R. 1993 .C4.5 : Programs for Machine Learning .Morgan Kaufmann , San Francisco .12 SCHAPIRE , R. E. 1990 .The strength of weak learnability .", "label": "", "metadata": {}, "score": "43.66535"}
{"text": "The decisions made by each classifier can then be combined by any of the combination rules described below .Confidence Estimation .The very structure of an ensemble based system naturally allows assigning a confidence to the decision made by such a system .", "label": "", "metadata": {}, "score": "43.670746"}
{"text": "J Am Stat Assoc 91(435):953 - 960 CrossRef .Polikar R ( 2006 )Ensemble based systems in decision making .IEEE Circuits Syst Mag 6(3 ) : 21 - 45 CrossRef .Prodromidis AL , Stolfo SJ , Chan PK ( 1999 ) Effective and efficient pruning of metaclassifiers in a distributed Data Mining system .", "label": "", "metadata": {}, "score": "43.75593"}
{"text": "In next review , the techniques may be tested in a controlled environment to obtain quantitative results .Then a ( nearly ) fool - proof system will be used to design an intrusion detection system for peer-2-peer communications .", "label": "", "metadata": {}, "score": "43.76081"}
{"text": "Boosting was the predecessor of the AdaBoost family of algorithms - which arguably became one of the most popular machine learning algorithms in recent times .Since these seminal works , research in ensemble systems have expanded rapidly , appearing often in the literature under many creative names and ideas .", "label": "", "metadata": {}, "score": "43.85405"}
{"text": "The approach proposed in this paper involves the creation of a new algorithm for analyzing correlation alerts and providing the correct information regarding the detection of various types of security attacks , such as DDoS. It also enables the evaluation of the attack status , the degree of danger fr ... \" .", "label": "", "metadata": {}, "score": "43.88337"}
{"text": "Some of these combination rules operate on class labels only , whereas others need continuous outputs that can be interpreted as support given by the classifier to each of the classes .Xu et al ( Xu 1992 ) .defines three types of base model outputs to be used for classifier combination .", "label": "", "metadata": {}, "score": "43.925095"}
{"text": "N. C. Oza and K. Tumer , \" Input Decimation Ensembles : Decorrelation through Dimensio - nality Reduction , \" 2nd Int .Workshop on Multiple Classifier Systems , in Lecture Notes in Computer Science , J. Kittler and F. Roli , Eds . , vol .", "label": "", "metadata": {}, "score": "43.94471"}
{"text": "Furthermore , on account of its inherent parallelism , the solution offers a possibility of implementation using reconfigurable hardware with the implementation cost much lower than the one of the traditional systems .The model is verified on KDD99 benchmark dataset and it has been proven that it is comparable to the solutions of the state - of - the - art , while exhibiting the mentioned advantaged .", "label": "", "metadata": {}, "score": "43.974674"}
{"text": "A narmax model representation for adaptive control based on local model - Modeling .Identif Control 13(1 ) : 25 - 39 .Jordan MI , Jacobs RA ( 1994 ) Hierarchical mixtures of experts and the EM algorithm .Neural Comput 6 : 181 - 214 CrossRef .", "label": "", "metadata": {}, "score": "44.059875"}
{"text": "Basic Bayesian decision theory relevant to the problem of learning classification rules is reviewed , then a Bayesian framework for such learning is presented .The framework has three components : the hypothesis space , the learning protocol , and criteria for successful learning .", "label": "", "metadata": {}, "score": "44.07618"}
{"text": "Machine Learning techniques such as Genetic Algorithms and Decision Trees have been applied to the field of intrusion detection for more than a decade .Machine Learning techniques can learn normal and anomalous patterns from training data and generate classifiers that then are used to detect attacks ... \" .", "label": "", "metadata": {}, "score": "44.109406"}
{"text": "In : Machine learning : proceedings of the thirteenth international conference , pp 325 - 332 .Friedman J , Hastie T , Tibshirani R ( 1998 )Additive logistic regression : a statistical view of boosting .Gama J ( 2004 )", "label": "", "metadata": {}, "score": "44.134354"}
{"text": "Data - driven grammar induction aims at producing wide - coverage grammars of human languages .Initial efforts in this field produced relatively shallow linguistic representations such as phrase - structure trees , which only encode constituent structure .Recent work on inducing deep grammars from treebanks addresses this shortcoming by also recovering non - local dependencies and grammatical relations .", "label": "", "metadata": {}, "score": "44.16497"}
{"text": "Feature selection .As mentioned earlier in this article , one way to improve diversity in the ensemble is to train individual classifiers different subsets of the available features .Selecting the feature subsets at random is known as the random subspace method , a term coined by ( Ho 1998 ) , who used it on constructing decision tree ensembles .", "label": "", "metadata": {}, "score": "44.289642"}
{"text": "Machine Learning , 15(3):321 - 329 .Xia , F. ( 1999 ) .Extracting Tree Adjoining Grammars from Bracketed Corpora .In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium , pages 398 - 403 .Page 174 .", "label": "", "metadata": {}, "score": "44.4088"}
{"text": "Morgan Kaufmann .Graefe , G. , Fayyad , U. , and Chaudhuri , S. 1998 .On the efficient gathering of sufficient statistics for classification from large SQL databases .In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining .", "label": "", "metadata": {}, "score": "44.51595"}
{"text": "Palmer , M. , Gildea , D. , and Kingsbury , P. ( 2005 ) .The proposition bank : An annotated corpus of semantic roles .Computational Linguistics , 31(1):71 - 106 .Ramshaw , L. and Marcus , M. ( 1995 ) .", "label": "", "metadata": {}, "score": "44.534126"}
{"text": "Machine Learning : Proceedings of the Eleventh International Conference ( pp .259 - 265 ) .Morgan Kaufmann .Schapire , R.E. ( 1990 ) .The strength of weak learnability .Machine Learning , 5 ( 2 ) , 197 - 227 .", "label": "", "metadata": {}, "score": "44.569607"}
{"text": "Abstract .Methods for voting classification algorithms , such as Bagging and AdaBoost , have been shown to be very successful in improving the accuracy of certain classifiers for artificial and real - world datasets .We review these algorithms and describe a large empirical study comparing several variants in conjunction with a decision tree inducer ( three variants ) and a Naive - Bayes inducer .", "label": "", "metadata": {}, "score": "44.621597"}
{"text": "Approximate algorithms for the 0/1 knapsack problem .Journal of the ACM , 22:115 - 124 .CrossRef MATH MathSciNet .Sarle , W.S. 1994 .Neural networks and statistical models .In Procedings of the Nineteenth Annual SAS Users Groups International Conference .", "label": "", "metadata": {}, "score": "44.623608"}
{"text": "Overall , backpropagation performs slightly better than the other two algorithms in terms of classification accuracy on new examples , but takes much longer to train .Experimental results suggest that backpropagation can work significantly better on data sets containing numerical data .", "label": "", "metadata": {}, "score": "44.65885"}
{"text": "Muggleton , S. ( 1991 ) .Inductive logic programming .New Generation Computing , 8(4):295 - 318 .Musillo , G. and Merlo , P. ( 2005 ) .Lexical and structural biases for function parsing .In Proceedings of the Ninth International Workshop on Parsing Technology , pages 83 - 92 .", "label": "", "metadata": {}, "score": "44.708767"}
{"text": "Levy , R. and Manning , C. ( 2004 ) .Deep dependencies from context - free statistical parsers : correcting the surface dependency approximation .In ACL 2004 : Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics , pages 328 - 335 .", "label": "", "metadata": {}, "score": "44.74306"}
{"text": "401 - 407 , 2001 .L. I. Kuncheva , \" Switching between selection and fusion in combining classifiers : An ex - periment , \" IEEE Transactions on Systems , Man , and Cybernetics , Part B : Cybernetics , vol .", "label": "", "metadata": {}, "score": "44.751194"}
{"text": "The research hypothesis is that by exploiting machine - learning algorithms to learn morphological features , lemmatization classes and grammatical functions from treebanks we can reduce the amount of manual specification and improve robustness , accuracy and domain- and language -independence for LFG parsing systems .", "label": "", "metadata": {}, "score": "44.8346"}
{"text": "Given current main memory costs , this requirement is readily met in most if not all workloads .Share .References .Agrawal , R. , Ghosh , S. , Imielinski , T. , Iyer , B. , and Swami , A. 1992 .", "label": "", "metadata": {}, "score": "44.876564"}
{"text": "The underlying complex decision boundary can then be approximated by an appropriate combination of different classifiers .In many applications that call for automated decision making , it is not unusual to receive data obtained from different sources that may provide complementary information .", "label": "", "metadata": {}, "score": "45.09325"}
{"text": "In Proceedings of the 8th European Conference on Machine Learning .N. Lavrac and S. Wrobel ( Eds . )Lecture Notes in Computer Science , vol .912 , Springer .Kohonen , T. 1995 .Self - Organizing Maps .", "label": "", "metadata": {}, "score": "45.126755"}
{"text": "8 KOHAVI , R. and WOLPERT , D. H. 1996 .Bias plus variance decomposition for zero - one loss functions .In Machine Learning : Proceedings of the Thirteenth International Z. ConferenceL. Saitta , ed .Morgan Kaufmann , San Francisco .", "label": "", "metadata": {}, "score": "45.17218"}
{"text": "Machine Learning techniques can learn normal and anomalous patterns from training data and generate classifiers that then are used to detect attacks on computer systems .In general , the input data to classifiers is in a high dimension feature space , but not all of features are relevant to the classes to be classified .", "label": "", "metadata": {}, "score": "45.294395"}
{"text": "AAAI Workshop .Craven , M.W. , & Shavlik , J.W. ( 1993 ) .Learning symbolic rules using artificial neural networks .Proceedings of the Tenth International Conference on Machine Learning ( pp .73 - 80 ) .Morgan Kaufmann .", "label": "", "metadata": {}, "score": "45.329384"}
{"text": "Zhang CX , Zhang JS ( 2008 )A local boosting algorithm for solving classification problems .Comput Stat Data Anal 52(4 ) : 1928 - 1941 MATH CrossRef .Zhou ZH , Tang W ( 2003 ) Selective ensemble of decision trees .", "label": "", "metadata": {}, "score": "45.35729"}
{"text": "Neural networks and the bias variance dilemma .Neural Computations 4 1 58 . Z. .HASTIE , T. and TIBSHIRANI , R. 1994 .Handwritten digit recognition via deformable prototy pes .Unpublished manuscript .Available at ftp stat.stanford.edu pub hastie/ zip.ps . Z. Z. .", "label": "", "metadata": {}, "score": "45.392925"}
{"text": "Technical Report , Institute for Social Research , University of Michigan , Ann Arbor , Michigan .Morimoto , Y. , Fukuda , T. , Matsuzawa , H. , Tokuyama , T. , and Yoda , K. 1998 .Algorithms for mining association rules for binary segmentations of huge categorical databases .", "label": "", "metadata": {}, "score": "45.396805"}
{"text": "In order for this process to be effective , the individual experts must exhibit some level of diversity among themselves , as described later in this article in more detail .Within the classification context , then , the diversity in the classifiers - typically achieved by using different training parameters for each classifier - allows individual classifiers to generate different decision boundaries .", "label": "", "metadata": {}, "score": "45.417213"}
{"text": "CrossRef .Agresti , A. 1990 .Categorical Data Analysis .John Wiley and Sons .Astrahan , M.M. , Schkolnick , M. , and Whang , K.-Y. Approximating the number of unique values of an attribute without sorting .Information Systems , 12(1):11 - 15 .", "label": "", "metadata": {}, "score": "45.47728"}
{"text": "It was confirmed that this system implementation could identify and analyze all types of intrusion by attackers in a managed network .Therefore , it provides a very effective means for security experts to cope with security threats in real time .", "label": "", "metadata": {}, "score": "45.487137"}
{"text": "Finally , the representational reason is to address to cases when the chosen model can not properly represent the sought decision boundary , which is discussed under divide and conquer section above .Perhaps one of the earliest work on ensemble systems is Dasarathy and Sheela 's 1979 paper ( Dasarathy 1979 ) , which first proposed using an ensemble system in a divide - and - conquer fashion , partitioning the feature space using two or more classifiers .", "label": "", "metadata": {}, "score": "45.509987"}
{"text": "Weston , J. and Watkins , C. ( 1999 ) .Support vector machines for multiclass pattern recognition .In Proceedings of the Seventh European Symposium On Artificial Neural Networks .White , A. and Liu , W. ( 1994 ) .", "label": "", "metadata": {}, "score": "45.56122"}
{"text": "Computer Speech & Language , 10(3):187 - 228 .Roth , D. ( 2001 ) .Reasoning with classifiers .In ECML 2001 : Proceedings of the Euro- pean Conference on Machine Learning , pages 506 - 510 .Page 172 .", "label": "", "metadata": {}, "score": "45.605946"}
{"text": "The statistical and neural - network methods perform the best on this particular problem and we discuss a potential reason for this ob- served difference .We also discuss the role of bias in machine ] earning and its importance in explaining performance differences observed on specific problems . by Jude W. Shavlik , Raymond J. Mooney , Geoffrey G. Towell - Machine Learning , 1991 . \" ...", "label": "", "metadata": {}, "score": "45.63416"}
{"text": "Grading classifiers , Austrian research institute for Artificial intelligence .Sharkey A ( 1996 )On combining artificial neural nets .Connect Sci 8 : 299 - 313 CrossRef .Sharkey N , Neary J , Sharkey A ( 1995 ) Searching weight space for backpropagation solution types , current trends in connectionism : Proceedings of the 1995 Swedish conference on connectionism , pp 103 - 120 .", "label": "", "metadata": {}, "score": "45.68702"}
{"text": "Research , vol .2 , pp .263 - 286 , 1995 .T. K. Ho , \" Random subspace method for constructing decision forests , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .20 , no . 8 , pp .", "label": "", "metadata": {}, "score": "45.790237"}
{"text": "Workshop on Multiple Classifier Systems , Lecture Notes in Computer Science , F. Roli , J. Kittler , and T. Windeatt , Eds . , vol .3077 , pp . 1 - 15 , 2004 .L. I. Kuncheva , Combining Pattern Classifiers , Methods and Algorithms .", "label": "", "metadata": {}, "score": "45.981735"}
{"text": "FREUND , Y. and SCHAPIRE , R. 1996 .Experiments with a new boosting algorithm .In Machine Z. Learning : Proceedings of the Thirteenth International Conference L. Saitta , ed .Morgan Kaufmann , San Francisco .Z. .JI , C. and MA . S. 1997 .", "label": "", "metadata": {}, "score": "46.035164"}
{"text": "Technical Report 401 , Univ .Chicago .2 AMIT , Y. and GEMAN , D. 1997 .Shape quantization and recognition with randomized trees .Neural Computation 9 1545 1588 .3 AMIT , Y. , GEMAN , D. and JEDy NAK , B. 1998 .", "label": "", "metadata": {}, "score": "46.04064"}
{"text": "However , it has been shown that a properly trained ensemble decision is usually correct if its confidence is high , and usually incorrect if its confidence is low .Using such an approach then , the ensemble decisions can be used to estimate the posterior probabilities of the classification decisions ( Muhlbaier 2005 ) .", "label": "", "metadata": {}, "score": "46.07978"}
{"text": "Maimon O , Rokach L ( 2002 ) Improving supervised learning by feature decomposition .In : Proceedings of foundations of information and knowledge systems , Salzan Castle , Germany , pp 178 - 196 .Margineantu D , Dietterich T ( 1997 )", "label": "", "metadata": {}, "score": "46.23009"}
{"text": "Machine learning is an area where both symbolic and neural approaches have been heavily investigated .However , there has been little research into the synergies achievable by combining these two learning paradigms .A hybrid approach that combines the symbolically - oriented explanation - based learning paradigm with the neural back - propagation algorithm is described .", "label": "", "metadata": {}, "score": "46.231247"}
{"text": "New York City , New York , pp .404 - 415 .Ripley , B.D. 1996 .Pattern Recognition and Neural Networks .Cambridge : Cambridge University Press .Rissanen , J. 1989 .Stochastic Complexity in Statistical Inquiry .World Scientific Publ . Co. .", "label": "", "metadata": {}, "score": "46.28268"}
{"text": "Morgan Kaufmann , San Francisco .Z. .HASHEM , S. 1993 .Optimal linear combinations of neural networks .Ph.D. dissertation , School of Industrial Engineering , Purdue Univ . , Lafay ette , IN . Z. .KONG , E. B. and DIETTERICH , T. G. 1995 .", "label": "", "metadata": {}, "score": "46.361385"}
{"text": "In ICML 2001 : Proceedings of the Eighteenth International Conference on Machine Learning , pages 282 - 289 .Lavra\u02c7 c , N. and D\u02c7 zeroski , S. ( 1994 ) .Inductive logic programming . E. Horwood New York .Le , Z. ( 2004 ) .", "label": "", "metadata": {}, "score": "46.385223"}
{"text": "Kamath C , Cant\u00fa E , Littau D ( 2002 ) Approximate splitting for ensembles of trees using histograms .In : Second SIAM international conference on data mining ( SDM-2002 ) .Kohavi R ( 1996 )Scaling up the accuracy of naive - bayes classifiers : a decision - tree hybrid .", "label": "", "metadata": {}, "score": "46.442947"}
{"text": "We used the KDDCUP 99 data set to train and test the decision tree classifiers .The experiments show that the resulting decision trees can have better performance than those built with all available . ... intrusion detection .A key problem is how to choose the features ( attributes ) of the input training data on which learning will take place .", "label": "", "metadata": {}, "score": "46.501137"}
{"text": "New York , NY : Oxford University Press .Breiman , L. , Friedman , J.H. , Olshen , R.A. , and Stone , C.J. 1984 .Classification and Regression Trees .Wadsworth : Belmont .Brodley , C.E. and Utgoff , P.E. 1992 .", "label": "", "metadata": {}, "score": "46.50416"}
{"text": "Baggi ... \" .An ensemble consists of a set of individually trained classifiers ( such as neural networks or decision trees ) whose predictions are combined when classifying novel instances .Previous research has shown that an ensemble is often more accurate than any of the single classifiers in the ensemble .", "label": "", "metadata": {}, "score": "46.536057"}
{"text": "Morgan Kaufmann .Murphy , O.J. and McCraw , R.L. 1991 .Designing storage efficient decision trees .IEEE Trans .on Comp . , 40(3):315 - 319 .CrossRef .Murthy , S.K. 1995 .On growing better decision trees from data .", "label": "", "metadata": {}, "score": "46.595825"}
{"text": "Merz CJ ( 1999 )Using correspondence analysis to combine classifier .Mach Learn 36(1 - 2 ) : 33 - 58 CrossRef .Michalski RS , Tecuci G ( 1994 ) Machine learning , a multistrategy approach .Morgan Kaufmann , San Francisco .", "label": "", "metadata": {}, "score": "46.800316"}
{"text": "Conference on Info . and Knowledge Mgmt . , pp .314 - 323 .Chan , P.K. and Stolfo , S.J. 1993b .Meta - learning for multistrategy and parallel learning .In Proc .Second Intl .Workshop on Multistrategy Learning , pp .", "label": "", "metadata": {}, "score": "46.80625"}
{"text": "G. Rogova , \" Combining the results of several neural network classifiers , \" Neural Networks , vol .7 , no .5 , pp .777 - 781 , 1994 .L. Lam and C. Y. Suen , \" Optimal combinations of pattern classifiers , \" Pattern Recognition Letters , vol .", "label": "", "metadata": {}, "score": "46.807266"}
{"text": "Breiman , L. ( 1996a ) .Arcing classifiers ( Technical Report ) .Berkeley : Statistics Department , University of California .Breiman , L. ( 1996b ) .Bagging predictors .Machine Learning , 24 , 123 - 140 .", "label": "", "metadata": {}, "score": "46.812153"}
{"text": "R. Polikar , \" Bootstrap inspired techniques in computational intelligence : ensemble of classifiers , incremental learning , data fusion and missing features , IEEE Signal Processing Magazine , v. 24 , no .4 , pp .59 - 72 , 2007 .", "label": "", "metadata": {}, "score": "46.85253"}
{"text": "Lim , T.-S. , Loh , W.-Y. , and Shih , Y.-S. An empirical comparison of decision trees and other classification methods .Technical Report 979 , Department of Statistics , University of Wisconsin , Madison .Liu , H. and Setiono , R. 1996 .", "label": "", "metadata": {}, "score": "46.88004"}
{"text": "In Proc . of the VLDB Conference .Vancouver , British Columbia , Canada , pp .560 - 573 .Agrawal , R. , Imielinski , T. , and Swami , A. 1993 .Database mining : A performance perspective .", "label": "", "metadata": {}, "score": "46.889038"}
{"text": "Miyao , Y. and Tsujii , J. ( 2005 ) .Probabilistic disambiguation models for wide - coverage HPSG parsing .In ACL 2005 : Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics , pages 83 - 90 .", "label": "", "metadata": {}, "score": "46.99909"}
{"text": "The training data subset for the second classifier \\(C_2\\ ) is chosen as the most informative subset , given \\(C_1\\ .\\ ) Specifically , \\(C_2\\ ) is trained on a training data only half of which is correctly classified by \\(C_1\\ , \\ ) and the other half is misclassified .", "label": "", "metadata": {}, "score": "47.048923"}
{"text": "It is , therefore , important to be able to perform morphological analysis in an accurate and robust way for morphologically rich languages .I propose a fully data - driven supervised method to simultaneously lemmatize and morphologically analyze text and obtain competitive or improved results on a range of typologically diverse languages .", "label": "", "metadata": {}, "score": "47.062706"}
{"text": "241 - 259 , 1992 .T. K. Ho , J. J. Hull , and S. N. Srihari , \" Decision combination in multiple classifier systems , \" IEEE Trans . on Pattern Analy .Machine Intel . , vol .16 , no . 1 , pp .", "label": "", "metadata": {}, "score": "47.068123"}
{"text": "Feature subset selection using the wrapper model : Overfitting and dynamic search space topology .The First International Conference on Knowledge Discovery and Data Mining ( pp .192 - 197 ) .Kohavi , R. , Sommerfield , D. , & Dougherty , J. ( 1997 ) .", "label": "", "metadata": {}, "score": "47.144142"}
{"text": "G. Fumera and F. Roli , \" A Theoretical and Experimental Analysis of Linear Combiners for Multiple Classifier Systems , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , Vol .27 , no .6 pp .942 - 956 , 2005 .", "label": "", "metadata": {}, "score": "47.2181"}
{"text": "Neural Computation 9 1545 1588 .Z. .BREIMAN , L. 1996c .Out - of - bag estimation .Available at ftp.stat users breimanas OOBestimation .Z. .DIETTERICH , T. 1998 .An experimental comparison of three methods for constructing ensembles of decision trees : bagging , boosting and randomization .", "label": "", "metadata": {}, "score": "47.221695"}
{"text": "In classifiers using SVM , term selection is often not needed , as SVMs tend to be fairly robust to over fitting and can scale up to considerable dimensionalities .Also there is no human and machine effort in parameter tuning on a validation set is needed , as there is a theoretically calculated \" default \" choice of parameter settings .", "label": "", "metadata": {}, "score": "47.29541"}
{"text": "Morgan Kaufmann Publishers , San Francisco , pp 774 - 780 .Ohno - Machado L , Musen MA ( 1997 )Modular neural networks for medical prognosis : quantifying the benefits of combining neural networks for survival prediction .Connect Sci 9(1 ) : 71 - 86 CrossRef .", "label": "", "metadata": {}, "score": "47.306183"}
{"text": "Therefore , individual classifiers in an ensemble system need to make different errors on different instances .The intuition , then , is that if each classifier makes different errors , then a strategic combination of these classifiers can reduce the total error , a concept not too dissimilar to low pass filtering of the noise .", "label": "", "metadata": {}, "score": "47.323586"}
{"text": "This is perhaps the primary reason why ensemble based systems are used in practice : what is the most appropriate classifier for a given classification problem ?The most commonly used procedure - choosing the classifiers with the smallest error on training data - is unfortunately a flawed one .", "label": "", "metadata": {}, "score": "47.345562"}
{"text": "Quinlan JR ( 1993 ) C4.5 : programs for machine learning .Morgan Kaufmann , Los Altos .Quinlan JR ( 1996 )Bagging , Boosting , and C4.5 .In : Proceedings of the thirteenth national conference on artificial intelligence , pp 725 - 730 .", "label": "", "metadata": {}, "score": "47.393642"}
{"text": "Workshop on Multiple Classifier Systems , Lecture Notes in Computer Science , Vol .1857 , pp . 1 - 15 , 2000 , Springer - Verlag .L. K. Hansen and P. Salamon , \" Neural network ensembles , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "47.406975"}
{"text": "Feature transformation by function decomposition .IEEE Intell Syst Appl 13 : 38 - 43 CrossRef Machine learning : driving significant improvements in biometric performance .As they improve , biometric authentication systems are becoming increasingly indispensable for protecting life and property .", "label": "", "metadata": {}, "score": "47.454575"}
{"text": "Schaffer C ( 1993 )Selecting a classification method by cross - validation .Mach Learn 13(1 ) : 135 - 143 .Schapire RE ( 1990 )The strength of weak learnability .Mach Learn 5(2 ) : 197 - 227 .", "label": "", "metadata": {}, "score": "47.47"}
{"text": "In ICML 2000 : Proceedings of the International Conference on Machine Learning , pages 591 - 598 .Merlo , P. and Musillo , G. ( 2005 ) .Accurate function parsing .In HLT - EMNLP 2005 : Proceedings of the Conference on Human Language Technology and Empirical Meth- ods in Natural Language Processing , pages 620 - 627 .", "label": "", "metadata": {}, "score": "47.48011"}
{"text": "Modular and Hierarchical Network .Introduction .Class - Based Modular Networks .Mixture - of - Experts Modular Networks .Hierarchical Machine Learning Models .Biometric Authentication Application Examples .Decision - Based Neural Networks .Introduction .Basic Decision - Based Neural Networks .", "label": "", "metadata": {}, "score": "47.57241"}
{"text": "Modular and Hierarchical Network .Introduction .Class - Based Modular Networks .Mixture - of - Experts Modular Networks .Hierarchical Machine Learning Models .Biometric Authentication Application Examples .Decision - Based Neural Networks .Introduction .Basic Decision - Based Neural Networks .", "label": "", "metadata": {}, "score": "47.57241"}
{"text": "The most popular algorithm use to compute association rules effectively are apriori algorithm [ 20 ] and FP - tree algorithm [ 21].The [ 22 ] uses a similar approach to construct a rule - base classifier with apriori - based algorithm but the results obtained on the Reuter-21578 collection are not promising as for five categories out of ten ; the precision / recall breakeven point is around 60 % .", "label": "", "metadata": {}, "score": "47.623955"}
{"text": "This approach overcomes problems that arise when using imperfect domain theories to build explanations and addresses the problem of choosing a good initial neural network configuration .Empirical results show that the hybrid system more accurately l ..", "label": "", "metadata": {}, "score": "47.708916"}
{"text": "Here , modified training sets are formed by resampling from the original training set , classifiers constructed using these training sets and then combined by voting .Freund and Schapire propose an algorithm the basis of which is to adaptively resample and combine ( hence the acronym \" arcing \" ) so that the weights in the resampling are increased for those cases most often misclassified and the combining is done by weighted voting .", "label": "", "metadata": {}, "score": "47.716587"}
{"text": "In [ 7 ] , An IDS combining with GA and BP is put forward .Decision Tree .Decision trees are powerful and popular tools for classification and prediction .The attractiveness of tree - based methods is due in large part to the fact that , in contrast to neural networks , decision trees represent rules .", "label": "", "metadata": {}, "score": "47.761032"}
{"text": "B. V. Dasarathy and B. V. Sheela ( 1979 ) , \" Composite classifier system design : concepts and me - thodology , \" Proceedings of the IEEE , vol .67 , no .5 , pp .708 - 713 .", "label": "", "metadata": {}, "score": "47.77928"}
{"text": "Neural networks and the bias / variance dilemma .Neural Computation , 4 , 1 - 48 .Good , I.J. ( 1965 ) .The estimation of probabilities : An essay on modern bayesian methods .M.I.T. Press .Holte , R.C. ( 1993 ) .", "label": "", "metadata": {}, "score": "47.859924"}
{"text": "We explore two arcing algorithms , compare them to each other and to bagging , and try to understand how arcing works .We introduce the definitions of bias and variance for a classifier as components of the test set error .", "label": "", "metadata": {}, "score": "47.875465"}
{"text": "Cheng , J. , Fayyad , U.M. , Irani , K.B. , and Qian , Z. 1988 .Improved decision trees : A generalized version of ID3 .In Proceedings of the Fifth International Conference on Machine Learning .Morgan Kaufman .", "label": "", "metadata": {}, "score": "47.918476"}
{"text": "In Proceedings of the Third ACL Workshop on Very Large Corpora , pages 82 - 94 .Cambridge MA , USA .Ratnaparkhi , A. ( 1996 ) .A maximum entropy model for part - of - speech tagging .In EMNLP 1996 : Proceedings of the 1st", "label": "", "metadata": {}, "score": "47.983288"}
{"text": "The idea of ensemble methodology is to build a predictive model by integrating multiple models .It is well - known that ensemble methods can be used for improving prediction performance .Researchers from various disciplines such as statistics and AI considered the use of ensemble methodology .", "label": "", "metadata": {}, "score": "48.14322"}
{"text": "To ensure that individual boundaries are adequately different , despite using substantially similar training data , weaker or more unstable classifiers are used as base models , since they can generate suffi - ciently different decision boundaries even for small perturbations in their training parameters .", "label": "", "metadata": {}, "score": "48.14446"}
{"text": "When the amount of training data is too large to make a single classifier training difficult , the data can be strategically partitioned into smaller subsets .Each partition can then be used to train a separate classifier which can then be combined using an appropriate combination rule ( see below for different combination rules ) .", "label": "", "metadata": {}, "score": "48.15286"}
{"text": "In order to reduce the optimization effort , various techniques are integrated that accelerate and improve the classifier significantly : hybrid k - NN , comparative cross validation .The feasibility and the benefits of the proposed approach are demonstrated by means of data mining problem : intrusion detection in computer networks .", "label": "", "metadata": {}, "score": "48.266167"}
{"text": "Comput . 8 GEMAN , S. , BIENENSTOCK , E. and DOURSAT , R. 1992 .Neural networks and the bias variance dilemma .Neural Computation 4 1 58 .9 SHLIEN , S. 1990 .Multiple binary decision tree classifiers .", "label": "", "metadata": {}, "score": "48.26738"}
{"text": "238 - 247 , 2001 .G. Brown , J. Wyatt , R. Harris , X. Yao , \" Diversity Creation Methods : A Survey and Categorisation , \" Journal of Information Fusion ( Special issue on Diversity in Multiple Classifier Systems ) .", "label": "", "metadata": {}, "score": "48.537075"}
{"text": "Lecture Notes in Computer Science 2639 , pp 476 - 483 .Zhou ZH , Wu J , Tang W ( 2002 ) Ensembling neural networks : many could be better than all .Artif Intell 137 : 239 - 263 MATH CrossRef MathSciNet .", "label": "", "metadata": {}, "score": "48.546326"}
{"text": "In Neural Networks for Speech and Image Processing R. J .. Mammone , ed .Chapman and Hall , London .Z. .QUINLAN , J. R. 1993 .C4.5 : Programs for Empirical Learning .Morgan Kaufmann , San Francisco .", "label": "", "metadata": {}, "score": "48.668278"}
{"text": "Now , assume that each classifier has a probability p of making a correct decision .Weighted majority voting .Other combination rules .There are several other combination rules , which are arguably more sophisticated than the ones listed above .", "label": "", "metadata": {}, "score": "48.70432"}
{"text": "Certain problems are just too difficult for a given classifier to solve .In fact , the decision boundary that separates data from different classes may be too complex , or lie outside the space of functions that can be implemented by the chosen classifier model .", "label": "", "metadata": {}, "score": "48.711693"}
{"text": "728 .MathSciNet .Maass , W. 1994 .Efficient agnostic pac - learning with simple hypothesis .In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory , pp .67 - 75 .Magidson , J. 1989 .", "label": "", "metadata": {}, "score": "48.763283"}
{"text": "Similar to bagging , boosting also creates an ensemble of classifiers by resampling the data , which are then combined by majority voting .However , in boosting , resampling is strategically geared to provide the most informative training data for each consecutive classifier .", "label": "", "metadata": {}, "score": "48.939194"}
{"text": "Speed and accuracy in shallow and deep stochastic parsing .In HLT - NAACL 2004 : Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics .Kurohashi , S. and Nagao , M. ( 2003 ) .", "label": "", "metadata": {}, "score": "49.012306"}
{"text": "Principle Component Analysis .PCA is a dimensionality reduction technique based on the statistical variance among data instances .In mathematical terms , PCA is a technique where n correlated random variables are transformed into d uncorrelated variables .The uncorrelated variables are linear combinations of the original variables and can be used to express the data in a reduced form .", "label": "", "metadata": {}, "score": "49.0322"}
{"text": "295 - 298 .About this Article .Title .RainForest - A Framework for Fast Decision Tree Construction of Large Datasets \" ...In the past , nearest neighbor algorithms for learning from examples have worked best in domains in which all features had numeric values .", "label": "", "metadata": {}, "score": "49.06112"}
{"text": "Dimitriadou E , Weingessel A , Hornik K ( 2003 )A cluster ensembles framework , Design and application of hybrid intelligent systems .IOS Press , Amsterdam .Domingos P ( 1996 )Using partitioning to speed up specific - to - general rule induction .", "label": "", "metadata": {}, "score": "49.106815"}
{"text": "We provide explanations for both results in terms of the properties of the natural language processing tasks and the learning algorithms . \" ...This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context .", "label": "", "metadata": {}, "score": "49.157055"}
{"text": "Both the experts themselves and the gating network requires the input instances for training .Several mixture - of - experts models can also be further combined to obtain a hierarchical mixture of experts ( Jordan 1994 ) .Mixture of experts are particularly useful when different experts are trained on different parts of the feature space , or when heterogeneous sets of features are available to be used for a data fusion problem .", "label": "", "metadata": {}, "score": "49.247974"}
{"text": "An O(ND ) difference algorithm and its variations .Algorithmica , 1(1):251 - 266 .Noreen , E. W. ( 1989 ) .Computer intensive methods for testing hypotheses .A Wiley- Interscience Publication , New York .O'Donovan , R. , Burke , M. , Cahill , A. , van Genabith , J. , and Way , A. ( 2004 ) .", "label": "", "metadata": {}, "score": "49.28253"}
{"text": "FREUND , Y. and SCHAPIRE , R. 1996 .Experiments with a new boosting algorithm .In Machine Z. Learning : Proceedings of the Thirteenth International Conference L. Saitta , ed .Morgan Kaufmann , San Francisco .Z. .FREUND , Y. and SCHAPIRE , R. 1997 .", "label": "", "metadata": {}, "score": "49.291443"}
{"text": "572 - 577 ) .Domingos , P. ( 1997 ) .Why does bagging work ?A Bayesian account and its implications .In D. Heckerman , H. Mannila , D. Pregibon , & R. Uthurusamy ( Eds . ) , Proceedings of the Third International Conference on Knowledge Discovery and Data Mining ( pp .", "label": "", "metadata": {}, "score": "49.43257"}
{"text": "Current information protection systems only detect and warn against individual intrusion , and are not able to provide a collective and synthesized alert message .In this paper , we propose a new Meta - IDS system which is called \" SIA System \" .", "label": "", "metadata": {}, "score": "49.446796"}
{"text": "Estimating probabilities : A crucial task in machine learning .In L.C. Aiello ( Ed . ) , Proceedings of the Ninth European Conference on Artificial Intelligence ( pp .147 - 149 ) .Chan , P. , Stolfo , S. , & Wolpert , D. ( 1996 ) .", "label": "", "metadata": {}, "score": "49.54254"}
{"text": "Only the so - called productions are observable .[ 1 ] Proposes a simple data preprocessing approach to speed up a hidden Markov model ( HMM ) training for system - call - based anomaly intrusion detection .Experiments based on a public database demonstrate that this data preprocessing approach can reduce training time by up to 50 percent with unnoticeable intrusion detection performance degradation , compared to a conventional batch HMM training scheme .", "label": "", "metadata": {}, "score": "49.65719"}
{"text": "A linear programming formulation for global inference in natural language tasks .In CONLL 2004 : Eighth Conference on Computational Natural Language Learning , pages 1 - 8 .Schluter , N. and van Genabith , J. ( 2007 ) .Preparing , restructuring and augmenting a French treebank : Lexicalised parsing or coherent treebanks ?", "label": "", "metadata": {}, "score": "49.676872"}
{"text": "Introduction .Traditional Derivation of EM .An Entropy Interpretation .Doubly - Stochastic EM .Concluding Remarks .Support Vector Machines .Introduction .Fisher 's Linear Discriminant Analysis .Linear SVMs : Separable Case .Linear SVMs : Fuzzy Separation .", "label": "", "metadata": {}, "score": "49.686676"}
{"text": "Kuncheva L ( 2005 ) Combining pattern classifiers .Wiley Press , New York .Kuncheva L , Whitaker C ( 2003 ) Measures of diversity in classifier ensembles and their relationship with ensemble accuracy .Mach Learn 51(2):181 - 207 MATH CrossRef .", "label": "", "metadata": {}, "score": "49.753067"}
{"text": "Pattern Recognit 23(7 ) : 757 - 763 CrossRef .Skurichina M , Duin RPW ( 2002 )Bagging , boosting and the random subspace method for linear classifiers .Pattern Anal Appl 5(2 ) : 121 - 135 MATH CrossRef MathSciNet .", "label": "", "metadata": {}, "score": "49.792587"}
{"text": "15 , pp .445 - 456 ) .MIT Press .Quinlan , J.R. ( 1996 ) .Bagging , boosting , and c4.5 .Proceedings of the Thirteenth National Conference on Artificial Intelligence ( pp .725 - 730 ) .", "label": "", "metadata": {}, "score": "49.916233"}
{"text": "59 - 72 , 2007 .R. Polikar , \" Ensemble based systems in decision making , \" IEEE Circuits and Systems Magazine , vol .6 , no.3 , pp .21 - 45 , 2006 .Muhlbaier M. , Topalis A. , Polikar R. , \" Ensemble confidence estimates posterior probability , \" 6th Int .", "label": "", "metadata": {}, "score": "49.95172"}
{"text": "Tiered tagging and combined language models classifiers .In TSD 1999 : Proceedings of the Second International Workshop on Text , Speech and Dialogue , pages 28 - 33 .Tufi\u00b8 s , D. and Dragomirescu , L. ( 2004 ) .", "label": "", "metadata": {}, "score": "49.98391"}
{"text": "497 - 501 , 1995 .L. I. Kuncheva , \" Using measures of similarity and inclusion for multiple classifier fusion by decision templates , \" Fuzzy Sets and Systems , L. I. Kuncheva , \" Using measures of similarity and inclusion for multiple classifier fusion by decision templates , \" vol .", "label": "", "metadata": {}, "score": "50.169647"}
{"text": "A theory of learning classification rules .Ph.D. thesis , University of Technology , Sydney , School of Computing Science .Blake , C. Keogh , E. , & Merz , C.J. ( 1998 ) .UCI repository of machine learning databases .", "label": "", "metadata": {}, "score": "50.226334"}
{"text": "In : AAAI Workshop in Knowledge Discovery in Databases , pp 227 - 240 .Chan PK , Stolfo SJ ( 1995 )A comparative evaluation of voting and meta - learning on partitioned data , Proceeding of 12th international conference On machine learning ICML-95 .", "label": "", "metadata": {}, "score": "50.302677"}
{"text": "Nonlinear SVMs .Biometric Authentication Application Examples .Multi - Layer Neural Networks .Introduction .Neuron Models .Multi - Layer Neural Networks .The Back - Propagation Algorithms .Two - Stage Training Algorithms .Genetic Algorithm for Multi - Layer Networks .", "label": "", "metadata": {}, "score": "50.380836"}
{"text": "Error - correcting output coding corrects bias and variance .In Proceedings of the Twelfth International Conference on Machine Learning Z. A. Prieditis and S. Russell , eds .Morgan Kaufmann , San Francisco .10 QUINLAN , J. R. 1996 .", "label": "", "metadata": {}, "score": "50.392254"}
{"text": "Hand , D.J. 1997 .Construction and Assessment of Classification Rules .Chichester , England : John Wiley & Sons .Hyafil , L. and Rivest , R.L. 1976 .Constructing optimal binary decision trees is NP - complete .Information Processing Letters , 5(1):15 - 17 .", "label": "", "metadata": {}, "score": "50.408062"}
{"text": "Addison - Wesley , Reading MATH .Tumer K , Ghosh J ( 1996 ) Error correlation and error reduction in ensemble classifiers .Connection science , special issue on combining artificial neural networks : ensemble approaches .Tumer K , Ghosh J ( 2000 )", "label": "", "metadata": {}, "score": "50.413002"}
{"text": "Learning multilingual morphology with CLOG .In Proceedings of the 8th International Conference on Inductive Logic Programming , pages 135 - 144 .Marcus , M. P. , Santorini , B. , and Marcinkiewicz , M. A. ( 1994 ) .Building a Large Anno- tated Corpus of English : The Penn Treebank .", "label": "", "metadata": {}, "score": "50.45575"}
{"text": "Error reduction through learning multiple descriptions .Machine Learning 24 173 202 .CHERKAUER , K. J. 1996 .Human expert - level performance on a scientific image analysis task by a sy stem using combined artificial neural networks .In Working Notes of the AAAI Z. Workshop on Integrating Multiple Learned Models P. Chan , ed .", "label": "", "metadata": {}, "score": "50.515877"}
{"text": "Rudin C , Daubechies I , Schapire RE ( 2004 )The dynamics of Adaboost : cyclic behavior and convergence of margins .J Mach Learn Res 5 : 1557 - 1595 MathSciNet .Rosen BE ( 1996 )Ensemble learning using decorrelated neural networks .", "label": "", "metadata": {}, "score": "50.53457"}
{"text": "In Twelfth International Conference on Machine Learning A. Prieditis and .S. Russell , eds .Morgan Kaufmann , San Francisco .Z. .MACKAY , D. 1992 .A practical bay esian framework for backpropagation networks .Neural Computation 4 448 472 .", "label": "", "metadata": {}, "score": "50.555237"}
{"text": "1538 - 1550 .Shafer , J. , Agrawal , R. , and Mehta , M. 1996 .SPRINT : A scalable parallel classifier for data mining .In Proc . of the 22nd Int'l Conference on Very Large Databases .Bombay , India .", "label": "", "metadata": {}, "score": "50.65724"}
{"text": "Such a classifier can not learn the boundary shown in Figure 2 .Now consider a collection of circular decision boundaries generated by an ensemble of such classifiers as shown in Figure 3 , where each classifier labels the data as class O or class X , based on whether the instances fall within or outside of its boundary .", "label": "", "metadata": {}, "score": "50.664932"}
{"text": "In Machine Learning : Proceedings of the 12th International Conference , A. Prieditis and S. Russell ( Eds . )Morgan Kaufmann .Fayyad , U.M. 1991 .On the induction of decision trees for multiple concept learning .PhD Thesis , EECS Department , The University of Michigan .", "label": "", "metadata": {}, "score": "50.841896"}
{"text": "In regard to arcing , probably the deterministic reweighting on misclassified examples produces new trees which are quite different from the previous ones .Moreover , the errors induced on data points which were correctly classified by the existing trees are sufficiently randomized to avoid any sy stematic deterioration . 1 AMIT , Y. and GEMAN , D. 1994 .", "label": "", "metadata": {}, "score": "50.883575"}
{"text": "We show that in language learning , contrary to received wisdom , keeping exceptional training instances in memory can be beneficial for generalization accuracy .We investigate this phenomenon empirically on a selection of benchmark natural language processing tasks : grapheme - to - phoneme conversion , part - of - speech tagging , prepositional - phrase attachment , and base noun phrase chunking .", "label": "", "metadata": {}, "score": "50.89225"}
{"text": "Results show that editing exceptional instances ( with low typicality or low class prediction strength ) tends to harm generalization accuracy .In a second series of experiments we compare memory - based learning and decision - tree learning methods on the same selection of tasks , and find that decision - tree learning often performs worse than memory - based learning .", "label": "", "metadata": {}, "score": "50.918747"}
{"text": "Analysis indicates that the performance of the Boosting methods is dependent on the characteristics of the data set being examined .In fact , further results show that Boosting ensembles may overfit noisy data sets , thus decreasing its performance .Finally , consistent with previous studies , our work suggests that most of the gain in an ensemble 's performance comes in the first few classifiers combined ; however , relatively large gains can be seen up to 25 classifiers when Boosting decision trees .", "label": "", "metadata": {}, "score": "50.968796"}
{"text": ".. s ?We will argue in this section that the answer to these related questions is \" no . \" \" ...This paper describes work in the StatLog project comparing classification algorithms on large real - world problems .The algorithms compared were from : symbolic learning ( CART , C4.5 , NewID , AC 2 , ITrule , Cal5 , CN2 ) , statistics ( Naive Bayes , k - nearest neighbor , kernel density , linear discriminant , qua ... \" .", "label": "", "metadata": {}, "score": "50.987377"}
{"text": "The combination of the classifiers is then based on the given instance : the classifier trained with data closest to the vicinity of the instance , according to some distance metric , is given the highest credit .One or more local experts can be nominated to make the decision ( Jacobs 1991 , Woods 1997 , Alpaydin 1996 , Giacinto 2001 ) .", "label": "", "metadata": {}, "score": "50.99437"}
{"text": "In this paper , authors explore the use of Genetic Programming ( GP ) for such a purpose .The approach is not new in some aspects , as GP has already been partially explored in the past .[ 9 ] .", "label": "", "metadata": {}, "score": "51.180473"}
{"text": "7 , no . 3 , pp .788 - 792 , 1996 .G. Giacinto and F. Roli , \" Approach to the automatic design of multiple classifier systems , \" Pattern Recognition Letters , vol .22 , no . 1 , pp .", "label": "", "metadata": {}, "score": "51.1891"}
{"text": "Rokach L , Maimon O , Arad O ( 2005 ) Improving supervised learning by sample decomposition .Int J Comput Intell Appl 5(1 ) : 37 - 54 CrossRef .Rokach L , Maimon O , Lavi I ( 2003 ) Space decomposition in data mining : A clustering approach .", "label": "", "metadata": {}, "score": "51.214695"}
{"text": "A comparison of methods for multi - class support vector machines .IEEE Trans Neural Netw 13(2 ) : 415 - 425 CrossRef .Hu Q , Yu D , Xie Z , Li X ( 2007 )EROS : ensemble rough subspaces .", "label": "", "metadata": {}, "score": "51.25111"}
{"text": "The ability to restructure a decision tree efficiently enables a variety of approaches to decision tree induction that would otherwise be prohibitively expensive .Two such approaches are described here , one being incremental tree induction ( ITI ) , and the other being non - incremental tree induction using a measure of tree quality instead of test quality ( DMTI ) .", "label": "", "metadata": {}, "score": "51.270508"}
{"text": "The advantage of using rules is that they tend to be simple and intuitive , unstructured and less rigid .As the drawbacks they are di\u00ef\u00ac\u0192cult to maintain , and in some cases , are inadequate to represent many types of information .", "label": "", "metadata": {}, "score": "51.32115"}
{"text": "Langley , P. , & Sage , S. ( 1997 ) .Scaling to domains withmany irrelevant features .In R. Greiner ( Ed . ) , Computational learning theory and natural learning systems ( Vol .MIT Press .Oates , T. , & Jensen , D. ( 1997 ) .", "label": "", "metadata": {}, "score": "51.36727"}
{"text": "In ACL 156 .Page 171 .2004 : Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics , pages 367 - 374 .O'Donovan , R. , Cahill , A. , van Genabith , J. , and Way , A. ( 2005 ) .", "label": "", "metadata": {}, "score": "51.368767"}
{"text": "It is shown in [ 24 ] that additive algorithms having inductive base like SVM work very well for problems with dense concepts and sparse instances .Most of the text categorization problems are linearly separable such as the reuters-21578 .SVMs are accurate , robust , and quick to apply to test instances .", "label": "", "metadata": {}, "score": "51.439415"}
{"text": "IEEE Trans .Neural Networks 8 32 42 . Z. .KONG , E. B. and DIETTERICH , T. G. 1995 .Error - correcting output coding corrects bias and variance .In Proceedings of the Twelfth International Conference on Machine Learning Z. A. Prieditis and S. Russell , eds .", "label": "", "metadata": {}, "score": "51.44502"}
{"text": "Rokach L , Maimon O ( 2005b )Feature Set decomposition for decision trees .J Intell Data Anal 9(2 ) : 131 - 158 .Rokach L , Maimon O ( 2008 )Data mining with decision trees : theory and applications .", "label": "", "metadata": {}, "score": "51.545357"}
{"text": "In D. Fisher ( Ed . ) , Machine Learning : Proceedings of the Fourteenth International Conference ( pp .254 - 262 ) .Morgan Kaufmann .Oliver , J. , & Hand , D. ( 1995 ) .On pruning and averaging decision trees .", "label": "", "metadata": {}, "score": "51.549076"}
{"text": "SOM .The network is required to self organize depending on some structure in the input data .Typically this structure is may be some form of redundancy in the input data or clusters in the data .Self Organizing Maps use unsupervised learning rule .", "label": "", "metadata": {}, "score": "51.58667"}
{"text": "1022 - 1027 .Fayyad , U.M. , Shapiro , G.P. , Smyth , P. , and Uthurusamy , R. ( Eds . )Advances in Knowledge Discovery and Data Mining .AAAI / MIT Press .Friedman , J.H. 1977 .", "label": "", "metadata": {}, "score": "51.616806"}
{"text": "Machine learning : driving significant improvements in biometric performance .As they improve , biometric authentication systems are becoming increasingly indispensable for protecting life and property .This book introduces powerful machine learning techniques that significantly improve biometric performance in a broad spectrum of application domains .", "label": "", "metadata": {}, "score": "51.756332"}
{"text": "A linear classifier , one that is capable of learning linear boundaries , can not learn this complex non - linear boundary .However , appropriate combination of an ensemble of such linear classifiers can learn any non - linear boundary .", "label": "", "metadata": {}, "score": "51.78263"}
{"text": "Clark P , Boswell R ( 1991 ) Rule induction with CN2 : some recent improvements .In : Proceedings of the European working session on learning , Pitman , pp 151 - 163 .Cohen S , Rokach L , Maimon O ( 2007 )", "label": "", "metadata": {}, "score": "51.818554"}
{"text": "In symbolic domains , a more sophisticated treatment of t ... \" .In the past , nearest neighbor algorithms for learning from examples have worked best in domains in which all features had numeric values .In such domains , the examples can be treated as points and distance metrics can use standard definitions .", "label": "", "metadata": {}, "score": "51.830643"}
{"text": "The Nineth European Conference on Machine Learning ,Poster Papers ' ( pp .78 - 87 ) .Kohavi , R. , & Kunz , C. ( 1997 ) .Option decision trees with majority votes .In D. Fisher ( Ed . ) , Machine Learning : Proceedings of theFourteenth International Conference ( pp .", "label": "", "metadata": {}, "score": "51.833294"}
{"text": "Communicated by Ji\u02c7r\u00ed \u02c7 Safa\u02c7r\u00edk Abstract .Current information protection systems only detect and warn against individual intrusion , and are not able to provide a collective and synthesized alert message .In this paper , we propose a new Meta - IDS system which is called \" SIA System \" .", "label": "", "metadata": {}, "score": "51.9302"}
{"text": "Technique .Architecture .Underlying principle .Markov Modeling .Markov Chain .The markov chains consist of sets of states and associated probabilities for transition to different states .Hidden Markov Models .In hidden markov models , the underlying states are not visible .", "label": "", "metadata": {}, "score": "51.937016"}
{"text": "Inf Fusion 6(1 ) : 3 - 4 CrossRef MathSciNet .Kusiak A ( 2000 ) Decomposition in data mining : an industrial case study .IEEE Trans Electron Packaging Manuf 23(4 ) : 345 - 353 CrossRef .Langdon WB , Barrett SJ , Buxton BF , ( 2002 )", "label": "", "metadata": {}, "score": "52.119667"}
{"text": "Learning Boolean formulae or finite automata is as hard as factoring .Technical Report TR-14 - 88 , Aiken Computation Laboratory , Harvard Univ .Z. .KEARNS , M. and VALIANT , L. G. 1989 .Cry ptographic limitations on learning Boolean formulae and finite automata .", "label": "", "metadata": {}, "score": "52.23776"}
{"text": "Miyao , Y. , Ninomiya , T. , and Tsujii , J. ( 2003 ) .Probabilistic modeling of argument structures including non - local dependencies .In RANLP 2003 : Proceedings of the Conference on Recent Advances in Natural Language Processing , pages 285 - 291 .", "label": "", "metadata": {}, "score": "52.30463"}
{"text": "A collection of classifiers are used and their output is combined using a decision combination function .Na\u00c3\u00afve Bayes .These classifiers work a generative framework in which each document is generated by a parametric distribution governed by a set of hidden parameters .", "label": "", "metadata": {}, "score": "52.359974"}
{"text": "[14 ] .Genetic Algorithms .Genetic algorithms are categorized as global search heuristics , and are a particular class of evolutionary algorithms ( also known as evolutionary computation ) that use techniques inspired by evolutionary biology such as inheritance , mutation , selection and recombination .", "label": "", "metadata": {}, "score": "52.413773"}
{"text": "In Abeill \u00b4 e , A. , editor , Treebanks : Building and Using Parsed Corpora , pages 249 - 260 .Kluwer Academic Publishers , Dordrecht .Lafferty , J. D. , McCallum , A. , and Pereira , F. C. N. ( 2001 ) .", "label": "", "metadata": {}, "score": "52.46318"}
{"text": "38 % ) tend to favor symbolic learning algorithms .We suggest how classification algorith ... . \" ...Machine learning is an area where both symbolic and neural approaches have been heavily investigated .However , there has been little research into the synergies achievable by combining these two learning paradigms .", "label": "", "metadata": {}, "score": "52.500626"}
{"text": "In : Monard C ( ed ) Advances on artificial intelligence - SBIA2000 .LNAI 1952 , Springer , Berlin , pp 269 - 279 .Gams M ( 1989 )New measurements highlight the importance of redundant knowledge .In : European working session on learning , Montpeiller , France , Pitman .", "label": "", "metadata": {}, "score": "52.685112"}
{"text": "Riezler , S. , King , T. H. , Kaplan , R. M. , Crouch , R. , John T. Maxwell , I. , and Johnson , M. ( 2001 ) .Parsing the Wall Street Journal using a Lexical - Functional Grammar and discriminative estimation techniques .", "label": "", "metadata": {}, "score": "52.740208"}
{"text": "Vapnik , V. N. ( 1998 ) .Statistical Learning Theory .Wiley - Interscience , New York , NY , USA .Vossen , P. , editor ( 1998 ) .EuroWordNet : A Multilingual Database with Lexical Semantic Networks .", "label": "", "metadata": {}, "score": "52.761253"}
{"text": "Introduction .Biometric Authentication Methods .Face Recognition : Reality and Challenge .Speaker Recognition : Reality and Challenge .Road Map of the Book .Biometric Authentication Systems .Introduction .Design Tradeoffs .Feature Extraction .Adaptive Classifiers .Visual - Based Feature Extraction and Pattern Classification .", "label": "", "metadata": {}, "score": "52.811413"}
{"text": "For any given instance , the class chosen by most number of classifiers is the ensemble decision .Since the training datasets may overlap substantially , additional measures can be used to increase diversity , such as using a subset of the training data for training each classifier , or using relatively weak classifiers ( such as decision stumps ) .", "label": "", "metadata": {}, "score": "52.87409"}
{"text": "Many of the above - discussed techniques are applied in combination to achiever better results and to overcome the deficiencies of other technique .An Intrusion Detection System ( IDS ) based on Principal Component Analysis ( PCA ) and Grey Neural Networks ( GNN ) is presented to improve the performance of BP neural networks in the field of intrusion detection .", "label": "", "metadata": {}, "score": "52.892715"}
{"text": "In CLIN 2001 : Computational Linguistics in the Netherlands , pages 8 - 22 .Vapnik , V. ( 2006 ) .Estimation of Dependences Based on Empirical Data .Springer .Vapnik , V. N. ( 1995 ) .The Nature of Statistical Learning Theory .", "label": "", "metadata": {}, "score": "53.00827"}
{"text": "Two recent tutorials written by the current curator of this article also provide a comprehensive overview of ensemble systems ( Polikar 2006 , Polikar 2007 ) .Diversity .The success of an ensemble system - that is , its ability to correct the errors of some of its members - rests squarely on the diversity of the classifiers that make up the ensemble .", "label": "", "metadata": {}, "score": "53.019295"}
{"text": "Induction of decision trees .Machine Learning , 1:81 - 106 .Quinlan , J.R. 1993 .C4.5 : Programs for Machine Learning .Morgan Kaufman .Rastogi , R. and Shim , K. 1998 .PUBLIC : A decision tree classifier that integrates building and pruning .", "label": "", "metadata": {}, "score": "53.158096"}
{"text": "Inf Sci 177 : 3592 - 3612 CrossRef .Dasarathy BV , Sheela BV ( 1979 )Composite classifier system design : concepts and methodology .Proc IEEE 67(5 ) : 708 - 713 CrossRef .Derbeko P , El - Yaniv R , Meir R ( 2002 ) Variance optimized bagging .", "label": "", "metadata": {}, "score": "53.238335"}
{"text": "A decision tree can then be used to classify a data point by starting at the root of the tree and moving through it until a leaf node is reached .The leaf node would then provide the classification of the data point .", "label": "", "metadata": {}, "score": "53.241028"}
{"text": "In the latter case , classifier outputs are often normalized to the [ 0 , 1 ] interval , and these values are interpreted as the support given by the classifier to each class , or as class - conditional posterior probabilities .", "label": "", "metadata": {}, "score": "53.268974"}
{"text": "Because of the independence assumption the parameters for every attribute can be learned separately .This makes the learning process very simple especially when the attributes are large .Fuzzy Logic .Fuzzy logic ( or fuzzy set theory ) is based on the concept of the fuzzy phenomenon to occur frequently in real world .", "label": "", "metadata": {}, "score": "53.318626"}
{"text": "Ensemble feature selection with the simple bayesian classification in medical diagnostics , In : Proceedings of 15th IEEE symposium on Computer - Based Medical Systems CBMS2002 , IEEE CS Press , Maribor , Slovenia , pp 225 - 230 .Tukey JW ( 1977 )", "label": "", "metadata": {}, "score": "53.528343"}
{"text": "Using LTAG based features in parse rerank- ing .In EMNLP 2003 : Proceedings of the ACL-03 Conference on Empirical Methods in Natural Language Processing , pages 89 - 96 .Stroppa , N. and Yvon , F. ( 2005 ) .", "label": "", "metadata": {}, "score": "53.639587"}
{"text": "Two - Class Probabilistic DBNNs .Multiclass Probabilistic DBNNs .Biometric Authentication Application Examples .Biometric Authentication by Face Recognition .Introduction .Facial Feature Extraction Techniques .Facial Pattern Classification Techniques .Face Detection and Eye Localization .PDBNN Face Recognition System Case Study .", "label": "", "metadata": {}, "score": "53.73881"}
{"text": "Two - Class Probabilistic DBNNs .Multiclass Probabilistic DBNNs .Biometric Authentication Application Examples .Biometric Authentication by Face Recognition .Introduction .Facial Feature Extraction Techniques .Facial Pattern Classification Techniques .Face Detection and Eye Localization .PDBNN Face Recognition System Case Study .", "label": "", "metadata": {}, "score": "53.73881"}
{"text": "LE CUN , Y. , BOSER , B. , DENKER , J. , HENDERSON , D. , HOWARD , R. , HUBBARD , W. and JACKEL , L. Z. 1990 .Handwritten digit recognition with a back - propagation network .Advances in Neural Information Processing Sy stems 2 396 404 .", "label": "", "metadata": {}, "score": "53.815285"}
{"text": "[ 11 ] .How to find the intrusion behaviors is a problem that troubled the intrusion detection field for years .Until now , there is not a good method to solve it , epically in a realistic context .Most methods are effective on small data sets , but when used to the massive data of IDS , the effectiveness seems to be unsatisfactory .", "label": "", "metadata": {}, "score": "53.907776"}
{"text": "Ensemble based on data envelopment analysis .ECML Meta Learning workshop .Tamon C , Xiang J ( 2000 )On the boosting pruning problem .In : Proceedings of the 11th European conference on machine learning , pp 404 - 412 .", "label": "", "metadata": {}, "score": "54.000465"}
{"text": "Arcing the edge ( Technical Report 486 ) .Berkeley : Statistics Department , University of California .Buntine , W. ( 1992a ) .Learning classification trees .Statistics and Computing , 2 ( 2 ) , 63 - 73 .", "label": "", "metadata": {}, "score": "54.029198"}
{"text": "The three classifiers are combined through a three - way majority vote .The pseudocode and implementation detail of boosting is shown in Figure 5 . \\ )Also , the ensemble error is a training error bound .Hence , a stronger classifier is generated from three weaker classifiers .", "label": "", "metadata": {}, "score": "54.21734"}
{"text": "Brown G , Wyatt JL ( 2003 )Negative correlation learning and the ambiguity family of ensemble methods .Proceedings of 4th international workshop , Mult Classifier Syst 2003 , Guilford , UK , June 11 - 13 , 2003 , Lecture Notes in Computer Science , vol 2709 , pp 266 - 275 .", "label": "", "metadata": {}, "score": "54.26689"}
{"text": "Yates W , Partridge D ( 1996 ) Use of methodological diversity to improve neural network generalization .Neural Comput Appl 4(2 ) : 114 - 128 CrossRef .Zhang Y , Burer S , Street WN ( 2006 ) Ensemble pruning via semi - definite programming .", "label": "", "metadata": {}, "score": "54.280327"}
{"text": "Naumov , G.E. 1991 .NP - completeness of problems of construction of optimal decision trees .Soviet Physics , Doklady , 36(4):270 - 271 .MATH MathSciNet .Quinlan , J.R. 1979 .Discovering rules by induction from large collections of examples .", "label": "", "metadata": {}, "score": "54.294582"}
{"text": "In : Genetic programming , proceedings of the 5th European conference , EuroGP 2002 , Kinsale , Ireland , pp 60 - 70 .Liu Y ( 2005 ) Generate different neural networks by negative correlation learning .ICNC 1 : 149 - 156 .", "label": "", "metadata": {}, "score": "54.449448"}
{"text": "The bracketing guidelines for the Penn Chinese treebank .Technical report , University of Pennsylvania .Yuret , D. and T\u00a8 ure , F. ( 2006 ) .Learning morphological disambiguation rules for Turk- ish .In HLT - NAACL 2006 : Proceedings of the Human Language Technology Confer- ence of the North American Chapter of the Association of Computational Linguistics , pages 328 - 334 .", "label": "", "metadata": {}, "score": "54.672295"}
{"text": "Two or more techniques are applied simultaneously .Rule Based Classification .Rules are inferred directly from the training data and used to classify the instance .Support Vector Machines .The SVM marks the region as normal where most of the normal instances are located .", "label": "", "metadata": {}, "score": "54.76554"}
{"text": "Parmanto B , Munro PW , Doyle HR ( 1996 ) Improving committee diagnosis with resampling techinques .In : Touretzky DS , Mozer MC , Hesselmo ME ( eds ) Advances in neural information processing systems , vol 8 .MIT Press , Cambridge , pp 882 - 888 .", "label": "", "metadata": {}, "score": "54.7697"}
{"text": "347 - 352 , 1993 .L. Xu , A. Krzyzak , and C.Y. Suen , Methods for combining multiple classifiers and their applications to handwriting recognition , IEEE Trans . on Systems , Man , and Cyb . , Vol .", "label": "", "metadata": {}, "score": "54.806576"}
{"text": "The simulations show that this architecture correctly classies attacks with detection rates exceeding 99 % and false alarms rates as low as 1.95 % .For next generation NIDS , anomaly detection methods must satisfy the demands of Gigabit Ethernet .FPGAs are an attractive medium to handle both high throughput and adaptability to the dynamic nature of intrusion detection .", "label": "", "metadata": {}, "score": "54.817993"}
{"text": "13 SCHAPIRE , R. E. , FREUND , Y. , BARTLETT , P. and LEE , W. S. 1998 .Boosting the margin : a new explanation for the effectiveness of voting methods .Ann .Statist .To appear .Q I , where X is one component of a fixed - length feature vector and m \u00c4 X c4 i i c is a constant .", "label": "", "metadata": {}, "score": "54.82376"}
{"text": "Chawla NV , Hall LO , Bowyer KW , Kegelmeyer WP ( 2004 )Learning ensembles from bites : a scalable and accurate approach .J Mach Learn Res Arch 5 : 421 - 451 MathSciNet .Chen K , Wang L , Chi H ( 1997 )", "label": "", "metadata": {}, "score": "54.83571"}
{"text": "Garey , M.R. and Johnson , D.S. 1979 .Computer and Intractability .Freeman and Company .Gillo , M.W. 1972 .MAID :A honeywell 600 program for an automatised survey analysis .Behavioral Science , 17:251 - 252 .Goldberg , D.E. 1989 .", "label": "", "metadata": {}, "score": "54.841324"}
{"text": "Decision Tree .Construct a tree based on the feature selection .k - NN .Soft Computing .The neighborhood is computed for each instance and depending on the neighbors ; instance is classified as normal or anomaly .This technique can be used both as supervised and un - supervised way .", "label": "", "metadata": {}, "score": "54.85279"}
{"text": "The partition is defined by selectin ... . by Walter Daelemans , Antal van den Bosch , Jakub Zavrel - MACHINE LEARNING , SPECIAL ISSUE ON NATURAL LANGUAGE LEARNING , 1999 . \" ...We show that in language learning , contrary to received wisdom , keeping exceptional training instances in memory can be beneficial for generalization accuracy .", "label": "", "metadata": {}, "score": "54.869728"}
{"text": "Freund , Y. ( 1996 ) .Boosting a weak learning algorithm by majority .Information and Computation , 121 ( 2 ) , 256 - 285 .Freund , Y. , & Schapire , R.E. ( 1995 ) .A decision - theoretic generalization of on - line learning and an application to boosting .", "label": "", "metadata": {}, "score": "54.906273"}
{"text": "Technical Report 8 , Department of Computer Science , University of Massachussetts , Amherst , MA .Catlett , J. 1991a .On changing continuos attributes into ordered discrete attributes .Proceedings of the European Working Session on Learning : Machine Learning , 482:164 - 178 .", "label": "", "metadata": {}, "score": "54.92336"}
{"text": "Conclusion & Future Works .This paper discussed the up to date work on intrusion detection using machine learning techniques .We have also discussed less - reviewed technique such as ILP , GP and PCA and provided a summary of work done in each of the technique .", "label": "", "metadata": {}, "score": "55.1397"}
{"text": "If a vast majority of the classifiers agree with their decisions , such an outcome can be interpreted as the ensemble having high confidence in its decision .If , however , half the classifiers make one decision and the other half make a different decision , this can be interpreted as the ensemble having low confidence in its decision .", "label": "", "metadata": {}, "score": "55.2513"}
{"text": "Backpropagation occasionally outperforms the other two systems when given relatively small amounts of training data .It is slightly more accurate than ID3 when examples are noisy or incompletely specified .Finally , backpropagation more effectively utilizes a & quot;distributed & quot ; output encoding .", "label": "", "metadata": {}, "score": "55.363575"}
{"text": "R. Polikar , L. Udpa , S. S. Udpa , and V. Honavar , \" Learn++ : An incremental learning algo - rithm for supervised neural networks , \" IEEE Transactions on Systems , Man and Cybernetics Part C : Applications and Reviews , vol .", "label": "", "metadata": {}, "score": "55.394547"}
{"text": "Everything else being equal , one may be tempted to choose at random , but with that decision comes the risk of choosing a particularly poor model .Using an ensemble of such models - instead of choosing just one - and combining their outputs by - for example , simply averaging them - can reduce the risk of an unfortunate selection of a particularly poorly performing classifier .", "label": "", "metadata": {}, "score": "55.42505"}
{"text": "Incremental learning refers to the ability of an algorithm to learn from new data that may become available after a classifier ( or a model ) has already been generated from a previously available dataset .Hence , an incremental learning algorithm must learn the new information , and retain previously acquired knowledge , without having access to previously seen data .", "label": "", "metadata": {}, "score": "55.49273"}
{"text": "146 - 156 , 2002 .R. E. Schapire , Y. Freund , P. Bartlett , and W. S. Lee , \" Boosting the Margin : A New Explanation for the Effectiveness of Voting Methods , \" Annals of Statistics , vol .", "label": "", "metadata": {}, "score": "55.513844"}
{"text": "Chapman & Hall .Elkan , C. ( 1997 ) .Boosting and naive bayesian learning ( Technical Report ) .San Diego : Department of Computer Science and Engineering , University of California .Fayyad , U.M. , & Irani , K.B. ( 1993 ) .", "label": "", "metadata": {}, "score": "55.656384"}
{"text": "Machine learning techniques .The day by day increase in network traffic has virtually eliminated the possibility of human supervision of network security .Even a system , assisting human administrators in taking decision over intrusion diction is not worthwhile as thousands of intrusion attempts may be made per minute .", "label": "", "metadata": {}, "score": "55.805893"}
{"text": "The generic algorithm is easy to instantiate with specific split selection methods from the literature ( including C4.5 , CART , CHAID , FACT , ID3 and extensions , SLIQ , SPRINT and QUEST ) .In addition to its generality , in that it yields scalable versions of a wide range of classification algorithms , our approach also offers performance improvements of over a factor of three over the SPRINT algorithm , the fastest scalable classification algorithm proposed previously .", "label": "", "metadata": {}, "score": "55.849735"}
{"text": "Rosenblatt , F. ( 1958 ) .The perceptron : A probabilistic model for information storage and organization in the brain .Psychological Review , ( 65):386 - 408 .Reprinted in Neurocomputing ( MIT Press , 1998 ) .Rosenfeld , R. ( 1996 ) .", "label": "", "metadata": {}, "score": "55.8949"}
{"text": "Anomaly based IDS work in the principle of learning from experience or learning by teacher .The anomaly detection algorithms have the advantage that they can detect new types of intrusions as deviations from normal usage .However , their weakness is the high false alarm rate .", "label": "", "metadata": {}, "score": "55.901096"}
{"text": "The error of this hypothesis with respect to the current distribution is calculated as the sum of distribution weights of the instances misclassified by \\(h_t\\ ) ( Equation 4 ) .AdaBoost . M1 requires that this error be less than \\(1/2\\ .", "label": "", "metadata": {}, "score": "55.959373"}
{"text": "A particular limitation of boosting is that it applies only to binary classification problems .This limitation is removed with the AdaBoost algorithm .AdaBoost .Arguably the best known of all ensemble - based algorithms , AdaBoost ( Adaptive Boosting ) extends boosting to multi - class and regression problems ( Freund 2001 ) .", "label": "", "metadata": {}, "score": "56.007935"}
{"text": "In Face Z. Recognition : From Theory to Applications H. Wechsler and J. Phillips , eds .Springer , Berlin .4 AMIT , Y. , GEMAN , D. and WILDER , K. 1997 .Joint induction of shape features and tree classifiers .", "label": "", "metadata": {}, "score": "56.05481"}
{"text": "Intern J Pattern Recognit Artif Intell 11(3 ) : 417 - 445 CrossRef .Cherkauer KJ ( 1996 ) Human expert - level performance on a scientific image analysis task by a system using combined artificial neural networks .In : Working notes , integrating multiple learned models for improving and scaling machine learning algorithms workshop , thirteenth national conference on artificial intelligence .", "label": "", "metadata": {}, "score": "56.084503"}
{"text": "J. Comput .Sy stem Sci .FRIEDMAN , J. H. 1996 .On bias , variance , 0 1-loss , and the curse of dimensionality .Journal of Knowledge Discovery and Data Mining .To appear .Z. .", "label": "", "metadata": {}, "score": "56.152916"}
{"text": "Samuel A ( 1967 )Some studies in machine learning using the game of checkers II : Recent progress .IBM J Res Develop 11 : 601 - 617 CrossRef .Saaty X ( 1996 )The analytic hierarchy process : A 1993 overview .", "label": "", "metadata": {}, "score": "56.16374"}
{"text": "Pattern Anal Appl 9 : 257 - 271 CrossRef MathSciNet .Rokach L ( 2008 )Genetic algorithm - based feature set partitioning for classification problems .Pattern Recognit 41(5 ) : 1676 - 1700 MATH CrossRef .Rokach L ( 2009 ) Collective - agreement - based pruning of ensembles .", "label": "", "metadata": {}, "score": "56.592834"}
{"text": "They demonstrate how to construct robust information processing systems for biometric authentication in both face and voice recognition systems , and to support data fusion in multimodal systems .Introduction .Biometric Authentication Methods .Face Recognition : Reality and Challenge .", "label": "", "metadata": {}, "score": "56.620575"}
{"text": "Z. .SCHAPIRE , R. , FREUND , Y. , BARTLETT , P. and LEE , W. S. 1998 .Boosting the margin : a new explanation for the effectiveness of voting methods .Ann .Statist .To appear .Z. Network Intrusion Detection Using Machine Techniques Information Technology Essay .", "label": "", "metadata": {}, "score": "56.621284"}
{"text": "In LREC 2004 : Pro- 158 .Page 173 . ceedings of the Fourth International Language Resources and Evaluation Conference , pages 39 - 42 .van den Bosch , A. ( 2004 ) .Wrapped progressive sampling search for optimizing learn- ing algorithm parameters .", "label": "", "metadata": {}, "score": "56.63024"}
{"text": "Figure 8 shows a particular code matrix for a 5-class problem that uses 15 encodings .This encoding , suggested in ( Dietterich 1995 ) , is a ( pseudo ) exhaustive coding because it includes all possible non - trivial and non - repeating codes .", "label": "", "metadata": {}, "score": "56.68803"}
{"text": "About this Article .Title .An Empirical Comparison of Voting Classification Algorithms : Bagging , Boosting , and Variants Arcing classifier ( with discussion and a rejoinder by the author ) .More by Leo Breiman .Abstract .Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error .", "label": "", "metadata": {}, "score": "56.796326"}
{"text": "Stacked generalization .Neural Networks , 5 , 241 - 259 .Wolpert , D.H. ( 1994 ) .The relationship between PAC , the statistical physics framework , the Bayesian framework , and the VC framework .In D.H. Wolpert ( Ed . ) , The mathematics of generalization .", "label": "", "metadata": {}, "score": "56.822453"}
{"text": "The sp ... \" .This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context .The algorithms tested include statistical , neural - network , decision - tree , rule - based , and case - based classification techniques .", "label": "", "metadata": {}, "score": "56.84448"}
{"text": "Figure 1 graphically illustrates this concept , where each classifier - trained on a different subset of the available training data - makes different errors ( shown as instances with dark borders ) , but the combination of the ( three ) classifiers provides the best decision boundary .", "label": "", "metadata": {}, "score": "56.855263"}
{"text": "In CoNNL 2005 : Proceedings of the 9th Conference on Computational Natural Language Learning , pages 120 - 127 .Taskar , B. , Guestrin , C. , and Koller , D. ( 2004 ) .Max - margin Markov networks .", "label": "", "metadata": {}, "score": "56.946487"}
{"text": "Therefore , need for an automated , highly accurate reliable intrusion detection system is more than ever .Machine learning is the area , capable of producing such desired system with required output .Even research is going on , a number of machine learning techniques have been used for intrusion detection and their summary is outlined on Table 1 .", "label": "", "metadata": {}, "score": "57.013275"}
{"text": "Three leading researchers bridge the gap between research , design , and deployment , introducing key algorithms as well as practical implementation techniques .They demonstrate how to construct robust information processing systems for biometric authentication in both face and voice recognition systems , and to support data fusion in multimodal systems .", "label": "", "metadata": {}, "score": "57.0412"}
{"text": "Diversity of classifiers in bagging is obtained by using bootstrapped replicas of the training data .That is , different training data subsets are randomly drawn - with replacement - from the entire training dataset .Each training data subset is used to train a different classifier of the same type .", "label": "", "metadata": {}, "score": "57.103127"}
{"text": "A new hybrid approach in combining multiple experts to recognize handwritten numerals .Pattern Recognit Lett 18 : 781 - 790 CrossRef .Ramamurti V , Ghosh J ( 1999 )Structurally adaptive modular networks for non - stationary environments .IEEE Trans Neural Netw 10(1 ) : 152 - 160 CrossRef .", "label": "", "metadata": {}, "score": "57.17357"}
{"text": "Stacked generalization .In : ( eds ) Neural Networks , vol 5 .Pergamon Press , Oxford , pp 241 - 259 .Yanim S , Kamel MS , Wong AKC , Wang Y ( 2007 )Cost - sensitive boosting for classification of imbalanced data .", "label": "", "metadata": {}, "score": "57.212852"}
{"text": "Minimum rule .\\(\\alpha\\rightarrow \\infty \\Rightarrow\\ ) maximum rule .Voting based methods .The ensemble then chooses class J that receives the largest total vote : .Majority ( plurality ) voting .Under the condition that the classifier outputs are independent , it can be shown the majority voting combination will always lead to a performance improvement .", "label": "", "metadata": {}, "score": "57.248085"}
{"text": "Levy , R. and Manning , C. ( 2003 ) .Is it harder to parse Chinese , or the Chinese tree- bank ?In ACL 2003 : Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics , pages 439 - 446 .", "label": "", "metadata": {}, "score": "57.41195"}
{"text": "Int J Neural Syst 6(5 ) : 373 - 399 CrossRef .Weston J , Watkins C ( 1999 ) Support vector machines for multi - class pattern recognition .In : Verleysen M ( ed ) Proceedings of the 7th European symposium on artificial neural networks ( ESANN-99 ) , Bruges , Belgium , pp 219 - 224 .", "label": "", "metadata": {}, "score": "57.421715"}
{"text": "5 , pp . 1651 - 1686 , 1998 .Y. S. Huang and C. Y. Suen , \" Behavior - knowledge space method for combination of mul - tiple classifiers , \" Proc . of IEEE Computer Vision and Pattern Recog .", "label": "", "metadata": {}, "score": "57.505585"}
{"text": "Artif Intell 70 : 119 - 165 MATH CrossRef .Tsoumakas G , Partalas I , Vlahavas I ( 2008 )A taxonomy and short review of ensemble selection .In : ECAI 2008 , workshop on supervised and unsupervised ensemble methods and their applications .", "label": "", "metadata": {}, "score": "57.589764"}
{"text": "The weighted majority voting then chooses the class \\(\\omega\\ ) receiving the highest total vote from all classifiers .This result may appear to go against the conventional wisdom ( see Occam 's razor ) indicating that adding too many classifiers - beyond a certain limit - would eventually lead to overfitting of the data .", "label": "", "metadata": {}, "score": "57.703983"}
{"text": "Learning them reliably permits grammar induction to depend less on language - specific LFG annotation rules .I therefore propose ways to improve acquisition of function labels from treebanks and translate those improvements into better - quality f - structure parsing .", "label": "", "metadata": {}, "score": "57.72225"}
{"text": "Morgan Kaufmann .Friedman , J.H. ( 1997 ) .On bias , variance , 0/1-loss , and the curse of dimensionality .Data Mining and Knowledge Discovery , 1 ( 1 ) , 55 - 77 .ftp://playfair.stanford.edu / pub / friedman / curse.ps .", "label": "", "metadata": {}, "score": "58.093018"}
{"text": "We provide a bias and variance decomposition of the error to show how different methods and variants influence these two terms .This allowed us to determine that Bagging reduced variance of unstable methods , while boosting methods ( AdaBoost and Arc - x4 ) reduced both the bias and variance of unstable methods but increased the variance for Naive - Bayes , which was very stable .", "label": "", "metadata": {}, "score": "58.224197"}
{"text": "In LFG 2005 : Proceedings of the Tenth International Conference on Lexical Functional Grammar , pages 334 - 352 .Oya , M. and van Genabith , J. ( 2007 ) .Automatic acquisition of Lexical - Functional Grammar resources from a Japanese dependency corpus .", "label": "", "metadata": {}, "score": "58.386597"}
{"text": "3541 , pp .326 - 335 , Seaside .Monterey , CA , June 2005 .M. Muhlbaier , A. Topalis , R. Polikar , \" Learn++ .NC : Combining Ensemble of Classifiers with Dynamically Weighted Consult - and - Vote for Efficient Incremental Learning of New Classes , \" IEEE Transactions on Neural Networks , In press , 2008 .", "label": "", "metadata": {}, "score": "58.496468"}
{"text": "In AdaBoost . M1 , bootstrap training data samples are drawn from a distribution \\(D\\ ) that is iteratively updated such that subsequent classifiers focus on increasingly difficult instances .This is done by adjusting \\(D\\ ) such that previously misclassified instances are more likely to appear in the next bootstrap sample .", "label": "", "metadata": {}, "score": "58.5321"}
{"text": "Biometric Authentication Application Examples .Multi - Layer Neural Networks .Introduction .Neuron Models .Multi - Layer Neural Networks .The Back - Propagation Algorithms .Two - Stage Training Algorithms .Genetic Algorithm for Multi - Layer Networks .", "label": "", "metadata": {}, "score": "58.542305"}
{"text": "Proceedings of the 13th International Joint Conference on Artificial Intelligence ( pp .1022 - 1027 ) .Morgan Kaufmann Publishers .Freund , Y. ( 1990 ) .Boosting a weak learning algorithm by majority .Proceedings of the Third Annual Workshop on Computational Learning Theory ( pp .", "label": "", "metadata": {}, "score": "58.56062"}
{"text": "The type of IDS built using ML techniques is called A - IDS or Anomaly based Intrusion Detection Systems .These IDS can work at the host level as well at the network level .The network - level IDS are preferred because of lower cost of ownership and overall protection of network .", "label": "", "metadata": {}, "score": "58.576225"}
{"text": "Ellis Horwood .Michie , D. , Spiegelhalter , D.J. , and Taylor , C.C. ( Eds . ) 1994b .Machine Learning , Neural and Statistical Classification .London : Ellis Horwood .Morgan , J.N. and Messenger , R.C. 1973 .", "label": "", "metadata": {}, "score": "58.672386"}
{"text": "Therefore , such systems are also known as multiple classifier systems , or just ensemble systems .There are several scenarios where using an ensemble based system makes statistical sense , which are discussed below in detail .For example , we typically ask the opinions of several doctors before agreeing to a medical procedure , we read user reviews before purchasing an item ( particularly big ticket items ) , we evaluate future employees by checking their references , etc .", "label": "", "metadata": {}, "score": "58.735985"}
{"text": "Suppose we examine some of the queries by constructing a single binary tree TT by the usual data - driven induction method : stepwise entropy reduction estimated from a training set LL .Since we can not entertain all possible splits at each node , we exploit a natural partial ordering on the set Q and examine only a tiny fraction of them .", "label": "", "metadata": {}, "score": "58.78076"}
{"text": "It aids people in many areas , such as business , entertainment and education .Business needs have motivated enterprises and governments across the globe to develop sophisticated , complex information networks .Such networks incorporate a diverse array of technologies , including distributed data storage systems , encryption and authentication techniques , voice and video over IP , remote and wireless access , and web services .", "label": "", "metadata": {}, "score": "59.15112"}
{"text": "IEEE Trans Neural Netw 14(4 ) : 820 - 834 CrossRef .Jenkins R , Yuhas BPA ( 1993 ) Simplified neural network solution through problem decomposition : The case of Truck backer - upper .IEEE Trans Neural Netw 4(4 ) : 718 - 722 CrossRef .", "label": "", "metadata": {}, "score": "59.232773"}
{"text": "Hu X ( 2001 )Using rough sets theory and database operations to construct a good ensemble of classifiers for data mining applications .ICDM01 .pp 233 - 240 .Islam MM , Yao X , Murase K ( 2003 )", "label": "", "metadata": {}, "score": "59.296585"}
{"text": "Before the flourishing of the Internet , computers were limited to the walls of the organization where computers were linked to each other but had little contact with computer systems outside .Now , we can reach farther , and can expose and link our computers to the entire world ... \" .", "label": "", "metadata": {}, "score": "59.453094"}
{"text": "This update rule ensures that the weights of all correctly classified instances and the weights of all misclassified instances always add up to \\ ( 1/2\\ .\\ )More specifically , the requirement for the training error of the base classifier to be less than \\ ( 1/2\\ ) forces teh algorithm to correct at least one mistake made by the previous base model .", "label": "", "metadata": {}, "score": "59.565323"}
{"text": "Feature forest models for probabilistic HPSG parsing .Computational Linguistics , 34(1):35 - 80 .Mooney , R. J. and Califf , M. E. ( 1995 ) .Induction of first - order decision lists : Results on learning the past tense of English verbs .", "label": "", "metadata": {}, "score": "59.6582"}
{"text": "The distribution \\(D\\ ) starts out as uniform ( Equation 3 in Figure 6 ) , so that all instances have equal probability to be drawn into the first data subset \\(S_1\\ .\\ )At each iteration t , a new training set is drawn , and a weak classifier is trained to produce a hypothesis \\(h_t\\ .", "label": "", "metadata": {}, "score": "60.123077"}
{"text": "In ACL 1999 : Proceedings of the 37th Annual Conference of the Association for Computational Linguistics , pages 535 - 541 .Jurafsky , D. and Martin , J. H. ( 2008 ) .Speech and Language Processing .Prentice Hall , 2 edition .", "label": "", "metadata": {}, "score": "60.128475"}
{"text": "Introduction .Sensor Fusion for Biometrics .Hierarchical Neural Networks for Sensor Fusion .Multisample Fusion .Audio and Visual Biometric Authentication .Concluding Remarks .Appendix A. Convergence Properties of EM .Appendix B. Average DET Curves .Appendix C. Matlab Projects .", "label": "", "metadata": {}, "score": "60.20227"}
{"text": "Introduction .Sensor Fusion for Biometrics .Hierarchical Neural Networks for Sensor Fusion .Multisample Fusion .Audio and Visual Biometric Authentication .Concluding Remarks .Appendix A. Convergence Properties of EM .Appendix B. Average DET Curves .Appendix C. Matlab Projects .", "label": "", "metadata": {}, "score": "60.20227"}
{"text": "Assuming that anomalies can be treated as outliers , an intrusion predictive model is constructed from the major and minor principal components of normal instances .A measure of the difference of an anomaly from the normal instance is the distance in the principal component space .", "label": "", "metadata": {}, "score": "60.230953"}
{"text": "If a classifier can not meet this requirement , the algorithm aborts .The normalized error \\(\\beta_t\\ , \\ ) is then computed ( Equation 5 ) so that the actual error that is in the [ 0 0.5 ] interval is mapped to [ 0 1 ] interval .", "label": "", "metadata": {}, "score": "60.23426"}
{"text": "J Mach Learn Res 8 : 1 - 33 MathSciNet .Hampshire JB , Waibel A ( 1992 )The meta - Pi network - building distributed knowledge representations for robust multisource pattern - recognition .Pattern Anal Mach Intell 14(7 ) : 751 - 769 CrossRef .", "label": "", "metadata": {}, "score": "60.257973"}
{"text": "Keywords : decision tree , incremental induction , direct metric , binary test , example incorporation , missing value , tree transposition , installed test , virtual pruning , update cost .Introduction Decision tree induction offers a highly practical method for generalizing from examples whose class membership is known .", "label": "", "metadata": {}, "score": "60.268265"}
{"text": "Statistical Variance .Components of data are formed based on the variance of data instances .Genetic Algorithm .Evolutionary Computing .Genetic algorithm work on the principal of evolutionary biological techniques .Inductive Logic Programming ( ILP ) .Logic Induction .", "label": "", "metadata": {}, "score": "60.72695"}
{"text": "Concluding Remarks .Expectation - Maximization Theory .Introduction .Traditional Derivation of EM .An Entropy Interpretation .Doubly - Stochastic EM .Concluding Remarks .Support Vector Machines .Introduction .Fisher 's Linear Discriminant Analysis .Linear SVMs : Separable Case .", "label": "", "metadata": {}, "score": "60.787003"}
{"text": "Rokach L , Arbel R , Maimon O ( 2006 ) Selective voting - getting more for less in sensor fusion .Intern J Pattern Recognit Artif Intell 20(3 ) : 329 - 350 CrossRef .Rokach L , Maimon O ( 2005a ) Top down induction of decision trees classifiers : A survey .", "label": "", "metadata": {}, "score": "61.058296"}
{"text": "Markting Information Systems , Report 11 - 130 .Magidson , J. 1993a .The CHAID approach to segmentation modeling .In Handbook of Marketing Research , R. Bagozzi ( Ed . )Blackwell .Magidson , J. 1993b .The use of the new ordinal algorithm in CHAID to target profitable segments .", "label": "", "metadata": {}, "score": "61.385555"}
{"text": "Computer Science , Oregon State Univ . , Corvallis , Oregon .Available from ftp : ftp.cs.orst .edu pub tgd papers tr-bias.ps gz .Z. .FREUND , Y. and SCHAPIRE , R. E. 1996 .Experiments with a new boosting algorithm .", "label": "", "metadata": {}, "score": "61.403725"}
{"text": "IEEE Trans Pattern Anal Mach Intell 20(8 ) : 832 - 844 CrossRef .Holmstrom L , Koistinen P , Laaksonen J , Oja E ( 1997 ) Neural and statistical classifiers - taxonomy and a case study .IEEE Trans Neural Netw 8 : 5 - 17 CrossRef .", "label": "", "metadata": {}, "score": "61.584053"}
{"text": "Engineering multiversion neural - net systems .Neural Comput 8(4 ) : 869 - 893 CrossRef .Passerini A , Pontil M , Frasconi P ( 2004 )New results on error correcting output codes of kernel machines .IEEE Trans Neural Netw 15 : 45 - 54 CrossRef .", "label": "", "metadata": {}, "score": "61.61511"}
{"text": "Now , we can reach farther , and can expose and link our computers to the entire world . by Keun - hee Han , Il - gon Kim , Kang - won Lee , Jin - young Choi , Sang - hun Jeon , 2004 . \" ...", "label": "", "metadata": {}, "score": "62.220024"}
{"text": "Twelve datasets were used : five from image analysis , three from medicine , and two each from engineering and finance .We found that which algorithm performed best depended critically on the dataset investigated .We therefore developed a set of dataset descriptors to help decide which algorithms are suited to particular datasets .", "label": "", "metadata": {}, "score": "62.295135"}
{"text": "10 WILDER , K. 1998 .Decision tree algorithms for handwritten digit recognition .Ph.D. dissertation , Univ .Massachusetts , Amherst .CHICAGO , ILLINOIS 60637 UNIVERSITY OF MASSACHUSETTS E - MAIL : amit@galton.uchicago.edu AMHERST , MASSACHUSETTS 01003 E - MAIL : geman@math.umass.edu .", "label": "", "metadata": {}, "score": "62.45874"}
{"text": "In each case , a final decision is made by combining the individual decisions of several experts .In doing so , the primary goal is to minimize the unfortunate selection of an unnecessary medical procedure , a poor product , an unqualified employee or even a poorly written and misguiding article .", "label": "", "metadata": {}, "score": "63.204643"}
{"text": "We compare the mean - squared error of voting methods to non - voting methods and show that the voting methods lead to large and significant reductions in the mean - squared errors .Practical problems that arise in implementing boosting algorithms are explored , including numerical instabilities and underflows .", "label": "", "metadata": {}, "score": "64.26615"}
{"text": "Support Vector Machine .SVM is learning methods introduced by [ 23].SVM are based on the structural risk minimization principal from the computational theory .SVM use the Vapnik - Chervonenkis ( VC ) dimensions of a problem to characterize its complexity , which can be independent of the dimensionality of the problem .", "label": "", "metadata": {}, "score": "64.39504"}
{"text": "The negative sign converts the distance metric into a support value , whose largest value can be zero in case of a perfect match .Note that this output does not match any of the code words exactly , and this is where the error correcting ability of the ECOC lies .", "label": "", "metadata": {}, "score": "65.00921"}
{"text": "Algebraic combiners .Algebraic combiners are non - trainable combiners , where continuous valued outputs of classifiers are combined through an algebraic expression , such as minimum , maximum , sum , mean , product , median , etc .Specifically .", "label": "", "metadata": {}, "score": "65.37755"}
{"text": "IEEE Trans Pattern Anal Mach Intell 12(10 ) : 993 - 1001 CrossRef .Hansen J ( 2000 )Combining predictors .Meta machine learning methods and bias , variance & ambiguity decompositions .PhD Dissertation .Aurhus University .Ho TK ( 1998 )", "label": "", "metadata": {}, "score": "65.60911"}
{"text": "Mart \u00b4 \u0131 , M. A. , Taul \u00b4 e , M. , Bertran , M. , and M ' arquez , L. ( 2007 ) .AnCora : Multilingual and multilevel annotated corpora .edu / ancora / ancora - corpus .", "label": "", "metadata": {}, "score": "65.66211"}
{"text": "That is , in fuzzy logic the degree of truth of a statement can range between 0 and 1 and it is not constrained to the two truth values ( i.e. true , false ) .Raining may be able to convert the circumstances from slight to violent .", "label": "", "metadata": {}, "score": "65.742676"}
{"text": "Next , five layers of the grey neural networks is designed based on BP neural networks and Grey theory , then the IDS composed of sniffer module , data processing module , grey neural network module and intrusion detection module is presented .", "label": "", "metadata": {}, "score": "66.11862"}
{"text": "Road Map of the Book .Biometric Authentication Systems .Introduction .Design Tradeoffs .Feature Extraction .Adaptive Classifiers .Visual - Based Feature Extraction and Pattern Classification .Audio - Based Feature Extraction and Pattern Classification .Concluding Remarks .", "label": "", "metadata": {}, "score": "66.50024"}
{"text": "Cross validation type selection is typically used for training the Tier 1 classifiers : the entire training dataset is divided into T blocks , and each Tier-1 classifier is first trained on ( a different set of ) T -1 blocks of the training data .", "label": "", "metadata": {}, "score": "67.71129"}
{"text": "Finite - state machines solving analogies on words .Technical report , ENST .Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .", "label": "", "metadata": {}, "score": "68.19884"}
{"text": "In order to enhance the availability and practicality of intelligent intrusion detection system based on machine learning in high - speed network , an improved fast inductive learning method for intrusion detection ( FILMID ) is designed and implemented .Accordingly , an efficient intrusion detection model based on FILMID algorithm is presented .", "label": "", "metadata": {}, "score": "68.30465"}
{"text": "Boosting the margin : A new explanation for the effectiveness of voting methods .In D. Fisher ( Ed . ) , Machine Learning : Proceedings of the Fourteenth International Conference ( pp .322 - 330 ) .Morgan Kaufmann .", "label": "", "metadata": {}, "score": "69.03114"}
{"text": "[5 ] .Inductive Login Programming .Earn money as a Freelance Writer !We 're looking for qualified experts .As we are always expanding we are looking to grow our team of freelance writers .To find out more about writing with us then please check our freelance writing jobs page .", "label": "", "metadata": {}, "score": "69.342834"}
{"text": "Caruana R , Niculescu - Mizil A , Crew G , Ksikes A ( 2004 )Ensemble selection from libraries of models , twenty - first international conference on Machine learning , July 04 - 08 , 2004 , Banff , Alberta , Canada .", "label": "", "metadata": {}, "score": "69.96425"}
{"text": "Kernel functions are used for nonlinear separation .The groups of vectors that lie near the separating hyperplane are called support vectors .Once the separating hyper plane is found the new examples can be classifies by simply checking that on which side of the hyperplane they fall .", "label": "", "metadata": {}, "score": "69.973305"}
{"text": "Therefore , information security of using Internet as the media needs to be carefully concerned .Intrusion detection is one major research problem for business and personal networks .There has been much discussion on explaining the intrusion detection .A good source for the readers to get familiar with IDS is [ 15].", "label": "", "metadata": {}, "score": "69.9899"}
{"text": "[ 3 ] .Most current intrusion detection systems are signature based ones or machine learning based methods .Despite the number of machine learning algorithms applied to KDD 99 cup , none of them have introduced a pre - model to reduce the huge information quantity present in the different KDD 99 datasets .", "label": "", "metadata": {}, "score": "70.40669"}
{"text": "Page 7 .Page 8 .List of Tables 2.1 LFG Grammatical functions . . . . . . . . . . . . . . . . . . . . . . . ..10 5.1 Features included in POS tags .", "label": "", "metadata": {}, "score": "70.42526"}
{"text": "Differing provisions from the publisher 's actual policy or licence agreement may be applicable .[ Show abstract ] [ Hide abstract ] ABSTRACT : Proceedings of the Ninth International Workshop on Treebanks and Linguistic Theories .Editors : Markus Dickinson , Kaili M\u00fc\u00fcrisep and Marco Passarotti .", "label": "", "metadata": {}, "score": "70.57591"}
{"text": "Learning the Past Tense of English Verbs : The Symbolic Pattern Associator vs. Connectionist Models .Journal of Artificial Intelligence Research , 1:209 - 229 .Magerman , D. ( 1994 ) .Natural Language Parsing as Statistical Pattern Recognition .PhD thesis , Department of Computer Science , Stanford University , CA .", "label": "", "metadata": {}, "score": "70.76373"}
{"text": "Voting variants , some of which are introduced in this paper , include : pruning versus no pruning , use of probabilistic estimates , weight perturbations ( Wagging ) , and backfitting of data .We found that Bagging improves when probabilistic estimates in conjunction with no - pruning are used , as well as when the data was backfit .", "label": "", "metadata": {}, "score": "71.23149"}
{"text": "[ Show abstract ] [ Hide abstract ] ABSTRACT : Proceedings of the Ninth International Workshop on Treebanks and Linguistic Theories .Editors : Markus Dickinson , Kaili M\u00fc\u00fcrisep and Marco Passarotti .NEALT Proceedings Series , Vol .\u00a9 2010 The editors and contributors .", "label": "", "metadata": {}, "score": "72.48111"}
{"text": "HMM is a double stochastic process .The upper layer is a Markov process whose states are not observable .The lower layer is a normal Markov process where emitted outputs can be observed .HMM is a powerful tool in modeling and analyzing complicated stochastic process .", "label": "", "metadata": {}, "score": "72.71246"}
{"text": "The classifier based on Z. Z. TT is then C Q , LL arg max P Y j TT .If the depths of the leaves of TT j Z. are far smaller than M , then evidently C Q , LL is not the Bay es classifier .", "label": "", "metadata": {}, "score": "73.18697"}
{"text": "\u00a9 2010 The editors and contributors .[ Show abstract ] [ Hide abstract ] ABSTRACT : Proceedings of the Ninth International Workshop on Treebanks and Linguistic Theories .Editors : Markus Dickinson , Kaili M\u00fc\u00fcrisep and Marco Passarotti .NEALT Proceedings Series , Vol .", "label": "", "metadata": {}, "score": "73.856224"}
{"text": "We have previously introduced ' inference functions ' to estimate the social inference ... \" .A serious threat to user privacy in new mobile and web2.0 applications stems from ' social inferences ' .These unwanted inferences are related to the users ' identity , current location and other personal information .", "label": "", "metadata": {}, "score": "73.99276"}
{"text": "The reasons that SVMs work well for TC is that during learning classifiers , one has to deal with many features such as more than 10,000 .Since SVM use over fitting protection that does not depend on the number of features and have the potential to deal with the large number of attributes .", "label": "", "metadata": {}, "score": "75.62988"}
{"text": "Towards a Machine - Learning Architecture for Lexical Functional Grammar Parsing Grzegorz Chrupa ? la A dissertation submitted in fulfilment of the requirements for the award of Doctor of Philosophy ( Ph.D. ) to the Dublin City University School of Computing Supervisor : Prof. Josef van Genabith April 2008 .", "label": "", "metadata": {}, "score": "78.36131"}
{"text": "Concluding Remarks .Biometric Authentication by Voice Recognition .Introduction .Speaker Recognition .Kernel - Based Probabilistic Speaker Models .Handset and Channel Distortion .Blind Handset - Distortion Compensation .Speaker Verification Based on Articulatory Features .Concluding Remarks .", "label": "", "metadata": {}, "score": "81.75012"}
{"text": "Concluding Remarks .Biometric Authentication by Voice Recognition .Introduction .Speaker Recognition .Kernel - Based Probabilistic Speaker Models .Handset and Channel Distortion .Blind Handset - Distortion Compensation .Speaker Verification Based on Articulatory Features .Concluding Remarks .", "label": "", "metadata": {}, "score": "81.75012"}
{"text": "Company Registration No : 4964706 .VAT Registration No : 842417633 .Registered Data Controller No : Z1821391 .Registered office : Venture House , Cross Street , Arnold , Nottingham , Nottinghamshire , NG5 7PJ . \" ...A serious threat to user privacy in new mobile and web2.0 applications stems from ' social inferences ' .", "label": "", "metadata": {}, "score": "82.907684"}
{"text": "around ten was about 90 % on test sets similar to the one discussed by Leo Breiman .TT , P Y k TT Y j , 1 n , m N. Naturally , small covariances lead to n m small errors .", "label": "", "metadata": {}, "score": "84.10596"}
{"text": "Signed ( Grzegorz Chrupa ? la ) Student ID 55130089 Date April 2008 i .Page 3 .Page 4 .Page 5 .Page 6 .Below the corresponding ( simplified ) LFG f - structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "label": "", "metadata": {}, "score": "93.69908"}
