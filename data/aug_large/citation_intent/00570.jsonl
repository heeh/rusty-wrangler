{"text": "The probability calculations are then output in the proper syntax of a programming language such that other code in the same programming language can interface with it , such as to request it to calculate the probabilities of variables in the Bayesian network .", "label": "", "metadata": {}, "score": "32.62272"}
{"text": "The probability calculations are then output in the proper syntax of a programming language such that other code in the same programming language can interface with it , such as to request it to calculate the probabilities of variables in the Bayesian network .", "label": "", "metadata": {}, "score": "32.62272"}
{"text": "We only consider situations where the data are complete , that is , ev- ery variable in X is assigned a value .( 1 ) We can then compute the posterior probability of any hypothesis of interest by averaging over all possible networks .", "label": "", "metadata": {}, "score": "32.964386"}
{"text": "If all values f ( V , Y , P ) are known , it is then easy to compute the optimal playing strategy .Recall the payoff matrix [ X ij ] from the previous subsection .The optimal strategy , for player one , will take the form of a list of probabilities x i of playing card V i .", "label": "", "metadata": {}, "score": "32.993786"}
{"text": "The first upper bound of spectral probabilities .Let q denote the right hand part of the above inequality .The value of q is an upper bound of SpecProb ( S , t , 1 ) .Next , we describe a dynamic programming algorithm for computing the value of q .", "label": "", "metadata": {}, "score": "33.508865"}
{"text": "In practical applications , however , there might only be partial information about the probability of a premise \u03b3 : its exact value is not known , but it is known to have a lower bound a and an upper bound b ( Walley 1991 ) .", "label": "", "metadata": {}, "score": "34.108364"}
{"text": "From Equation ( 1 ) and the union bound of probabilities , .Let q ' denote the right hand part of the above inequality .Compared with q , the value of q ' is a better upper bound for SpecProb ( S , t , 1 ) .", "label": "", "metadata": {}, "score": "34.52549"}
{"text": "Using this restricted probability language , we can reason about additivity in a less direct way .The formula .In Fagin et al .( 1990 ) , a sound and complete proof system is given for a logic involving linear combinations , where axioms are given for linear combinations .", "label": "", "metadata": {}, "score": "35.06664"}
{"text": "Finally , it should be noted that with comparative probability ( a binary operator ) , one can also express some absolute probabilistic properties ( unary operators ) .The semantics of propositional probability logic involves a probability function P , satisfying certain properties .", "label": "", "metadata": {}, "score": "35.164455"}
{"text": "In general , however , it will not be possible to calculate the exact probability of the conclusion , given the probabilities of the premises ; rather , the best we can hope for is a ( tight ) upper and/or lower bound for the conclusion 's probability .", "label": "", "metadata": {}, "score": "35.268646"}
{"text": "The values of other nodes are called evidence and represented by e. .[ 0021 ] Other probabilities are also useful .As one example , the joint probability of a node Q and a node F , P(Q , F\\e ) , is used in the calculation of mutual information .", "label": "", "metadata": {}, "score": "35.66663"}
{"text": "The values of other nodes are called evidence and represented by e. .[ 0021 ] Other probabilities are also useful .As one example , the joint probability of a node Q and a node F , P(Q , F\\e ) , is used in the calculation of mutual information .", "label": "", "metadata": {}, "score": "35.66663"}
{"text": "Adams ( 1998 , 154 ) also defines another logic for which his probabilistic semantics is sound and complete .However , this system involves a non - truth - functional connective ( the probability conditional ) , and therefore falls outside the scope of this section .", "label": "", "metadata": {}, "score": "36.171135"}
{"text": "In other words , a standard inference algorithm is modified by partial evaluation , to store in memory the calculations it would perform in order to compute the probabilities , instead of actually carrying out those calculations .Then a source code generator 230 outputs the probability calculations in a source code 240 in a desired programming language .", "label": "", "metadata": {}, "score": "36.198093"}
{"text": "In other words , a standard inference algorithm is modified by partial evaluation , to store in memory the calculations it would perform in order to compute the probabilities , instead of actually carrying out those calculations .Then a source code generator 230 outputs the probability calculations in a source code 240 in a desired programming language .", "label": "", "metadata": {}, "score": "36.198093"}
{"text": "stating that the probability of the union of two disjoint sets is at least the sum of lower bounds of the probabilities of each of the sets , and .Many probability logics are interpreted over a single , but arbitrary probability space .", "label": "", "metadata": {}, "score": "36.30695"}
{"text": "Let us first look at what can be expressed using linear combinations of a basic primitive form .Here are some examples of what can be expressed .Note that we do not even consider coefficients of the probability term .We can define .", "label": "", "metadata": {}, "score": "36.580563"}
{"text": "( 28 ) and ( 25 ) leads to ( 24 ) .Based on Proposition 2 , H(S ) can be computed in the manner of dynamic programming .Each H(S ) is computed in time ?We could store all F(S , T ) and compute all H(S ) in time O(3n ) .", "label": "", "metadata": {}, "score": "36.65827"}
{"text": "There is a way to significantly reduce the number of games needed to be analyzed .Sheldon Ross [ 2 ] describes a recursive rule expressing the value of a game as a function of the values of smaller games .We give a further simplification of his rule .", "label": "", "metadata": {}, "score": "36.750816"}
{"text": "We 'd like to have a probability distribution which , outside of these constraints , is as uniform as possible -- has the maximum entropy among all models which satisfy these constraints .Suppose we have a tagging task , where we want to assign a tag t to a word w based on the ' context ' h of w ( the words around w , including w itself ) .", "label": "", "metadata": {}, "score": "36.913292"}
{"text": "Then we describe how to com- bine the two computations to reduce the total compu- tation time .As in Section 3 we can show that H(S ) can be computed recursively .We have the following results .Page 5 .", "label": "", "metadata": {}, "score": "37.153862"}
{"text": "The output distribution for that state is Gaussian , with the mean and covariance matrix associated with the codeword .The transition probabilities for the model states are trained using the known Baum - Welch algorithm on the same training utterance .", "label": "", "metadata": {}, "score": "37.50245"}
{"text": "This topic is covered in detail in Gillies ( 2000 ) , Eagle ( 2010 ) , and the entry on \" Interpretations of Probability \" of this encyclopedia .Finally , although the success of probability logic is largely due to its various applications , we will not deal with these applications in any detail .", "label": "", "metadata": {}, "score": "37.50892"}
{"text": "It takes O(n3n+ kn22n ) total time to compute the posterior probabilities for all possible edges .The computation time of Step 2 can be further re- duced by a factor of n using the techniques described in [ Koivisto , 2006].", "label": "", "metadata": {}, "score": "37.81282"}
{"text": "As a result , it is impractical to sum over all possible structures unless for very small networks ( less than 8 variables ) .One solution is to compute ap- proximate posterior probabilities .Madigan and York ( 1995 ) used Markov chain Monte Carlo ( MCMC ) al- gorithm in the space of network structures .", "label": "", "metadata": {}, "score": "37.9729"}
{"text": "One can find such a system in Bacchus ( 1990 ) .Generally it is hard to provide proof systems for first - order probability logics , because the validity problem for these logics is generally undecidable .It is even not the case , as it is the case in classical first - order logic , that if an inference is valid , then one can find out in finite time ( see Abadi and Halpern ( 1994 ) ) .", "label": "", "metadata": {}, "score": "37.992622"}
{"text": "Results of coin flips , on the other hand , are often used examples of where we would assign probabilities to individual outcomes .One way to formalize the interaction between probability and qualitative uncertainty is by adding another relation to the model and a modal operator to the language as is done in Fagin and Halpern ( 1988 , 1994 ) .", "label": "", "metadata": {}, "score": "38.40055"}
{"text": "This model is created by letting each codeword represent a state , with an associated Gaussian output distribution as described above , and permitting transitions between all states .Alternatively , transitions between states can be weighted by bigram probabilities ( i.e. , the transition probabilities between codewords ) determined from the data used to create the codebook .", "label": "", "metadata": {}, "score": "38.48276"}
{"text": "The semantics for such operators will then have to provide a probability measure on subsets of D n .This approach is taken by Bacchus ( 1990 ) and Halpern ( 1990 ) , corresponding to the idea that selections are independent and with replacements .", "label": "", "metadata": {}, "score": "38.577675"}
{"text": "Intuitively , such a change may be caused by new information that invokes a probabilistic revision at each possible world .The dynamics of subjective probabilities is often modeled using conditional probabilities , such as in Kooi ( 2003 ) , Baltag and Smets ( 2008 ) , and van Benthem et al .", "label": "", "metadata": {}, "score": "38.839447"}
{"text": "This notion is not defined with respect to formulas , but rather with respect to pairs consisting of a formula and a subinterval of [ 0,1].In Haenni et al .( 2011 ) this is written as .and called the standard probabilistic semantics .", "label": "", "metadata": {}, "score": "39.23224"}
{"text": "In the next two subsections we will consider more interesting cases , when there is non - zero uncertainty about the premises , and ask how it carries over to the conclusion .Most of these systems are not based on unary probabilities P ( \u03d5 ) , but rather on conditional probabilities P ( \u03d5 , \u03c8 ) .", "label": "", "metadata": {}, "score": "39.276703"}
{"text": "We can then compute the posterior probability of any hypothesis of interest by averaging over all possible networks .In many applications we are interested in structural features .For example , in causal discovery , we are interested in the causal relations among vari- In ables , represented by the edges in the network struc- ture [ Heckerman et al . , 1999].", "label": "", "metadata": {}, "score": "39.51465"}
{"text": "This is similar to the scoring used in Rose et al 's April 1990 paper for post processing keyword segments found by dynamic programming .The start time t.sub.s for the keyword is chosen to maximize the score S(t , t.sub.e ) .", "label": "", "metadata": {}, "score": "39.604656"}
{"text": "G Bi(Pai ) , ( 7 ) .Page 3 .( 8) It is clear from Eq .In the next section , we show how the summation in Eq .( 7 ) can be done by dynamic programming in time complexity O(3n ) .", "label": "", "metadata": {}, "score": "39.880085"}
{"text": "We present empirical results on structural discovery over several real and synthetic data sets and show that the method outperforms the model selection method and the state of - the - art MCMC methods .\" Likewise , the posterior probability of an arbitrary fixed arc set can be computed by analogous dynamic programming techniques in time and space within a polynomial factor of 2 n ( Koivisto and Sood , 2004 ; Koivisto , 2006 ) .", "label": "", "metadata": {}, "score": "39.93326"}
{"text": "Thus constraints are often placed on the models to ensure that such sets are always in the \u03c3 - algebras .Although probabilities reflect quantitative uncertainty at one level , there can also be qualitative uncertainty about probabilities .There are many situations in which we might not want to assign numerical values to uncertainties .", "label": "", "metadata": {}, "score": "39.94184"}
{"text": "To compute the expected values , we use the values computed by the Viterbi algorithm .Assume the input is w 1 , ... w T , and the states are numbered 1 to N. The forward probability alpha j ( t ) is the probability ( for a given input ) of being in state j and generating the first t words of the input .", "label": "", "metadata": {}, "score": "39.96843"}
{"text": "With the spectral probabilities reported by TD - GF , the \" estimated \" FDR of a set of identified PrSMs for a cut - off p -value can be computed using the functions in [ 7 ] .For the same cut - off p -value , the \" correct \" FDR can be obtained by the target - decoy approach .", "label": "", "metadata": {}, "score": "40.218697"}
{"text": "In this paper , we study an extended generating function method for accurately computing spectral probabilities and statistical significance of PrSMs in top - down MS .Our method naturally extends the generating function method in bottom - up MS [ 8 ] .", "label": "", "metadata": {}, "score": "40.298656"}
{"text": "Furthermore , people are often willing to compare the probabilities of two statements ( ' \u03d5 is more probable than \u03c8 ' ) , without being able to assign explicit probabilities to each of the statements individually ( Szolovits and Pauker 1978 ; Halpern and Rabin 1987 ) .", "label": "", "metadata": {}, "score": "41.31788"}
{"text": "It can be shown that classical propositional logic is ( strongly ) sound and complete with respect to probabilistic semantics : .Some authors interpret probabilities as generalized truth values ( Reichenbach 1949 ; Leblanc 1983 ) .According to this view , probability logic is just a particular kind of many - valued logic , and probabilistic validity boils down to \" truth preservation \" : truth ( i.e. , probability 1 ) carries over from the premises to the conclusion .", "label": "", "metadata": {}, "score": "41.37788"}
{"text": "We present a new approach to model counting that is based on adding a carefully chosen number of so - called streamlining constraints to the input formula in order to cut down the size of its solution space in a controlled manner .", "label": "", "metadata": {}, "score": "41.498875"}
{"text": "We see that with the increase of k the computed probabili- ties gradually approach the true probabilities .Studying the effects of the ap- proximation due to the maximum indegree restriction in general need more substantial experiments and is beyond the scope of this paper .", "label": "", "metadata": {}, "score": "41.50705"}
{"text": "Then one can extend the language with arithmetical operations such as addition and multiplication , and with operators such as equality and inequalities to compare probability terms .Such an extension requires that the language contains two separate classes of terms : one for probabilities , numbers and the results of arithmetical operations on such terms , and one for the domain of discourse which the probabilistic operators quantify over .", "label": "", "metadata": {}, "score": "41.532585"}
{"text": "Both interpretations can use exactly the same formal framework .The reading of such a formula is that the probability of \u03d5 is at least q .This general reading of the formula does not reflect any difference between modal probability logic and other probability logics with the same formula ; where the difference lies is in the ability to embed probabilities in the arguments of probability terms and in the semantics .", "label": "", "metadata": {}, "score": "41.54631"}
{"text": "In one case the language is altered slightly ( Subsection 4.2 ) , and in other cases , the logic is extended to address interactions between qualitative and quantitative uncertainty ( Subsection 4.4 ) or dynamics ( Subsection 4.5 ) .The first two components of a basic modal probabilistic model are effectively the same as a Kripke frame whose relation is decorated with numbers ( probability values ) .", "label": "", "metadata": {}, "score": "41.73732"}
{"text": "Through partial evaluation , a modified version of a standard inference algorithm is performed , on the specific Bayesian network for whose nodes the probabilities are being sought .In this way , overhead found in generic inference engines is considerably reduced .", "label": "", "metadata": {}, "score": "41.774277"}
{"text": "Through partial evaluation , a modified version of a standard inference algorithm is performed , on the specific Bayesian network for whose nodes the probabilities are being sought .In this way , overhead found in generic inference engines is considerably reduced .", "label": "", "metadata": {}, "score": "41.774277"}
{"text": "The time complexity for computing T ( 0 , j , k ) and T ( 1 , j , k ) is similar to the method in the previous subsection .Since the scores CScore ( S , Q ) for are not independent , q ' is usually larger than the spectral probability SpecProb ( S , t , 1 ) .", "label": "", "metadata": {}, "score": "41.871197"}
{"text": "In this section , we will present a first family of probability logics , which are used to study questions of \" probability preservation \" ( or dually , \" uncertainty propagation \" ) .These systems do not extend the language with any probabilistic operators , but rather deal with a \" classical \" propositional language L , which has a countable set of atomic propositions , and the usual truth - functional ( Boolean ) connectives .", "label": "", "metadata": {}, "score": "41.952686"}
{"text": "From a computational perspective , there is a close connec - tion between various probabilistic reasoning tasks and the problem of counting or sampling satisfying assignments of a propositional theory .We consider the question of whether state - of - the - art satisfiability procedures , based on random walk ... \" .", "label": "", "metadata": {}, "score": "42.029697"}
{"text": "Let us assume for the rest of this dynamics section that every relevant set considered has positive probability .In a modal setting , an operator [ !Note that [ !However , [ ! \u03c8]\u03d5 does unfold too , but in more steps : .", "label": "", "metadata": {}, "score": "42.417114"}
{"text": "Haenni , R. , J.-W. Romeijn , G. Wheeler , and J. Williamson , 2011 , Probabilistic Logics and Probabilistic Networks , Dordrecht : Springer .Hailperin , T. , 1965 , \" Best Possible Inequalities for the Probability of a Logical Function of Events , \" American Mathematical Monthly , 72 : 343 - 359 .", "label": "", "metadata": {}, "score": "42.44873"}
{"text": "Each cluster is characterized by a Gaussian distribution , whose mean is the cluster center , and a covariance matrix .These probabilities are obtained by assuming a Gaussian distribution .The fuzzy clustering for learning means and covariance matrices tends to help when the amount of data available for clustering is limited .", "label": "", "metadata": {}, "score": "42.499924"}
{"text": "685 - 710 .Kavvadias , D. and C. H. Papadimitriou , 1990 , \" A Linear Programming Approach to Reasoning about Probabilities , \" Annals of Mathematics and Artificial Intelligence , 1 : 189 - 205 .Keisler , H. J. , 1985 , \" Probability Quantifiers , \" in Model - Theoretic Logics , J. Barwise and S. Feferman ( eds . ) , New York , NY : Springer , pp .", "label": "", "metadata": {}, "score": "42.53515"}
{"text": "Let f be a structural feature represented by an indicator func- tion such that f(G ) is 1 if the feature is present in G and 0 otherwise .In this paper we will assume that these local scores can be computed effi- ciently from data .", "label": "", "metadata": {}, "score": "42.60334"}
{"text": "The errors of these conditional spectral probabilities were obtained by comparing them with the \" correct \" ones ( Figure 1 ) .When the error is 0.5 , there is about a three fold difference between the conditional spectral probabilities reported by the two methods .", "label": "", "metadata": {}, "score": "42.69966"}
{"text": "In this section we will have a closer look at a particular first - order probability logic , whose language is as simple as possible , in order to focus on the probabilistic quantifiers .The language is very much like the language of classical first - order logic , but rather than the familiar universal and existential quantifier , the language contains a probabilistic quantifier .", "label": "", "metadata": {}, "score": "42.70719"}
{"text": "Results .Because of its formulation as a maximization of a piecewise - linear function , these probabilities are rational numbers .We shall argue that their denominators are so large as to make exact computations pointless .Implementation .We use the publicly available GLPK linear programming solver to solve repeatedly the matrix games .", "label": "", "metadata": {}, "score": "42.729958"}
{"text": "By the weighted inclusion - exclusion principle , H(S ) in Eq .( 25 ) H(S ) and F(S , T ) can be computed recursively as fol- lows .Repeatedly applying Eq .( 26 ) is applied in the last step .", "label": "", "metadata": {}, "score": "42.80657"}
{"text": "These three operations create a new potential from the original potential(s ) obtained by multiplying , dividing , or adding , respectively , elements of the original potential(s ) .[ 0033 ] The above algorithm as described , and implemented in current JT - based inference engines , works with any JT and any potentials .", "label": "", "metadata": {}, "score": "42.8627"}
{"text": "These three operations create a new potential from the original potential(s ) obtained by multiplying , dividing , or adding , respectively , elements of the original potential(s ) .[ 0033 ] The above algorithm as described , and implemented in current JT - based inference engines , works with any JT and any potentials .", "label": "", "metadata": {}, "score": "42.8627"}
{"text": "There , in the initial position , the optimal betting strategies , rounded to four digits , are .Although only the first move of the optimal strategy is given , it already points to some interesting and surprising properties .The probabilities are not at all unimodal ; on the contrary , they exhibit an even - odd phenomenon .", "label": "", "metadata": {}, "score": "42.869576"}
{"text": "Consider a deductively valid argument ( \u0393 , \u03d5 ) .If all premises in \u0393 have probability 1 , then the conclusion \u03d5 also has probability 1 .This theorem can be seen as a first , very partial clarification of the issue of probability preservation ( or uncertainty propagation ) .", "label": "", "metadata": {}, "score": "42.904945"}
{"text": "In a typical BN , observations are obtained for some variables in the BN , and probabilities are then calculated for other nodes .Either the human operator or the computer then uses these probabilities to make decisions .[ 0024 ] Current inference engines used to compute probabilities in BNs are written as generic algorithms in high - level programming languages , and thus have an inherent overhead .", "label": "", "metadata": {}, "score": "43.001026"}
{"text": "In a typical BN , observations are obtained for some variables in the BN , and probabilities are then calculated for other nodes .Either the human operator or the computer then uses these probabilities to make decisions .[ 0024 ] Current inference engines used to compute probabilities in BNs are written as generic algorithms in high - level programming languages , and thus have an inherent overhead .", "label": "", "metadata": {}, "score": "43.001026"}
{"text": "The most common strategy to obtain a concrete system of probability logic is to start with a classical ( propositional / modal / etc . ) system of logic and to \" probabilify \" it in one way or another , by adding probabilistic features to it .", "label": "", "metadata": {}, "score": "43.197388"}
{"text": "Thus , the AMG represents explicitly which elements of the clique and separator potentials to add and multiply , reducing much of the overhead inherent in the JT - based inference engine .[ 0040 ] The AMG - based inference engine also has its own overhead , however , including storing the AMG data structure and traversing through the AMG to calculate beliefs .", "label": "", "metadata": {}, "score": "43.278114"}
{"text": "Thus , the AMG represents explicitly which elements of the clique and separator potentials to add and multiply , reducing much of the overhead inherent in the JT - based inference engine .[ 0040 ] The AMG - based inference engine also has its own overhead , however , including storing the AMG data structure and traversing through the AMG to calculate beliefs .", "label": "", "metadata": {}, "score": "43.278114"}
{"text": "We can compute the summation over all the possi- ble DAGs in Eq .We can correct for those overlaps using the inclusion - exclusion principle .Then by the weighted inclusion - exclusion principle , Eq .We have the follow- ing results , which roughly correspond to the backward computation in [ Koivisto , 2006].", "label": "", "metadata": {}, "score": "43.322353"}
{"text": "Theorem 2 .Consider a valid argument ( \u0393 , \u03d5 ) and a probability function P .Formally : .If a valid argument has a small number of premises , each of which only has a small uncertainty ( i.e. , a high certainty ) , then its conclusion will also have a reasonably small uncertainty ( i.e. , a reasonably high certainty ) .", "label": "", "metadata": {}, "score": "43.426395"}
{"text": "For instance , Hoover ( 1978 ) and Keisler ( 1985 ) study completeness results .Bacchus ( 1990 ) and Halpern ( 1990 ) also provide complete axiomatizations as well as combinations of first - order probability logics and modal probability logics .", "label": "", "metadata": {}, "score": "43.621193"}
{"text": "[ We follow here the notation of J&M. ] We want to estimate the transition probabilities a ij ( the probability , being in state i , of making a transition to state j ) and b i ( w ) ( the probability , being in state i , of emitting word w ) .", "label": "", "metadata": {}, "score": "43.735214"}
{"text": "Similar to the proof of Equation ( 7 ) , consider a random protein .Let Q m , d be the modified protein of P whose PTM is on the last amino acid , and r the last amino acid of P .", "label": "", "metadata": {}, "score": "44.067116"}
{"text": "MAP has always been perceived to be significantly harder than the related problems of computing the probability of a variable instantiation ( Pr ) , or the problem of computing the most probable explanatio ... \" .MAP is the problem of finding a most probable instantiation of a set of variables given evidence .", "label": "", "metadata": {}, "score": "44.142105"}
{"text": "The last two constraint rows are needed to ensure that x 1 , x 2 and x 3 form a probability distribution .The variable v is unrestricted .Note that we are maximizing the expected profit , not the probability of winning .", "label": "", "metadata": {}, "score": "44.394573"}
{"text": "The verification score helps to prevent false alarms associated with peaks in the a posteriori probability of the keyword endstate .It is similar to that used as a post - processor to a Viterbi search with partial traceback as described by Rose et al in the April 1990 paper .", "label": "", "metadata": {}, "score": "44.60186"}
{"text": "Unlike previous approaches , our method does not require uniform or near - uniform samples .It instead converts local search sampling wit ... \" .We introduce a new technique for counting models of Boolean satisfiability problems .Our approach incorporates information obtained from sampling the solution space .", "label": "", "metadata": {}, "score": "44.680187"}
{"text": "The logic presented in the previous section is too simple to capture many forms of reasoning about probabilities .We will discuss three extensions here .5.2.1 Quantifying over More than One Variable .First of all one would like to reason about cases where more than one object is selected from the domain .", "label": "", "metadata": {}, "score": "44.680706"}
{"text": "[0007 ] A method of computing probabilities of variables in a belief network includes receiving data representative of the belief network .The method further includes carrying out a partial evaluation algorithm that determines the probability calculations that must be performed on the received data in order to compute the probabilities of the variables in the belief network .", "label": "", "metadata": {}, "score": "44.683903"}
{"text": "[0007 ] A method of computing probabilities of variables in a belief network includes receiving data representative of the belief network .The method further includes carrying out a partial evaluation algorithm that determines the probability calculations that must be performed on the received data in order to compute the probabilities of the variables in the belief network .", "label": "", "metadata": {}, "score": "44.683903"}
{"text": "We have implemented this framework for decision making under uncertainty in stochastic OPL , a language which is based on the OPL constraint modelling language [ Hentenryck et al . , 1999].To illustrate the potential of this framework , we model a wide range of problems in areas as diverse as portfolio diversification , agricultural planning and production / inventory management . .", "label": "", "metadata": {}, "score": "44.69594"}
{"text": "However , in some applications it might also be informative to have an upper bound for the conclusion 's probability .For example , if one knows that this probability has an upper bound of 0.4 , then one might decide to refrain from certain actions ( that one would have performed if this upper bound were ( known to be ) 0.9 ) .", "label": "", "metadata": {}, "score": "44.753212"}
{"text": "Experimental results indicate that using these approximation algorithms provides much better solutions than standard techniques , and provide accurate MAP estimates in many cases .( Shimony , 1994 ) .Pr is a completely different type of problem , characterized by counting instead of optimization , as we need to add up the probability of network instantiations .", "label": "", "metadata": {}, "score": "44.86183"}
{"text": "5.2.3 Probabilities as Terms .When one wants to compare the probability of different events , say of selecting a black ball and selecting a white ball , it may be more convenient to consider probabilities to be terms in their own right .", "label": "", "metadata": {}, "score": "44.90448"}
{"text": "Hamblin , C.L. , 1959 , \" The modal ' probably ' \" , Mind , 68 : 234 - 240 .Hansen , P. and B. Jaumard , 2000 , \" Probabilistic Satisfiability , \" in Handbook of Defeasible Reasoning and Uncertainty Management Systems .", "label": "", "metadata": {}, "score": "44.92866"}
{"text": "The idea is as follows .( 29 )We have the following results .It could be shown that we can also use this way of breaking DAGs to derive Proposition 3 .But this is not exploited in the paper .", "label": "", "metadata": {}, "score": "45.016685"}
{"text": "[0017 ]Bayesian networks are generally used to compute the probabilities that certain events will occur , possibly given the fact that certain other events have already occurred .The BN represents the joint probability distribution of all of its nodes ( or variables ) .", "label": "", "metadata": {}, "score": "45.062035"}
{"text": "[0017 ]Bayesian networks are generally used to compute the probabilities that certain events will occur , possibly given the fact that certain other events have already occurred .The BN represents the joint probability distribution of all of its nodes ( or variables ) .", "label": "", "metadata": {}, "score": "45.062035"}
{"text": "Our algorithm also assumes a bounded indegree but allows general structure priors .We demonstrate the applicability of the algorithm on several data sets with up to 20 variables .Full - text .iastate.edu Abstract We study the problem of learning Bayesian network structures from data .", "label": "", "metadata": {}, "score": "45.211853"}
{"text": "The states for the keyword HMM are obtained by vector quantizing the training utterance similarly to what is described by Bahl et al in their April 1988 paper .Adjacent frames corresponding to the same cluster , or Gaussian output distribution , are collapsed .", "label": "", "metadata": {}, "score": "45.230633"}
{"text": "For an initial N \u00d7 N game , this reduces the number of subgames that we need to solve and store to , a much more feasible number .Linear Programs .Linear programming is a standard technique .For the sake of completeness , we are going to explain how to use linear programming to solve a matrix game such as GOPS .", "label": "", "metadata": {}, "score": "45.23131"}
{"text": "Again , this can be attributed to the non uniform structure prior used by REBEL .We demonstrated its capability on data sets containing up to 20 variables .Our algorithm computes exact posterior probabilities and works in moderate size networks ( about 20 vari- ables ) , which make it a useful tool for studying several problems in learning Bayesian networks .", "label": "", "metadata": {}, "score": "45.33568"}
{"text": "The two upper bounds can be calculated accurately and efficiently using dynamic programming algorithms .The second upper bound is better than the first one and is used for estimating SpecProb ( S , t , 1 ) .Since the second upper bound is larger than SpecProb ( S , t , 1 ) , a constant K is introduced for correcting errors in estimated spectral probabilities .", "label": "", "metadata": {}, "score": "45.518234"}
{"text": "Herzig , A. and D. Longin , 2003 , \" On Modal Probability and Belief , \" in Proceedings of the 7th European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty ( ECSQARU 2003 ) , T.D. Nielsen and N.L. Zhang ( eds . ) , Lecture Notes in Computer Science 2711 , Berlin : Springer , pp .", "label": "", "metadata": {}, "score": "45.530376"}
{"text": "The very idea of combining logic and probability might look strange at first sight ( H\u00e1jek 2001 ) .After all , logic is concerned with absolutely certain truths and inferences , whereas probability theory deals with uncertainties .Furthermore , logic offers a qualitative ( structural ) perspective on inference ( the deductive validity of an argument is based on the argument 's formal structure ) , whereas probabilities are quantitative ( numerical ) in nature .", "label": "", "metadata": {}, "score": "45.596207"}
{"text": "In the previous subsection we discussed a first principle of probability preservation , which says that if all premises have probability 1 , then the conclusion also has probability 1 .Of course , more interesting cases arise when the premises are less than absolutely certain .", "label": "", "metadata": {}, "score": "45.67061"}
{"text": "Whether one agrees with the principles proposed in the literature on higher - order probabilities or not , the ability to represent them forces one to investigate the principles governing them .Subjective Interpretation : Suppose the elements a and b of A are players of a game .", "label": "", "metadata": {}, "score": "45.827988"}
{"text": "The code may set the proper values of the e [ ] [ ] array for later use in the probability calculation code ( i.e. the calculateBeliefs method given previously as an example ) .[ 0087 ] If CPT entries are allowed to change at run - time , the generated source code may also contain a function or method to allow clients to change them .", "label": "", "metadata": {}, "score": "45.88957"}
{"text": "The code may set the proper values of the e [ ] [ ] array for later use in the probability calculation code ( i.e. the calculateBeliefs method given previously as an example ) .[ 0087 ] If CPT entries are allowed to change at run - time , the generated source code may also contain a function or method to allow clients to change them .", "label": "", "metadata": {}, "score": "45.88957"}
{"text": "Our algo- rithm also assumes a bounded indegree but allows general structure priors .We demon- strate the applicability of the algorithm on several data sets with up to 20 variables .Koivisto 1 Introduction Bayesian networks are being widely used for prob- abilistic inference and causal modeling [ Pearl , 2000 , Spirtes et al . , 2001].", "label": "", "metadata": {}, "score": "45.91577"}
{"text": "The conditional spectral probability of the PrSM with respect to one PTM and threshold t was estimated as x/ 10 6 .Since the above method follows the definition of conditional spectral probabilities , the results are treated as \" correct \" conditional spectral probabilities .", "label": "", "metadata": {}, "score": "46.038063"}
{"text": "1c .The search technique uses a forward search through the network to hypothesize keyword and locations , followed by an immediate backward search through each network separately for keyword verification and start location .The forward search looks for peaks in the a posteriori probability of the end state of the keyword and is identical to that described in the Rohlicek et al May 1989 paper .", "label": "", "metadata": {}, "score": "46.133465"}
{"text": "We then vary the maximum indegree k and compare the edge pos- terior probabilities computed by our algorithm with the true probabilities .The results are shown as scat- ter plots in Figure 4 ( Note that in these graphs most of the points are located at ( 0,0 ) or closely nearby ) .", "label": "", "metadata": {}, "score": "46.15155"}
{"text": "The valuation function , as in a Kripke model , allows us to assign properties to the worlds .The semantics for formulas are given on pairs ( M , w ) , where M is a model and w is an element of the model .", "label": "", "metadata": {}, "score": "46.238594"}
{"text": "Thus , in the inventive system , during the forward search , as soon as a peak in probabilities is found representing a hypothesized end point of the keyword , backtracking is started immediately .In the typical forward search , not only will the correct word show up but also a number of incorrect words .", "label": "", "metadata": {}, "score": "46.25148"}
{"text": "As in an MCSP , variables are , in an SCSP , divided into controllable ones ( decision variables ) and uncontrollable ones ( state variables ) .The main difference from an MCSP is that a probability distri ... . \" ...", "label": "", "metadata": {}, "score": "46.28267"}
{"text": "It is thus natural to involve addition ( and more generally , linear combinations ) in a probability language with probability operators .But we will see that much can be expressed without linear combinations explicitly in the language .It is often desirable to have as few definitions as primitive and to generate further definitions from the primitive definitions .", "label": "", "metadata": {}, "score": "46.345524"}
{"text": "With accurate spectral probabilities and E -values , one can easily choose the correct PrSM from several candidate PrSMs for a spectrum , as well as separate correct PrSMs from incorrect ones identified from a large number of spectra .In addition , it provides a way to evaluate single PrSMs efficiently .", "label": "", "metadata": {}, "score": "46.36329"}
{"text": "The posteriors are then converted into scaled likelihoods and used as the observation probabilities within a conventional decoding paradigm ( e.g. , Viterbi decoding ) .The advantages of using recurrent networks are that they require a small number of parameters and provide a fast decoding capability ( relative 3 to conventional , large - vocabulary , HMM systems ) .", "label": "", "metadata": {}, "score": "46.529762"}
{"text": "To model decision problems involving uncertainty and probability , we propose stochastic constraint programming .Stochastic constraint programs contain both decision variables ( which we can set ) and stochastic variables ( which follow some probability distribution ) , and combine together the best ... \" .", "label": "", "metadata": {}, "score": "46.733055"}
{"text": "( In mathematics , probability functions are usually defined for a \u03c3 - algebra of subsets of a given set \u03a9 , and required to satisfy countable additivity ; cf .Subsection 4.3 of this entry .In logical contexts , however , it is often more natural to define probability functions \" immediately \" for the logic 's object language ( Williamson 2002 ) .", "label": "", "metadata": {}, "score": "46.821644"}
{"text": "To minimize the average error of the conditional spectral probabilities reported by TD - GF , the best value of log ( K ) is the average of the log ratio .Using the training data set , K was set to the best value 0.55 .", "label": "", "metadata": {}, "score": "46.83062"}
{"text": "Typically , dynamic programming works by computing in a forward pass the probability of being in each state at each time frame .The most likely state sequence is then determined in a backward pass , which begins when the entire forward pass is complete .", "label": "", "metadata": {}, "score": "46.84531"}
{"text": "From the above analysis , the spectral probabilities estimated by TD - GF are accurate when they are smaller than 0.016 .A comparison of the FDRs of PrSMs with two PTMs estimated by the target - decoy approach and computed based on spectral probabilities .", "label": "", "metadata": {}, "score": "46.874924"}
{"text": "Let be the set of negative / positive mass shifts of allowed PTMs .Any number in is a valid mass shift .Let S be an experimental PRM spectrum and P a random protein .Computing SpecProb ( S , t , 1 ) accurately and efficiently is a problem that has not been solved .", "label": "", "metadata": {}, "score": "47.021587"}
{"text": "The main advantage of our algorithm is that it can use very general structure prior P(G ) that can simply be left as uniform and can satisfy Markov equivalence requirement .We acknowledge here that our algorithm was inspired by and used many tech- niques in [ Koivisto and Sood , 2004 , Koivisto , 2006].", "label": "", "metadata": {}, "score": "47.089115"}
{"text": "Once the value or derivative of a vertex in the AMG has been calculated , it may be cached so that it does not need to be calculated again .In this way , partial calculations can be reused , which greatly speeds up the belief calculations .", "label": "", "metadata": {}, "score": "47.09665"}
{"text": "Once the value or derivative of a vertex in the AMG has been calculated , it may be cached so that it does not need to be calculated again .In this way , partial calculations can be reused , which greatly speeds up the belief calculations .", "label": "", "metadata": {}, "score": "47.09665"}
{"text": "Walley , P. , 1991 , Statistical Reasoning with Imprecise Probabilities , London : Chapman and Hall .Williamson , J. , 2002 , \" Probability Logic , \" in Handbook of the Logic of Argument and Inference : the Turn Toward the Practical , D. Gabbay , R. Johnson , H. J. Ohlbach , and J. Woods ( eds . ) , Amsterdam : Elsevier , pp .", "label": "", "metadata": {}, "score": "47.13852"}
{"text": "To formulate this as a LP , we introduce the variables x 1 , x 2 , and x 3 to represent the probabilities with which player one should play columns 1 , 2 and 3 respectively .We also introduce the variable v to represent the value of the game .", "label": "", "metadata": {}, "score": "47.494957"}
{"text": "[ 0006 ] A system for computing probabilities of variables in a belief network includes a data acquisition interface configured to receive data representative of the belief network .The system further includes a partial evaluator configured to carry out a partial evaluation algorithm that determines the probability calculations that must be performed on the received data in order to compute the probabilities of the variables in the belief network .", "label": "", "metadata": {}, "score": "47.68088"}
{"text": "[ 0006 ] A system for computing probabilities of variables in a belief network includes a data acquisition interface configured to receive data representative of the belief network .The system further includes a partial evaluator configured to carry out a partial evaluation algorithm that determines the probability calculations that must be performed on the received data in order to compute the probabilities of the variables in the belief network .", "label": "", "metadata": {}, "score": "47.68088"}
{"text": "Because many calculations may be required to compute probabilities in large BNs , the compiled calculations can easily exceed these limits .To prevent this from occurring , an extra processing step can be included in the source code generation step .", "label": "", "metadata": {}, "score": "47.718964"}
{"text": "Because many calculations may be required to compute probabilities in large BNs , the compiled calculations can easily exceed these limits .To prevent this from occurring , an extra processing step can be included in the source code generation step .", "label": "", "metadata": {}, "score": "47.718964"}
{"text": "Bayesian networks are generally used to compute the probabilities that certain events will occur , possibly given the fact that certain other events have already occurred .[0004 ] Current inference engines that are used to compute probabilities in BNs may have an inherent overhead , because they are written as generic algorithms in high - level programming languages .", "label": "", "metadata": {}, "score": "47.778107"}
{"text": "Bayesian networks are generally used to compute the probabilities that certain events will occur , possibly given the fact that certain other events have already occurred .[0004 ] Current inference engines that are used to compute probabilities in BNs may have an inherent overhead , because they are written as generic algorithms in high - level programming languages .", "label": "", "metadata": {}, "score": "47.778107"}
{"text": "We can take advantage of this overlap and reduce the total time for computing for all edges .Inspired [ Koivisto , 2006 ] , we developed an algorithm that can compute all edge posterior probabilities in O(n3n ) to- tal time .", "label": "", "metadata": {}, "score": "47.856293"}
{"text": "In FIG .2 , a specific BN is provided to the system as input , through the data acquisition interface .The partial evaluation algorithm then determines all of the calculations required to produce the desired probabilities in the BN .", "label": "", "metadata": {}, "score": "47.865868"}
{"text": "In FIG .2 , a specific BN is provided to the system as input , through the data acquisition interface .The partial evaluation algorithm then determines all of the calculations required to produce the desired probabilities in the BN .", "label": "", "metadata": {}, "score": "47.865868"}
{"text": "Probabilities are generally defined as measures in a measure space .The effect of the \u03c3 - algebra is to restrict the domain so that not every subset of \u03a9 need have a probability .This is crucial for some probabilities to be defined on uncountably infinite sets ; for example , a uniform distribution over a unit interval can not be defined on all subsets of the interval while also maintaining the countable additivity condition for probability measures .", "label": "", "metadata": {}, "score": "47.91388"}
{"text": "The system of claim 1 , wherein the system is further configured to split the probability calculations into a plurality of functions so as to prevent a machine code for a single programming unit within the system from becoming too large .", "label": "", "metadata": {}, "score": "47.92511"}
{"text": "The system of claim 1 , wherein the system is further configured to split the probability calculations into a plurality of functions so as to prevent a machine code for a single programming unit within the system from becoming too large .", "label": "", "metadata": {}, "score": "47.92511"}
{"text": "Propositional probability logics represent such uncertainties as probabilities , and study how they \" flow \" from the premises to the conclusion ; in other words , they do not study truth preservation , but rather probability preservation .The following three subsections discuss systems that deal with increasingly more general versions of this issue .", "label": "", "metadata": {}, "score": "47.93408"}
{"text": "Experimentally , we demonstrate that this approach can be used to obtain good bounds on the model counts for formulas that are far beyond the reach of exact counting methods .In fact , we obtain the first non - trivial solution counts for very hard , highly structured combinatorial problem instances .", "label": "", "metadata": {}, "score": "47.957935"}
{"text": "# # EQU00003 # # .The joint distribution of X 1 and X 2 is given by .The probabilities of interest to be computed for a BN fall into these two categories : 1 ) the probability of a subset of nodes in the BN , and 2 ) the conditional probability of one subset of nodes given another subset of nodes in the BN .", "label": "", "metadata": {}, "score": "48.050194"}
{"text": "# # EQU00003 # # .The joint distribution of X 1 and X 2 is given by .The probabilities of interest to be computed for a BN fall into these two categories : 1 ) the probability of a subset of nodes in the BN , and 2 ) the conditional probability of one subset of nodes given another subset of nodes in the BN .", "label": "", "metadata": {}, "score": "48.050194"}
{"text": "Considerable memory savings are achieved with the combination of a tree based lexicon and a new search technique .The search proceeds time - first , that is partial path hypotheses are extended into the future in the inner loop and a tree walk over the lexicon is performed as an outer loop .", "label": "", "metadata": {}, "score": "48.052067"}
{"text": "Stochastic constraint programs contain both decision variables ( which we can set ) and stochastic variables ( which follow some probability distribution ) , and combine together the best features of traditional constraint satisfaction , stochastic integer programming , and stochastic satisfiability .", "label": "", "metadata": {}, "score": "48.31328"}
{"text": "We define a new class of networks with bounded width , and introduce a new decision problem for Ba ... \" .This paper presents new results on the complexity of graph - theoretical models that represent probabilities ( Bayesian networks ) and that represent interval and set valued probabilities ( credal networks ) .", "label": "", "metadata": {}, "score": "48.35796"}
{"text": "Our algorithm directly sums over all possible DAG structures by exploiting sinks , nodes that have no outgoing edges , and roots , nodes that have no parents , and as a result the computations involved are more complicated .We note that the dynamic programming techniques have also been used to learn the opti- mal Bayesian networks in [ Singh and Moore , 2005 , Silander and Myllymaki , 2006].", "label": "", "metadata": {}, "score": "48.434677"}
{"text": "The system further includes a source code generator configured to output the probability calculations as a source code in a programming language .Claims : .The system of claim 1 , wherein the partial evaluation algorithm comprises a standard inference algorithm that is modified in such a way that at least some of the calculations needed to calculate the probabilities are stored in memory instead of being actually performed by the inference algorithm .", "label": "", "metadata": {}, "score": "48.52935"}
{"text": "The system further includes a source code generator configured to output the probability calculations as a source code in a programming language .Claims : .The system of claim 1 , wherein the partial evaluation algorithm comprises a standard inference algorithm that is modified in such a way that at least some of the calculations needed to calculate the probabilities are stored in memory instead of being actually performed by the inference algorithm .", "label": "", "metadata": {}, "score": "48.52935"}
{"text": "Fagin , R. , J. Y. Halpern , and N. Megiddo , 1990 , \" A Logic for Reasoning about Probabilities , \" Information and Computation , 87 : 78 - 128 .Fitelson , B. , 2006 , \" Inductive Logic , \" in The Philosophy of Science : An Encyclopedia , J. Pfeifer and S. Sarkar ( eds . ) , New York , NY : Routledge , pp .", "label": "", "metadata": {}, "score": "48.5806"}
{"text": "This paper investigates the complexity of MAP in Bayesian networks .Specifically , we show that MAP is complete for NP PP and provide further negative complexity results for algorithms based on variable elimination .We also show that MAP remains hard even when MPE and Pr become easy .", "label": "", "metadata": {}, "score": "48.583115"}
{"text": "In this case , pressing a button does not have a certain outcome .That is , .A significant feature of modal logics in general ( and this includes modal probabilistic logic ) is the ability to support higher - order reasoning , that is , the reasoning about probabilities of probabilities .", "label": "", "metadata": {}, "score": "48.586617"}
{"text": "The first set is the set W of worlds ( the base set of the model ) , but the other is an index set A often to be taken as a set of actions , agents , or players of a game .", "label": "", "metadata": {}, "score": "48.686035"}
{"text": "For more on inductive logic , the reader can consult Fitelson ( 2006 ) , Romeijn ( 2011 ) , and the entries on \" The Problem of Induction \" and \" Inductive Logic \" of this encyclopedia .We will also steer clear of the philosophical debate over the exact nature of probability .", "label": "", "metadata": {}, "score": "48.76463"}
{"text": "Non - negativity .Tautologies .Finite additivity .The definition of probability functions thus requires notions from classical logic , and in this sense probability theory can be said to presuppose classical logic ( Adams 1998 , 22 ) .We now turn to probabilistic semantics , as defined in Leblanc ( 1983 ) .", "label": "", "metadata": {}, "score": "48.775078"}
{"text": "Therefore , the belief equations consist of multiplications and additions of CPT entries and evidence values .These multiplications and additions may be represented in an AMG , and an algorithm made be implemented for traversing the AMG to calculate beliefs .", "label": "", "metadata": {}, "score": "48.80612"}
{"text": "Therefore , the belief equations consist of multiplications and additions of CPT entries and evidence values .These multiplications and additions may be represented in an AMG , and an algorithm made be implemented for traversing the AMG to calculate beliefs .", "label": "", "metadata": {}, "score": "48.80612"}
{"text": "( b ) providing a spoken keyword , .( d ) computing feature vectors for the recorded continuous speech , .( g ) indicating the keyword has been spotted in the recorded speech when the score computed in step ( f ) exceeds a pre - set value .", "label": "", "metadata": {}, "score": "48.864197"}
{"text": "This paper describes a new search technique for large vocabulary speech recognition based on a stack decoder .Considerable memory savings are achieved with the combination of a tree based lexicon and a new search technique .The search proceeds time - first , that is partial path hypotheses are extended ... \" .", "label": "", "metadata": {}, "score": "49.043243"}
{"text": "We will specify a set of K features in the form of binary - valued indicator functions f i ( h , t ) .For example , . where alpha i is the weight for feature i , and Z is a normalizing constant .", "label": "", "metadata": {}, "score": "49.11611"}
{"text": "alpha .sub.t ( i ) are computed time synchronously and are used to compute the a posteriori probability of the end state e of the keyword at time t as # # EQU1 # # where S.sub.t is the state at time t and X.sub.t is the output at time t. A known peak detector is then used to locate peaks in the a posteriori probability which might correspond to the end of the hypothesized keyword .", "label": "", "metadata": {}, "score": "49.16762"}
{"text": "Comparing the E -values of the two PrSMs can determine which one is better .Meng et al .developed a Poisson model for the problem , but the model does not include PTMs [ 18 ] .As top - down MS / MS spectra are often mapped to proteoforms with PTMs , accurate estimation of statistical significance of PrSMs with PTMs is useful and challenging .", "label": "", "metadata": {}, "score": "49.18964"}
{"text": "The keyword HMM is then retrained using the same utterance .In this case , transition probabilities as well as means for the Gaussian output distributions are updated during Baum - Welch training .Covariance matrices are not updated due to the limited amount of data provided by the single utterance .", "label": "", "metadata": {}, "score": "49.37016"}
{"text": "n k ?Next we show how to reduce the total time for computing the posterior probabilities of all edges by combining the contributions of H(S ) and RR(S ) .4.2 Computing posteriors for all edges Consider the summation over all the possible DAGs in Eq .", "label": "", "metadata": {}, "score": "49.42493"}
{"text": "This means that it is not a normal modal operator , and can not be given a Kripke ( relational ) semantics .Herzig and Longin ( 2003 ) and Arl\u00f3 Costa ( 2005 ) provide weaker systems of neighborhood semantics for such \" probably\"-operators , while Yalcin ( 2010 ) discusses their behavior from a more linguistically oriented perspective .", "label": "", "metadata": {}, "score": "49.452866"}
{"text": "The search technique for locating instances of a keyword in continuous speech is a \" forward - backward \" search which , as described in the Rohlicek et al May 1989 paper , uses peaks in the posteriorari probability of the keyword endstate to detect potential keyword endpoints .", "label": "", "metadata": {}, "score": "49.527676"}
{"text": "Consider the following example .Then Theorem 2 says that .This upper bound on the uncertainty of the conclusion is rather disappointing , and it exposes the main weakness of Theorem 2 .However , this premise is irrelevant , in the sense that the conclusion already follows from the other three premises .", "label": "", "metadata": {}, "score": "49.58759"}
{"text": "FIG .10 shows a quantization of the input speech which contains the word \" fee \" and the keyword \" tree \" , with the numerals 1 . . .7 representing the various quantization levels .FIG .11 shows peaks in the computed a posteriori probability .", "label": "", "metadata": {}, "score": "49.759346"}
{"text": "The statistics of the Gaussian distributions are learned by using fuzzy c - means clustering , as described in J. C. Bezdek , J. C. Dunn , \" Optimal Fuzzy Partitions : A Heuristic for Estimating the Parameters in a Mixture of Normal Distributions \" .", "label": "", "metadata": {}, "score": "49.77758"}
{"text": "397 - 438 .Larsen , K. and A. Skou , 1991 , \" Bisimulation through Probabilistic Testing , \" Information and Computation , 94 : 1 - 28 .Leblanc , H. , 1979 , \" Probabilistic Semantics for First - Order Logic , \" Zeitschrift f\u00fcr mathematische Logik und Grundlagen der Mathematik , 25 : 497 - 509 .", "label": "", "metadata": {}, "score": "49.876686"}
{"text": "By integrating the complementary perspectives of qualitative logic and numerical probability theory , probability logics are able to offer highly expressive accounts of inference .It should therefore come as no surprise that they have been applied in all fields that study reasoning mechanisms , such as philosophy , artificial intelligence , cognitive science and mathematics .", "label": "", "metadata": {}, "score": "49.90294"}
{"text": "Stochastic constraint programs contain both decision variables , which we can set , and stochastic variables , which follow a discrete probability distribution .We provide a semantics for stochastic constraint programs based on scenario trees .Using this semantics , we can compile stochastic constraint programs down into conventional ( nonstochastic ) constraint programs .", "label": "", "metadata": {}, "score": "49.909355"}
{"text": "It vastly generalizes the NP - complete problem of propositional satisfiability , and hence is both highly useful and extremely expensive to solve in practice .We present a new approach to mod ... \" .Model counting is the classical problem of computing the number of solutions of a given propositional formula .", "label": "", "metadata": {}, "score": "49.977573"}
{"text": "There is some discussion about the exact relation between inductive logic and probability logic , which is summarized in the introduction of Kyburg ( 1994 ) .The dominant position ( defended by Adams and Levine ( 1975 ) , among others ) , which is also adopted here , is that probability logic entirely belongs to deductive logic , and hence should not be concerned with inductive reasoning .", "label": "", "metadata": {}, "score": "50.03418"}
{"text": "0097 ] Optimizations may be made to the AMG , and thus , to the generated belief equations .These optimizations may use the principle of partial evaluation to simplify the belief equations using as much information about the BN that is known at compile - time .", "label": "", "metadata": {}, "score": "50.08099"}
{"text": "0097 ] Optimizations may be made to the AMG , and thus , to the generated belief equations .These optimizations may use the principle of partial evaluation to simplify the belief equations using as much information about the BN that is known at compile - time .", "label": "", "metadata": {}, "score": "50.08099"}
{"text": "The dynamic programming algorithm for computing the second upper bound can be extended to estimate E -values of PrSMs with multiple PTMs .Results .The extended generating function method , TD - GF ( Top - Down Generating Function ) , was implemented in JAVA and tested on a desktop with a 3.3GHz ( AMD Opteron 6204 ) CPU and 16 GB memory .", "label": "", "metadata": {}, "score": "50.08933"}
{"text": "The stack maintains information about groups of hypotheses and whole groups are extended by one word to form new stack entries .An implementation is described of a one - pass decoder employing a 65,000 word lexicon and a disk - based trigram language model .", "label": "", "metadata": {}, "score": "50.109795"}
{"text": "certainty about falsity ; however , in this entry we follow Adams ' terminology ( 1998 , 31 ) and interpret 0 as maximal uncertainty . )According to this interpretation , the following theorem follows from the strong soundness and completeness of probabilistic semantics : .", "label": "", "metadata": {}, "score": "50.118126"}
{"text": "Cambridge University Press , NY , 2000 .[ Silander and Myllymaki , 2006 ] T. P. Myllymaki .A simple approach for finding the globally optimal Bayesian network structure .Proceedings of the Conference on Uncertainty in Artificial Intelligence ( UAI ) , 2006 .", "label": "", "metadata": {}, "score": "50.17124"}
{"text": "The following example will serve to illustrate the training and operation of the wordspotting system of the invention . ng Several general algorithms are illustrated in FIGS . 2 - 4 , and will be referenced in the description that follows .", "label": "", "metadata": {}, "score": "50.356895"}
{"text": "The probability of correct keyword detection is 0.94 when the probability of a false alarm in a sentence is 0.1 .These statistics are based on per - sentence performance due to the nature of the database .Other systems report probability of detection as a function of false alarms per keyword per hour ( see , for example , J. G. Wilpon , L. G. Miller , P. Modi , \" Improvements and Applications for Key Word Recognition Using Hidden Markov Modeling Techniques \" .", "label": "", "metadata": {}, "score": "50.37342"}
{"text": "Only the evaluation method based on FDRs was applied .For 1 % FDR , the target - decoy approach and the spectral probability approach estimated similar cut - off p -values 0.0164 and 0.0116 , respectively .However , the FDRs based on spectral probabilities are not consistent with the \" correct \" FDRs ( reported by the target - decoy approach ) when the cut - off p -value is larger than 0.016 ( Figure 3 ) .", "label": "", "metadata": {}, "score": "50.5054"}
{"text": "The parameter K in Equation ( 13 ) was set to 1 .Since blind PTM search was used in MS - Align+ , the allowed mass shifts were set to and , where \u03b1 is the mass of a tryptophan ( W ) residue .", "label": "", "metadata": {}, "score": "50.65339"}
{"text": "A comparison of the FDRs of PrSMs with one PTM estimated by the target - decoy approach and computed based on spectral probabilities .Prefix , suffix and internal PrSMs .In this subsection , we describe the methods for estimating parameters C p , C s and C i introduced in Section Methods .", "label": "", "metadata": {}, "score": "50.675682"}
{"text": "In the case of a fixed Then Ai(S ) for all The functions RR may be computed more efficiently if we precompute the product of Aj .n k ? 4 Computing Posterior Probabilities for All Edges If we want to compute the posterior probabilities of all O(n2 ) potential edges , we can compute RR(V ) for each edge separately and solve the problem in O(n23n ) total time .", "label": "", "metadata": {}, "score": "50.722"}
{"text": "To model combinatorial decision problems involving uncertainty and probability , we introduce scenario based stochastic constraint programming .Stochastic constraint programs contain both decision variables , which we can set , and stochastic variables , which follow a discrete probability distribution .", "label": "", "metadata": {}, "score": "50.760582"}
{"text": "In summary , we propose the algorithm in Figure 3 to compute the posterior probabilities for all possible edges .The main result of the paper is summarized in the following theorem .5Experimental Results We have implemented the algorithm in Figure 3 in the C++ language and run some experiments to demon- strate its capabilities .", "label": "", "metadata": {}, "score": "50.891838"}
{"text": "As one example , a generic function may compute the inner product of two vectors .The inner product of two vectors v 1 and v 2 is defined as : .[ 0043 ] The inner product may be implemented generically in a high - level programming language as follows : .", "label": "", "metadata": {}, "score": "50.913418"}
{"text": "As one example , a generic function may compute the inner product of two vectors .The inner product of two vectors v 1 and v 2 is defined as : .[ 0043 ] The inner product may be implemented generically in a high - level programming language as follows : .", "label": "", "metadata": {}, "score": "50.913418"}
{"text": "AAAI Press and MIT Press .[ Kennes and Smets , 1990 ] R. Kennes and P. Smets .Computational aspects of the mobius transforma- tion .In P. B. Bonissone , M. Henrion , L. N. Kanal , and J. F. Lemmer , editors , Proceedings of the Con- ference on Uncertainty in Artificial Intelligence , pages 401 - 416 , 1990 .", "label": "", "metadata": {}, "score": "50.92513"}
{"text": "Let m be the number of amino acids in P and Q m , d the modified protein of P whose PTM is on the last amino acid .Thus , .Equation ( 10 ) is modified to .The value of q is , where N and n are the residue mass and the number of masses of S , respectively .", "label": "", "metadata": {}, "score": "51.021576"}
{"text": "These costs include maintaining abstraction mechanisms , dynamic memory allocation , and object - method dispatching .[ 0042 ] Partial evaluation transforms a general - purpose algorithm that will work with any input data into a specialized algorithm that works only with some specific input data .", "label": "", "metadata": {}, "score": "51.033478"}
{"text": "These costs include maintaining abstraction mechanisms , dynamic memory allocation , and object - method dispatching .[ 0042 ] Partial evaluation transforms a general - purpose algorithm that will work with any input data into a specialized algorithm that works only with some specific input data .", "label": "", "metadata": {}, "score": "51.033478"}
{"text": "The fundamental difference between connectionist systems and more conventional mixture - of - Gaussian systems is that connectionist models directly estimate posterior probabilities as opposed to likelihoods .Access to post ... \" .This paper describes connectionist techniques for recognition of Broadcast News .", "label": "", "metadata": {}, "score": "51.224533"}
{"text": "In Sec- tion 2 we briefly review the Bayesian approach to learn Bayesian networks from data .In Section 3 we present our algorithm for computing the posterior probability of a single edge and in Section 4 we present our al- gorithm for computing the posterior probabilities of all potential edges simultaneously .", "label": "", "metadata": {}, "score": "51.279736"}
{"text": "Therefore , before moving on to the actual discussion of the various approaches , we will first delineate the subject matter of this entry .The most important distinction is that between probability logic and inductive logic .Classically , an argument is said to be ( deductively ) valid if and only if it is impossible that the premises of A are all true , while its conclusion is false .", "label": "", "metadata": {}, "score": "51.308037"}
{"text": "53 - 98 .Goosens , W. K. , 1979 , \" Alternative Axiomatizations of Elementary Probability Theory , \" Notre Dame Journal of Formal Logic , 20 : 227 - 239 .H\u00e1jek , A. , 2001 , \" Probability , Logic , and Probability Logic , \" in The Blackwell Guide to Philosophical Logic , L. Goble ( ed . ) , Oxford : Blackwell , pp .", "label": "", "metadata": {}, "score": "51.388855"}
{"text": "We depict this example with the following diagram .Inside each circle is a labeling of the truth of each proposition letter for the world whose name is labelled right outside the circle .The arrows indicate the probabilities .Probabilities of 0 are not labelled .", "label": "", "metadata": {}, "score": "51.390038"}
{"text": "The upper bound provided by Theorem 2 can also be used to define a probabilistic notion of validity .Adams - probabilistic validity has an alternative , equivalent characterization in terms of probabilities rather than uncertainties .This characterization says that ( \u0393 , \u03d5 ) is Adams - probabilistically valid if and only if the conclusion 's probability can get arbitrarily close to 1 if the premises ' probabilities are sufficiently high .", "label": "", "metadata": {}, "score": "51.405266"}
{"text": "2011 ) .Contemporary approaches based on probabilistic argumentation systems and probabilistic networks are better capable of handling these computational challenges .Furthermore , probabilistic argumentation systems are closely related to Dempster - Shafer theory ( Dempster 1968 ; Shafer 1976 ; Haenni and Lehmann 2003 ) .", "label": "", "metadata": {}, "score": "51.41025"}
{"text": "Likelihoods for the keyword HMM and background HMM are computed separately , using the standard Baum - Welch backward probabilities , or b.sub.t ( j ) .Thus two sets of backward probabilities are computed : one for the keyword HMM and one for the background HMM .", "label": "", "metadata": {}, "score": "51.417892"}
{"text": "[ 0107 ] The three optimizations discussed above remove calculations involving the values one and zero .In an additional optimization technique , first values below a pre - specified lower bound can be set to zero and values above a pre - specified upper bound can be set to one .", "label": "", "metadata": {}, "score": "51.56698"}
{"text": "[ 0107 ] The three optimizations discussed above remove calculations involving the values one and zero .In an additional optimization technique , first values below a pre - specified lower bound can be set to zero and values above a pre - specified upper bound can be set to one .", "label": "", "metadata": {}, "score": "51.56698"}
{"text": "Halpern , J. Y. , 1990 , \" An analysis of first - order logics of probability \" , Artificial Intelligence , 46 : 311 - 350 . - , 1991 , \" The Relationship between Knowledge , Belief , and Certainty , \" Annals of Mathematics and Artificial Intelligence , 4 : 301 - 322 .", "label": "", "metadata": {}, "score": "51.57654"}
{"text": "The i , j'th entry of the matrix is the value of the game to player one when player one makes his i ' th play while player two makes his j ' th play ( such a formulation is called a matrix game ) .", "label": "", "metadata": {}, "score": "51.60151"}
{"text": "[ 0090 ] The outputs of the generated source code are the desired probabilities for nodes in the BN .There may generally be one or more functions or methods to allow clients to obtain these probabilities .This function or method generally first checks to see if the probabilities need to be recalculated , and if so it will call the function or method that contains the probability calculation code .", "label": "", "metadata": {}, "score": "51.65411"}
{"text": "[ 0090 ] The outputs of the generated source code are the desired probabilities for nodes in the BN .There may generally be one or more functions or methods to allow clients to obtain these probabilities .This function or method generally first checks to see if the probabilities need to be recalculated , and if so it will call the function or method that contains the probability calculation code .", "label": "", "metadata": {}, "score": "51.65411"}
{"text": "The duration normalized likelihood for the keyword starting at time t and ending at time t.sub.e is . where s is the start state of the keyword .A duration normalized background likelihood L.sup.back ( t , t.sub.e ) is computed similarly .", "label": "", "metadata": {}, "score": "51.677567"}
{"text": "When the protein is not ambiguous , we use shortened notations .Random proteins .Let Pr ( r ) be the probability that an amino acid is observed at a position in a random protein .In practice , the frequencies of amino acids in the Swiss - Prot database [ 24 ] can be used to estimate Pr ( r ) .", "label": "", "metadata": {}, "score": "51.723793"}
{"text": "Yalcin , S. , 2010 , \" Probability Operators , \" Philosophy Compass , 5 : 916 - 937 .Tools . \" ... decide which contexts are similar and can share parameters .A key feature of this approach is that it allows the construction of models which are dependent upon contextual effects occurring across word boundaries .", "label": "", "metadata": {}, "score": "51.76468"}
{"text": "But from a subjective view , the modal probabilistic models are static : the probabilities are concerned with what currently is the case .Although static in their interpretation , the modal probabilistic setting can be put in a dynamic context .", "label": "", "metadata": {}, "score": "52.11226"}
{"text": "Gerla , G. , 1994 , \" Inferences in Probability Logic , \" Artificial Intelligence , 70 : 33 - 52 .Gillies , D. , 2000 , Philosophical Theories of Probability , London : Routledge .Goldman , A. J. and A. W. Tucker , 1956 , \" Theory of Linear Programming , \" in Linear Inequalities and Related Systems .", "label": "", "metadata": {}, "score": "52.131783"}
{"text": "Computing probabilities : forward - backward algorithm .In general , our approach to training parameterized probabilistic models is to select the parameters which maximize the likelihood of the training corpus .Part of speech taggers are generally trained on annotated corpora .", "label": "", "metadata": {}, "score": "52.13408"}
{"text": "In cases where the actual display of the speech waveform is not required , the system could keep a list of start and end times for the keyword , so that these known portions of the utterance could be accessed at random .", "label": "", "metadata": {}, "score": "52.16478"}
{"text": "Using these algorithms , we observe phase transition behavior in stochastic constraint programs .Interestingly , the cost of both optimization and satisfaction peaks in the satisfaction phase boundary .Finally , we discuss a number of extensions of stochastic constraint programming to relax various assumptions like the independence between stochastic variables .", "label": "", "metadata": {}, "score": "52.175064"}
{"text": "0093 ] In one embodiment of the present disclosure , the data structures and algorithms for the BN , JT , and AMG may be written in Java , and the source code for calculating beliefs was be generated for Java .", "label": "", "metadata": {}, "score": "52.47001"}
{"text": "0093 ] In one embodiment of the present disclosure , the data structures and algorithms for the BN , JT , and AMG may be written in Java , and the source code for calculating beliefs was be generated for Java .", "label": "", "metadata": {}, "score": "52.47001"}
{"text": "In the single speaker case , the keyword HMM is refined by using Baum Welch training to update both the transition probabilities and mean vectors for the output distributions .It is also possible to update with additional training vectors : for example if the system missed a keyword after the first repetition .", "label": "", "metadata": {}, "score": "52.48545"}
{"text": "[0065 ]The BNs for which the development application generates source code for computing probabilities may contain temporal edges .A temporal edge is directed from a parent node to a child node in the BN and represents the parent node causing the child node in the next time step .", "label": "", "metadata": {}, "score": "52.521927"}
{"text": "[0065 ]The BNs for which the development application generates source code for computing probabilities may contain temporal edges .A temporal edge is directed from a parent node to a child node in the BN and represents the parent node causing the child node in the next time step .", "label": "", "metadata": {}, "score": "52.521927"}
{"text": "Each node in a BN has a conditional probability distribution ( CPD ) associated with it that describes the causal influences of its parents .[ 0016 ] A CPD includes the probability of the node 's variable being in a particular state as a function of the states of its parent nodes .", "label": "", "metadata": {}, "score": "52.58041"}
{"text": "Each node in a BN has a conditional probability distribution ( CPD ) associated with it that describes the causal influences of its parents .[ 0016 ] A CPD includes the probability of the node 's variable being in a particular state as a function of the states of its parent nodes .", "label": "", "metadata": {}, "score": "52.58041"}
{"text": "The transition probabilities are learned by using the standard Baum - Welch training procedure for HMM 's on the training data as described in the referenced papers .A bigram alternate HMM can be created similarly .Once the system is trained 38 , and the alternate or background HMM created , spotting is performed by speaking the keyword to be spotted .", "label": "", "metadata": {}, "score": "52.587654"}
{"text": "53 - 118 .Eagle , A. , 2010 , Philosophy of Probability : Contemporary Readings , London : Routledge .Fagin , R. and J. Y. Halpern , 1988 , \" Reasoning about Knowledge and Probability , \" in Proceedings of the 2nd conference on Theoretical aspects of reasoning about knowledge , M. Y. Vardi ( ed . ) , Pacific Grove , CA : Morgan Kaufmann , pp .", "label": "", "metadata": {}, "score": "52.58808"}
{"text": "In our dynamic programming method , we compute subgames in increasing size of hands .To conserve space , we only store the results of the subgames of the current size that we are working on and the subgames of the next smaller size .", "label": "", "metadata": {}, "score": "52.66948"}
{"text": "Adams ' results can be stated more easily in terms of uncertainty rather than certainty ( probability ) .If the probability function P is clear from the context , we will often simply write U instead of U P .In the remainder of this subsection ( and in the next one as well ) we will assume that all arguments have only finitely many premises ( which is not a significant restriction , given the compactness property of classical propositional logic ) .", "label": "", "metadata": {}, "score": "52.707634"}
{"text": "One prob- lem to the MCMC approach is that there is no guaran- tee on the quality of the approximation in finite runs . main drawback of the DP algorithm and the order MCMC algorithm is that they both require a spe- cial form of the structure prior P(G ) . ing prior P(G ) is non uniform , and does not re- spect Markov equivalence [ Friedman and Koller , 2003 , Koivisto and Sood , 2004].", "label": "", "metadata": {}, "score": "52.73344"}
{"text": "The output producing states are put into a parallel network , as shown in FIG .6 at 49 .An initial null state 50 is created , with transitions to each of the output producing states .The associated transition probabilities correspond to the prior probabilities of the clusters .", "label": "", "metadata": {}, "score": "52.834877"}
{"text": "Thus for a probability of detection of 0.9 , words with 4 or more syllables have a probability of false alarm of 0.01 per sentence , while for 2 syllable words the false alarm probability increases to 0.06 .A wordspotting system intended for use in voice editing and indexing has been demonstrated and evaluated .", "label": "", "metadata": {}, "score": "52.856537"}
{"text": "Since these values will not change at run - time , calculations involving them can be partially computed at compile - time .This simplification is similar to the inner - product example discussed above .[ 0098 ] As one example , the following calculation may appear in a generated source code : . [ 0102 ] In practice , not all nodes in the BN may receive evidence .", "label": "", "metadata": {}, "score": "52.85753"}
{"text": "Since these values will not change at run - time , calculations involving them can be partially computed at compile - time .This simplification is similar to the inner - product example discussed above .[ 0098 ] As one example , the following calculation may appear in a generated source code : . [ 0102 ] In practice , not all nodes in the BN may receive evidence .", "label": "", "metadata": {}, "score": "52.85753"}
{"text": "In the model , each player is certain of her own strategy ; for instance at x , player a is certain that she will play p and player b is certain that she will play \u00ac q , that is .", "label": "", "metadata": {}, "score": "52.860455"}
{"text": "For this task , our single - pass decodertook around 15\u00d7 realtime on an HP73 ... . ... ity estimates at each frame without much additional computational cost .In this paper we describe a new search algorithm that we have developed .", "label": "", "metadata": {}, "score": "52.89516"}
{"text": "For example , the element for m3 could be output in Java syntax as : . [ 0080 ]In another embodiment , variables m1 , m2 , and m3 may actually be elements of an array that is declared elsewhere in the source code .", "label": "", "metadata": {}, "score": "52.903862"}
{"text": "For example , the element for m3 could be output in Java syntax as : . [ 0080 ]In another embodiment , variables m1 , m2 , and m3 may actually be elements of an array that is declared elsewhere in the source code .", "label": "", "metadata": {}, "score": "52.903862"}
{"text": "Any compiler for that specific programming language can then compile this source code and it can be used to compute probabilities for that BN .[ 0071 ] Beliefs for a BN are multi - linear functions of CPT entries and evidence values .", "label": "", "metadata": {}, "score": "53.101704"}
{"text": "Any compiler for that specific programming language can then compile this source code and it can be used to compute probabilities for that BN .[ 0071 ] Beliefs for a BN are multi - linear functions of CPT entries and evidence values .", "label": "", "metadata": {}, "score": "53.101704"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .[0008 ] FIG .1 is a flowchart diagram of a standard generic IE that calculates probabilities of nodes in a BN , by taking as input the BN and produces as output the desired probabilities .", "label": "", "metadata": {}, "score": "53.112675"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .[0008 ] FIG .1 is a flowchart diagram of a standard generic IE that calculates probabilities of nodes in a BN , by taking as input the BN and produces as output the desired probabilities .", "label": "", "metadata": {}, "score": "53.112675"}
{"text": "Typically , significant overhead can be introduced into general - purpose algorithms that are useful to solve a wide range of problems , the overhead not being necessary to solve a specific problem relating to a particular input .[ 0025 ] The inference engines used to compute probabilities for a BN tend to be generic algorithms that can compute the probabilities for any BN .", "label": "", "metadata": {}, "score": "53.114258"}
{"text": "Typically , significant overhead can be introduced into general - purpose algorithms that are useful to solve a wide range of problems , the overhead not being necessary to solve a specific problem relating to a particular input .[ 0025 ] The inference engines used to compute probabilities for a BN tend to be generic algorithms that can compute the probabilities for any BN .", "label": "", "metadata": {}, "score": "53.114258"}
{"text": "Kooi B. P. , 2003 , \" Probabilistic Dynamic Epistemic Logic , \" Journal of Logic , Language and Information , 12 : 381 - 408 .Kyburg , H. E. , 1965 , \" Probability , Rationality , and the Rule of Detachment , \" in Proceedings of the 1964 International Congress for Logic , Methodology , and Philosophy of Science , Y. Bar - Hillel ( ed . ) , Amsterdam : North - Holland , pp .", "label": "", "metadata": {}, "score": "53.28051"}
{"text": "FIG .7 shows an idealized quantization of a sequence of feature vectors which might correspond to the keyword \" tree \" .The HMM for the keyword is obtained by creating a state ( labelled \" 4 \" , \" 3 \" , \" 6 \" ) for each unique quantization cluster .", "label": "", "metadata": {}, "score": "53.280975"}
{"text": "Given the difficulty of computing MAP exactly , and the difficulty of approximating MAP while providing useful guarantees on the resulting approximation , we investigate best effort approximations .We introduce a generic MAP approximation framework .We provide two instantiations of the framework ; one for networks which are amenable to exact inference ( Pr ) , and one for networks for which even exact inference is too hard .", "label": "", "metadata": {}, "score": "53.295013"}
{"text": "gorithms have been developed that try to fix this structure prior problem [ Eaton and Murphy , 2007 , Ellis and Wong , 2008].Page 2 . dard structure modularity ( see Eq .The memory requirement of our algorithm is about the same O(n2n ) as the DP algorithm .", "label": "", "metadata": {}, "score": "53.320774"}
{"text": "Carnap , R. , 1950 , Logical Foundations of Probability , Chicago , IL : University of Chicago Press .Cross , C. , 1993 , \" From Worlds to Probabilities : A Probabilistic Semantics for Modal Logic , \" Journal of Philosophical Logic , 22 : 169 - 192 .", "label": "", "metadata": {}, "score": "53.32278"}
{"text": "A comparison of the conditional spectral probabilities ( for PrSMs with one PTM ) estimated by the random database - based method and TD - GF .For each of the 101 test PrSMs , the error of the conditional spectral probability reported by TD - GF is computed .", "label": "", "metadata": {}, "score": "53.34539"}
{"text": "97 - 116 .Morgan , C. and H. Leblanc , 1983 , \" Probabilistic Semantics for Intuitionistic Logic , \" Notre Dame Journal of Formal Logic , 24 : 161 - 180 .Nilsson , N. , 1986 , \" Probabilistic Logic , \" Artificial Intelligence , 28 : 71 - 87 .", "label": "", "metadata": {}, "score": "53.38064"}
{"text": "We consider the question of whether state - of - the - art satisfiability procedures , based on random walk strategies , can be used to sample uniformly or near - uniformly from the space of satisfying assignments .We first show that random walk SAT procedures often do reach the full set of solutions of complex logical theories .", "label": "", "metadata": {}, "score": "53.39871"}
{"text": "It is also capable of generating a lattice of word hypotheses with little computational overhead .These lattices can be used to constrain further decoding , allowing efficient use of complex acoustic and language models .The effectiveness of these techniques has been assessed on a variety of large vocabulary continuous speech recognition tasks and results are presented which analyse performance in terms of computational complexity and recognition accuracy .", "label": "", "metadata": {}, "score": "53.41534"}
{"text": "This article follows a tutorial , given by the authors on dynamic constraint solving at CP 2003 [ 87].It aims at offering an overview of the main approaches and techniques that have been proposed in the domain of constraint satisfaction to deal with uncertain and dynamic environments .", "label": "", "metadata": {}, "score": "53.450645"}
{"text": "For example , when expressed in terms of probabilities rather than uncertainties , Theorem 4 looks as follows : .Adams ' results are restricted in at least two ways : .They only provide a lower bound for the probability of the conclusion ( given the probabilities of the premises ) .", "label": "", "metadata": {}, "score": "53.52309"}
{"text": "3 .The merged network is illustrated in FIG .1c and FIG .9 .Feature vectors are computed for the input speech in which the keyword is to be spotted .The merged network designated 80 in FIG .9 is used to find probable locations of the keyword in the input speech by computing for each successive feature vector the a posteriori probability of being in the keyword endstate \" E \" .", "label": "", "metadata": {}, "score": "53.525944"}
{"text": "Beliefs are the probability distribution of a single node , possibly given the values of some other nodes .Beliefs without values of other nodes are called prior beliefs .Prior beliefs for the node X are represented as P(X ) .", "label": "", "metadata": {}, "score": "53.556137"}
{"text": "Beliefs are the probability distribution of a single node , possibly given the values of some other nodes .Beliefs without values of other nodes are called prior beliefs .Prior beliefs for the node X are represented as P(X ) .", "label": "", "metadata": {}, "score": "53.556137"}
{"text": "One of the earliest qualitative probability logics is Hamblin 's ( 1959 ) .Burgess ( 1969 ) further develops these systems , focusing on the \" high numerical probability\"-interpretation .Both Hamblin and Burgess introduce additional operators into their systems ( expressing , for example , metaphysical necessity and/or knowledge ) , and study the interaction between the \" probably\"-operator and these other modal operators .", "label": "", "metadata": {}, "score": "53.574898"}
{"text": "When one plays 1 against the computed strategies , then in a minority of cases the computer will play a small value , keeping its 9 and gaining more of an advantage than one stands to get when the computer chooses to play 9 .", "label": "", "metadata": {}, "score": "53.634842"}
{"text": "[0115 ] While certain embodiments have been described of a system and method for computing probabilities of variables in a belief network , it is to be understood that the concepts implicit in these embodiments may be used in other embodiments as well .", "label": "", "metadata": {}, "score": "53.64641"}
{"text": "[0115 ] While certain embodiments have been described of a system and method for computing probabilities of variables in a belief network , it is to be understood that the concepts implicit in these embodiments may be used in other embodiments as well .", "label": "", "metadata": {}, "score": "53.64641"}
{"text": "In our computer program , we represent the card sets by bit - vectors .To conserve space , we use a perfect hash table , i.e. , a table whose entries correspond bijectively to subgames .The subgames are stored in lexicographic order ; each subgame is represented by the concatenation of the representations of player 1 's hand , player 2 's hand , and the deck .", "label": "", "metadata": {}, "score": "53.66646"}
{"text": "- , 1986 , Boole 's Logic and Probability , Amsterdam : North - Holland .- , 1996 , Sentential Probability Logic : Origins , Development , Current Status , and Technical Applications , Bethlehem , PA : Lehigh University Press .", "label": "", "metadata": {}, "score": "53.70268"}
{"text": "Thus both the emission and transition probabilities can be computed directly using the MLE .Can we train the HMM from an unannotated corpus ?The goal is the same ... to select the parameters maximizing the likelihood of the training corpus .", "label": "", "metadata": {}, "score": "53.718594"}
{"text": "One can study probabilistic semantics for classical languages ( which do not have any explicit probabilistic operators ) , in which case the consequence relation itself gets a probabilistic flavor : deductive validity becomes \" probability preservation \" , rather than \" truth preservation \" .", "label": "", "metadata": {}, "score": "53.731888"}
{"text": "2011 ) for a recent survey .In this section we will study probability logics that extend the propositional language L with rather basic probability operators .Subsection 2.1 discusses qualitative probability operators ; Subsection 2.2 discusses quantitative probability operators .There are several applications in which qualitative theories of probability might be useful , or even necessary .", "label": "", "metadata": {}, "score": "53.78189"}
{"text": "0073 ] Where v is any vertex in the AMG , p is a parent of v , v ' is one of p 's other children besides v , and c is a child of v. The derivative of the root vertex of the AMG is defined to be 1 .", "label": "", "metadata": {}, "score": "53.82309"}
{"text": "0073 ] Where v is any vertex in the AMG , p is a parent of v , v ' is one of p 's other children besides v , and c is a child of v. The derivative of the root vertex of the AMG is defined to be 1 .", "label": "", "metadata": {}, "score": "53.82309"}
{"text": "Many different IEs have been developed and used .Typically , an IE may be built from a particular BN and may then be used to compute probabilities using any evidence .The CPDs and evidence are inputs to the IE , and probabilities are the outputs .", "label": "", "metadata": {}, "score": "53.85781"}
{"text": "Many different IEs have been developed and used .Typically , an IE may be built from a particular BN and may then be used to compute probabilities using any evidence .The CPDs and evidence are inputs to the IE , and probabilities are the outputs .", "label": "", "metadata": {}, "score": "53.85781"}
{"text": "A text - based and spoken language processing framework based on the Constraint Dependency Grammar ( CDG ) developed by Maruyama [ 24 , 25 ] is discussed .The scope of CDG is expanded to allow for the analysis of sentences containing lexically ambiguous words , to allow feature analysis in constraints , and to efficiently process multiple sentence candidates that are likely to arise in spoken language processing .", "label": "", "metadata": {}, "score": "53.88972"}
{"text": "Finally , standard Baum - Welch training is performed to estimate the transition probabilities and update the mean vectors of the Gaussians for each state , where the same utterance of the keyword is used for training .The keyword HMM is then merged 70 in FIG .", "label": "", "metadata": {}, "score": "53.893974"}
{"text": "[ 0064 ] The BNs for which the development application generates source code for computing probabilities may contain both discrete and continuous nodes with no restrictions on topology or CPDs .Such a BN is commonly referred to as a hybrid Bayesian network ( HBN ) .", "label": "", "metadata": {}, "score": "54.09569"}
{"text": "[ 0064 ] The BNs for which the development application generates source code for computing probabilities may contain both discrete and continuous nodes with no restrictions on topology or CPDs .Such a BN is commonly referred to as a hybrid Bayesian network ( HBN ) .", "label": "", "metadata": {}, "score": "54.09569"}
{"text": "Let tau t ( i , j ) be the probability of being in state i for word t and state j for word t+1 : .Updated values for b i can be computed similarly .By iterating , we can gradually increase the training corpus probability .", "label": "", "metadata": {}, "score": "54.176437"}
{"text": "751 - 774 .Segerberg , K. , 1971 , \" Qualitative Probability in a Modal Setting \" , in Proceedings 2nd Scandinavian Logic Symposium , E. Fenstad ( ed . ) , Amsterdam : North - Holland , pp .341 - 352 .", "label": "", "metadata": {}, "score": "54.270752"}
{"text": "It instead converts local search sampling without any guarantees into very good bounds on the model count with guarantees .We give a formal analysis and provide experimental results showing the effectiveness of our approach . by Cassio Polpo De Campos , Fabio Gagliardi Cozman - In Proceedings of the International Joint Conference on Artificial Intelligence , 2005 . \" ...", "label": "", "metadata": {}, "score": "54.477455"}
{"text": "Data about event ... . ... hope to be able to reason about it more efficiently .It shares the advantages that constraint programming has over integer programming ( e.g. non - linear constraints , and constraint propagation ) .It also shares the advantages that constraint programming ... . by Carla P. Gomes , Ashish Sabharwal , Bart Selman - In 21st AAAI , 2006 . \" ...", "label": "", "metadata": {}, "score": "54.483135"}
{"text": "189 - 274 .Lewis , D. , 1980 , \" A Subjectivist 's Guide to Objective Chance , \" in Studies in Inductive Logic and Probability .Volume 2 , R. C. Jeffrey ( ed . ) , Berkeley , CA : University of California Press , pp .", "label": "", "metadata": {}, "score": "54.51462"}
{"text": "- , 2009 , \" Can Logic be Combined with Probability ?Probably , \" Journal of Applied Logic , 7 : 177 - 187 .Jeffrey , R. , 1992 , Probability and the Art of Judgement , Cambridge : Cambridge University Press .", "label": "", "metadata": {}, "score": "54.583477"}
{"text": "Access to posterior probabilities has enabled us to develop a number of novel approaches to confidence estimation , pronunciation modelling and search .In addition we have investigated a new feature extraction technique based on the modulation - filtered spectrogram ( MSG ) , and methods for combining multiple information sources .", "label": "", "metadata": {}, "score": "54.601532"}
{"text": "Suppose the last element is known to be 10 : . [0052 ] function inner - product4(vector v2 ) .[0054 ]The last multiplication can be computed at compile - time and stored in program memory so it does not have to be computed at run - time .", "label": "", "metadata": {}, "score": "54.62513"}
{"text": "Suppose the last element is known to be 10 : . [0052 ] function inner - product4(vector v2 ) .[0054 ]The last multiplication can be computed at compile - time and stored in program memory so it does not have to be computed at run - time .", "label": "", "metadata": {}, "score": "54.62513"}
{"text": ".. echnical details on the close connections between sampling from a logical theory and probabilistic reasoning are not overly complicated but not the focus of this paper .We will show that one can indeed exploit ideas from random walk based methods to obtain effective near - uniform sampling of the models of certain propositional theories .", "label": "", "metadata": {}, "score": "54.656555"}
{"text": "Another application is to study the effects of the approxima- tion due to the maximum indegree restriction . have shown some initial experimental results in Sec- tion 5 .Other potential applications include assessing the quality of approximate algorithms ( e.g. , MCMC al- gorithms ) , studying the effects of data sample size on the learning results , and studying the effects of model parameters ( such as parameter priors ) on the learning results .", "label": "", "metadata": {}, "score": "54.661827"}
{"text": "Cambridge : Cambridge University Press , 1990 , pp .52 - 94 .Reichenbach , H. , 1949 , The Theory of Probability , Berkeley , CA : University of California Press .Romeijn , J.-W. , 2011 , \" Statistics as Inductive Logic , \" in Handbook for the Philosophy of Science .", "label": "", "metadata": {}, "score": "54.689774"}
{"text": "When lit- tle space is available , we apply the Gurevich- Shelah recurrence - originally proposed for the Hamiltonian path problem - and obtain Affiliated with .Affiliated with .Abstract .Background .In mass spectrometry - based proteomics , the statistical significance of a peptide - spectrum or protein - spectrum match is an important indicator of the correctness of the peptide or protein identification .", "label": "", "metadata": {}, "score": "54.70517"}
{"text": "In this case , the element itself or some other part of the system must be able to convert the element into a string of characters that represents the line of source code in a programming language to be output to a source file .", "label": "", "metadata": {}, "score": "54.738007"}
{"text": "In this case , the element itself or some other part of the system must be able to convert the element into a string of characters that represents the line of source code in a programming language to be output to a source file .", "label": "", "metadata": {}, "score": "54.738007"}
{"text": "5.2.2 Conditional Probability .When one considers the initial example that more than 75 % of all birds fly , one finds that this can not be adequately captured in a model where the domain contains objects that are not birds .", "label": "", "metadata": {}, "score": "54.81272"}
{"text": "In one embodiment , the above traversal algorithm may be modified slightly in order to generate source code for calculating beliefs .Instead of actually performing the additions and multiplications , a list of variable assignments may be generated during traversal .", "label": "", "metadata": {}, "score": "54.81653"}
{"text": "In one embodiment , the above traversal algorithm may be modified slightly in order to generate source code for calculating beliefs .Instead of actually performing the additions and multiplications , a list of variable assignments may be generated during traversal .", "label": "", "metadata": {}, "score": "54.81653"}
{"text": "This approach is particularly well - suited to hybrid connectionist / hidden Markov model systems because posterior phone probabilities are directly computed by the acoustic model .On large vocabulary tasks , using a trigram language model , this increased the search speed by an order of magnitude , with 2 % or less relative search error .", "label": "", "metadata": {}, "score": "54.81655"}
{"text": "In some cases , an inter - language communication protocol may not even exist for certain programming languages .[ 0005 ] A method and system for computing probabilities in BNs without incurring the overhead described above , and without being restricted to a specific programming language , are highly desirable .", "label": "", "metadata": {}, "score": "54.817337"}
{"text": "In some cases , an inter - language communication protocol may not even exist for certain programming languages .[ 0005 ] A method and system for computing probabilities in BNs without incurring the overhead described above , and without being restricted to a specific programming language , are highly desirable .", "label": "", "metadata": {}, "score": "54.817337"}
{"text": "where freq(A ) is the number of words in the corpus unambiguously tagged with part - of - speech A , and incontext(A , C ) is the number of words unambiguously tagged with part - of - speech A in context C. In other words , we favor transformations which are validated by lots of unambiguous examples in the corpus .", "label": "", "metadata": {}, "score": "54.881447"}
{"text": "The method of claim 6 , wherein step ( e ) includes the step of conducting a backward search separately through each of the first and second HMMs .A wordspotting method for determining the location of a word in recorded continuous voiced - speech using a single spoken version of the word as a keyword , comprising : .", "label": "", "metadata": {}, "score": "55.191994"}
{"text": "Presumably , the score for \" fee \" would be lower than that for \" tree \" due to differences in the initial portions of the words , and this score could be used to reject the false keyword \" fee \" .", "label": "", "metadata": {}, "score": "55.235107"}
{"text": "[ 0069 ] FIG .3 is a flowchart diagram for compiling and executing the source code generated as in FIG .2 .FIG .3 illustrates how a source code 300 is executed to produce the probabilities .A compiler 310 takes as input the generated source code and produces as output machine code 320 for a specific processor 330 .", "label": "", "metadata": {}, "score": "55.304756"}
{"text": "[ 0069 ] FIG .3 is a flowchart diagram for compiling and executing the source code generated as in FIG .2 .FIG .3 illustrates how a source code 300 is executed to produce the probabilities .A compiler 310 takes as input the generated source code and produces as output machine code 320 for a specific processor 330 .", "label": "", "metadata": {}, "score": "55.304756"}
{"text": "Another method may be to provide a stand - alone computer program that takes as input the specification of a BN and produces as output the software or hardware to compute the beliefs of that BN .Another method may be to provide a library that takes as input a BN and produces as output the source code for calculating probabilities of that BN .", "label": "", "metadata": {}, "score": "55.33817"}
{"text": "Another method may be to provide a stand - alone computer program that takes as input the specification of a BN and produces as output the software or hardware to compute the beliefs of that BN .Another method may be to provide a library that takes as input a BN and produces as output the source code for calculating probabilities of that BN .", "label": "", "metadata": {}, "score": "55.33817"}
{"text": "Conf . on Acoustics , Speech and Signal Processing , New York , N.Y. , April 1988 , pp .497 - 500 ; .J. G. Wilpon , C. H. Lee and L. R. Rabiner , \" Application of Hidden Markov Models for Recognition of a Limited Set of Words in Unconstrained Speech \" , Proceedings of the International Conference on Acoustics , Speech and Signal Processing , Vol . 1 , pp .", "label": "", "metadata": {}, "score": "55.353592"}
{"text": "For example , if a prefix PrSM contains one mass shift ( PTM ) in and its PTM mass count score is t , the E -value of the PrSM is estimated as C p \u00b7 Z p \u00b7 CSP ( S , t , 1 ) .", "label": "", "metadata": {}, "score": "55.43304"}
{"text": "[ 0095 ] FIG .5 shows the AMG for this BN .As an appendix to the present application , an example is shown of an automatically generated source code for calculating beliefs in this BN .The source code was generated by inserting information into a pre - created template file , such as node and state names , CPT entries , and belief equations for the specific BN .", "label": "", "metadata": {}, "score": "55.43407"}
{"text": "[ 0095 ] FIG .5 shows the AMG for this BN .As an appendix to the present application , an example is shown of an automatically generated source code for calculating beliefs in this BN .The source code was generated by inserting information into a pre - created template file , such as node and state names , CPT entries , and belief equations for the specific BN .", "label": "", "metadata": {}, "score": "55.43407"}
{"text": "3 complexity estimate .To avoid this issue , we use a bottom - up approach storing the values f ( V , Y , P ) of the subgames as we go .We use these stored values when computing the larger subgames .", "label": "", "metadata": {}, "score": "55.488716"}
{"text": "In this paper , we attempt to systematically review the use of dynamic programming search strategies for small - vocabulary and large - vocabulary continuous speech recognition .The following methods are described in detail : search using a linear lexicon , search using a lexical tree , language - model look - ahead and word graph generation . \" ...", "label": "", "metadata": {}, "score": "55.59251"}
{"text": "Computing Posterior Probabilities of Structural Features in Bayesian Networks .We study the problem of learning Bayesian network structures from data .One main drawback of their algorithms is the requirement of a special structure prior that is non uniform and does not respect Markov equivalence .", "label": "", "metadata": {}, "score": "55.59691"}
{"text": "[ 0076 ]After the modified belief traversal algorithm is finished , the list may be transformed into source code by iterating over each element , and outputting each variable assignment to a text file .This text file may be a pre - existing source code file called a template file , in which case the variable declarations would be inserted at a specific position within the source code file .", "label": "", "metadata": {}, "score": "55.606163"}
{"text": "[ 0076 ]After the modified belief traversal algorithm is finished , the list may be transformed into source code by iterating over each element , and outputting each variable assignment to a text file .This text file may be a pre - existing source code file called a template file , in which case the variable declarations would be inserted at a specific position within the source code file .", "label": "", "metadata": {}, "score": "55.606163"}
{"text": "The belief equations are contained in the computeBeliefs method .Although this code will only compute beliefs for the simple BN , all overhead of a generic inference engine is eliminated , and only those additions and multiplications needed to compute beliefs are performed .", "label": "", "metadata": {}, "score": "55.688923"}
{"text": "The belief equations are contained in the computeBeliefs method .Although this code will only compute beliefs for the simple BN , all overhead of a generic inference engine is eliminated , and only those additions and multiplications needed to compute beliefs are performed .", "label": "", "metadata": {}, "score": "55.688923"}
{"text": "If the pre - set score is exceeded , the candidate and/or its location is outputted or otherwise indicated at 28 , and the process repeats via branch 29 for the next possible keyword candidate .In more detail , the speech is first digitized and feature vectors are computed at regular intervals .", "label": "", "metadata": {}, "score": "55.728485"}
{"text": "( 30 )The proof of Proposition 3 is given in the Appendix .Note that in Eq .( 30 ) the computations of H(U ) and Kv(U ) do not rely on Bvand all the contribution from Bv to P(f , D ) is represented by Av .", "label": "", "metadata": {}, "score": "55.77291"}
{"text": "Paris , J. B. , 1994 , The Uncertain Reasoner 's Companion , A Mathematical Perspective , Cambridge : Cambridge University Press .Parma , A. and R. Segala , 2007 , \" Logical Characterizations of Bisimulations for Discrete Probabilistic Systems , \" in Proceedings of the 10th International Conference on Foundations of Software Science and Computational Structures ( FOSSACS ) , H. Seidl ( ed . ) , Lecture Notes in Computer Science 4423 , Berlin : Springer , pp .", "label": "", "metadata": {}, "score": "55.977196"}
{"text": "The terms are defined inductively as follows : .Every individual variable x is a term .Every function symbol f of arity n followed by an n -tuple of terms ( t 1 , ... , t n ) is a term .", "label": "", "metadata": {}, "score": "56.04373"}
{"text": "60/591,269 entitled \" Modeless User Interface Incorporating Automatic Updates for Developing and Using Bayesian Belief Networks \" filed Jul. 26 , 2004 in the name of Cox , et al . , and incorporated herein by reference in its entirety .[ 0084 ] The input to the generated source code module may generally be new evidence for nodes in the BN .", "label": "", "metadata": {}, "score": "56.229164"}
{"text": "60/591,269 entitled \" Modeless User Interface Incorporating Automatic Updates for Developing and Using Bayesian Belief Networks \" filed Jul. 26 , 2004 in the name of Cox , et al . , and incorporated herein by reference in its entirety .[ 0084 ] The input to the generated source code module may generally be new evidence for nodes in the BN .", "label": "", "metadata": {}, "score": "56.229164"}
{"text": "Learning Bayesian networks : The combination of knowledge and statistical data .Machine Learning , 20:197 - 243 , 1995 .[Heckerman et al . , 1999 ] D. Heckerman , C. Meek , and G. Cooper .A Bayesian approach to causal discov- ery .", "label": "", "metadata": {}, "score": "56.318665"}
{"text": "4 .An HMM for the keyword is created as follows .First the endpoint 41 of the keyword is determined using a standard endpoint detection algorithm , and feature extraction is performed .Then each feature vector is quantized by labelling it according to the nearest cluster , where the nearest cluster is the most probable cluster to have generated the feature vector as computed by the Gaussian distribution , indicated in block 42 .", "label": "", "metadata": {}, "score": "56.33876"}
{"text": "If the CPD of any node in the BN changes in any way the IE must be built again before probabilities can be calculated again .[ 0023 ] The evidence e specifies the events that have already occurred .In particular , the evidence provides a real number between zero and one ( inclusive ) for each state of each node that represents an event that has already occurred .", "label": "", "metadata": {}, "score": "56.436592"}
{"text": "If the CPD of any node in the BN changes in any way the IE must be built again before probabilities can be calculated again .[ 0023 ] The evidence e specifies the events that have already occurred .In particular , the evidence provides a real number between zero and one ( inclusive ) for each state of each node that represents an event that has already occurred .", "label": "", "metadata": {}, "score": "56.436592"}
{"text": "One example of such a language is Verilog .After generating the source code for the hardware device , the source code may then be used to physically create or program a hardware device .In this way , the probability calculations may be carried out directly in hardware , instead of being represented as machine code instructions for a processor to execute .", "label": "", "metadata": {}, "score": "56.521824"}
{"text": "One example of such a language is Verilog .After generating the source code for the hardware device , the source code may then be used to physically create or program a hardware device .In this way , the probability calculations may be carried out directly in hardware , instead of being represented as machine code instructions for a processor to execute .", "label": "", "metadata": {}, "score": "56.521824"}
{"text": "Consider the following 2 \u00d7 2 matrix game : .If a value is a minimum value in its column and a maximum value in its row , then it is a saddle point .If the game has a saddle point , then the value of the game is the value of a saddle point entry ( there may be more than one saddle point ) .", "label": "", "metadata": {}, "score": "56.531525"}
{"text": "Despite the worst - case exponential run time of all known algorithms , satisfiability solvers are increasingly leaving their mark as a generalpurpose tool in areas as diverse as software and h ... \" .The past few years have seen an enormous progress in the performance of Boolean satisfiability ( SAT ) solvers .", "label": "", "metadata": {}, "score": "56.577255"}
{"text": "[0075 ]In the original AMG algorithm described above , these values and derivatives would actually be calculated , instead of generating code that represents the calculations .The list would be generated such that variables referenced in an assignment are themselves declared and assigned before that assignment to prevent illegal forward references and to ensure the referenced variables have already been assigned their proper value .", "label": "", "metadata": {}, "score": "56.667038"}
{"text": "[0075 ]In the original AMG algorithm described above , these values and derivatives would actually be calculated , instead of generating code that represents the calculations .The list would be generated such that variables referenced in an assignment are themselves declared and assigned before that assignment to prevent illegal forward references and to ensure the referenced variables have already been assigned their proper value .", "label": "", "metadata": {}, "score": "56.667038"}
{"text": "Furthermore , since the goal of these systems is re ... . \" ...The search problem in large vocabulary continuous speech recognition ( LVCSR ) is to locate the most probable string of words for a spoken utterance given the acoustic signal and a set of sentence models .", "label": "", "metadata": {}, "score": "56.758797"}
{"text": "Additionally , the development of CDG grammars using our grammar tools and parser is discussed . ... s ( e.g. , [ 21 , 40 ] ) ) have reduced recognition errors by incorporating some language model into their system to reduce perplexity .", "label": "", "metadata": {}, "score": "56.787537"}
{"text": "works by dynamic programming .Technical report , Carnegie Mellon University , School of Computer Science , 2005 .P.Singh and An- Finding optimal Bayesian net-[ Spirtes et al . , 2001 ] P. Spirtes , C. Glymour , and R. Scheines .", "label": "", "metadata": {}, "score": "56.84124"}
{"text": "Then the summation over all possible DAGs in Eq .( 40 )Plugging Eq .( 40 ) into Eq .Page 10 .References [ Asuncion and Newman , 2007 ] A. Asuncion and D.J. Newman .UCI machine learning repository , 2007 .", "label": "", "metadata": {}, "score": "56.888992"}
{"text": "As was explained in Section 1 of this entry , there are many ways in which a logic can have probabilistic features .The models of the logic can have probabilistic aspects , the notion of consequence can have a probabilistic flavor , or the language of the logic can contain probabilistic operators .", "label": "", "metadata": {}, "score": "56.89425"}
{"text": "We have discussed two views of modal probability logic .One is temporal or stochastic , where the probability distribution associated with each state determines the likelihood of transitioning into other states ; another is concerned with subjective perspectives of agents , who may reason about probabilities of other agents .", "label": "", "metadata": {}, "score": "56.934566"}
{"text": "In some cases , the belief traversal algorithm may have been specified to compute beliefs for all nodes in the BN .By simply restricting the evidence vertices to those for needed beliefs in which the traversal algorithm starts , however , unneeded beliefs may not have to be calculated , and the corresponding source code for computing those beliefs will not be generated .", "label": "", "metadata": {}, "score": "56.992733"}
{"text": "In some cases , the belief traversal algorithm may have been specified to compute beliefs for all nodes in the BN .By simply restricting the evidence vertices to those for needed beliefs in which the traversal algorithm starts , however , unneeded beliefs may not have to be calculated , and the corresponding source code for computing those beliefs will not be generated .", "label": "", "metadata": {}, "score": "56.992733"}
{"text": "In order to compu ... . by Carla P. Gomes , Joerg Hoffmann , Ashish Sabharwal , Bart Selman - In Proc .IJCAI'07 , 2007 . \" ...We introduce a new technique for counting models of Boolean satisfiability problems .", "label": "", "metadata": {}, "score": "57.032738"}
{"text": "The computer now proceeds to process the information as previously described and ultimately locates acoustic units which score above the pre - set threshold .That the system will operate nearly always with only a single spoken utterance of the keyword , makes the invention especially valuable for voice - editing applications .", "label": "", "metadata": {}, "score": "57.036438"}
{"text": "In the specific example where the programmatic representation is in the form of source code , the file may be in a format as is traditionally used to provide source code to a compiler or other processing tool .In this way , the programmatic representation may be further processed , such as by converting it to an executable file .", "label": "", "metadata": {}, "score": "57.04649"}
{"text": "In the specific example where the programmatic representation is in the form of source code , the file may be in a format as is traditionally used to provide source code to a compiler or other processing tool .In this way , the programmatic representation may be further processed , such as by converting it to an executable file .", "label": "", "metadata": {}, "score": "57.04649"}
{"text": "We also tested our algorithm on a synthetic data set coming with REBEL .For each data set , we ran our algorithm and REBEL to compute the posterior probabilities for all potential edges .The time taken under different maximum indegree k is reported in Table 1 , which also lists the number of variables n and the number of instances m for each data set .", "label": "", "metadata": {}, "score": "57.08275"}
{"text": "This codebook can be created from the recorded speech for which indexing is desired , or from other samples of recorded speech from the same talker .This codebook consists not only of the centroids , or means of the data associated with each codeword , but of the covariance matrices .", "label": "", "metadata": {}, "score": "57.09272"}
{"text": "Pearl , J. , 1991 , \" Probabilistic Semantics for Nonmonotonic Reasoning , \" in Philosophy and AI : Essays at the Interface , R. Cummins and J. Pollock ( eds . ) , Cambridge , MA : The MIT Press , pp .", "label": "", "metadata": {}, "score": "57.10576"}
{"text": "The present disclosure covers all probabilities that can be computed for a BN and is not limited to any specific type of probability .[ 0022 ] The probabilities for the nodes in the BN are calculated using the CPDs and the evidence .", "label": "", "metadata": {}, "score": "57.142845"}
{"text": "The present disclosure covers all probabilities that can be computed for a BN and is not limited to any specific type of probability .[ 0022 ] The probabilities for the nodes in the BN are calculated using the CPDs and the evidence .", "label": "", "metadata": {}, "score": "57.142845"}
{"text": "The search algorithm is based on stack decoding and uses both likelihood- and posterior - based pruning .The use of the posterior - based phone deactivation pruning techniques is well - suited to hybrid connectionist / HMM systems because posterior phone probabilities are directly computed by the connectionist acoustic model .", "label": "", "metadata": {}, "score": "57.178814"}
{"text": "van Benthem , J. , J. Gerbrandy , and B. Kooi , 2009 , \" Dynamic Update with Probabilities , \" Studia Logica , 93 : 67 - 96 .Boole , G. , 1854 , An Investigation of the Laws of Thought , on which are Founded the Mathematical Theories of Logic and Probabilities , London : Walton and Maberly .", "label": "", "metadata": {}, "score": "57.451645"}
{"text": "12 shows the backward search for the potential endpoints from FIG .11 .The score for the portion of the speech corresponding to \" fee \" was 60 , while the score for the keyword \" tree \" was 90 , for the illustrative example .", "label": "", "metadata": {}, "score": "57.58322"}
{"text": "If a different voice speaks the keyword , provided that feature vectors of the recorded voice were properly mapped to that of the user 's voice , then the system could still spot the keyword sought , but performance likely would be poorer .", "label": "", "metadata": {}, "score": "57.70327"}
{"text": "This counterstrategy will win every round except one resulting in a trouncing .Instead the strategy should have some random variations where one plays particular cards with some probability .How difficult is it to analyze this game ?Suppose the cards are valued from 1 through N .", "label": "", "metadata": {}, "score": "57.827507"}
{"text": "By fixing the probabilities of the nodes for which probabilities will not be changed through posting of evidence , the generated code may be simplified .[ 0103 ] Another simplification may arise by setting all evidence values of non - evidence nodes in the BN to one .", "label": "", "metadata": {}, "score": "57.83943"}
{"text": "By fixing the probabilities of the nodes for which probabilities will not be changed through posting of evidence , the generated code may be simplified .[ 0103 ] Another simplification may arise by setting all evidence values of non - evidence nodes in the BN to one .", "label": "", "metadata": {}, "score": "57.83943"}
{"text": "( 15 ) RR(S ) and RF(S , T ) can be computed recursively as follows .Page 4 .Combing Eqs .( 15 ) and ( 17 ) we obtain Eq .Given the functions Bi , the functions Ai as defined in Eq .", "label": "", "metadata": {}, "score": "57.94198"}
{"text": "Using the same method for computing C p , parameter C i was estimated as 0.508 .Spectral probabilities for PrSMs with two PTMs .Similar to PrSMs with one PTM , a mutated protein database was created to increase the number of identified PrSMs with two PTMs .", "label": "", "metadata": {}, "score": "57.948402"}
{"text": "A technique for wordspotting based on hidden Markov models ( HMM 's ) .The technique allows a speaker to specify keywords dynamically and to train the associated HMM 's via a single repetition of a keyword .Non - keyword speech is modeled using an HMM trained from a prerecorded sample of continuous speech .", "label": "", "metadata": {}, "score": "57.9781"}
{"text": "The embodiment described below can be adapted to any standard inference engine that can be used to calculate the desired probabilities in the desired type of BN .[0068 ]A development application may use the method illustrated in FIG .", "label": "", "metadata": {}, "score": "58.039658"}
{"text": "The embodiment described below can be adapted to any standard inference engine that can be used to calculate the desired probabilities in the desired type of BN .[0068 ]A development application may use the method illustrated in FIG .", "label": "", "metadata": {}, "score": "58.039658"}
{"text": "Hoover , D. N. , 1978 , \" Probability Logic , \" Annals of Mathematical Logic , 14 : 287 - 313 .Howson , C. , 2003 , \" Probability and Logic , \" Journal of Applied Logic , 1 : 151 - 165 .", "label": "", "metadata": {}, "score": "58.06653"}
{"text": "A key feature of this approach is that it allows the construction of models which are dependent upon contextual effects occurring across word boundaries .The use of cross word context dependent models presents problems for conventional decoders .The second part of the thesis therefore presents a new decoder design which is capable of using these models efficiently .", "label": "", "metadata": {}, "score": "58.06793"}
{"text": "The proposed extended generating function method can be applied to all types of spectra , such as CID and electron - transfer dissociation ( ETD ) spectra , because all these types of spectra can be converted to PRM spectra .All masses in PRM spectra are discretized by scaling the masses with a constant and rounding the values to integers [ 23 ] .", "label": "", "metadata": {}, "score": "58.122375"}
{"text": "Conf . on Acoustics , Speech and Signal Processing , April 1990 , pp .129 - 132 ; .J. G. Wilpon , L. R. Rabiner , C. H. Lee , E. R. Goldman , \" Automatic Recognition of Keywords in Unconstrained Speech Using Hidden Markov Models \" .", "label": "", "metadata": {}, "score": "58.166367"}
{"text": "835 - 838 ; and R. O. Duda , P. E. Hart , Pattern Classification and Scene Analysis , John Wiley and Sons , Inc. , New York , 1973 , on the training data to estimate the parameters of a mixture of Gaussians .", "label": "", "metadata": {}, "score": "58.297783"}
{"text": "A typical transformation might say change [ X , Y ] to Y in context C , where X and Y are part of speech tags .Words are initially assigned all their possible parts of speech , based on a dictionary , as in other supervised methods .", "label": "", "metadata": {}, "score": "58.392982"}
{"text": "DBNs may typically only contain discrete nodes .The present disclosure may also include , however , DBNs with both discrete and continuous nodes .These are commonly referred to as dynamic hybrid Bayesian networks ( DHBN ) .[ 0066 ] The development application may generate source code for calculating any probabilities of nodes in the BN .", "label": "", "metadata": {}, "score": "58.435818"}
{"text": "DBNs may typically only contain discrete nodes .The present disclosure may also include , however , DBNs with both discrete and continuous nodes .These are commonly referred to as dynamic hybrid Bayesian networks ( DHBN ) .[ 0066 ] The development application may generate source code for calculating any probabilities of nodes in the BN .", "label": "", "metadata": {}, "score": "58.435818"}
{"text": "835 - 838 .R. M. Gray , \" Vector Quantization \" , IEEE ASSP Magazine , Apr. 1984 , pp .4 - 29 . F. Jelinek , \" Continuous Speech Recognition by Statistical Methods \" .Proc . of the IEEE , vol . 64 , No . 4 , Apr. 1976 , pp .", "label": "", "metadata": {}, "score": "58.573227"}
{"text": "Due to the complexity of MS / MS spectra , many statistical models have limited accuracy .By contrast , Kim et al .proposed a probabilistic method for computing spectral probabilities and statistical significance of PSMs [ 8 ] .This method achieves high accuracy , but it is not obvious how to extend it to PSMs with post - translational modifications ( PTMs ) .", "label": "", "metadata": {}, "score": "58.60572"}
{"text": "The system of claim 2 , wherein the belief network comprises a dynamic belief network , and wherein the dynamic belief network comprises at least some temporal edges that connect at least one node in one time step to one or more other nodes in a subsequent time step .", "label": "", "metadata": {}, "score": "58.61487"}
{"text": "The system of claim 2 , wherein the belief network comprises a dynamic belief network , and wherein the dynamic belief network comprises at least some temporal edges that connect at least one node in one time step to one or more other nodes in a subsequent time step .", "label": "", "metadata": {}, "score": "58.61487"}
{"text": "( b ) providing the keyword , .( f ) indicating the keyword has been spotted in the recorded speech when the score computed in step ( e ) exceeds a pre - set value .The method of claim 1 , wherein steps ( d)-(f ) are repeated until the recorded speech finishes .", "label": "", "metadata": {}, "score": "58.65468"}
{"text": "Time com- plexity O(3n ) .Time com- plexity O(n3n ) .Time complexity O(n3n ) .Time complex- ity O(kn2n ) .Time complexity O(nk+2 ) .Figure 3 : Algorithm for computing the posterior prob- abilities for all possible edges in time complexity O(n3n ) assuming a fixed maximum indegree k. Table 1 : The speed of our algorithm ( in second ) .", "label": "", "metadata": {}, "score": "58.748642"}
{"text": "1b shows a typical keyword HMM .We assume a left - to - right Bakis model as described in F. Jelinkek , \" Continuous Speech Recognition by Statistical Methods \" .Proc . of the IEEE , Vol . 64 , No . 4 , April 1976 , pp .", "label": "", "metadata": {}, "score": "58.902775"}
{"text": "Scores are computed for each feature vector starting from the proposed endpoint and continuing back through 1.5 times the length of the keyword repetition used to generate the keyword HMM .The time where the keyword score is maximum , indicated by reference numerals 84 , 85 , is the start point of the keyword .", "label": "", "metadata": {}, "score": "58.94876"}
{"text": "Formally : .The proof of Theorem 4 is significantly more difficult than that of Theorem 2 : Theorem 2 requires only basic probability theory , whereas Theorem 4 is proved using methods from linear programming ( Adams and Levine 1975 ; Goldman and Tucker 1956 ) .", "label": "", "metadata": {}, "score": "58.980957"}
{"text": "Evaluation based on conditional spectral probabilities .To evaluate TD - GF , we generated a set of PrSMs with \" correct \" conditional spectral probabilities and compared the \" correct \" conditional spectral probabilities with those reported by TD - GF .", "label": "", "metadata": {}, "score": "58.99071"}
{"text": "Conf . on Acoustics , Speech and Signal Processing , Apr. 1990 , pp .129 - 132 .J. G. Wilpon , L. R. Rabiner , C. H. Lee , E. R. Goldman , \" Automatic Recognition of Keywords in Unconstrained Speech Using Hidden Markov Models \" .", "label": "", "metadata": {}, "score": "58.999153"}
{"text": "The system of claim 16 , wherein the plurality of functions comprises a main function that calls all of the remaining functions in a desired order .The system of claim 1 , wherein the belief network comprises a disconnected network that includes multiple groups of nodes that are not connected to each other , and wherein the system is further configured to produce probability calculations for the disconnected network .", "label": "", "metadata": {}, "score": "59.096943"}
{"text": "The system of claim 16 , wherein the plurality of functions comprises a main function that calls all of the remaining functions in a desired order .The system of claim 1 , wherein the belief network comprises a disconnected network that includes multiple groups of nodes that are not connected to each other , and wherein the system is further configured to produce probability calculations for the disconnected network .", "label": "", "metadata": {}, "score": "59.096943"}
{"text": "Other ways of creating the codebook could also be used .A hidden Markov model ( HMM ) for the spoken word to be located is created as follows .The spoken word is quantized according to the codebook previously created .", "label": "", "metadata": {}, "score": "59.12227"}
{"text": "Hailperin ( 1965 , 1984 , 1986 , 1996 ) and Nilsson ( 1986 ) use methods from linear programming to show that these two restrictions can be overcome .Their most important result is the following : .Theorem 5 .", "label": "", "metadata": {}, "score": "59.151657"}
{"text": "No intermediate result variable is needed .No for - loop overhead ( maintain i variable , increment i and check i against vector length each iteration ) .Calculations can be parallelized ( perform multiplications in parallel then add products together ) on some special - purpose processors .", "label": "", "metadata": {}, "score": "59.20173"}
{"text": "No intermediate result variable is needed .No for - loop overhead ( maintain i variable , increment i and check i against vector length each iteration ) .Calculations can be parallelized ( perform multiplications in parallel then add products together ) on some special - purpose processors .", "label": "", "metadata": {}, "score": "59.20173"}
{"text": "The solution is not implementable by a human , but only by a computer .The game is a model of decision making based on bidding , which is an important paradigm in game theory , because it can easily be shown ( see below ) that no deterministic strategy may succeed .", "label": "", "metadata": {}, "score": "59.223907"}
{"text": "0038 ]The JT - based inference engine also incurs overhead by keeping the JT and related data structures in memory , computing which cliques to send messages to , initialization of the clique potentials , marginalization of clique potentials to compute beliefs , etc .", "label": "", "metadata": {}, "score": "59.233795"}
{"text": "0038 ]The JT - based inference engine also incurs overhead by keeping the JT and related data structures in memory , computing which cliques to send messages to , initialization of the clique potentials , marginalization of clique potentials to compute beliefs , etc .", "label": "", "metadata": {}, "score": "59.233795"}
{"text": "Alternatively , one can add various kinds of probabilistic operators to the syntax of the logic .In Section 3 we will discuss some initial , rather basic examples of probabilistic operators .The full expressivity of modal probabilistic operators will be explored in Section 4 .", "label": "", "metadata": {}, "score": "59.426895"}
{"text": "We first study the case where all mass shifts are positive ; negative mass shifts will be discussed at the end of this subsection .A three dimensional table T ( i , j , k ) is computed to acquire the upper bound , where i is the number of PTMs in modified proteins .", "label": "", "metadata": {}, "score": "59.53399"}
{"text": "In the target - decoy approach , all spectra were searched against a concatenated target and shuffled decoy protein database .The FDRs estimated based spectral probabilities were consistent with those reported by the target - decoy approach .For example , the target - decoy approach and the spectral probability approach reported cut - off p -values 0.0327 and 0.0262 for 1 % FDR , respectively .", "label": "", "metadata": {}, "score": "59.535614"}
{"text": "To address these doubts , they wrote papers to compare ENGCG to stochastic taggers .Maximum entropy modeling .( M&S , sec .Maximum entropy modeling provides one mathematically well - founded method for combining such features in a probabilistic model .", "label": "", "metadata": {}, "score": "59.595917"}
{"text": "The vertices of a JT may be referred to as cliques , and the edges of a JT may be called separators .[ 0029 ] One example of a JT inference engine is the Hugin JT inference engine .In the Hugin JT inference engine , each clique i has an object \u03a6 i , called a potential , associated with it .", "label": "", "metadata": {}, "score": "59.613667"}
{"text": "The vertices of a JT may be referred to as cliques , and the edges of a JT may be called separators .[ 0029 ] One example of a JT inference engine is the Hugin JT inference engine .In the Hugin JT inference engine , each clique i has an object \u03a6 i , called a potential , associated with it .", "label": "", "metadata": {}, "score": "59.613667"}
{"text": "H\u00e1jek , A. and S. Hartmann , 2010 , \" Bayesian Epistemology , \" in A Companion to Epistemology , J. Dancy , E. Sosa , and M. Steup ( eds . ) , Oxford : Blackwell , pp .93 - 106 .", "label": "", "metadata": {}, "score": "59.719757"}
{"text": "Page 8 .Page 9 . see that the exact probabilities computed by REBEL without indegree bound sometimes still differ with the true probabilities .This is due to the highly non uni- form structure prior used by REBEL .results are shown in Figure 6 .", "label": "", "metadata": {}, "score": "59.859787"}
{"text": "There are two reasons why this has occurred : First , the dynamic programming strategy can be combined with avery efficient and practical pruning strategy so that very large search spaces can be handled .Second , the dynamic programming strategy has turned out to be extremely flexible in adapting to new requirements .", "label": "", "metadata": {}, "score": "59.871162"}
{"text": "The solution is then a Nash equilibrium of the game .This maximization problem is a linear program ( LP ) , and we will solve it using linear programming tools .The classical reference [ 3 ] remains an excellent introduction to linear programming .", "label": "", "metadata": {}, "score": "59.97702"}
{"text": "Proc . of the Int .Conf . on Acoustics , Speech and Signal Processing , New York , New York , Apr. 1988 , pp .497 - 500 .J. C. Bezdek , J. C. Dunn , \" Optical Fuzzy Partitions : A Heuristic for Estimating the Parameters in a Mixture of Normal Distributions \" .", "label": "", "metadata": {}, "score": "60.00461"}
{"text": "Tarski , A. , 1936 , \" Wahrscheinlichkeitslehre und mehrwertige Logik \" , Erkenntnis , 5 : 174 - 175 .Van Fraassen , B. , 1981a , \" A Problem for Relative Information Minimizers in Probability Kinematics , \" British Journal for the Philosophy of Science , 32:375 - 379 . - , 1981b , \" Probabilistic Semantics Objectified : I. Postulates and Logics , \" Journal of Philosophical Logic , 10 : 371 - 391 . - , 1983 , \" Gentlemen 's Wagers : Relevant Logic and Probability , \" Philosophical Studies , 43 : 47 - 61 . - , 1984 , \" Belief and the Will , \" Journal of Philosophy , 81 : 235 - 256 .", "label": "", "metadata": {}, "score": "60.100708"}
{"text": "It is vital to distinguish correct PSMs from those incorrect ones .Two main approaches have been proposed to address this problem .In the first approach , a large data set of MS / MS spectra is searched against a concatenated target - decoy protein database to find a best - scoring PSM for each spectrum , and the PSM is reported if its score exceeds a prespecified threshold .", "label": "", "metadata": {}, "score": "60.103355"}
{"text": "We count the number Z of proteins in the target database with a residue mass in .The E -value of the complete PrSM is estimated as Z \u00b7 CSP ( S , t , 1 ) .For prefix , suffix and internal PrSMs , we count the numbers Z p , Z s , and Z i of prefixes/ suffixes / internal sub - proteins in the target database with a residue mass in .", "label": "", "metadata": {}, "score": "60.105476"}
{"text": "This calculateBeliefs method would then be called by some other function or method when beliefs need to be recalculated .Details on when beliefs need to be recalculated and how this can be detected and carried out can be found in the co - pending application Ser .", "label": "", "metadata": {}, "score": "60.12827"}
{"text": "This calculateBeliefs method would then be called by some other function or method when beliefs need to be recalculated .Details on when beliefs need to be recalculated and how this can be detected and carried out can be found in the co - pending application Ser .", "label": "", "metadata": {}, "score": "60.12827"}
{"text": "[ 0091 ] For calculating beliefs in Java , the following may be performed : . [0092 ] If any modifications were made this method first calls the calculateBeliefs0 method to re - calculate beliefs and then returns the beliefs for the specified variable .", "label": "", "metadata": {}, "score": "60.20559"}
{"text": "[ 0091 ] For calculating beliefs in Java , the following may be performed : . [0092 ] If any modifications were made this method first calls the calculateBeliefs0 method to re - calculate beliefs and then returns the beliefs for the specified variable .", "label": "", "metadata": {}, "score": "60.20559"}
{"text": "The development application may contain one or more modules or other components that generate variable assignments and belief equations , with one such module available for each programming language in which source is to be generated .A user may specify which language is to be used and the appropriate module could then be invoked .", "label": "", "metadata": {}, "score": "60.284523"}
{"text": "The development application may contain one or more modules or other components that generate variable assignments and belief equations , with one such module available for each programming language in which source is to be generated .A user may specify which language is to be used and the appropriate module could then be invoked .", "label": "", "metadata": {}, "score": "60.284523"}
{"text": "Rather , it needs to be contained in some function or method and also needs to be supported by some kind of input and output code .To continue the example of generating Java source code , the belief equations could be contained in a method provided below : .", "label": "", "metadata": {}, "score": "60.417072"}
{"text": "Rather , it needs to be contained in some function or method and also needs to be supported by some kind of input and output code .To continue the example of generating Java source code , the belief equations could be contained in a method provided below : .", "label": "", "metadata": {}, "score": "60.417072"}
{"text": "There is in fact , in general , no closed solution to this problem .We must instead use an Expectation Maximization ( EM ) method , which is essentially an iterative , hill - climbing method to set the parameters .", "label": "", "metadata": {}, "score": "60.611275"}
{"text": "The generated source code may also provide a function or method for clients to call to set this new evidence .[0085 ] In Java , for example , this may be performed as follows : .[ 0086 ] In this case , evidence may be stored in the two - dimensional array e [ ] [ ] .", "label": "", "metadata": {}, "score": "60.648697"}
{"text": "The generated source code may also provide a function or method for clients to call to set this new evidence .[0085 ] In Java , for example , this may be performed as follows : .[ 0086 ] In this case , evidence may be stored in the two - dimensional array e [ ] [ ] .", "label": "", "metadata": {}, "score": "60.648697"}
{"text": "FIG .5 illustrates a set of 8 clusters which might be obtained from 2-dimensional features .The background , or non - keyword hidden Markov model ( HMM ) is formed at block 37 based on the above clusters as follows .", "label": "", "metadata": {}, "score": "60.670174"}
{"text": "r m is observed is . where L represents the length of the random protein .Despite the difference between the uniform distribution and the distribution of protein length in the target protein database , experimental results showed the uniform distribution does not introduce large errors into the computation of spectral probabilities .", "label": "", "metadata": {}, "score": "60.714462"}
{"text": "The background HMM is formed by a parallel connection of states , with one state for each Gaussian distribution .FIG .1a shows a typical background model .There are transitions from an initial null state at the left to each of the M Gaussian output generating states .", "label": "", "metadata": {}, "score": "60.749855"}
{"text": "The performance of the system as a function of the number of syllables in the keyword was also examined .FIG .14 shows the results when the number of syllables per word is 1,2,3 and 4 or more .The plot shows that the probability of detection increases with the number of syllables in the keyword .", "label": "", "metadata": {}, "score": "60.77295"}
{"text": "The search problem in large vocabulary continuous speech recognition ( LVCSR ) is to locate the most probable string of words for a spoken utterance given the acoustic signal and a set of sentence models .Searching the space of possible utterances is difficult because of the large vocabulary size and the complexity imposed when long - span language models are used .", "label": "", "metadata": {}, "score": "60.81137"}
{"text": "Clients call the method specifying the variable whose CPT is to be changed , the configuration representing the entry to be changed , and the new conditional probability value .The code sets the proper value of the c [ ] [ ] array for later use in the probability calculation code ( i.e. the calculateBeliefs method given previously as an example ) .", "label": "", "metadata": {}, "score": "60.830112"}
{"text": "Clients call the method specifying the variable whose CPT is to be changed , the configuration representing the entry to be changed , and the new conditional probability value .The code sets the proper value of the c [ ] [ ] array for later use in the probability calculation code ( i.e. the calculateBeliefs method given previously as an example ) .", "label": "", "metadata": {}, "score": "60.830112"}
{"text": "A novel aspect of this algorithm is a new pruning strategy , phone deactivation pruni ... . by A. J. Robinson , G. D. Cook , D. P. W. Ellis , E. Fosler - Lussier , S. J. Renals , D. A. G. Williams , 2002 . \" ...", "label": "", "metadata": {}, "score": "60.949234"}
{"text": "This is used to sort out the possible candidates from the input recorded speech at 25 which is inputted to block 26 .When a peak is detected , indicating a possible candidate , backtracking 27 through the network of FIG .", "label": "", "metadata": {}, "score": "60.975998"}
{"text": "38 , No . 11 , November 1990 , pp .1870 - 1878 ; . L. R. Bahl , P. F. Brown , P. V. de Souza , R. L. Mercer , M. A. Picheny , \" Acoustic Markov Models Used in the Tangora Speech Recognition System \" .", "label": "", "metadata": {}, "score": "61.04897"}
{"text": "To obtain an improved version of this theorem , a more fine - grained notion of \" essentialness \" is necessary .In argument A in the example above , premise s is absolutely irrelevant .The notion of essentialness is formalized as follows : .", "label": "", "metadata": {}, "score": "61.081337"}
{"text": "11/380,784 , filed Apr. 28 , 2006 which claims priority under 35 U.S.C. \u00a7 119(e ) to U.S. provisional patent application Ser .No .60/676,637 , filed Apr. 29 , 2005 .BACKGROUND .[0002 ] Computerized data models that can predict future data and/or explain past data are helpful in solving real - world problems .", "label": "", "metadata": {}, "score": "61.118095"}
{"text": "11/380,784 , filed Apr. 28 , 2006 which claims priority under 35 U.S.C. \u00a7 119(e ) to U.S. provisional patent application Ser .No .60/676,637 , filed Apr. 29 , 2005 .BACKGROUND .[0002 ] Computerized data models that can predict future data and/or explain past data are helpful in solving real - world problems .", "label": "", "metadata": {}, "score": "61.118095"}
{"text": "2 algorithm , previous utterances of the same talker are analyzed and a vector quantized codebook created at 20 .From this at 21 can be created an HMM representing non - keyword speech as a sequence of acoustic units , one per codeword .", "label": "", "metadata": {}, "score": "61.141567"}
{"text": "References .Ross , S.M. Goofspiel : The game of pure strategy .J. Appl .Probab .[ Google Scholar ] [ CrossRef ] .Dantzig , G.B. Linear Programming and Extensions ; Princeton University Press : Princeton , NJ , USA , 1963 .", "label": "", "metadata": {}, "score": "61.22796"}
{"text": "13 shows a plot of the probability of detecting a keyword that actually occurred in a sentence as a function of the probability of detecting a keyword in a sentence when none was present .This curve was obtained by varying a threshold on the keyword score of equation 3 .", "label": "", "metadata": {}, "score": "61.264763"}
{"text": "There would be no peaks for input speech words whose sounds bear little resemblance to that of the keyword .Each time a peak in the endstate probability is detected , for example , at locations 82 and 83 , in accordance with the invention , a backward search is initiated to locate a possible start time for the keyword and to compute a score .", "label": "", "metadata": {}, "score": "61.313164"}
{"text": "The use of HMMs in wordspotting in speech recognition applications is well known in the art .See , for example : .J. R. Rohlicek , W. Russel , S. Roukos , H. Gish , \" Continuous Hidden Markov Modeling for Speaker - Independent Word Spotting \" .", "label": "", "metadata": {}, "score": "61.324234"}
{"text": "The method of claim 9 , wherein the indication of step ( g ) is carried out by stopping the recorded utterance where the word has been found .The method of claim 9 , wherein the HMMs are formed by digitizing , dividing words into frames , deriving feature vectors for each frame , weighting the feature vectors , concatenating the weighted feature vectors to form the HMMs .", "label": "", "metadata": {}, "score": "61.326973"}
{"text": "Experiments show that the extended generating function method achieves high accuracy in computing spectral probabilities and FDRs .Methods .A top - down MS / MS spectrum generated from a protein consists of a precursor mass , corresponding to the molecular mass of the protein , and a list of peaks , corresponding to fragment ions of the protein .", "label": "", "metadata": {}, "score": "61.328056"}
{"text": "( 30 ) in time O(2n ) .To compute Kv(U ) , we can precompute the product of Aj .Precomputation .Compute functions Bi , Ai , RR , H , and Ki .( b ) Compute P(f , D ) using Eq .", "label": "", "metadata": {}, "score": "61.493664"}
{"text": "Logic and Probability .First published Thu Mar 7 , 2013 .Logic and probability theory are two of the main tools in the formal study of reasoning , and have been fruitfully applied in areas as diverse as philosophy , artificial intelligence , cognitive science and mathematics .", "label": "", "metadata": {}, "score": "61.494743"}
{"text": "One critical advantage of Kupiec was his use of ' ambiguity classes ' : infrequent words are grouped together based on their possible parts of speech .This greatly reduces the number of parameters to be estimated .Brill developed an unsupervised version of his TBL POS tagger .", "label": "", "metadata": {}, "score": "61.527885"}
{"text": "An internal PrSM corresponds to an intact protein with both N- and C - terminal truncations .Similar to the E -values defined in BLAST [ 25 ] , the E -value of a PrSM describes the number of hits one can \" expect \" to see by chance when searching the spectrum against a protein database of a particular size .", "label": "", "metadata": {}, "score": "61.57433"}
{"text": "The method of claim 9 , wherein step ( a ) includes the step of analyzing previous utterances of the same talker and creating a vector quantized codebook in which each codeword in the quantization sequence represents a state in an HMM .", "label": "", "metadata": {}, "score": "61.5756"}
{"text": "Ramsey , F. P. , 1926 , \" Truth and Probability \" , in Foundations of Mathematics and other Essays , R. B. Braithwaite ( ed . ) , London : Routledge and Kegan Paul , 1931 , pp .156 - 198 ; reprinted in Studies in Subjective Probability , H. E. Kyburg , Jr. and H. E. Smokler ( eds . ) , 2nd ed . , Malabar , FL : R. E. Krieger Publishing Company , 1980 , pp .", "label": "", "metadata": {}, "score": "61.611202"}
{"text": "The first - order flavor is what distinguishes these operators from the probabilistic modal operators of the previous section .Consider the following example from Bacchus ( 1990 ) .More than 75 % of all birds fly .There is a straightforward probabilistic interpretation of this sentence , namely when one randomly selects a bird , then the probability that the selected bird flies is more than 3/4 .", "label": "", "metadata": {}, "score": "61.808846"}
{"text": "0009 ] FIG .2 is a flowchart diagram for generating source code that computes probabilities of nodes in a BN .[0010 ]FIG .3 is a flowchart diagram for compiling and executing the source code generated as in FIG .", "label": "", "metadata": {}, "score": "61.83077"}
{"text": "0009 ] FIG .2 is a flowchart diagram for generating source code that computes probabilities of nodes in a BN .[0010 ]FIG .3 is a flowchart diagram for compiling and executing the source code generated as in FIG .", "label": "", "metadata": {}, "score": "61.83077"}
{"text": "L. R. Rabiner , \" A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition \" .Proc . of the IEEE , vol .77 , No . 2 , Feb. 1989 , pp .257 - 285 .", "label": "", "metadata": {}, "score": "61.835686"}
{"text": "Every predicate letter R of arity n followed by an n -tuple of terms ( t 1 , ... , t n ) is a formula .If \u03d5 is a formula , then so is \u00ac\u03d5. Every free occurrence of x in \u03d5 is bound by the operator .", "label": "", "metadata": {}, "score": "61.870564"}
{"text": "A keyword is modelled as a specific sequence of states , or acoustic units .Nonkeyword speech is modelled as an arbitrary sequence of these units .Previous speaker - dependent wordspotting systems have been based on template matching using dynamic time warping , as described in the following papers : .", "label": "", "metadata": {}, "score": "61.8742"}
{"text": "Conf . on Acoustics , Speech and Signal Processing , Denver , Colo. , April 1980 , pp .173 - 177 .While these techniques are applicable to wordspotting tasks , they are generally inferior to HMM 's in modeling the acoustic variability associated with multiple repetitions of a keyword due to speaking rate , context , etc .", "label": "", "metadata": {}, "score": "61.942154"}
{"text": "No .60/591,269 entitled \" Modeless User Interface Incorporating Automatic Updates for Developing and Using Bayesian Belief Networks \" filed Jul. 26 , 2004 in the name of Cox , et al .The description of the BN may also be provided by a user in any other suitable way .", "label": "", "metadata": {}, "score": "61.9908"}
{"text": "No .60/591,269 entitled \" Modeless User Interface Incorporating Automatic Updates for Developing and Using Bayesian Belief Networks \" filed Jul. 26 , 2004 in the name of Cox , et al .The description of the BN may also be provided by a user in any other suitable way .", "label": "", "metadata": {}, "score": "61.9908"}
{"text": "The system of the invention involves using HMMs to model speaker utterances .Hidden Markov models consists of a set of states with associated outputs , where the output of a state is a feature vector describing a sound .Transition probabilities between states allow modeling a sequence of sounds .", "label": "", "metadata": {}, "score": "62.0358"}
{"text": "0044 ] function inner - productl(vector v1 , vector v2 ) . if ( length(vl ) ![0045 ] If , for a specific problem , it is known that the length of the two vectors is 3 .The inner - product1 function can then be simplified to : . [ 0047 ] By using partial evaluation to incorporate knowledge of the input data , much of the overhead of the original general - purpose inner product algorithm has been eliminated .", "label": "", "metadata": {}, "score": "62.0388"}
{"text": "0044 ] function inner - productl(vector v1 , vector v2 ) . if ( length(vl ) ![0045 ] If , for a specific problem , it is known that the length of the two vectors is 3 .The inner - product1 function can then be simplified to : . [ 0047 ] By using partial evaluation to incorporate knowledge of the input data , much of the overhead of the original general - purpose inner product algorithm has been eliminated .", "label": "", "metadata": {}, "score": "62.0388"}
{"text": "A Bayesian method for the induc- tion of probabilistic networks from data .Machine Learning , 9:309 - 347 , 1992 .F. Cooper and [ Eaton and Murphy , 2007 ] D. Eaton and K. Murphy .Bayesian structure learning using dynamic program- ming and MCMC .", "label": "", "metadata": {}, "score": "62.070946"}
{"text": "In addition to overhead , another feature of generic inference engines used to compute probabilities in BNs is that they tend to be written by hand by computer programmers in a specific programming language .To use the inference engine in a different programming language , therefore , the inference engine must either be re - written in the other language or some sort of inter - language communication protocol must be used .", "label": "", "metadata": {}, "score": "62.151917"}
{"text": "In addition to overhead , another feature of generic inference engines used to compute probabilities in BNs is that they tend to be written by hand by computer programmers in a specific programming language .To use the inference engine in a different programming language , therefore , the inference engine must either be re - written in the other language or some sort of inter - language communication protocol must be used .", "label": "", "metadata": {}, "score": "62.151917"}
{"text": "In other words , the necessary calculations are written down and stored in a computer memory , rather than actually being performed .[ 0014 ]FIG .1 is a flowchart diagram of a standard generic IE 100 that calculates probabilities 120 of nodes in a BN 130 .", "label": "", "metadata": {}, "score": "62.380108"}
{"text": "In other words , the necessary calculations are written down and stored in a computer memory , rather than actually being performed .[ 0014 ]FIG .1 is a flowchart diagram of a standard generic IE 100 that calculates probabilities 120 of nodes in a BN 130 .", "label": "", "metadata": {}, "score": "62.380108"}
{"text": "Since multiplication of a number with one is the number , the corresponding evidence vertices can effectively be removed from the AMG .[ 0104 ] Further , beliefs may not be needed in practice for all nodes in the BN .", "label": "", "metadata": {}, "score": "62.41101"}
{"text": "Since multiplication of a number with one is the number , the corresponding evidence vertices can effectively be removed from the AMG .[ 0104 ] Further , beliefs may not be needed in practice for all nodes in the BN .", "label": "", "metadata": {}, "score": "62.41101"}
{"text": "In this paper , we study an extended generating function method for accurately computing the statistical significance of protein - spectrum matches with post - translational modifications .Experiments show that the extended generating function method achieves high accuracy in computing spectral probabilities and false discovery rates .", "label": "", "metadata": {}, "score": "62.516857"}
{"text": "The programming languages that the two programs are written in can be different .In general , the SCG helps the user to build some model and then generates a computer program that does something with that model , so the user does n't have to write the computer program by hand .", "label": "", "metadata": {}, "score": "62.581562"}
{"text": "The programming languages that the two programs are written in can be different .In general , the SCG helps the user to build some model and then generates a computer program that does something with that model , so the user does n't have to write the computer program by hand .", "label": "", "metadata": {}, "score": "62.581562"}
{"text": "For example , given two disjoint subsets of the nodes X , X 1 .OR right .X and X 2 . and similarly for X 2 , .# # EQU00002 # # .The joint probability of any subset of the nodes in a BN is obtained by marginalizing all other nodes out of the joint probability of all the nodes .", "label": "", "metadata": {}, "score": "62.586624"}
{"text": "For example , given two disjoint subsets of the nodes X , X 1 .OR right .X and X 2 . and similarly for X 2 , .# # EQU00002 # # .The joint probability of any subset of the nodes in a BN is obtained by marginalizing all other nodes out of the joint probability of all the nodes .", "label": "", "metadata": {}, "score": "62.586624"}
{"text": "FIG .5 illustrates a typical clustering distribution ; .FIG .6 shows a typical parallel network of an HMM ; .FIG .7 is an illustrative quantization sequence for a sample keyword , and FIG .8 is a typical HMM for that keyword ; .", "label": "", "metadata": {}, "score": "62.64926"}
{"text": "Sign up to receive free email alerts when patent applications with chosen keywords are published SIGN UP .Abstract : .A system for computing probabilities of variables in a belief network includes a data acquisition interface configured to receive data representative of the belief network .", "label": "", "metadata": {}, "score": "62.728054"}
{"text": "Sign up to receive free email alerts when patent applications with chosen keywords are published SIGN UP .Abstract : .A system for computing probabilities of variables in a belief network includes a data acquisition interface configured to receive data representative of the belief network .", "label": "", "metadata": {}, "score": "62.728054"}
{"text": "Playing either card with the same probability gives him an average gain of 12.5 ( the optimal strategy is to play high with 52 % probability on the king , resulting in an average gain of 12.52 ) .We note that , up to games , 2 \u00d7 2 the results are easily computed by hand .", "label": "", "metadata": {}, "score": "62.7747"}
{"text": "Abstract .This article follows a tutorial , given by the authors on dynamic constraint solving at CP 2003 [ 87].It aims at offering an overview of the main approaches and techniques that have been proposed in the domain of constraint satisfaction to deal with uncertain and dynamic environments .", "label": "", "metadata": {}, "score": "63.09856"}
{"text": "Simple , two - player games are important models for human decision making .They should have sufficiently elementary rules so that they can be studied both theoretically and empirically , yet be sufficiently rich to involve a non - trivial amount of human psychological experience .", "label": "", "metadata": {}, "score": "63.141228"}
{"text": "CPT entries may be changed at run - time in a system that adapts to new data over time , to model the system better over time .This may be referred to as on - line learning .[ 0088 ] One example of such a method in Java is provided below : .", "label": "", "metadata": {}, "score": "63.272987"}
{"text": "CPT entries may be changed at run - time in a system that adapts to new data over time , to model the system better over time .This may be referred to as on - line learning .[ 0088 ] One example of such a method in Java is provided below : .", "label": "", "metadata": {}, "score": "63.272987"}
{"text": "It is designed to take advantage of some of the best features of each framework .For example , we are able to write expressive models using non - linear and global constraints , and to exploit efficient ...G22.2591 - Advanced Natural Language Processing - Spring 2004 .", "label": "", "metadata": {}, "score": "63.34236"}
{"text": "Alternatively , the computer can be readily programmed to procure a sample from the actual voice recording itself .For best results , the voice used in making the recording should be the same as that of the user doing the wordspotting .", "label": "", "metadata": {}, "score": "63.34962"}
{"text": "i.e. , Theorem 4 yields in general a tighter upper bound than Theorem 2 .Hence Theorem 4 yields that .Given the uncertainties ( and degrees of essentialness ) of the premises of a valid argument , Adams ' theorems allow us to compute an upper bound for the uncertainty of the conclusion .", "label": "", "metadata": {}, "score": "63.723137"}
{"text": "Because today 's state - of - the - art recognizers are not designed to be situated naturally in an error feedback loop , they are ill - positioned for inclusion in multi- ... \" .This thesis is about modeling , analyzing , and predicting errorful behavior in large vocabulary continuous speech recognition systems .", "label": "", "metadata": {}, "score": "63.78122"}
{"text": "38 , No . 11 , Nov. 1990 , pp .1870 - 1878 .Primary Examiner : Kemeny ; Emanuel S. Assistant Examiner : Tung ; Kee M. Claims .What is claimed is : .A wordspotting method for determining the location of a word in recorded continuous voice - speech using a single spoken version of the word as a keyword , comprising : .", "label": "", "metadata": {}, "score": "63.968414"}
{"text": "In the 9-card game , suppose the initial upcard is a 9 .The computer player will play a 9 with a probability , rounded to four digits , of 0.7475 .Now if the human always plays 1 , then nearly 3/4 of the time he will gain an advantage : playing 1 against the computer 's 9 is to his advantage due to his increased betting strength for the remainder of the game .", "label": "", "metadata": {}, "score": "64.08885"}
{"text": "An inter - language communication protocol may not even exist for two specific programming languages , preventing this from even being a possibility .[ 0056 ] A computer science technique called automatic source code generation can be used to solve this inflexibility .", "label": "", "metadata": {}, "score": "64.09392"}
{"text": "An inter - language communication protocol may not even exist for two specific programming languages , preventing this from even being a possibility .[ 0056 ] A computer science technique called automatic source code generation can be used to solve this inflexibility .", "label": "", "metadata": {}, "score": "64.09392"}
{"text": "Kim S , Gupta N , Pevzner PA : Spectral probabilities and generating functions of tandem mass spectra : a strike against decoy databases .Journal of Proteome Research 2008 , 7 : 3354 - 3363 .PubMed Central PubMed View Article .", "label": "", "metadata": {}, "score": "64.167404"}
{"text": "Another object is to spot a keyword spoken by a user in previously recorded speech by another talker where both talkers are known .The wordspotting technique described here uses an HMM to model arbitrary , user - defined keywords in the context of continuous speech .", "label": "", "metadata": {}, "score": "64.22801"}
{"text": "Perkins DN , Pappin DJ , Creasy DM , Cottrell JS : Probability - based protein identification by searching sequence databases using mass spectrometry data .Electrophoresis 1999 , 20 : 3551 - 3567 .PubMed View Article .Geer LY , Markey SP , Kowalak JA , Wagner L , Xu M , Maynard DM , Yang X , Shi W , Bryant SH : Open mass spectrometry search algorithm .", "label": "", "metadata": {}, "score": "64.32344"}
{"text": "The recurrence function for filling out T ( 0 , j , k ) is the same to Equation ( 5 ) .We change the definition of T ( 1 , j , k ) by replacing with in Equation ( 6 ) .", "label": "", "metadata": {}, "score": "64.359"}
{"text": "The system of claim 1 , wherein the source code generator is further configured to generate assembly code for a processor .Description : .CROSS -REFERENCE TO RELATED APPLICATIONS .[ 0001 ] The present application is a continuation of application Ser .", "label": "", "metadata": {}, "score": "64.43045"}
{"text": "The system of claim 1 , wherein the source code generator is further configured to generate assembly code for a processor .Description : .CROSS -REFERENCE TO RELATED APPLICATIONS .[ 0001 ] The present application is a continuation of application Ser .", "label": "", "metadata": {}, "score": "64.43045"}
{"text": "321 - 367 .Hartmann , S. and J. Sprenger , 2010 , \" Bayesian Epistemology , \" in Routledge Companion to Epistemology , S. Bernecker and D. Pritchard ( eds . ) , London : Routledge , pp .609 - 620 .", "label": "", "metadata": {}, "score": "64.47443"}
{"text": "[ 0026 ] A BN is a representational model ( RM ) .It represents , or models , some domain of interest .However , in practice , computations are not performed on the actual BN .The BN first must be converted to a computational model ( CM ) and then an inference engine ( IE ) uses the CM to calculate beliefs .", "label": "", "metadata": {}, "score": "64.51844"}
{"text": "[ 0026 ] A BN is a representational model ( RM ) .It represents , or models , some domain of interest .However , in practice , computations are not performed on the actual BN .The BN first must be converted to a computational model ( CM ) and then an inference engine ( IE ) uses the CM to calculate beliefs .", "label": "", "metadata": {}, "score": "64.51844"}
{"text": "Scores of PrSMs .A PRM spectrum S is represented as an ordered list of integer masses , in which the largest one is M ( S ) .Let be the set of the 20 standard amino acids with integer residue masses M ( r ) for ( the residue masses of amino acids are discretized using the same discretization method for PRM spectra ) .", "label": "", "metadata": {}, "score": "64.58392"}
{"text": "3 .Feature extraction is performed , resulting in a sequence of feature vectors .During training , these feature vectors are clustered at block 35 using a variant of the known k - means clustering algorithm .The result of clustering is a set of M clusters 36 ( for example 64 ) .", "label": "", "metadata": {}, "score": "64.678116"}
{"text": "[ 0057 ] FIG .2 is a flowchart diagram for generating source code that computes probabilities of nodes in a BN .In overview , a system n accordance with one embodiment of the present disclosure receives data representative of a BN 210 , through a data acquisition interface 215 .", "label": "", "metadata": {}, "score": "64.68234"}
{"text": "[ 0057 ] FIG .2 is a flowchart diagram for generating source code that computes probabilities of nodes in a BN .In overview , a system n accordance with one embodiment of the present disclosure receives data representative of a BN 210 , through a data acquisition interface 215 .", "label": "", "metadata": {}, "score": "64.68234"}
{"text": "In fact , every deterministic strategy A can be defeated as follows .Use strategy A to find the card that my opponent is going to play .If my opponent is going to play a king , play the ace .", "label": "", "metadata": {}, "score": "64.69774"}
{"text": "One data modeling technique is probabilistic modeling .Probabilistic modeling provides mathematically rigorous methods for handling uncertainty when modeling a problem domain .Probabilistic modeling has an extremely wide range of applications , including medical diagnoses , bioinformatics , computer vision , signal processing , control systems , cognitive science , and financial modeling .", "label": "", "metadata": {}, "score": "64.69813"}
{"text": "One data modeling technique is probabilistic modeling .Probabilistic modeling provides mathematically rigorous methods for handling uncertainty when modeling a problem domain .Probabilistic modeling has an extremely wide range of applications , including medical diagnoses , bioinformatics , computer vision , signal processing , control systems , cognitive science , and financial modeling .", "label": "", "metadata": {}, "score": "64.69813"}
{"text": "De Morgan , A. , 1847 , Formal Logic , London : Taylor and Walton .de Finetti , B. , 1937 , \" La Pr\u00e9vision : Ses Lois Logiques , Ses Sources Subjectives \" , Annales de l'Institut Henri Poincar\u00e9 , 7 : 168 ; translated as \" Foresight .", "label": "", "metadata": {}, "score": "64.78981"}
{"text": "This chapter describes a use of recurrent neural networks ( i.e. , feedback is incorporated in the computation ) as an acoustic model for continuous speech recognition .The form of the recurrent neural network is described along with an appropriate parameter estimation procedure .", "label": "", "metadata": {}, "score": "65.04002"}
{"text": "This chapter describes a use of recurrent neural networks ( i.e. , feedback is incorporated in the computation ) as an acoustic model for continuous speech recognition .The form of the recurrent neural network is described along with an appropriate parameter estimation procedure .", "label": "", "metadata": {}, "score": "65.04002"}
{"text": "0062 ]The development application may output the high level programmatic representation of the BN in any desired form .The output may be presented for example , presented on a computer display , on a paper printout or in an electronic file of any desired format .", "label": "", "metadata": {}, "score": "65.23627"}
{"text": "0062 ]The development application may output the high level programmatic representation of the BN in any desired form .The output may be presented for example , presented on a computer display , on a paper printout or in an electronic file of any desired format .", "label": "", "metadata": {}, "score": "65.23627"}
{"text": "Abstract .: We numerically solve the classical \" Game of Pure Strategy \" using linear programming .We notice an intricate even - odd behaviour in the results of our computations that seems to encourage odd or maximal bids .Keywords : . goofspiel ; game theory ; linear programming ; dynamic programming .", "label": "", "metadata": {}, "score": "65.28508"}
{"text": "However , you are taking a chance by betting only one : if your opponent had bet a two or three , then he would have won 13 points at almost no cost .Solving GOPS .To be able to solve GOPS using game theory , we use an equivalent scoring system : the player with the higher card wins from the opponent the value of the upturned card , or wins nothing in the event of a tie .", "label": "", "metadata": {}, "score": "65.51109"}
{"text": "The dynamic training stage is novel in that it requires primarily only a single repetition of a keyword ; thus , there is no distinction between keyword training and wordspotting .The method for creating the keyword HMM from the training utterance has some similarities to that used in constructing fenonic baseforms as described in the April 1988 Bahl et al paper .", "label": "", "metadata": {}, "score": "65.62327"}
{"text": "The threshold which can be selected by the user is likely to be task dependent , and can be adjusted by the user to optimize performance of the system .Proc . of the Int .Conf . on Acoustics , Speech and Signal Processing , New York , N.Y. , April 1988 , pp .", "label": "", "metadata": {}, "score": "65.70667"}
{"text": "G\u00e4rdenfors , P. , 1975a , \" Qualitative Probability as an Intensional Logic , \" Journal of Philosophical Logic , 4 : 171 - 185 . - , 1975b , \" Some Basic Theorems of Qualitative Probability , \" Studia Logica , 34 : 257 - 264 .", "label": "", "metadata": {}, "score": "65.72495"}
{"text": "Different IEs may use different types of CMs and different algorithms for operating on the CM to compute the beliefs .[ 0027 ] When a modification is made to the BN that affects a CPD , the CM must be reconstructed .", "label": "", "metadata": {}, "score": "65.8892"}
{"text": "Different IEs may use different types of CMs and different algorithms for operating on the CM to compute the beliefs .[ 0027 ] When a modification is made to the BN that affects a CPD , the CM must be reconstructed .", "label": "", "metadata": {}, "score": "65.8892"}
{"text": "The method of claim 9 , further comprising the step of varying the pre - set value to vary the number of indications that a keyword has been spotted when step ( g ) is carried out .The method of claim 9 , wherein step ( a ) includes the step of creating a mapping of feature vectors of the voice that recorded the speech to feature vectors of the spoken keyword for use in step ( c ) in creating the second HMM .", "label": "", "metadata": {}, "score": "66.01572"}
{"text": "In this paper we present a novel , efficient search strategy for large vocabulary continuous speech recognition ( LVCSR ) .The search algorithm , based on stack decoding , uses posterior phone probability estimates to substantially increase its efficiency with minimal effect on accuracy .", "label": "", "metadata": {}, "score": "66.0645"}
{"text": "In this paper we present a novel , efficient search strategy for large vocabulary continuous speech recognition ( LVCSR ) .The search algorithm , based on stack decoding , uses posterior phone probability estimates to substantially increase its efficiency with minimal effect on accuracy .", "label": "", "metadata": {}, "score": "66.0645"}
{"text": "The second upper bound of spectral probabilities .The only difference between two modified proteins Q i , d and Q i +1 , d is the i th mass .Based on this observation , if p i does not equal any mass in S , Q i +1 , d is removed from .", "label": "", "metadata": {}, "score": "66.23663"}
{"text": "[ 0113 ] The hardware for computing probabilities in a BN produced by a development application as described above may be used in many different ways in many different systems .[0114 ]In sum , methods and systems have been described for computing probabilities , as well as a development application that generates programmatic representations of a model for computing probabilities .", "label": "", "metadata": {}, "score": "66.32803"}
{"text": "[ 0113 ] The hardware for computing probabilities in a BN produced by a development application as described above may be used in many different ways in many different systems .[0114 ]In sum , methods and systems have been described for computing probabilities , as well as a development application that generates programmatic representations of a model for computing probabilities .", "label": "", "metadata": {}, "score": "66.32803"}
{"text": "[ Ellis and Wong , 2008 ] B. Ellis and W. H. Wong .Learning causal Bayesian network structures from experimental data .J. Am .Stat .Assoc . , 103:778- 789 , 2008 .[ Friedman and Koller , 2003 ] Nir Daphne Koller . structure : A bayesian approach to structure dis- covery in bayesian networks .", "label": "", "metadata": {}, "score": "66.40365"}
{"text": "0105 ]Yet another optimization may remove any term of the equations that is multiplied by zero .Since any number multiplied by zero is zero , this term would not contribute to the equation and would just waste computations at run - time .", "label": "", "metadata": {}, "score": "66.56192"}
{"text": "0105 ]Yet another optimization may remove any term of the equations that is multiplied by zero .Since any number multiplied by zero is zero , this term would not contribute to the equation and would just waste computations at run - time .", "label": "", "metadata": {}, "score": "66.56192"}
{"text": "- , 1982b , \" Simple Probabilistic Semantics for Propositional K , T , B , S4 , and S5 , \" Journal of Philosophical Logic , 11 : 443 - 458 .- , 1983 , \" Probabilistic Semantics for Propositional Modal Logics \" .", "label": "", "metadata": {}, "score": "66.62898"}
{"text": "Specification for the BN may come from other sources , including other programs or development applications .[ 0061 ] The data describing the BN may be stored in any suitable form .In some embodiments , the data describing the BN may be stored in one or more data structures in computer readable memory associated with the work station or processor on which the development application executes .", "label": "", "metadata": {}, "score": "66.6588"}
{"text": "Specification for the BN may come from other sources , including other programs or development applications .[ 0061 ] The data describing the BN may be stored in any suitable form .In some embodiments , the data describing the BN may be stored in one or more data structures in computer readable memory associated with the work station or processor on which the development application executes .", "label": "", "metadata": {}, "score": "66.6588"}
{"text": "Wordspotting was performed on the 600 speaker - dependent training sentences .One repetition of each keyword was extracted manually for keyword training .Except for this labeling , only orthographic transcription of the sentences was available .Therefore , a keyword was assumed to have been detected correctly if it was detected in a sentence known to contain the keyword .", "label": "", "metadata": {}, "score": "66.67771"}
{"text": "0048 ] In some cases , the elements of vector v1 are known at compile - time .The inner product algorithm can then be partially evaluated even further : .[ 0049 ] function inner - product3(vector v2 ) .[ 0051 ] Now the function only takes a single argument and the elements of vl can be stored in program memory instead of taking up valuable space in random access memory ( RAM ) .", "label": "", "metadata": {}, "score": "66.71235"}
{"text": "0048 ] In some cases , the elements of vector v1 are known at compile - time .The inner product algorithm can then be partially evaluated even further : .[ 0049 ] function inner - product3(vector v2 ) .[ 0051 ] Now the function only takes a single argument and the elements of vl can be stored in program memory instead of taking up valuable space in random access memory ( RAM ) .", "label": "", "metadata": {}, "score": "66.71235"}
{"text": "Source code for computing beliefs can be generated for any programming language that supports storing numbers in variables and the addition , multiplication , and division operations .Some examples include , but are not limited to , assembly , C , C++ , C # , Perl , Lisp , VisualBasic , Fortran , Basic , Scheme , Smalltalk , Cobol , Haskell , Matlab , Mathematica , and Pascal .", "label": "", "metadata": {}, "score": "66.740944"}
{"text": "Source code for computing beliefs can be generated for any programming language that supports storing numbers in variables and the addition , multiplication , and division operations .Some examples include , but are not limited to , assembly , C , C++ , C # , Perl , Lisp , VisualBasic , Fortran , Basic , Scheme , Smalltalk , Cobol , Haskell , Matlab , Mathematica , and Pascal .", "label": "", "metadata": {}, "score": "66.740944"}
{"text": "The forward - backward search is described in more detail below .In accordance with one aspect of the present invention , the technique uses an HMM for non - keyword , or background , speech in order to normalize the probabilities used for word spotting .", "label": "", "metadata": {}, "score": "66.7431"}
{"text": "Advances in exact [ Madigan and York , 1995 ] D. Madigan and J. York .Bayesian graphical models for discrete data . Inter-national Statistical Review , 63:215 - 232 , 1995 .[ Pearl , 2000 ] J. Pearl .", "label": "", "metadata": {}, "score": "66.760506"}
{"text": "Another possibility is to interpret a sentence 's probability as a measure of its ( un)certainty .For example , the sentence \" Jones is in Spain at the moment \" can have any degree of certainty , ranging from 0 ( maximal uncertainty ) to 1 ( maximal certainty ) .", "label": "", "metadata": {}, "score": "66.875404"}
{"text": "Error feedback enables the construction of statistical models that map measurements of the recognizer 's internal states and behaviors to externally de ned error conditions . ...e major differences occur .All of the experimental techniques described in this thesis have been designed to be portable to all such systems .", "label": "", "metadata": {}, "score": "66.93875"}
{"text": "[ 0116 ] In these claims , reference to an element in the singular is not intended to mean \" one and only one \" unless specifically so stated , but rather \" one or more .\" All structural and functional equivalents to the elements of the various embodiments described throughout this disclosure that are known or later come to be known to those of ordinary skill in the art are expressly incorporated herein by reference , and are intended to be encompassed by the claims .", "label": "", "metadata": {}, "score": "66.969986"}
{"text": "[ 0116 ] In these claims , reference to an element in the singular is not intended to mean \" one and only one \" unless specifically so stated , but rather \" one or more .\" All structural and functional equivalents to the elements of the various embodiments described throughout this disclosure that are known or later come to be known to those of ordinary skill in the art are expressly incorporated herein by reference , and are intended to be encompassed by the claims .", "label": "", "metadata": {}, "score": "66.969986"}
{"text": "[ 0039 ]Another type of inference engine , called AMG , avoids much of the overhead of JT - based inference engines by representing all of the multiplications and additions in an Add - Multiply Graph ( AMG ) ( also called an Arithmetic Circuit ) .", "label": "", "metadata": {}, "score": "66.97678"}
{"text": "[ 0039 ]Another type of inference engine , called AMG , avoids much of the overhead of JT - based inference engines by representing all of the multiplications and additions in an Add - Multiply Graph ( AMG ) ( also called an Arithmetic Circuit ) .", "label": "", "metadata": {}, "score": "66.97678"}
{"text": "\" [ Show abstract ] [ Hide abstract ] ABSTRACT : The fastest known exact algorithms for score- based structure discovery in Bayesian net- works on n nodes run in time and space 2nnO(1 ) .The usage of these algorithms is limited to networks on at most around 25 nodes mainly due to the space requirement .", "label": "", "metadata": {}, "score": "67.089294"}
{"text": "Many computer programs are currently written in a very generic way in a high - level language , including but not limited to C++ , Java , and Lisp .In this way , they can be used on any input data and be developed fairly quickly .", "label": "", "metadata": {}, "score": "67.11102"}
{"text": "Many computer programs are currently written in a very generic way in a high - level language , including but not limited to C++ , Java , and Lisp .In this way , they can be used on any input data and be developed fairly quickly .", "label": "", "metadata": {}, "score": "67.11102"}
{"text": "The experimental data show that better keyword detection and lower false alarm rates are obtained using keywords with more syllables .In voice editing and indexing applications , there are relatively few restrictions on the keyword vocabulary .This suggests that users could optimize system performance by using phrases rather than single words for editing and indexing applications .", "label": "", "metadata": {}, "score": "67.15004"}
{"text": "Conf .Acoustics , Speech and Signal Processing , Glasgow , Scotland , May 1989 , pp .627 - 630 ; . R. C. Rose , D. B. Paul , \" A Hidden Markov Model Based Keyword Recognition System \" .", "label": "", "metadata": {}, "score": "67.4124"}
{"text": "Differing provisions from the publisher 's actual policy or licence agreement may be applicable .[ Show abstract ] [ Hide abstract ] ABSTRACT : We study the problem of learning Bayesian network structures from data .We develop an algorithm for finding the k - best Bayesian network structures .", "label": "", "metadata": {}, "score": "67.43783"}
{"text": "For example , an element may represent a variable called m3 that is assigned the product of ml , cl , and el added to the product of m2 , c2 , and e2 .This element may already be a string of characters that represent the line of source code in a programming language to be output to a source code file .", "label": "", "metadata": {}, "score": "67.48866"}
{"text": "For example , an element may represent a variable called m3 that is assigned the product of ml , cl , and el added to the product of m2 , c2 , and e2 .This element may already be a string of characters that represent the line of source code in a programming language to be output to a source code file .", "label": "", "metadata": {}, "score": "67.48866"}
{"text": "Suppes , P. , 1966 , \" Probabilistic Inference and the Concept of Total Evidence , \" in Aspects of Inductive Logic , J. Hintikka and P. Suppes ( eds . ) , Amsterdam : Elsevier , pp .49 - 65 .", "label": "", "metadata": {}, "score": "67.81299"}
{"text": "Gupta N , Bandeira N , Keich U , Pevzner PA : Target - decoy approach and false discovery rate : when things may go wrong .Journal of the American Society for Mass Spectrometry 2011 , 22 : 1111 - 20 .", "label": "", "metadata": {}, "score": "68.05597"}
{"text": "These rules were all written by hand , using the annotated corpus to check the correctness of the rules .The final ENGCG ( English constraint grammar ) system had 3600 rules !The rules do not eliminate all ambiguity ; a few ( 4 - 7 % of words ) are left with multiple tags .", "label": "", "metadata": {}, "score": "68.132965"}
{"text": "The database consists of sentences from a 1000-word naval resource management task .A set of 25 keywords was selected from ship names in the vocabulary .The data was downsampled to 8 KHz .A vector of 12 cepstral coefficients was computed on the pre - emphasized data every 10 milliseconds .", "label": "", "metadata": {}, "score": "68.16106"}
{"text": "It will also enable efficient and automatic means of indexing into long audio documents .The system , while restricted to a single speaker , or pairs of speakers is not restricted in vocabulary size .Examples of how the wordspotting system of the invention would actually be carried out in practice will be better understood from the detailed description that follows taken in conjunction with the accompanying drawings .", "label": "", "metadata": {}, "score": "68.162384"}
{"text": "Conf . on Acoustics , Speech and Signal Processing , Tampa , Fla. , March 1985 , pp .1233 - 1236 ; .C. S. Myers , L. R. Rabiner , A. E. Rosenberg , \" An Investigation of the Use of Dynamic Time Warping for Word Spotting and Connected Speech Recognition \" .", "label": "", "metadata": {}, "score": "68.32875"}
{"text": "Frank AM , Pesavento JJ , Mizzen CA , Kelleher NL , Pevzner PA : Interpreting top - down mass spectra using spectral alignment .Analytical Chemistry 2008 , 80 : 2499 - 2505 .PubMed View Article .Tsai YS , Scherl A , Shaw JL , MacKay CL , Shaffer SA , Langridge - Smith PRR , Goodlett DR : Precursor ion independent algorithm for top - down shotgun proteomics .", "label": "", "metadata": {}, "score": "68.4727"}
{"text": "Initially introduced in the late 1960s and early 1970s , dynamic programming algorithms have become increasingly popular in automatic speech recognition .There are two reasons why this has occurred : First , the dynamic programming strategy can be combined with avery efficient and practical pruning str ... \" .", "label": "", "metadata": {}, "score": "68.761055"}
{"text": "Horn DM , Zubarev RA , McLafferty FW : Automated reduction and interpretation of high resolution electrospray mass spectra of large molecules .Journal of the American Society for Mass Spectrometry 2000 , 11 : 330 - 332 .View Article .", "label": "", "metadata": {}, "score": "68.852295"}
{"text": "This threshold can be varied depending on the importance of missing the location of the spotted word relative to finding false occurrences of the word .Another way of viewing the distinctions between the present invention and that of the prior art is to compare the teaching of the invention with that described in the previously cited April 1990 Rose paper .", "label": "", "metadata": {}, "score": "68.93977"}
{"text": "A higher threshold would lower the false alarm rate , but might possibly lower the keyword detection probability , while a lower threshold would increase probability of detection while also increasing the false alarm rate .For example , a threshold of 50 would result in the words \" fee \" and \" tree \" being detected as keywords , whereas a threshold of 95 would result in the keyword \" tree \" being missed .", "label": "", "metadata": {}, "score": "68.97444"}
{"text": "The IE is generic , that is , works with any BN .[ 0015 ] A BN consists of nodes connected by directed edges .Each node represents a particular random variable having some number of states .Both discrete nodes , which have a finite number of states , and continuous nodes , which have an infinite number of states , can exist in a BN .", "label": "", "metadata": {}, "score": "69.020836"}
{"text": "The IE is generic , that is , works with any BN .[ 0015 ] A BN consists of nodes connected by directed edges .Each node represents a particular random variable having some number of states .Both discrete nodes , which have a finite number of states , and continuous nodes , which have an infinite number of states , can exist in a BN .", "label": "", "metadata": {}, "score": "69.020836"}
{"text": "In the first stage of training the wordspotting system , the statistics for the output distributions of the HMM 's are learned .These statistics are then used in creating a background model for the talker 's speech .Training data for this stage consists of an arbitrary segment of the talker 's speech .", "label": "", "metadata": {}, "score": "69.07158"}
{"text": "The extended generating function method is a non - trivial extension of the generating function method for bottom - up mass spectrometry .It can be used to choose the correct protein - spectrum match from several candidate protein - spectrum matches for a spectrum , as well as separate correct protein - spectrum matches from incorrect ones identified from a large number of tandem mass spectra .", "label": "", "metadata": {}, "score": "69.25831"}
{"text": "Because top - down MS / MS spectra are very complex , and the charge states of most fragment ions are high , high mass resolution and high mass accuracy spectra are absolutely required .The first step in top - down spectral interpretation is usually spectral deconvolution , which converts a complex top - down spectrum to a list of monoisotopic neutral masses ( a deconvoluted spectrum ) [ 19 , 20 ] .", "label": "", "metadata": {}, "score": "69.32213"}
{"text": "Conf . on Acoustics , Speech and Signal Processing , Toronto , Canada , May 1991 , pp .309 - 312 ) .Since sentences vary between three and five seconds , an approximate conversion to false alarms per keyword per hour can be obtained by multiplying the false alarm per sentence results by 900 .", "label": "", "metadata": {}, "score": "69.40635"}
{"text": "View Article .Fagerquist CK , Garbus BR , Williams KE , Bates AH , Boyle S , Harden LA : Web - based software for rapid top - down proteomic identification of protein biomarkers , with implications for bacterial identification .", "label": "", "metadata": {}, "score": "69.541954"}
{"text": "INTRODUCTION Search is an interesting problem in the field of large vocabulary speech recognition .Typically the acoustic vectors correspondi ... . by Mary P. Harper , Randall A. Helzerman - COMPUTER SPEECH AND LANGUAGE , 1995 . \" ...A text - based and spoken language processing framework based on the Constraint Dependency Grammar ( CDG ) developed by Maruyama [ 24 , 25 ] is discussed .", "label": "", "metadata": {}, "score": "69.70803"}
{"text": "All the experiments were run under Linux on an ordinary desktop PC with a 3.0GHz Intel Pentium processor and 2.0 GB of memory .Page 7 .Algorithm Computing posteriors of all edges given maximum indegree k 1 .Precomputation .Time complex- ity O(nk+1 ) .", "label": "", "metadata": {}, "score": "69.74115"}
{"text": "Outlook .The first author made a version of the program that stored the actual probability vectors associated with the optimal strategies .These strategies were then used in a simple program that actually played the 9-card game .Despite the counterintuitive nature of these results , the computer player did win the majority of the games .", "label": "", "metadata": {}, "score": "69.76558"}
{"text": "Initially \u03a6 i is the product of all CPTs and evidence potentials assigned to clique i and \u03a6 ij consists of all ones .A clique is chosen to be the root of the tree , and then messages are passed in two phases : from the leaves of the tree towards the root and then from the root towards the leaves .", "label": "", "metadata": {}, "score": "69.7832"}
{"text": "Initially \u03a6 i is the product of all CPTs and evidence potentials assigned to clique i and \u03a6 ij consists of all ones .A clique is chosen to be the root of the tree , and then messages are passed in two phases : from the leaves of the tree towards the root and then from the root towards the leaves .", "label": "", "metadata": {}, "score": "69.7832"}
{"text": "To use the inference engine in a different programming language , the inference engine may either have to be re - written in the other language or some sort of inter - language communication protocol may have to be used .It may be time - consuming and error - prone to rewrite a complex algorithm such as a BN inference engine in a different programming language .", "label": "", "metadata": {}, "score": "69.82894"}
{"text": "To use the inference engine in a different programming language , the inference engine may either have to be re - written in the other language or some sort of inter - language communication protocol may have to be used .It may be time - consuming and error - prone to rewrite a complex algorithm such as a BN inference engine in a different programming language .", "label": "", "metadata": {}, "score": "69.82894"}
{"text": "77 , No . 2 , February 1989 , pp .257 - 285 , whose contents are herein incorporated by reference .The Gaussian output statistics are not updated during this training .Other background HMMs are possible .For example , a bigram network ( see , for example , Kai - Fu Lee Phd Thesis 1988 , Carnegie Mellon , Large - Vocabulary Speaker - Independent Continuous Speech Recognition : The SPHINX System ) could be created from the states and trained similarly .", "label": "", "metadata": {}, "score": "69.87256"}
{"text": "Mysteriously , one 's initial bet should be a 1 only when the upcard is a 3 , 4 , 7 , 11 or 13 .Also when the initial card is N and N is even , one should never bet a 1 .", "label": "", "metadata": {}, "score": "69.91146"}
{"text": "Tishby , Naftali Z. , \" On the Application Mixture AR Hidden Markov Models to Text Independent Speaker Recognition \" .IEEE Transactions on Signal Processing , vol .39 , No . 3 , Mar. 1991 , pp .563 - 570 .", "label": "", "metadata": {}, "score": "69.953674"}
{"text": "However , should the system fail to find the word , in accordance with another aspect of the invention , a second spoken repetition of the word could be used to update or refine the original word model created , for example , using well - known Baum - Welch training .", "label": "", "metadata": {}, "score": "69.987976"}
{"text": "Also , there is the danger of overfitting , particularly if we are trying to train too many parameters from too little data .Using forward - backward for training a model without a tagged corpus : the Xerox tagger .More unsupervised tagging .", "label": "", "metadata": {}, "score": "70.122375"}
{"text": "As an example , consider a model of a vase containing nine marbles : five are black and four are white .Let us assume that P assigns a probability of 1/9 to each marble , which captures the idea that one is equally likely to pick any marble .", "label": "", "metadata": {}, "score": "70.45445"}
{"text": "Proc . of the Int .Conf . on Acoustics , Speech and Signal Processing , Glasgow , Scotland , May 1989 , pp .627 - 630 .R. C. Rose , D. B. Paul , \" A Hidden Markov Model Based Keyword Recognition System \" .", "label": "", "metadata": {}, "score": "70.897934"}
{"text": "9 shows the merged HMM for typical keyword and non - keyword speech ; .FIGS .10 - 12 illustrate various aspects of the invention implemented with several similar sample keywords ; .FIGS . 13 and 14 show the results of actually using the inventive method for wordspotting ; .", "label": "", "metadata": {}, "score": "71.10748"}
{"text": "4 shows a simple BN .These BN nodes all represent random variables .The variables can take on some state , however it is uncertain as to what states these can take on .The edges connecting these nodes represent causal relations .", "label": "", "metadata": {}, "score": "71.38049"}
{"text": "4 shows a simple BN .These BN nodes all represent random variables .The variables can take on some state , however it is uncertain as to what states these can take on .The edges connecting these nodes represent causal relations .", "label": "", "metadata": {}, "score": "71.38049"}
{"text": "nonfinite -ed forms .noun - adjective homographs .They claimed that they were able to achieve very high inter - annotator agreement .Their tagger begins with a dictionary look - up which assigns each word all possible parts of speech .", "label": "", "metadata": {}, "score": "71.49307"}
{"text": "I make improvements to the current approach to predicting and analyzing error behaviors , which is currently based only on the measurement ofword error rate .The speech recognizer 's functionality is extended to include con dence annotations , which are \\meta - level \" markings that indicate how certain the recognizer is that it has decoded its input correctly .", "label": "", "metadata": {}, "score": "71.54721"}
{"text": "Any compiler 310 suitable for the programming language the source code was generated in can then compile the output source code .[0070 ] One technique to generate source code in a programming language for computing probabilities uses partial evaluation to traverse the AMG to calculate probabilities , but instead of performing the calculations directly , the calculations that would be performed are output as code .", "label": "", "metadata": {}, "score": "71.54869"}
{"text": "Any compiler 310 suitable for the programming language the source code was generated in can then compile the output source code .[0070 ] One technique to generate source code in a programming language for computing probabilities uses partial evaluation to traverse the AMG to calculate probabilities , but instead of performing the calculations directly , the calculations that would be performed are output as code .", "label": "", "metadata": {}, "score": "71.54869"}
{"text": "If , however , we want to maximize the probability of winning , and not the amount won , then the results may be different .Indeed , suppose the remaining cards are queen , king , player one has 2,4 in his hand , and player two has ace , 3 in her hand .", "label": "", "metadata": {}, "score": "71.648315"}
{"text": "This invention relates to speech - recognition systems , and in particular to a system for wordspotting based on hidden Markov models ( HMMs ) .BACKGROUND OF INVENTION .Speech recognition is the ability of a computer controlled system to recognize speech .", "label": "", "metadata": {}, "score": "71.664116"}
{"text": "Generic algorithms may suffer from performance overhead , however , since they need to compute , at run - time , those computations that are necessary to perform the specific problem being solved .Generic BN inference engines may also have to compute , at run - time , those computations that are necessary to perform to calculate probabilities for the specific BN they are currently working with .", "label": "", "metadata": {}, "score": "71.7088"}
{"text": "Generic algorithms may suffer from performance overhead , however , since they need to compute , at run - time , those computations that are necessary to perform the specific problem being solved .Generic BN inference engines may also have to compute , at run - time , those computations that are necessary to perform to calculate probabilities for the specific BN they are currently working with .", "label": "", "metadata": {}, "score": "71.7088"}
{"text": "15 is a schematic block diagram to indicate use of the wordspotting invention for voice editing .Suppose a user has input at 87 to some kind of conventional voice recording apparatus 88 a dictated voice message .After finishing , the user recognizes that an error was made in the message , say an erroneous proposed meeting time or place .", "label": "", "metadata": {}, "score": "71.90073"}
{"text": "( The cepstral coefficients are a way of characterizing the short - time spectrum .Other spectral estimates are possible , for example a DFT could be used ) .This uses standard , well - known techniques .An arbitrary segment of the user 's speech is required for training ( for example 1 or 2 minutes ) .", "label": "", "metadata": {}, "score": "71.95314"}
{"text": "s j of S .The residue mass of S [ 1 : j ] is j .Let be set of all proteins with a residue mass j .Then , .Suppose P contains m amino acids and the residue mass of P is j .", "label": "", "metadata": {}, "score": "72.01361"}
{"text": "15 is a schematic block diagram to illustrate wordspotting in a voice editing application .DETAILED DESCRIPTION OF PREFERRED EMBODIMENTS .Training typically requires two stages .The first is a static stage in which statistics for a given talker are learned and a model for the background speech is obtained ( see the Rose et al , April 1990 paper listed above ) .", "label": "", "metadata": {}, "score": "72.05224"}
{"text": "MS - Align+ identified 2,404 complete PrSMs with two PTMs , and TD - GF was used to compute the spectral probabilities for the 2,404 PrSMs .The running time for computing spectral probabilities was 1,317 minutes ( about 22 hours ) .", "label": "", "metadata": {}, "score": "72.47533"}
{"text": "Constraint grammar tagger .Constraint grammar was developed by Fred Karlsson and his group at the University of Helsinki .It used a detailed tag set which , however , avoided some of the problematic ambiguities of other tag sets , such as .", "label": "", "metadata": {}, "score": "72.72457"}
{"text": "This approach is simple and powerful when a large population of PSMs is reported .However , it fails to decide the correctness of single PSMs .In addition , it is unable to compute accurate FDRs when the target protein database is small ( e.g. , a database with only one protein ) or when only a small number of PSMs are reported [ 7 ] .", "label": "", "metadata": {}, "score": "72.727844"}
{"text": "For example , as the input speech is being scanned for keywords , the sound waveform of the utterance can be displayed and scrolls along the screen during the scanning process , with a spotted keyword highlighted in the same manner that word finders highlight text in word - processed documents .", "label": "", "metadata": {}, "score": "72.75124"}
{"text": "Further suppose that you choose to bet one ( i.e. , the ace ) .When you turn your card up , you found out that your opponent bet his king winning 13 points .You are happy with this result because you now have 12 more betting points , which should more than make up for the lost 13 points .", "label": "", "metadata": {}, "score": "72.76526"}
{"text": "See the March 1985 Higgens et al paper .SUMMARY OF INVENTION .An object of the invention is a method and system for spotting a keyword spoken by a talker in previously recorded speech by the same talker .A further object of the invention is to spot keywords in previously recorded speech for voice editing and indexing purposes .", "label": "", "metadata": {}, "score": "72.97813"}
{"text": "The system of claim 2 , wherein the partial evaluation subsystem is configured to perform an optimization .The system of claim 9 , further comprising a user interface configured to specify one or more optimizations to be performed , and further configured to specify one or more parameters required by the selected optimizations .", "label": "", "metadata": {}, "score": "73.018295"}
{"text": "The system of claim 2 , wherein the partial evaluation subsystem is configured to perform an optimization .The system of claim 9 , further comprising a user interface configured to specify one or more optimizations to be performed , and further configured to specify one or more parameters required by the selected optimizations .", "label": "", "metadata": {}, "score": "73.018295"}
{"text": "[ 0030 ] Each message from clique i to clique j causes the separator potential \u03a6 ij the clique potential \u03a6 j to be updated as follows : .[0031 ]In the above equations , C is the set of nodes assigned to clique i and S is the set of nodes assigned to separator ( i , j ) .", "label": "", "metadata": {}, "score": "73.03119"}
{"text": "[ 0030 ] Each message from clique i to clique j causes the separator potential \u03a6 ij the clique potential \u03a6 j to be updated as follows : .[0031 ]In the above equations , C is the set of nodes assigned to clique i and S is the set of nodes assigned to separator ( i , j ) .", "label": "", "metadata": {}, "score": "73.03119"}
{"text": "Such an artifact may also contain structures providing a shell of functions for methods , or other programmatic objects into which source code statements may be inserted .In this case , the element for m3 could be output in Java syntax as : .", "label": "", "metadata": {}, "score": "73.143776"}
{"text": "Such an artifact may also contain structures providing a shell of functions for methods , or other programmatic objects into which source code statements may be inserted .In this case , the element for m3 could be output in Java syntax as : .", "label": "", "metadata": {}, "score": "73.143776"}
{"text": "The error tolerances for precursor masses and fragment masses were set to 15 ppm , and carbamidomethylation was set as the fixed PTM .By restricting the search space to only complete PrSMs with one PTM , MS - Align+ identified 4,291 PrSMs .", "label": "", "metadata": {}, "score": "73.17408"}
{"text": "R C. Rose , E. I. Chang , R. P. Lippman , \" Techniques for Information Retrieval from Voice Messages \" , Proc .Int .Conf .Acoustics , Speech and Signal Processing Toronto , Canada , May 1991 , pp .", "label": "", "metadata": {}, "score": "73.302704"}
{"text": "In the drawings : .FIG .1a is a typical background HMM ; .FIG .1b is a typical keyword HMM ; .FIG .1c is a typical HMM network used for spotting keywords in accordance with the invention ; .", "label": "", "metadata": {}, "score": "73.53273"}
{"text": "Degree of essentialness .If \u03b3 does not belong to any minimal essential premise set , then the degree of essentialness of \u03b3 is 0 .With these definitions , a refined version of Theorem 2 can be established : .Theorem 4 .", "label": "", "metadata": {}, "score": "73.768875"}
{"text": "Volume II , Oxford : Oxford University Press , 1987 , pp .83 - 113 .Miller , D. , 1966 , \" A Paradox of Information , \" British Journal for the Philosophy of Science , 17 : 59 - 61 .", "label": "", "metadata": {}, "score": "74.12442"}
{"text": "View Article .Shen Y , Toli\u0107 N , Hixson KK , Purvine SO , Anderson GA , Smith RD : De novo sequencing of unique sequence tags for discovery of post - translational modifications of proteins .Analytical Chemistry 2008 , 80 : 7742 - 7754 .", "label": "", "metadata": {}, "score": "74.33966"}
{"text": "[ 0011 ] FIG .4 illustrates is a simple BN that models weather and wet grass .[ 0012 ]FIG .5 illustrates an AMG for computing beliefs in the BN of FIG .4 . DETAILED DESCRIPTION .[ 0013 ] A method and system is described for computing probabilities of variables in a belief network .", "label": "", "metadata": {}, "score": "74.39845"}
{"text": "[ 0011 ] FIG .4 illustrates is a simple BN that models weather and wet grass .[ 0012 ]FIG .5 illustrates an AMG for computing beliefs in the BN of FIG .4 . DETAILED DESCRIPTION .[ 0013 ] A method and system is described for computing probabilities of variables in a belief network .", "label": "", "metadata": {}, "score": "74.39845"}
{"text": "1629 - 1632 ) , is computationally demanding .Once the keyword endpoints are determined , an additional computation is required to compute the score .In contrast , the system of the invention does backtracking only when a keyword endpoint is hypothesized .", "label": "", "metadata": {}, "score": "74.4532"}
{"text": "MS and MS / MS spectra was collected at a resolution of 60,000 .A total of 3,704 higher - energy C - trap dissociation ( HCD ) MS / MS spectra were obtained .Spectral probabilities for PrSMs with one PTM .", "label": "", "metadata": {}, "score": "74.53252"}
{"text": "View Article .Meng F , Cargile BJ , Miller LM , Forbes AJ , Johnson JR , Kelleher NL : Informatics and multiplexing of intact protein identification in bacteria and the archaea .Nature Biotechnology 2001 , 19 : 952 - 7 .", "label": "", "metadata": {}, "score": "74.5464"}
{"text": "Eng JK , McCormack AL , Yates JR : An approach to correlate tandem mass spectral data of peptides with amino acid sequences in a protein database .Journal of the American Society for Mass Spectrometry 1994 , 5 : 976 - 989 .", "label": "", "metadata": {}, "score": "74.74117"}
{"text": "Since some spectra in the EC data set were generated from truncated proteins , the biomarker mode was chosen for the analysis of the EC data set .The error tolerances for precursor masses and fragment masses were set as 15 ppm .", "label": "", "metadata": {}, "score": "74.78116"}
{"text": "Alternatively , the executable file may be loaded into one or more other device with a computers or processors for execution .As a specific example , the executable file may be loaded into a handheld device or other processor having significantly less processing capability than the workstation on which the development application executes .", "label": "", "metadata": {}, "score": "75.140724"}
{"text": "Alternatively , the executable file may be loaded into one or more other device with a computers or processors for execution .As a specific example , the executable file may be loaded into a handheld device or other processor having significantly less processing capability than the workstation on which the development application executes .", "label": "", "metadata": {}, "score": "75.140724"}
{"text": "A BN consists of nodes connected by directed edges .Each node represents a particular random variable having some number of states .Each edge is directed from a parent node to a child node and represents the causal influence of the parent node on the child node .", "label": "", "metadata": {}, "score": "75.144104"}
{"text": "A BN consists of nodes connected by directed edges .Each node represents a particular random variable having some number of states .Each edge is directed from a parent node to a child node and represents the causal influence of the parent node on the child node .", "label": "", "metadata": {}, "score": "75.144104"}
{"text": "Liu X , Mammana A , Bafna V : Speeding up tandem mass spectral identification using indexes .Bioinformatics 2012 , 28 : 1692 - 7 .PubMed View Article .Nucleic Acids Residue 2006 , 34 ( Database):187 - 91 .", "label": "", "metadata": {}, "score": "75.2424"}
{"text": "The development application may run on a computer with single processor or any combination of multiple or networked processors .[ 0060 ] In some embodiments , the representation of the BN may be supplied by a user .In such embodiments , the workstation , or other processor , executing the development application may have a user interface allowing a human user to input parameters describing the BN .", "label": "", "metadata": {}, "score": "75.38891"}
{"text": "The development application may run on a computer with single processor or any combination of multiple or networked processors .[ 0060 ] In some embodiments , the representation of the BN may be supplied by a user .In such embodiments , the workstation , or other processor , executing the development application may have a user interface allowing a human user to input parameters describing the BN .", "label": "", "metadata": {}, "score": "75.38891"}
{"text": "We prefer to call them \" acoustic units \" , since they are different than the phonetic units \" phonemes \" used by linguists .As will be explained later , the system works better for spotting multi - syllable words than for single syllable words .", "label": "", "metadata": {}, "score": "75.843704"}
{"text": "Rose has used a modification of dynamic programming , which does continual backtracking , as opposed to waiting until the end of the utterance to backtrack .While this alleviates the delay to some extent , the continual backtrack ( known as partial backtrace , see Brown et al . , Proc . of the Int .", "label": "", "metadata": {}, "score": "75.920975"}
{"text": "Intact proteins may have N or C - terminal truncations , e.g. , the removal of a signal peptide .If a top - down MS / MS spectrum is matched to an intact protein without N- or C - terminal truncations , the PrSM is called a complete PrSM .", "label": "", "metadata": {}, "score": "76.176216"}
{"text": "Arl\u00f3 Costa , H. , 2005 , \" Non - Adjunctive Inference and Classical Modalities , \" Journal of Philosophical Logic , 34 : 581 - 605 .Bacchus , F. , 1990 , Representing and Reasoning with Probabilistic Knowledge , Cambridge , MA : The MIT Press .", "label": "", "metadata": {}, "score": "76.29848"}
{"text": "0106 ] If a term in the equations is added by a value of zero , the addition does not need to take place since zero added to any number is that number .Similarly , if a term in the equations is multiplied by a value of one , the multiplication does not need to take place since any number multiplied by one is that number .", "label": "", "metadata": {}, "score": "76.30559"}
{"text": "0106 ] If a term in the equations is added by a value of zero , the addition does not need to take place since zero added to any number is that number .Similarly , if a term in the equations is multiplied by a value of one , the multiplication does not need to take place since any number multiplied by one is that number .", "label": "", "metadata": {}, "score": "76.30559"}
{"text": "The error tolerances for precursor masses and fragment masses were set as 15 ppm and two unknown PTMs were allowed .Using 1 % spectrum - level FDR , 1,478 spectra were identified .ProSightPC [ 10 ] was also applied to analyze the EC data set .", "label": "", "metadata": {}, "score": "76.45256"}
{"text": "0058 ]In one embodiment of the present disclosure , a development application is provided that produces a high level programmatic representation of a specific BN .The high level programmatic representation may be pseudo code , source code or other suitable representation .", "label": "", "metadata": {}, "score": "76.65666"}
{"text": "0058 ]In one embodiment of the present disclosure , a development application is provided that produces a high level programmatic representation of a specific BN .The high level programmatic representation may be pseudo code , source code or other suitable representation .", "label": "", "metadata": {}, "score": "76.65666"}
{"text": "The codebook for the talker of the recorded speech can be created as before .But a mapping between this codebook and the speech of the user must be established .An important application for the wordspotting system of the invention is not only indexing of recorded speech , but especially for interactive voice editing of recorded speech , such as voice mail , dictation , or audio documentation .", "label": "", "metadata": {}, "score": "77.39642"}
{"text": "[ 0108 ] Some programming languages specify that the compiled machine code of a particular function or method can not exceed a pre - specified amount .For example , the compiled bytecode of a method in Java can not exceed 65,535 bytes due to limitations in the Java bytecode instruction set .", "label": "", "metadata": {}, "score": "77.79091"}
{"text": "[ 0108 ] Some programming languages specify that the compiled machine code of a particular function or method can not exceed a pre - specified amount .For example , the compiled bytecode of a method in Java can not exceed 65,535 bytes due to limitations in the Java bytecode instruction set .", "label": "", "metadata": {}, "score": "77.79091"}
{"text": "IEEE Trans . on Acoustics , Speech and Signal Processing , Vol . ASSP-25 , No . 5 , October 1977 , pp .361 - 367 ; . A. L. Higgens , R. E. Wohlford , \" Keyword Recognition Using Template Concatenation \" .", "label": "", "metadata": {}, "score": "77.95159"}
{"text": "A round consists of turning up the next card from the middle pile and then the two players \" bet \" on the upturned card , each player choosing one card and then simultaneously displaying it to the other player .The player showing the highest card wins the value of the upturned card .", "label": "", "metadata": {}, "score": "77.95497"}
{"text": "The conditional spectral probabilities estimated using the new random databases are different from those using the random databases in Subsection \" Computation of correct conditional spectral probabilities \" because the protein sequences in the new random databases are not independent .Parameter C p was estimated as the average ratio 0.693 between the probabilities computed based on the new databases and the random databases in Subsection \" Computation of correct conditional spectral probabilities \" for the 202 PrSMs .", "label": "", "metadata": {}, "score": "78.15227"}
{"text": "At Patents you can conduct a Patent Search , File a Patent Application , find a Patent Attorney , or search available technology through our Patent Exchange .Patents are available using simple keyword or date criteria .If you are looking to hire a patent attorney , you 've come to the right place .", "label": "", "metadata": {}, "score": "78.181915"}
{"text": "The method of claim 1 , wherein the first HMM is formed by a parallel connection of states .The method of claim 4 , wherein the second HMM is formed by a left to right model wherein each state has a self transition and transitions to the next two following states .", "label": "", "metadata": {}, "score": "78.90061"}
{"text": "Examples of hardware may include , but are not limited to , electronic circuits , Field Programmable Gate Arrays ( FPGA ) , Application - Specific Integrated Circuits ( ASIC ) , biological computers , molecular computers , and quantum computers .", "label": "", "metadata": {}, "score": "78.919235"}
{"text": "Examples of hardware may include , but are not limited to , electronic circuits , Field Programmable Gate Arrays ( FPGA ) , Application - Specific Integrated Circuits ( ASIC ) , biological computers , molecular computers , and quantum computers .", "label": "", "metadata": {}, "score": "78.919235"}
{"text": "8 shows the HMM 60 which would be created for the quantization sequence in FIG .7 .Each state has a self transition 61 , a transition 62 to the next state , and a transition 63 which skips a state .", "label": "", "metadata": {}, "score": "79.13084"}
{"text": "If the upcard is a 1 , one should never bet a 1 but should bet a 4 nearly 50 % of the time !When the upcard is a 2 , one should never bet a 1 but should bet a 6 about 35 % of the time !", "label": "", "metadata": {}, "score": "79.42497"}
{"text": "Mendelson , E. Introducing Game Theory and Its Applications ; Chapman & Hall / CRC : Washington , DC , USA , 2004 .[ Google Scholar ] .\u00a9 2012 by the authors ; licensee MDPI , Basel , Switzerland .", "label": "", "metadata": {}, "score": "80.229744"}
{"text": "All the 627 spectra were identified by MS - Align+ coupled with TD - GF .The test results show that MS - Align+ coupled with TD - GF outperformed the biomarker mode of ProSightPC .Conclusions .The experiments showed that the extended generating function method achieves high accuracy in computing spectral probabilities of PrSMs with PTMs .", "label": "", "metadata": {}, "score": "80.32028"}
{"text": "All MS / MS spectra in the EC data set were deconvoluted by MS - Deconv [ 20 ] .The EC proteome database was downloaded from the Swiss - Prot database ; a combined protein database was generated by concatenating the EC proteome database and a shuffled decoy database .", "label": "", "metadata": {}, "score": "80.3837"}
{"text": "Molecular & Cellular Proteomics 2010 , 9 : 2772 - 2782 .View Article .Tanner S , Shu H , Frank A , Wang L - C , Zandi E , Mumby M , Pevzner PA , Bafna V : InsPecT : identification of posttranslationally modified peptides from tandem mass spectra .", "label": "", "metadata": {}, "score": "80.4216"}
{"text": "Altschul SF , Gish W , Miller W , Myers EW , Lipman DJ : Basic local alignment search tool .Journal of Molecular Biology 1990 , 215 : 403 - 10 .PubMed .Copyright .\u00a9 Liu et al . ; licensee BioMed Central Ltd. 2014 .", "label": "", "metadata": {}, "score": "80.949295"}
{"text": "PubMed View Article .Karabacak NM , Li L , Tiwari A , Hayward LJ , Hong P , Easterling ML , Agar JN : Sensitive and specific identification of wild type and variant proteins from 8 to 669 kDa using top - down mass spectrometry .", "label": "", "metadata": {}, "score": "81.19512"}
{"text": "As top - down mass spectrometry , which often identifies intact proteins with post - translational modifications , becomes available in many laboratories , the estimation of statistical significance of top - down protein identification results has come into great demand .", "label": "", "metadata": {}, "score": "81.197716"}
{"text": "Evaluation of TD - GF The 202PrSMs were randomly divided into a training data set ( 101 PrSMs ) and a test data set ( 101 PrSMs ) .The training data set was used to estimated the value of K in Equation ( 13 ) .", "label": "", "metadata": {}, "score": "81.2259"}
{"text": "PubMed View Article .Liu X , Hengel S , Wu S , Toli\u0107 N , Pasa - Toli\u0107 L , Pevzner PA : Identification of ultramodified proteins using top - down spectra .Journal of Proteome Research 2013 , 12 : 5830 - 5838 .", "label": "", "metadata": {}, "score": "81.79082"}
{"text": "Authors ' contributions .XL , SL , and SK designed the TD - GF method .XL implemented the TD - GF method in JAVA .XL and MS did the experiments on tandem mass spectrometry data sets .XL and SL wrote the manuscript .", "label": "", "metadata": {}, "score": "82.248184"}
{"text": "mass spectrometry spectral probabilities dynamic programming .Background .Peptide and protein identification in mass spectrometry ( MS)-based proteomics involves searching tandem mass spectrometry ( MS / MS ) spectra against a protein database using a search engine .In bottom - up MS , most search engines calculate a similarity score between a spectrum and a peptide and report a best - scoring peptide - spectrum match ( PSM ) for each spectrum [ 1 - 5 ] .", "label": "", "metadata": {}, "score": "83.69004"}
{"text": "Many novices bid in rounded numbers .\" The Game of Pure Strategy .Goofspiel , also called Game of Pure Strategy ( GOPS ) is a two person game .Take a standard 52 card deck and discard all of the cards of one suit .", "label": "", "metadata": {}, "score": "83.706566"}
{"text": "PubMed View Article .Craig R , Beavis RC : A method for reducing the time required to match protein sequences with tandem mass spectra .Rapid Communication of Mass Spectrometry 2003 , 17 : 2310 - 6 .View Article .", "label": "", "metadata": {}, "score": "84.634"}
{"text": "[ 0059 ] The development application may be a software program written in any suitable programming language .For example , the development application may be written in the Java programming language , but other programming languages such as C++ may be used .", "label": "", "metadata": {}, "score": "84.67993"}
{"text": "[ 0059 ] The development application may be a software program written in any suitable programming language .For example , the development application may be written in the Java programming language , but other programming languages such as C++ may be used .", "label": "", "metadata": {}, "score": "84.67993"}
{"text": "Molecular & Cellular Proteomics 2010 , 9 : 2840 - 2852 .View Article .Elias JE , Gygi SP : Target - decoy search strategy for mass spectrometry - based proteomics .Methods in Molecular Biology 2010 , 604 : 55 - 71 .", "label": "", "metadata": {}, "score": "84.72439"}
{"text": "At 23 a conventional endpoint detector is employed to detect the spoken keyword endpoints .Basically , the endpoint detector is deleting silences at the beginning and end of the keyword utterance .At 24 , a similar process is used for creating a HMM for the keyword to be spotted .", "label": "", "metadata": {}, "score": "85.080284"}
{"text": "Reconstructing the CM can also be described as rebuilding the IE .When a modification is made to the BN that affects evidence , the CM typically does not need to be reconstructed .The IE just needs to incorporate the new evidence and recalculate beliefs .", "label": "", "metadata": {}, "score": "85.174034"}
{"text": "Reconstructing the CM can also be described as rebuilding the IE .When a modification is made to the BN that affects evidence , the CM typically does not need to be reconstructed .The IE just needs to incorporate the new evidence and recalculate beliefs .", "label": "", "metadata": {}, "score": "85.174034"}
{"text": "A total of 14,041 collision - induced dissociation ( CID ) MS / MS spectra were acquired .The detailed experiment procedure can be found in [ 13 ] .The performance of TD - GF on proteoform identification was tested on an Escherichia coli ( EC ) data set .", "label": "", "metadata": {}, "score": "87.14476"}
{"text": "These three cards are then discarded .The game ends after 13 rounds and the winner is the person who obtained the most points ( one needs 46 points or more to win ) .Though the mechanics of the game are simple , the strategy is not .", "label": "", "metadata": {}, "score": "87.92651"}
{"text": "More than a thousand proteins can be identified in a single top - down MS experiment [ 9 ] and many methods have been proposed for identification of proteoforms using top - down tandem mass spectra [ 10 - 17 ] .", "label": "", "metadata": {}, "score": "89.04234"}
{"text": "A Salmonella typhimurium ( ST ) data set [ 13 ] was used to test TD - GF .A protein mixture of ST was analyzed using an LTQ - Orbitrap ( Thermo Fisher Scientific ) .MS and MS / MS spectra were collected at a resolution of 60,000 and 30,000 , respectively .", "label": "", "metadata": {}, "score": "90.44458"}
{"text": "PubMed Central PubMed View Article .Zamdborg L , LeDuc RD , Glowacz KJ , Kim Y - B , Viswanathan V , Spaulding IT , Early BP , Bluhm EJ , Babai S , Kelleher NL : ProSight PTM 2.0 : improved protein identification and characterization for top down mass spectrometry .", "label": "", "metadata": {}, "score": "90.58735"}
{"text": "The 203 PrSMs were selected for computing \" correct \" conditional spectral probabilities .Computation of \" correct \" conditional spectral probabilities For each of the 203 PrSMs ( spectra ) , a random database of 10 6 proteins was generated .", "label": "", "metadata": {}, "score": "91.21001"}
{"text": "PubMed Central PubMed View Article .Molecular BioSystems 2010 , 6 : 1532 - 9 .PubMed Central PubMed View Article .Liu X , Sirotkin Y , Shen Y , Anderson G , Tsai YS , Ting YS , Goodlett DR , Smith RD , Bafna V , Pevzner PA : Protein identification using top - down spectra .", "label": "", "metadata": {}, "score": "92.45214"}
{"text": "In some arguments , however , the truth of the premises does not fully guarantee the truth of the conclusion , but it still renders it highly likely .A typical example is the argument with premises \" The first swan I saw was white \" , ... , \" The 1000 th swan I saw was white \" , and conclusion \" All swans are white \" .", "label": "", "metadata": {}, "score": "92.82741"}
{"text": "Koivisto and Sood , 2004 ] M. Koivisto and K. Sood .Exact Bayesian structure discovery in Bayesian net- works .Journal of Machine Learning Research , 5:549 - 573 , 2004 .[ Koivisto , 2006 ] M. Koivisto .Bayesian structure discovery in Bayesian networks .", "label": "", "metadata": {}, "score": "96.09552"}
{"text": "It differs from the molecular mass of the protein by the mass of a water molecule .The mass shift of a PTM is the mass difference between the modified form ( with the PTM ) and the unmodified form of an amino acid residue .", "label": "", "metadata": {}, "score": "96.65677"}
{"text": "4 , therefore , it can be seen that clouds cause rain , and then rain causes grass to get wet .Because there is no edge directly from Cloudy to WetGrass , clouds do n't directly cause grass to be wet , however they do cause the grass to be wet by rain .", "label": "", "metadata": {}, "score": "97.4384"}
{"text": "4 , therefore , it can be seen that clouds cause rain , and then rain causes grass to get wet .Because there is no edge directly from Cloudy to WetGrass , clouds do n't directly cause grass to be wet , however they do cause the grass to be wet by rain .", "label": "", "metadata": {}, "score": "97.4384"}
{"text": "To increase the number of identified PrSMs with one PTM , a mutated ST protein database was generated by adding a glycine residue to the middle of each protein sequence in the ST proteome .When the mutated ST protein database is used , a PrSM without PTMs can be identified as a PrSM with one PTM .", "label": "", "metadata": {}, "score": "98.02916"}
{"text": "MIT Press , Cambridge , MA , 2001 .Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .", "label": "", "metadata": {}, "score": "101.93862"}
{"text": "Acknowledgements .This work was supported by a startup fund provided by Indiana University - Purdue University Indianapolis .Declarations .Publication of this article was funded by a startup fund provided by Indiana University - Purdue University Indianapolis .Competing interests .", "label": "", "metadata": {}, "score": "110.607635"}
{"text": "Authors ' Affiliations .Department of BioHealth Informatics , Indiana University - Purdue University Indianapolis .Center for Computational Biology and Bioinformatics , Indiana University School of Medicine .Department of Computer Science , City University of Hong Kong .Biological Sciences Division , Pacific Northwest National Laboratory .", "label": "", "metadata": {}, "score": "118.27691"}
{"text": "University of Maryland , University College , 3501 University Blvd .East , Adelphi , MD , USA ; Email : .Mathematical Institute , Georg - August University , Bunsenstra\u00dfe 3 - 5 , D-37073 G\u00f6ttingen , Germany ; Email : .", "label": "", "metadata": {}, "score": "122.60553"}
