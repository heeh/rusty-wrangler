{"text": "2 ) Sum Entropy . 3 ) Difference variance .( 12 ) Figure 4 : ( b ) Segmented Result .4 ) Difference entropy .In the classification stage , the extracted GLCM features are given to the two layers feed forward neural network to classify text and non text region .", "label": "", "metadata": {}, "score": "33.085724"}
{"text": "The classification with the highest entropy score is defined as the maximum entropy model .New documents are then classified according to their similarity with this model .With the perceptron learning method term vectors ( chapter 15 ) and iteratively induced weights are used to classify documents .", "label": "", "metadata": {}, "score": "33.943443"}
{"text": "Regarding classification accuracy , the Mirrors - based SEMANTIC - FEATURES seemed to suffer from including too broad semantic information and performed significantly worse than the other two knowledge sources .The Mirrors - based RELATED - WORDS , on the other hand , was as good as , and sometimes better , than the traditional word model , but the differences were not found to be statistically significant .", "label": "", "metadata": {}, "score": "34.862495"}
{"text": "Once a model is deemed sufficiently accurate , it can then be used to automatically predict information about new language data .These predictive models can be combined into systems that perform many useful language processing tasks , such as document classification , automatic translation , and question answering .", "label": "", "metadata": {}, "score": "37.614998"}
{"text": "Because the parameters for Maximum Entropy classifiers are selected using iterative optimization techniques , they can take a long time to learn .This is especially true when the size of the training set , the number of features , and the number of labels are all large .", "label": "", "metadata": {}, "score": "37.682384"}
{"text": "We also utilized entropy plots to determine the clustering of hot spot variability sites .We determined potential post - translational modification sites and correlated their positions and frequencies to MEME blocks , frequency of amino acid substitutions , antigenic sites and receptor binding sites .", "label": "", "metadata": {}, "score": "38.649582"}
{"text": "Since SEMANTIC - FEATURES may include a very high number of related words , a second knowledge source was also developed - RELATED - WORDS - which attempts to selects a stricter class of near - related word senses in the wordnet - like Mirrors network .", "label": "", "metadata": {}, "score": "38.706974"}
{"text": "The clustering results serve as training samples to train a Support Vector Machine ( SVM ) .The steerable pyramid transform is presented in [ 6].The features extracted from pyramid sub bands serve to locate and classify regions into text and non text in some noise infected , deformed , multilingual , multi script document images .", "label": "", "metadata": {}, "score": "38.826366"}
{"text": "The detection of text region is achieved by extracting the statistical features from the GLCM of the document image and these features are used as an input of neural network for classification .Experimental results show that our method gives better text extraction than other methods .", "label": "", "metadata": {}, "score": "39.603916"}
{"text": "The detection of text region is achieved by extracting the statistical features from the GLCM of the document image and these features are used as an input of neural network for classification .Experimental results show that our method gives better text extraction than other methods .", "label": "", "metadata": {}, "score": "39.603916"}
{"text": "The intuition that motivates Maximum Entropy classification is that we should build a model that captures the frequencies of individual joint - features , without making any unwarranted assumptions .An example will help to illustrate this principle .Suppose we are assigned the task of picking the correct word sense for a given word , from a list of ten possible senses ( labeled A - J ) .", "label": "", "metadata": {}, "score": "39.634567"}
{"text": "Since the number of irrelevant documents far outweighs the number of relevant documents , the accuracy score for a model that labels every document as irrelevant would be very close to 100 % .It is therefore conventional to employ a different set of measures for search tasks , based on the number of items in each of the four categories shown in 3.1 : .", "label": "", "metadata": {}, "score": "39.678406"}
{"text": "It consists of two stages - automatic localized multilevel thresholding , and multi - plane region matching and assembling .Two novel approaches for document image segmentation are presented in [ 10].In text line segmentation a Viterbi algorithm is proposed while an SVM - based metric is adopted to locate words in each text line .", "label": "", "metadata": {}, "score": "39.806618"}
{"text": "However , if we instead evaluate the classifier on a more balanced corpus , where the most frequent word sense has a frequency of 40 % , then a 95 % accuracy score would be a much more positive result .( A similar issue arises when measuring inter - annotator agreement in 2 . ) 3.3 Precision and Recall .", "label": "", "metadata": {}, "score": "39.829403"}
{"text": "Chapter 14 Clustering presents a number of methods and algorithms that classify items on the basis of some measure of similarity .Hierarchical ( section 14.1 ) and non - hierarchical ( section 14.2 ) approaches are covered .By using these techniques , words can be classified automatically into categories that reflect something - like semantic similarity .", "label": "", "metadata": {}, "score": "41.54505"}
{"text": "Continuing on , the classifier checks if the word ends in \" s \" .1.5 Exploiting Context .By augmenting the feature extraction function , we could modify this part - of - speech tagger to leverage a variety of other word - internal features , such as the length of the word , the number of syllables it contains , or its prefix .", "label": "", "metadata": {}, "score": "41.99736"}
{"text": "It follows that distribution may be viewed as the place of a lexical item relative to other lexical items on the level of semantic classes and sub - classes .The analysis of lexical collocability in word - groups is widely applied for different purposes : to find out typical , most commonly used collocations in modern English , to investigate the possibility / impossibility of certain types of meaning in certain types of collocations , and so on .", "label": "", "metadata": {}, "score": "42.022728"}
{"text": "Several methods are used .With Decision Trees ( section 16.1 ) a given document is described in terms of feature - value trees where possible values are labeled with probability scores .The combination of a document 's value scores gives the likelihood that it belongs to a given class .", "label": "", "metadata": {}, "score": "42.088097"}
{"text": "Co- occurrence statistics and vector similarity methods are used to obtain classes of semantically similar words .The chapter also gives good coverage of evaluation measures ( precision , recall and f - measure ) .Part III Grammar Chapter 9 presents Markov models , a variation of the language models presented in section 6.1 .", "label": "", "metadata": {}, "score": "42.13356"}
{"text": "Although it can be possible to gain insight by studying them , it typically takes a lot more work .But all explicit models can make predictions about new \" unseen \" language data that was not included in the corpus used to build the model .", "label": "", "metadata": {}, "score": "42.19423"}
{"text": "This chapter also covers the notions of hypothesis testing and significance ( section 5.3 ) .Chapter 6 concerns statistical inference and the application of probabilistic approaches to language modeling .This is a stochastic method where our expectation of seeing some word or category in a text is based only on the information we have about the preceding n words ( section 6.1 ) .", "label": "", "metadata": {}, "score": "42.32586"}
{"text": "A novel scheme for the extraction of textual areas of an image using globally matched wavelet filter is presented in [ 7].A clustering based technique has been devised for estimating globally matched wavelet filters using a collection of ground truth images .", "label": "", "metadata": {}, "score": "42.35241"}
{"text": "It is assumed that sameness / difference in distribution is indicative of sameness / difference in part - of - speech meaning .It is also observed that in a number of cases words have different lexical meanings in different distributional patterns .", "label": "", "metadata": {}, "score": "42.437756"}
{"text": "In the cases of this type distributional analysis traditionally understood as the analysis on the level of different parts of speech , as an abstraction on the syntagmatic level is of little help in the analysis of sameness or difference of lexical meaning .", "label": "", "metadata": {}, "score": "42.534668"}
{"text": "1.6 Sequence Classification .In order to capture the dependencies between related classification tasks , we can use joint classifier models , which choose an appropriate labeling for a collection of related inputs .In the case of part - of - speech tagging , a variety of different sequence classifier models can be used to jointly choose part - of - speech tags for all the words in a given sentence .", "label": "", "metadata": {}, "score": "42.563484"}
{"text": "Recently attempts have been made to elucidate this problem in different languages both on the level of theoretical and applied lexicology and lexicography .It should be pointed out , however , that the statistical study of vocabulary has some inherent limitations .", "label": "", "metadata": {}, "score": "42.939808"}
{"text": "An efficient and computationally fast method for segmenting text and graphics part of document images based on textural cues is presented in [ 1].The segmentation method uses the notion of multi scale wavelet analysis and statistical pattern recognition .", "label": "", "metadata": {}, "score": "42.967796"}
{"text": "The extent to which explicit models can give us insights into linguistic patterns depends largely on what kind of model is used .Some models , such as decision trees , are relatively transparent , and give us direct information about which factors are important in making decisions and about which factors are related to one another .", "label": "", "metadata": {}, "score": "43.208794"}
{"text": "Here , documents are classified according to their similarity to positively classified documents .Although much of the material here is also covered by ( Charniak 1996 ) and ( Krenn and Samuelsson 1997 ) and less so by ( Allen 1995 ) , Manning and Schetze 's work provides wider , more detailed coverage .", "label": "", "metadata": {}, "score": "43.624084"}
{"text": "In particular , for each consecutive word index i , a score is computed for each possible current and previous tag .This same basic approach is taken by two more advanced models , called Maximum Entropy Markov Models and Linear - Chain Conditional Random Field Models ; but different algorithms are used to find scores for tag sequences .", "label": "", "metadata": {}, "score": "43.635094"}
{"text": "The semantic count is based on the differentiation of the meanings in the OED and the frequencies are expressed as percentage , so that the teacher and textbook writer may find it easier to understand and use the list .An example will make the procedure clear .", "label": "", "metadata": {}, "score": "43.92469"}
{"text": "It is divided into two parts , one on probability theory and the other on information theory .Chapter 3 provides a self - contained introduction to the linguistic ( largely syntactic ) theory that will be used in subsequent chapters .", "label": "", "metadata": {}, "score": "44.371372"}
{"text": "However they do not mention any of the software tools that have been produced that make the annotation task less time consuming and therefore less expensive ( such as MITRE 's Alembic Workbench ) .Chapter 8 presents methods for Lexical Acquisition .", "label": "", "metadata": {}, "score": "44.44615"}
{"text": "In contrast , the Maximum Entropy classifier model leaves it up to the user to decide what combinations of labels and features should receive their own parameters .In particular , it is possible to use a single parameter to associate a feature with more than one label ; or to associate more than one feature with a given label .", "label": "", "metadata": {}, "score": "44.459393"}
{"text": "Chapter 15 Topics in Information Retrieval covers automatic term extraction from documents .One of the approaches uses a vector space model , following from material in chapter 8 , and the measures of Term Frequency and Term Frequency Inverse Document Frequency which are derived here ( section 15.2.2 ) .", "label": "", "metadata": {}, "score": "44.636406"}
{"text": "The two significant hot spots of variability were confirmed by looking at conserved regions generated by BIOEDIT , with a minimum length of 15 amino acids and maximum entropy 0.2 , and this region did not overlap with the conserved region analysis ( data not shown ) .", "label": "", "metadata": {}, "score": "45.159935"}
{"text": "In certain cases , however , the meaning or to be more exact one of the word - meanings may be structured differently .Firstly , in morphologically non - motivated words distributional structure is not correlated with certain meaning .This is observed in all non - motivated words .", "label": "", "metadata": {}, "score": "45.191463"}
{"text": "In this case , the classifier will make its decisions based only on information about which of the common suffixes ( if any ) a given word has .Now that we 've defined our feature extractor , we can use it to train a new \" decision tree \" classifier ( to be discussed in 4 ): .", "label": "", "metadata": {}, "score": "45.288902"}
{"text": "This allows feature values to interact , but can be problematic when two or more features are highly correlated with one another .Maximum Entropy classifiers use a basic model that is similar to the model used by naive Bayes ; however , they employ iterative optimization to find the set of feature weights that maximizes the probability of the training set .", "label": "", "metadata": {}, "score": "45.566315"}
{"text": "In summary , descriptive models provide information about correlations in the data , while explanatory models go further to postulate causal relationships .Most models that are automatically constructed from a corpus are descriptive models ; in other words , they can tell us what features are relevant to a given pattern or construction , but they ca n't necessarily tell us how those features and patterns relate to one another .", "label": "", "metadata": {}, "score": "45.816353"}
{"text": "For example , consider the part - of - speech tagging task .At one extreme , we could create the training set and test set by randomly assigning sentences from a data source that reflects a single genre ( news ) : .", "label": "", "metadata": {}, "score": "46.001907"}
{"text": "Thus , statistical analysis is applied in different branches of linguistics including lexicology as a means of verification and as a reliable criterion for the selection of the language data provided qualitative description of lexical items is available .Immediate Constituents Analysis .", "label": "", "metadata": {}, "score": "46.021294"}
{"text": "An assumption made in this chapter is that a parser should first try out the analysis of a word string that is most commonly observed in a treebank .However some best - first techniques based on human reading - time experiments suggest that this is not always the best approach ( Crocker and Pickering 1996 unpublished work ) .", "label": "", "metadata": {}, "score": "46.142693"}
{"text": "To check how reliable the resulting classifier is , we compute its accuracy on the test set .And once again , we can use show_most_informative_features ( ) to find out which features the classifier found to be most informative . 1.4 Part - of - Speech Tagging .", "label": "", "metadata": {}, "score": "46.464947"}
{"text": "Even more interestingly , despite the motivating example , verbs under - performed all other tests , while still being consistently better than random .The tests ranged from 60\\% to 67\\% accuracy , even sometimes doing worse than the 64\\% accurate human - based classifier from Pang 2002 .", "label": "", "metadata": {}, "score": "46.494736"}
{"text": "How do you think that your results might be different if you used a different feature extractor ?What features are relevant in this distinction ?Build a classifier that predicts when each word should be used .However , dialog acts are highly dependent on context , and some sequences of dialog act are much more likely than others .", "label": "", "metadata": {}, "score": "46.500145"}
{"text": "The contribution from each feature is then combined with this prior probability , to arrive at a likelihood estimate for each label .The label whose likelihood estimate is the highest is then assigned to the input value .5.1 illustrates this process .", "label": "", "metadata": {}, "score": "46.550415"}
{"text": "This is the approach taken by Hidden Markov Models .Hidden Markov Models are similar to consecutive classifiers in that they look at both the inputs and the history of predicted tags .However , rather than simply finding the single best tag for a given word , they generate a probability distribution over tags .", "label": "", "metadata": {}, "score": "46.79003"}
{"text": "To begin with , they 're simple to understand , and easy to interpret .This is especially true near the top of the decision tree , where it is usually possible for the learning algorithm to find very useful features .", "label": "", "metadata": {}, "score": "46.899925"}
{"text": "But note that history will only contain tags for words we 've already classified , that is , words to the left of the target word .Thus , while it is possible to look at some features of words to the right of the target word , it is not possible to look at the tags for those words ( since we have n't generated them yet ) .", "label": "", "metadata": {}, "score": "46.989334"}
{"text": "Scaling Up to Large Datasets .Python provides an excellent environment for performing basic text processing and feature extraction .If you plan to train classifiers with large amounts of training data or a large number of features , we recommend that you explore NLTK 's facilities for interfacing with external machine learning packages .", "label": "", "metadata": {}, "score": "47.054653"}
{"text": "When large amounts of annotated data are available , it is common to err on the side of safety by using 10 % of the overall data for evaluation .Another consideration when choosing the test set is the degree of similarity between instances in the test set and those in the development set .", "label": "", "metadata": {}, "score": "47.065994"}
{"text": "The resulting likelihood score can be thought of as an estimate of the probability that a randomly selected value from the training set would have both the given label and the set of features , assuming that the feature probabilities are all independent .", "label": "", "metadata": {}, "score": "47.13127"}
{"text": "PROPOSED SYSTEM .The block diagram of the proposed text extraction from document images is shown in Figure 3 , where Figure .( IJCSIS ) International Journal of Computer Science and Information Security , Vol . 8 , No . 5 , August 2010 .", "label": "", "metadata": {}, "score": "47.246735"}
{"text": "How can we identify particular features of language data that are salient for classifying it ?How can we construct models of language that can be used to perform language processing tasks automatically ?What can we learn about language from these models ?", "label": "", "metadata": {}, "score": "47.252182"}
{"text": "There are many probability distributions that we could choose for the ten senses , such as : .Although any of these distributions might be correct , we are likely to choose distribution ( i ) , because without any more information , there is no reason to believe that any word sense is more likely than any other .", "label": "", "metadata": {}, "score": "47.378967"}
{"text": "But there 's a lot to be learned from taking a closer look at how these learning methods select models based on the data in a training set .An understanding of these methods can help guide our selection of appropriate features , and especially our decisions about how those features should be encoded .", "label": "", "metadata": {}, "score": "47.40207"}
{"text": "When training a supervised classifier , you should split your corpus into three datasets : a training set for building the classifier model ; a dev - test set for helping select and tune the model 's features ; and a test set for evaluating the final model 's performance .", "label": "", "metadata": {}, "score": "47.503693"}
{"text": "First , we therefore need a sense inventory which is computationally tractable .Subjectively defined sense distinctions have been the norm in WSD research ( especially the Princeton WordNet , Fellbaum , 1998 ) .But WSD studies increasingly show that the WordNet senses are too fine - grained for efficient WSD , which has made WordNet less attractive for machine - learned WSD .", "label": "", "metadata": {}, "score": "47.55185"}
{"text": "We used GLCM features quantitatively to evaluate textural parameters and representations and to determine which parameter values and representations are best for extracting text region .The detection of text region is achieved by extracting the statistical features from the GLCM of the document image and these features are used as an input of neural network for classification .", "label": "", "metadata": {}, "score": "47.573643"}
{"text": "We could then use those interactions to adjust the contributions that individual features make .To make this more precise , we can rewrite the equation used to calculate the likelihood of a label , separating out the contribution made by each feature ( or label ) : .", "label": "", "metadata": {}, "score": "47.618565"}
{"text": "( b )During prediction , the same feature extractor is used to convert unseen inputs to feature sets .These feature sets are then fed into the model , which generates predicted labels .In the rest of this section , we will look at how classifiers can be employed to solve a wide variety of tasks .", "label": "", "metadata": {}, "score": "47.643494"}
{"text": "In general , simple classifiers always treat each input as independent from all other inputs .In many contexts , this makes perfect sense .For example , decisions about whether names tend to be male or female can be made on a case - by - case basis .", "label": "", "metadata": {}, "score": "47.7555"}
{"text": "The Mirrors method is applicable for any language pair for which a parallel corpus and word alignment is available .The appeal of the Mirrors method and its translational basis for lexical semantics is that it offers an objective and consistent - and hence , testable - criterion , as opposed to the traditional subjective judgements in lexicon classification ( cf .", "label": "", "metadata": {}, "score": "47.966133"}
{"text": "One solution is to make use of a lexicon , which describes how different words relate to one another .Using WordNet lexicon , augment the movie review document classifier presented in this chapter to use features that generalize the words that appear in a document , making it more likely that they will match words found in the training data .", "label": "", "metadata": {}, "score": "48.081917"}
{"text": "Make use of this fact to build a consecutive classifier for labeling dialog acts .Be sure to consider what features might be useful .See the code for the consecutive classifier for part - of - speech tags in 1.7 to get some ideas .", "label": "", "metadata": {}, "score": "48.114773"}
{"text": "Conversely , if there is information found in the hypothesis that is absent from the text , then there will be no entailment .Not all words are equally important - Named Entity mentions such as the names of people , organizations and places are likely to be more significant , which motivates us to extract distinct information for word s and ne s ( Named Entities ) .", "label": "", "metadata": {}, "score": "48.125736"}
{"text": "Because of the potentially complex interactions between the effects of related features , there is no way to directly calculate the model parameters that maximize the likelihood of the training set .Therefore , Maximum Entropy classifiers choose the model parameters using iterative optimization techniques , which initialize the model 's parameters to random values , and then repeatedly refine those parameters to bring them closer to the optimal solution .", "label": "", "metadata": {}, "score": "48.198895"}
{"text": "There is also discussion of the SGML encoding that is important for much current work ( section 4.3 ) .Part II Words Chapter 5 examines collocations and simple term extraction using Mutual Information ( 2.2.3 ) methods .There is some brief discussion of proper name recognition ( sections 5.5 and 5.6 ) , but a failure to highlight the particular problems associated with that subject .", "label": "", "metadata": {}, "score": "48.2714"}
{"text": "Relationship between post - translational modification sites , MEME blocks , amino acid substitutions and entropy .It was observed that MEME block 7 , 2 and 1 contain the greatest number of post - translational modification sites ( Prosite motifs ) ( Figure 4 ) .", "label": "", "metadata": {}, "score": "48.33171"}
{"text": "The process can then be repeated until all of the inputs have been labeled .This strategy is demonstrated in 1.7 .First , we must augment our feature extractor function to take a history argument , which provides a list of the tags that we 've predicted for the sentence so far .", "label": "", "metadata": {}, "score": "48.569187"}
{"text": "S.Mandal , S.P.Chowdhury , A.K.Das and Bhabatosh Chanda , \" Automated Detection and Segmentation of Table of Contents Page from Document Images \" , IEEE Seventh International Conference on Document Analysis and Recognition on 2003 .JiaLi and Robert M.Gray , \" Context Based Multiscale Classification of Document Images Using Wavelet Coefficient Distributions \" , IEEE transactions on image processing , Vol.9 , No.9 , September2000.R. Nicole , \" Title of paper with only first word capitalized , \" J. Name Stand .", "label": "", "metadata": {}, "score": "48.588192"}
{"text": "Do you find any of them surprising ?Using the same training and test data , and the same feature extractor , build three classifiers for the task : a decision tree , a naive Bayes classifier , and a Maximum Entropy classifier .", "label": "", "metadata": {}, "score": "48.60824"}
{"text": "From the discussion of the linguistic problems above we may conclude that an exact and exhaustive definition of the linguistic qualitative aspects of the items under consideration must precede the statistical analysis .Secondly , we must admit that not all linguists have the mathematical equipment necessary for applying statistical methods .", "label": "", "metadata": {}, "score": "48.652714"}
{"text": "Later chapters do introduce further statistical methods , but it is to chapters 2 and 3 that the reader will return for the fundamentals .Chapter 4 introduces the notion of corpus - based work and provides an overview of the low level formatting issues that must be addressed when using documents as an information source for further processing ( section 4.2 ) .", "label": "", "metadata": {}, "score": "48.87235"}
{"text": "The review is followed by a method for discourse segmentation ' TextTiling ' that is based on information about the distribution of terms in a document ( section 15.5 ) .Chapter 16 Text Categorisation introduces a number of statistical classification methods .", "label": "", "metadata": {}, "score": "48.896168"}
{"text": "On the other hand , if the input values have a wide variety of labels , then there are many labels with a \" medium \" frequency , where neither P(l ) nor log 2P(l ) is small , so the entropy is high .", "label": "", "metadata": {}, "score": "49.056137"}
{"text": "In our approach , GLCM features are used as a feature vector to extract the text region from the document images .In the following section gives the overview of feature extraction of the text region and the graphics /image part .", "label": "", "metadata": {}, "score": "49.0751"}
{"text": "We used a similar dataset , as released by the authors , and did our best to use the same libraries and pre - processing techniques .+ In addition to replicating Pang 's work as closely as we could , we extended the work by exploring an additional dataset , additional preprocessing techniques , and combining classifiers .", "label": "", "metadata": {}, "score": "49.117554"}
{"text": "In addition , it [ 11 ] presents the three GLCM implementations and evaluated them by a supervised Bayesian classifier on sea ice textural contexts .Texture is one of the important characteristics used in identifying objects or region of interest in an image , whether the image to be a photomicrograph , an aerial photograph , or a satellite image .", "label": "", "metadata": {}, "score": "49.569412"}
{"text": "This knowledge source functions as a point of reference to indicate how well a traditional word - based classifier could be expected to perform , given our specific data sample and using the Mirrors sense inventory .Second , two Mirrors - derived knowledge sources were tentatively implemented , both of which attempt to generalise from the actually occurring context words as a means of alleviating the sparse data problem in WSD .", "label": "", "metadata": {}, "score": "49.62262"}
{"text": "Once an initial set of features has been chosen , a very productive method for refining the feature set is error analysis .First , we select a development set , containing the corpus data for creating the model .This development set is then subdivided into the training set and the dev - test set .", "label": "", "metadata": {}, "score": "49.731598"}
{"text": "But by the time the decision tree learner has descended far enough to use these features , there is not enough training data left to reliably determine what effect they should have .If we could instead look at the effect of these features across the entire training set , then we might be able to make some conclusions about how they should affect the choice of label .", "label": "", "metadata": {}, "score": "49.750225"}
{"text": "Thus the fundamental aim of IC analysis is to segment a set of lexical units into two maximally independent sequences or ICs thus revealing the hierarchical structure of this set .Successive segmentation results in Ultimate Constituents ( UC ) , i.e. two - facet units that can not be segmented into smaller units having both sound - form and meaning .", "label": "", "metadata": {}, "score": "49.767746"}
{"text": "It goes without saying that to be useful in teaching statistics should deal with meanings as well as sound - forms as not all word - meanings are equally frequent .Besides , the number of meanings exceeds by far the number of words .", "label": "", "metadata": {}, "score": "49.81223"}
{"text": "Of course , this assumption is unrealistic ; features are often highly dependent on one another .We 'll return to some of the consequences of this assumption at the end of this section .This simplifying assumption , known as the naive Bayes assumption ( or independence assumption ) makes it much easier to combine the contributions of the different features , since we do n't need to worry about how they should interact with one another .", "label": "", "metadata": {}, "score": "49.819527"}
{"text": "However , when using bigrams , the MaxEnt and SVM classifiers did significantly better , achieving 3 - 4\\% better accuracy with part of speech tagging when measuring frequency and presence information .+ Intuitively , adjectives like ' ' beautiful ' ' , ' ' wonderful ' ' , and ' ' great ' ' hold valuable sentiment information , so we trained our classifiers after filtering out only the adjectives within reviews .", "label": "", "metadata": {}, "score": "49.840347"}
{"text": "This is exactly what the Maximum Entropy classifier does as well .In particular , for each joint - feature , the Maximum Entropy model calculates the \" empirical frequency \" of that feature - i.e. , the frequency with which it occurs in the training set .", "label": "", "metadata": {}, "score": "49.858738"}
{"text": "For document topic identification , we can define a feature for each word , indicating whether the document contains that word .To limit the number of features that the classifier needs to process , we begin by constructing a list of the 2000 most frequent words in the overall corpus .", "label": "", "metadata": {}, "score": "49.924843"}
{"text": "Distributional Analysis and Co - occurrence .Distributional analysis in its various forms is commonly used nowadays by lexicologists of different schools of thought .By the term distribution we understand the occurrence of a lexical unit relative to other lexical units of the same level ( words relative to words / morphemes relative to morphemes , etc . ) .", "label": "", "metadata": {}, "score": "49.952522"}
{"text": "Even though the individual folds might be too small to give accurate evaluation scores on their own , the combined evaluation score is based on a large amount of data , and is therefore quite reliable .A second , and equally important , advantage of using cross - validation is that it allows us to examine how widely the performance varies across different training sets .", "label": "", "metadata": {}, "score": "49.989986"}
{"text": "The average entropy over MEME block 7 and 2 CKII sites is therefore higher than for any other block .MEME block 1 has a wider boxplot than the others , indicating more CKII sites in this block .( B ) Average entropy of potential PKC phosphorylation site in the MEME blocks .", "label": "", "metadata": {}, "score": "50.04886"}
{"text": "One of the ASN sites ( amino acid 99 ) from MEME block 1 has the highest entropy ( 1.003 ) among all ASN sites .The width of the boxplots indicates that more N - glycosylation sites are observed in MEME sites 2 and 7 respectively ( D ) Average entropy of potential N - myristylation site in the MEME blocks .", "label": "", "metadata": {}, "score": "50.09407"}
{"text": "In particular , entropy is defined as the sum of the probability of each label times the log probability of that same label : .Figure 4.2 : The entropy of labels in the name gender prediction task , as a function of the percentage of names in a given set that are male .", "label": "", "metadata": {}, "score": "50.158245"}
{"text": "Note .Most classification methods require that features be encoded using simple value types , such as booleans , numbers , and strings .But note that just because a feature has a simple type , this does not necessarily mean that the feature 's value is simple to express or compute .", "label": "", "metadata": {}, "score": "50.20787"}
{"text": "The framework used by supervised classification is shown in 1.1 .Figure 1.1 : Supervised Classification .( a )During training , a feature extractor is used to convert each input value to a feature set .These feature sets , which capture the basic information about each input that should be used to classify it , are discussed in the next section .", "label": "", "metadata": {}, "score": "50.270668"}
{"text": "CORPUS REPRESENTATIVENESS FOR SYNTACTIC INFORMATION ACQUISITION N\u00faria Bel .FINDING ANCHOR VERBS FOR BIOMEDICAL IE USING PREDICATE - ARGUMENT STRUCTURES Akane YAKUSHIJI , Yuka TATEISI , Yusuke MIYAO , Jun'ichi TSUJII .DYNA :A LANGUAGE FOR WEIGHTED DYNAMIC PROGRAMMING Jason Eisner , Eric Goldlust , Noah A. Smith .", "label": "", "metadata": {}, "score": "50.351166"}
{"text": "Document image and the text extracted from the given input image are shown in Figure 4(a ) , 5(a ) and 4(b),5(b ) respectively .Figure 5 : ( b ) Segmented Result .( IJCSIS ) International Journal of Computer Science and Information Security , Vol . 8 , No . 5 , August 2010 .", "label": "", "metadata": {}, "score": "50.3952"}
{"text": "Individual features make their contribution to the overall decision by \" voting against \" labels that do n't occur with that feature very often .In particular , the likelihood score for each label is reduced by multiplying it by the probability that an input value with that label would have the feature .", "label": "", "metadata": {}, "score": "50.499825"}
{"text": "Chapter 11 Probabilistic Context Free Grammars describe an application of Hidden Markov Models to determine the probabilities of strings of words in a language .The authors present the Inside- Outside algorithm as a method for finding the most likely analysis for a sentence .", "label": "", "metadata": {}, "score": "50.64211"}
{"text": "The consensus years that exhibited the greatest frequency of substitutions in the alignment were determined .An entropy plot of the alignment was generated to highlight hot spots of variability .Additionally , conserved regions were determined using BIOEDIT [ 23 ] , and mapped to MEME blocks and functional motifs .", "label": "", "metadata": {}, "score": "50.666245"}
{"text": "It is observed that words are joined together according to certain rules .The linguistic structure of any string of words may be described as a network of grammatical and lexical restrictions.1 .The set of lexical restrictions is very complex .", "label": "", "metadata": {}, "score": "50.787918"}
{"text": "All the crucial information is here , presented from first principles .It is a very good reference book for anyone working in the field of NLP .Mikheev , A. , Grover , C. & Moens , M. ( 1998 )", "label": "", "metadata": {}, "score": "50.801567"}
{"text": "For example , the Expected Likelihood Estimation for the probability of a feature given a label basically adds 0.5 to each count(f , label ) value , and the Heldout Estimation uses a heldout corpus to calculate the relationship between feature frequencies and feature probabilities .", "label": "", "metadata": {}, "score": "50.952034"}
{"text": "The method is based on the noisy channel model ( chapter 2 ) .When reviewing the problems with machine translation techniques , the authors write , \" on the surface these are problems of the model , but they are all related to the lack of linguistic knowledge in the model .", "label": "", "metadata": {}, "score": "50.953312"}
{"text": "The first step in creating a classifier is deciding what features of the input are relevant , and how to encode those features .For this example , we 'll start by just looking at the final letter of a given name .", "label": "", "metadata": {}, "score": "51.089703"}
{"text": "Instead of just passing in the word to be tagged , we will pass in a complete ( untagged ) sentence , along with the index of the target word .This approach is demonstrated in 1.6 , which employs a context - dependent feature extractor to define a part of speech tag classifier . def pos_features ( sentence , i ) : . return features .", "label": "", "metadata": {}, "score": "51.112038"}
{"text": "In ML terms , it might be interesting to build a WSD model which learns , not how a word sense correlates with isolated words , but rather how a word sense correlates with certain classes of semantically related words .Such a tool for generalisation is clearly desirable in the face of sparse data and in view of the fact that most content words have a relatively low frequency even in larger text corpora .", "label": "", "metadata": {}, "score": "51.316006"}
{"text": "Classifiers can help us to understand the linguistic patterns that occur in natural language , by allowing us to create explicit models that capture those patterns .Typically , these models are using supervised classification techniques , but it is also possible to build analytically motivated models .", "label": "", "metadata": {}, "score": "51.371246"}
{"text": "An interesting attempt was made by G. K. Zipf to study the relation between polysemy and word frequency by statistical methods .Having discovered that there is a direct relationship between the number of different meanings of a word and its relative frequency of occurrence , Zipf proceeded to find a mathematical formula for this correlation .", "label": "", "metadata": {}, "score": "51.37833"}
{"text": "One way to capture this intuition that distribution ( i ) is more \" fair \" than the other two is to invoke the concept of entropy .In the discussion of decision trees , we described entropy as a measure of how \" disorganized \" a set of labels was .", "label": "", "metadata": {}, "score": "51.441467"}
{"text": "Segmentation of monochromatic document images into four classes are presented in [ 3].They are background , photograph , text , and graph .Features used for classification are based on .the distribution patterns of wavelet coefficients in high frequency bands .", "label": "", "metadata": {}, "score": "51.444336"}
{"text": "How likely is a given input value with a given label ?What is the most likely label for an input that might have one of two values ( but we do n't know which ) ?The Maximum Entropy classifier , on the other hand , is an example of a conditional classifier .", "label": "", "metadata": {}, "score": "51.571342"}
{"text": "I. .INTRODUCTION .The extraction of textual information from document images provides many useful applications in document analysis and understanding , such as optical character recognition , document retrieval , and compression .To - date , many effective techniques have been developed for extracting characters from monochromatic document images .", "label": "", "metadata": {}, "score": "51.580383"}
{"text": "The width of the boxplots indicates that more PKC sites are observed in MEME sites 2 , 3 and 7 respectively .( C ) Average entropy of potential N - glycosylation site in the MEME blocks .MEME blocks 4 and 5 have zero entropy at all of their ASN sites .", "label": "", "metadata": {}, "score": "51.822426"}
{"text": "Document Image Segmentation Based On Gray Level Co- Occurrence Matrices and Feed Forward Neural Network .This paper presents a new method for extracting text region from the document images employing the combination of gray level co - occurrence matrices ( GLCM ) and artificial neural networks ( ANN ) .", "label": "", "metadata": {}, "score": "51.850765"}
{"text": "Frequent use of will is indicative of news text ( 3 ) .These observable patterns - word structure and word frequency - happen to correlate with particular aspects of meaning , such as tense and topic .But how did we know where to start looking , which aspects of form to associate with which aspects of meaning ?", "label": "", "metadata": {}, "score": "51.86912"}
{"text": "These results illustrates that despite the high number of potential CKII sites at the highly conserved MEME 1 these sites remain conserved ( Figure 5a ) and the variable MEME block 2 and 7 undergo amino acid substitutions in CKII sites .", "label": "", "metadata": {}, "score": "51.947174"}
{"text": "In this paper , we have presented a novel technique for extracting the text part based on feed forward neural network by using GLCM .The GLCM features are serving as training samples to train the neural network .The performance depends on the representation generated by feature extraction from the images .", "label": "", "metadata": {}, "score": "51.956642"}
{"text": "Transformational joint classifiers work by creating an initial assignment of labels for the inputs , and then iteratively refining that assignment in an attempt to repair inconsistencies between related inputs .The Brill tagger , described in ( 1 ) , is a good example of this strategy .", "label": "", "metadata": {}, "score": "52.053947"}
{"text": "return features . words ( ' pos / cv957_8737 .The reason that we compute the set of all words in a document in , rather than just checking if word in document , is that checking whether a word occurs in a set is much faster than checking whether it occurs in a list ( 4.7 ) .", "label": "", "metadata": {}, "score": "52.089325"}
{"text": "\" But greetings , questions , answers , assertions , and clarifications can all be thought of as types of speech - based actions .Recognizing the dialogue acts underlying the utterances in a dialogue can be an important first step in understanding the conversation .", "label": "", "metadata": {}, "score": "52.170143"}
{"text": "II .METHODOLOGY . A. GLCM Gray level co occurrence matrix ( GLCM ) is the basis for the Haralick texture features [ 12].This matrix is square with dimension Ng , where Ng is the number of gray levels in the image .", "label": "", "metadata": {}, "score": "52.21337"}
{"text": "3.5 Cross - Validation .In order to evaluate our models , we must reserve a portion of the annotated data for the test set .As we already mentioned , if the test set is too small , then our evaluation may not be accurate .", "label": "", "metadata": {}, "score": "52.223503"}
{"text": "The highest myristylation sites entropy is at MEME block 9 and 7 ( Amino acid 78 and 160 respectively ) with an approximate entropy value of 1.2 .MEME block 1 and 7 have more N - myristylation sites than any other block , although MEME block 2 also has a fairly large number of myristylation sites .", "label": "", "metadata": {}, "score": "52.434258"}
{"text": "It 's important to understand what we can learn about language from an automatically constructed model .One important consideration when dealing with models of language is the distinction between descriptive models and explanatory models .Descriptive models capture patterns in the data but they do n't provide any information about why the data contains those patterns .", "label": "", "metadata": {}, "score": "52.441025"}
{"text": "\"Haralick 's Correlation \" where and are the mean and standard deviation of the row ( or column , due to symmetry ) sums .Learning to Classify Text .Detecting patterns is a central part of Natural Language Processing .", "label": "", "metadata": {}, "score": "52.49272"}
{"text": "This was summed up in the following formula where m stands for the number of meanings , F for relative frequency - tn - F1/2 .This formula is known as Zipf 's law .Though numerous corrections to this law have been suggested , still there is no reason to doubt the principle itself , namely , that the more frequent a word is , the more meanings it is likely to have .", "label": "", "metadata": {}, "score": "52.5625"}
{"text": "In our proposed method a one input layer , two hidden layers and one output layer feed forward neural network is used .Since adjacency can be defined to occur in each of four directions in a 2D , square pixel image ( horizontal , vertical , left and right diagonals as shown in Figure 1 , four such matrices can be calculated .", "label": "", "metadata": {}, "score": "52.59378"}
{"text": "we built a regular expression tagger that chooses a part - of - speech tag for a word by looking at the internal make - up of the word .However , this regular expression tagger had to be hand - crafted .", "label": "", "metadata": {}, "score": "52.860466"}
{"text": "One or the other component may prevail .Here naturally it is the extralinguistic factors that account for the difference in meaning .Of greatest importance for language teaching , however , is the investigation of lexical restrictions in collocability that are of purely intralinguistic nature and can not be accounted for by logical considerations .", "label": "", "metadata": {}, "score": "52.88089"}
{"text": "Now that we 've defined a feature extractor , we need to prepare a list of examples and corresponding class labels .Next , we use the feature extractor to process the names data , and divide the resulting list of feature sets into a training set and a test set .", "label": "", "metadata": {}, "score": "52.895245"}
{"text": "David A. Clausi and Yongping Zhao .Rapid extraction of image texture by co - occurrence using a hybrid data structure .Comput .Geosci .28 , 6 ( July 2002 ) , 763 - 774 .de O.Bastos , L. ; Liatsis , P. ; Conci , A. , Automatic texture segmentation based on k - means clustering and efficient calculation of co - occurrence features .", "label": "", "metadata": {}, "score": "53.07185"}
{"text": "In these cases , use the function nltk.classify.apply_features , which returns an object that acts like a list but does not store all the feature sets in memory : . 1.2Choosing The Right Features .Selecting relevant features and deciding how to encode them for a learning method can have an enormous impact on the learning method 's ability to extract a good model .", "label": "", "metadata": {}, "score": "53.11664"}
{"text": "Kernel - based methods have demonstrated excellent performances in a variety of pattern recognition problems .The kernel - based methods and Gabor wavelet to the segmentation of document image is presented in [ 5].The feature images are derived from Gabor filtered images .", "label": "", "metadata": {}, "score": "53.185608"}
{"text": "5 ) Energy Angular second moment ( ASM ) and Energy uses each Pij as a weight for itself .High values of ASM or Energy occur when the window is very orderly .The square root of the ASM is sometimes used as a texture measure , and is called Energy .", "label": "", "metadata": {}, "score": "53.45742"}
{"text": "IWSSIP 2008 .15th International Conference on , vol . , no . , pp.141,144 , 25 - 28 June 2008 doi : 10.1109/IWSSIP.2008.4604387 .Neighborhood size can be set using the SetRadius ( ) method .Offset for co - occurence estimation is set using the SetOffset ( ) method .", "label": "", "metadata": {}, "score": "53.642525"}
{"text": "Thus , this application computes the following Haralick textures over a neighborhood with user defined radius .Print references : .Haralick , R.M. , K. Shanmugam and I. Dinstein .Textural Features for Image Classification .IEEE Transactions on Systems , Man and Cybernetics .", "label": "", "metadata": {}, "score": "53.66205"}
{"text": "See the NLTK webpage for a list of recommended machine learning packages that are supported by NLTK .3 Evaluation .In order to decide whether a classification model is accurately capturing a pattern , we must evaluate that model .The result of this evaluation is important for deciding how trustworthy the model is , and for what purposes we can use it .", "label": "", "metadata": {}, "score": "53.70141"}
{"text": "The value of a feature in a given document is simply the number of times it appears in that document .There was no significant difference for SVMs and applying TF - IDF did not provide any improvement from using frequency for either .", "label": "", "metadata": {}, "score": "53.701885"}
{"text": "Each entry is therefore considered to be the probability that a pixel with value i will be found adjacent to a pixel of value j. .B. ANN An artificial neuron is a computational model inspired in the natural neurons .Natural neurons receive signals through synapses located on the dendrites or membrane of the neuron .", "label": "", "metadata": {}, "score": "53.71199"}
{"text": "Thompson JD , Higgins DG , Gibson TJ : CLUSTAL W : improving the sensitivity of progressive multiple sequence alignment through sequence weighting , position - specific gap penalties and weight matrix choice .Nucleic Acids Res 1994/11/11 Edition 1994 , 22 ( 22 ) : 4673 - 4680 .", "label": "", "metadata": {}, "score": "53.813656"}
{"text": "Availability : .( IJCSIS ) International Journal of Computer Science and Information Security , Vol . 8 , No . 5 , August 2010 .Dr. RM .Abstract - This paper presents a new method for extracting text region from the document images employing the combination of gray level co - occurrence matrices ( GLCM ) and artificial neural networks ( ANN ) .", "label": "", "metadata": {}, "score": "53.85049"}
{"text": "Distributional meaning of the lexical units accounts for the possibility of making up and understanding a lexical item that has never been heard or used before but whose distributional pattern is familiar to the speaker and the hearer .From the discussion of the distributional analysis above it should not be inferred that difference in distribution is always indicative of the difference in meaning and conversely that sameness of distribution is an absolutely reliable criterion of sameness of meaning .", "label": "", "metadata": {}, "score": "53.8576"}
{"text": "On the other hand , if scores vary widely across the N training sets , then we should probably be skeptical about the accuracy of the evaluation score .4 Decision Trees .In the next three sections , we 'll take a closer look at three machine learning methods that can be used to automatically build classification models : decision trees , naive Bayes classifiers , and Maximum Entropy classifiers .", "label": "", "metadata": {}, "score": "54.026398"}
{"text": "Although it 's often possible to get decent performance by using a fairly simple and obvious set of features , there are usually significant gains to be had by using carefully constructed features based on a thorough understanding of the task at hand .", "label": "", "metadata": {}, "score": "54.210907"}
{"text": "Thus , when using a more powerful model , we end up with less data that can be used to train each parameter 's value , making it harder to find the best parameter values .As a result , a generative model may not do as good a job at answering questions 1 and 2 as a conditional model , since the conditional model can focus its efforts on those two questions .", "label": "", "metadata": {}, "score": "54.26697"}
{"text": "The information gain is then equal to the original entropy minus this new , reduced entropy .The higher the information gain , the better job the decision stump does of dividing the input values into coherent groups , so we can build decision trees by selecting the decision stumps with the highest information gain .", "label": "", "metadata": {}, "score": "54.34499"}
{"text": "If we want to generate a probability estimate for each label , rather than just choosing the most likely label , then the easiest way to compute P(features ) is to simply calculate the sum over labels of P(features , label ) : . 5.2 Zero Counts and Smoothing .", "label": "", "metadata": {}, "score": "54.373203"}
{"text": "Space prevents me from making them explicit but examination of pages 384 , 385 and 391 should reveal them to the interested reader .Chapter 12 Probabilistic Parsing shows how annotated corpora ( treebanks ) can be used as the basis for finding a syntactic analysis for new sentences .", "label": "", "metadata": {}, "score": "54.399742"}
{"text": "View Article PubMed .Yalovsky S , Rodr Guez - Concepcion M , Gruissem W : Lipid modifications of proteins - slipping in and out of membranes .Trends Plant Sci 1999/10/26 Edition 1999 , 4 ( 11 ) : 439 - 445 .", "label": "", "metadata": {}, "score": "54.452072"}
{"text": "View Article PubMed .Copyright .\u00a9 Gendoo et al .2008 .This article is published under license to BioMed Central Ltd.+ We implement a series of classifiers ( Naive Bayes , Maximum Entropy , and SVM ) to distinguish positive and negative sentiment in critic and user reviews .", "label": "", "metadata": {}, "score": "54.476547"}
{"text": "This study reports unique regions in the protein that undergo either high mutation rates to possibly acquire new characteristics by post - translational modification or remain conserved for specific viral functional characteristics .Such changes are expected to enhance the ability of the virus in receptor binding , increase its infective state and escape the host immune response by modification of its antigenic sites .", "label": "", "metadata": {}, "score": "54.478558"}
{"text": "We therefore adjust our feature extractor to include features for two - letter suffixes : .Rebuilding the classifier with the new feature extractor , we see that the performance on the dev - test dataset improves by almost 2 percentage points ( from 76.5 % to 78.2 % ) : .", "label": "", "metadata": {}, "score": "54.531837"}
{"text": "The aim of pre - processing is an improvement of the image data that suppresses undesired distortions or enhances some image features relevant for further processing and analysis task .First , the given document image is converted into gray scale image .", "label": "", "metadata": {}, "score": "54.54187"}
{"text": "Let 's begin by finding out what the most common suffixes are : .Next , we 'll define a feature extractor function which checks a given word for these suffixes : . endswith(suffix ) ... return features .Feature extraction functions behave like tinted glasses , highlighting some of the properties ( colors ) in our data and making it impossible to see other properties .", "label": "", "metadata": {}, "score": "54.615246"}
{"text": "Gender Identification .In 4 we saw that male and female names have some distinctive characteristics .Names ending in a , e and i are likely to be female , while names ending in k , o , r , s and t are likely to be male .", "label": "", "metadata": {}, "score": "54.802246"}
{"text": "Chapter 7 applies the prior methods to word sense disambiguation .Several different algorithms are presented and reviewed .The authors set out supervised and unsupervised methods for disambiguation , the supervised ones being based on Bayes decision rule and Mutual Information techniques .", "label": "", "metadata": {}, "score": "54.97039"}
{"text": "( Nancy Milford ) .Distribution defined as the occurrence of a lexical unit relative to other lexical units can be interpreted as co - occurrence of lexical items and the two terms can be viewed as synonyms .It follows that by the term distribution we understand the aptness of a word in one of its meanings to collocate or to co - occur with a certain group , or certain groups of words having some common semantic component .", "label": "", "metadata": {}, "score": "54.988754"}
{"text": "We can treat RTE as a classification task , in which we try to predict the True / False label for each pair .Although it seems likely that successful approaches to this task will involve a combination of parsing , semantics and real world knowledge , many early attempts at RTE achieved reasonably good results with shallow analysis , based on similarity between the text and hypothesis at the word level .", "label": "", "metadata": {}, "score": "55.037727"}
{"text": "Naturally not all the meanings should be included in the list of the first two thousand most commonly used words .Statistical analysis of meaning frequencies resulted in the compilation of A General Service List of English Words with Semantic Frequencies .", "label": "", "metadata": {}, "score": "55.05433"}
{"text": "The highly variable MEME block 11 has the highest average PKC entropy followed by MEME block 7 ( Figure 5b ) .Four of the PKC sites at MEME block 7 are a part of the co - mutating pairs ( see below ) .", "label": "", "metadata": {}, "score": "55.131252"}
{"text": "It also helps language learners to become good observers of how language works and this is the only lasting way to become better users of language .Detailed Description .This class compute 8 local Haralick textures features .The 8 output image channels are : Energy , Entropy , Correlation , Inverse Difference Moment , Inertia , Cluster Shade , Cluster Prominence and Haralick Correlation .", "label": "", "metadata": {}, "score": "55.225193"}
{"text": "Part I Preliminaries Chapter 1 sets out the empiricist standpoint adopted throughout the volume , providing a critique of rationalist views on linguistics .The points concerning weaknesses with the ' categorical judgment ' approach to linguistics exemplified by the work of Chomsky are convincingly made with illustrative examples ( section 1.2.1 ) .", "label": "", "metadata": {}, "score": "55.262276"}
{"text": "Note that if most input values have the same label ( e.g. , if P(male ) is near 0 or near 1 ) , then entropy is low .In particular , labels that have low frequency do not contribute much to the entropy ( since P(l ) is small ) , and labels with high frequency also do not contribute much to the entropy ( since log 2", "label": "", "metadata": {}, "score": "55.325596"}
{"text": "At that point , we can use the test set to evaluate how well our model will perform on new input values . 1.3 Document Classification .In 1 , we saw several examples of corpora where documents have been labeled with categories .", "label": "", "metadata": {}, "score": "55.343536"}
{"text": "The Viterbi algorithm is presented as a means of finding the best probability traversal of Markov models .Chapter 10 Part of Speech Tagging sets out 4 different strategies and concludes with a discussion of performance and applications .The algorithms include methods based on the Markov model techniques introduced in chapter 9 and Brill 's transformation based learning method .", "label": "", "metadata": {}, "score": "55.39072"}
{"text": "View Article PubMed .Hall TA : BioEdit : a user - friendly biological sequence alignment editor and analysis program for Windows 95/98/NT .Nucleic Acids Symposium Series 1999 , 41 : 95 - 98 .Janin J , Wodak S : Conformation of amino acid side - chains in proteins .", "label": "", "metadata": {}, "score": "55.45217"}
{"text": "+ Our testbed supported testing various parameters : frequency vs. presence of features vs. term frequency - inverse document frequency , unigrams vs. bigrams vs. both , number of features , and type of feature tagging .The types of feature tagging were negation , part of speech ( POS ) , and position .", "label": "", "metadata": {}, "score": "55.518402"}
{"text": "To confirm these finding , we correlated hot spots of variability with MEME blocks , using an entropy plot of the HA alignment ( Figure 3 ) .Hot spots of variability are clustered around amino acid position 140 - 190 , and 200 - 240 .", "label": "", "metadata": {}, "score": "55.54838"}
{"text": "Thus , the input will never be assigned this label , regardless of how well the other features fit the label .In particular , just because we have n't seen a feature / label combination occur in the training set , does n't mean it 's impossible for that combination to occur .", "label": "", "metadata": {}, "score": "55.707497"}
{"text": "One study generated a model based on 131 positions in the five antigenic sites of the protein , and which could predict antigenic variants of H3N2 with an agreement rate of 83 % to existing serological data [ 5 ] .Later studies also identified twenty amino acids positions , which are potential immunodominant positions and contribute to antigenic difference between strains [ 6 ] .", "label": "", "metadata": {}, "score": "55.80606"}
{"text": "5.3 Non - Binary Features .We have assumed here that each feature is binary , i.e. that each input either has a feature or does not .Label - valued features ( e.g. , a color feature which could be red , green , blue , white , or orange ) can be converted to binary features by replacing them with binary features such as \" color - is - red \" .", "label": "", "metadata": {}, "score": "55.882248"}
{"text": "The names classifier that we have built generates about 100 errors on the dev - test corpus : .Looking through this list of errors makes it clear that some suffixes that are more than one letter can be indicative of name genders .", "label": "", "metadata": {}, "score": "56.12271"}
{"text": "The returned dictionary , known as a feature set , maps from feature names to their values .Feature names are case - sensitive strings that typically provide a short human - readable description of the feature , as in the example ' last_letter ' .", "label": "", "metadata": {}, "score": "56.20092"}
{"text": "Unfortunately , the number of possible tag sequences is quite large .Given a tag set with 30 tags , there are about 600 trillion ( 30 10 ) ways to label a 10-word sentence .In order to avoid considering all these possible sequences separately , Hidden Markov Models require that the feature extractor only look at the most recent tag ( or the most recent n tags , where n is fairly small ) .", "label": "", "metadata": {}, "score": "56.329052"}
{"text": "The simple algorithm for selecting decision stumps described above must construct a candidate decision stump for every possible feature , and this process must be repeated for every node in the constructed decision tree .A number of algorithms have been developed to cut down on the training time by storing and reusing information about previously evaluated examples .", "label": "", "metadata": {}, "score": "56.420166"}
{"text": "A 20X20 non overlapping window is used to extract the features .The GLCM features are extracted from the text region and the graphics parts and stored separately for training phase in the classifier stage .The following 10 GLCM features are selected for the be the the entry in a feature extraction phase .", "label": "", "metadata": {}, "score": "56.518982"}
{"text": "( A ) Average entropy of potential CKII phosphorylation sites in the MEME blocks .Blocks 1 , 5 and 9 have zero entropy at all CKII sites .The majority of MEME blocks 2 and 7 CKII sites have nonzero entropy .", "label": "", "metadata": {}, "score": "56.58589"}
{"text": "Document Image Document Image .N is the number of gray levels in the image . 1 ) Contrast To emphasize a large amount of contrast , create weights so that the calculation results in a larger figure when there is great contrast .", "label": "", "metadata": {}, "score": "56.629883"}
{"text": "Automated WSD is relevant for Natural Language Processing systems such as machine translation ( MT ) , information retrieval , information extraction and content analysis .The most successful WSD approaches to date are so - called supervised machine learning ( ML ) techniques , in which the system ' learns ' the contextual characteristics of each sense from a training corpus that contains concrete examples of contexts in which a word sense typically occurs .", "label": "", "metadata": {}, "score": "56.66904"}
{"text": "Amino acid positions that do not exhibit any changes over the years have entropy of 0 , whereas positions of high variability are represented by peak in the plot .Two hot spots of variability were observed and are clustered around amino acid position 140 - 190 , and 200 - 240 .", "label": "", "metadata": {}, "score": "56.673687"}
{"text": "For example , decision trees can be very effective at capturing phylogeny trees .However , decision trees also have a few disadvantages .One problem is that , since each branch in the decision tree splits the training data , the amount of training data available to train nodes lower in the tree can become quite small .", "label": "", "metadata": {}, "score": "56.70347"}
{"text": "The listing in 2.1 shows how this can be done . def segment_sentences ( words ) : . sents.append(words[start:i+1 ] ) . sents.append(words[start : ] ) .return sents . 2.2 Identifying Dialogue Act Types .When processing dialogue , it can be useful to think of utterances as a type of action performed by the speaker .", "label": "", "metadata": {}, "score": "56.898342"}
{"text": "Most evaluation techniques calculate a score for a model by comparing the labels that it generates for the inputs in a test set ( or evaluation set ) with the correct labels for those inputs .This test set typically has the same format as the training set .", "label": "", "metadata": {}, "score": "56.957054"}
{"text": "I recommend this book both as an exemplary teaching aid and a rigorous introduction to statistical NLP .It is to be commended for its readability and the coherent presentation of a notoriously difficult subject .This reviewer did note some flaws , but they represent very minor points in the context of a 680 page book .", "label": "", "metadata": {}, "score": "57.016186"}
{"text": "Of course , we do n't usually build naive Bayes classifiers that contain two identical features .However , we do build classifiers that contain features which are dependent on one another .For example , the features ends - with(a ) and ends - with(vowel ) are dependent on one another , because if an input value has the first feature , then it must also have the second feature .", "label": "", "metadata": {}, "score": "57.062805"}
{"text": "AUTOMATIC CLUSTERING OF COLLOCATION FOR DETECTING PRACTICAL SENSE BOUNDARY Saim Shin , Key - Sun Choi . CO - TRAINING FOR PREDICTING EMOTIONS WITH SPOKEN DIALOGUE DATA Beatriz Maeireizo , Diane Litman , Rebecca Hwa .FRAGMENTS AND TEXT CATEGORIZATION Jan Blat\u00e1k , Eva Mr\u00e1kov\u00e1 , Lubos Popel\u00ednsky .", "label": "", "metadata": {}, "score": "57.0844"}
{"text": "These disparities suggest evidence that the movie dataset does not satisfy the conditional independence assumption .+ One key decision in a bag - of - words feature set is which words to include .Using more words provides more information , but harms the performance of the classifiers , and words that appear only infrequently in the training data may not present accurate information due to the law of small numbers .", "label": "", "metadata": {}, "score": "57.134537"}
{"text": "A somewhat better approach is to ensure that the training set and test set are taken from different documents : .If we want to perform a more stringent evaluation , we can draw the test set from documents that are less closely related to those in the training set : .", "label": "", "metadata": {}, "score": "57.170486"}
{"text": "- occurring pairs of mutations .The ClustalW alignment of the 17 consensus sequence from each year was submitted into CRASP [ 24 ] to determine significantly correlated pairs of amino acids which co - mutate within the alignment .Correlating pairs were determined using the Pearson correlation coefficient matrix based on the average accessibility surface area of the amino acids , at a significance level of 95 % .", "label": "", "metadata": {}, "score": "57.186455"}
{"text": "The sense - tagged English - Norwegian Parallel Corpus ( the ENPC ) is comparable in size to the existing SemCor .The sense - tagged material formed the basis for a series of controlled experiments , in which the knowledge source is varied but where we maintain the same experimental framework in terms of the classification algorithm , data sets , lexical sample and sense inventory .", "label": "", "metadata": {}, "score": "57.19055"}
{"text": "One solution to this problem is to perform multiple evaluations on different test sets , then to combine the scores from those evaluations , a technique known as cross - validation .In particular , we subdivide the original corpus into N subsets called folds .", "label": "", "metadata": {}, "score": "57.218933"}
{"text": "The Maximum Entropy classifier model is a generalization of the model used by the naive Bayes classifier .Like the naive Bayes model , the Maximum Entropy classifier calculates the likelihood of each label for a given input value by multiplying together the parameters that are applicable for the input value and label .", "label": "", "metadata": {}, "score": "57.25886"}
{"text": "No Nh , m-1 Nh , m-2 Nh,1 Figure 2 .A multi layer networks with m layers of input .The Haralick statistics are calculated for co - occurrence matrices generated using each of these directions of adjacency .In our proposed system , based on the gray level occurrence matrix , 10 features are selected for text extraction .", "label": "", "metadata": {}, "score": "57.282234"}
{"text": "To generate a labeled input , the model first chooses a label for the input , then it generates each of the input 's features based on that label .Every feature is assumed to be entirely independent of every other feature , given the label .", "label": "", "metadata": {}, "score": "57.34881"}
{"text": "HIERARCHY EXTRACTION BASED ON INCLUSION OF APPEARANCE Eiko Yamamoto , Kyoko Kanzaki , and Hitoshi Isahara .MULTIMODAL DATABASE ACCESS ON HANDHELD DEVIDES Elsa Pecourt , Norbert Reithinger .CONSTRUCTING TRANSLITERATION LEXICONS FROM WEB CORPORA Jin - Shea Kuo , Ying - Kuei Yang .", "label": "", "metadata": {}, "score": "57.40471"}
{"text": "P04 - 1075 [ bib ] : Dan Shen ; Jie Zhang ; Jian Su ; Guodong Zhou ; Chew - Lim Tan Multi - Criteria - based Active Learning for Named Entity Recognition .P04 - 1076 [ bib ] : Cheng Niu ; Wei Li ; Rohini K. Srihari Weakly Supervised Learning for Cross - document Person Name Disambiguation Supported by Information Extraction .", "label": "", "metadata": {}, "score": "57.474586"}
{"text": "These features indicate that all important words in the hypothesis are contained in the text , and thus there is some evidence for labeling this as True .The module nltk.classify.rte_classify reaches just over 58 % accuracy on the combined RTE test data using methods like these .", "label": "", "metadata": {}, "score": "57.559578"}
{"text": "First , we construct a list of documents , labeled with the appropriate categories .For this example , we 've chosen the Movie Reviews Corpus , which categorizes each review as positive or negative .words(fileid ) ) , category ) ... for category in movie_reviews . categories ( ) ... for fileid in movie_reviews .", "label": "", "metadata": {}, "score": "57.646873"}
{"text": "We test our classifiers on an external dataset to see how well they generalize .+ Sentiment analysis , broadly speaking , is the set of techniques that allows detection of emotional content in text .This has a variety of applications : it is commonly used by trading algorithms to process news articles , as well as by corporations to better respond to consumer service needs .", "label": "", "metadata": {}, "score": "57.70315"}
{"text": "To put it in simplar terms quantitative research implies that one knows what to count and this knowledge is reached only through a long period of qualitative research carried on upon the basis of certain theoretical assumptions .For example , even simple numerical word counts presuppose a qualitative definition of the lexical items to be counted .", "label": "", "metadata": {}, "score": "57.771774"}
{"text": "The difference between a generative model and a conditional model is analogous to the difference between a topographical map and a picture of a skyline .Although the topographical map can be used to answer a wider variety of questions , it is significantly more difficult to generate an accurate topographical map than it is to generate an accurate skyline .", "label": "", "metadata": {}, "score": "57.85595"}
{"text": "For example , in multi - class classification , each instance may be assigned multiple labels ; in open - class classification , the set of labels is not defined in advance ; and in sequence classification , a list of inputs are jointly classified .", "label": "", "metadata": {}, "score": "57.876762"}
{"text": "The Maximum Entropy classifier uses a model that is very similar to the model employed by the naive Bayes classifier .But rather than using probabilities to set the model 's parameters , it uses search techniques to find a set of parameters that will maximize the performance of the classifier .", "label": "", "metadata": {}, "score": "58.002056"}
{"text": "Otherwise , your evaluation results may be unrealistically optimistic .Decision trees are automatically constructed tree - structured flowcharts that are used to assign labels to input values based on their features .Although they 're easy to interpret , they are not very good at handling cases where feature values interact in determining the proper label .", "label": "", "metadata": {}, "score": "58.014618"}
{"text": "We can systematically evaluate the classifier on a much larger quantity of unseen data : .Finally , we can examine the classifier to determine which features it found most effective for distinguishing the names ' genders : .This listing shows that the names in the training set that end in \" a \" are female 33 times more often than they are male , but names that end in \" k \" are male 32 times more often than they are female .", "label": "", "metadata": {}, "score": "58.060734"}
{"text": "Given a set of training data , the SVM classifier finds a hyperplane with the largest possible margin ; that is , it tries finds the hyperplane such that each training point is correctly classified and the hyperplane is as far as possible from the points closest to it .", "label": "", "metadata": {}, "score": "58.07972"}
{"text": "As was mentioned before , there are several methods for identifying the most informative feature for a decision stump .One popular alternative , called information gain , measures how much more organized the input values become when we divide them up using a given feature .", "label": "", "metadata": {}, "score": "58.172882"}
{"text": "Select only the instances where inst.attachment is N : .Using this sub - corpus , build a classifier that attempts to predict which preposition is used to connect a given pair of nouns .For example , given the pair of nouns \" team \" and \" researchers , \" the classifier should predict the preposition \" of \" .", "label": "", "metadata": {}, "score": "58.21138"}
{"text": "The prevalence of post - translational sites in MEME blocks of high variability , and the lack of conservation observed within post - translational modification sites indicate their importance in sustaining the virus against environmental factors , contribution to viral spread and pathogenicity , and ultimately increasing viral virulence .", "label": "", "metadata": {}, "score": "58.22445"}
{"text": "EXPERIMENTAL RESULTS Figure 5 : ( a ) Input Image .The performance of the proposed system is evaluated in this section .A set of 50 document images are employed for experiments on performance evaluation of text extraction .These test images are scanned from the newspapers and magazines .", "label": "", "metadata": {}, "score": "58.333237"}
{"text": "When performing classification tasks with three or more labels , it can be informative to subdivide the errors made by the model based on which types of mistake it made .A confusion matrix is a table where each cell [ i , j ] indicates how often label j was predicted when the correct label was i .", "label": "", "metadata": {}, "score": "58.41736"}
{"text": "Second , we need a method for sense - tagging context examples with the relevant sense given the context .Preparing such sense - tagged training corpora manually is costly and time - consuming , in particular because statistical methods require large amounts of training examples , and automated methods are therefore desirable .", "label": "", "metadata": {}, "score": "58.432793"}
{"text": "Henikoff S , Henikoff JG : Protein family classification based on searching a database of blocks .Genomics 1994/01/01 Edition 1994 , 19 ( 1 ) : 97 - 107 .View Article PubMed .Rice P , Longden I , Bleasby A : EMBOSS : the European Molecular Biology Open Software Suite .", "label": "", "metadata": {}, "score": "58.442596"}
{"text": "Additionally , certain blocks remain conserved for a few years of the cohort , but undergo amino acids substitutions towards the later years of the study .Notable examples include blocks 6 , 9 , and 13 ( data not shown ) .", "label": "", "metadata": {}, "score": "58.47398"}
{"text": "However , conditional models can not be used to answer the remaining questions 3 - 6 .However , this additional power comes at a price .Because the model is more powerful , it has more \" free parameters \" which need to be learned .", "label": "", "metadata": {}, "score": "58.51873"}
{"text": "In other words , f 2 is an exact copy of f 1 , and contains no new information .When the classifier is considering an input , it will include the contribution of both f 1 and f 2 when deciding which label to choose .", "label": "", "metadata": {}, "score": "58.5356"}
{"text": "However , Pang 's motivation for limiting the number of features was for improve testing performance , but our classifiers and processors were fast enough that this was not particularly noticeable .+ On average , limiting the number of features from 16165 to 2633 , as in the original Pang paper , caused accuracy to drop by 5.2\\% , 4.0\\% , and 2.8\\% for Naive Bayes , Maximum Entropy , and SVM , respectively .", "label": "", "metadata": {}, "score": "58.61812"}
{"text": "We will gloss over the mathematical and statistical underpinnings of these techniques , focusing instead on how and when to use them ( see the Further Readings section for more technical background ) .Before looking at these methods , we first need to appreciate the broad scope of this topic .", "label": "", "metadata": {}, "score": "58.62233"}
{"text": "The introduction is quite detailed and the authors have not been afraid to present some quite difficult examples ( complex NPs and the like ) .Having said this , there is not much coverage of analyses above the level of the sentence but this is reflective of the field itself .", "label": "", "metadata": {}, "score": "58.70037"}
{"text": "These posts have all been labeled with one of 15 dialogue act types , such as \" Statement , \" \" Emotion , \" \" ynQuestion \" , and \" Continuer .\" We can therefore use this data to build a classifier that can identify the dialogue act types for new instant messaging posts .", "label": "", "metadata": {}, "score": "58.766792"}
{"text": "In this case , we will have a harder time coming up with an appropriate distribution by hand ; however , we can verify that the following distribution looks appropriate : .Furthermore , the remaining probabilities appear to be \" evenly distributed .", "label": "", "metadata": {}, "score": "58.77726"}
{"text": "9 Further Reading .Many of the machine learning algorithms discussed in this chapter are numerically intensive , and as a result , they will run slowly when coded naively in Python .For information on increasing the efficiency of numerically intensive algorithms in Python , see ( Kiusalaas , 2005 ) .", "label": "", "metadata": {}, "score": "58.77884"}
{"text": "Antigenic sites A - D [ 11 ] were mapped to our consensus sequences and tabulated with overlapping MEME motif , entropy values and post - translational modifications sites .Site A average entropy is based on amino acid position 144 and 145 , while site B average entropy is based on amino acid position 188 and 189 .", "label": "", "metadata": {}, "score": "58.795704"}
{"text": "Following the branch that describes our input value , we arrive at a new decision node , with a new condition on the input value 's features .We continue following the branch selected by each node 's condition , until we arrive at a leaf node which provides a label for the input value .", "label": "", "metadata": {}, "score": "58.864574"}
{"text": "For classification tasks that have a small number of well - balanced labels and a diverse test set , a meaningful evaluation can be performed with as few as 100 evaluation instances .But if a classification task has a large number of labels , or includes very infrequent labels , then the size of the test set should be chosen to ensure that the least frequent label occurs at least 50 times .", "label": "", "metadata": {}, "score": "59.01219"}
{"text": "Here , we can see that the classifier begins by checking whether a word ends with a comma - if so , then it will receive the special tag \" , \" .Next , the classifier checks if the word ends in \" the \" , in which case it 's almost certainly a determiner .", "label": "", "metadata": {}, "score": "59.038147"}
{"text": "We will call xml_posts ( ) to get a data structure representing the XML annotation for each post : .Next , we 'll define a simple feature extractor that checks what words the post contains : .Finally , we construct the training and testing data by applying the feature extractor to each post ( using post.get ( ' class ' ) to get a post 's dialogue act type ) , and create a new classifier : . 2.3 Recognizing Textual Entailment .", "label": "", "metadata": {}, "score": "59.043816"}
{"text": "Modeling the linguistic data found in corpora can help us to understand linguistic patterns , and can be used to make predictions about new language data .Supervised classifiers use labeled training corpora to build models that predict the label of an input based on specific features of that input .", "label": "", "metadata": {}, "score": "59.185238"}
{"text": "This signal might be sent to another synapse , and might activate other neurons .The complexity of real neurons is highly abstracted when modeling artificial neurons .These basically consist of inputs ( like synapses ) , which are multiplied by weights ( strength of the respective signals ) , and then computed by a mathematical function which determines the activation of the neuron .", "label": "", "metadata": {}, "score": "59.265472"}
{"text": "Use the dev - test set to check your progress .Once you are satisfied with your classifier , check its final performance on the test set .How does the performance on the test set compare to the performance on the dev - test set ?", "label": "", "metadata": {}, "score": "59.274437"}
{"text": "Distribution of stems in a compound makes part of the lexical meaning of the compound word .Compare , e.g. , different lexical meanings of the words formed by the same stems bird and cage in bird - cage and cage - bird .", "label": "", "metadata": {}, "score": "59.51426"}
{"text": "One solution to this problem is to stop dividing nodes once the amount of training data becomes too small .Another solution is to grow a full decision tree , but then to prune decision nodes that do not improve performance on a dev - test .", "label": "", "metadata": {}, "score": "59.63939"}
{"text": "The pLSA model is originally developed for topic discovery in text analysis using \" bag - of - words \" document representation .The model is useful for image analysis by \" bag - of - visual words \" image representation .", "label": "", "metadata": {}, "score": "59.802032"}
{"text": "Interestingly , 4 out of the 6 amino acid positions at MEME block 7 participating in the co - mutating pairs , are potential PKC sites .Additionally , amino acid positions 151 participating in the co - occurring pairs of mutations at MEME block 7 is a potential glycosylation sites .", "label": "", "metadata": {}, "score": "59.802864"}
{"text": "Typically , the joint - features that are used to construct Maximum Entropy models exactly mirror those that are used by the naive Bayes model .In particular , a joint - feature is defined for each label , corresponding to w [ label ] , and for each combination of ( simple ) feature and label , corresponding to w [ f , label ] .", "label": "", "metadata": {}, "score": "60.060314"}
{"text": "Lexis accounts for the much wider possibilities of choice between , say , man , soldier , fireman and so on .Lexis is thus said to be a matter of choice between open sets of items while grammar is one between closed systems.1 The possibilities of choice between lexical items are not limitless however .", "label": "", "metadata": {}, "score": "60.072685"}
{"text": "Each time the error analysis procedure is repeated , we should select a different dev - test / training split , to ensure that the classifier does not start to reflect idiosyncrasies in the dev - test set .But once we 've used the dev - test set to help us develop the model , we can no longer trust that it will give us an accurate idea of how well the model would perform on new data .", "label": "", "metadata": {}, "score": "60.14141"}
{"text": "Notably , all of the blocks occur at least once within the HA1 domain ( 17 - 344 ) with the exception of blocks 8 and 14 , which only occurs in HA2 .Selected 14 MEME Blocks in the HA1 consensus sequence from 1968 - 1999 .", "label": "", "metadata": {}, "score": "60.159866"}
{"text": "The test set serves in our final evaluation of the system .For reasons discussed below , it is important that we employ a separate dev - test set for error analysis , rather than just using the test set .The division of the corpus data into different subsets is shown in 1.3 .", "label": "", "metadata": {}, "score": "60.24665"}
{"text": "When the method is used to select engines for document image recognition , it is preferable that the computation time is much faster with minimal error rate .REFERENCES [ 1 ] Mausumi Acharyya and Malay K.Kundu , \" Document Image Segmentation UsingWavelet Scale - Space Features \" , IEEE transactions on circuits and systems for video technology , Vol.12 , No.12 , December2002J. Clerk Maxwell , A Treatise on Electricity and Magnetism , 3rd ed . , vol .", "label": "", "metadata": {}, "score": "60.246765"}
{"text": "Some iterative optimization techniques are much faster than others .When training Maximum Entropy models , avoid the use of Generalized Iterative Scaling ( GIS ) or Improved Iterative Scaling ( IIS ) , which are both considerably slower than the Conjugate Gradient ( CG ) and the BFGS optimization methods .", "label": "", "metadata": {}, "score": "60.274757"}
{"text": "Here , tokens is a merged list of tokens from the individual sentences , and boundaries is a set containing the indexes of all sentence - boundary tokens .Next , we need to specify the features of the data that will be used in order to decide whether punctuation indicates a sentence - boundary : . isupper ( ) , ... ' prev - word ' :", "label": "", "metadata": {}, "score": "60.354183"}
{"text": "We also implemented and tested the effect of term frequency - inverse document frequency ( TF - IDF ) on classification results .+ For our experiments , we worked with movie reviews .The dataset contains 1000 positive reviews and 1000 negative reviews , each labeled with their true sentiment .", "label": "", "metadata": {}, "score": "60.361866"}
{"text": "( A ) Bar graph of amino acid substitutions within MEME blocks for each of the years .( C ) Behavior of the substitutions in MEME block 2 .Table 2 .Amino acid substitutions in the different isolates from 1969 - 1999 used to extrapolate the genetic distance in the different MEME blocks .", "label": "", "metadata": {}, "score": "60.42932"}
{"text": "In the absence of existing data sets for WSD for Norwegian , an automatically sense - tagged parallel corpus and a manually verified lexical sample of fifteen target words was developed for Norwegian as part of this thesis .The proposed automatic sense - tagging method is based on the Mirrors sense inventory and on the translational correspondents of each word occurrence .", "label": "", "metadata": {}, "score": "60.440994"}
{"text": "It 's common to start with a \" kitchen sink \" approach , including all the features that you can think of , and then checking to see which features actually are helpful .We take this approach for name gender features in 1.2 .", "label": "", "metadata": {}, "score": "60.465706"}
{"text": "But due to the lack of intersubjective \" gold standards \" for lexical semantics , it is not an easy task to evaluate the Mirrors method .The main research question of this thesis may thus be formulated as follows : are the translation - based senses and semantic relations in the Mirrors method linguistically motivated from a monolingual point of view ?", "label": "", "metadata": {}, "score": "60.505936"}
{"text": "It is only the distribution of otherwise completely identical lexical units that accounts for the difference in the meaning of water tap and tap water .Thus , as far as words are concerned the meaning by distribution may be defined as an abstraction on the syntagmatic level .", "label": "", "metadata": {}, "score": "60.547504"}
{"text": "The innovative aspect of applying the Mirrors method for WSD is two - fold : first , the Mirrors method is used to obtain sense - tagged data automatically ( using cross - lingual data ) , providing a SemCor - like corpus which allows us to exploit semantically analysed context features in a subsequent WSD classifier .", "label": "", "metadata": {}, "score": "60.613434"}
{"text": "3.2 Accuracy .The simplest metric that can be used to evaluate a classifier , accuracy , measures the percentage of inputs in the test set that the classifier correctly labeled .The function nltk.classify.accuracy ( ) will calculate the accuracy of a classifier model on a given test set : . format(nltk.classify.accuracy(classifier , test_set ) ) ) 0.75 .", "label": "", "metadata": {}, "score": "60.691277"}
{"text": "Based on this feature extractor , we can create a list of labeled featuresets by selecting all the punctuation tokens , and tagging whether they are boundary tokens or not : . ? ! ' ] Using these featuresets , we can train and evaluate a punctuation classifier : .", "label": "", "metadata": {}, "score": "60.70269"}
{"text": "Thus , e.g. , it is common practice to subdivide animate nouns into nouns denoting human beings and non - humans ( animals , birds , etc . ) .In other cases the classification of nouns into animate / inanimate may be insufficient for the semantic analysis , and it may be necessary to single out different lexico - semantic groups as , e.g. , in the case of the adjective blind .", "label": "", "metadata": {}, "score": "60.789642"}
{"text": "Once again , there are many distributions that are consistent with this new piece of information , such as : .But again , we will likely choose the distribution that makes the fewest unwarranted assumptions - in this case , distribution ( v ) .", "label": "", "metadata": {}, "score": "60.804466"}
{"text": "AN AUTOMATIC FILTER FOR NON - PARALLEL TEXTS Christopher Pike , I. Dan Melamed .GRAPH - BASED RANKING ALGORITHMS FOR SENTENCE EXTRACTION , APPLIED TO TEXT SUMMARIXATION Rada Mihalcea .EXPLOITING UNANNOTATED CORPORA FOR TAGGING AND CHUNKING Rie Kubota Ando .", "label": "", "metadata": {}, "score": "60.92956"}
{"text": "+ Using the most frequent unigrams is an extremely simple method of feature selection , and in this case , not a particularly robust one , since feature selection should look for words that identify a given class .Choosing frequent words does not discriminate between the two classes and will select common words like ' ' the ' ' and ' ' it ' ' , which likely are weak sentiment indicators .", "label": "", "metadata": {}, "score": "60.956123"}
{"text": "history.append(tag ) .history.append(tag ) .return zip(sentence , history ) .1.7 Other Methods for Sequence Classification .One shortcoming of this approach is that we commit to every decision that we make .For example , if we decide to label a word as a noun , but later find evidence that it should have been a verb , there 's no way to go back and fix our mistake .", "label": "", "metadata": {}, "score": "61.118294"}
{"text": "Note .Your Turn : Modify the gender_features ( ) function to provide the classifier with features encoding the length of the name , its first letter , and any other features that seem like they might be informative .Retrain the classifier with these new features , and test its accuracy .", "label": "", "metadata": {}, "score": "61.230225"}
{"text": "Classification is the task of choosing the correct class label for a given input .In basic classification tasks , each input is considered in isolation from all other inputs , and the set of labels is defined in advance .Some examples of classification tasks are : .", "label": "", "metadata": {}, "score": "61.291325"}
{"text": "In our example , we chose distribution ( i ) because its label probabilities are evenly distributed - in other words , because its entropy is high .In general , the Maximum Entropy principle states that , among the distributions that are consistent with what we know , we should choose the distribution whose entropy is highest .", "label": "", "metadata": {}, "score": "61.345875"}
{"text": "Mapping MEME blocks 1 , 2 , 3 and 7 onto the existing 3-D hemagglutinin structure revealed that these blocks lie on the surface of the protein ( Figure 6 ) , specifically on the characteristic 8 beta antiparallel strands of the protein .", "label": "", "metadata": {}, "score": "61.384773"}
{"text": "We additionally supported the ability to use the full movie dataset as a training set and using the yelp dataset as a test set .+ There are several ways to construct a probability model for a set of document n - grams .", "label": "", "metadata": {}, "score": "61.584026"}
{"text": "That movie was not very good . ' ' Diverging from Pang , we also added negation tags to bigrams .+ Negation tagging did not appear to have a significant effect on the data .For all the classifiers , the results from negation tagged data were almost the same as the results from the raw data .", "label": "", "metadata": {}, "score": "61.620354"}
{"text": "Additionally , one of the highly variable MEME block 7 N - glycosylation site is also involved in the co - mutation pairs ( see below ) .Myristylation sites were detected in MEME block 1 , 2 , 4 , 7 , and 9 .", "label": "", "metadata": {}, "score": "61.650276"}
{"text": "We can then examine individual error cases where the model predicted the wrong label , and try to determine what additional pieces of information would allow it to make the right decision ( or which existing pieces of information are tricking it into making the wrong decision ) .", "label": "", "metadata": {}, "score": "61.679726"}
{"text": "An analysis of other post - translational modification sites shows that PKC sites occur mainly within Blocks 2 , 3 and 7 while most of the ASN glycosylation sites appear within block 2 and 7 and most myristylation sites appear in MEME block 7 ( Figure 4 ) .", "label": "", "metadata": {}, "score": "61.83188"}
{"text": "IS CONECPTUAL COMBINATION INFLUENCED BY WORD ORDER ?Phil Maguire , Edward Loper .NLTK : THE NATURAL LANGUAGE TOOLKIT Steven Bird , Edward Loper .SUBSENTENTIAL TRANSLATION MEMORY FOR COMPUTER ASSISTED WRITING AND TRANSLATION Jian - Cheng Wu , Thomas C. Chuang , Wen - Chi Shei , Jason S. Chang .", "label": "", "metadata": {}, "score": "62.054844"}
{"text": "The mean and standard deviations for thr rows and columns of the matrix are . 3 ) Cluster Shade Cluster Shade represents the lack of symmetry in an image and is defined by ( 5 ) .4 ) Dissimilarity In the Contrast measure , weights increase exponentially ( 0 , 1 , 4 , 9 , etc . ) as one moves away from the diagonal .", "label": "", "metadata": {}, "score": "62.196243"}
{"text": "Interestingly , MEME blocks 2 and 7 had the highest incidence of potential post - translational modifications sites including phosphorylation sites , ASN glycosylation motifs and N - myristylation sites .Similarly , these 2 blocks overlap with previously identified antigenic sites and receptor binding sites .", "label": "", "metadata": {}, "score": "62.24079"}
{"text": "So what happens when we ignore the independence assumption , and use the naive Bayes classifier with features that are not independent ?One problem that arises is that the classifier can end up \" double - counting \" the effect of highly correlated features , pushing the classifier closer to a given label than is justified .", "label": "", "metadata": {}, "score": "62.37157"}
{"text": "The confusion matrix indicates that common errors include a substitution of NN for JJ ( for 1.6 % of words ) , and of NN for NNS ( for 1.5 % of words ) .Note that periods ( . ) indicate cells whose value is 0 , and that the diagonal entries - which correspond to correct classifications - are marked with angle brackets .", "label": "", "metadata": {}, "score": "62.378838"}
{"text": "It [ 9 ] presents a new method for extracting characters from various real - life complex document images .It applies a .( IJCSIS ) International Journal of Computer Science and Information Security , Vol . 8 , No . 5 , August 2010 .", "label": "", "metadata": {}, "score": "62.47261"}
{"text": "The score was simply the average of the two accuracies .+ Across the board , the classifiers has a harder time with the Yelp dataset as compared to the movie dataset , performing between 56.0\\% and 75.2\\% .The respective lowest and highest performing configurations scored at 67.0\\% and 84.0\\% on the movie dataset .", "label": "", "metadata": {}, "score": "62.479576"}
{"text": "The highest PKC entropy values were observed in MEME block 2 ( amino acid 205 ) and MEME block 7 ( amino acid 160 ) with 1.2 entropy values .MEME block 5 , 7 and 11 are unusual in that very few of their PKC sites have zero entropy .", "label": "", "metadata": {}, "score": "62.491196"}
{"text": "We expect these discussions to be informal and interactive ; and the author of the book discussed is cordially invited to join in .If you are interested in leading a book discussion , look for books announced on LINGUIST as \" available for discussion . \"", "label": "", "metadata": {}, "score": "62.614517"}
{"text": "5.5 The Cause of Double - Counting .The reason for the double - counting problem is that during training , feature contributions are computed separately ; but when using the classifier to choose labels for new inputs , those feature contributions are combined .", "label": "", "metadata": {}, "score": "62.619286"}
{"text": "return features .Example 1.2 ( code_gender_features_overfitting .py ) : Figure 1.2 : A Feature Extractor that Overfits Gender Features .The feature sets returned by this feature extractor contain a large number of specific features , leading to overfitting for the relatively small Names Corpus .", "label": "", "metadata": {}, "score": "62.654404"}
{"text": "The naive Bayes classification method , which we 'll discuss next , overcomes this limitation by allowing all features to act \" in parallel . \"5 Naive Bayes Classifiers .In naive Bayes classifiers , every feature gets a say in determining which label should be assigned to a given input value .", "label": "", "metadata": {}, "score": "62.662846"}
{"text": "And since the number of branches increases exponentially as we go down the tree , the amount of repetition can be very large .A related problem is that decision trees are not good at making use of features that are weak predictors of the correct label .", "label": "", "metadata": {}, "score": "62.781723"}
{"text": "Brief Bioinform 2002/09/17 Edition 2002 , 3 ( 3 ) : 265 - 274 .View Article PubMed .Bailey TL , Gribskov M : Combining evidence using p - values : application to sequence homology searches .Bioinformatics 1998/04/01 Edition 1998 , 14 ( 1 ) : 48 - 54 .", "label": "", "metadata": {}, "score": "62.98334"}
{"text": "Preprocessing .GLCM Feature Extratcion Feature Vector .GLCM Feature Extratcion Compare the Features with Feature Vector Extracted Text Region .( a ) .( b ) .Figure 3 : Our Proposed Text extraction method ( a ) Feature Extraction Phase ( b ) Classification Phase .", "label": "", "metadata": {}, "score": "63.01956"}
{"text": "Although Pang limited many of his tests to use only the 16165 most common ngrams , advanced processors have lifted this computational constraint , and so we additionally tested on all ngrams .We use a newer parameter estimation algorithm called Limited - Memory Variable Metric ( L - BFGS ) for maximum entropy classification .", "label": "", "metadata": {}, "score": "63.062042"}
{"text": "Once we have a decision tree , it is straightforward to use it to assign labels to new input values .What 's less straightforward is how we can build a decision tree that models a given training set .But before we look at the learning algorithm for building decision trees , we 'll consider a simpler task : picking the best \" decision stump \" for a corpus .", "label": "", "metadata": {}, "score": "63.076775"}
{"text": "We will learn more about the naive Bayes classifier later in the chapter .For now , let 's just test it out on some names that did not appear in its training data : .Observe that these character names from The Matrix are correctly classified .", "label": "", "metadata": {}, "score": "63.153694"}
{"text": "P04 - 1081 [ bib ] : Dekai Wu ; Weifeng Su ; Marine Carpuat A Kernel PCA Method for Superior Word Sense Disambiguation .P04 - 1082 [ bib ] : Richard Campbell Using Linguistic Principles to Recover Empty Categories .", "label": "", "metadata": {}, "score": "63.1558"}
{"text": "In particular , the identity of the previous word is included as a feature .It is clear that exploiting contextual features improves the performance of our part - of - speech tagger .For example , the classifier learns that a word is likely to be a noun if it comes immediately after the word \" large \" or the word \" gubernatorial \" .", "label": "", "metadata": {}, "score": "63.426247"}
{"text": "The fourteen MEME blocks spanning the consensus sequence alignment are presented , with the start and end positions and width of each block .Genetic distance and entropy analysis of MEME blocks reveals variable and conserved motifs .Amino acid substitutions over the 1968 - 1999 data set were extracted from the multiple sequence alignment using MEGA 4.0 [ 9 ] .", "label": "", "metadata": {}, "score": "63.623123"}
{"text": "Relationship between the high frequency mutation MEME Blocks and previously reported antigenic and receptor - binding sites .MEME blocks 1 , 2 , 3 and 7 were found to overlap with 4 previously identified antigenic sites ( Table 4 ) [ 12 ] .", "label": "", "metadata": {}, "score": "63.69467"}
{"text": "Mohamed Benjelil , Slim Kanoun , R\u00e9my Mullot and Adel M. Alimi , \" Steerable pyramid based complex documents images segmentation \" , IEEE 10th International Conference on Document Analysis and Recognition on 2009 .SunilKumar , RajatGupta , NitinKhanna and SantanuChaudhury , \" Text Extraction and Document Image Segmentation Using Matched Wavelets and MRF Model \" , IEEE transactions on image processing , vol.16,no.8 , August2007 .", "label": "", "metadata": {}, "score": "63.69519"}
{"text": "It contains one leaf for each possible feature value , specifying the class label that should be assigned to inputs whose features have that value .In order to build a decision stump , we must first decide which feature should be used .", "label": "", "metadata": {}, "score": "63.727715"}
{"text": "[ 11 ] .[ 12 ] .[ 13 ] .L. Pratap Reddy , A.S.C.S. Sastry and A.V. Srinivasa Rao , \" Canonical Syllable Segmentation of Telugu Document Images \" , TENCON 2008IEEE Region 10 Conference .Yen - Lin Chen and Bing - Fei Wu , \" Text Extraction from Complex Document Images Using the Multi - plane Segmentation Technique \" , IEEE International Conference on Systems , Man , and Cybernetics October 8 - 11 , 2006 .", "label": "", "metadata": {}, "score": "63.734245"}
{"text": "We speculate that mutation in these motifs results in the emergence of viral strains that are highly pathogenic and has the intrinsic character to overcome that host defense mechanisms .Results .14 MEME Blocks identified from HA1 consensus sequences ; representatives of strains isolated from 1968 to 1999 .", "label": "", "metadata": {}, "score": "63.78411"}
{"text": "Shows the Pictorial representations of feature set for sample text regions .In table 2 , we present the feature vector for non text samples of 20x20 windows and Figure 7 Shows the Pictorial representations of feature set for sample non text regions .", "label": "", "metadata": {}, "score": "63.932053"}
{"text": "Bailey TL , Williams N , Misleh C , Li WW : MEME : discovering and analyzing DNA and protein sequence motifs .Nucleic Acids Res 2006/07/18 Edition 2006 , 34 ( Web Server issue ) : W369 - 73 .View Article PubMed .", "label": "", "metadata": {}, "score": "63.999725"}
{"text": "Relationship between co - mutating amino acid pairs and MEME blocks .107 pairs based on 24 analyzed positions were generated .Of these , 77 pairs contained at least one amino acid within MEME blocks 1 , 2 , 3 and 7 .", "label": "", "metadata": {}, "score": "64.06855"}
{"text": "We call these values w[label ] and w[f , label ] the parameters or weights for the model .Using the naive Bayes algorithm , we set each of these parameters independently : .However , in the next section , we 'll look at a classifier that considers the possible interactions between these parameters when choosing their values .", "label": "", "metadata": {}, "score": "64.16121"}
{"text": "The corpus data is divided into two sets : the development set , and the test set .The development set is often further subdivided into a training set and a dev - test set .Having divided the corpus into appropriate datasets , we train a model using the training set , and then run it on the dev - test set .", "label": "", "metadata": {}, "score": "64.17592"}
{"text": "Findings were compared to other motif finding applications including PROSITE under the Expasy Server , PSite , and the ELM database .MEME blocks were also submitted into MAST to determine their functional significance [ 20 ] .Additionally , the 1968 consensus sequence was queried against the BLOCKS [ 21 ] and PRINTS [ 21 ] database to check for the existence of known protein motifs .", "label": "", "metadata": {}, "score": "64.48801"}
{"text": "Bioinformatics and computational approaches towards molecular understanding of HA have largely focused on the determination of mutation levels and evolution of the HA gene , and identification and prediction of antigenic variants of H3N2 by locating potential immunodominant positions on the HA protein .", "label": "", "metadata": {}, "score": "64.74646"}
{"text": "MEME block 7 contained more than 35 % of co - mutating pairs ( Table 6 ) .This block had the highest post - translational modification frequency ( Figure 4 ) , with the highest number of N - myristylation sites ( Figure 5b ) .", "label": "", "metadata": {}, "score": "64.95366"}
{"text": "[5 ] .[ 6 ] .[ 7 ] .( IJCSIS ) International Journal of Computer Science and Information Security , Vol . 8 , No . 5 , August 2010 .[ 8 ] .[ 9 ] .", "label": "", "metadata": {}, "score": "65.12273"}
{"text": "In contrast , explanatory models attempt to capture properties and relationships that cause the linguistic patterns .For example , we might introduce the abstract concept of \" polar verb \" , as one that has an extreme meaning , and categorize some verb like adore and detest as polar .", "label": "", "metadata": {}, "score": "65.1428"}
{"text": "TRANSTYPE2 - AN INNOVATIVE COMPUTER - ASSISTED TRANSLATION SYSTEM Jos\u00e9 Esteban , Jos\u00e9 Lorenzo , Antonio S. Valderr\u00e1banos and Guy Lapalme .IMPROVING DOMAIN - SPECIFIC WORD ALIGNMENT FOR COMPUTER ASSISTED TRANSLATION WU Hua , WANG Haifeng .INTERACTIVE GRAMMAR DEVELOPMENT WITH WCDG Kilian A. Foth , Michael Daum , Wolfgang Menzel .", "label": "", "metadata": {}, "score": "65.23755"}
{"text": "+ Pang applied the bag - of - words method to positive and negative sentiment classification , but the same method can be extended to various other domains , including topic classification .We additionally chose to work with a set of 5000 Yelp reviews , 1000 for each of their five \" star \" rating .", "label": "", "metadata": {}, "score": "65.24475"}
{"text": "Information from the table of contents ( TOC ) pages can be extracted to use in document database for effective retrieval of the required pages .Fully automatic identification and segmentation of table of contents ( TOC ) page from scanned document is discussed in [ 2].", "label": "", "metadata": {}, "score": "65.25462"}
{"text": "AHE computes the histogram of a local window centered at a given pixel to determine the mapping for that pixel , which provides a local contrast enhancement .B. Feature Extraction Feature extraction is an essential pre - processing step to pattern recognition and machine learning problems .", "label": "", "metadata": {}, "score": "65.28909"}
{"text": "CKII sites were detected in MEME blocks 1 , 2 , 5 , 7 , 9 and 12 ; MEME blocks 1 , 5 and 9 CKII sites have zero entropy .Unlike other MEME blocks , nearly all of CKII sites at MEME block 2 and 7 have non - zero entropy .", "label": "", "metadata": {}, "score": "65.37906"}
{"text": "+ However , when restricting from all features down to 16165 , the results were a wash .Naive Bayes did vaguely worse , Maximum Entropy remained unchanged , and SVMs did vaguely better .These results suggest that uncommon features do not carry much sentiment information .", "label": "", "metadata": {}, "score": "65.551285"}
{"text": "We use 30 iterations of the Limited - Memory Variable Metric ( L - BFGS ) parameter estimation .Pang used the Improved Iterative Scaling ( IIS ) method , but L - BFGS , a method that was invented after their paper was published , was found to out - perform both IIS and generalized iterative scaling ( GIS ) , yet another parameter estimation method .", "label": "", "metadata": {}, "score": "65.56079"}
{"text": "The training set and test set are taken from the same genre , and so we can not be confident that evaluation results would generalize to other genres .What 's worse , because of the call to random.shuffle ( ) , the test set contains sentences that are taken from the same documents that were used for training .", "label": "", "metadata": {}, "score": "65.798706"}
{"text": "We begin by selecting the overall best decision stump for the classification task .We then check the accuracy of each of the leaves on the training set .Leaves that do not achieve sufficient accuracy are then replaced by new decision stumps , trained on the subset of the training corpus that is selected by the path to the leaf .", "label": "", "metadata": {}, "score": "65.870834"}
{"text": "Once we 've picked a feature , we can build the decision stump by assigning a label to each leaf based on the most frequent label for the selected examples in the training set ( i.e. , the examples where the selected feature has that value ) .", "label": "", "metadata": {}, "score": "65.90029"}
{"text": "Whole - genome alignments , statistical analysis with construction of evolutionary trees were used to identify locations of mutations within H3N2 , predict their yearly frequency , and determine modes of antigenic drift and positive selection [ 2 ] .Some studies have concluded that H3 hemagglutinin gene exhibits positive selection in key regions of the HA molecule such as the receptor - binding site and antibody - binding sites [ 4 ] , which result in new antigenic and resistant strains .", "label": "", "metadata": {}, "score": "65.97568"}
{"text": "Then contact Andrew Carnie at carnie linguistlist.org .Directory .Aimed at graduate students and researchers , it should also be seen as a valuable teaching aid for courses in computational linguistics .Deriving mathematical formulae from basic principles with reference to specific language processing tasks prevents the descriptions from becoming too dry .", "label": "", "metadata": {}, "score": "66.21295"}
{"text": "Mitchell , T. ( 1997 ) Machine Learning , McGraw Hill .Richard Evans is a research assistant with the Computational .Linguistics Research Group at the University of Wolverhampton in the .UK .His current research interest is anaphor resolution and the . application of corpus - based machine learning and optimisation .", "label": "", "metadata": {}, "score": "66.53135"}
{"text": "For bigrams , it harmed performance by around 5\\% in most cases , and for unigrams , it was not helpful .If reviews end up not actually following the model specified or if the model has no bearing on where the relevant data is , position tagging will be harmful because it increases the dimensionality of the input without increasing the information content .", "label": "", "metadata": {}, "score": "66.536545"}
{"text": "The relatively high co - mutating pairs in this block remain unexplained .Interestingly , the minimal overlap of this conserved site with previously reported antigenic sites [ 12 ] is also in agreement with the conserved nature of this site .", "label": "", "metadata": {}, "score": "66.7065"}
{"text": "Of considerable significance in this respect is the fact that high frequency value of individual lexical items does not forecast high frequency of the word - group formed by these items .Thus , e.g. , the adjective able and the noun man are both included in the list of 2,000 most frequent words , the word - group an able man , however , is very rarely used .", "label": "", "metadata": {}, "score": "66.73255"}
{"text": "The conserved MEME blocks 1 and 4 posses PKC sites with zero entropy .The majority of MEME blocks 2 and 3 PKC sites have zero entropy .One amino acid position at MEME blocks 2 and 7 posses the highest entropy of all of PKC 's sites .", "label": "", "metadata": {}, "score": "66.80171"}
{"text": "Distributional pattern as such seems to possess a component of meaning not to be found in individual words making up the word - group or the sentence .Thus , the meaning ' make somebody do smth by means of something ' can not be traced back to the lexical meanings of the individual words in ' to coax somebody into accepting the suggestion ' .", "label": "", "metadata": {}, "score": "66.93529"}
{"text": "MEME blocks 4 and 5 have zero entropy at all of their ASN sites .MEME block 2 , 6 and 9 have nonzero entropy at the majority of their ASN sites .MEME block 1 and 7 are the only blocks with the majority of their glycosylation sites possessing nonzero entropy .", "label": "", "metadata": {}, "score": "66.95732"}
{"text": "COMPILING BOSSTEXTER RULES INTO A FINITE - STATE TRANSDUCER Srinivas Bangalore .COMBINING LEXICAL , SYNTACTIC , AND SEMANTIC FEATURES WITH MAXIMUM ENTROPY MODELS FOR INFORMATION EXTRACTION Nanda Kambhatla .PART - OF - SPEECH TAGGING CONSIDERING SURFACE FORM FOR AN AGGLUTINATIVE LANGUAGE Do - Gil Lee , Hae - Chang Rim .", "label": "", "metadata": {}, "score": "66.992966"}
{"text": "RS contributed in the initial idea , design and guiding of the project and contributed extensively to the analysis and interpretation of data into the formatted manuscript with extensive revision of the analysis and re - writing of the manuscript to elaborate on its scientific content .", "label": "", "metadata": {}, "score": "67.02927"}
{"text": "Consensus sequences were aligned using the ClustalW multiple alignment tool .Using both the 1968 sequence as a base year , and performing pairwise alignments for each two consecutive years using the LALIGN program of the EMBOSS package [ 22 ] , the percent change and the number of amino acid substitutions were calculated using the Info align tool .", "label": "", "metadata": {}, "score": "67.03653"}
{"text": "Our study of several potential myristylation sites in MEME block 7 and the exhibition of high variability in these sites imply a mechanism by the virus to escape from neutralizing antibodies .Unsurprisingly , minimal post - translational sites were observed in this un - variable block 1 .", "label": "", "metadata": {}, "score": "67.05275"}
{"text": "The authors declare that they have no competing interests .Authors ' contributions .DMAG is a major contributor in the material collection , data analysis and implementation , and writing of the manuscript .MME helped in guiding the study design , implementation and analysis of the data and revised the manuscript .", "label": "", "metadata": {}, "score": "67.05572"}
{"text": "Interestingly , 4 of these receptor binding sites overlap the variable MEME block 7 and the intermediately variable MEME block 2 ( Table 5 ) .The receptor binding sites described by Skehel and Wiley ( 2000 ) and their overlapping MEME motifs 1 , 2 , and 7 are presented in Table 5 .", "label": "", "metadata": {}, "score": "67.13345"}
{"text": "We also saw strong positive trends across all test configurations , classifying reviews with more stars more positively .Translation - based Word Sense Disambiguation .View/ Open .Date .Author .Share .Metadata .Abstract .This thesis investigates the use of the translation - based Mirrors method ( Dyvik , 2005 , inter alia ) for Word Sense Disambiguation ( WSD ) for Norwegian .", "label": "", "metadata": {}, "score": "67.335434"}
{"text": "The authors present a sample of the Penn treebank .Here we note that the analysis consists of many ' flat ' infrequent trees that do not contain X - Bar nodes , only X and XP ones .Many current systems are based on this treebank and the astute reader will be somewhat concerned about the quality of the analyses returned by such systems .", "label": "", "metadata": {}, "score": "67.34906"}
{"text": "14 MEME blocks were generated and comparative analysis of the MEME blocks identified blocks 1 , 2 , 3 and 7 to correlate with several biological functions .Analysis of the different Hemagglutinin sequences elucidated that the single block 7 has the highest frequency of amino acid substitution and the highest number of co - mutating pairs .", "label": "", "metadata": {}, "score": "67.38062"}
{"text": "It was discovered that combinations of such units are usually structured into hierarchically arranged sets of binary constructions .For example in the word - group a black dress in severe style we do not relate a to black , black to dress , dress to in , etc .", "label": "", "metadata": {}, "score": "67.44292"}
{"text": "Abbreviations .ASN : . asparagines glycosylation .PKC : .Protein Kinase C phosphorylation site .Myristyl : . myristylation .PPSearch : .Protein Motifs Search .Declarations .Acknowledgements .This study was supported by a grant from the Yousef - Jameel Science and Technology Research center funding to RS .", "label": "", "metadata": {}, "score": "67.47647"}
{"text": "Robert M. Haralick,''Statistical and structural approaches to texture , ' ' Proc .IEEE , vol .67 , no .5 , pp .786 - 804 , 1979 .AUTHORS PROFILE S.Audithan is currently a reseaech scholar at the Department of Computer Science and Engineering , Annamalai University , Annamalai Nagar , Tamilnadu , India .", "label": "", "metadata": {}, "score": "67.491745"}
{"text": "Given below is the mathematical explanation on the computation of each textures .: ( where each element in GLCIL is a pair of pixel index and it 's frequency , $ g(i , j ) $ is the frequency value of the pair having index is i , j ) . \"", "label": "", "metadata": {}, "score": "67.51279"}
{"text": "A classical approach in the segmentation of Canonical Syllables of Telugu document images is proposed in [ 8].The model consists of zone separation and component extraction phases as independent parts .The relation between zones and components is established in the segmentation process of canonical syllable .", "label": "", "metadata": {}, "score": "67.60694"}
{"text": "2.1 Sentence Segmentation .Sentence segmentation can be viewed as a classification task for punctuation : whenever we encounter a symbol that could possibly end a sentence , such as a period or a question mark , we have to decide whether it terminates the preceding sentence .", "label": "", "metadata": {}, "score": "67.6084"}
{"text": "Deciding what the topic of a news article is , from a fixed list of topic areas such as \" sports , \" \" technology , \" and \" politics . \"Deciding whether a given occurrence of the word bank is used to refer to a river bank , a financial institution , the act of tilting to the side , or the act of depositing something in a financial institution .", "label": "", "metadata": {}, "score": "67.81103"}
{"text": "+ We set out to replicate Pang 's work from 2002 on using classical knowledge - free supervised machine learning techniques to perform sentiment classification .They used the machine learning methods ( Naive Bayes , maximum entropy classification , and support vector machines ) , methods commonly used for topic classification , to explore the difference between and sentiment classification in documents .", "label": "", "metadata": {}, "score": "67.85442"}
{"text": "Additionally , a balance of glycosylation is needed to mediate the interaction of HA and NA molecules for receptor binding activity and viral release [ 15 ] .The importance of other post - translational sites that we have observed , such as myristylation sites , is seconded by several studies on the biological role of oligosaccharides and lipid modifications of proteins involved in protein translocation .", "label": "", "metadata": {}, "score": "68.01927"}
{"text": "Some blocks only undergo amino acids substitutions in one or two years of the cohort , as is the case with motifs 8 and 12 ( data not shown ) .MEME block 5 undergo amino acids substitutions from 1968 to 1984 , then remain conserved after this period ( data not shown ) .", "label": "", "metadata": {}, "score": "68.098495"}
{"text": "He is life member of the Computer Society of India , Indian Society for Technical Education , Institute of Engineers and Indian Science Congress Assciation .LINGUIST List 10.1349 .Mon Sep 13 1999 .Review : Manning & Schuetze : Statistical NLP .", "label": "", "metadata": {}, "score": "68.112076"}
{"text": "P04 - 1084 [ bib ] : I. Dan Melamed ; Giorgio Satta ; Benjamin Wellington Generalized Multitext Grammars .P04 - 1085 [ bib ] : Michel Galley ; Kathleen McKeown ; Julia Hirschberg ; Elizabeth Shriberg Identifying Agreement and Disagreement in Conversational Speech : Use of Bayesian Networks to Model Pragmatic Dependencies .", "label": "", "metadata": {}, "score": "68.14982"}
{"text": "Finally , the method of tagging all words up to the next punctuation mark is suspect .Only a few words after the not are actually negated , and these often occur after a comma or other punctuation mark .+ Reviews are split into a beginning , middle , and end , so to see if one section carries more sentiment than another , we split the reviews into a first quarter , a middle half , and a last quarter and tagged the words in each section .", "label": "", "metadata": {}, "score": "68.276215"}
{"text": "But contextual features often provide powerful clues about the correct tag - for example , when tagging the word \" fly , \" knowing that the previous word is \" a \" will allow us to determine that it is functioning as a noun , not a verb .", "label": "", "metadata": {}, "score": "68.33334"}
{"text": "ICASSP 2008 ] .Leen - Kiat Soh and Costas Tsatsoulis , \" Texture Analysis of SAR Sea Ice Imagery Using Gray Level Co - Occurrence Matrices \" , IEEE transactions on geosciences and remote sensing , vol.37 , no.2 , march1999 .", "label": "", "metadata": {}, "score": "68.382904"}
{"text": "Tamura K , Dudley J , Nei M , Kumar S : MEGA4 : Molecular Evolutionary Genetics Analysis ( MEGA ) software version 4.0 .Mol Biol Evol 2007/05/10 Edition 2007 , 24 ( 8) : 1596 - 1599 .View Article PubMed .", "label": "", "metadata": {}, "score": "68.402466"}
{"text": "He worked as a network engineer in RBCOMTEC at Hydrabad from 2000 to 2003 .Ha has presented and published more than 5 papers in conferences and journals .His research interests include Image processing , Network Security , and Artificial Intelligence .", "label": "", "metadata": {}, "score": "68.607376"}
{"text": "Clearly , this assumption does not hold .+ We found a huge difference between results of Naive Bayes and Maximum Entropy for positive testing accuracy and negative testing accuracy .Maximum Entropy , which makes no unfounded assumptions about the data , gave very similar results for positive tests and negative tests with a 0.2\\% difference on average .", "label": "", "metadata": {}, "score": "68.66307"}
{"text": "Any point on or inside the margin is referred to as a support vector , and the hyperplane , given by .+ For this paper , we use the PyML implementation of SVMs , which uses the liblinear optimizer to actually find the separating hyperplane .", "label": "", "metadata": {}, "score": "68.93317"}
{"text": "Our study identifies motifs in the Hemagglutinin protein with different amino acid substitution frequencies over a 31 years period , and derives relevant functional characteristics by correlation of these motifs with potential post - translational modifications sites , antigenic and receptor binding sites .", "label": "", "metadata": {}, "score": "68.97177"}
{"text": "Each combination of labels and features that receives its own parameter is called a joint - feature .Note that joint - features are properties of labeled values , whereas ( simple ) features are properties of unlabeled values .Note .", "label": "", "metadata": {}, "score": "69.04739"}
{"text": "This process is illustrated in 5.2 and 5.3 .Figure 5.2 : Calculating label likelihoods with naive Bayes .Naive Bayes begins by calculating the prior probability of each label , based on how frequently each label occurs in the training data .", "label": "", "metadata": {}, "score": "69.195404"}
{"text": "Back propagation can also be considered as a generalization of the delta rule for nonlinear activation functions and multi layer networks .A feed forward network has a layered structure .Each layer consists of units which receive their input from units from a layer directly below and send their output to units in a layer directly above the unit .", "label": "", "metadata": {}, "score": "69.28344"}
{"text": "10 Exercises .Find out what type and quantity of annotated data is required for developing such systems .Why do you think a large amount of data is required ?Begin by splitting the Names Corpus into three subsets : 500 words for the test set , 500 words for the dev - test set , and the remaining 6900 words for the training set .", "label": "", "metadata": {}, "score": "69.318275"}
{"text": "In this study we have utilized 17 HA consensus sequences generated from 32 Hong Kong H3N2 isolates spanning the years from 1968 and 1999 .We identified 14 MEME blocks , with the clustering of blocks 1 , 2 , 3 and 7 between positions 85 - 250 and 430 - 550 ( Figure 6 ) .", "label": "", "metadata": {}, "score": "69.33147"}
{"text": "+ Maximum Entropy is a general - purpose machine learning technique that provides the least biased estimate possible based on the given information .In other words , \" it is maximally noncommittal with regards to missing information \" [ src].", "label": "", "metadata": {}, "score": "69.3994"}
{"text": "Many of these sites are contained in MEME blocks 1 , 2 , 3 , and 7 , with more than 1/5 of the sites in block 2 alone .43 % of antigenic sites in blocks 2 and 80 % of antigenic sites in MEME block 3 are also part of a hot spot cluster ( 200 - 240 ) .", "label": "", "metadata": {}, "score": "69.73778"}
{"text": "Contrary to MEME 2 block , MEME block 7 revealed a peak frequency of substitution in 1980 , corresponding to one of the years with a high mutation rate and therefore this block largely follows the occurrence pattern of substitutions within the entire protein ( Figure 2b ) .", "label": "", "metadata": {}, "score": "69.746155"}
{"text": "Because of this , the term is used in non technical speech to mean irremediable chaos or disorder .Also , as with ASM , the equation used to calculate physical entropy is very similar to the one used for the texture measure .", "label": "", "metadata": {}, "score": "69.82324"}
{"text": "RESOURCE ANALYSIS FOR QUESTION ANSWERING Lucian Vlad Lita , Warren A. Hunt , Eric Nyberg .TANGO : BILINGUAL COLLOCATIONAL CONCORDANCER Jia - Yan Jian , Yu - Chia Chang , Jason S. Chang .A NEW FEATURE SELECTION SCORE FOR MULTINOMIAL NAIVE BAYES TEXT CLASSIFICATION BASED ON KL - DIVERGENCE Karl - Michael Schneider .", "label": "", "metadata": {}, "score": "70.0838"}
{"text": "During training , we use the annotated tags to provide the appropriate history to the feature extractor , but when tagging new sentences , we generate the history list based on the output of the tagger itself . def pos_features ( sentence , i , history ) : . return features class ConsecutivePosTagger ( nltk .", "label": "", "metadata": {}, "score": "70.14397"}
{"text": "These receptors binding sites mainly overlap MEME blocks 2 and 7 .Based on overlapping MEME blocks with hot spots , frequency of amino - acid substitutions , potential post - translational modification sites , receptor - binding sites and antigenic sites we mapped MEME blocks 1 , 2 , 3 and 7 onto the 3D hemagglutinin structure determined by Fleury and co - workers [ 13 ] .", "label": "", "metadata": {}, "score": "70.18213"}
{"text": "The Ni inputs are fed into the first layer of Nh , i hidden units .The input units are merely \" fan out \" units no processing takes place in these units .The activation of a hidden unit is a function Fi of the weighted inputs plus a bias .", "label": "", "metadata": {}, "score": "70.21068"}
{"text": "For example , consider a classifier that determines the correct word sense for each occurrence of the word bank .If we evaluate this classifier on financial newswire text , then we may find that the financial - institution sense appears 19 times out of 20 .", "label": "", "metadata": {}, "score": "70.48691"}
{"text": "ANNs combine artificial neurons in order to process information .A single layer network has severe restrictions the class of tasks that can be accomplished is very limited .The limitation is overcome by the two layer feed forward network .The central idea behind this solution is that the errors for the units of the hidden layer are determined by back propagating the errors of the units of the output layer .", "label": "", "metadata": {}, "score": "70.581375"}
{"text": "Precision , which indicates how many of the items that we identified were relevant , is TP/(TP+FP ) .Recall , which indicates how many of the relevant items that we identified , is TP/(TP+FN ) .The F - Measure ( or F - Score ) , which combines the precision and recall to give a single score , is defined to be the harmonic mean of the precision and recall : ( 2 \u00d7 Precision \u00d7 Recall ) / ( Precision + Recall ) .", "label": "", "metadata": {}, "score": "70.71475"}
{"text": "6 ) Entropy Entropy is a notoriously difficult term to understand ; the concept comes from thermodynamics .It refers to the quantity of energy that is permanently lost to heat ( \" chaos \" ) every time a reaction or a physical transformation occurs .", "label": "", "metadata": {}, "score": "70.78101"}
{"text": "Seventeen consensus sequences were generated , with an average length of 566 amino acids each .Motif and protein modification site prediction in the HA1 protein .Seventeen consensus sequences were submitted into the MEME program [ 18 ] to search for motifs that are common to all sequences .", "label": "", "metadata": {}, "score": "71.14519"}
{"text": "Additionally , 9 of the 14 N - myristylation sites are in MEME blocks 1 , 2 and 7 .Four sites overlap with MEME block 7 , three sites with MEME block1 , and two sites with MEME block 2 .", "label": "", "metadata": {}, "score": "71.602066"}
{"text": "They established a unique CUP for each subtype , and observed a possible divergence within human H3N2 isolates based on their synonymous CUPs .A study published earlier this year [ 8 ] has focused specifically on the H3N2 subtype , using nucleotide co - occurrence networks of human H3N2 strains to predict H3N2 evolution .", "label": "", "metadata": {}, "score": "71.634415"}
{"text": "MEME block 7 has the highest number of post - translational modification sites , followed by MEME block 2 , 1 and 3 respectively .High frequency of post - translational modification site was recorded when a frequency of 2 or above is observed .", "label": "", "metadata": {}, "score": "71.75991"}
{"text": "For example , habitually the word preceding ago denotes a certain period of time ( an hour , a month , a century , etc . ago ) and the whole word - group denotes a certain temporal unit .In this particular distributional pattern any word is bound to acquire an additional lexical meaning of a certain period of time , e.g. a grief ago ( E. Cummings ) , three cigarettes ago ( A. Christie ) , etc .", "label": "", "metadata": {}, "score": "71.78766"}
{"text": "These blocks have different amino acid substitution frequency and encompass different hot spot clusters , post - translational modification sites , antigenic sites and receptor - binding sites .Of these highlighted blocks , MEME 2 had multiple interesting characteristics .This block ( 29 amino acids ) is repeated three times at positions 14 - 42 , 179 - 207 and 478 - 506 of the HA protein , and was characterized as an intermediate mutation frequency block ( Figure 1 ) .", "label": "", "metadata": {}, "score": "71.90637"}
{"text": "5.4 The Naivete of Independence .The reason that naive Bayes classifiers are called \" naive \" is that it 's unreasonable to assume that all features are independent of one another ( given the label ) .In particular , almost all real - world problems contain features with varying degrees of dependence on one another .", "label": "", "metadata": {}, "score": "72.00454"}
{"text": "The highest aminoacid substitution ( 29 aa substitutions over the entire sequence ) was in Years 1980 .The genetic distance in each MEME block is calculated showing that MEME blocks 1 and 8 are conserved ( bold ) , MEME blocks 7 , 11 and 13 are highly variable and the other MEME blocks show intermediate variability .", "label": "", "metadata": {}, "score": "72.04898"}
{"text": "def rte_features ( rtepair ) : . return features .Example 2.2 ( code_rte_features . py ) : Figure 2.2 : \" Recognizing Text Entailment \" Feature Extractor .The RTEFeatureExtractor class builds a bag of words for both the text and the hypothesis after throwing away some stopwords , then calculates overlap and difference .", "label": "", "metadata": {}, "score": "72.21254"}
{"text": "6.3 Generative vs Conditional Classifiers .An important difference between the naive Bayes classifier and the Maximum Entropy classifier concerns the type of questions they can be used to answer .The naive Bayes classifier is an example of a generative classifier , which builds a model that predicts P(input , label ) , the joint probability of a ( input , label ) pair .", "label": "", "metadata": {}, "score": "72.35951"}
{"text": "Discussion .As opposed to previous molecular and computational approaches to understanding the dynamic nature of the human H3N2 influenza strain , our approach is one of few that attempts to understand and determine the functional importance of variable and conserved motifs in the hemagglutinin protein over time .", "label": "", "metadata": {}, "score": "72.50238"}
{"text": "+ While the Naive Bayes classifier seems very simple , it is observed to have high predictive power ; in our tests , it performed competitively with the more sophisticated classifiers we used .The Bayes classifier can also be implemented very efficiently .", "label": "", "metadata": {}, "score": "72.8098"}
{"text": "+ The ineffectiveness of negation tagging probably comes from a few sources .First , it increases the number of uncommon features , which , as discussed previously , harms effectiveness and cancels out the increase in semantic awareness .Second , the presence of a \" not \" does not always indicate negation .", "label": "", "metadata": {}, "score": "72.90172"}
{"text": "A decision tree is a simple flowchart that selects labels for input values .This flowchart consists of decision nodes , which check feature values , and leaf nodes , which assign labels .To choose the label for an input value , we begin at the flowchart 's initial decision node , known as its root node .", "label": "", "metadata": {}, "score": "73.05425"}
{"text": "If i and j differ by 1 , there is a ( i ) small contrast , and the weight is 1 .If i and j differ by 2 , contrast is increasing and the weight is 4 .The weights continue to increase exponentially as ( i - j ) increases .", "label": "", "metadata": {}, "score": "73.142944"}
{"text": "import math def entropy ( labels ) : .Once we have calculated the entropy of the original set of input values ' labels , we can determine how much more organized the labels become once we apply the decision stump .", "label": "", "metadata": {}, "score": "73.4805"}
{"text": "Blind followed by a noun denoting inanimate objects , or abstract concepts may have different meanings depending on the lexico - semantic group the noun belongs to .In the analysis of word - formation pattern the investigation on the level of lexico - semantic groups is commonly used to find out the word - meaning , the part of speech , the lexical restrictions of the stems , etc .", "label": "", "metadata": {}, "score": "73.61434"}
{"text": "+ Given a large ensemble of classifiers , an easy way to combine them is with a simple majority voting scheme .This tends to eliminate weaknesses that exist in only one classifier , but can also eliminate strengths that exist in only one classifier .", "label": "", "metadata": {}, "score": "73.67221"}
{"text": "However , it turns out that word disambiguation is a much more complicated problem , as POS says nothing to distinguish between the meaning of cold in ' ' I was a bit cold during the movie ' ' and ' '", "label": "", "metadata": {}, "score": "73.87487"}
{"text": "These results suggest that the limited information conveyed in adjectives is not representative of the full review itself .+ As in the motivating example for the use of POS tagging , it was in the case of the verb use of ' ' love ' ' ( ' ' I love this movie ' ' ) that conveyed sentimental information , rather than the adjective use of the word .", "label": "", "metadata": {}, "score": "75.09561"}
{"text": "It contains data for four words : hard , interest , line , and serve .Choose one of these four words , and load the corresponding data : .Using this dataset , build a classifier that predicts the correct sense tag for a given instance .", "label": "", "metadata": {}, "score": "75.233215"}
{"text": "Bush RM , Fitch WM , Bender CA , Cox NJ : Positive selection on the H3 hemagglutinin gene of human influenza virus A. Mol Biol Evol 1999/11/11 Edition 1999 , 16 ( 11 ) : 1457 - 1465 .PubMed .", "label": "", "metadata": {}, "score": "75.37377"}
{"text": "Sample Text 1 .Sample Text 2 .Sample Text 3 .Sample Text 4 . F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 .Figure 7 : Pictorial representations of feature set for sample Non text regions .V. .", "label": "", "metadata": {}, "score": "75.45201"}
{"text": "Unless otherwise indicated , sites have been observed in all 17 consensus sequences .Of the 12 ASN glycosylation sites found under PPSearch 7 ASN glycosylation sites have been have been cross - referenced to potential sites of HA in the Uniprot KnowledgeBase , UniProtKB / Swiss - Prot Entry Q91MA7 .", "label": "", "metadata": {}, "score": "75.91108"}
{"text": "These terms are also listed in the index .Each chapter is concluded by a fairly thorough ' Further Reading ' section and a set of exercises with tasks of varying difficulty .Several of the chapters are also broken up by small sets of exercises .", "label": "", "metadata": {}, "score": "75.92889"}
{"text": "Alternatively , such repetitive motif in the HA1 and HA2 subunits suggest common function in the 2 subunits possibly in guiding receptor binding and membrane fusion .A time course analysis to determine the frequency of substitution over the years was performed and lacked a distinct pattern in its amino acid substitution resulting in a zigzag behavior from 1982 onwards ( Figure 2c ) .", "label": "", "metadata": {}, "score": "76.0179"}
{"text": "We compared the percent change in amino acid substitution ( mutation frequency ) in the Hong Kong data set from 1968 - 1999 and calculated the genetic distance .Association between amino acids substitution and the MEME blocks were determined and are represented in Figure 2a .", "label": "", "metadata": {}, "score": "76.34031"}
{"text": "The difference between the normal lexical paradigm and the ad hoc paradigm can be represented as follows : . inches . feet yards , etc . . ) away ( normal ) . away ( deviant ) .Cf . also \" half an hour and ten thousand miles ago \" ( Arthur C. Clark ) .", "label": "", "metadata": {}, "score": "76.453964"}
{"text": "For example , the verb denationalise has both a prefix de-and a suffix -ise ( -ize ) .In fact no such sound - forms function as independent units in modern English .There are also numerous cases when identical morphemic structure of different words is insufficient proof of the identical pattern of their derivative structure which can be revealed only by IC analysis .", "label": "", "metadata": {}, "score": "76.5585"}
{"text": "For example , when classifying documents into topics ( such as sports , automotive , or murder mystery ) , features such as hasword(football ) are highly indicative of a specific label , regardless of what other the feature values are .", "label": "", "metadata": {}, "score": "76.78889"}
{"text": "+ In an effort to preserve the potential value of negation information while using dead - simple features , we tagged words between those expressing negation and the next punctuation mark with a postfix ' ' \\_NOT . ' ' This distinguishes sentences like ' '", "label": "", "metadata": {}, "score": "76.84662"}
{"text": "MEME block 2 also encompasses the entire length of antigenic site B , and 1/5 of antigenic sites C and D in HA are present in this block ( Table 4 ) .Three receptor binding sites overlap this block ( Table 5 ) .", "label": "", "metadata": {}, "score": "77.06432"}
{"text": "It was previously reported that the addition of new oligosaccharides to the HA of the H3N2 viruses contributes to the virus ability to elude antibody pressures by changing its antigenic potential [ 15 ] .Alterations in HA glycosylation may affect NK cell recognition of influenza virus - infected cells [ 16 ] .", "label": "", "metadata": {}, "score": "77.14511"}
{"text": "The highly variable MEME block 11 ( 171 - 172 - 174 - 176 ) participated with 4 sites in the co - occurring mutation pairs ( Table 6 ) .Interestingly , MEME blocks 3 , 4 , 5 , 8 , 10 and 12 had no co - occurring pairs of mutations ( Table 6 ) .", "label": "", "metadata": {}, "score": "77.39238"}
{"text": "D degree in 2006 from Annamalai University , Chidambaram .He has conducted workshops and conferences in the area of Multimedia , Business Intelligence , Analysis of Algorithms and Data Mining .Ha has presented and published more than 32 papers in conferences and journals and is the co - author of the book Numerical Methods with C++ Program ( PHI,2005 ) .", "label": "", "metadata": {}, "score": "78.012344"}
{"text": "One study has previously reported a CKII phosphorylation domain [ 10 ] .16 of the potential phosphorylation sites are Protein kinase C ( PKC ) phosphorylation site encompassing different regions of the protein .The clustering of the PKC phosphorylation site is at position 152 - 224 ( 9/16 sites are in this region ) in contrast to the clustering of CKII phosphorylation site from position 416 - 459 ) ; it is worth noting that CKII phosphorylation clustering is followed by two PKC phosphorylation sites .", "label": "", "metadata": {}, "score": "78.35382"}
{"text": "P04 - 1078 [ bib ] : Radu Soricut ; Eric Brill A Unified Framework For Automatic Evaluation Using 4-Gram Co - occurrence Statistics .P04 - 1079 [ bib ] : Bogdan Babych ; Tony Hartley Extending the BLEU MT Evaluation Method with Frequency Weightings .", "label": "", "metadata": {}, "score": "78.38252"}
{"text": "Protein Sequences .Sequence files in the FASTA format were collected from the National Center for Biotechnology ( NCBI ) Influenza virus resource website [ 23 , 24 ] , a specialized database based on the NCBI Genbank database .A total of 34 full - length annotated strains of the HA segment of Hong Kong H3N2 isolates between 1968 and 1999 were used in this study .", "label": "", "metadata": {}, "score": "78.4205"}
{"text": "MEME blocks 3 , 5 , 9 and 10 occur twice over the entire amino acid sequence with a motif size of 35 , 21 , 15 and 11 respectively .The remaining MEME blocks occur only once with varying motif sizes of 4 - 50 amino acids .", "label": "", "metadata": {}, "score": "78.50261"}
{"text": "The first part of hot spot cluster I between amino acid position 140 - 154 , is included within MEME block 7 ( 130 - 170 ) .The second part of hot spot cluster I , between position 170 - 180 , overlaps MEME block 11 entirely ( 171 - 178 ) and with one of the repetitive MEME block 2 ( 179 - 207 ) .", "label": "", "metadata": {}, "score": "78.59609"}
{"text": "View Article PubMed .Fitch WM , Bush RM , Bender CA , Cox NJ : Long term trends in the evolution of H(3 ) HA1 human influenza type A. Proc Natl Acad Sci U S A 1997/07/22 Edition 1997 , 94 ( 15 ) : 7712 - 7718 .", "label": "", "metadata": {}, "score": "78.79302"}
{"text": "The entire antigenic B site ( 187 - 196 ) was contained within one of the repetitive MEME block 2 ( 179 - 207 ) and also contains a potential phosphorylation site ( PKC ) .Notably , antigenic site A also overlaps a hot spot cluster ( 140 - 154 ) .", "label": "", "metadata": {}, "score": "78.9214"}
{"text": "Co- Chairs : Philippe Blache , CNRS & Universit\u00e9 de Provence Co - Chairs : Horacio Rodr\u00edguez , Universitat Polit\u00e8cnica de Catalunya .ACL-04 will include special sessions for interactive posters and demonstrations .Poster / Demo presentations present original work in progress , on - going research projects with novel ideas / applications , or late - breaking results that are best communicated in an interactive format .", "label": "", "metadata": {}, "score": "79.02196"}
{"text": "To date , there have been four RTE Challenges , where shared development and test data is made available to competing teams .Here are a couple of examples of text / hypothesis pairs from the Challenge 3 development dataset .The label True indicates that the entailment holds , and False , that it fails to hold .", "label": "", "metadata": {}, "score": "79.1728"}
{"text": "View Article PubMed .Ahn I , Son HS : Comparative study of the hemagglutinin and neuraminidase genes of influenza A virus H3N2 , H9N2 , and H5N1 subtypes using bioinformatics techniques .Can J Microbiol 2007/09/28 Edition 2007 , 53 ( 7 ) : 830 - 839 .", "label": "", "metadata": {}, "score": "79.50177"}
{"text": "Du X , Wang Z , Wu A , Song L , Cao Y , Hang H , Jiang T : Networks of genomic co - occurrence capture characteristics of human influenza A ( H3N2 ) evolution .Genome Res 2007/11/23 Edition 2008 , 18 ( 1 ) : 178 - 187 .", "label": "", "metadata": {}, "score": "79.51078"}
{"text": "Authors ' Affiliations .YJ - Science and Technology Research Center ( STRC ) , American University in Cairo .Department of Biology , American University in Cairo .Department of Informatics and Systems , Division of Engineering Sciences Research , National Research Centre ( NRC ) .", "label": "", "metadata": {}, "score": "79.56256"}
{"text": "It is readily observed that a certain component of the word - meaning is described when the word is identified distributionally .For example , in the sentence The boy - home the missing word is easily identified as a verb - The boy went , came , ran , etc . home .", "label": "", "metadata": {}, "score": "79.75838"}
{"text": "In the training corpus , most documents are automotive , so the classifier starts out at a point closer to the \" automotive \" label .But it then considers the effect of each feature .In this example , the input document contains the word \" dark , \" which is a weak indicator for murder mysteries , but it also contains the word \" football , \" which is a strong indicator for sports documents .", "label": "", "metadata": {}, "score": "80.07367"}
{"text": "Bioinformation 2008/01/12 Edition 2007 , 2 ( 2 ) : 57 - 61 .PubMed .Skehel JJ , Stevens DJ , Daniels RS , Douglas AR , Knossow M , Wilson IA , Wiley DC : A carbohydrate side chain on hemagglutinins of Hong Kong influenza viruses inhibits recognition by a monoclonal antibody .", "label": "", "metadata": {}, "score": "80.09003"}
{"text": "+ Interestingly , for Naive Bayes , the positive and negative tests performed very differently between presence and frequency tests .By comparison , SVMs exhibited an average aggregate difference of 0.7\\% .These results provide evidence that training on presence rather than frequency yields models with less bias .", "label": "", "metadata": {}, "score": "80.4059"}
{"text": "View Article PubMed .Igarashi M , Ito K , Kida H , Takada A : Genetically destined potentials for N - linked glycosylation of influenza virus hemagglutinin .Virology 2008/05/06 Edition 2008 .Klenk HD , Wagner R , Heuer D , Wolff T : Importance of hemagglutinin glycosylation for the biological functions of influenza virus .", "label": "", "metadata": {}, "score": "80.45469"}
{"text": "The HA1 and HA2 are represented in yellow and blue , respectively .A ) MEME blocks on HA : MEME2 ( Magenta ) , MEME7 ( Red ) , MEME3 ( Bright Green ) , MEME1 ( Orange ( 89 - 129 AA ) ) .", "label": "", "metadata": {}, "score": "80.60579"}
{"text": "Of these , blocks we selected 14 motifs for further analysis that occurred at more than 94 % of the sequences .Consensus sequences were submitted into the PPSearch ( Protein Motifs Search ) [ 19 ] tool available at the European Bioinformatics Institute website .", "label": "", "metadata": {}, "score": "80.663536"}
{"text": "IC analysis , however , shows that whereas snow - covered may be treated as a compound consisting of two stems snow + covered , blue - eyed is a suffixal derivative as the underlying structure as shown by IC analysis is different , i.e. ( blue+eye)+-ed .", "label": "", "metadata": {}, "score": "80.79202"}
{"text": "The authors succeed in ensuring that the material is relevant and interesting , one of the most important yet difficult criteria to meet when teaching statistics .The book has been written in LaTeX and has the format commonly associated with such documents .", "label": "", "metadata": {}, "score": "81.127075"}
{"text": "Mutation of glycosylation sites near receptor binding sites of HA1 was proposed to be an adaptation mechanism of the H7 viruses to a new host [ 18 ] .These associations suggest that MEME block 2 is a dynamic block in this protein that contributes to the ability of HA1 to mutate , modify its activity by post - translational modification , enhance pathogenicity by mutating receptor binding sites and escaping the host immune response by mutation in antigenic sites .", "label": "", "metadata": {}, "score": "81.232635"}
{"text": "Frequency of potential N - myristilation site in the MEME blocks reveals that MEME blocks 1 , 2 and 7 have a high myristilation sites frequency .Frequency of potential N - glycosylation site in the MEME blocks reveal that MEME block 2 and 7 has a high glycosylation sites frequency .", "label": "", "metadata": {}, "score": "81.72354"}
{"text": "+ Mostly out of curiosity , we wanted to see how our test configurations will perform when training on the movie dataset and testing on the Yelp dataset , an external out - of - domain dataset .We preprocessed the Yelp dataset such that it matched the format of the movie dataset and selected 1000 of each of the 1 - 5 star rating reviews .", "label": "", "metadata": {}, "score": "82.67172"}
{"text": "View Article PubMed .Skehel JJ , Wiley DC : Receptor binding and membrane fusion in virus entry : the influenza hemagglutinin .Annu Rev Biochem 2000/08/31 Edition 2000 , 69 : 531 - 569 .View Article PubMed .Fleury D , Grenningloh G , Lafanechere L , Antonsson B , Job D , Cohen - Addad C : Preliminary crystallographic study of a complex formed between the alpha / beta - tubulin heterodimer and the neuronal growth - associated protein SCG10 .", "label": "", "metadata": {}, "score": "82.94023"}
{"text": "Compare , e.g. , the meaning of the verb to move in the pattern to move+N : 1 . cause to change position ( e.g. move the chair , the piano , etc . ) , 2 . arouse , work on the feelings of smb .", "label": "", "metadata": {}, "score": "83.33028"}
{"text": "FEATURE SET FOR NON TEXT REGION SAMPLES .[ 2 ] .Feature set .Sample non Text 1 .Sample non Text 2 .Sample non Text 3 .Sample non Text 4 .[ 3 ] .F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 .", "label": "", "metadata": {}, "score": "83.99186"}
{"text": "One can say , e.g. , to kiss somebody into doing smth , to flatter somebody into doing smth , to beat somebody into doing something , . etc . ; in all these word - groups one finds the meaning ' to make somebody do something ' which is actually imparted by the distributional pattern .", "label": "", "metadata": {}, "score": "83.99649"}
{"text": "The topics of interest cover all aspects of computational linguistics , as outlined in the CFP for the Main Technical Session .: : Programme Committee : . : : List of accepted papers : .INCORPORATING TOPIC INFORMATION INTO SEMANTIC ANALYSIS MODELS Tony Mullen , Nigel Collier .", "label": "", "metadata": {}, "score": "84.17937"}
{"text": "Molecular and viral characterization of the hemagglutinin protein ( HA ) from different hosts has increased in the last three decades , in response to three worldwide outbreaks of influenza in the years 1918 , 1957 , and 1968 [ 1 ] .", "label": "", "metadata": {}, "score": "84.29338"}
{"text": "P04 - 1087 [ bib ] : Ben Hutchinson Acquiring the Meaning of Discourse Markers .P04 - 1088 [ bib ] : Riccardo Serafin ; Barbara Di Eugenio FLSA :Extending Latent Semantic Analysis with Features for Dialogue Act Classification Affiliated with .", "label": "", "metadata": {}, "score": "84.77745"}
{"text": "This serves as a rough way to disambiguate words that may hold different meanings in different contexts .For example , it would distinguish the different uses of \" love \" in ' ' I love this movie ' ' versus ' '", "label": "", "metadata": {}, "score": "84.82189"}
{"text": "Background .Variations in the influenza Hemagglutinin protein contributes to antigenic drift resulting in decreased efficiency of seasonal influenza vaccines and escape from host immune response .We performed an in silico study to determine characteristics of novel variable and conserved motifs in the Hemagglutinin protein from previously reported H3N2 strains isolated from Hong Kong from 1968 - 1999 to predict viral motifs involved in significant biological functions .", "label": "", "metadata": {}, "score": "84.85135"}
{"text": "Had he alone thought Mary left John ?As can be inferred from the above distributional analysis is mainly applied by the linguist to find out sameness or difference of meaning .It is assumed that the meaning of any lexical unit may be viewed as made up by the lexical meaning of its components and by the meaning of the pattern of their arrangement , i.e. their distributional meaning .", "label": "", "metadata": {}, "score": "84.90965"}
{"text": "It was previously reported that the addition of new oligosaccharide to the HA of the H3N2 virus contributes to the virus ability to elude antibody pressure by changing its antigenic potential [ 15 ] .Indeed , several studies have addressed the effect of glycosylation on the binding affinity of HA with sialic acid ( SA)-containing receptors [ 16 ] .", "label": "", "metadata": {}, "score": "84.99633"}
{"text": "References .Kilbourne ED : Influenza pandemics of the 20th century .Emerg Infect Dis 2006/02/24 Edition 2006 , 12 ( 1 ) : 9 - 14 .PubMed .Cox NJ , Subbarao K : Global epidemiology of influenza : past and present .", "label": "", "metadata": {}, "score": "85.75753"}
{"text": "Vaccine 2007/10/24 Edition 2007 , 25 ( 48 ) : 8133 - 8139 .View Article PubMed .Liao YC , Lee MS , Ko CY , Hsiung CA : Bioinformatics models for predicting antigenic variants of influenza A / H3N2 virus .", "label": "", "metadata": {}, "score": "85.846344"}
{"text": "Although a movie review and a Yelp review will differ in specialized vocabulary , audience , tone , etc . , the ways that people convey sentiment ( e.g. I loved it ! ) may not differ entirely .We wished to explore how training classifiers in one domain might generalize to neighbor domains .", "label": "", "metadata": {}, "score": "86.868004"}
{"text": "Potential post - translational modification sites in HA protein .Scanning the 17 consensus sequence against the existing Prosite Motifs database ( PPSearch ) revealed five potential post - translational modification sites .The sites detected include 24 phosphorylation , 12 glycosylation and 14 myristylation sites ( Table 3 ) .", "label": "", "metadata": {}, "score": "87.157196"}
{"text": "RM.Chandrasekaran is currently working as a Professor at the Department of Computer Science and Engineering , Annamalai University , Annamalai Nagar , Tamilnadu , India .From 1999 to 2001 he worked as a software consultant in Etiam , Inc , California , USA .", "label": "", "metadata": {}, "score": "88.200905"}
{"text": "True negatives are irrelevant items that we correctly identified as irrelevant .False positives ( or Type I errors ) are irrelevant items that we incorrectly identified as relevant .False negatives ( or Type II errors ) are relevant items that we incorrectly identified as irrelevant .", "label": "", "metadata": {}, "score": "90.18997"}
{"text": "Prosite motifs detected for the H3N2 sequences using PPSearch this includes 24 phosphorylation , 12 glycosylation and 14 myristylation sites .Potential phosphorylation sites include casein kinase II phosphorylation site , protein kinase C phosphorylation site and cAMP- and cGMP - dependent protein kinase phosphorylation site , ASN glycosylation motifs and N - myristylation sites .", "label": "", "metadata": {}, "score": "94.110245"}
{"text": "In this study , we report motifs and assign potential functional characteristics within the HA protein sequences of the gene of H3N2 human influenza isolates from Hong Kong between 1968 and 1999 .We identify motifs within the HA protein , and interrelate these motifs with amino acid substitutions frequency , co - mutating pairs , potential post - translation modification sites , antigenic sites , receptor - binding sites .", "label": "", "metadata": {}, "score": "96.23068"}
{"text": "For example , fat major 's wife may mean that either ' the major is fat ' or ' his wife is fat ' .It must be admitted that this kind of analysis is arrived at by reference to intuition and it should be regarded as an attempt to formalise one 's semantic intuition .", "label": "", "metadata": {}, "score": "98.35275"}
{"text": "Challenge 3 , Pair 81 ( False ) .T :According to NC Articles of Organization , the members of LLC company are H. Nelson Beavers , III , H. Chester Beavers and Jennie Beavers Stewart .H : Jennie Beavers Stewart is a share - holder of Carolina Analytical Laboratory .", "label": "", "metadata": {}, "score": "100.66451"}
{"text": "T : Parviz Davudi was representing Iran at a meeting of the Shanghai Co - operation Organisation ( SCO ) , the fledgling association that binds Russia , China and four former Soviet republics of central Asia together to fight terrorism .", "label": "", "metadata": {}, "score": "106.42666"}
{"text": "There are no restrictions inherent in the grammar or vocabulary of the English language that would make co - occurrence of the participle flying with the noun rhinoceros impossible , yet we may be reasonably certain that the two words are unlikely to co - occur .", "label": "", "metadata": {}, "score": "116.991646"}
