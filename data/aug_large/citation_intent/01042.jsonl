{"text": "LDA - based keyword selection in text categorization .In Proceedings of the 24th International Symposium on Computer and Information Sciences , 2009 .Yixun Liu et al .Modeling class cohesion as mixtures of latent topics , In Proceedings of the IEEE International Conference on Software Maintenance , 2009 .", "label": "", "metadata": {}, "score": "28.400665"}
{"text": "The paper is aimed at exploring the LDA to infer topics over direct translation , capture similarities between vocabularies in different languages , and detect differences in topic emphasis between languages .The evaluation is conducted on held out data using likelihood .", "label": "", "metadata": {}, "score": "30.272343"}
{"text": "We present LDA - SP , which utilizes LinkLDA ( Erosheva et al ., 2004 ) to model selectional preferences .By simultaneously inferring latent topics and topic distri ... \" .The computation of selectional preferences , the admissible argument values for a relation , is a well - known NLP task with broad applicability .", "label": "", "metadata": {}, "score": "30.97409"}
{"text": "The technique assumes that a document is comprised of a mixture of topics - as a result , one can assign a document to different topics with different probabilities .There have been a number of applications of LDA in bioinformatics with some applications focusing on topic models as way to cluster objects such as genes [ 1 , 2 ] , whereas others have used it in the more traditional document grouping context [ 3 ] .", "label": "", "metadata": {}, "score": "31.974022"}
{"text": "The technique assumes that a document is comprised of a mixture of topics - as a result , one can assign a document to different topics with different probabilities .There have been a number of applications of LDA in bioinformatics with some applications focusing on topic models as way to cluster objects such as genes [ 1 , 2 ] , whereas others have used it in the more traditional document grouping context [ 3 ] .", "label": "", "metadata": {}, "score": "31.974022"}
{"text": "The technique assumes that a document is comprised of a mixture of topics - as a result , one can assign a document to different topics with different probabilities .There have been a number of applications of LDA in bioinformatics with some applications focusing on topic models as way to cluster objects such as genes [ 1 , 2 ] , whereas others have used it in the more traditional document grouping context [ 3 ] .", "label": "", "metadata": {}, "score": "31.974022"}
{"text": ", 2004 ) to model selectional preferences .By simultaneously inferring latent topics and topic distributions over relations , LDA - SP combines the benefits of previous approaches : like traditional classbased approaches , it produces humaninterpretable classes describing each relation 's preferences , but it is competitive with non - class - based methods in predictive power .", "label": "", "metadata": {}, "score": "33.062138"}
{"text": "There have been a number of applications of LDA in bioinformatics with some applications focusing on topic models as way to cluster objects such as genes [ 1 , 2 ] , whereas others have used it in the more traditional document grouping context [ 3 ] .", "label": "", "metadata": {}, "score": "33.730583"}
{"text": "There have been a number of applications of LDA in bioinformatics with some applications focusing on topic models as way to cluster objects such as genes [ 1 , 2 ] , whereas others have used it in the more traditional document grouping context [ 3 ] .", "label": "", "metadata": {}, "score": "33.730583"}
{"text": "Just using the Conversational model was n't quite good enough since topic and dialogue structure were mixed in the results and the focus is on dialogue structure .They use an LDA framework to modify the model to account for topic and thus separate content words from dialogue indicators .", "label": "", "metadata": {}, "score": "36.63127"}
{"text": "This paper has two interesting extensions to LDA that account for the power - law distribution of word frequencies in real documents .First , a general \" background \" distribution represents common words .Second , a \" special words \" model allows each document to have some unique words .", "label": "", "metadata": {}, "score": "36.638023"}
{"text": "Weisi and Dong read \" Labeled LDA : A supervised topic model for credit attribution in multi - labeled corpora \" , Daniel Ramage et al , EMNLP 2009 .This paper presents the modified form of LDA that is used in the focus paper .", "label": "", "metadata": {}, "score": "36.861565"}
{"text": "In these experiments , SAM consistently outperforms existing models .Mark Steyvers , Tom Griffiths .Probabilistic Topic Models .In Landauer , T. , Mcnamara , D. , Dennis , S. , Kintsch , W. , Latent Semantic Analysis : A Road to Meaning .", "label": "", "metadata": {}, "score": "38.49969"}
{"text": "We also evaluate LDA - SP 's effectiveness at filtering improper applications of inference rules , where we show substantial improvement over Pantel et al . 's system ( Pantel et al . , 2007 ) . by Hui Lin , Jeff Bilmes - IN THE 49TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS : HUMAN LANGUAGE TECHNOLOGIES ( ACL - HLT , 2011 . \" ...", "label": "", "metadata": {}, "score": "39.036835"}
{"text": "I have read the paper \" Labeled LDA : A supervised topic model for credit attribution in multi - labeled corpora \" by Daniel Ramage et al . appeared in EMNLP 2009 .This paper tackles the problem on how to make the topics to respresent some known labels , which is not addressed by standard LDA .", "label": "", "metadata": {}, "score": "39.558296"}
{"text": "Overall , the features extracted from their topic model significantly improve performance on both tasks .Only three different related papers were read this week : a paper about modeling conversations in twitter , a paper about Labeled LDA , and a paper about analyzing twitter discourse .", "label": "", "metadata": {}, "score": "39.768997"}
{"text": "Although the features or combinations themselves may not have been that unique , the end result of the paper showed improved performance from such an approach .In somewhat of a contrast , Daniel read about novel syntactic features used to improve WSD performance .", "label": "", "metadata": {}, "score": "39.977535"}
{"text": "We demonstrate this algorithm on several collections of scientific abstracts .This model exemplifies a recent trend in statistical machine learning - the use of nonparametric Bayesian methods to infer distributions on flexible data structures .This is a longer version of Blei et al .", "label": "", "metadata": {}, "score": "40.656754"}
{"text": "TopicXP : Exploring Topics in Source Code using Latent Dirichlet Allocation , In Proceedings of the 26th IEEE International Conference on Software Maintenance , Romania , 2010 .Xin Jin et al .LDA - based related word detection in advertising , In Proceedings of the 2010 Seventh Web Information Systems and Applications Conference , 2010 .", "label": "", "metadata": {}, "score": "40.864414"}
{"text": "They evaluate the model on many different tasks , comparing to one - vs - rest SVM , although Weisi notes that comparing to vanilla LDA would have been good in order to understand the quantitative effects of adding the labels .", "label": "", "metadata": {}, "score": "41.74761"}
{"text": "In the focus paper , the distance between two clusters is represented by the sum of different clusterings for a pair of items .It may seem that only homogeneity and completeness are addressed over here .However , since none of the clusterings represent true clustering , constraints ( 3 ) and ( 4 ) probably are not required .", "label": "", "metadata": {}, "score": "42.15471"}
{"text": "Because for the ranking experiments , they do n't use the 4S dimensions but only the topic distribution of the tweet , it would have been nice if they had also compared with standard LDA .The meeting revolved mostly around evaluation of Topic Models .", "label": "", "metadata": {}, "score": "42.414177"}
{"text": "Detecting salient aspects in online reviews of health providers , In Proceedings of the AMIA Annual Symposium , 2010 .Georgiana Dinu and Mirella Lapata .Topic models for meaning similarity in context , In Proceedings of the 23rdInternational Conference on Computational Linguistics ( COLING ) , PA , USA , 2010 .", "label": "", "metadata": {}, "score": "43.055183"}
{"text": "In contrast to the cDTM , the original discrete - time dynamic topic model ( dDTM ) requires that time be discretized .Moreover , the complexity of variational inference for the dDTM grows quickly as time granularity increases , a drawback which limits fine - grained discretization .", "label": "", "metadata": {}, "score": "43.414574"}
{"text": "Introduces an auxiliary - variable method for Gibbs sampling in non - conjugate topic models .David Mimno , Hanna Wallach , Jason Naradowsky , David A. Smith , Andrew McCallum .Polylingual Topic Models .EMNLP ( 2009 ) .Cross - language .", "label": "", "metadata": {}, "score": "43.58093"}
{"text": "It is based on conversation model by Barzilay and Lee ( 2004 ) using HMMs for topic transitions , with each topic generating a message .Another latent variable - source is added that generates a particular word .A source may be one of ( 1 )", "label": "", "metadata": {}, "score": "43.689674"}
{"text": "Based on this observation I wondered what I would get ( and what it would mean ) by applying a technique like LDA to a collection structures and their fragments .My initial thought is that the use of LDA to determine a set of topics for a collection of chemical structures is essentially a clustering of the molecules , with the terms associated with the topics being representative substructures for that \" cluster \" .", "label": "", "metadata": {}, "score": "44.057507"}
{"text": "Based on this observation I wondered what I would get ( and what it would mean ) by applying a technique like LDA to a collection structures and their fragments .My initial thought is that the use of LDA to determine a set of topics for a collection of chemical structures is essentially a clustering of the molecules , with the terms associated with the topics being representative substructures for that \" cluster \" .", "label": "", "metadata": {}, "score": "44.057507"}
{"text": "Based on this observation I wondered what I would get ( and what it would mean ) by applying a technique like LDA to a collection structures and their fragments .My initial thought is that the use of LDA to determine a set of topics for a collection of chemical structures is essentially a clustering of the molecules , with the terms associated with the topics being representative substructures for that \" cluster \" .", "label": "", "metadata": {}, "score": "44.057507"}
{"text": "Based on this observation I wondered what I would get ( and what it would mean ) by applying a technique like LDA to a collection structures and their fragments .My initial thought is that the use of LDA to determine a set of topics for a collection of chemical structures is essentially a clustering of the molecules , with the terms associated with the topics being representative substructures for that \" cluster \" .", "label": "", "metadata": {}, "score": "44.057507"}
{"text": "Based on this observation I wondered what I would get ( and what it would mean ) by applying a technique like LDA to a collection structures and their fragments .My initial thought is that the use of LDA to determine a set of topics for a collection of chemical structures is essentially a clustering of the molecules , with the terms associated with the topics being representative substructures for that \" cluster \" .", "label": "", "metadata": {}, "score": "44.057507"}
{"text": "This paper further proposes to use the multi - modality manifold - ranking algorithm for extracting topic - focused summary from multiple documents by considering the within - document sentence relationships and the cross - document sentence relationships as two separate modalities ( graphs ) .", "label": "", "metadata": {}, "score": "44.10099"}
{"text": "This is because LDA can be used to discover the underlying topic structures of any kind of discrete data .Therefore , GibbsLDA++ is not limited to text and natural language processing but can also be applied to other kinds of data like images and biological sequences .", "label": "", "metadata": {}, "score": "44.4311"}
{"text": "The recent explosion of analysis on biological datasets , which are frequently overlapping , has led to new clustering models that allow hard assignment of data points to multiple clusters .One particularly appealing model was proposed by Segal et al . in the context of probabilistic relational models ( PRMs ) applied to the analysis of gene microarray data .", "label": "", "metadata": {}, "score": "44.62304"}
{"text": "This paper discusses unsupervised methods for analyzing dialog acts in series of tweets .They present three methods , the EM Conversation model , the Conversation+Topic model , and the Bayesian Conversation model , where the Conversation+Topic model was their principal contribution .", "label": "", "metadata": {}, "score": "44.83434"}
{"text": "Labeled LDA defines a one - to - one correspondence between LDA 's latent topics and user tags .In comparison with previous models such as Supervised LDA , this model allows documents to be associated with multiple labels .The inference is similar to the standard LDA model , except the topics of a particular document are restricted to the topic set that is associated with the labels of that document .", "label": "", "metadata": {}, "score": "44.921688"}
{"text": "We first develop an entity - aspect LDA model to simultaneously cluster both sentences and words into aspects .We then apply frequent subtree pattern mining on the dependency parse trees of the clustered and labeled sentences to discover sentence patterns that well represent the aspects .", "label": "", "metadata": {}, "score": "44.97769"}
{"text": "User interest analysis with hidden topic in news recommendation system , In Proceedings of the International Conference on Asian Language Processing , 2010 .Scott Grant and James R. Cordy .Estimating the optimal number of latent concepts in source code analysis , In Proceedings of the 10th IEEE Working Conference on Source Code Analysis and Manipulation , 2010 .", "label": "", "metadata": {}, "score": "45.238693"}
{"text": "An Unsupervised Aspect - Sentiment Model for Online Reviews , In Proceedings of the 2010 Human Language Technologies and The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics ( HLT - NAACL ) , PA , USA , 2010 .", "label": "", "metadata": {}, "score": "45.46973"}
{"text": "( 2009 ) for calculating held - out probability .This package implements latent Dirichlet allocation ( LDA ) and related models .This includes ( but is not limited to ) sLDA , corrLDA , and the mixed - membership stochastic blockmodel .", "label": "", "metadata": {}, "score": "45.59328"}
{"text": "Model - based Overlapping Clustering [ Details ] [ PDF ] A. Banerjee , C. Krumpelman , S. Basu , Raymond J. Mooney and Joydeep Ghosh In Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD-05 ) , 2005 .", "label": "", "metadata": {}, "score": "46.02678"}
{"text": "PLTM is an extension of latent Dirichlet allocation ( LDA ) and topic assignments can be inferred using Gibbs sampling .The generalization ability of the model is based on the probability of previously unseen held - out document given posterior estimates .", "label": "", "metadata": {}, "score": "46.0307"}
{"text": "In addition to the standar n - gram type features already in use by earlier models of WSD , the authors add features for dependencies to specific words being present with a given word sense , as well as features for subcategorization frames .", "label": "", "metadata": {}, "score": "46.032703"}
{"text": "Edoardo M. Airoldi , David M. Blei , Stephen E. Fienberg , Eric P. Xing .Mixed Membership Stochastic Blockmodels .JMLR ( 9 ) 2008 pp .1981 - 2014 .Networks .A dense but excellent review of inference in topic models .", "label": "", "metadata": {}, "score": "46.032795"}
{"text": "Daniel has read the paper \" Evaluation Methods for Topic Models . \" by Wallach et al .ICML 2009 .The paper explores various methods for evaluating LDA .The authors conduct the experiments in two settings : held - out documents , in which entire documents are held out , and document completion , in which only the latter half of each document is held out .", "label": "", "metadata": {}, "score": "46.12189"}
{"text": "The author proves that VI is the only function that satisfies all the desired properties , although some of these properties are somewhat nonintuitive , relating to the way the metric interacts with combinations of clusters .The principal advantage of this metric over others is the fact that it is comparable across datasets and experimental conditions without rescaling , which is generally not mathematically justified .", "label": "", "metadata": {}, "score": "46.255245"}
{"text": "I liked the way they evaluated , although I 'm wondering how sensitive their results are too the used inference methods / parameters .My related paper essentially builds on the work by Chang et al . by proposing automatic methods to measure topic coherence / interpretability .", "label": "", "metadata": {}, "score": "46.321693"}
{"text": "A multi - view approach for term translation spotting .In Proceedings of the 12th International Conference on Computational Linguistics and Intelligent Text Processing ( CICLing 2011 ) , 2011 .Viet Cuong Nguyen et al .Improving Text Segmentation with Non - systematic Semantic Relation .", "label": "", "metadata": {}, "score": "46.479515"}
{"text": "There have been several implementations of this model in C ( using Variational Methods ) , Java , and Matlab .We decided to release this implementation of LDA in C / C++ using Gibbs Sampling to provide an alternative to the topic - model community .", "label": "", "metadata": {}, "score": "46.589394"}
{"text": "Based on these criteria , the author suggest four constraints - ( 1 ) Cluster Homogeneity - a coarser cluster containing heterogeneous items have a lower score than finer clusters of homogenous items .( 2 ) Completeness - Homogenous items should feature in the same cluster .", "label": "", "metadata": {}, "score": "46.718834"}
{"text": "The methods are compared by seeing which assigns higher probabilities to the held - out data , as well as by looking at the variance and computational complexity of each method .The Chib - style estimation and left - to - right evaluation are determined to be the best .", "label": "", "metadata": {}, "score": "46.80607"}
{"text": "Topic detection and organization of mobile text messages , In Proceedings of the 19th ACM international Conference on Information and Knowledge Management ( ACM CIKM ) , 2010 .Wayne Zhao et al .Context modeling for ranking and tagging bursty features in text streams , In Proceedings of the 19th ACM International Conference on Information and Knowledge Management ( ACM CIKM ) , 2010 .", "label": "", "metadata": {}, "score": "46.84487"}
{"text": "The evaluation methods are compared by checking which method assigns higher probabilities to the held - out data , as well as the variance and computational complexity .The method for comparing evaluation methods does not seem well - justified , as noted by Daniel .", "label": "", "metadata": {}, "score": "46.876244"}
{"text": "None of this body of work consider the query information .More recently , some research effort has been made to incorporate the query information into the topic model .The model uses a multinomial distribution to select whether to sample a word from a document - specific , a query ... . \" ...", "label": "", "metadata": {}, "score": "47.059654"}
{"text": "This kind of summary templates can be useful in various applications .We first develop an entity - aspect LDA model to simultaneously cluster both sentences and words int ... \" .In this paper , we propose a novel approach to automatic generation of summary templates from given collections of summary articles .", "label": "", "metadata": {}, "score": "47.223427"}
{"text": "SAM maintains the same hierarchical structure as Latent Dirichlet Allocation ( LDA ) , but models documents as points on a high - dimensional spherical manifold , allowing a natural likelihood parameterization in terms of cosine distance .Furthermore , SAM can model word absence / presence at the document level , and unlike previous models can assign explicit negative weight to topic terms .", "label": "", "metadata": {}, "score": "47.370754"}
{"text": "SAM maintains the same hierarchical structure as Latent Dirichlet Allocation ( LDA ) , but models documents as points on a high - dimensional spherical manifold , allowing a natural likelihood parameterization in terms of cosine distance .Furthermore , SAM can model word absence / presence at the document level , and unlike previous models can assign explicit negative weight to topic terms .", "label": "", "metadata": {}, "score": "47.370754"}
{"text": "For the focus paper , one thing I feel that is missing is the inter - rater agreement , especially in such as situation where there are a group of not highly reliable raters .The agreement might be calculated using measures such as Fleiss ' Kappa .", "label": "", "metadata": {}, "score": "47.455093"}
{"text": "A PLSA model is obtained for each time period and for each new time period the estimates from the previous time period is used for intialisation .Comparison with randomly initialized PLSA model demonstrates a improvement of 5 % in perplexity , as noted by Dhananjay .", "label": "", "metadata": {}, "score": "47.669518"}
{"text": "Web search clustering and labeling with hidden topics , ACM Transactions on Asian Language Information Processing ( ACM TALIP ) , Vol.8 , No.3 , 2009 .Chenghua Lin and Yulan He .Joint sentiment / topic model for sentiment analysis , In Proceeding of the 18th ACM Conference on Information and Knowledge Management ( ACM CIKM ) , 2009 .", "label": "", "metadata": {}, "score": "47.685265"}
{"text": "We show that our generative models capture interesting qualitative structure in natural scenes , and more accurately categorize novel images than models which ignore spatial relationships among features .The paper introduces a blocked Gibbs sampler for learning a nonparametric Bayesian topic model whose topic assignments are coupled with a tree - structured graphical model .", "label": "", "metadata": {}, "score": "47.726757"}
{"text": "The proposed model is general ; it can be applied to any text collections with a mixture of topics and an associated network structure .Per - document Dirichlet priors over topic distributions are generated using a log - linear combination of observed document features and learned feature - topic parameters .", "label": "", "metadata": {}, "score": "47.99462"}
{"text": "More specifically , we propose two strategies to incorporate the query information into a probabilistic model .Experimental results on two different genres of data show that our proposed approach can effectively extract a multi - topic summary from a document collection and the summarization performance is better than baseline methods .", "label": "", "metadata": {}, "score": "48.113"}
{"text": "The cDTM is a dynamic topic model that uses Brownian motion to model the latent topics through a sequential collection of documents , where a \" topic \" is a pattern of word use that we expect to evolve over the course of the collection .", "label": "", "metadata": {}, "score": "48.182938"}
{"text": "Han Xiao and Thomas Stibor .Efficient Collapsed Gibbs Sampling for Latent Dirichlet Allocation , In Proceedings of the 2nd Asian Conference on Machine Learning ( ACML ) , 2010 , Tokyo .Sanae Fujita et al .MSS : Investigating the effectiveness of domain combinations and topic features for word sense disambiguation , In Proceedings of the 5th International Workshop on Semantic Evaluation , USA , 2010 .", "label": "", "metadata": {}, "score": "48.317665"}
{"text": "The factored representation prevents combinatorial explosion and leads to efficient parameterization .We derive the variational optimization algorithm for the new model .The model shows improved perplexity on text and image data , but no significant accuracy improvement when used for classification .", "label": "", "metadata": {}, "score": "48.74602"}
{"text": "Commonly used methods for estimating the probability of held - out words may be unstable .This paper presents more accurate methods .The use of an asymmetric Dirichlet prior on per - document topic distributions reduces sensitivity to very common words ( eg stopwords and near - stopwords ) and makes topic assignments more stable as the number of topics grows .", "label": "", "metadata": {}, "score": "48.757133"}
{"text": "In a graphical model view , the model basically inserts into the LDA model a path node between the topic node and the word node .The state space of the node spans all the paths from the root synset to the a specific synset that could generate the observed word in the WordNet .", "label": "", "metadata": {}, "score": "48.830383"}
{"text": "For Labeled LDA , It was noted that the inference for the labels for the test documents was not worked out well ( they had simplified it to standard LDA inference ) .We also discussed the paper for discovering dialogue acts in Twitter .", "label": "", "metadata": {}, "score": "48.866913"}
{"text": "This is a generative modeling approach , allowing one to associate a document ( composed of a set of words ) with a \" topic \" .Here , a topic is a group of words that have a higher probability of being generated from that topic than another topic .", "label": "", "metadata": {}, "score": "48.934464"}
{"text": "This is a generative modeling approach , allowing one to associate a document ( composed of a set of words ) with a \" topic \" .Here , a topic is a group of words that have a higher probability of being generated from that topic than another topic .", "label": "", "metadata": {}, "score": "48.934464"}
{"text": "and provide an alternative interpretation of the model as a generalization of mixture models , which makes it easily interpretable .The general model is applicable to several domains , including high - dimensional sparse domains , such as text and recommender systems .", "label": "", "metadata": {}, "score": "49.00358"}
{"text": "Explores methods for inferring topic distributions for new documents given a trained model .This paper includes the SparseLDA algorithm and data structure , which can dramatically improve time and memory performance in Gibbs sampling .Latent Dirichlet Allocation models a document by a mixture of topics , where each topic itself is typically modeled by a unigram word distribution .", "label": "", "metadata": {}, "score": "49.057953"}
{"text": "About the focus paper , I feel the authors could have provided the inter - rater agreement , because the raters are not very reliable .Dhananjay read the paper \" Topic Evolution in a Stream of Documents \" by A e Gohr et al . in SIAM : 2009 Data Mining .", "label": "", "metadata": {}, "score": "49.12665"}
{"text": "The goal is to represent a meta - clustering of the base level clusterings to the user , so the user would spend less time going through all the base level clusterings to find the best one .For 1 ) , random feature weights are assigned for clustering and PCA are conducted to remove correlated features .", "label": "", "metadata": {}, "score": "49.255287"}
{"text": "In real semantic applications , people could wish to get rid of such semantic duplicated topics , and could do so if the similarity between two topics are known .As the author suggested that the task specific evaluation measures should be preferred over perplexity , I decided to read the paper above .", "label": "", "metadata": {}, "score": "49.377575"}
{"text": "The paper evaluates various metrics for clustering .The authors list down the following required criteria for the constraints that contribute in evaluating a metric - ( 1 ) Each constraint should address a limitation of the metric .( 2 ) For any metric , there should be an analytical way to prove whether the metric satisfies the contraint and ( 3 )", "label": "", "metadata": {}, "score": "49.56257"}
{"text": "Researches and papers using GibbsLDA++ for conducting experiments should include the following citation : .Xuan - Hieu Phan and Cam - Tu Nguyen .GibbsLDA++ : A C / C++ implementation of latent Dirichlet allocation ( LDA ) , 2007 .", "label": "", "metadata": {}, "score": "49.591133"}
{"text": "Other possibilities .The example shown here is rather simplistic and is the equivalent of unsupervised clustering .One obvious next step is to search the parameter space of the LDA model , evaluate different approaches to estimating the posterior distribution ( EM or Gibbs sampling ) and so on .", "label": "", "metadata": {}, "score": "49.61302"}
{"text": "Other possibilities .The example shown here is rather simplistic and is the equivalent of unsupervised clustering .One obvious next step is to search the parameter space of the LDA model , evaluate different approaches to estimating the posterior distribution ( EM or Gibbs sampling ) and so on .", "label": "", "metadata": {}, "score": "49.61302"}
{"text": "Other possibilities .The example shown here is rather simplistic and is the equivalent of unsupervised clustering .One obvious next step is to search the parameter space of the LDA model , evaluate different approaches to estimating the posterior distribution ( EM or Gibbs sampling ) and so on .", "label": "", "metadata": {}, "score": "49.61302"}
{"text": "Other possibilities .The example shown here is rather simplistic and is the equivalent of unsupervised clustering .One obvious next step is to search the parameter space of the LDA model , evaluate different approaches to estimating the posterior distribution ( EM or Gibbs sampling ) and so on .", "label": "", "metadata": {}, "score": "49.61302"}
{"text": "Other possibilities .The example shown here is rather simplistic and is the equivalent of unsupervised clustering .One obvious next step is to search the parameter space of the LDA model , evaluate different approaches to estimating the posterior distribution ( EM or Gibbs sampling ) and so on .", "label": "", "metadata": {}, "score": "49.61302"}
{"text": "Evaluation .This is one of the first papers to address joint topic models of text and hyperlinks .Used as a baseline in the more recent Relational Topic Models .( R.N. ) .Models variation of topic content with time at various scales of resolution .", "label": "", "metadata": {}, "score": "49.728012"}
{"text": "The paper provides an adaptive model for computing PLSA as a substitute to relearning for every window .The PLSA model is parameterized by documents , words and topics .For every time window , the words and documents of the previous time window are discarded and new words are introduced .", "label": "", "metadata": {}, "score": "49.815308"}
{"text": "A Comparative Analysis of Latent Variable Models for Web Page Classification .In Proceedings of the Latin American Web Conference , 2008 .Xuan - Hieu Phan et al .Learning to Classify Short and Sparse Text & Web with Hidden Topics from Large - scale Data Collections .", "label": "", "metadata": {}, "score": "49.831192"}
{"text": "Both yield similar accuracy and perform generally 15 % better than baseline systems .I feel like a large part of the paper is to maybe reorient focus on extracting latent features as the authors leverage the difficulty of the task as the key to advancing our current extraction methods .", "label": "", "metadata": {}, "score": "50.09509"}
{"text": "We also explore HIERSUM 's capacity to produce multiple ' topical summaries ' in order to facilitate content discovery and navigation . \" ...We propose the first unsupervised approach to the problem of modeling dialogue acts in an open domain .", "label": "", "metadata": {}, "score": "50.223206"}
{"text": "Bridging domains using World Wide Web knowledge for transfer learning , IEEE Transactions on Knowledge and Data Engineering ( IEEE TKDE ) , Vol.22 , No.6 , 2010 .Kerui Min et al .Decomposing background topics from keywords by principal component persuit , In Proceedings of the 19th ACM International Conference on Information and Knowledge Management ( ACM CIKM ) , 2010 .", "label": "", "metadata": {}, "score": "50.27369"}
{"text": "SAM models documents as points on a high- dimensional spherical manifold , and is capable of representing negative word- topic correlations and word presence / absence , unlike models with multinomial document likelihood , such as LDA .In this paper , we evaluate SAM as a topic browser , focusing on its ability to model \" negative \" topic features , and also as a dimensionality reduction method , using topic proportions as features for difficult classification tasks in natural language processing and computer vision .", "label": "", "metadata": {}, "score": "50.329254"}
{"text": "The authors found that there was not an effective thresholding mechanism for AdaBoost , but with decision lists , by only using features with high confidences , it was possible to prune the results to 95 % precision and 7 % recall .", "label": "", "metadata": {}, "score": "50.41328"}
{"text": "Additionally , many of these documents precede consistent orthographic conventions , making the task even harder .We extend the state - of - the - art historical OCR model of Berg - Kirkpatrick et al .( 2013 ) to handle word - level code - switching between multiple languages .", "label": "", "metadata": {}, "score": "50.59581"}
{"text": "Graph - based manifold - ranking methods have been successfully applied to topic - focused multi - document summarization .This paper further proposes to use the multi - modality manifold - ranking algorithm for extracting topic - focused summary from multiple documents by considering the within - document sentence ... \" .", "label": "", "metadata": {}, "score": "50.622864"}
{"text": "The output of this model well summarizes topics in text , maps a topic on the network , and discovers topical communities .With concrete selection of a topic model and a graph - based regularizer , our model can be applied to text mining problems such as author - topic analysis , community discovery , and spatial text mining .", "label": "", "metadata": {}, "score": "50.654915"}
{"text": "Inferring functional modules of protein families with probabilistic topic models , BMC Bioinformatics , 12:141 , 2011 .Lidong Bing and Wai Lam .Investigation of Web Query Refinement via Topic Analysis and Learning with Personalization , In Proceedings of the ACM SIGIR Workshop on Query Representation and Understanding , 2011 .", "label": "", "metadata": {}, "score": "50.676853"}
{"text": "Only 10,000 random conversations are used , and scaling the models to the entire corpus is left for future work .The authors introduce three models , the EM Conversation model , the Conversation+Topic model , and the Bayesian Conversation model , the second being an extension of the first .", "label": "", "metadata": {}, "score": "50.76358"}
{"text": "A hidden topic - based framework toward building applications with short Web documents , IEEE Transactions on Knowledge and Data Engineering ( IEEE TKDE ) , Vol.23 , No.7 , 2011 .C. Lin et al .Weakly - supervised Joint Sentiment - Topic Detection from Text , IEEE Transactions on Knowledge and Data Engineering ( IEEE TKDE ) , to appear .", "label": "", "metadata": {}, "score": "50.802948"}
{"text": "The evaluation is done on different topic numbers and the results turn out to be inferior to the state - of - art .In general , this paper provides an elegant generative model and shows that more knowledge and sense specific features are needed in order to do better in WSD .", "label": "", "metadata": {}, "score": "51.054977"}
{"text": "Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation .KDD ( 2013 ) .Inference .David Hall , Daniel Jurafsky , Christopher D. Manning .Studying the History of Ideas Using Topic Models .EMNLP ( 2008 ) .Bibliometrics .", "label": "", "metadata": {}, "score": "51.332"}
{"text": "No third - party scientific libraries are required and all needed special functions are implemented and included .Method for analyzing group decision making based on the Author - Topic Model .Incorporates temporal information to generate directed graphs based upon topic models .", "label": "", "metadata": {}, "score": "51.3944"}
{"text": "Improving question recommendation by exploiting information need , In Proceedings of the 49thAnnual Meeting of the Association for Computational Linguistics ( ACL ) , 2011 .Xin Zhao et al .Comparing Twitter and traditional media using topic models .In Proceedings of the 33rd European Conference on Information Retrieval ( ECIR ) , 2011 .", "label": "", "metadata": {}, "score": "51.40821"}
{"text": "For evaluation , the authors used ACM - SIGIR conferences from 2000 - 2007 .They compared their model with independent PLSA for a window for every time .The difference was the initialization .While the current model used the MAP from the previous computation , the independent PLSA was initialized randomly .", "label": "", "metadata": {}, "score": "51.55484"}
{"text": "Theory .[ BibTeX ] .Replaces the standard multinomial distribution over topics with a Dirichlet - compound Multinomial ( DCM ) .The widely - reported Twitter dialects paper .Topics combine a word distribution with a bivariate normal over latitude and longitude .", "label": "", "metadata": {}, "score": "51.56365"}
{"text": "Learning topical transition probabilities in click through data with regression models , In Proceedings of 13th International Workshop on the Web and Databases ( WebDB ) , 2010 .Piotr Mirowski et al .Dynamic Auto - Encoders for Semantic Indexing , In Proceedings of the NIPS Workshop on Deep Learning and Unsupervised Feature Learning , 2010 .", "label": "", "metadata": {}, "score": "51.74717"}
{"text": "On these lines I have been playing with text mining and modeling tools in R , mainly via the excellent tm package .One of the techniques I have been playing around with is Latent Dirichlet Allocation .This is a generative modeling approach , allowing one to associate a document ( composed of a set of words ) with a \" topic \" .", "label": "", "metadata": {}, "score": "51.861748"}
{"text": "They formulated this problem of Tense Disambiguation , which is related to WSD but not exactly the same .The influence of the problem is manifold .It can be applied to help many other problem such as machine translation , text entailment and so on .", "label": "", "metadata": {}, "score": "51.91239"}
{"text": "Weisi read about a prototype model for coarse - to - grain learning with regards to multi - class classification .The model was pipelined with sets of independent classifiers trained at each level and since it does not use the overlap in the confusion lists on certain labels , there is the possibility for a better estimation of the feature weights .", "label": "", "metadata": {}, "score": "51.957687"}
{"text": "The paper seems to promote a more human - based cognition approach to the problem and NLP in general , targeting the information that currently remains difficult to extract .I feel Chinese may have just been chosen to demonstrated maybe the more interesting or extremal case of the tense classification problem .", "label": "", "metadata": {}, "score": "51.98194"}
{"text": "I myself read the paper \" A Topic Model for Word Sense Disambiguation \" by Jordan Boyd - Graber , David Blei , and Xiaojin Zhu in EMNLP 2007 .The paper is not exactly about the evaluation of topic models , but explores as an application of the semantic influence of the topics obtained by the topic models .", "label": "", "metadata": {}, "score": "52.013687"}
{"text": "Stopwords and Stylometry : A Latent DirichletAllocation Approach , In Proceedings of the NIPS Workshop on Applications of Topic Models : Text and Beyond , Canada , 2009 .Kai Tian et al .Using Latent Dirichlet Allocation for automatic categorization of software , In Proceedings 6th IEEE Working Conference on Mining Software Repositories , Canada 2009 .", "label": "", "metadata": {}, "score": "52.124573"}
{"text": "Then there follows a proof of relating this to a median partitioning problem and intra - class variance criterion .Seven different consensus functions are studied , where evaluation is done by measuring the mis - assignment rate of the consensus partition ( the true known number of clusters is made available ) .", "label": "", "metadata": {}, "score": "52.183804"}
{"text": "The paper describes meta - clustering which is mentioned in the focus paper .It also has lots of similarities to the focus paper and it seems that focus paper has taken some from this paper , eg .the similarity metric between clusterings .", "label": "", "metadata": {}, "score": "52.23213"}
{"text": "The paper explores 15 different automatic measures utilizing WordNet , Wiki , Google and a bunch of external methods .Dong suggests that more error analysis could be given and how well the method generalizes over domains is not known .The comparison on the ratings are also suggested to be not entirely convincing .", "label": "", "metadata": {}, "score": "52.31511"}
{"text": "One of the techniques I have been playing around with is Latent Dirichlet Allocation .This is a generative modeling approach , allowing one to associate a document ( composed of a set of words ) with a \" topic \" .", "label": "", "metadata": {}, "score": "52.433243"}
{"text": "One of the techniques I have been playing around with is Latent Dirichlet Allocation .This is a generative modeling approach , allowing one to associate a document ( composed of a set of words ) with a \" topic \" .", "label": "", "metadata": {}, "score": "52.433243"}
{"text": "This probably could be because of the length of the conversations and also that transitions with probability less than 0.1 are not shown .They are however not discussed .A comparison metric based on the ordering of the posts is proposed .", "label": "", "metadata": {}, "score": "52.702858"}
{"text": "Chang et al .NIPS 2009 Related Paper : Topic Evolution in a Stream of Documents .A e Gohr et al .The related paper addresses the problem of changing nature of document collections .It tries to adapt the feature space and underlying document model .", "label": "", "metadata": {}, "score": "52.76305"}
{"text": "I have read the paper \" A Sequential Model for Multi - Class Classification \" by Yair Even - Zohar and Dan Roth .The paper disusses a prototype model for coarse - to - grain learning , but in the domain of multi - class classification , instead of structured prediction .", "label": "", "metadata": {}, "score": "52.791615"}
{"text": "Furthermore , in an unsupervised setting like the one I 've described here , fishing for a correlation between ( some set of ) properties and groupings of molecules is probably not the way to go .A large part of today 's meeting was about Labeled LDA .", "label": "", "metadata": {}, "score": "52.94446"}
{"text": "The related paper introduces a polylingual topic model that discovers topics aligned across multiple languages .They look at documents that are translated word - for - word via the EuroParl corpus as well as those that are not directly translated but very likely to be about similar concepts ( wikipedia articles in various languages ) .", "label": "", "metadata": {}, "score": "53.085022"}
{"text": "Evalation is done by comparing the probability of the held out test data using Chib 's estimator .Training is performed on a set of 10000 randomly sampled conversations with 3 - 6 posts .An interpretation of the model based on the 10-act dialogue model is presented .", "label": "", "metadata": {}, "score": "53.251175"}
{"text": "A good way to disambiguate tense could definitely help .Besides the focused paper , people have read related papers including machine learning techniques , Word Sense Disambiguation methods , linguistics , translation , and temporal event ordering which is semantically related .", "label": "", "metadata": {}, "score": "53.40058"}
{"text": "This approach proves effective in supervised , unsupervised , and latent variable settings .Elena Erosheva , Stephen Fienberg , John Lafferty .Mixed Membership Models of Scientific Publications .PNAS ( 101 )2004 pp .5220 - 5227 .Bibliometrics .", "label": "", "metadata": {}, "score": "53.40895"}
{"text": "Thresholding was also used to trade recall for precision .In the end , the specific syntactic features along with AdaBoost performed the best , although AdaBoost has no effective thresholding mechanism .Dhananjay read about imposing global constraints over local pairwise order decisions .", "label": "", "metadata": {}, "score": "53.693474"}
{"text": "It aims to distill the most important information from a set of documents to generate a compressed summary .Given a sentence graph generated from a set of documents where vertices represent sentences and edges indic ... \" .Multi - document summarization has been an important problem in information retrieval .", "label": "", "metadata": {}, "score": "53.72058"}
{"text": "It does n't seem their features itself are very innovative or was their focus .Furthermore , it 's not clear how innovative the used classifier combinations were in general ( not restricted to WSD ) .The focus paper was interesting , but because I 'm not familiar with some of the approaches their work builds on ( sequential model , SNOW ) , some parts were not totally clear to me .", "label": "", "metadata": {}, "score": "53.790676"}
{"text": "The focus paper for this week , Characterizing Microblogs with Topic Models , uses a variation of LDA , Labeled LDA , to analyze the topics present in Twitter , and then uses these for the tasks of rating tweets and recommending users to follow .", "label": "", "metadata": {}, "score": "53.894268"}
{"text": "Related paper : Modeling Consensus : Classifier Combination for Word Sense Disambiguation , Radu Florian and David Yarowksy Focus paper : Tense Sense Disambiguation : a New Syntactic Polysemy Task , Roi Reichart and Ari Rappoport .The related paper by Florian and Yarowsky describe experiments with different methods of combining classifiers to improve the performance on the word sense disambiguation task .", "label": "", "metadata": {}, "score": "54.03772"}
{"text": "A hybrid unsupervised image re - ranking approach with latent topic contents , In Proceedings of the ACM International Conference on Image and Video Retrieval ( ACM CIVR ) , 2010 .Trevor Fountain and Mirella Lapata .Meaning representation in natural language categorization , In Proceedings of the Annual Meeting of the Cognitive Science Society ( COGSCI ) , 2010 .", "label": "", "metadata": {}, "score": "54.084114"}
{"text": "These functions each combine two terms , one which encourages the summary to be representative of the corpus , and the other which positively rewards diversity .Critically , our functions are monotone nondecreasing and su ... \" .We design a class of submodular functions meant for document summarization tasks .", "label": "", "metadata": {}, "score": "54.129997"}
{"text": "Link Analysis PageRank [ 22 ] and HITS [ 9 ] are two popular algorithms for link analysis between web pages and they have been success ... . byJie Tang , Limin Yao , Dewei Chen - SIAM International Conference Data Mining , 2009 . \" ...", "label": "", "metadata": {}, "score": "54.20576"}
{"text": ", 2004 ) to represent content specificity as a hierarchy of topic vocabulary distributions .At the task of producing generic DUC - style summaries , HIERSUM yields state - of - the - art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al .", "label": "", "metadata": {}, "score": "54.258812"}
{"text": "The final point of discussion was what role Topic models play .They are not useful as a pre - processing or a decision making criterion for any other tasks .However , it was accepted that with the advent of topic models , the research in generative graphical models matured .", "label": "", "metadata": {}, "score": "54.55892"}
{"text": "Specifically , we present an application to information retrieval in which documents are modeled as paths down a random tree , and the preferential attachment dynamics of the nCRP leads to clustering of documents according to sharing of topics at multiple levels of abstraction .", "label": "", "metadata": {}, "score": "54.803375"}
{"text": "Our results show average relative character error reductions of 14 % across a variety of historical texts .ML ID : 312 .We introduce tiered clustering , a mixture model capable of accounting for varying degrees of shared ( context - independent ) feature structure , and demonstrate its applicability to inferring distributed representations of word meaning .", "label": "", "metadata": {}, "score": "55.03504"}
{"text": "It would be great if the results turned out to meet people 's expectations , such as CTM performed the best in the tasks and so on .It is suggested a lot of work could be done to improve the focus paper .", "label": "", "metadata": {}, "score": "55.044205"}
{"text": "We demonstrate the effectiveness of our algorithm through experiments on synthetic data as well as subsets of 20-Newsgroups and EachMovie datasets .Tools . \" ...We present an exploration of generative probabilistic models for multi - document summarization .", "label": "", "metadata": {}, "score": "55.048416"}
{"text": "The focus paper introduces a task called tense sense disambiguation , where given a concrete tense syntactic form in a sentence , the objective is to select the correct sense among a given set of possible senses .As an experiment , an English syntactic sense dictionary was compiled and then used to annotate 3000 sentences from the British National Corpus .", "label": "", "metadata": {}, "score": "55.14244"}
{"text": "Michael Welch et al .Search Result Diversity for Information Queries .In Proceedings of The 20th International World Wide Web Conference ( WWW 2011 ) , Hyderabad , India .V. Suresh et al .A Non - syntactic Approach for Text SentimentClassi?cation with Stopwords , In Proceedings of The 20th International World Wide Web Conference ( WWW 2011 ) , Hyderabad , India .", "label": "", "metadata": {}, "score": "55.21769"}
{"text": "The Markov Random Walk model has been recently exploited for multi - document summarization by making use of the link relationships between sentences in the document set , under the assumption that all the sentences are indistinguishable from each other .However , a given document set usually covers a f ... \" .", "label": "", "metadata": {}, "score": "55.34703"}
{"text": "They view their upper bound as the inter - annotator agreement .It seems a bit strange that some of the methods , have an even higher score than that , and a lot of the methods are very close to the upperbound score .", "label": "", "metadata": {}, "score": "55.586452"}
{"text": "We extend latent Dirichlet allocation model by replacing the unigram word distributions with a factored representation conditioned on both the topic and the structure .In the resultant model each topic is equivalent to a set of unigrams , reflecting the structure a word is in .", "label": "", "metadata": {}, "score": "55.676052"}
{"text": "the notion that humans make use of different categorization systems for different kinds of generalization tasks - and can be applied to Web - scale corpora .Using these models , natural language systems will be able to infer a more comprehensive semantic relations , in turn improving question answering , text classification , machine translation , and information retrieval .", "label": "", "metadata": {}, "score": "55.986107"}
{"text": "I feel it matters here on how to calculate the probability of accident in the Fleiss Kappa .Pre - meeting ( Dong Nguyen ) .Related paper : Labeled LDA : A supervised topic model for credit attribution in multi - label corpora Focus paper : Characterizing Microblogs with Topic Models .", "label": "", "metadata": {}, "score": "55.98964"}
{"text": "There are some good examples illustrated in the link on Brendan 's post .It seems the relevance of the theory and details in terms of using it or comparing it in some applicable manner are still unclear .I read Goldberg 2003 , an overview of construction grammar .", "label": "", "metadata": {}, "score": "55.99421"}
{"text": "They first review two existing approaches to estimate proportions : 1 ) sample a subset and hand label them to estimate the category proportions , 2 ) Do individual classification , and aggregate the predictions to calculate a proportion .They explain why both approaches have problems , and then propose two new methods .", "label": "", "metadata": {}, "score": "56.274014"}
{"text": "For classifier , they used Decision List and AdaBoost .For temporal model , Dhananjay read a paper about ordering events based on temporal relations .The justifications of superiority of Integer linear programming against Greedy methods are discussed .The authors used SVM based transitivity to obtain output as pairwise ordering of temporal events .", "label": "", "metadata": {}, "score": "56.359158"}
{"text": "Nathalie Camelin et al .Unsupervised concept annotation using latent Dirichlet allocation and segmental methods , In Proceedings of the EMNLP Workshop on Unsupervised Learning in NLP , 2011 .Avishay Livne et al .Networks and language in the 2010 election , In Proceedings of the 4th Annual Political Networks Conference , 2011 .", "label": "", "metadata": {}, "score": "56.375793"}
{"text": "Slice sampling is also applied .To evaluate the set of generated dialogue acts , the authors examine both qualitative and quantitive evaluations .The qualitative evaluation really only focuses on the Conversation+Topic model , and they go through a 10-act model , showing the probability on the transitions between dialogue acts .", "label": "", "metadata": {}, "score": "56.522102"}
{"text": "As I noted above , each topic is really a set of \" words \" that have a higher probability of being generated by that topic .In the case of this model we obtain the following top 4 fragments associated with each topic ( most likely fragments are at the top of the table ) : .", "label": "", "metadata": {}, "score": "56.673374"}
{"text": "As I noted above , each topic is really a set of \" words \" that have a higher probability of being generated by that topic .In the case of this model we obtain the following top 4 fragments associated with each topic ( most likely fragments are at the top of the table ) : .", "label": "", "metadata": {}, "score": "56.673374"}
{"text": "As I noted above , each topic is really a set of \" words \" that have a higher probability of being generated by that topic .In the case of this model we obtain the following top 4 fragments associated with each topic ( most likely fragments are at the top of the table ) : .", "label": "", "metadata": {}, "score": "56.673374"}
{"text": "As I noted above , each topic is really a set of \" words \" that have a higher probability of being generated by that topic .In the case of this model we obtain the following top 4 fragments associated with each topic ( most likely fragments are at the top of the table ) : .", "label": "", "metadata": {}, "score": "56.673374"}
{"text": "As I noted above , each topic is really a set of \" words \" that have a higher probability of being generated by that topic .In the case of this model we obtain the following top 4 fragments associated with each topic ( most likely fragments are at the top of the table ) : .", "label": "", "metadata": {}, "score": "56.673374"}
{"text": "It seems the paper does a decent job at planting a first step in terms of unsupervised dialogue act tagging .The work done does n't seem overly complex , but the observations and data they collected seems like they could be useful for at least a couple more runs .", "label": "", "metadata": {}, "score": "56.94014"}
{"text": "The goal of this metric , called variation of information ( VI ) , is to be intuitive as well as to possess desirable mathematical characteristics .This function is a true metric , which makes possible more types of reasoning about the space of clusterings .", "label": "", "metadata": {}, "score": "56.955307"}
{"text": "This also serves to be the one of main motivations of this paper and the focus paper .Pre - meeting ( Dong Nguyen ) .Related paper A Method of Automated Nonparametric Content Analysis for Social Science , Daniel J. Hopkins , Gary King Focus paper : General Purpose Computer - Assisted Clustering and Conceptualization , Justin Grimmer and Gary King .", "label": "", "metadata": {}, "score": "57.305172"}
{"text": "I have submitted a paper on this , and now I am not sure if I am going to get a rejection ...This related paper is nice in that it has conducted numerous experiments to prove the model to be useful in certain situations .", "label": "", "metadata": {}, "score": "57.320488"}
{"text": "Starting with maximum likelihood , a posteriori and Bayesian estimation , central concepts like conjugate distributions and Bayesian networks are reviewed .As an application , the model of latent Dirichlet allocation ( LDA ) is explained in detail with a full derivation of an approximate inference algorithm based on Gibbs sampling , including a discussion of Dirichlet hyperparameter estimation .", "label": "", "metadata": {}, "score": "57.605076"}
{"text": "The most common unsupervised learning task is clustering , i.e. grouping instances into a discovered set of categories containing similar instances .Self - organizing maps in addition visualize the topology of the clusters on a map .Our work in this area includes applications on lexical semantics , topic modeling , and discovering latent class models , as well as methods for laterally connected , hierarchical , sequential - input , and growing self - organizing maps .", "label": "", "metadata": {}, "score": "57.72361"}
{"text": "Roi Reichart and Ari Rappoport .EMNLP 2010 Additional Reading : Syntactic features for high precision Word Sense Disambiguation .David Martinez et al .COLING 2002 .This paper explores novel syntactic features for use in improving precision in Word Sense Disambiguation .", "label": "", "metadata": {}, "score": "57.904823"}
{"text": "In this proposal I will outline a family of probabilistic models capable of accounting for the rich organizational structure found in human language that can predict contextual variation , selectional preference and feature - saliency norms to a much higher degree of accuracy than previous approaches .", "label": "", "metadata": {}, "score": "57.928684"}
{"text": "We demonstrate the applicability of tiered clustering , highlighting particular cases where modeling shared structure is beneficial and where it can be detrimental .ML ID : 252 .Cross - cutting Models of Distributional Lexical Semantics [ Details ] [ PDF ] [ Slides ] Joseph S. Reisinger June 2010 .", "label": "", "metadata": {}, "score": "57.949223"}
{"text": "The algorithm also caters to the task structure by allowing the restriction of the possible labels to each concrete syntactic form .The classifier outperforms the MFS baseline in all three conditions when the ASF ( abstract syntactic form ) type is known , unknown , or given by a simple rule - based classifier .", "label": "", "metadata": {}, "score": "58.037697"}
{"text": "In order to respond to increasing demand for natural language interfaces - and provide meaningful insight into user query intent - fast , scalable lexical semantic models with flexible representations are needed .Human concept organization is a rich epiphenomenon that has yet to be accounted for by a single coherent psychological framework : Concept generalization is captured by a mixture of prototype and exemplar models , and local taxonomic information is available through multiple overlapping organizational systems .", "label": "", "metadata": {}, "score": "58.047577"}
{"text": "The method of using the first few lines as the summary of the topic was challenged .It was concluded that the reader should have access to the complete article , should it be wanted .The behavior of the evaluation for models that generate n - grams was put forward .", "label": "", "metadata": {}, "score": "58.188705"}
{"text": "Experiments are combined with the consensus functions and applied in different dimensions for different values of k. .The idea in this paper seems pretty intuitive which probably explains why it was published some time ago .Theoretically it makes sense that you should be able form a more optimal cluster by combining multiple clusterings of a set of data , but because you could also telescope the argument , the practicality and marginal benefits seem limited in significance .", "label": "", "metadata": {}, "score": "58.20923"}
{"text": "One is telicity , which specifies whether the verb can be bound within a certain time frame .Another is punctuality , which says whether a verb can be associated with a single point event .The third is temporal ordering , which describes one of six relationships between two invents , namely precession , succession , inclusion , subsumation , overlap , and none .", "label": "", "metadata": {}, "score": "58.25407"}
{"text": "Roi Reichart and Ari Rappoport .EMNLP 2010 .Because the focus paper proposes a new task , it was less clear what work was going to be relevant .People read papers about a diverse set of topics , including WSD , Machine Learning and linguistics .", "label": "", "metadata": {}, "score": "58.318913"}
{"text": "Given a sentence graph generated from a set of documents where vertices represent sentences and edges indicate that the corresponding vertices are similar , the extracted summary can be described using the idea of graph domination .In this paper , we propose a new principled and versatile framework for multi - document summarization using the minimum dominating set .", "label": "", "metadata": {}, "score": "58.41751"}
{"text": "Dhananjay read a paper about the adapting the PLSA over time .Brendan asked about how exactly the perplexity on the test data is calculated , in other words how to fix the document distributions .People have pointed out various points that are confusing in focus paper ( eg .", "label": "", "metadata": {}, "score": "58.449234"}
{"text": "Focus Paper : Reading tea leaves : How humans interpret topic models .Chang et al .NIPS 2009 Related Paper : Evaluation Methods for Topic Models .Wallach et al .ICML 2009 .This paper explores various intrinsic methods of evaluation for topic models , focusing on LDA .", "label": "", "metadata": {}, "score": "58.478218"}
{"text": "Posterior predictive checks are useful in detecting lack of fit in topic models and identifying which metadata - enriched models might be useful .Claudiu Musat , Julien Velcin , Stefan Trausan - Matu , Marian - Andrei Rizoiu .Improving Topic Evaluation Using Conceptual Knowledge .", "label": "", "metadata": {}, "score": "58.69501"}
{"text": "Daniel read a paper about various statistical measures for topic models and suggested the comparison does not make enough sense .Alan read a paper about applying topic models to polylingual case to help machine translation , and it is suggested that machine translation is not helped a lot by topic models so far .", "label": "", "metadata": {}, "score": "58.711212"}
{"text": "GibbsLDA++ : A C / C++ Implementation of Latent Dirichlet Allocation .GibbsLDA++ is a C / C++ implementation of Latent Dirichlet Allocation ( LDA ) using Gibbs Sampling technique for parameter estimation and inference .It is very fast and is designed to analyze hidden / latent topic structures of large - scale datasets including large collections of text / Web documents .", "label": "", "metadata": {}, "score": "58.73696"}
{"text": "At first sight , it 's an interesting method that identifies groupings in an unsupervised manner .Of course , one could easily run k - means or any of the hierarchical clustering methods to achieve the same result .And then , how does one infer the meaning of a topic from fragments ?", "label": "", "metadata": {}, "score": "59.015465"}
{"text": "At first sight , it 's an interesting method that identifies groupings in an unsupervised manner .Of course , one could easily run k - means or any of the hierarchical clustering methods to achieve the same result .And then , how does one infer the meaning of a topic from fragments ?", "label": "", "metadata": {}, "score": "59.015465"}
{"text": "At first sight , it 's an interesting method that identifies groupings in an unsupervised manner .Of course , one could easily run k - means or any of the hierarchical clustering methods to achieve the same result .And then , how does one infer the meaning of a topic from fragments ?", "label": "", "metadata": {}, "score": "59.015465"}
{"text": "At first sight , it 's an interesting method that identifies groupings in an unsupervised manner .Of course , one could easily run k - means or any of the hierarchical clustering methods to achieve the same result .And then , how does one infer the meaning of a topic from fragments ?", "label": "", "metadata": {}, "score": "59.015465"}
{"text": "At first sight , it 's an interesting method that identifies groupings in an unsupervised manner .Of course , one could easily run k - means or any of the hierarchical clustering methods to achieve the same result .And then , how does one infer the meaning of a topic from fragments ?", "label": "", "metadata": {}, "score": "59.015465"}
{"text": "Transitivity .A soft classification ( before , after and unknown ) is done using SVM and confidence scores are calculated for each pairwise events .The objective function is maximized subject to the constraints that only one ( before , after , and unknown ) is chosen .", "label": "", "metadata": {}, "score": "59.027657"}
{"text": "Outputs .Outputs of Gibbs sampling estimation of GibbsLDA++ include the following files : . others . phi . theta . tassign . twords . in which : . : is the name of a LDA model corresponding to the time step it was saved on the hard disk .", "label": "", "metadata": {}, "score": "59.051666"}
{"text": "They also display word lists and example posts for each Dialogue Act which are fairly convincing .For quantitative evaluation the paper introduces a new task of conversation ordering , that is given a random set of conversations , all permutations of the conversations are generated and the probability of each permutation is evaluated as if it were an unseen conversation .", "label": "", "metadata": {}, "score": "59.081562"}
{"text": "In these experiments , SAM consistently outperforms existing models .ML ID : 248 .Spherical Topic Models [ Details ] [ PDF ] Joseph Reisinger , Austin Waters , Bryan Silverthorn , and Raymond Mooney In NIPS'09 workshop : Applications for Topic Models : Text and Beyond , 2009 .", "label": "", "metadata": {}, "score": "59.10119"}
{"text": "Perplexity is sometimes far superior to other methods .Chris Ding , Tao Li , Wei Peng .On the Equivalence between Non - negative Matrix Factorization and Probabilistic Latent Semantic Indexing .Computational Statistics and Data Analysis ( 52 ) 2008 pp .", "label": "", "metadata": {}, "score": "59.19133"}
{"text": "We present an exploration of generative probabilistic models for multi - document summarization .Beginning with a simple word frequency based model ( Nenkova and Vanderwende , 2005 ) , we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way .", "label": "", "metadata": {}, "score": "59.249702"}
{"text": "The meeting today concentrated on issues involving the evaluation of the topic models .The discussion has been mainly revolved around the focus paper , which presents tasks to demonstrate the semantics of the topics obtained from the topic models .The inter - rater agreement , if is there , would be nice , but as suggested by Noah , the statistical significance of the results is still valid .", "label": "", "metadata": {}, "score": "59.356815"}
{"text": "The held - out documents are either entirely held out , or used in a document completion setting , where only the latter half of each test document is held out .For the completely held - out setting , they compare two commonly used methods , harmonic mean sampling and annealed importance sampling , to methods that had not previously been used for topic model evaluation , Chib - style estimation and left - to - right evaluation .", "label": "", "metadata": {}, "score": "59.681602"}
{"text": "When no thresholding , they found that the specific syntactic features helped precision dramatically , but the subcategorization features led to a higher F score , with the combination of the two being slightly better than the subcategorization features .They further find that AdaBoost works significantly better than Decision Lists when syntactic features are taken into account , although not with the basic feature set .", "label": "", "metadata": {}, "score": "59.706696"}
{"text": "Moreover , most of existing work assumes that documents related to the query only talks about one topic .Unfortunately , statistics show that a large portion of summarization tasks talk about multiple topics .In this paper , we try to break limitations of the existing methods and study a new setup of the problem of multi - topic based query - oriented summarization .", "label": "", "metadata": {}, "score": "59.769657"}
{"text": "Social media .Merging tweets based on hashtags and imputed hashtags improves topic modeling .In this paper , we formally define the problem of topic modeling with network structure ( TMN ) .We propose a novel solution to this problem , which regularizes a statistical topic model with a harmonic regularizer based on a graph structure in the data .", "label": "", "metadata": {}, "score": "59.927742"}
{"text": "Related paper : Unsupervised Modeling of Twitter Conversations ; Alan Ritter , Colin Cherry , Bill Dolan , NAACL 2010 .Focus paper : Characterizing Microblogs with Topic Models .The related paper I read this week proposes an unsupervised method for discovering dialogue structure or \" dialogue acts \" in twitter conversations .", "label": "", "metadata": {}, "score": "60.218098"}
{"text": "Critically , our functions are monotone nondecreasing and submodular , which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality .When evaluated on DUC 2004 - 2007 corpora , we obtain better than existing state - of - art results in both generic and query - focused document summarization .", "label": "", "metadata": {}, "score": "60.35584"}
{"text": "This seems to suggest an explosion of classifiers if not well engineered .During training , the classifiers are trained based the pruned output space of the previous classifier and instances relevant to the pruned output space .The authors provide a proof on bigger output space induce more error and smaller output space reduce the training error .", "label": "", "metadata": {}, "score": "60.7676"}
{"text": "Conf . on Data Mining 2003 .This paper presented several interesting points on the topic .First it sets up a formal definition of the combination clustering problem , which basically says given some set of clusterings for a set of data , find a cluster which is some combination of one or more of those sets that yields a \" better \" cluster .", "label": "", "metadata": {}, "score": "60.816376"}
{"text": "They 're like the all the little features you get in MT or other tasks .It 's kind of like lexical semantics , except for more than just words .It 's kind of dissatisfying , in that there 's no theory how words combine into meaning .", "label": "", "metadata": {}, "score": "60.902138"}
{"text": "P2Prec : a P2P recommendation system for large - scale data sharing , Transactions on Large - Scale Data and Knowledge - Centered System III , 2011 .Hiroyuki Koga and Tadahiro Taniguchi .Developing a User Recommendation Engine on Twitter Using Estimated Latent Topics , In Proceedings of the Human Computer Interaction ( HCI ) , 2011 .", "label": "", "metadata": {}, "score": "61.00836"}
{"text": "( R.N. ) .Early paper on parallel implementations of variational EM for LDA .( R.N. ) .In this paper , we try to leverage a large - scale and multilingual knowledge base , Wikipedia , to help effectively analyze and organize Web information written in different languages .", "label": "", "metadata": {}, "score": "61.217106"}
{"text": "I think the paper gave a nice explanation of the goals and alternatives .Furthermore , it was interesting because I 've never thought about estimating proportions instead of individual classifications , and now know why you would want to use other methods instead of just aggregation individual classification predictions .", "label": "", "metadata": {}, "score": "61.335133"}
{"text": "I do n't see how it explains or makes testable hypotheses or predictions .Or even how you would use it to help design a grammar or NLP features , even .I do n't see how you could hope to compare it to LFG or CCG or HPSG or minimalism or what have you .", "label": "", "metadata": {}, "score": "61.44982"}
{"text": "We address the challenge of evaluating the emergent model with a qualitative visualization and an intrinsic conversation ordering task .This work is inspired by a corpus of 1.3 million Twitter conversations , which will be made publicly available .This huge amount of data , available only because Twitter blurs the line between chatting and publishing , highlights the need to be able to adapt quickly to a new medium . by Alan Ritter , Oren Etzioni - In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics ( ACL , 2010 . \" ...", "label": "", "metadata": {}, "score": "61.459785"}
{"text": "We apply our method on five Wikipedia entity categories and compare our method with two baseline methods .Both quantitative evaluation based on human judgment and qualitative comparison demonstrate the effectiveness and advantages of our method . \" ...We introduce a method to learn a mixture of submodular \" shells \" in a large - margin setting .", "label": "", "metadata": {}, "score": "61.491646"}
{"text": "Going one level up , one can consider fragments as words , which can be joined together to form larger structures ( with the linguistic analog being sentences ) .In a talk I gave at the ACS sometime back I compared fragments with n - grams ( though LINGO 's are probably a more direct analog ) .", "label": "", "metadata": {}, "score": "61.53029"}
{"text": "Going one level up , one can consider fragments as words , which can be joined together to form larger structures ( with the linguistic analog being sentences ) .In a talk I gave at the ACS sometime back I compared fragments with n - grams ( though LINGO 's are probably a more direct analog ) .", "label": "", "metadata": {}, "score": "61.53029"}
{"text": "Cross - cultural analysis of blogs and forums with mixed - collection topic models , In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , 2009 .Makoto P. Kato .Rhythmixearch : searching for unknown music by mixing known music , In Proceedings of the 10th International Society for Music Information Retrieval Conference , 2009 .", "label": "", "metadata": {}, "score": "61.62379"}
{"text": "The outputs of GibbsLDA++ inference are almost the same as those of the estimation process except that the contents of those files are of the new data .For example , we want to estimate a LDA model for a collection of documents stored in file called models / casestudy / trndocs . dat and then use that model to do inference for new data stored in file models / casestudy / newdocs . dat .", "label": "", "metadata": {}, "score": "61.754143"}
{"text": "Ramage , D. et al .2010 .Related Paper : Beyond Microblogging : Conversation and Collaboration via Twitter .Honeycutt , C. and Herring , S. 2009 .The paper that I read presents an analysis of dialog on Twitter , and specifically the use of the @ symbol .", "label": "", "metadata": {}, "score": "61.7947"}
{"text": "It appears that the groups differentiate from each other in terms of the extreme values .Indeed plotting summary statistics for each group confirms this - in fact the median Z - score has a range of 0.05 and the mean Z - score a range of 0.11 across the six groups .", "label": "", "metadata": {}, "score": "61.80066"}
{"text": "It appears that the groups differentiate from each other in terms of the extreme values .Indeed plotting summary statistics for each group confirms this - in fact the median Z - score has a range of 0.05 and the mean Z - score a range of 0.11 across the six groups .", "label": "", "metadata": {}, "score": "61.80066"}
{"text": "It appears that the groups differentiate from each other in terms of the extreme values .Indeed plotting summary statistics for each group confirms this - in fact the median Z - score has a range of 0.05 and the mean Z - score a range of 0.11 across the six groups .", "label": "", "metadata": {}, "score": "61.80066"}
{"text": "It appears that the groups differentiate from each other in terms of the extreme values .Indeed plotting summary statistics for each group confirms this - in fact the median Z - score has a range of 0.05 and the mean Z - score a range of 0.11 across the six groups .", "label": "", "metadata": {}, "score": "61.80066"}
{"text": "It appears that the groups differentiate from each other in terms of the extreme values .Indeed plotting summary statistics for each group confirms this - in fact the median Z - score has a range of 0.05 and the mean Z - score a range of 0.11 across the six groups .", "label": "", "metadata": {}, "score": "61.80066"}
{"text": "Furthermore , in an unsupervised setting like the one I 've described here , fishing for a correlation between ( some set of ) properties and groupings of molecules is probably not the way to go .For example , atoms and bonds can be considered an \" alphabet \" for chemical structures .", "label": "", "metadata": {}, "score": "61.87755"}
{"text": "Furthermore , in an unsupervised setting like the one I 've described here , fishing for a correlation between ( some set of ) properties and groupings of molecules is probably not the way to go .Publications : Unsupervised Learning , Clustering , and Self - Organization .", "label": "", "metadata": {}, "score": "61.89922"}
{"text": "Most ChEMBL molecules have multiple activities associated with them as many are tested in multiple assays .To allow comparison we converted activities in a given assay to Z - scores , allow comparison of activitives across assays .After removal of a few extreme outliers we obtain : .", "label": "", "metadata": {}, "score": "62.124672"}
{"text": "Most ChEMBL molecules have multiple activities associated with them as many are tested in multiple assays .To allow comparison we converted activities in a given assay to Z - scores , allow comparison of activitives across assays .After removal of a few extreme outliers we obtain : .", "label": "", "metadata": {}, "score": "62.124672"}
{"text": "Most ChEMBL molecules have multiple activities associated with them as many are tested in multiple assays .To allow comparison we converted activities in a given assay to Z - scores , allow comparison of activitives across assays .After removal of a few extreme outliers we obtain : .", "label": "", "metadata": {}, "score": "62.124672"}
{"text": "Most ChEMBL molecules have multiple activities associated with them as many are tested in multiple assays .To allow comparison we converted activities in a given assay to Z - scores , allow comparison of activitives across assays .After removal of a few extreme outliers we obtain : .", "label": "", "metadata": {}, "score": "62.124672"}
{"text": "Most ChEMBL molecules have multiple activities associated with them as many are tested in multiple assays .To allow comparison we converted activities in a given assay to Z - scores , allow comparison of activitives across assays .After removal of a few extreme outliers we obtain : .", "label": "", "metadata": {}, "score": "62.124672"}
{"text": "# i.e. , the vocabulary size .# i.e. , the Gibbs sampling iteration at which the model was saved .Each line is a document and each column is a topic . ..tassign : This file contains the topic assignments for words in training data .", "label": "", "metadata": {}, "score": "62.193855"}
{"text": "However , a given document set usually covers a few topic themes with each theme represented by a cluster of sentences .The topic themes are usually not equally important and the sentences in an important theme cluster are deemed more salient than the sentences in a trivial theme cluster .", "label": "", "metadata": {}, "score": "62.23832"}
{"text": "Furthermore , in an unsupervised setting like the one I 've described here , fishing for a correlation between ( some set of ) properties and groupings of molecules is probably not the way to go .Archive for the ' cluster ' tag .", "label": "", "metadata": {}, "score": "62.297085"}
{"text": "What if the dimensions of this metric space is a set of weak learners , and the human observation is used to generate weights that create a decent clustering application ?I do n't have a particular example or application to illustrate this point , but perhaps somebody might want to help me out over here ?", "label": "", "metadata": {}, "score": "62.331425"}
{"text": "Because it accounts for the sequential behaviour of these acts , the learned mode ... \" .We propose the first unsupervised approach to the problem of modeling dialogue acts in an open domain .Trained on a corpus of noisy Twitter conversations , our method discovers dialogue acts by clustering raw utterances .", "label": "", "metadata": {}, "score": "62.391884"}
{"text": "Transistivity uses Integer LP while time expression normalization normalizes everything to a single timeline .The results show 1 - 2 % absolute increase in accuracy .Brendan read an overview of construction grammar .A construction is a paring of a meaning and form .", "label": "", "metadata": {}, "score": "62.504913"}
{"text": "Similarly , the model was saved at the 1200 th iteration is model-01200 .The model name of the last Gibbs sampling iteration is model - final . ..others: This file contains some parameters of LDA model , such as : . # i.e. , number of topics .", "label": "", "metadata": {}, "score": "62.670063"}
{"text": "One way around this is to develop a model on something like the ChEMBL dataset I used here and then apply that to smaller datasets .Obviously , this goes towards ideas of applicability - but given the size of ChEMBL , it may indeed \" cover \" many smaller datasets .", "label": "", "metadata": {}, "score": "62.722534"}
{"text": "One way around this is to develop a model on something like the ChEMBL dataset I used here and then apply that to smaller datasets .Obviously , this goes towards ideas of applicability - but given the size of ChEMBL , it may indeed \" cover \" many smaller datasets .", "label": "", "metadata": {}, "score": "62.722534"}
{"text": "One way around this is to develop a model on something like the ChEMBL dataset I used here and then apply that to smaller datasets .Obviously , this goes towards ideas of applicability - but given the size of ChEMBL , it may indeed \" cover \" many smaller datasets .", "label": "", "metadata": {}, "score": "62.722534"}
{"text": "One way around this is to develop a model on something like the ChEMBL dataset I used here and then apply that to smaller datasets .Obviously , this goes towards ideas of applicability - but given the size of ChEMBL , it may indeed \" cover \" many smaller datasets .", "label": "", "metadata": {}, "score": "62.722534"}
{"text": "One way around this is to develop a model on something like the ChEMBL dataset I used here and then apply that to smaller datasets .Obviously , this goes towards ideas of applicability - but given the size of ChEMBL , it may indeed \" cover \" many smaller datasets .", "label": "", "metadata": {}, "score": "62.722534"}
{"text": "Integer Linear Programming is used to find the optimal solution .This however , did n't find any advantage over the Timebank Corpus as the graph was very sparse and connected components were very few .To eliminate this , they create a transitive closure .", "label": "", "metadata": {}, "score": "62.749634"}
{"text": "I dunno if I should bother reading the book though .This week , I read Jointly Combining Implicit Constraints Improves Temporal Ordering by Nathanael Chambers and Dan Jurafsky .The paper proposes imposing global constraints over the local pairwise order decisions .", "label": "", "metadata": {}, "score": "62.754036"}
{"text": "During the meeting we also talked about calculating perplexity .There seems to be multiple ways of calculating this for topic models .The author of my related paper seems to be working on presenting topic to humans .His recent publications contain a lot of papers regarding topics such as visualization of topics , external evaluation , topic labeling etc .", "label": "", "metadata": {}, "score": "62.85118"}
{"text": "This library contains Java source and class files implementing the Latent Dirichlet Allocation ( single - threaded collapsed Gibbs sampling ) and Hierarchical Dirichlet Process ( multi - threaded collapsed variational inference ) topic models .The models can be accessed through the command - line or through a simple Java API .", "label": "", "metadata": {}, "score": "62.86012"}
{"text": "Existing work can be mainly classified into two categories : supervised method and unsupervised method .The former requires training examples , which makes the method limited to predefined domains .While the latter usually utilizes clustering algorithms to find ' centered ' sentences as the summary .", "label": "", "metadata": {}, "score": "62.89984"}
{"text": "Istvan Biro et al .Latent Dirichlet Allocation in Web Spam Filtering .In Proceedings of the Fourth International Workshop on Adversarial Information Retrieval on the Web , WWW2008 , April 2008 , Beijing , China .Dieu - Thu Le et al .", "label": "", "metadata": {}, "score": "63.20246"}
{"text": "Exploring English lexicon knowledge for Chinese sentiment analysis .In Proceedings of the CIPS - SIGHAN Joint Conference on Chinese Language Processing , 2010 , Beijing , China .Mingrong Liu et al .Predicting best answerers for new questions in community question answering , In Proceedings of the 11th international conference on Web - age information management , 2010 .", "label": "", "metadata": {}, "score": "63.41304"}
{"text": "Further the use of the @ symbol was strongly related to discourse acts involving interacting with other people .Focus paper : Characterizing Microblogs with Topic Models Related paper : Unsupervised Modeling of Twitter Conversations ; Alan Ritter , Colin Cherry , Bill Dolan , NAACL 2010 .", "label": "", "metadata": {}, "score": "63.710228"}
{"text": "A priori there is no reason to choose one over the other .Blei in his original paper used \" perplexity \" as a measure of the models generalizability ( smaller values are better ) .Wallach et al have discussed various approaches to evaluating topic models .", "label": "", "metadata": {}, "score": "63.750736"}
{"text": "A priori there is no reason to choose one over the other .Blei in his original paper used \" perplexity \" as a measure of the models generalizability ( smaller values are better ) .Wallach et al have discussed various approaches to evaluating topic models .", "label": "", "metadata": {}, "score": "63.750736"}
{"text": "A priori there is no reason to choose one over the other .Blei in his original paper used \" perplexity \" as a measure of the models generalizability ( smaller values are better ) .Wallach et al have discussed various approaches to evaluating topic models .", "label": "", "metadata": {}, "score": "63.750736"}
{"text": "A priori there is no reason to choose one over the other .Blei in his original paper used \" perplexity \" as a measure of the models generalizability ( smaller values are better ) .Wallach et al have discussed various approaches to evaluating topic models .", "label": "", "metadata": {}, "score": "63.750736"}
{"text": "A priori there is no reason to choose one over the other .Blei in his original paper used \" perplexity \" as a measure of the models generalizability ( smaller values are better ) .Wallach et al have discussed various approaches to evaluating topic models .", "label": "", "metadata": {}, "score": "63.750736"}
{"text": "Also , in the Tagged Web Page task , they independently select 3000 docs for cross validation , and this suggests leak of test data .Although the leak is the same for both of the two models , I feel it would be nice if they could clearly separate the training and testing data .", "label": "", "metadata": {}, "score": "63.852623"}
{"text": "Since human translators still outperform current automated systems , the idea behind this is to identify where effort should be focused for advancing automatic extraction methods .The paper also details how these gold - standards are generated from all the human annotations they collected .", "label": "", "metadata": {}, "score": "64.00075"}
{"text": "Information retrieval and search ( analyzing semantic / latent topic / concept structures of large text collection for a more intelligent information search ) .Document classification / clustering , document summarization , and text / web mining community in general .", "label": "", "metadata": {}, "score": "64.07344"}
{"text": "It is very useful to help users grasp the main information related to a query .Existing work can be mainly classified into two categories : supervised method and unsupervised method .T ... \" .Query - oriented summarization aims at extracting an informative summary from a document collection for a given query .", "label": "", "metadata": {}, "score": "64.37045"}
{"text": "Individual features or wavelet coefficients are marginally described by Dirichlet process ( DP ) mixtures , yielding the heavy - tailed marginal distributions characteristic of natural images .Dependencies between features are then captured with a hidden Markov tree , and Markov chain Monte Carlo methods used to learn models whose latent state space grows in complexity as more images are observed .", "label": "", "metadata": {}, "score": "64.63345"}
{"text": "Data - driven approach for ontology learning , In Proceedings of the 6th International Conference on Electrical Engineering , Computing Science and Automatic Control , 2009 .Sao Carlos et al .Towards the automatic learning of ontologies , In Proceedings of the 7th Brazilian Symposium in Information and Human Language Technology , 2009 .", "label": "", "metadata": {}, "score": "64.73562"}
{"text": "The MAP estimates of the previous window , remain as the current iteration estimates for the current window .For the current time , the documents that came before the time window are discarded , and so is the vocabulary that is not present in the current window .", "label": "", "metadata": {}, "score": "64.820114"}
{"text": "The second one estimates proportions directly without doing individual classification .The problem can be framed as a regression problem and the class proportions are the regression coefficients .Because of computational and sparsity issues , they sample subsets of words , and estimate it for each set .", "label": "", "metadata": {}, "score": "65.01208"}
{"text": "It was fairly comfortable to understand the basic ideas here due to my relative fluency in Chinese , and I wo n't go into the details on this blog post .The paper describes experiments using both CRF ( conditional random fields ) learning experiments and classification tree learning experiments .", "label": "", "metadata": {}, "score": "65.01866"}
{"text": "Solutions to Plato 's problem : The latent semantic analysis theory of acquisition , induction , and representation of knowledge .Rishabh Mehrotra , Scott Sanner , Wray Buntine , Lexing Xie .Improving LDA Topic Models for Microblogs via Tweet Pooling and Automatic Labeling .", "label": "", "metadata": {}, "score": "65.069305"}
{"text": "Experimental results on the DUC2001 and DUC2002 datasets demonstrate the good effectiveness of our proposed summarization models .The results also demonstrate that the ClusterCMRW model is more robust than the ClusterHITS model , with respect to different cluster numbers .Categories and Subject Descriptors : . ...", "label": "", "metadata": {}, "score": "65.22755"}
{"text": "A thorough introduction for those wanting to understand the mathematical basics of topic models .In addition to dividing the corpus between processors , this work divides the vocabulary into the same number of partitions , such that each processor works on both its own documents and its own words at each epoch .", "label": "", "metadata": {}, "score": "65.26327"}
{"text": "I 'm not sure what to think of the focus paper .Much of the paper builds on the four dimensions : substance , style , status and social .Although they seem to make sense , these were identified by interviewing a small and not representative group .", "label": "", "metadata": {}, "score": "65.27196"}
{"text": "-savestep : The step ( counted by the number of Gibbs sampling iterations ) at which the LDA model is saved to hard disk .The default value is 200 .-twords : The number of most likely words for each topic .", "label": "", "metadata": {}, "score": "65.57039"}
{"text": "-savestep : The step ( counted by the number of Gibbs sampling iterations ) at which the LDA model is saved to hard disk .The default value is 200 .-twords : The number of most likely words for each topic .", "label": "", "metadata": {}, "score": "65.57039"}
{"text": "Set matching metrics fail on cluster completeness and rag bag as the bias is towards small clusters .Entropy based methods fail generally fail on rag bag .Pair counting satisfy both homogeneity and completeness , but do n't address rag bag and cluster size vs. quantity .", "label": "", "metadata": {}, "score": "65.78412"}
{"text": "Supposing that we are now at the home directory of GibbsLDA++ , We will execute the following command to estimate LDA model from scratch : .$ src / lda -est -alpha 0.5 -beta 0.1 -ntopics 100 -niters 1000 -savestep 100 -twords 20 -dfile models / casestudy / trndocs . dat .", "label": "", "metadata": {}, "score": "65.83899"}
{"text": "First , one could look at the structural homogenity of the molecules assigned to topics .Another way to evaluate the topics from chemical point of view is to look at some property or activity .Given that ChEMBL provides assay and target information for the molecules , we have many ways to perform this evaluation .", "label": "", "metadata": {}, "score": "66.01464"}
{"text": "First , one could look at the structural homogenity of the molecules assigned to topics .Another way to evaluate the topics from chemical point of view is to look at some property or activity .Given that ChEMBL provides assay and target information for the molecules , we have many ways to perform this evaluation .", "label": "", "metadata": {}, "score": "66.01464"}
{"text": "First , one could look at the structural homogenity of the molecules assigned to topics .Another way to evaluate the topics from chemical point of view is to look at some property or activity .Given that ChEMBL provides assay and target information for the molecules , we have many ways to perform this evaluation .", "label": "", "metadata": {}, "score": "66.01464"}
{"text": "First , one could look at the structural homogenity of the molecules assigned to topics .Another way to evaluate the topics from chemical point of view is to look at some property or activity .Given that ChEMBL provides assay and target information for the molecules , we have many ways to perform this evaluation .", "label": "", "metadata": {}, "score": "66.01464"}
{"text": "First , one could look at the structural homogenity of the molecules assigned to topics .Another way to evaluate the topics from chemical point of view is to look at some property or activity .Given that ChEMBL provides assay and target information for the molecules , we have many ways to perform this evaluation .", "label": "", "metadata": {}, "score": "66.01464"}
{"text": "The definition of construction is discussed as well as the tree substitution grammar and decision oriented parsing .For translation , Alan read a paper on translating verbs in Chinese to English , regarding the difficulty of tense .The authors explore latent features ( not exactly features involving hidden variables ) such as temporal features to show the effectiveness of such features .", "label": "", "metadata": {}, "score": "66.04402"}
{"text": "In other words , a molecule ( document ) is constructed from a set of fragments ( words ) .With the data arranged in this form we can go ahead and reuse code from the tm and topicmodels packages .Finally , we 're ready to develop some models , starting of with 6 topics .", "label": "", "metadata": {}, "score": "66.11279"}
{"text": "In other words , a molecule ( document ) is constructed from a set of fragments ( words ) .With the data arranged in this form we can go ahead and reuse code from the tm and topicmodels packages .Finally , we 're ready to develop some models , starting of with 6 topics .", "label": "", "metadata": {}, "score": "66.11279"}
{"text": "In other words , a molecule ( document ) is constructed from a set of fragments ( words ) .With the data arranged in this form we can go ahead and reuse code from the tm and topicmodels packages .Finally , we 're ready to develop some models , starting of with 6 topics .", "label": "", "metadata": {}, "score": "66.11279"}
{"text": "In other words , a molecule ( document ) is constructed from a set of fragments ( words ) .With the data arranged in this form we can go ahead and reuse code from the tm and topicmodels packages .Finally , we 're ready to develop some models , starting of with 6 topics .", "label": "", "metadata": {}, "score": "66.11279"}
{"text": "In other words , a molecule ( document ) is constructed from a set of fragments ( words ) .With the data arranged in this form we can go ahead and reuse code from the tm and topicmodels packages .Finally , we 're ready to develop some models , starting of with 6 topics .", "label": "", "metadata": {}, "score": "66.11279"}
{"text": "For example , atoms and bonds can be considered an \" alphabet \" for chemical structures .Going one level up , one can consider fragments as words , which can be joined together to form larger structures ( with the linguistic analog being sentences ) .", "label": "", "metadata": {}, "score": "66.140366"}
{"text": "Approximation algorithms for performing summarization are also proposed and empirical experiments are conducted to demonstrate the effectiveness of our proposed framework . byPeng Li , Jing Jiang , Shanghai Jiao - In : Proceedings of the 48th ACL . \" ...", "label": "", "metadata": {}, "score": "66.167206"}
{"text": "However , including all the text of users together gave very noisy results , because it was including a lot of text where users were talking about past experiences ( for example often when they were talking in present form when they were telling a story ) .", "label": "", "metadata": {}, "score": "66.22365"}
{"text": "It 'd also be useful to look at this method on a slightly smaller , labeled dataset - I 've run some preliminary experiments on the Bursi AMES but those results need a little more work .More generally , smaller datasets can be problematic as the number of unique fragments can be low .", "label": "", "metadata": {}, "score": "66.37027"}
{"text": "It 'd also be useful to look at this method on a slightly smaller , labeled dataset - I 've run some preliminary experiments on the Bursi AMES but those results need a little more work .More generally , smaller datasets can be problematic as the number of unique fragments can be low .", "label": "", "metadata": {}, "score": "66.37027"}
{"text": "It 'd also be useful to look at this method on a slightly smaller , labeled dataset - I 've run some preliminary experiments on the Bursi AMES but those results need a little more work .More generally , smaller datasets can be problematic as the number of unique fragments can be low .", "label": "", "metadata": {}, "score": "66.37027"}
{"text": "It 'd also be useful to look at this method on a slightly smaller , labeled dataset - I 've run some preliminary experiments on the Bursi AMES but those results need a little more work .More generally , smaller datasets can be problematic as the number of unique fragments can be low .", "label": "", "metadata": {}, "score": "66.37027"}
{"text": "It 'd also be useful to look at this method on a slightly smaller , labeled dataset - I 've run some preliminary experiments on the Bursi AMES but those results need a little more work .More generally , smaller datasets can be problematic as the number of unique fragments can be low .", "label": "", "metadata": {}, "score": "66.37027"}
{"text": "In a talk I gave at the ACS sometime back I compared fragments with n - grams ( though LINGO 's are probably a more direct analog ) .On these lines I have been playing with text mining and modeling tools in R , mainly via the excellent tm package .", "label": "", "metadata": {}, "score": "66.67294"}
{"text": "In a talk I gave at the ACS sometime back I compared fragments with n - grams ( though LINGO 's are probably a more direct analog ) .On these lines I have been playing with text mining and modeling tools in R , mainly via the excellent tm package .", "label": "", "metadata": {}, "score": "66.67294"}
{"text": "A construction is a \" pairing of form and meaning . \"Words , multiwords , morphological features , etc . are all constructions .To understand them you have to think about communicative functionality , meaning , and general cognitive constraints .", "label": "", "metadata": {}, "score": "66.91907"}
{"text": "Spherical Topic Models [ Details ] [ PDF ] [ Slides ] Joseph Reisinger , Austin Waters , Bryan Silverthorn , and Raymond J. Mooney In Proceedings of the 27th International Conference on Machine Learning ( ICML 2010 ) , 2010 .", "label": "", "metadata": {}, "score": "66.98328"}
{"text": "I like the range of methods they tried , but it would have been nice if they had said some more regarding error analysis etc .Now I felt most of the paper was about explaining all the different measures .They found that Wikipedia based method achieved very good results .", "label": "", "metadata": {}, "score": "67.01333"}
{"text": "Introduces hLDA , which models topics in a tree .Each document is generated by topics along a single path through the tree .We present the nested Chinese restaurant process ( nCRP ) , a stochastic process which assigns probability distributions to infinitely - deep , infinitely - branching trees .", "label": "", "metadata": {}, "score": "67.200714"}
{"text": "Furthermore , in an unsupervised setting like the one I 've described here , fishing for a correlation between ( some set of ) properties and groupings of molecules is probably not the way to go .Archive for January , 2012 .", "label": "", "metadata": {}, "score": "67.51406"}
{"text": "Their features included bigrams , trigrams , BOW , and syntactic features .They showed that high performance gains could be gained by combining them .Their final approach ( ' stacking ' ) , was a combination of different combinations of classifiers .", "label": "", "metadata": {}, "score": "67.62311"}
{"text": "I think the work was interesting and I 'm curious to see what other approaches people are going to take working on this problem .For my research , we have been looking at posts from users over time on a forum .", "label": "", "metadata": {}, "score": "68.44887"}
{"text": "twords is specified in the command line .GibbsLDA++ also saves a file called wordmap.txt that contains the maps between words and word 's IDs ( integer ) .This is because GibbsLDA++ works directly with integer IDs of words / terms inside instead of text strings .", "label": "", "metadata": {}, "score": "68.57618"}
{"text": "What our algorithm learns are the mixture weights over such shells .We provide a risk bound guarantee when learning in a large - margin structured - prediction setting using a projected subgradient method when only approximate submodular optimization is possible ( such as with submodular function maximization ) .", "label": "", "metadata": {}, "score": "68.79002"}
{"text": "This paper discusses and analyzes dialog in Twitter , particularly relating to the use of the @ symbol .The authors sampled 37k tweets over the course of the day , and hand - annotated a sample of these in order to answer questions about the presence and function of dialog in twitter , and the relation of the @ symbol to dialog .", "label": "", "metadata": {}, "score": "69.5532"}
{"text": "We were n't sure what evaluation method would fit better .We also discussed the overall popularity of looking at Twitter data nowadays .One of the main advantages is that it 's easy to get ( their API seems to be pretty good ) .", "label": "", "metadata": {}, "score": "69.64322"}
{"text": "In hindsight , maybe not the greatest of papers , but I thought I would try to clear some things up since it was pretty rough when I presented yesterday .It 's very possible I may have misunderstood some part so please let me know if something seems off .", "label": "", "metadata": {}, "score": "69.77356"}
{"text": "The extracted \" universal \" topics have multiple types of representations , with each type corresponding to one language .Accordingly , new documents of different languages can be represented in a space using a group of universal topics , which makes various multilingual Web applications feasible .", "label": "", "metadata": {}, "score": "69.867256"}
{"text": "At the end of this one has a series of documents , each one being represented as a bag of words .The collection of words across all documents can be converted to a document - term matrix ( documents in the rows , words in the columns ) which is then used as input to the LDA routine .", "label": "", "metadata": {}, "score": "69.93131"}
{"text": "At the end of this one has a series of documents , each one being represented as a bag of words .The collection of words across all documents can be converted to a document - term matrix ( documents in the rows , words in the columns ) which is then used as input to the LDA routine .", "label": "", "metadata": {}, "score": "69.93131"}
{"text": "At the end of this one has a series of documents , each one being represented as a bag of words .The collection of words across all documents can be converted to a document - term matrix ( documents in the rows , words in the columns ) which is then used as input to the LDA routine .", "label": "", "metadata": {}, "score": "69.93131"}
{"text": "At the end of this one has a series of documents , each one being represented as a bag of words .The collection of words across all documents can be converted to a document - term matrix ( documents in the rows , words in the columns ) which is then used as input to the LDA routine .", "label": "", "metadata": {}, "score": "69.93131"}
{"text": "At the end of this one has a series of documents , each one being represented as a bag of words .The collection of words across all documents can be converted to a document - term matrix ( documents in the rows , words in the columns ) which is then used as input to the LDA routine .", "label": "", "metadata": {}, "score": "69.93131"}
{"text": "Pre - meeting ( Dong Nguyen ) .Related paper Automatic Evaluation of Topic Coherence , Newman et al .Focus paper : Reading tea leaves : How humans interpret topic models , Chang et al . .I think the focus paper addressed an important topic .", "label": "", "metadata": {}, "score": "70.37142"}
{"text": "Of the instances of ' @ ' in a sample of 1500 tweets , 90 % were addresses , 5 % were references , and the last 5 % was distributed among the remaining categories .The authors devised a semantic coding scheme for classifying tweets into one of twelve categories , and manually tagged around 200 tweets .", "label": "", "metadata": {}, "score": "70.90027"}
{"text": "The default value of alpha is 50 / K ( K is the the number of topics ) .See [ Griffiths04 ] for a detailed discussion of choosing alpha and beta values .-beta : The value of beta , also the hyper - parameter of LDA .", "label": "", "metadata": {}, "score": "72.435555"}
{"text": "This is just a rough inspection of the most likely \" terms \" for each topic .It 's also interesting to look at how the molecules ( a.k.a . , documents ) are assigned to the topics .The barchart indicates the distribution of molecules amongst the 6 topics .", "label": "", "metadata": {}, "score": "72.513176"}
{"text": "This is just a rough inspection of the most likely \" terms \" for each topic .It 's also interesting to look at how the molecules ( a.k.a . , documents ) are assigned to the topics .The barchart indicates the distribution of molecules amongst the 6 topics .", "label": "", "metadata": {}, "score": "72.513176"}
{"text": "This is just a rough inspection of the most likely \" terms \" for each topic .It 's also interesting to look at how the molecules ( a.k.a . , documents ) are assigned to the topics .The barchart indicates the distribution of molecules amongst the 6 topics .", "label": "", "metadata": {}, "score": "72.513176"}
{"text": "This is just a rough inspection of the most likely \" terms \" for each topic .It 's also interesting to look at how the molecules ( a.k.a . , documents ) are assigned to the topics .The barchart indicates the distribution of molecules amongst the 6 topics .", "label": "", "metadata": {}, "score": "72.513176"}
{"text": "This is just a rough inspection of the most likely \" terms \" for each topic .It 's also interesting to look at how the molecules ( a.k.a . , documents ) are assigned to the topics .The barchart indicates the distribution of molecules amongst the 6 topics .", "label": "", "metadata": {}, "score": "72.513176"}
{"text": "Experimental results on the DUC benchmark datasets demonstrate the effectiveness of the proposed multi - modality learning algorithms with all the three fusion schemes .Archive for the ' dirichlet ' tag .For example , atoms and bonds can be considered an \" alphabet \" for chemical structures .", "label": "", "metadata": {}, "score": "72.93477"}
{"text": "Input data format .Both data for training / estimating the model and new data ( i.e. , previously unseen data ) have the same format as follows : . [M ] .[ document 1 ] .[ document 2 ] .", "label": "", "metadata": {}, "score": "73.60064"}
{"text": "This is a pretty cool problem since the conversational aspects of English or any language seem to be one of the harder problems one could pose in NLP .The authors crawled Twitter using its API and obtained the posts of a sample of users , and all the replies to their posts , extracting entire conversation trees .", "label": "", "metadata": {}, "score": "73.65944"}
{"text": "The document 's publication date was considered to be the current point in time , and phrases like \" last month \" , \" next Friday \" are normalized .After using global as well as transitivity , the authors report 1 - 2 % ( absolute ) increase in accuracy .", "label": "", "metadata": {}, "score": "73.81976"}
{"text": "If you set this parameter a value larger than zero , e.g. , 20 , GibbsLDA++ will print out the list of top 20 most likely words per each topic each time it save the model to hard disk according to the parameter savestep above .", "label": "", "metadata": {}, "score": "74.682846"}
{"text": "If you set this parameter a value larger than zero , e.g. , 20 , GibbsLDA++ will print out the list of top 20 most likely words per each topic each time it save the model to hard disk according to the parameter savestep above .", "label": "", "metadata": {}, "score": "74.682846"}
{"text": "The rest of this post describes a quick first look at this , using ChEMBL as the source of structures and R for performing pre - processing and modeling .Structures & fragments .We had previously fragmented ChEMBL ( v8 ) in house , so obtaining the data was just a matter of running an SQL query to identify all fragments that occured in 50 or molecules and retrieving their structures and the molecules they were associated with .", "label": "", "metadata": {}, "score": "75.326324"}
{"text": "The rest of this post describes a quick first look at this , using ChEMBL as the source of structures and R for performing pre - processing and modeling .Structures & fragments .We had previously fragmented ChEMBL ( v8 ) in house , so obtaining the data was just a matter of running an SQL query to identify all fragments that occured in 50 or molecules and retrieving their structures and the molecules they were associated with .", "label": "", "metadata": {}, "score": "75.326324"}
{"text": "The rest of this post describes a quick first look at this , using ChEMBL as the source of structures and R for performing pre - processing and modeling .Structures & fragments .We had previously fragmented ChEMBL ( v8 ) in house , so obtaining the data was just a matter of running an SQL query to identify all fragments that occured in 50 or molecules and retrieving their structures and the molecules they were associated with .", "label": "", "metadata": {}, "score": "75.326324"}
{"text": "The rest of this post describes a quick first look at this , using ChEMBL as the source of structures and R for performing pre - processing and modeling .Structures & fragments .We had previously fragmented ChEMBL ( v8 ) in house , so obtaining the data was just a matter of running an SQL query to identify all fragments that occured in 50 or molecules and retrieving their structures and the molecules they were associated with .", "label": "", "metadata": {}, "score": "75.326324"}
{"text": "The rest of this post describes a quick first look at this , using ChEMBL as the source of structures and R for performing pre - processing and modeling .Structures & fragments .We had previously fragmented ChEMBL ( v8 ) in house , so obtaining the data was just a matter of running an SQL query to identify all fragments that occured in 50 or molecules and retrieving their structures and the molecules they were associated with .", "label": "", "metadata": {}, "score": "75.326324"}
{"text": "To analyze dialog , the authors extracted all of the conversations that occurred in the sample of tweets , and found that the typical conversation was between two people , consisted of 3 - 5 tweets , and occurred over a period of 15 - 30 minutes .", "label": "", "metadata": {}, "score": "75.63484"}
{"text": "Each line after that is one document .[ document i ] is the i th document of the dataset that consists of a list of N i words / terms .N i ) are text strings and they are separated by the blank character .", "label": "", "metadata": {}, "score": "75.69586"}
{"text": "The data file I have is of the form .fragment_id , molregno , smiles , natom .where natom is the number of atoms in the fragment .The R code to generate ( relatively ) clean data , read to feed to the LDA function looks like : . frame ( do .", "label": "", "metadata": {}, "score": "75.871925"}
{"text": "The data file I have is of the form .fragment_id , molregno , smiles , natom .where natom is the number of atoms in the fragment .The R code to generate ( relatively ) clean data , read to feed to the LDA function looks like : . frame ( do .", "label": "", "metadata": {}, "score": "75.871925"}
{"text": "The data file I have is of the form .fragment_id , molregno , smiles , natom .where natom is the number of atoms in the fragment .The R code to generate ( relatively ) clean data , read to feed to the LDA function looks like : . frame ( do .", "label": "", "metadata": {}, "score": "75.871925"}
{"text": "The data file I have is of the form .fragment_id , molregno , smiles , natom .where natom is the number of atoms in the fragment .The R code to generate ( relatively ) clean data , read to feed to the LDA function looks like : . frame ( do .", "label": "", "metadata": {}, "score": "75.871925"}
{"text": "The data file I have is of the form .fragment_id , molregno , smiles , natom .where natom is the number of atoms in the fragment .The R code to generate ( relatively ) clean data , read to feed to the LDA function looks like : . frame ( do .", "label": "", "metadata": {}, "score": "75.871925"}
{"text": "A mixture of such shells can then also be so instantia ... \" .We introduce a method to learn a mixture of submodular \" shells \" in a large - margin setting .A submodular shell is an abstract submodular function that can be instantiated with a ground set and a set of parameters to produce a submodular function .", "label": "", "metadata": {}, "score": "75.883026"}
{"text": "See section \" Input data format \" for a description of input data format .Parameter estimation from a previously estimated model .Command line : .$ lda -estc -dir -model [ -niters ] [ -savestep ] [ -twords ] . in which ( parameters in [ ] are optional ) : .", "label": "", "metadata": {}, "score": "76.38577"}
{"text": "A C / C++ compiler and the STL library .The computational time of GibbsLDA++ much depends on the size of input data , the CPU speed , and the memory size .If your dataset is quite large ( e.g. , larger than 100,000 documents or so ) , it is better to train GibbsLDA++ on a minimum of 2GHz CPU , 1Gb RAM system .", "label": "", "metadata": {}, "score": "76.79615"}
{"text": "In addition I wonder how well their methods work when the domains are very specific and not well covered by the Web / Wordnet / Wikipedia .It also was n't clear to me also how they mapped terms to Wikipedia pages , which is not a trivial thing to do .", "label": "", "metadata": {}, "score": "76.89662"}
{"text": "The model is elegant in the sense that its components are modular .However , it does not work very well because of the structure of WordNet .The perk of the whole model is that it is modular and can be integrated into bigger models .", "label": "", "metadata": {}, "score": "76.95128"}
{"text": "You should download version 0.2 that includes bug fix and code optimization , and thus faster than the version 0.1 .A Java implementation ( JGibbLDA ) is also available .You can download at its project page .Environments .Unix , Linux , Cygwin , and MinGW .", "label": "", "metadata": {}, "score": "77.2446"}
{"text": "Because I have one meeting and one class consecutively Thursday afternoon before the meeting , I may not have enough time to cover posts submitted after 1:00pm .However , I will try my best to update and cover all the new posts in time .", "label": "", "metadata": {}, "score": "77.79314"}
{"text": "The default value is zero .If you set this parameter a value larger than zero , e.g. , 20 , GibbsLDA++ will print out the list of top 20 most likely words per each topic after inference .-dfile : The file containing new data .", "label": "", "metadata": {}, "score": "79.19144"}
{"text": "Dong read a paper that explore the combination of classifiers to address the WSD .The authors experimented 6 different classifiers and different ways to combine them .For WSD methods , beside the paper Dong read , Daniel read a paper that exploits novel syntactic features to improve the precision of WSD .", "label": "", "metadata": {}, "score": "80.39495"}
{"text": "-ntopics : The number of topics .Its default value is 100 .This depends on the input dataset .See [ Griffiths04 ] and [ Blei03 ] for a more careful discussion of selecting the number of topics .-niters : The number of Gibbs sampling iterations .", "label": "", "metadata": {}, "score": "80.96777"}
{"text": "Focus paper : Reading tea leaves : How humans interpret topic models .Yesterdays meeting focused mainly on ways of evaluating topic models .For the focus paper , the overall opinion was that the results were n't very strong .More details regarding the Mechanical Turk setup / results would have been nice , as well as a comparison with somewhat less similar models , such as LSA .", "label": "", "metadata": {}, "score": "81.474075"}
{"text": "\" The paper is a very basic introduction to the difficulties inherent in translating Chinese verbs to English , particularly with regards to the task of determining tense .The paper pushes the idea that the deeper features used by humans when translating needs to be identified and also should be the focus in terms of creating more advanced automatic extraction methods .", "label": "", "metadata": {}, "score": "82.15126"}
{"text": "How do English tweets use the @ symbol ?What topics are present in tweets , and how does the presence of the @ symbol affect this distribution ?How does the @ symbol function with regards to interactive exchanges ?In answer to the first question , they found that the use of the @ symbol does not vary significantly over time or language .", "label": "", "metadata": {}, "score": "82.39331"}
{"text": "Acknowledgements .Our code is based on the Java code of Gregor Heinrich and the theoretical description of Gibbs Sampling for LDA in [ Heinrich].I would like to thank Heinrich for sharing the code and a comprehensive technical report .", "label": "", "metadata": {}, "score": "82.737595"}
{"text": "-dir : The directory contain the previously estimated model .-model : The name of the previously estimated model .See section \" Outputs \" to know how GibbsLDA++ saves outputs on hard disk .-niters : The number of Gibbs sampling iterations to continue estimating .", "label": "", "metadata": {}, "score": "82.99379"}
{"text": "While a traditional text document - based modeling project would involved a series of pre - processing steps , the only one I need to perform in this scenario is the removal of small ( and thus likely very common ) fragments such as benzene - the cheminformatics equivalent of removing stopwords .", "label": "", "metadata": {}, "score": "83.17942"}
{"text": "While a traditional text document - based modeling project would involved a series of pre - processing steps , the only one I need to perform in this scenario is the removal of small ( and thus likely very common ) fragments such as benzene - the cheminformatics equivalent of removing stopwords .", "label": "", "metadata": {}, "score": "83.17942"}
{"text": "While a traditional text document - based modeling project would involved a series of pre - processing steps , the only one I need to perform in this scenario is the removal of small ( and thus likely very common ) fragments such as benzene - the cheminformatics equivalent of removing stopwords .", "label": "", "metadata": {}, "score": "83.17942"}
{"text": "While a traditional text document - based modeling project would involved a series of pre - processing steps , the only one I need to perform in this scenario is the removal of small ( and thus likely very common ) fragments such as benzene - the cheminformatics equivalent of removing stopwords .", "label": "", "metadata": {}, "score": "83.17942"}
{"text": "While a traditional text document - based modeling project would involved a series of pre - processing steps , the only one I need to perform in this scenario is the removal of small ( and thus likely very common ) fragments such as benzene - the cheminformatics equivalent of removing stopwords .", "label": "", "metadata": {}, "score": "83.17942"}
{"text": "Command line : .$ lda -est [ -alpha ] [ -beta ] [ -ntopics ] [ -niters ] [ -savestep ] [ -twords ] -dfile . in which ( parameters in [ ] are optional ) : .-est : Estimate the LDA model from scratch .", "label": "", "metadata": {}, "score": "84.399765"}
{"text": "-model : The name of the previously estimated model .See section \" Outputs \" to know how GibbsLDA++ saves outputs on hard disk .-niters : The number of Gibbs sampling iterations for inference .The default value is 20 .", "label": "", "metadata": {}, "score": "85.30846"}
{"text": "Command line : .$ lda -inf -dir -model [ -niters ] [ -twords ] -dfile . in which ( parameters in [ ] are optional ) : . -inf : Do inference for previously unseen ( new ) data using a previously estimated LDA model .", "label": "", "metadata": {}, "score": "85.40246"}
{"text": "$ src / lda -estc -dir models / casestudy/ -model model-01000 -niters 800 -savestep 100 -twords 30 .Now , look into the casestudy directory to see the outputs .$ src / lda -inf -dir models / casestudy/ -model model-01800 -niters 30 -twords 20 -dfile newdocs.dat .", "label": "", "metadata": {}, "score": "87.253204"}
{"text": "See the GNU General Public License for more details .You should have received a copy of the GNU General Public License along with GibbsLDA++ ; if not , write to the Free Software Foundation , Inc. , 59 Temple Place , Suite 330 , Boston , MA 02111 - 1307 USA .", "label": "", "metadata": {}, "score": "89.80977"}
{"text": "$ gunzip GibbsLDA++ . tar.gz .$ tar -xf GibbsLDA++ . tar .Compile .Go to the home directory of GibbsLDA++ ( i.e. GibbsLDA++ directory ) , and type : .$ make clean .$ make all .After compiling GibbsLDA++ , we have an executable file \" lda \" in the GibbsLDA++/src directory .", "label": "", "metadata": {}, "score": "96.57202"}
{"text": "Other potential applications in biological data .Contact us : all comments , suggestions , and bug reports are highly appreciated .And if you have any further problems , please contact us : .Xuan - Hieu Phan ( pxhieu at gmail dot com ) , was at Tohoku University , Japan ( now at Vietnam National University , Hanoi ) Cam - Tu Nguyen ( ncamtu at gmail dot com ) , was at Vietnam National University , Hanoi ( now at Google Japan ) .", "label": "", "metadata": {}, "score": "98.8263"}
