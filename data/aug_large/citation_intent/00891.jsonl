{"text": "For applications that are tolerant of a certain class of relatively innocuous errors ( where unseen n - grams may be accepted as rare n - grams ) , we can reduce the latter cost to below 1 byte per n - gram . ... de the use of entropy pruning techniques ( Stolcke , 1998 ) or clustering ( Jelinek et al . , 1990 ; Goodman and Gao , 2000 ) to reduce the number of n - grams that must be stored .", "label": "", "metadata": {}, "score": "32.52619"}
{"text": "In some implementations , either instead of or in addition to a smoothing algorithm , back - off weights are calculated to assign probability estimates to a second set of n - grams that are not present in the data files .", "label": "", "metadata": {}, "score": "34.34216"}
{"text": "The back - off weights are chosen such that the overall model is normalized , i.e. occurrence probabilities for every n - gram context sum to 1 .This can be achieved by using equation 2 : .Estimation of a back - off weight for a given context involves summing over all probabilities found in that context as well as he corresponding lower - order estimates .", "label": "", "metadata": {}, "score": "35.67195"}
{"text": "Another embodiment of the invention also includes smoothing the n - gram models by referring to a separate backoff model ( such as a uniform distribution model ) , instead of using self - loops across the back - off state in the n - gram model .", "label": "", "metadata": {}, "score": "35.693535"}
{"text": "Hence , the following property holds over the probabilities on edges : .A - inverted .n.di-elect cons .N\u03a3.sub .[ 0321 ]In other words , the probabilities on all outgoing edges from a given node n must sum to one .", "label": "", "metadata": {}, "score": "35.987797"}
{"text": "This probability stream can then be linearly interpolated with one from a standard n - gram model using this tool .Output : An optimal set of interpolation weights for these probability streams , and ( optionally ) a probability stream corresponding to the linear combination of all the input streams , according to the optimal weights .", "label": "", "metadata": {}, "score": "36.304836"}
{"text": "Therefore , in the example above , the text string corresponding to enumerated segmentations for the preterminals , together with its expected count collected in the expectation step of the EM algorithm , is used to train an n - gram for the preterminals .", "label": "", "metadata": {}, "score": "36.435776"}
{"text": "Another idea is to decompose the probability computation hierarchically , using a tree of binary probabilistic decisions , so as to replace \\(O(N)\\ ) computations by \\(O(\\log N)\\ ) computations ( Morin and Bengio 2005 ) .Yet another idea is to replace the exact gradient by a stochastic estimator obtained using a Monte - Carlo sampling technique ( Bengio and Senecal 2008 ) .", "label": "", "metadata": {}, "score": "36.802017"}
{"text": "This is much more than the number of operations typically involved in computing probability predictions for n - gram models .Several researchers have developed techniques to speed - up either probability prediction ( when using the model ) or estimating gradients ( when training the model ) .", "label": "", "metadata": {}, "score": "36.86546"}
{"text": "A fundamental obstacle to progress in this direction has to do with the diffusion of gradients through long chains of non - linear transformations , making it difficult to learn long - term dependencies ( Bengio et al 1994 ) in sequential data .", "label": "", "metadata": {}, "score": "36.942318"}
{"text": "Therefore , the identifiers define an ordering by P(term ) , which is the unigram probability of terms in the language .[ 0113 ]In the present system , n - gram probabilities are stored in a compressed manner to facilitate wide coverage and rapid access on memory - limited devices .", "label": "", "metadata": {}, "score": "38.83978"}
{"text": "Therefore , the identifiers define an ordering by P(term ) , which is the unigram probability of terms in the language .[ 0113 ]In the present system , n - gram probabilities are stored in a compressed manner to facilitate wide coverage and rapid access on memory - limited devices .", "label": "", "metadata": {}, "score": "38.83978"}
{"text": "Each of the edges is assigned a probability and , in a preferred embodiment , the DAG preferably also has the special property that each path is constrained to be the same length .This type of variant DAG is termed in this document as a probabilistic , constrained sequence graph ( PCSG ) .", "label": "", "metadata": {}, "score": "39.210747"}
{"text": "This set of identifiers can then be narrowed using a candidate filter 15 .However , in the present example , no filtering is required because the length of the current input will be less than the maximum depth of the approximate trie .", "label": "", "metadata": {}, "score": "39.400932"}
{"text": "c . )i .P .w .i . w .i . w .i . w .c .Cmd . )The n - gram model does not require an exact rule match .Instead of making binary decisions about rule applicability , it compares the probability that the observed word sequence is generated from a state ( preterminal ) sequence to find the most likely interpretation .", "label": "", "metadata": {}, "score": "39.686836"}
{"text": "Each of these estimates and how it may be computed is discussed below .- sub .S ) where \u03b8 S are the parameters of the model .A wide range of different techniques can be used to compute this estimate , for example , smoothed frequency analysis on the context training data , in a similar way to equation ( 21 ) , and as described in relation to the target sequence prior estimate .", "label": "", "metadata": {}, "score": "40.080254"}
{"text": "This yields a set of weighted topic category predictions for each term / phrase prediction in P. .[ 0136 ] The modification of prediction probabilities in P requires the definition of a ' similarity metric ' between topic category prediction sets .", "label": "", "metadata": {}, "score": "40.47917"}
{"text": "This yields a set of weighted topic category predictions for each term / phrase prediction in P. .[ 0136 ] The modification of prediction probabilities in P requires the definition of a ' similarity metric ' between topic category prediction sets .", "label": "", "metadata": {}, "score": "40.47917"}
{"text": "Optimizing the latter remains a difficult challenge .In addition , it could be argued that using a huge training set ( e.g. , all the text in the Web ) , one could get n - gram based language models that appear to capture semantics correctly .", "label": "", "metadata": {}, "score": "40.645596"}
{"text": "Therefore , the identifiers define an ordering by P(term ) , which is the unigram probability of terms in the language .[ 0082 ] In the present system , n - gram probabilities are stored in a highly compressed and computationally - efficient manner to facilitate wide coverage and rapid access on memory - limited devices .", "label": "", "metadata": {}, "score": "40.683907"}
{"text": "This represents a segmentation ambiguity which historically has not been resolved in the absence of additional information from the developer .In some prior systems , each of the possible segmentations was simply displayed to the user , and the user was allowed to choose one of those segmentations .", "label": "", "metadata": {}, "score": "40.835815"}
{"text": "In addition , even when the segmentations were adequately displayed for selection by the user , user 's often make errors in the segmentation or segment similar text strings inconsistently .[ 0078 ] .In accordance with one embodiment the expectation maximization ( EM ) algorithm is applied to segmentation ambiguities in model component 202 in order to disambiguate the segmentation choices .", "label": "", "metadata": {}, "score": "40.99061"}
{"text": "Conversely , we show how to save memory by collapsing probability and backoff into a single value without changing sentence - level scores , at the expense of less accurate estimates for sentence fragments .These changes can be stacked , achieving better estimates with unchanged memory usage .", "label": "", "metadata": {}, "score": "41.939377"}
{"text": "The higher the value chosen for the constant , the more slowly the probabilities from the user specific language model will increase ( because the probability for a particular term is determined by dividing its count by the value of its parent node ) .", "label": "", "metadata": {}, "score": "41.983414"}
{"text": "The higher the value chosen for the constant , the more slowly the probabilities from the user specific language model will increase ( because the probability for a particular term is determined by dividing its count by the value of its parent node ) .", "label": "", "metadata": {}, "score": "41.983414"}
{"text": "The higher the value chosen for the constant , the more slowly the probabilities from the user specific language model will increase ( because the probability for a particular term is determined by dividing its count by the value of its parent node ) .", "label": "", "metadata": {}, "score": "41.983414"}
{"text": "To compute the intersection of the set of candidates returned by the approximate trie and the n - gram map , an intersection mechanism 16 first determines which of the two sets is smaller .The smaller set of identifiers is used as a base set .", "label": "", "metadata": {}, "score": "42.14094"}
{"text": "Well - established techniques can be used for estimating vector similarity , e.g. by applying magnitude normalization and taking the inner ( dot ) product .At each stage , the language model will already possess a candidate prediction set , and if a threshold on computation time is exceeded , the candidate set can be returned and additional filters 19 can be easily bypassed .", "label": "", "metadata": {}, "score": "42.189964"}
{"text": "For example , one pruning mechanism provides that at each column of the trellis , no transition is made out of a node if its score is smaller than a threshold ( such as 5.0 ) less then the maximum score in the same column .", "label": "", "metadata": {}, "score": "42.21038"}
{"text": "M N may be updated as text is input into the system .[0248 ] Embodiments provide at least two methods for computing evidence likelihoods within a probabilistic framework by marginalising over candidate interpretations of the evidence , represented in a graph framework , although other methods may be utilised .", "label": "", "metadata": {}, "score": "42.420395"}
{"text": "Additional filters would operate in a similar manner .In general , if p is the final number of predictions requested , then the filtering process would operate on a number greater than p ( specified a - priori ) , such that the reordering process may result in a different set of predictions returned to the user .", "label": "", "metadata": {}, "score": "42.486786"}
{"text": "And finally a lexical variant analyser is deployed , as illustrated in FIG .13F .[0332 ]Note that due to PCSG property 3 , branches must converge before re - branching .This means that in some cases an empty node must be inserted , if two branch points occur contiguously .", "label": "", "metadata": {}, "score": "42.78476"}
{"text": "In this paper we investigate the extent to which Katz backoff language models can be compressed through a combination of parameter quantization ( width - wise compression ) and parameter pruning ( length - wise compression ) methods while preserving performance .", "label": "", "metadata": {}, "score": "43.0214"}
{"text": "A new prediction set is then constructed using the new probabilities .In general , if p is the final number of predictions requested , then the filtering process will operate on a number greater than p ( specified a - priori ) so that the reordering process may result in a different set of predictions returned to the user .", "label": "", "metadata": {}, "score": "43.214203"}
{"text": "A new prediction set is then constructed using the new probabilities .In general , if p is the final number of predictions requested , then the filtering process will operate on a number greater than p ( specified a - priori ) so that the reordering process may result in a different set of predictions returned to the user .", "label": "", "metadata": {}, "score": "43.214203"}
{"text": "The limit is applied by not updating them if they would exceed the limit ( an action which should not compromise the action of the other hyperparameter updates ) .This can be done relatively simply as the update equations are simply weighted averages of a few quantities , so it is reasonable to ' add in ' some quantity of the prior , as follows : .", "label": "", "metadata": {}, "score": "43.385487"}
{"text": "At each stage , the language model will already possess a candidate prediction set , and if a threshold on computation time is exceeded , the candidate set can be returned and additional filters 19 can be easily bypassed .[0139 ]", "label": "", "metadata": {}, "score": "43.631714"}
{"text": "At each stage , the language model will already possess a candidate prediction set , and if a threshold on computation time is exceeded , the candidate set can be returned and additional filters 19 can be easily bypassed .[0139 ]", "label": "", "metadata": {}, "score": "43.631714"}
{"text": "The model can be pruned down to a smaller size by manipulating the statistics or the estimated model .This paper shows how an n - gram model can be built by ad ... \" .Traditionally , when building an n - gram model , we decide the span of the model history , collect the relevant statistics and estimate the model .", "label": "", "metadata": {}, "score": "43.655357"}
{"text": "In this paper , we show that some of the commonly used pruning methods do not take into account how removing an - gram should modify the backoff distributions in the state - of - the - art Kneser - Ney smoothing .", "label": "", "metadata": {}, "score": "43.87712"}
{"text": "For instance , the context sequence model S can actually be reused to obtain an estimate of the likelihood of different orthographic variants , which can be used in combination with other probabilistic measures to yield branch probabilities .[0335 ]", "label": "", "metadata": {}, "score": "44.40543"}
{"text": "However , empirical testing may find better values for k. .is typically the amount of discounting found by Good - Turing estimation .In other words , if Good - Turing estimates as , then .If not , algorithm skips N-1 entirely and uses the Katz estimate for N-2 .", "label": "", "metadata": {}, "score": "44.48448"}
{"text": "In addition to the computational challenges briefly described above , several weaknesses of the neural network language model are being worked on by researchers in the field .One of them is the representation of a fixed - size context .To represent longer - term context , one may employ a recurrent network formulation , which learns a representation of context that summarizes the past word sequence in a way that preserves information predictive of the future .", "label": "", "metadata": {}, "score": "44.658367"}
{"text": "Instead of storing a single value at each node 21 associated with a path , an approximate the 13 stores a set of values for all subsequently - allowable sequences .This extension from a standard trie optimises computational efficiency and memory overheads .", "label": "", "metadata": {}, "score": "44.7255"}
{"text": "Instead of storing a single value at each node 21 associated with a path , an approximate the 13 stores a set of values for all subsequently - allowable sequences .This extension from a standard trie optimises computational efficiency and memory overheads .", "label": "", "metadata": {}, "score": "44.7255"}
{"text": "[ 0161 ] .FIGS .16 - 17B illustrate the topology of a model in such a way as to facilitate a discussion of how the model can be represented in a finite state representation , in a compact manner , even though it accounts for unseen words .", "label": "", "metadata": {}, "score": "44.800793"}
{"text": "These generalisations allow for instances where the user omitted one or more characters from the target sequence ( with the wildcard probability w ) or inserted one or more erroneous characters ( with the empty node probability e ) .It will be understood that the specifics of how these extra structures are added to the PCSG will vary with different instantiations of the system , depending on computational resources , sequence model strength , etc . .", "label": "", "metadata": {}, "score": "44.84288"}
{"text": "Other suitable modeling techniques can also be applied .[0177 ]In order to obtain the MAP estimate for \u03bc c , \u03a3 c , a prior distribution of those parameters must be specified .Other prior distributions can be used , some of which are discussed later .", "label": "", "metadata": {}, "score": "44.928604"}
{"text": "The wildcard structure is a constrained cycle and therefore violates the acyclic property of the standard PCSG .The EPCSG extension licenses the use of a wildcard cycle at convergence points only .The values e and w are pre - specified probability constants .", "label": "", "metadata": {}, "score": "44.9306"}
{"text": "Therefore , the n+1-gram map of the Bloom filter is searched for each n+1 term sequence , c+C+t , to determine whether that n+1-gram path exists and , if so , the probability associated with that path .", "label": "", "metadata": {}, "score": "44.943184"}
{"text": "Each sequence prediction has a probability value associated with it .However , in other embodiments , there may be a single trained model and single evidence source .In probabilistic terms , this equates to a ranking over sequences in a set S governed by the following : .", "label": "", "metadata": {}, "score": "44.980103"}
{"text": "References .Jelinek , F. and Mercer , R.L. ( 1980 ) Interpolated Estimation of Markov Source Parameters from Sparse Data .Pattern Recognition in Practice , Gelsema E.S. and Kanal L.N. eds , North - Holland .pp .381 - 397 .", "label": "", "metadata": {}, "score": "45.03997"}
{"text": "Because many different combinations of feature values are possible , a very large set of possible meanings can be represented compactly , allowing a model with a comparatively small number of parameters to fit a large training set .The dominant methodology for probabilistic language modeling since the 1980 's has been based on n - gram models ( Jelinek and Mercer , 1980;Katz 1987 ) .", "label": "", "metadata": {}, "score": "45.254124"}
{"text": "An early discussion can also be found in the Parallel Distributed Processing book ( 1986 ) , a landmark of the connectionist approach .For example , with \\(m\\ ) binary features , one can describe up to \\(2^m\\ ) different objects .", "label": "", "metadata": {}, "score": "45.320984"}
{"text": "Smoothing provides a \" smooth \" ( or \" discounted \" ) probability estimate to the observed n - grams .The back - off algorithm is used to compute probabilities of unseen n - grams .Equation 1 .Computing n - gram probabilities by interpolating with lower order estimates .", "label": "", "metadata": {}, "score": "45.459423"}
{"text": "[ 0051 ] Preferably , the language model comprises a mechanism to compute the intersection of the predictions determined by the approximate trie and optionally the candidate filter , and the n - gram map and the method includes the further step of computing the intersection of the predictions .", "label": "", "metadata": {}, "score": "45.589073"}
{"text": "Examples of Bayesian estimators are Variational Bayes and Gibbs sampling .[ 0187 ] The above described learning algorithm is a learning algorithm that accounts for each datapoint equally , whether recent or old , with a prior which is ( intentionally ) overwhelmed once a certain number of datapoints have been observed .", "label": "", "metadata": {}, "score": "45.61467"}
{"text": "In some cases ( e.g. a form factor change ) , the system may have sufficient information to explicitly choose a different input model , but in the case of multiple users , the appropriate model choice may not be obvious .", "label": "", "metadata": {}, "score": "45.667175"}
{"text": "0190 ] The additional parameters \u03b1 cmax , \u03b2 cmax , \u03b4 and .di - elect cons . may be set empirically , as required .They control the ' asymptotic strength ' of the prior , and the rate at which past observations are forgotten .", "label": "", "metadata": {}, "score": "45.715363"}
{"text": "These non - parametric learning algorithms are based on storing and combining frequency counts of word subsequences of different lengths , e.g. , 1 , 2 and 3 for 3-grams .\\ )Furthermore , a new observed sequence typically will have occurred rarely or not at all in the training set .", "label": "", "metadata": {}, "score": "45.76671"}
{"text": "The input probability generator filters out low - probability targets by retaining only those with a probability above a predetermined threshold ( which is set empirically or through experiment with the user ) .The target mapping stage 903 maps targets to word - fragments and tags each word fragment with the target from which it came .", "label": "", "metadata": {}, "score": "45.7858"}
{"text": "In general , if p is the final number of predictions requested , then the filtering process will operate on a number greater than p ( specified a - priori ) so that the reordering process may result in a different set of predictions returned to the user .", "label": "", "metadata": {}, "score": "45.842102"}
{"text": "[0341 ] The context prior provides a dual function : It helps to normalise the probability estimate ; and provides simple ' model detection ' when the context model is unable to provide useful information .If the context sequence estimate is uninformative ( such as when the last term is unknown to an N - gram model ) , the context prior estimate will weigh more heavily the model with the most likely context , promoting the predictions of this model above those from other models .", "label": "", "metadata": {}, "score": "45.848022"}
{"text": "At the same time , they can be mapped into memory quickly and be searched directly in time linear in the length of the key , without the need to decompress the entire file .The overhead for local decompression during search is marginal . \" ...", "label": "", "metadata": {}, "score": "45.864883"}
{"text": "According to one embodiment , after the n - gram count files have been merged , occurrence probability estimates are calculated for each n - gram to generate a language model .A language model includes a set of probabilities that a particular n - gram will occur in a previously unanalyzed input file ( an occurrence probability ) .", "label": "", "metadata": {}, "score": "46.152794"}
{"text": "However , other learning mechanisms can be used , some of which are discussed later .[0176 ] Rather than using a 2D Gaussian , the user 's input can be modeled by independently distributing the x and y co - ordinates of the user 's input events with a Laplace distribution .", "label": "", "metadata": {}, "score": "46.51611"}
{"text": "The program will also report the frequency of frequency of n - grams , and the corresponding recommended value for the -spec_num parameters of idngram2lm .The -fof_size parameter allows the user to specify the length of this list .A value of 0 will result in no list being displayed .", "label": "", "metadata": {}, "score": "46.596375"}
{"text": "Katz back - off is a generative n -gram language model that estimates the conditional probability of a word given its history in the n -gram .It accomplishes this estimation by \" backing - off \" to models with smaller histories under certain conditions .", "label": "", "metadata": {}, "score": "46.8595"}
{"text": "A set of candidates is represented by a set of numerical identifiers .[0124 ] To compute the intersection of the set of candidates returned by the approximate trie and the n - gram map , an intersection mechanism 16 first determines which of the two sets is smaller .", "label": "", "metadata": {}, "score": "46.96554"}
{"text": "A set of candidates is represented by a set of numerical identifiers .[0124 ] To compute the intersection of the set of candidates returned by the approximate trie and the n - gram map , an intersection mechanism 16 first determines which of the two sets is smaller .", "label": "", "metadata": {}, "score": "46.96554"}
{"text": "[ 0192 ] A robust way to perform model selection is to evaluate the data likelihood over a number of samples .This can be achieved as follows , where data likelihood is expressed as : .This can be written as , .", "label": "", "metadata": {}, "score": "46.981895"}
{"text": "[ 0103 ] .Similarly , rules which are no longer supported by the best segmentation of any examples can be removed from the enumerated rules shown in FIG .2G. [ 0104 ] .Application of the EM algorithm in this way is now described in more formal mathematical terms .", "label": "", "metadata": {}, "score": "46.998634"}
{"text": "[0079 ] Reverting to FIG .2 , a candidate filter 15 can be applied by the language model to narrow the set of predictions returned from the approximate trie 13 so that it contains only identifiers for candidates that are truly allowed by the current word input .", "label": "", "metadata": {}, "score": "47.017067"}
{"text": "[ 0090 ] The n - gram probabilities of a dynamic language model are not stored directly , rather frequency statistics are stored .An example of a dynamic n - gram map is shown in FIG .6 .Probabilities are computed on - the - fly from these frequency values by dividing the count for a particular term by the total value at its parent node .", "label": "", "metadata": {}, "score": "47.041496"}
{"text": "A technique to associate Bloom filter entries with probability values is disclosed in Talbot and Osborne , 2007 , Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , pp .468 - 479 .", "label": "", "metadata": {}, "score": "47.17155"}
{"text": "A technique to associate Bloom filter entries with probability values is disclosed in Talbot and Osborne , 2007 , Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , pp .468 - 479 .", "label": "", "metadata": {}, "score": "47.17155"}
{"text": "The depth of the approximate trie is specified a - priori for each language model .[0120 ]Using the tokenized context 12 , the n - gram map 14 is queried by the language model for a given n - gram order , i.e. a number of context terms .", "label": "", "metadata": {}, "score": "47.33719"}
{"text": "N - gram statistics yield estimates of prediction candidate probabilities based on local context , but global context also affects candidate probabilities .The present system utilizes a topic filter 18 that actively identifies the most likely topic for a given piece of writing and reorders the candidate predictions accordingly .", "label": "", "metadata": {}, "score": "47.38211"}
{"text": "Since the commands are read from standard input , a command file can be piped into it directly , thus removing the need for the program to run interactively : .Input : Files containing probability streams , as generated by the -probs option of the perplexity command of evallm .", "label": "", "metadata": {}, "score": "47.604584"}
{"text": "[0038 ]In another embodiment of the invention , the plurality of language models includes a user specific language model based on an n - gram language model that is updated to record the frequency of occurrence of n - gram paths input by a user in an n - gram map .", "label": "", "metadata": {}, "score": "47.61526"}
{"text": "N - gram statistics yield estimates of prediction candidate probabilities based on local context , but global context also affects candidate probabilities .The present system utilises a topic filter 18 that actively identifies the most likely topic for a given piece of writing and reorders the candidate predictions accordingly .", "label": "", "metadata": {}, "score": "47.709084"}
{"text": "N - gram statistics yield estimates of prediction candidate probabilities based on local context , but global context also affects candidate probabilities .The present system utilises a topic filter 18 that actively identifies the most likely topic for a given piece of writing and reorders the candidate predictions accordingly .", "label": "", "metadata": {}, "score": "47.709084"}
{"text": "Perhaps it 's not allowed by the rules of the grammar .[ 2 ] .^ Katz , S. M. ( 1987 ) .Estimation of probabilities from sparse data for the language model component of a speech recogniser .IEEE Transactions on Acoustics , Speech , and Signal Processing , 35(3 ) , 400 - 401 .", "label": "", "metadata": {}, "score": "47.714592"}
{"text": "[ 0299 ] This approximation is implemented in the system as follows .Let us consider a function z over a set of sequences T , such that : . sup .[ 0300 ] Z is computed as : .In essence , this means that each evidence conditional model is responsible for its own distributional smoothing , but this must be related to k which is proportional to the overall estimated number of \" unknown \" sequences .", "label": "", "metadata": {}, "score": "47.72433"}
{"text": "There are many different methods of implementing sim and any one is appropriate .For instance , the topic category prediction sets can be interpreted as vectors in an m - dimensional space where m is the number of topic categories .", "label": "", "metadata": {}, "score": "47.850517"}
{"text": "We contribute two changes that trade between accuracy of these estimates and memory , holding sentence - level scores constant .Common practi ... \" .Approximate search algorithms , such as cube pruning in syntactic machine translation , rely on the language model to estimate probabilities of sentence fragments .", "label": "", "metadata": {}, "score": "47.90452"}
{"text": "Probabilistic thresholding is used to prune paths that are relatively improbable .A threshold is set on the ratio between the current most probable sequence and the differential for less probable sequences .Given threshold t , and a currently investigated path length L , a path n 1 . . .", "label": "", "metadata": {}, "score": "47.9073"}
{"text": "Once the rewrite rules that support the segmentations are enumerated , they are each assigned a probability .Initially , all segmentations illustrated in FIG .2 G are assigned the same probability .This is indicated by block 308 in FIG .", "label": "", "metadata": {}, "score": "48.04335"}
{"text": "[0017 ]In another embodiment of the invention , the plurality of language models includes a user specific language model based on an n - gram language model that is updated to record the frequency of occurrence of n - gram paths input by a user in an n - gram map .", "label": "", "metadata": {}, "score": "48.079147"}
{"text": "[0017 ]In another embodiment of the invention , the plurality of language models includes a user specific language model based on an n - gram language model that is updated to record the frequency of occurrence of n - gram paths input by a user in an n - gram map .", "label": "", "metadata": {}, "score": "48.079147"}
{"text": "The input probability generator 902 then , preferably , retains the characters with an associated probability above a threshold probability value , where the threshold value is set empirically to a pre - defined level ( e.g. 10 -4 ) or is determined by experiments with the user .", "label": "", "metadata": {}, "score": "48.21353"}
{"text": "In other words , the total probability mass for the ShowFlightCmd terminal must add to one .Therefore , the counts for each rewrite rule are multiplied by a normalization factor in order to obtain a probability associated with that rewrite rule .", "label": "", "metadata": {}, "score": "48.378315"}
{"text": "[ 0119 ] A dynamic language model is updated in one of two ways : to include a term which is not previously present in a dynamic language model vocabulary ; and to update the frequency of an existing term in a particular n - gram context .", "label": "", "metadata": {}, "score": "48.571365"}
{"text": "[ 0119 ] A dynamic language model is updated in one of two ways : to include a term which is not previously present in a dynamic language model vocabulary ; and to update the frequency of an existing term in a particular n - gram context .", "label": "", "metadata": {}, "score": "48.571365"}
{"text": "[ 0039 ]In one embodiment , the text prediction engine comprises a mechanism to combine the predictions generated by each language model .Preferably , the mechanism is configured to insert the predictions into an STL ' multimap ' structure and return the p most probable terms as the predictions for provision to the user interface .", "label": "", "metadata": {}, "score": "48.660015"}
{"text": "2G. [ 0085 ] .Again , each of these segmentations is represented in the rewrite rules shown in FIG .2G. [ 0086 ] .Enumeration of all possible segmentations is indicated by block 306 in the flow diagram of FIG .", "label": "", "metadata": {}, "score": "48.662796"}
{"text": "Preferably , the plurality of language models utilise a beginning - of - sequence marker to determine word or phrase predictions in the absence of any preceding text input and/or after end - of - sentence punctuation and/or after new line entry .", "label": "", "metadata": {}, "score": "48.925556"}
{"text": "Preferably , the plurality of language models utilise a beginning - of - sequence marker to determine word or phrase predictions in the absence of any preceding text input and/or after end - of - sentence punctuation and/or after new line entry .", "label": "", "metadata": {}, "score": "48.925556"}
{"text": "To train the new model that models the preterminals with n - grams , the expected counts collected in the E - step are used to train and smooth the n - grams in the M - step ; and the n - grams are used by the EM algorithm to collect the expected counts for the segmentations .", "label": "", "metadata": {}, "score": "48.97281"}
{"text": "Essentially , this means that if the n -gram has been seen more than k times in training , the conditional probability of a word given its history is proportional to the maximum likelihood estimate of that n -gram .The more difficult part is determining the values for k , d and \u03b1 . is the least important of the parameters .", "label": "", "metadata": {}, "score": "49.075817"}
{"text": "[ 0102 ] The prediction of topic categories for individual terms / phrases from the prediction set P can be carried out in the same manner as for input text segments , using the classifier .This yields a set of weighted topic category predictions for each term / phrase prediction in P. .", "label": "", "metadata": {}, "score": "49.079723"}
{"text": "[0117 ] The n - gram map structure described thus far is used in static language models .Static language models are immutable once they have been constructed and directly store compressed n - gram probabilities ; they are generated from pre - existing data and are then compiled into binary format files which can be read at run - time .", "label": "", "metadata": {}, "score": "49.08049"}
{"text": "[0117 ] The n - gram map structure described thus far is used in static language models .Static language models are immutable once they have been constructed and directly store compressed n - gram probabilities ; they are generated from pre - existing data and are then compiled into binary format files which can be read at run - time .", "label": "", "metadata": {}, "score": "49.08049"}
{"text": "[ 0144 ] As stated previously , with reference to FIG .1 , the present system utilises a mechanism 5 , static pruning , across all static language models , to reduce the amount of information stored in the system .", "label": "", "metadata": {}, "score": "49.097828"}
{"text": "[ 0144 ] As stated previously , with reference to FIG .1 , the present system utilises a mechanism 5 , static pruning , across all static language models , to reduce the amount of information stored in the system .", "label": "", "metadata": {}, "score": "49.097828"}
{"text": "However , in the present example , no filtering is required because the length of the current input will be less than the maximum depth of the approximate trie .The depth of the approximate trie is specified a - priori for each language model .", "label": "", "metadata": {}, "score": "49.233795"}
{"text": "However , in the present example , no filtering is required because the length of the current input will be less than the maximum depth of the approximate trie .The depth of the approximate trie is specified a - priori for each language model .", "label": "", "metadata": {}, "score": "49.233795"}
{"text": "This is possible because , although lexically identical , the terms may have a different meaning in different languages and can therefore be treated as distinct .[ 0310 ] Thus , turning to FIG .12 , the set of terms S context generated by the model M context may comprise terms from any one of the language models ( or candidate models ) contained within M context .", "label": "", "metadata": {}, "score": "49.24686"}
{"text": "Even then , however , it need not be used .The depth of the approximate trie is specified a - priori for each language model .The candidate filter looks up the actual candidate term string values represented by the numerical identifiers in the set of numerical identifiers returned by the approximate trie and processes them one - by - one , comparing each with the current input .", "label": "", "metadata": {}, "score": "49.264214"}
{"text": "[ 0184 ] The present example uses batch - mode and incremental learning algorithms , which compute the posterior in a single update from the prior , and by adding one observation at a time respectively .One of these learning algorithms runs when the user ' selects ' a prediction by matching the locations of input events to the locations of characters which make up the selected prediction .", "label": "", "metadata": {}, "score": "49.331486"}
{"text": "Probabilities for character sequences such as ' t ( which can be used to automatically add an apostrophe before the character ' t ' ) must be established a - priori .[0092 ] Each language model utilises an approximate trie 13 ( see FIGS .", "label": "", "metadata": {}, "score": "49.403316"}
{"text": "Probabilities for character sequences such as ' t ( which can be used to automatically add an apostrophe before the character ' t ' ) must be established a - priori .[0092 ] Each language model utilises an approximate trie 13 ( see FIGS .", "label": "", "metadata": {}, "score": "49.403316"}
{"text": "0043 ] One example smoothing algorithm suitable for this calculation is the Knesser - Ney smoothing algorithm .Another example of a popular smoothing algorithm is the Witten - Bell smoothing algorithm .The lower order estimates are scaled with the corresponding back - off weight bow(w i -1 , . . .", "label": "", "metadata": {}, "score": "49.537304"}
{"text": "[0085 ] The generation of predictions from an n - gram map 14 is described further in the following illustrative example .The language model first looks up the identifier for \" in \" and then conducts a binary search in the first level of the map to locate the identifier ( if it exists ) .", "label": "", "metadata": {}, "score": "49.60788"}
{"text": "[0145 ] Given two language models L1 and L2 , the pruning of L1 is achieved by comparison to a reference language model , L2 .Each language model comprises an n - gram map , in which terms in the vocabulary are associated with numerical identifiers which are stored in the map and associated with probability values .", "label": "", "metadata": {}, "score": "49.622787"}
{"text": "[0145 ] Given two language models L1 and L2 , the pruning of L1 is achieved by comparison to a reference language model , L2 .Each language model comprises an n - gram map , in which terms in the vocabulary are associated with numerical identifiers which are stored in the map and associated with probability values .", "label": "", "metadata": {}, "score": "49.622787"}
{"text": "[ 0146 ] .Training is now described in greater detail with respect to the example shown in FIGS . 7 - 11 .To train the model , the EM algorithm automatically segments word sequences and aligns each segment \u03b1 to the corresponding preterminal NT in the preterminal sequence of a corresponding pair .", "label": "", "metadata": {}, "score": "49.64292"}
{"text": "[ 0088 ] A dynamic language model is updated in one of two ways : to include a term which is not previously present in a dynamic language model vocabulary ; and to update the frequency of an existing term in a particular n - gram context .", "label": "", "metadata": {}, "score": "49.758232"}
{"text": "[ 0365 ]As discussed in relation to the system , the general method comprises generating sequence predictions and associated probability values by a text prediction engine which comprises one or more models .In a preferred embodiment , the method comprises generating sequence predictions from a target prior model R and from at least one model M 1 , M 2 , etc . which uses at least one evidence source e 1 , e 2 , etc . to generate predictions .", "label": "", "metadata": {}, "score": "49.767525"}
{"text": "Common practice uses lowerorder entries in an N - gram model to score the first few words of a fragment ; this violates assumptions made by common smoothing strategies , including Kneser - Ney .Instead , we use a unigram model to score the first word , a bigram for the second , etc .", "label": "", "metadata": {}, "score": "49.910393"}
{"text": "[ 0059 ] An STL multimap is a specific type of ordered associative structure in which duplicate keys are allowed .In the STL multimap of the present system , a prediction is a string value mapped to a probability value , and the map is ordered on the basis of the probabilities , i.e. the probability values are used as keys in the multimap and the strings as values .", "label": "", "metadata": {}, "score": "49.944965"}
{"text": "[ 0059 ] An STL multimap is a specific type of ordered associative structure in which duplicate keys are allowed .In the STL multimap of the present system , a prediction is a string value mapped to a probability value , and the map is ordered on the basis of the probabilities , i.e. the probability values are used as keys in the multimap and the strings as values .", "label": "", "metadata": {}, "score": "49.944965"}
{"text": "0065 ]The text prediction engine has the capability of performing phrase - level prediction .Therefore , the text prediction engine has the necessary information to perform phrase - level prediction .[0067 ] The system limits the space of combinations of terms ( i.e. the space of potential phrase predictions ) to a tiny fraction of the full n m , thus reducing the computational costs .", "label": "", "metadata": {}, "score": "50.071243"}
{"text": "0065 ]The text prediction engine has the capability of performing phrase - level prediction .Therefore , the text prediction engine has the necessary information to perform phrase - level prediction .[0067 ] The system limits the space of combinations of terms ( i.e. the space of potential phrase predictions ) to a tiny fraction of the full n m , thus reducing the computational costs .", "label": "", "metadata": {}, "score": "50.071243"}
{"text": "The set of sequences S F associated with the final probability values P F can be presented , for example in a list format , on a user interface of the system , for user review and selection .The user interacts with the system by making prediction selections or manipulating the device on which the system resides in other ways , thus updating the evidence .", "label": "", "metadata": {}, "score": "50.09702"}
{"text": "It also allows a regular developer , with little linguistic knowledge , to build a semantic grammar for spoken language understanding .The system facilitates the semi - automatic generation of relatively high quality semantic grammars , with a small amount of data .", "label": "", "metadata": {}, "score": "50.273357"}
{"text": "In this work , we present an efficient data structure and optimized algorithms specifically designed for iterative parameter tuning .With the ... \" .Despite the availability of better performing techniques , most language models are trained using popular toolkits that do not support perplexity optimization .", "label": "", "metadata": {}, "score": "50.27783"}
{"text": "For a discussion of shallow vs deep architectures , see ( Bengio and LeCun 2007 ) .Whereas current models have two or three layers , theoretical research on deep architectures suggests that representing high - level semantic abstractions efficiently may require deeper networks .", "label": "", "metadata": {}, "score": "50.29664"}
{"text": "Because identifiers are assigned to terms such that the resulting ordering is from most - to - least frequent , the identifier that is assigned to a given term in one language model does not necessarily match the identifier assigned to the same term in a different language model .", "label": "", "metadata": {}, "score": "50.32366"}
{"text": "Since there are no further items in the first element of the KeyPressVector the system moves to the next element , first attempting to follow the character ' l ' and then the sequence \" ' l \" .Both options match the structure of the trie , so both paths are followed and the probability state splits into two , with the relevant multiplication performed in each .", "label": "", "metadata": {}, "score": "50.36638"}
{"text": "Since there are no further items in the first element of the KeyPressVector the system moves to the next element , first attempting to follow the character ' l ' and then the sequence \" ' l \" .Both options match the structure of the trie , so both paths are followed and the probability state splits into two , with the relevant multiplication performed in each .", "label": "", "metadata": {}, "score": "50.36638"}
{"text": "By associating the term with the model from which it is drawn , the system simplifies the way in which lexically identical terms are dealt with , since only the most probable term is retained from two or more lexically identical terms .", "label": "", "metadata": {}, "score": "50.43967"}
{"text": "In each case a dictionary of allowable terms is maintained and , given a particular input sequence , the system chooses a legitimate term ( or set of terms ) from the dictionary and presents it to the user as a potential completion candidate .", "label": "", "metadata": {}, "score": "50.448788"}
{"text": "The model scales down to an n - gram classifier represented by Equation 19 .c .^ .arg .max .c .P .c . )P . s .c . ) arg .max .c .", "label": "", "metadata": {}, "score": "50.49476"}
{"text": "0185 ] If a Laplace / Gamma distribution is chosen instead of a Gaussian distribution , the same process is followed as for the Gaussian : a prior is chosen ( for example the conjugate prior ) and learning rules are derived for the MAP learning goal .", "label": "", "metadata": {}, "score": "50.5177"}
{"text": "Preferably , in response to the input of text which is not represented in the language model , the method includes the step of inserting new paths in the n - gram language model .[ 0056 ] There is also provided , in accordance with an embodiment , a computer program product including a computer readable medium having stored thereon computer program means for causing a processor to carry out embodiments of the method described herein .", "label": "", "metadata": {}, "score": "50.557617"}
{"text": "[ 0235 ] Each model in M is trained on a particular data source .Thus , a particular data source is represented by a model in M , and the set S in expression ( 1 ) ranges over all distinct terms ( or sequences ) generated by the models in M. A model is queried to provide a predicted term .", "label": "", "metadata": {}, "score": "50.6681"}
{"text": "Laplace and Gamma distributions may be more appropriate than a Gaussian distribution because the distributions are more heavy - tailed , with the probability density decaying more slowly than that of a Gaussian as the input event ( e.g. touch location ) moves away from a target .", "label": "", "metadata": {}, "score": "50.74153"}
{"text": "The preferred system preferably utilises the following techniques , alone or in any combination , to handle this combinatorial explosion : a trie ; probabilistic thresholding to prune paths that are relatively improbable ; and the input sequence model T used for probabilistic thresholding .", "label": "", "metadata": {}, "score": "50.858723"}
{"text": "13c .[0330 ] The system deploys a probabilistic tokenizer , the result in illustrated in FIG .13D.Edge probabilities are added according to the model , which is explained in further detail below .Continuing the algorithm , a case variant analyser is deployed , as illustrated in FIG .", "label": "", "metadata": {}, "score": "50.872"}
{"text": "In the example described , each key / target has a position and covariance value .The alternative is to provide each key with a location , but force keys to share a covariance -- when any of the keys are trained , the same covariance is trained .", "label": "", "metadata": {}, "score": "50.970345"}
{"text": "[0340 ]In practice , this estimate would be smoothed , for example by positing an occurrence assumption on unseen sequences , or by backing off to restricted ( lower order ) estimates in instances where the full sequence is unseen .", "label": "", "metadata": {}, "score": "50.97448"}
{"text": "Therefore , if the word is in the vocabulary , it will not be modeled with zero probability by the statistical model .Deleted interpolation is used to find the weight for each model in the smoothing operation and linearly interpolate the models of different orders .", "label": "", "metadata": {}, "score": "51.00213"}
{"text": "For instance , the topic category prediction sets can be interpreted as vectors in an m - dimensional space where m is the number of topic categories .Under this interpretation , the weight assigned by the classifier to a particular category c is the extension of the vector in the c - dimension .", "label": "", "metadata": {}, "score": "51.076286"}
{"text": "For instance , the topic category prediction sets can be interpreted as vectors in an m - dimensional space where m is the number of topic categories .Under this interpretation , the weight assigned by the classifier to a particular category c is the extension of the vector in the c - dimension .", "label": "", "metadata": {}, "score": "51.076286"}
{"text": "Figure 1 : Example of 2-dimensional distributed representation for words obtained in ( Blitzer et al 2005 ) .In ( Bengio et al 2001 , Bengio et al 2003 ) , it was demonstrated how distributed representations for symbols could be combined with neural network probability predictions in order to surpass standard n - gram models on statistical language modeling tasks .", "label": "", "metadata": {}, "score": "51.100975"}
{"text": "[0120 ] .A CFG works well for high resolution understanding .High resolution understanding represents grammars which break down sentences into a large number of slots .The larger the number of slots , the higher the resolution understanding is exhibited by the grammar .", "label": "", "metadata": {}, "score": "51.116646"}
{"text": "[ 0363 ]Thus , embodiments provide a general text prediction engine and system , and a specific example of that text prediction engine or system , which is configured to generate a set of sequence predictions S F , each with an associated probability value P F . [", "label": "", "metadata": {}, "score": "51.1258"}
{"text": "18A : .[ 0175 ] .[0176 ] .[0177 ] .[ 0178 ] .[ 0179 ] .[ 0180 ] .In prior systems , this results in a self - loop over the back - off state O for every unseen word .", "label": "", "metadata": {}, "score": "51.19809"}
{"text": "The -probs switch allows the user to specify a filename in which to store the combined probability stream .The optimal lambdas can also be stored in a file by use of the -out_lambdas command .The program stops when the ratio of the test - set perplexity between two successive iterations is above the value specified in the -stop_ratio option .", "label": "", "metadata": {}, "score": "51.344788"}
{"text": "Given a set of coordinates representing an input event , the distances between this event and surrounding pre - specified locations within an ' auto - correcting region ' are used to construct a weighted set of characters assigned to that event .", "label": "", "metadata": {}, "score": "51.373493"}
{"text": "w .i . wj . k . i . ) k . m .j .n .Therefore if we can compute \u03bb s t ( p , q ) , we can be combine equations ( 9 ) , ( 11 ) and ( 13 ) to obtain the expected counts and reset the model parameters .", "label": "", "metadata": {}, "score": "51.380756"}
{"text": "A first - order Markov assumption might be more appropriate : that the current character input is conditionally independent of all previous characters , given just the last character entered .The probability of an entire input sequence would then be : . [", "label": "", "metadata": {}, "score": "51.39626"}
{"text": "The present system utilizes a technique for associating n - grams 14 with probability values in the Bloom filter 17 .A technique to associate Bloom filter entries with probability values is disclosed in Talbot and Osborne , 2007 , Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , pp .", "label": "", "metadata": {}, "score": "51.503105"}
{"text": "If a match is found , the associated probability value for the sequence is multiplied with the current cumulative probability state , and the process continues .The output from interrogation of the probabilistic trie is a sequence of term identifiers mapped to probability values , each term identifier representing a term for which a single path through the KeyPressVector is a ( potentially improper ) substring .", "label": "", "metadata": {}, "score": "51.594036"}
{"text": "If a match is found , the associated probability value for the sequence is multiplied with the current cumulative probability state , and the process continues .The output from interrogation of the probabilistic trie is a sequence of term identifiers mapped to probability values , each term identifier representing a term for which a single path through the KeyPressVector is a ( potentially improper ) substring .", "label": "", "metadata": {}, "score": "51.594036"}
{"text": "Confidence interval capping has been omitted from version 2 of the toolkit .Output : List of every word which occurred in the text , along with its number of occurrences .Notes : Uses a hash - table to provide an efficient method of counting word occurrences .", "label": "", "metadata": {}, "score": "51.64161"}
{"text": "5 .In the n - gram map 14 , terms in the vocabulary are associated with numerical identifiers ( short integers ) which are stored in the map and associated with probability values .The combined probabilities of child nodes for a single parent always sum to 1 .", "label": "", "metadata": {}, "score": "51.66674"}
{"text": "5 .In the n - gram map 14 , terms in the vocabulary are associated with numerical identifiers ( short integers ) which are stored in the map and associated with probability values .The combined probabilities of child nodes for a single parent always sum to 1 .", "label": "", "metadata": {}, "score": "51.66674"}
{"text": "5 .In the n - gram map 14 , terms in the vocabulary are associated with numerical identifiers ( short integers ) which are stored in the map and associated with probability values .The combined probabilities of child nodes for a single parent always sum to 1 .", "label": "", "metadata": {}, "score": "51.66674"}
{"text": "If we model the joint probability of \u03c0 , N and w with .P .N .w . )i . m .P .NT .i . )Such a partition can be found with Viterbi search .", "label": "", "metadata": {}, "score": "51.931595"}
{"text": "0333 ] Edge probabilities are preferably assigned to the PCSGs .The assignment of edge probabilities is preferably carried out with respect to the parameters of the context candidate model .The intuitive interpretation of these probabilities is twofold .First , they represent an estimate of the likelihood that the user intended the sequence assigned to a particular branch .", "label": "", "metadata": {}, "score": "51.93396"}
{"text": "Instead of storing a single value at each node 21 associated with a path , an approximate trie 13 stores a set of values for all subsequently - allowable sequences .This extension from a standard trie optimizes computational efficiency and memory overheads .", "label": "", "metadata": {}, "score": "51.945805"}
{"text": "[0070 ] The text prediction engine has the capability of performing phrase - level prediction .Therefore , the text prediction engine has the necessary information to perform phrase - level prediction .[ 0072 ] The system limits the space of combinations of terms ( i.e. the space of potential phrase predictions ) to a tiny fraction of the full n m , thus reducing the computational costs .", "label": "", "metadata": {}, "score": "52.03293"}
{"text": "This association can be implicit in the data .However , the term could be tagged with an identifier associated with the model from which it has been drawn .[ 0236 ] In this preferred process of combining predictions , two otherwise identical predictions that come from different data sources are considered different .", "label": "", "metadata": {}, "score": "52.130676"}
{"text": "This structure / multimap can subsequently be read from the upper value end to obtain a set of final ' most probable ' predictions 9 .[ 0061 ] In the preferred embodiment , the system further comprises a user specific language model 7 , which comprises a dynamic language model trained progressively on user input .", "label": "", "metadata": {}, "score": "52.146156"}
{"text": "This structure / multimap can subsequently be read from the upper value end to obtain a set of final ' most probable ' predictions 9 .[ 0061 ] In the preferred embodiment , the system further comprises a user specific language model 7 , which comprises a dynamic language model trained progressively on user input .", "label": "", "metadata": {}, "score": "52.146156"}
{"text": "According to one implementation , merging respective data files of the first set of data files with corresponding data files of the second set of data files results in a set of merged data files .Respective data files of the set of merged data files may then be pruned .", "label": "", "metadata": {}, "score": "52.192673"}
{"text": "In the simplest implementation of such a mechanism , the word fragments comprise single characters and the splitting mechanism splits the prediction into single characters to be mapped back to the targets .In this situation , the splitting may be carried out by the predictor 904 .", "label": "", "metadata": {}, "score": "52.205956"}
{"text": "[ 0077 ] By way of example , the term \" investigate \" , mapped to numerical identifier ' 9 ' , can be added to an approximate trie of depth 4 in a language model .It will then follow the path to the node represented by ' n ' and add ' 9 ' to its values , and the same for ' v ' , and ' e ' , at which point the maximum depth has been attained and so the procedure terminates .", "label": "", "metadata": {}, "score": "52.21704"}
{"text": "0374 ]The target modeling module is preferably comprised of the target model set 906 as described in reference to FIG .9 .The target model set 906 can be queried by the input probability generator 902 to return ( 1 ) one or more likely targets and ( 2 ) the probability for each target that the user had intended to select that target through an input event .", "label": "", "metadata": {}, "score": "52.247185"}
{"text": "The present system utilises a log - frequency Bloom filter ( Talbot and Osborne ) which maps a set of n - gram entries to respective probability estimates .In the present system , the language model generates a set of predictions P based on a set of up to n-1 context terms C. A log - frequency Bloom filter F , which associates n+1-gram term sequences with probability estimates , can be used to generate a new prediction set in which the previous predictions are reordered .", "label": "", "metadata": {}, "score": "52.327816"}
{"text": "The present system utilises a log - frequency Bloom filter ( Talbot and Osborne ) which maps a set of n - gram entries to respective probability estimates .In the present system , the language model generates a set of predictions P based on a set of up to n-1 context terms C. A log - frequency Bloom filter F , which associates n+1-gram term sequences with probability estimates , can be used to generate a new prediction set in which the previous predictions are reordered .", "label": "", "metadata": {}, "score": "52.327816"}
{"text": "The neural network learns to map that sequence of feature vectors to a prediction of interest , such as the probability distribution over the next word in the sequence .The advantage of this distributed representation approach is that it allows the model to generalize well to sequences that are not in the set of training word sequences , but that are similar in terms of their features , i.e. , their distributed representation .", "label": "", "metadata": {}, "score": "52.3363"}
{"text": "This paper shows how an n - gram model can be built by adding suitable sets of n - grams to a unigram model until desired complexity is reached .Very high order n - grams can be used in the model , since the need for handling the full unpruned model is eliminated by the proposed technique .", "label": "", "metadata": {}, "score": "52.33857"}
{"text": "[ 0094 ] By way of example , the term \" investigate \" , mapped to numerical identifier ' 9 ' , can be added to an approximate trie of depth 4 in a language model .It will then follow the path to the node represented by ' n ' and add ' 9 ' to its values , and the same for ' v ' , and ' e ' , at which point the maximum depth has been attained and so the procedure terminates .", "label": "", "metadata": {}, "score": "52.34024"}
{"text": "[ 0094 ] By way of example , the term \" investigate \" , mapped to numerical identifier ' 9 ' , can be added to an approximate trie of depth 4 in a language model .It will then follow the path to the node represented by ' n ' and add ' 9 ' to its values , and the same for ' v ' , and ' e ' , at which point the maximum depth has been attained and so the procedure terminates .", "label": "", "metadata": {}, "score": "52.34024"}
{"text": "0304 ] where V is the set of sequences contained in R unigram and the implementation of the models is according to known techniques for constructing smoothed frequency - based unigram language models and smoothed Markov chain character models .A number of applicable techniques for implementing these models are listed below .", "label": "", "metadata": {}, "score": "52.37309"}
{"text": "Katz , S.M. ( 1987 )Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer .IEEE Transactions on Acoustics , Speech and Signal Processing 3:400 - 401 .Hinton , G.E. ( 1989 )Connectionist Learning Procedures .", "label": "", "metadata": {}, "score": "52.37667"}
{"text": "Further phrase - level predictions will not be made if the joint probability falls below a threshold value .[0068 ] The generation of predictions from an individual language model is now described with reference to FIGS .2a - d , which are block diagrams of alternative language models of the prediction architecture according to the invention .", "label": "", "metadata": {}, "score": "52.37776"}
{"text": "Further phrase - level predictions will not be made if the joint probability falls below a threshold value .[0068 ] The generation of predictions from an individual language model is now described with reference to FIGS .2a - d , which are block diagrams of alternative language models of the prediction architecture according to the invention .", "label": "", "metadata": {}, "score": "52.37776"}
{"text": "If the -annotate parameter is used , then an annotation file will be created , containing information on the probability of each word in the test set according to the language model , as well as the back - off class for each event .", "label": "", "metadata": {}, "score": "52.38302"}
{"text": "The depth of the approximate trie is specified a - priori for each language model .The candidate filter looks up the actual candidate term string values represented by the numerical identifiers in the set of numerical identifiers returned by the approximate trie and processes them one - by - one , comparing each with the current input .", "label": "", "metadata": {}, "score": "52.40176"}
{"text": "The depth of the approximate trie is specified a - priori for each language model .The candidate filter looks up the actual candidate term string values represented by the numerical identifiers in the set of numerical identifiers returned by the approximate trie and processes them one - by - one , comparing each with the current input .", "label": "", "metadata": {}, "score": "52.40176"}
{"text": "The following properties hold of G : .[ 0315 ] G is a directed , acyclic graph ( DAG ) .[ 0316 ] .A - inverted .[0317 ] .E - backward .m , k.di-elect cons .", "label": "", "metadata": {}, "score": "52.441757"}
{"text": "Growing algorithm .Thus , the main differences to KP are the following : We modify the model after each -gram has been pruned , inst ... . \" ...We present three novel methods of compactly storing very large n - gram language models .", "label": "", "metadata": {}, "score": "52.486004"}
{"text": "However , the training examples obtained form the user via the authoring tool are illustratively pairs of pre - terminal sequences and terminal sequences .The partition or segmentation is a hidden variable and unknown to the tool .[ 0108 ] .", "label": "", "metadata": {}, "score": "52.538372"}
{"text": "[ 0098 ] The function of the topic filter is to accept a set of predictions and yield a variant of this set in which the probability values associated with the predicted terms may be altered , which may consequentially alter the ordering of predictions in the set .", "label": "", "metadata": {}, "score": "52.547173"}
{"text": "0166 ] .Unlike previous robust understanding technology , which relied on a robust parser to skip the words not covered by a grammar , the use of n - gram models for the pre - terminals in a grammar makes the model robust in itself .", "label": "", "metadata": {}, "score": "52.560932"}
{"text": "One can specify that the first or last n items are the test set by use of the -test_first n or -test_last n options .Or one can perform two - way cross validation using the -cv option .If none of these are specified then the whole of the data is used for weight estimation .", "label": "", "metadata": {}, "score": "52.574585"}
{"text": "The language model then utilizes an approximate trie 13 , along with an n - gram map , to generate a list of word predictions based on the current word input 11 .[ 0111 ]As stated previously , with reference to FIG .", "label": "", "metadata": {}, "score": "52.603725"}
{"text": "[ 0309 ] As stated previously , M context -sequence may comprise a plurality of language models corresponding to a plurality of different languages .To determine the conditional probability of equation ( 16 ) , the conditional probability is determined using the language model associated with the term .", "label": "", "metadata": {}, "score": "52.665573"}
{"text": "[0169 ] .To illustrate the operation of the model shown in FIG .18A , consider the probabilities associated with an utterance of the word \" a \" followed by an utterance of the word \" b \" .This corresponds to a combination of the following probabilities : . [", "label": "", "metadata": {}, "score": "52.70863"}
{"text": "This approach is reasonable due to the Zipfian nature of many natural language phenomena , where a minority of likely events carry the majority of the probability mass .The Zipfian distribution is an instance of a power law distribution , in which the frequency of a given event is approximately inversely proportional to its rank .", "label": "", "metadata": {}, "score": "52.720116"}
{"text": "Given a particular context , the system predicts the words that are most likely to follow by using estimates of probabilities .The predictions 918 can be generated by a multi - language model as well as a single language model predictor .", "label": "", "metadata": {}, "score": "52.752396"}
{"text": "[ 0146 ] The static pruning mechanism 5 traverses the n - gram map of L1 such that each node is visited exactly once .For each path followed in L1 , the corresponding path is attempted in L2 by using the conversion table to convert the path identifiers in L1 to those of L2 .", "label": "", "metadata": {}, "score": "52.795753"}
{"text": "[ 0146 ] The static pruning mechanism 5 traverses the n - gram map of L1 such that each node is visited exactly once .For each path followed in L1 , the corresponding path is attempted in L2 by using the conversion table to convert the path identifiers in L1 to those of L2 .", "label": "", "metadata": {}, "score": "52.795753"}
{"text": "The incremental learning and retraining of the target model set 906 allows the target model set 906 to adapt to the user 's writing style .[ 0375 ] As noted above , whenever a user makes a selection event ( by selecting a word / phrase prediction displayed on the prediction pane 64 ) , the one or more likely targets are matched to the corresponding input events .", "label": "", "metadata": {}, "score": "52.998055"}
{"text": "However , the system and method is not limited to the entry of text and is applicable to the entry of non - text data , where quanta of data are represented by target points on a user interface .In a preferred example , the system further comprises a word fragment map 903 which maps the likely targets to word fragments which are used by the predictor 904 to generate text predictions .", "label": "", "metadata": {}, "score": "53.00055"}
{"text": "Thus , to un - map is a trivial pairing of touch - locations and targets .However , instead of tagging the word fragments , the system can use other mechanisms to map the word fragments of a selected prediction 920 back to their corresponding target events .", "label": "", "metadata": {}, "score": "53.01539"}
{"text": "Normalizing the counts to obtain the new probability is indicated by block 312 in FIG .3B. [ 0096 ] .Component 302 iterates on this process ( re - estimating the counts and obtaining new probabilities ) until the counts and probabilities converge .", "label": "", "metadata": {}, "score": "53.027313"}
{"text": "Each model M 1 , M 2 , . . .may comprise one or more sub - models .The probability generator PG takes the sequences and associated conditional probabilities as input and outputs a final set of sequences S F associated with probability values P F .", "label": "", "metadata": {}, "score": "53.069054"}
{"text": "Each word corresponds to a point in a feature space .One can imagine that each dimension of that space corresponds to a semantic or grammatical characteristic of words .The hope is that functionally similar words get to be closer to each other in that space , at least along some directions .", "label": "", "metadata": {}, "score": "53.18251"}
{"text": "[ 0018 ] In a preferred embodiment the text prediction engine comprises a mechanism to combine the predictions generated by each language model .Preferably , the mechanism is configured to insert the predictions into an ordered associative structure or an STL ' multimap ' structure and return the p most probable terms as the predictions for provision to the user interface .", "label": "", "metadata": {}, "score": "53.200165"}
{"text": "[ 0018 ] In a preferred embodiment the text prediction engine comprises a mechanism to combine the predictions generated by each language model .Preferably , the mechanism is configured to insert the predictions into an ordered associative structure or an STL ' multimap ' structure and return the p most probable terms as the predictions for provision to the user interface .", "label": "", "metadata": {}, "score": "53.200165"}
{"text": "The probability generator PG combines the probability estimates P R , P context , P input output from the models to generate a set of probability values P F for the final sequence predications S F .[0295 ] The final predictions S F can be displayed to the user via a user interface for user review and selection , or used by the system to automatically correct erroneously entered text .", "label": "", "metadata": {}, "score": "53.21137"}
{"text": "In this case , an arbitrary proportion of the discount probability mass ( specified by the -oov_fraction option ) is reserved for OOV words .The discounting strategy and its parameters are specified by the -linear , -absolute , -good_turing and -witten_bell options .", "label": "", "metadata": {}, "score": "53.24034"}
{"text": "Figure 2 : Architecture of neural net language model introduced in ( Bengio et al 2001 ) .Note that the gradient on most of \\(C\\ ) is zero ( and need not be computed or used ) for most of the columns of \\(C\\ : \\ ) only those corresponding to words in the input subsequence have a non - zero gradient .", "label": "", "metadata": {}, "score": "53.2614"}
{"text": "[ 0056 ] The present system utilises a mechanism 5 , static pruning , across all static language models , to reduce the amount of information stored in the system .If duplicate or multiple ( in the case of a system comprising three or more language models ) entries are detected , the mechanism 5 ' prunes ' the language models by retaining only the most probable entry .", "label": "", "metadata": {}, "score": "53.279526"}
{"text": "[ 0056 ] The present system utilises a mechanism 5 , static pruning , across all static language models , to reduce the amount of information stored in the system .If duplicate or multiple ( in the case of a system comprising three or more language models ) entries are detected , the mechanism 5 ' prunes ' the language models by retaining only the most probable entry .", "label": "", "metadata": {}, "score": "53.279526"}
{"text": "If instead the user adds further input about the current word , by entering further characters , this is preferably added to the input evidence to alter the current probabilities assigned to the predictions .[0296 ] The particulars of how the specific system of this embodiment may be generated from the mathematical basis will now be explained .", "label": "", "metadata": {}, "score": "53.420288"}
{"text": "The parameters of the conjugate distribution ( both prior & posterior ) are then termed hyperparameters ( as they parameterize the distribution of the model 's actual parameters ) , and MAP learning can be made into a relatively simple hyperparameter recalculation .", "label": "", "metadata": {}, "score": "53.451042"}
{"text": "With the resulting implementation , we demonstrate the feasibility and effectiveness of such iterative techniques in language model estimation .Index Terms : language modeling , smoothing , interpolation 1 . ...[ 3 ] , most work in the field opts for simpler techniques with inferior results .", "label": "", "metadata": {}, "score": "53.45807"}
{"text": "However , it will be appreciated that the value of the smoothing constant is a matter of choice .[ 0091 ] The advantage of the dynamic language model structure is that it allows rapid updating .However , the disadvantage of this type of language model is that its memory and computational requirements are significantly higher than in its static counterpart .", "label": "", "metadata": {}, "score": "53.481247"}
{"text": "Our basic ... \" .We present three novel methods of compactly storing very large n - gram language models .These methods use substantially less space than all known approaches and allow n - gram probabilities or counts to be retrieved in constant time , at speeds comparable to modern language modeling toolkits .", "label": "", "metadata": {}, "score": "53.592117"}
{"text": "[ 0086 ] The n - gram map structure described thus far is used in static language models .Static language models are immutable once they have been constructed and directly store compressed n - gram probabilities ; they are generated from pre - existing data and are then compiled into binary format files which can be read at run - time .", "label": "", "metadata": {}, "score": "53.597336"}
{"text": "[ 0252 ] Applying this assumption , the dependence on s can be dropped from the evidence term : .[ 0253 ] The properties of the candidate model can also be encoded in the form of graphical models describing the relationship between the variables and models , for example as shown in FIG . 11A.", "label": "", "metadata": {}, "score": "53.598305"}
{"text": "[ 0240 ] and marginalised over target sequences in the denominator to yield : .This independence assumption can be written as : . di - elect cons .M ) ] ( 4 ) .[ 0242 ] and stated as : .", "label": "", "metadata": {}, "score": "53.813828"}
{"text": "However , it will be appreciated that the value of the smoothing constant is a matter of choice .[0122 ] The advantage of the dynamic language model structure is that it allows rapid updating .However , the disadvantage of this type of language model is that its memory and computational requirements are significantly higher than in its static counterpart .", "label": "", "metadata": {}, "score": "53.920418"}
{"text": "However , it will be appreciated that the value of the smoothing constant is a matter of choice .[0122 ] The advantage of the dynamic language model structure is that it allows rapid updating .However , the disadvantage of this type of language model is that its memory and computational requirements are significantly higher than in its static counterpart .", "label": "", "metadata": {}, "score": "53.920418"}
{"text": "In a preferred embodiment , a model R.di - elect cons .M is associated with a target sequence prior .Given this assumption we can restate ( 3 ) as follows : .[ 0246 ] The denominator in expression ( 5 ) is constant with respect to s and therefore does not affect the ranking , rather it is a normalisation factor on computed probability values .", "label": "", "metadata": {}, "score": "53.924995"}
{"text": "Each character of the one or more characters is independently mapped to one or more word fragments , where the word fragments are tagged with the characters from which they came .The probabilities associated with the word fragments need not be the same as the probability associated with the character .", "label": "", "metadata": {}, "score": "53.94416"}
{"text": "[0121 ] The n - gram probabilities of a dynamic language model are not stored directly , rather frequency statistics are stored .An example of a dynamic n - gram map is shown in FIG .6 .Probabilities are computed on - the - fly from these frequency values by dividing the count for a particular term by the total value at its parent node .", "label": "", "metadata": {}, "score": "53.967247"}
{"text": "[0121 ] The n - gram probabilities of a dynamic language model are not stored directly , rather frequency statistics are stored .An example of a dynamic n - gram map is shown in FIG .6 .Probabilities are computed on - the - fly from these frequency values by dividing the count for a particular term by the total value at its parent node .", "label": "", "metadata": {}, "score": "53.967247"}
{"text": "[ 0102 ] .In accordance with another illustrative embodiment , pruning component 304 eliminates all but a predetermined number of segmentations with high likelihood corresponding to each example , and only introduce rewrite rules to the grammar according to the remaining segmentations .", "label": "", "metadata": {}, "score": "53.99626"}
{"text": "\\ )Vector \\(C_k\\ ) contains the learned features for word \\(k\\ .Let us denote \\(\\theta\\ ) for the concatenation of all the parameters .The capacity of the model is controlled by the number of hidden units \\(h\\ ) and by the number of learned word features \\(d\\ .", "label": "", "metadata": {}, "score": "54.017372"}
{"text": "[ 0095 ] Hence , the language model can not guarantee that additional sequence values that are not compatible with a specified search sequence will not be returned , when the current term input exceeds the maximum depth of the approximate trie .", "label": "", "metadata": {}, "score": "54.05179"}
{"text": "[ 0095 ] Hence , the language model can not guarantee that additional sequence values that are not compatible with a specified search sequence will not be returned , when the current term input exceeds the maximum depth of the approximate trie .", "label": "", "metadata": {}, "score": "54.05179"}
{"text": "In an embodiment in which the unmapping comprises a reverse mapping , a splitting mechanism splits the selected prediction 920 into a combination of word fragments and the word fragment map is used to provide a reverse mapping of the word fragments to their corresponding targets .", "label": "", "metadata": {}, "score": "54.06032"}
{"text": "( t)+1 . sup . sup .( t ) .[0183 ] The NIWD is convenient because it is conjugate of the multivariate Gaussian , which gives simple and incremental learning rules .As an alternative , it is possible to use a uniform ( uninformative ) prior .", "label": "", "metadata": {}, "score": "54.064236"}
{"text": "Further phrase - level predictions will not be made if the joint probability falls below a threshold value .[0073 ]The generation of predictions from an individual language model is now described with reference to FIG .2 , which is a block diagram of a language model of the prediction architecture according to an embodiment .", "label": "", "metadata": {}, "score": "54.17325"}
{"text": "[ 0195 ] The user interface of FIG .8 can be configured for multiple word ( phrase ) input .An example of two - term phrase input is discussed in relation to a predicted phrase of \" and the \" .", "label": "", "metadata": {}, "score": "54.18226"}
{"text": "[ 0195 ] The user interface of FIG .8 can be configured for multiple word ( phrase ) input .An example of two - term phrase input is discussed in relation to a predicted phrase of \" and the \" .", "label": "", "metadata": {}, "score": "54.18226"}
{"text": "0174 ] .For an utterance of a word sequence that is not observed in the training data , the finite state machine will not assign probability of 0 to it .Instead , a path traverses through the back - off state , using the product of the back - off weight and the unigram probability as the bigram probability of the unseen event .", "label": "", "metadata": {}, "score": "54.191463"}
{"text": "[0313 ] Formally , a PCSG consists of a 4-tuple containing a set of nodes N , a root node r , a set of directed edges E , and a set of parameters ( probabilities ) \u03b8 : .", "label": "", "metadata": {}, "score": "54.239822"}
{"text": "Thus , the model probabilities are smoothed using lower level n - grams and with a uniform distribution .In other words , if the statistical model comprises a bigram , it is smoothed with unigrams which provide probabilities for the words modeled , regardless of context .", "label": "", "metadata": {}, "score": "54.293346"}
{"text": "[ 0078 ] Hence , the language model can not guarantee that additional sequence values that are not compatible with a specified search sequence will not be returned , when the current term input exceeds the maximum depth of the approximate trie .", "label": "", "metadata": {}, "score": "54.306675"}
{"text": "A large literature on techniques to smooth frequency counts of subsequences has given rise to a number of algorithms and variants .Distributed representations .The idea of distributed representation has been at the core of the revival of artificial neural network research in the early 1980 's , best represented by the connectionist bringing together computer scientists , cognitive psychologists , physicists , neuroscientists , and others .", "label": "", "metadata": {}, "score": "54.35493"}
{"text": "Words are generated from them according to the CFG model P CFG .[0143 ] .In the model shown in FIG .11 , the meaning of an input sentence s can be obtained by finding the Viterbi semantic class c and the state sequence \u03c3 that satisfy : . c .", "label": "", "metadata": {}, "score": "54.35854"}
{"text": "[0158 ] .The feature extraction module 508 produces a stream of feature vectors that are each associated with a frame of the speech signal .This stream of feature vectors is provided to a decoder 512 , which identifies a most likely sequence of words based on the stream of feature vectors , a lexicon 514 , language model 351 , and the acoustic model 518 .", "label": "", "metadata": {}, "score": "54.37074"}
{"text": "Once the iteration is complete , component 302 will have computed a new count and new probability associated with each of the enumerated rewrite rules .While this , in and of itself , is very helpful , because it has assigned a probability to each of the segmentations to the rules corresponding to the different segmentations obtained during training , it may not be a desired final result .", "label": "", "metadata": {}, "score": "54.389698"}
{"text": "If a match is found for the identifier in question , the intersection mechanism 16 places the identifier in a new set which represents the intersection between the two sets .[0093 ] The language model can be configured to apply one or more filters to the predictions generated by the intersection mechanism 16 .", "label": "", "metadata": {}, "score": "54.46978"}
{"text": "In order to generate a single set of predictions 20 for a given language model , the language model must compute the intersection of the set of candidates returned by the approximate trie 13 and optional candidate filter 15 , and that returned by the n - gram map 14 .", "label": "", "metadata": {}, "score": "54.509777"}
{"text": "In order to generate a single set of predictions 20 for a given language model , the language model must compute the intersection of the set of candidates returned by the approximate trie 13 and optional candidate filter 15 , and that returned by the n - gram map 14 .", "label": "", "metadata": {}, "score": "54.509777"}
{"text": "[ 0116 ] The generation of predictions from an n - gram map 14 is described further in the following illustrative example .The language model first looks up the identifier for \" in \" and then conducts a binary search in the first level of the map to locate the identifier ( if it exists ) .", "label": "", "metadata": {}, "score": "54.53099"}
{"text": "[ 0116 ] The generation of predictions from an n - gram map 14 is described further in the following illustrative example .The language model first looks up the identifier for \" in \" and then conducts a binary search in the first level of the map to locate the identifier ( if it exists ) .", "label": "", "metadata": {}, "score": "54.53099"}
{"text": "According to one feature , the language model described herein includes the probability estimates derived directly from the counts and also those derived from the smoothing and back - off algorithms described above .[0045 ] FIG .6 is a functional block diagram of a prior art method 600 of generating a language model 620 from multiple training corpora .", "label": "", "metadata": {}, "score": "54.581646"}
{"text": "G - Code Ripper reads g - code , scales , and rotates and/or splits the tool paths before outputting the modified tool path data to a new g - code file .It evaluates g - code expressions and parameters and interprets YZ and ZX arcs .", "label": "", "metadata": {}, "score": "54.59163"}
{"text": "2 , it can be seen that in the absence of a current word input 11 , the predictions are based on a context input only 12 .The system can also use \" beginning - of - sequence \" ( BOS ) markers to determine word or phrase predictions after end - of - sentence punctuation and/or after new line entry .", "label": "", "metadata": {}, "score": "54.682514"}
{"text": "Smoothing of this nature is the means by which the system takes into account the varying levels of confidence in the models associated with each evidence source .[ 0303 ] The target sequence prior is preferably computed as follows : . di - elect cons .", "label": "", "metadata": {}, "score": "54.69815"}
{"text": "So , for example , if a given language model has a maximum n - gram order of 3 , in the present example , the system would begin by searching for the path corresponding to the context phrase \" see you \" .", "label": "", "metadata": {}, "score": "54.698433"}
{"text": "So , for example , if a given language model has a maximum n - gram order of 3 , in the present example , the system would begin by searching for the path corresponding to the context phrase \" see you \" .", "label": "", "metadata": {}, "score": "54.698433"}
{"text": "The static pruning mechanism 5 traverses the n - gram map of L1 such that each node is visited exactly once .For each path followed in L1 , the corresponding path is attempted in L2 by using the conversion table to convert the path identifiers in L1 to those of L2 .", "label": "", "metadata": {}, "score": "54.837055"}
{"text": "In static language models , each child node contains a term identifier and a compressed probability value that can be extracted directly for use in prediction ordering .In dynamic language models , the node contains a frequency value which must be normalized by its parent ' total ' value to yield a probability .", "label": "", "metadata": {}, "score": "54.84652"}
{"text": "In the preferred implementation , the most probable estimate is retained for a given lexical term / sequence and any ( less probable ) lexical duplicates are discarded .This will result in two separate estimates for the term \" pain \" given a particular set of evidence ( where the evidence in this case is the context which precedes the predicted term \" pain \" ) .", "label": "", "metadata": {}, "score": "54.856617"}
{"text": "[ 0100 ] Note that for simplicity in this example , the third element of the KeyPressVector consists of a single element with probability 0.2 .In practice , each element would consist of a true probability distribution , i.e. summing to 1 .", "label": "", "metadata": {}, "score": "54.87992"}
{"text": "[ 0100 ] Note that for simplicity in this example , the third element of the KeyPressVector consists of a single element with probability 0.2 .In practice , each element would consist of a true probability distribution , i.e. summing to 1 .", "label": "", "metadata": {}, "score": "54.87992"}
{"text": "The tags file will contain an integer for each item in the probability streams corresponding to the class that the item belongs to .A file specified using the -captions option will allow the user to attach names to each of the classes .", "label": "", "metadata": {}, "score": "54.92733"}
{"text": "To facilitate this search , child nodes are ordered numerically by their identifiers at each parent node .The node that is being searched for may contain a large number of children , but it is only the high probability candidates that are of interest .", "label": "", "metadata": {}, "score": "54.934715"}
{"text": "To facilitate this search , child nodes are ordered numerically by their identifiers at each parent node .The node that is being searched for may contain a large number of children , but it is only the high probability candidates that are of interest .", "label": "", "metadata": {}, "score": "54.934715"}
{"text": "Briefly , n - grams are stored in a data structure which privileges memory saving rather than access time .In particular , single components of each n - gram are searched , via binary search , into blocks ... . by Vesa Siivola , Bryan L. Pellom - In Proceedings of 9th European Conference on Speech Communication and Technology , 2005 . \" ...", "label": "", "metadata": {}, "score": "54.94168"}
{"text": "The probabilities for the rules in the grammar library are illustratively tuned using domain specific data and are smoothed .The n - grams in the model 604 are trained with partially labeled training data .As discussed above , the EM algorithm can be used to train the n - grams in the network , where the alignments are treated as hidden variables .", "label": "", "metadata": {}, "score": "54.98307"}
{"text": "However , this has resulted in a number of problems .First , this type of interaction with the user is intrusive and time consuming .Also , when there are more possible preterminals , and more unaligned words in the input text string , the number of possibilities which must be presented to the user rises dramatically .", "label": "", "metadata": {}, "score": "54.989254"}
{"text": "[ 0131 ] The function of the topic filter is to accept a set of predictions and yield a variant of this set in which the probability values associated with the predicted terms may be altered , which may consequentially alter the ordering of predictions in the set .", "label": "", "metadata": {}, "score": "55.05667"}
{"text": "[ 0131 ] The function of the topic filter is to accept a set of predictions and yield a variant of this set in which the probability values associated with the predicted terms may be altered , which may consequentially alter the ordering of predictions in the set .", "label": "", "metadata": {}, "score": "55.05667"}
{"text": "However , if more than 65535 distinct counts need to be stored ( very unlikely , unless constructing 4-gram or higher language models using Good - Turing discounting ) , the -four_byte_counts option will need to be used .The floating point values of the back - off weights may be stored as two - byte integers , by using the -two_byte_alphas switch .", "label": "", "metadata": {}, "score": "55.117294"}
{"text": "The default is -buffer STD_MEM .The toolkit provides for three types of vocabulary , which each handle out - of - vocabulary ( OOV ) words in different ways , and which are specified using the -vocab_type flag .A closed vocabulary ( -vocab_type 0 ) model does not make any provision for OOVs .", "label": "", "metadata": {}, "score": "55.14392"}
{"text": "[0114 ]The n - gram maps can be further compressed by representing string values as short - integer - valued numerical identifiers and by storing higher - order entries \" on top of \" lower - order entries . [", "label": "", "metadata": {}, "score": "55.222572"}
{"text": "[0114 ]The n - gram maps can be further compressed by representing string values as short - integer - valued numerical identifiers and by storing higher - order entries \" on top of \" lower - order entries . [", "label": "", "metadata": {}, "score": "55.222572"}
{"text": "The system begins by taking the largest possible amount of context and querying the n - gram map to see if there is an entry for the path representing that context .So , for example , if a given language model has a maximum n - gram order of 3 , in the present example , the system would begin by searching for the path corresponding to the context phrase \" see you \" .", "label": "", "metadata": {}, "score": "55.2806"}
{"text": "Thus , in this example the unnormalised and normalised ( rounded ) probabilities assigned to each edge are as illustrated in FIG .14B , with the unnormalised probabilities on the left side of the figure and the normalised probabilities on the right side of the figure .", "label": "", "metadata": {}, "score": "55.3347"}
{"text": "In static language models , each child node contains a term identifier and a compressed probability value that can be extracted directly for use in prediction ordering .In dynamic language models , the node contains a frequency value which must be normalised by its parent ' total ' value to yield a probability .", "label": "", "metadata": {}, "score": "55.367847"}
{"text": "In static language models , each child node contains a term identifier and a compressed probability value that can be extracted directly for use in prediction ordering .In dynamic language models , the node contains a frequency value which must be normalised by its parent ' total ' value to yield a probability .", "label": "", "metadata": {}, "score": "55.367847"}
{"text": "This paper presents efficient algorithmic and architectural solutions which have been tested within the Moses decoder , an open source toolk ... \" .Statistical machine translation , as well as other areas of human language processing , have recently pushed toward the use of large scale n - gram language models .", "label": "", "metadata": {}, "score": "55.49418"}
{"text": "0092 ] As stated previously , each language model has two input feeds , the current word input 11 and the context input 12 .In order to generate a single set of predictions 20 for a given language model , the language model must compute the intersection of the set of candidates returned by the approximate trie 13 and optional candidate filter 15 , and that returned by the n - gram map 14 .", "label": "", "metadata": {}, "score": "55.51641"}
{"text": "The amount of memory allocated to store the probability streams is dictated by the -max_probs option , which indicates the maximum number of probabilities allowed in one stream .Compute the perplexity of the language model , with respect to some test text b.text . evallm -binary a.binlm", "label": "", "metadata": {}, "score": "55.742485"}
{"text": "In other words , the system will estimate the conditional probability , over the set of all sequences from which predictions can be drawn , of the sequence given the evidence e. The target sequence is denoted by s. .[0234 ]", "label": "", "metadata": {}, "score": "55.758255"}
{"text": "[ 0149]FIG .13 illustrates a runtime system using both the rules - based grammar portion for slots and the statistical model portion for preterminals .The system receives an input , and uses the grammar portion and n - gram portion and outputs an output 402 .", "label": "", "metadata": {}, "score": "55.76415"}
{"text": "[ 0305 ] By including a target sequence prior model R , the system provides an improved accuracy of intended sequence prediction .Furthermore , the target sequence prior model R enables character - based inference of unseen target sequences , i.e. the system is better able to infer unknown target sequences to approximate across all possible target sequences .", "label": "", "metadata": {}, "score": "55.81333"}
{"text": "For example , if the context comprises term1 and term2 , the static pruning mechanism 5 will first locate the node for term1 .Term2 is then the specified child node that will be searched for .By conducting such a search in L2 , identical paths can be identified .", "label": "", "metadata": {}, "score": "55.833637"}
{"text": "For example , if the context comprises term1 and term2 , the static pruning mechanism 5 will first locate the node for term1 .Term2 is then the specified child node that will be searched for .By conducting such a search in L2 , identical paths can be identified .", "label": "", "metadata": {}, "score": "55.833637"}
{"text": "For example , if the context comprises term1 and term2 , the static pruning mechanism 5 will first locate the node for term1 .Term2 is then the specified child node that will be searched for .By conducting such a search in L2 , identical paths can be identified .", "label": "", "metadata": {}, "score": "55.833637"}
{"text": "0070 ] As shown in FIGS .2c and 2d , the system can be configured to generate a KeyPressVector 31 from the current term input 11 .The KeyPressVector takes the form of an indexed series of probability distributions over character sequences .", "label": "", "metadata": {}, "score": "55.83716"}
{"text": "0070 ] As shown in FIGS .2c and 2d , the system can be configured to generate a KeyPressVector 31 from the current term input 11 .The KeyPressVector takes the form of an indexed series of probability distributions over character sequences .", "label": "", "metadata": {}, "score": "55.83716"}
{"text": "In embodiments where the predictive text engine also predicts punctuation , the punctuation items are stored in the n - gram maps with the text terms .Single punctuation items ( ' ! 'and blocks of punctuation ( ' ! ! ! ! ! ! ! ' , ' . . . ' ) are handled as single prediction units .", "label": "", "metadata": {}, "score": "55.84922"}
{"text": "In embodiments where the predictive text engine also predicts punctuation , the punctuation items are stored in the n - gram maps with the text terms .Single punctuation items ( ' ! 'and blocks of punctuation ( ' ! ! ! ! ! ! ! ' , ' . . . ' ) are handled as single prediction units .", "label": "", "metadata": {}, "score": "55.84922"}
{"text": "[ 0155 ] In the present example , the system has previously searched for the path corresponding to the context phrase \" see you \" .At this stage , the language model has obtained a set of predicted terms which are compatible with the context and the current input ( which may be represented by a KeyPressVector 31 ) , ordered by their respective probability values , as extracted from the n - gram map .", "label": "", "metadata": {}, "score": "55.8657"}
{"text": "[ 0155 ] In the present example , the system has previously searched for the path corresponding to the context phrase \" see you \" .At this stage , the language model has obtained a set of predicted terms which are compatible with the context and the current input ( which may be represented by a KeyPressVector 31 ) , ordered by their respective probability values , as extracted from the n - gram map .", "label": "", "metadata": {}, "score": "55.8657"}
{"text": "We then compare combinations of both methods .It is shown that a broadcast news language model can be compressed by up to 83 % to only 12.6Mb with no loss in performance on a broadcast news task .Compressing the language model further by quantization to 10.3Mb resulted in only a 0.4 % degradation in word error rate which is better than can be achieved through entropy - based pruning alone . ing all those elements in the list whose contribution to the language model entropy lies below some threshold [ 1].", "label": "", "metadata": {}, "score": "55.935936"}
{"text": "To perform interpolation , the probability of a word w i given context h is computed as a linear combination of the corresponding n - gram probabilities from the corpus language models 606a-606n : . di - elect cons .for all n -grams that are present in any of the language models 606a-606n ( i.e. the union of all n - grams ) .", "label": "", "metadata": {}, "score": "55.936455"}
{"text": "Other combinations are , of course , possible , and further examples of application specific language models include language models generated from newswires , blogs , academic papers , word processing and patents .[ 0060 ] In some embodiments , the system can further comprise additional language models 6 .", "label": "", "metadata": {}, "score": "56.044346"}
{"text": "c . )P . s .s .c . )The new model overcomes the limitations of a CFG model .For low resolution understanding ( task classification ) , no property preterminals are introduced into the template grammar .", "label": "", "metadata": {}, "score": "56.0832"}
{"text": "[ 0095 ] For a given set of prediction candidates and a certain number of context terms , the Bloom filter 17 reorders the predictions to reflect new probabilities .The present system utilizes a log - frequency Bloom filter ( Talbot and Osborne ) which maps a set of n - gram entries to respective probability estimates .", "label": "", "metadata": {}, "score": "56.102646"}
{"text": "The Multi - LM 8 generates the final predictions 9 by inserting the predictions 20 from each language model into an ordered STL ' multimap ' structure .[ 0064 ] An STL multimap is an associative key - value pair held in a binary tree structure , in which duplicate keys are allowed .", "label": "", "metadata": {}, "score": "56.146317"}
{"text": "This is illustrated in FIG .18C.This saves a substantial amount of space in the model 351 rendering it much more compact than prior systems .[ 0183 ] .This substantially reduces inefficiencies associated with sub - optimal two - pass systems and also includes prior knowledge in the language model to compensate for the lack of language model training data ( i.e. , to address data sparseness ) .", "label": "", "metadata": {}, "score": "56.205"}
{"text": "As yet another example , distributions can be generated on the basis of the distances ( or some function of the distances , such as the square etc . ) between the touch coordinate ( on a touch - screen virtual keyboard ) and the coordinates of designated keys .", "label": "", "metadata": {}, "score": "56.221848"}
{"text": "and blocks of punctuation ( ' ! ! ! ! ! ! ! ' , ' . . . ' ) are handled as single prediction units .[0115 ]The method further comprises the steps of generating concurrently , using a text prediction engine comprising a plurality of language models , text predictions from the multiple language models ; and providing text predictions for user selection .", "label": "", "metadata": {}, "score": "56.248417"}
{"text": "( RAID 2006 ) .Salad is based on n - gram models , that is , data is represented as all of its substrings of length n. During training these n - grams are stored in a Bloom filter .This enables the detector to represent a large number of n - grams in little memory and still being able to efficiently access the data .", "label": "", "metadata": {}, "score": "56.2493"}
{"text": "The -temp option allows the user to specify where the program should store its temporary files .The -files parameter is used to specify the number of files which can be open at one time .Output : A language model , in either binary format ( to be read by evallm ) , or in ARPA format .", "label": "", "metadata": {}, "score": "56.274067"}
{"text": "Our online models build character - level PAT trie structures on the fly using heavily data - unfolded implementations of an mutable daughter maps with a long integer count interface .Terminal nodes are shared .Character 8-gram training runs at 200,000 characters per second and allows online tuning of hyperparameters .", "label": "", "metadata": {}, "score": "56.43476"}
{"text": "0133 ] A classifier is employed in the topic filter 18 based on the principle of supervised learning in which a quantity of training data must first be collected and assigned labels representing topic categories .From this data , the classifier learns to infer likely topic category labels for new data .", "label": "", "metadata": {}, "score": "56.497513"}
{"text": "0133 ] A classifier is employed in the topic filter 18 based on the principle of supervised learning in which a quantity of training data must first be collected and assigned labels representing topic categories .From this data , the classifier learns to infer likely topic category labels for new data .", "label": "", "metadata": {}, "score": "56.497513"}
{"text": "Once a prediction is selected , the word fragments making up the prediction are known .Thus , the targets tagged to the word fragments can be paired to the input events , providing labeled training data .Alternatively , the targets of a selected prediction can be determined by reverse mapping word fragments to their targets at the target mapping stage 903 .", "label": "", "metadata": {}, "score": "56.56402"}
{"text": "The prior is therefore : . and the posterior is : . sup .( t),\u03b2 c . sup . sup . sup .( t ) ) , .\u03b1 c . sup . sup .[ 0180 ] Working through the known hyperparameter update relations for the NIWD distribution , the general update rules for this parameterization are : .", "label": "", "metadata": {}, "score": "56.56771"}
{"text": "[ 0013 ] The present invention represents a fundamental shift away from predominantly character - based text input to a predominately word- or phrase - based text input .[ 0015 ] Preferably , the text predictions are generated concurrently from the plurality of language models in real time .", "label": "", "metadata": {}, "score": "56.63646"}
{"text": "[ 0013 ] The present invention represents a fundamental shift away from predominantly character - based text input to a predominately word- or phrase - based text input .[ 0015 ] Preferably , the text predictions are generated concurrently from the plurality of language models in real time .", "label": "", "metadata": {}, "score": "56.63646"}
{"text": "[ 0009 ] .It is believed that prior systems which treated understanding and speech recognition as two separate , independent tasks , exhibit a number of disadvantages .First , the objective function being optimized when building an n - gram language model for a speech recognizer is the reduction of the test data perplexity , which is related to the reduction of the speech recognition word error rate .", "label": "", "metadata": {}, "score": "56.63958"}
{"text": "Evaluation of ARPA format language models .Version 2 of the toolkit includes the ability to calculate perplexities of ARPA format language models .Handling of context cues .In version 2 , one may have any number of context cues ( or none at all ) , and they may be represented by any symbols one chooses .", "label": "", "metadata": {}, "score": "56.69412"}
{"text": "Therefore , to achieve static pruning , the static pruning mechanism 5 generates a conversion table between the vocabulary identifiers in L1 and the vocabulary identifiers in L2 .The conversion table maps the identifier for a given term t in L1 , to the identifier for the term t in L2 .", "label": "", "metadata": {}, "score": "56.712418"}
{"text": "Therefore , to achieve static pruning , the static pruning mechanism 5 generates a conversion table between the vocabulary identifiers in L1 and the vocabulary identifiers in L2 .The conversion table maps the identifier for a given term t in L1 , to the identifier for the term t in L2 .", "label": "", "metadata": {}, "score": "56.712418"}
{"text": "Therefore , the input stream can comprise character , word and/or phrase inputs and/or punctuation inputs .In embodiments where the predictive text engine also predicts punctuation , the punctuation items are stored in the n - gram maps with the text terms .", "label": "", "metadata": {}, "score": "56.86396"}
{"text": "The set of predictions can be browsed by the user by using swipe gestures .The set of predictions can also be ordered based on their probability values , with the predictions with the highest probability values displayed first on the interface 60 .", "label": "", "metadata": {}, "score": "56.87377"}
{"text": "[ 0034 ] Embodiments described herein represent a fundamental shift away from predominantly character - based text input to a predominately word- or phrase - based text input .[0036 ] Preferably , the text predictions are generated concurrently from the plurality of language models in real time .", "label": "", "metadata": {}, "score": "56.89635"}
{"text": "No .7,083,342 , which also uses extra post - input disambiguation .[ 0014 ]Although reduced keyboards have a reduced error rate , they also quantize the input more roughly , meaning that there is less information content in the input the computer receives from a reduced keyboard than a full keyboard .", "label": "", "metadata": {}, "score": "56.910694"}
{"text": "2G. [ 0089 ] .Therefore , from the second example , the first rewrite rule is assigned a count of one third .[ 0090 ] .In the same way , the third example has three possible segmentations , one of which maps the preterminal ShowFlightCmd to \u03b5 .", "label": "", "metadata": {}, "score": "56.913857"}
{"text": "Each language model receives a copy of the current input 11 and the tokenized context 12 .[ 0119 ] Within each language model , the current input is fed into the approximate trie 13 , which in this case returns the set of identifiers for all vocabulary terms that begin with either ' v ' or ' x ' or ' z ' .", "label": "", "metadata": {}, "score": "56.921707"}
{"text": "To generate predictions from an n - gram map 14 , at each map node 21 the language model conducts a binary search to locate specified subsequent child nodes .For example , if the context comprises term1 and term2 , the language model will first locate the node for term1 .", "label": "", "metadata": {}, "score": "56.977562"}
{"text": "To generate predictions from an n - gram map 14 , at each map node 21 the language model conducts a binary search to locate specified subsequent child nodes .For example , if the context comprises term1 and term2 , the language model will first locate the node for term1 .", "label": "", "metadata": {}, "score": "56.977562"}
{"text": "0318 ] The context candidate model function computes the probability of a given path as follows ( equated with the context candidate estimate ) : . [ 0320 ] where K is the number of edges in the path .It can be noted that this preferred formulation amounts to an implicit independence assumption between nodes .", "label": "", "metadata": {}, "score": "57.006905"}
{"text": "0264 ] Character Model -- implements a distribution over sequences in a language without the concept of a fixed vocabulary .Usually implemented as a Markov model over character sequences .[ 0265 ] A character model is a sequence model built from characters instead of words .", "label": "", "metadata": {}, "score": "57.025955"}
{"text": "Harry is a small tool for comparing strings and measuring their similarity .It implements several common distance and kernel functions for strings , as well as some exotic similarity measures .For example , Harry supports the Levenshtein ( edit ) distance , the Jaro - Winkler distance , and the compression distance .", "label": "", "metadata": {}, "score": "57.07482"}
{"text": "An element consists of a key , for ordering the sequence , and a mapped value .In the STL multimap of the present system , a prediction is a string value mapped to a probability value , and the map is ordered on the basis of the probabilities , i.e. the probability values are used as keys in the multimap and the strings as values .", "label": "", "metadata": {}, "score": "57.097786"}
{"text": "However , in other embodiments the ordering of the applied filters or the types of applied filter can be changed .[ 0094 ] A Bloom filter 17 is a randomized data structure used to store sets of objects in a highly efficient manner using bit - arrays and combinations of hash functions .", "label": "", "metadata": {}, "score": "57.14829"}
{"text": "While T9 , iTap , SureType etc . are based on term dictionaries , the present system is based on adaptive probabilistic language model technology , which takes into account multiple contextual terms and combines information from multiple language domains in a mathematically well - founded and computationally efficient manner .", "label": "", "metadata": {}, "score": "57.150963"}
{"text": "While T9 , iTap , SureType etc . are based on term dictionaries , the present system is based on adaptive probabilistic language model technology , which takes into account multiple contextual terms and combines information from multiple language domains in a mathematically well - founded and computationally efficient manner .", "label": "", "metadata": {}, "score": "57.150963"}
{"text": "While T9 , iTap , SureType etc . are based on term dictionaries , the present system is based on adaptive probabilistic language model technology , which takes into account multiple contextual terms and combines information from multiple language domains in a mathematically well - founded and computationally efficient manner .", "label": "", "metadata": {}, "score": "57.150963"}
{"text": "The use of filters is described in detail above .[ 0391 ]In further reference to the target modeling module 82 , when a user touches the screen of the small screen device , an input event is generated .As described in reference to FIG .", "label": "", "metadata": {}, "score": "57.159206"}
{"text": "This is because the candidates returned from the approximate trie do not have a probability value associated with them .The approximate trie is interrogated to return possible candidates only .[0125 ] To compute the intersection of the set of candidates returned by the probabilistic trie 33 and the n - gram map , the intersection mechanism 16 follows the same procedure as set out with relation to the approximate tri 13 .", "label": "", "metadata": {}, "score": "57.16256"}
{"text": "This is because the candidates returned from the approximate trie do not have a probability value associated with them .The approximate trie is interrogated to return possible candidates only .[0125 ] To compute the intersection of the set of candidates returned by the probabilistic trie 33 and the n - gram map , the intersection mechanism 16 follows the same procedure as set out with relation to the approximate tri 13 .", "label": "", "metadata": {}, "score": "57.16256"}
{"text": "[ 0110 ] As the reader will understand , a candidate filter is not required to narrow the predictions returned by a probabilistic trie 33 ( see FIGS . 2b and 2d ) , because a probabilistic trie 33 is not restricted to a specified maximum depth .", "label": "", "metadata": {}, "score": "57.239956"}
{"text": "[ 0110 ] As the reader will understand , a candidate filter is not required to narrow the predictions returned by a probabilistic trie 33 ( see FIGS . 2b and 2d ) , because a probabilistic trie 33 is not restricted to a specified maximum depth .", "label": "", "metadata": {}, "score": "57.239956"}
{"text": "0121 ] Given a set of identifiers from the approximate trie 13 and the n - gram map 14 , the intersection is computed by an intersection mechanism 16 .If the number of predictions in the resulting set is less than p , or some multiple of p ( where p is the required number of predictions ) , the system continues to look for further predictions by returning to the n - gram map 14 and considering smaller contexts .", "label": "", "metadata": {}, "score": "57.258064"}
{"text": "If the system is generating a prediction for the nth term , the context input 12 will contain the preceding n-1 terms that have been selected and input into the system by the user .[0081 ] The language model utilizes an n - gram map 14 to generate word and/or phrase predictions based on the context input 12 .", "label": "", "metadata": {}, "score": "57.339985"}
{"text": "For example the ' user community ' specific language model could comprise a local dialect or interest grouping specific language model .[ 0055 ] The language models discussed so far are static language models .That is , they are generated from a representative body of text and thereafter are not changed .", "label": "", "metadata": {}, "score": "57.394875"}
{"text": "For example the ' user community ' specific language model could comprise a local dialect or interest grouping specific language model .[ 0055 ] The language models discussed so far are static language models .That is , they are generated from a representative body of text and thereafter are not changed .", "label": "", "metadata": {}, "score": "57.394875"}
{"text": "The probability distributions associated with each keystroke can be generated in a multiplicity of ways .As a non - limiting example , given a standard QWERTY keyboard layout , it can be assumed that if the user enters a particular character , there is some probability that he / she actually meant to press the characters immediately adjacent .", "label": "", "metadata": {}, "score": "57.45054"}
{"text": "The probability distributions associated with each keystroke can be generated in a multiplicity of ways .As a non - limiting example , given a standard QWERTY keyboard layout , it can be assumed that if the user enters a particular character , there is some probability that he / she actually meant to press the characters immediately adjacent .", "label": "", "metadata": {}, "score": "57.45054"}
{"text": "This approach allows the user to enter text by selecting word candidates as they are presented before the characters corresponding to the word have been completely entered , allowing minimal interaction with the character pane 62 and faster entry of text .", "label": "", "metadata": {}, "score": "57.459747"}
{"text": "The Multi - LM 8 generates the final predictions 9 by inserting the predictions 20 from each language model into an ordered associative structure which may be an ordered STL ' multimap ' structure .[0058 ]An ordered associative structure is an abstract data type composed of a collection of unique keys and a collection of values , where each key is associated with one value ( or set of values ) .", "label": "", "metadata": {}, "score": "57.467743"}
{"text": "The Multi - LM 8 generates the final predictions 9 by inserting the predictions 20 from each language model into an ordered associative structure which may be an ordered STL ' multimap ' structure .[0058 ]An ordered associative structure is an abstract data type composed of a collection of unique keys and a collection of values , where each key is associated with one value ( or set of values ) .", "label": "", "metadata": {}, "score": "57.467743"}
{"text": "3A ) is added for each occurrence of the word sequence .[ 0132 ] .Another difference from the embodiment described for segmentation disambiguation with respect to FIGS . 2 - 3B involves the E - step of the EM algorithm .", "label": "", "metadata": {}, "score": "57.55774"}
{"text": "Examples of such paths could include \" ab \" , \" ac \" , \" ad \" , \" a-\" etc . .[ 0107 ] A similar concept can be implemented to insert a null character , i.e. to ignore a character entered by a user .", "label": "", "metadata": {}, "score": "57.61306"}
{"text": "Examples of such paths could include \" ab \" , \" ac \" , \" ad \" , \" a-\" etc . .[ 0107 ] A similar concept can be implemented to insert a null character , i.e. to ignore a character entered by a user .", "label": "", "metadata": {}, "score": "57.61306"}
{"text": "0114 ]A method according to the present invention is now described with reference to FIG .7 which is a flow chart of a method for processing user text input and generating text predictions .In the particular method described , the first step comprises receipt of text input .", "label": "", "metadata": {}, "score": "57.629166"}
{"text": "This might yield the following prediction set ' much ' , ' soon ' , ' good ' , ' many ' , ' well ' .This process continues to iterate as input progresses .The dynamic language model 7 assigns numerical identifiers to the tokenized input 12 and updates the n - gram map 14 .", "label": "", "metadata": {}, "score": "57.633064"}
{"text": "The system of claim 55 , wherein the language model is configured to conduct a search of the n - gram map to determine word or phrase predictions for a next term on the basis of up to n-1 terms of preceding text input .", "label": "", "metadata": {}, "score": "57.663536"}
{"text": "It is also possible to model the probability distribution using a true Bayesian estimator such as Variational Bayes and Gibbs sampling .While the target modeling set and its prediction is described in terms of regression , alternative embodiments can use classification , consisting of a machine learner , to predict the targets selected by the user .", "label": "", "metadata": {}, "score": "57.704254"}
{"text": "[ 0159 ] .The most probable sequence of hypothesis words can be provided to an optional confidence measure module 520 .Confidence measure module 520 identifies which words are most likely to have been improperly identified by the speech recognizer .", "label": "", "metadata": {}, "score": "57.79914"}
{"text": "As explained in the discussion of FIG .1 , the output predictions 20 from each language model are aggregated by the multi - LM 8 to generate the final set of predictions 10 that are provided to a user interface for display and user selection . [", "label": "", "metadata": {}, "score": "57.871178"}
{"text": "As explained in the discussion of FIG .1 , the output predictions 20 from each language model are aggregated by the multi - LM 8 to generate the final set of predictions 10 that are provided to a user interface for display and user selection . [", "label": "", "metadata": {}, "score": "57.871178"}
{"text": "To achieve the reverse mapping the selected prediction 920 and the corresponding string of input events are passed to a splitting mechanism .The splitting mechanism may split the prediction 920 into word fragments by evaluating the combination of word fragments with the highest probability for that selected prediction 920 .", "label": "", "metadata": {}, "score": "57.89057"}
{"text": "Syntax : . perplexity -text . text [ -probs .fprobs ] [ -oovs .oov_file ][ -annotate . fblist ] [ -include_unks ] .If the -probs parameter is specified , then each individual word probability will be written out to the specified probability stream file .", "label": "", "metadata": {}, "score": "57.94291"}
{"text": "0389 ] In one embodiment , as described above in reference to at least FIGS .9 , 10 and 12 , the text prediction engine comprises at least one model configured to generate from an evidence source a first set of sequences with associated probability estimates .", "label": "", "metadata": {}, "score": "57.99504"}
{"text": "The size of the font used for the words displayed within the prediction pane 64 can also be dynamically modified to fit the space available within the predicted word panes 66 and 68 .For instance , longer words can be displayed in a smaller font than shorter words so that both a long word and a short word can be fully displayed within the same space .", "label": "", "metadata": {}, "score": "57.995316"}
{"text": "The candidate model implements a conditional distribution over context observations given a particular candidate interpretation .The sequence model implements a conditional distribution over sequences in a language , or set of languages , given a particular context .In FIG .", "label": "", "metadata": {}, "score": "57.996048"}
{"text": "This might yield the following prediction set ' much ' , ' soon ' , ' good ' , ' many ' , ' well ' .This process continues to iterate as input progresses .The dynamic language model 7 assigns numerical identifiers to the tokenised input 12 and updates the n - gram map 14 .", "label": "", "metadata": {}, "score": "58.00657"}
{"text": "This might yield the following prediction set ' much ' , ' soon ' , ' good ' , ' many ' , ' well ' .This process continues to iterate as input progresses .The dynamic language model 7 assigns numerical identifiers to the tokenised input 12 and updates the n - gram map 14 .", "label": "", "metadata": {}, "score": "58.00657"}
{"text": "Assumption 1 : Evidence can be separated into distinct sets , such that the evidence in each set is conditionally independent of all others , given the target sequence ; .[ 0244 ] where each e i has a model M i associated with it .", "label": "", "metadata": {}, "score": "58.02764"}
{"text": "According to one aspect , the invention provides a method of building a language model which begins with providing a text and a first set of count files .Respective count files are associated with one or more corresponding text elements .", "label": "", "metadata": {}, "score": "58.03694"}
{"text": "m. \" \" Sun at 3p .m. \" \" Sunday at 3 p.m. \" \" sunday at 3 p.m. \" \" Sun at 3 p.m. \" .[ 0324 ] Probabilities are assigned to the edges according to the context candidate model , following ( 19 ) , for instance as illustrated in FIG .", "label": "", "metadata": {}, "score": "58.03737"}
{"text": "Abstract : .Methods for building arbitrarily large language models are presented herein .The methods provide a scalable solution to estimating a language model using a large data set by breaking the language model estimation process into sub - processes and parallelizing computation of various portions of the process .", "label": "", "metadata": {}, "score": "58.111507"}
{"text": "[ 0053 ] .It should be noted that the present invention can be carried out on a computer system such as that described with respect to FIG .1 .However , the present invention can be carried out on a server , a computer devoted to message handling , or on a distributed system in which different portions of the present invention are carried out on different parts of the distributed computing system .", "label": "", "metadata": {}, "score": "58.12251"}
{"text": "[ 0138 ] .Further , some commands occur in the training data more frequently than others .Therefore , the prior probability of the commands is modeled in model 342 .[ 0139 ] .The present invention will now be described in greater detail with respect to another example .", "label": "", "metadata": {}, "score": "58.167492"}
{"text": "The present system utilizes a mechanism 5 , static pruning , across all static language models , to reduce the amount of information stored in the system .If duplicate or multiple ( in the case of a system comprising three or more language models ) entries are detected , the mechanism 5 ' prunes ' the language models by retaining only the most probable entry .", "label": "", "metadata": {}, "score": "58.188343"}
{"text": "Preferably , in response to the input of text which is not represented in the language model , the method includes the step of inserting new paths in the n - gram language model .[0038 ] There is also provided , in accordance with the invention a computer program product including a computer readable medium having stored thereon computer program means for causing a processor to carry out the method of the invention .", "label": "", "metadata": {}, "score": "58.213314"}
{"text": "Preferably , in response to the input of text which is not represented in the language model , the method includes the step of inserting new paths in the n - gram language model .[0038 ] There is also provided , in accordance with the invention a computer program product including a computer readable medium having stored thereon computer program means for causing a processor to carry out the method of the invention .", "label": "", "metadata": {}, "score": "58.213314"}
{"text": "Note that the system has traversed two states within the trie to follow the \" ' l \" path , but this is considered a single probabilistic transaction , as specified by the KeyPressVector because the user did not enter the apostrophe .", "label": "", "metadata": {}, "score": "58.224525"}
{"text": "Note that the system has traversed two states within the trie to follow the \" ' l \" path , but this is considered a single probabilistic transaction , as specified by the KeyPressVector because the user did not enter the apostrophe .", "label": "", "metadata": {}, "score": "58.224525"}
{"text": "The computer - readable storage of claim 42 , wherein providing a first language model includes calculating a plurality of probabilities related to the likelihood of selected n - grams and storing the plurality of probabilities in the data files of the first set of data files .", "label": "", "metadata": {}, "score": "58.342323"}
{"text": "The search continues from left to right until it reaches the right most column .When it makes a transition , a score is obtained by adding an appropriate log probability to the score of the starting node .The score is then compared with that of the destination node , and replaces it if the new score is higher .", "label": "", "metadata": {}, "score": "58.373947"}
{"text": "From the above PCSG example , the branch illustrated in FIG .14A can be considered .[0336 ] As \" sunday \" is the original observation , it will initially be assigned a probability of 0.8 by stage one of the above algorithm , and the other edges will each be assigned 0.1 .", "label": "", "metadata": {}, "score": "58.39631"}
{"text": "The input probability generator 902 queries the target model set 906 to determine the one or more likely targets .That is , the input probability generator 902 generates a probability that the user intended to select that target , through the input event , by querying the model for that target .", "label": "", "metadata": {}, "score": "58.419914"}
{"text": "This is a success , and the relevant probabilistic multiplications are made in both cases , yielding current states of 0.1 .As there are no further elements in the KeyPressVector , the system returns the values in the nodes at each end point , along with their respective probability state values , in this case the indentifiers 2 and 4 , both mapped to the probability value 0.1 .", "label": "", "metadata": {}, "score": "58.4505"}
{"text": "This is a success , and the relevant probabilistic multiplications are made in both cases , yielding current states of 0.1 .As there are no further elements in the KeyPressVector , the system returns the values in the nodes at each end point , along with their respective probability state values , in this case the indentifiers 2 and 4 , both mapped to the probability value 0.1 .", "label": "", "metadata": {}, "score": "58.4505"}
{"text": "These have not been widely adopted due to limitations in accuracy and the adverse effects of noisy environments .[0008 ] Touch - screen devices offer a highly flexible platform for different styles of text input , and there are many different models currently available .", "label": "", "metadata": {}, "score": "58.569313"}
{"text": "In any case , the output does not need to be sorted in order to serve as input for wfreq2vocab .wfreq .Higher values for the -hash parameter require more memory , but can reduce computation time . vocab .The -top parameter allows the user to specify the size of the vocabulary ; if the program is called with the command -top 20000 , then the vocabulary will consist of the most common 20,000 words .", "label": "", "metadata": {}, "score": "58.571438"}
{"text": "The system of claim 56 , wherein the language model comprises a mechanism to compute the intersection of the predictions determined by the approximate or probabilistic trie , and the n - gram map , by searching for and retaining only identifiers that are present in both prediction sets .", "label": "", "metadata": {}, "score": "58.617016"}
{"text": "0017 ] .[ 0017]FIG .2E illustrates an example parse tree .[ 0018 ] .[ 0018]FIG .2F illustrates a table of possible preterminals for words in examples .[ 0019 ] .[ 0019]FIG .2 G is a table of re - write rules with associated counts and probabilities .", "label": "", "metadata": {}, "score": "58.623283"}
{"text": "This type of model might be used in a command / control environment where the vocabulary is restricted to the number of commands that the system understands , and we can therefore guarantee that no OOVs will occur in the training or test data .", "label": "", "metadata": {}, "score": "58.636494"}
{"text": "0007 ] Many of the input models discussed above utilise some form of text prediction technology .Known prediction models for enhancing text input have two main functions : . [0008 ] 1 ) Disambiguation of multiple - character keystrokes .", "label": "", "metadata": {}, "score": "58.65059"}
{"text": "0007 ] Many of the input models discussed above utilise some form of text prediction technology .Known prediction models for enhancing text input have two main functions : . [0008 ] 1 ) Disambiguation of multiple - character keystrokes .", "label": "", "metadata": {}, "score": "58.65059"}
{"text": "Preferably , the language model is configured to conduct a search of the approximate trie to ascertain word predictions based on at least one inputted character .[ 0042 ] Additionally , the language model may include a candidate filter to narrow the predictions determined by the approximate trie , wherein the candidate filter is configured to discard all candidate strings for which the current input is not a substring .", "label": "", "metadata": {}, "score": "58.670677"}
{"text": "[ 0352 ] The input candidate EPCSG generation process starts with an ordered list of sets of sequence - probability pairs , generated by the system from user interaction , where each subset represents a probability distribution over user input sequence intentions .", "label": "", "metadata": {}, "score": "58.67547"}
{"text": "In an alternative embodiment , the number of words displayed in the prediction pane 64 can be based on the length of the words predicted .For instance , if two words are displayed in the prediction pane 64 leaving sufficient space for a third word , then three words can be dynamically displayed instead .", "label": "", "metadata": {}, "score": "58.73223"}
{"text": "No .6,307,549 , which provides a method for disambiguating the output from a reduced keyboard ( usually about 3 characters per key ) .Word - level disambiguation is provided using a selection button which enables a user to cycle through possible matches to the current input .", "label": "", "metadata": {}, "score": "58.76237"}
{"text": "The -records parameter allows the user to specify how many of the word and count records to allocate memory for .If the number of words in the input exceeds this number , then the program will fail , but a high number will obviously result in a higher memory requirement .", "label": "", "metadata": {}, "score": "58.877842"}
{"text": "These have not been widely adopted due to limitations in accuracy and the adverse effects of noisy environments .[ 0005 ] Touch - screen devices offer a highly flexible platform for different styles of text input , and there are many different models currently available .", "label": "", "metadata": {}, "score": "58.887085"}
{"text": "These have not been widely adopted due to limitations in accuracy and the adverse effects of noisy environments .[ 0005 ] Touch - screen devices offer a highly flexible platform for different styles of text input , and there are many different models currently available .", "label": "", "metadata": {}, "score": "58.887085"}
{"text": "[ 0184 ] .Although the present invention has been described with reference to particular embodiments , workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention .", "label": "", "metadata": {}, "score": "58.918365"}
{"text": "If an identical path exists in L2 , then the static pruning mechanism 5 makes a comparison of the probabilities at each node .If the L1 probability is smaller than the L2 probability , and the node is terminal , then the static pruning mechanism 5 removes this node from L1 .", "label": "", "metadata": {}, "score": "58.935978"}
{"text": "If an identical path exists in L2 , then the static pruning mechanism 5 makes a comparison of the probabilities at each node .If the L1 probability is smaller than the L2 probability , and the node is terminal , then the static pruning mechanism 5 removes this node from L1 .", "label": "", "metadata": {}, "score": "58.935978"}
{"text": "If an identical path exists in L2 , then the static pruning mechanism 5 makes a comparison of the probabilities at each node .If the L1 probability is smaller than the L2 probability , and the node is terminal , then the static pruning mechanism 5 removes this node from L1 .", "label": "", "metadata": {}, "score": "58.935978"}
{"text": "The method generates a set of pruned merged counts 602a-602n for each input text corpus , as described in FIG .1 .The pruned merged counts 602a-602n are used to estimate language models ( steps 604a-604n ) .The language models 602a-602n are estimated using n - gram probability estimates derived directly from the merged counts 602a-602n , as well as those derived using smoothing and back - off algorithms to assign probabilities to unobserved or pruned n - grams .", "label": "", "metadata": {}, "score": "58.98003"}
{"text": "5 .During the M - step , the counts are normalized .[ 0131 ] .In other words , in training the n - gram , a full count need not be added for each occurrence of a word sequence .", "label": "", "metadata": {}, "score": "59.01748"}
{"text": "The computer - readable storage of claim 51 , comprising instructions for causing the computing device to prune , in parallel , respective data files of the set of merged data files .Description : .BACKGROUND .[0002 ] Language models provide probabilities for sequences of words and are a primary component in most modern speech and language applications .", "label": "", "metadata": {}, "score": "59.0276"}
{"text": "According to one feature , the method includes generating a combined language model .The combined language model is generated by merging respective data files of the first language model with corresponding data files of the second language model .The corresponding data files are merged in parallel .", "label": "", "metadata": {}, "score": "59.03863"}
{"text": "To enter a term which does not exist in the vocabulary of the language model D , a user can insert the term by inputting it character - by - character into the user interface of the system .When a user enters the term t at a later time , the language model D increments a count value stored at the node , of the n - gram map , representing the term t by one , and increments the total value of its parent node by one also .", "label": "", "metadata": {}, "score": "59.197357"}
{"text": "To enter a term which does not exist in the vocabulary of the language model D , a user can insert the term by inputting it character - by - character into the user interface of the system .When a user enters the term t at a later time , the language model D increments a count value stored at the node , of the n - gram map , representing the term t by one , and increments the total value of its parent node by one also .", "label": "", "metadata": {}, "score": "59.197357"}
{"text": "To enter a term which does not exist in the vocabulary of the language model D , a user can insert the term by inputting it character - by - character into the user interface of the system .When a user enters the term t at a later time , the language model D increments a count value stored at the node , of the n - gram map , representing the term t by one , and increments the total value of its parent node by one also .", "label": "", "metadata": {}, "score": "59.197357"}
{"text": "In an embodiment , each of the plurality of language models comprises an n - gram map and an approximate trie and the method includes the step of conducting a search of the approximate trie to ascertain word predictions based on at least one inputted character .", "label": "", "metadata": {}, "score": "59.203644"}
{"text": "The probabilities are estimated from data observed in training and through dynamic usage of the system .Here , \" context \" denotes both the terms that have occurred previously in the sequence , as well as any knowledge the system has about the current term ( e.g. it starts with a specific character or characters , or it is indicative of a certain topic ) .", "label": "", "metadata": {}, "score": "59.234825"}
{"text": "The probabilities are estimated from data observed in training and through dynamic usage of the system .Here , \" context \" denotes both the terms that have occurred previously in the sequence , as well as any knowledge the system has about the current term ( e.g. it starts with a specific character or characters , or it is indicative of a certain topic ) .", "label": "", "metadata": {}, "score": "59.234825"}
{"text": "The evidence sources can include the user inputted text , including the phrase already entered by the user and the current word being typed by the user .[ 0390 ]In one embodiment , the choice of predictions displayed on the prediction pane 64 of interface 60 can be based on the probability values associated with the predictions generated by the text prediction engine .", "label": "", "metadata": {}, "score": "59.23706"}
{"text": "[ 0229 ]Yet In general , but not exclusive terms , the present embodiment can be implemented as shown in FIG .10 .FIG .10 is a block diagram of one high level text prediction architecture according to the invention .", "label": "", "metadata": {}, "score": "59.253445"}
{"text": "Computational issues and applications .The experiments have been mostly on small corpora , where training a neural network language model is easier , and show important improvements on both log - likelihood and speech recognition accuracy .Resampling techniques may be used to train the neural network language model on corpora of several hundreds of millions of words ( Schwenk and Gauvain 2004 ) .", "label": "", "metadata": {}, "score": "59.34462"}
{"text": "For instance , the input evidence might actually be a series of touch coordinates from a virtual ' keyboard ' .As can be seen in FIG .12 , the prediction engine preferably comprises a target sequence prior model R. Although preferred , the system is not limited to embodiments which include a target sequence prior model R. .", "label": "", "metadata": {}, "score": "59.457535"}
{"text": "0122 ]In the present example , the system has previously searched for the path corresponding to the context phrase \" see you \" .At this stage , the language model has obtained a set of predicted terms which are compatible with the context and the current input , ordered by their respective probability values , as extracted from the n - gram map .", "label": "", "metadata": {}, "score": "59.50894"}
{"text": "Output : List of every i d n - gram which occurred in the text , along with its number of occurrences .Notes : Maps each word in the text stream to a short integer as soon as it has been read , thus enabling more n - grams to be stored and sorted in memory .", "label": "", "metadata": {}, "score": "59.51983"}
{"text": "[0257 ] Assumption 3 : The likelihood , under the model in question , can be expressed as a marginalization over candidates , where the target sequence is conditionally independent of the evidence , given the candidate .[ 0258 ] Applying this assumption , the dependence on s can be dropped from the evidence term : . [", "label": "", "metadata": {}, "score": "59.531"}
{"text": "-calc_mem demands that the i d n - gram file should be read twice , so that we can accurately calculate the amount of memory required .-buffer allows the user to specify an amount of memory to grab , and divides this memory equally between the 2,3 , ... , n - gram tables .", "label": "", "metadata": {}, "score": "59.597588"}
{"text": "The -min_alpha , -max_alpha and -out_of_range_alphas are parameters used by the functions for using two - byte alphas .Their values should only be altered if the program instructs it .For further details , see the comments in the source file src / two_byte_alphas.c . compute - PP - Output is the perplexity of the language model with respect to the input text stream .", "label": "", "metadata": {}, "score": "59.6003"}
{"text": "claim 12 wherein referring to a separate backoff model comprises : . referring to a uniform distribution n - gram .The method of . claim 13 wherein assigning a backoff probability comprises : . assigning a uniform distribution score to every word in the vocabulary .", "label": "", "metadata": {}, "score": "59.60189"}
{"text": "However , naive implementations of the above equations yield predictors that are too slow for large scale natural language applications .Schwenk and Gauvain ( 2004 ) were able to build systems in which the neural network component took less than 5 % of real - time ( the duration of the speech being analyzed ) .", "label": "", "metadata": {}, "score": "59.630848"}
{"text": "The -temp option allows the user to specify where the program should store its temporary files .In the case of really huge quantities of data , it may be the case that more temporary files are generated than can be opened at one time by the filing system .", "label": "", "metadata": {}, "score": "59.648594"}
{"text": "FDPCity . )Then , the new counts are multiplied by the normalization factor to obtain the new probabilities .As shown in FIG .3B , component 302 iterates on this process , re - estimating the new counts and the new probabilities until the probabilities converge .", "label": "", "metadata": {}, "score": "59.701767"}
{"text": "0068 ] Thus the present system utilizes two types of language models , static and dynamic .The user specific language model 7 is an example of a dynamic language model .The probabilities are estimated from data observed in training and through dynamic usage of the system .", "label": "", "metadata": {}, "score": "59.708214"}
{"text": "[0168 ] .However , one significant modification is made to the conventional algorithm in order to significantly reduce the size of the model .This is illustrated by FIGS .18A , 18B and 18 C. FIGS .18A and 18C illustrate finite state representations of the bigram language models for two pre - terminals , each has been trained with two observed words ( designated a , b and c , d , respectively ) .", "label": "", "metadata": {}, "score": "59.801712"}
{"text": "h before compiling the toolkit .The program will also report the frequency of frequency of n - grams , and the corresponding recommended value for the -spec_num parameters of idngram2lm .The -fof_size parameter allows the user to specify the length of this list .", "label": "", "metadata": {}, "score": "59.80641"}
{"text": "This task is time consuming , error prone , and requires a significant amount of expertise in the domain .[ 0006 ] .In order to advance the development of speech enabled applications and services , an example - based grammar authoring tool has been introduced .", "label": "", "metadata": {}, "score": "59.80679"}
{"text": "In some embodiments , especially if the language models 606a-606n are large , interpolation of several models 606a-606n may exceed the computer 's physical memory , and thus each model 606a-606n is pruned prior to interpolation .[ 0046 ]FIG .", "label": "", "metadata": {}, "score": "59.85457"}
{"text": "If the language model predicts punctuation , the punctuation terms appear in the same location as the predicted words or phrases , at the bottom of the prediction pane 25 , thereby providing the user with consistency .[ 0161 ] The user interface can be configured for multiple word ( phrase ) input .", "label": "", "metadata": {}, "score": "59.9478"}
{"text": "[0007 ] .However , a purely rules - based grammar in a NLU system can still lack robustness and exhibit brittleness .[0008 ] .In addition , most , if not all , prior approaches treat understanding as a separate problem , independent of speech recognition .", "label": "", "metadata": {}, "score": "59.966892"}
{"text": "We present Tightly Packed Tries ( TPTs ) , a compact implementation of read - only , compressed trie structures with fast on - demand paging and short load times .We demonstrate the benefits of TPTs for storing n - gram back - off language models and phrase tables for statistical machine translation .", "label": "", "metadata": {}, "score": "60.000633"}
{"text": "We present Tightly Packed Tries ( TPTs ) , a compact implementation of read - only , compressed trie structures with fast on - demand paging and short load times .We demonstrate the benefits of TPTs for storing n - gram back - off language models and phrase tables for statistical machine translation .", "label": "", "metadata": {}, "score": "60.000633"}
{"text": "The user can enter multi - character ' v / x / z ' input to prompt the predictive text engine 100 to provide more relevant predictions .[0151 ] The Multi - LM 8 then passes the tokenised sequence data 12 and the multi - character current input 11 to each of the language models .", "label": "", "metadata": {}, "score": "60.10393"}
{"text": "The user can enter multi - character ' v / x / z ' input to prompt the predictive text engine 100 to provide more relevant predictions .[0151 ] The Multi - LM 8 then passes the tokenised sequence data 12 and the multi - character current input 11 to each of the language models .", "label": "", "metadata": {}, "score": "60.10393"}
{"text": "2a - d , it can be seen that in the absence of a current word input 11 , and therefore the absence of a KeypressVector 31 also , the predictions are based on a context input only 12 .The system can also use \" beginning - of - sequence \" ( BOS ) markers to determine word or phrase predictions after end - of - sentence punctuation and/or after new line entry .", "label": "", "metadata": {}, "score": "60.1082"}
{"text": "2a - d , it can be seen that in the absence of a current word input 11 , and therefore the absence of a KeypressVector 31 also , the predictions are based on a context input only 12 .The system can also use \" beginning - of - sequence \" ( BOS ) markers to determine word or phrase predictions after end - of - sentence punctuation and/or after new line entry .", "label": "", "metadata": {}, "score": "60.1082"}
{"text": "[ 0014 ]In one implementation , the data files of the first set of data files and the data files of the second set of data files store probability measurements .The probability measurements indicate the probability of occurrence of various selected n - grams .", "label": "", "metadata": {}, "score": "60.116787"}
{"text": "0106 ] The language model returns its predictions 20 as a set of terms / phrases mapped to probability values .As explained in the discussion of FIG .1 , the output predictions 20 from each language model are aggregated by the multi - LM 8 to generate the final set of predictions 10 that are provided to a user interface for display and user selection .", "label": "", "metadata": {}, "score": "60.12755"}
{"text": "16 used on a personal watch with a small screen ; and .[ 0033 ] FIG .18 illustrates a high level architecture of a text entry system , the system comprising of a small screen device with a text entry interface communicating with a target modeling module and a text prediction engine .", "label": "", "metadata": {}, "score": "60.13591"}
{"text": "17A and 17B ) not only in natural language understanding , but also in speech recognition .This overcomes the suboptimal two - pass approach used in prior systems to accomplish speech recognition and natural language understanding , and it uses prior knowledge so that it can illustratively generalize better than prior systems .", "label": "", "metadata": {}, "score": "60.145393"}
{"text": "The updating of a dynamic language model is described in a later section of the description , with reference to the structure of a single language model .[ 0063 ] Thus the present system utilises two types of language models , static and dynamic .", "label": "", "metadata": {}, "score": "60.21055"}
{"text": "The updating of a dynamic language model is described in a later section of the description , with reference to the structure of a single language model .[ 0063 ] Thus the present system utilises two types of language models , static and dynamic .", "label": "", "metadata": {}, "score": "60.21055"}
{"text": "During input , the user is presented with a set of ' objects ' that are considered likely intended character sequences .As the user selects his / her intended input sequences , the ( potentially weighted ) horizontal and vertical offsets between the input event coordinates and the character coordinates are computed , and the relevant moving average used to calibrate input event coordinates to reflect a user 's typing style .", "label": "", "metadata": {}, "score": "60.304825"}
{"text": "If the system is generating a prediction for the nth term , the context input 12 will contain the preceding n-1 terms that have been selected and input into the system by the user .[ 0112 ] The language model utilises an n - gram map 14 to generate word and/or phrase predictions based on the context input 12 .", "label": "", "metadata": {}, "score": "60.375404"}
{"text": "If the system is generating a prediction for the nth term , the context input 12 will contain the preceding n-1 terms that have been selected and input into the system by the user .[ 0112 ] The language model utilises an n - gram map 14 to generate word and/or phrase predictions based on the context input 12 .", "label": "", "metadata": {}, "score": "60.375404"}
{"text": "The type of a slot specifies the requirement for its \" fillers \" .For both the ACity and DCity slots , the filler must be an expression modeled in the grammar library that refers to an object of the type \" City \" .", "label": "", "metadata": {}, "score": "60.423454"}
{"text": "Preferably , the user interface is configured for word or phrase input , dependent on which term is chosen for input in a given sequence of words .[ 0046 ]In one embodiment , the word prediction pane includes one or more word keys to present predicted words and wherein , in response to a word key press , the user interface is configured to display the word in the typing pane .", "label": "", "metadata": {}, "score": "60.440353"}
{"text": "The method of claim 20 , further comprising calculating back - off weights to assign probability estimates to a second set of n - grams that are not present in the first or second sets of data files .The method of claim 19 , wherein merging respective ones of the first set of data files with corresponding ones of the second set of data files comprises interpolating corresponding probability measurements .", "label": "", "metadata": {}, "score": "60.4476"}
{"text": "In response to the input of end - of - sentence punctuation or a ' return ' character , or at an otherwise predetermined time , the user inputted text sequence is passed to the Multi - LM 8 which splits the text sequence into ' tokens ' as described later .", "label": "", "metadata": {}, "score": "60.508175"}
{"text": "In response to the input of end - of - sentence punctuation or a ' return ' character , or at an otherwise predetermined time , the user inputted text sequence is passed to the Multi - LM 8 which splits the text sequence into ' tokens ' as described later .", "label": "", "metadata": {}, "score": "60.508175"}
{"text": "Note : For this program to be successful , it is important that the vocabulary file is in alphabetical order .If you are using vocabularies generated by the wfreq2vocab tool then this should not be an issue , as they will already be alphabetically sorted .", "label": "", "metadata": {}, "score": "60.534744"}
{"text": "The -context parameter allows the user to specify a file containing a list of words within the vocabulary which will serve as context cues ( for example , markers which indicate the beginnings of sentences and paragraphs ) .-calc_mem , -buffer and -spec_num x y ...", "label": "", "metadata": {}, "score": "60.69134"}
{"text": "[ 0063 ] The text prediction engine 100 operates to generate concurrently text predictions 20 from the multiple language models present .It does this by employing a multi - language model 8 ( Multi - LM ) to combine the predictions 20 sourced from each of the multiple language models to generate final predictions 9 that are provided to a user interface for display and user selection .", "label": "", "metadata": {}, "score": "60.816727"}
{"text": "[0004 ] The invention , in various embodiments , addresses the computational challenge of estimating a language model using a large data set .More particularly , according to one aspect , the invention provides a scalable solution by breaking the language model estimation process into sub - processes and parallelizing computation of various portions of the process .", "label": "", "metadata": {}, "score": "60.82015"}
{"text": "Candidate filtering is only necessary when the length of the current input exceeds the maximum depth of the approximate trie , which , to be of any use , must be at least 1 , and values of around 3 - 5 are usually appropriate .", "label": "", "metadata": {}, "score": "60.853035"}
{"text": "Candidate filtering is only necessary when the length of the current input exceeds the maximum depth of the approximate trie , which , to be of any use , must be at least 1 , and values of around 3 - 5 are usually appropriate .", "label": "", "metadata": {}, "score": "60.853035"}
{"text": "0017 ]FIG .1 is a block diagram of a prior art method of generating counts used in language models .[ 0018 ] FIG .2 is a block diagram of a method of generating counts according to an illustrative embodiment the invention .", "label": "", "metadata": {}, "score": "61.043114"}
{"text": "[0048 ] Those skilled in the art will know or be able to ascertain using no more than routine experimentation , many equivalents to the embodiments and practices described herein .Accordingly , it will be understood that the invention is not to be limited to the embodiments disclosed herein , but is to be understood from the following claims , which are to be interpreted as broadly as allowed under the law .", "label": "", "metadata": {}, "score": "61.109184"}
{"text": "The method further comprises the steps of generating concurrently , using a text prediction engine comprising a plurality of language models , text predictions from the multiple language models ; and providing text predictions for user selection .The loop is formed by the insertion of an end - of - sequence punctuation mark , or a ' return ' keypress for example .", "label": "", "metadata": {}, "score": "61.125786"}
{"text": "The method further comprises the steps of generating concurrently , using a text prediction engine comprising a plurality of language models , text predictions from the multiple language models ; and providing text predictions for user selection .The loop is formed by the insertion of an end - of - sequence punctuation mark , or a ' return ' keypress for example .", "label": "", "metadata": {}, "score": "61.125786"}
{"text": "The converted n - grams and the top level HMM structure ( such as 602 shown in FIG .17A ) , together with the rules in the library grammar , form a probabilistic context - free grammar ( PCFG ) language model .", "label": "", "metadata": {}, "score": "61.14032"}
{"text": "FIG .6 is a block diagram of a prior art method of generating a language model .[ 0023 ] FIG .7 is a block diagram of a method of generating a language model according to an illustrative embodiment of the invention .", "label": "", "metadata": {}, "score": "61.171043"}
{"text": "17A illustrates a top - level structure 602 that has two branches , one leading to ShowFLight subnetwork 604 and the other leading to GroundTransport network 606 .The transition weights on each of the branches are the probabilities for the two tasks .", "label": "", "metadata": {}, "score": "61.256977"}
{"text": "To find an n - gram , one would start from the ( em ... . \" ...Trigram language models are compressed using a Golomb coding method inspired by the original Unix spell program .Compression methods trade off space , time and accuracy ( loss ) .", "label": "", "metadata": {}, "score": "61.26018"}
{"text": "[ 0292 ] The input evidence model M input is comprised of a candidate model and a sequence model .The candidate model implements a conditional distribution over input observations given a particular candidate interpretation .The sequence model implements a conditional distribution over candidates given an intended target sequence .", "label": "", "metadata": {}, "score": "61.30327"}
{"text": "Upon receiving the input , the decoder first uses a bottom - up chart parser to find the library grammar non - terminals that cover some input spans .The decoder then searches through the trellis starting from the semantic class nodes at the first column ( the example only shows the semantic class NewAppt ) .", "label": "", "metadata": {}, "score": "61.491295"}
{"text": "A distributed representation is opposed to a local representation , in which only one neuron ( or very few ) is active at each time , i.e. , as with grandmother cells .One can view n - gram models as a mostly local representation : only the units associated with the specific subsequences of the input sequence are turned on .", "label": "", "metadata": {}, "score": "61.49819"}
{"text": "In a preferred embodiment , the system further comprises a target mapping stage 903 .The target mapping stage 903 comprises a word fragment map which provides a mapping from target inputs to word fragments ( usually one or two characters ) that can be used to build predictions .", "label": "", "metadata": {}, "score": "61.611794"}
{"text": "0156 ] The probabilities mapped to the terms \" very \" , \" visit \" and \" x \" in the current prediction set would then be replaced by the values returned from the Bloom filter and consequentially reordered .Additional filters would operate in a similar manner .", "label": "", "metadata": {}, "score": "61.68351"}
{"text": "0156 ] The probabilities mapped to the terms \" very \" , \" visit \" and \" x \" in the current prediction set would then be replaced by the values returned from the Bloom filter and consequentially reordered .Additional filters would operate in a similar manner .", "label": "", "metadata": {}, "score": "61.68351"}
{"text": "The set of topic categories is pre - defined , and may be hierarchical , e.g. ' football ' might be a subcategory of ' sport ' .The numerical values can be interpreted as an estimate of the level of representation of that particular topic in the given text segment .", "label": "", "metadata": {}, "score": "61.691376"}
{"text": "The set of topic categories is pre - defined , and may be hierarchical , e.g. ' football ' might be a subcategory of ' sport ' .The numerical values can be interpreted as an estimate of the level of representation of that particular topic in the given text segment .", "label": "", "metadata": {}, "score": "61.691376"}
{"text": "[ 0250 ] where c j is a single candidate , and there are now two submodels of M for a given evidence source : the candidate model M candidate and the sequence model M sequence .The key assumption here is as follows : . [ 0251 ]", "label": "", "metadata": {}, "score": "61.73376"}
{"text": "These predictions are communicated to the text prediction engine 84 for generating word / phrase predictions that are displayed to the user on interface 60 .[0394 ] The target model set of the target modeling module 82 consists of at least one target model .", "label": "", "metadata": {}, "score": "61.830593"}
{"text": "[0157 ] .The frames of data created by frame constructor 507 are provided to feature extractor 508 , which extracts a feature from each frame .Examples of feature extraction modules include modules for performing Linear Predictive Coding ( LPC ) , LPC derived cepstrum , Perceptive Linear Prediction ( PLP ) , Auditory model feature extraction , and Mel - Frequency Cepstrum Coefficients ( MFCC ) feature extraction .", "label": "", "metadata": {}, "score": "61.966637"}
{"text": "If the user selects the term \" the \" , the sequence \" and the \" is entered .However , if the user selects \" and \" , only \" and \" is entered .The same principle can be applied to arbitrary length phrase prediction .", "label": "", "metadata": {}, "score": "62.12897"}
{"text": "If the user selects the term \" the \" , the sequence \" and the \" is entered .However , if the user selects \" and \" , only \" and \" is entered .The same principle can be applied to arbitrary length phrase prediction .", "label": "", "metadata": {}, "score": "62.12897"}
{"text": "The speech processing system of . claim 5 wherein the statistical model portion of the composite language model further comprises : . a backoff model portion which , when accessed , is configured to assign a backoff score to a word in the vocabulary .", "label": "", "metadata": {}, "score": "62.143196"}
{"text": "Experiments are reported with a high performing baseline , trained on the Chinese - English NIST 2006 Evaluation task and running on a standard Linux 64-bit PC architecture .Comparative tests show that our representation halves the memory required by SRI LM Toolkit , at the cost of 44 % slower translation speed .", "label": "", "metadata": {}, "score": "62.236298"}
{"text": "0010 ] Many of the input models discussed above utilize some form of text prediction technology .Known prediction models for enhancing text input have two main functions : ( 1 ) disambiguation of multiple - character keystrokes , and ( 2 ) offering potential completions for partially - entered sequences .", "label": "", "metadata": {}, "score": "62.296772"}
{"text": "di - elect cons .E i.e. all paths branching from a given node rejoin immediately at a subsequent common node .This property severely constrains the structure of the graph and implies that all paths have the same length , mitigating normalisation requirements on path probability computations .", "label": "", "metadata": {}, "score": "62.311745"}
{"text": "Command Line Syntax : . tags ] [ -captions . captions ] [ -in_lambdas . lambdas ] [ -out_lambdas . lambdas ] [ -stop_ratio 0.999 ] [ -probs .fprobs ] [ -max_probs 6000000 ] .The probability stream filenames are prefaced with a + ( or a + - to indicate that the weighting of that model should be fixed ) .", "label": "", "metadata": {}, "score": "62.35427"}
{"text": "Learning Distributed Representations of Concepts .Proceedings of the Eighth Annual Conference of the Cognitive Science Society:1 - 12 .Rumelhart , D. E. and McClelland , J. L ( 1986 ) Parallel Distributed Processing : Explorations in the Microstructure of Cognition .", "label": "", "metadata": {}, "score": "62.394814"}
{"text": "When the number of input variables increases , the number of required examples can grow exponentially .The curse of dimensionality arises when a huge number of different combinations of values of the input variables must be discriminated from each other , and the learning algorithm needs at least one example per relevant combination of values .", "label": "", "metadata": {}, "score": "62.415684"}
{"text": "Also , in order to facilitate the development of speech enabled applications and services , semantic - based robust understanding systems are currently under development .Such systems are widely used in conversational , research systems .However , they are not particularly practical for use by conventional developers in implementing a conversational system .", "label": "", "metadata": {}, "score": "62.430943"}
{"text": "In fact , the lower ... . by E. W. D. Whittaker , B. Raj - the European Conference on Speech Communication and Technology , 2001 . \" ...In this paper we investigate the extent to which Katz backoff language models can be compressed through a combination of parameter quantization ( width - wise compression ) and parameter pruning ( length - wise compression ) methods while preserving performance .", "label": "", "metadata": {}, "score": "62.44587"}
{"text": "0143 ] As the user specific language model 7 is a dynamic language model , over time it will learn a user 's language style , thereby generating predictions that are more likely to reflect a particular user 's language style .", "label": "", "metadata": {}, "score": "62.51189"}
{"text": "0143 ] As the user specific language model 7 is a dynamic language model , over time it will learn a user 's language style , thereby generating predictions that are more likely to reflect a particular user 's language style .", "label": "", "metadata": {}, "score": "62.51189"}
{"text": "A new prediction set is generated , with the previous predictions re - ordered , by using the Bloom filter component 17 .In this case , the Bloom filter might contain 4-gram sequences associated with probability estimates .Hence , in this example , the Bloom filter would be queried using the following sequences : \" to see you very \" ; \" to see you visit \" ; and \" to see you x \" .", "label": "", "metadata": {}, "score": "62.54486"}
{"text": "A new prediction set is generated , with the previous predictions re - ordered , by using the Bloom filter component 17 .In this case , the Bloom filter might contain 4-gram sequences associated with probability estimates .Hence , in this example , the Bloom filter would be queried using the following sequences : \" to see you very \" ; \" to see you visit \" ; and \" to see you x \" .", "label": "", "metadata": {}, "score": "62.54486"}
{"text": "A new prediction set is generated , with the previous predictions re - ordered , by using the Bloom filter component 17 .In this case , the Bloom filter might contain 4-gram sequences associated with probability estimates .Hence , in this example , the Bloom filter would be queried using the following sequences : \" to see you very \" ; \" to see you visit \" ; and \" to see you x \" .", "label": "", "metadata": {}, "score": "62.54486"}
{"text": "Term2 is then the specified child node that will be searched for .To facilitate this search , child nodes are ordered numerically by their identifiers at each parent node .The node that is being searched for may contain a large number of children , but it is only the high probability candidates that are of interest .", "label": "", "metadata": {}, "score": "62.549213"}
{"text": "[ 0003 ] Many such applications , in particular , automatic speech recognition ( ASR ) and machine translation ( MT ) , have evolved over the past decade , offering high performance and usability .Today , despite extensive research on novel approaches , the standard back - off n - gram language model remains the model of choice in most applications due to its efficiency and reliability .", "label": "", "metadata": {}, "score": "62.57424"}
{"text": "This may include organization specific language enabling prediction of preferred or standard wording , for example , during text input .However , it will be appreciated that the additional language models 6 can comprise any ' user community ' specific language model .", "label": "", "metadata": {}, "score": "62.60646"}
{"text": "In the following section static pruning is described in relation to the pruning of a single language model .[ 0112 ] Given two language models L1 and L2 , the pruning of L1 is achieved by comparison to a reference language model , L2 .", "label": "", "metadata": {}, "score": "62.687634"}
{"text": "The method of . claim 10 and further comprising : . mapping the word hypotheses to slots derived from an input schema based on the rules - based models in the composite language model .The method of .claim 11 and further comprising : . mapping the word hypotheses to pre - terminals derived from the input schema based on probabilities assigned by the n - gram models and the backoff model in the composite language model .", "label": "", "metadata": {}, "score": "62.849575"}
{"text": "[0201 ] The current system includes automatic capitalisation at the beginning of sentences .[ 0202 ] It will be appreciated that this description is by way of example only ; alterations and modifications may be made to the described embodiment without departing from the scope of the invention as defined in the claims .", "label": "", "metadata": {}, "score": "62.871906"}
{"text": "It accomplishes this by following the initial paths to the nodes corresponding to the characters ' v ' , ' x ' and ' z ' , concatenating the identifier value sets found at each node and returning the combined set .", "label": "", "metadata": {}, "score": "62.87511"}
{"text": "It accomplishes this by following the initial paths to the nodes corresponding to the characters ' v ' , ' x ' and ' z ' , concatenating the identifier value sets found at each node and returning the combined set .", "label": "", "metadata": {}, "score": "62.87511"}
{"text": "0358 ] The input sequence model T is also used for probabilistic thresholding .[0359 ]Other techniques which are suitable for handling the combinatorial explosion can also be employed by themselves , or in combination with one or all of the above techniques .", "label": "", "metadata": {}, "score": "62.88758"}
{"text": "[0325 ] Candidate probabilities for the twelve sequences above are then generated from the PCSG as follows ( showing just three examples for brevity ) : .[ 0326 ] The specifics of the model used to construct the DAG and assign probabilities to nodes will vary depending on the particular instance of the system .", "label": "", "metadata": {}, "score": "62.94429"}
{"text": "Done .evallm : quit .Alternatively , some of these processes can be piped together : .For further details see \" Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer \" , Slava M. Katz , in \" IEEE Transactions on Acoustics , Speech and Signal Processing \" , volume ASSP-35 , pages 400 - 401 , March 1987 .", "label": "", "metadata": {}, "score": "63.001877"}
{"text": "An n - gram is a particular series of n text elements .An n - gram count is the number of occurrences of that n - gram observed in a corpus of text .Next , n - gram probabilities are estimated from the n - gram counts .", "label": "", "metadata": {}, "score": "63.049965"}
{"text": "0163 ] .[ 0163]FIGS .17A and 17B show a statistical model portion ( an HMM ) that incorporates the semantic constraints of schema 600 into a natural language understanding rules - based grammar ( such as a CFG ) .", "label": "", "metadata": {}, "score": "63.133896"}
{"text": "Second , a large amount of training data is rarely available for the development of many speech applications .An n - gram trained on a small amount of data often yields poor recognition accuracy .[0010 ] .The present invention thus uses a composite statistical model and rules - based grammar language model to perform both the speech recognition task and the natural language understanding task .", "label": "", "metadata": {}, "score": "63.21424"}
{"text": "For instance , the gesture for generating additional word predictions can be changed from a left swipe to a down swipe .In embodiments , it is also possible to associate the relative length and velocity of a swiping motion with different commands .", "label": "", "metadata": {}, "score": "63.216263"}
{"text": "An element consists of a key , for ordering the sequence , and a mapped value .In the present system , the structure is ordered by the values ( rather than by the keys which are the prediction strings ) .", "label": "", "metadata": {}, "score": "63.231964"}
{"text": "An element consists of a key , for ordering the sequence , and a mapped value .In the present system , the structure is ordered by the values ( rather than by the keys which are the prediction strings ) .", "label": "", "metadata": {}, "score": "63.231964"}
{"text": "Extensions of this approach exploit distributional characteristics of n - gram data to reduce storage costs , including variable length coding of values and the use of tiered structures that partition the data for more efficient storage .We apply our approach to storing the full Google Web1 T n - gram set and all 1-to-5 grams of the Gigaword newswire corpus .", "label": "", "metadata": {}, "score": "63.241814"}
{"text": "0127 ]A Bloom filter 17 is a randomised data structure used to store sets of objects in a highly efficient manner using bit - arrays and combinations of hash functions .The present system uses an implementation of a multi - bit - array Bloom filter 17 to reorder prediction candidates , generated at the intersection 16 , on the basis of higher - order n - gram statistics which for memory reasons can not be stored in the n - gram map 14 .", "label": "", "metadata": {}, "score": "63.265823"}
{"text": "0127 ]A Bloom filter 17 is a randomised data structure used to store sets of objects in a highly efficient manner using bit - arrays and combinations of hash functions .The present system uses an implementation of a multi - bit - array Bloom filter 17 to reorder prediction candidates , generated at the intersection 16 , on the basis of higher - order n - gram statistics which for memory reasons can not be stored in the n - gram map 14 .", "label": "", "metadata": {}, "score": "63.265823"}
{"text": "The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network .In a distributed computing environment , program modules may be located in both local and remote computer storage media including memory storage devices .", "label": "", "metadata": {}, "score": "63.283234"}
{"text": "[ 0083 ]The n - gram maps can be further compressed by representing string values as short - integer - valued numerical identifiers and by storing higher - order entries \" on top of \" lower - order entries .[ 0084 ] To generate predictions from an n - gram map 14 , at each map node 21 the language model conducts a binary search to locate specified subsequent child nodes .", "label": "", "metadata": {}, "score": "63.305622"}
{"text": "One example of such a model is shown in FIG .11 .The term \" Att \" is an abbreviation for \" Attendee \" , and \" ST \" is an abbreviation for \" StartTime \" .The emission probabilities b are preterminal - dependent n - grams ( in the figure they are depicted as a unigram , but high order emission distribution will result in a high order HMM ) and the transition probabilities a are the slot transition bigrams .", "label": "", "metadata": {}, "score": "63.394264"}
{"text": "0009 ]According to one implementation , the method includes merging the first set of count files to a single count file .The method may also include a second set of count files , and each count file of the second set of count files may correspond to a respective count file of the first set of count files .", "label": "", "metadata": {}, "score": "63.496185"}
{"text": "The target modeling module 82 generates one or more likely targets from the input event , with each likely target having a probability associated with it and indicating the likelihood that the user intended to select that particular target .For instance , if the input data indicated that the user touched in the vicinity of the letter \" U \" , then the likely targets with the highest probabilities may be those letters in the vicinity of the letter \" U \" .", "label": "", "metadata": {}, "score": "63.515762"}
{"text": "0075 ]Alternatively , probability values might be assigned to characters on the basis of their distance from the point of contact on a touchscreen device .For instance , let 's assume that the user touched on the ' h ' character key , with the following distances from surrounding key centroids : .", "label": "", "metadata": {}, "score": "63.65663"}
{"text": "0075 ]Alternatively , probability values might be assigned to characters on the basis of their distance from the point of contact on a touchscreen device .For instance , let 's assume that the user touched on the ' h ' character key , with the following distances from surrounding key centroids : .", "label": "", "metadata": {}, "score": "63.65663"}
{"text": "The target modeling module estimates , based on the area / region of the screen touched by the user , the most likely targets selected by the user .The text prediction engine can be any engine or language model for predicting words or phrases , or it can be a text prediction engine as described above in reference to FIGS . 1 - 15 .", "label": "", "metadata": {}, "score": "63.673515"}
{"text": "The swipe gestures can also be predefined , or they can be custom created by a user , and each swipe gesture can be associated with a particular command .Thus , a user can draw a wavy gesture and associate that gesture with the command for saving a message as a draft .", "label": "", "metadata": {}, "score": "63.67483"}
{"text": "If a human were to choose the features of a word , he might pick grammatical features like gender or plurality , as well as semantic features like animate \" or invisible .With a neural network language model , one relies on the learning algorithm to discover these features , and the features are continuous - valued ( making the optimization problem involved in learning much simpler ) .", "label": "", "metadata": {}, "score": "63.79768"}
{"text": "[ 0033 ] Previous n - gram counting methods stored counts for all n - grams in a single data file .As this file can grow in size rapidly , such methods have often required pruning the count files ( discarding the least - frequent n - grams ) prior to estimating a language model to decrease count file size to fit into system RAM .", "label": "", "metadata": {}, "score": "63.81404"}
{"text": "[0126 ] The language model can be configured to apply one or more filters to the predictions generated by the intersection mechanism 16 .In one embodiment , the first filter that is applied is a bloom filter 17 , which is followed by a topic filter 18 and optionally additional filters 19 to generate the output predictions 20 for a given language model .", "label": "", "metadata": {}, "score": "63.87973"}
{"text": "[0126 ] The language model can be configured to apply one or more filters to the predictions generated by the intersection mechanism 16 .In one embodiment , the first filter that is applied is a bloom filter 17 , which is followed by a topic filter 18 and optionally additional filters 19 to generate the output predictions 20 for a given language model .", "label": "", "metadata": {}, "score": "63.87973"}
{"text": "The computer - readable storage of claim 46 , comprising instructions for causing the computing device to calculate back - off weights to assign probability estimates to a second set of n - grams that are not present in the first or second sets of data files .", "label": "", "metadata": {}, "score": "63.88256"}
{"text": "Embodiments of the text prediction engine can consist of the text prediction engine as described in reference to FIGS . 1 - 8 , the text prediction engine as described in reference to FIG .9 , or the text prediction engine as described in reference to FIGS .", "label": "", "metadata": {}, "score": "64.00207"}
{"text": "The predictions from each language model 20 are based on BOS markers .Preferably , BOS markers are automatically inserted into the context when a user enters end - of - sentence punctuation ( period , exclamation mark , question mark ) or enters the ' return ' character .", "label": "", "metadata": {}, "score": "64.020584"}
{"text": "The predictions from each language model 20 are based on BOS markers .Preferably , BOS markers are automatically inserted into the context when a user enters end - of - sentence punctuation ( period , exclamation mark , question mark ) or enters the ' return ' character .", "label": "", "metadata": {}, "score": "64.020584"}
{"text": "The tokenized sequence data 12 is then passed to the user specific language model 7 .The updating of a dynamic language model is described in a later section of the description , with reference to the structure of a single language model .", "label": "", "metadata": {}, "score": "64.021225"}
{"text": "The predictor considers a number of the most probable paths through the Input Sequence Intention Structure 916 , optionally also considering all words for which that path is a prefix .This is then fed into an n - gram language model to give an ordering of prediction candidates .", "label": "", "metadata": {}, "score": "64.04218"}
{"text": "[ 0040 ]The present invention will now be described in detail with reference to the accompanying drawings , in which : .[ 0041 ] FIG .1 is a schematic of a high level prediction architecture according to the invention ; .", "label": "", "metadata": {}, "score": "64.0859"}
{"text": "[ 0040 ]The present invention will now be described in detail with reference to the accompanying drawings , in which : .[ 0041 ] FIG .1 is a schematic of a high level prediction architecture according to the invention ; .", "label": "", "metadata": {}, "score": "64.0859"}
{"text": "[0017 ] The above described existing virtual keyboards model each character as a location specified by a set of coordinates .What is needed is a system and method that models a user 's actual input as he or she targets characters on a virtual keyboard .", "label": "", "metadata": {}, "score": "64.15722"}
{"text": "The result of stage one for the above ISIS is illustrated in FIG . 15A.Stage two embellishes the existing PCSG with two additional structures .The first is an empty node sub - path ( which falls within the PCSG framework ) , and the second is a ' wildcard ' structure ( which converts the PCSG into an EPCSG ) .", "label": "", "metadata": {}, "score": "64.16785"}
{"text": "w . q . if .p . q .P .NT .s . if .p . q .[ 0116]FIG .4 illustrates another embodiment of a model authoring component 350 in accordance with a different aspect of the invention .", "label": "", "metadata": {}, "score": "64.182526"}
{"text": "0167 ] The current system includes automatic capitalization at the beginning of sentences .[0168 ]Yet another embodiment is directed to a system and method providing a user interface comprising a plurality of targets and being configured to receive user input .", "label": "", "metadata": {}, "score": "64.22378"}
{"text": "The n - gram is assigned to one or more count files of the first set of count files based on the presence of a selected text element in the n - gram .[ 0006 ] The process of assigning the n - gram to a count file includes , for example , increasing a count corresponding to the n - gram .", "label": "", "metadata": {}, "score": "64.43451"}
{"text": "For example , swiping the screen of the small screen device from left to right can query the text prediction system and cause it to display alternative word predictions on the prediction pane 64 .Swiping the screen of the device from right to left can be used to delete single characters , delete an entire word , or perform an undo operation .", "label": "", "metadata": {}, "score": "64.508675"}
{"text": "Where there are alternative characters associated with each key ( e.g. French accented characters or where each key represents multiple characters ) , those characters may be added as additional word fragments .The probabilities corresponding to each target may additionally be discounted by some factor for each alternative word fragment .", "label": "", "metadata": {}, "score": "64.51553"}
{"text": "The input probability generator 902 then uses the model parameters to calculate the value of the Gaussian probability density function , G(x;\u03bc , \u03a3 ) , evaluated at the input location x. The calculated probability value can be represented as a per - event probability vector , which can be concatenated to form part of the Target Sequence Intention Structure 914 .", "label": "", "metadata": {}, "score": "64.537964"}
{"text": "12 .[ 0147 ] .In one illustrative embodiment , the threshold value illustrated in the last line of FIG .12 is set to 0.01 .Of course , other threshold values can be used as well . [ 0148 ] .", "label": "", "metadata": {}, "score": "64.56198"}
{"text": "[0008 ] In one embodiment , the series of text elements may comprise a single text element , and thus the n - gram is a unigram .In some implementations , unigrams are assigned to more than one count file of a set of count files .", "label": "", "metadata": {}, "score": "64.57038"}
{"text": "0048 ] In accordance with an embodiment , there is also provided a method for processing user text input and generating text predictions for user selection .The method includes the steps of receiving text input into a user interface , generating concurrently , using a text prediction engine comprising a plurality of language models , text predictions from the multiple language models , and providing text predictions to the user interface for user selection .", "label": "", "metadata": {}, "score": "64.58661"}
{"text": "[0203 ] The target mapping stage 903 also maps the word fragments of a selected prediction 920 back to their corresponding target events .In a preferred embodiment , each of the word fragments is tagged with the target from which it came to allow the pairing by a predictor 904 of a target to its touch location when a selection event has occurred .", "label": "", "metadata": {}, "score": "64.603"}
{"text": "[ 0057 ] The text prediction engine 100 operates to generate concurrently text predictions 20 from the multiple language models present .It does this by employing a multi - language model 8 ( Multi - LM ) to combine the predictions 20 sourced from each of the multiple language models to generate final predictions 9 that are provided to a user interface for display and user selection .", "label": "", "metadata": {}, "score": "64.68477"}
{"text": "[ 0057 ] The text prediction engine 100 operates to generate concurrently text predictions 20 from the multiple language models present .It does this by employing a multi - language model 8 ( Multi - LM ) to combine the predictions 20 sourced from each of the multiple language models to generate final predictions 9 that are provided to a user interface for display and user selection .", "label": "", "metadata": {}, "score": "64.68477"}
{"text": "[ 0171 ] .[ 0172 ] .[0173 ] .The first probability is the n - gram probability of the word \" a \" following the beginning of sentence symbol .This corresponds to the transition from the initial state I to the state a. The second probability is the probability of the word \" b \" following the word \" a \" , which is labeled on the transition from state a to state b. The third probability is the probability of the end of sentence symbol following the word \" b \" , that is labeled on the transition from state b to the final state F. The sequential transitions form a path in the finite state machine that accepts the utterance \" a b \" and computes the probability of the utterance by multiplying the probabilities of the transitions in the path .", "label": "", "metadata": {}, "score": "64.75089"}
{"text": "Version 1 of the toolkit allowed only Good - Turing discounting to be used in the construction of the models .Version 2 allows any of the following discounting strategies : .Use of n - grams with arbitrary n .The tools in the toolkit are no longer limited to the construction and testing of bigram and trigram language models .", "label": "", "metadata": {}, "score": "64.763695"}
{"text": "The second set of count files , in one embodiment , are populated with n - grams derived from a second text .According to various implementations , the method includes generating a language model from the first set of count files .", "label": "", "metadata": {}, "score": "64.90048"}
{"text": "In order to produce the same behaviour from version 2 as from version 1 , the context cues file should contain the following lines : .Compact data storage .The data structures used to store the n - grams are more compact than those of version 1 , with the result that language models construction is a less memory intensive task .", "label": "", "metadata": {}, "score": "64.918335"}
{"text": "[ 0198 ] One choice of distribution would first classify the previous touch location , x , with a separate Gaussian for each target and class of previous input location .The form of this distribution should be chosen to model typical data collected from virtual keyboard users .", "label": "", "metadata": {}, "score": "64.92935"}
{"text": "[ 0011 ] In one embodiment , providing language model includes calculating a plurality of probabilities related to the likelihood of selected n - grams and storing the plurality of probabilities in data files corresponding to the language model .According to one embodiment , the data files are language model subsets .", "label": "", "metadata": {}, "score": "65.02693"}
{"text": "The method as recited in claim 30 , further comprising the step of enabling the user to perform a swipe gesture among the one or more swipe gestures to terminate text entry .Description : .CROSS -REFERENCES TO RELATED APPLICATIONS .", "label": "", "metadata": {}, "score": "65.06017"}
{"text": "[ 0344 ] where freq(s ) is the frequency of the target sequence in the training data and the denominator is the sum of all the frequencies of all target sequences in the training data .The denominator can equivalently be approximately determined as the total number of terms in the training data ( counting duplicates ) .", "label": "", "metadata": {}, "score": "65.07192"}
{"text": "claim 6 wherein each statistical n - gram model includes a reference to the backoff model portion for all unseen words .The speech processing system of .claim 7 wherein the backoff model portion comprises : . a uniform distribution n - gram that assigns a uniform score to every word in the vocabulary .", "label": "", "metadata": {}, "score": "65.09202"}
{"text": "In Finnish speech recognition tests , the models trained by the growing method outperform the entropy pruned models of similar size .We need to encode which words actually make up the ngram .The tree contains all the n - grams of the model , regardless of the order of the n - gram .", "label": "", "metadata": {}, "score": "65.09389"}
{"text": "0174 ] The model set 906 comprises a plurality of trained models representing the plurality of targets ( which may be characters ) of the system .A target is modeled as a distribution which models the user 's actual input if he was ' targeting ' that target .", "label": "", "metadata": {}, "score": "65.13737"}
{"text": "The predictor 904 might , for example , predict : [ \" I \" , \" I 'm \" , \" in\"].[0221 ] The user then makes a second touch event , quite far from the second character ' t ' , actually closer to ' r ' .", "label": "", "metadata": {}, "score": "65.16237"}
{"text": "The second example in FIG .2F supports a number of different segmentation alternatives as well .For example , in , accordance with one segmentation alternative , the words \" Flight from \" are both mapped to the perterminal ShowFlightCmd and the preterminal FlightPreDepatureCity is mapped to \u03b5 .", "label": "", "metadata": {}, "score": "65.35087"}
{"text": "[ 0061 ] The language models discussed so far are static language models .That is , they are generated from a representative body of text and thereafter are not changed .Of course , new language models can be created and used , but the existing ones in the system of the invention remain unchanged until replaced or removed . [", "label": "", "metadata": {}, "score": "65.371735"}
{"text": "[0201 ] The current system includes automatic capitalisation at the beginning of sentences .[ 0202 ] It will be appreciated that this description is by way of example only ; alterations and modifications may be made to the described embodiment without departing from the scope of the invention as defined in the claims . \" ...", "label": "", "metadata": {}, "score": "65.37387"}
{"text": "[0181 ] .The single loop 620 refers to a shared uniform distribution model such as that shown in FIG .18B.[ 0182 ] .In the uniform distribution model shown in FIG .18B , the letter n refers the number of words in the vocabulary where w 1 refers the first word \" a \" ; w 2 refers to the second word \" b \" , etc .", "label": "", "metadata": {}, "score": "65.4218"}
{"text": "[0054 ]In some embodiments , the system can further comprise additional language models 6 .For instance , it might be efficacious to construct a company - specific language model for use within a particular organisation .This may include organisation specific language enabling prediction of preferred or standard wording , for example , during text input .", "label": "", "metadata": {}, "score": "65.501175"}
{"text": "[0054 ]In some embodiments , the system can further comprise additional language models 6 .For instance , it might be efficacious to construct a company - specific language model for use within a particular organisation .This may include organisation specific language enabling prediction of preferred or standard wording , for example , during text input .", "label": "", "metadata": {}, "score": "65.501175"}
{"text": "There is no generalization at all .[0124 ] .One embodiment of the invention is drawn to , instead of modeling preterminals ( such as commands , preambles and postambles ) with rules in the template grammar , a statistical model ( such as an n - gram ) is used to model the preterminals .", "label": "", "metadata": {}, "score": "65.50182"}
{"text": "[ 0030 ] In accordance with the present invention , there is also provided a method for processing user text input and generating text predictions for user selection .The method includes the steps of receiving text input into a user interface , generating concurrently , using a text prediction engine comprising a plurality of language models , text predictions from the multiple language models , and providing text predictions to the user interface for user selection .", "label": "", "metadata": {}, "score": "65.55811"}
{"text": "[ 0030 ] In accordance with the present invention , there is also provided a method for processing user text input and generating text predictions for user selection .The method includes the steps of receiving text input into a user interface , generating concurrently , using a text prediction engine comprising a plurality of language models , text predictions from the multiple language models , and providing text predictions to the user interface for user selection .", "label": "", "metadata": {}, "score": "65.55811"}
{"text": "[0058 ]In general , but not exclusive terms , the system of embodiments can be implemented as shown in FIG .1 .FIG .1 is a block diagram of a high level text prediction architecture according to the invention .", "label": "", "metadata": {}, "score": "65.70035"}
{"text": "The method of claim 66 , wherein combining the predictions comprises inserting the predictions into an ordered associative structure or a multimap structure , and returning the p most probable terms for provision to the user interface .Description : .[ 0001 ] The present invention relates generally to a system and method for inputting text into electronic devices .", "label": "", "metadata": {}, "score": "65.74703"}
{"text": "Also , in some parsing components , a large number of rules render the parser less effective .[ 0101 ] .Thus , in accordance with one illustrative embodiment , component 302 provides the rules and associated probabilities to pruning component 304 where the rules can be pruned .", "label": "", "metadata": {}, "score": "65.78553"}
{"text": "The first factor and the third factor in equation 15 are nonzero because they correspond to bigrams that actually exist in the training data ( i.e. , [ show ] and [ flight ] ) .The second factor does not correspond to a bigram that showed up in the training data but , because of smoothing techniques like backoff ( described below ) it will also have a nonzero probability and can be represented as follows : . [", "label": "", "metadata": {}, "score": "65.79701"}
{"text": "[0007 ] In one embodiment , the series of consecutive text elements used to form the n - gram includes a current text element and a history of text elements .The selected text element , i.e. the text element used to assign the n - gram to a count file , is the most recent text element in the history , where the history includes the text elements which occurred prior to the current text element .", "label": "", "metadata": {}, "score": "65.85365"}
{"text": "18B is shared for all of the n - grams in composite model 351 .Each of the n - grams simply has a loop , similar to loop 620 shown in FIG .18A , which refers to the uniform distribution model shown in FIG .", "label": "", "metadata": {}, "score": "65.943146"}
{"text": "Using the tokenised context 12 , the n - gram map 14 is queried by the language model for a given n - gram order , i.e. a number of context terms .Each language model contains n - grams up to a maximum value of n. For example , a particular language model may contain 1 , 2 and 3-grams , in which the maximum n - gram order would be 3 .", "label": "", "metadata": {}, "score": "66.0872"}
{"text": "Using the tokenised context 12 , the n - gram map 14 is queried by the language model for a given n - gram order , i.e. a number of context terms .Each language model contains n - grams up to a maximum value of n. For example , a particular language model may contain 1 , 2 and 3-grams , in which the maximum n - gram order would be 3 .", "label": "", "metadata": {}, "score": "66.0872"}
{"text": "ShowFlightCmd . )Pr .s . flight .ShowFlightCmd . )Also , in accordance with one embodiment of the present invention , training the statistical model for the preterminals includes applying a smoothing algorithm .For example , the training data for training the statistical model for preterminals may be relatively sparse , since it only includes the text strings enumerated for segmentation associated with the given preterminal .", "label": "", "metadata": {}, "score": "66.13385"}
{"text": "The improvements in the Finnish speech recognition over the other Kneser - Ney smoothed models are statistically significant , as well .Index Terms - Modeling , natural languages , smoothing methods , speech recognition .I. . ... probability of the training data given by the current model .", "label": "", "metadata": {}, "score": "66.15577"}
{"text": "0109 ] .Q .P .P .N .w .c .N .w . )P .N .w . ) log .P .N .w . )P .N .w . )", "label": "", "metadata": {}, "score": "66.21296"}
{"text": "[ 0018 ] FIG .1 is a schematic of a high level prediction architecture in accordance with an embodiment ; . [ 0019 ]FIG .2 is a schematic of a language model of the prediction architecture according to an embodiment ; .", "label": "", "metadata": {}, "score": "66.280754"}
{"text": "[ 0043 ] .The invention is operational with numerous other general purpose or special purpose computing system environments or configurations .[0044 ] .The invention may be described in the general context of computer - executable instructions , such as program modules , being executed by a computer .", "label": "", "metadata": {}, "score": "66.32097"}
{"text": "The word fragment map maintains a mapping from a target ( a location on a keyboard , an abstraction of a key ) to one or more word fragments ( portions of a word that the user wants to enter ) .", "label": "", "metadata": {}, "score": "66.41433"}
{"text": "T9 requires the user to enter a number of characters equal to the length of the target input sequence , and thus only offers function 1 ) above , whereas the other systems offer both 1 ) and 2 ) .[ 0011 ] In all of these technologies , the basic dictionary can be augmented with new terms entered by the user .", "label": "", "metadata": {}, "score": "66.4656"}
{"text": "T9 requires the user to enter a number of characters equal to the length of the target input sequence , and thus only offers function 1 ) above , whereas the other systems offer both 1 ) and 2 ) .[ 0011 ] In all of these technologies , the basic dictionary can be augmented with new terms entered by the user .", "label": "", "metadata": {}, "score": "66.4656"}
{"text": "T9 uses a static dictionary , meaning that words sharing the same key sequence are always suggested to the user in the same order .In contrast , Motorola 's iTap utilises a dynamic dictionary , meaning that the first word predicted for a given key sequence may not remain the same each time the key sequence is entered .", "label": "", "metadata": {}, "score": "66.592"}
{"text": "T9 uses a static dictionary , meaning that words sharing the same key sequence are always suggested to the user in the same order .In contrast , Motorola 's iTap utilises a dynamic dictionary , meaning that the first word predicted for a given key sequence may not remain the same each time the key sequence is entered .", "label": "", "metadata": {}, "score": "66.592"}
{"text": "[0356 ]With the addition of the generalisation structures ( especially the wildcard branch ) , the number of paths through the PCSG grows rapidly .For instance , given a character set size of 50 there are 1020 unique paths through the simple PCSG above .", "label": "", "metadata": {}, "score": "66.64795"}
{"text": "Libpointmatcher is a modular \" Iterative Closest Point \" library , useful for robotics and computer vision .This library is designed with modularity and performance in mind .It provides building blocks to construct various ICP chains often seen in research .", "label": "", "metadata": {}, "score": "66.652756"}
{"text": "0004 ] Handwriting recognition has been widely used in the PDA market where input is mostly stylus - based .While it has some advantages for users raised on paper - based handwriting , the use of this technology has declined in recent years because it is relatively slow in comparison with most keyboard - based methods .", "label": "", "metadata": {}, "score": "66.78225"}
{"text": "0004 ] Handwriting recognition has been widely used in the PDA market where input is mostly stylus - based .While it has some advantages for users raised on paper - based handwriting , the use of this technology has declined in recent years because it is relatively slow in comparison with most keyboard - based methods .", "label": "", "metadata": {}, "score": "66.78225"}
{"text": "By default , the i d n - gram file is written out as binary file , unless the -write_ascii switch is used .The size of the buffer which is used to store the n - grams can be specified using the -buffer parameter .", "label": "", "metadata": {}, "score": "66.78491"}
{"text": "[ 0014]FIG .2B illustrates an example schema .[ 0015 ] .[ 0015]FIG .2C illustrates an example set of rules generated for the example schema .[ 0016 ] .[ 0016]FIG .2D illustrates an example of an annotated sentence .", "label": "", "metadata": {}, "score": "66.78853"}
{"text": "11B , and the full evidence likelihood is : .However , as stated above , the system is not limited to taking the context and input sources as evidence .If other , or additional , evidence sources are used the system will be correspondingly configured to generate predictions on the basis of such evidence sources .", "label": "", "metadata": {}, "score": "66.80246"}
{"text": "The un - mapping stage is a trivial pairing of touch - locations and targets , because the system keeps track of the list of targets and input events that make up each prediction via the tagging of the word fragments with their targets and input events .", "label": "", "metadata": {}, "score": "66.87596"}
{"text": "The slot n - gram constrains the interpretation of slots lacking preambles and postambles .The resulting model is a composite of a statistical model ( or HMM ) and a CFG .The HMM models the template rules and the n - gram preterminals , and the CFG models library grammar .", "label": "", "metadata": {}, "score": "66.909"}
{"text": "Accordingly , the invention should not be limited to just the particular description and various drawing figures contained in this specification that merely illustrate various embodiments and application of the principles of such embodiments .a composite language model including a rules - based model portion and a statistical model portion ; and .", "label": "", "metadata": {}, "score": "67.00462"}
{"text": "Confidence measure module 520 then provides the sequence of hypothesis words to an output module 522 along with identifiers indicating which words may have been improperly identified .Those skilled in the art will recognize that confidence measure module 520 is not necessary for the practice of the present invention .", "label": "", "metadata": {}, "score": "67.04684"}
{"text": "[ 0028 ] FIGS .11A and 11B illustrate examples of the properties of a candidate model encoded in the form of graphical models describing the relationship between the variables and models ; .[ 0029 ] FIG .12 illustrates an embodiment of an instantiation of the system from FIG .", "label": "", "metadata": {}, "score": "67.07416"}
{"text": "The system of claim 48 , wherein the text prediction engine comprises a mechanism to combine the predictions generated by each language model .The system of claim 53 , wherein the mechanism is configured to insert the predictions into an ordered associative structure or a multimap structure and return the p most probable terms as the predictions for provision to the user interface .", "label": "", "metadata": {}, "score": "67.079575"}
{"text": "The user can specify the cutoffs for the 2-grams , 3-grams , ... , n - grams by using the -cutoffs parameter .If the parameter is omitted , then all the cutoffs are set to zero .", "label": "", "metadata": {}, "score": "67.16839"}
{"text": "The text prediction engine 84 uses the likely targets and their probabilities to generate word / phrase predictions , with the generated word / phrase predictions also having probabilities associated with them .For instance , if the user input had previously consisted of the letters \" T \" and \" h \" , then the word prediction \" The \" may have a higher probability than the word prediction \" There \" .", "label": "", "metadata": {}, "score": "67.18305"}
{"text": "0109 ] The language models use BOS markers which are used as context 12 in the absence of any preceding user input .The predictions from each language model 20 are based on BOS markers .Preferably , BOS markers are automatically inserted into the context when a user enters end - of - sentence punctuation ( period , exclamation mark , question mark ) or enters the ' return ' character .", "label": "", "metadata": {}, "score": "67.295845"}
{"text": "STATEMENTS AS TO THE RIGHTS TO INVENTIONS MADE UNDER FEDERALLY SPONSORED RESEARCH OR DEVELOPMENT .[ 0003 ] Not applicable .REFERENCE TO A \" SEQUENCE LISTING , \" A TABLE , OR A COMPUTER PROGRAM LISTING APPENDIX SUBMITTED ON A COMPACT DISK .", "label": "", "metadata": {}, "score": "67.323326"}
{"text": "( canceled ) .The method of claim 16 , wherein each of the first set of data files is associated with a set of text elements , and each of the corresponding second set of data files is associated with the same set of text elements .", "label": "", "metadata": {}, "score": "67.43068"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .[ 0016 ] The foregoing and other objects and advantages of the invention will be appreciated more fully from the following further description thereof , with reference to the accompanying drawings .These depicted embodiments are to be understood as illustrative of the invention and not as limiting in any way .", "label": "", "metadata": {}, "score": "67.43399"}
{"text": "The method of claim 62 , wherein the text prediction engine comprises a mechanism to combine the predictions generated from each language model among the plurality of language models , the method further comprising combining the predictions .The method of claim 66 , wherein combining the predictions comprises inserting the predictions into an ordered associative structure or a multimap structure , and returning the p most probable terms for provision to the user interface .", "label": "", "metadata": {}, "score": "67.44345"}
{"text": "[ 0185 ] \" soon \" \" ! \"[ 0186 ] \" very \" \" soon \" \" ! \"[ 0187 ] \" you \" \" very \" \" soon \" \" ! \"[ 0188 ] For each n - gram path , the dynamic language model 7 increments the frequency value of the corresponding node by one , and also increments the total value for the parent by one .", "label": "", "metadata": {}, "score": "67.48982"}
{"text": "[ 0185 ] \" soon \" \" ! \"[ 0186 ] \" very \" \" soon \" \" ! \"[ 0187 ] \" you \" \" very \" \" soon \" \" ! \"[ 0188 ] For each n - gram path , the dynamic language model 7 increments the frequency value of the corresponding node by one , and also increments the total value for the parent by one .", "label": "", "metadata": {}, "score": "67.48982"}
{"text": "[ 0022]FIG .4 illustrates a model - authoring component in accordance with another embodiment of the present invention .[ 0023 ] .[ 0023]FIG .5 shows an example of enumerated segmentations .[ 0024 ] .[ 0024]FIG .", "label": "", "metadata": {}, "score": "67.538376"}
{"text": "FIG .1 is a block diagram of a high level text prediction architecture according to the invention .The system of the present invention comprises a text prediction engine 100 which generates concurrently text predictions 20 from multiple language models .", "label": "", "metadata": {}, "score": "67.559105"}
{"text": "FIG .1 is a block diagram of a high level text prediction architecture according to the invention .The system of the present invention comprises a text prediction engine 100 which generates concurrently text predictions 20 from multiple language models .", "label": "", "metadata": {}, "score": "67.559105"}
{"text": "[ 0255 ] In an embodiment , the evidence conditional sequence probability can be re - written as : .[ 0256 ] where c j is a single candidate , and as before , there are two submodels of M for a given evidence source : the candidate model M candidate and the sequence model M sequence .", "label": "", "metadata": {}, "score": "67.56073"}
{"text": "0156 ] .A - to - D converter 506 converts the analog signal from microphone 504 into a series of digital values .In several embodiments , A - to - D converter 506 samples the analog signal at 16 kHz and 16 bits per sample , thereby creating 32 kilobytes of speech data per second .", "label": "", "metadata": {}, "score": "67.61906"}
{"text": "Version 2 of the toolkit seeks to maintain the structure of version 1 , to include all ( or very nearly all ) of the functionality of version 1 , and to provide useful improvements in terms of functionality and efficiency .", "label": "", "metadata": {}, "score": "67.625725"}
{"text": "2 G is again assigned a count of one third .[ 0091 ] .[ 0092 ] .[0093 ] .Component 302 assigns counts to each of the enumerated rewrite rules in FIG .2 G in this way , and those counts are illustrated in the second column of the table shown in FIG .", "label": "", "metadata": {}, "score": "67.6447"}
{"text": "3B. Pruning component 304 can prune the rules ( as indicated by block 320 ) in one of a number of different ways .For example , pruning component 304 can simply prune out rules that have a probability below a desired threshold level .", "label": "", "metadata": {}, "score": "67.67988"}
{"text": "[0045 ] Embodiments also provide a touch - screen interface that includes a single or multi - character entry mechanism , a word prediction pane , and a typing pane to display inputted text .Preferably , the interface includes a menu button which toggles the screen between prediction , numbers and punctuation , and further punctuation screens .", "label": "", "metadata": {}, "score": "67.72803"}
{"text": "The conversion table maps the identifier for a given term t in L1 , to the identifier for the term t in L2 .For example , if the term \" the \" is identified by the numerical identifier 1 in L1 and the identifier 2 in L2 , then given the identifier 1 for L1 , the conversion table will yield the identifier 2 for L2 .", "label": "", "metadata": {}, "score": "67.74922"}
{"text": "Experiments on Finnish and English text corpora show that the proposed pruning algorithm provides considerable improvements over previous pruning algorithms on Kneser - Ney - smoothed models and is also better than the baseline entropy pruned Good - Turing smoothed models .", "label": "", "metadata": {}, "score": "67.77782"}
{"text": "[0073 ]As the skilled reader will be aware , the KeyPressVector embodiment is not restricted to individual character keystrokes , and could be used for multi - character keystrokes as well .In the case of multi - character keystrokes , the first element of the KeyPressVector 31 will comprise probability values relating to the multiple characters associated with that keystroke .", "label": "", "metadata": {}, "score": "67.78859"}
{"text": "[0073 ]As the skilled reader will be aware , the KeyPressVector embodiment is not restricted to individual character keystrokes , and could be used for multi - character keystrokes as well .In the case of multi - character keystrokes , the first element of the KeyPressVector 31 will comprise probability values relating to the multiple characters associated with that keystroke .", "label": "", "metadata": {}, "score": "67.78859"}
{"text": "Thus , selection events create concrete associations between the locations of touches on the screen of interface 60 with likely targets .[ 0377 ] The use of a QWERTY - style character layout on a small screen device is nonobvious as it is common practice to use a multi - tap or disambiguative keyboard to enter text in small screen devices .", "label": "", "metadata": {}, "score": "67.81544"}
{"text": "Two types of open vocabulary model are implemented in the toolkit .The first type ( -vocab_type 1 ) treats this symbol the same way as any other word in the vocabulary .The second type ( -vocab_type 2 ) of open vocabulary model is to cover situations where no OOVs occurred in the training data , but we wish to allow for the situation where they could occur in the test data .", "label": "", "metadata": {}, "score": "67.87114"}
{"text": "claim 1 wherein the rules - based model portion comprises : . a context free grammar ( CFG ) .A method of assigning probabilities to word hypotheses during speech processing , comprising : . receiving a word hypothesis ; . accessing a composite language model having a plurality of statistical models and a plurality of rules - based models ; . assigning an n - gram probability , with an n - gram model , to the word hypothesis if the word hypothesis corresponds to a word seen during training of the n - gram model ; and .", "label": "", "metadata": {}, "score": "67.885254"}
{"text": "0075 ] Each language model utilizes an approximate trie 13 to generate word predictions based on the current term input 11 .An approximate trie 13 is an extended version of a standard trie .A standard trie , or prefix tree , as is known in the art , is an ordered tree - like data structure used to store an associative array of character sequences .", "label": "", "metadata": {}, "score": "67.88657"}
{"text": "[0387 ] FIG .18 illustrates a high level architecture of the present embodiment , with the device 80 having a small screen with a text entry interface .The interface communicates with a target modeling module 82 and a text prediction engine 84 .", "label": "", "metadata": {}, "score": "68.08514"}
{"text": "\" [ 0153 ] \" very \" \" soon \" \" ! \" [ 0154 ] \" you \" \" very \" \" soon \" \" ! \" [ 0155 ] For each n - gram path , the dynamic language model 7 increments the frequency value of the corresponding node by one , and also increments the total value for the parent by one .", "label": "", "metadata": {}, "score": "68.11131"}
{"text": "Efficient implementations and effective caching speed comparison of strings .Erudite is an application for training and testing back propogation neural networks using the ANNeML ( Artifical Neural Network Markup Language ) XML format .It supports testing and training neural nets with CSV files and has support for randomized training sets , optional adapting learning rate , sigmoid or hyperbolic tangent transfer functions , optional bias and weight adjustment locking , and more .", "label": "", "metadata": {}, "score": "68.12964"}
{"text": "[ 0205 ] As one skilled in the art will understand , any type of predictor 904 and any number of predictors 904 can be used to generate text predictions in a text prediction system .Preferably , the predictor 904 generates text predictions 918 based on the context of the user inputted text , i.e. the predictor 904 generates text predictions 918 for the nth term , based on up to n-1 terms of context .", "label": "", "metadata": {}, "score": "68.189316"}
{"text": "The sequence \" t \" in expression ( 21 ) is the current context , as input into the system .The context prior weights the probability values of predictions according to the probability that the corresponding model from which the prediction was drawn , comprises a given context sequence .", "label": "", "metadata": {}, "score": "68.209"}
{"text": "[ 0013 ] In one implementation , each of the first set of data files is associated with a set of text elements , and each of the corresponding second set of data files is associated with the same set of text elements .", "label": "", "metadata": {}, "score": "68.40558"}
{"text": "The EM segmentation component 302 uses the template grammar to find the segmentation ambiguities in the training examples .Component 302 then operates to disambiguate any segmentation ambiguities .Based on that disambiguation , rewrite rules can be pruned from the grammar using pruning component 304 to provide the rules - based grammar 210 .", "label": "", "metadata": {}, "score": "68.41611"}
{"text": "The method of claim 16 , wherein the data files of the first set of data files and the data files of the second set of data files store probability measurements .The method of claim 19 , wherein the respective probability measurements indicate the probability of occurrence of respective selected n - grams .", "label": "", "metadata": {}, "score": "68.502266"}
{"text": "[ 0035 ] Factors other than the number of times a text element occurs may contribute to the assignment of a text element to a skeleton count file .[0036 ] Referring back to FIG .2 , when assigning counts to count files 212a-212k and 214a-214k , it is not necessary for all text elements to have an explicit assignment to a skeleton count file 204a-204k .", "label": "", "metadata": {}, "score": "68.5848"}
{"text": "Each corpus may be used to build an independent language model , and these language models may be combined to form a larger , more accurate , language model .[ 0027 ] Building large language models from text data typically involves two steps .", "label": "", "metadata": {}, "score": "68.651"}
{"text": "A language model is a function , or an algorithm for learning such a function , that captures the salient statistical characteristics of the distribution of sequences of words in a natural language , typically allowing one to make probabilistic predictions of the next word given preceding ones .", "label": "", "metadata": {}, "score": "68.666534"}
{"text": "A predictor 904 is a device which , given some input , which in this case comprises an Input Sequence Intention Structure 916 containing a sequence of sets of ' word fragments ' and probabilities , generates a set of predictions 918 .", "label": "", "metadata": {}, "score": "68.72563"}
{"text": "12 as a \" prefix match model \" .[ 0293 ] Each model , including the target sequence prior model R , may be updated with user entered text , depending on the scenario .By using dynamic language models , the system is able to more accurately predict a given user 's intended text sequence .", "label": "", "metadata": {}, "score": "68.753944"}
{"text": "0165 ] Furthermore , the interface can optionally include an ' undo ' button or it can be configured to be responsive to an ' undo ' gesture , which is a gesture on a delete button ( e.g. movement from left to right ) .", "label": "", "metadata": {}, "score": "68.91325"}
{"text": "The letter I represents the initial state , while the letter O represents the back - off state , and the letter F represents the final state .FIG .18B is a finite state representation of a shared uniform distribution which is used to assign a back - off probability to unseen words ( i.e. , those words that were not seen in the training data for the specific n - gram model as shown in FIG .", "label": "", "metadata": {}, "score": "69.1494"}
{"text": "[0190 ]In the embodiment according to FIG .8 , the prediction pane 25 comprises a set of buttons , each button displaying a word from a set of words or phrases that has been predicted by a text prediction engine .", "label": "", "metadata": {}, "score": "69.18576"}
{"text": "[0190 ]In the embodiment according to FIG .8 , the prediction pane 25 comprises a set of buttons , each button displaying a word from a set of words or phrases that has been predicted by a text prediction engine .", "label": "", "metadata": {}, "score": "69.18576"}
{"text": "-include_unks results in a perplexity calculation in which the probability estimates for the unkown word are included .validate Calculate the sum of the probabilities of all the words in the vocabulary given the context specified by the user .Syntax : . fblist ] word1 word2 ... word_(n-1 ) .", "label": "", "metadata": {}, "score": "69.2386"}
{"text": "w . )c .NT .N .w . )P .NT .P .N .w . )P .N .w . )c .NT .N .w . )P .NT . hence .", "label": "", "metadata": {}, "score": "69.43565"}
{"text": "Abstract : .An analogous method and an interface for use with the system and method are also provided .Claims : .( canceled ) .The system of claim 48 , wherein the text predictions are generated concurrently from the plurality of language models in real time .", "label": "", "metadata": {}, "score": "69.47655"}
{"text": "HashTBO made it possible to ship a trigram contextual speller in Microsoft Office 2007 . \" ...We describe the implementation steps required to scale high - order character language models to gigabytes of training data without pruning .Our online models build character - level PAT trie structures on the fly using heavily data - unfolded implementations of an mutable daughter maps with a long intege ... \" .", "label": "", "metadata": {}, "score": "69.493835"}
{"text": "In the central prediction pane , a button containing the word \" and \" will be displayed next to , and to the left of , a button containing the word \" the \" .If the user selects the term \" the \" , the sequence \" and the \" is entered .", "label": "", "metadata": {}, "score": "69.51208"}
{"text": "For instance , touch-1 may have resulted in likely target \" T \" with a probability of 0.5 , likely target \" R \" with a probability of 0.25 , and likely target \" F \" with a probability of 0.25 .", "label": "", "metadata": {}, "score": "69.530136"}
{"text": "Embodiments further enable the design of interfaces without the use of buttons for controlling the entry of text on the small screen device .Claims : .The interface as recited in claim 1 , further comprising the step of changing the character pane in response to a first set of swipe gestures .", "label": "", "metadata": {}, "score": "69.53717"}
{"text": "[ 0215 ] The virtual keyboard 901 may be configured to display the predictions generated by the predictor .The selection 920 of the correct prediction can be achieved by either user selection of a given prediction displayed to the user or auto - accepting by the system of the most probable prediction .", "label": "", "metadata": {}, "score": "69.583984"}
{"text": "From this data , the classifier learns to infer likely topic category labels for new data .In the present case , an individual data sample is a segment of text .The set of topic categories is pre - defined , and may be hierarchical , e.g. ' football ' might be a subcategory of ' sport ' .", "label": "", "metadata": {}, "score": "69.61689"}
{"text": "The input model updater 905 applies the incremental learning update rules to the plurality of models in response to each selection event 920 on the virtual keyboard 901 .The prior hyperparameters and learning parameters are all set to fixed values -- these set the initial target positions and expected accuracy , as well as controlling the speed at which learning takes place .", "label": "", "metadata": {}, "score": "69.63087"}
{"text": "The target model can also be selected manually by using swipe gestures .For example , if a target model is not predicting accurately the desired targets , the user may switch to a different target model that can better predict the likely targets .", "label": "", "metadata": {}, "score": "69.642456"}
{"text": "7 which is a flow chart of a method for processing user text input and generating text predictions .In the particular method described , the first step comprises receipt of text input .Analogous to the foregoing discussion of the system according to the present invention , the text input can comprise current word input 11 ( which can be represented by a KeyPressVector 31 ) and/or context input 12 .", "label": "", "metadata": {}, "score": "69.659195"}
{"text": "7 which is a flow chart of a method for processing user text input and generating text predictions .In the particular method described , the first step comprises receipt of text input .Analogous to the foregoing discussion of the system according to the present invention , the text input can comprise current word input 11 ( which can be represented by a KeyPressVector 31 ) and/or context input 12 .", "label": "", "metadata": {}, "score": "69.659195"}
{"text": "The fourth rule indicates that the object Flight in the schema does not have a command portion , but only has a properties portion ( FlightProperties ) , because Flight is an object in the schema while ShowFlight is a command .", "label": "", "metadata": {}, "score": "69.66199"}
{"text": "The same principle can be applied to arbitrary length phrase prediction .[ 0162 ] Character buttons can be displayed in the two side panes 24 , 26 .The character buttons can have dual or tri character behavior .In an embodiment this is the default behavior on the default screen .", "label": "", "metadata": {}, "score": "69.66769"}
{"text": "[ 0003 ] .Recognizing and understanding spoken human speech is believed to be integral to future computing environments .To date , the tasks of recognizing and understanding spoken speech have been addressed by two different systems .The first is a speech recognition system , and the second is a natural language understanding system .", "label": "", "metadata": {}, "score": "69.69001"}
{"text": "Standard tries facilitate rapid retrieval with efficient storage overheads .[0093 ] FIG .4a illustrates an approximate trie 13 according to the invention and used to store the same set of text strings as the standard trie of FIG .", "label": "", "metadata": {}, "score": "69.72107"}
{"text": "Standard tries facilitate rapid retrieval with efficient storage overheads .[0093 ] FIG .4a illustrates an approximate trie 13 according to the invention and used to store the same set of text strings as the standard trie of FIG .", "label": "", "metadata": {}, "score": "69.72107"}
{"text": "The result is compiled models that are larger than the training models , but execute at 2 million characters per second on a desktop PC .Cross - entropy on held - out data shows these models to be state of the art in terms of performance . ... piled to a less compact but more efficient static representation . \" ...", "label": "", "metadata": {}, "score": "69.75209"}
{"text": "The composite language model of .claim 20 wherein the backoff model portion comprises : . a uniform distribution n - gram that assigns a uniform score to every word in the vocabulary .Description .[ 0001 ] .The present application is a continuation - in - part of and claims priority of U.S. patent application Ser .", "label": "", "metadata": {}, "score": "69.7692"}
{"text": "[ 0040 ] Preferably , the plurality of language models utilize a beginning - of - sequence marker to determine word or phrase predictions in the absence of any preceding text input and/or after end - of - sentence punctuation and/or after new line entry .", "label": "", "metadata": {}, "score": "69.807846"}
{"text": "In the re - write rules shown in FIG .2 G , some of the words that form the preterminal names are abbreviated .From these examples , the other notation in the re - write rule portion of the table shown in FIG .", "label": "", "metadata": {}, "score": "69.82174"}
{"text": "1 is a block diagram of a prior art method 100 of generating n - gram counts .The method 100 begins with a plurality of text files 102a-102 m .For each text file 102a-102 m , the n - grams ( usually unigram , bigrams , and/or trigrams ) occurring in the text files 102a-102 m are counted ( step 104a-104 m ) , resulting in n - gram counts 106a-106 m .", "label": "", "metadata": {}, "score": "69.83842"}
{"text": "Preferably , the language model is configured to conduct a search of the approximate trie or the probabilistic trie to ascertain word predictions based on at least one inputted character .[ 0027 ] The present invention also provides a touch - screen interface that includes a single or multi - character entry mechanism , a word prediction pane , and a typing pane to display inputted text .", "label": "", "metadata": {}, "score": "69.876755"}
{"text": "Preferably , the language model is configured to conduct a search of the approximate trie or the probabilistic trie to ascertain word predictions based on at least one inputted character .[ 0027 ] The present invention also provides a touch - screen interface that includes a single or multi - character entry mechanism , a word prediction pane , and a typing pane to display inputted text .", "label": "", "metadata": {}, "score": "69.876755"}
{"text": "The interpolation results in an interpolated language model 722 comprising k interpolated language model subsets 724a-724k .Optionally , the interpolated language model 722 is pruned ( step 728 ) by pruning each of the interpolated language model subsets 724a-724k .Pruning results in a pruned language model 732 including k pruned language model subsets 734a-734k .", "label": "", "metadata": {}, "score": "69.92469"}
{"text": "P . cmd . )P . from .FPDCity . )P . from . cmd .FPDCity . )C ._ . from . cmd . )P . from . cmd . )P .FPDCity . )", "label": "", "metadata": {}, "score": "69.93651"}
{"text": "All information can be exported by email and the application 's language can be changed at any time .Kelvin is a units converter intended for scientists , teachers , and students who need a reliable tool for fast conversion .Sign up to receive free email alerts when patent applications with chosen keywords are published SIGN UP .", "label": "", "metadata": {}, "score": "69.963196"}
{"text": "[ 0392 ] The target model set comprises a plurality of trained models representing the plurality of targets ( which may be characters ) of the system .A target is modeled as a distribution which models the user 's actual input if he was \" targeting \" that target .", "label": "", "metadata": {}, "score": "69.99968"}
{"text": "The interface as recited in claim 1 , wherein each model among the one or more target models is a probability distribution modeling the location of the one or more previous input events corresponding to the particular targets .The interface as recited in claim 7 , wherein the probability distribution includes a 2D Gaussian distribution , a Laplace distribution , and a Gamma distribution .", "label": "", "metadata": {}, "score": "70.152054"}
{"text": "In a German - English Moses system with target - side syntax , improved estimates yielded a 63 % reduction in CPU time ; for a Hiero - style version , the reduction is 21 % .The compressed language model uses 26 % less RAM while equivalent search quality takes 27 % more CPU .", "label": "", "metadata": {}, "score": "70.1553"}
{"text": "Preferably , the interface includes a send button to send the inputted text to an email application .Preferably , the user interface is configured for word or phrase input , dependent on which term is chosen for input in a given sequence of words .", "label": "", "metadata": {}, "score": "70.18559"}
{"text": "Preferably , the interface includes a send button to send the inputted text to an email application .Preferably , the user interface is configured for word or phrase input , dependent on which term is chosen for input in a given sequence of words .", "label": "", "metadata": {}, "score": "70.18559"}
{"text": "[ 0013 ] Previously , work on enhancing user interaction with keyboards of limited size has provided a number of methods based on reduced - size keyboards , which do not cover the full range of characters for a given language .", "label": "", "metadata": {}, "score": "70.23532"}
{"text": "The mechanism 16 iterates through the base set of identifiers and looks up each identifier in the base set in the other set .If a match is found for the identifier in question , the intersection mechanism 16 places the identifier in a new set which represents the intersection between the two sets .", "label": "", "metadata": {}, "score": "70.24982"}
{"text": "The mechanism 16 iterates through the base set of identifiers and looks up each identifier in the base set in the other set .If a match is found for the identifier in question , the intersection mechanism 16 places the identifier in a new set which represents the intersection between the two sets .", "label": "", "metadata": {}, "score": "70.24982"}
{"text": "a statistical model portion accessed to map portions of the input speech signal to pre - terminals derived from the schema .The composite language model of .claim 15 wherein the statistical model portion comprises : . a plurality of statistical n - gram models , one statistical n - gram model corresponding to each pre - terminal .", "label": "", "metadata": {}, "score": "70.30327"}
{"text": "[ 0011 ] In all of these technologies , the basic dictionary can be augmented with new terms entered by the user .This is limited only by the amount of device memory available .T9 uses a static dictionary , meaning that words sharing the same key sequence are always suggested to the user in the same order .", "label": "", "metadata": {}, "score": "70.313965"}
{"text": "[ 0071 ] .However , there are still a number of words in the input sentence which are not yet mapped to the tree .Those words include \" Flight \" , \" from \" , and \" to \" .", "label": "", "metadata": {}, "score": "70.361404"}
{"text": "However , very large data sets ( e.g. data sets including billions of words ) pose a computational challenge where one must be able to estimate billions of parameters .Systems and methods are needed for reducing the memory requirements of language models without reducing model accuracy .", "label": "", "metadata": {}, "score": "70.41397"}
{"text": "The current term input 11 comprises information the system has about the term the system is trying to predict , i.e. the term the user is attempting to enter .This could be a sequence of multi - character keystrokes , individual character keystrokes or a mixture of both .", "label": "", "metadata": {}, "score": "70.42152"}
{"text": "The current term input 11 comprises information the system has about the term the system is trying to predict , i.e. the term the user is attempting to enter .This could be a sequence of multi - character keystrokes , individual character keystrokes or a mixture of both .", "label": "", "metadata": {}, "score": "70.42152"}
{"text": "The current term input 11 comprises information the system has about the term the system is trying to predict , i.e. the term the user is attempting to enter .This could be a sequence of multi - character keystrokes , individual character keystrokes or a mixture of both .", "label": "", "metadata": {}, "score": "70.42152"}
{"text": "The probabilistic trie 33 is compressed by concatenating adjacent non - branching nodes .FIG .4c shows the result of the compression process on the probabilistic trie of FIG .4b .Paths within the probabilistic trie may now contain multiple characters and some of the arrows begin and end at the same node .", "label": "", "metadata": {}, "score": "70.43414"}
{"text": "The probabilistic trie 33 is compressed by concatenating adjacent non - branching nodes .FIG .4c shows the result of the compression process on the probabilistic trie of FIG .4b .Paths within the probabilistic trie may now contain multiple characters and some of the arrows begin and end at the same node .", "label": "", "metadata": {}, "score": "70.43414"}
{"text": "0106 ] The KeyPressVector 31 can be generalised to account for a missing character entry after each and every character inputted into a system .In this example of a KeyPressVector 31 , every single character input is associated with a potential double character input , but the identity of the second character is left unspecified .", "label": "", "metadata": {}, "score": "70.46844"}
{"text": "0106 ] The KeyPressVector 31 can be generalised to account for a missing character entry after each and every character inputted into a system .In this example of a KeyPressVector 31 , every single character input is associated with a potential double character input , but the identity of the second character is left unspecified .", "label": "", "metadata": {}, "score": "70.46844"}
{"text": "[0193 ] The interface is arranged such that when a word button from the prediction pane 25 , 28 is pressed , that word is selected and appears in the typing pane 23 , 27 .The updated input sequence comprising the selected word and its preceding context is passed to the text prediction engine for new prediction generation .", "label": "", "metadata": {}, "score": "70.5663"}
{"text": "[0193 ] The interface is arranged such that when a word button from the prediction pane 25 , 28 is pressed , that word is selected and appears in the typing pane 23 , 27 .The updated input sequence comprising the selected word and its preceding context is passed to the text prediction engine for new prediction generation .", "label": "", "metadata": {}, "score": "70.5663"}
{"text": "7 is a flow chart of a method for processing user text input and generating text predictions for user selection according to an embodiment ; .[ 0025 ] FIG .8 is a schematic of the user interface according to an embodiment ; .", "label": "", "metadata": {}, "score": "70.641525"}
{"text": "An arbitrary input observation i is encoded in an input sequence intention structure ( ISIS ) , which is an ordered list of sets of sequences mapped to probabilities : .Consider the following ISIS example : . [0350 ] The method by which these probability distributions are generated is not the subject of this disclosure .", "label": "", "metadata": {}, "score": "70.7051"}
{"text": "[ 0096 ] An alternative to the approximate trie is shown in FIG .4b which schematically shows a probabilistic trie 33 .The arrows illustrate how the probabilistic trie is interrogated , given a particular KeyPressVector 31 .In FIG .", "label": "", "metadata": {}, "score": "70.71141"}
{"text": "[ 0096 ] An alternative to the approximate trie is shown in FIG .4b which schematically shows a probabilistic trie 33 .The arrows illustrate how the probabilistic trie is interrogated , given a particular KeyPressVector 31 .In FIG .", "label": "", "metadata": {}, "score": "70.71141"}
{"text": "Standard tries facilitate rapid retrieval with efficient storage overheads .[ 0076 ] FIG .4 illustrates an approximate trie 13 according to the invention and used to store the same set of text strings as the standard trie of FIG .", "label": "", "metadata": {}, "score": "70.74492"}
{"text": "0007 ] Handwriting recognition has been widely used in the PDA market where input is mostly stylus - based .While it has some advantages for users raised on paper - based handwriting , the use of this technology has declined in recent years because it is relatively slow in comparison with most keyboard - based methods .", "label": "", "metadata": {}, "score": "70.75416"}
{"text": "Next , component 302 assigns new expected counts to the enumerated rewrite rules , based upon the possible occurrences of those counts in the examples shown in FIG .2F. This is indicated by block 310 .The first rewrite rule says that the ShowFlightCmd preterminal maps to \u03b5 ( the empty set ) .", "label": "", "metadata": {}, "score": "70.77625"}
{"text": "This model generally works well in practice , but fails in some circumstances .For example , suppose that the bigram \" a b \" and the unigram \" c \" are very common , but the trigram \" a b c \" is never seen .", "label": "", "metadata": {}, "score": "70.81401"}
{"text": "Analogous to the example of the insertion of an apostrophe which was omitted by the user , the present system can be configured to insert a repeated character which was omitted by the user .For example , if the user were trying to type ' accommodation ' , but typed the characters ' a - c - o ' , the system can account for the missing ' c ' by inserting a ' c ' .", "label": "", "metadata": {}, "score": "70.86974"}
{"text": "Analogous to the example of the insertion of an apostrophe which was omitted by the user , the present system can be configured to insert a repeated character which was omitted by the user .For example , if the user were trying to type ' accommodation ' , but typed the characters ' a - c - o ' , the system can account for the missing ' c ' by inserting a ' c ' .", "label": "", "metadata": {}, "score": "70.86974"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .[ 0012 ] .[ 0012]FIG . 1 is a block diagram of one exemplary environment in which the present invention can be used .[ 0013 ] .[ 0013]FIG .2A is a block diagram of one embodiment of a model - authoring component in accordance with one embodiment of the present invention .", "label": "", "metadata": {}, "score": "70.938416"}
{"text": "To further illustrate the operation of EM segmentation component 302 , FIGS .2F and 2 G provide exemplary tables .FIG .2F shows a table that includes a set of examples .The first of which shows that the word \" from \" can possibly map to either the preterminal ShowFlightCmd or the perterminal FlightPreDepartureCity .", "label": "", "metadata": {}, "score": "70.94636"}
{"text": "One example of a rules - based grammar is a context free grammar ( or CFG ) that allows a computer to map an input to a semantic representation of text .[ 0056 ] .Schema 206 is illustratively a semantic description of the domain being modeled .", "label": "", "metadata": {}, "score": "70.94939"}
{"text": "[ 0162]FIG .16 is one illustrative embodiment of another application schema 600 .Schema 600 simply states that the application supports two types of information queries : those for flight information ( the ShowFlight task ) and those for ground transportation information ( the GoundTransport task ) .", "label": "", "metadata": {}, "score": "71.06268"}
{"text": "The interface as recited in claim 1 , further comprising the steps of : generating one or more text predictions with a text prediction engine ; and displaying on the touch - screen interface the one or more text predictions generated with the text prediction engine .", "label": "", "metadata": {}, "score": "71.10216"}
{"text": "[ 0037 ] .[ 0037 ]17 C illustrates use of the HMM structure shown in FIG .17B.[ 0038 ] .[ 0038]FIG .18A illustrates a finite state representation of a bigram language model with two observed words ( a , b ) .", "label": "", "metadata": {}, "score": "71.13667"}
{"text": "0353 ] The algorithm to generate an input candidate EPCSG from an input ISIS has two stages : ( 1 ) Convert the ISIS into a PCSG ; ( 2 ) Insert additional generalising structures , resulting in an EPCSG .Stage one is straightforward .", "label": "", "metadata": {}, "score": "71.21452"}
{"text": "0226 ] The above example relates to a target representing a single character .However , a target may represent multiple characters .If a target represents multiple characters e.g. A / B / C , the model for that target models the user input events corresponding to the target A / B / C. Where the selected prediction 918 comprises A , B or C , the target A / B / C is mapped back to the input event .", "label": "", "metadata": {}, "score": "71.22909"}
{"text": "Conventional speech recognition systems receive a speech signal indicative of a spoken language input .Acoustic features are identified in the speech signal and the speech signal is decoded , using both an acoustic model and a language model , to provide an output indicative of words represented by the input speech signal .", "label": "", "metadata": {}, "score": "71.29484"}
{"text": "1 include a local area network ( LAN ) 171 and a wide area network ( WAN ) 173 , but may also include other networks .Such networking environments are commonplace in offices , enterprise - wide computer networks , intranets and the Internet .", "label": "", "metadata": {}, "score": "71.44287"}
{"text": "For example , a swipe gesture may switch the QWERTY character layout screen for a screen that only lists word predictions , while another swipe gesture may switch the screen to only list numbers .[ 0386 ] Swiping gestures need not be linear .", "label": "", "metadata": {}, "score": "71.57034"}
{"text": "In response to this gesture , or button press , the interface undoes the previous term selection , placing the user back at the position prior to term selection .[ 0200 ] For example , if a user has entered ' us ' by character input , they will be presented with a selection of word predictions based on this input .", "label": "", "metadata": {}, "score": "71.57309"}
{"text": "In response to this gesture , or button press , the interface undoes the previous term selection , placing the user back at the position prior to term selection .[ 0200 ] For example , if a user has entered ' us ' by character input , they will be presented with a selection of word predictions based on this input .", "label": "", "metadata": {}, "score": "71.57309"}
{"text": "The context input 12 comprises the sequence entered so far by the user , directly preceding the current word .This sequence is split into ' tokens ' by the Multi - LM 8 , where a token is an individual term , punctuation entity , number etc .", "label": "", "metadata": {}, "score": "71.63345"}
{"text": "The context input 12 comprises the sequence entered so far by the user , directly preceding the current word .This sequence is split into ' tokens ' by the Multi - LM 8 , where a token is an individual term , punctuation entity , number etc .", "label": "", "metadata": {}, "score": "71.63345"}
{"text": "Suffice it to say that each possible segmentation for the examples shown in FIG .2F is enumerated .[ 0083 ] .From the first example in FIG .2F , one segmentation indicates that the word \" from \" is mapped to ShowFlightCmd and another segmentation indicates that the word \" from \" is mapped to FlightPreDepatureCity .", "label": "", "metadata": {}, "score": "71.901"}
{"text": "E . ij . k . ij . k . i . ) k . m .j .n .P .NT .w .i . w .j . )P .N .w . )P .", "label": "", "metadata": {}, "score": "71.92439"}
{"text": "[ 0080 ] The context input 12 comprises the sequence entered so far by the user , directly preceding the current word .This sequence is split into ' tokens ' by the Multi - LM 8 , where a token is an individual term , punctuation entity , number etc .", "label": "", "metadata": {}, "score": "71.99101"}
{"text": "This enables the user to input words which are not recognised by the system and which would not therefore be predicted by the system .The prediction pane 28 also comprises a most likely word button 48 which displays the word with the greatest probability associated with it from a set of words or phrases predicted by a text prediction engine .", "label": "", "metadata": {}, "score": "72.01567"}
{"text": "This enables the user to input words which are not recognised by the system and which would not therefore be predicted by the system .The prediction pane 28 also comprises a most likely word button 48 which displays the word with the greatest probability associated with it from a set of words or phrases predicted by a text prediction engine .", "label": "", "metadata": {}, "score": "72.01567"}
{"text": "Command Line Syntax : .Notes : evallm can receive and process commands interactively .When it is run , it loads the language model specified at the command line , and waits for instructions from the user .The user may specify one of the following commands : . perplexity Computes the perplexity of a given text .", "label": "", "metadata": {}, "score": "72.05153"}
{"text": "Alternatively , each screen mode of the interface 60 ( such as all capital letters , lowercase letters , numbers and punctuation , etc . ) can have a single model for the corresponding screen mode .In yet another embodiment , a plurality of target models can be maintained for different applications , allowing for a first target model to be trained for a first application , a second target model to be trained for a second application , etc .", "label": "", "metadata": {}, "score": "72.254684"}
{"text": "[0165 ] .In the subnetwork model 604 , the command , preambles and post - ambles are modeled with statistical n - gram models .These are illustrated by the oval portions in subnetwork model 604 .The slot fillers are modeled with probabilistic CFG rules from a grammar library .", "label": "", "metadata": {}, "score": "72.25699"}
{"text": "The -binary , -ascii , -words correspond to the format of the input and output ( Note that the output file will be in the same format as the input file ) .-ascii and -binary denote i d n - gram files , in ASCII and binary formats respectively , and -words denotes a word n - gram file .", "label": "", "metadata": {}, "score": "72.38959"}
{"text": "[0121 ] .However , many applications require low resolution understanding , in which there are not a large number of slots to be filled .One such application is command and control .For example , in a command and control application , some commands which must be recognized include \" ChangePassword \" , \" ChangeBackground \" , and \" ChangeLoginPicture \" .", "label": "", "metadata": {}, "score": "72.391525"}
{"text": "0097 ] The topic filter 18 takes into account the fact that topical context affects term usage .For instance , given the sequence \" was awarded a \" , the likelihood of the following term being either \" penalty \" or \" grant \" is highly dependent on whether the topic of discussion is ' football ' or ' finance ' .", "label": "", "metadata": {}, "score": "72.49312"}
{"text": "The speech processing system of .claim 1 wherein the statistical model portion of the composite language model comprises : . a plurality of statistical n - gram models , one statistical n - gram model corresponding to each pre - terminal .", "label": "", "metadata": {}, "score": "72.56736"}
{"text": "The input events 910 paired with their targets are forwarded to the input model updater 905 , which updates the relevant models of the set of models 906 .[ 0216 ]An example is now discussed , to illustrate the functioning of the system .", "label": "", "metadata": {}, "score": "72.60514"}
{"text": "Therefore , it can be determined that the word \" to \" maps to the FlightPreArrivalCity perterminal in parse tree 216 .[0073 ] .However , it is still unknown where the words \" Flight \" and \" from \" should reside in parse tree 216 .", "label": "", "metadata": {}, "score": "72.750305"}
{"text": "[ 0039]FIG .18B illustrates a finite state representation of a model that models a uniform distribution over the back - off state shown in FIG .18A.[ 0040 ] .[ 0040]FIG .18C shows a finite state representation of a bigram language model with two observed words ( c , d ) .", "label": "", "metadata": {}, "score": "72.89233"}
{"text": "claim 15 wherein the rules - based model portion comprises : . a context free grammar ( CFG ) .The composite language model of .The composite language model of .claim 18 wherein the statistical model portion of the composite language model further comprises : . a backoff model portion which , when accessed , is configured to assign a backoff score to a word in the vocabulary .", "label": "", "metadata": {}, "score": "72.91381"}
{"text": "Each slot is bracketed by a preamble and post - amble , which serve as the linguistic context for the slot .For example , the word \" from \" is a preamble for the DCity slot .It signals that the City following it is likely a departure city .", "label": "", "metadata": {}, "score": "72.97854"}
{"text": "7 shows one exemplary simplified example of a semantic class in a schema that defines the semantics for an appointment scheduling command NewAppt . [0140 ] .[ 0140]FIG .8 illustrates template rules that can be automatically generated for the semantic class NewAppt , where symbols inside braces are optional .", "label": "", "metadata": {}, "score": "72.982635"}
{"text": "The system of claim 50 , wherein the plurality of language models comprises at least one additional language model .The system of claim 48 , wherein the text prediction engine comprises a mechanism to combine the predictions generated by each language model .", "label": "", "metadata": {}, "score": "72.99402"}
{"text": "0127 ] .The ultimate interpretation of the input sentence will depend on a comparison with other interpretation candidates .[ 0128 ] .Thus , during runtime , input sentences are evaluated with the statistical model portion 326 to identify preterminals , and with the rules - based grammar portion 210 to fill slots .", "label": "", "metadata": {}, "score": "73.03828"}
{"text": "wngram2idngram -vocab . idngram .The size of the buffer which is used to store the n - grams can be specified using the -buffer parameter .This value is in megabytes , and the default value can be changed from 100 by changing the value of STD_MEM in the file src / toolkit .", "label": "", "metadata": {}, "score": "73.146545"}
{"text": "0010 ]According to another aspect , the invention provides a method of building a language model .The method includes providing a first language model comprising a first set of data files and a second language model comprising a second set of data files .", "label": "", "metadata": {}, "score": "73.293655"}
{"text": "This is commonly known in the art as \" predictive text \" .[ 0378 ] Embodiments of the present interface 60 , illustrated in FIG .16A , differ at least in its use of a QWERTY - style character layout given the size of the screen space and by not using buttons .", "label": "", "metadata": {}, "score": "73.39392"}
{"text": "3 .Each node 21 contains a pointer 22 to subsequent nodes .Terminal nodes ( i.e. nodes which end a word ) also contain a value associated with the current path .In a trie , as depicted , characters associated with a given node are ordered alphabetically and the nodes are assigned values according to the alphabetical ordering of the paths .", "label": "", "metadata": {}, "score": "73.45729"}
{"text": "3 .Each node 21 contains a pointer 22 to subsequent nodes .Terminal nodes ( i.e. nodes which end a word ) also contain a value associated with the current path .In a trie , as depicted , characters associated with a given node are ordered alphabetically and the nodes are assigned values according to the alphabetical ordering of the paths .", "label": "", "metadata": {}, "score": "73.45729"}
{"text": "3 .Each node 21 contains a pointer 22 to subsequent nodes .Terminal nodes ( i.e. nodes which end a word ) also contain a value associated with the current path .In a trie , as depicted , characters associated with a given node are ordered alphabetically and the nodes are assigned values according to the alphabetical ordering of the paths .", "label": "", "metadata": {}, "score": "73.45729"}
{"text": "To force back - off from all unknown words , use the -backoff_from_unk_inc or -backoff_from_unk_exc flag ( the difference being the difference between inclusive or exclusive forced back - off ) .To force back - off from all context - cues , use the -backoff_from_ccs_inc or -backoff_from_ccs_inc flag .", "label": "", "metadata": {}, "score": "73.61842"}
{"text": "[0156 ] Embodiments also relate to a user interface .In particular it relates to a touch - screen interface , through which the system of the present invention can be operated .FIG .8 provides a schematic representation of a generic user interface according to the preferred embodiment .", "label": "", "metadata": {}, "score": "73.65344"}
{"text": "The Target Sequence Intention Structure 912 can contain all of the targets of the system or it can contain only the targets which have an associated probability higher than a certain threshold probability value .The threshold value is a system parameter , that can be set empirically to a pre - defined level ( e.g. 10 -4 ) .", "label": "", "metadata": {}, "score": "73.660034"}
{"text": "[0189 ]The present invention also relates to a user interface .In particular it relates to a touch - screen interface , through which the system of the present invention can be operated .FIG .8 provides a schematic representation of a generic user interface .", "label": "", "metadata": {}, "score": "73.75616"}
{"text": "[0189 ]The present invention also relates to a user interface .In particular it relates to a touch - screen interface , through which the system of the present invention can be operated .FIG .8 provides a schematic representation of a generic user interface .", "label": "", "metadata": {}, "score": "73.75616"}
{"text": "Additionally , of course , it is not known how the words \" Flight to \" are to be mapped between the possible preterminals ShowFlightCmd and FlightPreArrivalCity .[0081 ] .[ 0081]FIG .2 G is a table further illustrating the operation of the EM algorithm application component 203 .", "label": "", "metadata": {}, "score": "73.8042"}
{"text": "An approximate trie 13 or a probabilistic trie 33 is an extended version of a standard trie .A standard trie , or prefix tree , as is known in the art , is an ordered tree - like data structure used to store an associative array of character sequences .", "label": "", "metadata": {}, "score": "73.84338"}
{"text": "An approximate trie 13 or a probabilistic trie 33 is an extended version of a standard trie .A standard trie , or prefix tree , as is known in the art , is an ordered tree - like data structure used to store an associative array of character sequences .", "label": "", "metadata": {}, "score": "73.84338"}
{"text": "However , with touch - sensitive ' soft keyboards ' , it is possible to use the almost continuous co - ordinates of touches on the soft keyboard .[ 0016 ] Many existing virtual keyboard systems , e.g. United States patent application publication number 2009/0284471 and predecessors , incorporate mechanisms for automatically ' correcting ' user input in instances where the system receives a character that is not the one intended by the user .", "label": "", "metadata": {}, "score": "73.889"}
{"text": "[ 0026 ] Large language models are generally built using several corpora of data .Each corpus usually includes text data of a particular origin .For example , one corpus may include text taken from several years of Wall Street Journal newspapers .", "label": "", "metadata": {}, "score": "73.923386"}
{"text": "[ 0077 ] 2 ) j:0.3 .[ 0078 ] 3 ) g:0.25 .[0079 ] 4 ) y:0.5 .[ 0080 ] 5 ) n:0.45 .[0081 ] 6 ) b:0.7 .[ 0082 ] 7 ) u:0.7 .", "label": "", "metadata": {}, "score": "73.96068"}
{"text": "[ 0077 ] 2 ) j:0.3 .[ 0078 ] 3 ) g:0.25 .[0079 ] 4 ) y:0.5 .[ 0080 ] 5 ) n:0.45 .[0081 ] 6 ) b:0.7 .[ 0082 ] 7 ) u:0.7 .", "label": "", "metadata": {}, "score": "73.96068"}
{"text": "[ 0133 ] .For example , in the rules - based grammar discussed above , the rule : .[ 0134 ] .has an atomic probability associated with it .However , in composite model 351 , the probability for the rule can be computed as follows : .", "label": "", "metadata": {}, "score": "73.970764"}
{"text": "Thus , for example 1 , assume that the segmentation that mapped the word \" from \" to the preterminal FlightPreDepartureCity had a higher probability than the segmentation which assigned the word \" from \" to the preterminal ShowFlightCmd .In that instance , the second segmentation ( the one which mapped \" from \" to ShowFlightCmd ) is eliminated .", "label": "", "metadata": {}, "score": "73.979"}
{"text": "2a - d are schematics of alternative language models of the prediction architecture according to the invention ; .[ 0049 ] FIG .7 is a flow chart of a method for processing user text input and generating text predictions for user selection according to the invention ; .", "label": "", "metadata": {}, "score": "73.9897"}
{"text": "2a - d are schematics of alternative language models of the prediction architecture according to the invention ; .[ 0049 ] FIG .7 is a flow chart of a method for processing user text input and generating text predictions for user selection according to the invention ; .", "label": "", "metadata": {}, "score": "73.9897"}
{"text": "The computer - readable storage of claim 42 , wherein each of the first set of data files is associated with a set of text elements , and each of the corresponding second set of data files is associated with the same set of text elements .", "label": "", "metadata": {}, "score": "74.07492"}
{"text": "Even when the user has not entered any text , the text prediction engine can generate preliminary predictions and communicate those to the interface 60 to be displayed on the prediction pane 64 .For example , the text prediction engine can predict definite or indefinite articles when no text has been entered by the user .", "label": "", "metadata": {}, "score": "74.07924"}
{"text": "The method 700 provides a means for creating large interpolated language models without requiring pruning prior to interpolation .The method 700 begins with the provision or generation of merged counts 702a-702n .Each merged count includes k merged count files 704a-704k and 706a-706k , which are substantially the same as the merged count files 222a-222k of FIG .", "label": "", "metadata": {}, "score": "74.08388"}
{"text": "3 .Each skeleton count file 204a-204k is associated with one or more text elements from the text files 202a-202 m .A text element associated with a particular skeleton count file is referred to herein as an \" assigned element .", "label": "", "metadata": {}, "score": "74.183685"}
{"text": "Component 202 or 350 learns the probabilities associated with the probabilistic context free grammar ( PCFG ) that can comprise grammar 209 , from the annotated training data 208 .This can be done in the same way as the other probabilities discussed above are learned .", "label": "", "metadata": {}, "score": "74.246506"}
{"text": "The subset of a particular count file 212a-212k and 214a-214k is based on the assigned elements of a corresponding skeleton count file 204a-204k .A count file includes the counts of all n - grams in which the most recent element in the history of an n - gram is one of the count file 's assigned elements .", "label": "", "metadata": {}, "score": "74.3092"}
{"text": "234 .Sign up to receive free email alerts when patent applications with chosen keywords are published SIGN UP .Abstract : .An analogous method and an interface for use with the system and method are also provided .Claims : .", "label": "", "metadata": {}, "score": "74.34813"}
{"text": "Abstract --gram models are the most widely used language models in large vocabulary continuous speech recognition .Since the size of the model grows rapidly with respect to the model order and available training data , many methods have been proposed for pruning the least relevant - grams from the model .", "label": "", "metadata": {}, "score": "74.35382"}
{"text": "These characters may represent any selected integer , with the same character representing the same selected integer throughout the figures .DETAILED DESCRIPTION OF THE DRAWINGS .[ 0025 ] To provide an overall understanding of the invention , certain illustrative embodiments will now be described , including systems , methods and devices for building arbitrarily large language models .", "label": "", "metadata": {}, "score": "74.35495"}
{"text": "FIG .9 is a schematic of a system architecture of a system that adapts to learn a user and learns the style in which a user inputs text by modeling the user 's historical interaction with the system ; .[ 0027 ] FIG .", "label": "", "metadata": {}, "score": "74.412865"}
{"text": "p . q . )p .r . q .s .t .p .r . ) t .r . q . ) s .p . q . )P .NT .s .w .", "label": "", "metadata": {}, "score": "74.47597"}
{"text": "The probability of an \" arrival time \" slot following an \" arrival date \" slot will likely be higher than the probability of a \" departure time \" slot following an \" arrival date \" slot .If such slot transitions are modeled , the slot transition model will prefer that \" 11:00 a.m. \" be matched to the \" arrival time \" slot .", "label": "", "metadata": {}, "score": "74.49979"}
{"text": "By way of example , and not limitation , FIG .1 illustrates operating system 134 , application programs 135 , other program modules 136 , and program data 137 .[0048 ] .The computer 110 may also include other removable / non - removable volatile / nonvolatile computer storage media .", "label": "", "metadata": {}, "score": "74.58363"}
{"text": "[ 0210 ] The use of the present embodiment will now be described by way of a non - limiting example .A user interacts with a continuous coordinate system of a virtual keyboard 901 by entering input , for example by touching a location on a touch - sensitive keyboard 901 .", "label": "", "metadata": {}, "score": "74.66313"}
{"text": "For example , it can be seen that the total number of counts for the preterminal ShowFlightCmd is 3 .Component 302 processes the counts for each rewrite rule , and each preterminal , in order to obtain this probability .[ 0095 ] .", "label": "", "metadata": {}, "score": "74.678665"}
{"text": "[ 0001 ] The present invention relates generally to a system and method for inputting text into electronic devices .In particular the invention relates to a system for generating text predictions for display and user selection and a method for doing so .", "label": "", "metadata": {}, "score": "74.838295"}
{"text": "The system of claim 48 , wherein the text predictions are generated concurrently from the plurality of language models in real time .The system of claim 48 , wherein the plurality of language models comprises a model of human language and at least one language model specific to an application .", "label": "", "metadata": {}, "score": "75.183365"}
{"text": "8a , the user enters the most likely word by pressing the most likely word button 48 or by entering a space .The word is not completed ( i.e. a space is not automatically entered after the word ) , but the letters are used as input for further prediction .", "label": "", "metadata": {}, "score": "75.233185"}
{"text": "8a , the user enters the most likely word by pressing the most likely word button 48 or by entering a space .The word is not completed ( i.e. a space is not automatically entered after the word ) , but the letters are used as input for further prediction .", "label": "", "metadata": {}, "score": "75.233185"}
{"text": "The word fragment map may be a fixed map stored in a memory of the system .The map varies with the language used , e.g. the punctuation will differ with the chosen language and accented characters , umlauts etc . may feature in the word fragments dependent on the chosen language of the system .", "label": "", "metadata": {}, "score": "75.29323"}
{"text": "As described further in reference to FIG .3 , the skeleton count files 204a-204k are generated such that the count files 212a-214k are small enough such that the computer system can perform the merging in steps 218a-218k without accessing the non - volatile storage of the computer system .", "label": "", "metadata": {}, "score": "75.40947"}
{"text": "Each target has already been mapped by the word fragment map at the target mapping stage 903 to one or more word fragments , depending on language , which are unique .Furthermore , in the preferred embodiment , the Input Sequence Intention Structure 916 stores the tagging of the word fragments to their targets and input events .", "label": "", "metadata": {}, "score": "75.43471"}
{"text": "[ 0006 ] Text input for mobile phones , however , has focused primarily on methods of entering alphabetic characters using a 9-digit keypad , where each key usually represents either three or four characters .There are various techniques employed to reduce the number of keystrokes required .", "label": "", "metadata": {}, "score": "75.5695"}
{"text": "By doing so , the training data is required to be labeled , i.e. input events require association with target labels .The labeling is not provided in the data , which is a stream of input events 910 and targets , so must be inferred from selection events 920 .", "label": "", "metadata": {}, "score": "75.5887"}
{"text": "Q .P .P .P .NT .P .NT .Since .P .N .w . )NT .P .NT .c .NT .N .w . )Q .P .", "label": "", "metadata": {}, "score": "75.60831"}
{"text": "The interface as recited in claim 20 , wherein the typing pane is positioned on a top portion of the interface , wherein the prediction pane is positioned below the typing pane , and wherein the QWERTY - style character pane is positioned below the prediction pane .", "label": "", "metadata": {}, "score": "75.651886"}
{"text": "The interface 60 could also be arranged with the prediction pane 64 on one side and the typing pane 70 on the other , or both on one side , in the case of languages that write words going up or down .", "label": "", "metadata": {}, "score": "75.74697"}
{"text": "For example , in one alternative , the word \" Flight \" can be mapped to ShowFlightCmd while the word \" from \" is mapped to ShowFlightPreFlight .In that case , the preterminal FlightPreDepatureCity is mapped to an empty set .", "label": "", "metadata": {}, "score": "75.78598"}
{"text": "Command Line Syntax : . wngram .The maximum numbers of charactors and words that can be stored in the buffer are given by the -chars and -words options .The default number of characters and words are chosen so that the memory requirement of the program is approximately that of STD_MEM , and the number of charactors is seven times greater than the number of words .", "label": "", "metadata": {}, "score": "75.940475"}
{"text": "0130 ] The topic filter 18 takes into account the fact that topical context affects term usage .For instance , given the sequence \" was awarded a \" , the likelihood of the following term being either \" penalty \" or \" grant \" is highly dependent on whether the topic of discussion is ' football ' or ' finance ' .", "label": "", "metadata": {}, "score": "75.95039"}
{"text": "0130 ] The topic filter 18 takes into account the fact that topical context affects term usage .For instance , given the sequence \" was awarded a \" , the likelihood of the following term being either \" penalty \" or \" grant \" is highly dependent on whether the topic of discussion is ' football ' or ' finance ' .", "label": "", "metadata": {}, "score": "75.95039"}
{"text": "[0159 ]The interface is arranged such that when a word button from the prediction pane 25 is pressed , that word is selected and appears in the typing pane 23 .The updated input sequence comprising the selected word and its preceding context is passed to the text prediction engine for new prediction generation .", "label": "", "metadata": {}, "score": "76.030495"}
{"text": "During training , this may well result in a rule such as : . [0122 ] .[ 0123 ] .Since \" ChangeLoginPicture \" is a command , there is not a property portion to the rule .Therefore , the grammar learner simply \" remembers \" the full sentence in the rule it acquired .", "label": "", "metadata": {}, "score": "76.05365"}
{"text": "However , these efforts only support simple s .. by Vesa Siivola , Teemu Hirsim\u00e4ki , Sami Virpioja - IEEE Trans .on Audio , Speech , and Language Processing . \" ...Abstract --gram models are the most widely used language models in large vocabulary continuous speech recognition .", "label": "", "metadata": {}, "score": "76.11584"}
{"text": "FIG .8 is a schematic of the user interface according to the invention ; .[ 0051 ] FIG .8a is a schematic of an alternative user interface according to the invention .[0052 ] In general , but not exclusive terms , the system of the invention can be implemented as shown in FIG .", "label": "", "metadata": {}, "score": "76.16331"}
{"text": "FIG .8 is a schematic of the user interface according to the invention ; .[ 0051 ] FIG .8a is a schematic of an alternative user interface according to the invention .[0052 ] In general , but not exclusive terms , the system of the invention can be implemented as shown in FIG .", "label": "", "metadata": {}, "score": "76.16331"}
{"text": "8a provides a schematic of an alternative generic user interface .In the alternative embodiment , as shown in FIG .8a , the basic user interface comprises a typing pane 27 , a prediction pane 28 and a single pane for single / multi character , punctuation or symbol entry 29 .", "label": "", "metadata": {}, "score": "76.17351"}
{"text": "8a provides a schematic of an alternative generic user interface .In the alternative embodiment , as shown in FIG .8a , the basic user interface comprises a typing pane 27 , a prediction pane 28 and a single pane for single / multi character , punctuation or symbol entry 29 .", "label": "", "metadata": {}, "score": "76.17351"}
{"text": "Trigram language models are normally consid ... \" .Trigram language models are compressed using a Golomb coding method inspired by the original Unix spell program .Compression methods trade off space , time and accuracy ( loss ) .The proposed HashTBO method optimizes space at the expense of time and accuracy .", "label": "", "metadata": {}, "score": "76.38432"}
{"text": "Second , they represent a \" backoff \" likelihood that a particular branch is a valid orthographic variant of the observed sequence .For example , if the user has entered \" See you on Thur \" , an alternative orthographic form of \" Thur \" would be \" Thurs \" .", "label": "", "metadata": {}, "score": "76.38855"}
{"text": "N .w . )c .NT .N .w . )P .NT .P .N .w . )P .N .w . )P .NT .Then .P .N .", "label": "", "metadata": {}, "score": "76.45692"}
{"text": "The interface of claim 59 , wherein the interface further comprises a menu button which toggles the screen between prediction , numbers and punctuation , and further punctuation screens .The method of claim 63 , wherein the language model comprises a mechanism to compute the intersection of the predictions determined by the approximate or probabilistic trie and the n - gram map , the method further comprising computing the intersection of the predictions .", "label": "", "metadata": {}, "score": "76.529205"}
{"text": "0009 ] 2 ) Offering potential completions for partially - entered sequences .[0010 ] Examples of such technologies include Tegic Communications ' ' T9 ' , Motorola 's ' iTap ' , Nuance 's ' XT9 ' , Blackberry 's ' SureType ' and Zi Technology 's ' eZiType ' and ' eZiText ' .", "label": "", "metadata": {}, "score": "76.559395"}
{"text": "0009 ] 2 ) Offering potential completions for partially - entered sequences .[0010 ] Examples of such technologies include Tegic Communications ' ' T9 ' , Motorola 's ' iTap ' , Nuance 's ' XT9 ' , Blackberry 's ' SureType ' and Zi Technology 's ' eZiType ' and ' eZiText ' .", "label": "", "metadata": {}, "score": "76.559395"}
{"text": "The speech processing system of .claim 1 wherein the decoder is configured to map portions of the natural language speech input to the slots based on the rules - based model portion of the composite language model .The speech processing system of .", "label": "", "metadata": {}, "score": "76.60765"}
{"text": "However , to use the model 351 in a speech recognition system ( such as a language model shown in FIG .15 ) , the model is first converted into a format that decoder 512 can accept as its language model .", "label": "", "metadata": {}, "score": "76.66598"}
{"text": "9 illustrates one embodiment of an annotated sentence \" New meeting with Peter at 5:00 \" .FIG .10 illustrates two rules which can be added once segmentation disambiguation has been performed as discussed above .[ 0141 ] .However , as discussed , the purely rules - based grammar can lack robustness and exhibit brittleness .", "label": "", "metadata": {}, "score": "76.705696"}
{"text": "[ 0025 ] .[ 0025]FIG .7 is an example of a simplified schema .[ 0026 ] .[ 0026]FIG .8 is an example of a set of rules generated from the schema in FIG .7 . [ 0027 ] .", "label": "", "metadata": {}, "score": "76.73002"}
{"text": "Embodiments further enable the design of interfaces without the use of buttons for controlling the entry of text on the small screen device .[ 0370 ] FIG .16A illustrates an embodiment of an interface 60 for entering text on a small screen device .", "label": "", "metadata": {}, "score": "76.78944"}
{"text": "The computer - readable storage of claim 42 , wherein the data files of the first set of data files and the data files of the second set of data files store probability measurements .The computer - readable storage of claim 45 , wherein the respective probability measurements indicate the probability of occurrence of respective selected n - grams .", "label": "", "metadata": {}, "score": "76.81003"}
{"text": "[ 0041 ] .The present invention deals with a speech recognition and natural language understanding system .More specifically , the present invention deals with a composite rules - based grammar and statistical model used to perform both speech recognition and natural language understanding .", "label": "", "metadata": {}, "score": "76.82262"}
{"text": "0054 ] .[ 0054]FIG .2A is a block diagram of a model authoring system 200 in accordance with one embodiment of the present invention .Model authoring system 200 includes model authoring component 202 and an optional user interface 204 .", "label": "", "metadata": {}, "score": "76.978"}
{"text": "[0166 ] For example , if a user has entered ' us ' by character input , they will be presented with a selection of word predictions based on this input .This saves the user from repeatedly pressing the delete button to remove the characters accidentally entered by the incorrect word selection .", "label": "", "metadata": {}, "score": "76.99985"}
{"text": "[0002 ] The present invention relates generally to a system and method for inputting text into small screen devices .In particular the invention relates to a system for generating text predictions for display and user selection and a method for doing so .", "label": "", "metadata": {}, "score": "77.04733"}
{"text": "[ 0208 ] The system also comprises an input model updater 905 .The input model updater 905 receives input event to target correspondences 914 in response to a selection event 920 .The selection event 920 identifies the targets which the user had intended to input and thus allows the predictor 904 to match the targets to the input events 910 .", "label": "", "metadata": {}, "score": "77.31779"}
{"text": "Version 2 requires only 8 bytes per bigram and 4 bytes per trigram .Support for gzip compression .As well as the compress data compression utility used in version 1 of the toolkit , there is now also support for gzip .", "label": "", "metadata": {}, "score": "77.38584"}
{"text": "To reduce the computation , small values can be filtered out of the per - event probability vector .These per - event probability vectors are concatenated to form the Target Sequence Intention Structure 914 , which forms the input to the next stage : target mapping 903 .", "label": "", "metadata": {}, "score": "77.41757"}
{"text": "Optional grammar library 209 ( shown in FIGS .2A , 4 and 13 ) can be adapted to the training data 208 statistically .For example , assume that the grammar library 209 includes a relatively large city list that contains both large and small international and domestic cities .", "label": "", "metadata": {}, "score": "77.65303"}
{"text": "In an alternative embodiment , punctuation symbols , special characters , numbers , etc . , such as shown in FIG .16c , are accessible by changing the screen through various gesture movements .[0373 ] Embodiments of the interface 60 do not require the use of buttons .", "label": "", "metadata": {}, "score": "77.67966"}
{"text": "The following stages would occur : The Multi - LM 8 tokenizes the sequence and inserts the BOS marker , \" Hope to see you very soon ! \" becomes , for example , \" A \" \" Hope \" \" to \" \" see \" \" you \" \" very \" \" soon \" \" !", "label": "", "metadata": {}, "score": "77.73752"}
{"text": "[ 0059 ] .Rule three shows that the ShowFlightProperty portion includes a ShowFlightPreFlight portion , a Flight portion , and a ShowFlightPostFlight portion .This indicates that the slot Flight in the schema can have both a preamble and a postamble .", "label": "", "metadata": {}, "score": "77.74931"}
{"text": "0004 ] Not applicable .BACKGROUND .[ 0005 ] There currently exists a wide range of text input techniques for use with electronic devices .QWERTY - style keyboards are the de facto standard for text input on desktop and laptop computers .", "label": "", "metadata": {}, "score": "77.82213"}
{"text": "The KeyPressVector can be generalised to ' ignore ' each character entered by the user , by inserting a null character with a certain probability after each character entered .The null character , \" \" , is implemented in the probabilistic trie by the KeyPressVector remaining at the current node .", "label": "", "metadata": {}, "score": "78.130264"}
{"text": "The KeyPressVector can be generalised to ' ignore ' each character entered by the user , by inserting a null character with a certain probability after each character entered .The null character , \" \" , is implemented in the probabilistic trie by the KeyPressVector remaining at the current node .", "label": "", "metadata": {}, "score": "78.130264"}
{"text": "In theory , with an adequate error - correction system , a full keyboard with error correction should require less disambiguation keystrokes than a reduced keyboard .On a hard keyboard , an input event relates to a user pressing down on a key of the keyboard .", "label": "", "metadata": {}, "score": "78.17031"}
{"text": "FIG .3 is a block diagram of a method of generating count files .[ 0020 ] FIG .4 is a block diagram of an exemplary count file .[ 0021 ] FIG .5 is a block diagram of two exemplary count files being combined into a merged count file .", "label": "", "metadata": {}, "score": "78.218445"}
{"text": "0238 ] These estimates refer to two distinct sequences ( one from French , one from English ) ; however , because they are lexically identical , it is not necessary to present them both to the user .Thus , in accordance with the preferred embodiment , the most probable estimate for a given lexical sequence is retained and any lexical duplicates are discarded .", "label": "", "metadata": {}, "score": "78.26668"}
{"text": "However , the segmentation of the examples is ambiguous .In other words , it is not yet known whether the word \" from \" in the first example is to be mapped to the preterminal ShowFlightCmd or to the preterminal FlightPreDepatureCity .", "label": "", "metadata": {}, "score": "78.44168"}
{"text": "The method of claim 16 , wherein merging respective data files of the first set of data files with corresponding data files of the second set of data files results in a set of merged data files .The method of claim 25 , further comprising pruning , in parallel , respective data files of the set of merged data files .", "label": "", "metadata": {}, "score": "78.47095"}
{"text": "All of the tools have been written in C , so there is no longer the reliance on shell scripts and UNIX tools such as sort and awk .The tools now run much faster , due to requiring much less disk I / O , although they do now require more RAM than the tools of version 1 .", "label": "", "metadata": {}, "score": "78.48477"}
{"text": "[ 0383 ] FIG .17 illustrates the interface 60 as displayed on a personal watch 80 , resulting in interface 60 with dimensions of approximately 1 inch by 1 inch .As can be appreciated from FIG .17 , it would be impossible to use buttons for each of the characters on such a small screen , as the size of a user 's finger would result in the user selecting multiple buttons or the wrong buttons .", "label": "", "metadata": {}, "score": "78.50633"}
{"text": "A touch - screen interface that comprises : a single or multi - character entry mechanism ; a word prediction pane ; and a typing pane to display inputted text .The interface of claim 59 , wherein the interface further comprises a menu button which toggles the screen between prediction , numbers and punctuation , and further punctuation screens .", "label": "", "metadata": {}, "score": "78.54495"}
{"text": "9 is an example of an annotated sentence .[ 0028 ] .[0028]FIG .10 shows generated rules .[ 0029 ] .[ 0029]FIG .11 illustrates a state diagram for a composite model .[ 0030 ] . [", "label": "", "metadata": {}, "score": "78.5643"}
{"text": "With reference to FIG .1 , an exemplary system for implementing the invention includes a general purpose computing device in the form of a computer 110 .Components of computer 110 may include , but are not limited to , a processing unit 120 , a system memory 130 , and a system bus 121 that couples various system components including the system memory to the processing unit 120 .", "label": "", "metadata": {}, "score": "78.64347"}
{"text": "The input data communicated to the target modeling module 82 can consist of the absolute Cartesian coordinates of the user 's touch , relative location information , relative area / region information , character layout information , and contextual information .Specifically , relative location information may depend on the layout of the character pane 62 , such as the user 's input being on the top left quadrant of the pane , or the touch being to the left of the letter \" J \" , etc . .", "label": "", "metadata": {}, "score": "78.65509"}
{"text": "0381 ]The interface 60 also includes a typing pane 70 , which displays the text that has been typed by the user , and a next pane 72 , used to finalize the text entry .The next pane 72 is optional , as swipe gestures can also be used to finalize text entry .", "label": "", "metadata": {}, "score": "78.664505"}
{"text": "0163 ] For numbers or punctuation , all buttons are multitap .For example , with tricharacter buttons , the user can press once to enter the first term ( of the three term button ) , press twice to enter the second term , or press three times for the third term to be entered .", "label": "", "metadata": {}, "score": "78.72031"}
{"text": "In yet another alternative , the words \" Flight \" and \" from \" are split such that the word \" Flight \" is mapped to the preterminal ShowFlightCmd and the word \" from \" is mapped to the preterminal FlightPreDepartureCity .", "label": "", "metadata": {}, "score": "78.81755"}
{"text": "The tools in version 2 of this toolkit enable these models to be constructed and evaluated .Interactive language model evaluation .The program evallm is used to test the language models produced by the toolkit .Commands to this program are read in from the standard input after the language model has been read , so the user can issue commands interactively , rather than simply from the shell command line .", "label": "", "metadata": {}, "score": "78.82065"}
{"text": "[ 0037 ]FIG .4 is a block diagram of an exemplary count file 400 generated , for example , from text file 202a .The exemplary count file 400 includes the counts 410a-410c , 414a-414c and 418a-418c .Counts 410a-410c , etc . , correspond to n - grams 408a-408c ( bigrams ) , 412a-412c ( trigrams ) and 416a-416c ( trigrams ) .", "label": "", "metadata": {}, "score": "78.8969"}
{"text": "NT .P .N .w . )P .NT .c .NT .N .w . )NT .P .NT .c .NT .N .w . )P .NT .P .", "label": "", "metadata": {}, "score": "78.908554"}
{"text": "Another example of a system requiring a plurality of application specific language models 4 is that of a computer which can be used for word processing , emailing and sending SMS messages to mobile devices through the internet , thus requiring three application specific language models 4 .", "label": "", "metadata": {}, "score": "79.23927"}
{"text": "Another example of a system requiring a plurality of application specific language models 4 is that of a computer which can be used for word processing , emailing and sending SMS messages to mobile devices through the internet , thus requiring three application specific language models 4 .", "label": "", "metadata": {}, "score": "79.23927"}
{"text": "8a , where the most likely word button 48 can be configured to display a phrase input .For example , the most likely word button 48 can be divided into two or more buttons if the most likely prediction is a two or more term phrase , and the alternative word button 58 can show the next most likely phrase prediction .", "label": "", "metadata": {}, "score": "79.268005"}
{"text": "8a , where the most likely word button 48 can be configured to display a phrase input .For example , the most likely word button 48 can be divided into two or more buttons if the most likely prediction is a two or more term phrase , and the alternative word button 58 can show the next most likely phrase prediction .", "label": "", "metadata": {}, "score": "79.268005"}
{"text": "[ 0071 ] The KeyPressVector 31 contains a number of elements equal to the number of keystrokes made by the user , in this example two .[ 0072 ] The first element states that the user intended to enter the character ' i ' with 0.7 probability , and the character ' I ' with 0.3 probability .", "label": "", "metadata": {}, "score": "79.43303"}
{"text": "[ 0071 ] The KeyPressVector 31 contains a number of elements equal to the number of keystrokes made by the user , in this example two .[ 0072 ] The first element states that the user intended to enter the character ' i ' with 0.7 probability , and the character ' I ' with 0.3 probability .", "label": "", "metadata": {}, "score": "79.43303"}
{"text": "12 shows pseudo code describing a training technique .[ 0031 ] .[ 0031]FIG .13 is a block diagram illustrating a runtime system for using a model generated in accordance with the present invention .[ 0032 ] .[ 0032]FIG .", "label": "", "metadata": {}, "score": "79.44444"}
{"text": "[0117 ] The intended next term \" very \" is not in the currently - predicted list of terms .The user can enter multi - character ' v / x / z ' input to prompt the predictive text engine 100 to provide more relevant predictions .", "label": "", "metadata": {}, "score": "79.45404"}
{"text": "[0062 ] .For example , there is no rule which would indicate that the phrase \" please show me the flights . . .\" is mapped to the ShowFlightCmd .Similarly , there is no rewrite rule which indicates which words would specifically map to , for example , the FlightPreArrivalCity preamble , etc .", "label": "", "metadata": {}, "score": "79.45423"}
{"text": "In another example , as shown in the trigrams 416a-416c of FIG .4 , \" Spot \" is preceded by the word \" watch .\" [ 0040 ] FIG .5 is a functional block diagram of a method of merging two exemplary count files 502 and 504 .", "label": "", "metadata": {}, "score": "79.46828"}
{"text": "[ 0035]FIG .17A illustrates a Hidden Markov Model ( HMM ) structure corresponding to a top - level grammar generated from the schema shown in FIG .16 .[ 0036 ] .[ 0036]FIG .17B illustrates the HMM structure for the \" ShowFlight \" model from FIG .", "label": "", "metadata": {}, "score": "79.79593"}
{"text": "[ 0003 ] Text input for mobile phones , however , has focused primarily on methods of entering alphabetic characters using a 9-digit keypad , where each key usually represents either three or four characters .There are various techniques employed to reduce the number of keystrokes required .", "label": "", "metadata": {}, "score": "79.92071"}
{"text": "[ 0003 ] Text input for mobile phones , however , has focused primarily on methods of entering alphabetic characters using a 9-digit keypad , where each key usually represents either three or four characters .There are various techniques employed to reduce the number of keystrokes required .", "label": "", "metadata": {}, "score": "79.92071"}
{"text": "Merged n - gram count 112 includes the total number of occurrences of the n - grams observed in the combined set of text files 102a-102 m .According to this method , the individual n - gram counts 106a-106 m are serially merged to the merged n - gram count 112 .", "label": "", "metadata": {}, "score": "79.92514"}
{"text": "The decoder runs an order of magnitude faster than the robust parser after pruning .[ 0153 ] .In accordance with one embodiment of the present invention , the composite model 351 is not only used in a natural language understanding system , but can also be used in a speech recognition system .", "label": "", "metadata": {}, "score": "79.95959"}
{"text": "Schema 212 also illustrates the semantic class for Flight in greater detail indicating that it has four slots that correspond to a departure time , an arrival time , a departure city and an arrival city .[ 0057 ] .From schema 212 , model authoring component 202 can generate a set of rules illustrated in FIG .", "label": "", "metadata": {}, "score": "79.97271"}
{"text": "[ 0164 ] .[ 0164]FIG .17B illustrates the ShowFlight subnetwork 604 model in greater detail , and use of the subnetwork model is illustrated by FIG .17C.The ShowFlight subnetwork model shown in FIG .17B models the linguistic expressions that users may use to issue a ShowFlight command .", "label": "", "metadata": {}, "score": "80.03241"}
{"text": "By way of example , and not limitation , communication media includes wired media such as a wired network or direct - wired connection , and wireless media such as acoustic , FR , infrared and other wireless media .Combinations of any of the above should also be included within the scope of computer readable media .", "label": "", "metadata": {}, "score": "80.30614"}
{"text": "Similarly , an application specific language model 4 is generated from text 2 from that specific application .Similarly , in the case of a mobile device , the application specific language model 4 will be generated from mobile SMS text language 2 .", "label": "", "metadata": {}, "score": "80.55301"}
{"text": "Similarly , an application specific language model 4 is generated from text 2 from that specific application .Similarly , in the case of a mobile device , the application specific language model 4 will be generated from mobile SMS text language 2 .", "label": "", "metadata": {}, "score": "80.55301"}
{"text": "The method as recited in claim 30 , further comprising the step of enabling the user to associate a first gesture with a first function of the interface .The method as recited in claim 30 , further comprising the step of enabling the user to perform a swipe gesture among the one or more swipe gestures to change the character pane from all capitals to all lowercase .", "label": "", "metadata": {}, "score": "80.63658"}
{"text": "These trigrams 412a-412c and 416a-416c also have \" Spot \" as the most recent history element .They further include the element which occurred before \" Spot \" in the text file .For example , in the trigrams 412a-412c , \" Spot \" is preceded by \" see . \"", "label": "", "metadata": {}, "score": "80.79404"}
{"text": "In one embodiment , the system further comprises a user specific language model 7 , which comprises a dynamic language model trained progressively on user input .The user input text stream 10 refers to the evolving text data generated by the user which is then fed back into the dynamic user specific language model as progressive training data .", "label": "", "metadata": {}, "score": "80.849"}
{"text": "The interface as recited in claim 1 , wherein the one or more target models are selected from a plurality of target models based on one or more conditions .The interface as recited in claim 10 , wherein the one or more conditions include an application type associated with the interface , a screen size , and a layout of the character pane .", "label": "", "metadata": {}, "score": "80.8835"}
{"text": "In an embodiment this is optional behavior on the default screen .Multitap is used when it is necessary to type characters unambiguously ( e.g. for entry of a new word that is not in the language model vocabulary ) .In an embodiment to enter characters unambiguously , a user presses a toggle button to go to a numbers / punctuation screen where all buttons are multitap .", "label": "", "metadata": {}, "score": "81.330536"}
{"text": "In the preferred system , an input candidate is a sequence , and the set of input candidates is represented as an extended PCSG ( EPCSG ) .An EPCSG is a PCSG but with one additional structure that violates standard PCSG properties ( defined below ) .", "label": "", "metadata": {}, "score": "81.56726"}
{"text": "In accordance with another alternative , both words \" Flight \" and \" from \" are mapped to ShowFlightCmd \" while the other preterminals ShowFlightPreFlight and FlightPreDepartureCity are both mapped to empty sets .[0075 ] .In still another alternative , \" Flight \" is mapped to ShowFlightCmd and \" from \" is mapped to FlightPreDepartureCity , while the remaining preterminal ShowFlightPreFlight is mapped to an empty set .", "label": "", "metadata": {}, "score": "81.90756"}
{"text": "[ 0030 ] FIGS .13 - 15 illustrate various steps involved in the construction of a probabilistic , constrained sequence graph ; .[0031 ] FIGS . 16A-16C illustrate an alternative embodiment of an interface without buttons for a small screen device ; . [ 0032 ] FIG .", "label": "", "metadata": {}, "score": "81.97471"}
{"text": "The counts 306a-306x are stored in memory with the corresponding text elements 304a-304x .Alternatively , counts 306a-306x are generated from a representative sample of text from one or more of the text files .For example , w 1 may occur 3 times , w 2 may occur 96 times , w 3 may occur 32 times , and w x may occur 58 times .", "label": "", "metadata": {}, "score": "81.98451"}
{"text": "ShowFlightCmd . )Pr . flight .show .ShowFlightCmd . )Pr .s . flight .ShowFlightCmd . )While the rules would not have identified \" show flight \" as a ShowFlightCmd , the above n - gram probability in Eq .", "label": "", "metadata": {}, "score": "82.08723"}
{"text": "3B is a flow diagram illustrating the operation of component 203 and will be described along with FIGS .2F and 2G. [ 0082 ] .First , component 302 enumerates all possible segmentations .This is shown in the left column of FIG .", "label": "", "metadata": {}, "score": "82.10481"}
{"text": "2 ) .[0038 ]One of the text elements assigned to count file 400 is the word \" Spot .\"As shown in the figure , the bigrams 408a-408c have \" Spot \" as the most recent text element in their history .", "label": "", "metadata": {}, "score": "82.3089"}
{"text": "[ 0217 ] The user touches the screen very near to the first character ' i ' .The virtual keyboard 901 generates an input event 910 ( e.g. Cartesian coordinates ) representing that location , and sends it to the input probability generator 902 .", "label": "", "metadata": {}, "score": "82.46314"}
{"text": "The thick path 410 represents the Viterbi interpretation .A higher thin path 412 , identifies the correct tasks , but neither of the slots .A lower thin path 414 ( which shares part of Viterbi path 410 ) identifies the attendee but not the start time slot .", "label": "", "metadata": {}, "score": "82.71735"}
{"text": "In an embodiment to enter characters unambiguously , a user presses a toggle button to go to a numbers / punctuation screen where all buttons are multitap .[0197 ] For numbers or punctuation , all buttons are multitap .For example , with tricharacter buttons , the user can press once to enter the first term ( of the three term button ) , press twice to enter the second term , or press three times for the third term to be entered .", "label": "", "metadata": {}, "score": "82.74608"}
{"text": "In an embodiment to enter characters unambiguously , a user presses a toggle button to go to a numbers / punctuation screen where all buttons are multitap .[0197 ] For numbers or punctuation , all buttons are multitap .For example , with tricharacter buttons , the user can press once to enter the first term ( of the three term button ) , press twice to enter the second term , or press three times for the third term to be entered .", "label": "", "metadata": {}, "score": "82.74608"}
{"text": "More preferably , the at least one language model specific to an application comprises one or more of an email , SMS text , newswire , academic , blog , or product review specific language model .Alternatively , the at least one language model specific to an application comprises an email and an SMS text specific language model and the text predictions are generated using one or both of the email and SMS text specific language models .", "label": "", "metadata": {}, "score": "82.774254"}
{"text": "More preferably , the at least one language model specific to an application comprises one or more of an email , SMS text , newswire , academic , blog , or product review specific language model .Alternatively , the at least one language model specific to an application comprises an email and an SMS text specific language model and the text predictions are generated using one or both of the email and SMS text specific language models .", "label": "", "metadata": {}, "score": "82.774254"}
{"text": "More preferably , the at least one language model specific to an application comprises one or more of an email , SMS text , newswire , academic , blog , or product review specific language model .Alternatively , the at least one language model specific to an application comprises an email and an SMS text specific language model and the text predictions are generated using one or both of the email and SMS text specific language models .", "label": "", "metadata": {}, "score": "82.774254"}
{"text": "Informally , the context represents the observed evidence about what the user has already entered , while the input represents the observed evidence about what the user is currently entering .For instance , in English , if the user has entered the sequence \" My name is B \" , we might consider the context evidence to be the sequence \" My name is \" , and the input evidence to be the sequence \" B \" .", "label": "", "metadata": {}, "score": "82.82284"}
{"text": "The Input Sequence Intention Structure 916 need only be used as a guide ; it may , for example , add additional characters as if the user has missed some or ignore characters as if the user has entered extra characters .", "label": "", "metadata": {}, "score": "82.830696"}
{"text": "P .NT .N .w .c .N .w . )P .N .w . )c .NT .N .w . )P .NT .P .N .w . )", "label": "", "metadata": {}, "score": "82.84171"}
{"text": "The log probability for each of the first nine transitions shown in FIG .14 are listed below for the Viterbi path 410 .// Word bigram .// PCFG .// Word bigram .// Slot bigram .[ 0152 ] .", "label": "", "metadata": {}, "score": "82.8772"}
{"text": "The interface as recited in claim 20 , further comprising the step of accepting a selection of a predicted word among the set of word predictions displayed on the prediction pane .The interface as recited in claim 20 , wherein the set of word predictions include a set of probabilities associated with each word prediction among the set of word predictions , the set of probabilities indicating a likelihood that the user intended to type each word prediction .", "label": "", "metadata": {}, "score": "83.0603"}
{"text": "[ 0061 ] .Rules six - nine correspond to the four slots in schema 212 shown in FIG .2B. Rule six defines the first property as having a departure city slot that is preceded by a preamble ( FlightPreDepartureCity ) and is followed by a postamble ( FlightPostDepartureCity ) .", "label": "", "metadata": {}, "score": "83.133545"}
{"text": "No .13/262,190 , filed on Sep. 29 , 2011 , which is a national stage application of International Application Number PCT / GB2010/000622 , filed Mar. 30 , 2010 , which claims priority to GB Patent Application No .0905457.8 , filed Mar. 30 , 2009 .", "label": "", "metadata": {}, "score": "83.17351"}
{"text": "The language model subsets 714a-716k are generated for each merged count using the methodology described above , treating each merged count file 704a-706k as an individual merged count .[ 0047 ] The next step in the method 700 is interpolation of the corpus language models 712a-712n ( step 718 ) .", "label": "", "metadata": {}, "score": "83.184425"}
{"text": "Other devices such as the Blackberry Storm use multi - character soft keyboards with various methods of disambiguation and completion .There are also many third - party systems that offer alternative methods of text entry for touch - screen devices .", "label": "", "metadata": {}, "score": "83.22935"}
{"text": "Other devices such as the Blackberry Storm use multi - character soft keyboards with various methods of disambiguation and completion .There are also many third - party systems that offer alternative methods of text entry for touch - screen devices .", "label": "", "metadata": {}, "score": "83.22935"}
{"text": "Other devices such as the Blackberry Storm use multi - character soft keyboards with various methods of disambiguation and completion .There are also many third - party systems that offer alternative methods of text entry for touch - screen devices .", "label": "", "metadata": {}, "score": "83.22935"}
{"text": "2 is a block diagram of a method 200 of generating counts according to an illustrative embodiment of the invention .The method 200 begins with the provision of a plurality of text files 202a-202 m .Each text file 202a may be an independent training corpus , or it may be a portion of a larger training corpus , which has been split into m files .", "label": "", "metadata": {}, "score": "83.30411"}
{"text": "[ 0020]FIG .3A is a block diagram showing a grammar authoring component in greater detail .[ 0021 ] .[ 0021]FIG .3B is a flow diagram illustrating the operation of the grammar - authoring component shown in FIG .", "label": "", "metadata": {}, "score": "83.3414"}
{"text": "There are other mechanisms for user selection of a prediction , for example a gesture on a soft keyboard such as a swipe - right across the screen can be a mechanism for inserting the most probable prediction .The selected prediction 920 corresponds to the character sequence a user intended to enter through the input events 910 .", "label": "", "metadata": {}, "score": "83.482796"}
{"text": "Decoding is described in greater detail with respect to FIG .14 .FIG .14 illustrates a dynamic programming trellis structure representing a dynamic programming decoder for an input \" new meeting with Peter at five \" .[0151 ] .", "label": "", "metadata": {}, "score": "83.50894"}
{"text": "16A the first predicted pane 66 displays the text \" I \" and the second predicted pane 68 displays the text \" The \" .The prediction pane 64 need not display exactly two words , but it can also display only a single word or more than two words .", "label": "", "metadata": {}, "score": "83.54266"}
{"text": "Additionally , the count files may include a count of all unigrams in the corresponding text file .[0031 ]The n - gram counts 210a-210 m from the text files 202a-202 m are merged in parallel at 218a-218k by merging each count file 212a-214k with the respective corresponding count files 212a-214k of the n - gram counts 210a-210 m .", "label": "", "metadata": {}, "score": "83.58652"}
{"text": "Identical n - grams from the count files 502 and 504 are combined by adding their respective counts .N - grams unique to each text file are added to the merged count file 506 .As mentioned above , the corresponding count files of all the text files of a corpus are preferably merged in parallel .", "label": "", "metadata": {}, "score": "83.732285"}
{"text": "Swiping diagonally from the top left corner of the screen to the bottom right corner of the screen can change the font .Swiping diagonally from the top right corner of the screen to the bottom left corner of the screen can change the layout of the screen to display special characters , such as letters with tildes , dieresis , breves , accent marks , etc . .", "label": "", "metadata": {}, "score": "83.921455"}
{"text": "[ 0033 ] .[0033]FIG .15 is a simplified block diagram of a speech recognition system .[ 0034 ] .[ 0034]FIG .16 illustrates another simplified application schema which is used in the example shown in FIGS .", "label": "", "metadata": {}, "score": "83.93663"}
{"text": "10/427,604 , filed May 1 , 2003 , the content of which is hereby incorporated by reference in its entirety .BACKGROUND OF THE INVENTION .[0002 ] .The present invention relates to speech recognition and natural language understanding .", "label": "", "metadata": {}, "score": "83.95441"}
{"text": "[ 0042 ] .[ 0042]FIG .1 illustrates an example of a suitable computing system environment 100 on which the invention may be implemented .The computing system environment 100 is only one example of a suitable computing environment and is not intended to suggest any limitation as to the scope of use or functionality of the invention .", "label": "", "metadata": {}, "score": "83.971054"}
{"text": "[ 0224 ]The predictor 904 makes predictions 918 : [ \" it \" , \" it 's \" , \" or \" ] which are displayed to the user .The predictions may be displayed by the virtual keyboard .The user selects the prediction \" it 's \" .", "label": "", "metadata": {}, "score": "84.00596"}
{"text": "Each n - gram count 210a-210 m includes k count files , such as count files 212a-212k , generated from text file 202a , and 214a-214k , generated from text file 202 m .The count files 212a-212k and 214a-214k correspond to the previously generated skeleton count files 204a-204k .", "label": "", "metadata": {}, "score": "84.07797"}
{"text": "[ 0379 ] In further reference to FIGS . 16A-16C , a prediction pane 64 is displayed on top of the character pane 62 .The prediction pane 64 includes a first predicted word pane 66 and a second predicted word pane 68 .", "label": "", "metadata": {}, "score": "84.21637"}
{"text": "If the Input Sequence Intention Structure 916 comprises the word fragments tagged with their targets , the predictor 904 is configured to track the target tags , such that matching input events 910 to their targets comprises pairing the input events 910 to their targets .", "label": "", "metadata": {}, "score": "84.57234"}
{"text": "The computer 110 may operate in a networked environment using logical connections to one or more remote computers , such as a remote computer 180 .The remote computer 180 may be a personal computer , a hand - held device , a server , a router , a network PC , a peer device or other common network node , and typically includes many or all of the elements described above relative to the computer 110 .", "label": "", "metadata": {}, "score": "84.63229"}
{"text": "1016385.5 , filed Sep. 29 , 2010 .The present application also claims priority to GB Patent Application No .1108200.5 , filed May 16 , 2011 .Wherein all of the above mentioned references are incorporated herein by reference in their entirety .", "label": "", "metadata": {}, "score": "84.69229"}
{"text": "[0058 ] .Rule two indicates that the ShowFlightProperties portion can have one or more properties in it .For example , rule two indicates that the ShowFlightProperties portion includes at least one ShowFlightProperty which can be followed by an optional ShowFlightProperties portion .", "label": "", "metadata": {}, "score": "84.703804"}
{"text": "The selection event would result in ( 1 ) the matching of touch-1 with the letter \" T \" and ( 2 ) the matching of touch-2 with the letter \" h \" .The matched data can then be used as additional training points for the incremental learning of the target model set 906 .", "label": "", "metadata": {}, "score": "84.760185"}
{"text": "0213 ] Preferably , the word fragment map comprises targets mapped to word fragments , where the word fragments are tagged with the targets from which they came .[ 0214 ] The Input Sequence Intention Structure 916 contains the word fragments , preferably tagged to the characters and input events from which they came , and probabilities corresponding to a sequence of input events 910 .", "label": "", "metadata": {}, "score": "84.912796"}
{"text": "In other embodiments , the n - gram may be a bigram , a trigram , a four - gram , a five - gram , a six - gram , a seven - gram , and eight - gram , or longer than an eight - gram .", "label": "", "metadata": {}, "score": "84.98961"}
{"text": "[ 0171 ] The only requirement for the virtual keyboard 901 is that there are distinct ' target points ' for the user , which correspond to intended input .Thus , each target of the virtual keyboard 901 can represent single or multiple characters , or any other item of information for user selection .", "label": "", "metadata": {}, "score": "85.0266"}
{"text": "Sequences of n words are referred to as n - grams .N - grams are classified based on the number of words included in the n - gram .For example , a unigram is a single word , a bigram is an ordered sequence of two words , a trigram includes three words , and a 5-gram includes five words .", "label": "", "metadata": {}, "score": "85.05925"}
{"text": "Component 350 trains composite model 351 using , in part , the EM algorithm techniques discussed above .For example , assume that FIG .5 shows all enumerated rules for the ShowFlightCmd according to different sample segmentations .[0130 ] .", "label": "", "metadata": {}, "score": "85.66548"}
{"text": "The method as recited in claim 30 , further comprising the step of enabling the user to perform a swipe gesture among the one or more swipe gestures to change a font of the character pane .The method as recited in claim 30 , further comprising the step of enabling the user to perform a swipe gesture among the one or more swipe gestures to change the character pane to special characters .", "label": "", "metadata": {}, "score": "85.763824"}
{"text": "The probabilistic trie has the advantage of mapping probability values to character strings .Furthermore , the probabilistic trie 33 is not restricted to a specified maximum depth .However , the choice of trie will be dependent on such factors as the available memory . [", "label": "", "metadata": {}, "score": "85.87736"}
{"text": "The probabilistic trie has the advantage of mapping probability values to character strings .Furthermore , the probabilistic trie 33 is not restricted to a specified maximum depth .However , the choice of trie will be dependent on such factors as the available memory . [", "label": "", "metadata": {}, "score": "85.87736"}
{"text": "However , this method does not keep track of which completion is the most probable ; it merely chooses the one used most recently .[0012 ] Blackberry 's SureType , Nuance 's XT9 and Zi Technology 's eZiType offer somewhat more sophisticated models , in which candidate completions are ordered on the basis of usage frequency statistics .", "label": "", "metadata": {}, "score": "86.03783"}
{"text": "However , this method does not keep track of which completion is the most probable ; it merely chooses the one used most recently .[0012 ] Blackberry 's SureType , Nuance 's XT9 and Zi Technology 's eZiType offer somewhat more sophisticated models , in which candidate completions are ordered on the basis of usage frequency statistics .", "label": "", "metadata": {}, "score": "86.03783"}
{"text": "In the alternative user interface of FIG .8a , the current input sequence is displayed in the typing pane 27 in real time .The actual character entry button 38 also displays the current input sequence , and this is shown alongside the current most likely word button 48 and the alternative word prediction 58 .", "label": "", "metadata": {}, "score": "86.18375"}
{"text": "In the alternative user interface of FIG .8a , the current input sequence is displayed in the typing pane 27 in real time .The actual character entry button 38 also displays the current input sequence , and this is shown alongside the current most likely word button 48 and the alternative word prediction 58 .", "label": "", "metadata": {}, "score": "86.18375"}
{"text": "Similarly , in the case of a mobile device , the application specific language model 4 will be generated from mobile SMS text language 2 .In some embodiments of the system a plurality of application specific language models 4 are required , for example a mobile device can be used for emailing and SMS text messaging , thus requiring an SMS specific language model and an email specific language model .", "label": "", "metadata": {}, "score": "86.26296"}
{"text": "Therefore , based on the annotation 214 , model authoring component 202 will know which slots map to the words \" Seattle \" and \" Boston \" .[ 0064 ] .From the annotated example and the template grammar rules shown in FIG .", "label": "", "metadata": {}, "score": "86.416565"}
{"text": "Reduced - size QWERTY - style keyboards are also used for text entry on mobile devices , such as PDAs and some mobile phones .These keyboards are generally operated using both thumbs , and their advantage lies in the fact that users are almost always familiar with the QWERTY layout .", "label": "", "metadata": {}, "score": "86.524254"}
{"text": "Reduced - size QWERTY - style keyboards are also used for text entry on mobile devices , such as PDAs and some mobile phones .These keyboards are generally operated using both thumbs , and their advantage lies in the fact that users are almost always familiar with the QWERTY layout .", "label": "", "metadata": {}, "score": "86.524254"}
{"text": "Component 350 can also train additional statistical model components in accordance with different embodiments of the present invention .This is illustrated in greater detail in the block diagram shown in FIG .6 .For instance , in that block diagram , statistical model portion 326 is shown as not only including a statistical model component for preterminals 340 , but also a plurality of other statistical models .", "label": "", "metadata": {}, "score": "87.08511"}
{"text": "2E. The first level 218 of parse tree 216 ( the portion that shows that ShowFlight is formed of ShowFlightCmd followed by ShowFlightProperties ) is formed from rule 1 in FIG .2C. [ 0065 ] .The second level 220 ( the portion indicating that ShowFlightProperties is formed of ShowFlightProperty ) is generated from rule 2 where the optional ShowFlightProperties protion is not used .", "label": "", "metadata": {}, "score": "87.109856"}
{"text": "Rather , completions are suggested in order of most recent use .However , this method does not keep track of which completion is the most probable ; it merely chooses the one used most recently .[0012 ] Blackberry 's SureType , Nuance 's XT9 and Zi Technology 's eZiType offer somewhat more sophisticated models , in which candidate completions are ordered on the basis of usage frequency statistics .", "label": "", "metadata": {}, "score": "87.1154"}
{"text": "[ 0046 ] .Computer 110 typically includes a variety of computer readable media .Computer readable media can be any available media that can be accessed by computer 110 and includes both volatile and nonvolatile media , removable and non - removable media .", "label": "", "metadata": {}, "score": "87.2901"}
{"text": "[0157 ] The prediction pane 25 comprises a set of buttons , each button displaying a word from a set of words or phrases that has been predicted by a text prediction engine .The typing pane 23 comprises a pane in which user inputted text is displayed .", "label": "", "metadata": {}, "score": "87.39294"}
{"text": "For example , text element 304a may be assigned to skeleton count file 310c and text element 304c may be assigned to skeleton count file 310a , etc .Splitting the count data into count files of approximately equal size allows an even distribution of computational load among the merging processes 218 .", "label": "", "metadata": {}, "score": "87.443985"}
{"text": "In a networked environment , program modules depicted relative to the computer 110 , or portions thereof , may be stored in the remote memory storage device .By way of example , and not limitation , FIG .1 illustrates remote application programs 185 as residing on remote computer 180 .", "label": "", "metadata": {}, "score": "87.54454"}
{"text": "It has proven highly effective given a measure of training and/or experience .Reduced - size QWERTY - style keyboards are also used for text entry on mobile devices , such as PDAs and some mobile phones .These keyboards are generally operated using both thumbs , and their advantage lies in the fact that users are almost always familiar with the QWERTY layout .", "label": "", "metadata": {}, "score": "87.56102"}
{"text": "2B. FIG .2B illustrates a greatly simplified schema 212 that can be input into system 200 , by a developer .Schema 212 is a schema that represents the meaning of various text strings for an input from a user to show flights departing from and arriving to different cities and having different departure and arrival times .", "label": "", "metadata": {}, "score": "87.56477"}
{"text": "This process results in the merged count 220 , having merged count files 222a-222k .[ 0032 ]According to various embodiments , the method 200 is performed on a conventional computer system having a processor , non - volatile storage ( e.g. a hard drive or optical drive ) , and random access memory ( RAM ) .", "label": "", "metadata": {}, "score": "88.004684"}
{"text": "Swiping from down to up can change the case of the character pane 62 , such as from all caps to lowercase and vice versa .Naturally , any of a number of different swiping or gesture movements could be used in place of those noted .", "label": "", "metadata": {}, "score": "88.10543"}
{"text": "The following stages would occur : The Multi - LM 8 tokenises the sequence and inserts the BOS marker , \" Hope to see you very soon ! \" becomes , for example , \" \" \" Hope \" \" to \" \" see \" \" you \" \" very \" \" soon \" \" ! \" [ 0178 ] \" see \" \" you \" \" very \" .", "label": "", "metadata": {}, "score": "88.15109"}
{"text": "The following stages would occur : The Multi - LM 8 tokenises the sequence and inserts the BOS marker , \" Hope to see you very soon ! \" becomes , for example , \" \" \" Hope \" \" to \" \" see \" \" you \" \" very \" \" soon \" \" ! \" [ 0178 ] \" see \" \" you \" \" very \" .", "label": "", "metadata": {}, "score": "88.15109"}
{"text": "Therefore , the probability that a sentence with \" Show flight \" in it will be recognized as a ShowFlightCmd can be calculated as follows : .Pr .s . showflight .s .ShowFlightCmd . )Pr . show .", "label": "", "metadata": {}, "score": "88.80592"}
{"text": "The next level 226 ( the portion indicating that the FlightProperties portion is formed of a FlightProperty portion followed by a FlightProperties portion ) is generated from rule 5 in FIG .2C. [ 0069 ] .[0070 ] .Thus , model authoring component 202 can learn how to map from the words \" Seattle \" and \" Boston \" in the input sentence into the CFG parse tree and into the rules generated in FIG .", "label": "", "metadata": {}, "score": "88.98092"}
{"text": "The interface as recited in claim 2 , wherein a swipe gesture among the first set of swipe gestures changes the character pane from letters to numbers and symbols .The interface as recited in claim 2 , wherein a swipe gesture among the first set of gestures changes a font of the character pane .", "label": "", "metadata": {}, "score": "89.00444"}
{"text": "Computer storage media includes both volatile and nonvolatile , removable and non - removable media implemented in any method or technology for storage of information such as computer readable instructions , data structures , program modules or other data .Communication media typically embodies computer readable instructions , data structures , program modules or other data in a modulated data signal such as a carrier WAV or other transport mechanism and includes any information delivery media .", "label": "", "metadata": {}, "score": "89.42609"}
{"text": "An example will help to clarify these concepts .Consider the following twelve context candidate sequences : .TABLE -US-00001 \" Sunday at 3 pm \" \" sunday at 3 pm \" \" Sun at 3 pm \" \" Sunday at 3 pm \" \" sunday at 3 pm \" \" Sun at 3 pm \" \" Sunday at 3p .", "label": "", "metadata": {}, "score": "89.80463"}
{"text": "The virtual keyboard 901 may have input event regions and selection event regions to distinguish between an input event 910 and a selection event 920 .[0170 ] For a soft keyboard , a selection event 920 relates to a user selecting the intended prediction by touching / clicking an area on the keyboard which displays that prediction , thus entering the prediction into the system .", "label": "", "metadata": {}, "score": "89.85916"}
{"text": "0002 ] There currently exists a wide range of text input techniques for use with electronic devices .QWERTY - style keyboards are the de facto standard for text input on desktop and laptop computers .The QWERTY layout was designed for two - handed , multi - digit typing on typewriters in 1878 and has been in wide use ever since .", "label": "", "metadata": {}, "score": "89.88771"}
{"text": "[ 0055 ] .The detailed operation of system 200 is described at greater length below .Briefly , however , a user provides model authoring component 202 with schema 206 and training example text strings 208 .This can be done either through optional user interface 204 , or through some other user input mechanism , or through automated means .", "label": "", "metadata": {}, "score": "90.05957"}
{"text": "Pressing either of the buttons 48 and 58 will cause the associated word to be entered .[ 0192 ] In both embodiments of the user interface , the typing pane 23 displays the text entered by a user .A user is able to scroll up or down previously typed text which is displayed in the typing pane , enabling the user to view and edit the text .", "label": "", "metadata": {}, "score": "90.31851"}
{"text": "Pressing either of the buttons 48 and 58 will cause the associated word to be entered .[ 0192 ] In both embodiments of the user interface , the typing pane 23 displays the text entered by a user .A user is able to scroll up or down previously typed text which is displayed in the typing pane , enabling the user to view and edit the text .", "label": "", "metadata": {}, "score": "90.31851"}
{"text": "In the case of the default screen , the side panes 24 , 26 comprise character buttons .However , in other configurations panes 24 , 25 , 26 are all used for character , punctuation and numeral buttons , and the screens are toggled so that a user can move between prediction , numeral and punctuation screens .", "label": "", "metadata": {}, "score": "90.320984"}
{"text": "0158 ] The typing pane 23 displays the text entered by a user .A user is able to scroll up or down previously typed text which is displayed in the typing pane , enabling the user to view and edit the text .", "label": "", "metadata": {}, "score": "90.406586"}
{"text": "The interface as recited in claim 1 , wherein a swipe gesture among the set of swipe gestures terminates text entry .The interface as recited in claim 1 , wherein a swipe gesture among the set of swipe gestures is a linear gesture .", "label": "", "metadata": {}, "score": "90.55933"}
{"text": "For example , if the context is \" The dog chased \" , it would be expected that this is significantly more likely to appear in English than in French .[ 0342 ] Thus , the context prior estimate weights more heavily the most appropriate language model from a plurality of language models relating to a plurality of languages , given the context .", "label": "", "metadata": {}, "score": "90.67826"}
{"text": "During training , a speech signal corresponding to training text 526 is input to trainer 524 [ YW1 ] , along with a lexical transcription of the training text 526 .Trainer 524 trains acoustic model 518 based on the training inputs .", "label": "", "metadata": {}, "score": "90.69495"}
{"text": "0395 ] It is to be understood that embodiments of the interface 60 for small screen devices are not limited to the particular layouts illustrated in FIGS .16 and 17 .For example , the character pane 62 can be displayed on the top of the interface 60 , with the typing pane 70 at the bottom of the interface 60 .", "label": "", "metadata": {}, "score": "90.81742"}
{"text": "Hence , predictions are constantly updated based upon previous sequence inputs .[ 0116 ] By way of an example , say a user has already entered the sequence \" Hope to see you \" and is intending to enter the terms \" very \" and \" soon \" in that order .", "label": "", "metadata": {}, "score": "90.876274"}
{"text": "[ 0034 ] FIG .3 is a block diagram of a method 300 of generating k skeleton count files 310a-310k from m text files 302a-302 m .Text elements 304a-304x are assigned to the skeleton count files 310a-310k as described in relation to FIG .", "label": "", "metadata": {}, "score": "91.08748"}
{"text": "0173 ] The input probability generator 902 receives an input event 910 and generates or updates a Target Sequence Intention Structure 912 .The Target Sequence Intention Structure 912 contains one or more targets and for each target the probability that the user had intended to select that target through the input event 910 .", "label": "", "metadata": {}, "score": "91.10861"}
{"text": "[ 0149 ] By way of an example , say a user has already entered the sequence \" Hope to see you \" and is intending to enter the terms \" very \" and \" soon \" in that order .The final prediction set 9 that is provided by the text prediction engine 100 to a user interface for display and user selection , may comprise ' all ' , ' soon ' , ' there ' , ' at ' , ' on ' , ' in ' .", "label": "", "metadata": {}, "score": "91.66492"}
{"text": "[ 0149 ] By way of an example , say a user has already entered the sequence \" Hope to see you \" and is intending to enter the terms \" very \" and \" soon \" in that order .The final prediction set 9 that is provided by the text prediction engine 100 to a user interface for display and user selection , may comprise ' all ' , ' soon ' , ' there ' , ' at ' , ' on ' , ' in ' .", "label": "", "metadata": {}, "score": "91.66492"}
{"text": "In our example , the prediction set 9 presented to the user might be ' very ' , ' via ' , ' visit ' , ' view ' , ' x ' .[0158 ] The intended term \" very \" now appears on the prediction list and can be selected .", "label": "", "metadata": {}, "score": "91.76835"}
{"text": "In our example , the prediction set 9 presented to the user might be ' very ' , ' via ' , ' visit ' , ' view ' , ' x ' .[0158 ] The intended term \" very \" now appears on the prediction list and can be selected .", "label": "", "metadata": {}, "score": "91.76835"}
{"text": "[0285 ] Unigram Model -- implements a distribution over sequences in a language without taking context into account , internally treating each sequence as an atomic entity .[0286 ] For example , if the training set is \" the dog chased the cat \" , the corresponding unigram language model might be : .", "label": "", "metadata": {}, "score": "92.015945"}
{"text": "max .c .s . )P .c . s .s . ) arg .max .c .s . )P .c . s .s . ) arg .max .c .s . )", "label": "", "metadata": {}, "score": "92.04199"}
{"text": "The keyboard 901 builds up a stream of input events 910 ( which in the present example are Cartesian coordinates representing the locations of the touches on the keyboard ) which is passed to an input probability generator 902 .[ 0211 ] The input probability generator 902 generates or updates a Target Sequence Intention Structure 912 .", "label": "", "metadata": {}, "score": "92.29973"}
{"text": "[0125 ]The intended term \" very \" now appears on the prediction list and can be selected .Once selected , the context , now including the term \" very \" , becomes \" Hope to see you very \" and the current input is empty .", "label": "", "metadata": {}, "score": "92.34021"}
{"text": "Associated with each of the characters is a probability that the user had intended to enter that character when he touched the screen at the location and a tag to the input event 910 .To generate the Target Sequence Intention Structure 912 the input probability generator 902 calculates the probability that the user had intended to enter each of the one or more characters by querying the associated model for each character with the input event 910 .", "label": "", "metadata": {}, "score": "92.56021"}
{"text": "Respective count files 212a-212k and 214a-214k of the n - gram counts 210a-210 m , generated from the various text files 202a-202 m , include counts of corresponding n - grams .Similarly , the n - gram counts included in count file 212b correspond to the n - gram counts of count file 214b .", "label": "", "metadata": {}, "score": "92.81073"}
{"text": "[0002 ] There currently exists a wide range of text input techniques for use with electronic devices .QWERTY - style keyboards are the de facto standard for text input on desktop and laptop computers .The QWERTY layout was designed for two - handed , multi - digit typing on typewriters in 1878 and has been in wide use ever since .", "label": "", "metadata": {}, "score": "92.88417"}
{"text": "The distinction between the two slots in the Flight slot is made only by the name of the slot .One is referred to as the \" Arrival \" city and the other is referred to as the \" Departure \" city .", "label": "", "metadata": {}, "score": "92.97703"}
{"text": "[ 0137 ] .For example , if a runtime input sentence is \" Show flights to Boston arriving on Tuesday , 11:00 a.m. \" The term \" arriving on \" will be analyzed as indicating that \" Tuesday \" corresponds to an arrival date .", "label": "", "metadata": {}, "score": "93.200676"}
{"text": "The interface as recited in claim 20 , further comprising the steps of : using a swipe gesture to query the text prediction engine to generate additional text predictions ; and displaying the additional text predictions on the prediction pane .The interface as recited in claim 20 , wherein the typing pane is positioned on a top portion of the interface , wherein the prediction pane is positioned below the typing pane , and wherein the QWERTY - style character pane is positioned below the prediction pane .", "label": "", "metadata": {}, "score": "94.28133"}
{"text": "The next level 222 ( the portion indicating that ShowFlightProperty is formed of ShowFlightPreFlight followed by Flight followed by ShowFlightPostFlight ) is generated from rule 3 in FIG .2C. [ 0067 ] .The next level 224 ( indicating that the Flight object is formed of a FlightProperties section ) is generated from rule 4 in FIG .", "label": "", "metadata": {}, "score": "94.3033"}
{"text": "The character buttons can have dual or tri character behaviour .In an embodiment this is the default behaviour on the default screen .The dual character buttons are designed with multi - region behaviour .In an embodiment this is optional behaviour on the default screen .", "label": "", "metadata": {}, "score": "94.35693"}
{"text": "The character buttons can have dual or tri character behaviour .In an embodiment this is the default behaviour on the default screen .The dual character buttons are designed with multi - region behaviour .In an embodiment this is optional behaviour on the default screen .", "label": "", "metadata": {}, "score": "94.35693"}
{"text": "[ 0006 ] A somewhat different model of text entry is offered by the University of Cambridge 's ' Dasher ' system , in which text input is driven by natural , continuous pointing gestures rather than keystrokes .It relies heavily on advanced language model - based character prediction , and is aimed primarily at improving accessibility for handicapped users , although it can also be used in mobile and speech recognition - based applications .", "label": "", "metadata": {}, "score": "94.53328"}
{"text": "[ 0006 ] A somewhat different model of text entry is offered by the University of Cambridge 's ' Dasher ' system , in which text input is driven by natural , continuous pointing gestures rather than keystrokes .It relies heavily on advanced language model - based character prediction , and is aimed primarily at improving accessibility for handicapped users , although it can also be used in mobile and speech recognition - based applications .", "label": "", "metadata": {}, "score": "94.53328"}
{"text": "The number k of skeleton count files 310a-310k may be determined based on the amount of RAM available in the computer system .In various embodiments , about 10 , about 25 , about 50 , about 75 , about 100 , about 150 , about 200 , about 250 , or about 300 count files are used .", "label": "", "metadata": {}, "score": "94.856155"}
{"text": "According to the count file 400 , the text file from which the count file 400 was populated included the word \" Spot \" followed by the word \" run \" 25 times ( 410a ) . \"Spot \" was followed by the word \" jump \" 20 times ( 410b ) , and was followed by the word \" beg \" 5 times ( 410c ) .", "label": "", "metadata": {}, "score": "94.95378"}
{"text": "[0009 ] A somewhat different model of text entry is offered by the University of Cambridge 's \" Dasher \" system , in which text input is driven by natural , continuous pointing gestures rather than keystrokes .It relies heavily on advanced language model - based character prediction , and is aimed primarily at improving accessibility for handicapped users , although it can also be used in mobile and speech recognition - based applications .", "label": "", "metadata": {}, "score": "97.34126"}
{"text": "A key is pressed multiple times to access the list of letters on that key .For example , a first key may provide access to letters \" ABC \" .In order to type the letter \" B \" , the first key would have to be pressed twice , and to type the letter \" C \" would require the first key to be pressed three times .", "label": "", "metadata": {}, "score": "97.38054"}
{"text": "When used in a LAN networking environment , the computer 110 is connected to the LAN 171 through a network interface or adapter 170 .When used in a WAN networking environment , the computer 110 typically includes a modem 172 or other means for establishing communications over the WAN 173 , such as the Internet .", "label": "", "metadata": {}, "score": "97.99992"}
{"text": "Corpus language models 712a-712n are generated in parallel at step 708 for each of the merged counts 702a-706k .Each corpus language model 712a-712n includes k corpus language model subsets .Each corpus language model subset 714a-716k corresponds to a merged count file 704a-706k .", "label": "", "metadata": {}, "score": "98.03694"}
{"text": "The second example indicates that the words \" Flight from \" can be mapped to preterminals \" ShowFlightCmd and FlightPreDepatureCity .The example may be harvested by component 302 from an example sentence like \" Flight from Seattle to Boston \" .", "label": "", "metadata": {}, "score": "98.43796"}
{"text": "The side panes 24 , 26 can comprise a set of buttons corresponding to character , punctuation or numeral keys .In the case of the default screen , the side panes 24 , 26 comprise character buttons .However , in other configurations panes 24 , 25 , 26 are all used for character , punctuation and numeral buttons , and the screens are toggled so that a user can move between prediction , numeral and punctuation screens .", "label": "", "metadata": {}, "score": "98.44533"}
{"text": "The side panes 24 , 26 can comprise a set of buttons corresponding to character , punctuation or numeral keys .In the case of the default screen , the side panes 24 , 26 comprise character buttons .However , in other configurations panes 24 , 25 , 26 are all used for character , punctuation and numeral buttons , and the screens are toggled so that a user can move between prediction , numeral and punctuation screens .", "label": "", "metadata": {}, "score": "98.44533"}
{"text": "Before describing this in greater detail , a brief discussion of the overall operation of system 500 is provided .[ 0155 ] .In FIG .15 , a speaker 501 speaks into a microphone 504 .The audio signals detected by microphone 504 are converted into electrical signals that are provided to analog - to - digital ( A - to - D ) converter 506 .", "label": "", "metadata": {}, "score": "98.48609"}
{"text": "16B , the interface 60 is shown after the user has swiped up to change the case of the character pane 62 from all capitals letters to lowercase letters .FIG .16c illustrates the interface 60 after swiping down to change the character pane 62 from the QWERTY - style layout to numbers and punctuation .", "label": "", "metadata": {}, "score": "99.51834"}
{"text": "[0079 ] .[ 0079]FIG .3A shows a block diagram illustrating model authoring component 202 in greater detail .FIG .3A shows that model authoring component 202 illustratively includes template grammar generator 300 , segmentation EM application component 302 and pruning component 304 .", "label": "", "metadata": {}, "score": "99.99644"}
{"text": "P . from .ShowFlightCmd .FPDCity . )P .ShowFlightCmd . )P . from .FPDCity . )P . from .ShowFlightCmd . )P .FPDCity . )[ . .][ . . ]C .", "label": "", "metadata": {}, "score": "100.2148"}
{"text": "[ 0327 ] It will be understood that any type of variation can be encoded in this framework .[0329 ] For example , the PCSG construction algorithm operating on an original sequence \" sunday at 3 pm \" is considered .", "label": "", "metadata": {}, "score": "101.367424"}
{"text": "In one embodiment , the system comprises a model 3 of a human language , in this embodiment the English language , and at least one language model 4 specific to an application , although in other embodiments only one of these need be present .", "label": "", "metadata": {}, "score": "102.17946"}
{"text": "The language models are generated from language texts .Therefore , the model 3 of the English language is generated from English language text 1 .The English language text 1 would usually , but need not , constitute a large corpus of English text , sourced from a wide variety of genres and language demographics .", "label": "", "metadata": {}, "score": "102.22696"}
{"text": "9 , the user interface comprises a virtual keyboard 901 , e.g. a keyboard on a softscreen .The system is configured to interpret the user input as either an input event 910 or a selection event 920 .An input event 910 is an event in which a user selects ( e.g. by ' touch ' or ' click ' ) a location in 2D space on the virtual keyboard , this event being detected by the virtual keyboard 901 and represented as Cartesian coordinates .", "label": "", "metadata": {}, "score": "102.238235"}
{"text": "The drives and their associated computer storage media discussed above and illustrated in FIG .1 , provide storage of computer readable instructions , data structures , program modules and other data for the computer 110 .In FIG .1 , for example , hard disk drive 141 is illustrated as storing operating system 144 , application programs 145 , other program modules 146 , and program data 147 .", "label": "", "metadata": {}, "score": "103.09422"}
{"text": "The system memory 130 includes computer storage media in the form of volatile and/or nonvolatile memory such as read only memory ( ROM ) 131 and random access memory ( RAM ) 132 .A basic input / output system 133 ( BIOS ) , containing the basic routines that help to transfer information between elements within computer 110 , such as during start - up , is typically stored in ROM 131 .", "label": "", "metadata": {}, "score": "103.11022"}
{"text": "Similarly , since the word \" to \" resides between the words \" Seattle \" and \" Boston \" in input text string 213 , the word \" to \" can map to either FlighPostDepatureCity or FlightPreArrivalCity .[ 0072 ] .", "label": "", "metadata": {}, "score": "103.92352"}
{"text": "[ 0154 ] .[ 0154]FIG .15 illustrates a block diagram of one illustrative speech recognition system 500 in which composite model 351 is used .Of course , since composite model 351 is implemented in the speech recognition system shown in FIG .", "label": "", "metadata": {}, "score": "104.14914"}
{"text": "The interface 60 includes a character pane 62 , which allows character entry and is analogous to the soft keyboard ( on - screen keyboard ) in existing touchscreen or virtual text entry systems .The character pane 62 covers about half of the interface 60 and displays a QWERTY - style character layout .", "label": "", "metadata": {}, "score": "104.15137"}
{"text": "A monitor 191 or other type of display device is also connected to the system bus 121 via an interface , such as a video interface 190 .In addition to the monitor , computers may also include other peripheral output devices such as speakers 197 and printer 196 , which may be connected through an output peripheral interface 190 .", "label": "", "metadata": {}, "score": "105.32262"}
{"text": "Operating system 144 , application programs 145 , other program modules 146 , and program data 147 are given different numbers here to illustrate that , at a minimum , they are different copies .[ 0050 ] .A user may enter commands and information into the computer 110 through input devices such as a keyboard 162 , a microphone 163 , and a pointing device 161 , such as a mouse , trackball or touch pad .", "label": "", "metadata": {}, "score": "107.00607"}
{"text": "For example , assume , during training that the following rules are generated , to model the following pre - terminals : . [ 0117 ] .[ 0118 ] .[ 0119 ] .Further assume that , during runtime , the sentence input is \" Show flight to Boston .", "label": "", "metadata": {}, "score": "108.81064"}
{"text": "For example , trigrams from the phrase \" see Spot run to Jane \" include \" see Spot run \" , \" Spot run to \" , and \" run to Jane \" .Note that a text element may be a word , a comma , a period , a beginning - of - sentence marker , an end - of - sentence marker , or any other grammatical or formatting element .", "label": "", "metadata": {}, "score": "109.2154"}
{"text": "[ 0180 ] \" soon \" .[0181 ] \" very \" \" soon \" .[ 0182 ] \" you \" \" very \" \" soon \" .[ 0183 ] \" see \" \" you \" \" very \" \" soon \" .", "label": "", "metadata": {}, "score": "111.21417"}
{"text": "[ 0180 ] \" soon \" .[0181 ] \" very \" \" soon \" .[ 0182 ] \" you \" \" very \" \" soon \" .[ 0183 ] \" see \" \" you \" \" very \" \" soon \" .", "label": "", "metadata": {}, "score": "111.21417"}
{"text": "ShowFlightCmd . show . me .the . flight . )Pr . show .s .ShowFlightCmd . )Pr . me .show .ShowFlightCmd . )Pr .the . me .ShowFlightCmd . )Pr . flight .", "label": "", "metadata": {}, "score": "111.943344"}
{"text": "In other embodiments , the human language model is of a language other than English .The language models are generated from language texts .Therefore , the model 3 of the English language is generated from English language text 1 .", "label": "", "metadata": {}, "score": "120.25403"}
{"text": "In other embodiments , the human language model is of a language other than English .The language models are generated from language texts .Therefore , the model 3 of the English language is generated from English language text 1 .", "label": "", "metadata": {}, "score": "120.25403"}
{"text": "[ 0063 ] .[ 0063]FIG .2D illustrates one example of an example text string 213 \" Flight from Seattle to Boston \" along with a semantic annotation 214 that corresponds to text string 213 .Semantic annotation 214 is provided by the developer and indicates the semantic meaning of string 213 .", "label": "", "metadata": {}, "score": "124.36866"}
