{"text": "Our HDP - PCFG model allows the complexity of the grammar to grow as more training data is available .In addition to presenting a fully Bayesian model for the PCFG , we also develop an efficient variational inference procedure .On synthetic data , we recover the correct grammar without having to specify its complexity in advance .", "label": "", "metadata": {}, "score": "30.64466"}
{"text": "The HDP subsumes the DPMM and multiple previous psychological models , including prototypes , exemplars , and the Rational Model of Categorization .In addition , the HDP contains a family of previously unexplored models which make interesting predictions about how information can be shared between multiple categories .", "label": "", "metadata": {}, "score": "39.997337"}
{"text": "This sharing of information can improve the speed and accuracy of learning and explained certain transfer learning effects that were observed in people 's judgments .I introduce an extension of the HDP , called the tree - HDP , which is designed to infer systems of hierarchically related categories .", "label": "", "metadata": {}, "score": "41.197"}
{"text": "In contrast , we have developed an adaptive nonparametric method for constructing smooth estimates of G0 .We combine this method with a technique for estimating # , the other Dirichlet process parameter , that is inspired by an existing characterization of its maximum - likelihood estimator .", "label": "", "metadata": {}, "score": "44.140526"}
{"text": "In contrast , we have developed an adaptive nonparametric method for constructing smooth estimates of G0 .We combine this method with a technique for estimating # , the other Dirichlet process parameter , that is inspired by an existing characterization of its maximum - likelihood estimator .", "label": "", "metadata": {}, "score": "44.140537"}
{"text": "Such a treatment is useful in situations where smooth point estimates of G0 are of intrinsic interest , or where the structure of G0 can not be conveniently modeled with the usual parametric prior families .Analysis of simulated and real - world datasets illustrates the robustness of this approach . \" ...", "label": "", "metadata": {}, "score": "44.2288"}
{"text": "Such a treatment is useful in situations where smooth point estimates of G0 are of intrinsic interest , or where the structure of G0 can not be conveniently modeled with the usual parametric prior families .Analysis of simulated and real - world datasets illustrates the robustness of this approach . \" ...", "label": "", "metadata": {}, "score": "44.22883"}
{"text": "4.2 The mIBP - HDP Model One direct application of the mIBP is to integrate it into the HDP models proposed in Section 3 .In this way , the new nonparametric extension will have the benefits of capturing uncertainty regarding thenumberofmixturecomponentsthatarecharacterizedbyapotentiallyinfinitenumberoffeatures .", "label": "", "metadata": {}, "score": "44.70321"}
{"text": "In the proposed approach , the data are distributed across P processors , and processors independently perform Gibbs sampling on their local data and communicate their information in a local asynchronous manner with other processors .We demonstrate that our asynchronous algorithms are able to learn global topic models that are statistically as accurate as those learned by the standard LDA and HDP samplers , but with significant improvements in computation time and memory .", "label": "", "metadata": {}, "score": "44.83205"}
{"text": "Finally , we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1 - 2 orders of magnitude performance gains over Hadoop - based implementations .and communication protocols while simultaneously developing mathematically complex models and algorithms .", "label": "", "metadata": {}, "score": "45.116673"}
{"text": "For evaluating the cross - document coreference annotations , we adopted the same approach as described in [ 3 ] by merging all the documents from the same topic into a meta - document and then scoring this docu- ment as performed for within - document evaluation .", "label": "", "metadata": {}, "score": "45.307877"}
{"text": "The annotation process is described in [ 7].Page 8 .The set of feature types used to achieve these results consists of combinations of types from all feature categories described in Section 2.2 .For the results of the HDPstructmodel listed in Table 2 , we also explored the conditional dependencies between the HL , FR , and FEA types .", "label": "", "metadata": {}, "score": "45.356422"}
{"text": "Bayesian nonparametrics offer a declarative framework for specifying one 's prior beliefs over the number of symbols , which could possibly unbounded .We have developed an extension of probabilistic context - free grammars using hierarchical Dirichlet processes ( HDP - PCFG )", "label": "", "metadata": {}, "score": "45.543205"}
{"text": "Our algorithm is based o .. by Arthur Asuncion , Max Welling , Padhraic Smyth , Yee Whye Teh - In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence , 2009 . \" ...Latent Dirichlet analysis , or topic modeling , is a flexible latent variable framework for modeling high - dimensional sparse count data .", "label": "", "metadata": {}, "score": "45.69373"}
{"text": "We present a phrasal synchronous gram - mar model of translational equivalence .Unlike previous approaches , we do not resort to heuristics or constraints from a word - alignment model , but instead directly induce a synchronous grammar from parallel sentence - aligned corpora .", "label": "", "metadata": {}, "score": "46.28263"}
{"text": "One model was inspired by the fully generative Bayesian model proposed by Haghighi and Klein [ 16 ] ( henceforth , H&K ) .However , to employ the H&K 's model for tasks that require clustering objects with rich linguistic features ( such as event coreferenceresolution ) , or to extend this model in order to enclose additional observable properties is a challenging task [ 22 , 25].", "label": "", "metadata": {}, "score": "46.539986"}
{"text": "Figure 1 : A graphical model depiction of the hierarchical Dirichlet process coreference resolution model .[ 1 ] . A. Haghighi and D. Klein , \" Unsupervised Coreference Resolution in a Nonparametric Bayesian Model , \" Proceedings of ACL , 2007 .", "label": "", "metadata": {}, "score": "46.574528"}
{"text": "In this paper , we present asynchronous distributed learning algorithms for two well - known unsupervised learning frameworks : Latent Dirichlet Allocation ( LDA ) and Hierarchical Dirichlet Processes ( HDP ... \" .Distributed learning is a problem of fundamental interest in machine learning and cognitive science .", "label": "", "metadata": {}, "score": "47.551815"}
{"text": "mIBP - HDP In spite of its advantage of working with a potentially infinite number of features in an HDP framework , the mIBP - HDP model did not achieve a satisfactory performance in comparison with the other proposed models .However , the results were obtained by automatically selecting only 2 % of distinct feature values from the entire set of values extracted from both corpora .", "label": "", "metadata": {}, "score": "48.147713"}
{"text": "In this paper , we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees .We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency .", "label": "", "metadata": {}, "score": "48.428543"}
{"text": "[ 1 ] .P. Liang , S. Petrov , M. I. Jordan , and D. Klein , \" The Infinite PCFG Using Hierarchical Dirichlet Processes , \" Empirical Methods in Natural Language Processing and Computational Natural Language Learning ( EMNLP / CoNLL ) , 2007 .", "label": "", "metadata": {}, "score": "48.7903"}
{"text": "As a stepping stone in the development of asynchronous HDP , a parallel HDP sampler is also introduced . ... computer vision .There are two somewhat distinct motivations for distributed computation in this context : ( 1 ) to address the memory issue when the original data and count matrices used by the algorithm exceed the ma ... . \" ...", "label": "", "metadata": {}, "score": "48.93251"}
{"text": "Fitting a topic model given a set of training documents requires approximate inference techniques that are computationally expensive .With today 's large - scal ... \" .Topic models provide a powerful tool for analyzing large text collections by representing high dimensional data in a low dimensional subspace .", "label": "", "metadata": {}, "score": "49.26843"}
{"text": "Topic models such as latent Dirichlet allocation ( LDA ) and hierarchical Dirichlet processes ( HDP ) are simple solutions to discover topics from a set of unannotated documents .While they are simple and popular , a major shortcoming of LDA and HDP is that they do not organize the top - ics into a hierarc ... \" .", "label": "", "metadata": {}, "score": "50.025475"}
{"text": "Topic models such as latent Dirichlet allocation ( LDA ) and hierarchical Dirichlet processes ( HDP ) are simple solutions to discover topics from a set of unannotated documents .While they are simple and popular , a major shortcoming of LDA and HDP is that they do not organize the top - ics into a hierarc ... \" .", "label": "", "metadata": {}, "score": "50.025475"}
{"text": "Dual - HDP advances the existing Hierarchical Dirichlet Processes ( HDP ) language model .HDP only clusters co - occurring words from documents into topics and automatically decides the number of topics .Dual - HDP co - clusters both words and documents .", "label": "", "metadata": {}, "score": "50.228233"}
{"text": "While they are simple and popular , a major shortcoming of LDA and HDP is that they do not organize the top - ics into a hierarchical structure which is naturally found in many datasets .We introduce the recursive Chinese restau - rant process ( rCRP ) and a nonparametric topic model with rCRP as a prior for discovering a hierarchical topic structure with unbounded depth and width .", "label": "", "metadata": {}, "score": "50.407997"}
{"text": "While they are simple and popular , a major shortcoming of LDA and HDP is that they do not organize the top - ics into a hierarchical structure which is naturally found in many datasets .We introduce the recursive Chinese restau - rant process ( rCRP ) and a nonparametric topic model with rCRP as a prior for discovering a hierarchical topic structure with unbounded depth and width .", "label": "", "metadata": {}, "score": "50.407997"}
{"text": "David B. Dunson , Natesh Pillai - JOURNAL OF THE ROYAL STATISTICAL SOCIETY B , 2007 . \" ...This article considers Bayesian methods for density regression , allowing a random probability distribution to change flexibly with multiple predictors .The conditional response dis - tribution is expressed as a nonparametric mixture of parametric densities , with the mixture distri - bution changing acc ... \" .", "label": "", "metadata": {}, "score": "50.706604"}
{"text": "Under our problem settings , HDP only clusters observations of objects , while Dual - HDP clusters both observations and trajectories .Experiments are evaluated on two data sets , radar tracks collected from a maritime port and visual tracks collected from a parking lot .", "label": "", "metadata": {}, "score": "50.8751"}
{"text": "The DPMM can be generalized into a larger framework of models based on the hierarchical Dirichlet process ( HDP ) .The HDP subsumes the DPMM and multiple previous psychological models , including prototypes , exemplars , and the Rational Model of Categorization .", "label": "", "metadata": {}, "score": "51.09015"}
{"text": "To help fill ... \" .While high - level data parallel frameworks , like MapReduce , simplify the design and implementation of large - scale data processing systems , they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems .", "label": "", "metadata": {}, "score": "51.18862"}
{"text": "( 2010 ) can be obtained from the characterization of the beta process as a Poisson process .Specifically , we show that the mean measure of the underlying Poisson process is equal to that of the beta process .We use ... \" .", "label": "", "metadata": {}, "score": "51.223248"}
{"text": "( 2010 ) can be obtained from the characterization of the beta process as a Poisson process .Specifically , we show that the mean measure of the underlying Poisson process is equal to that of the beta process .We use ... \" .", "label": "", "metadata": {}, "score": "51.223263"}
{"text": "In this thesis , we instead take a Bayesian nonparametric approach in defining a prior on the model parameters that allows for flexibility in the complexity of the learned model and for development of efficient inference algorithms .We start by considering dynamical phenomena that can be well - modeled as a hidden discrete Markov process , but in which there is uncertainty about the cardinality of the state space .", "label": "", "metadata": {}, "score": "51.245728"}
{"text": "For all of the presented models , we develop efficient Gibbs sampling algorithms employing a truncated approximation to the HDP that allows incorporation of dynamic programming techniques , greatly improving mixing rates .In many applications , one would like to discover and model dynamical behaviors which are shared among several related time series .", "label": "", "metadata": {}, "score": "51.30098"}
{"text": "Section 2 proposes the semiparametric latent response model and prior structure .Section 3 outlines a hybrid Gibbs sampler and Metropolis algorithm for posterior computation , and discusses inferenc ... . \" ...This paper considers modelling spatially varying regression effects for multivariate mortality count outcomes .", "label": "", "metadata": {}, "score": "51.41384"}
{"text": "Section 2 proposes the semiparametric latent response model and prior structure .Section 3 outlines a hybrid Gibbs sampler and Metropolis algorithm for posterior computation , and discusses inferenc ... . \" ...This paper considers modelling spatially varying regression effects for multivariate mortality count outcomes .", "label": "", "metadata": {}, "score": "51.413864"}
{"text": "To facilitate this extension , we assume that feature variables are conditionally independent given Z. The graphical representation corresponding to this model is illustrated in Figure 2(b ) .Drawing an analogy , the graphical representation involving feature variables and Z variables resembles the graphical repre- sentation of a Naive Bayes classifier .", "label": "", "metadata": {}, "score": "51.707798"}
{"text": "In this manuscript , we propose a novel way to extract the whole time varying distribution of the market implied asset price from option prices .We use a Bayesian nonparametric method that makes use of the Sethuraman representation for Dirichlet processes to take into account the evolution of distributions in time .", "label": "", "metadata": {}, "score": "51.85144"}
{"text": "In this manuscript , we propose a novel way to extract the whole time varying distribution of the market implied asset price from option prices .We use a Bayesian nonparametric method that makes use of the Sethuraman representation for Dirichlet processes to take into account the evolution of distributions in time .", "label": "", "metadata": {}, "score": "51.85144"}
{"text": "Journal of the American Statistical Association , 101(476):1566 - 1581 .[29 ] Jurgen Van Gael , Yunus Saatci , Yee Whye Teh , and Zoubin Ghahramani .Beam Sampling for the Infinite Hidden Markov Model .In Proceedings of ICML , pages 1088 - 1095 .", "label": "", "metadata": {}, "score": "51.966408"}
{"text": "Also , these results indicate that the iFHMM - iHMM model is a better framework than HDP in capturing the event mention dependencies simulated by the mIBP feature sampling scheme .Similar to the mIBP - HDP model , to achieve these results , the iFHMM- iHMM model uses only 2 % values from the entire set of distinct feature values .", "label": "", "metadata": {}, "score": "52.139633"}
{"text": "Factorial Hidden Markov Models .Machine Learning , 29:245 - 273 .[14 ] Zoubin Ghahramani , T. L. Griffiths , and Peter Sollich , 2007 .Bayesian Statistics 8 , chapter Bayesian nonparametric latent feature models , pages 201 - 225 .", "label": "", "metadata": {}, "score": "52.31675"}
{"text": "We also explore various combinations of features presented above .Examples include HW+POS , HL+FR , FE+A1 , etc . 3 Finite Feature Models Inthis section , we presenta sequenceof HDP mixturemodelsforsolvingeventcoreference .Forthis type of approach , a Dirichlet Process ( DP ) [ 10 ] is associated with each document , and each mixture component , which in our case corresponds to an event , is shared across documents .", "label": "", "metadata": {}, "score": "52.363487"}
{"text": "With today 's large - scale , constantly expanding document collections , it is useful to be able to infer topic distributions for new documents without retraining the model .In this paper , we empirically evaluate the performance of several methods for topic inference in previously unseen documents , including methods based on Gibbs sampling , variational inference , and a new method inspired by text classification .", "label": "", "metadata": {}, "score": "52.40029"}
{"text": "The experimental results proved that these models are able to solve real data applications in which the feature and cluster numbers are treated as free parameters , and the selection of features is per- formed automatically .While the results of event coreference resolution are promising , we believe that the classes of models proposed in this paper have a real utility for a wide range of applications .", "label": "", "metadata": {}, "score": "52.545322"}
{"text": "We demonstrate that the natural , fully synchronous parallelization of belief propagation is highly ineffici ... \" .As computer architectures move towards multicore we must build a theoretical understanding of parallelism in machine learning .In this paper we focus on parallel inference in graphical models .", "label": "", "metadata": {}, "score": "52.583393"}
{"text": "In Proceedings of ACL-2004 .[21 ] Radford M. Neal .Slice Sampling .The Annals of Statistics , 31:705 - 741 .[ 22 ] Vincent Ng .Unsupervised Models for Coreference Resolution .In Proceedings of EMNLP .[ 23 ] Martha Palmer , Daniel Gildea , and Paul Kingsbury .", "label": "", "metadata": {}, "score": "52.598053"}
{"text": "SkipCor clearly outperforms two baseline systems that detect coreferentiality using the same features as SkipCor .The obtained results are at least comparable to the current state - of - the - art in coreference resolution .Grammar Learning Using Bayesian Nonparametrics .", "label": "", "metadata": {}, "score": "52.64807"}
{"text": "We propose an automated technique that combines traceability with a machine learning technique known as topic modeling .Our approach automatically records traceability links during the software development process and learns a probabilistic topic model over artifacts .The learned model allows for the semantic categorization of artifacts and the topical visualization of the software system .", "label": "", "metadata": {}, "score": "52.693523"}
{"text": "Nonparametric Hierarchical Bayesian Models of Categorization .Kevin Canini .EECS Department University of California , Berkeley Technical Report No .UCB / EECS-2011 - 133 December 14 , 2011 .Categorization , or classification , is a fundamental problem in both cognitive psychology and machine learning .", "label": "", "metadata": {}, "score": "53.873657"}
{"text": "In particular , we exploit the finite dynamical system induced by a fixed set of behaviors to efficiently compute acceptance probabilities , and reversible jump birth and death proposals to explore new behaviors .We present results on unsupervised segmentation of data from the CMU motion capture database .", "label": "", "metadata": {}, "score": "54.09201"}
{"text": "This article considers Bayesian methods for density regression , allowing a random probability distribution to change flexibly with multiple predictors .The conditional response dis - tribution is expressed as a nonparametric mixture of parametric densities , with the mixture distri - bution changing acc ... \" .", "label": "", "metadata": {}, "score": "54.185505"}
{"text": "Through the use of the hierarchical Dirichlet process ( HDP ) , one can examine an HMM with an unbounded number of possible states .We revisit this HDPHMM and develop a generalization of the model , the sticky HDP - HMM , that allows more robust learning of smoothly varying state dynamics through a learned bias towards self - transitions .", "label": "", "metadata": {}, "score": "54.216904"}
{"text": "Latent Dirichlet analysis , or topic modeling , is a flexible latent variable framework for modeling high - dimensional sparse count data .Various learning algorithms have been developed in recent years , including collapsed Gibbs sampling , variational inference , and maximum a posteriori estimation , and this variety motivates the need for careful empirical comparisons .", "label": "", "metadata": {}, "score": "54.41907"}
{"text": "We study the performance of online LDA in several ways , including by fitting a 100-topic topic model to 3.3 M articles from Wikipedia in a single pass .We demonstrate that online LDA finds topic models as good or better than those found with batch VB , and in a fraction of the time . ... ments . to summarize the latent structure of massive document collections that can not be annotated by hand .", "label": "", "metadata": {}, "score": "54.67413"}
{"text": "In addition to these inference methods , we present SparseLDA , an algorithm and data structure for evaluating Gibbs sampling distributions .Empirical results indicate that SparseLDA can be approximately 20 times faster than traditional LDA and provide twice the speedup of previously published fast sampling methods , while also using substantially less memory . .", "label": "", "metadata": {}, "score": "54.963882"}
{"text": "Categories and Subject Descriptors . ...l show timing results when applying CVB0 to software artifacts .We take advantage of the efficiency of CVB0 inference in our traceability tools .3.2 Applications of LDA Once an LDA model is learned on a corpus , one can use the learned probability distribution ov ... . \" ...", "label": "", "metadata": {}, "score": "55.06379"}
{"text": "( 2010 ) can be obtained from the characterization of the beta process as a Poisson process .Specifically , we show that the mean measure of the underlying Poisson process is equal to that of the beta process .We use this underlying representation to derive error bounds on truncated beta processes that are tighter than those in the literature .", "label": "", "metadata": {}, "score": "55.194237"}
{"text": "( 2010 ) can be obtained from the characterization of the beta process as a Poisson process .Specifically , we show that the mean measure of the underlying Poisson process is equal to that of the beta process .We use this underlying representation to derive error bounds on truncated beta processes that are tighter than those in the literature .", "label": "", "metadata": {}, "score": "55.19426"}
{"text": "While most other categorization models learn each individual category in isolation and independently of the others , these HDP models share information between categories .This sharing of information can improve the speed and accuracy of learning and explained certain transfer learning effects that were observed in people 's judgments .", "label": "", "metadata": {}, "score": "55.21563"}
{"text": "Instead , one can seek to understand categorization by viewing it as a problem of statistical inference and attempting to characterize the inductive biases of human learners .These inductive biases can be directly exposed using an experimental method called iterated learning , which provides direct insight into human categorization in a way that is independent of any proposed models .", "label": "", "metadata": {}, "score": "55.407143"}
{"text": "[ 15 ] Tom Griffiths and Zoubin Ghahramani .Infinite Latent Feature Models and the Indian Buffet Process .In Proceedings of NIPS , pages 475 - 482 .[ 16 ] Aria Haghighi and Dan Klein .Unsupervised Coreference Resolution in a Nonparametric Bayesian Model .", "label": "", "metadata": {}, "score": "55.79467"}
{"text": "To describe the beam sampler for event coreference resolution , we introduce additional notation .The observation parameters \u03c6 are iid drawn from a prior base distribution H. The beam sampling algorithm combines the ideas of slice sampling and dynamic programming for an efficient sampling of state trajectories .", "label": "", "metadata": {}, "score": "55.945442"}
{"text": "By bounding the achievable parallel performance in chain graphical models we develop a theoretical understanding of the parallel limitations of belief propagation .We then provide a new parallel belief propagation algorithm which achieves optimal performance .Using two challenging real - world tasks , we empirically evaluate the performance of our algorithm on large cyclic graphical models where we achieve near linear parallel scaling and out perform alternative algorithms . \" ...", "label": "", "metadata": {}, "score": "56.4535"}
{"text": "Event coreference was also tried by using the semantictypesofanontology[17].However , thefeaturesusedbytheseapproachesarehardtoselect and require the design of domain specific constraints .To address this problems , we have explored a sequence of unsupervised , nonparametric Bayesian models that are used to probabilistically infer coreference clusters of event mentions from a collection of unlabeled documents .", "label": "", "metadata": {}, "score": "56.563786"}
{"text": "A future research area for improving this model is to consider other distributions for automatic selection of salient feature values .iFHMM - iHMMInspiteoftheautomaticfeatureselectionemployedfortheiFHMM - iHMMmodel , its results remain competitive against the results of the HDP extensions ( where the feature types were hand tuned ) .", "label": "", "metadata": {}, "score": "56.695026"}
{"text": "This paper considers modelling spatially varying regression effects for multivariate mortality count outcomes .Alternative approaches to spatial regression heterogeneity are considered : the multivariate normal conditional autoregressive ( MCAR ) model is contrasted with a flexible set of priors based on the multiple membership approach .", "label": "", "metadata": {}, "score": "57.01046"}
{"text": "This paper considers modelling spatially varying regression effects for multivariate mortality count outcomes .Alternative approaches to spatial regression heterogeneity are considered : the multivariate normal conditional autoregressive ( MCAR ) model is contrasted with a flexible set of priors based on the multiple membership approach .", "label": "", "metadata": {}, "score": "57.010483"}
{"text": "In Proceedings of ACL / HLT-2008 , pages 45 - 48 .[ 12 ] Stuart Geman and Donald Geman .Stochastic relaxation , Gibbs distributions and the Bayesian restoration of images .IEEE Transactions on Pattern Analysis and Machine Intelligence , 6:721 - 741 .", "label": "", "metadata": {}, "score": "57.17318"}
{"text": "This specifica - tion results in a coherent prior for the joint measure , with the marginal random measure at each location being a finite mixture of DP basis measures .Integrating out the infinite - dimensional col - lection of mixing measures , we obtain a simple expression for the conditional distribution of the subject - specific random variables , which generalizes the P\u00f3lya urn scheme .", "label": "", "metadata": {}, "score": "57.17887"}
{"text": "This specifica - tion results in a coherent prior for the joint measure , with the marginal random measure at each location being a finite mixture of DP basis measures .Integrating out the infinite - dimensional col - lection of mixing measures , we obtain a simple expression for the conditional distribution of the subject - specific random variables , which generalizes the P\u00f3lya urn scheme .", "label": "", "metadata": {}, "score": "57.17887"}
{"text": "Additionally , the sticky HDP - HMM enables learning more complex , multimodal emission distributions .( cont . )We demonstrate the utility of the sticky HDP - HMM on the NIST speaker diarization database , segmenting audio files into speaker labels while simultaneously identifying the number of speakers present .", "label": "", "metadata": {}, "score": "57.5689"}
{"text": "2 Models for solving event coreference and event identity can lead to the generation of ad - hoc event hierarchies from text .A sample of a hierarchy capturing corefering and identical events , including those from the example presented in Section 1 , is illustrated in Figure 1 .", "label": "", "metadata": {}, "score": "57.685795"}
{"text": "References [ 1 ] David Ahn .The stages of event extraction .In Proceedings of the Workshop on Annotating and Reasoning about Time and Events , pages 1 - 8 .[ 2 ] Amit Bagga and Breck Baldwin .Algorithms for Scoring Coreference Chains .", "label": "", "metadata": {}, "score": "57.814587"}
{"text": "The Infinite Factorial Hidden Markov Model .In Proceedings of NIPS .[ 31 ] Marc Vilain , John Burger , John Aberdeen , Dennis Connolly , and Lynette Hirschman .A Model-Theoretic Coreference Scoring Scheme .In Proceedings of MUC-6 , pages 45 - 52 .", "label": "", "metadata": {}, "score": "57.828587"}
{"text": "We develop an online variational Bayes ( VB ) algorithm for Latent Dirichlet Allocation ( LDA ) .Online LDA is based on online stochastic optimization with a natural gradient step , which we show converges to a local optimum of the VB objective function .", "label": "", "metadata": {}, "score": "57.949577"}
{"text": "We develop an online variational Bayes ( VB ) algorithm for Latent Dirichlet Allocation ( LDA ) .Online LDA is based on online stochastic optimization with a natural gradient step , which we show converges to a local optimum of the VB objective function .", "label": "", "metadata": {}, "score": "57.949577"}
{"text": "4.1The Markov Indian Buffet Process Asdescribedin[30],themIBPdefinesadistributionoveranunboundedsetofbinaryMarkovchains , where each chain can be associated to a binary latent feature that evolves over time according to Markov dynamics .F1 t , F2 t, ... ,FM t ?Therefore , F decomposes the observations and represents them as feature factors , which can then be associated to hidden variables in an iFHMM as depicted in Figure 3(a ) .", "label": "", "metadata": {}, "score": "58.363136"}
{"text": "[ 7 ] CosminAdrian Bejan and Sanda Harabagiu .A Linguistic Resource for Discovering Event Structures and Resolving Event Coreference .In Proceedings of LREC-2008 .[ 8 ] CosminAdrian Bejan and ChrisHathaway .UTD - SRL :A Pipeline Architecturefor ExtractingFrame Semantic Structures .", "label": "", "metadata": {}, "score": "58.372593"}
{"text": "Treating categorization as a type of statistical inference , I describe a family of nonparametric Bayesian models of categorization based on the Dirichlet process mixture model ( DPMM ) .These models represent categories as combinations of clusters of objects and , together , produce a continuum of representational complexities where prototype and exemplar models are special cases , occupying opposite ends of the spectrum .", "label": "", "metadata": {}, "score": "58.421417"}
{"text": "( cont . )In the latter part of this thesis , we consider a Bayesian nonparametric approach to this problem by harnessing the beta process to allow each time series to have infinitely many potential behaviors , while encouraging sharing of behaviors amongst the time series .", "label": "", "metadata": {}, "score": "58.534267"}
{"text": "The methods are illustrated using simulated data examples and epidemiologic studies . ... \" ...The Infinite Hidden Markov Model ( IHMM ) extends hidden Markov models to have a countably infinite number of hidden states ( Beal et al . , 2002 ; Teh et al . , 2006 ) .", "label": "", "metadata": {}, "score": "58.61106"}
{"text": "The methods are illustrated using simulated data examples and epidemiologic studies . ... \" ...The Infinite Hidden Markov Model ( IHMM ) extends hidden Markov models to have a countably infinite number of hidden states ( Beal et al . , 2002 ; Teh et al . , 2006 ) .", "label": "", "metadata": {}, "score": "58.61106"}
{"text": "In particular we focus our attention on marginal samplers , which exploit the P\u00f2lya UrnMEASURING EXPECTATIONS IN OPTIONS MARKETS : AN APPLICATION ... .by Jon Mcauliffe David , David M. Blei , Michael I. Jordan - Statistics and Computing , 2004 . \" ...", "label": "", "metadata": {}, "score": "58.83863"}
{"text": "In particular we focus our attention on marginal samplers , which exploit the P\u00f2lya UrnMEASURING EXPECTATIONS IN OPTIONS MARKETS : AN APPLICATION ... .by Jon Mcauliffe David , David M. Blei , Michael I. Jordan - Statistics and Computing , 2004 . \" ...", "label": "", "metadata": {}, "score": "58.83863"}
{"text": "In Proceedings of COLING - ACL .[5 ] Matthew J. Beal , Zoubin Ghahramani , and Carl Edward Rasmussen .The Infinite Hidden Markov Model .In Proceedings of NIPS .[ 6 ] Cosmin Adrian Bejan .Deriving Chronological Information from Texts through a Graph - based Algorithm .", "label": "", "metadata": {}, "score": "59.611343"}
{"text": "We demonstrate the benefits of our two sampling algorithms on large synthetic and real - world models using a 32 processor multi - core system . ... stics and machine learning .On many models , however , the Gibbs sampler can be slow mixing [ Kuss and Rasmussen , 2005 , Barbu and Zhu , 2005].", "label": "", "metadata": {}, "score": "59.689045"}
{"text": "For sampling the whole hidden state trajectory s , this algorithm employs a forward filtering - backward sampling technique .t , F2 t, ... ,FM t ?Page 7 . 5 Evaluation Event Coreference Data One corpus used for evaluation is ACE 2005 [ 18].", "label": "", "metadata": {}, "score": "60.258636"}
{"text": "We also apply the Lidstone 's smoothing method to this distribution .3.2Adding More Features A model in which observable components are represented only by one feature has the tendency to cluster these components based on their feature value .To address this limitation , H&K proposed a more complex model that is strictly customized for entity coreference resolution .", "label": "", "metadata": {}, "score": "60.408806"}
{"text": "During the process of learning the model described in Section 3 , it was observedthat a large amount of time was requiredto incorporateand tune new features .This lead us to the challengeof creating a framework which considers an unbounded number of features where the most relevant are selected automatically .", "label": "", "metadata": {}, "score": "61.212708"}
{"text": "[ 9 ] Christiane Fellbaum .WordNet : An Electronic Lexical Database .MIT Press .[ 10 ] Thomas S. Ferguson .A Bayesian Analysis of Some Nonparametric Problems .The Annals of Statistics , 1(2):209 - 230 .[11 ] Jenny Rose Finkel and Christopher D. Manning .", "label": "", "metadata": {}, "score": "61.235504"}
{"text": "We present an application of this model to artificial data , a video gesture classification task , and a musical theme labeling task , and show that components of the model can also be applied to graph segmentation .With these steps , the posterior for a given vt hidden state assignment invokes a truncated analog to the familiar Chinese Restaurant process for Dirichlet process inference twice , once to account f .. by Joon Hee Kim , Dongwoo Kim , Suin Kim , Alice Oh - In Proceedings of the 21st ACM international conference on Information and knowledge management , 783 - 792 .", "label": "", "metadata": {}, "score": "61.3059"}
{"text": "We present an application of this model to artificial data , a video gesture classification task , and a musical theme labeling task , and show that components of the model can also be applied to graph segmentation .With these steps , the posterior for a given vt hidden state assignment invokes a truncated analog to the familiar Chinese Restaurant process for Dirichlet process inference twice , once to account f .. by Joon Hee Kim , Dongwoo Kim , Suin Kim , Alice Oh - In Proceedings of the 21st ACM international conference on Information and knowledge management , 783 - 792 .", "label": "", "metadata": {}, "score": "61.3059"}
{"text": "The first incorporates a Markov Indian Buffet Process ( mIBP ) [ 30 ] into a Hierarchical Dirichlet Process ( HDP ) [ 28].The second uses an Infinite Hidden Markov Model ( iHMM ) [ 5 ] coupled to an Infinite Factorial Hidden Markov Model ( iFHMM ) [ 30].", "label": "", "metadata": {}, "score": "61.33944"}
{"text": "In Tab ... . by Joseph E. Gonzalez , Yucheng Low , Carlos Guestrin - In In Artificial Intelligence and Statistics ( AISTATS , 2009 . \" ...As computer architectures move towards multicore we must build a theoretical understanding of parallelism in machine learning .", "label": "", "metadata": {}, "score": "61.594574"}
{"text": "We evaluated these models for the task of within- and cross - document event coreference on two corpora .All the models we investigated show significant improvements when c ompared against an existing baseline for this task .Full - text .", "label": "", "metadata": {}, "score": "61.630325"}
{"text": "Inference is per - formed using a novel Gibbs sampler over synchronous derivations .This sam - pler side - steps the intractability issues of previous models which required inference over derivation forests .Instead each sam - pling iteration is highly efficient , allowing the model to be applied to larger transla - tion corpora than previous approaches . ... endencies between all the sentences in the training corpus .", "label": "", "metadata": {}, "score": "61.797653"}
{"text": "Page 5 .For all these extended models , we compute the prior and likelihood factors as described in the one feature model .Also , following H&K , in the inference mechanism we assign soft counts for missing features ( e.g. , unspecified PB argument ) .", "label": "", "metadata": {}, "score": "61.87166"}
{"text": "The conditional response dis - tribution is expressed as a nonparametric mixture of parametric densities , with the mixture distri - bution changing according to location in the predictor space .A new class of priors for dependent random measures is proposed for the collection of random mixing measures at each location .", "label": "", "metadata": {}, "score": "61.934452"}
{"text": "The conditional response dis - tribution is expressed as a nonparametric mixture of parametric densities , with the mixture distri - bution changing according to location in the predictor space .A new class of priors for dependent random measures is proposed for the collection of random mixing measures at each location .", "label": "", "metadata": {}, "score": "61.934452"}
{"text": "We find that the main differences are attributable to the amount of smoothing applied to the counts .When the hyperparameters are optimized , the differences in performance among the algorithms diminish significantly .The ability of these algorithms to achieve solutions of comparable accuracy gives us the freedom to select computationally efficient approaches .", "label": "", "metadata": {}, "score": "62.057804"}
{"text": "[ 3 ] Amit Bagga and Breck Baldwin .Cross - Document Event Coreference : Annotations , Experiments , and Observations .In Proceedings of the ACL-99 Workshop on Coreference and its Applications .[ 4 ] Collin F. Baker , Charles J. Fillmore , and John B. Lowe .", "label": "", "metadata": {}, "score": "62.44369"}
{"text": "These inductive biases can be directly exposed using an experimental method called iterated learning , which provides direct insight into human categorization in a way that is independent of any proposed models .I describe the results of an iterated learning study of human categorization which supports previous findings by psychologists that people 's representations seem to be more flexible than would be implied by either prototype or exemplar models alone .", "label": "", "metadata": {}, "score": "62.561787"}
{"text": "This assumption is often insufficient for capturing the temporal dependencies of the observations in real data .To address this issue , we develop extensions of the sticky HDP - HMM for learning two classes of switching dynamical processes : the switching linear dynamical system ( SLDS ) and the switching vector autoregressive ( SVAR ) process .", "label": "", "metadata": {}, "score": "62.81636"}
{"text": "EndNote citation : .Bayesian nonparametric learning of complex dynamical phenomena .Other Contributors : Massachusetts Institute of Technology .Dept . of Electrical Engineering and Computer Science .Advisor : Alan S. Willsky and John W. Fisher , III .Department : Massachusetts Institute of Technology .", "label": "", "metadata": {}, "score": "62.89933"}
{"text": "HL ? ) \u03c6hl We assume that this emission distribution is drawn from a symmetric Dirichlet distribution with concentration \u03bbHL : Z ? 3A0 annotates in PB a specific type of semantic role which represents the AGENT , the DOER , or the ACTOR of a specific event .", "label": "", "metadata": {}, "score": "63.6302"}
{"text": "In this paper we present two general types of Gibbs samplers that can be used to fit posteriors of Bayesian hierarchical models based on stick - breaking priors .The first type of Gibbs sampler , referred to as a Polya urn Gibbs sampler , is a generalized version of a widely used Gibbs sampling method currently employed for Dirichlet process computing .", "label": "", "metadata": {}, "score": "63.662086"}
{"text": "In this paper we present two general types of Gibbs samplers that can be used to fit posteriors of Bayesian hierarchical models based on stick - breaking priors .The first type of Gibbs sampler , referred to as a Polya urn Gibbs sampler , is a generalized version of a widely used Gibbs sampling meth ... \" . ...", "label": "", "metadata": {}, "score": "63.69729"}
{"text": "The Infinite Hidden Markov Model ( IHMM ) extends hidden Markov models to have a countably infinite number of hidden states ( Beal et al . , 2002 ; Teh et al . , 2006 ) .We present a generalization of this framework that introduces nearly block - diagonal structure in the transitions between the hidden states , where blocks correspond to \" subbehaviors \" exhibited by data sequences .", "label": "", "metadata": {}, "score": "64.03891"}
{"text": "The Infinite Hidden Markov Model ( IHMM ) extends hidden Markov models to have a countably infinite number of hidden states ( Beal et al . , 2002 ; Teh et al . , 2006 ) .We present a generalization of this framework that introduces nearly block - diagonal structure in the transitions between the hidden states , where blocks correspond to \" subbehaviors \" exhibited by data sequences .", "label": "", "metadata": {}, "score": "64.03891"}
{"text": "In Corpus Linguistics , pages 647 - 656 .[ 27 ] Lawrence R. Rabiner .A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition .In Proceedings of the IEEE , pages 257 - 286 .[28 ] Yee Whye Teh , Michael Jordan , Matthew Beal , and David Blei .", "label": "", "metadata": {}, "score": "64.342705"}
{"text": "5In this section , a feature is represented by a ( feature type : feature value ) pair .6Technical details for computing this probability are described in [ 30].Page 6 .( b )The iFHMM - iHMM model .", "label": "", "metadata": {}, "score": "64.70013"}
{"text": "Prototype and exemplar models both use a single , fixed level of complexity in their representations of categories , with prototype models exhibiting the simplest representations , and exemplar models using the most complex representations .Treating categorization as a type of statistical inference , I describe a family of nonparametric Bayesian models of categorization based on the Dirichlet process mixture model ( DPMM ) .", "label": "", "metadata": {}, "score": "64.891556"}
{"text": "In this work , we propose two methods to construct parallel Gibbs samplers guaranteed to draw from the targeted distribution .The first method , called the Chromatic sampler , uses graph coloring to construct a direct parallelization of the classic sequential scan Gibbs sampler .", "label": "", "metadata": {}, "score": "64.994736"}
{"text": "In grammar learning applications such as inducing a grammar from unannotated sentences or refining an existing grammar , the question of how to select the complexity of the grammar , i.e. , how many symbols to allocate , is an important problem .", "label": "", "metadata": {}, "score": "65.440926"}
{"text": "Computational Linguistics , 31(1):71 - 105 .[ 24 ] Ron Papka .On - line New Event Detection , Clustering and Tracking .Ph.D. thesis , Department of Computer Science , University of Massachusetts .[ 25 ] Hoifung Poon and Pedro Domingos .", "label": "", "metadata": {}, "score": "65.548454"}
{"text": "We apply rCRP to a corpus of New York Times articles , a dataset of MovieLens ratings , and a set of Wikipedia articles and show the discovered topic hierarchies .We compare the predictive power of rCRP with LDA , HDP , and nested Chinese restaurant process ( nCRP ) using held - out likelihood to show that rCRP outperforms the others .", "label": "", "metadata": {}, "score": "65.99155"}
{"text": "We apply rCRP to a corpus of New York Times articles , a dataset of MovieLens ratings , and a set of Wikipedia articles and show the discovered topic hierarchies .We compare the predictive power of rCRP with LDA , HDP , and nested Chinese restaurant process ( nCRP ) using held - out likelihood to show that rCRP outperforms the others .", "label": "", "metadata": {}, "score": "65.99155"}
{"text": "Our second method , the Splash sampler , is a complementary strategy which can be used when the variables are tightly coupled .This constructs and samples multiple blocks in parallel , using a novel locking protocol and an iterative junction tree generation algorithm .", "label": "", "metadata": {}, "score": "66.36039"}
{"text": "The first type of Gibbs sampler , referred to as a Polya urn Gibbs sampler , is a generalized version of a widely used Gibbs sampling method currently employed for Dirichlet process computing .This method applies to stick - breaking priors with a known P'olya urn characterization ; that is priors with an explicit and simple prediction rule .", "label": "", "metadata": {}, "score": "66.529236"}
{"text": "We evaluated these models for the task of within- and cross - document event coreference on two corpora .All the modelswe investigatedshow significantimprovementswhen comparedagainst an existing baseline for this task . 1In Natural Language Processing ( NLP ) , the task of event coreference has numerous applications , including question answering , multi - document summarization , and information extraction .", "label": "", "metadata": {}, "score": "66.76907"}
{"text": "The iFHMM - iHMM Model The iFHMM is a nonparametric Bayesian factor model that extends the Factorial Hidden Markov Model ( FHMM ) [ 13 ] by letting the number of parallel Markov chains M be learned from data .Although the iFHMM allows a more flexible representation of the latent structure , it can not be used as a framework where the number of clustering components K is infinite .", "label": "", "metadata": {}, "score": "66.88739"}
{"text": "Recent work in parallel Gibbs sampling has focused on update schedules which do not guarantee convergence to the intended stationary distribution .In this work , we prop ... \" .We explore the task of constructing a parallel Gibbs sampler , to both improve mixing and the exploration of high likelihood states .", "label": "", "metadata": {}, "score": "66.92362"}
{"text": "The results show that rCRP discovers a hierarchy in which the topics become more specialized to - ward the leaves , and topics in the immediate family exhibit more affinity than topics beyond the immediate family . . ..In this work , we employ the Po\u0301lya urn scheme by incorporating the CRP metaphor for approximate in ... . \" ...", "label": "", "metadata": {}, "score": "67.47896"}
{"text": "The results show that rCRP discovers a hierarchy in which the topics become more specialized to - ward the leaves , and topics in the immediate family exhibit more affinity than topics beyond the immediate family . . ..In this work , we employ the Po\u0301lya urn scheme by incorporating the CRP metaphor for approximate in ... . \" ...", "label": "", "metadata": {}, "score": "67.47896"}
{"text": "by Phil Blunsom , Chris Dyer , Trevor Cohn , Miles Osborne - In Proceedings of the Association for Computational Linguistics , 2009 . \" ...We present a phrasal synchronous gram - mar model of translational equivalence .Unlike previous approaches , we do not resort to heuristics or constraints from a word - alignment model , but instead directly induce a synchronous grammar from parallel sentence - aligned corpora .", "label": "", "metadata": {}, "score": "67.76817"}
{"text": "To extract features that characterize participants and properties of event mentions , we use s semantic parser [ 8 ] trained on PropBank(PB ) [ 23 ] and FrameNet(FN ) [ 4 ] cor- pora .( For instance , for the apprehended mention from our example , Jackson is the feature value 1For consistency , we try to preserve the notation of the original models .", "label": "", "metadata": {}, "score": "68.192245"}
{"text": "The global distribution drawnfrom this DP prior , denotedas \u03b20 in Figure 2(a ) , encodes the event mixing weights .Thus , same global events are used for each document , but each event has a document specific distribution \u03b2ithat is drawn from a DP prior centered on \u03b20 .", "label": "", "metadata": {}, "score": "68.31398"}
{"text": "In this model , which is depicted graphically in Figure 2(a ) , the observable components are characterized by only one feature .Since this setting enables a clustering of event mentions at the document level , it is desirable that events are shared across documents and the number of events K is inferred from data .", "label": "", "metadata": {}, "score": "68.717735"}
{"text": "Many categorization studies in psychology attempt to understand how people solve this problem by comparing their inferences to those of formal computational models such as prototype or exemplar models .From this perspective , different models make different predictions about the representations and mechanisms people use to make categorization judgments .", "label": "", "metadata": {}, "score": "69.23853"}
{"text": "The need for automated traceability increases as projects become more complex and as the number of artifacts increases .We propose an automated technique that combines traceability with a machine learning technique know ... \" .Software traceability is a fundamentally important task in software engineering .", "label": "", "metadata": {}, "score": "70.00171"}
{"text": "Even with fast sampling methods , training topic models remains computationally expensive .We would like to be able to train Evaluation measures 0.6 0.7 0.8 0.9 1.0 Accuracy over Training size on Pubmed 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Training proportion Cosine distance KL divergence F1 Figur ... . by Yucheng Low , Danny Bickson , Joseph Gonzalez , Carlos Guestrin , Aapo Kyrola , Joseph M. Hellerstein . \" ...", "label": "", "metadata": {}, "score": "70.313416"}
{"text": "[17 ] Kevin Humphreys , Robert Gaizauskas , Saliha Azzam .Event Coreference for Information Extrac- tion .In Proceedings of the Workshop on Operational Factors in Practical , Robust Anaphora Resolution for Unrestricted Texts , 35th Meeting of ACL , pages 75 - 81 .", "label": "", "metadata": {}, "score": "70.393555"}
{"text": "Beside entity extraction and relation extraction , it represents one of the three complementary tasks in Information Extraction .In this paper we describe a novel coreference resolution system SkipCor that reformulates the problem as a sequence labeling task .None of the existing supervised , unsupervised , pairwise or sequence - based models are similar to our approach , which only uses linear - chain conditional random fields and supports high scalability with fast model training and inference , and a straightforward parallelization .", "label": "", "metadata": {}, "score": "70.47039"}
{"text": "Conference : Advances in Neural Information Processing Systems 22 : 23rd Annual Conference on Neural Information Processing Systems 2009 .Proceedings of a meeting held 7 - 10 December 2009 , Vancouver , British Columbia , Canada .We present a sequence of unsupervised , nonparametric Bayesian models for clus- tering complex linguistic objects .", "label": "", "metadata": {}, "score": "70.63197"}
{"text": "WordNet Features ( WF )We build three types of clusters over all the words from WordNet [ 9 ] and use them as features for the mention HW .First cluster type associates an unique i d to each ( word : HWC ) pair ( WNW ) .", "label": "", "metadata": {}, "score": "70.83836"}
{"text": "In our approach , trajectories are treated as documents and observations of an object on a trajectory are treated as words in a document .Trajectories are clustered into different activities .Abnormal trajectories are detected as samples with low likelihoods .", "label": "", "metadata": {}, "score": "70.86867"}
{"text": "ACE ( Automatic Content Extraction ) English Annotation Guidelines for Events .[19 ] X. Luo .On Coreference Resolution Performance Metrics .In Proceedings of EMNLP .[20 ] X. Luo , A. Ittycheriah , H. Jing , N. Kambhatla , and S.Roukos 2004 .", "label": "", "metadata": {}, "score": "70.90658"}
{"text": "The number of mixture components is not specified in advance and can grow as new data come in .However , the behavior of the model is sensitive to the choice of the parameters , including an infinite - dimensional distributional parameter G0 .", "label": "", "metadata": {}, "score": "70.94394"}
{"text": "The number of mixture components is not specified in advance and can grow as new data come in .However , the behavior of the model is sensitive to the choice of the parameters , including an infinite - dimensional distributional parameter G0 .", "label": "", "metadata": {}, "score": "70.943954"}
{"text": "The Baseline A simple baseline for event coreference consists in grouping events by their event classes [ 1].To extract event classes , we employed the event identifier described in [ 6].Therefore , this baseline will categorize events into a small number of clusters , since the event identifier is trained to predict the five event classes annotated in TimeBank [ 26].", "label": "", "metadata": {}, "score": "71.183304"}
{"text": "Thesis ( Ph .D.)--Massachusetts Institute of Technology , Dept . of Electrical Engineering and Computer Science , 2009 .Cataloged from PDF version of thesis .Includes bibliographical references ( p. 257- 270 ) .Trajectory Analysis and Semantic Region Modeling Using A Nonparametric Bayesian Model .", "label": "", "metadata": {}, "score": "71.40184"}
{"text": "Classical psychological models of categorization fall into two main groups : prototype models and exemplar models , which are equivalent , respectively , to the statistical methods of parametric density estimation and kernel density estimation .Many categorization studies in psychology attempt to understand how people solve this problem by comparing their inferences to those of formal computational models such as prototype or exemplar models .", "label": "", "metadata": {}, "score": "71.49231"}
{"text": "In this regard , we ran the event identifier described in [ 6 ] on the ACE and ECB corpora , and extracted 45289 and 21175 system mentions respectively .The Experimental Setup Table 2 lists the recall ( R ) , precision ( P ) , and F - score ( F ) of our exper- iments averaged over 5 runs of the generative models .", "label": "", "metadata": {}, "score": "72.778915"}
{"text": "The number of mixture components is not specified in advance and can grow as new data come in .However , the behavior of the model is sensitive to the choice of the parameters , including an infinite - dimensional distribution ... \" .", "label": "", "metadata": {}, "score": "73.17081"}
{"text": "The number of mixture components is not specified in advance and can grow as new data come in .However , the behavior of the model is sensitive to the choice of the parameters , including an infinite - dimensional distribution ... \" .", "label": "", "metadata": {}, "score": "73.17081"}
{"text": "Our second method , the blocked Gibbs sampler , is based on a entirely different approach that works by directly sampling values from the posterior of the random measure .The blocked Gibbs sampler can be viewed as a more general approach as it works without requiring an explicit prediction rule .", "label": "", "metadata": {}, "score": "73.4903"}
{"text": "For instance , a baseline that groups all entity mentions into the same entity achievesthehighestMUC scorethananypublishedsystemforthetaskofentitycoreference .Similar behaviour of the MUC metric is observed for event coreference resolution .For example , for cross- document evaluation on ECB , a baseline that clusters all mentions into one event achieves 73.2 % MUC F - score , while the baseline listed in Table 2 achieves 72.9 % MUC F - score .", "label": "", "metadata": {}, "score": "74.28726"}
{"text": "Finally , the third cluster type considers as grouping criteria the category from WordNet lexicographer 's files that is associated to each word ( WNL ) .For cases when a new word does not belong to any of these WordNet clusters , we create a new cluster with a new i d for each of the three cluster types .", "label": "", "metadata": {}, "score": "74.71384"}
{"text": "After an initial processing phase , we extracted from ACE 6553 event mentions and 4946 events .To increase the diversity of events and to evaluate the models for both within- and cross- document event coreference , we created the EventCorefBank corpus ( ECB).8This new corpus con- tains 43 topics , 1744eventmentions,1302within - documentevents , and339cross - documentevents .", "label": "", "metadata": {}, "score": "75.03244"}
{"text": "In Proceedings of EMNLP .[26 ] J. Pustejovsky , P. Hanks , R. Sauri , A. See , R. Gaizauskas , A. Setzer , D. Radev , B. Sundheim , D. Day , L. Ferro , and M. Lazo .", "label": "", "metadata": {}, "score": "75.53401"}
{"text": "Page 3 . for A0 PB argument3and the SUSPECT frame element ( FEA0 ) of the ARREST frame . )Another se- mantic feature is the semantic frame ( FR ) that is evoked by an event mention .( For instance , all the emphasized mentions from our example evoke the ARREST frame from FN . )", "label": "", "metadata": {}, "score": "76.21292"}
{"text": "Publisher : Massachusetts Institute of Technology .The complexity of many dynamical phenomena precludes the use of linear models for which exact analytic techniques are available .However , inference on standard nonlinear models quickly becomes intractable .In some cases , Markov switching processes , with switches between a set of simpler models , are employed to describe the observed dynamics .", "label": "", "metadata": {}, "score": "78.04303"}
{"text": "Moreover , two event mentions are identical if they have the same causes and effects .For example , the threedocumentslisted in Table1 containsfourmentionsofidenticaleventsbutonlythe arrested , apprehended , and arrest mentions from the documents 1 and 2 are coreferential .These definitions were used in the tasks of Topic Detection and Tracking ( TDT ) , as reported in [ 24].", "label": "", "metadata": {}, "score": "79.42886"}
{"text": "A case study considers varying regression effects for a bivariate suicide outcome , namely male and female suicides in 354 English local authorities with social deprivation , social fragmentation and rurality as predictors .Tools . \" ... ...In this paper we present two general types of Gibbs samplers that can be used to fit posteriors of Bayesian hierarchical models based on stick - breaking priors .", "label": "", "metadata": {}, "score": "80.7303"}
{"text": "The blocked Gibbs sampler can be viewed as a more general approach as it works without requiring an explicit prediction rule .We find that the blocked Gibbs avoids some of the limitations seen with the Polya urn approach and should be simpler for non - experts to use . by", "label": "", "metadata": {}, "score": "82.18636"}
{"text": "2.1 Notation As input for our models , we consider a collection of I documents , each document i having Jievent mentions .Each event mention is characterized by L feature types , FT , and each feature type is represented by a finite number of feature values , fv .", "label": "", "metadata": {}, "score": "84.57057"}
{"text": "( FT1:fv1i), ... ,(FTL : fvLi ) ? , where each feature value index i ranges in the feature value space associated with a feature type .Class Features ( CF )These features aim to classify mentions into several types of classes : the mention HW 's part - of - speech ( POS ) , the word class of the HW ( HWC ) , which can take one of the following values ? verb , noun , adjective , other ? , and the event class of the mention ( EC ) .", "label": "", "metadata": {}, "score": "86.26464"}
{"text": "Coreference resolution is the task of partitioning the noun phrase mentions according to their underlying referent .We approach this problem as a clustering problem where the number of entities , or clusters , are not known in advance .We assume there is a global distribution over entities in a document corpus and that each document draws its own distribution over entities mentioned in the document .", "label": "", "metadata": {}, "score": "86.761185"}
{"text": "Page 4 .Each node corresponds to a random variable .In particular , shaded nodes denotes observable variables .Each rectangle captures the replication of the structure it contains .The number of replications is indicated in the bottom - right corner of the rectangle .", "label": "", "metadata": {}, "score": "86.76347"}
{"text": "A case study considers varying regression effects for a bivariate suicide outcome , namely male and female suicides in 354 English local authorities with social deprivation , social fragmentation and rurality as predictors .BibTeX .Share .OpenURL .Abstract .", "label": "", "metadata": {}, "score": "86.98544"}
{"text": "In option markets , the most popular way has been to extract implied volatilities to assess the future variability of the underlying with the use of the Black ... \" .Extracting market expectations has always been an important issue when making national policies and investment decisions in financial markets .", "label": "", "metadata": {}, "score": "89.98177"}
{"text": "In option markets , the most popular way has been to extract implied volatilities to assess the future variability of the underlying with the use of the Black ... \" .Extracting market expectations has always been an important issue when making national policies and investment decisions in financial markets .", "label": "", "metadata": {}, "score": "89.98177"}
{"text": "Each process maintains a s .. by Jason Wolfe , Aria Delier Haghighi , Daniel Klein , Jason Wolfe , Aria Haghighi , Dan Klein , 2007 . \" ... personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page .", "label": "", "metadata": {}, "score": "95.3284"}
{"text": "Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .Publisher conditions are provided by RoMEO .Differing provisions from the publisher 's actual policy or licence agreement may be applicable .", "label": "", "metadata": {}, "score": "109.38533"}
{"text": "personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page .To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission . \" ...", "label": "", "metadata": {}, "score": "112.88461"}
{"text": "Document 2 : Despite his arrest on suspicion of driving under the influence yesterday , Chargers receiver Vincent Jackson will play in Sunday 's AFC divisional playoff game at Pittsburgh .Document 3 : In another anti - piracy operation , Navy warship on Saturday repulsed an attack on a merchant vessel in the Gulf of Aden and nabbed 23 Somali and Yemeni sea brigands .", "label": "", "metadata": {}, "score": "113.250534"}
{"text": "We evaluated the models on the ACE 2005 event corpus [ 18 ] and on a new annotated corpus encoding within- and cross - document event coreference information ( Section 5 ) .Page 2 .Document 1 : San Diego Chargers receiver Vincent Jackson was arrested on suspicion of drunk driving on Tuesday morning , five days before a key NFL playoff game . ...", "label": "", "metadata": {}, "score": "120.581"}
