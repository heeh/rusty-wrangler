{"text": "They are important for a few reasons .First , at present the best performing parsers on the WSJ treebank ( Ratnaparkhi 1997 ; Charniak 1997 , 1999 ; Collins 1997 , 1999 ) are all cases of history - based mo .. \" ...", "label": "", "metadata": {}, "score": "35.59182"}
{"text": "In order to obtain linguistically adequate CCG analyses , and to eliminate noise and inconsistencies in the original annotation , an extensive analysis of the constructions and annotations in the Penn Treebank was called for , and a substantial number of changes to the Treebank were necessary .", "label": "", "metadata": {}, "score": "38.04725"}
{"text": "In order to obtain linguistically adequate CCG analyses , and to eliminate noise and inconsistencies in the original annotation , an extensive analysis of the constructions and annotations in the Penn Treebank was called for , and a substantial number of changes to the Treebank were necessary .", "label": "", "metadata": {}, "score": "38.04725"}
{"text": "This has led to concer ... \" .Statistical parsers trained and tested on the Penn Wall Street Journal ( WSJ ) treebank have shown vast improvements over the last 10 years .Much of this improvement , however , is based upon an ever - increasing number of features to be trained on ( typically ) the WSJ treebank data .", "label": "", "metadata": {}, "score": "40.825146"}
{"text": "Analysis of TueSBL 's errors is then presented .Since it is debatable whether PARCEVAL 's measures offer an accurate picture of a parser 's output , a dependency - based evaluation is proposed ( cf .( Lin , 1995 ) ) .", "label": "", "metadata": {}, "score": "43.064873"}
{"text": "These state - of - the - art methods achieve roughly similar accuracy on the Wall Street Journal corpus of about 96.36 % to 96.82 % ( Brill et al ., 1998 ) .All of them use words and tags surrounding a word in a small window ( 1 - 3 on either side ) to assign a tags to all words in a sentence .", "label": "", "metadata": {}, "score": "43.306046"}
{"text": "The experiments in this project were conducted on one of the most commonly used such corpus , the Wall Street Journal articles from the Penn Treebank project ( Marcus et al . , 1994 ) , which contains over a million tagged words .", "label": "", "metadata": {}, "score": "43.90972"}
{"text": "A Fundamental Algorithm for Dependency Parsing .In Proceedings of the 39th Annual ACM Southeast Conference , pp .95 - 102 .Fan , R.-E. , Chang , K.-W. , Hsieh , C.-J. , Wang , X.-R. and Lin , C.-J. LIBLINEAR :", "label": "", "metadata": {}, "score": "45.085205"}
{"text": "French TreeBank ( FTB , Abeill\u00e9 et al ; 2003 )Le Monde , December 2007 version , 28-tag tagset ( CC tagset , Crabb\u00e9 and Candito , 2008 ) .Classical data split ( 10 - 10 - 80 ) : . Brants ( 2000 ) reports 96.7 % token accuracy and 85.5 % unknown word accuracy on a 10-fold cross - validation of the Penn WSJ corpus .", "label": "", "metadata": {}, "score": "46.601185"}
{"text": "A second group of papers does parsing by a sequence of independent , discriminative decisions , either greedily or with use of a small beam ( Ratnaparkhi , 1997 ; Henderson , 2004 ) .This paper extends th ...Corpus Linguistics : Readings in a Widening Discipline .", "label": "", "metadata": {}, "score": "46.84088"}
{"text": "Complete trees are thus viewed as classification classes , a decision reminiscent to DOP .To quickly batter away doubts on the ' trees as classes'-concept , the hybrid parsing architecture of TueSBL is presented .Its indispensable preprocessing module CASS , Abney 's ( 1991 ) chunk parser and its adoption to German , and the present task , are described in section 5.3 .", "label": "", "metadata": {}, "score": "46.86564"}
{"text": "A sample parse and the weighting scheme are then presented .The latter is also an example of non - standard approach .The chapter ends with an explanation of the backing - off strategy of TueSBL , which is triggered in those case where no tree structures were discovered for input chunks or no sentences were matched to the input sentence .", "label": "", "metadata": {}, "score": "47.62599"}
{"text": "Black , E. , F. Jelinek , J. D. Lafferty , D. M. Magerman , R. L. Mercer and S. Roukos ( 1992 ) .Towards history - based grammars : Using richer models for probabilistic parsing .In Proceedings of the 5th DARPA Speech and Natural Language Workshop , pp .", "label": "", "metadata": {}, "score": "47.72251"}
{"text": "To learn the tree structures we use greedy hill - climbing with Bayesian scoring to evaluate next candidates ( Chickering et al . , 1997 ) .The remaining words are either unambiguous or there is not enough data to learn contextualized cpds .", "label": "", "metadata": {}, "score": "48.083138"}
{"text": "Deterministic parsing algorithms for building labeled dependency graphs ( Kudo and Matsumoto,2002 ; Yamada and Matsumoto , 2003 ; Nivre,2003 ) .History - based models for predicting the next parser action at nondeterministic choice points ( Black et al . , 1992 ; Magerman , 1995 ; Ratnaparkhi , 1997 ; Collins , 1999 ) .", "label": "", "metadata": {}, "score": "48.70122"}
{"text": "Finally , we present multilingual experiments which show that parsing with hierarchical state - splitting is fast and accurate in multiple languages and domains , even without any language - specific tuning . ... g ( Matsuzaki et al . , 2005 ; Petrov et al . , 2006 ) .", "label": "", "metadata": {}, "score": "48.71691"}
{"text": "The former being a generation of a parsed forest , while the latter is the finding of the most probable derivation ( an NP - hard problem , unlike in standard probabilistic context - free parsing ) .Two evaluations of DOP1 are discussed in section 4.2 - one on the Wall Street Journal section of the Penn Treebank ( Marcus et al . , 1993 ) and the other on the Air Travel Information System corpus .", "label": "", "metadata": {}, "score": "48.748867"}
{"text": "I currently serve as chair of the Advisory Committee of the Center of Excellence in Human Language Technology at John Hopkins University .I was named a Fellow of the American Association of Artificial Intelligence in 1992 .I created and ran the Penn Treebank Project through the mid-1990s which developed the primary training corpus that led to a breakthrough in the accuracy of natural language parsers for unrestricted text .", "label": "", "metadata": {}, "score": "48.813023"}
{"text": "49 - 56 .Ratnaparkhi , A. ( 1997 ) .A linear observed time statistical parser based on maximum entropy models .In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pp . 1 - 10 .", "label": "", "metadata": {}, "score": "49.607735"}
{"text": "The splits of data for this task were not standardized early on ( unlike for parsing ) and early work uses various data splits defined by counts of tokens or by sections .Most work from 2002 on adopts the following data splits , introduced by Collins ( 2002 ) : .", "label": "", "metadata": {}, "score": "50.124832"}
{"text": "Because the conditioning is local , efficient polynomial time parsing algorithms exist for computing inside , outside , and Viterbi parses .PFGs can produce probabilities of strings , making them potentially useful for language modeling .Precision and recall results are comparable to the state of the art with words , and the best reported without words . 1 Introduction Recently , many researchers have worked on statistical parsing techniques which try to capture additional context beyond that of simple probabilistic context - free grammars ( PCFGs ) , including Magerman ( 1995 ) , Charniak ( 1996 ) , Collins ( 1996 ; 1997 ) , ... . ... is somewhat inelegant ; also , for the probabilities to sum to one , it requires an additional step of normalization , which they appear not to have implemented .", "label": "", "metadata": {}, "score": "50.26363"}
{"text": "Because the conditioning is local , efficient polynomial time parsing algorithms exist for computing inside , outside , and Viterbi parses .PFGs can produce probabilities of strings , making them potentially useful for language modeling .Precision and recall results are comparable to the state of the art with words , and the best reported without words . 1 Introduction Recently , many researchers have worked on statistical parsing techniques which try to capture additional context beyond that of simple probabilistic context - free grammars ( PCFGs ) , including Magerman ( 1995 ) , Charniak ( 1996 ) , Collins ( 1996 ; 1997 ) , ... . ... is somewhat inelegant ; also , for the probabilities to sum to one , it requires an additional step of normalization , which they appear not to have implemented .", "label": "", "metadata": {}, "score": "50.26363"}
{"text": "Such worries have merit .The standard \" Charniak parser \" checks in at a labeled precisionrecall f - measure of 89.7 % on the Penn WSJ test set , but only 82.9 % on the test set from the Brown treebank corpus .", "label": "", "metadata": {}, "score": "50.73278"}
{"text": "We introduce a new method for the reranking task , based on the boosting approach to ranking problems described in Freund et al .( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .", "label": "", "metadata": {}, "score": "51.11531"}
{"text": "S. Pradhan , E. Hovy , M. Marcus , M. Palmer , L. Ramshaw , and R. Weischedel , OntoNotes : A Unified Relational Semantic Representation , International Journal of Semantic Computing , Vol . 1 , No . 4 , 2007 .", "label": "", "metadata": {}, "score": "51.38258"}
{"text": "The Penn Treebank has recently implemented a new syntactic annotation scheme , designed to highlight aspects of predicate - argument structure .This paper discusses the implementation of crucial aspects of this new annotation scheme .It incorporates a more consistent treatment of a wide range of gramma ... \" .", "label": "", "metadata": {}, "score": "51.66323"}
{"text": "This paper discusses the implementation of crucial aspects of this new annotation scheme .INTRODUCTION During the first phase of the The Penn Treebank project [ 10 ] , ending in December 1992 , 4.5 million words of text were tagged for part - of - speech , with about two - thirds of this material also annotated with a skeletal syntactic bracketing .", "label": "", "metadata": {}, "score": "52.18741"}
{"text": "These have internal structure and allow for modifications , such as omission of words and chunks from a tree ( see pages 127 - 132 ) .This particular decision is very well motivated in the book and also stems from the design and organisation of the treebank ( TueBa - D / S ) .", "label": "", "metadata": {}, "score": "52.828075"}
{"text": "Since syntactically annotated corpora is after all of modest size , the same goes for TueBa - D / S , TueSBL is given access to all possible information in the treebank , yet not simultaneously but \" ordered according to their reliability \" ( p. 160 ) .", "label": "", "metadata": {}, "score": "53.03383"}
{"text": "First , we compare a complete sentence in a stack with ... .by Eduard Hovy , Chin - yew Lin , Liang Zhou - Proceedings of DUC-2005 , 2005 . \" ...In this paper we introduce Basic Elements , a new way of automating the evaluation of text summaries .", "label": "", "metadata": {}, "score": "53.16303"}
{"text": "First , we compare a complete sentence in a stack with ... .by Eduard Hovy , Chin - yew Lin , Liang Zhou - Proceedings of DUC-2005 , 2005 . \" ...In this paper we introduce Basic Elements , a new way of automating the evaluation of text summaries .", "label": "", "metadata": {}, "score": "53.16303"}
{"text": "The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence .", "label": "", "metadata": {}, "score": "53.18172"}
{"text": "( 1993 ) proposed a probabilistic model for combining these features : .This model makes the approximation that those features are independent given the tag to keep the number of parameters small , but ignores certain correlations , for example , between capitalized and unknown .", "label": "", "metadata": {}, "score": "54.050797"}
{"text": "Section 6.7 not only gives the results of the dependency - based evaluation , but it also offers a comparison of the two metrics .Bringing us back to the wider concept of MBL , Chapter 7 compares K\u00fcbler 's parser to other approaches already mentions in Chapters", "label": "", "metadata": {}, "score": "56.71136"}
{"text": "Therefore it covers the domain of business appointments , travel scheduling and hotel reservations and contains approximately 67000 trees .In quite some detail the annotation scheme is presented with appropriate examples and a comparison to the Penn Treebank is given in section 5.1 .", "label": "", "metadata": {}, "score": "56.99875"}
{"text": "We could also introduce new variables , e.g. , nonterminal refinements ( Matsuzaki et al . , 2005 ) , or secondary links Mij ( not constrai ... . by Jin - dong Kim , Tomoko Ohta , Sampo Pyysalo , Yoshinobu Kano - In Proceedings of Natural Language Processing in Biomedicine ( BioNLP )", "label": "", "metadata": {}, "score": "57.079895"}
{"text": "E. Brill , D. Magerman , M. Marcus , and B. Santorini , Deducing linguistic structure from the statistics of large corpora , Proceedings of DARPA Speech and Natural Language Workshop , June , 1990 , Morgan - Kaufmann . D. Magerman , M. Marcus , Parsing a natural language using mutual information statistics , Proceedings of AAAI 90 .", "label": "", "metadata": {}, "score": "57.165997"}
{"text": "In addition , many words may have not been previously encountered , so a tag must be decided upon based on various features of the word and its context .However , POS tagging is a simpler task than full syntactic parsing , since no attempt is made to create a tree - structured model of the sentence .", "label": "", "metadata": {}, "score": "57.21748"}
{"text": "The paper presents the design and implementation of the BioNLP'09 Shared Task , and reports the final results with analysis .The shared task consists of three sub - tasks , each of which addresses bio - molecular event extraction at a different level of specificity .", "label": "", "metadata": {}, "score": "57.48201"}
{"text": "The paper presents the design and implementation of the BioNLP'09 Shared Task , and reports the final results with analysis .The shared task consists of three sub - tasks , each of which addresses bio - molecular event extraction at a different level of specificity .", "label": "", "metadata": {}, "score": "57.48201"}
{"text": "It is therefore necessary to carefully select the relevant ones and have proper metrics for calculating their weights .This is clearly explained in section 2.3 , where the author discusses two of the most widely used - mutual information and class projection , relying mainly , but not only , on the work of ( Wettschereck et al , 1997 ) .", "label": "", "metadata": {}, "score": "57.667706"}
{"text": "( Several models were tried , including max , min , product and mixture , but this one seemed to work best . )Since test data contains words not seen in the training data , we must predict tags for unknown words .", "label": "", "metadata": {}, "score": "57.670662"}
{"text": "Microsoft Technical Report MSR - TR-00 - 16 .[ Marcus et al . , 1994 ] Mitchel P. Marcus , Beatrice Santorini , and Mary Ann Marcinkiewicz .Building a large annotaded corpus of English : the Penn Treebank .Computational Linguistics 19(2):313 - 330 .", "label": "", "metadata": {}, "score": "57.785507"}
{"text": "Building this kind of resource is expensive and labor - intensive .This work proposes to use sample selection to find helpful training examples and reduce human effort spent on annotating less informative ones .We consider several criteria for predicting whether unlabeled data might be a helpful training example .", "label": "", "metadata": {}, "score": "58.61828"}
{"text": "Building this kind of resource is expensive and labor - intensive .This work proposes to use sample selection to find helpful training examples and reduce human effort spent on annotating less informative ones .We consider several criteria for predicting whether unlabeled data might be a helpful training example .", "label": "", "metadata": {}, "score": "58.61828"}
{"text": "Darpa Speech and Natural Language Workshop , 1994 . \" ... Parser development is generally viewed as a primarily linguistic enterprise .A grammarian examines sentences , skillfully extracts the linguistic generalizations evident in the data , and writes grammar rules which cover the language .", "label": "", "metadata": {}, "score": "58.77301"}
{"text": "Darpa Speech and Natural Language Workshop , 1994 . \" ... Parser development is generally viewed as a primarily linguistic enterprise .A grammarian examines sentences , skillfully extracts the linguistic generalizations evident in the data , and writes grammar rules which cover the language .", "label": "", "metadata": {}, "score": "58.77301"}
{"text": "This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this i ... \" .", "label": "", "metadata": {}, "score": "58.88894"}
{"text": "In 2008 the shared task was dedicated to the joint parsing of syntactic and semantic depe ... \" .The Conference on Computational Natural Language Learning is accompanied every year by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting .", "label": "", "metadata": {}, "score": "59.07147"}
{"text": "In these results , the generative model performs significantly better than the others , and does about equally well at assigning part - of - speech tags . \" ...Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .", "label": "", "metadata": {}, "score": "59.242493"}
{"text": "The power of transformation - based approach comes partly from that fact that the initial assignment is already very accurate ( around 93 % ) .However , although the learning phase uses corpus statistics to induce rules , tagging itself is deterministic .", "label": "", "metadata": {}, "score": "59.45439"}
{"text": "In this paper we introduce Basic Elements , a new way of automating the evaluation of text summaries .We show that this method correlates better with human judgments than any other automated procedure to date , and overcomes the subjectivity / variability problems of manual methods that require humans to preprocess summaries to be evaluated .", "label": "", "metadata": {}, "score": "59.583656"}
{"text": "In this paper we introduce Basic Elements , a new way of automating the evaluation of text summaries .We show that this method correlates better with human judgments than any other automated procedure to date , and overcomes the subjectivity / variability problems of manual methods that require humans to preprocess summaries to be evaluated .", "label": "", "metadata": {}, "score": "59.583656"}
{"text": "While the author claims that TueSBL can be trained on any treebank that conforms to the TueBa - D / S format , it is well known that treebanks are time- and resource - consuming .It would have been interesting to know whether TueSBL could be trained on other treebanks ( not strictly following the TueBa - D / S format ) or even on non - phrase structure treebanks , i.e. dependency - based representations .", "label": "", "metadata": {}, "score": "59.65486"}
{"text": "There , TiMBL ( Tilburg Memory - Based Learner , ( Daelemans et al . , 2003 ) ) is used to predict the next step of a deterministic dependency based parser .( Nivre et al ., 2004 ) present results for Swedish and in ( Nivre and Scholz , 2004 ) the results for English are given .", "label": "", "metadata": {}, "score": "59.821404"}
{"text": "Nivre , Joakim , Johan Hall and Jens Nilsson .Memory - Based Dependency Parsing .In Ng , H. T. and Riloff , E. ( eds . )Proceedings of the Eighth Conference on Computational Natural Language Learning ( CoNLL ) .", "label": "", "metadata": {}, "score": "60.119247"}
{"text": "We also present a proof that owl - qn is guaranteed to converge to a globally optimal parameter vector . \" ...Statistical parsers trained and tested on the Penn Wall Street Journal ( WSJ ) treebank have shown vast improvements over the last 10 years .", "label": "", "metadata": {}, "score": "60.122303"}
{"text": "LIBLINEAR --A Library for Large Linear Classification ( Fan et al . , 2008 ) .MaltParser can also be turned into a phrase structure parser that recovers both continuous and discontinuous phrases with both phrase labels and grammatical functions ( Hall and Nivre , 2008a ; Hall and Nivre , 2008b ) .", "label": "", "metadata": {}, "score": "60.271606"}
{"text": "The reader is given enough detail to understand the essence of memory - based learning , yet not too much to be distracted from the central topic - TueSBL .Recently , another way to exploit the benefits of MBL for syntactic parsing has been presented , namely the work of ( Nivre and Sholz , 2004 ) and ( Nivre et al .", "label": "", "metadata": {}, "score": "60.366093"}
{"text": "A Dependency - based Method for Evaluating Broad - Coverage Parsers .In Proceedings of the 14th International Joint Conference on Artificial Intelligence , IJCAI-95 .Montreal , Canada .Marcus , Mitchell , Beatrice Santorini and Mary Ann Marcinkiewicz .Building a Large Annotated Corpus of English : The Penn Treebank .", "label": "", "metadata": {}, "score": "60.40815"}
{"text": "This project investigates a probabilistic method of exploiting this high accuracy of tagging most words to bootstrap tagging of difficult ones .The success of the above state - of - the - art models has shown that the tags of surrounding words provide a lot of information about the tag of a word .", "label": "", "metadata": {}, "score": "60.424942"}
{"text": "The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .", "label": "", "metadata": {}, "score": "60.745113"}
{"text": "About thirty different suffixes we distinguished , of which around twenty actually ended up being used by the induced decision tree .Tagging .Test sentences are tagged one at a time .N .n . select most commonly sampled tag for each T i .", "label": "", "metadata": {}, "score": "61.087803"}
{"text": "M. Marcus , B. Santorini , and M. Marcinkiewicz , Building a large annotated corpus of English : the Penn Treebank , Corpus Linguistics : Readings in a Widening Discipline , G. Sampson and D. McCarthy ( eds . ) , Continuum , 2004 .", "label": "", "metadata": {}, "score": "61.16442"}
{"text": "First , the initial samples must be discarded and sequential samples are not independent , so samples are actually counted after a short burn - in phase and with counts incremented every several iterations .Second , tags do not have to be sampled sequentially , and indeed , performance is improved when a random order is used .", "label": "", "metadata": {}, "score": "61.31772"}
{"text": ", 2004 ; Hall et al . , 2006 ) .MaltParser allows users to define feature models of arbitrary complexity .MaltParser currently includes two machine learning packages ( thanks to Sofia Cassel for her work on LIBLINEAR ) : .", "label": "", "metadata": {}, "score": "61.38008"}
{"text": "35 .The Syntax of Disfluency in Spontaneous Spoken Language ( 1998 )David McKelvie 36 .The Use of Large Text Corpora for Evaluating Text - to - Text Speech Systems ( 1998 ) Louis C.W.Pols et al .37 .", "label": "", "metadata": {}, "score": "61.4091"}
{"text": "Hall , J. , J. Nivre and J. Nilsson ( 2006 ) .Discriminative Classifiers for Deterministic Dependency Parsing .In Proceedings of the 21stInternational Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics , pp .", "label": "", "metadata": {}, "score": "61.770306"}
{"text": "Ratnaparkhi , 1996 finds that distribution of tags for the word \" about \" ( as well as several others ) is fairly different for different annotators of the dataset , suggesting that there is a real limit to the level of achievable accuracy .", "label": "", "metadata": {}, "score": "62.45119"}
{"text": "We apply this idea to dependency and constituent parsing , generating results that surpass state - of - theart ... \" .We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers .", "label": "", "metadata": {}, "score": "62.51404"}
{"text": "Semi - supervised Training for the Averaged Perceptron POS Tagger .Proceedings of the 12 EACL , pages 763 - 771 .What follows is a review or discussion note contributed to our Book Discussion Forum .We expect discussions to be informal and interactive ; and the author of the book discussed is cordially invited to join in .", "label": "", "metadata": {}, "score": "62.540756"}
{"text": "Chang , C.-C. and C.-J. Lin ( 2001 ) .LIBSVM : A Library for Support Vector Machines .[ pdf ] .Collins , M. ( 1999 ) .Head - Driven Statistical Models for Natural Language Parsing .Ph . D. thesis , University of Pennsylvania .", "label": "", "metadata": {}, "score": "62.656"}
{"text": "This project explores a novel approach to Part - of - Speech tagging that uses statistical techniques to train a model from a large POS - tagged corpus and assign tags to previously unseen text .The model uses decision trees based on tags of surrounding words and other features of a word to predict its tag .", "label": "", "metadata": {}, "score": "62.717834"}
{"text": "TueSBL is designed to exploit a relatively small treebank to its maximum when trained on it .Complete trees are stored in the memory and also represent the classification classes .This is something that resembles DOP very much yet one do not get the weak points of pure DOP - large storage space and intractability of the parsing model .", "label": "", "metadata": {}, "score": "62.726295"}
{"text": "Certain enhancements of DOP to tackle unknown words ( the so called DOP3 model ) are presented in section 4.4 .Further , the possibility to render DOP to PCFG in order to achieve faster search for the most probable derivation and alleviate the exponential growth of grammar , as well as a memory - based approach to DOP are cited in the remaining two sections .", "label": "", "metadata": {}, "score": "62.90172"}
{"text": "Kudo , T. and Y. Matsumoto ( 2002 ) .Japanese Dependency Analysis Using Cascaded Chunking .In Proceedings of the Sixth Workshop on Computational Language Learning ( CoNLL ) , pp .63 - 69 .Magerman , D. M. ( 1995 ) .", "label": "", "metadata": {}, "score": "62.991726"}
{"text": "ACL 1998 .[Chickering et al . , 1997 ] David Chickering , David Heckerman , Christopher Meek .A Bayesian Approach to Learning Bayesian Networks with Local Structure .Microsoft Technical Report MSR - TR-97 - 07 .[Heckerman et al . , 2000 ] David Chickering , Christopher Meek , Robert Rounthwaite , Carl Kadie .", "label": "", "metadata": {}, "score": "63.066772"}
{"text": "The shared task was run over 12 weeks , drawing initial interest from 42 teams .Of these teams , 24 submitted final results .The evaluation results are encouraging , indicating that state - of - the - art performance is approaching a practically applicable level and revealing some remaining challenges . ... parsers . \" ...", "label": "", "metadata": {}, "score": "63.20845"}
{"text": "We present a detailed case study of this learni ... \" .this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance .", "label": "", "metadata": {}, "score": "63.841522"}
{"text": "TueSBL resembles DOP , since both methods \" rely on trees or tree fragments that go beyond the local information present in rules . \"[ p. 252].Yet memory requirements and levels of generalisation are quite different in the two approaches .", "label": "", "metadata": {}, "score": "63.988358"}
{"text": "The Conference on Computational Natural Language Learning is accompanied every year by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting .In 2008 the shared task was dedicated to the joint parsing of syntactic and semantic dependencies .", "label": "", "metadata": {}, "score": "64.0998"}
{"text": "We also give an overview of the parsing approaches that participants took and the results that they achieved .Finally , we try to draw general conclusions about multi - lingual parsing : What makes a particular language , treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser ?", "label": "", "metadata": {}, "score": "64.11519"}
{"text": "Nivre , J. ( 2006 ) Inductive Dependency Parsing .Springer .Nivre , J. , Hall , J. and Nilsson , J. ( 2004 )Memory - Based Dependency Parsing .In Ng , H. T. and Riloff , E. ( eds . )", "label": "", "metadata": {}, "score": "64.17972"}
{"text": "Furhter morphological features can be used for tagging of unknown words .Recent work by Brill et al .( 1998 ) showed that combining several different state - of - the - art taggers ( HMM , MaxEnt , Transformation ) in a classifier ensemble can achieve performance of up to 97.2 % percent .", "label": "", "metadata": {}, "score": "64.439674"}
{"text": "Annual Meeting of the ACL , 1983 .M. Marcus , A Theory of Syntactic Recognition for Natural Language , MIT Press , 1980 .( Please note : Under many circumstances , I do n't put my name on my students ' papers .", "label": "", "metadata": {}, "score": "64.45592"}
{"text": "This system outperforms previou ... \" .We describe a parsing system based upon a language model for English that is , in turn , based upon assigning probabilities to possible parses for a sentence .This model is used in a parsing system by finding the parse for the sentence with the highest probability .", "label": "", "metadata": {}, "score": "64.52288"}
{"text": "F.G.A.M. Aarts 5 .Predicting Text Segmentation into Tone Units ( 1986 )Bengt Altenberg 6 .Typicality and Meaning Potentials ( 1986 ) Patrick Hanks 7 .Historical Drift in Three English Genres ( 1987 )Douglas Biber and Edward Finegan 8 .", "label": "", "metadata": {}, "score": "64.78232"}
{"text": "( reprinted from Computational Linguistics , 19(2 ) , 1993 ) .M. Marcus ( ed . ) , HLT 2002 : Proceedings of the Second International Conference on Human Language Technology Research , Morgan Kaufmann , 2002 .L. Ramshaw , M. Marcus , Text Chunking using Transformation - Based Learning , Natural Language Processing Using Very Large Corpora , Armstrong et al .", "label": "", "metadata": {}, "score": "64.84459"}
{"text": "Documentation .Resources .Contact .Introduction .MaltParser is a system for data - driven dependency parsing , which can be used to induce a parsing model from treebank data and to parse new data using an induced model .MaltParser is developed by Johan Hall , Jens Nilsson and Joakim Nivre at V\u00e4xj\u00f6 University and Uppsala University , Sweden .", "label": "", "metadata": {}, "score": "64.90713"}
{"text": "Statistical Dependency Analysis with Support Vector Machines .In Proceedings of the 8th International Workshop on Parsing Technologies ( IWPT ) , pp .195 - 206 .Hall , J. and J. Nivre ( 2008a )A Dependency - Driven Parser for German Dependency and Constituency Representations .", "label": "", "metadata": {}, "score": "65.00397"}
{"text": "This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar ( CCG ) derivations augmented with local and long - range word - word dependencies .The resulting corpus , CCGbank , includes 99.4 % of the sentences in the Penn Treebank .", "label": "", "metadata": {}, "score": "65.01511"}
{"text": "This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar ( CCG ) derivations augmented with local and long - range word - word dependencies .The resulting corpus , CCGbank , includes 99.4 % of the sentences in the Penn Treebank .", "label": "", "metadata": {}, "score": "65.01511"}
{"text": "This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar ( CCG ) derivations augmented with local and long - range word - word dependencies .The resulting corpus , CCGbank , includes 99.4 % of the sentences in the Penn Treebank .", "label": "", "metadata": {}, "score": "65.01511"}
{"text": "This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar ( CCG ) derivations augmented with local and long - range word - word dependencies .The resulting corpus , CCGbank , includes 99.4 % of the sentences in the Penn Treebank .", "label": "", "metadata": {}, "score": "65.01511"}
{"text": "( 1999 ) Alena Bohmova and Eva Hajicova 38 .Reflections of a Dendrographer ( 1999 )Geoffrey Sampson 39 .A Generic Approach to Software Support for Linguistic Annotation Using XML ( 2000 )Jean Carletta , et al .40 .", "label": "", "metadata": {}, "score": "65.12588"}
{"text": "Experiments .The experiments were conducted by randomly splitting the Wall St. Journal corpus into a training and testing in roughly 90/10 proportion .There are several model parameters that need to be set .This maybe due to overfitting in the cpds for the larger contexts , but I did not investigate an alternative estimate smoothing or tree induction method .", "label": "", "metadata": {}, "score": "65.309494"}
{"text": "Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .On the other hand , our grammars are much more compact and substantially more accurate than previous work on automatic annotation .Despite its simplicity , our best grammar achieves an F1 of 90.2 % on the Penn Treebank , higher than fully lexicalized systems . ... reebank , higher than fully lexicalized systems .", "label": "", "metadata": {}, "score": "65.531845"}
{"text": "Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .The tenth CoNLL ( CoNLL - X ) saw a shared task on Multilingual Dependency Parsing .", "label": "", "metadata": {}, "score": "65.886765"}
{"text": "Comput .Linguist . , 2010 . \" ...Information - extraction ( IE ) systems seek to distill semantic relations from naturallanguage text , but most systems use supervised learning of relation - specific examples and are thus limited by the availability of training data .", "label": "", "metadata": {}, "score": "66.020645"}
{"text": "It does not use cascades of MBL classifiers and does not only return grammatical relations between chunks but delivers a complete parse .On the other hand it is not a purely k - nearest neighbour approach .As such certain modifications are necessary .", "label": "", "metadata": {}, "score": "66.25683"}
{"text": "This project investigated a novel combination of statistical methods to define a flexible , though implicit , probability distribution for prediction of Part - of - Speech tags .The model can be improved upon in several ways .It is possible that using surrounding words , not just tags may be advantageous .", "label": "", "metadata": {}, "score": "66.37424"}
{"text": "Chapter 3 , therefore , describes how different authors solve the partial tasks or adopt the idea of MBL by cascaded classifiers .Section 3.1 summarises two approaches to noun - phrase ( NP ) chunking .Next , shallow parsing and grammatical relation finding are discussed .", "label": "", "metadata": {}, "score": "66.53504"}
{"text": "While not an embodiment of the k - nearest neighbour model , DOP shares many similarities with MBL , which K\u00fcbler summarises on p. 58 .The basic DOP model , i.e. DOP1 , for phrase structure trees is presented in section 4.1 .", "label": "", "metadata": {}, "score": "66.539925"}
{"text": "In particular we note the effects of two comparatively recent techniques for parser improvement .Then a reranking phase uses more detailed features , features which would ( mostly ) be ... . \" ...We formulate dependency parsing as a graphical model with the novel ingredient of global constraints .", "label": "", "metadata": {}, "score": "66.54556"}
{"text": "We investigate the precise relationship between these two formalisms , showing that , while they define the same classes of probabilis- tic languages , they appear to impose different inductive biases . ... fining the probability of a parse tree as the probability that a certain top - down stochastic generative process produces that tree .", "label": "", "metadata": {}, "score": "66.69246"}
{"text": "We investigate the precise relationship between these two formalisms , showing that , while they define the same classes of probabilis- tic languages , they appear to impose different inductive biases . ... fining the probability of a parse tree as the probability that a certain top - down stochastic generative process produces that tree .", "label": "", "metadata": {}, "score": "66.69246"}
{"text": "The Stanford Parser is used to derive dependencies from CJ50 and gold parse trees .Figure 8 shows the detailed P / R curves .We can see that although today ... .by Jenny Rose Finkel , Alex Kleeman , Christopher D. Manning - In Proc .", "label": "", "metadata": {}, "score": "67.04382"}
{"text": "In particular , we show that the reranking parser described in Charniak and Johnson ( 2005 ) improves performance of the parser on Brown to 85.2 % .Furthermore , use of the self - training techniques described in ( Mc - Closky et al . , 2006 ) raise this to 87.8 % ( an error reduction of 28 % ) again without any use of labeled Brown data .", "label": "", "metadata": {}, "score": "67.413345"}
{"text": "It is a careful review of the existing albeit not a very extensive literature .The difficulty in utilising MBL for parsing stems from the fact that \" [ n]atural language parsing [ ... ] is not a task that can easily be defined [ as a classification problem ] \" ( p. 34 ) .", "label": "", "metadata": {}, "score": "67.50265"}
{"text": "First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar without a treebank .In our experiments , hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy .", "label": "", "metadata": {}, "score": "67.61305"}
{"text": "In this paper , we define the shared task and describe how the data sets were created .Furthermore , we report and analyze the results and describe the approaches of the participating systems . ... dependent relations rather than phrases , the task of the conversion procedure is to identify and label the head - dependent pairs .", "label": "", "metadata": {}, "score": "67.67265"}
{"text": "Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . .", "label": "", "metadata": {}, "score": "67.85584"}
{"text": "On WSJ15 , we attain a state - of - the - art F - score of 90.9 % , a 14 % relative reduction in error over previous models , while being two orders of magnitude faster .On sentences of length 40 , our system achieves an F - score of 89.0 % , a 36 % relative reduction in error over a generative baseline . ...", "label": "", "metadata": {}, "score": "67.99957"}
{"text": "Shay , 2009 . \" ...We present a family of priors over probabilistic grammar weights , called the shared logistic normal distribution .This family extends the partitioned logistic normal distribution , enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar , prov ... \" .", "label": "", "metadata": {}, "score": "68.06997"}
{"text": "Important papers can be difficult to track down .This volume reprints 42 corpus linguistics articles which first appeared at dates ranging from 1952 to 2002 , and which between them illustrate all the main directions in which the sunject is developing .", "label": "", "metadata": {}, "score": "68.13249"}
{"text": "In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics ( ACL ) , pp .276 - 283 .Nivre , J. ( 2003 ) .An Efficient Algorithm for Projective Dependency Parsing .In Proceedings of the 8th International Workshop on Parsing Technologies ( IWPT 03 ) , pp .", "label": "", "metadata": {}, "score": "68.36716"}
{"text": "While prior feature - based dynamic programming parsers have restricted training and evaluation to artificially short sentences , we present the first general , featurerich discriminative parser , based on a conditional random field model , which has been successfully scaled to the full WSJ parsing data .", "label": "", "metadata": {}, "score": "68.387924"}
{"text": "Another strong point of the monograph is that the work described in it is clearly placed in the context of other memory - based approaches to parsing .Chapters 3 and 4 give in enough detail to what previous authors have done in this field .", "label": "", "metadata": {}, "score": "68.49636"}
{"text": "An Improved Oracle for Dependency Parsing with Online Reordering .In Proceedings of the 11th International Conference on Parsing Technologies ( IWPT ) , 73 - 76 . \" ...The Conference on Computational Natural Language Learning is accompanied every year by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting .", "label": "", "metadata": {}, "score": "68.90983"}
{"text": "References .[ Brill , 1995 ] Eric Brill .Transformation - based error - driven learning and natural language processing : A case study in part of speech tagging .Computational Linguistics 21:543 - 565 .[ Brill , 1998 ] Eric Brill and Jun Wu .", "label": "", "metadata": {}, "score": "69.42847"}
{"text": "The previous versions 0.1 - 0.4 of MaltParser were implemented in C. The Java implementation ( version 1.0.0 and later releases ) replaces the C implementation ( version 0 .x ) and MaltParser 0.x will not be supported and updated any more .", "label": "", "metadata": {}, "score": "69.79149"}
{"text": "Manning , Christopher D. 2011 .Part - of - Speech Tagging from 97 % to 100 % : Is It Time for Some Linguistics ?In Alexander Gelbukh ( ed . ) , Computational Linguistics and Intelligent Text Processing , 12th International Conference , CICLing 2011 , Proceedings , Part I. Lecture Notes in Computer Science 6608 , pp .", "label": "", "metadata": {}, "score": "70.02026"}
{"text": "Certain emphasis is also given to the work of Sabine Buchholz ( e.g. ( Buchholz , 2002 ) ) , especially the optimal results and parameters involved in discovering relations between verb chunks and other chunks in a sentence .Last , in section 3.3 , an approach to full parsing using MBL is reviewed .", "label": "", "metadata": {}, "score": "70.044334"}
{"text": "We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .", "label": "", "metadata": {}, "score": "70.12072"}
{"text": "We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .", "label": "", "metadata": {}, "score": "70.12072"}
{"text": "Constantine Lignos , Erwin Chan , Mitchell P. Marcus , and Charles Yang , A rule - based acquisition model adapted for morphological analysis , Multilingual Information Access Evaluation I. Text Retrieval Experiments .Lecture Notes in Computer Science , 6241 , 658 - 665 .", "label": "", "metadata": {}, "score": "70.23411"}
{"text": "22 .Automatically Extracting Collocations from Corpora for Language Learning ( 1994 ) Kenji Kita , et al .23 .Developing and Evaluating a Probabilistic LR Parser of Part - of - Speech and Punctuation Labels ( 1995 ) E.J. Briscoe and J.A. Carroll 24 .", "label": "", "metadata": {}, "score": "70.50655"}
{"text": "The standard PARSEVAL metrics of bracketed / labelled precision and recall are described and TueSBL 's performance in these terms is given .Since TueBa - D / S contains functional labels ( e.g. HD for head , described in Chapter 5 ) , \" functional\"-labelled precision and recall are also calculated .", "label": "", "metadata": {}, "score": "70.66695"}
{"text": "We present a new formalism , probabilistic feature grammar ( PFG ) .PFGs combine most of the best properties of several other formalisms , including those of Collins , Magerman , and Charniak , and in experiments have comparable or better performance .", "label": "", "metadata": {}, "score": "71.206184"}
{"text": "We present a new formalism , probabilistic feature grammar ( PFG ) .PFGs combine most of the best properties of several other formalisms , including those of Collins , Magerman , and Charniak , and in experiments have comparable or better performance .", "label": "", "metadata": {}, "score": "71.206184"}
{"text": "We present a new formalism , probabilistic feature grammar ( PFG ) .PFGs combine most of the best properties of several other formalisms , including those of Collins , Magerman , and Charniak , and in experiments have comparable or better performance .", "label": "", "metadata": {}, "score": "71.206184"}
{"text": "We present a new formalism , probabilistic feature grammar ( PFG ) .PFGs combine most of the best properties of several other formalisms , including those of Collins , Magerman , and Charniak , and in experiments have comparable or better performance .", "label": "", "metadata": {}, "score": "71.206184"}
{"text": "Qualification and Certainty in L1 and L2 Students ' Writing ( 1997 ) Kenneth Hyland and John Milton 33 .Analysing and Predicting Patterns of DAMSL Utterance Tags ( 1998 )Mark G. Core 34 .Assessing Claims about Language Use with Corpus Data - Swearing and Abuse ( 1998 )", "label": "", "metadata": {}, "score": "71.29854"}
{"text": "Matti Rissanen 15 .Encoding the British National Corpus ( 1992 )Gavin Burnage and Dominic Dunlop 16 .Computer Corpora - What Do They Tell Us about Culture ?( 1992 )Geoffrey Leech and Roger Fallon 17 .Representativeness in Corpus Design ( 1992 ) Douglas Biber 18 .", "label": "", "metadata": {}, "score": "71.61222"}
{"text": "non - parallel , multilingual corpus . 1 Introduction Probabilistic grammars have become an important tool in natural language processing .An attractive property of probabilistic grammars is that the ... . by Fei Wu , Daniel S. Weld - in Proc . 48th Annu .", "label": "", "metadata": {}, "score": "71.61924"}
{"text": "We propose ( a ) a lexical affinity model where words struggle to modify each other , ( b ) a sense tagging model where words fluctuate randomly in their selectional prefe ... \" .After presenting a novel O(n\u00b3 ) parsing algorithm for dependency grammar , we develop three contrasting ways to stochasticize it .", "label": "", "metadata": {}, "score": "71.88702"}
{"text": "This naive grammar ... . \" ...We present several improvements to unlexicalized parsing with hierarchically state - split PCFGs .First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar wi ... \" .", "label": "", "metadata": {}, "score": "71.90958"}
{"text": "In this work , we present 1 . an effective method for pruning in split PCFGs 2 . a comparison of objective functions for infe ... . \" ...The l - bfgs limited - memory quasi - Newton method is the algorithm of choice for optimizing the parameters of large - scale log - linear models with L2 regularization , but it can not be used for an L1-regularized loss due to its non - differentiability whenever some parameter is zero .", "label": "", "metadata": {}, "score": "72.118866"}
{"text": "Both probabilistic context - free grammars ( PCFGs ) and shift - reduce probabilistic pushdown automata ( PPDAs ) have been used for language modeling and maximum likelihood parsing .We investigate the precise relationship between these two formalisms , showing that , while they define the same classes of pr ... \" .", "label": "", "metadata": {}, "score": "72.21449"}
{"text": "Both probabilistic context - free grammars ( PCFGs ) and shift - reduce probabilistic pushdown automata ( PPDAs ) have been used for language modeling and maximum likelihood parsing .We investigate the precise relationship between these two formalisms , showing that , while they define the same classes of pr ... \" .", "label": "", "metadata": {}, "score": "72.21449"}
{"text": "Information - extraction ( IE ) systems seek to distill semantic relations from naturallanguage text , but most systems use supervised learning of relation - specific examples and are thus limited by the availability of training data .Open IE systems such as TextRunner , on the other hand , aim to handle the unbounded number of relations found on the Web .", "label": "", "metadata": {}, "score": "72.33491"}
{"text": "Corpus - based statistical parsing relies on using large quantities of annotated text as training examples .Building this kind of resource is expensive and labor - intensive .This work proposes to use sample selection to find helpful training examples and reduce human effort spent on annotating less inf ... \" .", "label": "", "metadata": {}, "score": "72.41772"}
{"text": "Corpus - based statistical parsing relies on using large quantities of annotated text as training examples .Building this kind of resource is expensive and labor - intensive .This work proposes to use sample selection to find helpful training examples and reduce human effort spent on annotating less inf ... \" .", "label": "", "metadata": {}, "score": "72.41772"}
{"text": "Ingrid Kristine Hasund and Anna - Brita Stenstrom 29 .Assessing Agreement on Classification Tasks : The Kappa Statistic ( 1996 )Jean Carletta 30 .Linguistic and Interactional Features of Internet Relay Chat ( 1996 )Christopher C. Werry 31 .", "label": "", "metadata": {}, "score": "72.42322"}
{"text": "A maximum Entropy Model for Part - Of - Speech Tagging .In EMNLP 1 , pp .133 - 142 .[ Weischedel et al , 1993 ] Ralph Weischedel , Marie Meteer , Richar Schwartz , Lance Ramshaw and Jeff Palmucci .", "label": "", "metadata": {}, "score": "72.52467"}
{"text": "MaltParser can be characterized as a data - driven parser - generator .While a traditional parser - generator constructs a parser given a grammar , a data - driven parser - generator constructs a parser given a treebank .MaltParser is an implementation of inductive dependency parsing , where the syntactic analysis of a sentence amounts to the derivation of a dependency structure , and where inductive machine learning is used to guide the parser at nondeterministic choice points ( Nivre , 2006 ) .", "label": "", "metadata": {}, "score": "72.68112"}
{"text": "Discriminative feature - based methods are widely used in natural language processing , but sentence parsing is still dominated by generative methods .While prior feature - based dynamic programming parsers have restricted training and evaluation to artificially short sentences , we present the first gene ... \" .", "label": "", "metadata": {}, "score": "72.69595"}
{"text": "Anthony McEnery 41 .Semi - automatic Tagging of Intonation in French Spoken Corpora ( 2001 ) Estelle Campione and Jean Veronis 42 .Web as Corpus ( 2001 ) Adam Kilgarriff 43 .Intonational Variation in the British Isles ( 2002 )", "label": "", "metadata": {}, "score": "73.01785"}
{"text": "We present a maximum - likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently , using as examples several problems in natural language processing . \" ... this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .", "label": "", "metadata": {}, "score": "73.02874"}
{"text": ".. rning deserves further study .There are many different ways one could try to construct a language learner .In [ 65 ] , a selforganizing language learner is proposed to be used for language modelling .In this work we take a different approach , namely starting with a s ..", "label": "", "metadata": {}, "score": "73.358505"}
{"text": "Section 7.3 gives a comparison to a conceptually different full memory - based parser .Chapter 8 concludes the book and brings up the fact that the presented approach \" is especially tailored towards processing spontaneous speech . \"[ p. 262].", "label": "", "metadata": {}, "score": "73.56586"}
{"text": "Hall , J. and J. Nivre ( 2008b )Parsing Discontinuous Phrase Structure with Grammatical Functions .In Proceedings of the 6th International Conference on Natural Language Processing ( GoTAL 2008 ) , August 25 - 27 , 2008 , Gothenburg , Sweden .", "label": "", "metadata": {}, "score": "73.71988"}
{"text": "Hidden Markov models decompose the distribution P(W , T ) over words W 1 , ... , W n and tags T 1 , ... , T n as .In contrast , transformation - based method starts with an initial assignment of tags to words using the most common tag regardless of context .", "label": "", "metadata": {}, "score": "73.79396"}
{"text": "In Bunt , H. , Merlo , P. and Nivre , J. ( eds . )New Trends in Parsing Technology .Springer .Nivre , J. ( 2009 ) Non - Projective Dependency Parsing in Expected Linear Time .In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP , 351 - 359 .", "label": "", "metadata": {}, "score": "73.96446"}
{"text": "Abney , Steven .Parsing By Chunks .In : Robert Berwick , Steven Abney and Carol Tenny ( eds . ) , Principle - Based Parsing .Kluwer Academic Publishers , Dordrecht .Buchholz , Sabine .Memory - Based Grammatical Relation Finding .", "label": "", "metadata": {}, "score": "73.96642"}
{"text": "However , it can be implicitly defined through Gibbs sampling process .A sequential Gibbs sampler instantiates the variables to arbitrary initial values and loops over them , sampling from the conditionals .It can be shown ( Heckerman et al . , 2000 ) that if conditionals are positive , the process converges to a unique stationary distribution .", "label": "", "metadata": {}, "score": "74.12341"}
{"text": "Instead he was born in London Ontario , where he grew up on a strawberry farm .He attended the University of Waterloo where he re - ceived a Bachelors of Mathematics with a joint degree in Pure Mathematics and Com - puter Science in the spring of 1987 .", "label": "", "metadata": {}, "score": "74.396225"}
{"text": "Instead he was born in London Ontario , where he grew up on a strawberry farm .He attended the University of Waterloo where he re - ceived a Bachelors of Mathematics with a joint degree in Pure Mathematics and Com - puter Science in the spring of 1987 .", "label": "", "metadata": {}, "score": "74.396225"}
{"text": "By iteratively reassigning tags based on the current assignment of other tags , and keeping track of the most common assignments , we can infer the most likely tags for each word .Probability Model .HMM methods learn a joint distribution over both words and tags of a sentence by making conditional independence assumptions ( limited horizon dependence for states and independence of words given their tags ) that are only rough approximations .", "label": "", "metadata": {}, "score": "74.555466"}
{"text": "Parser development is generally viewed as a primarily linguistic enterprise .A grammarian examines sentences , skillfully extracts the linguistic generalizations evident in the data , and writes grammar rules which cover the language .The grammarian . ... the 1100 sentences .", "label": "", "metadata": {}, "score": "74.8956"}
{"text": "Parser development is generally viewed as a primarily linguistic enterprise .A grammarian examines sentences , skillfully extracts the linguistic generalizations evident in the data , and writes grammar rules which cover the language .The grammarian . ... the 1100 sentences .", "label": "", "metadata": {}, "score": "74.8956"}
{"text": "POS Tagging ( State of the art ) .Contents .Performance measure : per token accuracy .( The convention is for this to be measured on all tokens , including punctuation tokens and other unambiguous tokens . )English .", "label": "", "metadata": {}, "score": "75.01329"}
{"text": "Boston , Massachusetts .pp .49 - 56 .Nivre , Joakim and Mario Scholz .Deterministic Dependency Parsing of English Text .In Proceedings of COLING 2004 , Geneva , Switzerland , August 23 - 27 , 2004 .Wettschereck , Dietrich , David W. Aha and Takao Mohri .", "label": "", "metadata": {}, "score": "75.15047"}
{"text": "Furthermore , we report and analyze the results and describe the approaches of the participating systems . ... dependent relations rather than phrases , the task of the conversion procedure is to identify and label the head - dependent pairs .", "label": "", "metadata": {}, "score": "75.40105"}
{"text": "Computational Linguistics 21:543 - 565 .I 'm the RCA Professor of Artificial Intelligence in the Department of Computer and Information Science at the University of Pennsylvania , where I 'm also Professor of Linguistics .I received my Ph.D. in 1978 from the MIT Artificial Intelligence Lab , and was a Member of Technical Staff at AT&T Bell Laboratories before coming to Penn in 1987 .", "label": "", "metadata": {}, "score": "75.69595"}
{"text": "This family extends the partitioned logistic normal distribution , enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar , providing a new way to encode prior knowledge about an unknown grammar .We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors .", "label": "", "metadata": {}, "score": "75.73867"}
{"text": "Introduction We present a statistical parser that induces its grammar and probabilities from a hand - parsed corpus ( a tree - bank ) .Parsers induced from corpora are of interest both as simply exercises in machine learning and also because they are often the best parsers obtainable by any method .", "label": "", "metadata": {}, "score": "75.77212"}
{"text": "( 1996 )Jan Tent and France Mugler 25 .Treebank Grammars ( 1996 )Eugene Charniak 26 .English Corpus Linguistics and the Foreign Language Teaching Syllabus ( 1996 ) Dieter Mindt 27 .Data - oriented Language Processing : An Overview ( 1996 ) L.W.M. Bod and R.J.H. Scha 28 .", "label": "", "metadata": {}, "score": "75.96251"}
{"text": "We present an algorithm Orthant - Wise Limited - memory Quasi - Newton ( owlqn ) , based on l - bfgs , that can efficiently optimize the L1-regularized log - likelihood of log - linear models with millions of parameters .", "label": "", "metadata": {}, "score": "76.27377"}
{"text": "The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .", "label": "", "metadata": {}, "score": "76.289764"}
{"text": "The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .", "label": "", "metadata": {}, "score": "76.289764"}
{"text": "We find that sample selection can significantly reduce the size of annotated training corpora and that uncertainty is a robust predictive criterion that can be easily applied to different learning models . ource sentence is a short one , the decoder will never be able to find it , for the hypotheses leading to it have been pruned permanently .", "label": "", "metadata": {}, "score": "76.44832"}
{"text": "We find that sample selection can significantly reduce the size of annotated training corpora and that uncertainty is a robust predictive criterion that can be easily applied to different learning models . ource sentence is a short one , the decoder will never be able to find it , for the hypotheses leading to it have been pruned permanently .", "label": "", "metadata": {}, "score": "76.44832"}
{"text": "In addition , many words are rare , so parameter estimation is unreliable because of sparsity of the data .Since many words only appear rarely and most words appear overwhelmingly with one tag , we should devote more attention to predicting tags for the common and difficult to tag words .", "label": "", "metadata": {}, "score": "76.58856"}
{"text": "The l - bfgs limited - memory quasi - Newton method is the algorithm of choice for optimizing the parameters of large - scale log - linear models with L2 regularization , but it can not be used for an L1-regularized loss due to its non - differentiability whenever some parameter is zero .", "label": "", "metadata": {}, "score": "76.671524"}
{"text": "Although the model does not achieve state - of - the - art accuracy ( 96.4 - 96.8 % ) , it comes respectably close ( 96.2 % ) .Introduction .Part - of - speech tagging consists of labeling each word in a sentence by its appropriate part of speech , e.g. verb , noun , adjective , adverb .", "label": "", "metadata": {}, "score": "76.77253"}
{"text": "Product Details .Related Subjects .Table of Contents .Introduction 2 .From The Sstructure of English ( 1952 )Charles Carpenter Fries 3 .A Standard Corpus of Edited Present -day American English ( 1965 ) W. Nelson Francis 4 .", "label": "", "metadata": {}, "score": "76.84506"}
{"text": "Top words according to such a criterion are the ones that are commonly reported as difficult : that , about , up , 's , etc . .Learning .In addition , there are usually many context - specific independencies in the conditional probability distribution ( cpd ) , e.g. given that the next tag is comma , it does not matter what the tag after the next tag is .", "label": "", "metadata": {}, "score": "76.972855"}
{"text": "Gill Francis 19 .Structural Ambiguity and Lexical Relations ( 1993 ) Donald Hindle and Mats Rooth 20 .Irony in the Text or Insincerity in the Writer ?The Diagnostic Potential of Semantic Prosodies ( 1993 )William Louw 21 .", "label": "", "metadata": {}, "score": "77.0033"}
{"text": "This level of performance , although not quite state - of - the - art , is quite reasonable .Some words are very difficult to classify correctly , perhaps due to the limited context window and linguistic depth of this model and other current state - of - the - art models .", "label": "", "metadata": {}, "score": "77.39816"}
{"text": "Eraall : brill@cs.jhu.edu .Word sense disambiguation , a problem which once seemed out of reach for systems without a great deal of hand cr ... . \" ...We describe a parsing system based upon a language model for English that is , in turn , based upon assigning probabilities to possible parses for a sentence .", "label": "", "metadata": {}, "score": "78.06087"}
{"text": "Even with second - order features or latent variables , which would make exact parsing considerably slower or NP - hard , BP needs only O(n3 ) time with a small constant factor .Furthermore , such features significantly improve parse accuracy over exact first - order methods .", "label": "", "metadata": {}, "score": "78.16979"}
{"text": "In particular , the suffix of a word is often a good predictor ( e.g. -tion , -ed , -ly , -ing ) .Capitalization and whether the word comes after a period or quotation marks are also indicative .In addition , numbers are rarely seen in the training data , but often can be easily classified as such ( note that numbers can also act as list markers ) .", "label": "", "metadata": {}, "score": "78.96063"}
{"text": "Iwould like toacknowledge the following people for their contribution to my education : I thank my advisor Mitch Marcus , who gave me the intellectual freedom to pursue what I believed to be the best way to approach natural language processing , and also gave me direction when necessary .", "label": "", "metadata": {}, "score": "79.117035"}
{"text": "I am currently the principal investigator for an ARO - funded MURI project to investigate natural language understanding for human - robot interaction with co - PIs at Stanford , Cornell , UMass Amherst , UMass Lowell and George Mason .My research interests include : statistical natural language processing , human - robot communication , and cognitively plausible models for automatic acquisition of linguistic structure .", "label": "", "metadata": {}, "score": "79.123764"}
{"text": "Appendix B gives the syntactic categories and functional labels of TueBa - D / S. CRITICAL EVALUATION .The book by Sandra K\u00fcbler is an important contribution to the area of syntactic parsing in several respects .First , this is the monograph 's main point - a memory - based robust parser for German spontaneous speech .", "label": "", "metadata": {}, "score": "79.189445"}
{"text": "This paper presents WOE , an open IE system which improves dramatically on TextRunner 's precision and recall .The key to WOE 's performance is a novel form of self - supervised learning for open extractors - using heuristic matches between Wikipedia infobox attribute values and corresponding sentences to construct training data .", "label": "", "metadata": {}, "score": "79.48543"}
{"text": "The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .", "label": "", "metadata": {}, "score": "79.64874"}
{"text": "The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .", "label": "", "metadata": {}, "score": "79.64874"}
{"text": "I thank all of my thesis committee members : John La erty from Carnegie Mellon University , Aravind Joshi , Lyle Ungar , and Mark Liberman , for their extremely valuable suggestions and comments about my thesis research .I thank Mike Collins , Jason Eisner , and Dan Melamed , with whom I 've had many stimulating and impromptu discussions in the LINC lab .", "label": "", "metadata": {}, "score": "81.20126"}
{"text": "Current Projects .Selected Publications .Qiuye Zhao and Mitch Marcus .Long tail distributions and Unsupervised learning of Morphology , Coling 2012 .Qiuye Zhao and Mitch Marcus .Exploring Deterministic Constraints : from a Constrained English POS Tagger to an Efficient ILP Solution to Chinese Word Segmentation , Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics ( ACL ) , pages 1054 - -1062 , 2012 .", "label": "", "metadata": {}, "score": "81.79144"}
{"text": "John Sinclair 9 .Cleft and Pseudo - cleft Constructions in English Spoken and Written Discourse ( 1987 )Peter C. Collins 10 .What is Wrong with Adding One ?( 1989 )William Gale and Kenneth Church 11 .A Statistical Approach to Machine Translation ( 1990 )", "label": "", "metadata": {}, "score": "82.40905"}
{"text": "Peter Heeman was born October 22 , 1963 , and much to his dismay his parents had already moved away from Toronto .Instead he was born in London Ontario , where he grew up on a strawberry farm .He attended the University of Waterloo where he re - ceived a Bachelors of Mathematics with a joint degree in Pu ... \" .", "label": "", "metadata": {}, "score": "82.915634"}
{"text": "Peter Heeman was born October 22 , 1963 , and much to his dismay his parents had already moved away from Toronto .Instead he was born in London Ontario , where he grew up on a strawberry farm .He attended the University of Waterloo where he re - ceived a Bachelors of Mathematics with a joint degree in Pu ... \" .", "label": "", "metadata": {}, "score": "82.915634"}
{"text": "As a parsing algorithm , BP is both asymptotically and empirically efficient .E ... \" .We formulate dependency parsing as a graphical model with the novel ingredient of global constraints .We show how to apply loopy belief propagation ( BP ) , a simple and effective tool for approximate learning and inference .", "label": "", "metadata": {}, "score": "84.150696"}
{"text": "12 .A Point of Verb Syntax in South - western British English : An Analysis of a Dialect Continuum ( 1991 )Ossi Ihalainen 13 .Using Corpus Data in the Swedish Academy Grammar ( 1991 )Staffan Hellberg 14 .", "label": "", "metadata": {}, "score": "84.60028"}
{"text": "Springer .S\u00f8gaard , Anders .Semi - supervised condensed nearest neighbor for part - of - speech tagging .The 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies ( ACL - HLT ) .Portland , Oregon .", "label": "", "metadata": {}, "score": "85.691666"}
{"text": "Brief introduction to MBL is given and the place of ML in Natural Language Processing ( NLP ) is also mentioned .Chapter 2 gives the reader a more extensive introduction to MBL .The algorithms extending the basic model in order to achieve faster retrieval , reduction of storage requirements and tolerance to noise are given in section 2.2 together with results of their performance .", "label": "", "metadata": {}, "score": "85.81594"}
{"text": "Overview .A corpus is a collection of specimens of a language as used in real life , in writing and/or speech .Corpus lingustics is research , carried out in university linguistics departments and computing departments ( and nowadays in industrial research labs too ) , which uses corpora as crucial sources of evidence on the structure and properties of languages .", "label": "", "metadata": {}, "score": "87.032814"}
{"text": "WOE can operate in two modes : when restricted to POS tag features , it runs as quickly as TextRunner , but when set to use dependency - parse features its precision and recall rise even higher . ... h recall .", "label": "", "metadata": {}, "score": "87.57735"}
{"text": "Artificial Intelligence Review 11(1- 5 ) : 273 - 314 .ABOUT THE REVIEWER .I am a PhD student in Computational Linguistics at Gothenburg University as part of GSLT ( Swedish Graduate School of Language Technology ) .I am currently working on a thesis about data - driven approaches to syntactic parsing of Bulgarian .", "label": "", "metadata": {}, "score": "90.47446"}
{"text": "Dependency Parsing of Turkish .Computational Linguistics 34(3 ) , 357 - 389 .Nivre , J. ( 2008 ) Algorithms for Deterministic Incremental Dependency Parsing .Computational Linguistics 34(4 ) , 513 - 553 .Hall , J. , Nilsson , J. and Nivre , J. ( 2010 ) Single Malt or Blended ?", "label": "", "metadata": {}, "score": "96.56216"}
{"text": "What better way to wipe the slate clear than by going to graduate school at the University of Toronto , but not without first spending the sum - mer in Europe .After spending two months in countries where he could n't speak the language , Peter became fascinated by language , and so decided to give computational linguistics a try .", "label": "", "metadata": {}, "score": "96.76368"}
{"text": "What better way to wipe the slate clear than by going to graduate school at the University of Toronto , but not without first spending the sum - mer in Europe .After spending two months in countries where he could n't speak the language , Peter became fascinated by language , and so decided to give computational linguistics a try . \" ...", "label": "", "metadata": {}, "score": "99.06271"}
{"text": "Corpus lingustics is research , carried out in university linguistics departments and computing departments ( and nowadays in industrial research labs too ) , which uses corpora as crucial sources of evidence on the structure and properties of languages .Because corpus .", "label": "", "metadata": {}, "score": "99.768875"}
{"text": "Svetoslav Marinov , Gothenburg University / University College Skoevde , Sweden .SYNOPSIS .The first chapter ( Introduction ) sets up the stage for the two central topics in this monograph -- parsing and machine - learning .The notion of syntactic parsing is explained and the basic distinction between data - driven and discrete approaches to parsing is given .", "label": "", "metadata": {}, "score": "100.7952"}
{"text": "University of Tilburg , The Netherlands .Daelemans , Walter , Jakub Zavrel , Ko van der Sloot and Antal van den Bosch .TiMBL : Tilburg Memory Based Learner , version 5.0 , Reference Guide . 'ILK Research Group Technical Report Series no . 03 - 10 , 56 pages .", "label": "", "metadata": {}, "score": "107.30482"}
