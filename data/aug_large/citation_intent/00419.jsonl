{"text": "This paper proposes a system for text - dependent writer identification based on Arabic handwriting .First , a database of words was assembled and used as a test base .Next , features vectors were extracted from writers ' word images .", "label": "", "metadata": {}, "score": "21.470346"}
{"text": "These classes are then utilized to retrieve HAH manuscripts .This technique is robust to different styles and font sizes .A word - level recognition system for machine - printed Arabic text has been reported in [ 19 ] .The Arabic recognition system has computed a vector of image - morphological features on a query word image .", "label": "", "metadata": {}, "score": "23.826664"}
{"text": "This research aims to provide a new algorithm , that returns roots of Arabic words using n - gram technique without using morphological rules in order to avoid the complexity arising from the morphological richness of the language in one hand and the multiplicity of morphological rules in other hand .", "label": "", "metadata": {}, "score": "24.262207"}
{"text": "The selection of a suitable dataset is a critical step for successfully comparing results .To our knowledge , there is no existing large database with a good collection of Arabic handwriting documents specifically designed for writer identification research and application .", "label": "", "metadata": {}, "score": "26.556587"}
{"text": "In this paper , novel features for writers identification were contributed .The performance of the new edge - based directional probability distributions and other features in Arabic writer identification was evaluated .The recognition rate for top ten writers is greater than 90 % for certain words .", "label": "", "metadata": {}, "score": "27.65775"}
{"text": "In this work , we studied the feature extraction and recognition operations of Arabic text on the identification rate of writers .Because there is no well - known database containing Arabic handwritten words for researchers to test , we have built a new database of offline Arabic handwriting text to be used by the writer identification research community .", "label": "", "metadata": {}, "score": "29.068401"}
{"text": "Comparison of the final results obtained in this study with other research is difficult because of differences in experimental details , the actual handwriting used , the method of data collection , and the use of real Arabic offline handwritten words .", "label": "", "metadata": {}, "score": "29.409906"}
{"text": "The results obtained showed that the algorithm extracts the correct roots with an accuracy rate up to 95 % . \"Weights and ranks are assigned to letters using a little bit information on language ( Al - Serhan et al . , 2003 ) .", "label": "", "metadata": {}, "score": "29.419792"}
{"text": "First , the HAH manuscript 's image is segmented into words , and then each word is segmented into its connected parts .Second , several structural and statistical features which are devised for Arabic texts are extracted from these connected parts and combined to represent a word with one consolidated feature vector .", "label": "", "metadata": {}, "score": "29.523254"}
{"text": "Vectors from the database with the highest match score are returned as hypotheses for the unknown image .In the proposed AHTR ( Arabic handwritten text recognition ) system [ 20 ] , smoothing and segmentation processes have been used to segment the subwords of text into characters .", "label": "", "metadata": {}, "score": "29.588436"}
{"text": "Z. A. Aghbari and S. Brook , \" HAH manuscripts : a holistic paradigm for classifying and retrieving historical Arabic handwritten documents , \" Expert Systems with Applications , vol .36 , no . 8 , pp .10942 - 10951 , 2009 .", "label": "", "metadata": {}, "score": "29.973045"}
{"text": "Abstract : Text Categorization ( classification ) is the process of classifying documents into a predefined set of categories based on their content .In this paper , an intelligent Arabic text categorization system is presented .Machine learning algorithms are used in this system .", "label": "", "metadata": {}, "score": "30.596148"}
{"text": "Abstract : Text Categorization ( classification ) is the process of classifying documents into a predefined set of categories based on their content .In this paper , an intelligent Arabic text categorization system is presented .Machine learning algorithms are used in this system .", "label": "", "metadata": {}, "score": "30.596148"}
{"text": "( 2006 ) proposed a key phrase extraction approach based on neural networks .They adopted the following features to extract key phrases from given documents : term frequency and inverted document frequency respectively , and frequency of appearance in the paragraphs of the documents .", "label": "", "metadata": {}, "score": "30.943714"}
{"text": "In this paper , we address the problems of Arabic Text Classification and root extraction using transducers and rational kernels .We introduce a new root extraction approach on the basis of the use of Arabic patterns ( Pattern Based Stemmer ) .", "label": "", "metadata": {}, "score": "30.980938"}
{"text": "In this section , we explain the key phrase extraction technique based on the key phrase extraction algorithm ( Witten et al . , 1999 ) that is used for this study .This algorithm was chosen because it is an open source system written in Java and provides the flexibility of customising the procedure .", "label": "", "metadata": {}, "score": "31.176702"}
{"text": "Albadr and Haralick have described the design of an Arabic word recognition system that does not segment word into characters [ 1 ] .This system recognizes the input word image by detecting a set of shape primitives on the word as features for describing the recognized word .", "label": "", "metadata": {}, "score": "31.207783"}
{"text": "Hence , we used it for our key phrase extraction experiment .There are some studies which added new features to the existing techniques .Barker et al .( 2000 ) presented a system for choosing noun phrases from a document as key phrases .", "label": "", "metadata": {}, "score": "32.425232"}
{"text": "Figure 1 shows how Arabic handwriting differs from writer to writer .Our goal in this work was to automate the process of writer identification using scanned images of handwriting and thus to provide a computerized analysis of individual handwriting .The task of writer identification is equivalent to answering one question : \" who wrote this sample ? \"", "label": "", "metadata": {}, "score": "33.360474"}
{"text": "Using transducers for extracting roots , documents are transformed into finite state transducers .This document representation allows us to use and explore rational kernels as a framework for Arabic Text Classification .Root extraction experiments are conducted on three word collections and yield 75.6 % of accuracy .", "label": "", "metadata": {}, "score": "33.424583"}
{"text": "( 2007 ) extended the algorithm for extracting key phrases from scientific publications .They added two more features to the algorithm : the positions of phrases in documents and morphological factors found in scientific key phrases , such as whether a candidate key phrase is an acronym or uses terminologically productive suffixes .", "label": "", "metadata": {}, "score": "33.605103"}
{"text": "We employ the well - known key phrase extraction technique called the key phrase extraction algorithm for our study .In particular , we extract key phrases from three different datasets : 1 ) papers in the same journal , 2 ) papers from different journals in the same field , and 3 ) papers from journals in different fields .", "label": "", "metadata": {}, "score": "33.70936"}
{"text": "( 2008 ) proposed a key phrase identification program algorithm which extracts key phrases based on the composition of noun phrases .In this algorithm , the more keywords a phrase contains , the more likely this phrase is to be a key phrase .", "label": "", "metadata": {}, "score": "34.41412"}
{"text": "The algorithm produced high - quality dictionaries for several semantic categories .How would you handle automatic text classification in noisy conditions ?This is what has been done , to my knowledge , in Automatic web Genre Idintefication ( AGI ) .", "label": "", "metadata": {}, "score": "34.58232"}
{"text": "Edge Detection .The Sobel edge detection technique was used in this work .This method gives more accurate results for Arabic handwriting .Feature Extraction .The edge - direction distribution [ 19 ] , moment invariants , and word measurements feature extraction methods were used in this study .", "label": "", "metadata": {}, "score": "34.62764"}
{"text": "Then , Fourier descriptors of the subword contour have been used to classify the input subword images into the members of these 10 clusters .Postprocessing has been done using the dot features of the subwords to recognize correct subword .Ebrahimi and Kabir have reported on the use of characteristic loci features to cluster 113,340 printed Farsi subwords of 4 fonts and 3 sizes to 300 clusters , based on their holistic shapes [ 14 ] .", "label": "", "metadata": {}, "score": "34.629547"}
{"text": "Based on these qualitative observations , they identified that the indexer 's subjectivity and inconsistency could be seen reflected in representing concepts in academic works .Mai ( 2001 ) manually examined several existing indexing methods of academic documents .Through exhaustive analysis to understand the indexer 's cognitive process , the study revealed that indexing is not a neutral and objective representation of a document 's subject matter but an interpretation for future use .", "label": "", "metadata": {}, "score": "34.692642"}
{"text": "The usual approach for automatic summarization is sentence extraction , where key sentences from the input documents are selected based on a suite of features .While word frequency often is used as a feature in summarization , its impact on system performance has not been isolated .", "label": "", "metadata": {}, "score": "34.8322"}
{"text": "The usual approach for automatic summarization is sentence extraction , where key sentences from the input documents are selected based on a suite of features .While word frequency often is used as a feature in summarization , its impact on system performance has not been isolated .", "label": "", "metadata": {}, "score": "34.8322"}
{"text": "859 - 866 , 2007 .View at Google Scholar \u00b7 View at Scopus .J. H. AlKhateeb , J. Jiang , J. Ren , F. Khelifi , and S. S. Ipson , \" Multiclass classification of unconstrained handwritten Arabic words using machine learning approaches , \" The Open Signal Processing Journal , vol .", "label": "", "metadata": {}, "score": "34.96123"}
{"text": "The most common phrases were counted manually from one thousand handwritten Arabic letters , while the most common words were taken from previous study [ 20 ] .The process of creating the data set proceeded as follows .( i )", "label": "", "metadata": {}, "score": "35.01496"}
{"text": "One can also categorize previous studies according to the number of writers and the nature of training samples used by the system [ 8 ] .On the one hand , the system is required to deal with as many writers as possible .", "label": "", "metadata": {}, "score": "35.345966"}
{"text": "The obtained dictionary is used to recognize the subwords in different text documents .The capabilities of this wavelet operator in detecting patterns with particular properties in the image are used appropriately to accomplish different essential tasks in a pattern recognition process such as holistic Persian subword recognition .", "label": "", "metadata": {}, "score": "35.670235"}
{"text": "This highlights the importance of our experiments to check whether the indexer effect really exists in automatic key phrase extraction or not .The three research hypotheses we set in this study are as follows : .In the same journal , the results of key phrase extraction based on indexer - provided terms are more effective than author - provided keywords , in terms of comparing the number of matched words .", "label": "", "metadata": {}, "score": "35.686237"}
{"text": "The second strategy , which owns the majority in the literature , segments each word to containing characters as the building blocks and recognizes each character then .In this work , the first approach is used .There are many works reported on the recognition of Arabic and Farsi texts [ 1 , 10 - 23 ] .", "label": "", "metadata": {}, "score": "35.844166"}
{"text": "( ii )Scanning the forms using the same scanner ( 300 dpi , millions of colors ) .( iii ) Cropping and naming each word image using a program that we created for cropping .Table 1 : Arabic words and phrases most likely to appear in handwritten letters .", "label": "", "metadata": {}, "score": "35.901157"}
{"text": "At the same time , it should be noted that information users might not prefer those results , which may bring them to irrelavant resources in the information retrieval process .Secondly , we proved that the automatic key phrase extraction performance with indexer - assigned terms is more efficient than with those of authors .", "label": "", "metadata": {}, "score": "35.92176"}
{"text": "The ability to correctly classify sentences that describe events is an important task for many natural language applications such as Question Answering ( QA ) and Text Summarisation .In this paper , we treat event detection as a sentence level text classification problem .", "label": "", "metadata": {}, "score": "36.076653"}
{"text": "This also means the possibility of displaying distinct results when automatically extracting words to describe documents , because it uses either keywords or index terms individually .We assume key phrase extraction performance from one particular viewpoint is more efficient than the other , which means selecting the right terms that the group intends to pick for descriptors .", "label": "", "metadata": {}, "score": "36.294327"}
{"text": "This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents .This is important because in many text classification problems obtaining training labels is expensive , while large qua ... \" .", "label": "", "metadata": {}, "score": "36.431183"}
{"text": "In this work , we describe the design and implementation of a Farsi subword recognition system .To recognize a subword , the system does not segment it into characters in advance ; rather , it recognizes the input subword using wavelet packet features as holistic shape information .", "label": "", "metadata": {}, "score": "36.48561"}
{"text": "Additionally , we used the average number of key phrases correctly extracted per document .We compared the extraction results of 5 , 10 , and 15 key phrases .The KEA package provides this evaluation metric to measure the effectiveness of automatic key phrase extraction systems with manually extracted ones , and we used them for our evaluation .", "label": "", "metadata": {}, "score": "36.929756"}
{"text": "Other possible applications involve the detection of the various handwritings present in a document or the dating of documents compared to the chronology of the author 's work .One can consider these two applications from the perspective of either textual or graphical information retrieval problems .", "label": "", "metadata": {}, "score": "37.13785"}
{"text": "However , a limitation when extracting key phrases can be experienced if an important term is not recognised as a noun .Turney ( 2002 ) presented a new feature , the concept of a coherence set to measure cohesiveness between candidate phrases .", "label": "", "metadata": {}, "score": "37.187424"}
{"text": "Next , the Extractor presents the top - ranked key phrases as outcomes to the user , while the Genitor algorithm searches for overlap among the key phrases .Witten et al .( 1999 ) suggested a competing key phrase extraction system called the key phrase extraction algorithm , which identifies candidate key phrases using lexical methods , calculates feature values for each candidate and uses a machine - learning algorithm to predict which candidates are proper key phrases .", "label": "", "metadata": {}, "score": "37.31069"}
{"text": "This paper explores novel approaches to taking advantage of cross - document IE for multi - document summarization .We propose multiple approaches to IE - based summarization and analyze their strengths and weaknesses .One of them , re - ranking the output of a high performing summarization system with IE - informed metrics , leads to improvements in both manually - evaluated content quality and readability .", "label": "", "metadata": {}, "score": "37.510708"}
{"text": "According to Ameur Bensefia et al .[ 10 ] , handwritten documents generally exhibit two kinds of use , corresponding with two different types of requests .( i ) Handwritten documents can be analyzed for their textual content .In this case , the query of a handwritten document database would require one to resort to a transcription phase of the handwritten texts prior to the indexing of their textual content using standard techniques dedicated to information retrieval .", "label": "", "metadata": {}, "score": "37.58839"}
{"text": "Most previous studies found that the majority of these features are relevant for classification , and that the performance of text categorization with support vector machines peaks when no feature selection is performed . \" ... Abstract .This paper proposes an ensemble method for multilabel classification .", "label": "", "metadata": {}, "score": "37.683662"}
{"text": "In this paper , we develop a novel cluster - sensitive graph model for query - oriented multi - document summarization .Upon it , an iterative algorithm , namely QoCsR , is built .As there is existence of natural clusters in the graph in the case that a document comprises a collection of sentences , ... \" .", "label": "", "metadata": {}, "score": "37.78704"}
{"text": "101 - 105 , Seattle , Wash , USA , 2001 .S. Al - Ma'adeed , D. Elliman , and C. A. Higgins , \" A data base for arabic handwritten text recognition research , \" in Proceedings of the 8th International Workshop on Frontiers in Handwriting Recognition ( IWFHR'02 ) , p. 485 , 2002 .", "label": "", "metadata": {}, "score": "37.818695"}
{"text": "In the recent years , wavelet analysis has been successfully applied in the field of pattern recognition .Wavelet descriptors of a character , as a set , can be used to replace the combination of many conventional features used in character recognition systems .", "label": "", "metadata": {}, "score": "38.14035"}
{"text": "We carry out our analysis using datasets from the Document Understanding Conferences , studying not only the impact of these features on automatic summarizers , but also their role in human summarization .Our research shows that a frequency based summarizer can achieve performance comparable to that of state - of - the - art systems , but only with a good composition function ; context sensitivity improves performance and significantly reduces repetition . .", "label": "", "metadata": {}, "score": "38.174965"}
{"text": "We also demonstrate how a frequency - based summarizer can incorporate con - text adjustment in a natural way , and show that this adjustment contributes to the good performance of the summarizer and is sufficient means for duplica - tion removal in multi - document summarization .", "label": "", "metadata": {}, "score": "38.280403"}
{"text": "The result with 2011 data reveals that the performance from the indexers ' intellectual base is better than that from the authors ' but the cases in 2010 turn out to be the opposite ( Table 7 ) .Automatic key phrase extraction based on keywords proved to be more precise in this case .", "label": "", "metadata": {}, "score": "38.340023"}
{"text": "A 24-dimensional feature vector representing the relative energy and zero crossing was extracted from the output signal of the DWT .The recognition of each character is continued with classifying a set of characters using these features .In [ 21 ] , five various methods have been used to extract the Arabic word image features .", "label": "", "metadata": {}, "score": "38.369793"}
{"text": "We analysed extracted key phrases in exact ( 100 % ) and fair ( 70 % ) matching by the average number of key phrases extracted correctly per document .To verify the research hypotheses addressed , we selected a variety of journals in the same field and different fields .", "label": "", "metadata": {}, "score": "38.38427"}
{"text": "Previous works on key phrase generation can be categorised into two major approaches : key phrase extraction and key phrase assignment .In key phrase extraction , phrases in a document are identified on the basis of properties such as term frequency and length . key phrase assignment , however , aims at finding appropriate key phrases based on a pre - defined set of terms such as a thesaurus .", "label": "", "metadata": {}, "score": "38.47769"}
{"text": "Analysis .We analyse those two groups of extracted key phrases in terms of exact ( 100 % ) and fair ( 70 % ) matching , which is based on the average number of key phrases extracted correctly per document .", "label": "", "metadata": {}, "score": "38.53592"}
{"text": "The length of a candidate phrase measured in words boosts the probability of two - word candidates being key phrases .The statistical analysis of the experimental data revealed that indexers prefer to assign descriptors consisting of two words whereas most terms in the corresponding thesaurus are one word ( Medelyan et al . , 2008 ) .", "label": "", "metadata": {}, "score": "38.571735"}
{"text": "For example , the author assigns experts and novices to this document , which is very similar to expert users and novice users used by the indexer .In addition , new words are offered from only one viewpoint .For instance , the author deems Web databases and gender differences as important terms , which are not included in the index list .", "label": "", "metadata": {}, "score": "38.617867"}
{"text": "57 - 62 , 2005 .View at Google Scholar .M. S. Jelodar , M. J. Fadaeieslam , N. Mozayani , and M. Fazeli , \" A Persian OCR system using morphological operators , \" in Proceedings of the 2nd World Enformatika Conference ( WEC ' 05 ) , pp .", "label": "", "metadata": {}, "score": "38.69654"}
{"text": "A pictorial dictionary that can be used in a word recognition system to eliminate the search space is yielded .The mean of each cluster is used as its entry in the pictorial dictionary .A method for recognition of Persian characters in machine printed documents has been developed based on the morphological hit / miss transformation [ 15 ] .", "label": "", "metadata": {}, "score": "38.80342"}
{"text": "In Srihari et al .[ 18 ] , eleven global macro ( document level ) and microfeatures ( character level ) are employed ; however , the performance based on macrofeatures extracted from words was very low .Features used for the writer identification task mainly include global features based on statistical measurements extracted from the entire block of text to be identified .", "label": "", "metadata": {}, "score": "39.11023"}
{"text": "They used loci characteristic features , Fourier descriptors , and upper contour labels of printed Persian subwords to construct three pictorial dictionaries .In each method based on its description the entry for that subword is defined .In [ 13 ] , a two - step method for the recognition of printed Farsi subwords has been proposed .", "label": "", "metadata": {}, "score": "39.18407"}
{"text": "This implies that the retrieval results will be different between a keyword - based and an index - term - based search ; information users might retrieve less relevant documents from the latter .We should also note that authors tend to list the exact words they use in their articles for keywords , so the exact matching rate from authors is higher than that from indexers .", "label": "", "metadata": {}, "score": "39.26065"}
{"text": "Menhaj and Adab have proposed a new segmentation and recognition method for multifont , multisize Farsi / Arabic texts using multilayer feed forward neural networks [ 17 ] .The OCR recognition method has used MLP type neural networks with Fourier descriptors .", "label": "", "metadata": {}, "score": "39.378098"}
{"text": "The task of automatic handwriting analysis falls into the writer identification paradigm .The identification of the writer based on a piece of handwriting is a challenging task for pattern recognition .Automatic handwriting analysis techniques allow us to consider specific applications , as described in the following papers .", "label": "", "metadata": {}, "score": "39.398605"}
{"text": "It can then automatically re - label sample pages and use them as input to the automatic re - learning of the grammatical classifier , by using any of existing methods for wrapper induction .If instead the recovery is incomplete and some strings in the page remained unlabeled , no trusted samples can be prepared for automatic re - learning and therefore the wrapper repairing can not be successful .", "label": "", "metadata": {}, "score": "39.627354"}
{"text": "The results of the experiments show that key phrase extraction based on indexer - assigned terms , which are brought from a professional index database , performs better in general than those based on author - provided keywords .This implies that the concept of the indexer effect does exist in key phrase extraction .", "label": "", "metadata": {}, "score": "39.636707"}
{"text": "The obtained feature vectors yield a pictorial dictionary for which an entry is the mean of each group that consists of the same subword with 4 fonts in 3 sizes .The sets of these features are congregated by combining them with the dot features for the recognition of printed Persian subwords .", "label": "", "metadata": {}, "score": "39.720318"}
{"text": "M. Omidyeganeh , K. Nayeb , R. Azmi , and A. Javadtalab , \" A new segmentation technique for multi font farsi / arabic texts , \" in Proceedings of the IEEE International Conference on Acoustics , Speech , and Signal Processing ( ICASSP ' 05 ) , vol .", "label": "", "metadata": {}, "score": "39.77018"}
{"text": "These results show that our approach , when compared with other approaches , is promising specially in terms of accuracy and F1 .\" Each Arabic word is formed from the root word and a suffix , a prefix or an infix .", "label": "", "metadata": {}, "score": "39.813023"}
{"text": "El - Beltagy , S. R. & Rafea , A. ( 2009 ) .KP - Miner : a key phrase extraction system for English and Arabic documents .Information Systems , 34 ( 1 ) , 132 - 144 .Healey , P. , Rothman , H. & Hoch , P. ( 1986 ) .", "label": "", "metadata": {}, "score": "39.832085"}
{"text": "The work suggested in [ 11 ] , for example , makes it possible to identify 95 % of the 40 writers that the system can handle through the analysis of text lines of handwriting .The work presented in [ 14 ] reports a correct writer identification performance of 92.48 % among 50 writers using 45 samples of the same word that the participants were asked to write .", "label": "", "metadata": {}, "score": "39.93618"}
{"text": "Considering the different situations these two groups face , their intellectual base might also be different , which leads to using different terms for description , even in the same document .We have already confirmed this by comparing the word list for an article based on each point of view above .", "label": "", "metadata": {}, "score": "39.9809"}
{"text": "It is used to adjust the subword size to a certain standard .Since the sizes of Persian subwords greatly vary , size normalization is often used to scale subwords to a fixed size and to center the subword before feature extraction .", "label": "", "metadata": {}, "score": "40.029182"}
{"text": "757 - 760 , March 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .M. Sarfraz , S. Nawaz , and A. Al - Khuraidly , \" Offline arabic text recognition system , \" in Proceedings of the International Conference on Geometric Modeling and Graphics ( GMAG ' 03 ) , pp .", "label": "", "metadata": {}, "score": "40.40198"}
{"text": "This paper investigates a closely related problem , which leads to a novel approach to semi - supervised learning .Specifically we consider learning predictive structures on hypothesis spaces ( that is , what kind of classifiers have good predictive power ) from multiple learning tasks .", "label": "", "metadata": {}, "score": "40.623703"}
{"text": "Since most of news articles report several events and these events are referred in many related documents , we propose an event - based approach to visualize documents as graph on different conceptual granularities .With graphbased ranking algorithm , we illustrate the application of document graph to multi - document summarization .", "label": "", "metadata": {}, "score": "40.72264"}
{"text": "In this paper , we develop a novel cluster - sensitive graph model for query - oriented multi - document summarization .Upon it , an iterative algorithm , namely QoCsR , is built .In our model , five kinds of relations are involved among the three objects , i.e. document , sentence and query .", "label": "", "metadata": {}, "score": "40.813"}
{"text": "This new feature is calculated as the statistical association between a candidate and a small group of top candidate phrases .Other innovative algorithms to extract key phrases also have been devised .Their features and approaches usually prove to be efficient and effective .", "label": "", "metadata": {}, "score": "41.009895"}
{"text": "Considered within the general context of biometrics , automatic writer identification and verification is presently a thriving research topic .Writer identification is used in forensic and biometric applications , in which the writer of a document can be identified based on handwriting samples .", "label": "", "metadata": {}, "score": "41.037964"}
{"text": "2660 of Proceedings of SPIE , pp .63 - 70 , March 1996 .View at Scopus .I. A. Jannoud , \" Automatic Arabic hand written text recognition system , \" American Journal of Applied Sciences , vol .4 , no .", "label": "", "metadata": {}, "score": "41.054268"}
{"text": "We are also interested in using other kinds of data sets for supervised learning to confirm the indexer effect in automatic key phrase extraction .In addition , future work will focus more on adding new features to the algorithm and conducting experiments with other training data .", "label": "", "metadata": {}, "score": "41.057545"}
{"text": "N. Ben Amor and N. E. Ben Amara , \" Multifontarabic characters recognition using hough transform and HMM / ANN classification , \" Journal of Multimedia , vol .1 , no . 2 , pp .50 - 54 , 2006 .", "label": "", "metadata": {}, "score": "41.092587"}
{"text": "We conclude that automatic key phrase extraction based on index terms performs better than its counterpart based on author - provided keywords in most cases .However , it also reveals that indexers tend to assign terms more inconsistently .Conclusions .", "label": "", "metadata": {}, "score": "41.095627"}
{"text": "We also suggest the possibility of using index terms as training data especially in the situation where keywords are not provided in the articles .Table 1 gives an example of an article with keywords and index terms .We extracted keywords from the article , and used index terms from Library and Information Science Abstracts .", "label": "", "metadata": {}, "score": "41.207508"}
{"text": "Traditionally in text categorization , the same scoring or ranking criterion is adopted for all target dimensionalities , which considers both the discriminability and the coverage of a term , such as \u03c7 2 or IG .In this paper , the poor accuracy at a low dimensionality is imputed to the small average vector length of the documents .", "label": "", "metadata": {}, "score": "41.227158"}
{"text": "This is because individuals interpret documents differently when they read , thus the words that authors provide sometimes differ from those indexers choose for the index .This also implies that the results of automatic key phrase extraction based on index terms might be quite distinct from those based on keywords .", "label": "", "metadata": {}, "score": "41.260933"}
{"text": "It is also worth noting that important terms in articles are usually comprised of more than one word , emphasising the contextual meaning in the reading .Healey et al .( 1986 ) pointed out a possible indexer effect in choosing words for documents properly .", "label": "", "metadata": {}, "score": "41.3006"}
{"text": "We employed several distinct features for each page : bag - of - words , page structure , abstract , titles , and entity mentions .We report high accuracies for several of the classifiers built .As a result of this work , a Web service that classifies any Wikipedia page has been made available to the academic community . by J. Scott Olsson - Proc . the 15th ACM international conference on Information and knowledge management , 2006 . \" ...", "label": "", "metadata": {}, "score": "41.39547"}
{"text": "In this paper we investigate the machine learning method for extracting ... \" .Automatic document summarization is a problem of creating a document surrogate that adequately represents the full document content .We aim at a summarization system that can replicate the quality of summaries created by humans .", "label": "", "metadata": {}, "score": "41.41758"}
{"text": "For this method , whole pages of handwritten text are required .Similarly , in [ 12 , 13 ] , a system for writer verification is described .This system takes two pages of handwritten text as input and determines whether they were produced by the same writer .", "label": "", "metadata": {}, "score": "41.430893"}
{"text": "Abstract .The ability to correctly classify sentences that describe events is an important task for many natural language applications such as Question Answering ( QA ) and Text Summarisation .In this paper , we treat event detection as a sentence level text classification problem .", "label": "", "metadata": {}, "score": "41.532486"}
{"text": "Since most of news articles report several events and these events are referred in many related documents , we propose an event - based approach to visualize documents as graph on different conceptual granularities .With graphbased ranking algorithm , we illustrate the application of document ... \" .", "label": "", "metadata": {}, "score": "41.812286"}
{"text": "First , in the same journal , the results of key phrase extraction based on terms that an indexer assigns would be more effective than author - provided keywords in terms of the number of matched words .Second , in different journals in the same field , the results of key phrase extraction based on indexer - provided terms would be more effective than author - provided keywords in terms of matched words .", "label": "", "metadata": {}, "score": "41.82978"}
{"text": "It is a contradictory finding that even though automatic key phrase extraction with index terms performs better than with keywords , the consistency in giving representative words of the author group is higher than the indexer group .In other words , indexers have greater variance when assigning terms to articles than authors do because indexers use a larger number of terms .", "label": "", "metadata": {}, "score": "41.83104"}
{"text": "In particular , we explore how the Support Vector Machines ( SVM ) learning method is affected by the quality of linguistic analyses and the corresponding semantic graph representations .We apply two types of linguistic analysis : ( 1 ) a simple part - of - speech tagging of noun phrases and verbs and ( 2 ) full logical form analysis which identifies Subject - Predicate - Object triples , and then build the semantic graphs .", "label": "", "metadata": {}, "score": "42.22119"}
{"text": "S. N. Srihari , S.-H. Cha , H. Arora , and S. Lee , \" Individuality of handwriting , \" Journal of Forensic Sciences , vol .47 , no .4 , pp . 1 - 17 , 2002 .View at Google Scholar \u00b7 View at Scopus . U.", "label": "", "metadata": {}, "score": "42.247623"}
{"text": "After that , machine learning is used to generate a classifier that determines which candidates should be chosen as key phrases .Figure 1 describes the general procedure of the key phrase extraction system based on supervised learning .Candidate identification .", "label": "", "metadata": {}, "score": "42.34493"}
{"text": "View at Google Scholar . H. E. S. Said , T. N. Tan , and K. D. Baker , \" Personal identification based on handwriting , \" Pattern Recognition , vol .33 , no . 1 , pp .149 - 160 , 2000 .", "label": "", "metadata": {}, "score": "42.42933"}
{"text": "Classifiers built with content features classify strings of a file by analyzing their content .Multiple classifiers are used to recover information extraction when a wrapper runs in error .Having a number of alternative classifiers is beneficial in the case of concept shift .", "label": "", "metadata": {}, "score": "42.439606"}
{"text": "In [ 11 ] , subword features have been extracted from upper contour of each subword image .These features are given into a clustering stage to construct a dictionary .The subwords with same features or entry constitute their own neighbors within the dictionary .", "label": "", "metadata": {}, "score": "42.45743"}
{"text": "Subwords are separated by short spaces , and each subword includes one or more characters [ 6 ] .Figure 1 shows a sample Persian script where \" a \" represents the space between two different words and \" b \" represents the short space between subwords .", "label": "", "metadata": {}, "score": "42.49311"}
{"text": "Different dimensionalities are expected under different practical resource restrictions of time or space .Traditionally in text categorization , the same scoring or r ... \" .In text categorization , term selection is an important step for the sake of both categorization accuracy and computational efficiency .", "label": "", "metadata": {}, "score": "42.553726"}
{"text": "Thus , in this work 87804 subwords of 4 fonts and 3 sizes are selected for feature extraction to build pictorial dictionary .Some samples of subword images with various fonts and sizes in the database are shown in Figure 3 .", "label": "", "metadata": {}, "score": "42.641224"}
{"text": "In this work , we use wavelet packet transform with symlet 8 as basis function to decompose subword image into following subbands and extract features from subband . of level two .These features are used as subword shape descriptors in a pictorial dictionary and are used in recognition system .", "label": "", "metadata": {}, "score": "42.843155"}
{"text": "Ameur Bensefia et al .[ 10 ] have presented two complementary approaches to writer recognition .They have adapted and applied an information retrieval approach to handwritten documents that has traditionally been used on electronic documents .In addition , they have proposed a hypothesis test that allows for the verification of the compatibility between the handwritings of two different documents .", "label": "", "metadata": {}, "score": "42.86611"}
{"text": "As input , our technique require ... \" .Information extraction systems usually require two dictionaries : a semantic lexicon and a dictionary of extraction patterns for the domain .We present a multilevel bootstrapping algorithm that generates both the semantic lexicon and extraction patterns simultaneously .", "label": "", "metadata": {}, "score": "42.968063"}
{"text": "First , we obtain the letter 's skeleton and then identify the number of isolated parts of the letters by utilizing connected component analysis and location of dots [ 39 ] .After identifying all dots in subword , a code is assigned to it .", "label": "", "metadata": {}, "score": "43.06209"}
{"text": "The average number over five experiments of samples needed to learn 98%-accurate wrappers is reported in the Table 2 below .For content classifiers , in each of five tests , a decision tree been learned from five randomly selected samples and tested against five remaining samples .", "label": "", "metadata": {}, "score": "43.449135"}
{"text": "Writer identification has a long history , perhaps dating to the origins of handwriting itself .For example , many textbooks and research papers have been published that describe the methodologies employed by forensic document examiners [ 4 - 7 ] .", "label": "", "metadata": {}, "score": "43.455715"}
{"text": "A number of different systems have been used in Europe and the United States for writer verification and identification , as described above .However , most of these systems have been implemented for handwriting using only Latin - derived alphabets , and these systems are increasingly becoming outdated .", "label": "", "metadata": {}, "score": "43.47329"}
{"text": "We explore a technique for extracting such tables from document collections that requires only a handful of training examples from users .These examples are used to generate extraction patterns , that in turn result in new tuples being extracted from the document collection .", "label": "", "metadata": {}, "score": "43.49943"}
{"text": "Providing proper terms about each document manually , however , is laborious and time - consuming , and it becomes a daunting task due to the rapid increase of information .Based on these empirical and qualitative observations , therefore , we seek to investigate 1 ) whether the indexer 's effect exists in the automatic key phrase extraction technique , and 2 ) whether the indexer 's subjectivity and inconsistency can be measured quantitatively .", "label": "", "metadata": {}, "score": "43.51358"}
{"text": "Experiments with the DUC 2002 and CAST datasets show that the SVM based extraction of sentences does not differ significantly for the simple and the sophisticated syntactic analysis .In both cases the graph attributes used in learning are essential for the classifier performance and the quality of extracted summaries . .", "label": "", "metadata": {}, "score": "43.591076"}
{"text": "The size and the proportion of this structured noise are not underpinned by any hypotheses .Their genre palette is supposed to represent all the genres on the web ( but their proportions seems to be arbitrary ) , and they build 20 individual subclassifiers and perform a binary classification , i.e. one class against the remaining 19 .", "label": "", "metadata": {}, "score": "43.702217"}
{"text": "This was done because any error from training data can influence the results of key phrases extraction .Hence , we checked if any different occurrence exists when changing the training set .For research hypothesis 2 and 3 , we kept using two sets of training data from the Journal of Information Science .", "label": "", "metadata": {}, "score": "43.731647"}
{"text": "Text categorization algorithms usually represent documents as bags of words and consequently have to deal with huge numbers of features .Most previous studies found that the majority of these features are relevant for classification , and that the performance of text categorization with support ... \" .", "label": "", "metadata": {}, "score": "43.747147"}
{"text": "E.g. , in Stubbe et al ( 2007 ) \" noise \" refers to orthographcal errors .10 Responses to \" AGI : Structured and Unstructured Noise \" .Then you train a classifier by dividing the corpus into a training set and test set .", "label": "", "metadata": {}, "score": "43.766518"}
{"text": "In journals from different fields , the results of key phrase extraction based on indexer - provided terms are more effective than author - provided keywords , in terms of comparing the number of matched words .Methods .Data collection .", "label": "", "metadata": {}, "score": "43.791775"}
{"text": "First , apply the decomposition algorithm to every single image in the available database .Then , for any subword , take the average of each group consisting of the same subword with special fonts and sizes .By now there are 7317 decomposition vectors , and one vector for each group consists of 12 different shapes for fixed subword .", "label": "", "metadata": {}, "score": "43.836884"}
{"text": "The same rule also applies to research hypothesis 3 .Using a journal in the field of sociology , which is considered less connected to information science than education , has a greater effect on the extraction performance with index terms because of its more general characteristics .", "label": "", "metadata": {}, "score": "43.86942"}
{"text": "59 , pp .29 - 43 , 2004 .View at Google Scholar . A. Ebrahimi and E. Kabir , \" A two step method for the recognition of printed sub - words , \" Iranian Journal of Electrical and Computer Engineering , vol .", "label": "", "metadata": {}, "score": "43.89267"}
{"text": ", 2009 ; Harmanani et al . , 2006 ; Momani and Faraj , 2007 ) , a rule - based approach was used .For instance , ( Harmanani et al . , 2006 ) proposed a method in which roots are extracted based on a set of language dependent rules that are interpreted by a rule engine .", "label": "", "metadata": {}, "score": "43.912872"}
{"text": "In this case , the supervised learning technique matches a greater number of human - offered words that refer to more general descriptors from the indexers ' perspective .We assume that this disparity comes from motivational variance from each group ; some authors may assign keywords for their article just because journals ask them to do so .", "label": "", "metadata": {}, "score": "44.009304"}
{"text": "S.-H. Cha and S. Srihari , \" Writer identification : statistical analysis and dichotomizer , \" in SSPR and SPR 2000 , F. J. Ferrie , et al . , Ed . , vol .1876 of Lecture Notes in Computer Science , pp .", "label": "", "metadata": {}, "score": "44.12794"}
{"text": "Under this framework , algorithms for structural learning will be proposed , and computational issues will be investigated .Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi - supervised learning setting ..1999 ) , there has also been criticism pointing out that this method may not behave well under some circumstances ( Zhang and Oles , 2000 ) .", "label": "", "metadata": {}, "score": "44.150097"}
{"text": "We used them for training as well as test data for research hypothesis 1 .Two sets of training were compiled : for the first set , we selected articles published from 2004 to 2010 as training data set 1 .We automatically extracted key phrases from articles issued in 2011 based on the model built .", "label": "", "metadata": {}, "score": "44.153778"}
{"text": "The greater the number of correctly extracted words those two different groups share , the higher the hit rate that machine - extracted phrases show .Figure 2 .The performance graph with the Journal of Information Science .To test the statistical significance of the two different perspectives , we conducted a t - test .", "label": "", "metadata": {}, "score": "45.009014"}
{"text": "The method may further include classifying the extracted strings based on content features of the labeled extracted strings from the forward direction ; and validating those labeled extracted strings which satisfy the label rules within some first threshold value .The method of information extraction can be used to build alternative and redundant views of provider pages , using content features of extracted information .", "label": "", "metadata": {}, "score": "45.031403"}
{"text": "The objective was to identify the writer of one , or several , lines of handwritten text .Related Work .The identification of persons based on biometric measurements is currently a very active area of research [ 1 - 3 ] .", "label": "", "metadata": {}, "score": "45.091904"}
{"text": "Second , you must build a training set that correctly represents your classification problem .Third , you must choose a classificaton algorithm that maximizes the power of your features and solves at best your classification problem .My suggestion is that your try out your current dataset with many different classification algorighms , not only Naive Bayses .", "label": "", "metadata": {}, "score": "45.24882"}
{"text": "These experiments focus on the combination of classifiers ( relying on texts , images and addresses ) , dealing wi ... \" .In this paper , we describe the experiments that we have carried out during the European Research Project NetProtect II that aims at filtering harmful Web pages in order to protect children .", "label": "", "metadata": {}, "score": "45.27318"}
{"text": "This transform represents relatively recent mathematical developments , and it has not found any applications in printed Persian OCR systems .In this paper , we will evaluate the use of the more general WPT .The subword images are decomposed through the wavelet packet decomposition using the Symlet 8 with basis function up to level two to get the 20 subband images .", "label": "", "metadata": {}, "score": "45.2966"}
{"text": "Morphological features obtained by transforming the projection of the thinned writing have been computed in [ 14 ] .Franke et al .[ 15 ] developed a computer system , known as the FISH system , for retrieving a small set of documents from a larger set .", "label": "", "metadata": {}, "score": "45.35372"}
{"text": "Anomaly Detection .The technologies that are most used in text mining are classification , clustering , and feature extraction .A large number of document classification applications fall into one of the following : .Assigning multiple labels to a document .", "label": "", "metadata": {}, "score": "45.38997"}
{"text": "The next section discusses related works to our paper .Then we introduce the procedure for key phrase extraction .We describe the research questions and methodology .We compare the results , and these are analysed in the Discussion .Lastly , we conclude the paper with a summary and suggestions for future work .", "label": "", "metadata": {}, "score": "45.413292"}
{"text": "As shown below , it is revealed that indexers would select the greater variety of terms for documents while authors choose more consistent words for representing main concepts of the documents .This is especially true when the discipline of the journal between training and test data is not similar .", "label": "", "metadata": {}, "score": "45.44666"}
{"text": "Wikipedia is the largest organized knowledge repository on the Web , increasingly employed by natural language processing and search tools .In this paper , we investigate the task of labeling Wikipedia pages with standard named entity tags , which can be used further by a range of information extraction and language processing tools .", "label": "", "metadata": {}, "score": "45.65189"}
{"text": "Abstract .In this paper , we present a new approach to offline OCR ( optical character recognition ) for printed Persian subwords using wavelet packet transform .The proposed algorithm is used to extract font invariant and size invariant features from 87804 subwords of 4 fonts and 3 sizes .", "label": "", "metadata": {}, "score": "45.745224"}
{"text": "G. R. McMenamin , Forensic Linguistics- Advances in Forensic Stylistics , CRC press , USA , 2002 .R. A. Huber and A. M. Headrick , Handwriting Identification-Facts and Fundamentals , CRC Press , USA , 1999 .R. Plamondon and G. Lorette , \" Automatic signature verification and writer identification - the state of the art , \" Pattern Recognition , vol .", "label": "", "metadata": {}, "score": "45.794205"}
{"text": "( i ) Features extracted from textures : in this case , the document image is seen simply as an image , not as handwriting .For example , the application of Gabor filters and cooccurrence matrices was considered in [ 11 ] .", "label": "", "metadata": {}, "score": "45.81465"}
{"text": "Handwriting recognition remains poorly controlled in omniwriter applications when calling upon large lexicons .( ii )Handwritten documents can also be considered for their graphical content .In this case , queries of handwritten document databases can be carried out using graphical requests .", "label": "", "metadata": {}, "score": "45.85479"}
{"text": ", 2004 ) , or directly assign weight to s .. by Jure Leskovec , Natasa Milic - frayling , Marko Grobelnik - In AAAI ' 05 , 2005 . \" ...Automatic document summarization is a problem of creating a document surrogate that adequately represents the full document content .", "label": "", "metadata": {}, "score": "45.868797"}
{"text": "We used edge - direction distributions for four , eight , twelve , and sixteen angles .To find the edge direction for all of these angles , the technique uses the Sobel edge detection method .The program then labels the connected component of the image 8-pixel connected neighborhood .", "label": "", "metadata": {}, "score": "45.869286"}
{"text": "147 - 166 , 1998 .View at Publisher \u00b7 View at Google Scholar . H. Pirsiavash , R. Mehran , and F. Razzazi , \" A robust free size OCR for omni - font persian / arabic printed document using combined MLP / SVM , \" in Proceedings of the 18th", "label": "", "metadata": {}, "score": "45.91758"}
{"text": "Two of the popular methods are known as key phrase assignment and key phrase extraction .key phrase assignment selects the phrases that describe a document from a controlled vocabulary , while key phrase extraction , the approach used in this study , chooses key phrases from the text itself without using a controlled vocabulary .", "label": "", "metadata": {}, "score": "45.94696"}
{"text": "NMF has been found to provide superior text retrieval when compared to SVD and other traditional decomposition methods .NMF takes as input a term - document matrix and generates a set of topics that represent weighted sets of co - occurring terms .", "label": "", "metadata": {}, "score": "46.03208"}
{"text": "From these journals , we collected full - text articles with keywords and converted the full - text articles into text files .To find index data for each article , we referred to an index database such as Library and Information Science Abstracts .", "label": "", "metadata": {}, "score": "46.047554"}
{"text": "Instead , it depends on mathematical rules and some relations between letters .A series of tests are conducted on ARBLS to compare the effectiveness of this new Arabic stemmer with the effectiveness of another well known Arabic stemmer .Test shows clearly ARBLS is more effective than the other tested Arabic stemmer .", "label": "", "metadata": {}, "score": "46.054607"}
{"text": "In this paper , we propose a new technique based on feature extraction using wavelet packets for the recognition of Farsi text .There are two approaches to the automatic recognition of cursive scripts : holistic and segmentation - based [ 1 , 8 , 9 ] .", "label": "", "metadata": {}, "score": "46.08407"}
{"text": "The relationships between characters and the shape and style of writing will vary for different people .Handwriting , however , is a personal skill with individual characteristics .Therefore , it can be challenging to determine the best method for correctly identifying a writer .", "label": "", "metadata": {}, "score": "46.10929"}
{"text": "When it comes to journals in different scholarly areas , however , the characteristics of the training data have an impact on the key phrase extraction outcomes more significantly , resulting in quite a different gap in its performance .This is because the keywords usually come from their own fields , revealing their contextual meaning , so they fail to mirror core words in other domains ; index terms might be more efficient , as they take a more general approach to scholarly areas .", "label": "", "metadata": {}, "score": "46.227768"}
{"text": "Data Set .The evaluation of our project requires a suitable data set of handwriting images .To obtain a suitable data set , a form was distributed and completed by 100 people using the same pen .The form consisted of four common Arabic phrases and twelve common words that were to be copied twenty times by the writers into ruled empty squares .", "label": "", "metadata": {}, "score": "46.258793"}
{"text": "Candidate phrases are ranked according to this value , and the first n phrases are returned , where n is the number of key phrases requested by user .Research hypotheses .This study investigates the existence of the indexer effect when using index terms in place of keywords for automatic key phrase extraction .", "label": "", "metadata": {}, "score": "46.280663"}
{"text": "One can determine , for example , the average height , width , slope , and legibility of characters [ 19 ] .It is worth noting that it is also possible to combine these two families of features [ 12 ] .", "label": "", "metadata": {}, "score": "46.282833"}
{"text": "It should be noted that index terms are not available for Journal of Informetrics before 2010 , and there are no keywords provided in articles before 2010 for Scientometrics .As a result , we collected articles published in 2010 for test data instead of those in 2004 in those journals .", "label": "", "metadata": {}, "score": "46.30504"}
{"text": "When we compared the exact ( 100 % ) and the fair ( 70 % ) matching number of key phrases based on the two intellectual bases , the difference between the two was noticeable when training was done with index terms .", "label": "", "metadata": {}, "score": "46.32614"}
{"text": "Afterwards wavelet packet transform is performed to get proper features .In the recognition process , we use the dot feature vectors to post - process the printed Persian subwords .The integration of these methods provides a high accuracy and reliable recognition method .", "label": "", "metadata": {}, "score": "46.36325"}
{"text": "We use a mutual bootstrapping technique to alternately select the best extraction pattern for the category and bootstrap its extractions into the semantic lexicon , which is the basis for selecting the next extraction pattern .To make this approach more robust , we add a second level of bootstrapping ( metabootstrapping ) that retains only the most reliable lexicon entries produced by mutual bootstrapping and then restarts the process .", "label": "", "metadata": {}, "score": "46.411697"}
{"text": "Thus , we excluded those articles from the test collection that did not meet these conditions .Performance measure .In the experiment , we compare key phrases extracted by the machine to those assigned by the humans .We consider that there is a match if the stem of the automatically extracted key phrase matches with the stem of either the author - assigned keywords or indexer - provided index terms .", "label": "", "metadata": {}, "score": "46.453667"}
{"text": "First , the alternative content - based classifiers help validate information extracted by a transducer - based wrapper .Second , when the transducer based wrapper fails to extract information , the alternative classifier is used to resume the information extraction .", "label": "", "metadata": {}, "score": "46.48181"}
{"text": "Text features are usually keywords , frequencies of words , or other document - derived features .Once you derive text features , you mine them just as you would any other data .Classify or categorize documents .Integrate search capabilities with classification and clustering of documents returned from a search .", "label": "", "metadata": {}, "score": "46.600357"}
{"text": "To evaluate automatic key phrase extraction techniques , articles should have keywords assigned and be found in the index database with appropriate index terms .For journal selection , we first referred to the Web of Science Journal Citation Reports .Two important criteria for the journal selection were : 1 ) whether the subject matter of the journals was suitable for testing our hypotheses and 2 ) whether the journals had articles to which keywords as well as indexing terms had been assigned .", "label": "", "metadata": {}, "score": "46.816647"}
{"text": "Experiments and Results .The implementation of this system is based on a dataset containing 32,000 Arabic text images corresponding to 16 different words repeated 20 times each and written by 100 people using the same pen .The K - nearest neighbor classifier was trained using 75 % of the words , and the remaining 25 % were used for testing .", "label": "", "metadata": {}, "score": "46.827225"}
{"text": "Post - processing stage in the recognition system is shown in Figure 11 .In this stage , location and number of dots in subwords are extracted as second features .These features are useful in distinguishing letters and subwords that are only different in the number and the location of their dots with respect to the main body of subword [ 39 ] .", "label": "", "metadata": {}, "score": "46.893333"}
{"text": "Table 6 .The performance of the key phrase extraction algorithm with the Journal of Information Science .2004 keyword .2004 index .We graphed the average of correctly extracted key phrases divided by the total number of key phrases extracted in Figure 2 and confirmed that the efficiency of the indexers ' perspective is higher than that of authors .", "label": "", "metadata": {}, "score": "46.93006"}
{"text": "We also observe that overall performance is greatly affected by the type of corpus used to train the algorithms .Our heterogeneous dataset is provided by the ACE ( Automatic Content Extraction ) initiative , while our novel homogeneous dataset consists of news articles and annotated Die events from the Iraq Body Count ( IBC ) database .", "label": "", "metadata": {}, "score": "47.13821"}
{"text": "Assigning a document to one of many labels .For example , automatically assigning a mail message to a folder and spam filtering .This application requires multi - class classification .The Support Vector Machine ( SVM ) algorithm provides powerful classifiers that have been used successfully in document classification applications .", "label": "", "metadata": {}, "score": "47.17321"}
{"text": "First , authors are more professional in giving keywords for articles compared to indexers .Authors have knowledge of what people want to find in their field , so they are able to give more appropriate access points for information .In addition , authors provide the words for the articles they write , so they know the precise words for representing their writing .", "label": "", "metadata": {}, "score": "47.19579"}
{"text": "We evaluated the performance of edge - based directional probability distributions as features , among other characteristics , in Arabic writer identification .Results suggest that longer Arabic words and phrases have higher impact on writer identification .Introduction .Two fundamental concepts are considered to be critical to writer identification : no two people write exactly alike , and no one person writes exactly the same way twice .", "label": "", "metadata": {}, "score": "47.21492"}
{"text": "Most work in the field of writer recognition has concentrated on signature verification because signatures typically present more individuality .However , in many cases , only words or characters rather than signatures are available for analysis .Word - based analysis began with the work of Steinke [ 17 ] .", "label": "", "metadata": {}, "score": "47.270397"}
{"text": "Then Euclidian distance as correlation between feature vector of unknown subword and each vector in dictionary is computed to measure similarity of two vectors .The ranking of similar subwords is produced .Then 10 closest subword vectors with maximum correlation value are chosen as entries to the next stage .", "label": "", "metadata": {}, "score": "47.323578"}
{"text": "5 , no . 7 , pp .37 - 40 , 2005 .View at Google Scholar .S. Mallat , A Wavelet Tour of Signals Processing , Academic Press , New York , NY , USA , 1999 .M. SalmaniJelodar , M.J. Fadaeieslam , N. Mozayani , and M. Fazeli , \" A persian OCR system using morphological operators , \" in Proceedings of the 2nd World Enformatika Conference ( WEC ' 05 ) , vol .", "label": "", "metadata": {}, "score": "47.408356"}
{"text": "Extraction then resumes and all following authors , title , conference and pages with be extracted in a regular way .A new error will occur again at the beginning of the next answer item , and so on .In the general case , the majority of wrapper rules may have label - dependent prefixes and the recovery by skipping tokens till one that matches a label - independent rule may be too generous .", "label": "", "metadata": {}, "score": "47.40943"}
{"text": "Indexers , however , consider words that will represent an article well and thoroughly examine an individual document ; they sometimes select key phrases with greater variation when representing certain concepts , leaving the potential for a wrong decision .For example , in one of the articles used in this study , authors provide a keyword sliding window , but indexers give the index term sliding window method to indicate that keyword , which might be quite similar in some way but not exactly .", "label": "", "metadata": {}, "score": "47.42946"}
{"text": "View at Google Scholar . A. Bensefia , T. Paquet , and L. Heutte , \" Handwritten document analysis for automatic writer recognition , \" Electronic Letters on Computer Vision and Image Analysis , vol .5 , no . 2 , pp .", "label": "", "metadata": {}, "score": "47.563118"}
{"text": "We used word measurements , such as area , length , height , length from the baseline to the upper edge , and the length from the baseline to the lower edge , as features .After computing each of these features , as discussed below , we used them together to create one feature vector and used standardization on this vector .", "label": "", "metadata": {}, "score": "47.56964"}
{"text": "The interesting observations might inspire further investigations . ...o the sparseness problem .Everyone of them adopt a criterion scoring and ranking the terms ; for a target dimensionality d , the term selection is simply done by picking out the top - d t .. \" ...", "label": "", "metadata": {}, "score": "47.569954"}
{"text": "We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution .We focus on a semi - supervised framework that incorporates labeled and unlabeled data in a general - purpose learner .", "label": "", "metadata": {}, "score": "47.63002"}
{"text": "We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution .We focus on a semi - supervised framework that incorporates labeled and unlabeled data in a general - purpose learner .", "label": "", "metadata": {}, "score": "47.63002"}
{"text": "39 - 49 , 2001 .View at Google Scholar . A. Azmi and E. Kabir , \" Design of three pictorial dictionaries for recognition of printed Farsi sub - words recognition , \" Amirkabir Journal of Science and Technology , vol .", "label": "", "metadata": {}, "score": "47.652348"}
{"text": "System Overview .In the classification step , we used a k - nearest neighbor classifier .The other steps are described in Sections 4 , 5 , and 6 .Our algorithm is based on text - dependent writer identification and consists of a training phase and a testing phase ; a conceptual illustration is presented in Figure 2 .", "label": "", "metadata": {}, "score": "47.726562"}
{"text": "We found that there is a noticeable discrepancy between author - provided terms and indexer - provided terms .This disparity could be attributed to the way authors view disciplines from the perspective of insiders , while indexers from that of outsiders , as suggested by Marion and McCain ( 2001 ) .", "label": "", "metadata": {}, "score": "47.769135"}
{"text": "A particular document may fit into two or more different categories .This type of classification can often be represented as a multi target classification problem where a supervised model is built for each category .ODM provides infrastructure for developing data mining applications suitable for addressing a variety of business problems involving text .", "label": "", "metadata": {}, "score": "47.825066"}
{"text": "Our system then searches the image to find pixels ( black pixels ) and the direction of each of these pixels .In this paper , we have calculated the edge - distribution features for four , eight , twelve , and sixteen angles .", "label": "", "metadata": {}, "score": "47.846336"}
{"text": "33 , no . 3 , pp .385 - 398 , 2000 .View at Google Scholar \u00b7 View at Scopus .K. Franke , L. Schomaker , L. Vuurpijl , and St. Giesler , \" FISH - new : a common ground for computer based forensic writer identification , \" in Proceedings of the 3rd European Academy of Forensic Science Triennial Meeting , p. 84 , Istanbul , Turkey , 2003 .", "label": "", "metadata": {}, "score": "47.88395"}
{"text": "All these relations are then appropriately formulated in the QoCsR algorithm though in different ways .ROUGE evaluations shows that QoCsR can outperform the best DUC 2005 participating systems .Keywords : Query - Oriented Summarization , Multi - document Summarization , Graph Model and Ranking Algorithm . .", "label": "", "metadata": {}, "score": "47.972557"}
{"text": "Figure 11 : Recognition subsystem is combined with dot recognition modules and postprocessing blocks to recognize subwords .In the experiment to evaluate the represented method , a set of 2000 subwords is used .For multifont and different - size printed Persian subwords , the average recognition rate of 97.9 % was measured for this algorithm .", "label": "", "metadata": {}, "score": "48.01095"}
{"text": "Kim and Ross considered the 24 classes of KRYS-01 as noise with respect to the performance of their classifier on the 7-webgenre collection .In their case , noise is represented by 24 well defined genre classes , each of them represented by a relatively small number of documents ( at most 90 ) , while the 7 web genres are represented by 190 web pages each .", "label": "", "metadata": {}, "score": "48.024834"}
{"text": "He is the corresponding author for this paper .His research interests are text mining , social media mining , and bio - literature mining .He can be reached at min.song@yonsei.ac.kr .References .Ahmed , S. Z. , McKnight , C. & Oppenheim , C. ( 2004 ) .", "label": "", "metadata": {}, "score": "48.032204"}
{"text": "Hahm , J.E. , Kim , S.Y. , Kim , M.C. , Song , M. ( 2013 ) .Investigation into the existence of the indexer effect in key phrase extraction .Information Research , 18 ( 4 ) paper 594 .", "label": "", "metadata": {}, "score": "48.085373"}
{"text": "No empirical studies , however , have been carried out to isolate the contribution made by frequency information from that of other features .Here , we examine the impact of frequency on var - ious ... \" .Most multi - document summarizers utilize term fre - quency related features to determine sentence im - portance .", "label": "", "metadata": {}, "score": "48.099487"}
{"text": "Such a . ffl Exploit more linguistic structure .We plan to explore ways in which noun , verb , and prepositional phrases extracted from the text can be used as features for information extraction .We have cond ... . \" ...", "label": "", "metadata": {}, "score": "48.110294"}
{"text": "J. Bigun and I. Smeraldi , Eds . , Audio- and Video - Based Biometric Person Authentication , Proceedings of the 3rd International Conference , AVBPA , Halmstadt , Sweden , 2001 .R. N. Morris , Forensic Handwriting Identification- Fundamental Concepts and Principles , Academic Press , New York , NY , USA , 2000 .", "label": "", "metadata": {}, "score": "48.132942"}
{"text": "Another solution for wrapper repair finds the most frequent patterns ( such as starting or ending words ) in the content of labeled strings and then searches for these patterns in a page when the wrapper is broken .It would be desirable to have a method for wrapper repairing that accurately and automatically repairs wrappers in a large number of situations .", "label": "", "metadata": {}, "score": "48.15077"}
{"text": "Another relevant work on query - based summarization [ 6 , ... .by Heng Ji , Benoit Favre , Wen - pin Lin , Dan Gillick , Dilek Hakkani - tur , Ralph Grishman . \" ...Abstract .Information Extraction ( IE ) and Summarization share the same goal of extracting and presenting the relevant information of a document .", "label": "", "metadata": {}, "score": "48.180843"}
{"text": "The performance histogram of the edge - based directional features is presented in Figure 11 .The histogram shows the results of applying the features mentioned in the feature extraction section .Figure 11 : Performance histogram of the edge - based directional features for different Arabic words .", "label": "", "metadata": {}, "score": "48.222626"}
{"text": "Actually , the adjective \" small \" was used mainly for the convenience of explanation and not to constrain the proposed recovery routines .The success or failure of the information extraction recovery is determined by a number of aspects , including the type of changes , their density or sparseness in pages , etc .", "label": "", "metadata": {}, "score": "48.26309"}
{"text": "Conclusion and future work .In this study , we investigated whether the indexer effect exists in key phrase extraction .We also presented a new key phrase extraction approach based on indexers ' terms as training data instead of keywords , which is built based on the key phrase extraction algorithm .", "label": "", "metadata": {}, "score": "48.305122"}
{"text": "When structure features are used together with content features for the string classification , the classification error level is much lower than the classification by content features only .This allows us to repair the information extraction for a given wrapper with much higher accuracy .", "label": "", "metadata": {}, "score": "48.399834"}
{"text": "In this paper we have introduced an enhanced root - based algorithm that handles the problems of affixes , including prefixes , suffixes , and infixes depending on the morphological pattern of the word .The stemming concept has been used to eliminate all kinds of affixes , including infixes .", "label": "", "metadata": {}, "score": "48.403908"}
{"text": "Assume that a certain article has information retrieval , ranking model and query expansion as author 's keywords , and information retrieval and relevance feedback as indexer 's terms .This evaluation is usually used to compare the inter - indexer consistency among several indexers and to prove the validity of the low performance of a machine learning algorithm ; the difference between indexers is similar to that between the human and the machine .", "label": "", "metadata": {}, "score": "48.444046"}
{"text": "This data set will allow us to compare our results with other research results .In future work , we will test our system on text - independent writer identification with an improved set of features and classification methods .Acknowledgments .", "label": "", "metadata": {}, "score": "48.534164"}
{"text": "How to you retrain your classifier for the classification of such a sample with a minimum annotation effort ?Realistically , i do not think you are going to annotate 10gb+ manually .In these situations a supervised classifier is facing \" unstructured noise \" , i.e. unknown classes and unknown proportions .", "label": "", "metadata": {}, "score": "48.58967"}
{"text": "A large number of rules is needed for coverage of the domain , suggesting that a fairly large number of labeled examples should be required to train a classifier .However , we show that the use of unlabe ... \" .", "label": "", "metadata": {}, "score": "48.621395"}
{"text": "We evaluated performance of several variant methods on top velocity \" celebrity name \" queries from Yahoo , using news stories from several sources for event extraction .Results bear out the event - causation hypothesis , in that ZED currently finds acceptable event - based explanations for about 90 % of the queries examined . ... ractive text summarization which ZED 's event extraction algorithm is based on .", "label": "", "metadata": {}, "score": "48.737038"}
{"text": "Support for assigning documents to one of many labels and also for assigning documents to multiple labels at the same time .One - Class SVM .The Java API handles the process supports the feature extraction process that transforms a text column to a nested table .", "label": "", "metadata": {}, "score": "48.73899"}
{"text": "Removing Background .The colored squared borders of the words in each form were removed using a thresholding operation .We used Otsu 's method , which maximizes the likelihood that the threshold will be chosen , to split the image between an object and its background .", "label": "", "metadata": {}, "score": "48.802372"}
{"text": "Though there might exist certain correlations between particular tags and content , for example , a page title is often surrounded by HTML tags or , the two sets of features are considered to be uncorrelated .Content , context and structure features may be used to build a number of independent classifiers .", "label": "", "metadata": {}, "score": "48.850044"}
{"text": "Moreover , the document is represented using several term weighting schemes and finally the k - nearest neighbor and Rocchio classifiers are used for classification process .Experiments are performed over self collected data corpus and the results show that the suggested hybrid method of statistical and light stemmers is the most suitable stemming algorithm for Arabic language .", "label": "", "metadata": {}, "score": "49.139503"}
{"text": "Easily constructed combinations of feature selectors are shown to improve peak R - precision and F1 at statistically significant levels . ...did this term achieve a higher fraction of the total achieved score across all the terms ?We refer to this approach as DLOR ( divide by length then OR ) .", "label": "", "metadata": {}, "score": "49.14531"}
{"text": "Ben Amor and Ben Amara presented a hybrid technique based on both neural networks and the hidden Markov models for classification in a multifont Arabic OCR system [ 16 ] .This hybrid approach has been tested by a method based on the Hough transform for features extraction .", "label": "", "metadata": {}, "score": "49.249813"}
{"text": "Specifically , the gap between extracted results from authors ' and indexers ' base in the Journal of Informetrics is small : however , Scientometrics shows a bigger discrepancy .We also graphed the results in Figure 5 and 6 .In the American Educational Research Journal , the performance rate in 2011 idx is even greater than 1.0 .", "label": "", "metadata": {}, "score": "49.274765"}
{"text": "In this paradigm , the test set can not contain any class that is NOT included in the training set .Imagine the situation where you have a fantastic supervised classifier trained on 50 categories that you want to use to classify a random sample of 10gb+ web documents .", "label": "", "metadata": {}, "score": "49.306778"}
{"text": "For our experiment , we employ a new feature , index term as training data for key phrase extraction .In addition , we compare the performance of index term - based key phrase extraction to that of keyword - based extraction to identify the existence of the indexer effect .", "label": "", "metadata": {}, "score": "49.379166"}
{"text": "After extracting all subwords features and building a pictorial dictionary , we use this feature extraction method in a recognition scheme .The proposed recognition scheme consists of two stages : a feature extraction stage for extracting multiresolution features and a postprocessing stage that uses dot features of subwords .", "label": "", "metadata": {}, "score": "49.40757"}
{"text": "We utilize properties of Reproducing Kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms .As a result ( in contrast to purely graph - based approaches ) we obtain a natural out - of - sample extension to novel examples and so are able to handle both transductive and truly semi - supervised settings .", "label": "", "metadata": {}, "score": "49.41561"}
{"text": "Results from a large investigation of these combinations are summarized .Easily constructed combinations of feature selectors are shown to improve peak R - precision and F1 at statistically significant levels .We introduce several methods of combining feature selectors for text classification .", "label": "", "metadata": {}, "score": "49.447685"}
{"text": "To alleviate this cluttering , it is useful to select a small subset of the social activities within the specified timeperiod as representative , i.e. , as summary , for this time - period .In this paper , we study the novel problem of social activity log summarization .", "label": "", "metadata": {}, "score": "49.545998"}
{"text": "33 , no . 1 , pp . 1 - 12 , 2006 .View at Google Scholar .R. Azmi , E. Kabir , and K. Badie , \" An algorithm for clustering and recognition of omnifont farsi sub - words , \" International Journal of Engineering Science , vol .", "label": "", "metadata": {}, "score": "49.567493"}
{"text": "In addition , we analyse the results to explain what makes it different .Performance is measured by comparing the results of the supervised - learning technique with pre - defined key phrases .We conduct two sets of experiments with the same datasets in the training phrase ; one with keywords that authors provide and the other with index terms that indexers assign .", "label": "", "metadata": {}, "score": "49.601128"}
{"text": "The Process of Subwords and Text Recognition .OCR is the process of converting a raster image representation of a document into a format that a computer can process [ 36 , 37 ] .Printed Persian subwords recognition , in its most general form , is the transcription in printed document and machine written text [ 38 ] .", "label": "", "metadata": {}, "score": "49.61222"}
{"text": "If the performance is still poor regardless of the algorithm you use , there is something that must be adjusted in your feature set and in your training set .So you must revise and reflect on your classification problem .You can also subscribe to the Weka mailing list and see if somebody else came across the same problem before and ask for advice .", "label": "", "metadata": {}, "score": "49.676704"}
{"text": "This graphical representation which does not require training corpora can be potentially adapted to other languages . ... rity of words and consistency of date .However , choosing sentences as nodes within graph limits the representation ability of information in documents and the flexibility for further applications .", "label": "", "metadata": {}, "score": "49.76886"}
{"text": "Clues to syntactic boundaries such as punctuation marks , digits and paragraph separators are retained .Next , all word n - grams that do not cross phrase boundaries are extracted and the number of occurrences of each n - gram in the documents is counted .", "label": "", "metadata": {}, "score": "49.79911"}
{"text": "When a wrapper is broken , the HTML context of strings can not be used any longer .The automatic wrapper repair method and system uses other features to build a classifier for content strings .The wrapping of HTML pages may be considered as a special case of a classification problem .", "label": "", "metadata": {}, "score": "49.811516"}
{"text": "View at Google Scholar .G. Raju and K. Revathy , \" Wavepackets in the recognition of isolated handwritten characters , \" in Proceedings of the World Congress on Engineering ( WCE ' 07 ) , vol .1 , pp .", "label": "", "metadata": {}, "score": "49.87264"}
{"text": "The study of Arabic handwriting identification , however , has been much more limited .The recognition of Arabic characters is also important for certain non - Arabic - speaking languages , such as Farsi , Kurd , Persian , and Urdu .", "label": "", "metadata": {}, "score": "49.91526"}
{"text": "For example , a particular Web wrapper might target the extraction of one tuple from a page , such as a book price or a weather forecast , where another wrapper might target lists of nested tuples with multiple labels .Three exemplary types of format changes that may occur in a Web page include : context shift , content shift and structural shift .", "label": "", "metadata": {}, "score": "49.915665"}
{"text": "One of the main concerns of designing every OCR system is to make it robust to the font and size variations [ 4 ] .It is clear that OCR of multifont documents is more difficult than OCR of single - font documents .", "label": "", "metadata": {}, "score": "49.968536"}
{"text": "In order to build the model , a training set of documents with known key phrases is required .For each training document , candidate pseudo - phrases are identified and their feature values described above are calculated .Each phrase is marked as a key phrase or not , using the actual key phrases for those document .", "label": "", "metadata": {}, "score": "50.073547"}
{"text": "This is the basic idea of supervised machine learning .When a document in the test set is not represented in the training set , the algorithm is confused .Additionally , the proportions of the classes in the training set affect also the results .", "label": "", "metadata": {}, "score": "50.13282"}
{"text": "Experimental results on common multilabel domains involving protein , document and scene classification show that better performance can be achieved compared to popular multilabel classification approaches . ... t label .We then selected the top 500 features based on the their maximum rank over all labels . 4.2", "label": "", "metadata": {}, "score": "50.153687"}
{"text": "We evaluate LogRank and its variants on a real dataset from the Google+ social network and show that they outperform baseline approaches . des .Instead , our method is based on selecting a subset of nodes to form representative summary of the entire graphs .", "label": "", "metadata": {}, "score": "50.229713"}
{"text": "Examples include the addition of new labels , removal of old ones , order permutations , etc . .Referring to .FIG .1 , a block diagram of wrapper architecture includes a wrapper Generation component , a wrapper Employment component and a wrapper Recovery component .", "label": "", "metadata": {}, "score": "50.249176"}
{"text": "In 1993 , Oncina et al . , in Learning subsequential transducers for pattern recognition interpretation , IEEE Trans . on Pattern Analysis , 15:448 - 458 , proposed the OSTI algorithm that allows inference of regular transducers from positive samples and minimizes the delays between input token consumption and output emission for all ambiguity cases in the result transducer .", "label": "", "metadata": {}, "score": "50.267"}
{"text": "Typically , assumptions are made to constrain the tracking problem in the context of a particular application .In this survey , we categorize the tracking methods on the basis of the object and motion representations used , provide detailed descriptions of representative methods in each category , and examine their pros and cons .", "label": "", "metadata": {}, "score": "50.269947"}
{"text": "Document Segmentation .In the preprocessing stage and after scanning the forms , we needed to crop the text images from the pages of the forms .To achieve this , a MATLAB program was written .Because we knew the dimensions of the edge pixels of the forms and the edge pixels of the words , the program uses this information to segment pages into words .", "label": "", "metadata": {}, "score": "50.321266"}
{"text": "In addition , we used na\u00efve Bayes and decision tree classifiers , and identified that the former performs better than the latter in every case .We , therefore , only reported the results with the na\u00efve Bayes classifier , which is also used as the default algorithm in the key phrase extraction algorithm .", "label": "", "metadata": {}, "score": "50.3219"}
{"text": "M. B. Menhaj and M. Adab , \" Simultaneous segmentation and recognition of farsi / latin printed texts with MLP , \" in Proceedings of the IEEE International Joint Conference on Neural Networks ( IJCNN ' 02 ) , pp .1534 - 1539 , May 2002 .", "label": "", "metadata": {}, "score": "50.428726"}
{"text": "Abstract .This paper proposes an ensemble method for multilabel classification .The RAndom k - labELsets ( RAKEL ) algorithm constructs each member of the ensemble by considering a small random subset of labels and learning a single - label classifier for the prediction of each element in the powerset of this subset .", "label": "", "metadata": {}, "score": "50.50318"}
{"text": "The result gives us the length of the text .Height .The method for determining the height of the image is similar to the method for finding the length of the image .However , when searching the binary image , the algorithm scans each row of the image to find the first and last pixels in the image and store their row numbers .", "label": "", "metadata": {}, "score": "50.59391"}
{"text": "DETAILED DESCRIPTION OF THE EMBODIMENTS .The system and method of the invention is applicable to the automatic repair of any type of wrapper .For convenience , the following discussion will focus on exemplary wrappers used to extract information from Web pages .", "label": "", "metadata": {}, "score": "50.657303"}
{"text": "The area of the image was found by searching for any black pixels in the image .If the program finds a black pixel , it adds 1 to the total area value .Length .In this paper , the length of a text segment or word is found by sequentially searching each column in the binary image to find the first and last pixels in the image and store their column numbers .", "label": "", "metadata": {}, "score": "50.699875"}
{"text": "Many web documents might simply not belong to any genre or embody several genres .Santini ( 2010 ) explored a simple way to deal with unstructured noise based on the odd - likelihook form Bayes ' theorem .Results were encouraging , but preliminary .", "label": "", "metadata": {}, "score": "50.738007"}
{"text": "If no exact rule is found for P and S , the wrapper results in an error .A method for detecting optimal and minimal prefix - suffix pairs for extraction rules is described in Chidlovskii , Wrapping Web Information Providers by Transducer Induction .", "label": "", "metadata": {}, "score": "50.78889"}
{"text": "The goal of the research described here is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the World Wide Web .Such a ... \" .The World Wide Web is a vast source of information accessible to computers , but understandable only to humans .", "label": "", "metadata": {}, "score": "50.9171"}
{"text": "Common approaches for identifying important sentences to include in the summary include training a binary classifier ( e.g. , [ 12 ] ) , training a Markov model ( e.g. , [ 4 ] ) , or directly assigning weights ... . \" ...", "label": "", "metadata": {}, "score": "50.93236"}
{"text": "For 17 reported format changes , context shifts occurred in all 17 cases , and structural shifts occurred in 11 cases .In experiments , three recovery methods were tested , namely , the basic recovery , the multi - pass recovery with backward transducers and multi - pass recovery with backward wrappers and content classifiers .", "label": "", "metadata": {}, "score": "51.15176"}
{"text": "This paper is a comparative study of feature selection in drug discovery .The focus is on aggressive dimensionality reduction .Five methods were evaluated , including information gain , mutual information , a \u03c7 2-test , odds ratio , and GSS coefficient .", "label": "", "metadata": {}, "score": "51.184017"}
{"text": "Examples of semantic features include number of nouns , data strings , and numeric strings .Structure features represent relative co - occurrence of extracted and labeled content information in the file .Wrapper repair depends , to a large extent , on how much information can be extracted from the file using the broken wrapper .", "label": "", "metadata": {}, "score": "51.215446"}
{"text": "In the actual classification phase , the test sample ( where the writer is not known ) is represented as a vector in the feature space after the preprocessing and feature extraction processes .Distances from the new vector to all stored vectors are computed , and the k closest samples are selected .", "label": "", "metadata": {}, "score": "51.219746"}
{"text": "Is there a science behind it or just a freaky coincidence .I also suspect our algorithm itself specially as in the case of 1 very large negative category experiment it was not reducing false positives .I am afraid there is not such a magic algorithm that does document classification and takes care of noise , in whtatever situation , with good performance .", "label": "", "metadata": {}, "score": "51.229507"}
{"text": "FIG .5 these separators are marked with blocks ; for all other strings , the ( decision tree ) classifier gives 68 % to 92 % of prediction accuracy which is insufficient for a reliable repairing of information extraction .In the method of wrapper repair , content features are extended with structural features .", "label": "", "metadata": {}, "score": "51.37123"}
{"text": "Journal of Information Science , 30 ( 5 ) , 459 - 468 .Barker , K. & Cornacchia , N. ( 2000 ) .Using noun phrase heads to extract document key phrases .In : H. J. Hamilton , ( Ed . ) , Advances in Artificial Intelligence .", "label": "", "metadata": {}, "score": "51.40413"}
{"text": "Domain - independent automatic key phrases indexing with small training sets .Journal of the American Society for Information Science and Technology , 59 ( 7 ) , 1026 - 1040 .Nguyen , T. D. & Kan , M. Y. ( 2007 ) .", "label": "", "metadata": {}, "score": "51.406876"}
{"text": "Next , all of the verified angles of each pixel are counted into an eight - bin histogram that is then normalized to a probability distribution that gives the probability of finding an edge fragment oriented at the angle measured from the horizontal in the image .", "label": "", "metadata": {}, "score": "51.4253"}
{"text": "The main concept of the proposed algorithm is based on the property that the wavelet packet compressed image is a decomposition vector which can uniquely represent the input image to be correctly reconstructed later at a decompression stage .This property can be effectively used to recognize the subword 's image .", "label": "", "metadata": {}, "score": "51.449585"}
{"text": "We test and compare different combination formulas ( Voting methods , logical methods , k Nearest Neighbors , evidence - based k Nearest Neighbors , Naive Bayes , Artificial Neural Network and Support Vector Machine ) on a five thousand webpages database .", "label": "", "metadata": {}, "score": "51.492325"}
{"text": "The performance of the key phrase extraction algorithm with journals in same field .2010 keyword .2010 index .We graphed the average of correctly extracted key phrases divided by the total number of key phrases extracted in Figures 3 and 4 , and confirmed again that index term - based extraction mostly performs better than extraction based on author - provided keywords .", "label": "", "metadata": {}, "score": "51.53606"}
{"text": "However , we did not observe statistical significance for the 2004 data .Research hypothesis 2 : evaluation for different journals in the same field .We extracted key phrases from two other journals in the same field with the same training data used in research hypothesis 1 .", "label": "", "metadata": {}, "score": "51.543724"}
{"text": "Dictionary is built once and does not need any iteration or learning algorithm .Our dictionary contains only 7317 entries that represent 7317 Persian subwords .These entries are decomposition vectors driven from subwords ( 12 groups with different sizes and fonts \u00d7 7317 subwords of the available database ) .", "label": "", "metadata": {}, "score": "51.553974"}
{"text": "This application claims priority from provisional Application No .60/397,152 filed Jul. 18 , 2002 , which is incorporated herein by reference .This invention is related to co - assigned , co - pending U.S. Pat .No .6,792,576 issued Sep. 14 , 2004 for \" System and Method for Automatic Wrapper Grammar Generation \" , which is incorporated herein by reference .", "label": "", "metadata": {}, "score": "51.654915"}
{"text": "Arabic Stemming is not an easy task , since Arabic language uses many inflectional forms .Researchers are divided on the idea that is beneficial to use stemming in fields like IR , NLP ... etc , since in Arabic the morphological variants of a certain word are not always semantically related .", "label": "", "metadata": {}, "score": "51.665222"}
{"text": "New York , NY : ACM Press .Wu , Y. B. & Li , Q. ( 2008 ) .Document key phrases as subject metadata : incorporating document key concepts in search results .Information Retrieval , 11 ( 3 ) , 229 - 249 .", "label": "", "metadata": {}, "score": "51.66748"}
{"text": "Content classifier C is generated from the content feature set F C of textual tokens in sample pages .Any existing techniques for classifier generation can be used here ; we use decision trees from Borgelt 's publicly available package .It is now straightforward to extend the initial recovery strategy described above with a content classifier .", "label": "", "metadata": {}, "score": "51.713516"}
{"text": "The information extraction recovery is triggered by wrapper errors on a changed page ; it applies Algorithm 1 ( 3 ) and possibly Algorithm 2 ( 4 ) to accurately label tokens in the page using alternative content classifiers and backward wrappers .", "label": "", "metadata": {}, "score": "51.744267"}
{"text": "The results showed that when articles that have fewer interrelated topics are included in the training data , the results are influenced more by index terms than by keywords .Thus , the latter is influenced more strongly by the terms that indexers offer .", "label": "", "metadata": {}, "score": "51.887383"}
{"text": "The minimality of extraction rules does not guarantee wrapper uniqueness .In certain cases , several alternative OCER wrappers may exit for a provider .The DBLP sample page after the change is shown in .FIG .4 .The format change concerns both page mark - up and structure .", "label": "", "metadata": {}, "score": "51.903038"}
{"text": "However , indexers just guide people to the right paths without any profit for themselves .This implies that words based on authors ' intellectual structures might be subjective and perhaps even deceptive compared to words in an index , even for the same article .", "label": "", "metadata": {}, "score": "51.915787"}
{"text": "We chose the Journal of Informetrics and Scientometrics for the second research hypothesis , which addresses different journal data in the same field .Lastly , we included two additional fields for our experiment ; one close to information science and the other a little farther from this field , and selected one journal from each discipline .", "label": "", "metadata": {}, "score": "51.952183"}
{"text": "While the initial wrapper may be broken , parts of the initial wrapper may still work .In the case of a web wrapper , the initial set of rules is typically a set of grammar / transducer rules tuned to parsing files from the beginning of the file to the end of the file .", "label": "", "metadata": {}, "score": "52.08235"}
{"text": "In this scenario , Vidulin et al . 's ( 2007 ) accuracy results are high ( 94 % ) , while their F - measure average on 20 genres is moderate ( 50 % ) .One problem with structured noise is that it requires a major annotation effort because all the classes that the supervised classifier should consider as negative examples must be clearly defined and labelled .", "label": "", "metadata": {}, "score": "52.112144"}
{"text": "It implies that it is more appropriate to use index terms as training data in key phrase extraction but automatically extracted key phrases might lead users to less relevant documents in information retrieval .One of the limitations of our current work is that the training data are constructed with one journal .", "label": "", "metadata": {}, "score": "52.12008"}
{"text": "Secondly , authors want people to cite their articles , so they sometimes exaggerate the contents of their writing .Information users usually decide to continue their reading by looking through the first few paragraphs , so making a good first impression is important for authors .", "label": "", "metadata": {}, "score": "52.132973"}
{"text": "To quantify the performance of developed recovery mechanisms , information extracted from \" after - change \" pages using different methods were compared .Measures of precision and recall , widely used in Information Retrieval were used .Precision is a portion of correctly labeled textual tokens in the extracted information , and recall is a portion of correctly labeled textual tokens in the correctly labeled information : . precision .", "label": "", "metadata": {}, "score": "52.15628"}
{"text": "Second , when a wrapper runs into errors , the combined information extraction recovery will not simply skip tokens with unrecognized context , but will apply the corresponding content classifier in order to label such tokens .Algorithm 3 below scans page E from the beginning to the end .", "label": "", "metadata": {}, "score": "52.15885"}
{"text": "To avoid redundancy , our algorithm checks only the upper two quadrants in the neighborhood because without online information , we do not know which way the writer traveled along the found oriented - edge fragment , which will give us only 4 possible angles ( see Figure 3 ) .", "label": "", "metadata": {}, "score": "52.23861"}
{"text": "Witten , I. H. , Paynter , G. W. , Frank , E. , Gutwin , C. & Nevill - Manning , C. G. ( 1999 ) .KEA : practical automatic key phrase extraction .In Proceedings of the Fourth ACM conference on Digital libraries , ( pp .", "label": "", "metadata": {}, "score": "52.25778"}
{"text": "When a wrapper is generated , it is assumed that the layout and structure of the document pages do not change .However , Web page owners frequently update and revise their pages , which often involves changing the layout and structure of their pages .", "label": "", "metadata": {}, "score": "52.257915"}
{"text": "If you wish to work with noisy classification , you should look at alternative models : you can try out semi - supervised learning , adaptive learning , unsupervised learning , AI models .Thanks a lot for the reply .I need some heads up on following statement of yours ( our NB classifier is behaving the other way round ) : .", "label": "", "metadata": {}, "score": "52.28412"}
{"text": "Research Policy , 15 ( 5 ) , 233 - 251 .Iivonen , M. & Kivimaki , K. ( 1998 ) .Common entities and missing properties : similarities and differences in the indexing of concepts .Knowledge Organization , 25 ( 3 ) , 90 - 102 .", "label": "", "metadata": {}, "score": "52.29369"}
{"text": "Against this backdrop , we will briefly discuss what previous research has revealed , especially for key phrase extraction , which serves as a foundation for our experiment .The first system , GenEx ( Generic Extractor ) , employs machine learning - based key phrase extraction devised by Turney ( 1999 ) .", "label": "", "metadata": {}, "score": "52.323883"}
{"text": "Comput .Sci . , Vol .44 , No . 5 , 2004 1825 Figure 1 .Effect of different feature selection methods in ... . \" ... Wikipedia is the largest organized knowledge repository on the Web , increasingly employed by natural language processing and search tools .", "label": "", "metadata": {}, "score": "52.36235"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1 is a block diagram of an architecture for wrapper generation and maintenance .FIG .2 is a sample answer produced by a wrapper for the DBLP web site .FIG .3 is the corresponding HTML source for the answer shown in .", "label": "", "metadata": {}, "score": "52.391235"}
{"text": "This data is best exploited if available as a relational table that we could use for answering precise queries or for running data mining tasks .We explore a technique for extracting such tables fr ... \" .Text documents often contain valuable structured data that is hidden in regular English sentences .", "label": "", "metadata": {}, "score": "52.43663"}
{"text": "Thus , the shape descriptor is obtained from the recognized word .The feature extraction stage , characteristic loci features are extracted from printed Persian subword images to describe their shapes [ 10 ] .In feature extraction stage , the number of crossings with subword bodies is restricted to 2 .", "label": "", "metadata": {}, "score": "52.657825"}
{"text": "Structural features are a complement to content ones ; they express the mutual co - allocation of strings in the page marked with different labels .A set of content features for the HTML string classification is identified for a particular wrapper .", "label": "", "metadata": {}, "score": "52.686584"}
{"text": "A computer - related method of information extraction from a Web page using a broken wrapper , comprising : . wherein a wrapper comprises a set of rules for extracting information from HTML context of strings and for assigning labels from a wrapper set of labels to the extracted information ; . wherein a broken wrapper comprises a wrapper in which HTML context of at least one string can not be used to classify the at least one string ; . extracting strings from the Web page parsed in a forward direction using the broken wrapper ; .", "label": "", "metadata": {}, "score": "52.714638"}
{"text": "This context - based classifier is partial , so it runs in an error when no exact rule is found .When a wrapper can not label a token , an initial recovery strategy is to skip one or more textual tokens in the input until the first token that does match some rule .", "label": "", "metadata": {}, "score": "52.7284"}
{"text": "The removal of irrelevant and redundant information often improves the performance of learning algorithms .This paper is a comparative study of feature selection in drug discovery .The focus is on aggressive dimensiona ... \" .Feature selection is frequently used as a preprocessing step to machine learning .", "label": "", "metadata": {}, "score": "52.804733"}
{"text": "3773 , pp .601 - 610 , Havana , Cuba , 2005 .M. S. M. El - Mahallawy , A Large scale HMM - based omni front - written OCR system for cursive scripts [ Ph.D. thesis ] , Department of Computer Engineering , Faculty of Engineering , Cairo University , Cairo , Egypt , April 2008 .", "label": "", "metadata": {}, "score": "52.807194"}
{"text": "In Section 2 Farsi writing characteristics are presented .The dataset that we use for building a dictionary is introduced in Section 3 .Wavelet packet transform theory is summarized in Section 4 .Then a brief overview of feature extraction architecture and dictionary building is explained in Section 5 .", "label": "", "metadata": {}, "score": "52.823"}
{"text": "A typical approach to key phrase assignment or extraction uses supervised learning , which is a machine learning technique of inferring a document 's characteristics from labeled training data .key phrases are groups of keywords , which consist of more than one word .", "label": "", "metadata": {}, "score": "52.830338"}
{"text": "Coherent key phrase extraction via web mining .In Proceedings of the 18th international joint conference on Artificial intelligence , ( pp .434 - 442 ) .San Francisco , CA : Morgan - Kaufmann .Wang , H. , Peng , H. & Hu , J. S. ( 2006 ) .", "label": "", "metadata": {}, "score": "52.855278"}
{"text": "I have triied TF , TF - IDF , MI , but the result is far from satisfactory .I think the reason is that all the keywords can represent the feature of the domain concretely or abstractly .The above method can not distinguish the degree .", "label": "", "metadata": {}, "score": "52.855785"}
{"text": "Finally , Rocchio classifier has the advantage over k - nearest neighbor classifier in the classification process .The experimental results illustrate that the proposed model is an efficient method and gives generalization accuracy of about 98 % . ... \" ...", "label": "", "metadata": {}, "score": "52.92086"}
{"text": "Therefore , it is possible to use valid classifiers to identify reliable components in input data and reuse them to automatically re - learn the wrapper .The set of features used for the wrapper repairing classifier include content , context and structure features .", "label": "", "metadata": {}, "score": "53.009827"}
{"text": "Wrapper example 1 .A sample answer produced by the wrapper for the DBLP site is shown in .FIG .2 and its corresponding HTML source is shown in .FIG .3 .In February 2001 , the page layout used for answers to title - relevant queries underwent some changes .", "label": "", "metadata": {}, "score": "53.028152"}
{"text": "\" We have A , B , C class of positive ( that the end user is interested in ) samples and then we intentionally created a much larger class D of negative samples(not belonging to any of the positive class ) .", "label": "", "metadata": {}, "score": "53.083485"}
{"text": "Next , all the verified angles of each pixel are counted into a 16-bin histogram that is then normalized to a probability distribution that gives the probability of finding an edge fragment oriented at the angle measured from the horizontal in the image .", "label": "", "metadata": {}, "score": "53.144344"}
{"text": "A graph model - driven concept extraction technique for biomedical literatures .International Journal of Data Mining and Bioinformatics .Turney , P. D. ( 2000 ) .Learning algorithms for key phrase extraction .Information Retrieval , 2 ( 3 ) , 303 - 336 .", "label": "", "metadata": {}, "score": "53.201763"}
{"text": "The feature vector is compressed by PCA , reducing the number of features from 729 to 100 .Computing the Average of 12 Feature Vectors Obtained from Subband [ 2 0 ] .The average computing stage of proposed algorithm is shown in Figure 7 .", "label": "", "metadata": {}, "score": "53.208744"}
{"text": "A large number of rules is needed for coverage of the domain , suggesting that a fairly large number of labeled examples should be required to train a classifier .However , we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple \" seed \" rules .", "label": "", "metadata": {}, "score": "53.294388"}
{"text": "These theoretical findings are supported by experiments on three test collections .The experiments show substantial improvements over inductive methods , especially for small training sets , cutting the number of labeled training examples down to a twentieth on some tasks .", "label": "", "metadata": {}, "score": "53.317207"}
{"text": "( 2004 ) .They compared the performance of two classifiers on three subgenres ( i.e. 93 PERSONAL HOME PAGE , 94 CORPORATE HOME PAGE and 74 ORGANIZATIONAL HOME PAGES ) with and without noise ( i.e. 77 non - home pages ) .", "label": "", "metadata": {}, "score": "53.37826"}
{"text": "First , we computed the number of exact match ( 100 % ) key phrases , i.e. whether the machine learning finds the exact words a human intends to describe , based on each perspective .The results were near to zero but the number of matching words suddenly increased when we applied fair ( 70 % ) match key phrases .", "label": "", "metadata": {}, "score": "53.465446"}
{"text": "the key phrase extraction algorithm uses the na\u00efve Bayes technique , which is simple and yields good results .In addition , we also use a decision tree technique to compare the performance with that from na\u00efve Bayes .The final component of the model is the number of positive and negative examples in the training data .", "label": "", "metadata": {}, "score": "53.471355"}
{"text": "To obtain the invariant features , preprocessing and feature extraction stages are done 12 times for 12 different shapes of the same subword .Then 12 feature vectors are entered to the averaging stage .Preprocessing and Size Normalization .Normalization methods aim to remove the variations of the fonts and obtain standardized data [ 8 ] .", "label": "", "metadata": {}, "score": "53.493458"}
{"text": "However , extracting f ... \" .Abstract .Information Extraction ( IE ) and Summarization share the same goal of extracting and presenting the relevant information of a document .While IE was a primary element of early abstractive summarization systems , it 's been left out in more recent extractive systems .", "label": "", "metadata": {}, "score": "53.50571"}
{"text": "We present two algorithms .The first method uses a similar algorithm to that of ( Yarowsky 95 ) , with modifications motivated by ( Blum and Mitchell 98 ) .The second algorithm extends ideas from boosting algorithms , designed for supervised learning tasks , to the framework suggested by ( Blum and Mitchell 98 ) . \" ...", "label": "", "metadata": {}, "score": "53.507736"}
{"text": "Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework . ... beled and unlabeled data ( semi - supervised and transductive learning ) has attracted considerable attention in recent years .We also note two regularization based techniques [ 16 , 7].", "label": "", "metadata": {}, "score": "53.549038"}
{"text": "View at Google Scholar .S.-H. Cha and S. Srihari , \" Multiple feature integration for writer verification , \" in Proceedings of the 7th International Workshop Frontiers in Handwriting Recognition , L. R. B. Schomaker and L. G. Vuurpijl , Eds . , pp .", "label": "", "metadata": {}, "score": "53.655247"}
{"text": "We review the literature on semi - supervised learning , which is an area in machine learning and more generally , artificial intelligence .There has been a whole spectrum of interesting ideas on how to learn from both labeled and unlabeled data , i.e. semi - supervised learning .", "label": "", "metadata": {}, "score": "53.692978"}
{"text": "We review the literature on semi - supervised learning , which is an area in machine learning and more generally , artificial intelligence .There has been a whole spectrum of interesting ideas on how to learn from both labeled and unlabeled data , i.e. semi - supervised learning .", "label": "", "metadata": {}, "score": "53.692978"}
{"text": "Any element of content in input data is labeled with a class l from a set L of classification labels .Consider two disjoint feature sets for input data , context features and content features for pages in questions .A context feature of a content string in HTML characterizes its surroundings , that is , the tag and textual tokens that precede ( prefix ) or follow the string ( suffix ) .", "label": "", "metadata": {}, "score": "53.829224"}
{"text": "FIG .4 .Each answer item on the original page ( .FIG .2 ) contains a number , title , one or more authors , conference , pages , reference and ( possibly ) a hyper - link to the electronic edition .", "label": "", "metadata": {}, "score": "53.877464"}
{"text": "Syntactic features may be the length , word counts , density of digits , upper - case and lower - case characters and standard delimiters ( comma , semicolon , blank , dot , etc . , dash ) and some others .", "label": "", "metadata": {}, "score": "53.902786"}
{"text": "Between three provider groups , the best results are obtained for the second group , where tokens extracted with semantic labels ( not none ) expose particular syntactic features and/or extracted information represents an important part of the page .Instead , for the first and third groups , extracted information represents a rather tiny part , making it difficult to distinguish between extracted and non - extracted ( labeled with none ) information , which results in a higher classification error .", "label": "", "metadata": {}, "score": "53.935623"}
{"text": "Snowball introduces novel strategies for generating patterns and extracting tuples from plain - text documents .At each iteration of the extraction process , Snowball evaluates the quality of these patterns and tuples without human intervention , and keeps only the most reliable ones for the next iteration .", "label": "", "metadata": {}, "score": "53.938175"}
{"text": "We collected full - text articles from Journal of Information Science published between 2004 and 2011 .The number of articles available is 330 in total ( Table 3 ) .Table 3 .The number of articles in each year in Journal of Information Science .", "label": "", "metadata": {}, "score": "53.966427"}
{"text": "They can be static like a CNN news page or dynamic like pages generated by a search engine in response to a user request .They can contain advertisements which change at the download time ; they can be well - formatted ( XHTML ) or not .", "label": "", "metadata": {}, "score": "53.972355"}
{"text": "by Ellen Riloff , Rosie Jones - in AAAI'99/IAAI'99 - Proceedings of the 16th National Conference on Artificial Intelligence & amp ; 11th Innovative Applications of Artificial Intelligence Conference . \" ...Information extraction systems usually require two dictionaries : a semantic lexicon and a dictionary of extraction patterns for the domain .", "label": "", "metadata": {}, "score": "54.07247"}
{"text": "A classifier built from simple content features is rarely 100 % accurate ; often only some strings can be correctly identified with high accuracy .In the DBLP sample , only comma separators between authors ( labeled with none ) are accurately identified from content features .", "label": "", "metadata": {}, "score": "54.150623"}
{"text": "In my experience , noise is a hard problem to solve , and it is still an open research question .Handling noise with supervised machine learning is challenging and often unrewarding .But you might have a different experience and find a good solution .", "label": "", "metadata": {}, "score": "54.321754"}
{"text": "Due to problems associated with nonobjective measurements and nonreproducible decisions , recent attempts have been made to support traditional methods with computerized semiautomated and interactive systems .For a survey covering work in automatic writer identification and signature verification until the end of the 1980 's see [ 8 ] .", "label": "", "metadata": {}, "score": "54.360832"}
{"text": "This causes inequality in the results of machine - learning , when using different people 's document representations for intellectual clues .Regarding this , Marion and McCain ( 2001 ) argue that authors view a discipline from the perspective of insiders , whereas indexers do so as outsiders .", "label": "", "metadata": {}, "score": "54.406303"}
{"text": "The algorithm considers each edge pixel in the middle of a .-square neighborhood , as shown in Figure 6 .The algorithm then checks in all directions using the logical AND operator starting from the central pixel and ending on one of the edges of the . square ; all the pixels shall be in the same connected component .", "label": "", "metadata": {}, "score": "54.54933"}
{"text": "There are no works based on holistic recognition of printed Farsi subwords by their wavelet packet features which are font invariant and size invariant .In this paper , we are going to present a novel shape descriptor for the recognition of the printed Persian subwords independent of font and size variations that build a dictionary .", "label": "", "metadata": {}, "score": "54.56749"}
{"text": "Introduction .The indexer effect has been studied in several research studies in the field of information science to reveal intellectual structures .In this study , we bring that concept into document classification to verify whether it also influences the results in key phrase extraction .", "label": "", "metadata": {}, "score": "54.632427"}
{"text": "The degree of a term is the number of semantic links that connect the term to other phrases in the document that have been identified as candidate phrases .Additionally , we use the length of phrases in words feature in our study .", "label": "", "metadata": {}, "score": "54.69479"}
{"text": "This is important because in many text classification problems obtaining training labels is expensive , while large quantities of unlabeled documents are readily available .We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation - Maximization ( EM ) and a naive Bayes classifier .", "label": "", "metadata": {}, "score": "54.721344"}
{"text": "The Data Set .The used database in this work is prepared from Persian subwords which are printed by a laser printer in four fonts : Nazanin , Zar , Lotus , and Mitra and three sizes : 12 , 14 , and 16 and scanned in 400 dpi .", "label": "", "metadata": {}, "score": "54.765522"}
{"text": "s idea .More recently , Filatova and Hatzivassiloglou [ 21 ] considered the contexts involving any pair of names as general ' events ' and used them to improve extractive summarization .Biadsy et al .[ 23 ] exploited entity and time facts extracted from IE to improve s .. \" ... Online Social Networks ( OSNs ) allow users to create and share content ( e.g. , posts , status updates , comments ) in real - time .", "label": "", "metadata": {}, "score": "54.811657"}
{"text": "There has been a great attempt to produce Omnifont OCR systems for Persian / Arabic languages , but the overall performance of such systems is far from being perfect .Persian written language which uses modified Arabic alphabet is written cursively , and this intrinsic feature makes it difficult for automatic recognition [ 6 ] .", "label": "", "metadata": {}, "score": "54.896732"}
{"text": "An algorithm for suffix stripping . program , 14 ( 3 ) , 130 - 137 .Rolling , L. ( 1981 ) .Indexing consistency , quality and efficiency .Information Processing & Management , 17 ( 2 ) , 69 - 76 .", "label": "", "metadata": {}, "score": "54.992577"}
{"text": "Second , they must be diverse in terms of topic , e.g. , cover several of the major topics in the activity log .Third , they should be time - dispersed , that is , be spread across the specified time range of the activity log .", "label": "", "metadata": {}, "score": "55.25067"}
{"text": "Preprocessing for Recognition Process .In preprocessing stage , we have divided lines and then set apart subwords [ 37 ] .There is a free space between each two words as well , and we used this feature to segment words .", "label": "", "metadata": {}, "score": "55.257736"}
{"text": "They select words to represent documents and thereby make information retrieval more effective and efficient .However , providing proper terms about each document manually is laborious and time - consuming , and it becomes a daunting task due to the rapid increase in electronic data .", "label": "", "metadata": {}, "score": "55.346855"}
{"text": "The reason for using these fonts is their popularity in Farsi magazines , newspapers , books , and official documents .Wavelet Packet Transform Theory .The wavelet packet method is a generalization of wavelet decomposition [ 29 - 32 ] that offers a richer signal analysis than wavelets .", "label": "", "metadata": {}, "score": "55.375675"}
{"text": "This feature vector for one subword with four fonts , Zar , Mitra , Lotus , and Nazanin , and in three sizes 12 , 14 and 16 is invariant .The obtained feature vectors are used in dictionary building .From Section 5.1 to Section 5.3 , we illustrate the preprocessing and feature extraction stages to generate 7317 entries that represent 7317 Persian subwords in one pictorial dictionary .", "label": "", "metadata": {}, "score": "55.414375"}
{"text": "It then trains a new classifier using the labels for all the documents , and iterates to convergence .This basic EM procedure works well when the data conform to the generative assumptions of the model .However these assumptions are often violated in practice , and poor performance can result .", "label": "", "metadata": {}, "score": "55.417194"}
{"text": "4768 of Lecture Notes in Computer Science , pp .22 - 35 , Springer , 2008 .R. Coifman , Y. Meyer , and V. Wickerhauser , \" Wavelet analysis and signal processing , \" in Wavelets and Their Applications , M. B. Ruskai , Ed . , pp .", "label": "", "metadata": {}, "score": "55.47017"}
{"text": "In this paper , a feature extraction system for complete set of printed Persian subwords employing wavelet packet descriptors is presented .We used Symlet 8 basis function for wavelet packet transform to obtain the subword 's image features .We apply the wavelet packet of level two of subband . since higher level of subbands did not give better results , yet caused more complexity .", "label": "", "metadata": {}, "score": "55.472622"}
{"text": "In their case , the \" noise \" was represented by documents that did not fall into the three subgenres to be identified , but belonged to other genres .In their experiment , Shepherd et al .( 2004 ) conflated into one single class all the genre classes not being \" home pages \" .", "label": "", "metadata": {}, "score": "55.5586"}
{"text": "The feature set size was reduced by 99 % , while losing only a few percent in terms of sensitivity ( from 58.7 % to 52.5 % ) and specificity ( from 98.4 % to 97.2 % ) .In contrast to information gain and \u03c7 2-test , mutual information had relatively poor performance due to its bias toward favoring rare features and its sensitivity to probability estimation errors .", "label": "", "metadata": {}, "score": "55.59375"}
{"text": "Finally , the edge detection process is described in Section 5.4 .Page Scanning .The completed forms were scanned at a resolution of 200 by 200 pixels .The forms were stored using the writers ' identification numbers .Because we had a total of 100 writers , each writer was assigned a number from 001 - 100 .", "label": "", "metadata": {}, "score": "55.645283"}
{"text": "For all providers we have generated context and content classifiers .For context classifiers , the OCER algorithm implemented in Iwrap toolkit developed at Xerox Research Centre Europe was used .For content classifiers , decision trees were used as the underlying learning system .", "label": "", "metadata": {}, "score": "55.658615"}
{"text": "137 - 140 , Istanbul , Turkey , February 2005 .J. Shanbehzadeh , H. Pezashki , and A. Sarrafzadeh , \" Features extraction from persian hand written letters , \" in Proceedings of the Image and Vision Computing New Zealand ( IVCNZ ' 07 ) , pp .", "label": "", "metadata": {}, "score": "55.713703"}
{"text": "However , the precision is down - valued by few misclassified tokens , as the format change confused some wrapper extraction rules .Finally , for the Financial Times wrapper , the basic recovery finds 3 elements of 5 and the backward wrapper and content classifier help find two missing ones .", "label": "", "metadata": {}, "score": "55.776062"}
{"text": "Moment Invariants .For text - dependent writer identification , we extracted the moment features .Pattern recognition using moment invariants uses a set of seven equations [ 21 ] as follows : . 2 . ]The seven moments were added in one feature vector , and we used standardization on this vector .", "label": "", "metadata": {}, "score": "55.870255"}
{"text": "One solution to the problem of wrapper maintenance detects page changes within a defined level of accuracy .When the change is detected , the designer is notified so that the wrapper can be regenerated from samples of the changed pages .", "label": "", "metadata": {}, "score": "56.016922"}
{"text": "Adequate representation of natural language semantics requires access to vast amounts of common sense and domain - specific world knowledge .Prior work in the field was based on purely statistical techniques that did not make use of background knowledge , on limited lexicographic knowledge bases such as WordNet , or on huge manual efforts such as the CYC project .", "label": "", "metadata": {}, "score": "56.074326"}
{"text": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data .Methods that use both labeled and unlabeled data are generally referred to as semi - supervised learning .", "label": "", "metadata": {}, "score": "56.101654"}
{"text": "One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data .Methods that use both labeled and unlabeled data are generally referred to as semi - supervised learning .", "label": "", "metadata": {}, "score": "56.101654"}
{"text": "The frequency score compares the frequency of a phrase 's use in a particular document with the frequency of that phrase in general use .General usage is identified by the number of documents containing the phrase in the global corpus of documents .", "label": "", "metadata": {}, "score": "56.210117"}
{"text": "After splitting , we obtain a subimage of smooth ( low pass ) coefficients and three subimages corresponding to detail coefficients along horizontal , vertical , and diagonal directions .This is followed by splitting the low - pass subimage , while the successive details are never reanalyzed .", "label": "", "metadata": {}, "score": "56.21885"}
{"text": "In recent years , many OCR researches have extracted subword features .In the number of these researches , holistic shape information from subwords is extracted for modeling subwords [ 2 ] .In this work , we want to extract holistic shape features of printed Persian subwords using wavelet packet transform to build a pictorial dictionary .", "label": "", "metadata": {}, "score": "56.285133"}
{"text": "Measuring the inter - indexer consistency between authors and indexers .In this section , we firstly measured the inter - indexer consistency between authors and indexers for the same articles using Rolling 's measure .All the inter - indexer consistency between the two groups is quite low ( 0.11 - 0.21 ) , except for articles in the Journal of Information Science in 2004 , which is 0.49 ( Table 5 ) .", "label": "", "metadata": {}, "score": "56.351784"}
{"text": "To solve this chicken - and - egg problem , we use the basic classifier C 1 on content feature set F C for bootstrapping of the wrapper repair process .Then , structure features for all labels detected by the basic classifier are combined with the content features to build a new , more refined classifier C 2 .", "label": "", "metadata": {}, "score": "56.41372"}
{"text": "For instance , terms like retrieval , retrieving ' , and retrieved are stemmed to the stem retriev .However , the stemming technique alone can not resolve the issue of mismatching due to term variation .In our experiment , any partial match is empirically set to 70 % .", "label": "", "metadata": {}, "score": "56.545815"}
{"text": "The higher the Jaro - Winkler distance for two strings , the more similar the strings are .The score is normalised such that 0 equates to no similarity and 1 is an exact match .For evaluation , inter - indexer consistency and agreement are commonly used in the context of human indexing , while precision and recall are used to evaluate automatic systems ( Medelyan et al . , 2008 ) .", "label": "", "metadata": {}, "score": "56.580425"}
{"text": "Algorithm 2 below completes the information extraction and recovery performed by Algorithm 1 .Algorithm 2 runs when Algorithm 1 returns false and fails to accurately complete the information extraction .Algorithm 2 switches the file scan direction and tries to classify not yet classified textual tokens in E probing their prefixes and suffixes with forward and backward wrappers , respectively .", "label": "", "metadata": {}, "score": "56.706257"}
{"text": "For example , DBLP wrapper has three such labels , namely number , ee and pages , they can be accurately identified by their content ( see .FIG .2 ) .Recovery tests .Methods of information extraction recovery described above have been implemented in the Iwrap prototype at Xerox Research Centre Europe .", "label": "", "metadata": {}, "score": "56.727383"}
{"text": "For each provider , 10 \" before - change \" pages have been used for learning extraction rules and content classifiers before the format change and 10 \" after - change \" pages have been used for testing the recovery routine .", "label": "", "metadata": {}, "score": "56.767643"}
{"text": "A complete handwritten document management system known as CEDAR - FOX has recently been developed by Srihari et al .[16 ] for writer verification .As a document management system for forensic analysis , CEDAR - FOX provides users with three major functionalities : a document analysis system ; a system for creating a digital library ; and a database management system for document retrieval and writer identification .", "label": "", "metadata": {}, "score": "56.768517"}
{"text": "But our NB classifier gives more false positives with increasing negative class size ( Do n't know if the algorithm has some bug or it is inline with the principles of NBC ? )After this we also experimented with multiple negative classes but none of them bigger than any of the positive classes .", "label": "", "metadata": {}, "score": "56.820408"}
{"text": "Discriminability and coverage are separately measured ; by adjusting the ratio of their weights in a combined criterion , the expected average vector length can be reached , which means a good compromise between the specificity and the exhaustivity of the term subset .", "label": "", "metadata": {}, "score": "56.861465"}
{"text": "Object tracking , in general , is a challenging problem .Difficulties in tracking objects can arise due to abrupt object motion , changing appearance patterns o ... \" .The goal of this article is to review the state - of - the - art tracking methods , classify them into different categories , and identify new trends .", "label": "", "metadata": {}, "score": "56.95447"}
{"text": "Even though they are information specialists , indexers may not be deeply knowledgeable in many academic areas .In addition , compared to the small number of indexers , the enormous number of works they are dealing with forces them to complete their work hastily and , perhaps , imprecisely , spending not enough time reading an article in detail .", "label": "", "metadata": {}, "score": "56.956116"}
{"text": "The most notable feature of Farsi writing is its cursiveness .Compared to other features , the cursiveness of Farsi / Arabic words poses the most difficult problem for recognition algorithms [ 1 ] ; this is why segmentation is a crucial step for many Farsi / Arabic character recognition systems .", "label": "", "metadata": {}, "score": "56.98056"}
{"text": "Here , we examine the impact of frequency on var - ious aspects of summarization and the role of fre - quency in the design of a summarization system .We describe SumBasic , a summarization system that exploits frequency exclusively to create sum - maries .", "label": "", "metadata": {}, "score": "57.00733"}
{"text": "Although seven Farsi characters out of 32 do not connect to their left neighbors or the next letter [ 14 , 24 , 25 ] , the others join to the neighboring characters to make a word or subword .These seven letters are ( ( alef ) \u0627 , ( dal ) \u062f , ( zal ) \u0630 , ( re ) \u0631 , ( ze ) \u0632 , ( je )", "label": "", "metadata": {}, "score": "57.010933"}
{"text": "Based on the preprocessed input images , the feature extraction is performed [ 36 ] .The obtained subword image from normalization stage is decomposed using wavelet packet transform ( WPT ) with basis function Symlet 8 up to level two .", "label": "", "metadata": {}, "score": "57.224014"}
{"text": "A wrapper may be generated using one of various techniques , including those techniques which use an induction method .In an induction technique , several labeled sample HTML pages 12 from Web provider 10 are provided to wrapper induction engine 14 .", "label": "", "metadata": {}, "score": "57.26699"}
{"text": "These techniques are used in the method of automatic wrapper maintenance and are described generally below .The most significant feature of regular transducers is that they can be learned from positive examples .Most wrappers target the extraction of textual tokens and components of some tag tokens , for example , href attribute of , tags .", "label": "", "metadata": {}, "score": "57.348923"}
{"text": "4-Angle Edge - Direction Distribution .After finding a black edge pixel , the program considers this pixel as the center of a .-square neighborhood .Then the black edge was checked using the logical AND operator , in all directions starting from the central pixel and ending at one of the edges of the . square .", "label": "", "metadata": {}, "score": "57.457287"}
{"text": "Feature calculation .A simple but robust machine - learning scheme is used to determine the final set of key phrases for a document .It uses a set of attributes as input , which is defined for each candidate term .", "label": "", "metadata": {}, "score": "57.476753"}
{"text": "Our method represents meaning in a high - dimensional space of concepts derived from Wikipedia , the largest encyclopedia in existence .We explicitly represent the meaning of any text in terms of Wikipedia - based concepts .We evaluate the effectiveness of our method on text categorization and on computing the degree of semantic relatedness between fragments of natural language text .", "label": "", "metadata": {}, "score": "57.570755"}
{"text": "Content classifiers .In this section , token classification by content features only are considered .Syntactic features include token length , word counts , density of digits , upper - case and lower - case characters and standard delimiters ( comma , semicolon , dot , etc . ) .", "label": "", "metadata": {}, "score": "57.74885"}
{"text": "As for the extraction rules ( see Table 1 , right part ) , not only rules for number and ee disappear , but one of two rules for author changes , too .The only rule for author that does not change , r 9 refers to the extraction of second , third , etc . authors of a given paper .", "label": "", "metadata": {}, "score": "57.821358"}
{"text": "recall . correct . extracted . correct .First the changes which happened to all providers were classified .Among three possible format changes , of primary interest are context and structural ones .In the case of content change , the wrapper action is to notify the designer and it does not influence the recovery mechanism .", "label": "", "metadata": {}, "score": "57.84433"}
{"text": "A successful recovery is sent to validation of extracted data 22 .Any changes detected are provided to change detection 28 , which results in automatic re - labeling of new samples that can then be used to generate a new wrapper ( or repair the old wrapper ) that accommodates the new page format .", "label": "", "metadata": {}, "score": "57.954445"}
{"text": "The results showed that Na\u00efve Bayesian benefited significantly from the feature selection , while SVM performed better when all features were used .In this experiment , information gain and \u03c7 2-test were most effective feature selection methods .Using information gain with a Na\u00efve Bayesian classifier , removal of up to 96 % of the features yielded an improved classification accuracy measured by sensitivity .", "label": "", "metadata": {}, "score": "58.121536"}
{"text": "Subband .coefficients formed a 27 \u00d7 27 matrix from which a 1 \u00d7 729 vector is formed .Then the obtained vector is compressed by PCA , reducing the number of features to 100 to produce the subword feature vector .", "label": "", "metadata": {}, "score": "58.15236"}
{"text": "To create the feature vector , the subword image is normalized to 64 \u00d7 64 pixels .After the subword 's image is scanned in the system , the proposed algorithm will produce a decomposition vector .This vector is uniquely representing input image .", "label": "", "metadata": {}, "score": "58.181496"}
{"text": "Extraction recovery targets the identification of labeled strings in the page ; it is aimed at extracting as much relevant data as possible , yet it does not necessarily lead to wrapper repair .Wrapper repair may be accomplished if the extraction recovery is sufficiently successfully .", "label": "", "metadata": {}, "score": "58.196457"}
{"text": "The position of first occurrence of a term is calculated as the distance of a phrase from the beginning of a document in words , normalised by the total number of words in the document .The result represents the proportion of the document preceding the phrase 's first appearance .", "label": "", "metadata": {}, "score": "58.294422"}
{"text": "The volunteers who completed the forms came from various age groups and levels of education .Preprocessing .The preprocessing step in this paper begins with page scanning , as detailed in Section 5.1 .Next , we describe the document segmentation step , which segments a page into words and stores each word in a separate file .", "label": "", "metadata": {}, "score": "58.484337"}
{"text": "Accurate OCER wrappers have been successfully regenerated for all 17 providers .Providers in the first and third groups have often advertisement - padded pages ; corresponding wrappers have multiple extraction rules for the none class .Although the complexity of extracted information is higher in the second group , the third group requires more pages to learn accurate extraction rules .", "label": "", "metadata": {}, "score": "58.566017"}
{"text": "Table 9 : T - test of performance between author 's keywords and indexers ' terms from journals in different fields .Test Set .t - value .p - value .Discussion .Findings from the experiment are summarised as follows .", "label": "", "metadata": {}, "score": "58.583885"}
{"text": "The support for text data in ODM is different from that provided by Oracle Text .Oracle Text is dedicated to text document processing .ODM allows the combination of text ( unstructured ) columns and non - text ( categorical and numerical ) columns of data as input for clustering , classification , and feature extraction .", "label": "", "metadata": {}, "score": "58.748737"}
{"text": "All contexts are minimal and optimal ; shortening any context would make the rule set R ambiguous .The sum of prefix and suffix lengths in a rule gives a context width .For the DBLP wrapper , the maximal content width equals 3 ; in more complex cases , the maximal context might be wider .", "label": "", "metadata": {}, "score": "58.769806"}
{"text": "\" Most studies conclude that stemming of English text is beneficial , but this issue is controversial in studies related to Arabic stemming .Therefore a number of studies such as [ 4 ] [ 5 ] concludes and asserts the effectiveness of stemming , while other studies conclude it is harmful and it degrades the performance of the system using it .", "label": "", "metadata": {}, "score": "58.800957"}
{"text": "The paper concludes in Section 7 .Some Characteristic of Farsi Script .In this section , we will briefly describe some of the main characteristics of Persian script to point out the main difficulties which an OCR system should overcome .", "label": "", "metadata": {}, "score": "58.810543"}
{"text": "Sociological Methods and Research ( 2004 ) .Except for the data in 2004 , the average of the inter - indexer consistency of documents included in Journal of Information science from 2005 to 2011 is 0.18 .Other journals maintain similar inter - indexer consistency without being influenced by the temporal variance .", "label": "", "metadata": {}, "score": "58.93164"}
{"text": "Their average is computed in a 1 \u00d7 100 vector as the subword feature that is invariant in font and size .The pictorial dictionary is built using the feature vectors of all subwords .The dataset consists of 12 groups with the four fonts , including Mitra , Zar , Nazanin , and Lotus , in three sizes 12 , 14 , and 16 of Persian subwords .", "label": "", "metadata": {}, "score": "59.43422"}
{"text": "ODM also supports feature extraction from text .Most text mining is focused on this problem .Extract semantic features or higher - level features from the basic features uncovered when features are extracted from actual text .Statistical techniques , such as single value decomposition ( SVD ) and non - negative matrix factorization ( NMF ) , are important in solving this kind of problem .", "label": "", "metadata": {}, "score": "59.45166"}
{"text": "This invention relates generally to wrappers , and more particularly to a method for automatic repair of wrappers .BACKGROUND AND SUMMARY .A wrapper is a type of software component or interface that is tied to data which encapsulates and hides the intricacies of an information source in accordance with a set of rules .", "label": "", "metadata": {}, "score": "59.54455"}
{"text": "SVM is known to work well with text data .For more information about SVM , see Chapter 3 .There are two kinds of text mining problems for which feature extraction is useful : .Extract features from actual text .", "label": "", "metadata": {}, "score": "59.735474"}
{"text": "Thank you again ^-^ .Actually i m working on unsupervised naive bayes algorithm for text classification .Here lies my problem : .Now i want my algo should signify me that this new document is not from the trained(above ) categories .", "label": "", "metadata": {}, "score": "59.746117"}
{"text": "The joint use of forward and backward wrappers in combination transforms the recovery procedure from one - pass scan into multi - pass one ; moreover during the recovery the direction of the file scan can change one or more times .", "label": "", "metadata": {}, "score": "59.761375"}
{"text": "The number of articles , index terms , and keywords for each year is as follows in Table 4 : .Table 4 .The number of articles , index terms and keywords for test data .Sociological Methods and Research ( 2011 ) .", "label": "", "metadata": {}, "score": "59.785843"}
{"text": "In addition , the OCER is incremental ; it aligns an input token consumption with the classification .It replaces emission delays with corresponding lookaheads in the input data ; these lookaheads are given by suffixes in rules .Finally , OCER method disregards variations in input that are irrelevant to the result information extraction .", "label": "", "metadata": {}, "score": "59.820168"}
{"text": "During the implementation stage , each word image is decomposed by 4 levels of wavelet transform .Wavelet features used in this system are extracted from the LL 4 subband that represents the low - frequency components in level 4 .The wavelet transform is a tool that has been applied in many disciplines , including image processing [ 22 , 23 ] .", "label": "", "metadata": {}, "score": "59.872017"}
{"text": "In all datasets we experiment with 9 different threshold values for RAKEL , ranging from 0.1 to 0.9 with a 0.1 step .We also exp ... . \" ...Adequate representation of natural language semantics requires access to vast amounts of common sense and domain - specific world knowledge .", "label": "", "metadata": {}, "score": "59.88816"}
{"text": "When Web provider 10 changes the format of a Web page , the wrapper 16 may be unable to match some strings to extraction rules .The wrapper runs in an error and triggers the recovery component which attempts to resume the information extraction and repair the wrapper .", "label": "", "metadata": {}, "score": "59.95933"}
{"text": "Height from the Baseline to the Upper Edge .To find the height of the text from the baseline to the upper edge ( see Figure 9 ) , the algorithm first determines the baseline position in the image .This is performed by forming an array where the index is the row number in the image .", "label": "", "metadata": {}, "score": "60.01942"}
{"text": "An encouraging recognition rate of 97.9 % is got at subword level recognition .Introduction .Optical character recognition ( OCR ) is one of the oldest subfields of pattern recognition with a rich contribution for the recognition of printed documents .", "label": "", "metadata": {}, "score": "60.345383"}
{"text": "Acknowledgment .This work is supported by the East Azarbaijan Telecommunication company , Tabriz , Iran .References .B. Albadr and R. M. Haralick , \" A segmentation - free approach to text recognition with application to arabic text , \" International Journal on Document Analysis and Recognition , vol .", "label": "", "metadata": {}, "score": "60.385117"}
{"text": "The invention has been described with reference to a particular embodiment .Modifications and alterations will occur to others upon reading and understanding this specification taken together with the drawings .The embodiments are but examples , and various alternatives , modifications , variations or improvements may be made by those skilled in the art from this teaching which are intended to be encompassed by the following claims .", "label": "", "metadata": {}, "score": "60.564796"}
{"text": "Policy and the mapping of scientific chance : a co - word analysis of research into environmental acidification .Scientometrics , 14 ( 3 ) , 251 - 264 .Mai , J. E. ( 2001 ) .Semiotics and indexing : an analysis of the subject indexing process .", "label": "", "metadata": {}, "score": "60.767426"}
{"text": "If the extraction recovery is not sufficiently successful or sufficiently complete , the wrapper may not be repaired automatically and user intervention may be required .If the provider 10 changes the format of its HTML pages , and the wrapper 16 fails to complete parsing and information extraction , the web page 18 is provided to extraction recovery 26 .", "label": "", "metadata": {}, "score": "60.796665"}
{"text": "However , users find that it is often easier to relearn or regenerate a broken wrapper than to repair it .However , relearning requires user intervention that is not always available .Moreover , a regenerated wrapper is not scalable if changes occur frequently .", "label": "", "metadata": {}, "score": "60.966286"}
{"text": "In Figure 3 , the pixels from center to pixel 1 are considered as long edge fragments .Figure 3 : Extraction of the edge - direction distribution using four angles .8-Angle Edge - Direction Distribution .As in the 4-angle edge - directional distribution , our program considers a pixel in the middle of a .", "label": "", "metadata": {}, "score": "60.984535"}
{"text": "The following equation has been used for this purpose : . where . and . are the mean and standard deviation , respectively , of feature .in the training examples [ 21 ] .The following sections describe in detail how we computed the area , length , height , length from the baseline to the upper edge , and the length from the baseline to the lower edge .", "label": "", "metadata": {}, "score": "61.149223"}
{"text": "FIG .3 .The rule for these later tokens ( labeled as none ) does not change either .A transducer wrapper is considered a partial classifier , where each label , including none , is characterized by a set of associated extraction rules ( see Table 1 ) .", "label": "", "metadata": {}, "score": "61.293625"}
{"text": "This double role confirms the use of two threshold parameters in Algorithm 1 .Algorithm 3 is a sequential combination of two alternative views of input data , given by an OCER wrapper ( W - classifier ) and content ( C- ) classifier .", "label": "", "metadata": {}, "score": "61.494827"}
{"text": "We hypothesize that the variance in volume of high - velocity queries over time can be explained by observing that these queries are formulated in response to events in the world that users are interested in .Based on it , this paper describes a system , ZED , which automatically finds explanations for high velocity queries , by extracting descriptions of relevant and temporally - proximate events from the news stream .", "label": "", "metadata": {}, "score": "61.55462"}
{"text": "Many Farsi characters have dots , which are positioned above or below the letter body .Dots can be single , double , or triple [ 26 ] .( vi )Each word , machine - printed or handwritten , may consist of several separated subwords .", "label": "", "metadata": {}, "score": "61.652863"}
{"text": "FIG .7 illustrates iterative repair of the wrapper for the DBLP web site .FIG .8 illustrates recovery with two classifiers in algorithm 3 .FIG .9 illustrates recovery with forward and backward T - content classifiers .FIGS .", "label": "", "metadata": {}, "score": "61.690147"}
{"text": "Wrappers fetch HTML pages , static or ones generated dynamically upon user requests , extract relevant information and deliver it to the application , often in XML format .Web wrappers include a set of extraction rules that instruct an HTML parser how to extract and label content of a web page .", "label": "", "metadata": {}, "score": "61.80977"}
{"text": "Marion , L. S. & McCain , K. W. ( 2001 ) .Contrasting views of software engineering journals : author co - citation choices and indexer vocabulary assignments .Journal of the American Society for Information Science , 52 ( 4 ) , 297 - 306 .", "label": "", "metadata": {}, "score": "61.82741"}
{"text": "-level decomposition [ 31 ] .In the corresponding wavelet packet situation , each detail coefficient subimage is also decomposed into four parts using the same approach as in low - pass subimage splitting [ 30 - 33 ] .So , there are . which provide a better tool for image analysis [ 31 , 33 ] .", "label": "", "metadata": {}, "score": "61.90223"}
{"text": "If tag precedes a current textual token , then it will be labeled as title if the previous textual token is author and as pages if the previous token is conference .The DBLP wrapper in this example contains 12 extraction rules , which are listed in Table 1 .", "label": "", "metadata": {}, "score": "61.90471"}
{"text": "Europ .Conf Machine Learning , Germany , Freiburg , volume 2167 of Lect .Notes Comp .Sci . , pages 61 - 72 .Springer , 2001 , describes a method that applies the transducer induction of the OSTI algorithm to the conventional representation of wrappers as sets of extraction rules .", "label": "", "metadata": {}, "score": "61.962242"}
{"text": "A re - designed site will usually require regenerating the wrapper .However , most changes to Web pages are small and localized in nature , including small changes in the page mark - up , small changes in the content information , and possibly the addition or deletion of a label .", "label": "", "metadata": {}, "score": "62.0111"}
{"text": "Abstract .We hypothesize that the variance in volume of high - velocity queries over time can be explained by observing that these queries are formulated in response to events in the world that users are interested in .Based on it , this paper describes a system , ZED , which automatically finds explanat ... \" .", "label": "", "metadata": {}, "score": "62.03148"}
{"text": "Figure 5 : Extraction of the edge - direction distribution using 12 angles .All of the verified angles of each pixel are counted into a twelve - bin histogram that is then normalized to a probability distribution that gives the probability of finding an edge fragment oriented at the angle measured from the horizontal in the image .", "label": "", "metadata": {}, "score": "62.045124"}
{"text": "Importantly , due to the use of natural concepts , the ESA model is easy to explain to human users . in Section 4.2 , and rare features occurring in fewer than 3 documents are removed .The ... . \" ...", "label": "", "metadata": {}, "score": "62.224045"}
{"text": "The indexer 's effect has been widely investigated in co - word analysis ( Callon et al .1986 ; Law et al .1988 ; Whittaker 1989 ; Marion and McCain 2001 ) , but rarely studied in other topics .", "label": "", "metadata": {}, "score": "62.33093"}
{"text": "They present an algorithm that classifies named entities with high accuracy .[ 13 ] presents a bootstrapping technique to extract patterns to recognize and classify named entities in text .While the underlying pri ... .by Mark Craven , Dan DiPasquo , Dayne Freitag , Andrew McCallum , Tom Mitchell , Kamal Nigam , Sean Slattery , 1998 . \" ...", "label": "", "metadata": {}, "score": "62.366035"}
{"text": "To test the statistical significance of the two different perspectives , we conducted a t - test .Table 10 below shows a statistically significant difference of performance between author 's keyword ( kyw ) and indexer 's term ( idx ) exists .", "label": "", "metadata": {}, "score": "62.57817"}
{"text": "2 . FIG .4 is the sample answer produced by a wrapper for the DBLP web site after the web site was changed .FIG .5 illustrates repairing information extraction from the sample of .FIG .4 .FIG .", "label": "", "metadata": {}, "score": "62.897766"}
{"text": "If not let me know .Cheers , Marina .I think i see .and , i have another question about feature extraction .It is a question in my dissertation .I want to extrat core concepts from keywords set which are the keywords in the dissertations of information science .", "label": "", "metadata": {}, "score": "63.44035"}
{"text": "View at Google Scholar .M. Feil and A. Uhl , \" 2-D Wavelet packet decomposition on multicomputers , \" in Proceedings of the 8th Euromicro Workshop on Parallel and Distributed Processing , pp .351 - 356 , Rhodos , Greece , January , 2000 .", "label": "", "metadata": {}, "score": "63.68051"}
{"text": "I will call this type of noise structured noise because this noise is represented by well - defined genre classes that should always be a negative for a classifier .A slightly different approach to structured noise is used by Kim and Ross ( 2010 and Vidulin et al .", "label": "", "metadata": {}, "score": "63.706593"}
{"text": "Meen Chul Kim is an assistant researcher of Library and Information Science at Yonsei University , Seoul , Korea .His research interests include Information retrieval and Text Mining from the Web and Social Media , and Information System Evaluation .He can be contacted at andrewevans@yonsei.ac.kr .", "label": "", "metadata": {}, "score": "64.273705"}
{"text": "( vii )The neighboring letters in words or subwords may overlap vertically depending on their shapes [ 1 , 14 , 24 , 25 ] .Some characteristics of Farsi script are shown in Figure 2 [ 24 , 25 , 27 ] .", "label": "", "metadata": {}, "score": "64.46407"}
{"text": "On the other hand , it should be also noted that automatically extracted key phrases might lead users to irrelevant documents in information retrieval .Introduction .Drowning in the flood of data that is overwhelming in this information age , people barely keep their head above water .", "label": "", "metadata": {}, "score": "64.537125"}
{"text": "The first group includes Altavista , Google , Excite , Yahoo , Metasearcher , Go , Deja and CNN search engines .The second group includes DBLP , ACM , IEEE , Elsevier and Cora search facilities .Wrappers in the two groups extract \" multi - slot multi - value \" information , that is , the result is a list of items and each item contains a number of ( value , label ) pairs .", "label": "", "metadata": {}, "score": "64.85163"}
{"text": "107 - 131 , 1989 .View at Google Scholar \u00b7 View at Scopus .F. Leclerc and R. Plamondon , \" Automatic signature verification : the state of the art 1989 - 1993 , \" in Progress in Automatic Signature Verification , R. Plamandon , Ed . , pp .", "label": "", "metadata": {}, "score": "65.25369"}
{"text": "( iii )The Farsi characters of a word are connected along a baseline [ 1 , 26 ] .A baseline is the line with the highest density of black pixels .The existence of the baseline calls for different segmentation methods from those used in other unconnected scripts .", "label": "", "metadata": {}, "score": "65.408195"}
{"text": "The filter used here is symlet 8 , and only two levels of decomposition are performed resulting in 16 subimages ( subbands ) .are differently corresponding to the different wavelet basis functions .Wavelet packet decomposition has been successfully applied to image analysis [ 32 ] .", "label": "", "metadata": {}, "score": "65.42658"}
{"text": "For a given image , the wavelet packet transform produces a low - frequency subband image reflecting its basic shape and three subband images that contain the high - frequency components of the image at horizontal , vertical , and diagonal directions .", "label": "", "metadata": {}, "score": "65.43746"}
{"text": "Web of Science .Web of Science .As shown above , although some words from each perspective are identical , others differ slightly .Among the given words , six of them are exactly the same : online information retrieval , searching , user interface , user satisfaction , retrieval performance measures , and Web of Science .", "label": "", "metadata": {}, "score": "65.61972"}
{"text": "Recovery 3 that adds the perfect content classification rules , improves recall values further , although its gain is less important than with Recovery 2 .In total , for 6 providers , the recovery routines allowed to re - enter the initial 0.98-accuracy box , and for 10 more providers , the recovery achieved 0.95 values for both precision and recall .", "label": "", "metadata": {}, "score": "65.63046"}
{"text": "Wavelet transform has been widely used in image processing and signal processing for image / signal enhancement , denoising , texture segmentation [ 7 ] based on its properties of short support , orthogonality , symmetry , higher order of vanishing moments , and more importantly its multiresolution decomposition analysis .", "label": "", "metadata": {}, "score": "65.8303"}
{"text": "Seventeen ( 17 )Web information providers were selected for the experiments ; for any of them , at least one format change has been detected during the period from July 1999 to October 2001 .For the sake of comparison , the providers are divided into three groups .", "label": "", "metadata": {}, "score": "65.86473"}
{"text": "She can be contacted at jungeunhahm@yonsei.ac.kr .Su Yeon Kim is a Ph.D. candidate of Library and Information Science at Yonsei University , Seoul , Korea .Her research interests include text mining , bibliometrics , information retrieval and text categorisation .", "label": "", "metadata": {}, "score": "65.99782"}
{"text": "14 - 17 May 2000 , Montreal .( pp .40 - 52 ) .Berlin : Springer .Callon , M. , Law , J. & Rip , A. ( 1986 ) .Mapping the dynamics of science and technology : sociology of science in the real world .", "label": "", "metadata": {}, "score": "66.01283"}
{"text": "Connected component labeling is used to separate subwords .The subword is normalized to 64 \u00d7 64 pixels and entered to feature extraction stage .Feature Extraction .Block diagram of proposed feature extraction system for a subword is presented in Figure 6 .", "label": "", "metadata": {}, "score": "66.239334"}
{"text": "FIG .8 .Algorithm below completes Algorithm 3 with backward wrappers and multi - scan recovery in the same way as Algorithm 2 completed Algorithm 1 .Algorithm 4 runs when Algorithm 3 returns false and fails to accurately complete the information extraction .", "label": "", "metadata": {}, "score": "66.28084"}
{"text": "Before changes all wrappers reported 0.98 values of recall / precision ; this \" before - change \" status is referred by a rectangle in the upper - left corner in .FIGS .10 , 11 and 12 .Any format change results in the fall of precision / recall values , and the goal of all recovery routines is to return precision / recall as close as possible to the initial rectangle .", "label": "", "metadata": {}, "score": "66.39273"}
{"text": "When the wrapper is brittle , the wrapper may fail to find specific \" landmarks \" in the page and may fail to apply the corresponding extraction rules , thus becoming inoperable and incapable of completing the task of information extraction .", "label": "", "metadata": {}, "score": "66.621765"}
{"text": "We observed that the combination of features in F3 led to a high recognition rate in most of the tested words , and longer words had a higher recognition rate than shorter ones .Table 3 : Identification rates for different sets of features : F1 , F2 , F3 , and F4 .", "label": "", "metadata": {}, "score": "66.65497"}
{"text": "Inspection of the rules would provide context for sheep in the document collection .Such associations can improve information retrieval engines .For more information about association models , see \" Association \" .Support Vector Machine ( SVM ) algorithm supports text data only or text and non - text data .", "label": "", "metadata": {}, "score": "66.71267"}
{"text": "( i ) Unlike English , Farsi texts , are written from right to left [ 24 - 26 ] .( ii )Farsi scripts include 32 characters , and each character can appear in four different shapes / forms depending on the position of the word ( beginning form , middle form , isolated form , and end form ) [ 24 - 26 ] .", "label": "", "metadata": {}, "score": "66.91944"}
{"text": "This is due to changes of the training data .To be specific , in the American Educational Research Journal which is regarded as publishing studies on very similar topics with the articles of the training journal , the gap between the kyw and idx is not big .", "label": "", "metadata": {}, "score": "67.01505"}
{"text": "Rules are grouped by classification labels .All rules , except r 12 , have the empty suffix , which means that the processing of token prefixes is enough in most cases .The right side of Table 1 shows the effect of the changes .", "label": "", "metadata": {}, "score": "67.38269"}
{"text": "if the classifier C validates the label l w for extracted string t within some threshold value , then assigning label l w , to extracted string t , else not assigning l w to t. .The method of .claim 1 , further comprising : for each unlabeled string t , .", "label": "", "metadata": {}, "score": "67.476944"}
{"text": "The method of . claim 2 , wherein the recovery value is greater than the threshold value .Description .CROSS - REFERENCE TO RELATED APPLICATIONS .This application is a divisional of U.S. application Ser .No .10/277,662 filed Oct. 21 , 2002 , now U.S. Pat .", "label": "", "metadata": {}, "score": "67.701645"}
{"text": "To form a subword , the letters are connected to each other [ 14 ] .For example , ( samar ) \u062b\u0645\u0631 is a subword formed by three letters : ( se ) \u062b , ( mim ) \u0645 , and ( re ) \u0631 .", "label": "", "metadata": {}, "score": "67.81522"}
{"text": "Recovery 1 fails to extract 5.1 % to 50 % of relevant information , thus showing a bottom line for the recovery performance and measuring implicitly the information extraction damage due to the format change .As the figure shows , recall suffers more that precision , as format changes disallow wrappers to classify correctly some tokens , but those tokens it does identify are relevant .", "label": "", "metadata": {}, "score": "67.87038"}
{"text": "Proceedings of the 10th International Conference on Asian Digital Libraries : Looking Back 10 Years and Forging New Frontiers , ( pp .317 - 326 ) .Berlin : Springer - Verlag .( Lecture Notes in Computer Science , Vol .", "label": "", "metadata": {}, "score": "68.00139"}
{"text": "Backward wrappers .The initial recovery routine helps wrappers resume information extraction .To further improve the accuracy of extraction recovery , an alternative view of pages in questions may be used , namely backward wrappers .In contrast to forward wrappers , backward wrappers scan HTML files from the end to the beginning .", "label": "", "metadata": {}, "score": "68.105934"}
{"text": "Before size normalization , the subword image is cropped with the minimum bounding box .In the present work , normalized size for each subword is considered to be 64 \u00d7 64 pixels ; then , normalized subword is entered to feature extraction stage .", "label": "", "metadata": {}, "score": "68.184685"}
{"text": "Table 2 .Index database list for journal data .Discipline .Journal title .Index database .Information science .Journal of Information Science , Journal of Informetrics , Scientometrics .LISA .Education .American Educational Research Journal .ERIC .", "label": "", "metadata": {}, "score": "68.21854"}
{"text": "Difficulties in tracking objects can arise due to abrupt object motion , changing appearance patterns of both the object and the scene , nonrigid object structures , object - to - object and object - to - scene occlusions , and camera motion .", "label": "", "metadata": {}, "score": "68.380775"}
{"text": "Sociological Methods and Research .Social Services Abstracts .We selected journals in the field of information science as the main training data in our study under the assumption that authors in this discipline might provide keywords in a similar way indexers do .", "label": "", "metadata": {}, "score": "68.68053"}
{"text": "The example in .FIG .5 shows how the bootstrapping works in the DBLP wrapper case .The basic classifier C 1 identifies ( some ) none strings in the DBLP page .Then , this classifier allows us to identify title and conference labels .", "label": "", "metadata": {}, "score": "68.889275"}
{"text": "Recovery 1 : one - scan basic recovery ( Algorithm 1 ) .Recovery 2 : multi - scan recovery with backward transducers ( Algorithms 1 and 2 ) .Recovery results .FIGS .10 - 12 report values of recall and precision for all providers in the three groups .", "label": "", "metadata": {}, "score": "68.894196"}
{"text": "In the Wall Street case , the basic recovery is able to accurately extract 4 elements of 7 ; recovery with the backward wrapper extracts 2 more elements .Using perfect content rules has no impact .Instead , relaxing thRecovery to 0.95 does not improve recall , but slightly decreases the precision , because of one misclassified token over 5 tests .", "label": "", "metadata": {}, "score": "69.30248"}
{"text": "A context shift does not change the extracted information .A content shift is a change in content of information extracted by the wrapper .Examples include replacing abbreviations used for a conference ( \" SIGIR \" ) with their full title ( \" ACM Conference on Research and Development in Information Retrieval \" ) or adding the prefix \" pp . \" to page strings .", "label": "", "metadata": {}, "score": "69.3147"}
{"text": "Stemming is carried out by the Porter stemmer ( 1980 ) .Stemming is a process for removing the common morphological and inflectional endings from words in English .Its main use is as part of a term normalisation process that is usually done when setting up an information retrieval system .", "label": "", "metadata": {}, "score": "69.66853"}
{"text": "21 - 28 , 2009 .View at Google Scholar .I. Daubechies , Ten Lectures on Wavelets , Prentice Hall , New Jersey , NJ , USA , 1998 .Cham , Ed . , vol .4351 of Lecture Notes in Computer Science , part 1 , pp .", "label": "", "metadata": {}, "score": "69.78439"}
{"text": "Note that , like Algorithm 1 , Algorithm 3 scans the file only once .Algorithm 3 .Information extraction with initial and content recovery .The content classifier C plays a double role in the extraction and recovery routine .First , it validates labels for strings found by extraction rules .", "label": "", "metadata": {}, "score": "69.887665"}
{"text": "CO .. Text - Dependent Writer Identification for Arabic Handwriting .Department of Computer Science and Engineering , College of Engineering , Qatar University , P.O. Box 2713 , Doha , Qatar .Received 1 September 2011 ; Revised 7 November 2011 ; Accepted 28 November 2011 .", "label": "", "metadata": {}, "score": "69.9933"}
{"text": "ODM allows the same degree of control as Oracle Text .Feature extraction ( higher order features ) .Non - negative matrix factorization ( NMF ) supports either text or text and non - text data .Support for text columns .", "label": "", "metadata": {}, "score": "70.29939"}
{"text": "However the author plans to update the online version frequently to incorporate the latest development in the field . ...r specific base learners , there has been some analyzer 's on convergence .See e.g. ( Haffari & Sarkar , 2007 ; Culp & Michailidis , 2007 ) . by Michael Collins , Yoram Singer - In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora , 1999 . \" ...", "label": "", "metadata": {}, "score": "70.336685"}
{"text": "This study was supported by a National Research Foundation of Korea Grant funded by the Korean Government ( NRF-2012 - 2012S1A3A2033291 ) .About the authors .Jung Eun Hahm is a MA candidate of Library and Information Science at Yonsei University , Seoul , Korea .", "label": "", "metadata": {}, "score": "70.40485"}
{"text": "Wrapper 16 may then be used in the Employment component to process any new HTML page 18 ( which is generated by provider 10 in response to HTTP request 30 for a user , for example ) from the provider 10 .", "label": "", "metadata": {}, "score": "70.633606"}
{"text": "Like forward wrappers , a backward wrapper is partial and can run in error when the format changes .However , because of the backward scanning , it would fail at positions different from those where the forward wrapper would fail .", "label": "", "metadata": {}, "score": "70.877525"}
{"text": "Printed Persian Subword Recognition Using Wavelet Packet Descriptors .Faculty of Electrical Engineering , Sahand University of Technology , Tabriz , Iran .Received 30 May 2013 ; Accepted 7 August 2013 .Copyright \u00a9 2013 Samira Nasrollahi and Afshin Ebrahimi .", "label": "", "metadata": {}, "score": "71.36991"}
{"text": "\" [ Show abstract ] [ Hide abstract ] ABSTRACT : Arabic language is a Semitic language used by 5 % of people around the world , and it is one of the UN official languages .Natural languages like Arabic and English usually use words which are derived from the same root .", "label": "", "metadata": {}, "score": "71.72813"}
{"text": "633 - 641 ) .Berlin : Springer - Verlag .( Lecture Notes in Artificial Intelligence , Volume 3930 ) .Whittaker , J. ( 1989 ) .Creativity and conformity in science : titles , keywords , and co - word analysis .", "label": "", "metadata": {}, "score": "71.96985"}
{"text": "We also investigate a rule - based method that uses hand - crafted lists of ' trigger ' terms derived from WordNet .We use two datasets in our experiments and test each approach using six different event types , i.e , Die , Attack , Injure , Meet , Transport and Charge - Indict .", "label": "", "metadata": {}, "score": "72.13521"}
{"text": "Definition 1 .An OCER wrapper W parses a page E from the beginning to the end and applies the extraction rules in R as follows .Pair ( P , S ) forms the full context of token t. The wrapper then compares P and S to prefixes and suffixes in the extraction rules .", "label": "", "metadata": {}, "score": "72.17502"}
{"text": "The contents of this publication are solely the responsibility of the authors and do not necessarily represent the official views of Qatar University or the Qatar National Research Fund .References . A. K. Jain , R. Bolle , and S. Pankanti , Eds . , Biometrics .", "label": "", "metadata": {}, "score": "72.94826"}
{"text": "Figure 5 shows a two - dimensional example of a subword image for the wavelet packet decomposition with depth 2 .Figure 5 : A wavelet packet decomposition : ( a ) the original image , ( b ) wavelet packet decomposition subbands in level 2 .", "label": "", "metadata": {}, "score": "73.18517"}
{"text": "If a string t is preceded with one or more skipped strings , then the prefix of t can not match any of label - dependent rules .Therefore , the recovery will skip strings until a label - independent rule is matched .", "label": "", "metadata": {}, "score": "73.35576"}
{"text": "A study of users ' performance and satisfaction with the Web of Science IR interface .Journal of Information Science , 30 ( 5 ) , 459 - 468 .Evaluation .Experts .Expert users .Novices .Novice users .", "label": "", "metadata": {}, "score": "73.469406"}
{"text": "Algorithm stops when none of the tokens is labeled during the last scan .Schematically , the combination of these three classifiers is presented in .FIG .9 .Algorithm 4 .IE multi - scan recovery with forward and backward wrappers .", "label": "", "metadata": {}, "score": "73.60516"}
{"text": "Rolling 's measure relies on simple formulaic representations of term sets assigned by each indexer as well as the common terms between the two sets .It defines the level of consistency between two indexers as the total number of terms in agreement divided by the total number of distinct terms used by both indexers ( Wolfram and Olson , 2007 ) .", "label": "", "metadata": {}, "score": "73.61751"}
{"text": "FIG .5 , dark strips cover newly labeled strings are while light strips cover previously labeled strings .Wrapper example 2 ( DBLP Wrapper is an OCER wrapper ) .In regular transducers , consuming an input token does not necessarily lead to emitting an output symbol .", "label": "", "metadata": {}, "score": "73.81978"}
{"text": "For more information about NMF , see \" Algorithm for Feature Extraction \" .Association models can be used to uncover the semantic meaning of words .For example , suppose that the word sheep co - occurs with words like sleep , fence , chew , grass , meadow , farmer , and shear .", "label": "", "metadata": {}, "score": "74.344696"}
{"text": "With time , the activity logs of users , which ... \" .Online Social Networks ( OSNs ) allow users to create and share content ( e.g. , posts , status updates , comments ) in real - time .These activities are collected in an activity log , ( e.g. Facebook Wall , Google+ Stream , etc . ) on the user 's social network profile .", "label": "", "metadata": {}, "score": "74.990295"}
{"text": "For example , HTTP wrappers interact with HTTP servers and HTML documents ; JDBC wrappers work with ODBC - compliant databases ; and DMA wrappers work with DMA - compliant document management systems .The World Wide Web ( Web ) represents a rich source of information in various domains of human activities and integrating Web data into various user applications has become a common practice .", "label": "", "metadata": {}, "score": "75.047775"}
{"text": "In Daniel S. Yeung , Zhi - Qiang Liu , Xi - Zhao Wang and Hong Yan , ( Eds . )In Advances in Machine Learning and Cybernetics : 4th International Conference , ICMLC 2005 , Guangzhou , China , August 2005 .", "label": "", "metadata": {}, "score": "75.38383"}
{"text": "Scandinavian Style .During these months , I will review some recent doctoral theses completed in Sweden .Many of the ideas or approaches explored in these doctoral investigations have been incorporated into state - of - the - art technologies .", "label": "", "metadata": {}, "score": "75.86174"}
{"text": "Baseline to the Lower Edge .To compute the length of the binary image from the baseline to the lower edge , we first determine the baseline row number , as described in Section 6.11 .Then , we compute the last pixel row number of the image .", "label": "", "metadata": {}, "score": "77.12367"}
{"text": "LONG , VARCHAR2 , XMLType , CHAR , RAW , LONG RAW using an appropriate transformation .Supports table columns of type CLOB , BLOB , BFILE .LONG , VARCHAR2 , XMLType , CHAR , RAW , LONG RAW Stemming is one of many tools used in information retrieval to combat the vocabulary mismatch problem , in which query words do not match document words .", "label": "", "metadata": {}, "score": "77.67894"}
{"text": "After completing the entire image , the program finds the maximum value of the array and stores the row number of this maximum value as the baseline .Then , the program searches for the first pixel in the image and stores its row number .", "label": "", "metadata": {}, "score": "77.8115"}
{"text": "This paper introduces Transductive Support Vector Machines ( TSVMs ) for text classification .While regular Support Vector Machines ( SVMs ) try to induce a general decision function for a learning task , Transductive Support Vector Machines take into account a particular test set and try to minimiz ... \" .", "label": "", "metadata": {}, "score": "78.61177"}
{"text": "In an OCER wrapper , the classification label of a textual token may depend on labels of previous textual tokens .Prefix for label ee in the DBLP wrapper is label - independent ; so any textual token preceded with tag will be labeled as ee .", "label": "", "metadata": {}, "score": "79.5589"}
{"text": "While regular Support Vector Machines ( SVMs ) try to induce a general decision function for a learning task , Transductive Support Vector Machines take into account a particular test set and try to minimize misclassifications of just those particular examples .", "label": "", "metadata": {}, "score": "80.85936"}
{"text": "Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .Publisher conditions are provided by RoMEO .Differing provisions from the publisher 's actual policy or licence agreement may be applicable .", "label": "", "metadata": {}, "score": "84.74625"}
{"text": "Copyright \u00a9 2012 Somaya Al - Maadeed .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "98.983"}
