{"text": "6 ) , can be used to keep track of all n grams requested for translating a particular sentence and guarantee that probabilities are available while this sentence is being processed .The translator signals the end of the sentence , at which time the high - level cache can be emptied .", "label": "", "metadata": {}, "score": "37.97203"}
{"text": "For example , predetermined values assigned to n grams in a dummy lookup table and language model scores stored in a local cache including a high - level cache and a low - level cache ( step 740 ) may be used .", "label": "", "metadata": {}, "score": "38.568653"}
{"text": "Second , we introduce topic - dependent dynamic cache adaptation techniques in the framework of the mixture model .Experiments with the static ( or unadapted ) mixture model on the 1994 WSJ task indicated a 21 % reduction in perplexity and a 3 - 4 % improvement in recognition accuracy over a general n - gram model .", "label": "", "metadata": {}, "score": "38.700806"}
{"text": "For each possible phrase or extension from the translation model for a segment to be translated , the decoder first looks up the high - level cache for any of possible n grams in the target language for each possible translation and associated statistical data ( steps 810 and 820 ) .", "label": "", "metadata": {}, "score": "38.77472"}
{"text": "The number of word n - grams added may be controlled , and the resulting tradeoff between size and accuracy is shown to surpass that of standard techniques based on n - gram cutoffs .The second technique ad- dresses longer - range word - pair relationships that arise due to factors such as the topic or the style of the text .", "label": "", "metadata": {}, "score": "38.937126"}
{"text": "The number of word n - grams added may be controlled , and the resulting tradeoff between size and accuracy is shown to surpass that of standard techniques based on n - gram cutoffs .The second technique ad- dresses longer - range word - pair relationships that arise due to factors such as the topic or the style of the text .", "label": "", "metadata": {}, "score": "38.937126"}
{"text": "2.6.Longer - term dependencies When based on n - grams , a language model is able to discriminate only among word histories that differ in the last while they differ in an important respect due to some event that has taken place in the more distant past .", "label": "", "metadata": {}, "score": "39.62342"}
{"text": "2.6.Longer - term dependencies When based on n - grams , a language model is able to discriminate only among word histories that differ in the last while they differ in an important respect due to some event that has taken place in the more distant past .", "label": "", "metadata": {}, "score": "39.62342"}
{"text": "The often local syntactic patterns in English text are captured conveniently by the n - gram structure , and reduced sparseness of the data allows larger that optimises the length of individual n - grams is proposed , and experimental tests show it to lead to improved results .", "label": "", "metadata": {}, "score": "39.90672"}
{"text": "The often local syntactic patterns in English text are captured conveniently by the n - gram structure , and reduced sparseness of the data allows larger that optimises the length of individual n - grams is proposed , and experimental tests show it to lead to improved results .", "label": "", "metadata": {}, "score": "39.90672"}
{"text": "A technique that optimises the length of individual n - grams is proposed , and experimental tests show it to lead to improved results .The model allows words to belong to multiple categories in order to cater for different grammatical functions , and may be employed as a tagger to assign category classifications to new text .", "label": "", "metadata": {}, "score": "41.972042"}
{"text": "A technique that optimises the length of individual n - grams is proposed , and experimental tests show it to lead to improved results .The model allows words to belong to multiple categories in order to cater for different grammatical functions , and may be employed as a tagger to assign category classifications to new text .", "label": "", "metadata": {}, "score": "41.972042"}
{"text": "Since in this space familiar clustering techniques can be applied , it becomes possible to derive several families of large - span language models , with various smoothing properties .Because of their semantic nature , the new language models are well suited to complement conventional , more syntactically oriented n - grams , and the combination of the two paradigms naturally yields the benefit of a multispan context .", "label": "", "metadata": {}, "score": "43.496185"}
{"text": "Second , n - gram distributions from multiple domains are combined , via a POS - dependent n - gram framework that separately compensate for word and POS usage differences .Two variations are explored : explicitly transforming the out - of - domain distribution before combining with an in - domain model , and separately estimating components of the POS - dependent n - gram model using multidomain data .", "label": "", "metadata": {}, "score": "43.763077"}
{"text": "These non - parametric learning algorithms are based on storing and combining frequency counts of word subsequences of different lengths , e.g. , 1 , 2 and 3 for 3-grams .\\ )Furthermore , a new observed sequence typically will have occurred rarely or not at all in the training set .", "label": "", "metadata": {}, "score": "44.204895"}
{"text": "The objective of this work has been to develop a model that deals separately with each type of pattern in the above classification , so as to reduce the data fragmentation inherent in word n - grams .In particular , since syntactic behaviour will be modelled explicitly , we may take advantage of available prior linguistic knowledge that is neglected by word - basedapproaches .", "label": "", "metadata": {}, "score": "44.33856"}
{"text": "The objective of this work has been to develop a model that deals separately with each type of pattern in the above classification , so as to reduce the data fragmentation inherent in word n - grams .In particular , since syntactic behaviour will be modelled explicitly , we may take advantage of available prior linguistic knowledge that is neglected by word - basedapproaches .", "label": "", "metadata": {}, "score": "44.33856"}
{"text": "( 71 ) where lated for the word probability distribution .P?l j ?For example , a variant of k - means clus- tering is employed in [ 89].A considerable fraction of the computation required during tree - growing is devoted to set construction .", "label": "", "metadata": {}, "score": "44.486485"}
{"text": "( 71 ) where lated for the word probability distribution .P?l j ?For example , a variant of k - means clus- tering is employed in [ 89].A considerable fraction of the computation required during tree - growing is devoted to set construction .", "label": "", "metadata": {}, "score": "44.486485"}
{"text": "Chapter 4 presents a technique by means of which important word n - grams may be combined with the syntactic model component within a backoff framework .In particular , the word n - gram is used to calculate a probability whenever possible , while the syntactic Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .", "label": "", "metadata": {}, "score": "45.078255"}
{"text": "Chapter 4 presents a technique by means of which important word n - grams may be combined with the syntactic model component within a backoff framework .In particular , the word n - gram is used to calculate a probability whenever possible , while the syntactic Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .", "label": "", "metadata": {}, "score": "45.078255"}
{"text": "In particular , they have led to improvements relative to isolated - word methods , which treat the misspelling without taking into account the surrounding words and are hence unable to address real - word errors .When a nonword error is detected , a number of similar valid spellings are determined , generally by means of a database of valid words or subword units ( such as character n - grams ) as well as suitable distancemeasures indicating similarity betweenstrings .", "label": "", "metadata": {}, "score": "45.199318"}
{"text": "In particular , they have led to improvements relative to isolated - word methods , which treat the misspelling without taking into account the surrounding words and are hence unable to address real - word errors .When a nonword error is detected , a number of similar valid spellings are determined , generally by means of a database of valid words or subword units ( such as character n - grams ) as well as suitable distancemeasures indicating similarity betweenstrings .", "label": "", "metadata": {}, "score": "45.199318"}
{"text": "Generally , the elimination of n - grams with very low counts is believed to not significantly affect model performance .This project investigates the degradation of a trigram backoff model 's perplexity and word error rates as bigram and trigram cutoffs are increased .", "label": "", "metadata": {}, "score": "45.204334"}
{"text": "Caches are usually employed in conjunction with n - gram models , and have led to performance improvements .However , they do not capture relationships be- tween different words , and hence work has been carried out in finding associations between word pairs .", "label": "", "metadata": {}, "score": "45.223305"}
{"text": "Caches are usually employed in conjunction with n - gram models , and have led to performance improvements .However , they do not capture relationships be- tween different words , and hence work has been carried out in finding associations between word pairs .", "label": "", "metadata": {}, "score": "45.223305"}
{"text": "Furthermore , n - grams based on categories are intrinsically able to generalise to word n - tuples never witnessed during training .Since these categories embed syntactic information , this generalisation proceeds according to the measure of grammatical cor- rectness assigned to the unseen sequence by the model .", "label": "", "metadata": {}, "score": "45.356125"}
{"text": "Furthermore , n - grams based on categories are intrinsically able to generalise to word n - tuples never witnessed during training .Since these categories embed syntactic information , this generalisation proceeds according to the measure of grammatical cor- rectness assigned to the unseen sequence by the model .", "label": "", "metadata": {}, "score": "45.356125"}
{"text": "Keywords - Latent semantic analysis , multispan integration , n - grams , speech recognition , statistical language modeling .I. . ...S OF THE IEEE , VOL . 88 , NO . 8 , AUGUST 2000 1279 In ( 1 ) , the prediction can be done with the help of a bigram language model ( ) .", "label": "", "metadata": {}, "score": "45.638565"}
{"text": "An experimental comparison with word - based n - gram approaches reveals this ability to be important to language model quality , and consequently two methods allowing the inclusion of word relations are developed .The first allows the incorporation of selected word n - grams within a backoff framework .", "label": "", "metadata": {}, "score": "45.684708"}
{"text": "An experimental comparison with word - based n - gram approaches reveals this ability to be important to language model quality , and consequently two methods allowing the inclusion of word relations are developed .The first allows the incorporation of selected word n - grams within a backoff framework .", "label": "", "metadata": {}, "score": "45.684708"}
{"text": "One approach is to partition the language model data over a set of distributed language model servers across multiple machines , possibly with replication for each partitioned piece of the language model state .FIGS . 1 and 2 illustrate examples of such systems , where the language model can include large n - grams with n greater than 3 .", "label": "", "metadata": {}, "score": "45.753876"}
{"text": "The first section , in chapter 3 , develops a model for syntactic dependencies based on word - category n - grams .The second section , in chapter 4 , extends this model by allowing short - range word relations to be captured through the incorporation of selected word n - grams .", "label": "", "metadata": {}, "score": "45.809914"}
{"text": "The first section , in chapter 3 , develops a model for syntactic dependencies based on word - category n - grams .The second section , in chapter 4 , extends this model by allowing short - range word relations to be captured through the incorporation of selected word n - grams . \" ...", "label": "", "metadata": {}, "score": "46.40898"}
{"text": "In the case of real - word errors , the language model was used both to detect the misspelled words by sensing the associated low language model probability , as well as to choose the most likely from the list of subsequently generated alternatives [ 46].", "label": "", "metadata": {}, "score": "46.516518"}
{"text": "In the case of real - word errors , the language model was used both to detect the misspelled words by sensing the associated low language model probability , as well as to choose the most likely from the list of subsequently generated alternatives [ 46].", "label": "", "metadata": {}, "score": "46.516518"}
{"text": "This paper will investigate some of the possible reasons for this apparent discrepancy , and will explore the circumstances under which adaptive language models can be useful .We will concentrate on cache - based and mixture - based models and their use on the Broadcast News task .", "label": "", "metadata": {}, "score": "46.930046"}
{"text": "Mixture adaptation techniques contributed a further 14 % reduction in perplexity and a small improvement in recognition accuracy . ... en their simplicity , n - gram models are constrained in their inability to take advantage of dependencies longer than n. However , cache models do not account for dependencies within a sentence .", "label": "", "metadata": {}, "score": "47.35382"}
{"text": "Page 38 .A highvalueof be expectedwere they to be generated independently .The procedure is then to find all consecutiveword pairs with high mutual information and add these as single units to the vocabulary .Since the estimate in equation ( 60 ) may be unreliable for small counts , only word pairs occurring at least a threshold number of times are considered .", "label": "", "metadata": {}, "score": "47.61284"}
{"text": "Page 38 .A highvalueof be expectedwere they to be generated independently .The procedure is then to find all consecutiveword pairs with high mutual information and add these as single units to the vocabulary .Since the estimate in equation ( 60 ) may be unreliable for small counts , only word pairs occurring at least a threshold number of times are considered .", "label": "", "metadata": {}, "score": "47.61284"}
{"text": "w ? ? ?i ? ? ? , i.e. : P ?w?i?jw ? ? ?i ?F ?P ?w?i?jw?i ? ? ?P ?w?i?jw?i ? ? ?P ? w?i?jw ?( 62 ) where explosion associated with n - grams for large ourselves to consecutive words , the number of pairs is now , unlike in the bigram case , a significant fraction of possible word pairs , retaining only those conveying a useful amount of information .", "label": "", "metadata": {}, "score": "47.72748"}
{"text": "w ? ? ?i ? ? ? , i.e. : P ?w?i?jw ? ? ?i ?F ?P ?w?i?jw?i ? ? ?P ?w?i?jw?i ? ? ?P ? w?i?jw ?( 62 ) where explosion associated with n - grams for large ourselves to consecutive words , the number of pairs is now , unlike in the bigram case , a significant fraction of possible word pairs , retaining only those conveying a useful amount of information .", "label": "", "metadata": {}, "score": "47.72748"}
{"text": "In the absence of further information , it seems reasonable to assume all events equally likely , and therefore to redistribute the probability mass language modelling applications , further information is often available in the form of a more general ? iin ? jdrawn from a population ? jwhen the sample is sparse .", "label": "", "metadata": {}, "score": "47.77298"}
{"text": "In the absence of further information , it seems reasonable to assume all events equally likely , and therefore to redistribute the probability mass language modelling applications , further information is often available in the form of a more general ? iin ? jdrawn from a population ? jwhen the sample is sparse .", "label": "", "metadata": {}, "score": "47.77298"}
{"text": "Mutual information is used as a filter for word pairs also in [ 47].The pairs found in this way are used in conjunction with a conventional trigram language model by means of the maximum entropy princi- ple , which allows the combination of various knowledge sources while making minimal assumptions concerning the distributions .", "label": "", "metadata": {}, "score": "48.08294"}
{"text": "Mutual information is used as a filter for word pairs also in [ 47].The pairs found in this way are used in conjunction with a conventional trigram language model by means of the maximum entropy princi- ple , which allows the combination of various knowledge sources while making minimal assumptions concerning the distributions .", "label": "", "metadata": {}, "score": "48.08294"}
{"text": "Instead of skipping a component completely , the translation server may also choose to make fewer requests to the language model , thereby reducing the amount of communication and speeding up the translation .Hence , the translation server may decide to only request 3-grams or 4-grams instead of 5-grams from the language model .", "label": "", "metadata": {}, "score": "48.555977"}
{"text": "This thesis focuses on improving the estimation of domain - dependent n - gram models by usi ... \" .Standard statistical language models , or n - gram models , which represent the probability of word sequences , suffer from sparse - data problems in tasks where large amounts of domain - specific text are not available .", "label": "", "metadata": {}, "score": "48.89656"}
{"text": "Often the request is at the granularity of a single sentence .The translation servers retrieve the appropriate pieces of language model data from the language model servers .It is often useful to partition the n grams so that n grams whose values are likely to be needed as part of handling the same or similar translation requests that reside in the same partition .", "label": "", "metadata": {}, "score": "49.183952"}
{"text": "In particular , an approach that aims to capture both general grammatical patterns as well as particular word dependencies using different model components is proposed , developed and evaluated .To account for grammatical patterns , a model employing variable - length n - grams of part - of - speech word categories is developed .", "label": "", "metadata": {}, "score": "49.19152"}
{"text": "In particular , an approach that aims to capture both general grammatical patterns as well as particular word dependencies using different model components is proposed , developed and evaluated .To account for grammatical patterns , a model employing variable - length n - grams of part - of - speech word categories is developed .", "label": "", "metadata": {}, "score": "49.19152"}
{"text": "The quality of translations can generally be improved if the system is able to utilize a larger language model , such as n - grams with n greater than 3 .[ 0086 ] As part of the translation process , a statistical translation system needs information about how often various words , phrases , or sequences of words occur in order in a target language .", "label": "", "metadata": {}, "score": "49.374706"}
{"text": "The underlying philosophy of a cache is that , due to local text characteristics such as topic and author , words or word patterns that have occurred recently are more likely to recur in the immediate future than a static language model would predict .", "label": "", "metadata": {}, "score": "49.43749"}
{"text": "The underlying philosophy of a cache is that , due to local text characteristics such as topic and author , words or word patterns that have occurred recently are more likely to recur in the immediate future than a static language model would predict .", "label": "", "metadata": {}, "score": "49.43749"}
{"text": "We investigate a number of variants on this approach , including cross - domain versus dynamic adaptation ; linear versus loglinear mixtures ; language and translation model adaptation ; different methods of assigning weights ; and granularity of the source unit being adapted to .", "label": "", "metadata": {}, "score": "49.4488"}
{"text": "Therefore , similar to other translation model approaches ( phrase - based or hierarchical ) , the sparseness problem of the units being modeled leads to unreliable probability estimates , even under conditions where large bilingual corpora are available .In order to tackle this problem , we extend the n - gram - based approach to SMT by tightly integrating more general word representations , such as lemmas and morphological classes , and we use the flexible framework of FLMs to apply a number of different back - off techniques .", "label": "", "metadata": {}, "score": "49.595028"}
{"text": "Therefore , similar to other translation model approaches ( phrase - based or hierarchical ) , the sparseness problem of the units being modeled leads to unreliable probability estimates , even under conditions where large bilingual corpora are available .In order to tackle this problem , we extend the n - gram - based approach to SMT by tightly integrating more general word representations , such as lemmas and morphological classes , and we use the flexible framework of FLMs to apply a number of different back - off techniques .", "label": "", "metadata": {}, "score": "49.595028"}
{"text": "This is much more than the number of operations typically involved in computing probability predictions for n - gram models .Several researchers have developed techniques to speed - up either probability prediction ( when using the model ) or estimating gradients ( when training the model ) .", "label": "", "metadata": {}, "score": "49.629124"}
{"text": "N - gram models use only the last N-1 words to predict the next word .Typical values of N that are used range from 2 - 4 .N - gram language models thus lack the long - term context information .", "label": "", "metadata": {}, "score": "49.65657"}
{"text": "Once again the bigram model structure of equation ( 59 ) is employed , and the training set log probability is used as an optimisation criterion although now both the word as well as the category it is to be moved to are chosen by Monte Carlo selection .", "label": "", "metadata": {}, "score": "49.691658"}
{"text": "Once again the bigram model structure of equation ( 59 ) is employed , and the training set log probability is used as an optimisation criterion although now both the word as well as the category it is to be moved to are chosen by Monte Carlo selection .", "label": "", "metadata": {}, "score": "49.691658"}
{"text": "In each case perplexities are reduced , although word error rate reductions were only reported in [ 44].n ?-gram distribution is reasonable when the data are sparse , and convenient 2.5 .Category - based language models Instead of finding patterns among individual words , a language model may be designed to discover relationships betweenwordgroupingsor categories .", "label": "", "metadata": {}, "score": "49.695442"}
{"text": "In each case perplexities are reduced , although word error rate reductions were only reported in [ 44].n ?-gram distribution is reasonable when the data are sparse , and convenient 2.5 .Category - based language models Instead of finding patterns among individual words , a language model may be designed to discover relationships betweenwordgroupingsor categories .", "label": "", "metadata": {}, "score": "49.695442"}
{"text": "Methods by means of which related word pairs may be identified from a large corpus , as well as techniques allow- ing the estimation of the parameters of the functional dependence , are presented and shown to lead to performance improvements .", "label": "", "metadata": {}, "score": "50.10392"}
{"text": "Methods by means of which related word pairs may be identified from a large corpus , as well as techniques allow- ing the estimation of the parameters of the functional dependence , are presented and shown to lead to performance improvements .", "label": "", "metadata": {}, "score": "50.10392"}
{"text": "j?f ? ? ? ? ? ? ? ?N p ? ?g and N p ? ?X j ? ?j ?N p ? ? ?( 42 )Thesmoothingparameters of a cross - validation set , and hence the component probabilities of the right hand side of equation ( 41 ) will be weighted according to their utility with respect to the predictive quality of the language model .", "label": "", "metadata": {}, "score": "50.11805"}
{"text": "j?f ? ? ? ? ? ? ? ?N p ? ?g and N p ? ?X j ? ?j ?N p ? ? ?( 42 )Thesmoothingparameters of a cross - validation set , and hence the component probabilities of the right hand side of equation ( 41 ) will be weighted according to their utility with respect to the predictive quality of the language model .", "label": "", "metadata": {}, "score": "50.11805"}
{"text": "[ 0136 ] The size of a language model can be significantly reduced and partitions can be made more evenly sized by removing certain entries from the language model with only minimal impact on the quality of the language model .One way is to remove longer n grams that end in frequently used shorter n grams .", "label": "", "metadata": {}, "score": "50.20111"}
{"text": "[ 0135 ] Partitioning by last word may lead to very unbalanced partitions .Balance can be improved by partitioning based on the last two words or even longer sequences of length S. In order to ensure that shorter n grams are on the same server , unigrams ( or sequences of length S-1 and shorter ) can be replicated on all partitions .", "label": "", "metadata": {}, "score": "50.208206"}
{"text": "To avoid spurious co- occurrences , a minimum threshold of 3 was placed on the word counts of were found to be noncritical , and were made constant over the entire window .Various window sizes were investigated , and the optimum was found to lie at approximately was interpolated with a unigram language model , and resulted in a 15 % perplexity reduction .", "label": "", "metadata": {}, "score": "50.32621"}
{"text": "To avoid spurious co- occurrences , a minimum threshold of 3 was placed on the word counts of were found to be noncritical , and were made constant over the entire window .Various window sizes were investigated , and the optimum was found to lie at approximately was interpolated with a unigram language model , and resulted in a 15 % perplexity reduction .", "label": "", "metadata": {}, "score": "50.32621"}
{"text": "P ? w?i?jw?i?j ?This equation is used iteratively to determine the memory weights uniform set of memory weights constituting a suitable initial condition [ 53].c k ?k?f ? ? ? ? ? ? ? ?pg , with a Equation ( 64 ) was applied to the LOB corpus in [ 53].", "label": "", "metadata": {}, "score": "50.67774"}
{"text": "P ? w?i?jw?i?j ?This equation is used iteratively to determine the memory weights uniform set of memory weights constituting a suitable initial condition [ 53].c k ?k?f ? ? ? ? ? ? ? ?pg , with a Equation ( 64 ) was applied to the LOB corpus in [ 53].", "label": "", "metadata": {}, "score": "50.67774"}
{"text": "H ? ? ? is many - to - many , returning for each particular word sequence w ? ? ?i ? the H?w ? ? ?i ? ? ? ? for which the category and word n - grams match : H?w ? ? ?", "label": "", "metadata": {}, "score": "50.737072"}
{"text": "H ? ? ? is many - to - many , returning for each particular word sequence w ? ? ?i ? the H?w ? ? ?i ? ? ? ? for which the category and word n - grams match : H?w ? ? ?", "label": "", "metadata": {}, "score": "50.737072"}
{"text": "One way of achieving this is to partition by the first or last M words in the n gram , e.g. , partition by the last two words of the n gram .[0092 ] Within each server , the lookup of an n gram value within the partition should be configured to be efficient .", "label": "", "metadata": {}, "score": "50.903233"}
{"text": "Although performance im- provements have been achieved , these have been shown to be mostly due to correlations of words with themselves , an effect already addressed by a cache .n words .Consequently the model is generally unable to capture long - range relations State - of - the - art speech - recognition systems continue to employ n - gram language models , based on words when the training sets are large enough , or based on word - categories when they are smaller , with the possible addition of a cache component .", "label": "", "metadata": {}, "score": "51.044426"}
{"text": "Although performance im- provements have been achieved , these have been shown to be mostly due to correlations of words with themselves , an effect already addressed by a cache .n words .Consequently the model is generally unable to capture long - range relations State - of - the - art speech - recognition systems continue to employ n - gram language models , based on words when the training sets are large enough , or based on word - categories when they are smaller , with the possible addition of a cache component .", "label": "", "metadata": {}, "score": "51.044426"}
{"text": "Then , a post - processing step is performed using a language model based on Part - of - Speech ( POS ) tags which is combined to the n - gram model previously used .Thus , error hypotheses can be further recognized and POS tags can be assigned to the OOV words .", "label": "", "metadata": {}, "score": "51.04795"}
{"text": "Then , a post - processing step is performed using a language model based on Part - of - Speech ( POS ) tags which is combined to the n - gram model previously used .Thus , error hypotheses can be further recognized and POS tags can be assigned to the OOV words .", "label": "", "metadata": {}, "score": "51.04795"}
{"text": "Overall probability estimates The effect of backoffs Per - category analysis Per - n - gram analysis Robustness to domain - change ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "51.097847"}
{"text": "Overall probability estimates The effect of backoffs Per - category analysis Per - n - gram analysis Robustness to domain - change ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "51.097847"}
{"text": "In this approach , ( discrete ) words and documents are mapped onto a ( continuous ) semantic vector space , in which familiar clustering techniques can be applied .This leads to the specification of a powerful framework for automatic semantic classification , as well as the derivation of several lan - guage model families with various smoothing properties .", "label": "", "metadata": {}, "score": "51.115807"}
{"text": "Page 44 . of the leaf probability distributions uncertainty associated with the prediction of the next word .The entropy at leaf .Minimising this figure directly minimises the overall is : ? h j ?W ? ? ?N w ? ?", "label": "", "metadata": {}, "score": "51.152313"}
{"text": "Page 44 . of the leaf probability distributions uncertainty associated with the prediction of the next word .The entropy at leaf .Minimising this figure directly minimises the overall is : ? h j ?W ? ? ?N w ? ?", "label": "", "metadata": {}, "score": "51.152313"}
{"text": "Page 6 . 2.6.Longer - term dependencies 2.6.1 .Pairwise dependencies 2.6.2 .Cache models 2.6.3 .Stochastic decision tree based language models 29 29 32 34 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "51.45527"}
{"text": "Page 6 . 2.6.Longer - term dependencies 2.6.1 .Pairwise dependencies 2.6.2 .Cache models 2.6.3 .Stochastic decision tree based language models 29 29 32 34 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "51.45527"}
{"text": "References .Jelinek , F. and Mercer , R.L. ( 1980 ) Interpolated Estimation of Markov Source Parameters from Sparse Data .Pattern Recognition in Practice , Gelsema E.S. and Kanal L.N. eds , North - Holland .pp .381 - 397 .", "label": "", "metadata": {}, "score": "51.570206"}
{"text": "The low - level cache 632 is searched by the segment translation server 130 for language model data for any n grams not found in the high - level cache 631 with the language model ( steps 961 and 962 ) .", "label": "", "metadata": {}, "score": "51.807922"}
{"text": "Figure 1 : Example of 2-dimensional distributed representation for words obtained in ( Blitzer et al 2005 ) .In ( Bengio et al 2001 , Bengio et al 2003 ) , it was demonstrated how distributed representations for symbols could be combined with neural network probability predictions in order to surpass standard n - gram models on statistical language modeling tasks .", "label": "", "metadata": {}, "score": "51.826126"}
{"text": "Category - Based Statistical Language Models .Private Profile . this document .The first section , in chapter 3 , develops a model for syntactic dependencies based on word - category n - grams .The second section , in chapter 4 , extends this model by allowing short - range word relations to be captured through the incorporation of selected word n - grams .", "label": "", "metadata": {}, "score": "52.138477"}
{"text": "Because many different combinations of feature values are possible , a very large set of possible meanings can be represented compactly , allowing a model with a comparatively small number of parameters to fit a large training set .The dominant methodology for probabilistic language modeling since the 1980 's has been based on n - gram models ( Jelinek and Mercer , 1980;Katz 1987 ) .", "label": "", "metadata": {}, "score": "52.149376"}
{"text": "i ? w iand N ?N wwhere N wis the vocabulary size .The trigram computes w?i ? considering only the previous two words w?i ? ? ?i ? ? ? , referred to as fw a ? w b g , so that each w?i ? follows a different occurrence of this bigram .", "label": "", "metadata": {}, "score": "52.190845"}
{"text": "i ? w iand N ?N wwhere N wis the vocabulary size .The trigram computes w?i ? considering only the previous two words w?i ? ? ?i ? ? ? , referred to as fw a ? w b g , so that each w?i ? follows a different occurrence of this bigram .", "label": "", "metadata": {}, "score": "52.190845"}
{"text": "The proposed method can reduce the perplexity of the baseline language model by 37 % , indicating the predictive power of the topic - dependent language model .s and trigrams using maximum entropy .Tree - based language model in [ 1 ] is another approach to use longer context while limiting the number of parameters .", "label": "", "metadata": {}, "score": "52.228752"}
{"text": "In this section we will assume these items w?i ? conditioned on the n ? ?words w?i?n ? ? ?i ? ? ? , termed the ?n ?-gram context .From section ?n ? h ?h N H o ?", "label": "", "metadata": {}, "score": "52.279594"}
{"text": "In this section we will assume these items w?i ? conditioned on the n ? ?words w?i?n ? ? ?i ? ? ? , termed the ?n ?-gram context .From section ?n ? h ?h N H o ?", "label": "", "metadata": {}, "score": "52.279594"}
{"text": "It is found that all three approaches give practically identical results , indicating that the estimator is insensitive to the choice of D. The estimator ( 45 ) is employed successfully in [ 32 ] , [ 33].H N - gram models h An n - gram is a sequence of to be words , bearing in mind that the principles may easily be extended to other forms of n - gram13 .", "label": "", "metadata": {}, "score": "52.354202"}
{"text": "It is found that all three approaches give practically identical results , indicating that the estimator is insensitive to the choice of D. The estimator ( 45 ) is employed successfully in [ 32 ] , [ 33].H N - gram models h An n - gram is a sequence of to be words , bearing in mind that the principles may easily be extended to other forms of n - gram13 .", "label": "", "metadata": {}, "score": "52.354202"}
{"text": "Such statistical language models assign to each word in an utterance a probability value according to its deemed likelihood within the context of the surrounding word se- quence .The probabilities are inferred from a large body of example text , referred to as the training corpus .", "label": "", "metadata": {}, "score": "52.36839"}
{"text": "Such statistical language models assign to each word in an utterance a probability value according to its deemed likelihood within the context of the surrounding word se- quence .The probabilities are inferred from a large body of example text , referred to as the training corpus .", "label": "", "metadata": {}, "score": "52.36839"}
{"text": "This may be accomplished by employing some form of cross - validation , for instance the use of a heldout set [ 3].P?l j ? and P?w k jl j ? are assumed to be the true probabilities , valid for the language in The form of the questions ( 70 ) is simple and conveniently illustrates the concept of the tree - based lan- guagemodel , but it may bearguedthat theseare alsounacceptablyrestrictive [ 3].", "label": "", "metadata": {}, "score": "52.42185"}
{"text": "This may be accomplished by employing some form of cross - validation , for instance the use of a heldout set [ 3].P?l j ? and P?w k jl j ? are assumed to be the true probabilities , valid for the language in The form of the questions ( 70 ) is simple and conveniently illustrates the concept of the tree - based lan- guagemodel , but it may bearguedthat theseare alsounacceptablyrestrictive [ 3].", "label": "", "metadata": {}, "score": "52.42185"}
{"text": "I argue that this problem can be addressed by clustering the sentences in a training corpus automatically into subcorpora on the criterion of entropy reduc ... \" .Many of the kinds of language model used in speech understanding suffer from imperfect modeling of intra - sentential contextual influences .", "label": "", "metadata": {}, "score": "52.496788"}
{"text": "A token in n - grams can be either a text word of the target language or a symbol in the target language .In addition , each language model partition can be replicated on one or more other replica servers , as shown in FIG .", "label": "", "metadata": {}, "score": "52.498608"}
{"text": "In this mode of operation , either each language model probability request would have to store a pointer to the search hypothesis where it is needed or each translation hypothesis would have a pointer to the missing language model probabilities .In this variant , the intermediate hypotheses scores would normally be approximate .", "label": "", "metadata": {}, "score": "52.529816"}
{"text": "K ? ? ?P?z ? ? ?K ?Furthermore , the length of sequences z ? ? ?K ? at ?K ? log ?P ?z ? ? ?K ? ? ?Thus the entropy of the source is approximated by the average per - event log probability of the observed sequence .", "label": "", "metadata": {}, "score": "52.598686"}
{"text": "K ? ? ?P?z ? ? ?K ?Furthermore , the length of sequences z ? ? ?K ? at ?K ? log ?P ?z ? ? ?K ? ? ?Thus the entropy of the source is approximated by the average per - event log probability of the observed sequence .", "label": "", "metadata": {}, "score": "52.598686"}
{"text": "The perplexity measure was first proposed by Jelinek , Mercer and Bahl [ 34].x ? x?K ? ? ? and thus minimising the perplexity is equivalent to maximising the log proba-Perplexity allows an independent(and thus computationallyless demanding)assessmentof the language model quality .", "label": "", "metadata": {}, "score": "52.644108"}
{"text": "The perplexity measure was first proposed by Jelinek , Mercer and Bahl [ 34].x ? x?K ? ? ? and thus minimising the perplexity is equivalent to maximising the log proba-Perplexity allows an independent(and thus computationallyless demanding)assessmentof the language model quality .", "label": "", "metadata": {}, "score": "52.644108"}
{"text": "The proposed methods are assessed through a series of machine translation experiments within the framework of the EuTrans project . . ..The ( smoothed ) n - gram model obtained from the set of extended symbols is represented as a stochastic finite - state automaton ( Llorens , Vilar , and Casacuberta 2002 ) .", "label": "", "metadata": {}, "score": "52.837063"}
{"text": "The system can be implemented to include one or more replica data servers for each of the data servers .In one implementation , the collection of data is data for a language model for a target language .The language model includes n grams in the target language and statistical data for each of the n grams .", "label": "", "metadata": {}, "score": "52.848392"}
{"text": "n to be employed .A technique While the category - based model has the important advantage of generalisation to unseen word se- quences , it is by nature not able to capture relationships between particular words .An experimental comparison with word - based n - gram approaches reveals this ability to be important to language model quality , and consequently two methods allowing the inclusion of word relations are developed .", "label": "", "metadata": {}, "score": "52.869614"}
{"text": "n to be employed .A technique While the category - based model has the important advantage of generalisation to unseen word se- quences , it is by nature not able to capture relationships between particular words .An experimental comparison with word - based n - gram approaches reveals this ability to be important to language model quality , and consequently two methods allowing the inclusion of word relations are developed .", "label": "", "metadata": {}, "score": "52.869614"}
{"text": "j ? fw c j ?w i g ? and the total number of events in the sample is given by the number of occurrences of the context itself : N ?N ? w j j ?j ?As number of words per sample decreases for a fixed training corpus .", "label": "", "metadata": {}, "score": "52.879364"}
{"text": "j ? fw c j ?w i g ? and the total number of events in the sample is given by the number of occurrences of the context itself : N ?N ? w j j ?j ?As number of words per sample decreases for a fixed training corpus .", "label": "", "metadata": {}, "score": "52.879364"}
{"text": "While the requested TM data is being retrieved , the decoder sets a value for each n gram to be looked up from the LM model at some initial value , e.g. , a random value or a constant value .As an example , the candidate translations can be set to zero ( step 720 ) .", "label": "", "metadata": {}, "score": "53.002632"}
{"text": "When a trigram backoff language model is created from a large body of text , trigrams and bigrams that occur few times in the training text are often excluded from the model in order to decrease the model size .Generally , the elimination of n - grams with very low counts is believed to not significantl ... \" .", "label": "", "metadata": {}, "score": "53.06748"}
{"text": "Most existing statistical language models exploit only the immediate history of a text .To extract information from further back in the document 's history , we propose and use trigger pairs as the basic information bearing elements .This allows the model to adapt its expectations to the topic of discourse .", "label": "", "metadata": {}, "score": "53.108864"}
{"text": "w?i?jw?i ? ? ?i ? ? ? ? must be recomputedafter Although in a truly adaptive language model the values of current text , fixed values were employed and optimal values for Viterbi - type reestimation approach [ 28].", "label": "", "metadata": {}, "score": "53.121857"}
{"text": "w?i?jw?i ? ? ?i ? ? ? ? must be recomputedafter Although in a truly adaptive language model the values of current text , fixed values were employed and optimal values for Viterbi - type reestimation approach [ 28].", "label": "", "metadata": {}, "score": "53.121857"}
{"text": "In contrast 2.3 .Probability estimation from sparse data In many instances language modelling involves the estimation of probabilities from a sparse set of measurements .Specialised estimation techniques are required under such circumstances , and the most prevalent of these in the language model field are described in this section .", "label": "", "metadata": {}, "score": "53.131554"}
{"text": "In contrast 2.3 .Probability estimation from sparse data In many instances language modelling involves the estimation of probabilities from a sparse set of measurements .Specialised estimation techniques are required under such circumstances , and the most prevalent of these in the language model field are described in this section .", "label": "", "metadata": {}, "score": "53.131554"}
{"text": "This kind of clustering offers a way to represent impor- tant contextual effects and can therefore significantly improve the performance of a model .As evidence for these claims , I present results showing that clustering improves some models but not others for the ATIS domain .", "label": "", "metadata": {}, "score": "53.172497"}
{"text": "In addition to the computational challenges briefly described above , several weaknesses of the neural network language model are being worked on by researchers in the field .One of them is the representation of a fixed - size context .To represent longer - term context , one may employ a recurrent network formulation , which learns a representation of context that summarizes the past word sequence in a way that preserves information predictive of the future .", "label": "", "metadata": {}, "score": "53.20539"}
{"text": "In particular , consider the maximum likelihood probability estimate for the event sample ?f ? ? ? ? ? ? ? ?N H ? ?g and N His the number of distinct history classifications found in the corpus .", "label": "", "metadata": {}, "score": "53.28968"}
{"text": "In particular , consider the maximum likelihood probability estimate for the event sample ?f ? ? ? ? ? ? ? ?N H ? ?g and N His the number of distinct history classifications found in the corpus .", "label": "", "metadata": {}, "score": "53.28968"}
{"text": "Page 37 .A greedy agglomerative algorithm for clustering words into word categories is presented in [ 8].A bigram language model structure is assumed , and it is shown that for this model the training set log probability may be written as the sum of two terms : the unigram distribution entropy average mutual information15between adjacent categories and the I m ?", "label": "", "metadata": {}, "score": "53.319756"}
{"text": "Page 37 .A greedy agglomerative algorithm for clustering words into word categories is presented in [ 8].A bigram language model structure is assumed , and it is shown that for this model the training set log probability may be written as the sum of two terms : the unigram distribution entropy average mutual information15between adjacent categories and the I m ?", "label": "", "metadata": {}, "score": "53.319756"}
{"text": "Traditionally , linear interpolation and its variants have been used , but these are shown here to be seriously deficient .Instead , we apply the principle of Maximum Entropy ( ME ) .Each information source gives rise to a set of constraints , to be imposed on the combined estimate .", "label": "", "metadata": {}, "score": "53.44407"}
{"text": "Next , the obtained language scores for the possible phrase are updated by using received language model data from the LM servers ( step 750 ) .This is the second pass in processing each phrase .Based on the updated scores , the decoder removes translations with poor scores ( step 760 ) and further determines whether the end of the current segment is reached ( step 770 ) .", "label": "", "metadata": {}, "score": "53.476025"}
{"text": "The system of claim 13 , wherein : the segment translation server cache is further operable to store history information of translation of an assigned segment .The method of claim 21 , wherein : the collection of machine language translation resource data comprises a language model for the target language which further includes n - grams with n greater than 3 .", "label": "", "metadata": {}, "score": "53.494633"}
{"text": "K ? ? ?Pz ? ? ?K ? ? ? ? ? and ?z ? ? ?K ? ? ?Pz ? ? ?K ? the estimated probability of the observed ( and therefore valid ) sequence must be lower than the exact figure and we have : ? ? ? ?", "label": "", "metadata": {}, "score": "53.533424"}
{"text": "K ? ? ?Pz ? ? ?K ? ? ? ? ? and ?z ? ? ?K ? ? ?Pz ? ? ?K ? the estimated probability of the observed ( and therefore valid ) sequence must be lower than the exact figure and we have : ? ? ? ?", "label": "", "metadata": {}, "score": "53.533424"}
{"text": "First , we develop a sentence - level mixture language model that takes advantage of the topic constraints in a sentence or article .Second , we introduce topic - d ... \" .In this paper , we investigate a new statistical language model which captures topic - related dependenciesof words within and across sentences .", "label": "", "metadata": {}, "score": "53.57615"}
{"text": "For large vocabularies , this leads to an extremely high number of possible alternativesegmentationsof the acousticsignal into words .In this context , the languagemodelevaluates the linguistic plausibility of partial or complete hypotheses .In conjunction with the remainder of the recognition system , this estimate assists in finding those hypotheses which are most likely to lead to the correct result , as well as those which may be discarded in order to limit the number to practical levels .", "label": "", "metadata": {}, "score": "53.630657"}
{"text": "For large vocabularies , this leads to an extremely high number of possible alternativesegmentationsof the acousticsignal into words .In this context , the languagemodelevaluates the linguistic plausibility of partial or complete hypotheses .In conjunction with the remainder of the recognition system , this estimate assists in finding those hypotheses which are most likely to lead to the correct result , as well as those which may be discarded in order to limit the number to practical levels .", "label": "", "metadata": {}, "score": "53.630657"}
{"text": "N - gram language models are frequently used by the speech recognition systems to constrain and guide the search .N - gram models use only the last N-1 words to predict the next word .Typical values of N that are used range from 2 - 4 .", "label": "", "metadata": {}, "score": "53.72093"}
{"text": "Thesummationin ( 75)accountsforallpart - of - speechcategories w?i ? , and the summation in ( 76 ) for all the history equivalenceclasses matching w ? ? ?i ? ? ?w(0)w(i-1 )First word w v P ( ) V ( ) w i ( ) Figure 3.1 : Operation of the category - based language model .", "label": "", "metadata": {}, "score": "53.776226"}
{"text": "Thesummationin ( 75)accountsforallpart - of - speechcategories w?i ? , and the summation in ( 76 ) for all the history equivalenceclasses matching w ? ? ?i ? ? ?w(0)w(i-1 )First word w v P ( ) V ( ) w i ( ) Figure 3.1 : Operation of the category - based language model .", "label": "", "metadata": {}, "score": "53.776226"}
{"text": "For example , a communication protocol may be used to provide monitoring communications between the load balancing mechanism and each machine under monitoring .[ 0050 ]FIG .2 shows one implementation 200 of the distributed machine translation system 100 of FIG .", "label": "", "metadata": {}, "score": "53.864944"}
{"text": "3B shows an example of a data structure that can be used in a translation cache .[ 0023 ] FIG .3C shows an example method for the operation of a translation cache .[ 0024 ] FIG .4 shows an example of a translation cache that can be connected to a load balancer .", "label": "", "metadata": {}, "score": "53.918533"}
{"text": "This approachis taken in [ 25 ] and in [ 75 ] , the first minimising the perplexity on a dedicated cross validation set , while the second minimising the training set leaving - one - out proba- bility .Determination of the phrases is automatic , and proceedsby identifying the pair which would most decrease perplexity by merging , subsequent execution of this merge , and repetition .", "label": "", "metadata": {}, "score": "53.92906"}
{"text": "This approachis taken in [ 25 ] and in [ 75 ] , the first minimising the perplexity on a dedicated cross validation set , while the second minimising the training set leaving - one - out proba- bility .Determination of the phrases is automatic , and proceedsby identifying the pair which would most decrease perplexity by merging , subsequent execution of this merge , and repetition .", "label": "", "metadata": {}, "score": "53.92906"}
{"text": "Translating an individual sentence usually requires a relatively small number of different histories , e.g. , thousands or millions , compared to all possible n grams ( 10 20 and more ) , so only a few bytes are sufficient for the integer .", "label": "", "metadata": {}, "score": "53.936203"}
{"text": "\" This use of the ID numbers compresses the size of the data for the language model and the effect can become significant for very large language models .The following is an example of n grams for the language model grouped into a set of blocks showing a sequence of n grams and associated values in a sorted order : . where , from the left to the right , is the ID numbers , the number of occurrences , and the corresponding text .", "label": "", "metadata": {}, "score": "53.988365"}
{"text": "In this paper we investigate the use of linguistic information given by language models to deal with word recognition errors on handwritten sentences .We focus especially on errors due to out - of - vocabulary ( OOV ) words .First , word posterior probabilities are computed and used to detect error hypotheses on output sentences .", "label": "", "metadata": {}, "score": "54.108437"}
{"text": "In this paper we investigate the use of linguistic information given by language models to deal with word recognition errors on handwritten sentences .We focus especially on errors due to out - of - vocabulary ( OOV ) words .First , word posterior probabilities are computed and used to detect error hypotheses on output sentences .", "label": "", "metadata": {}, "score": "54.108437"}
{"text": "The mutual information between two events x iand x jis given by : I m ? x i ? x j ? log ?P?x i ? x j ?P?x i ? ?P?x j ?( 60 )Now if the events are taken to be adjacently - occurring words , then immediately follows Using relative frequency approximations , we may estimate these probabilities as : P?x i ? x j ? is the probability that x j x i , and P?x i ? and P?x j ? are the unigram distributions of x iand x jrespectively .", "label": "", "metadata": {}, "score": "54.12104"}
{"text": "The mutual information between two events x iand x jis given by : I m ? x i ? x j ? log ?P?x i ? x j ?P?x i ? ?P?x j ?( 60 )Now if the events are taken to be adjacently - occurring words , then immediately follows Using relative frequency approximations , we may estimate these probabilities as : P?x i ? x j ? is the probability that x j x i , and P?x i ? and P?x j ? are the unigram distributions of x iand x jrespectively .", "label": "", "metadata": {}, "score": "54.12104"}
{"text": "A similar approach16is taken in [ 8 ] , where such associations are termed \" sticky pairs \" .Treatment of such frequently occurring word groups as single lexicon entries indicates thathas followedmuchmorefrequentlyin thetextthanwould has led to 3 ? 8 % word error rate improvements relative to a word - based bigram model for a small task ( 35 - 45,000 words of training data ) [ 88].", "label": "", "metadata": {}, "score": "54.13124"}
{"text": "A similar approach16is taken in [ 8 ] , where such associations are termed \" sticky pairs \" .Treatment of such frequently occurring word groups as single lexicon entries indicates thathas followedmuchmorefrequentlyin thetextthanwould has led to 3 ? 8 % word error rate improvements relative to a word - based bigram model for a small task ( 35 - 45,000 words of training data ) [ 88].", "label": "", "metadata": {}, "score": "54.13124"}
{"text": "While knowledge - based approaches are widely and successfully used in dialogue s .. . .....A specific language model is generated with the preprint papers and then merged into the topic - independent model .A probability of word w given a history h is defined as a sum of the probTable 1 : Coverage , perplexity and word error rate for lecture - style sentences language model lecture corpus newspaper ... . \" ...", "label": "", "metadata": {}, "score": "54.197166"}
{"text": "For this reason methods of explicitly taking such longer range dependencies into account are relevant .n ? ?words , where n very rarely exceeds ?This may classify two histories as equivalent 2.6.1 .Pairwise dependencies Certain long - range dependencies17may be taken into account by postulating that the probability of the next word probabilities w?i ? given the history w ? ? ?", "label": "", "metadata": {}, "score": "54.248337"}
{"text": "For this reason methods of explicitly taking such longer range dependencies into account are relevant .n ? ?words , where n very rarely exceeds ?This may classify two histories as equivalent 2.6.1 .Pairwise dependencies Certain long - range dependencies17may be taken into account by postulating that the probability of the next word probabilities w?i ? given the history w ? ? ?", "label": "", "metadata": {}, "score": "54.248337"}
{"text": "The translation cache 310 may also include the information on the context in which a translation appears in a document .For example , the context information for a translated segment may include tokens , phrases , or segments at both sides of the translated segments .", "label": "", "metadata": {}, "score": "54.335163"}
{"text": "j ?This would lead to unnecessarily large trees and An alternative form of question employing a directed acyclical graph structure termed a trellis is pre- sented in [ 89].The trellis structure allows nodes to have more than two children and children to have multiple parents , thus allowing sum - of - products Boolean expressions to be encoded while preserving the use of elementary questions at the nodes themselves .", "label": "", "metadata": {}, "score": "54.401257"}
{"text": "j ?This would lead to unnecessarily large trees and An alternative form of question employing a directed acyclical graph structure termed a trellis is pre- sented in [ 89].The trellis structure allows nodes to have more than two children and children to have multiple parents , thus allowing sum - of - products Boolean expressions to be encoded while preserving the use of elementary questions at the nodes themselves .", "label": "", "metadata": {}, "score": "54.401257"}
{"text": "[0143 ] A translation server , e.g. , a segment translation server , can be configured to keep track of n gram histories and then evaluate different continuations of tracked n grams .For example , a history may be \" A B C D , \" then explored continuations may be \" A B C D E , \" \" A B C D F , \" etc .", "label": "", "metadata": {}, "score": "54.472107"}
{"text": "However , in Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 28 . distribution obtained from a less specific definition of the history equivalence classes11 .For example , when using a trigram , the bigram distribution may be considered .", "label": "", "metadata": {}, "score": "54.485954"}
{"text": "However , in Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 28 . distribution obtained from a less specific definition of the history equivalence classes11 .For example , when using a trigram , the bigram distribution may be considered .", "label": "", "metadata": {}, "score": "54.485954"}
{"text": "Optimizing the latter remains a difficult challenge .In addition , it could be argued that using a huge training set ( e.g. , all the text in the Web ) , one could get n - gram based language models that appear to capture semantics correctly .", "label": "", "metadata": {}, "score": "54.545937"}
{"text": "l ?This recursion terminates once a ? iis no longer unseen is reached .Note also that Katz 's method is computationally efficient since the ? ? ?j ? may be precomputed for each j?f ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "54.575485"}
{"text": "l ?This recursion terminates once a ? iis no longer unseen is reached .Note also that Katz 's method is computationally efficient since the ? ? ?j ? may be precomputed for each j?f ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "54.575485"}
{"text": "6 ) can be operated in other configurations .[0079 ]After receiving a segment to translate from the load balancer 120 , the segment translation sever 130 requests and retrieves all possible translations in the target language for the segment from the translation model stored on the servers 220 ( step 910 ) .", "label": "", "metadata": {}, "score": "54.578686"}
{"text": "An integrative formulation is proposed for harnessing this synergy , in which the latent se - mantic information is used to adjust the standard n - gram proba - bility .Such hybrid language modeling compares favorably with the correspondingn - gram baseline : experiments conducted on the Wall Street Journal domain show a reduction in average word error rate of over 20 % .", "label": "", "metadata": {}, "score": "54.638683"}
{"text": "Furthermore , denote the l leaves of the tree by the set L?fl ? l ?l l g , so L?T. The development in [ 3 ] assumes that a set of Denote these by y : Q \" predictor variables \" are extracted from the history .", "label": "", "metadata": {}, "score": "54.744587"}
{"text": "Furthermore , denote the l leaves of the tree by the set L?fl ? l ?l l g , so L?T. The development in [ 3 ] assumes that a set of Denote these by y : Q \" predictor variables \" are extracted from the history .", "label": "", "metadata": {}, "score": "54.744587"}
{"text": "To allow efficient access to the language model , the number of requested probabilities from the distributed language model is kept small to reduce access and search time . [0140 ] The two - pass language model access per decoder iteration may be implemented on different levels of granularity and integrated into different search architectures .", "label": "", "metadata": {}, "score": "54.75391"}
{"text": "The load balancing logic may also take into account other factors .For example , segments from sections higher up on a Web page or earlier in a document , which can get more attention from the user , can be translated at a higher level of quality .", "label": "", "metadata": {}, "score": "54.836853"}
{"text": "A cache consists of a buffer of the most recent are calculated [ 39 ] , [ 45].The cache language model probabilities model probabilities L words of text from which language model probabilities P cacheare combined with the static P sby linear interpolation : P ?", "label": "", "metadata": {}, "score": "54.84971"}
{"text": "A cache consists of a buffer of the most recent are calculated [ 39 ] , [ 45].The cache language model probabilities model probabilities L words of text from which language model probabilities P cacheare combined with the static P sby linear interpolation : P ?", "label": "", "metadata": {}, "score": "54.84971"}
{"text": "The distance measure has been defined with the particular goal of minimising the effect that syntax has on word co - occurrences , and takes advantage of the grammatical word classifications implicit in the oper- ation of the syntactic model .By assuming an exponentially decaying functional dependence between the occurrence probability of a word , and the distance to a related word , long - range correlations may be captured .", "label": "", "metadata": {}, "score": "54.976135"}
{"text": "The distance measure has been defined with the particular goal of minimising the effect that syntax has on word co - occurrences , and takes advantage of the grammatical word classifications implicit in the oper- ation of the syntactic model .By assuming an exponentially decaying functional dependence between the occurrence probability of a word , and the distance to a related word , long - range correlations may be captured .", "label": "", "metadata": {}, "score": "54.976135"}
{"text": "Since words generally have more than one grammatical func- tion , it is necessary for the model to maintain several possible classifications of the sentence history in terms of category sequences .The generally much smaller number of categories than words lowers the training corpus sparseness and allows be accounted for .", "label": "", "metadata": {}, "score": "55.011326"}
{"text": "Since words generally have more than one grammatical func- tion , it is necessary for the model to maintain several possible classifications of the sentence history in terms of category sequences .The generally much smaller number of categories than words lowers the training corpus sparseness and allows be accounted for .", "label": "", "metadata": {}, "score": "55.011326"}
{"text": "K ?( 3 ) where denotes an estimate .This probability can assist the speech recognition system in deciding upon one of possibly several acoustically - similar , competing ways of segmenting the observation vectors into words according to their linguistic likelihood .", "label": "", "metadata": {}, "score": "55.076363"}
{"text": "K ?( 3 ) where denotes an estimate .This probability can assist the speech recognition system in deciding upon one of possibly several acoustically - similar , competing ways of segmenting the observation vectors into words according to their linguistic likelihood .", "label": "", "metadata": {}, "score": "55.076363"}
{"text": "The statistical approach uses deleted interpolation of n - gram frequencies as basis and determines the interpolation weights by a modi ed version of the original algorithm .Additionally , we present andevaluate di erent approaches to improve the prediction process , e.g. including knowledge from a dialogue grammar .", "label": "", "metadata": {}, "score": "55.09037"}
{"text": "Secondly , a maximum entropy model is presented to distinguish verb subclasses .Finally , a statistical parser is developed to evaluate the verb subdivision .Experimental results indicate that the use of verb subclasses has a good influence on parsing performance .", "label": "", "metadata": {}, "score": "55.097557"}
{"text": "Secondly , a maximum entropy model is presented to distinguish verb subclasses .Finally , a statistical parser is developed to evaluate the verb subdivision .Experimental results indicate that the use of verb subclasses has a good influence on parsing performance .", "label": "", "metadata": {}, "score": "55.097557"}
{"text": "For example , when the vocabulary size is we assume z ? ? ?K ? ? ? , N , there is no grammar5 , and z ? ? ?K ? ? ? to be the sequence of words w ? ? ?", "label": "", "metadata": {}, "score": "55.10436"}
{"text": "For example , when the vocabulary size is we assume z ? ? ?K ? ? ? , N , there is no grammar5 , and z ? ? ?K ? ? ? to be the sequence of words w ? ? ?", "label": "", "metadata": {}, "score": "55.10436"}
{"text": "A language model can be used to keep information for all word sequences up to n in length by using an n gram language model .Various machine translation systems use n grams with relatively small n values in their language models , e.g. , 2-gram or 3-gram language models , so that the language models can be sufficiently small to be stored on a single machine .", "label": "", "metadata": {}, "score": "55.153942"}
{"text": "Conversely , various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination .In certain circumstances , multitasking and parallel processing may be advantageous .[ 0171 ]", "label": "", "metadata": {}, "score": "55.33493"}
{"text": "A fundamental limitation of the n - gram approach is that it is not possible to capture dependencies span- ning more than arising from such factors as the topic and style of the text .Empirical evidence suggests that a word which has already been seen in a passage is significantly more likely to recur in the near future than would otherwise be expected .", "label": "", "metadata": {}, "score": "55.3775"}
{"text": "A fundamental limitation of the n - gram approach is that it is not possible to capture dependencies span- ning more than arising from such factors as the topic and style of the text .Empirical evidence suggests that a word which has already been seen in a passage is significantly more likely to recur in the near future than would otherwise be expected .", "label": "", "metadata": {}, "score": "55.3775"}
{"text": "Then any of several model structures could be used . \" ... decide which contexts are similar and can share parameters .A key feature of this approach is that it allows the construction of models which are dependent upon contextual effects occurring across word boundaries .", "label": "", "metadata": {}, "score": "55.404694"}
{"text": "Incorporation of the models has led to a significant improvement in the word error rate of a high - performance baseline speech - recognition system .Page 3 .Decla ration This thesis is the result of my own original work , and where it draws on the work of others , this is acknowledgedat the appropriatepoints in thetext .", "label": "", "metadata": {}, "score": "55.4665"}
{"text": "Incorporation of the models has led to a significant improvement in the word error rate of a high - performance baseline speech - recognition system .Page 3 .Decla ration This thesis is the result of my own original work , and where it draws on the work of others , this is acknowledgedat the appropriatepoints in thetext .", "label": "", "metadata": {}, "score": "55.4665"}
{"text": "HMMs may either be trained to model entire words directly , or to model subword units ( such as phonemes ) which are concatenated to obtain words .The latter approach is usually adopted since , even for moderately - sized vocabulary , there may not be sufficient training material to determine whole - word models reliably .", "label": "", "metadata": {}, "score": "55.514862"}
{"text": "HMMs may either be trained to model entire words directly , or to model subword units ( such as phonemes ) which are concatenated to obtain words .The latter approach is usually adopted since , even for moderately - sized vocabulary , there may not be sufficient training material to determine whole - word models reliably .", "label": "", "metadata": {}, "score": "55.514862"}
{"text": "Themodelemploysn - gramsofpart - of - speechword - categoriestocapturesequentialgrammaticaldepen- dencies [ 56 ] , [ 57].Since there are far fewer parts - of - speech than there are words in a typical vocabulary , the number of different n - grams is much smaller for a given value of model .", "label": "", "metadata": {}, "score": "55.66577"}
{"text": "Themodelemploysn - gramsofpart - of - speechword - categoriestocapturesequentialgrammaticaldepen- dencies [ 56 ] , [ 57].Since there are far fewer parts - of - speech than there are words in a typical vocabulary , the number of different n - grams is much smaller for a given value of model .", "label": "", "metadata": {}, "score": "55.66577"}
{"text": "Page 46 . measured relative to a language model constructed from the general sample only , as well as a model constructedfrom the pooled data .For these two cases , word error rate improvements of between4.6 and 6.0 % , and between 3.4 and 4.3 % were achieved respectively .", "label": "", "metadata": {}, "score": "55.668518"}
{"text": "Page 46 . measured relative to a language model constructed from the general sample only , as well as a model constructedfrom the pooled data .For these two cases , word error rate improvements of between4.6 and 6.0 % , and between 3.4 and 4.3 % were achieved respectively .", "label": "", "metadata": {}, "score": "55.668518"}
{"text": "i ?( 4 ) where in practice speech recognition process usually requires evaluation of the conditional probabilities appearing on the right hand side of equation ( 4 ) , these are normally estimated directly .In developing a language model , the task is to find suitable structures for modelling the probabilistic dependencies between words in nat- ural language , and then to use these structures to estimate either the conditional or the joint probabilities of word sequences .", "label": "", "metadata": {}, "score": "55.67755"}
{"text": "i ?( 4 ) where in practice speech recognition process usually requires evaluation of the conditional probabilities appearing on the right hand side of equation ( 4 ) , these are normally estimated directly .In developing a language model , the task is to find suitable structures for modelling the probabilistic dependencies between words in nat- ural language , and then to use these structures to estimate either the conditional or the joint probabilities of word sequences .", "label": "", "metadata": {}, "score": "55.67755"}
{"text": "It is also capable of generating a lattice of word hypotheses with little computational overhead .These lattices can be used to constrain further decoding , allowing efficient use of complex acoustic and language models .The effectiveness of these techniques has been assessed on a variety of large vocabulary continuous speech recognition tasks and results are presented which analyse performance in terms of computational complexity and recognition accuracy .", "label": "", "metadata": {}, "score": "55.725395"}
{"text": "j ?X ? ?i ?N ? ?i j ?j ? ? ?D?M ?j N ?j 12This comparison was carried out for both a bigram and a trigram language model using a 750 000 word corpus of office correspondence .", "label": "", "metadata": {}, "score": "55.815666"}
{"text": "j ?X ? ?i ?N ? ?i j ?j ? ? ?D?M ?j N ?j 12This comparison was carried out for both a bigram and a trigram language model using a 750 000 word corpus of office correspondence .", "label": "", "metadata": {}, "score": "55.815666"}
{"text": "Another idea is to decompose the probability computation hierarchically , using a tree of binary probabilistic decisions , so as to replace \\(O(N)\\ ) computations by \\(O(\\log N)\\ ) computations ( Morin and Bengio 2005 ) .Yet another idea is to replace the exact gradient by a stochastic estimator obtained using a Monte - Carlo sampling technique ( Bengio and Senecal 2008 ) .", "label": "", "metadata": {}, "score": "55.879997"}
{"text": "June 1997 .Language models are computational techniques and structures that describe word sequences produced by human subjects , and the work presented here considers primarily their application to automatic speech - recognition systems .Due to the very complex nature of natural languages as well as the need for robust recognition , statistically - based language models , which assign probabilities to word sequences , have proved most successful .", "label": "", "metadata": {}, "score": "55.934784"}
{"text": "3B , a cache can map a source - language section to one or more target - language sections .Each target - language section is marked with information about its translation quality level .Thus , the cache can contain translations at different levels of translation quality .", "label": "", "metadata": {}, "score": "56.017178"}
{"text": "K ? , and that any imperfections in the ?P ? ? ? will lead to a lower probability .A good model is therefore one ?P ?z ? ? ?K ?K ( 5 ) or , taking the logarithm to base 2 , we obtain once more the entropy estimate : ? K ? log ?", "label": "", "metadata": {}, "score": "56.07785"}
{"text": "K ? , and that any imperfections in the ?P ? ? ? will lead to a lower probability .A good model is therefore one ?P ?z ? ? ?K ?K ( 5 ) or , taking the logarithm to base 2 , we obtain once more the entropy estimate : ? K ? log ?", "label": "", "metadata": {}, "score": "56.07785"}
{"text": "The performance of the resulting multispan language models , as measured by perplexity , compares favorably with the corresponding n - gram performance .Index Terms - Latent semantic analysis , n - gram adaptation , perplexity reduction , statistical language modeling . . ..", "label": "", "metadata": {}, "score": "56.124474"}
{"text": "Assuming that the information source is ergodic4we may write the above ensemble average as : K that may be producedby the ? lim K ? ?K ? log ?P ?z ? ? ?K ?Up to this point it has been assumed that the true probability associated with each sequence is known , so that the above equation will yield the actual entropy of the source .", "label": "", "metadata": {}, "score": "56.203716"}
{"text": "Assuming that the information source is ergodic4we may write the above ensemble average as : K that may be producedby the ? lim K ? ?K ? log ?P ?z ? ? ?K ?Up to this point it has been assumed that the true probability associated with each sequence is known , so that the above equation will yield the actual entropy of the source .", "label": "", "metadata": {}, "score": "56.203716"}
{"text": "N?w a ? w b ?L W ?N w ? ?L W ?N?w a ?N w ?N?w b ?N w ?( 63 ) Here within awindowof length corpus .The association ratio differs from the mutual information in that it is not symmetric but encodes linear precedence .", "label": "", "metadata": {}, "score": "56.233444"}
{"text": "N?w a ? w b ?L W ?N w ? ?L W ?N?w a ?N w ?N?w b ?N w ?( 63 ) Here within awindowof length corpus .The association ratio differs from the mutual information in that it is not symmetric but encodes linear precedence .", "label": "", "metadata": {}, "score": "56.233444"}
{"text": "The second technique addresses longer - range word - pair relationships , that arise due to factors such as the topic or the style of the text .Empirical evidence is presented of an approximately exponentially decaying behaviour when considering the probabilities of related words as a function of an appropriately defined separating distance .", "label": "", "metadata": {}, "score": "56.265137"}
{"text": "The second technique addresses longer - range word - pair relationships , that arise due to factors such as the topic or the style of the text .Empirical evidence is presented of an approximately exponentially decaying behaviour when considering the probabilities of related words as a function of an appropriately defined separating distance .", "label": "", "metadata": {}, "score": "56.265137"}
{"text": "[ 0039 ] A replication design , when implemented , can incorporate a load balancing mechanism to monitor the work load of different machines for the replication and , based on the work load , to manage or distribute incoming work load to different machines .", "label": "", "metadata": {}, "score": "56.283905"}
{"text": "However , naive implementations of the above equations yield predictors that are too slow for large scale natural language applications .Schwenk and Gauvain ( 2004 ) were able to build systems in which the neural network component took less than 5 % of real - time ( the duration of the speech being analyzed ) .", "label": "", "metadata": {}, "score": "56.322617"}
{"text": "Computational issues and applications .The experiments have been mostly on small corpora , where training a neural network language model is easier , and show important improvements on both log - likelihood and speech recognition accuracy .Resampling techniques may be used to train the neural network language model on corpora of several hundreds of millions of words ( Schwenk and Gauvain 2004 ) .", "label": "", "metadata": {}, "score": "56.370483"}
{"text": "Although this approach has not been applied to wordsense disambiguation , there is a strong similarity between that method of model formulation and our own .A maximum entropy model fo ... . by R. Iyer , M. Ostendorf - IEEE Transactions on Speech and Audio Processing , 1996 . \" ...", "label": "", "metadata": {}, "score": "56.38487"}
{"text": "234 . \" ...An adaptive statistical languagemodel is described , which successfullyintegrates long distancelinguistic information with other knowledge sources .Most existing statistical language models exploit only the immediate history of a text .To extract information from further back in the document 's h ... \" .", "label": "", "metadata": {}, "score": "56.429672"}
{"text": "Other embodiments are within the scope of the following claims .A language model is a function , or an algorithm for learning such a function , that captures the salient statistical characteristics of the distribution of sequences of words in a natural language , typically allowing one to make probabilistic predictions of the next word given preceding ones .", "label": "", "metadata": {}, "score": "56.483143"}
{"text": "Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 32 .Now consider a sample space , with the words of the vocabulary as events , i.e. : ii ? with ? w With reference to the formalism introduced in section 2.3 , we denote the population and the sample corresponding to each history classification by ? jand ? jrespectively , so that : ? j ?", "label": "", "metadata": {}, "score": "56.518486"}
{"text": "Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 32 .Now consider a sample space , with the words of the vocabulary as events , i.e. : ii ? with ? w With reference to the formalism introduced in section 2.3 , we denote the population and the sample corresponding to each history classification by ? jand ? jrespectively , so that : ? j ?", "label": "", "metadata": {}, "score": "56.518486"}
{"text": "Baseline word - based n - gram models ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 111 E. Appendix : LOB corpus word - categories 112 E.1.Part - of - speech tags found in the LOB corpus ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "56.521065"}
{"text": "Baseline word - based n - gram models ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 111 E. Appendix : LOB corpus word - categories 112 E.1.Part - of - speech tags found in the LOB corpus ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "56.521065"}
{"text": "j w c jthen comprises all words that have followed or will ever follow ? jof this population consists of all words that have in fact The number of occurrences of an event formed by the concatenation of the context w iin the sample ? jis given by the number of times the n - gram w c jand the word w iis seen in the training corpus : N ?", "label": "", "metadata": {}, "score": "56.531616"}
{"text": "j w c jthen comprises all words that have followed or will ever follow ? jof this population consists of all words that have in fact The number of occurrences of an event formed by the concatenation of the context w iin the sample ? jis given by the number of times the n - gram w c jand the word w iis seen in the training corpus : N ?", "label": "", "metadata": {}, "score": "56.531616"}
{"text": "( 45 )This approach has the advantage of resulting in strong smoothing when the proportion of the number of different events to the total number of events occurring in the context is large , a condition that will hold when these counts are sparse .", "label": "", "metadata": {}, "score": "56.593903"}
{"text": "( 45 )This approach has the advantage of resulting in strong smoothing when the proportion of the number of different events to the total number of events occurring in the context is large , a condition that will hold when these counts are sparse .", "label": "", "metadata": {}, "score": "56.593903"}
{"text": "However , when employing large data sets , this extension leads to a large increase in required computation , and it is necessary to limit the number of categories to such an extent that it remains possible to build language models with better performance using the aforementioned bigram clustering .", "label": "", "metadata": {}, "score": "56.6031"}
{"text": "However , when employing large data sets , this extension leads to a large increase in required computation , and it is necessary to limit the number of categories to such an extent that it remains possible to build language models with better performance using the aforementioned bigram clustering .", "label": "", "metadata": {}, "score": "56.6031"}
{"text": "In some implementations , a user could supply a specific translation database to be used for a specific translation request .Applications of Distributed Models in Automated Processing Systems Beyond Machine Translation .[0159 ]The above and other distributed system designs for automated machine translation based on partition , replication and load balancing to access large models and to provide scalable and adaptive processing can be applied to other automated processing systems beyond machine translation .", "label": "", "metadata": {}, "score": "56.638973"}
{"text": "An optimisation algorithm that maximises the training set probability of the language model is described in [ 42 ] and [ 53].A category - based bigram language model of the form ( 59 ) is used , so that each word may not belong to more than one category .", "label": "", "metadata": {}, "score": "56.65844"}
{"text": "An optimisation algorithm that maximises the training set probability of the language model is described in [ 42 ] and [ 53].A category - based bigram language model of the form ( 59 ) is used , so that each word may not belong to more than one category .", "label": "", "metadata": {}, "score": "56.65844"}
{"text": "[ 0148 ]To reduce translation processing , an automated machine translation system can include a cache to store translated text and document sections .Document sections can be words , phrases , sentence fragments , sentences , paragraphs , entire texts / documents , etc .", "label": "", "metadata": {}, "score": "56.665222"}
{"text": "While statistical n - gram modeling can readily take local constraints into account , global constraints have b ... \" .Abstract - A new framework is proposed to construct multispan language models for large vocabulary speech recognition , by exploiting both local and global constraints present in the language .", "label": "", "metadata": {}, "score": "56.746117"}
{"text": "For example , a trigram language model based on part - of - speech categories is used in conjunction with a unigram cache in [ 45].Let category hypothesised for v jdenote a w?i ? , so that the trigram component of the language model computes : P ?", "label": "", "metadata": {}, "score": "56.749947"}
{"text": "For example , a trigram language model based on part - of - speech categories is used in conjunction with a unigram cache in [ 45].Let category hypothesised for v jdenote a w?i ? , so that the trigram component of the language model computes : P ?", "label": "", "metadata": {}, "score": "56.749947"}
{"text": "The result is a variable - length category - based n - gram language model .n to be increased , so that longer - range syntactic correlations may 1.3.2 .Modelling fixed short - range semantic relations Certain word combinations occur much more frequently than an extrapolation from the overall syntactic behaviour would suggest , such as the bigram of proper nouns \" United Kingdom \" .", "label": "", "metadata": {}, "score": "56.841217"}
{"text": "The result is a variable - length category - based n - gram language model .n to be increased , so that longer - range syntactic correlations may 1.3.2 .Modelling fixed short - range semantic relations Certain word combinations occur much more frequently than an extrapolation from the overall syntactic behaviour would suggest , such as the bigram of proper nouns \" United Kingdom \" .", "label": "", "metadata": {}, "score": "56.841217"}
{"text": "Page 25 .Taking a different approach , the development in [ 55 ] describes methods by means of which constraints may be placed on the probability estimates , such as : q r ? ? q r which requires the probability of more frequent events to equal or exceed that of less frequent ones , or : r ? ?", "label": "", "metadata": {}, "score": "56.911972"}
{"text": "Page 25 .Taking a different approach , the development in [ 55 ] describes methods by means of which constraints may be placed on the probability estimates , such as : q r ? ? q r which requires the probability of more frequent events to equal or exceed that of less frequent ones , or : r ? ?", "label": "", "metadata": {}, "score": "56.911972"}
{"text": "Moreover , advantage may be taken of the recent vast increases in the amount of available training text , which now runs into hundreds of millions of words .A speech - recognition system must find the most likely sentence hypothesis for each spoken utterance .", "label": "", "metadata": {}, "score": "56.951912"}
{"text": "Moreover , advantage may be taken of the recent vast increases in the amount of available training text , which now runs into hundreds of millions of words .A speech - recognition system must find the most likely sentence hypothesis for each spoken utterance .", "label": "", "metadata": {}, "score": "56.951912"}
{"text": "i ?( 50 )Backing off to the from an implementational point of view , but in some cases it may lead to overestimation of an unseen probability .In particular , the absence of an n - gram may be as a result of linguistic improbability rather than lack of data .", "label": "", "metadata": {}, "score": "57.04727"}
{"text": "i ?( 50 )Backing off to the from an implementational point of view , but in some cases it may lead to overestimation of an unseen probability .In particular , the absence of an n - gram may be as a result of linguistic improbability rather than lack of data .", "label": "", "metadata": {}, "score": "57.04727"}
{"text": "[0070 ] In other implementations , a single local cache may be used to have a high - level cache section and a low - level cache section that correspond to the separate high - level cache and low - level cache , respectively .", "label": "", "metadata": {}, "score": "57.19329"}
{"text": "The context information may also include the context for the segment in the source language .[ 0063 ] FIG .3C shows an example method for the operation of the translation cache 310 .This example uses the quality information as the selection parameter to illustrate the operation .", "label": "", "metadata": {}, "score": "57.21162"}
{"text": "Shorter n grams can be generated by stripping words from one end , either at the front end or the rear end of an n gram .As an example , a client requesting for machine translation may ask for the sequence \" A B C D E \" , where each letter represents one word , and require stripping from the front of the sequence .", "label": "", "metadata": {}, "score": "57.258972"}
{"text": "Despite many attempts to incorporate more sophisticated information into the models , the n - gram model remains the state of the art , used in virtually all speech recognition systems .In this paper we address the question of whether there is hope in improving language modeling by incorporating more sophisticated linguistic and world knowledge , or whether the n - grams are already capturing the majority of the information that can be employed . \" ...", "label": "", "metadata": {}, "score": "57.33417"}
{"text": "Here again the language model the hypothesised sequence of words accuracy [ 27 ] , [ 83 ] , [ 87].P?w ? ? ?K ? ? supplies an estimate of the grammatical plausibility of w ? ? ?", "label": "", "metadata": {}, "score": "57.393684"}
{"text": "Here again the language model the hypothesised sequence of words accuracy [ 27 ] , [ 83 ] , [ 87].P?w ? ? ?K ? ? supplies an estimate of the grammatical plausibility of w ? ? ?", "label": "", "metadata": {}, "score": "57.393684"}
{"text": "w?i?jw ? ? ?i ? ? ?( 8) We will henceforth refer to number of possible different histories , statistics can not be gathered for each , and the estimation of the conditional probability must be made on the grounds of some broader classification of w ? ? ?", "label": "", "metadata": {}, "score": "57.471233"}
{"text": "w?i?jw ? ? ?i ? ? ?( 8) We will henceforth refer to number of possible different histories , statistics can not be gathered for each , and the estimation of the conditional probability must be made on the grounds of some broader classification of w ? ? ?", "label": "", "metadata": {}, "score": "57.471233"}
{"text": "w?i ? g ( 13 ) where to the approach taken by equations ( 12 ) and ( 13 ) , a decision tree is employed in [ 3 ] to map the word histories onto the equivalence classes .w?i ? n?i ? refers to the sequence of n ? ? words fw?i?n ? ?", "label": "", "metadata": {}, "score": "57.53956"}
{"text": "w?i ? g ( 13 ) where to the approach taken by equations ( 12 ) and ( 13 ) , a decision tree is employed in [ 3 ] to map the word histories onto the equivalence classes .w?i ? n?i ? refers to the sequence of n ? ? words fw?i?n ? ?", "label": "", "metadata": {}, "score": "57.53956"}
{"text": "If the segment translation server 130 can not find any information for an n gram in either cache , the generated request is then placed in the LM lookup queue 620 and is sent out to the language model servers 210 ( step 964 ) .", "label": "", "metadata": {}, "score": "57.542397"}
{"text": "The move resulting in the largest increase in the log probability is then selected , executed , and the process repeated .This continues until some convergence criterion is met .In [ 53 ] the number of categories is assumed fixed for every optimisation run , this number being varied between 30 and 700 in repeated trials .", "label": "", "metadata": {}, "score": "57.557266"}
{"text": "The move resulting in the largest increase in the log probability is then selected , executed , and the process repeated .This continues until some convergence criterion is met .In [ 53 ] the number of categories is assumed fixed for every optimisation run , this number being varied between 30 and 700 in repeated trials .", "label": "", "metadata": {}, "score": "57.557266"}
{"text": "We investigate the use of mixture modelling to adapt our models for this specific task .Individual models , created from different in - domain and out - of - domain data sources , are combined using linear and log - linear weighting methods for the different components of an SMT system .", "label": "", "metadata": {}, "score": "57.564247"}
{"text": "Each distinct bigram context j?j .In general the history classification8 H?w ? segments the training corpus into N Hsubsets ?j , where j As the history classifications to a trigram ) , the discrimination of the language model improves since it can treat a greater variety of histories separately , but at the same time decreases .", "label": "", "metadata": {}, "score": "57.623894"}
{"text": "Each distinct bigram context j?j .In general the history classification8 H?w ? segments the training corpus into N Hsubsets ?j , where j As the history classifications to a trigram ) , the discrimination of the language model improves since it can treat a greater variety of histories separately , but at the same time decreases .", "label": "", "metadata": {}, "score": "57.623894"}
{"text": "In some implementations , the translation front end 110 may first determine whether a proper translation for a segment is available in the system 100 and retrieves that translation as the translated segment without sending that segment to the load balancer 120 .", "label": "", "metadata": {}, "score": "57.652725"}
{"text": "Otherwise , the decoder extracts the best translation for the segment and sends the translated segment 702 to the load balancer 120 .[ 0076 ] FIG .8 shows an example of a processing step ( step 740 ) in FIG .", "label": "", "metadata": {}, "score": "57.684452"}
{"text": "C ? w?i?p ?i ?M ?w?i?p?i ?( 49 ) 14Results presented in [ 64 ] indicate small performance improvements when using larger information enquiry database ) , but not for a larger and more diverse domain ( the 1 million word Brown corpus ) .", "label": "", "metadata": {}, "score": "57.764175"}
{"text": "C ? w?i?p ?i ?M ?w?i?p?i ?( 49 ) 14Results presented in [ 64 ] indicate small performance improvements when using larger information enquiry database ) , but not for a larger and more diverse domain ( the 1 million word Brown corpus ) .", "label": "", "metadata": {}, "score": "57.764175"}
{"text": "It minimises the effect syntax has on word co - occurrences while taking particular advantage of the grammatical word classifications implicit in the operation of the category model .Since only related words are treated , the model size may be constrained to reasonable levels .", "label": "", "metadata": {}, "score": "57.768547"}
{"text": "It minimises the effect syntax has on word co - occurrences while taking particular advantage of the grammatical word classifications implicit in the operation of the category model .Since only related words are treated , the model size may be constrained to reasonable levels .", "label": "", "metadata": {}, "score": "57.768547"}
{"text": "By describing the observation vectors as a probabilistic time series , the HMM takes the inherent natural variability of human speechcharacteristics into account .This likeli- hood is used to make a statistical decision regarding the utterance to be recognised , as illustrated in the x , the parameters of the model M may be adjusted to best represent this data in a P?xjM ?", "label": "", "metadata": {}, "score": "57.77155"}
{"text": "By describing the observation vectors as a probabilistic time series , the HMM takes the inherent natural variability of human speechcharacteristics into account .This likeli- hood is used to make a statistical decision regarding the utterance to be recognised , as illustrated in the x , the parameters of the model M may be adjusted to best represent this data in a P?xjM ?", "label": "", "metadata": {}, "score": "57.77155"}
{"text": "[ 0152 ] The translated segments are assembled by the system , e.g. , the translation front end server 130 ( FIG .1 or 2 ) , to form the translated text or document which is returned to the requestor .", "label": "", "metadata": {}, "score": "57.7775"}
{"text": "In particular , the optimal number of categories may be determined automatically in this way .The leaving - one - out method of cross - validation [ 19 ] is employed in [ 42 ] to achieve this , and in conjunction with the described optimisation algorithm it has indeed been possible to estimate the best number of categories fairly accurately .", "label": "", "metadata": {}, "score": "57.82215"}
{"text": "In particular , the optimal number of categories may be determined automatically in this way .The leaving - one - out method of cross - validation [ 19 ] is employed in [ 42 ] to achieve this , and in conjunction with the described optimisation algorithm it has indeed been possible to estimate the best number of categories fairly accurately .", "label": "", "metadata": {}, "score": "57.82215"}
{"text": "j ? ?N p ? ? ? arechosento maximisetheprobability calculatedusing P di ? ?j ? ?In order to obtain the optimal smoothing parameters , an approach based on a hidden Markov model interpretation of equation ( 41 ) is proposed in [ 38].", "label": "", "metadata": {}, "score": "57.84174"}
{"text": "j ? ?N p ? ? ? arechosento maximisetheprobability calculatedusing P di ? ?j ? ?In order to obtain the optimal smoothing parameters , an approach based on a hidden Markov model interpretation of equation ( 41 ) is proposed in [ 38].", "label": "", "metadata": {}, "score": "57.84174"}
{"text": "For efficiency , we do not normalize the model ; that is , we do not require that the \" probabilities \" in the language model sum to 1 .With these techniques , we were able to achieve a modest reduction in speech recognition word - error rate in the Broadcast News domain . by Eric Brill , Radu Florian , John C. Henderson , Lidia Mangu - In Proc . of COLING - ACL-98 , 1998 . \" ...", "label": "", "metadata": {}, "score": "57.85227"}
{"text": "In some cases , during the first pass the sentence would be translated completely with the dummy language model .The first pass may produce an efficient pruned representation of the search space .The second pass then re - scores the representation of the search space using the probabilities requested from the distributed language model .", "label": "", "metadata": {}, "score": "57.903202"}
{"text": "A large literature on techniques to smooth frequency counts of subsequences has given rise to a number of algorithms and variants .Distributed representations .The idea of distributed representation has been at the core of the revival of artificial neural network research in the early 1980 's , best represented by the connectionist bringing together computer scientists , cognitive psychologists , physicists , neuroscientists , and others .", "label": "", "metadata": {}, "score": "57.91725"}
{"text": "w?i?jw ? ? ?i ? ? ?P ?w?i?jH?w?i ? ?( 10 ) Ingeneral , however , theoperator H ? ? ? is many - to - many[38 ] , inwhichcasecalculationoftheprobability Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .", "label": "", "metadata": {}, "score": "57.94529"}
{"text": "w?i?jw ? ? ?i ? ? ?P ?w?i?jH?w?i ? ?( 10 ) Ingeneral , however , theoperator H ? ? ? is many - to - many[38 ] , inwhichcasecalculationoftheprobability Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .", "label": "", "metadata": {}, "score": "57.94529"}
{"text": "The word categories found in this way are used to build a category - based trigram language model in analogy to equation ( 59 ) .This model is found to have a perplexity that is 11 % higher than that of a a word - based trigram model .", "label": "", "metadata": {}, "score": "57.95203"}
{"text": "The word categories found in this way are used to build a category - based trigram language model in analogy to equation ( 59 ) .This model is found to have a perplexity that is 11 % higher than that of a a word - based trigram model .", "label": "", "metadata": {}, "score": "57.95203"}
{"text": "KeywordsStatistical machine translation - Bilingual n - gram language models - Factored language models . \" In [ 6 ] , an approach based on word posterior probabilities computed on a confusion network is proposed to detect OOV words .Finally , works on tagging texts containing OOV words rely on POS categories which are used in conjunction with n - gram LMs to achieve better results [ 10].", "label": "", "metadata": {}, "score": "57.969692"}
{"text": "KeywordsStatistical machine translation - Bilingual n - gram language models - Factored language models . \" In [ 6 ] , an approach based on word posterior probabilities computed on a confusion network is proposed to detect OOV words .Finally , works on tagging texts containing OOV words rely on POS categories which are used in conjunction with n - gram LMs to achieve better results [ 10].", "label": "", "metadata": {}, "score": "57.969692"}
{"text": "X ?v?v?V?w?i ? ?P ?w?i?jv ?P ? vjw ? ? ?i ? ? ?( 75 )Furthermore , employing the decomposition ( 54 ) and the n - gram assumption ( 73 ) , we may decompose the second term on the right hand side of equation ( 75 ) as follows : P ?", "label": "", "metadata": {}, "score": "58.050194"}
{"text": "X ?v?v?V?w?i ? ?P ?w?i?jv ?P ? vjw ? ? ?i ? ? ?( 75 )Furthermore , employing the decomposition ( 54 ) and the n - gram assumption ( 73 ) , we may decompose the second term on the right hand side of equation ( 75 ) as follows : P ?", "label": "", "metadata": {}, "score": "58.050194"}
{"text": "The function with the highest entropy within that set is the ME solution ... .by Rebecca Bruce , Janyce Wiebe - In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics , 1994 . \" ...Most probabilistic classifiers used for word - sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features .", "label": "", "metadata": {}, "score": "58.07881"}
{"text": "Category - basedmodels share statistics between words of the same category , and are therefore able to generalise to word patterns never encountered in the training corpus .This ability to sensibly process unseen events is termed language model robustness .Grouping words into categories can reducethe number of contexts in a model , and thereby counter training set sparseness .", "label": "", "metadata": {}, "score": "58.080048"}
{"text": "Category - basedmodels share statistics between words of the same category , and are therefore able to generalise to word patterns never encountered in the training corpus .This ability to sensibly process unseen events is termed language model robustness .Grouping words into categories can reducethe number of contexts in a model , and thereby counter training set sparseness .", "label": "", "metadata": {}, "score": "58.080048"}
{"text": "The normalised number of content words common to both clusters was used as a similarity measure , and further cluster refinement is achieved by reassigning member sentences according to a greedy algorithm so as to maximise the training set probability .Note that the componenttrigram models every reassignment using the new partition of the training corpus .", "label": "", "metadata": {}, "score": "58.097412"}
{"text": "The normalised number of content words common to both clusters was used as a similarity measure , and further cluster refinement is achieved by reassigning member sentences according to a greedy algorithm so as to maximise the training set probability .Note that the componenttrigram models every reassignment using the new partition of the training corpus .", "label": "", "metadata": {}, "score": "58.097412"}
{"text": "Such information may be used to determine whether a particular translation is a suitable translation to be used for the segment to be translated .The quality information in the translation cache , for example , may be used to select a suitable translation with a desired quality level .", "label": "", "metadata": {}, "score": "58.112026"}
{"text": "P ?z ? ? ?K ?The self information is an information - theoretic measure of the amount of information gained by wit- nessing the sequence carry a larger amount of information than sequences seen very frequently .The per - event entropy , which z ? ? ?", "label": "", "metadata": {}, "score": "58.114"}
{"text": "P ?z ? ? ?K ?The self information is an information - theoretic measure of the amount of information gained by wit- nessing the sequence carry a larger amount of information than sequences seen very frequently .The per - event entropy , which z ? ? ?", "label": "", "metadata": {}, "score": "58.114"}
{"text": "i ? cache ? ?P s ?w?i?jw ? ? ?i ? ? ? cache ?P cache ?w?i?jw ? ? ?i ?( 66 )The interpolation weights set [ 24 ] , [ 29 ] , although deleted interpolation has been employed in [ 45].", "label": "", "metadata": {}, "score": "58.132755"}
{"text": "i ? cache ? ?P s ?w?i?jw ? ? ?i ? ? ? cache ?P cache ?w?i?jw ? ? ?i ?( 66 )The interpolation weights set [ 24 ] , [ 29 ] , although deleted interpolation has been employed in [ 45].", "label": "", "metadata": {}, "score": "58.132755"}
{"text": "Statistical language models used in large - vocabulary speech recognition must properly encapsulate the various constraints , both local and global , present in the language .While local constraints are readily captured through n - gram modeling , global constraints , such as long - term semantic dependencies , have been more diffi - cult to handle within a data - driven formalism .", "label": "", "metadata": {}, "score": "58.156834"}
{"text": "Here each word is assigned to one or more categories by human experts according to its syntactic function .For the LOB corpus it is found that summing over all category and history equivalence class assignments ( as opposed to choosing the most likely ones ) reduces the perplexity by 20 % .", "label": "", "metadata": {}, "score": "58.202248"}
{"text": "Here each word is assigned to one or more categories by human experts according to its syntactic function .For the LOB corpus it is found that summing over all category and history equivalence class assignments ( as opposed to choosing the most likely ones ) reduces the perplexity by 20 % .", "label": "", "metadata": {}, "score": "58.202248"}
{"text": "n y ?y ?y Q o For the particular model studied in [ 3 ] , these variables were chosen simply to be the most recent words , and hence we have : Q y ?n y ?y ? q Q o ?", "label": "", "metadata": {}, "score": "58.20616"}
{"text": "n y ?y ?y Q o For the particular model studied in [ 3 ] , these variables were chosen simply to be the most recent words , and hence we have : Q y ?n y ?y ? q Q o ?", "label": "", "metadata": {}, "score": "58.20616"}
{"text": "Within a block , all the entries are segregated into the different n gram length and rewritten in terms of local IDs .The actual block format for the language model data is : .[0099 ] shared_prefix_size : byte .", "label": "", "metadata": {}, "score": "58.2375"}
{"text": "On - line algorithms developed for this problem have parameters that are updated dynamically to adapt to a data set during evaluation .On - line analysis provides guarantees that these algorithms will perform nearly as well as the best model chosen in hindsight from a large class of models , e.g. , the set of all static mixtures .", "label": "", "metadata": {}, "score": "58.267807"}
{"text": "As an example , the load balancing mechanism may be implemented to reduce the delay in accessing a particular function or a piece of information in the replicated part of the system by directing new requests to a replicated machine operating under a light load .", "label": "", "metadata": {}, "score": "58.289715"}
{"text": "It is found that , for the first two types of association , the words usually co - occur within a narrow range .This is not so for the lexical and semantic associations , the latter in particular exhibiting a large variance of the association range .", "label": "", "metadata": {}, "score": "58.344658"}
{"text": "It is found that , for the first two types of association , the words usually co - occur within a narrow range .This is not so for the lexical and semantic associations , the latter in particular exhibiting a large variance of the association range .", "label": "", "metadata": {}, "score": "58.344658"}
{"text": "The perplexities of the scaled down models were computed using the official ARPA 1994 Language Model Development Set , an ... . \" ...This thesis is about modeling , analyzing , and predicting errorful behavior in large vocabulary continuous speech recognition systems .", "label": "", "metadata": {}, "score": "58.438354"}
{"text": "For many problems , this type of modeling can be viewed as maximum likelihood ( ML ) training for exponential models , and like other ML methods is prone to overfitting of training data .While several s .. a fixed w i\\Gamma1 i\\Gamman+1 , as opposed to counts in the global n - gram distribution .", "label": "", "metadata": {}, "score": "58.48593"}
{"text": "Most of this improvement was noted to be due to correlations of words with themselves ( self - triggers ) .Having selected word pairs with significant correlation , functional forms for the probability distribution F ? ? ? in ( 62)maybepostulated .", "label": "", "metadata": {}, "score": "58.49225"}
{"text": "Most of this improvement was noted to be due to correlations of words with themselves ( self - triggers ) .Having selected word pairs with significant correlation , functional forms for the probability distribution F ? ? ? in ( 62)maybepostulated .", "label": "", "metadata": {}, "score": "58.49225"}
{"text": "However if we know the domain(s ) of application a - priori , we may produce a static model already adapted to the particular task .w?i?jw ? ? ?i ? ? ? ? in accordance with the particular nature of the text .", "label": "", "metadata": {}, "score": "58.625305"}
{"text": "However if we know the domain(s ) of application a - priori , we may produce a static model already adapted to the particular task .w?i?jw ? ? ?i ? ? ? ? in accordance with the particular nature of the text .", "label": "", "metadata": {}, "score": "58.625305"}
{"text": "Using this approach , the joint distribution of all variables is described by only the most systematic variable interactions , thereby limiting the number of parameters to be estimated , supporting computational efficiency , and providing an understanding of the data . .", "label": "", "metadata": {}, "score": "58.721058"}
{"text": "But even researchers using rule - based approaches have found it beneficial to introduce some elements of SLM and statistical estimation [ 2].In information retrieval , a language modeling approach was recently proposed by [ 3 ] , and a statistical / information theoretical approach was developed by [ 4].", "label": "", "metadata": {}, "score": "58.738293"}
{"text": "But even researchers using rule - based approaches have found it beneficial to introduce some elements of SLM and statistical estimation [ 2].In information retrieval , a language modeling approach was recently proposed by [ 3 ] , and a statistical / information theoretical approach was developed by [ 4].", "label": "", "metadata": {}, "score": "58.738293"}
{"text": "But even researchers using rule - based approaches have found it beneficial to introduce some elements of SLM and statistical estimation [ 2].In information retrieval , a language modeling approach was recently proposed by [ 3 ] , and a statistical / information theoretical approach was developed by [ 4].", "label": "", "metadata": {}, "score": "58.738293"}
{"text": "0093 ] One technique for encoding the n gram data is to assign each word a unique integer ID with more common words being assigned lower numbers .This ID assignment happens during the building phase of the language model .Consider the training data from a corpus of documents below : .", "label": "", "metadata": {}, "score": "58.76876"}
{"text": "The systems described above for machine translation , e.g. , systems shown in FIGS . 1 , 2 , 3A , 3B , 4 , 5 and 6 , can be adapted to implement automated processing other than machine translation .Furthermore , large language models can be used to filter emails based on the content in the emails in spam filtering applications .", "label": "", "metadata": {}, "score": "58.782364"}
{"text": "A fundamental obstacle to progress in this direction has to do with the diffusion of gradients through long chains of non - linear transformations , making it difficult to learn long - term dependencies ( Bengio et al 1994 ) in sequential data .", "label": "", "metadata": {}, "score": "58.80377"}
{"text": "Assume now in general that we have N p ?i ? i?f ? ? ? ? ? ? ? ?N p ? ?g. The method of deleted interpolation combines the probability estimates P di ?i j ?N p ? ? ?", "label": "", "metadata": {}, "score": "58.816772"}
{"text": "Assume now in general that we have N p ?i ? i?f ? ? ? ? ? ? ? ?N p ? ?g. The method of deleted interpolation combines the probability estimates P di ?i j ?N p ? ? ?", "label": "", "metadata": {}, "score": "58.816772"}
{"text": "become more refined ( for example , when we move from a bigram N Hincreases and the number of events in each sample ?j ? jmakes it more difficult to estimate event probabilities ? iin the ?j , which corresponds to the relative frequency [ 18 ] : P ML ? ?", "label": "", "metadata": {}, "score": "58.821255"}
{"text": "become more refined ( for example , when we move from a bigram N Hincreases and the number of events in each sample ?j ? jmakes it more difficult to estimate event probabilities ? iin the ?j , which corresponds to the relative frequency [ 18 ] : P ML ? ?", "label": "", "metadata": {}, "score": "58.821255"}
{"text": "Page 10 .Chapter 1 Introduction Research into language modelling aims to develop computational techniques and structures that de- scribe word sequences as produced by human subjects .Such models can be employed to deliver assess- ments of the correctness and plausibility of given samples of text , and have become essential tools in several research fields .", "label": "", "metadata": {}, "score": "58.834343"}
{"text": "Page 10 .Chapter 1 Introduction Research into language modelling aims to develop computational techniques and structures that de- scribe word sequences as produced by human subjects .Such models can be employed to deliver assess- ments of the correctness and plausibility of given samples of text , and have become essential tools in several research fields .", "label": "", "metadata": {}, "score": "58.834343"}
{"text": "N ?X r ? ?C r ?d r ? ?Finally apply equation ( 16 ) and obtain : q ?C ?N ?X r ? ?C r ?d r ( 29 )The probability of unseen events by means of a more general distribution according to Katz 's backing - off approach , which will be de- scribed in section 2.3.4 .", "label": "", "metadata": {}, "score": "58.878662"}
{"text": "N ?X r ? ?C r ?d r ? ?Finally apply equation ( 16 ) and obtain : q ?C ?N ?X r ? ?C r ?d r ( 29 )The probability of unseen events by means of a more general distribution according to Katz 's backing - off approach , which will be de- scribed in section 2.3.4 .", "label": "", "metadata": {}, "score": "58.878662"}
{"text": "In particular , the language model is important for distinguishing between acoustically similar word hypotheses on grounds of their possible linguistic dissimilarity .Conversely , the language model need not discriminate strongly between words that are significantly dissimilar from an acoustic point of view .", "label": "", "metadata": {}, "score": "58.905273"}
{"text": "In particular , the language model is important for distinguishing between acoustically similar word hypotheses on grounds of their possible linguistic dissimilarity .Conversely , the language model need not discriminate strongly between words that are significantly dissimilar from an acoustic point of view .", "label": "", "metadata": {}, "score": "58.905273"}
{"text": "The segment translation server 130 looks up the high - level cache 631 to determine whether all possible n grams are present ( step 940 ) .If so , the segment translation server 130 completes the translation for the segment without sending out the generated requests to the language model servers 210 ( Step 950 ) .", "label": "", "metadata": {}, "score": "58.97489"}
{"text": "[0007 ] In one aspect , a system is described to include computer data servers each storing and operable to serve a partition of a collection of data .The respective partitions together constitute the collection of data and each respective partition is less than the collection of data .", "label": "", "metadata": {}, "score": "59.163994"}
{"text": "Assuming the symbols tohavebeenemittedbythesource , andtheprobabilityofemissionofthis sequencetobe the per - event self information of the sequence z?i ? at discrete time intervals i?f ? ? ? ? ? ? ? ?g from a certain finite set according to z ? ? ?", "label": "", "metadata": {}, "score": "59.207664"}
{"text": "Assuming the symbols tohavebeenemittedbythesource , andtheprobabilityofemissionofthis sequencetobe the per - event self information of the sequence z?i ? at discrete time intervals i?f ? ? ? ? ? ? ? ?g from a certain finite set according to z ? ? ?", "label": "", "metadata": {}, "score": "59.207664"}
{"text": "P ? ? ? will assign a nonzero probability to invalid sequences(for which P ? ? ? ? ? ) , and thus , since 4The property of ergodicity implies that the ensemble averages of a certain desired statistical characteristic ( a mean , for example ) equal the corresponding time averages [ 84].", "label": "", "metadata": {}, "score": "59.253254"}
{"text": "P ? ? ? will assign a nonzero probability to invalid sequences(for which P ? ? ? ? ? ) , and thus , since 4The property of ergodicity implies that the ensemble averages of a certain desired statistical characteristic ( a mean , for example ) equal the corresponding time averages [ 84].", "label": "", "metadata": {}, "score": "59.253254"}
{"text": "Both techniques yield a significant reduction in perplexity over the baseline trigram language model when faced with multi - domain test text , the mixture - based model giving a 24 % reduction and the cache - based model giving a 14 % reduction .", "label": "", "metadata": {}, "score": "59.29347"}
{"text": "2 shows one implementation of the distributed machine translation system 100 in FIG .1 . [ 0021 ] FIG .3A shows a translation cache that can be connected to and shared by translation front ends in a system of the kind shown in FIG .", "label": "", "metadata": {}, "score": "59.304718"}
{"text": "Katz , S.M. ( 1987 )Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer .IEEE Transactions on Acoustics , Speech and Signal Processing 3:400 - 401 .Hinton , G.E. ( 1989 )Connectionist Learning Procedures .", "label": "", "metadata": {}, "score": "59.374023"}
{"text": "w k ? is the number of occurrences of the word w kin the cache of length L. As a matter of Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 42 . , , and choosing the association probabilities to be [ 53 ] : Pw?i?jw?i?j ? ? if otherwise ?", "label": "", "metadata": {}, "score": "59.37886"}
{"text": "w k ? is the number of occurrences of the word w kin the cache of length L. As a matter of Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 42 . , , and choosing the association probabilities to be [ 53 ] : Pw?i?jw?i?j ? ? if otherwise ?", "label": "", "metadata": {}, "score": "59.37886"}
{"text": "By using a fixed class n - gram history and variable word - given - class probabilities we obtain large improvements in the performance of the class - based language model , giving it similar ... \" .This paper describes the use of a weighted mixture of classbased n - gram language models to perform topic adaptation . 1 Introduction Previously , weighted mixtures of word n - gram language models have been used to provide a topic adaptation method for large vocabulary speech recognition [ 3].", "label": "", "metadata": {}, "score": "59.4795"}
{"text": "P?w?i?jw?i ? ? ?i ? q X j ? ?j ?j ?P j ?w?i?jw?i ? ? ?i ? ? ?j ? ?P G ?w?i?jw?i ? ? ?i ? ? ?The division of the corpus into the topic partitions themselves proceeds by agglomerative clustering , assuming each paragraph of text to be concerned with only one particular topic .", "label": "", "metadata": {}, "score": "59.48665"}
{"text": "P?w?i?jw?i ? ? ?i ? q X j ? ?j ?j ?P j ?w?i?jw?i ? ? ?i ? ? ?j ? ?P G ?w?i?jw?i ? ? ?i ? ? ?The division of the corpus into the topic partitions themselves proceeds by agglomerative clustering , assuming each paragraph of text to be concerned with only one particular topic .", "label": "", "metadata": {}, "score": "59.48665"}
{"text": "i ? ? ? ? take on the form of a linear combination of the individual pairwise probabilities P?w?i?j w?j ? ?over all words in the history , i.e. j?f ? ? ? ? ? ? ? ?pg : P ?", "label": "", "metadata": {}, "score": "59.527"}
{"text": "i ? ? ? ? take on the form of a linear combination of the individual pairwise probabilities P?w?i?j w?j ? ?over all words in the history , i.e. j?f ? ? ? ? ? ? ? ?pg : P ?", "label": "", "metadata": {}, "score": "59.527"}
{"text": "A key feature of this approach is that it allows the construction of models which are dependent upon contextual effects occurring across word boundaries .The use of cross word context dependent models presents problems for conventional decoders .The second part of the thesis therefore presents a new decoder design which is capable of using these models efficiently .", "label": "", "metadata": {}, "score": "59.528435"}
{"text": "Letting n increases , the potential number of distinct contexts N Hincreases exponentially , and hence the n the data in each n must in practice be restricted to 2 ( bigrams ) or 3 ( trigrams)14 .Each ? n ? ? ?", "label": "", "metadata": {}, "score": "59.563736"}
{"text": "Letting n increases , the potential number of distinct contexts N Hincreases exponentially , and hence the n the data in each n must in practice be restricted to 2 ( bigrams ) or 3 ( trigrams)14 .Each ? n ? ? ?", "label": "", "metadata": {}, "score": "59.563736"}
{"text": "Consider two populations , general definition of the history equivalence class .Denote the sample we have from each population by ? jand ?k , the first based on a more specific and the second on a more ?Finally , assume a method for calculating the probabilities of seen events to exist .", "label": "", "metadata": {}, "score": "59.597916"}
{"text": "Consider two populations , general definition of the history equivalence class .Denote the sample we have from each population by ? jand ?k , the first based on a more specific and the second on a more ?Finally , assume a method for calculating the probabilities of seen events to exist .", "label": "", "metadata": {}, "score": "59.597916"}
{"text": "When these language models have been applied to speech recognition , however , they have seldom resulted in a corresp ... \" .Adaptive language models have consistently been shown to lead to a significant reduction in language model perplexity compared to the equivalent static trigram model on many data sets .", "label": "", "metadata": {}, "score": "59.625862"}
{"text": "The synonym lists are compiled automatically from the training corpus by identifying the word(s )S wto the category membership definitions .Finally , when we restrict ourselves to deterministic membership , equation ( 57 ) simplifies to : P ?w?i?jw ? ? ?", "label": "", "metadata": {}, "score": "59.65474"}
{"text": "The synonym lists are compiled automatically from the training corpus by identifying the word(s )S wto the category membership definitions .Finally , when we restrict ourselves to deterministic membership , equation ( 57 ) simplifies to : P ?w?i?jw ? ? ?", "label": "", "metadata": {}, "score": "59.65474"}
{"text": "No figures were given for a single model obtained when pooling the training data from the two samples .These constraints include , for instance , unigram and bigram probabilities .The technique was applied to the Switchboard corpus , and performance was 19These reductions are relative to a language model trained exclusively on the general sample .", "label": "", "metadata": {}, "score": "59.656834"}
{"text": "No figures were given for a single model obtained when pooling the training data from the two samples .These constraints include , for instance , unigram and bigram probabilities .The technique was applied to the Switchboard corpus , and performance was 19These reductions are relative to a language model trained exclusively on the general sample .", "label": "", "metadata": {}, "score": "59.656834"}
{"text": "Results for a 70,000 word train information enquiry database , as well as the 1 million word Brown corpus , show perplexities to be increased only slightly by this simplification .Modified absolute discounting In [ 53 ] a variation of the discounting model ( 28 ) is presented .", "label": "", "metadata": {}, "score": "59.710342"}
{"text": "Results for a 70,000 word train information enquiry database , as well as the 1 million word Brown corpus , show perplexities to be increased only slightly by this simplification .Modified absolute discounting In [ 53 ] a variation of the discounting model ( 28 ) is presented .", "label": "", "metadata": {}, "score": "59.710342"}
{"text": "Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 49 .Chapter 3 Variable - length category - based n - grams 3.1 .Introduction and overview In this chapter we develop a language model designed to capture syntactic patterns in English text .", "label": "", "metadata": {}, "score": "59.761917"}
{"text": "Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 49 .Chapter 3 Variable - length category - based n - grams 3.1 .Introduction and overview In this chapter we develop a language model designed to capture syntactic patterns in English text .", "label": "", "metadata": {}, "score": "59.761917"}
{"text": "v ?LL ? ? ?h?w ? ?I m ?v ?v ?The optimal set of category assignments is that which maximises the mutual information component , since the entropy is independent of this choice .The greedy algorithm proposed in [ 8 ] initially assigns each word in the training corpus to its own category , and then at each iteration merges that category pair ( not necessarily adjacent ) which least decreases number of categories has been reached .", "label": "", "metadata": {}, "score": "59.7806"}
{"text": "v ?LL ? ? ?h?w ? ?I m ?v ?v ?The optimal set of category assignments is that which maximises the mutual information component , since the entropy is independent of this choice .The greedy algorithm proposed in [ 8 ] initially assigns each word in the training corpus to its own category , and then at each iteration merges that category pair ( not necessarily adjacent ) which least decreases number of categories has been reached .", "label": "", "metadata": {}, "score": "59.7806"}
{"text": "3B shows a data structure that can be used in the translation cache 310 .One or more source language bins 320 store segments ( S0 , S1 , S2 , etc . ) in the source language and one or more target language bins 330 store translated segments ( T0 , T1 , T2 , etc . ) in the target language .", "label": "", "metadata": {}, "score": "59.783394"}
{"text": "r ?y i ? ?i ?y j ? ?j ? \" , for example , it would be split across two nodes , ? r ?y i ? ?i ? and one with ?r ?", "label": "", "metadata": {}, "score": "59.841278"}
{"text": "r ?y i ? ?i ?y j ? ?j ? \" , for example , it would be split across two nodes , ? r ?y i ? ?i ? and one with ?r ?", "label": "", "metadata": {}, "score": "59.841278"}
{"text": "The novel top layer of hierarchy consists of a mechanism to couple together multiple language models such that they share statistical strength .Intuitively this sharing results in the \" adaptation \" of a latent shared language model to each domain .", "label": "", "metadata": {}, "score": "59.877686"}
{"text": "We investigate an alternative approach to topic mixtures within the n - gram paradigm by using a class - based n- ... . ... rd - based n - gram language model .A disadvantage of this method , however , is that these models require large numbers of parameters per topic , which in turn necessitates a large quantity of training data for each topic and associated ... . \" ...", "label": "", "metadata": {}, "score": "59.88898"}
{"text": "A technique for inferring finite - state transducers is proposed in this article .This technique is based on formalrelations between finite - state transducers and rational grammars .Given a training corpus of source - target pairs of sentences , the proposed approach uses statistical alignment methods to produce a set of conventional strings from which a stochastic rational grammar ( e.g. , an n - gram ) is inferred .", "label": "", "metadata": {}, "score": "59.903687"}
{"text": "i ? the trigram estimate for the j thtopic , and ? lead to undertrained topic models , equation ( 72 ) was linearly interpolated with a general trigram trained on the entire training set .jthe weighting given to the contribution of the j thtopic .", "label": "", "metadata": {}, "score": "59.930504"}
{"text": "i ? the trigram estimate for the j thtopic , and ? lead to undertrained topic models , equation ( 72 ) was linearly interpolated with a general trigram trained on the entire training set .jthe weighting given to the contribution of the j thtopic .", "label": "", "metadata": {}, "score": "59.930504"}
{"text": "i ? ? ?( 46 )For clarity define w c jto be the ?n ?-gram context corresponding to h j , i.e. : w c j ?w?i?n ? ? ?i ? when H?w?i ? ? ?", "label": "", "metadata": {}, "score": "59.988426"}
{"text": "i ? ? ?( 46 )For clarity define w c jto be the ?n ?-gram context corresponding to h j , i.e. : w c j ?w?i?n ? ? ?i ? when H?w?i ? ? ?", "label": "", "metadata": {}, "score": "59.988426"}
{"text": "The client may use hash functions to map between integers and histories , may use the integer as an index into an array of n grams , or use other means of keeping track of the histories .Machine Translation in Adaptive and Scalable Manner .", "label": "", "metadata": {}, "score": "60.06537"}
{"text": "In another aspect , a system for machine translation can include language model servers , a translation model server , and a translation server .Each language model server stores and is operable to serve a partition of a language model for a target language .", "label": "", "metadata": {}, "score": "60.10684"}
{"text": "Each has associated P?w k jl j ? ?k?f ? ? ? ? ? ? ? ?N w ? ?g , reflecting the probability of w?i ? given the equivalence class l j. Note that the number of indicator variables Q may Starting with only the root node ( i.e. from the training corpus .", "label": "", "metadata": {}, "score": "60.12223"}
{"text": "Each has associated P?w k jl j ? ?k?f ? ? ? ? ? ? ? ?N w ? ?g , reflecting the probability of w?i ? given the equivalence class l j. Note that the number of indicator variables Q may Starting with only the root node ( i.e. from the training corpus .", "label": "", "metadata": {}, "score": "60.12223"}
{"text": "[ 0107 ] Each block is given a key that is a string representation of the last n gram stored in the block .The block contents are encoded as the value in an sstable and this key is used as the stable key .", "label": "", "metadata": {}, "score": "60.142773"}
{"text": "Since this language model is not Markov and has a large search space , it is used only in the last stage of a multi - pass search strategy in the recognizer .Second , we introduce topic - dependent dynamic adaptation techniques in the framework of the mixture model .", "label": "", "metadata": {}, "score": "60.143036"}
{"text": "The tree - based model contained 10,000 leaves and exhibited a perplexity of 90.7 .This was a slight improvement in relation to the trigram , which had a perplexity of 94.9 .Although the tree had only 10,015 distinct probability distributions as opposed to the 796,000 of the trigram , the storage required by both was approximately equal since the distributions of the latter generally have only a few nonzero entries while this is not so for the former .", "label": "", "metadata": {}, "score": "60.16475"}
{"text": "The tree - based model contained 10,000 leaves and exhibited a perplexity of 90.7 .This was a slight improvement in relation to the trigram , which had a perplexity of 94.9 .Although the tree had only 10,015 distinct probability distributions as opposed to the 796,000 of the trigram , the storage required by both was approximately equal since the distributions of the latter generally have only a few nonzero entries while this is not so for the former .", "label": "", "metadata": {}, "score": "60.16475"}
{"text": "This paper presents two techniques for language model adaptation .The first is based on the use of mixtures of language models : the training text is partitioned according to topic , a language model is constructed for each component , and at recognition time appropriate weightings are assigned to each ... \" .", "label": "", "metadata": {}, "score": "60.171165"}
{"text": "Since a cache models local variations in text character , it is usually reset at a point when this is known to change , for instance at article boundaries .Once this has occurred it is necessary to wait until a few entries are present in the cache before sensible probability estimates can be expected ( for example , a threshold of 5 is employed in [ 45 ] ) .", "label": "", "metadata": {}, "score": "60.22412"}
{"text": "Since a cache models local variations in text character , it is usually reset at a point when this is known to change , for instance at article boundaries .Once this has occurred it is necessary to wait until a few entries are present in the cache before sensible probability estimates can be expected ( for example , a threshold of 5 is employed in [ 45 ] ) .", "label": "", "metadata": {}, "score": "60.22412"}
{"text": "For a particular leaf corresponding addition of two descendant nodes ( which become the new leaf nodes ) .T?ft o g and L ? ? ) , the tree structure is grown incrementally l j ?t rinvolves the determination and assignment of a suitable new question ? rand the In order to control this process , a criterion indicating how the growth of a terminal node affects the performance of the tree is needed .", "label": "", "metadata": {}, "score": "60.253498"}
{"text": "For a particular leaf corresponding addition of two descendant nodes ( which become the new leaf nodes ) .T?ft o g and L ? ? ) , the tree structure is grown incrementally l j ?t rinvolves the determination and assignment of a suitable new question ? rand the In order to control this process , a criterion indicating how the growth of a terminal node affects the performance of the tree is needed .", "label": "", "metadata": {}, "score": "60.253498"}
{"text": "In this paper , we argue that these problems must be resolved together , and that they must be resolved ... \" .To understand a speaker 's turn of a conversation , one needs to segment it into intonational phrases , clean up any speech repairs that might have occurred , and identify discourse markers .", "label": "", "metadata": {}, "score": "60.259483"}
{"text": "i ? ? ?The probability of the word w?i ? given its category v jis estimated from the relative frequency : P s ?w?i?jv j ?N ?w?i?jv j ?N?v j ?( 68 ) and a cache is maintained for each individual category : P cache?v w j ?", "label": "", "metadata": {}, "score": "60.27986"}
{"text": "i ? ? ?The probability of the word w?i ? given its category v jis estimated from the relative frequency : P s ?w?i?jv j ?N ?w?i?jv j ?N?v j ?( 68 ) and a cache is maintained for each individual category : P cache?v w j ?", "label": "", "metadata": {}, "score": "60.27986"}
{"text": "When the training set is sparse , the assumption ( 21 ) that the expectations may be approximated by the event counts may not be well founded .For example , it may occur that the estimate ( 22 ) requires a division by zero .", "label": "", "metadata": {}, "score": "60.28874"}
{"text": "When the training set is sparse , the assumption ( 21 ) that the expectations may be approximated by the event counts may not be well founded .For example , it may occur that the estimate ( 22 ) requires a division by zero .", "label": "", "metadata": {}, "score": "60.28874"}
{"text": "P?w i jw k ? were w iand w kwithin a window of length p. Bigrams w iand w k. The weights c j p ?The resulting model 2.6.2 .Cache models Languagemodelsusuallyemployprobabilityestimatesthat havebeenchosentoperformwell onaverage over the entire training corpus .", "label": "", "metadata": {}, "score": "60.303066"}
{"text": "P?w i jw k ? were w iand w kwithin a window of length p. Bigrams w iand w k. The weights c j p ?The resulting model 2.6.2 .Cache models Languagemodelsusuallyemployprobabilityestimatesthat havebeenchosentoperformwell onaverage over the entire training corpus .", "label": "", "metadata": {}, "score": "60.303066"}
{"text": "[0105 ] # of entries of length K : varint32 . [ 0106 ] \" 0 \" : byte ( literal 0 marking end of table ) .The above data block is followed by a separate section for each of the different lengths of n grams in the block .", "label": "", "metadata": {}, "score": "60.308163"}
{"text": "However , since parsing is a very computationally intensive operation , it may be impractical to process the entire training corpus .In [ 90 ] a parser is used to tag an initial part of the training corpus , which is then used to initialise a statistical tagger , with which the remaining text is assigned with the most likely category labels .", "label": "", "metadata": {}, "score": "60.345943"}
{"text": "However , since parsing is a very computationally intensive operation , it may be impractical to process the entire training corpus .In [ 90 ] a parser is used to tag an initial part of the training corpus , which is then used to initialise a statistical tagger , with which the remaining text is assigned with the most likely category labels .", "label": "", "metadata": {}, "score": "60.345943"}
{"text": "for some ?i. In order to make this conceptual framework more concrete , consider as an example the word trigram language model , for which details are presented in section 2.4 .In this case the events are the words of the vocabulary , i.e. the probability of word the bigram context .", "label": "", "metadata": {}, "score": "60.364624"}
{"text": "for some ?i. In order to make this conceptual framework more concrete , consider as an example the word trigram language model , for which details are presented in section 2.4 .In this case the events are the words of the vocabulary , i.e. the probability of word the bigram context .", "label": "", "metadata": {}, "score": "60.364624"}
{"text": "y i ? ?r ?( 70 ) where of the known vocabulary .Thus certain set , and returns either a TRUE or a FALSE result .Denote these two possible outcomes by and ? ris a subset of the set of values y imay take on .", "label": "", "metadata": {}, "score": "60.38244"}
{"text": "y i ? ?r ?( 70 ) where of the known vocabulary .Thus certain set , and returns either a TRUE or a FALSE result .Denote these two possible outcomes by and ? ris a subset of the set of values y imay take on .", "label": "", "metadata": {}, "score": "60.38244"}
{"text": "Figure 2 : Architecture of neural net language model introduced in ( Bengio et al 2001 ) .Note that the gradient on most of \\(C\\ ) is zero ( and need not be computed or used ) for most of the columns of \\(C\\ : \\ ) only those corresponding to words in the input subsequence have a non - zero gradient .", "label": "", "metadata": {}, "score": "60.406113"}
{"text": "Baseline word - based n - gram models ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?108 D.3 .The Switchboard ( SWBD ) corpus ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "60.41751"}
{"text": "Baseline word - based n - gram models ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?108 D.3 .The Switchboard ( SWBD ) corpus ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "60.41751"}
{"text": "Finally a discounting estimator which does not fit neatly into either of these categories is described .N ? jis the total number of events in ?j.This estimator will assign a probability of zero to any ?j.But when ? jis sparse , manyeventsin ? jmaybeexpected ?", "label": "", "metadata": {}, "score": "60.45073"}
{"text": "Finally a discounting estimator which does not fit neatly into either of these categories is described .N ? jis the total number of events in ?j.This estimator will assign a probability of zero to any ?j.But when ? jis sparse , manyeventsin ? jmaybeexpected ?", "label": "", "metadata": {}, "score": "60.45073"}
{"text": "j ?D ? ?G ? with the classical Bayes estimate : ? B ay es ?E n ? j ?D ? ?G o ?Z ?P ? ?j ?D ? ?G ? ?", "label": "", "metadata": {}, "score": "60.504425"}
{"text": "j ?D ? ?G ? with the classical Bayes estimate : ? B ay es ?E n ? j ?D ? ?G o ?Z ?P ? ?j ?D ? ?G ? ?", "label": "", "metadata": {}, "score": "60.504425"}
{"text": "Determining trigger - target pairs 5.4.1.First - pass 5.4.2.Second - pass 5.4.3 .Regulating memory usage 5.4.4 .Example pairs ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "60.546104"}
{"text": "Determining trigger - target pairs 5.4.1.First - pass 5.4.2.Second - pass 5.4.3 .Regulating memory usage 5.4.4 .Example pairs ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "60.546104"}
{"text": "[0044 ] In some implementations , a machine translation system based on the distributed machine processing includes machine translation resource servers and at least one translation server .Each machine translation resource server stores and is operable to serve a partition of a collection of machine translation resource data for translation from a source natural language to a target natural language .", "label": "", "metadata": {}, "score": "60.55724"}
{"text": "Other MT systems may also implement these features .[0154 ] First , a wide range of documents requested by users for machine translation are analyzed to determine which translations will be performed manually .This process can include analysis of previously translated documents to determine which sections in the analyzed documents appear frequently , and such frequently used sections can be candidates for manual translation .", "label": "", "metadata": {}, "score": "60.57616"}
{"text": "This approachdoes not constrain the probability estimate to be conditionedon the precedingword , 18The number of words separating the word pair in question .Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 40 .but allows it also to depend on a more informative word occurring further back in the sentence .", "label": "", "metadata": {}, "score": "60.596073"}
{"text": "This approachdoes not constrain the probability estimate to be conditionedon the precedingword , 18The number of words separating the word pair in question .Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 40 .but allows it also to depend on a more informative word occurring further back in the sentence .", "label": "", "metadata": {}, "score": "60.596073"}
{"text": "However , the weights as representing a type of window function over the most recent possible , including rectangular , triangular , Gaussian and Hamming - type functions .Alternatively , as- suming the probabilities the training data .In order to find this maximum , the method of Lagrange multipliers is applied in [ 53].", "label": "", "metadata": {}, "score": "60.620697"}
{"text": "However , the weights as representing a type of window function over the most recent possible , including rectangular , triangular , Gaussian and Hamming - type functions .Alternatively , as- suming the probabilities the training data .In order to find this maximum , the method of Lagrange multipliers is applied in [ 53].", "label": "", "metadata": {}, "score": "60.620697"}
{"text": "An early discussion can also be found in the Parallel Distributed Processing book ( 1986 ) , a landmark of the connectionist approach .For example , with \\(m\\ ) binary features , one can describe up to \\(2^m\\ ) different objects .", "label": "", "metadata": {}, "score": "60.649685"}
{"text": "Since the first significant model was proposed in 1980 , many attempts have been made to improve the state of the art .We review them here ... \" .Statistical Language Models estimate the distribution of various natural language phenomena for the purpose of speech recognition and other language technologies .", "label": "", "metadata": {}, "score": "60.656563"}
{"text": "Since the first significant model was proposed in 1980 , many attempts have been made to improve the state of the art .We review them here ... \" .Statistical Language Models estimate the distribution of various natural language phenomena for the purpose of speech recognition and other language technologies .", "label": "", "metadata": {}, "score": "60.656563"}
{"text": "Since the first significant model was proposed in 1980 , many attempts have been made to improve the state of the art .We review them here ... \" .Statistical Language Models estimate the distribution of various natural language phenomena for the purpose of speech recognition and other language technologies .", "label": "", "metadata": {}, "score": "60.656563"}
{"text": "In particular , the probability of 3A sequence of n consecutive words .Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 13 .a word is calculated using the frequency of the n - tuple , constituted by the preceding ( the utterance and the word itself .", "label": "", "metadata": {}, "score": "60.759995"}
{"text": "In particular , the probability of 3A sequence of n consecutive words .Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 13 .a word is calculated using the frequency of the n - tuple , constituted by the preceding ( the utterance and the word itself .", "label": "", "metadata": {}, "score": "60.759995"}
{"text": "We demonstrate that in some situations , on - line techniques can significantly outperform static mixtures ( by over 10 % in terms of perplexity ) , and are especially effective when the nature of the test data is unknown or changesover time .", "label": "", "metadata": {}, "score": "60.77944"}
{"text": "2 ) may require a number of other back - ends , or servers that provide information it uses in the course of translation .Examples include a translation phrase dictionary server , a language model server , a transliteration server , etc .", "label": "", "metadata": {}, "score": "60.79862"}
{"text": "Consequently the language model will first be viewed in isolation .Let us regard the natural language under study to have been produced by an information source that emits symbols some statistical law .The symbols might be the words of the language themselves , or they might refer to a more general concept , such as syntactic word categories .", "label": "", "metadata": {}, "score": "60.823547"}
{"text": "Consequently the language model will first be viewed in isolation .Let us regard the natural language under study to have been produced by an information source that emits symbols some statistical law .The symbols might be the words of the language themselves , or they might refer to a more general concept , such as syntactic word categories .", "label": "", "metadata": {}, "score": "60.823547"}
{"text": "For the part - of - speech trigram model in [ 45 ] , the addition of a cache with leads to a 14 % drop in perplexity on the LOB corpus .Tests with word - based n - gram models show the addition of a unigram and bigram cache with 87 - 89 corpus [ 24 ] and lead to small ( 2 % relative ) word error rate improvements [ 29].", "label": "", "metadata": {}, "score": "60.848534"}
{"text": "For the part - of - speech trigram model in [ 45 ] , the addition of a cache with leads to a 14 % drop in perplexity on the LOB corpus .Tests with word - based n - gram models show the addition of a unigram and bigram cache with 87 - 89 corpus [ 24 ] and lead to small ( 2 % relative ) word error rate improvements [ 29].", "label": "", "metadata": {}, "score": "60.848534"}
{"text": "This use of the translation cache frees up the system resources , reduces traffic between the front and back ends of the MT system , and can be beneficial to MT systems serving a large volume of translation requests , such as a MT system for a popular website .", "label": "", "metadata": {}, "score": "60.926937"}
{"text": "Similar observations have been made in [ 69 ] , where the category assignments were made by hand according to syntactic and semantic word functions .The following arguments account for this behaviour .Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .", "label": "", "metadata": {}, "score": "60.97044"}
{"text": "Similar observations have been made in [ 69 ] , where the category assignments were made by hand according to syntactic and semantic word functions .The following arguments account for this behaviour .Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .", "label": "", "metadata": {}, "score": "60.97044"}
{"text": "Page 27 . and then applying ( 16 ) we find : a ?N ?( 31 )This solution has been shown to optimise the leaving - one - out probability [ 55 ] , and has been employed in the speech recognition system described in [ 36].", "label": "", "metadata": {}, "score": "61.04909"}
{"text": "Page 27 . and then applying ( 16 ) we find : a ?N ?( 31 )This solution has been shown to optimise the leaving - one - out probability [ 55 ] , and has been employed in the speech recognition system described in [ 36].", "label": "", "metadata": {}, "score": "61.04909"}
{"text": "Specialisation to a target domain 2.7.2 .Mixtures of topic - specific language models ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "61.06349"}
{"text": "Specialisation to a target domain 2.7.2 .Mixtures of topic - specific language models ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "61.06349"}
{"text": "The content of a segment may vary in different implementations and may range from a few words to multiple sentences .Each translated segment is then sent back to the original requesting translation front end 110 via the load balancer 120 .", "label": "", "metadata": {}, "score": "61.072105"}
{"text": "We show that the predictive power of the N - gram language models can be improved by using long - term context information about the topic of discussion .We use information retrieval techniques to generalize the available context information for topic - dependent language modeling .", "label": "", "metadata": {}, "score": "61.072807"}
{"text": "Note that , in the formalism introduced here , bigrams and trigrams ( for example ) differ since they represent different measurements .n , which is used as basis for an n - gram language Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .", "label": "", "metadata": {}, "score": "61.101364"}
{"text": "Note that , in the formalism introduced here , bigrams and trigrams ( for example ) differ since they represent different measurements .n , which is used as basis for an n - gram language Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .", "label": "", "metadata": {}, "score": "61.101364"}
{"text": "The resulting discrete - time sequence of observation vectors is the output of the front - end , and is passed to the recognition algorithm .The acoustic model Central toeveryspeechrecognitionsystemis ameansof encodingthesoundscomprisinghumanspeech .This has been most successfullyachievedthrough the use of hidden Markov models ( HMMs )", "label": "", "metadata": {}, "score": "61.12078"}
{"text": "The resulting discrete - time sequence of observation vectors is the output of the front - end , and is passed to the recognition algorithm .The acoustic model Central toeveryspeechrecognitionsystemis ameansof encodingthesoundscomprisinghumanspeech .This has been most successfullyachievedthrough the use of hidden Markov models ( HMMs )", "label": "", "metadata": {}, "score": "61.12078"}
{"text": "N ?X r ? ? r?C r ?C ?N ?so that , applying ( 16 ) we obtain the result : C ? q ?C ?N ?( 24 ) where the quantity probability corresponding to any one particular unseen event .", "label": "", "metadata": {}, "score": "61.18918"}
{"text": "N ?X r ? ? r?C r ?C ?N ?so that , applying ( 16 ) we obtain the result : C ? q ?C ?N ?( 24 ) where the quantity probability corresponding to any one particular unseen event .", "label": "", "metadata": {}, "score": "61.18918"}
{"text": "involves summing over all history equivalence classes that correspond to : Pw?i?jw ? ? ?i ?h?h?H?w?i ? ?Pw?i?jh?Phjw ? ? ?i ?( 11 ) where lence class P?hjw ? ? ?i ? gives the probability that the word history w ? ? ?", "label": "", "metadata": {}, "score": "61.19986"}
{"text": "involves summing over all history equivalence classes that correspond to : Pw?i?jw ? ? ?i ?h?h?H?w?i ? ?Pw?i?jh?Phjw ? ? ?i ?( 11 ) where lence class P?hjw ? ? ?i ? gives the probability that the word history w ? ? ?", "label": "", "metadata": {}, "score": "61.19986"}
{"text": "Structure of the language model Drawing on the exposition of section 2.5.1 , denote the N vdifferent part - of - speech categories by : V ? n v ?v ?v N v ? ?o Becauseawordmayhavemultiplegrammaticalfunctions , the mappingoperator hence in section 2.2 , we now define a history equivalenceclass to be an n - gram of categories .", "label": "", "metadata": {}, "score": "61.264835"}
{"text": "Structure of the language model Drawing on the exposition of section 2.5.1 , denote the N vdifferent part - of - speech categories by : V ? n v ?v ?v N v ? ?o Becauseawordmayhavemultiplegrammaticalfunctions , the mappingoperator hence in section 2.2 , we now define a history equivalenceclass to be an n - gram of categories .", "label": "", "metadata": {}, "score": "61.264835"}
{"text": "i ?P ? w?i?jv?i ?( 52 )For stochastic category membership , this allows us to decompose the conditional probability estimates in the following way : P ?w?i?jw ? ? ?i ?X ?v?v?V?w?i ? ?P ?", "label": "", "metadata": {}, "score": "61.279896"}
{"text": "i ?P ? w?i?jv?i ?( 52 )For stochastic category membership , this allows us to decompose the conditional probability estimates in the following way : P ?w?i?jw ? ? ?i ?X ?v?v?V?w?i ? ?P ?", "label": "", "metadata": {}, "score": "61.279896"}
{"text": "n h ?h ?h N H ? ?o Let the length of the category n - gram associatedwith the particular history equivalenceclass h ibe given 20This work employs the definitions used for the LOB corpus , a listing of which appears in appendix E. Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .", "label": "", "metadata": {}, "score": "61.30425"}
{"text": "n h ?h ?h N H ? ?o Let the length of the category n - gram associatedwith the particular history equivalenceclass h ibe given 20This work employs the definitions used for the LOB corpus , a listing of which appears in appendix E. Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .", "label": "", "metadata": {}, "score": "61.30425"}
{"text": "In particular , the interaction with the acoustic probabilities is not considered , and therefore the perplexity measure does not account for the varying acoustic difficulty of distinguishing words .An improved quality measure that addresses this aspect is proposed in [ 38 ] , but has the disadvantageof becoming specific to the nature of the particular acoustic component used in the speech recognition system .", "label": "", "metadata": {}, "score": "61.308517"}
{"text": "In particular , the interaction with the acoustic probabilities is not considered , and therefore the perplexity measure does not account for the varying acoustic difficulty of distinguishing words .An improved quality measure that addresses this aspect is proposed in [ 38 ] , but has the disadvantageof becoming specific to the nature of the particular acoustic component used in the speech recognition system .", "label": "", "metadata": {}, "score": "61.308517"}
{"text": "K ? ? ? is ergodic , it can be shown that ? t ? ?e. Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 18 .the probabilities must sum to unity , i.e. : ?", "label": "", "metadata": {}, "score": "61.395863"}
{"text": "K ? ? ? is ergodic , it can be shown that ? t ? ?e. Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 18 .the probabilities must sum to unity , i.e. : ?", "label": "", "metadata": {}, "score": "61.395863"}
{"text": "Methods are presented by means of which related word pairs may be identified from a large corpus , as well as techniques allowing the parameters of the functional dependence to be estimated .Thesis organisation Chapter 2 presents a detailed exposition of the statistical language modelling field .", "label": "", "metadata": {}, "score": "61.4022"}
{"text": "Methods are presented by means of which related word pairs may be identified from a large corpus , as well as techniques allowing the parameters of the functional dependence to be estimated .Thesis organisation Chapter 2 presents a detailed exposition of the statistical language modelling field .", "label": "", "metadata": {}, "score": "61.4022"}
{"text": "[ 0101 ] # of entries in lexicon : varint32 .[ 0102 ] N word ids ( lexicon ) : N values .[ 0103 ] # entries table : pairs repeated each distinct n gram length in the block .", "label": "", "metadata": {}, "score": "61.449955"}
{"text": "H?w?i ? ? ?h j o ( 47 ) while bearing in mind that all corresponding to each such context it in the language of interest , while a subset followed the context in the training corpus . jcontinue to have the same sample space j?j .", "label": "", "metadata": {}, "score": "61.5191"}
{"text": "H?w?i ? ? ?h j o ( 47 ) while bearing in mind that all corresponding to each such context it in the language of interest , while a subset followed the context in the training corpus . jcontinue to have the same sample space j?j .", "label": "", "metadata": {}, "score": "61.5191"}
{"text": "Language models play an important role in improving the accuracy of a continuous speech recognizer .In this thesis , we introduce a new statistical language model which captures long term topic dependencies of words within and across sentences .The model includes two main contributions .", "label": "", "metadata": {}, "score": "61.549004"}
{"text": "Language models play an important role in improving the accuracy of a continuous speech recognizer .In this thesis , we introduce a new statistical language model which captures long term topic dependencies of words within and across sentences .The model includes two main contributions .", "label": "", "metadata": {}, "score": "61.549004"}
{"text": "L ? ? ? ? ? to improve the perplexity by 17 % on the WSJ Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 43 .Stochastic decision tree based language models As a fundamental alternative to the n - gram history equivalenceclass defined in equation ( 46 ) , a stochas- tic decision tree has been used in [ 3 ] to perform the many - to - one mapping 2.2 .", "label": "", "metadata": {}, "score": "61.61914"}
{"text": "L ? ? ? ? ? to improve the perplexity by 17 % on the WSJ Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 43 .Stochastic decision tree based language models As a fundamental alternative to the n - gram history equivalenceclass defined in equation ( 46 ) , a stochas- tic decision tree has been used in [ 3 ] to perform the many - to - one mapping 2.2 .", "label": "", "metadata": {}, "score": "61.61914"}
{"text": "Particular attention is given to the range18and sequential order of the association .w b ?L W ? the number of times w bfollows w ain the corpus L W(typically 5words ) , and N?w a ? is thenumberofobservationsof w ainthe ?", "label": "", "metadata": {}, "score": "61.63582"}
{"text": "Particular attention is given to the range18and sequential order of the association .w b ?L W ? the number of times w bfollows w ain the corpus L W(typically 5words ) , and N?w a ? is thenumberofobservationsof w ainthe ?", "label": "", "metadata": {}, "score": "61.63582"}
{"text": "Word groups and phrases In order to improve the modelling of frequent word groups , it has been proposed that the language model vocabulary not be restricted to single words only , but that it be allowed also to contain frequently occurring phrases [ 38].", "label": "", "metadata": {}, "score": "61.66983"}
{"text": "Word groups and phrases In order to improve the modelling of frequent word groups , it has been proposed that the language model vocabulary not be restricted to single words only , but that it be allowed also to contain frequently occurring phrases [ 38].", "label": "", "metadata": {}, "score": "61.66983"}
{"text": "Taking the unigram cache as an example [ 45 ] : ? cacheare typically chosen to optimise performance on a development test P cacheis P cache ?w k jw ? ? ?i ? ? ?N cache ?w k ?", "label": "", "metadata": {}, "score": "61.671154"}
{"text": "Taking the unigram cache as an example [ 45 ] : ? cacheare typically chosen to optimise performance on a development test P cacheis P cache ?w k jw ? ? ?i ? ? ?N cache ?w k ?", "label": "", "metadata": {}, "score": "61.671154"}
{"text": "For example , a translation system may have multiple phrase based translation lexica trained on different data sources .These multiple models are served from a single partitioned virtual server and re - partitioned on the client side of the translation engine .", "label": "", "metadata": {}, "score": "61.67624"}
{"text": "The described automated machine processing techniques and systems can also be implemented with sufficient processing capacity to respond to a large volume of requests for automated processing .In these and other implementations of automated processing , a distributed design can be used to implement the system resources or the system processing capacity .", "label": "", "metadata": {}, "score": "61.796314"}
{"text": "j ? ?N ? ?i j ?j ?N ?j where eventthathasnotbeenseenin thesample not to occur in of probabilities for such unseen events .The following sections review the Good - Turing , deleted and discounted estimation techniques , each of which address the estimation of marginal probabilities from sparsedataandgiveparticularattentionto the computationof theunseeneventprobability .", "label": "", "metadata": {}, "score": "61.850197"}
{"text": "j ? ?N ? ?i j ?j ?N ?j where eventthathasnotbeenseenin thesample not to occur in of probabilities for such unseen events .The following sections review the Good - Turing , deleted and discounted estimation techniques , each of which address the estimation of marginal probabilities from sparsedataandgiveparticularattentionto the computationof theunseeneventprobability .", "label": "", "metadata": {}, "score": "61.850197"}
{"text": "We describe a mixture - model approach to adapting a Statistical Machine Translation System for new domains , using weights that depend on text distances to mixture components .We investigate a number of variants on this approach , including cross - domain versus dynamic adaptation ; linear versus loglinea ... \" .", "label": "", "metadata": {}, "score": "61.859688"}
{"text": "3A , 3B and 3C show features of an example implementation of a translation cache .The translation cache 310 stores translations of selected segments and may also store translations of tokens and strings of tokens such as phrases that are smaller than segments but bigger than tokens .", "label": "", "metadata": {}, "score": "61.89106"}
{"text": "In this work , they are captured via a paradigm first formulated in the context of information retrieval , called latent semantic analysis ( LSA ) .This paradigm seeks to automatically uncover the salient semantic relationships between words and documents in a given corpus .", "label": "", "metadata": {}, "score": "61.92724"}
{"text": "The shortest sequence is \" E \" if nothing longer can be found .In order to do the search efficiently , the n grams can be partitioned by their last word .In the above example , all n grams ending in \" E \" can be grouped into the same partition and stored in the same machine .", "label": "", "metadata": {}, "score": "61.948303"}
{"text": "This thesis focuses on the use of linguistically defined word categories as a means of improving the performance of statistical language models .In particular , an approach that aims to capture both general grammatical patterns , as well as particular word dependencies , using different model components is proposed , developed and evaluated .", "label": "", "metadata": {}, "score": "61.94905"}
{"text": "This thesis focuses on the use of linguistically defined word categories as a means of improving the performance of statistical language models .In particular , an approach that aims to capture both general grammatical patterns , as well as particular word dependencies , using different model components is proposed , developed and evaluated .", "label": "", "metadata": {}, "score": "61.94905"}
{"text": "( 43 ) where the discounting function and d ? ?X i j ?j ? is in general different for every event ?i , and the samples ?j ? kare related as in section 2.3.4 .Now from : N ?", "label": "", "metadata": {}, "score": "61.972763"}
{"text": "( 43 ) where the discounting function and d ? ?X i j ?j ? is in general different for every event ?i , and the samples ?j ? kare related as in section 2.3.4 .Now from : N ?", "label": "", "metadata": {}, "score": "61.972763"}
{"text": "Testing n - gram counts 4.4.2 .Testing the effect on the overall probability W T ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "61.990765"}
{"text": "Testing n - gram counts 4.4.2 .Testing the effect on the overall probability W T ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "61.990765"}
{"text": "All 4- and 5-grams ending in \" D E \" can be removed ( e.g. , the n gram \" A B C D E \" ) , and only the trigrams ( e.g. , \" C D E \" ) are kept .", "label": "", "metadata": {}, "score": "62.034264"}
{"text": "For example , the same breaking news may be available in several languages and the system can use the news reports in the target language to obtain a translation .Also , companies and organizations may make the same information available in several languages on line and such on - line documents can be searched by the system to obtain an existing translation .", "label": "", "metadata": {}, "score": "62.07236"}
{"text": "The language model scoring information can also include information other than statistical data .The SMT decoder computes statistics on all possible translations from various arrangements of tokens in the target language and searches for the best translation .The respective segment translation server 130 sends the translation output from the SMT decoder to the originating translation front end server 110 through the load balancer 120 .", "label": "", "metadata": {}, "score": "62.08152"}
{"text": "Page 45 .Domain adaptation Adaptivity in a language model concerns its capacity to alter the probability estimate P distinguished by attributes such as , for example , the topic of discussion , style of writing , and the time of writing .", "label": "", "metadata": {}, "score": "62.09906"}
{"text": "Page 45 .Domain adaptation Adaptivity in a language model concerns its capacity to alter the probability estimate P distinguished by attributes such as , for example , the topic of discussion , style of writing , and the time of writing .", "label": "", "metadata": {}, "score": "62.09906"}
{"text": "Therefore , among other beneficial features , partitions can be used to handle high load and allow for scalability and reliability .The scale and other features of a partition will vary depending on the requirements and restraints in a particular automated processing system .", "label": "", "metadata": {}, "score": "62.14216"}
{"text": "Hence , partition provides an effective approach to high - quality MT systems using the distributed machine processing .Replication and load balancing can also be used in such DMT systems and other MT systems based on large language and translation models .", "label": "", "metadata": {}, "score": "62.202057"}
{"text": "Langlais ( 2002 ) introduced the concept of domain adaptation in SMT by integrating domain - specific lexicons in the t ..Sign up to receive free email alerts when patent applications with chosen keywords are published SIGN UP .Abstract : .", "label": "", "metadata": {}, "score": "62.2237"}
{"text": "Each word corresponds to a point in a feature space .One can imagine that each dimension of that space corresponds to a semantic or grammatical characteristic of words .The hope is that functionally similar words get to be closer to each other in that space , at least along some directions .", "label": "", "metadata": {}, "score": "62.233322"}
{"text": "Each respective partition server can include all n - grams in the language model satisfying a partitioning criterion .For example , each respective partition server can store and serve a partition that includes all n - grams in the language model having a common token in a predetermined position .", "label": "", "metadata": {}, "score": "62.279007"}
{"text": "N cache?v j ?w k ?L j ( 69 ) where number of occurrences of the word P cache?v j ?w k ? is the probability estimate from the cache for category v j , N cache?v j ?", "label": "", "metadata": {}, "score": "62.301575"}
{"text": "N cache?v j ?w k ?L j ( 69 ) where number of occurrences of the word P cache?v j ?w k ? is the probability estimate from the cache for category v j , N cache?v j ?", "label": "", "metadata": {}, "score": "62.301575"}
{"text": "More importantly , this project also investigates alternative ways of excluding bigrams and trigrams from a backoff language model , using criteria other than the number of times an n - gram occurs in the training text .Specifically , a difference method has been investigated where the difference in the logs of the original and backed off trigram and bigram probabilities is used as a basis for n - gram exclusion from ... .", "label": "", "metadata": {}, "score": "62.35389"}
{"text": "Considering the intrinsic defect of the POS and word form , some other information with different granularity has been investigated .One of them is word categories , which have been used to improve the performance of statistical language models [ 4 ] .", "label": "", "metadata": {}, "score": "62.37349"}
{"text": "Considering the intrinsic defect of the POS and word form , some other information with different granularity has been investigated .One of them is word categories , which have been used to improve the performance of statistical language models [ 4 ] .", "label": "", "metadata": {}, "score": "62.37349"}
{"text": "Rare sequences , with correspondingly low associated probabilities , Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 17 .K ?z ? ? ?K ? where the summation is takenoverall possibleeventsequencesof length source .Entropy is an average measure of the amount of information contained in the set of sequences the source is capable of producing .", "label": "", "metadata": {}, "score": "62.39486"}
{"text": "Rare sequences , with correspondingly low associated probabilities , Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 17 .K ?z ? ? ?K ? where the summation is takenoverall possibleeventsequencesof length source .Entropy is an average measure of the amount of information contained in the set of sequences the source is capable of producing .", "label": "", "metadata": {}, "score": "62.39486"}
{"text": "X ?i ?N ? ?i j ?j ? ? ?P ? i j ?j ?( 35 ) Katz distributes this probability mass among the unseen events according to the lower - order probability P ? ?", "label": "", "metadata": {}, "score": "62.40779"}
{"text": "X ?i ?N ? ?i j ?j ? ? ?P ? i j ?j ?( 35 ) Katz distributes this probability mass among the unseen events according to the lower - order probability P ? ?", "label": "", "metadata": {}, "score": "62.40779"}
{"text": "84 Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 8 .Word error rate performance85 6.1 .Language models in the recognition search ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "62.522354"}
{"text": "84 Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 8 .Word error rate performance85 6.1 .Language models in the recognition search ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "62.522354"}
{"text": "[ 0142 ] The Wait ( ) --calls in the translation servers can be used to achieve synchronization of requests to different servers for translating the same segment .One method to reduce the wait time is to interleave different iterations of language model requests .", "label": "", "metadata": {}, "score": "62.52249"}
{"text": "This thesis is about modeling , analyzing , and predicting errorful behavior in large vocabulary continuous speech recognition systems .Because today 's state - of - the - art recognizers are not designed to be situated naturally in an error feedback loop , they are ill - positioned for inclusion in multi - modal interfaces , multi - media databases , and other interesting applications .", "label": "", "metadata": {}, "score": "62.54274"}
{"text": "This process takes time and uses resources in the system .For segments that are frequently translated , the respective translations may be stored in a memory unit such as a translation cache accessible by the translation front ends 110 so that the routine process for translating such a segment in the back end of the system can be avoided .", "label": "", "metadata": {}, "score": "62.61732"}
{"text": "D individually for each ? , and determining the optimal values by iterative adjustment 2 .Modelling in a similar fashion to that used above .D to be the same for all ? , thus pooling their counts , and determining the optimal value 3 .", "label": "", "metadata": {}, "score": "62.662437"}
{"text": "D individually for each ? , and determining the optimal values by iterative adjustment 2 .Modelling in a similar fashion to that used above .D to be the same for all ? , thus pooling their counts , and determining the optimal value 3 .", "label": "", "metadata": {}, "score": "62.662437"}
{"text": "The proposed combination of the three modelling approaches is shown to lead to considerable perplexity reductions , especially for sparse training sets .Incorporation of the models has lead to a significant improvement in the word error rate of a high - performance baseline speech - recognition system .", "label": "", "metadata": {}, "score": "62.689354"}
{"text": "The proposed combination of the three modelling approaches is shown to lead to considerable perplexity reductions , especially for sparse training sets .Incorporation of the models has lead to a significant improvement in the word error rate of a high - performance baseline speech - recognition system .", "label": "", "metadata": {}, "score": "62.689354"}
{"text": "( 1 ) but since P?x ? is constant for a given acoustic signal , an equivalent strategy is to find : arg max ?w n P?xjw ? ?P?w ? o ( 2 )The acoustic component of the speech recogniser must compute must estimate the prior probability of a certain sequence of words , on the latter , we will present some background on the acoustic processing . P?xjw ? , whereas the language model P?w ?", "label": "", "metadata": {}, "score": "62.84948"}
{"text": "( 1 ) but since P?x ? is constant for a given acoustic signal , an equivalent strategy is to find : arg max ?w n P?xjw ? ?P?w ? o ( 2 )The acoustic component of the speech recogniser must compute must estimate the prior probability of a certain sequence of words , on the latter , we will present some background on the acoustic processing . P?xjw ? , whereas the language model P?w ?", "label": "", "metadata": {}, "score": "62.84948"}
{"text": "( 40 )Note that , if there are events unseen to both employ a backoff to yet more general distribution , say distribution to which ? jand ?k , the probability estimate P ? ?i j ? k ? must itself P ? ?", "label": "", "metadata": {}, "score": "62.85951"}
{"text": "( 40 )Note that , if there are events unseen to both employ a backoff to yet more general distribution , say distribution to which ? jand ?k , the probability estimate P ? ?i j ? k ? must itself P ? ?", "label": "", "metadata": {}, "score": "62.85951"}
{"text": "[0075 ]FIG .7 illustrates one exemplary operation of the translation decoder 610 of the segment translation server 130 in FIG .6 in translating a segment 701 .Initially , the decoder requests and retrieves from TM servers the TM data associated with the translations of the segment to be translated , e.g. , all possible translations of phrases or extensions in the target language ( step 710 ) .", "label": "", "metadata": {}, "score": "62.963036"}
{"text": "Most of the translation models are word - based .While the approach has achieved surprisingly good performance comparable to the best commercial systems , many questions remain in the machine translation community .Can the statistical word - based translation still perform well on language pairs with radically different linguistic structures ?", "label": "", "metadata": {}, "score": "62.98372"}
{"text": "b N ?X r ? ?C r ?M ?b N ? and applying equation ( 24 ) it follows that : b ?C ?M ?( 33 )As opposed to the Good - Turing estimator ( 22 ) , this estimate is robust to the sample counts , since C ?", "label": "", "metadata": {}, "score": "62.98671"}
{"text": "b N ?X r ? ?C r ?M ?b N ? and applying equation ( 24 ) it follows that : b ?C ?M ?( 33 )As opposed to the Good - Turing estimator ( 22 ) , this estimate is robust to the sample counts , since C ?", "label": "", "metadata": {}, "score": "62.98671"}
{"text": "By restricting ourselves to word pairs , the combinational n is avoided .However , since we are no longer limiting N ?w , where N wis the vocabulary size [ 47].For this reason it is necessary to filter the set of 16No experimental results treating the incorporation of such pairs into a language model have been given in either source , however .", "label": "", "metadata": {}, "score": "62.99489"}
{"text": "By restricting ourselves to word pairs , the combinational n is avoided .However , since we are no longer limiting N ?w , where N wis the vocabulary size [ 47].For this reason it is necessary to filter the set of 16No experimental results treating the incorporation of such pairs into a language model have been given in either source , however .", "label": "", "metadata": {}, "score": "62.99489"}
{"text": "In this work , we compare on - line algorithms from machine learning to existing algorithms for combining language models .On - line algorithms developed for this problem have parameters that a ... \" .Multiple language models are combined for many tasks in language modeling , such as domain and topic adaptation .", "label": "", "metadata": {}, "score": "62.997025"}
{"text": "Compound : multiple - word expressions referring to single concepts , e.g. \" Computer scientist \" .Fixed : the words of interest are separated by a fixed number of words , e.g. \" bread and butter \" .Lexical : the words are related by syntactic factors .", "label": "", "metadata": {}, "score": "62.99716"}
{"text": "Compound : multiple - word expressions referring to single concepts , e.g. \" Computer scientist \" .Fixed : the words of interest are separated by a fixed number of words , e.g. \" bread and butter \" .Lexical : the words are related by syntactic factors .", "label": "", "metadata": {}, "score": "62.99716"}
{"text": "In each iteration , the decoder performs a two - pass processing as follows .First all possible extensions of the current set of candidate translations are computed and possible extensions translation model scores to the possible extensions ( step 730 ) .", "label": "", "metadata": {}, "score": "62.99795"}
{"text": "Previous approaches for estimating language models from multi - domain data have not accounted for the characteristic variations of style and content across domains .In contrast , this thesis introduces two approaches that compensate for multi - domain differences , both representing \" style \" by part - of - speech ( POS ) sequences and \" content \" by the particular choice of words .", "label": "", "metadata": {}, "score": "63.04928"}
{"text": "In some of the lower - quality translation modes , the translation server can skip using some of these back end resources to reduce the translation time at a price of a reduced translation quality .[ 0146 ] For example , the translation server can skip using the transliteration resource and handle words that would otherwise be transliterated in another way , e.g. , omitting the words , or keeping the original words in the translation .", "label": "", "metadata": {}, "score": "63.05859"}
{"text": "C ?C ?C ?( 34 ) Empirical tests using n - gram language models show absolute discounting to give a 10 % perplexity reduction over linear discounting for both the LOB as well as a 100,000-word German corpus [ 53].", "label": "", "metadata": {}, "score": "63.19548"}
{"text": "C ?C ?C ?( 34 ) Empirical tests using n - gram language models show absolute discounting to give a 10 % perplexity reduction over linear discounting for both the LOB as well as a 100,000-word German corpus [ 53].", "label": "", "metadata": {}, "score": "63.19548"}
{"text": "An example of ( K+1 ) partitions are shown in FIG .2 , where K is an integer .The respective partition servers together constitute the entire translation model , and each respective partition is less than the whole of the translation model .", "label": "", "metadata": {}, "score": "63.21691"}
{"text": "( 57 ) Equation ( 57 ) has been used in the synonym based language model proposed in [ 38 ] , which is very similar to the part - of - speech approachexcept in that the categories need not have strict grammatical def- initions .", "label": "", "metadata": {}, "score": "63.251297"}
{"text": "( 57 ) Equation ( 57 ) has been used in the synonym based language model proposed in [ 38 ] , which is very similar to the part - of - speech approachexcept in that the categories need not have strict grammatical def- initions .", "label": "", "metadata": {}, "score": "63.251297"}
{"text": "For example , servers storing different language models may also be managed by a load balancing mechanism .For another example , several processing servers , such as machine translation servers , may operate based on different language translation resources using the same machine translation scheme , e.g. , all are statistical machine translation ( SMT ) servers .", "label": "", "metadata": {}, "score": "63.44253"}
{"text": "( 7 )Thus the perplexity of a passage of text for a given language model may be calculated by substituting the conditional probabilities computed by the model into the right hand side of equation ( 7 ) .Apart from the constant ?", "label": "", "metadata": {}, "score": "63.477013"}
{"text": "( 7 )Thus the perplexity of a passage of text for a given language model may be calculated by substituting the conditional probabilities computed by the model into the right hand side of equation ( 7 ) .Apart from the constant ?", "label": "", "metadata": {}, "score": "63.477013"}
{"text": "The conditions under which this may occur are governed by a control parameter which follows an annealing schedule such that moves leading to perplexity in- creases become increasingly unlikely with time .Monte Carlo minimisations were carried out with and without simulated annealing , and the 11 % lower perplexities achieved in the former case demonstrate the existence of locally optimal category assignments .", "label": "", "metadata": {}, "score": "63.482224"}
{"text": "The conditions under which this may occur are governed by a control parameter which follows an annealing schedule such that moves leading to perplexity in- creases become increasingly unlikely with time .Monte Carlo minimisations were carried out with and without simulated annealing , and the 11 % lower perplexities achieved in the former case demonstrate the existence of locally optimal category assignments .", "label": "", "metadata": {}, "score": "63.482224"}
{"text": "The linear discounted estimate ( 31 ) is employed in [ 41 ] , while nonlinear discounting is advocated in [ 1].C rcounts , and in practice the 2.3.5 .Deleted interpolation The backing - off approach is a means of combining probability estimates from two different populations ? and such populations , each of differing generality .", "label": "", "metadata": {}, "score": "63.521767"}
{"text": "The linear discounted estimate ( 31 ) is employed in [ 41 ] , while nonlinear discounting is advocated in [ 1].C rcounts , and in practice the 2.3.5 .Deleted interpolation The backing - off approach is a means of combining probability estimates from two different populations ? and such populations , each of differing generality .", "label": "", "metadata": {}, "score": "63.521767"}
{"text": "C ? is in fact typically distributed among the possible unseen events 2.3.3.1 .Linear discounting The linear discounting method chooses the discounting factor in equation ( 28 ) to be proportional to r : d r ? a?r with ? ?", "label": "", "metadata": {}, "score": "63.557766"}
{"text": "C ? is in fact typically distributed among the possible unseen events 2.3.3.1 .Linear discounting The linear discounting method chooses the discounting factor in equation ( 28 ) to be proportional to r : d r ? a?r with ? ?", "label": "", "metadata": {}, "score": "63.557766"}
{"text": "Furthermore , utterances that are clearly not grammatical occur often in natural language , but can not be dealt with by such analyses .This significant likelihood of failure under such naturally occurring cir- cumstances has led to infrequent use of the rule - based approach in speech recognition systems .", "label": "", "metadata": {}, "score": "63.591682"}
{"text": "Furthermore , utterances that are clearly not grammatical occur often in natural language , but can not be dealt with by such analyses .This significant likelihood of failure under such naturally occurring cir- cumstances has led to infrequent use of the rule - based approach in speech recognition systems .", "label": "", "metadata": {}, "score": "63.591682"}
{"text": "DETAILED DESCRIPTION .[ 0035 ] Automated machine processing techniques and systems described in this specification can be implemented to operate with a large volume of resources to improve the processing performance , e.g. , the quality and speed of the processing .", "label": "", "metadata": {}, "score": "63.66091"}
{"text": "The processor and the memory can be supplemented by , or incorporated in , special purpose logic circuitry .[0167 ] The components of a computing system can be interconnected by any form or medium of digital data communication , e.g. , a communication network .", "label": "", "metadata": {}, "score": "63.66167"}
{"text": "Let j?j may be defined as N ? , and each individual event denoted by ? iwith i?f ? ? ? ? ? ? ? ?N ?g. 6By way of illustration , these outcomes will often correspond to the words of the vocabulary .", "label": "", "metadata": {}, "score": "63.72196"}
{"text": "Let j?j may be defined as N ? , and each individual event denoted by ? iwith i?f ? ? ? ? ? ? ? ?N ?g. 6By way of illustration , these outcomes will often correspond to the words of the vocabulary .", "label": "", "metadata": {}, "score": "63.72196"}
{"text": "Other statistical data may also be used as part of the scoring information .The language model includes a collection of possible language strings in the target language and corresponding language model scoring information for each string .A string includes one or more language tokens .", "label": "", "metadata": {}, "score": "63.73976"}
{"text": "The decoder communicates with language model servers 210 to request for information on each of n grams for possible translations of the segment and associated statistical data .[0067 ] FIG .6 shows an example of a segment translation server 130 having a translation decoder 610 , a LM lookup request queue 620 , and a local cache 630 .", "label": "", "metadata": {}, "score": "63.790894"}
{"text": "Although some smaller corpora for which the words have been tagged with part - of - speech labels by linguistic experts are available , such hand - labelling is impractical for large amounts of text , and consequently methods allowing automatic grouping of words into categories have been investigated .", "label": "", "metadata": {}, "score": "63.80819"}
{"text": "Although some smaller corpora for which the words have been tagged with part - of - speech labels by linguistic experts are available , such hand - labelling is impractical for large amounts of text , and consequently methods allowing automatic grouping of words into categories have been investigated .", "label": "", "metadata": {}, "score": "63.80819"}
{"text": "N tl ?w t ? ? ?N tl ?P ? w s ? ? ?N sl ? j?w t ? ? ?N tl ?P ? w t ? ? ?N tl ?Here words express the ideas presented in French , and reflects the extent to which the English hypothesisis grammatical .", "label": "", "metadata": {}, "score": "63.84341"}
{"text": "N tl ?w t ? ? ?N tl ?P ? w s ? ? ?N sl ? j?w t ? ? ?N tl ?P ? w t ? ? ?N tl ?Here words express the ideas presented in French , and reflects the extent to which the English hypothesisis grammatical .", "label": "", "metadata": {}, "score": "63.84341"}
{"text": "s output probabilities .When the context to each of the states ? s ?s N p ? ?The ? j ? ?N p ? ? ? are interpreted as transition probabilities , and the P ? ?j ? as ?", "label": "", "metadata": {}, "score": "63.871647"}
{"text": "s output probabilities .When the context to each of the states ? s ?s N p ? ?The ? j ? ?N p ? ? ? are interpreted as transition probabilities , and the P ? ?j ? as ?", "label": "", "metadata": {}, "score": "63.871647"}
{"text": "Various choices are P?w i jw k ? to be known , the c jmay be chosen to maximise the probability of p X j ? ?c j ? ? ? ?and the likelihood function : LL ?N X i ? ? log ?", "label": "", "metadata": {}, "score": "63.88874"}
{"text": "Various choices are P?w i jw k ? to be known , the c jmay be chosen to maximise the probability of p X j ? ?c j ? ? ? ?and the likelihood function : LL ?N X i ? ? log ?", "label": "", "metadata": {}, "score": "63.88874"}
{"text": "are employed in [ 53 ] to construct a bigram language model based on linguistic part - of - speech When the history equivalence class mapping is many - to - one , equation ( 54 ) simplifies to : P ? v?i?jw ? ? ?", "label": "", "metadata": {}, "score": "63.93684"}
{"text": "are employed in [ 53 ] to construct a bigram language model based on linguistic part - of - speech When the history equivalence class mapping is many - to - one , equation ( 54 ) simplifies to : P ? v?i?jw ? ? ?", "label": "", "metadata": {}, "score": "63.93684"}
{"text": "Page 29 . and the complete probability estimate for an eventis given by : P ? ?i j ?j ? ? k ? ?j ? ?P ? ?i j ? k ? ?N ? ?i j ?", "label": "", "metadata": {}, "score": "64.0013"}
{"text": "Page 29 . and the complete probability estimate for an eventis given by : P ? ?i j ?j ? ? k ? ?j ? ?P ? ?i j ? k ? ?N ? ?i j ?", "label": "", "metadata": {}, "score": "64.0013"}
{"text": "The lexicon can be encoded using any convenient method .The actual n gram data is then encoded in terms of local IDs .Lookup of a particular n gram first translates the desired word IDs into local IDs , and then performs a fast scanning for the appropriate sequence of local IDs to find the right value .", "label": "", "metadata": {}, "score": "64.04143"}
{"text": "A comparison between the Good - Turing and deleted estimates has been made using approximately 22 million words of AP news text during training , and as much during testing .The results indicate the former method to be consistently superior [ 15 ] , and thus no further emphasis will be given to the latter . 9By comparing equations ( 17 ) and ( 26 ) , note that 10This corresponds to the \" leaving - one - out \" method of cross - validation [ 19].", "label": "", "metadata": {}, "score": "64.04552"}
{"text": "A comparison between the Good - Turing and deleted estimates has been made using approximately 22 million words of AP news text during training , and as much during testing .The results indicate the former method to be consistently superior [ 15 ] , and thus no further emphasis will be given to the latter . 9By comparing equations ( 17 ) and ( 26 ) , note that 10This corresponds to the \" leaving - one - out \" method of cross - validation [ 19].", "label": "", "metadata": {}, "score": "64.04552"}
{"text": "The respective partitions together constitute the collection of machine translation resource data and each respective partition is less than the collection of machine translation resource data .The translation server is further operable to translate text in the source language into the target language using the obtained translation model data and language model data .", "label": "", "metadata": {}, "score": "64.077095"}
{"text": "The adaptation is conducted by changing the weights each time a dialogue act has been processed .We call this an iteration step for a weight qk . \" ...We demonstrate the applications of Markov Chains and HMMs to modeling of the underlying structure in spontaneous spoken language .", "label": "", "metadata": {}, "score": "64.114265"}
{"text": "P ?v j jh ?P ? hjw ? ? ?i ? ? ?( 54 )In this framework a natural choice for the history equivalence class mapping is the identity of the most recent n ? ?categories : H?w?i ? ? ?", "label": "", "metadata": {}, "score": "64.15309"}
{"text": "P ?v j jh ?P ? hjw ? ? ?i ? ? ?( 54 )In this framework a natural choice for the history equivalence class mapping is the identity of the most recent n ? ?categories : H?w?i ? ? ?", "label": "", "metadata": {}, "score": "64.15309"}
{"text": "Accordingly , the large database may be partitioned into a number of smaller database partitions so that each of a number of selected machines has a sufficient storage to store each database partition .Different machines may be networked to operate as a \" virtual \" single database to a client accessing the database .", "label": "", "metadata": {}, "score": "64.29126"}
{"text": "Various machine translation techniques including statistical machine translation techniques , have been developed to improve different aspects of machine translation , such as the translation quality and the translation speed .SUMMARY .[ 0006 ] This specification describes distributed machine processing systems , techniques , methods and apparatus that can be implemented to use resource partition , replication , and load balancing to access large models and to provide scalable and adaptive processing .", "label": "", "metadata": {}, "score": "64.311935"}
{"text": "Locating appropriate subsets of the training corpus , and using them to build a specific model .Combining the specific model with a corpus - wide model ( in statistical terminolo ... . \" ...Speech recognition performance is severely affected when the lexical , syntactic , or semantic characteristics of the discourse in the training and recognition tasks differ .", "label": "", "metadata": {}, "score": "64.42691"}
{"text": "[ 0137 ] In some implementations of the language model , the client code uses a simple interface that permits the value for a particular n gram to be requested .Internally , the client library that stores language model data decides which partition the requested n gram resides in , and queues a request for the n gram to be sent to that partition .", "label": "", "metadata": {}, "score": "64.51507"}
{"text": "Since use of a language model in the 1.2 .Dominant language modelling techniques This section presents a very brief summary of language modelling approaches prevalent in speech- recognition systems .The aim here is merely to place the scope of this thesis in context , and a more extensive review is presented in chapter 2 .", "label": "", "metadata": {}, "score": "64.5155"}
{"text": "Since use of a language model in the 1.2 .Dominant language modelling techniques This section presents a very brief summary of language modelling approaches prevalent in speech- recognition systems .The aim here is merely to place the scope of this thesis in context , and a more extensive review is presented in chapter 2 .", "label": "", "metadata": {}, "score": "64.5155"}
{"text": "Estimating Estimating Estimating Beam - pruning Employing the language model as a tagger P?v j jh m ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "64.53562"}
{"text": "Estimating Estimating Estimating Beam - pruning Employing the language model as a tagger P?v j jh m ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "64.53562"}
{"text": "Private Profile . this document .The first section , in chapter 3 , develops a model for syntactic dependencies based on word - category n - grams .The second section , in chapter 4 , extends this model by allowing short - range word relations to be captured through the incorporation of selected word n - grams .", "label": "", "metadata": {}, "score": "64.558304"}
{"text": "For a discussion of shallow vs deep architectures , see ( Bengio and LeCun 2007 ) .Whereas current models have two or three layers , theoretical research on deep architectures suggests that representing high - level semantic abstractions efficiently may require deeper networks .", "label": "", "metadata": {}, "score": "64.56692"}
{"text": "With the aim of finding anoptimal valuefor the discountingfactor , the leaving - one - outlog probability is determined andsubsequentlydifferentiated with respectto not be solved exactly , although the following bound on the optimal was determined : b in [ 55].", "label": "", "metadata": {}, "score": "64.666595"}
{"text": "With the aim of finding anoptimal valuefor the discountingfactor , the leaving - one - outlog probability is determined andsubsequentlydifferentiated with respectto not be solved exactly , although the following bound on the optimal was determined : b in [ 55].", "label": "", "metadata": {}, "score": "64.666595"}
{"text": "Each language model server stores and is operable to serve a partition of the language model .This method also includes translating the segment into the target language using the retrieved selected language model data .[ 0013 ]In another aspect , a method for machine translation can include using a machine translation system to receive text in a source language from a client and to translate the text into a target language .", "label": "", "metadata": {}, "score": "64.68106"}
{"text": "The queued requests are then sent out to one or more LM servers 210 , e.g. , sequentially in a first - in - first - out manner .The LM lookup queue 620 allows the requests to be made and thus served by the LM servers 210 at different times to reduce the wait time by the decoder 610 .", "label": "", "metadata": {}, "score": "64.81551"}
{"text": "In another aspect , a system for machine translation can include a translation server module and a translation cache .The translation server module is operable to obtain translation model data from a translation model for translation between a source language and a target language and language model data from a language model for the target language .", "label": "", "metadata": {}, "score": "64.82364"}
{"text": "If a human were to choose the features of a word , he might pick grammatical features like gender or plurality , as well as semantic features like animate \" or invisible .With a neural network language model , one relies on the learning algorithm to discover these features , and the features are continuous - valued ( making the optimization problem involved in learning much simpler ) .", "label": "", "metadata": {}, "score": "64.87633"}
{"text": "The category definitions themselves may be available a - priori , for instance as part - of - speech classifications indicating the grammatical function of each word , or they may be determined automati- cally by means of an optimisation process .", "label": "", "metadata": {}, "score": "64.89792"}
{"text": "The category definitions themselves may be available a - priori , for instance as part - of - speech classifications indicating the grammatical function of each word , or they may be determined automati- cally by means of an optimisation process .", "label": "", "metadata": {}, "score": "64.89792"}
{"text": "r ?C r ? q r r ? ? ? ? ? ? ? ? ?R ( 26 ) Assume now that the training set has been partitioned into two sets termed the retained and heldout parts respectively .Let the counts estimate C rbe determined from the former , so that the maximum likelihood ? ?", "label": "", "metadata": {}, "score": "64.917145"}
{"text": "r ?C r ? q r r ? ? ? ? ? ? ? ? ?R ( 26 ) Assume now that the training set has been partitioned into two sets termed the retained and heldout parts respectively .Let the counts estimate C rbe determined from the former , so that the maximum likelihood ? ?", "label": "", "metadata": {}, "score": "64.917145"}
{"text": "The neural network learns to map that sequence of feature vectors to a prediction of interest , such as the probability distribution over the next word in the sequence .The advantage of this distributed representation approach is that it allows the model to generalize well to sequences that are not in the set of training word sequences , but that are similar in terms of their features , i.e. , their distributed representation .", "label": "", "metadata": {}, "score": "64.945435"}
{"text": "Moreover , since the number of n - tuples becomes extremely large as n increases , the models are very complex in terms of the number of parameters they employ .Both their large size ( and consequent memory requirements ) , and the training set sparseness associated with large numbers of parameters , limits n to 2 , 3 or perhaps 4 .", "label": "", "metadata": {}, "score": "64.999"}
{"text": "Moreover , since the number of n - tuples becomes extremely large as n increases , the models are very complex in terms of the number of parameters they employ .Both their large size ( and consequent memory requirements ) , and the training set sparseness associated with large numbers of parameters , limits n to 2 , 3 or perhaps 4 .", "label": "", "metadata": {}, "score": "64.999"}
{"text": "Surprisingly , linear combination outperforms log - linear combination of the models .The best adapted systems provide a statistically significant improvement of 1.78 absolute BLEU points ( 6.85 % relative ) and 2.73 absolute BLEU points ( 8.05 % relative ) over the baseline system for English - German and English - French , respectively . .", "label": "", "metadata": {}, "score": "65.076004"}
{"text": "i ? ? ?c j ?P ? w?i?jw?i?j ?( 64 ) where c j ? ? and p X j ? ?c j ? ?( 65 )The constraint ( 65 ) guarantees that the probabilities sum to unity .", "label": "", "metadata": {}, "score": "65.090195"}
{"text": "i ? ? ?c j ?P ? w?i?jw?i?j ?( 64 ) where c j ? ? and p X j ? ?c j ? ?( 65 )The constraint ( 65 ) guarantees that the probabilities sum to unity .", "label": "", "metadata": {}, "score": "65.090195"}
{"text": "0053 ] The translation quality of a statistical machine translation ( SMT ) system can generally be improved by increasing the size of either or both of the translation model ( TM ) and the language model ( LM ) of the system .", "label": "", "metadata": {}, "score": "65.206406"}
{"text": "This definition , which is fundamental to the approach , is made in terms of the category assign- ments of the words .It minimises the effect syntax has on word co - occurrences while taking particular advantageof the grammatical word classifications implicit in the operation of the category model .", "label": "", "metadata": {}, "score": "65.21846"}
{"text": "This definition , which is fundamental to the approach , is made in terms of the category assign- ments of the words .It minimises the effect syntax has on word co - occurrences while taking particular advantageof the grammatical word classifications implicit in the operation of the category model .", "label": "", "metadata": {}, "score": "65.21846"}
{"text": "[ 0084 ] Further details of various features described above and other features for automated machine translation are provided in the following sections .Encoding and Accessing a Distributed Language Model .[0085 ] This section describes aspects of MT systems for translating text and document from one natural language , such as Chinese , to another natural language , such as English .", "label": "", "metadata": {}, "score": "65.28776"}
{"text": "[0151 ] Portions or sections within Web pages or documents that are to be translated at a high - quality can be identified in a variety of ways .One strategy is to translate the initial part of a document at the high quality because this part is likely to be carefully examined by the user or may be the only portion read by the user .", "label": "", "metadata": {}, "score": "65.297165"}
{"text": "Mixtures of topic - specific language models In orderto accountfor aset numberof distinct themes appearingin a corpus , the textmay bedividedinto partitions corresponding to common subject matter , termed topics , following which a trigram language model is built for each individual topic [ 28].", "label": "", "metadata": {}, "score": "65.29819"}
{"text": "Mixtures of topic - specific language models In orderto accountfor aset numberof distinct themes appearingin a corpus , the textmay bedividedinto partitions corresponding to common subject matter , termed topics , following which a trigram language model is built for each individual topic [ 28].", "label": "", "metadata": {}, "score": "65.29819"}
{"text": "Because of the categorical nature of language , and the large vocabularies people naturally use , statistical techniques must estimate a large number of parameters , and consequently depend critically on the availability of large amounts of training data . ...", "label": "", "metadata": {}, "score": "65.37155"}
{"text": "Because of the categorical nature of language , and the large vocabularies people naturally use , statistical techniques must estimate a large number of parameters , and consequently depend critically on the availability of large amounts of training data . . ..", "label": "", "metadata": {}, "score": "65.37155"}
{"text": "This aspect of the operations for the load balancer 120 is similar to the operation in FIG .3 for each translation front end 110 .[0065 ] The concept of using translated segments stored in a cache to reduce processing and communication traffic in a MT system may be extended to the back end of the MT system for access by the segment translation servers 130 .", "label": "", "metadata": {}, "score": "65.40314"}
{"text": "Denote the general sample by parameters of the probability distribution defined on the sample space general sample ?Gand the sample obtained from the target domain by ? D. Let the j?j be ?Then , starting with the P ? ?", "label": "", "metadata": {}, "score": "65.429794"}
{"text": "Denote the general sample by parameters of the probability distribution defined on the sample space general sample ?Gand the sample obtained from the target domain by ? D. Let the j?j be ?Then , starting with the P ? ?", "label": "", "metadata": {}, "score": "65.429794"}
{"text": "More specifically , the automated machine processing techniques and systems in this specification can be used for various on - line machine processing such as machine translation , speech recognition , spam detection , optical character recognition , spelling correction , and others .", "label": "", "metadata": {}, "score": "65.55082"}
{"text": "Learning Distributed Representations of Concepts .Proceedings of the Eighth Annual Conference of the Cognitive Science Society:1 - 12 .Rumelhart , D. E. and McClelland , J. L ( 1986 ) Parallel Distributed Processing : Explorations in the Microstructure of Cognition .", "label": "", "metadata": {}, "score": "65.57381"}
{"text": "We put forward a statistical language model that resolves these problems , does POS tagging , and can be used as the language model of a speech recognizer .We find that by accounting for the interactions between these tasks that the performance on each task improves , as does POS tagging and perplexity . by Jerome R. Bellegarda , Senior Member - IEEE Transactions on Speech and Audio Processing , 1998 . \" ...", "label": "", "metadata": {}, "score": "65.60968"}
{"text": "g. In principle any probability estimator that takes unseen events into account may be used in the backing off framework .In particular , Turing 's estimate ( 22 ) may either be used directly or in its modified form ( 25 ) .", "label": "", "metadata": {}, "score": "65.63281"}
{"text": "g. In principle any probability estimator that takes unseen events into account may be used in the backing off framework .In particular , Turing 's estimate ( 22 ) may either be used directly or in its modified form ( 25 ) .", "label": "", "metadata": {}, "score": "65.63281"}
{"text": "Each string is an n - gram , which is a sequence of n tokens in the target language , where n is a positive integer .Various tokenization techniques may be used to construct a tokens from one or more of symbols and marks , including diacritical marks and punctuation marks , letters , and character in a language .", "label": "", "metadata": {}, "score": "65.64065"}
{"text": "In addition , the partition and replication for the distributed machine processing of this specification can apply to the load balancing mechanism , when needed , with different machines so that the load balancing mechanism is partitioned into , or replicated on , the different machines .", "label": "", "metadata": {}, "score": "65.76279"}
{"text": "Most previous work on adaptive SMT focuses on the use of IR techniques to identify a relevant subset ... . by Norbert Reithinger , Ralf Engel , Martin Klesen - In Proceedings of the International Conference on Spoken Language Processing , 1996 . \" ...", "label": "", "metadata": {}, "score": "65.79393"}
{"text": "P ?r ?Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 26 .This interpretation is used in [ 55 ] to postulate the form of the probability estimate as one which explicitly contains a discounting factor d r : q r ? r?d r N ? if r ? ? q ? if r ? ?", "label": "", "metadata": {}, "score": "65.83383"}
{"text": "P ?r ?Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 26 .This interpretation is used in [ 55 ] to postulate the form of the probability estimate as one which explicitly contains a discounting factor d r : q r ? r?d r N ? if r ? ? q ? if r ? ?", "label": "", "metadata": {}, "score": "65.83383"}
{"text": "Interestingly , state of the art language models for speech recognition are based on a very crude linguistic model , namely conditioning the probability of a word on a sm ... \" .It seems obvious that a successful model of natural language would incorporate a great deal of both linguistic and world knowledge .", "label": "", "metadata": {}, "score": "65.885574"}
{"text": "For an n gram that is frequently used , its language model data can be saved in the low - level cache 632 .For an n gram that is used in translating the current segment but is not likely to be used frequently in the target language , the received data can be stored in the high - level cache 631 , which is frequently emptied .", "label": "", "metadata": {}, "score": "65.89143"}
{"text": "z ? ? ?K ?e ? lim K ? ?K ?P ?z ? ? ?K ?n P ?z ? ? ?K ?z ? ? ?K ?o ? , where the summation is over all member functions .", "label": "", "metadata": {}, "score": "65.92593"}
{"text": "z ? ? ?K ?e ? lim K ? ?K ?P ?z ? ? ?K ?n P ?z ? ? ?K ?z ? ? ?K ?o ? , where the summation is over all member functions .", "label": "", "metadata": {}, "score": "65.92593"}
{"text": "Because it can be difficult to determine the set of n gram values that will be needed by a particular translation iteration , the translation system can be structured to run each iteration as two passes .The translation iteration first is run with the dummy language model , then \" Wait ( ) \" is called to ensure that all the pending language model lookups complete .", "label": "", "metadata": {}, "score": "65.9366"}
{"text": "( b ) Loose long - range relations : For example , the nouns \" sand \" and \" desert \" may be expected to occur within the same sentence .Word - based n - grams attempt to model all this information simultaneously by treating each possible n - tuple individually .", "label": "", "metadata": {}, "score": "66.0311"}
{"text": "( b ) Loose long - range relations : For example , the nouns \" sand \" and \" desert \" may be expected to occur within the same sentence .Word - based n - grams attempt to model all this information simultaneously by treating each possible n - tuple individually .", "label": "", "metadata": {}, "score": "66.0311"}
{"text": "Furthermore , but it may be practically advantageous to tie its value in some way .Empirical tests were performed in [ 53 ] for the following choices : D should be chosen to maximise a cross validation function , but exact solution D should strictly be different for every ?", "label": "", "metadata": {}, "score": "66.0367"}
{"text": "Furthermore , but it may be practically advantageous to tie its value in some way .Empirical tests were performed in [ 53 ] for the following choices : D should be chosen to maximise a cross validation function , but exact solution D should strictly be different for every ?", "label": "", "metadata": {}, "score": "66.0367"}
{"text": "I m ?v ?v ?This process continues until the desired Due to the large number of potential merges that must be investigated at every step , considerable care is required during the implementation of the algorithm in order to ensure its computational feasibility .", "label": "", "metadata": {}, "score": "66.044495"}
{"text": "I m ?v ?v ?This process continues until the desired Due to the large number of potential merges that must be investigated at every step , considerable care is required during the implementation of the algorithm in order to ensure its computational feasibility .", "label": "", "metadata": {}, "score": "66.044495"}
{"text": "As an example , large language models for English can be derived from about 200 billion words to 8 trillion words and are from about 1 Terabyte to 4 Terabytes in size .A large TM may be on the order of magnitude of 200 million words or larger .", "label": "", "metadata": {}, "score": "66.06807"}
{"text": "N tl ?Results indicate improved recognition performance when the additional knowledge of porated in this way .w s ? ? ?N sl ? incor-2.8.3 .Spelling correction Automatic methods for correcting spelling errors are important in a number of areas , including doc- ument preparation , database interaction and text - to - speech systems [ 46].", "label": "", "metadata": {}, "score": "66.12758"}
{"text": "N tl ?Results indicate improved recognition performance when the additional knowledge of porated in this way .w s ? ? ?N sl ? incor-2.8.3 .Spelling correction Automatic methods for correcting spelling errors are important in a number of areas , including doc- ument preparation , database interaction and text - to - speech systems [ 46].", "label": "", "metadata": {}, "score": "66.12758"}
{"text": "Lexicon augmentation 3.3.1.2 .Beam pruning 3.3.1.3 .Tagging the Switchboard and WSJ corpora 3.3.2 .Constructing category trees 3.3.3.Word - perplexity ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "66.24596"}
{"text": "Lexicon augmentation 3.3.1.2 .Beam pruning 3.3.1.3 .Tagging the Switchboard and WSJ corpora 3.3.2 .Constructing category trees 3.3.3.Word - perplexity ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "66.24596"}
{"text": "We demonstrate the applications of Markov Chains and HMMs to modeling of the underlying structure in spontaneous spoken language .Experiments with supervised training cover the detection of the current dialog state and identification of the speech act as used by the speech translation component in our JANUS Speech - to - Speech Translation System .", "label": "", "metadata": {}, "score": "66.253296"}
{"text": "In the process a greater amount of evidence is extracted than is available in the most likely transcription hypothesis , and overall retrieval precision and recall are improved .The term weighting scheme known as the inverse document frequenc ... . \" ...", "label": "", "metadata": {}, "score": "66.316956"}
{"text": "Page 5 .Table of contents 1 .Introduction1 1.1 .The speech recognition problem 1.1.1 .Preprocessing of the speech signal 1.1.2 .The acoustic model 1.1.3 .The language model ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "66.40752"}
{"text": "Page 5 .Table of contents 1 .Introduction1 1.1 .The speech recognition problem 1.1.1 .Preprocessing of the speech signal 1.1.2 .The acoustic model 1.1.3 .The language model ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "66.40752"}
{"text": "Excepting the leaf ( terminal ) nodes , each has two descendants , one for each possible outcome of the question .Denote a decision tree T as a set of T nnodes , T?ft ? t ? t T n ? ?", "label": "", "metadata": {}, "score": "66.418304"}
{"text": "Excepting the leaf ( terminal ) nodes , each has two descendants , one for each possible outcome of the question .Denote a decision tree T as a set of T nnodes , T?ft ? t ? t T n ? ?", "label": "", "metadata": {}, "score": "66.418304"}
{"text": "We present encouraging language model domain adaptation results that both illustrate the potential benefits of our new model and suggest new avenues of inquiry . \" ...This paper reports experiments on adapting components of a Statistical Machine Translation ( SMT ) system for the task of translating online user - generated forum data from Symantec .", "label": "", "metadata": {}, "score": "66.43756"}
{"text": "In within - domain adaptation , test data comes from the same source as the training data , but the latter is heterogeneous , consisting of many subsets with vary ... . \" ...Speech recognition performance is severely affected when the lexical , syntactic , or semantic characteristics of the discourse in the training and recognition tasks differ .", "label": "", "metadata": {}, "score": "66.477516"}
{"text": "One or more replica language model servers can be included for each of the language model servers 210 .This system can also include translation model servers 220 respectively storing and operable to serve different partitions of a translation model for translation between the target language and a source language .", "label": "", "metadata": {}, "score": "66.4857"}
{"text": "At the same time , it is useful to represent the language model data compactly , so that the total amount of memory needed to hold the language model is reduced .Accordingly , the number of partitions can be reduced and the number of machines required to serve the language model can also be reduced .", "label": "", "metadata": {}, "score": "66.638115"}
{"text": "The language model data received from the language model servers 210 is stored in the high - level cache 631 ( step 1070 ) .This process of placing the generated request in the LM lookup request queue 620 and storing the received language model data in the high - level cache 631 is performed for all n grams whose language model information is initially not in the high - level cache 631 .", "label": "", "metadata": {}, "score": "66.64429"}
{"text": "P?w k jl j ? ? log ?P?w k jl j ?so that the average entropy of the tree is : ? h tr ee ?W ? ?L X j ? ?h j ?W ? ?", "label": "", "metadata": {}, "score": "66.7052"}
{"text": "P?w k jl j ? ? log ?P?w k jl j ?so that the average entropy of the tree is : ? h tr ee ?W ? ?L X j ? ?h j ?W ? ?", "label": "", "metadata": {}, "score": "66.7052"}
{"text": "When the information is not available in the high - level cache , the decoder then looks up the low - level cache ( steps 840 and 850 ) .If the information is in the low - level cache , the decoder marks this status ( step 860 ) and proceeds to use information in both the high - level cache and low level cache to complete the translation ( step 830 ) .", "label": "", "metadata": {}, "score": "66.74611"}
{"text": "0036 ] Partition and replication are two examples of techniques available for implementing the distributed design .[ 0037 ]In partition , a particular item within an automated processing system is divided or partitioned into different partitions that are physically located on different machines , e.g. , computers .", "label": "", "metadata": {}, "score": "66.74808"}
{"text": "N H ? ?g and R L a ?f ? ? ? ? ? ? ? ?L H ? h a ? ? ?g. Assume as in equation ( 52 ) that P ?w?i?jw ? ? ?", "label": "", "metadata": {}, "score": "66.86076"}
{"text": "N H ? ?g and R L a ?f ? ? ? ? ? ? ? ?L H ? h a ? ? ?g. Assume as in equation ( 52 ) that P ?w?i?jw ? ? ?", "label": "", "metadata": {}, "score": "66.86076"}
{"text": "[ 0069 ] The low - level cache 632 may be implemented as an optional feature in the segment translation server 130 to store frequently used LM data .Generally , the data in the low - level cache 632 is not emptied after translation of each segment and is retrained for a period longer than the data in the high - level cache 631 .", "label": "", "metadata": {}, "score": "66.9106"}
{"text": "2 . [ 0056 ] FIG .2 further shows one or more servers 230 for other translation resources and data in addition to the LM and TM servers 210 and 220 .This feature may be an optional feature to further improve various properties of the system 200 .", "label": "", "metadata": {}, "score": "66.92465"}
{"text": "The speech recognition problem In speechrecognition , the aim is to find themost likely sequenceof words data Bayes rule , w giventhe observedacoustic x. This is achieved by finding that w which maximises the conditional probability P?wjx ?From P?wjx ?P?w ?", "label": "", "metadata": {}, "score": "66.973724"}
{"text": "The speech recognition problem In speechrecognition , the aim is to find themost likely sequenceof words data Bayes rule , w giventhe observedacoustic x. This is achieved by finding that w which maximises the conditional probability P?wjx ?From P?wjx ?P?w ?", "label": "", "metadata": {}, "score": "66.973724"}
{"text": "v h a ?j ? ?V ?w?j?i?L H ? h a ? ?a?R H ? j?R L a ? i?L H ? h a ?( 74 ) where the word occurrence probability depends only on its believed category , and hence employ ( 53 ) to write : R H ?", "label": "", "metadata": {}, "score": "67.0116"}
{"text": "v h a ?j ? ?V ?w?j?i?L H ? h a ? ?a?R H ? j?R L a ? i?L H ? h a ?( 74 ) where the word occurrence probability depends only on its believed category , and hence employ ( 53 ) to write : R H ?", "label": "", "metadata": {}, "score": "67.0116"}
{"text": "The set of all possible unique measurements in ? is the sample space , j?j .Furthermore , define the events both of the population and of the sample ? and in the measurements be the detection of particular word patterns , the specific nature of which will depend on the method by which we choose to model the language7 .", "label": "", "metadata": {}, "score": "67.045815"}
{"text": "The set of all possible unique measurements in ? is the sample space , j?j .Furthermore , define the events both of the population and of the sample ? and in the measurements be the detection of particular word patterns , the specific nature of which will depend on the method by which we choose to model the language7 .", "label": "", "metadata": {}, "score": "67.045815"}
{"text": "5 shows an example of a segment translation server cache that can be shared by the segment translation servers .[ 0026 ]FIG .6 shows an example of a segment translation server having a translation decoder , a LM lookup request queue , and a local cache .", "label": "", "metadata": {}, "score": "67.147385"}
{"text": "For decoders that structure the search space by iteratively extending each hypothesis in a set of hypotheses by appending a finite set of possible extensions , dummy requests can be issued whenever a set of hypotheses is extended .[ 0141 ]", "label": "", "metadata": {}, "score": "67.14773"}
{"text": "[0038 ] Replication is another technique for the distributed design and is different from partition .In replication , a particular item within such a system , e.g. , a database server or a processing server , is duplicated or cloned onto one or more replica machines such as computers .", "label": "", "metadata": {}, "score": "67.17916"}
{"text": "1 and implemented with a translation cache , there can be variations in how the translation cache with translations at different quality levels interacts with the load balancer 120 .For example , the system may be designed to look up every segment in the cache , send everything to the load balancer , and let the load balancer determine whether to use the cache entry or whether to send the segment to a translation engine .", "label": "", "metadata": {}, "score": "67.20371"}
{"text": "P cache?v j ?w?i ?The language model probability is calculated using equation ( 57 ) : P ? w?i?jv?i ? ? ?i ? ? ?X v?v?V?w?i ? ?P ?w?i?jv ?P ? vjv?i ? ? ?", "label": "", "metadata": {}, "score": "67.21677"}
{"text": "P cache?v j ?w?i ?The language model probability is calculated using equation ( 57 ) : P ? w?i?jv?i ? ? ?i ? ? ?X v?v?V?w?i ? ?P ?w?i?jv ?P ? vjv?i ? ? ?", "label": "", "metadata": {}, "score": "67.21677"}
{"text": "Despite these restrictions , they remain the most successful type of language model currently used . ) words of In order to counter the sparseness of the training corpus , and to improve model generalisation , language models that group words into categories have been proposed .", "label": "", "metadata": {}, "score": "67.2236"}
{"text": "Despite these restrictions , they remain the most successful type of language model currently used . ) words of In order to counter the sparseness of the training corpus , and to improve model generalisation , language models that group words into categories have been proposed .", "label": "", "metadata": {}, "score": "67.2236"}
{"text": "The first is based on the use of mixtures of language models : the training text is partitioned according to topic , a language model is constructed for each component , and at recognition time appropriate weightings are assigned to each component to model the observed style of language .", "label": "", "metadata": {}, "score": "67.231995"}
{"text": "Baseline word - based n - gram models ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?109 D.4 .The Wall Street Journal ( WSJ ) corpus ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "67.25543"}
{"text": "Baseline word - based n - gram models ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?109 D.4 .The Wall Street Journal ( WSJ ) corpus ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "67.25543"}
{"text": "model servesas a fallback for all othercases .This allows frequent word n - tuples to be modelleddirectly , whileothersarecapturedbythesyntacticmodel .Careisnecessarywhendesigningthismechanismsince the multiple classifications of the sentencehistory into category sequencescomplicate the normalisation of the statistical model .Modelling long - range semantic relations Semantic relations due to factors such as the topic or the style of the text may span many words , and can therefore not be modelled by fixed - length sequences .", "label": "", "metadata": {}, "score": "67.318"}
{"text": "model servesas a fallback for all othercases .This allows frequent word n - tuples to be modelleddirectly , whileothersarecapturedbythesyntacticmodel .Careisnecessarywhendesigningthismechanismsince the multiple classifications of the sentencehistory into category sequencescomplicate the normalisation of the statistical model .Modelling long - range semantic relations Semantic relations due to factors such as the topic or the style of the text may span many words , and can therefore not be modelled by fixed - length sequences .", "label": "", "metadata": {}, "score": "67.318"}
{"text": "Due to the extremely large w ? ? ?i ?Define an operator distinct history equivalence classes .Denote these by number of different equivalence classes found in the training corpus , so that the classification segments the words of the corpus into H?w?i ? ? which maps the history w ? ? ?", "label": "", "metadata": {}, "score": "67.322464"}
{"text": "Due to the extremely large w ? ? ?i ?Define an operator distinct history equivalence classes .Denote these by number of different equivalence classes found in the training corpus , so that the classification segments the words of the corpus into H?w?i ? ? which maps the history w ? ? ?", "label": "", "metadata": {}, "score": "67.322464"}
{"text": "i ? ? ?P ?w?i?jV?w?i ? ?P ?V?w?i ? ?jV?w?i ?( 59 ) which is the category - basedbigram languagemodelusedin [ 30 ] , [ 31 ] , [ 32 ] , [ 42 ] and[53 ] in conjunction with automatically - determined category membership , as will be described in the following section .", "label": "", "metadata": {}, "score": "67.34427"}
{"text": "i ? ? ?P ?w?i?jV?w?i ? ?P ?V?w?i ? ?jV?w?i ?( 59 ) which is the category - basedbigram languagemodelusedin [ 30 ] , [ 31 ] , [ 32 ] , [ 42 ] and[53 ] in conjunction with automatically - determined category membership , as will be described in the following section .", "label": "", "metadata": {}, "score": "67.34427"}
{"text": "Finding the best English translation of the French sentence is a search for the English hypothesis maximising P?w s ? ? ?N sl ? j?w t ? ? ?N tl ? ? is the translation model , reflecting the extent to which the English P ? ?", "label": "", "metadata": {}, "score": "67.364655"}
{"text": "Finding the best English translation of the French sentence is a search for the English hypothesis maximising P?w s ? ? ?N sl ? j?w t ? ? ?N tl ? ? is the translation model , reflecting the extent to which the English P ? ?", "label": "", "metadata": {}, "score": "67.364655"}
{"text": "This structure comprises the current topic , discourse state ... . \" ...NLP researchers face a dilemma : on one side , it is unarguably accepted that languages have internal structure rather than strings of words .On the other side , they find it very difficult and expensive to write grammars that have good coverage of language structures .", "label": "", "metadata": {}, "score": "67.39493"}
{"text": "We outline the conventional language modeling technology , as implemented in the toolkit , and describe the extra efficiency and functionality that the new toolkit provides as compared to previous software for this task .Finally , we give an example of the use of the toolkit in constructing and testing a simple language model . \" ...", "label": "", "metadata": {}, "score": "67.408905"}
{"text": "0068 ]In operation , the high - level cache 631 may be emptied in some manner so that the stored LM data does not accumulated beyond a certain limit .In some implementations , the segment translation server 130 periodically deletes the content of the high - level cache 631 .", "label": "", "metadata": {}, "score": "67.41865"}
{"text": "1 and the translation and language models are examples of the machine translation resource data in the system 100 in FIG .1 .SMT decoders can be implemented in at least some of the segment translation servers 130 to perform the translation of each segment using the translation and language models in servers 210 and 220 .", "label": "", "metadata": {}, "score": "67.42342"}
{"text": "i ?X ?h?h?H?w?i ? ?P?v j jh ? ?P ? hjw ? ? ?i ? ? ?( 76 ) where that may be assignedto thewordhistory in figure 3.1 , and the subsequent sections treat the estimation of each individually .", "label": "", "metadata": {}, "score": "67.59988"}
{"text": "i ?X ?h?h?H?w?i ? ?P?v j jh ? ?P ? hjw ? ? ?i ? ? ?( 76 ) where that may be assignedto thewordhistory in figure 3.1 , and the subsequent sections treat the estimation of each individually .", "label": "", "metadata": {}, "score": "67.59988"}
{"text": "Associated with each word V coreis defined , consisting a set of words that are assumed to exhibit w Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 35 . in to in the core vocabulary with which the context of the new word agrees best .", "label": "", "metadata": {}, "score": "67.69499"}
{"text": "Associated with each word V coreis defined , consisting a set of words that are assumed to exhibit w Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 35 . in to in the core vocabulary with which the context of the new word agrees best .", "label": "", "metadata": {}, "score": "67.69499"}
{"text": "A user - level \" Wait ( ) \" operation can be used by a client to force all pending lookups to be sent to the appropriate servers .The operation waits until they complete before returning to the caller .One example of this cache is the low - level cache 632 ( FIG .", "label": "", "metadata": {}, "score": "67.96562"}
{"text": "[ 0018 ] The disclosed and other embodiments can be implemented as one or more computer program products , i.e. , one or more modules of computer program instructions encoded on a computer readable medium for execution by , or to control the operation of , data processing apparatus .", "label": "", "metadata": {}, "score": "67.97603"}
{"text": "Most probabilistic classifiers used for word - sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features .In this paper , a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguafion of the noun interest .", "label": "", "metadata": {}, "score": "68.07605"}
{"text": "Page 47 .Language models for other applications Although this thesis focuses on their application to speech - recognition systems , language models are importantcomponentsalsoinotherfields , someofwhichareintroducedbrieflyinthefollowingsections .In particular , instead of an acoustic model , a model for the geometry or trajectory of the writing is employed .", "label": "", "metadata": {}, "score": "68.084"}
{"text": "Page 47 .Language models for other applications Although this thesis focuses on their application to speech - recognition systems , language models are importantcomponentsalsoinotherfields , someofwhichareintroducedbrieflyinthefollowingsections .In particular , instead of an acoustic model , a model for the geometry or trajectory of the writing is employed .", "label": "", "metadata": {}, "score": "68.084"}
{"text": "The high - level cache 631 and low - level cache 632 can be used to store selected LM data for use by the segment translation server 130 without accessing the LM servers 210 .The high - level cache 631 may be configured to store selected LM data obtained during translating a segment so that all n grams needed for the current segment are available within the segment translation server 130 .", "label": "", "metadata": {}, "score": "68.10828"}
{"text": "The DMT system 100 includes segment translation servers 130 that retrieve translation resource data from translation resource servers 140 and use the retrieved data to perform translation tasks .The load balancer server 120 can be replicated on one or more replica load balancer servers .", "label": "", "metadata": {}, "score": "68.14416"}
{"text": "Examples of the category membership after optimisation indicate that , while the most frequent words in each cluster often appear to be grouped by syntactic or semantic function , this is not always so .Indeed some categories are very difficult to justify intuitively [ 30 ] , [ 31 ] , [ 42 ] , [ 53].", "label": "", "metadata": {}, "score": "68.26639"}
{"text": "Examples of the category membership after optimisation indicate that , while the most frequent words in each cluster often appear to be grouped by syntactic or semantic function , this is not always so .Indeed some categories are very difficult to justify intuitively [ 30 ] , [ 31 ] , [ 42 ] , [ 53].", "label": "", "metadata": {}, "score": "68.26639"}
{"text": "Modelling syntactic relations Word order is important for grammatical correctness in English and is captured naturally and effectively by n - grams .Furthermore , English has many local syntactic constructs [ 37 ] which may be captured by n - grams despite their limited range .", "label": "", "metadata": {}, "score": "68.3232"}
{"text": "Modelling syntactic relations Word order is important for grammatical correctness in English and is captured naturally and effectively by n - grams .Furthermore , English has many local syntactic constructs [ 37 ] which may be captured by n - grams despite their limited range .", "label": "", "metadata": {}, "score": "68.3232"}
{"text": "0165 ] Processors suitable for the execution of a computer program include , by way of example , both general and special purpose microprocessors , and any one or more processors of any kind of digital computer .Generally , a processor will receive instructions and data from a read only memory or a random access memory or both .", "label": "", "metadata": {}, "score": "68.344086"}
{"text": "When a suitable translation for a segment to be translated is not in the translation cache 310 , that segment is then further processed and translated ( step 345 ) .After completing one segment , the translation front end 110 moves on to the next segment to be processed until all of the divided segments are processed ( steps 347 and 348 ) .", "label": "", "metadata": {}, "score": "68.40451"}
{"text": "The language model information is usually collected by computing the frequency of occurrence of sequences of words in a large training corpus of documents .As an example , a collection of such data may yield the following information : . where the strings of words on the left represent various possible sequences of the words and the numbers on the right represent the number of occurrences in the training corpus of documents .", "label": "", "metadata": {}, "score": "68.433525"}
{"text": "Let the member function be be .The time average for this case is given by z ? ? ?K ? , and the statistical property and the ensemble average by ?z ? ? ?K ? ? ?t ? lim K ? ?", "label": "", "metadata": {}, "score": "68.43497"}
{"text": "Let the member function be be .The time average for this case is given by z ? ? ?K ? , and the statistical property and the ensemble average by ?z ? ? ?K ? ? ?t ? lim K ? ?", "label": "", "metadata": {}, "score": "68.43497"}
{"text": "s ? s ?s N p ? ? according to the transition probabilities , following which an event ? iis emitted at each state accordingto the appropriate output probability .In this framework , the optimal ? initial choice of j ? ?", "label": "", "metadata": {}, "score": "68.44913"}
{"text": "s ? s ?s N p ? ? according to the transition probabilities , following which an event ? iis emitted at each state accordingto the appropriate output probability .In this framework , the optimal ? initial choice of j ? ?", "label": "", "metadata": {}, "score": "68.44913"}
{"text": "The first relies on syn- tactic and semantic analyses of the sample text to determine the hierarchical sentence structure .Such analyses employ a set of rules to ascertain whether a sentence is permissible or not , and although it has been possible to describe a significant proportion of English usage in this way , complete coverage has remainedelusive .", "label": "", "metadata": {}, "score": "68.47214"}
{"text": "The first relies on syn- tactic and semantic analyses of the sample text to determine the hierarchical sentence structure .Such analyses employ a set of rules to ascertain whether a sentence is permissible or not , and although it has been possible to describe a significant proportion of English usage in this way , complete coverage has remainedelusive .", "label": "", "metadata": {}, "score": "68.47214"}
{"text": "\\ )Vector \\(C_k\\ ) contains the learned features for word \\(k\\ .Let us denote \\(\\theta\\ ) for the concatenation of all the parameters .The capacity of the model is controlled by the number of hidden units \\(h\\ ) and by the number of learned word features \\(d\\ .", "label": "", "metadata": {}, "score": "68.472374"}
{"text": "We will describe two simple adaptive language models , and show that they do indeed lead to a significant pe ... . by Adam Kalai , Stanley Chen , Avrim Blum , Ronald Rosenfeld - Proceedings of the International Conference on Accoustics , Speech , and Signal Processing , 1998 . \" ...", "label": "", "metadata": {}, "score": "68.58933"}
{"text": "r N ? which requires the estimates to lie close to the relative frequencies .Since other estimation methods have been found to be both simpler and more successful , these modifications will not be considered further .Deleted estimation The probabilities q rmay be estimated by employingthe rotation method of cross - validation[35].", "label": "", "metadata": {}, "score": "68.59863"}
{"text": "r N ? which requires the estimates to lie close to the relative frequencies .Since other estimation methods have been found to be both simpler and more successful , these modifications will not be considered further .Deleted estimation The probabilities q rmay be estimated by employingthe rotation method of cross - validation[35].", "label": "", "metadata": {}, "score": "68.59863"}
{"text": "For example , a database server that primarily stores data or a processing server that primarily executes one or more processing tasks in the automated processing system can be an item that is partitioned .Partition allows a large item to be implemented in the system without being limited to the capacity of a single machine .", "label": "", "metadata": {}, "score": "68.64463"}
{"text": "A highly used database , for example , may be replicated on different database servers .As another example , a processing server may be replicated into one or more replica processing servers that can operate in parallel with one another .", "label": "", "metadata": {}, "score": "68.73823"}
{"text": "During the wait from the back end resource servers such as the LM servers , the translation server can perform the parts of the computation not dependent on the back end results .The back end latencies themselves can be reduced by partitioning the data across multiple physical machines which are accessed by the translation server as a single virtual server .", "label": "", "metadata": {}, "score": "68.795166"}
{"text": "Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 39 .A study of word association types discovered using a measure termed the association ratio which is closely related to mutual information , is presented in [ 14].", "label": "", "metadata": {}, "score": "68.822876"}
{"text": "Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 39 .A study of word association types discovered using a measure termed the association ratio which is closely related to mutual information , is presented in [ 14].", "label": "", "metadata": {}, "score": "68.822876"}
{"text": "Inclusion of word n - grams ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "68.82588"}
{"text": "Inclusion of word n - grams ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "68.82588"}
{"text": "This prediction is used by di erent modules of the speech - to - speech translation system VERBMOBIL .The statistical approach uses deleted interpolation of n - gram frequencies as basis and de ... \" .We present the application of statistical language modeling methods for the prediction of the next dialogue act .", "label": "", "metadata": {}, "score": "68.84101"}
{"text": "V?w i ?j?f ? ? ? ? ? ? ? ?N v ? ?g and i?f ? ? ? ? ? ? ? ?N w ? ?g ( 51 ) where to - one we will speak of deterministic category membership , while referring to stochastic membership when it is many - to - many .", "label": "", "metadata": {}, "score": "68.85629"}
{"text": "V?w i ?j?f ? ? ? ? ? ? ? ?N v ? ?g and i?f ? ? ? ? ? ? ? ?N w ? ?g ( 51 ) where to - one we will speak of deterministic category membership , while referring to stochastic membership when it is many - to - many .", "label": "", "metadata": {}, "score": "68.85629"}
{"text": "The translation cache 310 may be partitioned or replicated like resources such as language model and translation model servers .The translation cache 310 may store translations of different categories , e.g. , human translations and machine translations . [0062 ]", "label": "", "metadata": {}, "score": "68.873474"}
{"text": "P ?w?i?jV?w?i ? ?P ?V?w?i ? ?jH?w?i ? ?( 58 )Furthermore , using the category n - gram of equation ( 55 ) with n ? ? , we obtain from ( 58 ) : P ?", "label": "", "metadata": {}, "score": "68.92355"}
{"text": "P ?w?i?jV?w?i ? ?P ?V?w?i ? ?jH?w?i ? ?( 58 )Furthermore , using the category n - gram of equation ( 55 ) with n ? ? , we obtain from ( 58 ) : P ?", "label": "", "metadata": {}, "score": "68.92355"}
{"text": "The thesis work investigated these questions .In summary , word - based alignment model is a major cause of errors in German - English statistical spoken language ... . ... poken language understanding .This structure comprises the current topic , discourse state , and speech act , etc .", "label": "", "metadata": {}, "score": "68.931946"}
{"text": "In addition a given segment of ... . by Matthew A. Siegler - COMPUTER SCIENCE DEPARTMENT , CARNEGIE MELLON UNIVERSITY . \" ...Traditionally , indexing and searching of speech content in multimedia databases have been achieved through a combination of separately constructed speech recognition and information retrieval engines .", "label": "", "metadata": {}, "score": "69.019905"}
{"text": "K ? ? ?N. Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 19 .Using the decomposition formula ( 4 ) , the perplexity corresponds to : log?PP ? K i ? ?logPz?i?jz ? ? ?", "label": "", "metadata": {}, "score": "69.31438"}
{"text": "K ? ? ?N. Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 19 .Using the decomposition formula ( 4 ) , the perplexity corresponds to : log?PP ? K i ? ?logPz?i?jz ? ? ?", "label": "", "metadata": {}, "score": "69.31438"}
{"text": "Experi- ments encompass the adaptation of a language model obtained from a 1.9 million word corpus of radi- ological reports from a particular hospital , to a smaller set of reports obtained from another .It is found that MAP outperforms Bayes adaptation , but also that even better results are obtained from the optimal linear interpolation of language models built using the general and domain - specific samples directly .", "label": "", "metadata": {}, "score": "69.34562"}
{"text": "Experi- ments encompass the adaptation of a language model obtained from a 1.9 million word corpus of radi- ological reports from a particular hospital , to a smaller set of reports obtained from another .It is found that MAP outperforms Bayes adaptation , but also that even better results are obtained from the optimal linear interpolation of language models built using the general and domain - specific samples directly .", "label": "", "metadata": {}, "score": "69.34562"}
{"text": ".. hm for the construction of the NAB trigram language model is widely referred to as the \" Katz \" approach , and is described in [ 31].The run - time calculation of language model probabilities from a Katz trigram language model can be expressed as o .. ...", "label": "", "metadata": {}, "score": "69.36078"}
{"text": "A distributed representation is opposed to a local representation , in which only one neuron ( or very few ) is active at each time , i.e. , as with grandmother cells .One can view n - gram models as a mostly local representation : only the units associated with the specific subsequences of the input sequence are turned on .", "label": "", "metadata": {}, "score": "69.37645"}
{"text": "[ 0019 ]FIG .1 shows a distributed machine translation ( DMT ) system 100 to illustrate with specific examples the partition , replication and load balancing that can be realized using the distributed machine processing techniques described in this specification .", "label": "", "metadata": {}, "score": "69.38147"}
{"text": "As translation at the higher quality becomes available , the system can replace the lower - quality translations already delivered to the user with higher - quality translations .The previously translated page may be in part or entirely replaced in a dynamic manner when the higher quality translation is produced in the background .", "label": "", "metadata": {}, "score": "69.39485"}
{"text": "Another way to mitigate this problem is to replicate the same language model partition multiple times on different machines so that a different replica can be used for obtaining the language model data after there is a timeout for the initial server for the partition .", "label": "", "metadata": {}, "score": "69.40051"}
{"text": "7 illustrates one exemplary operation of a translation decoder .[ 0028 ] FIG .8 shows an example of a processing step of a translation decoder .[ 0029 ] FIGS .9A and 9B show an alternative operation of a translation decoder .", "label": "", "metadata": {}, "score": "69.40402"}
{"text": "8As introduced in section 2.2 .Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 22 .We begin by lettingrefer to the number of different events occurring exactly times in , so that : C r ? i ? ?", "label": "", "metadata": {}, "score": "69.41557"}
{"text": "8As introduced in section 2.2 .Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 22 .We begin by lettingrefer to the number of different events occurring exactly times in , so that : C r ? i ? ?", "label": "", "metadata": {}, "score": "69.41557"}
{"text": "12/092,820 filed on May 6 , 2008 , which is a National Stage of PCT International Application No .PCT / US2007/004196 filed on Feb. 16 , 2007 , which claims the benefit of U.S. Provisional Application No .60/775,570 filed on Feb. 21 , 2006 and U.S. Provisional Application No .", "label": "", "metadata": {}, "score": "69.44092"}
{"text": "G ? as a prior distribution , we may write the posterior distribution of ? as : P ? ?j ?D ? ?G ? ?P ? ?D j ? ? ?G ? ?P ? ?", "label": "", "metadata": {}, "score": "69.47006"}
{"text": "G ? as a prior distribution , we may write the posterior distribution of ? as : P ? ?j ?D ? ?G ? ?P ? ?D j ? ? ?G ? ?P ? ?", "label": "", "metadata": {}, "score": "69.47006"}
{"text": "Category - based language models ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?Word categories Automatic category membership determination Word groups and phrases ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "69.51844"}
{"text": "Category - based language models ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?Word categories Automatic category membership determination Word groups and phrases ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "69.51844"}
{"text": "w?i?Q ?w?i ? ? ?o Manyotherchoicesare possible , however , suchas syntactic word categoryidentities and semanticlabels for instance .The questions themselves concern the nature of the history w ? ? ?i ?Denote a particular question by the symbol the ?", "label": "", "metadata": {}, "score": "69.5577"}
{"text": "w?i?Q ?w?i ? ? ?o Manyotherchoicesare possible , however , suchas syntactic word categoryidentities and semanticlabels for instance .The questions themselves concern the nature of the history w ? ? ?i ?Denote a particular question by the symbol the ?", "label": "", "metadata": {}, "score": "69.5577"}
{"text": "Such analyses are often important as the first steps in discovering higher - level linguistic structure , for instance the identification of noun - phrases .Since words often have more than one possible part - of - speech assignment ( for instance \" light \" , which may act as adjective , verb or noun ) , this classi- fication may be ambiguous when considering only the lexical identity of the word .", "label": "", "metadata": {}, "score": "69.6121"}
{"text": "Such analyses are often important as the first steps in discovering higher - level linguistic structure , for instance the identification of noun - phrases .Since words often have more than one possible part - of - speech assignment ( for instance \" light \" , which may act as adjective , verb or noun ) , this classi- fication may be ambiguous when considering only the lexical identity of the word .", "label": "", "metadata": {}, "score": "69.6121"}
{"text": "INTRODUCTION In constructing a language model intended for general text , one is fac ... . \" ...Statistical language models used in large - vocabulary speech recognition must properly encapsulate the various constraints , both local and global , present in the language .", "label": "", "metadata": {}, "score": "69.67209"}
{"text": "Furthermore , the part - of - speech based model is found to have slightly lower perplexity than a word bigram , while employing fewer parameters .Larger improvements are obtained for a smaller German corpus of approximately 100,000 words , illustrating the improved generalisation of category - based models .", "label": "", "metadata": {}, "score": "69.67911"}
{"text": "Furthermore , the part - of - speech based model is found to have slightly lower perplexity than a word bigram , while employing fewer parameters .Larger improvements are obtained for a smaller German corpus of approximately 100,000 words , illustrating the improved generalisation of category - based models .", "label": "", "metadata": {}, "score": "69.67911"}
{"text": "0045 ] As an example of such implementations , FIG .1 shows a distributed machine translation ( DMT ) system 100 .Multiple translation front ends 110 , which may be computer servers , are arranged logically in parallel with one another and are used to receive with client requests for translating client documents 102 and deliver translated documents 103 to clients 101 .", "label": "", "metadata": {}, "score": "69.780426"}
{"text": "Language models are computational techniques and structures that describe word sequences produced by human subjects , and the work presented here considers primarily their application to automatic speech - recognition systems .Due to the very complex nature of natural languages as well as the need for robust recognition , statistically - based language models , which assign probabilities to word sequences , have proved most successful .", "label": "", "metadata": {}, "score": "69.806114"}
{"text": "Page 9 . A. Appendix : Major text corpora 102 B. Appendix : Leaving - one - out cross - validation103 C. Appendix : Dealing with unknown words 105 D. Appendix : Experimental corpora and baseline language models D.1 .Introduction 107 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "69.820206"}
{"text": "Page 9 . A. Appendix : Major text corpora 102 B. Appendix : Leaving - one - out cross - validation103 C. Appendix : Dealing with unknown words 105 D. Appendix : Experimental corpora and baseline language models D.1 .Introduction 107 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "69.820206"}
{"text": "j?f ? ? ? ? ? ? ? ?N H ? ?g , where N His the H ? ? ?N Hsubsets referred to collectively as H : H ?n h ?h ?h N H ? ?", "label": "", "metadata": {}, "score": "70.00852"}
{"text": "j?f ? ? ? ? ? ? ? ?N H ? ?g , where N His the H ? ? ?N Hsubsets referred to collectively as H : H ?n h ?h ?h N H ? ?", "label": "", "metadata": {}, "score": "70.00852"}
{"text": "However , since each n - tuple is treated independently , these models fail to capture general linguistic patterns ( such as the fact that adjectives do not normally precede verbs ) .Instead , they attempt to model each possible English n - tuple individually .", "label": "", "metadata": {}, "score": "70.03622"}
{"text": "However , since each n - tuple is treated independently , these models fail to capture general linguistic patterns ( such as the fact that adjectives do not normally precede verbs ) .Instead , they attempt to model each possible English n - tuple individually .", "label": "", "metadata": {}, "score": "70.03622"}
{"text": "K ? ? ? ?w ? w ?w?K ? is the sequence of K words in question , and the caret ?P ? w ? ? ?K ? ? ?K ? ?Y i ? ?P ?", "label": "", "metadata": {}, "score": "70.08086"}
{"text": "K ? ? ? ?w ? w ?w?K ? is the sequence of K words in question , and the caret ?P ? w ? ? ?K ? ? ?K ? ?Y i ? ?P ?", "label": "", "metadata": {}, "score": "70.08086"}
{"text": "The details of one or more embodiments of the described systems , techniques and apparatus are set forth in the accompanying drawings and the description below .Other features , aspects , and advantages associated with the described systems , techniques and apparatus will become apparent from the description , the drawings , and the claims .", "label": "", "metadata": {}, "score": "70.11665"}
{"text": "3A shows a translation cache 310 that is connected to and shared by the translation front ends 110 in a system of FIG .1 or 2 .The translations in the translation cache 310 can be used to provide a translation for a segment without going through the routine machine translation process at the segment translation server and the load balancing process .", "label": "", "metadata": {}, "score": "70.11913"}
{"text": "N tl ? ?the English language model , which P ? ?w t ? ? ?N tl ? jw s ? ? ?N sl ? ?Since human translators have been observed to translate almost four times more quickly when dictating than when having to write or type their translations , the integration of such a translation system into a speech - recogniserhas beeninvestigatedin [ 9].", "label": "", "metadata": {}, "score": "70.128136"}
{"text": "N tl ? ?the English language model , which P ? ?w t ? ? ?N tl ? jw s ? ? ?N sl ? ?Since human translators have been observed to translate almost four times more quickly when dictating than when having to write or type their translations , the integration of such a translation system into a speech - recogniserhas beeninvestigatedin [ 9].", "label": "", "metadata": {}, "score": "70.128136"}
{"text": "A load balancing mechanism may be implemented to control the translation tasks of different segments of a client document or different client documents based on one or more considerations , such as the quality and timing requirements and constraints .In this example , the load balancing mechanism , although its name still suggesting some \" load \" balancing operations , does balance something that is not necessarily the work load of different machines .", "label": "", "metadata": {}, "score": "70.217514"}
{"text": "r ? rrespectively .w ? ? ?i ? ? ? , the tree is traversed from its root to one of the ? rposed at each node t ron this path .The leaves of l j ?h j ?", "label": "", "metadata": {}, "score": "70.239624"}
{"text": "r ? rrespectively .w ? ? ?i ? ? ? , the tree is traversed from its root to one of the ? rposed at each node t ron this path .The leaves of l j ?h j ?", "label": "", "metadata": {}, "score": "70.239624"}
{"text": "During this period , the segment translation server 130 uses language data from another available translation resource ( e.g. , the resource server 230 ( FIG .2 ) ) to process translation of the segment ( Step 1161 ) .The segment translation server 130 continues the processing with the data from the other available translation resource until all requested LM data is available ( Step 1162 ) .", "label": "", "metadata": {}, "score": "70.25599"}
{"text": "j ? ?D if if N ? ?i j ?j ? ? ?N ? ?i j ?j ? ? ?where ? ?D ? ?With this choice , the probability mass Q dmay be found from ( 15 ) and ( 44 ) to be : Q d ?", "label": "", "metadata": {}, "score": "70.302155"}
{"text": "j ? ?D if if N ? ?i j ?j ? ? ?N ? ?i j ?j ? ? ?where ? ?D ? ?With this choice , the probability mass Q dmay be found from ( 15 ) and ( 44 ) to be : Q d ?", "label": "", "metadata": {}, "score": "70.302155"}
{"text": "The processing server is a translation server operable to translate a text in a source language in the input into the target language using the obtained data from the language model .[0008 ]In another aspect , a system for machine translation can include machine translation resource servers , and at least one translation server .", "label": "", "metadata": {}, "score": "70.30512"}
{"text": "There can be a variety of load balancing policies .There can be more than one type of translation engines ( e.g. , the segment translation servers 130 ) in the system , and this provides additional flexibility to the load balancing .", "label": "", "metadata": {}, "score": "70.33078"}
{"text": "The system 1310 can serve multiple clients at the same time .[0169 ] While this specification contains many specifics , these should not be construed as limitations on the scope of what being claims or of what may be claimed , but rather as descriptions of features specific to particular embodiments .", "label": "", "metadata": {}, "score": "70.33489"}
{"text": "The total number of events in may now be expressed as : N ?i ? ?N ? ?i ? where N ? ?i ? is the number of times ? ioccurs in ? , and we may have N ? ?", "label": "", "metadata": {}, "score": "70.37875"}
{"text": "The total number of events in may now be expressed as : N ?i ? ?N ? ?i ? where N ? ?i ? is the number of times ? ioccurs in ? , and we may have N ? ?", "label": "", "metadata": {}, "score": "70.37875"}
{"text": "v?i?n ? ? ? ? ? ? ? ?v?i ?o ( 55 ) from which we obtain category - based n - gram language models .Note that equation ( 55 ) represents a many - to - many mapping when the operator V ? ? ? is one - to - many .", "label": "", "metadata": {}, "score": "70.383606"}
{"text": "v?i?n ? ? ? ? ? ? ? ?v?i ?o ( 55 ) from which we obtain category - based n - gram language models .Note that equation ( 55 ) represents a many - to - many mapping when the operator V ? ? ? is one - to - many .", "label": "", "metadata": {}, "score": "70.383606"}
{"text": "The received LM data is placed in the high - level cache and low level cache ( step 880 ) and the translation is completed ( step 830 ) .One example for such a language model resource is a second language model that is smaller than the language model stored on the language model servers 210 , for example .", "label": "", "metadata": {}, "score": "70.45084"}
{"text": "[0010 ]In another aspect , a method for machine translation can divide a collection of machine language translation resource data for translation from a source language to a target language into partitions each being less than the collection of machine language translation resource data .", "label": "", "metadata": {}, "score": "70.466934"}
{"text": "z ? ? ?K ? ? ?It has become customary in language modelling applications to use the perplexity PP as a measure of model quality .P ?z ? ? ?K ? ? ?K ( 6 )", "label": "", "metadata": {}, "score": "70.54"}
{"text": "z ? ? ?K ? ? ?It has become customary in language modelling applications to use the perplexity PP as a measure of model quality .P ?z ? ? ?K ? ? ?K ( 6 )", "label": "", "metadata": {}, "score": "70.54"}
{"text": "NLP researchers face a dilemma : on one side , it is unarguably accepted that languages have internal structure rather than strings of words .On the other side , they find it very difficult and expensive to write grammars that have good coverage of language structures .", "label": "", "metadata": {}, "score": "70.55854"}
{"text": "The local cache 630 and the LM lookup request queue 620 can operate in combination to process each segment efficiently .Different techniques can be used to operate the decoder 610 and the queue 620 .Two examples are described below .", "label": "", "metadata": {}, "score": "70.57717"}
{"text": "N ?X i ? ?E n ?N ? ?i ? ?r ?o where the subscript of the expectation operator denotes the sample size .Note furthermore that : E N ?n ?N ? ?", "label": "", "metadata": {}, "score": "70.63906"}
{"text": "N ?X i ? ?E n ?N ? ?i ? ?r ?o where the subscript of the expectation operator denotes the sample size .Note furthermore that : E N ?n ?N ? ?", "label": "", "metadata": {}, "score": "70.63906"}
{"text": "G ?R ?P ? ?D j ? ? ?G ? ?P ? ?j ?G ?The study in [ 22 ] compares the maximum a - posteriori ( MAP ) estimate : ? MAP ? arg max ?", "label": "", "metadata": {}, "score": "70.658165"}
{"text": "G ?R ?P ? ?D j ? ? ?G ? ?P ? ?j ?G ?The study in [ 22 ] compares the maximum a - posteriori ( MAP ) estimate : ? MAP ? arg max ?", "label": "", "metadata": {}, "score": "70.658165"}
{"text": "Traditionally , indexing and searching of speech content in multimedia databases have been achieved through a combination of separately constructed speech recognition and information retrieval engines .Although each technology has a legacy of research , only recently have efforts been made to study the potential suboptimality of this strategy , and none of these efforts specifically addresses the presence of uncertainty in automatically generated transcriptions .", "label": "", "metadata": {}, "score": "70.68955"}
{"text": "In a system with the redundancy of replication , if one machine for a replicated item fails , one or more other replicated machines for the item can be made available to replace the failed machine and thus reduce the effect caused by the machine failure to the system .", "label": "", "metadata": {}, "score": "70.699814"}
{"text": "P ? vjw ? ? ?i ?( 53 )Furthermore , classifying the history into equivalence classes , we find from equation ( 11 ) : P ?v j jw ? ? ?i ? ? ?X ?", "label": "", "metadata": {}, "score": "70.76508"}
{"text": "P ? vjw ? ? ?i ?( 53 )Furthermore , classifying the history into equivalence classes , we find from equation ( 11 ) : P ?v j jw ? ? ?i ? ? ?X ?", "label": "", "metadata": {}, "score": "70.76508"}
{"text": "c j ?P?w?i?jw?i?j ?the function to be maximised is : F?c ?c ?c p ? ?N ?N X i ? ? log ?p X j ? ?c j ?P ? w?i?jw?i?j ?p X j ? ?", "label": "", "metadata": {}, "score": "70.7816"}
{"text": "c j ?P?w?i?jw?i?j ?the function to be maximised is : F?c ?c ?c p ? ?N ?N X i ? ? log ?p X j ? ?c j ?P ? w?i?jw?i?j ?p X j ? ?", "label": "", "metadata": {}, "score": "70.7816"}
{"text": "for some r in which case C rbefore application r since sparse counts are more r [ 41].In particular , when r exceeds the threshold k , the use of the maximum q r ?r N ? r?k Requiring that the probability estimate of unseen events remain choice of C ?", "label": "", "metadata": {}, "score": "70.826126"}
{"text": "for some r in which case C rbefore application r since sparse counts are more r [ 41].In particular , when r exceeds the threshold k , the use of the maximum q r ?r N ? r?k Requiring that the probability estimate of unseen events remain choice of C ?", "label": "", "metadata": {}, "score": "70.826126"}
{"text": "Chapter 2 Overview of language modelling techniques This chapter introduces the major statistical language modelling concepts and approaches .Appendix A givesabriefsummaryofthetextcorporaforwhichexperimentalresultsareencounteredmostfrequently , and these will be referred to simply by name in the following .However , less frequent corpora will be described individually as they become relevant .", "label": "", "metadata": {}, "score": "70.8808"}
{"text": "Chapter 2 Overview of language modelling techniques This chapter introduces the major statistical language modelling concepts and approaches .Appendix A givesabriefsummaryofthetextcorporaforwhichexperimentalresultsareencounteredmostfrequently , and these will be referred to simply by name in the following .However , less frequent corpora will be described individually as they become relevant .", "label": "", "metadata": {}, "score": "70.8808"}
{"text": "A load balancing module 120 can also be included and is operable to , based on translation load at the segment translation servers 130 , selectively assign the segments to one or more of the segment translation servers 130 for translation .", "label": "", "metadata": {}, "score": "70.88267"}
{"text": "Such variation can be caused by , e.g. , varying number of requests , requests of varying degrees of difficulty , varying proportions of requests for different language pairs , etc .This section describes features in automated machine translation systems to handle varying amounts of load caused by these and other variations in the systems and to reduce degradation in the quality of the service .", "label": "", "metadata": {}, "score": "70.98411"}
{"text": "( 30 )If we require the unseen probability to correspond to that delivered by the Good - Turing estimate ( 24 ) , we obtain from ( 29 ) and ( 30 ) : C ?N ?N ?X r ? ?", "label": "", "metadata": {}, "score": "71.047455"}
{"text": "( 30 )If we require the unseen probability to correspond to that delivered by the Good - Turing estimate ( 24 ) , we obtain from ( 29 ) and ( 30 ) : C ?N ?N ?X r ? ?", "label": "", "metadata": {}, "score": "71.047455"}
{"text": "X j ? ?P ? h j jw ? ? ?i ? ? ?w ? ? ?i ? ? ?Note that equation ( 11 ) reduces to equation ( 10 ) when w ? ? ?i ? is many - to - one , since then P define : ? hjw ? ? ?", "label": "", "metadata": {}, "score": "71.05449"}
{"text": "X j ? ?P ? h j jw ? ? ?i ? ? ?w ? ? ?i ? ? ?Note that equation ( 11 ) reduces to equation ( 10 ) when w ? ? ?i ? is many - to - one , since then P define : ? hjw ? ? ?", "label": "", "metadata": {}, "score": "71.05449"}
{"text": "Word - to - category backoff model ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?Category model with long - range correlations ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "71.21435"}
{"text": "Word - to - category backoff model ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?Category model with long - range correlations ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "71.21435"}
{"text": "Each language model server stores and is operable to serve a partition of a language model for the target natural language and the respective partitions together constitute the entire language model .The translation server is operable to perform machine translation , obtaining translation model data from the translation model server and obtaining language model data from language model servers .", "label": "", "metadata": {}, "score": "71.222916"}
{"text": "The possible use of the model for perplexity reduction in a continuous speech recognition system is also demonstrated .To achieve improvement over a state independent bigram language model , great care must be taken to keep the number of model parameters small in the face of limited amounts of training data from transcribed spontaneous speech .", "label": "", "metadata": {}, "score": "71.23193"}
{"text": "N , so that we may estimate the mutual information as : I m ? x i ? x j ? log ?N?x i ? x j ? ?N N?x i ? ?N?x j ?( 61 ) 15Mutual information will be defined in section 2.5.3 .", "label": "", "metadata": {}, "score": "71.257935"}
{"text": "N , so that we may estimate the mutual information as : I m ? x i ? x j ? log ?N?x i ? x j ? ?N N?x i ? ?N?x j ?( 61 ) 15Mutual information will be defined in section 2.5.3 .", "label": "", "metadata": {}, "score": "71.257935"}
{"text": "The segment translation server 130 then generates requests for all possible n grams in the target language for each possible translation and associated statistical data from the language model ( Step 1120 ) .[ 0083 ]The segment translation server 130 determines whether the requested LM data is in the segment translation server 130 ( Step 1130 ) .", "label": "", "metadata": {}, "score": "71.303024"}
{"text": "N?x i ? x j ?N ? ? ? ? ?P?x i ? ?N?x i ?N ? ? ?P?x j ? ?N?x j ?N ? ? ?For a large corpus , N ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "71.338776"}
{"text": "N?x i ? x j ?N ? ? ? ? ?P?x i ? ?N?x i ?N ? ? ?P?x j ? ?N?x j ?N ? ? ?For a large corpus , N ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "71.338776"}
{"text": "Consider a population practice we obtain a subset the population , i.e. which will be denoted by space to correspond exactly to the outcomes of the measurements6 .From this we note that all events in ? of measurements taken from all possible text in the language of interest .", "label": "", "metadata": {}, "score": "71.352554"}
{"text": "Consider a population practice we obtain a subset the population , i.e. which will be denoted by space to correspond exactly to the outcomes of the measurements6 .From this we note that all events in ? of measurements taken from all possible text in the language of interest .", "label": "", "metadata": {}, "score": "71.352554"}
{"text": "3A , 4 and 5 .The database of manual translations can then be used in an automated machine translation system .Each stored manual translation can be labeled as a high translation quality so that a system , e.g. , the system of FIG .", "label": "", "metadata": {}, "score": "71.383644"}
{"text": "The implementation shown in FIGS .11A and 11B is one such example where the translation using the other resources and data is combined with translation with the SMT processing with the language and translation models .[ 0057 ] The system 200 is one example of a MT system using language and translation models .", "label": "", "metadata": {}, "score": "71.5671"}
{"text": "N tl ?w t ? ? ?N tl ?P ? x t ? ? ?T ? ?w s ? ? ?N sl ? ? ?w t ? ? ?N tl ? where x tis the acoustic observation in the target language , obtained from the speech of the dictation .", "label": "", "metadata": {}, "score": "71.654396"}
{"text": "N tl ?w t ? ? ?N tl ?P ? x t ? ? ?T ? ?w s ? ? ?N sl ? ? ?w t ? ? ?N tl ? where x tis the acoustic observation in the target language , obtained from the speech of the dictation .", "label": "", "metadata": {}, "score": "71.654396"}
{"text": "12 shows an example of a distributed processing system that can be configured to provide a language processing function based on a large language model .[ 0033 ] FIG .13 shows an example computer system in a communication network that provides distributed processing .", "label": "", "metadata": {}, "score": "71.69208"}
{"text": "It can be shown that , for an ergodic source , it is always true that : ? h ? ?h Due to the ergodicity of the source , the analysis of any sufficiently long sequence results in the same entropy estimate , thus : lim K ? ?", "label": "", "metadata": {}, "score": "71.835464"}
{"text": "It can be shown that , for an ergodic source , it is always true that : ? h ? ?h Due to the ergodicity of the source , the analysis of any sufficiently long sequence results in the same entropy estimate , thus : lim K ? ?", "label": "", "metadata": {}, "score": "71.835464"}
{"text": "The entire disclosures of each of the above applications are incorporated herein by reference .BACKGROUND .[0002 ] The specification of this application relates to machine processing using machines such as computers to perform processing tasks such as machine translation .", "label": "", "metadata": {}, "score": "71.86603"}
{"text": "Replication can be used to increase the availability or the capacity for a function of the item being replicated , reduce the latency or delay in accessing a function of the item being replicated , and provide redundancy for a function of the item .", "label": "", "metadata": {}, "score": "71.96428"}
{"text": "N ?r ? ?r ?N ?X i ? ?p r ? ?i ?p i ?N ?r ? ?N ?r ? ?P ? ?r ?( 20 )Now , from equations ( 18 ) and ( 19 ) : NW?r ? ? ?", "label": "", "metadata": {}, "score": "71.99563"}
{"text": "N ?r ? ?r ?N ?X i ? ?p r ? ?i ?p i ?N ?r ? ?N ?r ? ?P ? ?r ?( 20 )Now , from equations ( 18 ) and ( 19 ) : NW?r ? ? ?", "label": "", "metadata": {}, "score": "71.99563"}
{"text": "The combined probability estimate of of ( 68 ) and ( 69 ) : w?i ? , giventhat it belongs to v j , is obtained by linear combination P ? i?jv j ?j ?P s ?w?i?jv j ?", "label": "", "metadata": {}, "score": "72.24716"}
{"text": "The combined probability estimate of of ( 68 ) and ( 69 ) : w?i ? , giventhat it belongs to v j , is obtained by linear combination P ? i?jv j ?j ?P s ?w?i?jv j ?", "label": "", "metadata": {}, "score": "72.24716"}
{"text": "When this mapping is many- Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 34 .Now assume that the probability of witnessing a word category to which it belongs , so that we may write : is completely defined by a knowledge of the P ?", "label": "", "metadata": {}, "score": "72.29219"}
{"text": "When this mapping is many- Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 34 .Now assume that the probability of witnessing a word category to which it belongs , so that we may write : is completely defined by a knowledge of the P ?", "label": "", "metadata": {}, "score": "72.29219"}
{"text": "Because of the categorical nature of language , and the large vocabularies people naturally use , statistical techniques must estimate a large number of parameters , and consequently depend critically on the availability of large amounts of training data . ... one to compare and perfect smoothing techniques under various conditions .", "label": "", "metadata": {}, "score": "72.333206"}
{"text": "A machine translation server is operated to access and use the collection of machine language translation resource data on the different computer servers to perform translation from the source language into the target language .[ 0011 ]In another aspect , a method is described for machine translation of text from a source language into a target language using a translation model for translation between the source language and the target language and a language model for the target language .", "label": "", "metadata": {}, "score": "72.34151"}
{"text": "Probabilistic framework 5.3.1 .Estimating 5.3.2 .Estimating 5.3.3 .Typical estimates ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "72.45387"}
{"text": "Probabilistic framework 5.3.1 .Estimating 5.3.2 .Estimating 5.3.3 .Typical estimates ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "72.45387"}
{"text": "Much of the previous work has been carried out to model natural languages , such as French or English , with varied characteristics and singularities .\" [ Show abstract ] [ Hide abstract ] ABSTRACT :In this work , we present an extension of n - gram - based translation models based on factored language models ( FLMs ) .", "label": "", "metadata": {}, "score": "72.48242"}
{"text": "Much of the previous work has been carried out to model natural languages , such as French or English , with varied characteristics and singularities .\" [ Show abstract ] [ Hide abstract ] ABSTRACT :In this work , we present an extension of n - gram - based translation models based on factored language models ( FLMs ) .", "label": "", "metadata": {}, "score": "72.48242"}
{"text": "j ? ?N p ? ? ? such that the constraints ( 42 ) are satisfied .Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 30 .Although each distinct context generally has different smoothing parameters , the limited quantity of training data may require them to be tied appropriately .", "label": "", "metadata": {}, "score": "72.49727"}
{"text": "j ? ?N p ? ? ? such that the constraints ( 42 ) are satisfied .Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 30 .Although each distinct context generally has different smoothing parameters , the limited quantity of training data may require them to be tied appropriately .", "label": "", "metadata": {}, "score": "72.49727"}
{"text": "In addition , contents and other features of the analyzed documents can also be analyzed to identify additional candidates for manual translation .Examples include newspaper headlines , titles and subtitles in articles , frequently used words and phrases from Web searches , and important navigational phrases from Web pages .", "label": "", "metadata": {}, "score": "72.620605"}
{"text": "The load balancing component maintains some information about the overall system load and the load of the segment translation servers .Using this information as well as information about the available capacity , the load balancing component applies a load balancing policy to determine how to process the section translation requests .", "label": "", "metadata": {}, "score": "72.655106"}
{"text": "i ? ? ? ?N ?w?i?n ? ? ?i ? where operator N His here the number of distinct ?n ? ? ?-tuples in the training corpus .Furthermore , the mapping H ? ? ? is many - to - one and defined by : H?w?i ? ? ?", "label": "", "metadata": {}, "score": "72.81627"}
{"text": "i ? ? ? ?N ?w?i?n ? ? ?i ? where operator N His here the number of distinct ?n ? ? ?-tuples in the training corpus .Furthermore , the mapping H ? ? ? is many - to - one and defined by : H?w?i ? ? ?", "label": "", "metadata": {}, "score": "72.81627"}
{"text": "0168 ]FIG .13 shows an example computer system in a communication network that provides distributed processing .This system includes a communication network 1300 that enables communications for communication devices connected to the network 1300 , such as computers .", "label": "", "metadata": {}, "score": "72.8242"}
{"text": "j ? def ?P u ? ?j ?j ?( 38 ) so that from ( 35 ) , ( 37 ) and ( 38 ) we find : ? ? ?j ? ?P ? i ?", "label": "", "metadata": {}, "score": "72.89061"}
{"text": "j ? def ?P u ? ?j ?j ?( 38 ) so that from ( 35 ) , ( 37 ) and ( 38 ) we find : ? ? ?j ? ?P ? i ?", "label": "", "metadata": {}, "score": "72.89061"}
{"text": "Language model requests to different machines can thus return at different points in time .The Wait ( ) operations used in the segment translation servers have to wait for the slowest partition .If that slowest machine has a problem that can not be corrected quickly , e.g. , lost power or a network problem , the wait time can be prolonged and unacceptable .", "label": "", "metadata": {}, "score": "72.91837"}
{"text": "i ? ?r ?N ?r ?p i ? ? ? ?p i ?N ?r where for notational convenience we have defined p i ?P ? ?i ?Now , from ( 14 ) , note that : r E N ?", "label": "", "metadata": {}, "score": "73.00929"}
{"text": "i ? ?r ?N ?r ?p i ? ? ? ?p i ?N ?r where for notational convenience we have defined p i ?P ? ?i ?Now , from ( 14 ) , note that : r E N ?", "label": "", "metadata": {}, "score": "73.00929"}
{"text": "X j ? ?j ?N p ? ? ?P ? i j ?j ?( 41 ) where ?N p ? ? ? is assumed to correspond to the most refined history equivalence class , and : ? j ?", "label": "", "metadata": {}, "score": "73.012436"}
{"text": "X j ? ?j ?N p ? ? ?P ? i j ?j ?( 41 ) where ?N p ? ? ? is assumed to correspond to the most refined history equivalence class , and : ? j ?", "label": "", "metadata": {}, "score": "73.012436"}
{"text": "As opposed to Turing 's formula ( 22 ) , which is sensitive to sparse robust since it guarantees that the designer and independent of the counts obtained from the training set .r C rcounts , the estimate ( 28 ) is very ? ? q r ? ? as long as d r ?", "label": "", "metadata": {}, "score": "73.18043"}
{"text": "As opposed to Turing 's formula ( 22 ) , which is sensitive to sparse robust since it guarantees that the designer and independent of the counts obtained from the training set .r C rcounts , the estimate ( 28 ) is very ? ? q r ? ? as long as d r ?", "label": "", "metadata": {}, "score": "73.18043"}
{"text": "A second load balancing module is also included to assign the segments to one or more of the second segment translation servers for translation based on translation load at the second segment translation servers .The language model servers 210 for storing and serving the target language model can be shared by the segment translation servers 130 for translating the source language and the second segment translation servers for translating the second source language .", "label": "", "metadata": {}, "score": "73.29549"}
{"text": "At the other end of the spectrum , it is possible to consider the entire sentence , as opposed to j ..Abstract for niesler_thesis .PhD Thesis , Cambridge University .CATEGORY - BASED STATISTICAL LANGUAGE MODELS .Thomas Niesler .", "label": "", "metadata": {}, "score": "73.3042"}
{"text": "z ? ? ?K ?The corresponding figure obtained from the probability estimate is : ? P ?z ? ? ?K ? ? ?When the approximation is perfect , mation ?P ? ? ? ?P ? ? ? and therefore ? ? ? ?", "label": "", "metadata": {}, "score": "73.32226"}
{"text": "z ? ? ?K ?The corresponding figure obtained from the probability estimate is : ? P ?z ? ? ?K ? ? ?When the approximation is perfect , mation ?P ? ? ? ?P ? ? ? and therefore ? ? ? ?", "label": "", "metadata": {}, "score": "73.32226"}
{"text": "61 Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 7 .Word - to - category backoff models62 4.1 .Introduction ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "73.40756"}
{"text": "61 Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 7 .Word - to - category backoff models62 4.1 .Introduction ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "73.40756"}
{"text": "This paper presents an overview of the major approaches proposed to address this issue , and offers some perspectives regarding their comparative merits and associated tradeoffs .\u00d3 2003 Elsevier B.V. All rights reserved .For example , the interpolation can be performed at the sentence rather than the word level ( Iyer et al . , 1994 ) .", "label": "", "metadata": {}, "score": "73.47432"}
{"text": "Computer servers 210 are used to store the target language model and computer servers 220 are used to store the source - target translation model .In some implementations , a single translation model server 210 may be sufficient to store and serve the entire translation model .", "label": "", "metadata": {}, "score": "73.50128"}
{"text": "Generally , a computer will also include , or be operatively coupled to receive data from or transfer data to , or both , one or more mass storage devices for storing data , e.g. , magnetic , magneto optical disks , or optical disks .", "label": "", "metadata": {}, "score": "73.503075"}
{"text": "T ? ? ? be a parameterisation arg max w ? ? ? ?w ? ? ? ? ? ? ?w?K ?P ? x ? ? ?T?jw ? ? ?K ?P ? w ? ? ?", "label": "", "metadata": {}, "score": "73.53627"}
{"text": "T ? ? ? be a parameterisation arg max w ? ? ? ?w ? ? ? ? ? ? ?w?K ?P ? x ? ? ?T?jw ? ? ?K ?P ? w ? ? ?", "label": "", "metadata": {}, "score": "73.53627"}
{"text": "Probability estimation from sparse data ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?The Good - Turing estimate Deleted estimation Discounting methods 2.3.3.1 .", "label": "", "metadata": {}, "score": "73.66457"}
{"text": "Probability estimation from sparse data ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?The Good - Turing estimate Deleted estimation Discounting methods 2.3.3.1 .", "label": "", "metadata": {}, "score": "73.66457"}
{"text": "P ? ?i j ? k ?It is more convenient to calculate : ? ? ? ?j ? ? ? ?X ?i ?N ? ?i j ?j ? ? ?P ? ?i j ? k ?", "label": "", "metadata": {}, "score": "73.66935"}
{"text": "P ? ?i j ? k ?It is more convenient to calculate : ? ? ? ?j ? ? ? ?X ?i ?N ? ?i j ?j ? ? ?P ? ?i j ? k ?", "label": "", "metadata": {}, "score": "73.66935"}
{"text": "As examples , bigram language models H?w?i ? ?def ?w?i ? ? ?i ? ? ? ?fw?i ? g ( 12 ) and trigram language models define : H?w?i ? ?def ?w?i ? ? ?i ? ? ? ?", "label": "", "metadata": {}, "score": "73.728874"}
{"text": "As examples , bigram language models H?w?i ? ?def ?w?i ? ? ?i ? ? ? ?fw?i ? g ( 12 ) and trigram language models define : H?w?i ? ?def ?w?i ? ? ?i ? ? ? ?", "label": "", "metadata": {}, "score": "73.728874"}
{"text": "Speech recognition performance is severely affected when the lexical , syntactic , or semantic characteristics of the discourse in the training and recognition tasks differ .The aim of language model adaptation is to exploit specific , albeit limited , knowledge about the recognition task to compensate for this mismatch .", "label": "", "metadata": {}, "score": "73.77805"}
{"text": "Speech recognition performance is severely affected when the lexical , syntactic , or semantic characteristics of the discourse in the training and recognition tasks differ .The aim of language model adaptation is to exploit specific , albeit limited , knowledge about the recognition task to compensate for this mismatch .", "label": "", "metadata": {}, "score": "73.77805"}
{"text": "Page 14 .Scope of this thesis A basic assumption made in this thesis is that one may classify patterns found in language in the follow- ing manner : 1 .Syntactic patterns , which refer to aspects of text structure imposed by grammatical constraints , for instance the phenomenon that adjectives are often followed by nouns .", "label": "", "metadata": {}, "score": "73.800674"}
{"text": "Page 14 .Scope of this thesis A basic assumption made in this thesis is that one may classify patterns found in language in the follow- ing manner : 1 .Syntactic patterns , which refer to aspects of text structure imposed by grammatical constraints , for instance the phenomenon that adjectives are often followed by nouns .", "label": "", "metadata": {}, "score": "73.800674"}
{"text": "For an existing translation in the translation cache 310 , the translation front end 110 also determines whether the quality of the translation is satisfactory ( step 344 ) .This can be done , for example , by using the quality information stored in the translation cache 310 .", "label": "", "metadata": {}, "score": "73.876236"}
{"text": "The CMU Statistical Language Modeling toolkit was released in 1994 in order to facilitate the construction and testing of bigram and trigram language models .It is currently in use in over 40 academic , government and industrial laboratories in over 12 countries .", "label": "", "metadata": {}, "score": "73.916824"}
{"text": "The CMU Statistical Language Modeling toolkit was released in 1994 in order to facilitate the construction and testing of bigram and trigram language models .It is currently in use in over 40 academic , government and industrial laboratories in over 12 countries .", "label": "", "metadata": {}, "score": "73.916824"}
{"text": "This system 200 uses a statistical machine translation ( SMT ) technique to perform translation from a source natural language into a target natural language based on a source - target translation model and a target language model .The source language and the target language may be two different natural languages such as Chinese and English in many translation applications .", "label": "", "metadata": {}, "score": "74.08914"}
{"text": "The following sections introduce a formal notation for dealing with word categories and describe how these may be applied to the development of language models .Word categories A category will be taken to refer to any grouping of words .Let there be them by : N vsuch categories and denote V?fv ?", "label": "", "metadata": {}, "score": "74.13998"}
{"text": "The following sections introduce a formal notation for dealing with word categories and describe how these may be applied to the development of language models .Word categories A category will be taken to refer to any grouping of words .Let there be them by : N vsuch categories and denote V?fv ?", "label": "", "metadata": {}, "score": "74.13998"}
{"text": "The mapping information can include a relation between ( 1 ) one or more tokens in the source language and ( 2 ) one or more tokens in the target language .In one implementation , for example , the mapping information between the source language and the target language is all possible pairs of language strings between the target and source languages .", "label": "", "metadata": {}, "score": "74.27504"}
{"text": "The current principal ob- jective is the developmentof large - vocabulary recognisers for natural , unconstrained , connectedspeech .Despite consistent progress , considerable advances are still necessary before widespread industrial ap- plication becomes feasible .However , the very broad spectrum of potential applications1will ensure that this technology will gain extreme importance once it matures .", "label": "", "metadata": {}, "score": "74.34686"}
{"text": "The current principal ob- jective is the developmentof large - vocabulary recognisers for natural , unconstrained , connectedspeech .Despite consistent progress , considerable advances are still necessary before widespread industrial ap- plication becomes feasible .However , the very broad spectrum of potential applications1will ensure that this technology will gain extreme importance once it matures .", "label": "", "metadata": {}, "score": "74.34686"}
{"text": "The method of claim 32 , wherein : the different translation resource is a second language model for the target language that is smaller than the language model .Description : .CROSS -REFERENCE TO RELATED APPLICATIONS .[ 0001 ] This application is a continuation of U.S. patent application Ser .", "label": "", "metadata": {}, "score": "74.364815"}
{"text": "The front end receives text or document translation requests .The text or document to be translated is divided into sections or segments .The translation for a segment can be looked up in the translation cache , which may yield a translation of a certain translation quality .", "label": "", "metadata": {}, "score": "74.40509"}
{"text": "Statistical language modeling is crucial for a large variety of language technology applications .These include speech recognition ( where SLM got its start ) , machine translation , document classification and routing , optical character recognition , information retrieval , handwriting recognition , spelling correction , and many more .", "label": "", "metadata": {}, "score": "74.50021"}
{"text": "Statistical language modeling is crucial for a large variety of language technology applications .These include speech recognition ( where SLM got its start ) , machine translation , document classification and routing , optical character recognition , information retrieval , handwriting recognition , spelling correction , and many more .", "label": "", "metadata": {}, "score": "74.50021"}
{"text": "Statistical language modeling is crucial for a large variety of language technology applications .These include speech recognition ( where SLM got its start ) , machine translation , document classification and routing , optical character recognition , information retrieval , handwriting recognition , spelling correction , and many more .", "label": "", "metadata": {}, "score": "74.50021"}
{"text": "The system of claim 11 , wherein : the machine translation resource server which serves the other machine translation resource data stores a second , different language model for the target language .The system of claim 13 , wherein : the segment translation server cache is operable to delete the obtained language model data after the segment is translated .", "label": "", "metadata": {}, "score": "74.794205"}
{"text": "Accordingly , the segment translation server 130 completes the translation of that segment based on the language model data ( Step 950 ) .[0081 ] FIG .10 shows an example operation of a segment translation server 130 having only a high - level cache 631 without a low - level cache 632 .", "label": "", "metadata": {}, "score": "74.83026"}
{"text": "C C r ? q q r ? ? and solve for C ? q ? as follows : C ? q ?X r ? ?C r ?d r N ?N ?X r ? ?C r ?", "label": "", "metadata": {}, "score": "74.914566"}
{"text": "C C r ? q q r ? ? and solve for C ? q ? as follows : C ? q ?X r ? ?C r ?d r N ?N ?X r ? ?C r ?", "label": "", "metadata": {}, "score": "74.914566"}
{"text": "Page 48 .One may assume thatis independent of the text of the source language , i.e. : Px t jw s ? ? ?N sl ? ? ?w t ? ? ?N tl ? ?Px t j?w t ? ? ?", "label": "", "metadata": {}, "score": "74.98667"}
{"text": "Page 48 .One may assume thatis independent of the text of the source language , i.e. : Px t jw s ? ? ?N sl ? ? ?w t ? ? ?N tl ? ?Px t j?w t ? ? ?", "label": "", "metadata": {}, "score": "74.98667"}
{"text": "Absolute discounting 2.3.4 .Backing off to less refined distributions 2.3.5 .Deleted interpolation 2.3.6 .Modified absolute discounting ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "75.138596"}
{"text": "Absolute discounting 2.3.4 .Backing off to less refined distributions 2.3.5 .Deleted interpolation 2.3.6 .Modified absolute discounting ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "75.138596"}
{"text": "Claims : .The system of claim 1 , wherein : the segment translation server cache is operable to store the at least part of the obtained machine translation resource data during a period when an assigned segment is being translated by each segment translation server .", "label": "", "metadata": {}, "score": "75.1692"}
{"text": "[ 0161 ] FIG .12 shows an example of a distributed processing system 1200 that can be configured to provide a language processing function based on a large language model .The processing function of the system 1200 may be different from machine translation .", "label": "", "metadata": {}, "score": "75.19417"}
{"text": "The output 1202 is text that is a transcription of the input speech 1201 .The processing servers 1230 can also be implemented as OCR servers for OCR applications , spelling correction servers for spelling correction applications , and spam filtering servers for spam filtering applications .", "label": "", "metadata": {}, "score": "75.23655"}
{"text": "Q d ?P ? ?i j ? k ? ? ?Q d ?N ?j ?N ?X i ? ?d ? ?i j ?j ?( 44 )Now choose the discounting function : d ? ?", "label": "", "metadata": {}, "score": "75.24417"}
{"text": "Q d ?P ? ?i j ? k ? ? ?Q d ?N ?j ?N ?X i ? ?d ? ?i j ?j ?( 44 )Now choose the discounting function : d ? ?", "label": "", "metadata": {}, "score": "75.24417"}
{"text": "The following general discounting function is postulated to achieve this : P ? ?i j ?j ?i j ?j ? ?d ? ?i j ?j ?N ?j ?Q d ?P ? ?", "label": "", "metadata": {}, "score": "75.65694"}
{"text": "The following general discounting function is postulated to achieve this : P ? ?i j ?j ?i j ?j ? ?d ? ?i j ?j ?N ?j ?Q d ?P ? ?", "label": "", "metadata": {}, "score": "75.65694"}
{"text": "r ?Y r R P k ? ?Now assume D disjoint sets , and the quantity Y rfor the case where the i thsuch Y i r. Let each of the D partitions form the heldout part D ? ? constituting the retained part .", "label": "", "metadata": {}, "score": "75.67247"}
{"text": "r ?Y r R P k ? ?Now assume D disjoint sets , and the quantity Y rfor the case where the i thsuch Y i r. Let each of the D partitions form the heldout part D ? ? constituting the retained part .", "label": "", "metadata": {}, "score": "75.67247"}
{"text": "The translation cache stores translations of selected tokens and segments .Each segment includes a combination of tokens from the source language to the target language .A segment translation server cache is included in this device to store data retrieved from the language model for translating the segment and to serve the stored data to the decoder .", "label": "", "metadata": {}, "score": "75.74934"}
{"text": "C r ? q r ? ?C ? q ?X r ? ?C r ? q r ? ?C ? q ?N ?X r ? ?r ?C r ? ?( using 23 ) ?", "label": "", "metadata": {}, "score": "75.96845"}
{"text": "C r ? q r ? ?C ? q ?X r ? ?C r ? q r ? ?C ? q ?N ?X r ? ?r ?C r ? ?( using 23 ) ?", "label": "", "metadata": {}, "score": "75.96845"}
{"text": "Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 11 .Although this work focuses on their application to speech - recognitionsystems , languagemodels are im- portant components also in other areas2 , such as handwriting recognition , machine translation , spelling correction , and part - of - speech tagging .", "label": "", "metadata": {}, "score": "76.19436"}
{"text": "Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 11 .Although this work focuses on their application to speech - recognitionsystems , languagemodels are im- portant components also in other areas2 , such as handwriting recognition , machine translation , spelling correction , and part - of - speech tagging .", "label": "", "metadata": {}, "score": "76.19436"}
{"text": "r ? i ? ?p r i ?p i ?N ? r ( 18 )Now , since the events ?i ? i?f ? ? ? ? ? ? ? ?N ?g form partitions of ?", "label": "", "metadata": {}, "score": "76.28723"}
{"text": "r ? i ? ?p r i ?p i ?N ? r ( 18 )Now , since the events ?i ? i?f ? ? ? ? ? ? ? ?N ?g form partitions of ?", "label": "", "metadata": {}, "score": "76.28723"}
{"text": "0130 ] [ 002 003 004 006]:1 .[ 0131 ] [ 002 003 004 007]:1 .[ 0132 ] [ 002 003 004 008]:1 .[ 0133 ] [ 002 003 009 010]:1 .[ 0134 ] Using language modeling for machine translation often requires a system to look up shorter n grams in case a longer n gram is not found .", "label": "", "metadata": {}, "score": "76.41829"}
{"text": "After all of the requested LM data is received , the decoder 610 then uses the received LM data to finalize the processing .The processing by the decoder 610 uses the coarse LM and produces an initial translation result .After the requested LM data is received from the LM servers 210 , the decoder 610 then proceeds to update the initial translation result by using the received LM data .", "label": "", "metadata": {}, "score": "76.418915"}
{"text": "[0048 ] The DMT system 100 has replica servers 141 for each partition resource server 140 .Hence , an additional load balancing mechanism that is different from the load balancer 120 may be implemented between the resource servers 140 and 141 and segment translation servers 130 as a back end load balancing mechanism .", "label": "", "metadata": {}, "score": "76.44896"}
{"text": "The method of claim 21 , further comprising : operating each translation server to wait for all the requests in the request queue to be served before translating a respective segment .The method of claim 21 , further comprising : using another different collection of machine language translation resource data to perform the initial translation of the respective segment .", "label": "", "metadata": {}, "score": "76.52739"}
{"text": "Machine translation Thegreateravailability of alignedbilingualcorporahas promptedrenewedresearchinto statistical meth- ods ofautomatictranslation[4 ] , [ 6 ] , [ 7 ] , [ 86].In particular , thework in [ 4 ] and[7 ] considerstheproblem of translating a sentence w s ? ? ?", "label": "", "metadata": {}, "score": "76.57269"}
{"text": "Machine translation Thegreateravailability of alignedbilingualcorporahas promptedrenewedresearchinto statistical meth- ods ofautomatictranslation[4 ] , [ 6 ] , [ 7 ] , [ 86].In particular , thework in [ 4 ] and[7 ] considerstheproblem of translating a sentence w s ? ? ?", "label": "", "metadata": {}, "score": "76.57269"}
{"text": "10 is a flowchart for one exemplary processing flow of a segment translation server having a high - level cache without a low - level cache .[0031 ] FIGS .11A and 11B show another example processing flow of a segment translation server .", "label": "", "metadata": {}, "score": "76.59828"}
{"text": "Absolute discounting The method of absolute discounting chooses the discounting factor in equation ( 28 ) to be a positive constant b less than unity , i.e. : d r ?b with ? ?b ? ?( 32 )We may once again require the unseen probability to equal that obtained with Turing 's estimate so that , from equations ( 15 ) , ( 29 ) and ( 32 ) , we find : q ?", "label": "", "metadata": {}, "score": "76.69485"}
{"text": "Absolute discounting The method of absolute discounting chooses the discounting factor in equation ( 28 ) to be a positive constant b less than unity , i.e. : d r ?b with ? ?b ? ?( 32 )We may once again require the unseen probability to equal that obtained with Turing 's estimate so that , from equations ( 15 ) , ( 29 ) and ( 32 ) , we find : q ?", "label": "", "metadata": {}, "score": "76.69485"}
{"text": "The translation server is operable to receive source text in the source language to be translated into the target language and is further operable to obtain machine translation resource data from the machine translation resource servers .The translation server then uses the obtained machine translation resource data to translate the source text into the target language .", "label": "", "metadata": {}, "score": "76.84695"}
{"text": "Optimisation algorithms for the cat- egory assignments allow this gap to be narrowed , particularly because the number of categories can be increased in sympathy with the size of the training set , but they suffer from high computational com- plexity .", "label": "", "metadata": {}, "score": "76.871086"}
{"text": "Optimisation algorithms for the cat- egory assignments allow this gap to be narrowed , particularly because the number of categories can be increased in sympathy with the size of the training set , but they suffer from high computational com- plexity .", "label": "", "metadata": {}, "score": "76.871086"}
{"text": "For ... \" .This paper reports experiments on adapting components of a Statistical Machine Translation ( SMT ) system for the task of translating online user - generated forum data from Symantec .Such data is monolingual , and differs from available bitext MT training resources in a number of important respects .", "label": "", "metadata": {}, "score": "76.963264"}
{"text": "r ?D P i ? ?Y i r D P i ? ?R P k ? ?Y i k ( 27 )When we choose reduces to the Good - Turing estimate [ 52 ] , thus establishing the optimality of the latter within a cross- validation framework . D?N ?", "label": "", "metadata": {}, "score": "76.98356"}
{"text": "r ?D P i ? ?Y i r D P i ? ?R P k ? ?Y i k ( 27 )When we choose reduces to the Good - Turing estimate [ 52 ] , thus establishing the optimality of the latter within a cross- validation framework . D?N ?", "label": "", "metadata": {}, "score": "76.98356"}
{"text": "N ?( 16 ) where events in R is the largest number of times any event occurs in ? , and M ? denotes the number of different ?so that M ?N ?Finally , denote the set of all events that occur exactly r times in ? by ?", "label": "", "metadata": {}, "score": "77.067795"}
{"text": "N ?( 16 ) where events in R is the largest number of times any event occurs in ? , and M ? denotes the number of different ?so that M ?N ?Finally , denote the set of all events that occur exactly r times in ? by ?", "label": "", "metadata": {}, "score": "77.067795"}
{"text": "One or more computer servers 1310 are connected to the communication network 1300 to form a distributed processing system such as a system in FIG .1 , 2 , or 12 .The computer servers 1310 may be located at the same location or at different locations .", "label": "", "metadata": {}, "score": "77.141464"}
{"text": "5 shows an example implementation of a segment translation server cache 510 shared by the segment translation servers 130 of FIG .1 or 2 .Upon receiving a segment to be translated from the load balancer 120 , a segment translation server 130 first looks up the segment translation server cache 510 for an existing translation for the segment .", "label": "", "metadata": {}, "score": "77.16881"}
{"text": "Segment translation servers 130 can implement the same or different machine translation decoding schemes , such as rule - based MT and statistical MT decoders .Upon receiving a client document 102 , a receiving translation front end 110 divides the client document 102 into multiple smaller segments where each segment includes one or more tokens .", "label": "", "metadata": {}, "score": "77.269394"}
{"text": "Applying SLM techniques like trigram language models to Chinese is challenging because ( 1 ) there is no standard definition of words in Chinese ; ( 2 ) word boundaries are not marked by spaces ; and ( 3 ) there is a dearth of training data .", "label": "", "metadata": {}, "score": "77.297165"}
{"text": "2 may also include resources to provide automated translation between the target language and a second source language .One or more translation model servers can be included in the system 200 for a second translation model for translation between the target language and the second source language .", "label": "", "metadata": {}, "score": "77.34238"}
{"text": "Conclusion and topics for future investigation ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?Tagging ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "77.56747"}
{"text": "Conclusion and topics for future investigation ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?Tagging ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "77.56747"}
{"text": "In large part because of this inability to reliably capture large - span behavior , the performance ... .by Francisco Casacuberta , Enrique Vidal - COMPUTATIONAL LINGUISTICS , 2004 . \" ...Finite - state transducers are models that are being used in different areas of pattern recognition and computational linguistics .", "label": "", "metadata": {}, "score": "77.786865"}
{"text": "Full - text .Category?based statistical languagemo dels Thomas Niesler St ? John?sCollege June ? ? ? ?Thesissubmittedtothe University of Cam bridgeinpartial ful?lment ofthe requirements for the degree ofDo ctorof Philosophy .Page 2 .Synopsis Languagemodelsarecomputationaltechniquesandstructuresthatdescribewordsequencesproducedby human subjects , and the work presented here considers primarily their application to automatic speech- recognition systems .", "label": "", "metadata": {}, "score": "77.98781"}
{"text": "Full - text .Category?based statistical languagemo dels Thomas Niesler St ? John?sCollege June ? ? ? ?Thesissubmittedtothe University of Cam bridgeinpartial ful?lment ofthe requirements for the degree ofDo ctorof Philosophy .Page 2 .Synopsis Languagemodelsarecomputationaltechniquesandstructuresthatdescribewordsequencesproducedby human subjects , and the work presented here considers primarily their application to automatic speech- recognition systems .", "label": "", "metadata": {}, "score": "77.98781"}
{"text": "Contactions and their part - of - speech tag assignments ? ? ? ? ? ? ? ? ? ? ? ? ? ?115 F. Appendix : OALD tag mappings116 G. Appendix : Trigger approximations 117 G.1 .First approximation ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "78.032135"}
{"text": "Contactions and their part - of - speech tag assignments ? ? ? ? ? ? ? ? ? ? ? ? ? ?115 F. Appendix : OALD tag mappings116 G. Appendix : Trigger approximations 117 G.1 .First approximation ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "78.032135"}
{"text": "Manual translations of newly identified text sections are obtained to update the existing database for the manual translations .[0157 ] The process for obtaining a manual translation of an identified text section may include a manual or automatic search on the web or other on - line repositories for existing translations .", "label": "", "metadata": {}, "score": "78.10899"}
{"text": "r ? ioccurring r times in ?N ?p r i ? ? ? ?p i ?N ?r Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 23 . and therefore : E N ?", "label": "", "metadata": {}, "score": "78.13115"}
{"text": "r ? ioccurring r times in ?N ?p r i ? ? ? ?p i ?N ?r Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 23 . and therefore : E N ?", "label": "", "metadata": {}, "score": "78.13115"}
{"text": "0054 ]The language model servers 210 include multiple partition servers that store and serve different partitions of the language model .An example of ( P+1 ) partitions are shown in FIG .2 , where P is an integer .", "label": "", "metadata": {}, "score": "78.21236"}
{"text": "0158 ] Some manual translations may only be appropriate in certain contexts .For example , a certain translation for \" home \" may only make sense for a Web page where it is a label for a link to the Web site 's home page .", "label": "", "metadata": {}, "score": "78.25901"}
{"text": "w?i?jw?i ? ? ?i ?N topic X j ? ?j ?P j ?w?i?jw?i ? ? ?i ?( 72 ) with N topic X j ? ?j ? ?where N topicis the numberof topics , P j ?", "label": "", "metadata": {}, "score": "78.306564"}
{"text": "w?i?jw?i ? ? ?i ?N topic X j ? ?j ?P j ?w?i?jw?i ? ? ?i ?( 72 ) with N topic X j ? ?j ? ?where N topicis the numberof topics , P j ?", "label": "", "metadata": {}, "score": "78.306564"}
{"text": "One or more replica translation model servers can be included for each of the translation model servers 220 .Translation front ends 110 can be provided in the system 200 to interface with clients and each translation front end 110 is operable to divide source text into segments .", "label": "", "metadata": {}, "score": "78.36126"}
{"text": "Character and handwriting recognition 2.8.2 .Machine translation 2.8.3 .Spelling correction 2.8.4 .Tagging 38 38 38 39 39 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "78.421295"}
{"text": "Character and handwriting recognition 2.8.2 .Machine translation 2.8.3 .Spelling correction 2.8.4 .Tagging 38 38 38 39 39 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "78.421295"}
{"text": "i ? ?r ( 14 ) where : ? ?x ? ?when when x is true ?x is false From the definition ( 14 ) note that : R X r ? ?C r ?M ?", "label": "", "metadata": {}, "score": "78.489876"}
{"text": "i ? ?r ( 14 ) where : ? ?x ? ?when when x is true ?x is false From the definition ( 14 ) note that : R X r ? ?C r ?M ?", "label": "", "metadata": {}, "score": "78.489876"}
{"text": "For example , a system trained to recognise speech read from the Wall Street Journal will be equipped with a language model trained on many millions of words from previous editions of the newspaper , and will perform very well on its specified task ... .", "label": "", "metadata": {}, "score": "78.510284"}
{"text": "i j ?j ? and P ? ?i j ? k ? jis hence given by : P u ? ?j ?i ?N ? ?i j ?j ? ? ?P ? i j ?", "label": "", "metadata": {}, "score": "78.632614"}
{"text": "i j ?j ? and P ? ?i j ? k ? jis hence given by : P u ? ?j ?i ?N ? ?i j ?j ? ? ?P ? i j ?", "label": "", "metadata": {}, "score": "78.632614"}
{"text": "r. For a certain y j , the process of finding an optimal ? rtherefore reduces to the task of y jinto the subset ? rand its complement ?r. Since Note that both general .In practice they must be replacedby their estimates , which are derivedfrom the training corpus .", "label": "", "metadata": {}, "score": "78.94186"}
{"text": "r. For a certain y j , the process of finding an optimal ? rtherefore reduces to the task of y jinto the subset ? rand its complement ?r. Since Note that both general .In practice they must be replacedby their estimates , which are derivedfrom the training corpus .", "label": "", "metadata": {}, "score": "78.94186"}
{"text": "Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 12 . following figure , which depicts a simple speech recognition system capable of distinguishing between the words \" hello \" and \" goodbye \" .Front - end preprocessor Acoustic model for \" Hello \" Acoustic model for \" Goodbye \" ? ?", "label": "", "metadata": {}, "score": "78.980156"}
{"text": "Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 12 . following figure , which depicts a simple speech recognition system capable of distinguishing between the words \" hello \" and \" goodbye \" .Front - end preprocessor Acoustic model for \" Hello \" Acoustic model for \" Goodbye \" ? ?", "label": "", "metadata": {}, "score": "78.980156"}
{"text": "N tl ? in the target language ( English ) according the following probabilistic model : w t ? ? ?N tl ?w t ? ? ?N tl ?P ? w s ? ? ?N sl ? ? ?", "label": "", "metadata": {}, "score": "79.15506"}
{"text": "N tl ? in the target language ( English ) according the following probabilistic model : w t ? ? ?N tl ?w t ? ? ?N tl ?P ? w s ? ? ?N sl ? ? ?", "label": "", "metadata": {}, "score": "79.15506"}
{"text": "[0164 ]The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output .The processes and logic flows can also be performed by , and apparatus can also be implemented as , special purpose logic circuitry , e.g. , an FPGA ( field programmable gate array ) or an ASIC ( application specific integrated circuit ) .", "label": "", "metadata": {}, "score": "79.22629"}
{"text": "The Good - Turing estimate The Good - Turing probability estimation technique hinges on the symmetry requirement : the assump- tion that two events each occurring the same number of times in the sample must have the same prob- ability of occurrence .", "label": "", "metadata": {}, "score": "79.65272"}
{"text": "The Good - Turing estimate The Good - Turing probability estimation technique hinges on the symmetry requirement : the assump- tion that two events each occurring the same number of times in the sample must have the same prob- ability of occurrence .", "label": "", "metadata": {}, "score": "79.65272"}
{"text": "i j ?j ? ? ?P ? ?i j ?j ?P ? i ?N ? ?i j ?j ? ? ?P ? ?i j ? k ?( 39 ) 11Refer to sections 2.2 and 2.3 .", "label": "", "metadata": {}, "score": "79.715416"}
{"text": "i j ?j ? ? ?P ? ?i j ?j ?P ? i ?N ? ?i j ?j ? ? ?P ? ?i j ? k ?( 39 ) 11Refer to sections 2.2 and 2.3 .", "label": "", "metadata": {}, "score": "79.715416"}
{"text": "Preprocessing of the speech signal Speech obtained from a microphone or recording device is in the form of an analogue electrical signal .Thisis processedintoaformsuitableforusewithin aspeech - recognitionsystembyaseriesofoperations commonly referred to collectivelyas the front end .Usually thesesteps includeband - limiting the signal , samplingit , andthen applyingsomespectral transformation to encodeits frequencycharacteristics .", "label": "", "metadata": {}, "score": "79.89322"}
{"text": "Preprocessing of the speech signal Speech obtained from a microphone or recording device is in the form of an analogue electrical signal .Thisis processedintoaformsuitableforusewithin aspeech - recognitionsystembyaseriesofoperations commonly referred to collectivelyas the front end .Usually thesesteps includeband - limiting the signal , samplingit , andthen applyingsomespectral transformation to encodeits frequencycharacteristics .", "label": "", "metadata": {}, "score": "79.89322"}
{"text": "First , the decoder 610 processes the segment without all the LM data needed from the language model before the LM data requested is received .A dummy lookup model may be used to allow for the decoder 610 to process the segment while waiting for the requested LM data .", "label": "", "metadata": {}, "score": "80.04665"}
{"text": "The speech recognizer 's functionality is extended to include con dence annotations , which are \\meta - level \" markings that indicate how certain the recognizer is that it has decoded its input correctly .This is accomplished by feeding externally de ned error conditions back to the recognizer .", "label": "", "metadata": {}, "score": "80.113846"}
{"text": "The translation front end 110 receives a client document 102 to be translated from the source language into the target language and divides the source text in the document 102 into segments ( step 341 ) .Next , the translation front end 110 looks up each of the divided segments in a translation cache to determine whether a translation exists ( steps 342 and 343 ) .", "label": "", "metadata": {}, "score": "80.14255"}
{"text": "The computer readable medium can be a machine - readable storage device , a machine - readable storage substrate , a memory device , a composition of matter effecting a machine - readable propagated signal , or a combination of one or more them .", "label": "", "metadata": {}, "score": "80.21234"}
{"text": "[ 0046 ] The translation front ends 110 are replicas of one another and operate in parallel with one another .The segment translation servers 130 are also replicas of one another and operate in parallel .The resource servers 140 are partition servers that store partitions of the entire translation resource data and other resources and information for the segment translation servers 130 to perform the translation tasks .", "label": "", "metadata": {}, "score": "80.28639"}
{"text": "Probability in Expected number of events occurring exactly ? of any event occurring r times in ?r times in ?P ? ?r ? EfC r g ( 17 )Assuming that the events occur independently ( Bernoulli trials ) , the probability that there are precisely r sightings of ? iin ? is given by the binomial probability distribution [ 18 ] : P ?", "label": "", "metadata": {}, "score": "80.4621"}
{"text": "Probability in Expected number of events occurring exactly ? of any event occurring r times in ?r times in ?P ? ?r ? EfC r g ( 17 )Assuming that the events occur independently ( Bernoulli trials ) , the probability that there are precisely r sightings of ? iin ? is given by the binomial probability distribution [ 18 ] : P ?", "label": "", "metadata": {}, "score": "80.4621"}
{"text": "[0156 ] The database of manual translations in an automated machine translation system can be updated or revised .Requests to translate additional material by human can be derived automatically from information obtained from the running machine translation system .For example , system statistical data on the machine translation activities can be used to extract information on frequently translated text sections and other information that can identify text sections to be manually translated .", "label": "", "metadata": {}, "score": "80.70839"}
{"text": "Page 31 .so that substitution into ( 43 ) yields [ 53 ] : P ? ?i j ?j ? ?N ?j ?D ?M ? j N ?j ?P ? ?i j ?", "label": "", "metadata": {}, "score": "80.775925"}
{"text": "Page 31 .so that substitution into ( 43 ) yields [ 53 ] : P ? ?i j ?j ? ?N ?j ?D ?M ? j N ?j ?P ? ?i j ?", "label": "", "metadata": {}, "score": "80.775925"}
{"text": "Introduction and overview ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "81.03552"}
{"text": "Introduction and overview ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "81.03552"}
{"text": "i j ?j ? def ?P u ? ?j ?j ?P ? ?i j ? k ? ? ?i ?N ? ?i j ?j ? ? ?( 36 )It follows from ( 35 ) and ( 36 ) that : P u ? ?", "label": "", "metadata": {}, "score": "81.04064"}
{"text": "i j ?j ? def ?P u ? ?j ?j ?P ? ?i j ? k ? ? ?i ?N ? ?i j ?j ? ? ?( 36 )It follows from ( 35 ) and ( 36 ) that : P u ? ?", "label": "", "metadata": {}, "score": "81.04064"}
{"text": "Rescoring results ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "81.04878"}
{"text": "Rescoring results ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "81.04878"}
{"text": "Overgeneralisation : Whentherearefewcategories , thelanguagemodelis notabletodiscriminate strongly among different patterns in the text .Tradeoff : As the number of categories increases , the extent to which the model must generalise diminishes , and the discrimination ability improves .Overfitting : When there are too many categories , the language model begins to reflect the pecu- liarities of the training set , and generalises less well to the test set .", "label": "", "metadata": {}, "score": "81.07364"}
{"text": "Overgeneralisation : Whentherearefewcategories , thelanguagemodelis notabletodiscriminate strongly among different patterns in the text .Tradeoff : As the number of categories increases , the extent to which the model must generalise diminishes , and the discrimination ability improves .Overfitting : When there are too many categories , the language model begins to reflect the pecu- liarities of the training set , and generalises less well to the test set .", "label": "", "metadata": {}, "score": "81.07364"}
{"text": "This article presents a unified approach to Chinese statistical language modeling ( SLM ) .Applying SLM techniques like trigram language models to Chinese is challenging because ( 1 ) there is no standard definition of words in Chinese ; ( 2 ) word boundaries are not marked by spaces ; and ( 3 ) there is a de ... \" .", "label": "", "metadata": {}, "score": "81.274536"}
{"text": "v N v g Now define an operator V ? ? ?that maps each word w i ? i?f ? ? ? ? ? ? ? ?N w g to one or more categories v j ?j?f ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "81.3042"}
{"text": "v N v g Now define an operator V ? ? ?that maps each word w i ? i?f ? ? ? ? ? ? ? ?N w g to one or more categories v j ?j?f ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "81.3042"}
{"text": "Its bottom layer of hierarchy consists of multiple hierarchical Pitman - Yor process language models , one each for some number of domains .The novel top layer of hierarchy consists of a mechanism to couple together multi ... \" .In this paper we present a doubly hierarchical Pitman - Yor process language model .", "label": "", "metadata": {}, "score": "81.453186"}
{"text": "The entire segment load balancers together constitute the back end load balancing mechanism .Each segment load balancer can be a separate machine in some implementations and may be replicated or partitioned if needed .[ 0049 ] Each load balancing mechanism , e.g. , the front end load balancer 120 and the back end load balancing mechanism , can include a monitoring mechanism to monitor activities , conditions and operations of various machines involved in the operations of that load balancing mechanism .", "label": "", "metadata": {}, "score": "81.542465"}
{"text": "Combining Manual and Automated Translation in Automated Machine Translation Systems .[ 0153 ] This section describes techniques that combine manual translations and automated translations in an automated machine translation system to provide various translation options .The following process may be used to build a digital library for the manual translation and to use the library during the automated machine translation .", "label": "", "metadata": {}, "score": "81.76412"}
{"text": "Lattice rescoring .Recognition experiments ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "81.84564"}
{"text": "Lattice rescoring .Recognition experiments ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "81.84564"}
{"text": "Chapter 6 shows how language models may be integrated into a speech recognition system , and presents some recognition results using the models developed in this thesis .Finally , chapter 7 presents a summary and conclusions .Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .", "label": "", "metadata": {}, "score": "82.37123"}
{"text": "Chapter 6 shows how language models may be integrated into a speech recognition system , and presents some recognition results using the models developed in this thesis .Finally , chapter 7 presents a summary and conclusions .Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .", "label": "", "metadata": {}, "score": "82.37123"}
{"text": "z ?z?K ? ? ?g P?z ? ? ?K ? ? ? ? , z ? ? ?K ? is [ 74 ] , [ 85 ] : I s ?z ? ? ?K ? ? ?", "label": "", "metadata": {}, "score": "82.48529"}
{"text": "z ?z?K ? ? ?g P?z ? ? ?K ? ? ? ? , z ? ? ?K ? is [ 74 ] , [ 85 ] : I s ?z ? ? ?K ? ? ?", "label": "", "metadata": {}, "score": "82.48529"}
{"text": "Machine translation uses computers and other machines to automate part of or the entire translation process to reduce the translation cost and expedite the translation process .Rule - based machine translation and statistical machine translation are two examples of machine translation techniques .", "label": "", "metadata": {}, "score": "82.512146"}
{"text": "The translation model server stores and is operable to serve a translation model for translation between the target language and a source language .The translation server is operable to obtain translation model data from the translation model server and language model data from the language model servers and translate a source text in the source language into the target language based on obtained translation model data and language model data .", "label": "", "metadata": {}, "score": "82.659"}
{"text": "C r ? ?N ?C r ( 22 ) Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 24 .This is often approximated as [ 52 ] : q r ?N ?C r ( 23 ) Section 2.3.2 discusses how the expression ( 22 ) may be shown to be optimal within a cross - validation framework [ 52].", "label": "", "metadata": {}, "score": "82.67228"}
{"text": "C r ? ?N ?C r ( 22 ) Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 24 .This is often approximated as [ 52 ] : q r ?N ?C r ( 23 ) Section 2.3.2 discusses how the expression ( 22 ) may be shown to be optimal within a cross - validation framework [ 52].", "label": "", "metadata": {}, "score": "82.67228"}
{"text": "B B ?r ?C r ? ? r?C r ? k ?C k ? ?C ? k ?C k ? ?C ?C C A ? r N ? r?k ( 25 )A typical value for k is 5 .", "label": "", "metadata": {}, "score": "82.71765"}
{"text": "B B ?r ?C r ? ? r?C r ? k ?C k ? ?C ? k ?C k ? ?C ?C C A ? r N ? r?k ( 25 )A typical value for k is 5 .", "label": "", "metadata": {}, "score": "82.71765"}
{"text": "Specialisation to a target domain Language models trained on large quantities of text covering many subject areas and styles of writing displaygoodaverageperformance , butarenot ableto takeadvantageoftheparticularities ofthedomains to which they are applied .Moreover , often there are insufficient data from the specialised domain to allow specialised models to be built directly .", "label": "", "metadata": {}, "score": "82.73123"}
{"text": "Specialisation to a target domain Language models trained on large quantities of text covering many subject areas and styles of writing displaygoodaverageperformance , butarenot ableto takeadvantageoftheparticularities ofthedomains to which they are applied .Moreover , often there are insufficient data from the specialised domain to allow specialised models to be built directly .", "label": "", "metadata": {}, "score": "82.73123"}
{"text": "P ? ?i j ?j ? ? ?we have : N ?X i ? ?N ? ?i j ?j ? ?d ? ?i j ?j ?N ?j ?N ?", "label": "", "metadata": {}, "score": "82.81532"}
{"text": "P ? ?i j ?j ? ? ?we have : N ?X i ? ?N ? ?i j ?j ? ?d ? ?i j ?j ?N ?j ?N ?", "label": "", "metadata": {}, "score": "82.81532"}
{"text": "r ?n ? i ?N ? ?i ? ?r and i?f ? ? ? ? ? ? ? ?N ? g o The strategy employed by Good , as advocated by Turing [ 26 ] , is to estimate the probability with which an event occurs in from the symmetry requirement , we may write : r ? when it is known to appear exactly r times in ?", "label": "", "metadata": {}, "score": "83.054504"}
{"text": "r ?n ? i ?N ? ?i ? ?r and i?f ? ? ? ? ? ? ? ?N ? g o The strategy employed by Good , as advocated by Turing [ 26 ] , is to estimate the probability with which an event occurs in from the symmetry requirement , we may write : r ? when it is known to appear exactly r times in ?", "label": "", "metadata": {}, "score": "83.054504"}
{"text": "0004 ] Translation from one human language or natural language ( a source natural language ) to another natural language ( a target natural language ) can be done in various ways .A person can manually translate a text in the source natural language ( e.g. , Chinese ) by first reading and understanding the Chinese text and then writing down the corresponding text in the target language ( e.g. , English ) .", "label": "", "metadata": {}, "score": "83.06712"}
{"text": "Machine translation is increasingly used in a wide range of applications .For example , resources are available on many computer networks such as the Internet to provide machine translation to allow for easy access to information in different natural languages .", "label": "", "metadata": {}, "score": "83.13355"}
{"text": "Given training text labeled with topic information , we automatically identify the most relevant topics for new text .We adapt our language model toward these topics using an exponential model , by a ... \" .Given training text labeled with topic information , we automatically identify the most relevant topics for new text .", "label": "", "metadata": {}, "score": "83.40761"}
{"text": "Preprocessing ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "83.890274"}
{"text": "Preprocessing ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "83.890274"}
{"text": "Preprocessing ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "83.890274"}
{"text": "Preprocessing ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "83.890274"}
{"text": "Preprocessing ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "83.890274"}
{"text": "Preprocessing ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "83.890274"}
{"text": "The system of claim 4 , wherein : the segment translation server cache is operable to delete the at least part of the obtained machine translation resource data periodically .The system of claim 1 , wherein : the segment translation server cache is operable to further store history information of translation of an assigned segment .", "label": "", "metadata": {}, "score": "83.89606"}
{"text": "Summary and conclusion ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "84.150536"}
{"text": "Summary and conclusion ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "84.150536"}
{"text": "Summary and conclusion ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "84.150536"}
{"text": "Summary and conclusion ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "84.150536"}
{"text": "Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 41 .Setting it may be shown that : c k ?N ?N X i ? ?c k ? Pw?i?jw?i?k ? p P j ? ?", "label": "", "metadata": {}, "score": "84.18698"}
{"text": "Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 41 .Setting it may be shown that : c k ?N ?N X i ? ?c k ? Pw?i?jw?i?k ? p P j ? ?", "label": "", "metadata": {}, "score": "84.18698"}
{"text": "gz ' , which are gzip compressed , then you may be able to find tools to uncompress them at the gzip web site .If you have difficulty viewing files that are in PostScript , ( ending ' .ps ' or ' .", "label": "", "metadata": {}, "score": "84.20838"}
{"text": "gz ' , which are gzip compressed , then you may be able to find tools to uncompress them at the gzip web site .If you have difficulty viewing files that are in PostScript , ( ending ' .ps ' or ' .", "label": "", "metadata": {}, "score": "84.20838"}
{"text": "E N ? fC ? ?g so that , using ( 20 ) it follows that : E N ?fC r ? ?g ?N ?r ? ?P ? ?r ? and finally substitution of this result into ( 17 ) leads us to : q r ?", "label": "", "metadata": {}, "score": "84.44403"}
{"text": "E N ? fC ? ?g so that , using ( 20 ) it follows that : E N ?fC r ? ?g ?N ?r ? ?P ? ?r ? and finally substitution of this result into ( 17 ) leads us to : q r ?", "label": "", "metadata": {}, "score": "84.44403"}
{"text": "Scope of this thesis ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "84.78667"}
{"text": "Scope of this thesis ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "84.78667"}
{"text": "The system 1200 also includes multiple front ends 1210 to handle input 1201 and output 1202 , and multiple processing servers 1230 to process the input 1201 and produce the output 1202 .Each front end 1210 , after receiving the input 1201 , divides the input 1201 into segments for processing by the processing servers 1230 .", "label": "", "metadata": {}, "score": "85.10875"}
{"text": "A propagated signal is an artificially generated signal , e.g. , a machine - generated electrical , optical , or electromagnetic signal , that is generated to encode information for transmission to suitable receiver apparatus .A computer program does not necessarily correspond to a file in a file system .", "label": "", "metadata": {}, "score": "85.15065"}
{"text": "Corpus statistics ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "85.17341"}
{"text": "Corpus statistics ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "85.17341"}
{"text": "Corpus statistics ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "85.17341"}
{"text": "Corpus statistics ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "85.17341"}
{"text": "Corpus statistics ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "85.17341"}
{"text": "Corpus statistics ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "85.17341"}
{"text": "We review them here , point to a few promising directions , and argue for a Bayesian approach to integration of linguistic theories with data .OUTLINE Statistical language modeling ( SLM ) is the attempt to capture regularities of natural language for the purpose of improving the performance of various natural language applications .", "label": "", "metadata": {}, "score": "85.26758"}
{"text": "We review them here , point to a few promising directions , and argue for a Bayesian approach to integration of linguistic theories with data .OUTLINE Statistical language modeling ( SLM ) is the attempt to capture regularities of natural language for the purpose of improving the performance of various natural language applications .", "label": "", "metadata": {}, "score": "85.26758"}
{"text": "We review them here , point to a few promising directions , and argue for a Bayesian approach to integration of linguistic theories with data .OUTLINE Statistical language modeling ( SLM ) is the attempt to capture regularities of natural language for the purpose of improving the performance of various natural language applications .", "label": "", "metadata": {}, "score": "85.26758"}
{"text": "Some oftheworkhasbeenpublished previously in conference proceedings ( [ 57 ] , [ 60 ] , [ 62 ] ) .The length of this thesis , including appendices and footnotes , is approximately 43,000 words .Page 4 .Acknowledgements First and foremost I would like to thank my supervisor , Phil Woodland .", "label": "", "metadata": {}, "score": "85.45565"}
{"text": "Some oftheworkhasbeenpublished previously in conference proceedings ( [ 57 ] , [ 60 ] , [ 62 ] ) .The length of this thesis , including appendices and footnotes , is approximately 43,000 words .Page 4 .Acknowledgements First and foremost I would like to thank my supervisor , Phil Woodland .", "label": "", "metadata": {}, "score": "85.45565"}
{"text": "Final summary ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "86.30266"}
{"text": "Final summary ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "86.30266"}
{"text": "Finite - state transducers are models that are being used in different areas of pattern recognition and computational linguistics .One of these areas is machine translation , in which the approaches that are based on building models automatically from training examples are becoming more and more attractive .", "label": "", "metadata": {}, "score": "86.45872"}
{"text": "\" [ Show abstract ] [ Hide abstract ] ABSTRACT : This paper proposes a new way to improve the performance of dependency parser : subdividing verbs according to their grammatical functions and integrating the information of verb subclasses into lexicalized parsing model .", "label": "", "metadata": {}, "score": "86.82501"}
{"text": "\" [ Show abstract ] [ Hide abstract ] ABSTRACT : This paper proposes a new way to improve the performance of dependency parser : subdividing verbs according to their grammatical functions and integrating the information of verb subclasses into lexicalized parsing model .", "label": "", "metadata": {}, "score": "86.82501"}
{"text": "It hasbeenaprivilegeto work underhis supervision .Then I would like to express my gratitude to St. John 's College , who have provided finan- cial support in the form of a Benefactor 's Scholarship , without which my doctoral studies would not have been possible .", "label": "", "metadata": {}, "score": "86.95506"}
{"text": "It hasbeenaprivilegeto work underhis supervision .Then I would like to express my gratitude to St. John 's College , who have provided finan- cial support in the form of a Benefactor 's Scholarship , without which my doctoral studies would not have been possible .", "label": "", "metadata": {}, "score": "86.95506"}
{"text": "P ?v j jH?w?i ? ?( 56 ) so that we may rewrite ( 53 ) as : P ?w?i?jw ? ? ?i ?X ?v?v?V?w?i ? ?P ?w?i?jV?w?i ? ?P ?V?w?i ? ?", "label": "", "metadata": {}, "score": "87.04462"}
{"text": "P ?v j jH?w?i ? ?( 56 ) so that we may rewrite ( 53 ) as : P ?w?i?jw ? ? ?i ?X ?v?v?V?w?i ? ?P ?w?i?jV?w?i ? ?P ?V?w?i ? ?", "label": "", "metadata": {}, "score": "87.04462"}
{"text": "Certain tasks that were difficult for machine to handle in the past are increasingly being automated due to advances in computer information technology and communication technology .Language translation and speech recognition are two examples of machine processing tasks that are being automated .", "label": "", "metadata": {}, "score": "87.33728"}
{"text": "X i ? ?p r ? ?i ?p i ?N ?r For clarity , define : NW?r?N ? ?N r ?N ?X i ? ?p r i ?p i ? N?r ( 19 ) and note that : NW?r ? ? ?", "label": "", "metadata": {}, "score": "87.50939"}
{"text": "X i ? ?p r ? ?i ?p i ?N ?r For clarity , define : NW?r?N ? ?N r ?N ?X i ? ?p r i ?p i ? N?r ( 19 ) and note that : NW?r ? ? ?", "label": "", "metadata": {}, "score": "87.50939"}
{"text": "4 shows a different implementation of the translation cache 310 .Different from the design in FIG .3A , in the design of FIG .4 , the interaction of the translation cache 310 is with the load balancer 120 .", "label": "", "metadata": {}, "score": "87.710365"}
{"text": "E N ?fC r ? ?g ?N ?E N ?fC r g Approximating the expectations by the counts : E N ?fC r ? ?g?C r ? ?and E N ?fC r g?C r ( 21 ) we obtain Turing 's formula [ 26 ] : q r ?", "label": "", "metadata": {}, "score": "87.865166"}
{"text": "E N ?fC r ? ?g ?N ?E N ?fC r g Approximating the expectations by the counts : E N ?fC r ? ?g?C r ? ?and E N ?fC r g?C r ( 21 ) we obtain Turing 's formula [ 26 ] : q r ?", "label": "", "metadata": {}, "score": "87.865166"}
{"text": "Furthermore I owe a great deal to Patrick Gosling , Andrew Gee and Carl Seymour , the maintainers of the excellent computing facilities , without which this work would certainly not have been possible .The recognition experiments in this thesis wereaccomplishedusingthe EntropicLattice andLanguageModellingToolkit , andI thank Julian Odell for his major contribution to the preparation of this software , which has made the last phase of my research much simpler .", "label": "", "metadata": {}, "score": "87.92811"}
{"text": "Furthermore I owe a great deal to Patrick Gosling , Andrew Gee and Carl Seymour , the maintainers of the excellent computing facilities , without which this work would certainly not have been possible .The recognition experiments in this thesis wereaccomplishedusingthe EntropicLattice andLanguageModellingToolkit , andI thank Julian Odell for his major contribution to the preparation of this software , which has made the last phase of my research much simpler .", "label": "", "metadata": {}, "score": "87.92811"}
{"text": "When the number of input variables increases , the number of required examples can grow exponentially .The curse of dimensionality arises when a huge number of different combinations of values of the input variables must be discriminated from each other , and the learning algorithm needs at least one example per relevant combination of values .", "label": "", "metadata": {}, "score": "88.25107"}
{"text": "We have attempted to provide automatically generated PDF copies of documents for which only PostScript versions have previously been available .These are clearly marked in the database - due to the nature of the automatic conversion process , they are likely to be badly aliased when viewed at default resolution on screen by acroread . \" ...", "label": "", "metadata": {}, "score": "88.3013"}
{"text": "We have attempted to provide automatically generated PDF copies of documents for which only PostScript versions have previously been available .These are clearly marked in the database - due to the nature of the automatic conversion process , they are likely to be badly aliased when viewed at default resolution on screen by acroread . \" ...", "label": "", "metadata": {}, "score": "88.3013"}
{"text": "Perplexity ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "88.60194"}
{"text": "Perplexity ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "88.60194"}
{"text": "A communication network that can be used to implement the described distributed processing may use various communication links to transmit data and signals , such as electrically conductor cables , optic fiber links and wireless communication links ( e.g. , RF wireless links ) .", "label": "", "metadata": {}, "score": "88.82797"}
{"text": "P?h m jw ? ? ?i ?Performance evaluation ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "89.007515"}
{"text": "P?h m jw ? ? ?i ?Performance evaluation ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "89.007515"}
{"text": "The model is implemented in the BU speech recognition system and provides a significant improvement in recognition accuracy .An important advantage of the framework of our model is that it is a simple extension of existing language modeling techniques that can easily be integrated with other language modeling advances . \" ...", "label": "", "metadata": {}, "score": "89.14595"}
{"text": "Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 33 . and from ( 39 ) : ?w?i?p?i ? ? ?w i ?N?w i jw?i ? p?i ?P w i ?N?w i jw?i ? p?i ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "90.07738"}
{"text": "Ph.D. dissertation , Thomas Niesler , Cambridge University , June 1997 .Page 33 . and from ( 39 ) : ?w?i?p?i ? ? ?w i ?N?w i jw?i ? p?i ?P w i ?N?w i jw?i ? p?i ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "90.07738"}
{"text": "Results ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "91.03246"}
{"text": "Results ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "91.03246"}
{"text": "The client 1301 , for example , can send a request to the system 1310 for translation of a document .The client 1301 sends the document to the system 1310 .After receiving the document , the system 1310 performs the requested processing .", "label": "", "metadata": {}, "score": "91.087006"}
{"text": "r ?X i ? ?P?N ? ?i ?z ?N ?r ?p r i ? ? ? ?p i ?N ?r ?P ? ?i ?z ?N ?r ?", "label": "", "metadata": {}, "score": "91.79503"}
{"text": "r ?X i ? ?P?N ? ?i ?z ?N ?r ?p r i ? ? ? ?p i ?N ?r ?P ? ?i ?z ?N ?r ?", "label": "", "metadata": {}, "score": "91.79503"}
{"text": "N tl ?w t ? ? ?N tl ?P ? x t ? ? ?T?j?w t ? ? ?N tl ?P ? w s ? ? ?N sl ? j?w t ? ? ?N tl ?", "label": "", "metadata": {}, "score": "91.83397"}
{"text": "N tl ?w t ? ? ?N tl ?P ? x t ? ? ?T?j?w t ? ? ?N tl ?P ? w s ? ? ?N sl ? j?w t ? ? ?N tl ?", "label": "", "metadata": {}, "score": "91.83397"}
{"text": "Punctuation ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "91.97029"}
{"text": "Punctuation ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "91.97029"}
{"text": "I am very grateful to Claudia Arena , Selwyn Blieden , Anna Lindsay , Christophe Molina , Eric Ringger , Piotr Romanowski , and my sister Carola , for proofreading all or sections of this manuscript .Without theirhelp , Icouldnothavepresentedtheresultofmyresearchasit appears here .", "label": "", "metadata": {}, "score": "92.63599"}
{"text": "I am very grateful to Claudia Arena , Selwyn Blieden , Anna Lindsay , Christophe Molina , Eric Ringger , Piotr Romanowski , and my sister Carola , for proofreading all or sections of this manuscript .Without theirhelp , Icouldnothavepresentedtheresultofmyresearchasit appears here .", "label": "", "metadata": {}, "score": "92.63599"}
{"text": "Thesis organisation ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "93.07509"}
{"text": "Thesis organisation ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "93.07509"}
{"text": "Approximate model ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "93.852295"}
{"text": "Approximate model ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "93.852295"}
{"text": "Each processing server 1230 is operable to access the LM servers 1240 to obtain LM data and use the obtained LM data in processing a segment of the input 1201 .This system 1200 can be a speech recognition system where each processing server 1230 is a speech recognition server .", "label": "", "metadata": {}, "score": "96.06852"}
{"text": "This paper presents an overview of the major approaches proposed to address this issue , and offers some perspectives regarding their comparative merits and associated tradeoffs .\u00d3 2003 Elsevier B.V. All rights reserved .ach trained on a separate topic .", "label": "", "metadata": {}, "score": "96.35032"}
{"text": "P u ? ?j ?j ?X ?i ?N ? ?i j ?j ? ? ?P ? ?i j ? k ? j ? ?X ?i ?N ? ?i j ?", "label": "", "metadata": {}, "score": "98.30712"}
{"text": "P u ? ?j ?j ?X ?i ?N ? ?i j ?j ? ? ?P ? ?i j ? k ? j ? ?X ?i ?N ? ?i j ?", "label": "", "metadata": {}, "score": "98.30712"}
{"text": "w?i?jw?i ? p?i ? ? ?N ?w?i?p?i ?b N ?w?i?p?i ? ? ? if N ?w?i?p?i ?w?i ? p?i ? ? ?P ? w?i?jw?i?p ? ? ?i ? if N ?w?i ? p?i ?", "label": "", "metadata": {}, "score": "98.91272"}
{"text": "w?i?jw?i ? p?i ? ? ?N ?w?i?p?i ?b N ?w?i?p?i ? ? ? if N ?w?i?p?i ?w?i ? p?i ? ? ?P ? w?i?jw?i?p ? ? ?i ? if N ?w?i ? p?i ?", "label": "", "metadata": {}, "score": "98.91272"}
{"text": "Discussion ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "99.004135"}
{"text": "Discussion ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "99.004135"}
{"text": "The LOB corpus ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "99.75748"}
{"text": "The LOB corpus ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "99.75748"}
{"text": "Introduction ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "100.47568"}
{"text": "Introduction ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "100.47568"}
{"text": "Second approximation ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "102.35745"}
{"text": "Second approximation ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "102.35745"}
{"text": "Review of conducted work ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "102.76056"}
{"text": "Review of conducted work ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "102.76056"}
{"text": "[0124 ] [ 002 003]:2 .[0125 ] 4-gram section : . [0126 ] [ 002 003 004]:3 .[0127 ][ 002 003 009]:3 .[ 0128 ] 5-gram section : .[ 0129 ] [ 002 003 004 005]:1 .", "label": "", "metadata": {}, "score": "112.0755"}
{"text": "Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .Publisher conditions are provided by RoMEO .", "label": "", "metadata": {}, "score": "113.65498"}
{"text": "Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .Publisher conditions are provided by RoMEO .", "label": "", "metadata": {}, "score": "113.65498"}
{"text": "Experiments are", "label": "", "metadata": {}, "score": "121.7158"}
