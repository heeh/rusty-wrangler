{"text": "The toolkit supports creation and evaluation of a variety of language model types based on N - gram statistics , as well as several related tasks , such as statistical tagging and manipulation of N - best lists and word lattices .", "label": "", "metadata": {}, "score": "31.190464"}
{"text": "The toolkit supports creation and evaluation of a variety of language model types based on N - gram statistics , as well as several related tasks , such as statistical tagging and manipulation of N - best lists and word lattices .", "label": "", "metadata": {}, "score": "31.190464"}
{"text": "The toolkit supports creation and evaluation of a variety of language model types based on N - gram statistics , as well as several related tasks , such as statistical tagging and manipulation of N - best lists and word lattices .", "label": "", "metadata": {}, "score": "31.190464"}
{"text": "This is a recipe to train word n - gram language models using the newswire text provided in the English Gigaword corpus ( 1200 M words of NYT , APW , AFE , XIE ) .It also prepares dictionaries needed to use the LMs with the HTK and Sphinx speech recognizers .", "label": "", "metadata": {}, "score": "31.483242"}
{"text": "We performed experiments on meeting transcription using the NIST RT06s evaluation data and the AMI corpus , with a vocabulary of 50,000 words and a language model training set of up to 211 million words .Our results indicate that power law discounting results in statistically significant reductions in perplexity and word error rate compared to both interpolated and modified Kneser - Ney smoothing , while producing similar results to the hierarchical Pitman - Yor process language model .", "label": "", "metadata": {}, "score": "33.17024"}
{"text": "Improving Language Model Adaptation using Automatic Data Selection and Neural Network .S Jalalvand - RANLP , 2013 - aclweb.org ...On this data we trained a 4-gram back - off LM using the modified shift beta smoothing method as supplied by the IRSTLM toolkit ( Federico , 2008 ) .", "label": "", "metadata": {}, "score": "34.05932"}
{"text": "Related articles All 4 versions Cite Save .Improving Word Translation Disambiguation by Capturing Multiword Expressions with Dictionaries L Bungum , B Gamb\u00e4ck , A Lynum , E Marsi - NAACL HLT 2013 , 2013 - aclweb.org ... performance .The n - gram models were built using the IRSTLM toolkit ( Federico et al . , 2008 ; Bungum and Gamb\u00e4ck , 2012 ) on the DeWaC corpus ( Baroni and Kilgarriff , 2006 ) , using the stopword list from NLTK ( Loper and Bird , 2002 ) . ...", "label": "", "metadata": {}, "score": "35.397766"}
{"text": "The global 5-gram LM smoothed through the improved Kneser - Ney technique is estimated on the target monolingual side of the parallel train- ing data using the IRSTLM toolkit ( Federico et al . , 2008 ) .Models are case - sensitive .", "label": "", "metadata": {}, "score": "35.978043"}
{"text": "The most commonly used language models are very simple ( e.g. a Katz - smoothed trigram model ) .There are many improvements over this simple model however , including caching , clustering , higherorder n - grams , skipping models , and sentence - mixture models , all of which we will describe below .", "label": "", "metadata": {}, "score": "36.3172"}
{"text": "The most commonly used language models are very simple ( e.g. a Katz - smoothed trigram model ) .There are many improvements over this simple model however , including caching , clustering , higherorder n - grams , skipping models , and sentence - mixture models , all of which we will describe below .", "label": "", "metadata": {}, "score": "36.3172"}
{"text": "We propose a new hierarchical Bayesian n - gram model of natural languages .Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman - Yor processes which produce power - law distributions more closely resembling those in natural languages .", "label": "", "metadata": {}, "score": "37.151577"}
{"text": "We propose a new hierarchical Bayesian n - gram model of natural languages .Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman - Yor processes which produce power - law distributions more closely resembling those in natural languages .", "label": "", "metadata": {}, "score": "37.151577"}
{"text": "We contribute two changes that trade between accuracy of these estimates and memory , holding sentence - level scores constant .Common practi ... \" .Approximate search algorithms , such as cube pruning in syntactic machine translation , rely on the language model to estimate probabilities of sentence fragments .", "label": "", "metadata": {}, "score": "37.181038"}
{"text": "In this approach , ( discrete ) words and documents are mapped onto a ( continuous ) semantic vector space , in which familiar clustering techniques can be applied .This leads to the specification of a powerful framework for automatic semantic classification , as well as the derivation of several lan - guage model families with various smoothing properties .", "label": "", "metadata": {}, "score": "37.387638"}
{"text": "Furthermore , our system improves significantly over a baseline system when applied to text from a different domain , and it reduces the sample complexity of sequence labeling . ...5 ; Mnih and Hinton , 2007 ) .One of the benefits of our smoothing technique is that it allows for domain adaptation , a topic that has received a great deal of attention from the NLP ... . ... lar to ours , but they only considered the log - likelihood loss function .", "label": "", "metadata": {}, "score": "38.598236"}
{"text": "This offers a principled approach to language model smoothing , embedding the power - law distribution for natural language .Experiments on the recognition of conversational speech in multiparty meetings demonstrate that by using hierarchical Bayesian language models , we are able to achieve significant reductions in perplexity and word error rate .", "label": "", "metadata": {}, "score": "39.134735"}
{"text": "The model can be pruned down to a smaller size by manipulating the statistics or the estimated model .This paper shows how an n - gram model can be built by ad ... \" .Traditionally , when building an n - gram model , we decide the span of the model history , collect the relevant statistics and estimate the model .", "label": "", "metadata": {}, "score": "39.21633"}
{"text": "The scheme can represent any standard n - gram model and is easily combined with existing model reduction techniques such as entropy - pruning .We demonstrate the space - savings of the scheme via machine translation experiments within a distributed language modeling framework .", "label": "", "metadata": {}, "score": "39.448322"}
{"text": "The scheme can represent any standard n - gram model and is easily combined with existing model reduction techniques such as entropy - pruning .We demonstrate the space - savings of the scheme via machine translation experiments within a distributed language modeling framework .", "label": "", "metadata": {}, "score": "39.448322"}
{"text": "Related articles Cite Save .Sentence simplification as tree transduction D Feblowitz , D Kauchak - Proc . of the Second Workshop on Predicting ... , 2013 - aclweb.org ...The probability of the output tree 's yield , as given by an n - gram language model trained on the simple side of the training corpus using the IRSTLM Toolkit ( Federico et al . , 2008 ) .", "label": "", "metadata": {}, "score": "39.520813"}
{"text": "Experiments verify that our model gives cross entropy results superior to interpolated Kneser - Ney and comparable to modified Kneser - Ney . \" ...Statistical language models used in large - vocabulary speech recognition must properly encapsulate the various constraints , both local and global , present in the language .", "label": "", "metadata": {}, "score": "40.408676"}
{"text": "We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .We investigate for the first time how factors such as training data size , corpus ( e.g. , Br ... \" .", "label": "", "metadata": {}, "score": "40.428905"}
{"text": "Experiments are reported with a high performing baseline , trained on the Chinese - English NIST 2006 Evaluation task and running on a standard Linux 64-bit PC architecture .Comparative tests show that our representation halves the memory required by SRI LM Toolkit , at the cost of 44 % slower translation speed .", "label": "", "metadata": {}, "score": "40.585785"}
{"text": "Common practice uses lowerorder entries in an N - gram model to score the first few words of a fragment ; this violates assumptions made by common smoothing strategies , including Kneser - Ney .Instead , we use a unigram model to score the first word , a bigram for the second , etc .", "label": "", "metadata": {}, "score": "41.009857"}
{"text": "Related articles All 2 versions Cite Save More .Applying Pairwise Ranked Optimisation to Improve the Interpolation of Translation Models .B Haddow - HLT - NAACL , 2013 - aclweb.org ...Schroeder , 2007 ) ) , and is implemented in popular language modelling tools like IRSTLM ( Federico et al . , 2008 ) and SRILM ( Stolcke , 2002 ) .", "label": "", "metadata": {}, "score": "41.180984"}
{"text": "First , requiring no modification to the model , but po- tentially further improving test performance , we will experiment with mutiple domain adaptation by adding more than two corpora into the DHPYLM .Secondly the DHPYLM can integrated into topic models such that the bag - of - words assumption can be avoided .", "label": "", "metadata": {}, "score": "41.316525"}
{"text": "Related articles All 3 versions Cite Save .Dynamically Shaping the Reordering Search Space of Phrase - Based Statistical Machine Translation .A Bisazza , M Federico - TACL , 2013 - transacl.org ...As proposed by Johnson et al .( 2007 ) , statistically improbable phrase pairs are removed from the translation model .", "label": "", "metadata": {}, "score": "41.852318"}
{"text": "We propose a novel interpretation of interpolated Kneser - Ney as approxima ... \" .Interpolated Kneser - Ney is one of the best smoothing methods for n - gram language models .Previous explanations for its superiority have been based on intuitive and empirical justifications of specific properties of the method .", "label": "", "metadata": {}, "score": "41.934425"}
{"text": "In this we have trained our language model using IRSTLM toolkit [ 9].Our transliteration system follows the steps which are represented in figure 1 .Example : ... Cited by 2 Related articles All 5 versions Cite Save .", "label": "", "metadata": {}, "score": "42.020256"}
{"text": "In this paper , we show that some of the commonly used pruning methods do not take into account how removing an - gram should modify the backoff distributions in the state - of - the - art Kneser - Ney smoothing .", "label": "", "metadata": {}, "score": "42.255974"}
{"text": "Our online models build character - level PAT trie structures on the fly using heavily data - unfolded implementations of an mutable daughter maps with a long integer count interface .Terminal nodes are shared .Character 8-gram training runs at 200,000 characters per second and allows online tuning of hyperparameters .", "label": "", "metadata": {}, "score": "42.370255"}
{"text": "Our basic ... \" .We present three novel methods of compactly storing very large n - gram language models .These methods use substantially less space than all known approaches and allow n - gram probabilities or counts to be retrieved in constant time , at speeds comparable to modern language modeling toolkits .", "label": "", "metadata": {}, "score": "42.541977"}
{"text": "This model can be estimated from a single training sequence , yet shares statistical strength between subse- quent symbol predictive distributions in such a way that predictive performance general- izes well .The model builds on a specific pa- rameterization of an unbounded - depth hier- archical Pitman - Yor process .", "label": "", "metadata": {}, "score": "42.698334"}
{"text": "We used N - gram models ( the most successful SLM applied until now [ 8 ] ) of order 1 , 2 and 3 ( called unigrams , bigrams and trigrams respectively ) .Previous works typically preferred the application of ... . by Chengxiang Zhai , Atulya Velivelli , Bei Yu - In Proc . of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining ( KDD'04 , 2004 . \" ...", "label": "", "metadata": {}, "score": "42.725983"}
{"text": "Moreover , using AvgSimC4 which takes contexts into account , the multi - prototype model obtains the best performance ( 65.7 ) .To address this , we introduce a recursive neural network architecture for jointly parsing natural language and learning vector space representations for variable - sized inputs .", "label": "", "metadata": {}, "score": "43.163425"}
{"text": "This paper presents efficient algorithmic and architectural solutions which have been tested within the Moses decoder , an open source toolk ... \" .Statistical machine translation , as well as other areas of human language processing , have recently pushed toward the use of large scale n - gram language models .", "label": "", "metadata": {}, "score": "43.20321"}
{"text": "Related articles All 6 versions Cite Save More .Efficient solutions for word reordering in German - English phrase - based statistical machine translation A Bisazza , M Federico - 8th Workshop on Statistical Machine Translation , 2013 - aclweb.org ... ley Aligner ( Liang et al . , 2006 ) .", "label": "", "metadata": {}, "score": "43.216255"}
{"text": "If the translation model is trained on a parallel corpus , then the language model should be trained on the output side of that corpus , although using additional training data is often beneficial .The toolkits all come with programs that create a language model file , as required by our decoder .", "label": "", "metadata": {}, "score": "43.3841"}
{"text": "Differing provisions from the publisher 's actual policy or licence agreement may be applicable .\" Experiments on the AP News corpus showed that the novel hierarchical Pitman - Yor process language model produces results superior to hierarchical Dirichlet language models and n - gram LMs smoothed by interpolated Kneser - Ney ( IKN ) , and comparable to those smoothed by modified Kneser - Ney ( MKN ) [ 12].", "label": "", "metadata": {}, "score": "43.38616"}
{"text": "\" 0 - 0 \" will learn OSM model over lexical forms and \" 1 - 1 \" will learn OSM model over second factor ( POS / Morph / Cluster - id etc . ) .Learning operation sequences over generalized representations such as POS / Morph tags / word classes , enables the model to overcome data sparsity Durrani et al .", "label": "", "metadata": {}, "score": "43.425632"}
{"text": "The general idea is to build a language model D for each document in the collection , and rank the d .. by Jeff A. Bilmes , Katrin Kirchhoff - in Proceedings of HLT / NACCL , 2003 . \" ...We introduce factored language models ( FLMs ) and generalized parallel backoff ( GPB ) .", "label": "", "metadata": {}, "score": "43.47738"}
{"text": "We built a trigram language model using the IRSTLM lan- guage modeling toolkit ( Federico et al . , 2008 ) .The advantage of this language model was that it con- tained both MSA and dialectal text .IRSTLM : an open source toolkit for handling large scale language models . ...", "label": "", "metadata": {}, "score": "43.562355"}
{"text": "Although we focus on and report performance of these methods as applied to training large neural networks , the underlying algorithms are applicable to any gradient - based machine learning algorithm . ... promise in many practical applications .It has also been observed that increasing the scale of deep learning , with respect to the number of training examples , the number of model parameters , or both , can drastically improve ultimate class ... . \" ... Interpolated Kneser - Ney is one of the best smoothing methods for n - gram language models .", "label": "", "metadata": {}, "score": "43.753185"}
{"text": "The PDP - based n - gram models correspond well to versions of Kneser - Ney smoothing ( Teh , 2006b ) , the state of the art method in applications .\" [ Show abstract ] [ Hide abstract ] ABSTRACT : Exact Bayesian network inference exists for Gaussian and multinomial distributions .", "label": "", "metadata": {}, "score": "44.119102"}
{"text": "These two streams of research motivate us to examine the use of passages in a language model framework .This paper reports on experiments using passages in a simple language model and a relevance model , and compares the results with document - based retrieval .", "label": "", "metadata": {}, "score": "44.314316"}
{"text": "In this work , we present an efficient data structure and optimized algorithms specifically designed for iterative parameter tuning .With the ... \" .Despite the availability of better performing techniques , most language models are trained using popular toolkits that do not support perplexity optimization .", "label": "", "metadata": {}, "score": "44.336773"}
{"text": "Extensions of this approach exploit distributional characteristics of n - gram data to reduce storage costs , including variable length coding of values and the use of tiered structures that partition the data for more efficient storage .We apply our approach to storing the full Google Web1 T n - gram set and all 1-to-5 grams of the Gigaword newswire corpus .", "label": "", "metadata": {}, "score": "44.455997"}
{"text": "We describe a novel neural network architecture for the problem of semantic role labeling .Our method instead learns a direct mapping from source sentence to semantic tags for a given predicate without the aid of a parser or a chunker .", "label": "", "metadata": {}, "score": "44.62446"}
{"text": "This paper shows how an n - gram model can be built by adding suitable sets of n - grams to a unigram model until desired complexity is reached .Very high order n - grams can be used in the model , since the need for handling the full unpruned model is eliminated by the proposed technique .", "label": "", "metadata": {}, "score": "44.672226"}
{"text": "In Proceedings of In- terspeech , Brisbane , Australie . ...Cited by 2 Related articles All 6 versions Cite Save More .Graph Model for Chinese Spell Checking Z Jia , P Wang , H Zhao - Sixth International Joint Conference on Natural ... , 2013 - aclweb.org ...", "label": "", "metadata": {}, "score": "44.717926"}
{"text": "We then compare combinations of both methods .It is shown that a broadcast news language model can be compressed by up to 83 % to only 12.6Mb with no loss in performance on a broadcast news task .Compressing the language model further by quantization to 10.3Mb resulted in only a 0.4 % degradation in word error rate which is better than can be achieved through entropy - based pruning alone . ing all those elements in the list whose contribution to the language model entropy lies below some threshold [ 1].", "label": "", "metadata": {}, "score": "44.839508"}
{"text": "Growing algorithm .Thus , the main differences to KP are the following : We modify the model after each -gram has been pruned , inst ... . \" ...We present three novel methods of compactly storing very large n - gram language models .", "label": "", "metadata": {}, "score": "45.004646"}
{"text": "We show that cluster - based retrieval can perform consistently across collections of realistic size , and significant improvements over document - based retrieval can be obtained in a fully automatic manner and without relevance information provided by human . ... w for parameter tuning .", "label": "", "metadata": {}, "score": "45.018303"}
{"text": "The baseline language model in t ..Power Law Discounting for N - Gram Language Models .Download .Date .Author .Metadata .Abstract .We present an approximation to the Bayesian hierarchical Pitman - Yor process language model which maintains the power law distribution over word tokens , while not requiring a computationally expensive approximate inference process .", "label": "", "metadata": {}, "score": "45.07113"}
{"text": "Proc . of the ... , 2013 - workshop2013.iwslt.org ... Translation and lexicalized reordering models were trained on the parallel training data ; 5-gram LMs with im- proved Kneser - Ney smoothing were estimated on the target side of the training parallel data with the IRSTLM toolkit [ 42]. ...", "label": "", "metadata": {}, "score": "45.145958"}
{"text": "We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .In addition , we introduce two novel smoothing techniques , one a variation of Jelinek - Mercer smoothing and one a very simple linear interpolation technique , both of which outperform existing methods . ... he differences in performance seem to be less when cutoffs are used .", "label": "", "metadata": {}, "score": "45.50967"}
{"text": "We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models .Downpour SGD and Sandblaster L - BFGS both increase the scale and speed of deep network training .We have successfully used our system to train a deep network 30x larger than previously reported in the literature , and achieves state - of - the - art performance on ImageNet , a visual object recognition task with 16 million images and 21k categories .", "label": "", "metadata": {}, "score": "45.629852"}
{"text": "Given a set of comparable text collections , the task of comparative text mining is to discover any latent common themes across all collections as well as summarize the similarity and differ ... \" .In this paper , we define and study a novel text mining prob - lem , which we refer to as comparative text mining .", "label": "", "metadata": {}, "score": "45.67196"}
{"text": "These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit .This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles .Significantly , FLMs with GPB can produce bigrams with signif - icantly lower perplexity , sometimes lower than highly - optimized baseline trigrams .", "label": "", "metadata": {}, "score": "45.71135"}
{"text": "Statistical language models used in large - vocabulary speech recognition must properly encapsulate the various constraints , both local and global , present in the language .While local constraints are readily captured through n - gram modeling , global constraints , such as long - term semantic dependencies , have been more diffi - cult to handle within a data - driven formalism .", "label": "", "metadata": {}, "score": "45.862957"}
{"text": "Two decades of statistical language model- ing : where do we go from here ?In Proceedings of the IEEE , volume 88 , pages 1270 - 1278 , 2000 .SOUCorpus . Y. W. Teh .A hierarchical Bayesian language model based on Pitman - Yor processes .", "label": "", "metadata": {}, "score": "46.014267"}
{"text": "( Teh 2006 ) such that a latent shared , non - domain - specific language model as well as domain specific models are esti- mated together .We call the resulting model the dou- bly hierarchical Pitman - Yor process language model ( DHPYLM ) ( Section 3 ) .", "label": "", "metadata": {}, "score": "46.218967"}
{"text": "..Various software packages for statistical language modeling have been in use for many years - the basic algorithms are simple enough that one can easily implement them with reasonable effort for resea ... . \" ...Previous research on cluster - based retrieval has been inconclusive as to whether it does bring improved retrieval effectiveness over document - based retrieval .", "label": "", "metadata": {}, "score": "46.278725"}
{"text": "An integrative formulation is proposed for harnessing this synergy , in which the latent se - mantic information is used to adjust the standard n - gram proba - bility .Such hybrid language modeling compares favorably with the correspondingn - gram baseline : experiments conducted on the Wall Street Journal domain show a reduction in average word error rate of over 20 % .", "label": "", "metadata": {}, "score": "46.34696"}
{"text": "Its bottom layer of hierarchy consists of multi- ple hierarchical Pitman - Yor process language models , one each for some number of do- mains .The novel top layer of hierarchy con- sists of a mechanism to couple together mul- tiple language models such that they share statistical strength .", "label": "", "metadata": {}, "score": "46.519363"}
{"text": "We present Tightly Packed Tries ( TPTs ) , a compact implementation of read - only , compressed trie structures with fast on - demand paging and short load times .We demonstrate the benefits of TPTs for storing n - gram back - off language models and phrase tables for statistical machine translation .", "label": "", "metadata": {}, "score": "46.668648"}
{"text": "We present Tightly Packed Tries ( TPTs ) , a compact implementation of read - only , compressed trie structures with fast on - demand paging and short load times .We demonstrate the benefits of TPTs for storing n - gram back - off language models and phrase tables for statistical machine translation .", "label": "", "metadata": {}, "score": "46.668648"}
{"text": "We propose a succinct randomized language model which employs a perfect hash function to encode fingerprints of n - grams and their associated probabilities , backoff weights , or other parameters .The scheme can represent any standard n - gram model and is easily combined with existing model reduction te ... \" .", "label": "", "metadata": {}, "score": "46.736504"}
{"text": "We propose a succinct randomized language model which employs a perfect hash function to encode fingerprints of n - grams and their associated probabilities , backoff weights , or other parameters .The scheme can represent any standard n - gram model and is easily combined with existing model reduction te ... \" .", "label": "", "metadata": {}, "score": "46.736504"}
{"text": "We demonstrate our method on a toy example , a database of human wordassociation data , a large set of images of handwritten digits , and a set of feature vectors that represent words . ...We describe a novel neural network architecture for the problem of semantic role labeling .", "label": "", "metadata": {}, "score": "46.75715"}
{"text": "This general problem subsumes many interesting applications , in - cluding business intelligence , summarizing reviews of similar products , and comparing different opinions about a common topic .We propose a generative probabilistic mixture model for comparative text mining .The model simultaneously per - forms cross - collection clustering and within - collection clus - tering , and can be applied to an arbitrary set of compara - ble text collections .", "label": "", "metadata": {}, "score": "46.774086"}
{"text": "To train RDLM on additional monolingual data , or test it on some held - out test / dev data , parse and process it in the same way that the parallel corpus has been processed .This includes tokenization , parsing , truecasing , compound splitting etc . .", "label": "", "metadata": {}, "score": "46.927853"}
{"text": "We then assume that a document is a sample of a mixture model with these theme models as components .We fit such a mixture model to the union of all the text collections we have , and the obtained c .. by Xiaoyong Liu , W. Bruce Croft - In Proceedings of the eleventh international conference on Information and knowledge management , 2002 . \" ...", "label": "", "metadata": {}, "score": "46.96424"}
{"text": "This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings .We present a new neural network architecture which 1 ) learns word embeddings that better capture the semantics of words by incorporating both local and global document context , and 2 ) accounts for homonymy and polysemy by learning multiple embeddings per word .", "label": "", "metadata": {}, "score": "46.97062"}
{"text": "This technique , provided by the IRSTLM toolkit , consists in the linear interpolation of the n - gram probabilities from all component LMs . ...Related articles All 3 versions Cite Save More . ...Related articles Cite Save . 01", "label": "", "metadata": {}, "score": "47.012306"}
{"text": "A number of ... Related articles Cite Save More .Omnifluent English - to - French and Russian - to - English systems for the 2013 Workshop on Statistical Machine Translation E Matusov , G Leusch - Proceedings of the Eighth Workshop on Statistical ... , 2013 - aclweb.org ...", "label": "", "metadata": {}, "score": "47.14804"}
{"text": "An Online Service for SUbtitling by MAchine G van Loenhout , A Walker , Y Georgakopoulou ... - 2013 - sumat - project . eu ... model building plus decoding .To build the language models we have used the state - of - the - art open - source IRSTLM toolkit [ Federico & Cettolo , 2007].", "label": "", "metadata": {}, "score": "47.3796"}
{"text": "The system incorporates two new kinds of acoustic model : triphone models conditioned on speaking rate , and an explicit joint model of within - word phone durations .We also obtained an unusually large improvement from modeling crossword pronunciation variants in \" multiword \" vocabulary items .", "label": "", "metadata": {}, "score": "47.461246"}
{"text": "The system incorporates two new kinds of acoustic model : triphone models conditioned on speaking rate , and an explicit joint model of within - word phone durations .We also obtained an unusually large improvement from modeling crossword pronunciation variants in \" multiword \" vocabulary items .", "label": "", "metadata": {}, "score": "47.461246"}
{"text": "In this paper we investigate the extent to which Katz backoff language models can be compressed through a combination of parameter quantization ( width - wise compression ) and parameter pruning ( length - wise compression ) methods while preserving performance .", "label": "", "metadata": {}, "score": "47.62979"}
{"text": "Community - based post - editing of machine - translated content : monolingual vs. bilingual L Mitchell , J Roturier , S O'Brien - Machine Translation Summit XIV - accept.unige.ch ... the available monolingual English forum data ( approx .a million sentences ) .", "label": "", "metadata": {}, "score": "47.654877"}
{"text": "Applying SLM techniques like trigram language models to Chinese is challenging because ( 1 ) there is no standard definition of words in Chinese ; ( 2 ) word boundaries are not marked by spaces ; and ( 3 ) there is a de ... \" .", "label": "", "metadata": {}, "score": "48.10036"}
{"text": "Applying SLM techniques like trigram language models to Chinese is challenging because ( 1 ) there is no standard definition of words in Chinese ; ( 2 ) word boundaries are not marked by spaces ; and ( 3 ) there is a de ... \" .", "label": "", "metadata": {}, "score": "48.10036"}
{"text": "However , these efforts only support simple s .. by Vesa Siivola , Teemu Hirsim\u00e4ki , Sami Virpioja - IEEE Trans .on Audio , Speech , and Language Processing . \" ...Abstract --gram models are the most widely used language models in large vocabulary continuous speech recognition .", "label": "", "metadata": {}, "score": "48.12452"}
{"text": "Briefly , n - grams are stored in a data structure which privileges memory saving rather than access time .In particular , single components of each n - gram are searched , via binary search , into blocks ... . by Vesa Siivola , Bryan L. Pellom - In Proceedings of 9th European Conference on Speech Communication and Technology , 2005 . \" ...", "label": "", "metadata": {}, "score": "48.215458"}
{"text": "The baseline language model in t ..Tools . \" ...We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .", "label": "", "metadata": {}, "score": "48.269676"}
{"text": "We evaluate the model on two different text data sets ( i.e. , a news article data set and a laptop review data set ) , and compare it with a baseline clustering method also based on a mixture model .Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model . tion of documents .", "label": "", "metadata": {}, "score": "48.31051"}
{"text": "Experiments on Finnish and English text corpora show that the proposed pruning algorithm provides considerable improvements over previous pruning algorithms on Kneser - Ney - smoothed models and is also better than the baseline entropy pruned Good - Turing smoothed models .", "label": "", "metadata": {}, "score": "48.404243"}
{"text": "For applications that are tolerant of a certain class of relatively innocuous errors ( where unseen n - grams may be accepted as rare n - grams ) , we can reduce the latter cost to below 1 byte per n - gram . ... de the use of entropy pruning techniques ( Stolcke , 1998 ) or clustering ( Jelinek et al . , 1990 ; Goodman and Gao , 2000 ) to reduce the number of n - grams that must be stored .", "label": "", "metadata": {}, "score": "48.470215"}
{"text": "Overall increasing the training data size from 360h to 2200h and optimising the training procedure reduced the word error rate on the DARPA / NIST 2003 eval set by about 20 % relative . ...n - grams are trained on different text corpora and then interpolated together with the interpolation weights optimised on a development test set .", "label": "", "metadata": {}, "score": "48.536335"}
{"text": "Overall increasing the training data size from 360h to 2200h and optimising the training procedure reduced the word error rate on the DARPA / NIST 2003 eval set by about 20 % relative . ...n - grams are trained on different text corpora and then interpolated together with the interpolation weights optimised on a development test set .", "label": "", "metadata": {}, "score": "48.536335"}
{"text": "Applying SLM techniques like trigram language models to Chinese is challenging because ( 1 ) there is no standard definition of words in Chinese ; ( 2 ) word boundaries are not marked by spaces ; and ( 3 ) there is a dearth of training data .", "label": "", "metadata": {}, "score": "48.55848"}
{"text": "Applying SLM techniques like trigram language models to Chinese is challenging because ( 1 ) there is no standard definition of words in Chinese ; ( 2 ) word boundaries are not marked by spaces ; and ( 3 ) there is a dearth of training data .", "label": "", "metadata": {}, "score": "48.55848"}
{"text": "A thorough exploration of this space should yield techniques that ultimately will supersede the hidden Markov model . ... irected graphs in a similar way .Moreover , many language models are members of the family of exponential models[44].", "label": "", "metadata": {}, "score": "49.084084"}
{"text": "Intuitively this sharing results in the \" adaptation \" of a latent shared language model to each domain .We intro- duce a general formalism capable of describ- ing the overall model which we call the graph- ical Pitman - Yor process and explain how to perform Bayesian inference in it .", "label": "", "metadata": {}, "score": "49.150085"}
{"text": "Page 7 .286 L. Wang et al . built using GIZA++ [ 26 ] and the training script of Moses .A 5-gram language model was trained using the IRSTLM toolkit [ 27 ] , exploiting improved Modified Kneser- Ney smoothing , and quantizing both , probabilities and back - off weights .", "label": "", "metadata": {}, "score": "49.18104"}
{"text": "To quantize to 8 bits , use -q 8 .If you want to separately control probability and backoff quantization , use -q for probability and -b for backoff .The trie pointers comprise a sorted array .These can be compressed using a technique from Raj and Whittaker by chopping off bits and storing offsets instead .", "label": "", "metadata": {}, "score": "49.18521"}
{"text": "2.2 Examples of EBMs EBM for Classification : Traditional multi - class classifierscan be viewed as particula ... .by Eric H. Huang , Richard Socher , Christopher D. Manning , Andrew Y. Ng - In Proc . of the Annual Meeting of the Association for Computational Linguistics ( ACL , 2012 . \" ...", "label": "", "metadata": {}, "score": "49.260967"}
{"text": "We apply parsimonious models at three stages of the retrieval process:1 ) at indexing time ; 2 ) at search time ; 3 ) at feedback time .Experimental results show that we are able to build models that are significantly smaller than standard models , but that still perform at least as well as the standard approaches . .", "label": "", "metadata": {}, "score": "49.457336"}
{"text": "We apply parsimonious models at three stages of the retrieval process:1 ) at indexing time ; 2 ) at search time ; 3 ) at feedback time .Experimental results show that we are able to build models that are significantly smaller than standard models , but that still perform at least as well as the standard approaches . .", "label": "", "metadata": {}, "score": "49.457336"}
{"text": "Y. W. Teh , M. I. Jordan , M. J. Beal , and D. M. Blei .Hi- erarchical Dirichlet processes .Journal of the American Statistical Association , 101(476):1566 - 1581 , 2006 .X. Zhu and R. Rosenfeld .modeling with the world wide web .", "label": "", "metadata": {}, "score": "49.4617"}
{"text": "Furthermore , we do not have simple frequency cut - offs .Parsimonious language m .. \" ...Language modeling is the art of determining the probability of a sequence of words .This is useful in a large variety of areas including speech recognition , optical character recognition , handwriting recognition , machine translation , and spelling correction ( Church , 1988 ; Brown et al . , 1990 ; Hull , 1 ... \" .", "label": "", "metadata": {}, "score": "49.59824"}
{"text": "Furthermore , we do not have simple frequency cut - offs .Parsimonious language m .. \" ...Language modeling is the art of determining the probability of a sequence of words .This is useful in a large variety of areas including speech recognition , optical character recognition , handwriting recognition , machine translation , and spelling correction ( Church , 1988 ; Brown et al . , 1990 ; Hull , 1 ... \" .", "label": "", "metadata": {}, "score": "49.59824"}
{"text": "for more options , run train_rdlm.py --help .Parameters you may want to adjust include the size of the vocabulary and the neural network layers , and the number of training epochs .A Hierarchical Nonparametric Bayesian Approach to Statistical Language Model Domain Adaptation .", "label": "", "metadata": {}, "score": "49.706413"}
{"text": "To find an n - gram , one would start from the ( em ... . \" ...Trigram language models are compressed using a Golomb coding method inspired by the original Unix spell program .Compression methods trade off space , time and accuracy ( loss ) .", "label": "", "metadata": {}, "score": "49.926296"}
{"text": "These developments include measures to make training from large data sets more efficient , to implement additional language modeling techniques ( such as for adaptation and smoothing ) , and for client / server operation .In addition , the functionality for lattice processing has been greatly expanded .", "label": "", "metadata": {}, "score": "49.95384"}
{"text": "Sources of training data suitable for language modeling of conversational speech are limited .In this paper , we show how training data can be supplemented with text from the web filtered to match the style and/or topic of the target recognition task , but also that it is possible to get bigger perfor ... \" .", "label": "", "metadata": {}, "score": "50.015854"}
{"text": "Sources of training data suitable for language modeling of conversational speech are limited .In this paper , we show how training data can be supplemented with text from the web filtered to match the style and/or topic of the target recognition task , but also that it is possible to get bigger perfor ... \" .", "label": "", "metadata": {}, "score": "50.015854"}
{"text": "Conversely , we show how to save memory by collapsing probability and backoff into a single value without changing sentence - level scores , at the expense of less accurate estimates for sentence fragments .These changes can be stacked , achieving better estimates with unchanged memory usage .", "label": "", "metadata": {}, "score": "50.234795"}
{"text": "Equivalent test corpus perplexity could be had using a SOU - only HPYLM model by more than doubling the amount of SOU domain specific training words .In applica- tion domains where adding more out - of - domain data is significantly cheaper than acquiring more in - domain training data this could result in substantial savings .", "label": "", "metadata": {}, "score": "50.411163"}
{"text": "Recent developments in language modeling approach to IR provided a new effective alternative to traditional retrieval models .These two stre ... \" .Previous research has shown that passage - level evidence can bring added benefits to document retrieval when documents are long or span different subject areas .", "label": "", "metadata": {}, "score": "50.657562"}
{"text": "We show how to perform inference in such a model without truncation approximation and introduce fragmentation operators nec- essary to do predictive inference .We demon- strate the sequence memoizer by using it as a language model , achieving state - of - the - art results .", "label": "", "metadata": {}, "score": "50.737778"}
{"text": "Cited by 6 Related articles All 4 versions Cite Save .Statistical machine translation system for English to Urdu RB Mishra - International Journal of Advanced Intelligence ... , 2013 - Inderscience ...Modified Kneser - Ney discounting is used as smoothing scheme for training 5-gram language model .", "label": "", "metadata": {}, "score": "50.791855"}
{"text": "GPB extends sta ... \" .We introduce factored language models ( FLMs ) and generalized parallel backoff ( GPB ) .An FLM represents words as bundles of features ( e.g. , morphological classes , stems , data - driven clusters , etc . ) , and induces a prob - ability model covering sequences of bundles rather than just words .", "label": "", "metadata": {}, "score": "50.844616"}
{"text": "With the resulting implementation , we demonstrate the feasibility and effectiveness of such iterative techniques in language model estimation .Index Terms : language modeling , smoothing , interpolation 1 . ...[ 3 ] , most work in the field opts for simpler techniques with inferior results .", "label": "", "metadata": {}, "score": "51.046024"}
{"text": "..the interpolation weights are estimated by using the EM algorithm .4.4 Language model pruning Our system can produce an SLM given a memory constraint .The basic idea is to remove as many useless probabilities as possible without increasing the perplexity .", "label": "", "metadata": {}, "score": "51.197422"}
{"text": "..the interpolation weights are estimated by using the EM algorithm .4.4 Language model pruning Our system can produce an SLM given a memory constraint .The basic idea is to remove as many useless probabilities as possible without increasing the perplexity .", "label": "", "metadata": {}, "score": "51.197422"}
{"text": "Related articles Cite Save More .Robustness of Distant - Speech Recognition and Speaker Identification - Development of Baseline System G Potamianos , A Abad , A Brutti , M Hagmuller , G Kubin ... - 2013 - dirha.fbk.eu ... industrial applications .", "label": "", "metadata": {}, "score": "51.30435"}
{"text": "Cited by 4 Related articles All 12 versions Cite Save .Large - scale multiple language translation accelerator at the United Nations B Pouliquen , C Elizalde , M Junczys - Dowmunt ... - mtsummit2013.info ... Language models are being computed with the IRSTLM toolkit ( Federico et al . , 2008 ) .", "label": "", "metadata": {}, "score": "51.348312"}
{"text": "We train a Support Vector Machine classifier to distinguish the positive from the negative instances .We show how to partition the examples for efficient training with 57 thousand features and 6.5 million training instances .The model outperforms other recent approaches , achieving excellent correlation with human plausibility judgments .", "label": "", "metadata": {}, "score": "51.524193"}
{"text": "We demonstrate that distributional representations of word types , trained on unannotated text , can be used to improve performance on rare words .We incorporate aspects of these representations into the feature space of our sequence - labeling systems .In an experiment on a standard chunking dataset , our best technique improves a chunker from 0.76 F1 to 0.86 F1 on chunks beginning with rare words .", "label": "", "metadata": {}, "score": "51.63496"}
{"text": "3 we plot the \" baseline \" test perplexity for a HPYLM model trained on SOU corpus data alone ( this is the same baseline as was established in Fig . 2 ) .Tests were run for each combination of Brown and SOU training corpus sizes shown by the small crosses and absolute test perplexity improvement was inter- polated between these points to produce isosurfaces of test perplexity improvement .", "label": "", "metadata": {}, "score": "51.665222"}
{"text": "In Finnish speech recognition tests , the models trained by the growing method outperform the entropy pruned models of similar size .We need to encode which words actually make up the ngram .The tree contains all the n - grams of the model , regardless of the order of the n - gram .", "label": "", "metadata": {}, "score": "51.798157"}
{"text": "We intro- duce a general formalism capable of describ- ing the overall model which we call the graph- ical Pitman - Yor process and explain how to perform Bayesian inference in it .We present encouraging language model domain adapta- tion results that both illustrate the potential benefits of our new model and suggest new avenues of inquiry .", "label": "", "metadata": {}, "score": "51.81864"}
{"text": "2013 ) , except that ' --num_hidden 0 ' results in a model with a single hidden layer , which is recommended for decoder integration .Vaswani et al .( 2013 ) recommend using special null words which are the weighted average of all input embeddings to pad lower - order estimates .", "label": "", "metadata": {}, "score": "51.83646"}
{"text": "As such , ... \" .We systematically investigate a new approach to estimating the parameters of language models for information retrieval , called parsimonious language models .Parsimonious language models explicitly address the relation between levels of language models that are typically used for smoothing .", "label": "", "metadata": {}, "score": "51.972122"}
{"text": "As such , ... \" .We systematically investigate a new approach to estimating the parameters of language models for information retrieval , called parsimonious language models .Parsimonious language models explicitly address the relation between levels of language models that are typically used for smoothing .", "label": "", "metadata": {}, "score": "51.972122"}
{"text": "Section 5 compares the DH- PYLM to previous language model domain adapta- tion approaches and Section 6 reports on experiments showing the effectiveness of the new model .We start in the next section by reviewing language modeling and the HPYLM in particular .", "label": "", "metadata": {}, "score": "51.973923"}
{"text": "( 2014 ) , a neural network language model that uses a target - side history as well as source - side context , is implemented in Moses as BilingualLM .It uses NPLM as back - end ( check its installation instructions ) .", "label": "", "metadata": {}, "score": "52.10479"}
{"text": "Our first contribution is the development of a sensible construction for it .Our second contribu- tion is the development of a new class of nonparametric Bayesian models which we call graphical Pitman - Yor processes and the derivation of generic inference al- gorithms for them ( Section 4 ) .", "label": "", "metadata": {}, "score": "52.274513"}
{"text": "Lexica of variable size ( from 10,000 to 50,000 words ) have been used .Our approach is described in detail and compared with other methods presented in the literature to deal with the same problem .An experimental setup to correctly deal with unconstrained text recognition is proposed . .", "label": "", "metadata": {}, "score": "52.32792"}
{"text": "[ Show abstract ] [ Hide abstract ] ABSTRACT : Traditional n -gram language models are widely used in state - of - the - art large vocabulary speech recognition systems .This simple model suffers from some limitations , such as overfitting of maximum - likelihood estimation and the lack of rich contextual knowledge sources .", "label": "", "metadata": {}, "score": "52.459583"}
{"text": "input - type : The format of the input data .The following four formats are supported .counts n - gram counts file ( one count and one n - gram per line ) ; .Given a ' corpus ' file the toolkit will create a ' counts ' file which may be reused ( see examples below ) .", "label": "", "metadata": {}, "score": "52.664135"}
{"text": "The .Page 8 .614 A Hierarchical Nonparametric Bayesian Approach to Statistical Language Model Domain Adaptation adding more data and another ( likely more expensive ) resource cost to acquire more in - domain training data .While the costs specific to each application domain are different , Fig .", "label": "", "metadata": {}, "score": "53.259254"}
{"text": "Finally , we applied a generalized ROVER algorithm to combine the N - best hypotheses from several systems based on different acoustic models .ts had been optimized for perplexity on prior evaluation data .Lattice expansion used an unpruned , trigram backoff LM ( 4.8 M bigrams , 11.5 M trigrams ) constructed in the same fashion .", "label": "", "metadata": {}, "score": "53.28398"}
{"text": "Finally , we applied a generalized ROVER algorithm to combine the N - best hypotheses from several systems based on different acoustic models .ts had been optimized for perplexity on prior evaluation data .Lattice expansion used an unpruned , trigram backoff LM ( 4.8 M bigrams , 11.5 M trigrams ) constructed in the same fashion .", "label": "", "metadata": {}, "score": "53.28398"}
{"text": "MIT Press , 2007 .R. Iyer , M. Ostendorf , and H. Gish .Using out - of - domain data to improve in - domain language models .IEEE Sig- nal processing letters , 4:221 - 223 , 1997 .", "label": "", "metadata": {}, "score": "53.288612"}
{"text": "First , create a special directory stat under your working directory , where the script will save lots of temporary files ; then , simply run the script build - lm . sh as in the example : .The script builds a 3-gram LM ( option -n ) from the specified input command ( -i ) , by splitting the training procedure into 10 steps ( -k ) .", "label": "", "metadata": {}, "score": "53.637726"}
{"text": "In a German - English Moses system with target - side syntax , improved estimates yielded a 63 % reduction in CPU time ; for a Hiero - style version , the reduction is 21 % .The compressed language model uses 26 % less RAM while equivalent search quality takes 27 % more CPU .", "label": "", "metadata": {}, "score": "53.66619"}
{"text": "Positive examples are taken from observed predicate - argument pairs , while negatives are constructed from unobserved combinations .We train a Support Vector Machine classifier to distinguish the positive from ... \" .We present a discriminative method for learning selectional preferences from unlabeled text .", "label": "", "metadata": {}, "score": "53.852386"}
{"text": "In Proceedings of the IEEE International Conference on Acoustics , Speech , and Sig- nal Processing , pages 586 - 589 , 1993 . H. Kucera and W. N. Francis .Computational analysis of present - day American English .Brown University Press , Providence , RI , 1967 .", "label": "", "metadata": {}, "score": "53.90283"}
{"text": "In fact , the lower ... . by E. W. D. Whittaker , B. Raj - the European Conference on Speech Communication and Technology , 2001 . \" ...In this paper we investigate the extent to which Katz backoff language models can be compressed through a combination of parameter quantization ( width - wise compression ) and parameter pruning ( length - wise compression ) methods while preserving performance .", "label": "", "metadata": {}, "score": "54.150852"}
{"text": "by Jeffrey A. Bilmes - Mathematical Foundations of Speech and Language Processing , 2003 . \" ...Graphical models provide a promising paradigm to study both existing and novel techniques for automatic speech recognition .This paper first provides a brief overview of graphical models and their uses as statistical models .", "label": "", "metadata": {}, "score": "54.194527"}
{"text": "The processing aims at collapsing the sequence of microtags defining a chunk to the label of that chunk .The chunk LM is then queried with n - grams of chunk labels , in an asynchronous manner with respect to the sequence of words , as in general chunks consist of more words .", "label": "", "metadata": {}, "score": "54.3797"}
{"text": "Cited by 2 Related articles All 2 versions Cite Save More .An English - to - Hungarian Morpheme - based Statistical Machine Translation System with Reordering Rules LJ Laki , A Nov\u00e1k , B Sikl\u00f3si - ACL 2013 , 2013 - aclweb.org ... task .", "label": "", "metadata": {}, "score": "54.590878"}
{"text": "Alessandro Vinciarelli , Samy Bengio , Horst Bunke - IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE , 2004 . \" ...This paper presents a system for the offline recognition of large vocabulary unconstrained handwritten texts .The only assumption made about the data is that it is written in English .", "label": "", "metadata": {}, "score": "54.634407"}
{"text": "However , most of these models are built with only local context and one representation per word .This is problematic because words are often polysemous and ... \" .Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems .", "label": "", "metadata": {}, "score": "54.738205"}
{"text": "There are more open source statistical language modeling toolkits available like IRSTLM , RandLM and KenLM .D ... Related articles Cite Save More . ...Cited by 2 Related articles All 2 versions Cite Save .Unsupervised and Semi - supervised Myanmar Word Segmentation Approaches for Statistical Machine Translation YK Thu , A Finch , E Sumita , Y Sagisaka - saki.siit.tu.ac.th ... blog data[16].", "label": "", "metadata": {}, "score": "54.832123"}
{"text": "[ 7 ] Marcello Federico , Nicola Bertoldi , Mauro Cettolo .2008 IRSTLM : an Open Source Toolkit for Handling Large Scale Language Models .In Proceedings of Interspeech 2008 , 1618 - 1621 . ...Cited by 1 Related articles Cite Save More .", "label": "", "metadata": {}, "score": "54.90702"}
{"text": "ICASSP , 2005 . \" ...Typical systems for large vocabulary conversational speech recognition ( LVCSR ) have been trained on a few hundred hours of carefully transcribed acoustic training data .This paper describes an LVCSR system for the conversational telephone speech ( CTS ) task trained on more than 2000 hours of data for ... \" .", "label": "", "metadata": {}, "score": "55.003777"}
{"text": "ICASSP , 2005 . \" ...Typical systems for large vocabulary conversational speech recognition ( LVCSR ) have been trained on a few hundred hours of carefully transcribed acoustic training data .This paper describes an LVCSR system for the conversational telephone speech ( CTS ) task trained on more than 2000 hours of data for ... \" .", "label": "", "metadata": {}, "score": "55.003777"}
{"text": "Keywords - Latent semantic analysis , multispan integration , n - grams , speech recognition , statistical language modeling .I. . ...n word sequences .In the two past decades , statistical -grams have steadily emerged as the preferred way to impose such constraints in a wide range of domains [ 21].", "label": "", "metadata": {}, "score": "55.107834"}
{"text": "Abstract --gram models are the most widely used language models in large vocabulary continuous speech recognition .Since the size of the model grows rapidly with respect to the model order and available training data , many methods have been proposed for pruning the least relevant - grams from the model .", "label": "", "metadata": {}, "score": "55.158752"}
{"text": "The buildlm binary ( in randlm / bin ) preprocesses and builds randomized language models .The toolkit provides three ways for building a randomized language models : . from a tokenised corpus ( this is useful for files around 100 million words or less ) .", "label": "", "metadata": {}, "score": "55.380745"}
{"text": "We describe SRI 's large vocabulary conversational speech recognition system as used in the March 2000 NIST Hub-5E evaluation .The system performs four recognition passes : ( 1 ) bigram recognition with phone - loop - adapted , within - word triphone acoustic models , ( 2 ) lattice generation with transcription - m ... \" .", "label": "", "metadata": {}, "score": "55.517708"}
{"text": "We describe SRI 's large vocabulary conversational speech recognition system as used in the March 2000 NIST Hub-5E evaluation .The system performs four recognition passes : ( 1 ) bigram recognition with phone - loop - adapted , within - word triphone acoustic models , ( 2 ) lattice generation with transcription - m ... \" .", "label": "", "metadata": {}, "score": "55.517708"}
{"text": "In addition , we introduce two novel smoothing techniques , one a variation of Jelinek - Mercer smoothing and one a very simple linear interpolation technique , both of which outperform existing methods . ... he differences in performance seem to be less when cutoffs are used .", "label": "", "metadata": {}, "score": "55.80421"}
{"text": "Cross - entropy and speech recognition In this section , we briefly examine how the performance of a language model meas ... . by Andreas Stolcke - IN PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING ( ICSLP 2002 , 2002 . \" ...", "label": "", "metadata": {}, "score": "56.040882"}
{"text": "Cross - entropy and speech recognition In this section , we briefly examine how the performance of a language model meas ... . by Andreas Stolcke - IN PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING ( ICSLP 2002 , 2002 . \" ...", "label": "", "metadata": {}, "score": "56.040882"}
{"text": "The result is compiled models that are larger than the training models , but execute at 2 million characters per second on a desktop PC .Cross - entropy on held - out data shows these models to be state of the art in terms of performance . ... piled to a less compact but more efficient static representation . \" ...", "label": "", "metadata": {}, "score": "56.1389"}
{"text": "ucl.ac.uk Abstract In this paper we present a doubly hierarchi- cal Pitman - Yor process language model .Its bottom layer of hierarchy consists of multi- ple hierarchical Pitman - Yor process language models , one each for some number of do- mains .", "label": "", "metadata": {}, "score": "56.2582"}
{"text": "HashTBO made it possible to ship a trigram contextual speller in Microsoft Office 2007 . \" ...We describe the implementation steps required to scale high - order character language models to gigabytes of training data without pruning .Our online models build character - level PAT trie structures on the fly using heavily data - unfolded implementations of an mutable daughter maps with a long intege ... \" .", "label": "", "metadata": {}, "score": "56.287674"}
{"text": "To determine the amount of RAM each data structure will take , provide only the arpa file : .Bear in mind that this includes only language model size , not the phrase table or decoder state .Building the trie entails an on - disk sort .", "label": "", "metadata": {}, "score": "56.320633"}
{"text": "This paper describes an LVCSR system for the conversational telephone speech ( CTS ) task trained on more than 2000 hours of data for which only approximate transcriptions were available .The challenges of dealing which such a large data set and the accuracy improvements over the small baseline system are discussed .", "label": "", "metadata": {}, "score": "56.403267"}
{"text": "This paper describes an LVCSR system for the conversational telephone speech ( CTS ) task trained on more than 2000 hours of data for which only approximate transcriptions were available .The challenges of dealing which such a large data set and the accuracy improvements over the small baseline system are discussed .", "label": "", "metadata": {}, "score": "56.403267"}
{"text": "Collobert and Weston [ CW08 ] have shown that neural networks can perform well on sequence labeling language processing tasks while also learning appropriate featur ... .by Shane Bergsma , Dekang Lin , Google Inc , Randy Goebel - In Proc . of EMNLP , 2008 . \" ...", "label": "", "metadata": {}, "score": "56.602325"}
{"text": "To automatically achieve this , an unsupervised clustering approach ... Related articles All 4 versions Cite Save More .Identifying multilingual Wikipedia articles based on cross language similarity and activity KN Tran , P Christen - Proceedings of the 22nd ACM international ... , 2013 - dl.acm.org ...", "label": "", "metadata": {}, "score": "56.765312"}
{"text": "py It depends on NPLM for neural network training and querying .RDLM is trained on a corpus annotated with dependency syntax .The training scripts support the same format as used for training a string - to - tree translation model .", "label": "", "metadata": {}, "score": "56.85997"}
{"text": "1INTRODUCTIONConsider the problem of statistical natural language model domain adaptation .Statistical language mod- els typically have a very large number of parameters and thus need a large quantity of training data to pro- duce good estimates of those parameters .", "label": "", "metadata": {}, "score": "56.885506"}
{"text": "We show how to visualize a set of pairwise similarities between objects by using several different two - dimensional maps , each of which captures different aspects of the similarity structure .When the objects are ambiguous words , for example , different senses of a word occur in different maps , so \" ri ... \" .", "label": "", "metadata": {}, "score": "57.04973"}
{"text": "This is specified by another arguments in the feature function for the KENLM feature function : .I recommend fully loading if you have the RAM for it ; it actually takes less time to load the full model and use it because the disk does not have to seek during decoding .", "label": "", "metadata": {}, "score": "57.085575"}
{"text": "..The ability to approxima ... . by Djoerd Hiemstra , Stephen Robertson , Hugo Zaragoza - In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , 2004 . \" ...We systematically investigate a new approach to estimating the parameters of language models for information retrieval , called parsimonious language models .", "label": "", "metadata": {}, "score": "57.093452"}
{"text": "..The ability to approxima ... . by Djoerd Hiemstra , Stephen Robertson , Hugo Zaragoza - In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , 2004 . \" ...We systematically investigate a new approach to estimating the parameters of language models for information retrieval , called parsimonious language models .", "label": "", "metadata": {}, "score": "57.093452"}
{"text": "model.counts.sorted : This is a file in the RandLM ' counts ' format with one count followed by one n - gram per line .It can be specified as shown in Example 3 below to avoid recomputation when building multiple randomized language models from the same corpus . would construct a new randomized language model ( model4.BloomMap ) from the same data as used in Example 1 but with a different error rate ( here -falsepos 4 ) .", "label": "", "metadata": {}, "score": "57.207794"}
{"text": "Proc . of IWSLT , 2013 - eu - bridge . ...Cited by 2 Related articles All 7 versions Cite Save More . ...Related articles Cite Save .Context Dependent Bag of words generation SA Jadhav , DVLN Somayajulu ... - Advances in ... , 2013 - ieeexplore.ieee.org ... ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?", "label": "", "metadata": {}, "score": "57.25613"}
{"text": "Related articles All 2 versions Cite Save More .Federico , M. , Bertoldi , N. , Cettolo , M. : IRSTLM : an Open Source Toolkit for Handling Large Scale Language Models . ...Cited by 2 Related articles All 4 versions Cite Save .", "label": "", "metadata": {}, "score": "57.37269"}
{"text": "Related articles All 2 versions Cite Save More .Statistical sentiment analysis performance in Opinum B Bonev , G Ram\u00edrez - S\u00e1nchez , SO Rojas - arXiv preprint arXiv:1303.0446 , 2013 - arxiv.org ...In our setup we use the IRSTLM open - source library for building the language model . ...", "label": "", "metadata": {}, "score": "57.75686"}
{"text": "Cited by 5 Related articles All 9 versions Cite Save More .Simple , readable sub - sentences .S Klerke , A S\u00f8gaard - ACL ( Student Research Workshop ) , 2013 - aclweb.org ... dsl .Generative and discriminative methods for online adaptation in smt K W\u00e4schle , P Simianer , N Bertoldi ... - Proceedings of the ... , 2013 - wiki.cl.uni-heidelberg .", "label": "", "metadata": {}, "score": "57.8299"}
{"text": "Marcello Federico , Nicola Bertoldi , and Mauro Cet- tolo .IRSTLM : an open source toolkit for handling large scale language models .In Proceed- ings of Interspeech , pages 1618 - 1621 . ...Cited by 3 Related articles All 8 versions Cite Save More .", "label": "", "metadata": {}, "score": "57.864006"}
{"text": "Many of these approaches learn both the feature weights and the feature representation .Vectors must be kept low - dimensional for tractability , while learning and inference on ... . by Jeffrey Dean , Greg S. Corrado , Rajat Monga , Kai Chen , Matthieu Devin , Quoc V. Le , Mark Z. Mao , Andrew Senior , Paul Tucker , Ke Yang , Andrew Y. Ng . \" ...", "label": "", "metadata": {}, "score": "58.04416"}
{"text": "Improving trigram language .Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .", "label": "", "metadata": {}, "score": "58.43933"}
{"text": "The improvements in the Finnish speech recognition over the other Kneser - Ney smoothed models are statistically significant , as well .Index Terms - Modeling , natural languages , smoothing methods , speech recognition .I. . ... probability of the training data given by the current model .", "label": "", "metadata": {}, "score": "58.579216"}
{"text": "As opposed to past explanations , our interpretation can recover exactly the formulation of interpolated Kneser - Ney , and performs better than interpolated Kneser - Ney when a better inference procedure is used . by James Cook , Ilya Sutskever , Andriy Mnih , Geoffrey Hinton - In AI and Statistics , 2007 .", "label": "", "metadata": {}, "score": "58.687943"}
{"text": "Consequently , they have difficulty estimating parameters for types which appear in the test set , but seldom ( or never ) appear in the ... \" .Supervised sequence - labeling systems in natural language processing often suffer from data sparsity because they use word types as features in their prediction tasks .", "label": "", "metadata": {}, "score": "58.76847"}
{"text": "Graphical Pitman - Yor pro- cesses form a general framework within which to ex- plore a large variety of language models while retaining the same inference engine .We intend to undertake a more detailed and thorough theoretical treatment of graphical Pitman - Yor processes .", "label": "", "metadata": {}, "score": "58.773018"}
{"text": "Moses offers the option to add an additional LM feature that counts the number of occurrences of unknown words in a hypothesis .Most language model implementations in Moses support this feature .Moses can also use language models created with the IRSTLM toolkit ( see Federico & Cettolo , ( ACL WS - SMT , 2007 ) ) .", "label": "", "metadata": {}, "score": "58.86657"}
{"text": "Brocki - Intelligent Tools for Building a Scientific ... , 2013 - Springer ...Page 6 .494 D. Kor\u017einek , K. Marasek , and ?Brocki Table 1 .Experiment results comparing our system to the Julius baseline using models from IRSTLM on a 30k and 60k vocabulary . ...", "label": "", "metadata": {}, "score": "59.078354"}
{"text": "rpus .Combining several N - grams can produce a model with a very large number of parameters , which is costly in decoding .In such cases N - grams are typically pruned .The same pruning parameters were applied to all models in our experiments .", "label": "", "metadata": {}, "score": "59.33359"}
{"text": "rpus .Combining several N - grams can produce a model with a very large number of parameters , which is costly in decoding .In such cases N - grams are typically pruned .The same pruning parameters were applied to all models in our experiments .", "label": "", "metadata": {}, "score": "59.33359"}
{"text": "To estimate the fluency of the descriptions we use IRSTLM [ 6 ] which is based on n - gram statistics of TACoS. The final ...Cited by 3 Related articles All 9 versions Cite Save .System Description of BJTU - NLP MT for NTCIR-10 PatentMT P Wu , J Xu , Y Yin , Y Zhang - Proceedings of NTCIR , 2013 - research.nii.ac.jp ...", "label": "", "metadata": {}, "score": "59.34841"}
{"text": "Cited by 5 Related articles All 16 versions Cite Save More .Parameter Optimization for Iterative Confusion Network Decoding in Weather - Domain Speech Recognition S Jalalvand , D Falavigna - eu - bridge .iCPE : A Hybrid Data Selection Model for SMT Domain Adaptation L Wang , DF Wong , LS Chao , Y Lu , J Xing - ...", "label": "", "metadata": {}, "score": "59.35274"}
{"text": "Final model building will still use the amount of memory needed to store the model .The -T option lets you customize where to place temporary files ( the default is based on the output file name ) .KenLM supports lazy loading via mmap .", "label": "", "metadata": {}, "score": "59.75825"}
{"text": "At the same time , they can be mapped into memory quickly and be searched directly in time linear in the length of the key , without the need to decompress the entire file .The overhead for local decompression during search is marginal . \" ...", "label": "", "metadata": {}, "score": "59.964466"}
{"text": "At some point you will discover that you can not build a LM using your data .RandLM natively uses a disk - based method for creating n - grams and counts , but this will be slow for large corpora .", "label": "", "metadata": {}, "score": "60.28349"}
{"text": "In this paper , we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores .We have developed a ... \" .Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance .", "label": "", "metadata": {}, "score": "60.308506"}
{"text": "The folder should contain files as ( for example ( tune.de , tune.en , tune.align ) .Interpolation script does not work with LMPLZ and will require SRILM installation .RDLM ( Sennrich 2015 ) is a language model for the string - to - tree decoder with a dependency grammar .", "label": "", "metadata": {}, "score": "60.354527"}
{"text": "I used the top 5 K , 20 K , and 64 K words occurring in the training text as vocabularies .Both verbalized punctuation ( VP ) and non - verbalized punctuation ( NVP ) LMs are built .Here are some OOV% and perplexity results measured on three different held - out evaluation test sets : Tools . by Andreas Stolcke - IN PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING ( ICSLP 2002 , 2002 . \" ...", "label": "", "metadata": {}, "score": "60.39277"}
{"text": "It works in much the same way as SRI and IRST 's inverted option .Like probing , unigram lookup is an array index .Records in the trie have a word index , probability , backoff , and pointer .All of the records for n - grams of the same order are stored consecutively in memory .", "label": "", "metadata": {}, "score": "60.426445"}
{"text": "randlm : The path of the randomized language model built using the buildlm tool as described above .test - path : The location of test data to be scored by the model .test - type : The format of the test data : currently corpus and ngrams are supported .", "label": "", "metadata": {}, "score": "60.439323"}
{"text": "A fast and flexible architecture for very large word n - gram datasets M Flor - Natural Language Engineering , 2013 - Cambridge Univ Press Page 1 .Natural Language Engineering 19 ( 1 ) : 61 - 93 .c Cambridge University Press 2012 doi:10.1017/S1351324911000349 61 A fast and flexible architecture for very large word n - gram datasets MICHAEL FLOR NLP and ...", "label": "", "metadata": {}, "score": "60.7586"}
{"text": "IRSTLM : an open source toolkit for handling large scale language models . ...Related articles All 10 versions Cite Save .Edit Distance : A New Data Selection Criterion for Domain Adaptation in SMT . 3.3 Baseline System ... Related articles All 2 versions Cite Save More . ...", "label": "", "metadata": {}, "score": "61.083588"}
{"text": "Previous research on cluster - based retrieval has been inconclusive as to whether it does bring improved retrieval effectiveness over document - based retrieval .Recent developments in the language modeling approach to IR have motivated us to re - examine this problem within this new retrieval framework .", "label": "", "metadata": {}, "score": "61.204723"}
{"text": "S. F. Chen and J. T. Goodman .An empirical study of smoothing techniques for language modeling .Technical Report TR-10 - 98 , Dept . of Comp .Sci . , Harvard , 1998 .S. Della Pietra , V. Della Pietra , R. Mercer , and S. Roukos .", "label": "", "metadata": {}, "score": "61.75817"}
{"text": "IRSTLM : an open source toolkit for handling large scale language models .In Interspeech 2008 , pages 1618 - 1621 , Brisbane , Australia . ...Cited by 3 Related articles All 2 versions Cite Save More .Constrained grammatical error correction using Statistical Machine Translation Z Yuan , M Felice - CoNLL-2013 , 2013 - aclweb.org ... systems . ...", "label": "", "metadata": {}, "score": "61.845737"}
{"text": "Since this is a time - space tradeoff ( time is linear in the number of bits chopped ) , you can set the upper bound number of bits to chop using -a .To minimize memory , use -a 64 .", "label": "", "metadata": {}, "score": "62.04094"}
{"text": "This format can be properly managed through the compile - lm command in order to produce a compiled version or a standard ARPA version of the LM .For a detailed description of the procedure and other commands available under IRSTLM please refer to the user manual supplied with the package .", "label": "", "metadata": {}, "score": "62.336494"}
{"text": "Probability is always non - positive , so the sign bit is also removed .Since the trie stores many vocabulary ids and uses the minimum number of bits to do so , vocabulary filtering is highly effective for reducing overall model size even if less n - grams of higher order are removed .", "label": "", "metadata": {}, "score": "62.52343"}
{"text": "The phrase - based baseline decoder includes ...Cited by 1 Related articles All 11 versions Cite Save More .How hard is it to automatically translate phrasal verbs from English to French ? statmt .org / moses/ ? Baseline . ing the grow - diag - final heuristic .", "label": "", "metadata": {}, "score": "62.752697"}
{"text": "Minimum error rate ... Related articles Cite Save More .( 2 ) You 'd better run the baseline of Moses .( 3 ) Python 2.7 .Here we use irstlm to train language model , as this baseline suggest .", "label": "", "metadata": {}, "score": "62.80367"}
{"text": "NAIST at 2013 CoNLL grammatical error correction shared task I Yoshimoto , T Kose , K Mitsuzawa , K Sakaguchi ... -CoNLL-2013 , 2013 - aclweb.org ... 515 as the alignment tool .The grow - diag - final heuristics was applied for phrase extraction .", "label": "", "metadata": {}, "score": "62.889877"}
{"text": "Graphical models provide a promising paradigm to study both existing and novel techniques for automatic speech recognition .This paper first provides a brief overview of graphical models and their uses as statistical models .Moreover , this paper shows that many advanced models for speech recognition and language processing can also be simply described by a graph , including many at the acoustic- , pronunciation- , and language - modeling levels .", "label": "", "metadata": {}, "score": "62.91311"}
{"text": "IRSTLM provides a simple way to split LM training into smaller and independent steps , which can be distributed among independent processes .The procedure relies on a training script that makes little use of computer memory and implements the Witten - Bell smoothing method .", "label": "", "metadata": {}, "score": "62.936127"}
{"text": "In this paper we present generalized networks of Dirichlet distributions , and show how , using the two - parameter Poisson - Dirichlet distribution and Gibbs sampling , one can do approximate inference over them .This involves integrating out the proba - bility vectors but leaving auxiliary discrete count vectors in their place .", "label": "", "metadata": {}, "score": "63.09553"}
{"text": "Phrase - Based Machine Translation of Under - Resourced Languages A Drummer - people.cs.uct.ac.za ...The Moses toolkit was used along with Giza++ for alignment and IRSTLM for the language model .The researcher was unsuccessful in ... in the training pipeline .", "label": "", "metadata": {}, "score": "63.173058"}
{"text": "Cited by 7 Related articles All 9 versions Cite Save More . ...The method is available in the IRSTLM toolkit ( Fed- erico et al . , 2008 ) .28 Page 3 . 3 Data for Development ...Cited by 3 Related articles All 2 versions Cite Save More . ...", "label": "", "metadata": {}, "score": "63.197285"}
{"text": "In Proceedings of the IEEE International Conference on Acoustics , Speech , and Sig- nal Processing , pages 633 - 636 , 1992 .S. Goldwater , T. L. Griffiths , and M. Johnson .Interpolat- ing between types and tokens by estimating power law generators .", "label": "", "metadata": {}, "score": "63.322365"}
{"text": "Abstract - We review developments in the SRI Language Modeling Toolkit ( SRILM ) since 2002 , when a previous paper on SRILM was published .These developments include measures to make training from large data sets more efficient , to implement additional language modeling techniques ( such as for adaptatio ... \" .", "label": "", "metadata": {}, "score": "63.765823"}
{"text": "IRSTLM is open source and can be downloaded from here .Typically , LM estimation starts with the collection of n - grams and their frequency counters .Then , smoothing parameters are estimated for each n - gram level ; infrequent n - grams are possibly pruned and , finally , a LM file is created containing n - grams with probabilities and back - off weights .", "label": "", "metadata": {}, "score": "63.856224"}
{"text": "Irstlm(L ( Op ) )Mn ?Irstlm(L ( On ) ) ...Cited by 1 Related articles All 2 versions Cite Save .FBK 's Machine Translation Systems for the IWSLT 2013 Evaluation Campaign N Bertoldi , MA Farajian , P Mathur , N Ruiz , M Federico ... - hlt.fbk.eu ... 2.4.1 .", "label": "", "metadata": {}, "score": "64.04493"}
{"text": "Trigram language models are normally consid ... \" .Trigram language models are compressed using a Golomb coding method inspired by the original Unix spell program .Compression methods trade off space , time and accuracy ( loss ) .The proposed HashTBO method optimizes space at the expense of time and accuracy .", "label": "", "metadata": {}, "score": "64.09926"}
{"text": "When other domains are sufficiently larger and/or different than the in - domain , the probability distribution can skew away from the target domain resulting in poor performance .The LM - like nature of the model provides motivation to apply methods such as perplexity optimization for model weighting .", "label": "", "metadata": {}, "score": "64.24839"}
{"text": "Test data include two heterogeneous and one homogeneous document collections .Our experiments show that passage retrieval is feasible in the language modeling context , and more importantly , it can provide more reliable performance than retrieval based on full documents . \" ...", "label": "", "metadata": {}, "score": "64.449776"}
{"text": "K Sakaguchi , Y Arase , M Komachi - ACL ( 2 ) , 2013 - aclweb.org ...Table 3 : Ratio of appropriate distractors ( RAD ) with a 95 % confidence interval and inter - rater agreement statistics ? model score trained on Google 1 T Web Corpus ( Brants and Franz , 2006 ) with IRSTLM toolkit12 . ... net / projects / irstlm / files / irstlm/ ... Related articles All 2 versions Cite Save More .", "label": "", "metadata": {}, "score": "64.450676"}
{"text": "biblio.unitn.it Page 1 . PhD Dissertation International Doctorate School in Information and Communication Technologies DISI - University of Trento Linguistically Motivated Reordering Modeling for Phrase - Based Statistical Machine Translation Arianna Bisazza Advisor : ... Cited by 1 Related articles All 3 versions Cite Save \" ...", "label": "", "metadata": {}, "score": "64.47972"}
{"text": "IRSTLM : an open source toolkit for handling large scale language models . ...Cited by 1 Related articles Cite Save More .Rule Based Transliteration Scheme for English to Punjabi D Bhalla , N Joshi , I Mathur - arXiv preprint arXiv:1307.4300 , 2013 - arxiv.org ...", "label": "", "metadata": {}, "score": "64.69261"}
{"text": "SRILM is freely available for noncommercial purposes .The toolkit supports creation ... \" .SRILM is a collection of C++ libraries , executable programs , and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications .", "label": "", "metadata": {}, "score": "64.708"}
{"text": "SRILM is freely available for noncommercial purposes .The toolkit supports creation ... \" .SRILM is a collection of C++ libraries , executable programs , and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications .", "label": "", "metadata": {}, "score": "64.708"}
{"text": "SRILM is freely available for noncommercial purposes .The toolkit supports creation ... \" .SRILM is a collection of C++ libraries , executable programs , and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications .", "label": "", "metadata": {}, "score": "64.708"}
{"text": "Given an arpa file the toolkit will create a ' backoff ' file which may be reused ( see examples below ) .output - prefix : Prefix added to all output files during the construction of a randomized language model . model .", "label": "", "metadata": {}, "score": "64.91395"}
{"text": "The LM is built on the Academia Sinica corpus ( Emerson , 2005 ) with IRSTLM toolkit ( Federico et al . , 2008 ) .Irstlm : an open source toolkit for handling large scale language models . ...Related articles All 4 versions Cite Save More . ...", "label": "", "metadata": {}, "score": "64.92923"}
{"text": "It is easy , however , to obtain non - domain - specific data , e.g. text from the world wide web .Unfortunately models trained using such data are often ill - suited for domain - specific ap-Appearing in Proceedings of the 12thInternational Confe- rence on Artificial Intelligence and Statistics ( AISTATS ) 2009 , Clearwater Beach , Florida , USA .", "label": "", "metadata": {}, "score": "64.95692"}
{"text": "from a set of precomputed ngram - count pairs ( this is useful if you need to build LMs from billions of words .RandLM has supporting Hadoop scripts ) .The former type of model will be referred to as a CountRandLM while the second will be referred to as a BackoffRandLM .", "label": "", "metadata": {}, "score": "65.0379"}
{"text": "Severa ... \" .This paper presents a system for the offline recognition of large vocabulary unconstrained handwritten texts .The only assumption made about the data is that it is written in English .This allows the application of Statistical Language Models in order to improve the performance of our system .", "label": "", "metadata": {}, "score": "65.06273"}
{"text": "I. . ... ased container abstractions .Clearly , adding some of these new approaches to the toolkit would serve its users well , and we hope that contributions from the community will continue to be forthcoming .ACKNOWLEDGMENTS We thank al ..", "label": "", "metadata": {}, "score": "65.06993"}
{"text": "Scalable Modified Kneser - Ney Language Model Estimation . ...SRILM and IRSTLM were run un- til the test machine ran out of RAM ( 64 GB ) . ...Cited by 26 Related articles All 9 versions Cite Save More .", "label": "", "metadata": {}, "score": "65.27455"}
{"text": "Improved backing - off for m - gram language modeling .In Proceedings of the IEEE Interna- tional Conference on Acoustics Speech and Signal Pro- cessing , volume 1 , pages 181 - 184 , 1995 .R. Kneser and V. Steinbiss .", "label": "", "metadata": {}, "score": "65.42776"}
{"text": "net / projects/ irstlm/ 17consisting of entries through 2012 . ...Cited by 6 Related articles All 9 versions Cite Save More .To build the language models ( LM ) , we used the state - of - the - art open - source IRSTLM toolkit ( Federico and Cettolo , 2007 ) .", "label": "", "metadata": {}, "score": "65.58219"}
{"text": "In Proceed- ings of Interspeech , Brisbane , Australia .Najeh Hajlaoui and Andrei Popescu - Belis . ...Cited by 1 Related articles All 10 versions Cite Save .Statistical Machine Translation Model for English to Urdu Machine Translation RB Mishra - Artificial Intelligence and Soft Computing - researchgate.net ... translations .", "label": "", "metadata": {}, "score": "65.79311"}
{"text": "It is entirely possible that two techniques that work well separately will not work well together , and , as we will show , even possible that some techniques will work better together than either one does by itself .In this ... . \" ...", "label": "", "metadata": {}, "score": "65.98523"}
{"text": "It is entirely possible that two techniques that work well separately will not work well together , and , as we will show , even possible that some techniques will work better together than either one does by itself .In this ... . \" ...", "label": "", "metadata": {}, "score": "65.98523"}
{"text": "Typically , LMs employed by Moses provide the probability of n - grams of single factors .In addition to the standard way , the IRSTLM toolkit allows Moses to query the LMs in other different ways .Similarly to factored models , where the word is not anymore a simple token but a vector of factors that can represent different levels of annotation , here the word can be the concatenation of different tags for the surface form of a word , e.g. : .", "label": "", "metadata": {}, "score": "65.99704"}
{"text": "New dedicated ...Cited by 1 Related articles All 2 versions Cite Save More .The weights ... Related articles Cite Save More .Lexicon - supported OCR of eighteenth century Dutch books : a case study J de Does , K Depuydt - IS&T / SPIE ... , 2013 - proceedings.spiedigitallibrary.org ... 5th ACL - HLT Workshop on Language Technology for Cultural Heritage , Social Sciences and Humanities , 33 - 38 ( 2011 ) .", "label": "", "metadata": {}, "score": "66.17522"}
{"text": "get - counts : Return the counts of n - grams rather than conditional log probabilities ( only supported by CountRandLM ) .checks : Applies sequential checks to n - grams to avoid unnecessary false positives .KenLM is a language model that is simultaneously fast and low memory .", "label": "", "metadata": {}, "score": "66.27995"}
{"text": "Related articles All 3 versions Cite Save More .Are ACT 's scores increasing with better translation quality ?N Hajlaoui - Are ACT \" s scores increasing with better translation ... , 2013 - infoscience.epfl.ch ...Marcello Federico , Nicola Bertoldi , and Mauro Cet- tolo .", "label": "", "metadata": {}, "score": "66.337746"}
{"text": "MAP adaptation of stochastic grammars .and Language , 20:41 - 68 , 2006 .Computer Speech J. R. Bellegarda .Statistical language model adaptation : review and perspectives .Speech Communication , 42:93- 108 , 2004 .J. Carletta .Unleashing the killer corpus : experiences in creating the multi - everything AMI meeting corpus .", "label": "", "metadata": {}, "score": "66.34398"}
{"text": "This takes a very different approach to either the SRILM or the IRSTLM .It represents LMs using a randomized data structure ( technically , variants of Bloom filters ) .This can result in LMs that are ten times smaller than those created using the SRILM ( and also smaller than IRSTLM ) , but at the cost of making decoding about four times slower .", "label": "", "metadata": {}, "score": "66.78439"}
{"text": "To know more read Durrani et al .( 2015 ) .Usage .Provide tuning files as additional parameter in the settings .For example : .This method requires word - alignment for the source and reference tuning files to generate operation sequences .", "label": "", "metadata": {}, "score": "66.99544"}
{"text": "CountRandLMs use either StupidBackoff or else Witten - Bell smoothing .BackoffRandLM models can use any smoothing scheme that the SRILM implements .Generally , CountRandLMs are smaller than BackoffRandLMs , but use less sophisticated smoothing .When using billions of words of training material there is less of a need for good smoothing and so CountRandLMs become appropriate .", "label": "", "metadata": {}, "score": "66.99631"}
{"text": "Automatically detected acoustic landmarks for assessing natural emotion from speech Herv\u00e9 SIERRO herve.sierro@unifr.ch , BENEFRI Master Student Document , Image and Voice Analysis group University of Fribourg Thesis Supervisor : Dr. Fabien RINGEVAL ...Cited by 1 Related articles All 5 versions Cite Save More .", "label": "", "metadata": {}, "score": "67.07085"}
{"text": "Cited by 1 Related articles All 8 versions Cite Save More .This document is part of the Project \" Machine Translation Enhanced Computer Assisted Translation ( MateCat ) \" , funded by the 7th Framework Programme of the European Commission through grant agreement no . : 287688 .", "label": "", "metadata": {}, "score": "67.07257"}
{"text": "The field is ignored .By contrast , SRI silently returns incorrect probabilities if you get it wrong ( Kneser - Ney smoothed probabilties for lower - order n - grams are conditioned on backing off ) .This will build a binary file that can be used in place of the ARPA file .", "label": "", "metadata": {}, "score": "67.25302"}
{"text": "The value of p can be set at binary building time e.g. .The trie data structure uses less memory than all other options ( except RandLM with stupid backoff ) , has the best memory locality , and is still faster than any other toolkit .", "label": "", "metadata": {}, "score": "67.43863"}
{"text": "Cited by 1 Related articles All 3 versions Cite Save .Quality Estimation Software Extensions L Specia , K Shah , E Avramidis - 2013 - qt21.eu Page 1 .FP7-ICT Coordination and Support Action ( CSA ) QTLaunchPad( No . 296347 )", "label": "", "metadata": {}, "score": "68.00433"}
{"text": "The end of this block is found by reading the next entry 's pointer .Records within the block are sorted by word index .Because the vocabulary ids are randomly permuted , a uniform key distribution applies .Interpolation search within each block finds the word index and its correspoding probability , backoff , and pointer .", "label": "", "metadata": {}, "score": "68.40321"}
{"text": "You can also specify the data structure to use : . where valid values are probing , sorted , and trie .The default is probing .Generally , I recommend using probing if you have the memory and trie if you do not .", "label": "", "metadata": {}, "score": "68.61725"}
{"text": "No special setting of the configuration file is required : Moses compiled with the IRSTLM toolkit is able to read the necessary information from the header of the file .It is possible to avoid the loading of the LM into the central memory by exploiting the memory mapping mechanism .", "label": "", "metadata": {}, "score": "68.72563"}
{"text": "IRSTLM toolkit handles LM formats which permit to reduce both storage and decoding memory requirements , and to save time in LM loading .In particular , it provides tools for : .Training a language model from huge amounts of data can be definitively memory and time expensive .", "label": "", "metadata": {}, "score": "68.92682"}
{"text": "The additional parameter is a file containing ( at least ) the following header : . -1 : the strings are used are they are ; if the map is given , it is applied to the whole string before the LM query . 0 - 9 : the field number is selected ; if the map is given , it is applied to the selected field .", "label": "", "metadata": {}, "score": "69.07997"}
{"text": "- 99 : the two fields corresponding to the two digits are selected and concatenated together using the character _ as separator .If the map is given , it is applied to the field corresponding to the first digit .The last case is useful for lexicalization of LMs : if the fields n. 2 and 1 correspond to the POS and lemma of the actual word respectively , the LM is queried with n - grams of POS_lemma .", "label": "", "metadata": {}, "score": "69.24185"}
{"text": "Additionally , this paper includes a novel graphical analysis regarding why derivative ( or delta ) features improve hidden Markov model - based speech recognition by improving structural discriminability .It also includes an example where a graph can be used to represent language model smoothing constraints .", "label": "", "metadata": {}, "score": "69.349625"}
{"text": "Pass the order ( -o ) , an amount of memory to use for building ( -S ) , and a location to place temporary files ( -T ) .It scales to much larger models than SRILM can handle and does not resort to approximation like IRSTLM does . perl -lmplz $ lmplz \" .", "label": "", "metadata": {}, "score": "69.84826"}
{"text": "Cited by 2 Related articles All 2 versions Cite Save .Model for English - Urdu Statistical Machine Translation A Ali , A Hussain , MK Malik - World Applied Sciences Journal , 2013 - idosi.org ...The model is trained on TrainSet using Moses Conclusion and Future Work : There are certain words in translation setup with language modeling toolkit IRSTLM .", "label": "", "metadata": {}, "score": "70.74873"}
{"text": "Both those sequences are collapsed into a single chunk label ( let us say CHNK ) as long as ( TAG / TAG ( , TAG+ and TAG ) are all mapped into the same label CHNK .The map into different labels or a different use / position of characters ( , + and ) in the lexicon of tags prevent the collapsing operation .", "label": "", "metadata": {}, "score": "70.83859"}
{"text": "Warning : In case of parallel decoding in a cluster of computers , each process will access the same file .The possible large number of reading requests could overload the driver of the hard disk which the LM is stored on , and/or the network .", "label": "", "metadata": {}, "score": "71.2439"}
{"text": "For a CountRandLM quantisation is performed by taking a logarithm .The base of the logarithm is set as 2 1/ values .For a BackoffRandLM a binning quantisation algorithm is used .The size of the codebook is set as 2 values .", "label": "", "metadata": {}, "score": "71.28758"}
{"text": "When the objects are ambiguous words , for example , different senses of a word occur in different maps , so \" river \" and \" loan \" can both be close to \" bank \" without being at all close to each other .", "label": "", "metadata": {}, "score": "71.42855"}
{"text": "However , benchmarks report the entire cost of running Moses .NPLM is a neural network language model toolkit ( homepage ) .We currently recommend installing a fork which allows pre - multiplication of the input embedding and training with a single hidden layer for faster decoding . trainNeuralNetwork --train_file train.ngrams \\ --validation_file validation.ngrams \\ --num_epochs 10 \\ --words_file words \\ --model_prefix model \\ --input_embedding_dimension 150 \\ --num_hidden 0 \\ --output_embedding_dimension 750 .", "label": "", "metadata": {}, "score": "72.16127"}
{"text": "The original strings are kept at the end of the binary file and passed to Moses at load time to obtain or generate Moses IDs .This is why lazy binary loading still takes a few seconds .KenLM stores a vector mapping from Moses ID to KenLM ID .", "label": "", "metadata": {}, "score": "72.49931"}
{"text": "It is maintained by Ken Heafield , who provides additional information on his website , such as benchmarks comparing speed and memory use against the other language model implementations .KenLM is distributed with Moses and compiled by default .KenLM is fully thread - safe for use with multi - threaded Moses .", "label": "", "metadata": {}, "score": "72.51335"}
{"text": "mosesdecoder / scripts / training / bilingual - lm / train_nplm.py \\ --working - dir \\ --corpus \\ --nplm - home \\ --ngram - size 14 \\ --hidden 0 \\ --output - embedding 750 \\ --threads . '--hidden 0 ' results in a neural network with a single hidden layer , which is recommended for fast SMT decoding .", "label": "", "metadata": {}, "score": "73.29927"}
{"text": "/path - to - moses / scripts / OSM / OSM - Train . perl --corpus - f corpus.fr --corpus - e corpus.en --alignment aligned.grow-diag-final-and --order 5 --out - dir /path - to - experiment / model / OSM --moses - src - dir /path - to - moses/ --srilm - dir /path - to - srilm / bin / i686-m64 --factor 0 - 0 .", "label": "", "metadata": {}, "score": "73.30209"}
{"text": "All 3 versions Cite Save More .Topic models for translation quality estimation for gisting purposes R Rubino , J de Souza , J Foster , L Specia - 2013 - doras.dcu.ie ... and SYSTRAN . ...Cited by 2 Related articles All 2 versions Cite Save .", "label": "", "metadata": {}, "score": "73.64357"}
{"text": "The RandLM release has Hadoop scripts which take raw text files and create ngram - counts .gz .Moses uses its own interface to the randLM , but it may be interesting to query the language model directly .The querylm binary ( in randlm / bin ) allows a randomized language model to be queried .", "label": "", "metadata": {}, "score": "75.541"}
{"text": "order : The order of the n - gram model e.g. , 3 for a trigram model .falsepos : The false positive rate of the randomized data structure on an inverse log scale so -falsepos 8 produces a false positive rate of 1/2 8 .", "label": "", "metadata": {}, "score": "75.83837"}
{"text": "An example command for training follows : . mkdir working_dir_head mkdir working_dir_label mosesdecoder / scripts / training / rdlm / train_rdlm.py --nplm - home /path / to / nplm --working - dir working_dir_head \\ --output - dir /path / to / output_directory --output - model rdlm_head \\ --mode head --output - vocab - size 500000 --noise - samples 100 .", "label": "", "metadata": {}, "score": "77.12836"}
{"text": "Machine Translation of Film Subtitles from English to Spanish J Isele - 2013 - mlta.uzh.ch Page 1 .Institut f\u00fcr Computerlinguistik Machine Translation of Film Subtitles from English to Spanish Combining a Statistical System with Rule - based Grammar Checking Masterarbeit der Philosophischen Fakult\u00e4t der Universit\u00e4t Z\u00fcrich Referent : Prof. Dr. M. Volk Verfasserin : ... Related articles Cite Save More .", "label": "", "metadata": {}, "score": "78.331726"}
{"text": "Probing is the fastest and default data structure .Unigram lookups happen by array index .Bigrams and longer n - grams are hashed to 64-bit integers which have very low probability of collision , even with the birthday attack .", "label": "", "metadata": {}, "score": "79.22049"}
{"text": "Allow the user to set this probability ( IRSTLM)4 . net / apps / mediawiki / irstlm / index . ...Related articles Cite Save More . 3.3 Results ...Marcello Federico , Nicola Bertoldi , and Mauro Cet- tolo .IRSTLM : an Open Source Toolkit for Page 7 . ...", "label": "", "metadata": {}, "score": "79.2937"}
{"text": "\" We used the CRF sampler outlined in Section 5 with the addition of Metropolis - Hastings updates for the discount parameters ( Wood & Teh , 2009 ) .The discounts in the collapsed node restaurants are products of subsets of discount parameters making other approaches difficult .", "label": "", "metadata": {}, "score": "79.507065"}
{"text": "Related articles Cite Save More .Computing n - gram statistics in MapReduce K Berberich , S Bedathur - ... of the 16th International Conference on ... , 2013 - dl.acm.org Page 1 .Computing n - Gram Statistics in MapReduce Klaus Berberich Max Planck Institute for Informatics Saarbr\u00fccken , Germany kberberi@mpi-inf.mpg.de Srikanta Bedathur Indraprastha Institute of Information Technology New Delhi , India bedathur@iiitd.ac.in ...", "label": "", "metadata": {}, "score": "80.23406"}
{"text": "These networks can induce distributed feature representations for unseen phrases and provide syntactic information to accurately predict phrase structure trees .Most excitingly , the representation of each phrase also captures semantic information : For instance , the phrases \" decline to comment \" and \" would not disclose the terms \" are close by in the induced embedding space .", "label": "", "metadata": {}, "score": "80.80405"}
{"text": "A linear probing hash table is an array consisting of blanks ( zeros ) and entries with non - zero keys .Lookup proceeds by hashing the key modulo the array size , starting at this point in the array , and scanning forward until the entry or a blank is found .", "label": "", "metadata": {}, "score": "84.56992"}
{"text": "Copyright 2009 by the authors . plications ( Rosenfeld 2000 ) .The phrase domain adap- tation describes procedures that take a model trained on a large amount of non - specific data and adapt it to work well for a specific domain for which less training data is available .", "label": "", "metadata": {}, "score": "85.148254"}
{"text": "Wherever ...Cited by 1 Related articles All 5 versions Cite Save More . ...Cited by 1 Related articles All 9 versions Cite Save .Translating video content to natural language descriptions M Rohrbach , W Qiu , I Titov , S Thater ... - ... Vision ( ICCV ) , 2013 ... , 2013 - ieeexplore.ieee.org ... probability .", "label": "", "metadata": {}, "score": "86.44641"}
{"text": "In order to activate the access through the memory mapping , simply add the suffix .mm to the name of the LM file ( which must be stored in the binary format ) and update the Moses configuration file accordingly .", "label": "", "metadata": {}, "score": "88.56745"}
