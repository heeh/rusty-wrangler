{"text": "In an example , an active learning module identifies regions in a feature space from which the observations are drawn using the clusters and confidence information ; new observations from the identified regions are used to train the random decision forest .", "label": "", "metadata": {}, "score": "24.68823"}
{"text": "In examples , a random decision forest comprising a plurality of hierarchical data structures is trained using both unlabeled and labeled observations .In examples , a training objective is used which seeks to cluster the observations based on the labels and similarity of the observations .", "label": "", "metadata": {}, "score": "26.498241"}
{"text": "This may be provided by human experts , may be machine generated or may be provided by other processes .Labeled training data is typically difficult and/or expensive to obtain as compared with unlabeled training data .By using a semi - supervised random decision forest it is possible to exploit both labeled and unlabeled training data .", "label": "", "metadata": {}, "score": "27.78926"}
{"text": "The same predictions would not be obtainable from a model which induces a function based only on the training cases .Some people may call this an example of the closely related semi - supervised learning , since Vapnik 's motivation is quite different .", "label": "", "metadata": {}, "score": "30.650234"}
{"text": "Rather , it is intended that all suchvariations not departing from the spirit of the invention be considered as within the scope thereof as limited solely by the claims appended hereto .Morning Session .We present an online learning algorithm for training structured prediction models with extrinsic loss functions .", "label": "", "metadata": {}, "score": "31.398756"}
{"text": "However , the results of semi - supervised feature selection can be at times unsatisfactory , and the culprit is on how to effectively use the unlabeled data .Quite different from both supervised and semi - supervised feature selection , we propose a \" hybrid \" framework based on graph models .", "label": "", "metadata": {}, "score": "31.653322"}
{"text": "First , the proposed solution is agnostic to the underlying binary classifiers that are used .Second , the methodology builds upon the active sets chosen by the underlying active learning strategy for each binary classification problem and is thus independent of the specific active learning method employed .", "label": "", "metadata": {}, "score": "31.656866"}
{"text": "In this example , both training set 1 and set 2 have \" conflicting \" regions from the in - domain data .Thus the models trained can not be adopted directly by simple combination or averaging .However , using the proposed approach , as shown in the bottom right plot below , we can successfully transfer the right knowledge from several different domains .", "label": "", "metadata": {}, "score": "31.731865"}
{"text": "Generally speaking , active - learning methods construct training sets iteratively , starting from a small initial set and then expanding that set incrementally by selecting examples deemed \" most interesting \" by the classifier at each iteration .The\"most interesting \" samples ordinarily are those that are closest to the decision boundary or where there otherwise is greater uncertainty as to whether the classification predicted by the classifier is correct .", "label": "", "metadata": {}, "score": "32.247143"}
{"text": "An earlier paper demonstrates that when there is sample selection bias model averaging based methods can significantly outperform bagging and boosting .Traditionally , cross - validation has been widely used in inductive learning to choose the best model .However , the implicit assumption is that the training and test data follow the same distribution .", "label": "", "metadata": {}, "score": "32.487812"}
{"text": "A method as claimed in claim 13 comprising obtaining new observations in the identified regions and training the random decision forest using the new observations .A machine learning system as claimed in claim 16 comprising a transducer arranged to assign labels and certainties of those labels to the unlabeled observations by giving unlabeled observations the labels of close labeled observations where close labeled observations are selected on the basis of the clusters .", "label": "", "metadata": {}, "score": "32.94701"}
{"text": "Normally , inductive learning assumes that the training data and future testing data follow the same distribution .However , in reality , this may not be true .For example , when a bank approves a loan the risk model can only be constructed from the data of its own customers .", "label": "", "metadata": {}, "score": "33.78012"}
{"text": "In a most recently proposed approach , we address the problem of transfer learning between datasets of high dimensions .Transferring knowledge from one domain to another is challenging due to a number of reasons .Since both conditional and marginal distribution of the training data and test data are non - identical , model trained in one domain , when directly applied to a different domain , is usually low in accuracy .", "label": "", "metadata": {}, "score": "33.878292"}
{"text": "In this paper , a co - training style semi - supervised regression algorithm , i.e. COREG , is proposed .This algorithm uses two regressors each labels the unlabeled data for the other regressor , where the confidence in labeling an unlabeled example is estimated through the amount of reduction in mean square error over the labeled neighborhood of that example .", "label": "", "metadata": {}, "score": "34.101837"}
{"text": "Given a set of training examples , each marked as belonging to one of two categories , an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other .Cluster analysis is the assignment of a set of observations into subsets ( called clusters ) so that observations within the same cluster are similar according to some predesignated criterion or criteria , while observations drawn from different clusters are dissimilar .", "label": "", "metadata": {}, "score": "34.112774"}
{"text": "For a given input object ( such as part of an image or text document or other item ) a decision tree estimates an unknown property of the object by asking successive questions ( or making tests ) about its known properties .", "label": "", "metadata": {}, "score": "34.295837"}
{"text": "Importantly , these initial features might otherwise be missed when selection is performed on the labeled and unlabeled examples simultaneously .Next , this initial feature set is expanded and corrected with the use of unlabeled data .We formally analyze why the expected performance of the hybrid framework is better than both supervised and semi - supervised feature selection .", "label": "", "metadata": {}, "score": "34.311474"}
{"text": "Other methods are based on estimated density and graph connectivity .Clustering is a method of unsupervised learning , and a common technique for statistical data analysis .Reinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize some notion of long - term reward .", "label": "", "metadata": {}, "score": "34.342987"}
{"text": "During training the structure of the trees is learnt and tests are selected ( from randomly generated tests in the case of random decision forests ) for use at the split nodes .The data at the leaf nodes may be aggregated .", "label": "", "metadata": {}, "score": "34.431137"}
{"text": "Two applications on cost - sensitive learning appear in an EDBT'04 Industry paper for extremely skewed trading anomaly detection as well as in a SDM'05 paper to predict customers who may default their monthly payment .Most inductive algorithms can only model from well - structured feature vectors , but in reality , raw data from many applications do not have any feature vectors available .", "label": "", "metadata": {}, "score": "34.544647"}
{"text": "Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts , an ILP system will derive a hypothesized logic program that entails all positive and no negative examples .Inductive programming is a related field that considers any kind of programming languages for representing hypotheses ( and not only logic programming ) , such as functional programs .", "label": "", "metadata": {}, "score": "34.64704"}
{"text": "Building on recent developments in regularization theory for graphs and corresponding Laplacian - based methods for classification , we develop an algorithmic framework for learning ranking functions on graph data .We provide generalization guarantees for our algorithms via recent results based on the notion of algorithmic stability , and give experimental evidence of the potential benefits of our framework . of the graph ranking problem that we have focused on in this paper falls under the setting of transductive learning .", "label": "", "metadata": {}, "score": "34.674404"}
{"text": "For this purpose , the training samples often are selected from the much larger group of samples to be classified .In some cases , the training samples are randomly selected .In others , the training samples are selected in a systematic manneraccording to pre - specified criteria .", "label": "", "metadata": {}, "score": "35.013798"}
{"text": "While some solutions focus exclusively on binary categorization problems , some are restricted to specific types of classifiers , and some others require a number of extra classifiers to be trained for each classification task .These approaches become infeasible , however , when dealing with a large , multiclass categorization problem such as the ones that abound in real - world web content .", "label": "", "metadata": {}, "score": "35.152023"}
{"text": "The decision is then made based on the terminal node on the path ( which has associated stored data ) .[ 0033 ]In the examples described herein random decision forests are trained using both unlabeled data and labeled data .", "label": "", "metadata": {}, "score": "35.807228"}
{"text": "With this problem , however , the supervised learning algorithm will only have five labeled points to use as a basis for building a predictive model .It will certainly struggle to build a model that captures the structure of this data .", "label": "", "metadata": {}, "score": "35.910347"}
{"text": "One family of methods that other researchers and I have been working on is Random Decision Trees .You can find its description , source code and datasets here .The use of ensemble approach for cost - sensitive learning that estimate the risk can be found in this SDM'02 paper , its distributed adaptation appears in the ICDCS'02 paper ( the longer version ) .", "label": "", "metadata": {}, "score": "36.04978"}
{"text": "However the author plans to update the online version frequently to incorporate the latest development in the field . ... labeled data set , and are required to make similar predictions on any given unlabeled instance .Multiview learning has a long history ( de Sa , 1993 ) .", "label": "", "metadata": {}, "score": "36.153923"}
{"text": "To achieve this , the learner has to generalize from the presented data to unseen situations in a \" reasonable \" way ( see inductive bias ) .( Compare with unsupervised learning . )The parallel task in human and animal psychology is often referred to as concept learning .", "label": "", "metadata": {}, "score": "36.2468"}
{"text": "I. .In one approach , we combine models previously trained from different domains via locally weighted framework to transfer the knowledge into a new domain .One important difference is that the out - of - domain models in our approach did not use any in - domain data at all during their training .", "label": "", "metadata": {}, "score": "36.265743"}
{"text": "This procedure is repeated iteratively until convergence of the performance , or in a more realistic restriction , while labeling resources are available .In a more realistic setting , to limit the turnaround cycle , active learning selects not just one but a batch of informative examples to be labeled during each active learning iteration .", "label": "", "metadata": {}, "score": "36.339516"}
{"text": "Note that this is caused by transductive inference on different test sets producing mutually inconsistent predictions .Try to get the answer that you really need but not a more general one . \"An example of learning which is not inductive would be in the case of binary classification , where the inputs tend to cluster in two groups .", "label": "", "metadata": {}, "score": "36.579456"}
{"text": "In examples , a training objective is used which seeks to cluster the observations based on the labels and similarity of the observations .In an example , a transducer assigns labels to the unlabeled observations on the basis of the clusters and confidence information .", "label": "", "metadata": {}, "score": "36.959427"}
{"text": "The new observations may then be used to train the semi - supervised decision forest to produce a trained system which is able to cope with a wider range of observations at higher confidence levels .[ 0027 ] An inducer 114 may take the class labels generated by the transducer and apply those to the clusters produced by the semi - supervised decision forest in order to find a generic function 116 for classifying test observations not previously seen by the random decision forest .", "label": "", "metadata": {}, "score": "37.039524"}
{"text": "In active learning , an initially - trained classifier may be used to guide the selection of examples that should be labeled , e.g. , knowing the labels of which examples would be most informative , and would improve the performance of that initial classifier the most .", "label": "", "metadata": {}, "score": "37.114624"}
{"text": "For example , boundary samples may be identified by selecting those examples having soft classification scores that are close to the decision threshold forspecific hard classifications .Alternatively , boundary samples can be identified using a query - by - committee approach in which differently trained classifiers arrive at significantly different predictions ( hard or soft ) for the same sample .", "label": "", "metadata": {}, "score": "37.1191"}
{"text": "For example , this may be to optimize a mixed information gain that relates to density of all the observations ( both labeled and unlabeled ) and to an entropy of the labeled observations alone ; a detailed example is given later in this document .", "label": "", "metadata": {}, "score": "37.221985"}
{"text": "The bias - variance decomposition is one way to quantify generalization error .In addition to performance bounds , computational learning theorists study the time complexity and feasibility of learning .In computational learning theory , a computation is considered feasible if it can be done in polynomial time .", "label": "", "metadata": {}, "score": "37.491062"}
{"text": "Unlike in classification , the groups are not known beforehand , making this typically an unsupervised task .As a scientific endeavour , machine learning grew out of the quest for artificial intelligence .Already in the early days of AI as an academic discipline , some researchers were interested in having machines learn from data .", "label": "", "metadata": {}, "score": "37.540863"}
{"text": "[11 ] Work on symbolic / knowledge - based learning did continue within AI , leading to inductive logic programming , but the more statistical line of research was now outside the field of AI proper , in pattern recognition and information retrieval .", "label": "", "metadata": {}, "score": "37.5475"}
{"text": "Active learning is a well - studied methodology that attempts to maximize the efficiency of the labeling process in such scenarios .Active learning typically proceeds by first training an initial model on a small labeled dataset .Provided that there are a large number of unlabeled examples , it then selects an unlabeled example that it believes is \" informative \" and will improve the classification performance the most if its label is revealed .", "label": "", "metadata": {}, "score": "37.678036"}
{"text": "To collect these labeled samples we use an active learning technique .Thus , ultimately our human model is learnt by the combination of virtual- and real - world labeled samples which , to the best of our knowledge , was not done before .", "label": "", "metadata": {}, "score": "37.773773"}
{"text": "In an example , an inducer forms a generic clustering function by counting examples of class labels at leaves of the trees in the forest .In an example , an active learning module identifies regions in a feature space from which the observations are drawn using the clusters and certainty information ; new observations from the identified regions are used to train the random decision forest .", "label": "", "metadata": {}, "score": "37.786057"}
{"text": "However , these are not always as good as the classical approach of training and testing with data coming from the same camera and the same type of scenario .Accordingly , in Vazquez et al .we cast the problem as one of supervised domain adaptation .", "label": "", "metadata": {}, "score": "38.087715"}
{"text": "Evaluated with respect to known knowledge , an uninformed ( unsupervised ) method will easily be outperformed by supervised methods , while in a typical KDD task , supervised methods can not be used due to the unavailability of training data .", "label": "", "metadata": {}, "score": "38.266747"}
{"text": "A method as claimed in claim 1 where each random decision tree is a hierarchical tree data structure comprising split nodes and leaf nodes and having a test associated with each split node .A method as claimed in claim 1 wherein training the random decision trees comprises using a training objective which applies to unlabeled observations as well as to labeled observations .", "label": "", "metadata": {}, "score": "38.31305"}
{"text": "A semi - supervised random decision forest may be trained using both the unlabeled and the labeled data so as to cluster the data into two clusters depicted by the S shaped groups of data visible in the figure .A transducer may be used to propagate the ground truth labels to the many unlabeled data points as illustrated in graph 202 .", "label": "", "metadata": {}, "score": "38.39203"}
{"text": "Fourth , there is a straight - forward interpretation as to why a particular active set of examples is chosen during an iteration , namely , that the selected examples are the ones that are informative for the maximum number of classes .", "label": "", "metadata": {}, "score": "38.60562"}
{"text": "These are [ 10 ] .Supervised learning : The computer is presented with example inputs and their desired outputs , given by a \" teacher \" , and the goal is to learn a general rule that maps inputs to outputs .", "label": "", "metadata": {}, "score": "38.787514"}
{"text": "In such a scenario the learning algorithm can actively query the user / teacher for labels .This type of iterative supervised learning is called active learning .Since the learner chooses the examples , the number of examples to learn a concept can often be much lower than the number required in normal supervised learning .", "label": "", "metadata": {}, "score": "38.845745"}
{"text": "Probabilistic reasoning was also employed , especially in automated medical diagnosis .[ 10 ] : 488 .However , an increasing emphasis on the logical , knowledge - based approach caused a rift between AI and machine learning .Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation .", "label": "", "metadata": {}, "score": "38.862675"}
{"text": "In this problem , the learning machine is given pairs of examples that are considered similar and pairs of less similar objects .It then needs to learn a similarity function ( or a distance metric function ) that can predict if new objects are similar .", "label": "", "metadata": {}, "score": "38.87999"}
{"text": "This can be computationally expensive if the data is made available incrementally in a stream .Further , this might cause the predictions of some of the old points to change ( which may be good or bad , depending on the application ) .", "label": "", "metadata": {}, "score": "38.966"}
{"text": "A third possible motivation which leads to transduction arises through the need to approximate .If exact inference is computationally prohibitive , one may at least try to make sure that the approximations are good at the test inputs .In this case , the test inputs could come from an arbitrary distribution ( not necessarily related to the distribution of the training inputs ) , which would n't be allowed in semi - supervised learning .", "label": "", "metadata": {}, "score": "39.08246"}
{"text": "Afterwards , it looks for those comparable sub - structures in the \" latent - space \" as mapped from the expanded feature vector , where both marginal and conditional distribution are similar .With these sub - structures in latent space , the proposed approach then find common concepts that are transferable across domains with high probability .", "label": "", "metadata": {}, "score": "39.110817"}
{"text": "The training set needs to be characteristic of the real - world use of the function .Thus , a set of input objects is gathered and corresponding outputs are also gathered , either from human experts or from measurements .Determine the input feature representation of the learned function .", "label": "", "metadata": {}, "score": "39.151146"}
{"text": "Supervised learning can generate models of two types .Most commonly , supervised learning generates a global model that maps input objects to desired outputs .In some cases , however , the map is implemented as a set of local models ( such as in case - based reasoning or the nearest neighbor algorithm ) .", "label": "", "metadata": {}, "score": "39.204086"}
{"text": "Positive results show that a certain class of functions can be learned in polynomial time .Negative results show that certain classes can not be learned in polynomial time .There are many similarities between machine learning theory and statistical inference , although they use different terms .", "label": "", "metadata": {}, "score": "39.22882"}
{"text": "Therefore , semi - supervised learning has attracted much attention .Previous research on semi - supervised learning mainly focuses on semi - supervised classification .Although regression is almost as important as classification , semi - supervised regression is largely understudied .", "label": "", "metadata": {}, "score": "39.289787"}
{"text": "The risk associated with a function f is then defined as the expectation of the loss function , as follows : .Statistical learning theory investigates under what conditions empirical risk minimization is admissible and how good the approximations can be expected to be .", "label": "", "metadata": {}, "score": "39.313416"}
{"text": "Regardless of the theoretical guarantees and the simplicity of the method , however , it is usually ignored as a comparative baseline in multiclass active learning .Our experiments show that this approach often outperforms the GlobalMinConf method .We henceforth refer to this strategy of active set selection as LocalMinConf .", "label": "", "metadata": {}, "score": "39.360554"}
{"text": "Ideally , one would like a pipeline of models that first cheaply filter out those easy cases than use more costly models to concentrate on those more difficult cases .We have several papers discussing this issue and its implementation in a data mining - based intrusion detection prototype .", "label": "", "metadata": {}, "score": "39.543907"}
{"text": "Another categorization of machine learning tasks arises when one considers the desired output of a machine - learned system : [ 4 ] :3 .In classification , inputs are divided into two or more classes , and the learner must produce a model that assigns unseen inputs to one ( or multi - label classification ) or more of these classes .", "label": "", "metadata": {}, "score": "39.614258"}
{"text": "[ 12 ] .Machine learning and statistics are closely related fields .According to Michael I. Jordan , the ideas of machine learning , from methodological principles to theoretical tools , have had a long pre - history in statistics .", "label": "", "metadata": {}, "score": "39.63599"}
{"text": "We propose several methods for learning effectively in this setting , and test them empirically on the real - world tasks of malicious URL classification and adversarial advertisement detection .Domain adaptation is important for practical applications of supervised learning , as the distribution of inputs can differ significantly between available sources of training data and the test data in a particular target domain .", "label": "", "metadata": {}, "score": "39.65263"}
{"text": "By using an ensemble ( plurality ) of trees in the forest the ability of the system to generalize is improved .Generalization is the ability to deal well with new examples which differ from training data .The component trees in a semi - supervised forest are randomly different from one another and this leads to de - correlation between individual tree predictions .", "label": "", "metadata": {}, "score": "39.654686"}
{"text": "Algorithms that seek to predict continuous labels tend to be derived by adding partial supervision to a manifold learning algorithm .Partitioning transduction can be thought of as top - down transduction .It is a semi - supervised extension of partition - based clustering .", "label": "", "metadata": {}, "score": "39.70235"}
{"text": "The culprit is due to the inherent nature of this widely adopted batch approach .We proposes a new and different approach to mine frequent patterns as discriminative features .It builds a decision tree that sorts or partitions the data onto nodes of tree .", "label": "", "metadata": {}, "score": "40.08693"}
{"text": "This is quite natural because it is harder to differentiate a parent node from its child nodes than from all other nodes and vice versa .Therefore , examples that help such differentiation are informative both for the parent and for the child nodes .", "label": "", "metadata": {}, "score": "40.142998"}
{"text": "Such class - specific weighting can be achieved by spreading the data points l across the m classes .Note also that the above optimization problem ( 2 ) represents a single active learning iteration , and we can also adjust k i from one iteration to the next according to a current belief in the performance of the underlying binary classifiers .", "label": "", "metadata": {}, "score": "40.143677"}
{"text": "Fortunately , the GSupervisedLearner class provides a method that calibrates the predicted distributions , so you often do n't need to worry too much about making sure your distributions precisely express true probabilities .That is , it can automatically learn a function that maps from the distributions that you predict to the actual distributions in the training data .", "label": "", "metadata": {}, "score": "40.576794"}
{"text": "We study the prevalent problem when a test distribution differs from the training distribution .We consider a setting where our training set consists of a small number of sample domains , but where we have many samples in each domain .", "label": "", "metadata": {}, "score": "40.717808"}
{"text": "In this paper , we introduce a method for multi - domain adaptation with unknown domain labels , based on learning nonlinear crossdomain transforms , and apply it to image classification .Our key contribution is a novel version of constrained clustering ; unlike many existing constrained clustering algorithms , ours can be shown to provably converge locally while satisfying all constraints .", "label": "", "metadata": {}, "score": "40.9725"}
{"text": "Machine learning and data mining often employ the same methods and overlap significantly .They can be roughly distinguished as follows : .Machine learning focuses on prediction , based on known properties learned from the training data .The two areas overlap in many ways : data mining uses many machine learning methods , but often with a slightly different goal in mind .", "label": "", "metadata": {}, "score": "41.01147"}
{"text": "A method as claimed in claim 7 comprising selecting the close labeled observations in a manner which optimizes geodesic distances between each unlabeled point and the labeled point it takes its label from .A method as claimed in claim 8 wherein the geodesic distances are calculated on the basis of the clusters and using a distance measure based on correlations between variables which gauges similarity of an unknown sample set to a known one .", "label": "", "metadata": {}, "score": "41.11622"}
{"text": "After human editors label the active set of unlabeled examples , the computer may use the editorially - labeled examples to retrain the classifiers , thereby improving the overall accuracy of the classification system .[ 0023 ] Further to that discussed above , active learning is an automatic technique for selecting examples , which if labeled , can most improve the classifiers .", "label": "", "metadata": {}, "score": "41.278664"}
{"text": "Transduction has the advantage of being able to consider all of the points , not just the labeled points , while performing the labeling task .In this case , transductive algorithms would label the unlabeled points according to the clusters to which they naturally belong .", "label": "", "metadata": {}, "score": "41.60889"}
{"text": "One method is to synthetically generate \" artificial anomalies \" from normal data , and the KAIS journal paper can be found here .Another method we proposed is to mine the correlation among feature values to predict an anomaly score based on normal data , and the extended ICDCS'03 paper can be found here .", "label": "", "metadata": {}, "score": "41.620987"}
{"text": "Its sole purpose is to present a selection of concepts disclosed herein in a simplified form as a prelude to the more detailed description that is presented later .[ 0005 ] Semi - supervised random decision forests for machine learning are described , for example , for interactive image segmentation , medical image analysis , and many other applications .", "label": "", "metadata": {}, "score": "41.6692"}
{"text": "[0067 ] Now suppose that we are allowed to draw and label the top k most - informative data points for each class independently and combine the data points from all classes to obtain the active set , A(t ) , for iteration t. This is a local minimum confidence ( LocalMinConf ) active learning strategy .", "label": "", "metadata": {}, "score": "41.729748"}
{"text": "The essence of learning is to find a model M to approximate an unknown target function F(x ) .[ One could also estimate the joint probability P(x , y ) and this formulation is particularly useful for semi - supervised learning where the label of some data is unknown].", "label": "", "metadata": {}, "score": "42.070923"}
{"text": "Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances ( for example , in classification , one wants to assign a label to instances , and models are trained to correctly predict the pre - assigned labels of a set examples ) .", "label": "", "metadata": {}, "score": "42.368896"}
{"text": "[ 0077 ] Local Minimum Confidence : This is our adaptation of the local methods .As with the GlobalMinConf method , however , we first obtain the vector s j of confidence scores for each x j in U. Unlike the global strategy , each set of scores for the i th classification problem is treated independently . sup . left brkt - top .", "label": "", "metadata": {}, "score": "42.387386"}
{"text": "Some active learning algorithms are built upon Support vector machines ( SVMs ) and exploit the structure of the SVM to determine which data points to label .Such methods usually calculate the margin , , of each unlabeled datum in and treat as an n - dimensional distance from that datum to separating hyperplane .", "label": "", "metadata": {}, "score": "42.479675"}
{"text": "This means that selecting some examples may be most informative across multiple categories of classifiers , and thus can more efficiently train the classifiers as a group .[ 0037 ]To provide clarity to the discussion that follows , the following are typical notations used to describe the active learning process that is followed herein .", "label": "", "metadata": {}, "score": "42.629326"}
{"text": "The source code , some graphical examples , related papers and some sample dataset can be found here .I am interested in inductive mining of data streams .One of our early work is on the use of weighted ensembles to combine models trained from most recent data with models trained in the past in order to reduce the variance term of error bias and variance decomposition .", "label": "", "metadata": {}, "score": "42.750145"}
{"text": "We cast all our multiclass categorization problems in the one - versus - rest framework .For each binary problem , a Linear Support Vector Machine ( SVM ) was trained using the LIBLINEAR package with default parameter settings .For one set of experiments , we set the size of the active set A(t ) that is l to be equal to the number of classes .", "label": "", "metadata": {}, "score": "42.7759"}
{"text": "A machine learning system as claimed in claim 17 comprising an inducer arranged to generate a clustering function by counting the labels for each class in each cluster .Description : .BACKGROUND .[ 0001 ] Random decision forests are ensembles of tree data structures which may be used for a variety of machine learning tasks .", "label": "", "metadata": {}, "score": "42.800938"}
{"text": "17 ] Deep learning algorithms discover multiple levels of representation , or a hierarchy of features , with higher - level , more abstract features defined in terms of ( or generating ) lower - level features .It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data .", "label": "", "metadata": {}, "score": "42.83895"}
{"text": "DESCRIPTION OF THE DRAWINGS .[0007 ] The present description will be better understood from the following detailed description read in light of the accompanying drawings , wherein : . [0008 ] FIG .1 is a schematic diagram of a machine learning system comprising a semi - supervised random decision forest ; .", "label": "", "metadata": {}, "score": "42.937748"}
{"text": "On theother hand , the present inventors have found that users often respond better when presented with a group of samples to be labeled , particularly when the samples are similar to each other and/or arranged in a logical or intuitive manner .", "label": "", "metadata": {}, "score": "42.937984"}
{"text": "The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory .Because training sets are finite and the future is uncertain , learning theory usually does not yield guarantees of the performance of algorithms .", "label": "", "metadata": {}, "score": "43.211548"}
{"text": "This user input may be used to set parameters of the mixed information gain for training , to input training or test data , to access output of the inducer , transducer or random decision forest , or for other purposes .", "label": "", "metadata": {}, "score": "43.231747"}
{"text": "( This would be a very poor learning algorithm . )The \" predictDistribution \" method is similar to \" predict \" , except that it predicts a distribution instead of just a single value .If you are feeling lazy , and you are pretty - sure that you will never use your algorithm with any applications that require predicted distributions , then it might be reasonable to simply implement this method as follows : .", "label": "", "metadata": {}, "score": "43.260212"}
{"text": "For a formal description of how s affects the quality of the dataset , please refer to this paper .Besides formulating the problem , the same paper also provides a model averaging approach and the use of unlabeled data to correct the effect of sample selection bias .", "label": "", "metadata": {}, "score": "43.27663"}
{"text": "A method as claimed in claim 5 comprising calculating the entropy as the negative of the sum over possible ground truth label values of an empirical probability distribution extracted from the training points times the log of the empirical probability distribution .", "label": "", "metadata": {}, "score": "43.52217"}
{"text": "We consider the problem of learning such a ranking function when the data is represented as a ... \" .In ranking , one is given examples of order relationships among objects , and the goal is to learn from these examples a real - valued ranking function that induces a ranking or ordering over the object space .", "label": "", "metadata": {}, "score": "43.690483"}
{"text": "The semi - supervised decision forest 100 seeks to assign labels to all unlabeled points .This is achieved by training the forest 100 using both the labeled observations 104 and the unlabeled observations 106 .[ 0024 ] Data stored at a leaf of each tree in the forest may be combined to form a forest density .", "label": "", "metadata": {}, "score": "43.75974"}
{"text": "For example , as the volume of digital data has exploded in recent years , there issignificant demand for techniques to organize , sort and/or identify such data in a manner that allows it to be useful for a specified purpose .", "label": "", "metadata": {}, "score": "43.897892"}
{"text": "We review the literature on semi - supervised learning , which is an area in machine learning and more generally , artificial intelligence .There has been a whole spectrum of interesting ideas on how to learn from both labeled and unlabeled data , i.e. semi - supervised learning .", "label": "", "metadata": {}, "score": "43.91117"}
{"text": "We review the literature on semi - supervised learning , which is an area in machine learning and more generally , artificial intelligence .There has been a whole spectrum of interesting ideas on how to learn from both labeled and unlabeled data , i.e. semi - supervised learning .", "label": "", "metadata": {}, "score": "43.91117"}
{"text": "3 is a block diagram illustrating a system for machine learning according to a representative embodiment of the present invention .FIG .4 illustrates a process for assigning labels according to a representative embodiment of the present invention .FIG .", "label": "", "metadata": {}, "score": "43.981934"}
{"text": "We have proposed a graph - based hybrid feature selection method that combines the advantage of both supervised and semi - supervised feature selection .When the number of labeled examples is limited , traditional supervised feature selection techniques often fail due to sample selection bias or unrepresentative sample problem .", "label": "", "metadata": {}, "score": "44.08031"}
{"text": "[ 0056 ] With reference to FIG .5 an unseen test data point is received 1000 .A tree is selected 1004 from the already trained semi - supervised random decision forest .The leaves of the trees in the trained tree have associated clusters of training data and labels have been propagated to all these training data points using the transducer .", "label": "", "metadata": {}, "score": "44.28955"}
{"text": "In this case , all examples are assumed to be informative for each of the classes .Naturally , all solutions of problem ( 3 ) will lead to the same objective value and the system 100 will pick data points at random .", "label": "", "metadata": {}, "score": "44.472336"}
{"text": "Preferably , this objective includes metrics designed to identify any samples in the training set 45 that can be considered to be outliers , i.e. , samples whose assigned classification labels 8 are significantly different than what the classifier 3 ( orprediction / modeling module 52 would predict for them ) .", "label": "", "metadata": {}, "score": "44.522766"}
{"text": "Initial training set 45 could have been generated in the conventional manner .That is , referring to FIG .3 , various samples 7 are selected anddesignated for labeling .Ordinarily , the samples 7 are chosen so as to be representative of the types of samples which one desires to classify using the resulting machine - learning classifier ( e.g. , unlabeled samples 8) .", "label": "", "metadata": {}, "score": "44.604797"}
{"text": "A core objective of a learner is to generalize from its experience .[ 4 ] [ full citation needed ] [ 16 ] Generalization in this context is the ability of a learning machine to perform accurately on new , unseen examples / tasks after having experienced a learning data set .", "label": "", "metadata": {}, "score": "44.614693"}
{"text": "Due to its wide applicability , the problem of semi - supervised classification is attracting increasing attention in machine learning .Semi - Supervised Support Vector Machines ( S 3 VMs ) are based on applying the margin maximization principle to both labeled and unlabeled examples .", "label": "", "metadata": {}, "score": "44.619286"}
{"text": "Due to its wide applicability , the problem of semi - supervised classification is attracting increasing attention in machine learning .Semi - Supervised Support Vector Machines ( S 3 VMs ) are based on applying the margin maximization principle to both labeled and unlabeled examples .", "label": "", "metadata": {}, "score": "44.619286"}
{"text": "Reinforcement learning differs from the supervised learning problem in that correct input / output pairs are never presented , nor sub - optimal actions explicitly corrected .Several learning algorithms , mostly unsupervised learning algorithms , aim at discovering better representations of the inputs provided during training .", "label": "", "metadata": {}, "score": "44.71054"}
{"text": "In this paper we address the subproblem of domain adaptation , in which a model trained over a source domain is generalized to ... \" .The problem of transfer learning , where information gained in one learning task is used to improve performance in another related task , is an important new area of research .", "label": "", "metadata": {}, "score": "44.738083"}
{"text": "Any of a variety of different ( e.g. , conventional ) techniques can beused for selecting the initial training samples 7 .Alternatively , or in addition , depending for example upon when the process illustrated in FIG .2 is invoked , the initial training samples 7 could have been generated in whole or in part by thetechniques of the present invention .", "label": "", "metadata": {}, "score": "44.78425"}
{"text": "Typically , the input object is transformed into a feature vector , which contains a number of features that are descriptive of the object .The number of features should not be too large , because of the curse of dimensionality ; but should be large enough to accurately predict the output .", "label": "", "metadata": {}, "score": "44.85282"}
{"text": "However , in many practical applications , unlabeled training examples are readily available but labeled ones are fairly expensive to obtain .Therefore , semi - sup ... \" .The traditional setting of supervised learning requires a large amount of labeled training examples in order to achieve good generalization .", "label": "", "metadata": {}, "score": "44.868763"}
{"text": "0009 ] FIG .2 is an example of data input to a semi - supervised random decision forest and how that data may be labeled using a transducer and an inducer ; .[ 0015 ] FIG .8 illustrates an exemplary computing - based device in which embodiments of a machine learning system with a semi - supervised random decision forest may be implemented .", "label": "", "metadata": {}, "score": "44.92109"}
{"text": "[ 0074 ]The IPMCAL method was tested against three other active learning strategies as discussed below .[0075 ]Random selection : This is the baseline approach that any active learning strategy should out - perform .In this method , the active set A(t ) for each active learning iteration was selected by sampling l data points from the unlabeled active pool U uniformly at random .", "label": "", "metadata": {}, "score": "45.126854"}
{"text": "right brkt - bot .data points from it , where / is the size of the active set A(t ) .If some of the top k examples repeat across multiple L i sets , then the system randomly selects a subset from the ( k+1 ) st least - confident examples for each classification problem until the system guarantees that l distinct examples for labeling in A(t ) .", "label": "", "metadata": {}, "score": "45.212646"}
{"text": "We introduce a new datadependent regularizer based on smoothness assumption into Least - Squares SVM ( LS - SVM ) , which enforces that the target classifier shares similar decision values with the auxiliary classifiers from relevant source domains on the unlabeled patterns of the target domain .", "label": "", "metadata": {}, "score": "45.241447"}
{"text": "A method according to claim 12 , wherein the selected sample is selected in step ( c ) based on calculated scores from among samples in the training set and samples in the set of unlabeled samples .A method according to claim 10 , wherein the specified selection criterion comprises a variety constraint which biases toward selecting additional training samples in subspaces where the unlabeled samples are inadequately represented by thetraining samples .", "label": "", "metadata": {}, "score": "45.35318"}
{"text": "The engineer then runs the learning algorithm on the gathered training set .Parameters of the learning algorithm may be adjusted by optimizing performance on a subset ( called a validation set ) of the training set , or via cross - validation .", "label": "", "metadata": {}, "score": "45.55306"}
{"text": "However , there is an apparent trade - off .The larger the q , the higher the number of data points that appear across different active sets A i ( t ) , but the average informativeness of each individual active set also decreases .", "label": "", "metadata": {}, "score": "45.629066"}
{"text": "A good example is a data - mining based intrusion detection system ( IDS ) where the target is to catch as many malicious attacks as possible and in the same minimize the overhead of prediction and maximize system throughput .This appears in the following ECML'00 paper .", "label": "", "metadata": {}, "score": "45.68871"}
{"text": "The training elements may be stored individually or a compact representation of the accumulated training elements may be stored 430 at each leaf .For example , a probability distribution representing the training elements accumulated at a leaf may be stored .", "label": "", "metadata": {}, "score": "45.754704"}
{"text": "Both works are demonstrated in VLDB'04 , and the software and dataset is available from Software and Dataset Download .Another related problem in data stream mining is that labeled data may not be available instantly but it takes time and money to obtain them .", "label": "", "metadata": {}, "score": "45.8303"}
{"text": "This is repeated 1010 for the other trees in the forest .The cluster representations are aggregated , for example by computing an average to obtain at least one cluster for the test data point .The test data point is then associated with a class label with a confidence .", "label": "", "metadata": {}, "score": "45.884487"}
{"text": "Previous work has studied the supervised version of this problem in which labeled data from both source and target domains are available for training .In this work , however , we study the more challenging problem of unsupervised transductive transfer learning , where no labeled data from the target domain are available at training time , but instead , unlabeled target test data are available during training .", "label": "", "metadata": {}, "score": "45.903908"}
{"text": "Disparity measures of the foregoing types generally focus on individual training samples .In this embodiment , rather than identifying individualsamples that are farthest from their predicted labels , the system 40 identifies , e.g. , a cluster of training samples within a region or subspace of the overall feature set that does not appear to conform to the model .", "label": "", "metadata": {}, "score": "45.983696"}
{"text": "8 is a flow diagram illustrating a machine - learning process according to a second representative embodiment of the present invention .FIG .9 is a block diagram illustrating the selection of samples from among labeled training samples and unlabeled samples , according to a representative embodiment of the present invention .", "label": "", "metadata": {}, "score": "46.036835"}
{"text": "The code and dataset written by Jing Gao can be found here .II .In a separate approach , we propose a framework ( as shown below ) to actively transfer examples from out - domain into in - domain .", "label": "", "metadata": {}, "score": "46.04961"}
{"text": "Transduction algorithms can be broadly divided into two categories : those that seek to assign discrete labels to unlabeled points , and those that seek to regress continuous labels for unlabeled points .Algorithms that seek to predict discrete labels tend to be derived by adding partial supervision to a clustering algorithm .", "label": "", "metadata": {}, "score": "46.169052"}
{"text": "( The GAutoFilter class uses these methods to determine how to convert data types before passing them to your learning algorithm . )The following virtual methods are implemented by default : .These methods basically say that your algorithm can handle both nominal and continuous features , both nominal and continuous labels ( both classification and regression ) , that it can handle any range for continuous values , and that it can handle missing feature values .", "label": "", "metadata": {}, "score": "46.370235"}
{"text": "We try to satisfy as many constraints as possible , and from all solutions that satisfy the maximum number of constraints , we pick the one that has most shared variables across the satisfied constraints .The interpretation now is as follows .", "label": "", "metadata": {}, "score": "46.386223"}
{"text": "This datum with each assumed class is added to and then the new is cross - validated .It is assumed that when the datum is paired up with its correct label , the cross - validated accuracy ( or correlation coefficient ) of will most improve .", "label": "", "metadata": {}, "score": "46.48507"}
{"text": "This paper discusses this problem as well as provides a solution called \" ReverseTesting \" .One earlier paper clarifies the concept that \" feature bias \" does n't affect the learning accuracy of some learners .The long version including experimental studies can be found here .", "label": "", "metadata": {}, "score": "46.605442"}
{"text": "we have proposed the use of a virtual world where the labels of the different objects are obtained automatically .This means that the human models ( classifiers ) are learnt using the appearance of realistic computer graphics .Later , these models are used for human detection in images of the real world .", "label": "", "metadata": {}, "score": "46.74189"}
{"text": "Having a very expensive active set selection method can make this process extremely inefficient .To remedy the efficiency problem , many active set selection methods consider active pool sets of unreasonably small sizes .In reality , however , practitioners are likely to choose examples for labeling from active pools of sizes similar or even larger than the pools of the DMOZ and User Queries data sets described below .", "label": "", "metadata": {}, "score": "46.78639"}
{"text": "Such samples 2 typically are represented forpurposes of classification by automated classifier 3 as a set of feature values , as discussed in more detail below .Classifier 3 applies a predetermined algorithm based on a supervised learning technique ( e.g. , Support Vector Machines or Naive Bayes ) in order to obtain class predictions 4 .", "label": "", "metadata": {}, "score": "46.804"}
{"text": "Supervised learning is a machine learning technique for deducing a function from training data .The training data consist of pairs of input objects ( typically vectors ) , and desired outputs .The output of the function can be a continuous value ( called regression ) , or can predict a class label of the input object ( called classification ) .", "label": "", "metadata": {}, "score": "46.861496"}
{"text": "Even for such active pools , we show that q is not excessively large .This is because the larger a taxonomy is , the more correlation there is likely to be among the categories represented by its nodes .In this case , many examples that are highly informative for one category are likely to be highly informative for similar categories , too .", "label": "", "metadata": {}, "score": "47.034073"}
{"text": "Other factors preferably also are taken into consideration .For example , certain embodiments incorporate a bias toward selecting samples from training set 45 whose assigned labels have a lower presumption of accuracy .Such abias is applied , e.g. , by multiplying the cumulative adjusted score of each sample in training set 45 by a factor that is based on the presumed accuracy of its corresponding assigned classification label 8 .", "label": "", "metadata": {}, "score": "47.101933"}
{"text": "This might lead to almost no improvement in the rest of the binary classifiers .[ 0046 ]In contrast , proposed herein is an integer programming based approach that finds active learning sets , A(t ) , containing examples that are informative across the maximum number of classifiers as opposed to just one .", "label": "", "metadata": {}, "score": "47.175255"}
{"text": "Related Art .[0004 ] The rapid growth and ever - changing nature of web content demands automated methods of managing it .One such methodology is categorization in which document ( and other types of ) content is automatically placed into nodes of a human - induced taxonomy .", "label": "", "metadata": {}, "score": "47.229458"}
{"text": "For instance , in the case of large margin classifiers , such an ordering can be provided by the distance of each data point to the separation plane for the subproblem .Similarly , for generative models , the ordering may be provided by the uncertainty sampling method .", "label": "", "metadata": {}, "score": "47.244507"}
{"text": "This line , too , was continued outside the AI / CS field , as \" connectionism \" , by researchers from other disciplines including Hopfield , Rumelhart and Hinton .Their main success came in the mid-1980s with the reinvention of backpropagation .", "label": "", "metadata": {}, "score": "47.350227"}
{"text": "The most promising human detectors rely on discriminatively learnt classifiers , i.e. , trained with labeled samples .However , labelling is a manual intensive task , especially in cases like human detection where it is necessary to provide at least bounding boxes framing the humans for training .", "label": "", "metadata": {}, "score": "47.551926"}
{"text": "The method iteratively finds a relaxed real value solution , for problem ( 3 ) and then rounds it to the closest binary solution .This is repeated until convergence .The evaluation presented in the experimental evaluation discussed below uses the freeware lpsolve for binary optimization as it can solve sparse problems with thousands of constraints and a few hundreds of thousands of examples .", "label": "", "metadata": {}, "score": "47.59658"}
{"text": "In an example , a plurality of labeled observations are accessed , each labeled observation having a label indicating one of a plurality of classes that the labeled observation is a member of ; a plurality of unlabeled observations are also accessed .", "label": "", "metadata": {}, "score": "47.80246"}
{"text": "We then adapt these models to the problem of transfer learning for protein name extraction .In the process , we introduce a novel maximum entropy based technique , Iterative Feature Transformation ( IFT ) , and show that it achieves comparable performance with state - of - the - art transductive SVMs .", "label": "", "metadata": {}, "score": "47.874626"}
{"text": "Our experiments use the same test set T at each iteration , and a separate pool of unlabeled examples U for active learning .We keep the size 1 of the active set A(t ) fixed across all iterations .[0073 ] All the data sets were pre - processed to convert the text to lower case and to remove all non alpha - numeric characters .", "label": "", "metadata": {}, "score": "47.933228"}
{"text": "In such a case , the best alternative mightbe to provide the training sample 77 for a third labeling .The provided information then can be incorporated into the prediction model and used , e.g. , to inform the training module to put less weight on such samples ; for thispurpose , there exist several known training techniques that consider weighted training samples .", "label": "", "metadata": {}, "score": "48.084892"}
{"text": "[ 0042 ] As mentioned above a training objective function is used to train a semi - supervised random decision forest which comprises a plurality of randomly trained trees .[ 0043 ] The training objective function ( given by the mixed information gain ) seeks to encourage both separation of differently labeled training data as well as separating different high density regions from one another .", "label": "", "metadata": {}, "score": "48.121735"}
{"text": "This is normally performed thru regularization in order to resolve related problems such as ill - posed problem , numerical instability etc .One big challenge for these methods is to apply to a new application where the true physics is unknown or too complicated to understand is to choose the right form of function .", "label": "", "metadata": {}, "score": "48.186554"}
{"text": "by Olivier Chapelle , Mingmin Chi , Alexander Zien - In International Conference on Machine Learning , 2006 . \" ...Semi - Supervised Support Vector Machines ( S3VMs ) are an appealing method for using unlabeled data in classification : their objective function favors decision boundaries which do not cut clusters .", "label": "", "metadata": {}, "score": "48.20108"}
{"text": "A related problem on is how to evolve the model with a small number of labeled examples .Some stream applications have extremely skewed distributions ( like 1 % or so positives ) .Specialized sampling techniques have to be proposed in order to handle these problems .", "label": "", "metadata": {}, "score": "48.263775"}
{"text": "Since the number of examples towards leaf level is relatively small , the new approach is able to examine patterns with extremely low global support that could not be enumerated on the whole dataset by the batch method .The discovered feature vectors are more accurate on some of the most difficult graph and frequent itemset problems than most recently proposed algorithms but the total size is typically 50 % or more smaller .", "label": "", "metadata": {}, "score": "48.40631"}
{"text": "A suite of algorithms have recently been proposed for solving S 3 VMs .This paper reviews key ideas in this literature .The performance and behavior of various S 3 VM algorithms is studied together , under a common experimental setting . .", "label": "", "metadata": {}, "score": "48.505615"}
{"text": "[ 0026 ] The active learning module 118 may identify low confidence regions in the observed data space by finding regions in that space which have few observations and/or which have observations with low confidences .Once the low confidence regions are identified , new observations in those low confidence regions are obtained .", "label": "", "metadata": {}, "score": "48.549004"}
{"text": "We present experiments with sequence models on part - of - speech tagging and named entity recognition tasks , and with syntactic parsers on dependency parsing and machine translation reordering tasks .Afternoon Session .We study a novel variant of the domain adaptation problem , in which the loss function on test data changes due to dependencies on prior predictions .", "label": "", "metadata": {}, "score": "48.653152"}
{"text": "Another term for supervised learning is classification .A wide range of classifiers are available , each with its strengths and weaknesses .Classifier performance depends greatly on the characteristics of the data to be classified .There is no single classifier that works best on all given problems ; this is also referred to as the No free lunch theorem .", "label": "", "metadata": {}, "score": "48.77677"}
{"text": "Defining robustly \" informative \" examples for such poor performing models is hard and active set selection methods are likely to introduce little to no improvement .[ 0095 ] Testing with Behavioral Targeting Categorization .[ 0096 ] In an additional test , we built initial models for the 195 most - important classifiers in terms of revenue in behavioral targeting categories .", "label": "", "metadata": {}, "score": "48.849716"}
{"text": "One bottom line question is : given a stream mining application , what is the most important assumption that one can safely assume and what techniques would guarantee perform and return of technology investment ?Our analysis on this problem can be found here .", "label": "", "metadata": {}, "score": "48.863914"}
{"text": "8 provides a flow diagram illustrating certain exemplary techniques embodying this approach , with additional references to the block diagram shown in FIG .9 .Preferably , the entire process illustrated in FIG .8 is implemented in software , e.g . , by reading and executing software code from a computer - readable medium .", "label": "", "metadata": {}, "score": "48.93953"}
{"text": "In Sra , Suvrit ; Nowozin , Sebastian ; Wright , Stephen J. Optimization for Machine Learning .MIT Press .p. 404 .Ian H. Witten and Eibe Frank ( 2011 ) .Data Mining : Practical machine learning tools and techniques Morgan Kaufmann , 664pp . , ISBN 978 - 0 - 12 - 374856 - 0 .", "label": "", "metadata": {}, "score": "48.957756"}
{"text": "The computer may order the sampled unlabeled pages in ascending order of informativeness for each classifier .The computer may also determine a minimum subset of the unlabeled examples that are most informative for a maximum number of the classifiers to form an active set for learning .", "label": "", "metadata": {}, "score": "49.00183"}
{"text": "The generated labels 112 may then be used by an active learning module 118 .For example , in a medical image application , the labels and uncertainty information generated by the transducer may be used to identify low confidence regions in the observed data space .", "label": "", "metadata": {}, "score": "49.00405"}
{"text": "A semi - supervised decision forest 808 may be stored at memory 812 and may be trained using training engine 806 as described herein with reference to FIG .4 .A data store 810 may be provided to store training parameters , training data , probability distributions and other information .", "label": "", "metadata": {}, "score": "49.022156"}
{"text": "A collection of points is given , such that some of the points are labeled ( A , B , or C ) , but most of the points are unlabeled ( ? )The goal is to predict appropriate labels for all of the unlabeled points .", "label": "", "metadata": {}, "score": "49.05613"}
{"text": "A method according to claim 1 , wherein the reply classification label has been assigned by a human being .A method according to claim 1 , wherein said requesting step ( d ) comprises providing the predicted label for said at least one training sample to a user and asking the user to indicate whether the predicted label for said at least onetraining sample is correct .", "label": "", "metadata": {}, "score": "49.16782"}
{"text": "[ 4 ] :3 .Between supervised and unsupervised learning is semi - supervised learning , where the teacher gives an incomplete training signal : a training set with some ( often many ) of the target outputs missing .Transduction is a special case of this principle where the entire set of problem instances is known at learning time , except that part of the targets are missing .", "label": "", "metadata": {}, "score": "49.393967"}
{"text": "Aggregating the probability distributions associated with leaves of each tree forms a forest density which is an estimate of an unknown probability density function from which the training data is generated .[ 0021 ] Unlabeled observations 106 are stored at a database or are accessible from another source .", "label": "", "metadata": {}, "score": "49.394768"}
{"text": "Active learning can be especially useful in biological research problems such as Protein engineering where a few proteins have been discovered with a certain interesting function and one wishes to determine which of many possible mutants to make next that will have a similar function [ 1 ] .", "label": "", "metadata": {}, "score": "49.4281"}
{"text": "Variety preferably is achieved bydetermining how well the various unlabeled samples 2 are represented by the training samples 7 .In one embodiment of the invention , samples 2 are divided into clusters ( e.g. , using conventional clustering techniques ) , and each cluster is evaluated todetermine whether it is proportionally represented by samples in training set 45 .", "label": "", "metadata": {}, "score": "49.526207"}
{"text": "6 an example of process at the transducer is described .Clusters stored at the leaf nodes of the trained semi - supervised random decision forest are used 600 as neighborhoods which give good approximations of pairwise geodesic distances .The inducer assigned 602 labels to previously unlabeled training data so that geodesic distances between pairs of points with the same label are minimal , where the geodesic distance takes into account symmetric Mahalanobis distances as well as cluster neighborhoods .", "label": "", "metadata": {}, "score": "49.58332"}
{"text": "In this extended abstract we summarize our proposal , and include quantitative results from Vazquez et al . showing its validity .Sign up to receive free email alerts when patent applications with chosen keywords are published SIGN UP .Abstract : .", "label": "", "metadata": {}, "score": "49.603645"}
{"text": "Transfer learning attempts to use labelled examples from one domain ( \" out - domain \" ) to construct accurate models to predict on examples from another domain ( \" in - domain \" ) .The main challenge is that in - domain and out - domain data explicitly do not follow the same distribution , and importantly there is a large number of labeled examples from out - domain and none or very few examples from in - domain .", "label": "", "metadata": {}, "score": "49.63823"}
{"text": "In order to enumerate these low support patterns , state - of - the - art frequent pattern algorithm either can not finish due to huge memory consumption or have to enumerate 10 ^ 1 to 10 ^ 3 times more patterns before they can even be found .", "label": "", "metadata": {}, "score": "49.650867"}
{"text": "In these cases , it is inadequate to estimate a parameter for an evolving test distribution with a validation set .We , therefore , devised an unsupervised procedure for determining the value of q. Note again the unlabeled active pool U can be \" refreshed \" with newer examples from a distribution similar to that of the test environment .", "label": "", "metadata": {}, "score": "49.6546"}
{"text": "Such an expectation can be provided by a soft classification score output by the classifier 3 itself .In such embodiments , the classifier 3 preferably is roughly calibrated to provide soft classification scores that correspond to suchexpectations , e.g. , using calibration techniques known in the art , such as calibration based on a cross - validation calculation performed using later - identified actual classification labels .", "label": "", "metadata": {}, "score": "49.705204"}
{"text": "The article describes Rebellion Research 's prediction of the financial crisis and economic recovery .[28 ] .In 2014 it has been reported that a machine learning algorithm has been applied in Art History to study fine art paintings , and that it may have revealed previously unrecognized influences between artists .", "label": "", "metadata": {}, "score": "49.716484"}
{"text": "We formally show that importing feature values across domains and latentsemantic index can jointly make the distributions of two related domains easier to measure than in original feature space , the nearest neighbor method employed to retrieve related out domain examples is bounded in error when predicting in - domain examples .", "label": "", "metadata": {}, "score": "49.89524"}
{"text": "Associated with each leaf node is data accumulated during a training phase when the forest is trained with both labeled observations 104 and unlabeled observations 106 .During training the structure of the trees is learnt .The data at the leaf nodes may be aggregated .", "label": "", "metadata": {}, "score": "50.17988"}
{"text": "The main advantage for this framework is that it is straightforward and robust .One earlier work that uses ensemble for cost - sensitive learning appears in SDM'02 can be found here .In particular , the results can \" make more money \" on the donation dataset of KDD'98 winner .", "label": "", "metadata": {}, "score": "50.577866"}
{"text": "Often , a classifier generates a soft classification score and then makes a hard classification decision based on that score .In either event , in one representative embodiment module 53 selects one or more of the samples from training set 45 that have the largest disparity between the label assigned through module 43 and the label predicted by module 52 , designatingsuch selections as selected samples 77 .", "label": "", "metadata": {}, "score": "50.60137"}
{"text": "It is well understood that frequent pattern mining is non - trivial since the number of unique patterns is exponential but many are non - discriminative and correlated .Currently , frequent pattern mining is performed in batch mode of two sequential steps : enumerating a set of frequent patterns as candidate features , followed by feature selection .", "label": "", "metadata": {}, "score": "50.606216"}
{"text": "Linear Discriminant Analysis ( LDA ) has been a popular method for extracting features which preserve class separability .The projection vectors are commonly obtained by maximizing the between class covariance and simultaneously minimizing the within class covariance .In practice , when there is no sufficient training samples , the covariance matrix of each class may not be accurately estimated .", "label": "", "metadata": {}, "score": "50.62731"}
{"text": "If some other type of data is passed to your algorithm , it will be automatically converted to a type that your algorithm can handle before your code ever receives it .The third step to complete your algorithm is to implement all of the pure virtual methods required by your base class .", "label": "", "metadata": {}, "score": "50.688053"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1 is a block diagram illustrating a system according to a representative embodiment of the present invention .FIG .2 is a flow diagram illustrating a machine - learning process according to a first representative embodiment of the present invention .", "label": "", "metadata": {}, "score": "50.741467"}
{"text": "A method according to claim 10 , wherein plural selected samples are selected in step ( c ) and submitted in step ( d ) for labeling as a group , arranged based on predicted labels for said selected samples .A method according to claim 10 , wherein in step ( d ) the selected sample is submitted together with a designated label and a request to confirm whether the designated label is correct .", "label": "", "metadata": {}, "score": "50.800026"}
{"text": "As shown in FIG . 8 ( b ) , for the RCV1 dataset , 50 examples were labeled per iteration for 92 classes .As shown in FIG . 8 ( c ) , for the DMOZ dataset , 200 examples were labeled per iteration for 367 classes .", "label": "", "metadata": {}, "score": "50.90246"}
{"text": "But still , it is a non - trivial task to look for the most appropriate form of function or sometimes functionals .For example , for most effective use of SVM , one ought to choose the right kernel .Given these observations , we have been interested in developing a family of methods that do not require deep understanding of the algorithms , and do not require too much tuning and parameterization .", "label": "", "metadata": {}, "score": "50.966667"}
{"text": "29 ] .^ a b c d e Machine learning and pattern recognition \" can be viewed as two facets of the same field . \"^ Le Roux , Nicolas ; Bengio , Yoshua ; Fitzgibbon , Andrew ( 2012 ) .", "label": "", "metadata": {}, "score": "51.040775"}
{"text": "1 .A random decision forest is a plurality of random decision trees each having a root node , a plurality of split nodes and a plurality of leaf nodes .The root nodes , leaf nodes and split nodes may be represented using data structures in memory and/or as computer implemented instructions .", "label": "", "metadata": {}, "score": "51.063145"}
{"text": "In general , it should be noted that , except asexpressly noted otherwise , any of the functionality described above can be implemented in software , hardware , firmware or any combination of these , with the particular implementation being selected based on known engineering tradeoffs .", "label": "", "metadata": {}, "score": "51.08689"}
{"text": "Testing it .To test your new algorithm , you might find the GSupervisedLearner::basicTest method to be helpful .It will exercise your algorithm with a few very simple synthetic datasets .Of course , your own testing is likely to be better - suited for the strengths and capabilities of your algorithm .", "label": "", "metadata": {}, "score": "51.15178"}
{"text": "The labeled data points are used to maximize the separability between different classes and the unlabeled data points are used to estimate the intrinsic geometric structure of the data .Specifically , we aim to learn a discriminant function which is as smooth as possible on the data manifold .", "label": "", "metadata": {}, "score": "51.166027"}
{"text": "The structure of a tree is the number and arrangement of the internal nodes .Training is often time consuming and computationally expensive .[ 0034 ] Using both unlabeled and labeled data for training means that a training function that works for both types of data needs to be designed .", "label": "", "metadata": {}, "score": "51.17182"}
{"text": "An advantage of transduction is that it may be able to make better predictions with fewer labeled points , because it uses the natural breaks found in the unlabeled points .One disadvantage of transduction is that it builds no predictive model .", "label": "", "metadata": {}, "score": "51.191704"}
{"text": "Random decision forest technology is useful in many application domains such as gesture recognition for natural user interfaces , computer vision , robotics , medical image analysis and others .[0002 ] A decision forest is a plurality of decision trees each having a root node , a plurality of split nodes and a plurality of leaf nodes .", "label": "", "metadata": {}, "score": "51.2501"}
{"text": "[ 0036 ] FIG .4 is a flow diagram of a method of training a semi - supervised forest .Each individual tree in the forest may be trained independently and this training may take place in parallel .The example in FIG .", "label": "", "metadata": {}, "score": "51.4367"}
{"text": "A method according to claim 1 , wherein plural similar training samples are selected and presented for confirmation / re - labeling , as a group of similar training samples , in said requesting step ( d ) .A method according to claim 1 , further comprising a step of tagging the reply classification label for said at least one training sample as having a stronger presumption of accuracy than the assigned classification label for said at leastone training sample .", "label": "", "metadata": {}, "score": "51.48271"}
{"text": "Within the clusters some of the data is labeled since some of the observations are labeled 104 .A transducer 110 may be used to generate labels 112 for the previously unlabeled training data and also to produce confidences for those labels .", "label": "", "metadata": {}, "score": "51.55726"}
{"text": "This method evaluates the three matrices that are passed in , and returns a matrix of labels that corresponds with features2 .( Theoretically , transduction algorithms can achieve the best accuracy because they operate using the most information .That is , they can see the features of the test set , features2 , while they are learning .", "label": "", "metadata": {}, "score": "51.71473"}
{"text": "An active learning module 826 may be provided to use information from the transducer and semi - supervised decision forest 808 .[ 0061 ] The computer executable instructions may be provided using any computer - readable media that is accessible by computing based device 800 .", "label": "", "metadata": {}, "score": "51.918175"}
{"text": "12 is a flow chart of a second exemplary method , executable for each active learning iteration of the method of FIG .11 .[ 0021 ] FIG .13 illustrates a general computer system , which may represent any of the computing devices referenced herein .", "label": "", "metadata": {}, "score": "51.944443"}
{"text": "Such sub - criteria are implemented , in representative embodiments , by clustering the trainingsamples based on their feature sets ( e.g. , using known clustering techniques ) .It is noted that the number of samples from training set 45 to be selected or the criterion for determining such number preferably is specified by a user of system 40 .", "label": "", "metadata": {}, "score": "51.955353"}
{"text": "3 , such selection is performed by processing block 50,which includes prediction / modeling module 52 and comparison / selection module 53 , with module 53 optionally considering certain other inputs 54 .In one representative embodiment , labels are predicted for some or all of the samples in training set 45 using prediction / modeling module 52 .", "label": "", "metadata": {}, "score": "52.030174"}
{"text": "Then we show the performance of the classifier on the sample .Again , the system 100 may need to sample a few hundred examples per each one - versus - all model , which for 1000 categories , means a few hundred thousand examples to be labeled .", "label": "", "metadata": {}, "score": "52.069542"}
{"text": "[ 0025 ] Each tree in the forest acts to cluster the labeled and the unlabeled observations and to produce certainties of those clusters .Each tree is trained independently and so the clusters of each tree are different .Thus the semi - supervised decision forest 100 of FIG .", "label": "", "metadata": {}, "score": "52.17228"}
{"text": "[0114 ]The method and system may also be embedded in a computer program product , which includes all the features enabling the implementation of the operations described herein and which , when loaded in a computer system , is able to carry out these operations .", "label": "", "metadata": {}, "score": "52.29721"}
{"text": "Thus , using the computed risk factors , the bank may either accept customers that may default a loan or refuse customers that can be loyal and profitable .All these types of problems are called Sample Selection Bias .Assume that P(x , y ) is the true distribution to model .", "label": "", "metadata": {}, "score": "52.33177"}
{"text": "However , the same or equivalent functions and sequences may be accomplished by different examples .[ 0018 ]Although the present examples are described and illustrated herein as being implemented in an image processing system , the system described is provided as an example and not a limitation .", "label": "", "metadata": {}, "score": "52.38925"}
{"text": "[ 0003 ] The embodiments described below are not limited to implementations which solve any or all of the disadvantages of known machine learning systems which use random decision forests .SUMMARY .[0004 ]The following presents a simplified summary of the disclosure in order to provide a basic understanding to the reader .", "label": "", "metadata": {}, "score": "52.424065"}
{"text": "We used the 10%-40%-50 % random partitioning of this data as well to obtain the initial labeled set L(0 ) , the active pool U , and the test set T , respectively .[ 0083 ]Note that for all data sets , only those classes that have at least five ( 5 ) data points in all three sets ( L(0 ) , U , and T ) are retained .", "label": "", "metadata": {}, "score": "52.439774"}
{"text": "In addition , we show how simple relaxations , such as providing additional information like the proportion of positive examples in the test data , can significantly improve the performance of some of the transductive transfer learners . ... onvenience .In the paradigm of inductive learning , ( Xtrain , Ytrain ) are known , while both Xtest and Ytest are completely hidden during training time .", "label": "", "metadata": {}, "score": "52.608067"}
{"text": "Comprehensive experiments on the challenging TRECVID 2005 corpus demonstrate that DAM outperforms the existing multiple source domain adaptation methods for video concept detection in terms of effectiveness and efficiency .need to be used for predicting labels of the target patterns , making it inefficient for large - scale applications ( e.g. , video concept detection ) . \" ...", "label": "", "metadata": {}, "score": "52.674446"}
{"text": "In step 134 , samples are selected 175 from among both training set 45 and prediction set 171 , e.g. , by evaluating samples in both such sets and selecting one or more based on a specified selection criterion .In other embodiments , the prediction - confidence consideration 178 can be replaced with any known active - learningtechnique that evaluates the importance of labeling individual cases or any other technique that attempts to identify samples for labeling .", "label": "", "metadata": {}, "score": "52.74176"}
{"text": "[ 0022 ] By way of introduction , disclosed is a system and methods for training classifiers in multiple categories through active learning .A computer may first train an initial set of m binary one - versus - all classifiers , one for each category in a taxonomy , on a labeled dataset of examples stored in a database .", "label": "", "metadata": {}, "score": "52.81252"}
{"text": "[ 1 ] In logic , statistical inference , and supervised learning , transduction or transductive inference is reasoning from observed , specific ( training ) cases to specific ( test ) cases .In contrast , induction is reasoning from observed training cases to general rules , which are then applied to the test cases .", "label": "", "metadata": {}, "score": "52.8462"}
{"text": "Later , we proposed a method that detects anomaly based on the correlation among multiple features in the data stream , and the paper appeared in ICDCS'03 .Since labeled data is difficult to find in real - world intrusion detection systems , anomaly detection is an important alternative .", "label": "", "metadata": {}, "score": "52.86982"}
{"text": "The system of claim 8 , where the processor is further programmed to receive the active set of examples as labeled by editors into multiple categories of the taxonomy .Description : .BACKGROUND .[ 0001 ] 1 .Technical Field .", "label": "", "metadata": {}, "score": "52.874863"}
{"text": "The cost - effectiveness can be measured from two perspectives .First , various types of intrusions incur significantly different damage to the system , and in the same time , different features has different levels of operational cost to collect .", "label": "", "metadata": {}, "score": "52.902"}
{"text": "Melville , P. and Mooney , R , \" Diverse ensembles for active learning , \" Proc . of the 21st Int'l Conf . on Machine Learning ( ICML , Banff ) , 584 - 591,2004 . cited by other .L. Li , A. Pratap , H.-T. Lin , and Y. S. Abu - Mostafa , \" Improving Generalization by Data Categorization , \" in A. Jorge et al . , eds . , Knowledge Discovery in Databases : PKDD 2005 , vol .", "label": "", "metadata": {}, "score": "53.00101"}
{"text": "19 ] A popular heuristic method for sparse dictionary learning is K - SVD .Sparse dictionary learning has been applied in several contexts .In classification , the problem is to determine which classes a previously unseen datum belongs to .", "label": "", "metadata": {}, "score": "53.02803"}
{"text": "7 is an exemplary graph showing a hypothetical ordering of examples ( x i ) in an active pool U according to their informativeness with respect to two of the one - versus - rest classifiers ( c 1 and c 2 ) .", "label": "", "metadata": {}, "score": "53.044228"}
{"text": "Machine learning , reorganized as a separate field , started to flourish in the 1990s .The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature .It shifted focus away from the symbolic approaches it had inherited from AI , and toward methods and models borrowed from statistics and probability theory .", "label": "", "metadata": {}, "score": "53.209213"}
{"text": "0007 ] One straight - forward multiclass , active - learning strategy is to apply binary active learning techniques on decomposed , binary subproblems and select the topmost - informative examples independently for each binary classifier .These are examples of local active learning methods , which have been criticized for their lack of ability to scale to real - world problems .", "label": "", "metadata": {}, "score": "53.347027"}
{"text": "[ 0089 ] Table 2 , below , summarizes the results of two sets of experiments performed for each data set , where the bold text indicates the winner , or the top two winners if they are not significantly different .", "label": "", "metadata": {}, "score": "53.358723"}
{"text": "A transducer 110 and an inducer 114 may be used together with the trained semi - supervised random decision forest 100 to obtain solutions to the image segmentation together with confidence information associated with those solutions .In another example , a random decision forest is trained using both labeled and unlabeled medical images in order to automatically detect body organs .", "label": "", "metadata": {}, "score": "53.369698"}
{"text": "The tree posterior probabilities are then aggregated to obtain a forest class posterior probability function .[ 0059 ] FIG .8 illustrates various components of an exemplary computing - based device 800 which may be implemented as any form of a computing and/or electronic device , and in which embodiments of a machine learning system having a semi - supervised random decision forest may be implemented .", "label": "", "metadata": {}, "score": "53.451847"}
{"text": "The proper way to implement this method is to create a distribution for each label element .For example , Suppose you want to predict a categorical distribution for label element 0 with a little bit of the likelihood distributed over categories 0 and 1 , and most of the likelihood on category 2 : . and suppose that you wish to predict a Normal distribution for label element 1 with a mean of 0.1 and a variance of 0.2 : .", "label": "", "metadata": {}, "score": "53.52368"}
{"text": "However , according to the present embodiment , previously labeled samples 7 are selected from training set 45 and submittedfor confirmation / re - labeling , e.g. , as set forth in the following discussion .As used herein , \" confirmation / re - labeling \" refers to the process of submitting an existing training sample for labeling so that its previously assigned label is either confirmed or contradicted , e.g. , by a domain expert or other person .", "label": "", "metadata": {}, "score": "53.525375"}
{"text": "However , functionality generally may be redistributed as desired among any different modules or components , in some cases completelyobviating the need for a particular component or module and/or requiring the addition of new components or modules .The precise distribution of functionality preferably is made according to known engineering tradeoffs , with reference to the specificembodiment of the invention , as will be understood by those skilled in the art .", "label": "", "metadata": {}, "score": "53.573135"}
{"text": "Writing a New Learning Algorithm .Suppose you want to develop a new learning algorithm .This page will describe how this is done .Our interfaces are carefully designed to be friendly to developers , and we expect that you will find it a pleasure to develop with our framework .", "label": "", "metadata": {}, "score": "53.573463"}
{"text": "We show how the Concave - Convex Procedure can be applied to Transductive SVMs , which traditionally require solving a combinatorial search problem .This provides for the first time a highly scalable algorithm in the nonlinear case .Detailed experiments verify the utility of our approach .", "label": "", "metadata": {}, "score": "53.68522"}
{"text": "We show how the Concave - Convex Procedure can be applied to Transductive SVMs , which traditionally require solving a combinatorial search problem .This provides for the first time a highly scalable algorithm in the nonlinear case .Detailed experiments verify the utility of our approach .", "label": "", "metadata": {}, "score": "53.68522"}
{"text": "In the preferred embodiments , module 53 compares such predicted labels to the labels 8 that were assigned via interface module 43 .Then , based onthat comparison , one or more sample(s ) 77 are selected from training set 45 .", "label": "", "metadata": {}, "score": "53.707066"}
{"text": "[0070 ] It will be understood that the benefits and advantages described above may relate to one embodiment or may relate to several embodiments .The embodiments are not limited to those that solve any or all of the stated problems or those that have any or all of the stated benefits and advantages .", "label": "", "metadata": {}, "score": "53.722687"}
{"text": "If you selected GIncrementalLearner for your base class , then it must implement three additional methods ( in addition to all of the methods required by GSupervisedLearner ) : .The \" enableIncrementalLearning \" method tells the model about the number and types of features and labels , so that it has enough information to begin training in an incremental manner .", "label": "", "metadata": {}, "score": "53.77461"}
{"text": "Note that k is the size of a pool of most - informative data points , and q is the size of a relaxed pool of most - informative data points .At block 630 , the system 100 determines if the selected examples l were labeled by editors .", "label": "", "metadata": {}, "score": "53.782875"}
{"text": "In any event , the confirmation / re - labeling information is received for the submitted training examples and , in step 18 , that information is used to modify the training set 45 and then retrain the classifier 3 , e.g. , using training module 5 andthe revised training set 45 .", "label": "", "metadata": {}, "score": "53.840397"}
{"text": "An inducer may be used to learn a generic function that may be applied to previously unseen test points ( black blobs in graph 204 ) in order to label those test points .Training a conventional classifier on the labeled data only would produce a sub - optimal classification surface , i.e. a vertical line in this example .", "label": "", "metadata": {}, "score": "53.95681"}
{"text": "However , in the preferred embodiments samples are selected from boththe set of unlabeled samples 2 and the set of training samples 45 , with the particular selections based on a criterion that attempts to maximize overall benefit to the training process 5 .", "label": "", "metadata": {}, "score": "53.95777"}
{"text": "This provides a simple alternative to ... \" .We show how nonlinear embedding algorithms popular for use with shallow semisupervised learning techniques such as kernel methods can be applied to deep multilayer architectures , either as a regularizer at the output layer , or on each layer of the architecture .", "label": "", "metadata": {}, "score": "53.961052"}
{"text": "Because the system is substituting s variables that only appear in the pool for c i with other s variables that appear only in the same pool , the objective function preserves its value and none of the constraints are violated .", "label": "", "metadata": {}, "score": "54.01904"}
{"text": "The system 100 can force this selection by multiplying every variable in the objective function that appears in a single constraint with a discount factor proportional to its position in the informative ordering .This , however , introduces new parameters and complicates the problem unnecessarily .", "label": "", "metadata": {}, "score": "54.062607"}
{"text": "For example , in a protein engineering problem , would include all proteins that are known to have a certain interesting activity and all additional proteins that one might want to test for that activity . :A subset of that is chosen to be labeled .", "label": "", "metadata": {}, "score": "54.118954"}
{"text": "[ 0039 ] At block 600 , the method starts by training an initial set of m one - versus - rest classifiers 604 , one for each category in the taxonomy .The classifiers can be trained sequentially on one computer 110 or in parallel on multiple computers 110 .", "label": "", "metadata": {}, "score": "54.170288"}
{"text": "FIG .7 is an exemplary graph showing a hypothetical ordering of examples ( x i ) in an active pool U according to their informativeness with respect to two of the one - versus - rest classifiers ( c 1 and c 2 ) .", "label": "", "metadata": {}, "score": "54.17734"}
{"text": "[0070 ] Experimental Evaluation .[ 0071 ] Below , we evaluate IPMCAL on several data sets , some of which are of significantly larger size and with significantly more categories than demonstrated by previous works on the topic .The active set selection method is very important for such large problems .", "label": "", "metadata": {}, "score": "54.580658"}
{"text": "In the latter types of embodiments , when applying the prediction - confidence objective 178 to the training samples in set 45 , the previously assigned labels forsuch samples in set 45 preferably are ignored .In other words , the training samples in set 45 preferably are treated as unlabeled samples .", "label": "", "metadata": {}, "score": "54.592754"}
{"text": "However , in alternate embodiments of the invention : ( i ) module 52 uses the production classifier 3 ( i.e.,which is used to generate predictions 4 for unlabeled input samples 2 ) ; or ( ii ) a different classifier is used by prediction / modeling module 52 .", "label": "", "metadata": {}, "score": "54.768135"}
{"text": "Sort D in ascending order .Consider each point to be a cluster of size 1 .Sign up to receive free email alerts when patent applications with chosen keywords are published SIGN UP .Abstract : .Claims : .The method of claim 1 , further comprising : receiving , by the computer , the active set of examples as labeled by editors into multiple categories of the taxonomy .", "label": "", "metadata": {}, "score": "54.785072"}
{"text": "FIG .2 is a flow diagram illustrating a machine - learning process according to a representative embodiment of the present invention .Ordinarily , the entire process illustrated in FIG .2 is implemented entirely in software , e.g. , by readingsoftware code from a computer - readable medium .", "label": "", "metadata": {}, "score": "54.85933"}
{"text": "5.6.1 EFFECT OF MULTIPLE CLUSTERSIn Chapelle et al .( 2006a ) , cS 3 VM exhibited poor performance in multiclass problems with oneversus - the - rest training , but worked well on pairwise binary problems ... . \" ...", "label": "", "metadata": {}, "score": "54.947224"}
{"text": "Here , animal , person , and building are the categories , i.e. an object can not simultaneously be a person and an animal .In multilabel problems , an example may have multiple labels , e.g. , be assigned to multiple categories simultaneously .", "label": "", "metadata": {}, "score": "55.08894"}
{"text": "[ 0101 ] FIG .10 is an exemplary diagram showing one constraint per score bucket ( q i ) from which to sample in a multiple - class test set sampling .Here , there are multiple values for q i , in this case , divided into the five ( 5 ) buckets covering all of x in .", "label": "", "metadata": {}, "score": "55.181034"}
{"text": ".. le .Semi - supervised kernel [ 37 ] has also been studied in regression tasks [ 18 ] , [ 30 ] , [ 42].III . by Andrew Arnold , Ramesh Nallapati , William W. Cohen - In ICDM Workshop on Mining and Management of Biological Data , 2007 . \" ...", "label": "", "metadata": {}, "score": "55.199528"}
{"text": "Determine the type of training examples .Before doing anything else , the engineer should decide what kind of data is to be used as an example .For instance , this might be a single handwritten character , an entire handwritten word , or an entire line of handwriting .", "label": "", "metadata": {}, "score": "55.248768"}
{"text": "You do n't have to implement support for all those cases .You just need to tell it which ones you do n't want to handle .For example , if your algorithm can do classification , but not regression , then you would add this method to your class : . of if your algorithm can only handle continuous features ( inputs ) that fall in the range from 0 to 1 , then you would add this method to your class : .", "label": "", "metadata": {}, "score": "55.46649"}
{"text": "In the same time , these problems are usually high dimensional , such as , several thousands of features .Thus , the combination of high dimensionality and missing values make the relationship in conditional probabilities between two domains hard to measure and model .", "label": "", "metadata": {}, "score": "55.5198"}
{"text": "Any kind of computer system or other apparatus adapted for carrying out the methods described herein is suited .A typical combination of hardware and software may be a general - purpose computer system with a computer program that , when being loaded and executed , controls the computer system such that it carries out the methods described herein .", "label": "", "metadata": {}, "score": "55.520905"}
{"text": "At block 1160 , the computer uses the editorially - labeled examples of the active set to retrain the classifiers , thereby improving the accuracy of at least some of the classifiers .As mentioned , this method may be used for each of a plurality of score buckets q i for a group of sampled examples , each bucket including a constraint as explained with reference to FIG .", "label": "", "metadata": {}, "score": "55.56974"}
{"text": "Or in total , we would need 3,250 editorial work hours or 406 work days ( or about 81 work days ) for two editors to obtain two editorial judgments .[0097 ] Instead , with use of the present system 100 and the IPMCAL method , we managed to achieve the same model accuracy by selecting only 15,000 pages to be labeled .", "label": "", "metadata": {}, "score": "55.736565"}
{"text": "Supervised learners , however , are suitable for a larger set of problems because they can generalize for feature vectors that were not available at training time . )If you selected GSupervisedLearner for your base class ( which is the most common case ) , then you need to implement the following pure virtual methods : .", "label": "", "metadata": {}, "score": "55.81335"}
{"text": "[0036 ] The new classifier 500 is better as it separates correctly all known points ( old labeled plus new labeled ) .Note , that the classifier 500 assigns different , new scores than the original classifier to the examples .", "label": "", "metadata": {}, "score": "55.89226"}
{"text": "The maximum such F1 value is then used to characterize the performance of the binary classifier in question .To obtain a single measure that characterizes the performance of the multi - class categorizer , we take the arithmetic average of the maximum F1-values that are obtained for the underlying binary problems to obtain the Macro - F1 score .", "label": "", "metadata": {}, "score": "56.069393"}
{"text": "At least one of the training samples is selected and confirmation / re - labeling of it is requested .In response , a reply classification label is received and is used to retrain the automated classifier .Claim : .What is claimed is : .", "label": "", "metadata": {}, "score": "56.08948"}
{"text": "10 , there will be five ( 5 ) constraints as well .As a review , the a p 1 j variables are indicator constants , which in this case , correspond to a particular bucket , which is known in advance .", "label": "", "metadata": {}, "score": "56.13284"}
{"text": "To see this , consider the fact that the number of constraints in the present formulation is equal to m+1 .Note that every class constraint in problem ( 3 ) has exactly q nonzero a ij indicator constants , e.g. , one for each variable that enters the relaxed informative pool from which we are to select A i ( t ) .", "label": "", "metadata": {}, "score": "56.190964"}
{"text": "In system aspects , the following papers describes our work in building a data mining - based intrusion detection system prototype , DISCEX'01 paper , DISCEX'00 paper , and Journal of Computer Security paper .One of my earlier interests is to adapt boosting techniques to focus on examples with higher misclassification cost and this appears in this ICML'99 paper .", "label": "", "metadata": {}, "score": "56.299507"}
{"text": "Deep Learning via Semi - Supervised Embedding Table 1 .Datasets used in our experiments .The following six datasets are large scale .The Mnist 1h,6h,1k,3k and 60k variants are MNIST with a labeled subset of data , following the experimental setup in ( Collobert et ... .", "label": "", "metadata": {}, "score": "56.348404"}
{"text": "( It should operate on an input vector of the same size as one of the rows in the features matrix passed to trainInner , and it should predict a label vector of the same size as one of the rows in the labels matrix . )", "label": "", "metadata": {}, "score": "56.393246"}
{"text": "10 is an exemplary diagram showing one constraint per score bucket ( q i ) from which to sample in a multiple - class test set sampling .[ 0019 ]FIG .11 is a flow chart of an exemplary method for training classifiers in multiple categories through an active learning system .", "label": "", "metadata": {}, "score": "56.394577"}
{"text": "The unlabeled set U is usually referred to as an active pool .The active pool should be maintained distinct from the test set for proper evaluation .At each active learning iteration , t , let A(t ) , also called the \" active set , \" be the set of data points that are selected from U for labeling .", "label": "", "metadata": {}, "score": "56.57547"}
{"text": "In the above example , 100 suitably - selected pages can result in a relatively accurate adult classifier .In advertising , however , taxonomies are known to include hundreds even thousands of nodes .Behavioral Targeting ( BT ) may categorize users and content into thousands of categories of interest to the advertisers ( and many more custom categories ) , such as automotive , finance , education , apparel , entertainment , etc .", "label": "", "metadata": {}, "score": "56.729156"}
{"text": "[ 13 ] .Leo Breiman distinguished two statistical modelling paradigms : data model and algorithmic model , [ 14 ] wherein ' algorithmic model ' means more or less the machine learning algorithms like Random forest .Some statisticians have adopted methods from machine learning , leading to a combined field that they call statistical learning .", "label": "", "metadata": {}, "score": "56.804375"}
{"text": "In response , a \" reply classification label \" is received .Such a reply classification label can comprise a mere designation ( explicit or implicit ) that the previously assigned classification label is correct and/or a differentclassification label which is believed to be more appropriate than the previously assigned classification label .", "label": "", "metadata": {}, "score": "56.816536"}
{"text": "In addition to the foregoing considerations , a measure of the effect of requesting that particular sample be labeled 180 ( either for the first time or for confirmation ) preferably is taken into account .In other embodiments , a cumulative score reflecting all three considerations ( consistency 177 , prediction confidence 178 and variety 179 ) is assigned to each sample , and the samples having the highest cumulative scores are evaluated for expectedeffect , e.g. , in a similar manner .", "label": "", "metadata": {}, "score": "56.836998"}
{"text": "Accordingly , if the user believes such label to be correct , he or she can implicitly designate it as such , e.g.,by not selecting another label .Thus , in step 14 of FIG .2 , one or more training sample(s ) are selected from training set 45 and designated as selected training samples 77 .", "label": "", "metadata": {}, "score": "56.84053"}
{"text": "2 , and the same considerations apply here .Thereafter , processing returns to step 134 in order to select 175 the next sample or set of samples and to repeat the foregoing process .In the preferred implementation of the foregoing process , 5 - 20 samples are selected 175 for labeling , sorted by their prediction strength ( e.g. probability of belonging to the positive class according to the current classifier ) , and presented tothe user 57 in a single screen .", "label": "", "metadata": {}, "score": "56.935783"}
{"text": "Once a solution is identified , for the constraint of every class c i , the system 100 identifies those variables in the final solution that appear only in this constraint .Let the number of such variables be s. The system 100 then takes all variables from the A i ( t ) for c i that do not appear in any of the other active sets , while preserving their ordering in A i ( t ) .", "label": "", "metadata": {}, "score": "56.95894"}
{"text": "For each data point x j in U , a vector s j \u03b5 m is first computed with each component s ji indicating the functional margin of x j with the corresponding hyperplane for the i th classification problem .The data point x j is then associated with the global minimum confidence score in s j , that is we get ( x j , ( min i ( s ji ) ) pairs .", "label": "", "metadata": {}, "score": "57.098686"}
{"text": "The following discussion also references the block diagram of FIG .3 , which shows one exampleof a system 40 for implementing the process ( again , with the individual components preferably implemented in software ) .Referring to FIG .2 , initially ( in step 10 ) an initial training set 45 ( shown in FIG .", "label": "", "metadata": {}, "score": "57.25195"}
{"text": "We call each of these ranges , q i , \" score buckets . \"To evaluate the classifier , the system 100 may need to sample say 100 examples from each bucket .We now have not one inequality in the integer formulation for this classifier c 1 , but five inequalities , one for each bucket .", "label": "", "metadata": {}, "score": "57.27205"}
{"text": "Unsupervised learning can be a goal in itself ( discovering hidden patterns in data ) or a means towards an end ( feature learning ) .Reinforcement learning : A computer program interacts with a dynamic environment in which it must perform a certain goal ( such as driving a vehicle ) , without a teacher explicitly telling it whether it has come close to its goal .", "label": "", "metadata": {}, "score": "57.351387"}
{"text": "This is a somewhat greedy setting which may end up selecting data points that are informative only for a subset of classes .In this case , if we set c i to a large constant , both formulations ( 3 ) and ( 4 ) converge to the same value , but formulation ( 3 ) does not use the m additional variables \u03be m and converges slightly faster .", "label": "", "metadata": {}, "score": "57.38233"}
{"text": "Finally , if your algorithm has a model that can be trained in an incremental manner , then GIncrementalLearner is the best choice for the base class .Algorithms that inherit from GIncrementalLearner are suitable for use with the widest set of problems , but they must implement the most specific functionality .", "label": "", "metadata": {}, "score": "57.535038"}
{"text": "In the various embodiments , different types of devices are used depending upon the size and complexity of the tasks .Suitable devices includemainframe computers , multiprocessor computers , workstations , personal computers , and even smaller computers such as PDAs , wireless telephones or any other appliance or device , whether stand - alone , hard - wired into a network or wirelessly connected to anetwork .", "label": "", "metadata": {}, "score": "57.56722"}
{"text": "If additional editorially resources are available , the method may proceed back to block 600 to begin another training iteration .If , at block 630 , no examples were labeled , then the system 100 may use the final set of trained classifiers , at block 640 , as labeled examples cease to come back from the editors .", "label": "", "metadata": {}, "score": "57.763107"}
{"text": "Following any of the naive active learning approaches would have required 97,500 pages to be labeled .To eliminate editorial disagreement , we also wanted to have each page labeled by two editors .This effectively doubles the size of effort required to label the above pages , e.g. , we would need about 195,000 labels total .", "label": "", "metadata": {}, "score": "57.835884"}
{"text": "Combining this objective function with the inequality ( 1 ) , we obtain the following integer optimization problem for an active learning iteration : . t .[ 0056 ] where we have indicated with z the vector of all selector variables z j .", "label": "", "metadata": {}, "score": "57.926125"}
{"text": "K. Brinker , On active learning in multi - label classification , In GfKI , pages 206 - 213 , 2005 . A. Esuli and F. Sebastiani , Active learning strategies for multi - label text classification , In ECIR ' 09 : Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval , pages 102 - 113 , 2009 .", "label": "", "metadata": {}, "score": "58.00923"}
{"text": "A classifier is a decision function , examples on one side of it are assumed to be positive ( e.g. , automotive ) and on the other side are assumed to be negative ( e.g. , nonautomotive ) .Note that the one - versus - all classifier is an instance of a multiclass classifier that can be used in the system 100 .", "label": "", "metadata": {}, "score": "58.16526"}
{"text": "FIG .6 illustrates an example of a user interface for presenting a training sample for confirmation / re - labeling according to a second representative embodiment of the present invention .FIG .7 illustrates an example of a user interface for presenting a training sample for confirmation / re - labeling according to a third representative embodiment of the present invention .", "label": "", "metadata": {}, "score": "58.325867"}
{"text": "To behaviorally target users based on their online search activity , classifiers are trained so that such search activity can be accurately classified and relevant , targeted advertisements ( \" ads \" ) served to the users .Note that while behavioral targeting is focused on herein , the disclosed training of multiclass classifiers may be applied in a wide variety of contexts , such as , for instance , to train a news classifier for accurately classifying news articles into categories .", "label": "", "metadata": {}, "score": "58.43424"}
{"text": "We used a random 10%-40%-50 % split of this data to obtain the initial labeled set L(0 ) , the active pool U , and the test set T , respectively .[ 0082 ] User Queries : This data set consists of 1.2 million queries placed by human editors into 1019 categories .", "label": "", "metadata": {}, "score": "58.46744"}
{"text": "[0068 ]Any range or device value given herein may be extended or altered without losing the effect sought , as will be apparent to the skilled person .[ 0069 ] Although the subject matter has been described in language specific to structural features and/or methodological acts , it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above .", "label": "", "metadata": {}, "score": "58.518715"}
{"text": "The area under the precision - recall curve ( APRC ) is then measured to estimate the performance of the binary classifier .We then average the APRC from all the binary categorization problems to obtain a single measure for the performance of the multiclass categorization task .", "label": "", "metadata": {}, "score": "58.594517"}
{"text": "[ 0029 ] Classifier training is performed to accurately label large datasets into a taxonomy , or a hierarchy of categories .Each node in the taxonomy or hierarchy can be assigned multiple examples and each example can possibly be labeled into multiple nodes .", "label": "", "metadata": {}, "score": "58.62835"}
{"text": "[ 0071 ] The steps of the methods described herein may be carried out in any suitable order , or simultaneously where appropriate .Additionally , individual blocks may be deleted from any of the methods without departing from the spirit and scope of the subject matter described herein .", "label": "", "metadata": {}, "score": "58.745758"}
{"text": "0031 ] FIG .2 is an exemplary taxonomy in hierarchical flow chart format depicting a multi - label problem in which examples may be assigned to multiple categories .Note that the \" / \" between subcategories indicates a parent / child relationship in the taxonomy .", "label": "", "metadata": {}, "score": "58.81584"}
{"text": "For example , in a specific embodiment label 102 is the previously assigned label and label 103 is the predicted label , or vice versa .In still further embodiments , two or more alternates are highlighted where there are two or more close possibilities .", "label": "", "metadata": {}, "score": "58.835274"}
{"text": "( The user will actually call a method named \" train \" , which will call \" trainInner \" . )As an example , suppose you are writing an instance - based learning algorithm that simply stores the training data for its model : .", "label": "", "metadata": {}, "score": "58.92576"}
{"text": "The symbol U represents the set of all the unlabeled observations .The term .arg min v l .di - elect cons .L D ( v u , v l ) # # EQU00004 # # . indicates the minimal geodesic distance D is sought between an unlabeled point and all the labeled points and the solution is global as indicated by the symbols .", "label": "", "metadata": {}, "score": "59.173763"}
{"text": "In addition , or instead , such commands or information may be input by an automated ( e.g. , computer - executed ) process .Several different embodiments of the present invention are described above , with each such embodiment described as including certain features .", "label": "", "metadata": {}, "score": "59.33877"}
{"text": "Instill further embodiments , the cumulative score is adjusted to account for the effect measure , and the sample(s ) having the highest adjusted scores are selected .In addition , in certain embodiments of the invention the selections are constrained so that the samples are of the same type ( e.g. , clustered within the same region of the feature space ) , thereby resulting in a group of similar samples to beprovided for labeling 180 .", "label": "", "metadata": {}, "score": "59.564186"}
{"text": "[ 0100 ] The difference from the active learning case is that , in the active learning case , there is usually only one constraint , while in testing scenario , there may be multiple constraints per classifier .So in FIG .", "label": "", "metadata": {}, "score": "59.680927"}
{"text": "This provides the system 100 an initial training set ( labeled examples ) that can be used to train an initial classifier .The classifiers trained by the system 100 may be one - versus - all multiclass classifiers such that the system 100 trains a classifier to discriminate automotive ( one ) versus all other categories ( the rest ) , and this is done for all categories in the taxonomy .", "label": "", "metadata": {}, "score": "59.687927"}
{"text": "1 , search engines such as Yahoo ! of Sunnyvale , Calif. maintain large data centers 102 full of indexed data from which to draw to provide as search results to user queries .Additional data centers 102 have been created to index a multitude of other information related to user search behavior , such as queries for instance .", "label": "", "metadata": {}, "score": "59.705574"}
{"text": "n the new space are small if they are in the same cluster or on the same manifold .Other notable methods include generalizations of nearest - neighbor or Parzen window type approaches to learning manifolds given labeled data ( ... . by", "label": "", "metadata": {}, "score": "59.780228"}
{"text": "Determining a suitable classifier for a given problem is however still more an art than a science .where Y is the codomain of g , and L maps into the nonnegative real numbers ( further restrictions may be placed on L ) .", "label": "", "metadata": {}, "score": "59.789307"}
{"text": "For example , suppose we try to build a classifier that differentiates all adult pages from the rest of the pages on the web and that we have editorial resources that can label only 100 pages for the machine learning classifier to use as a training set .", "label": "", "metadata": {}, "score": "59.837097"}
{"text": "Our theoretical analysis shows that we can select many more features than domains while avoiding overfitting by utilizing data - dependent variance properties .We present a greedy feature selection algorithm based on using T - statistics .Our experiments validate this theory showing that our T - statistic based greedy feature selection is more robust at avoiding overfitting than the classical greedy procedure .", "label": "", "metadata": {}, "score": "59.926315"}
{"text": "8(d ) , for the User Queries dataset , 200 examples were labeled per iteration for 917 classes .[ 0091 ] We decided to see whether this behavior -- that LocalMinConf and IPMCAL outperform GlobalMinConf consistently -- is preserved if we were to start with a relatively small training set size .", "label": "", "metadata": {}, "score": "59.955765"}
{"text": "[ 0022 ] Labeled observations 104 are stored at a database or are accessible from another source .The labeled observations are typically of the same type as the unlabeled observations except that ground truth data are available .The ground truth data may be obtained from human judges or automated processes .", "label": "", "metadata": {}, "score": "60.027275"}
{"text": "The manner of deciding which examples to select for labeling will be discussed in more detail below .Note also that the editors may return multiple labels to the selected examples .They may say , for instance , that the top - most circled example in FIG .", "label": "", "metadata": {}, "score": "60.029613"}
{"text": "The content of the web is ever - growing , so classifiers must be continually updated with newly - labeled examples .[ 0005 ] Labeling data is an expensive task , especially when the categorization problem is multiclass in nature and the available editorial resources have to be used efficiently .", "label": "", "metadata": {}, "score": "60.521427"}
{"text": "4 , depicting a line representing a newly - trained classifier after the editors labeled the most - informative examples .[ 0014 ]FIG .6 is an exemplary flow diagram of a method for sampling examples with an integer programming - based solver , to reduce the number of examples that need to be editorially labeled .", "label": "", "metadata": {}, "score": "60.564377"}
{"text": "Thus , a vector of valuescorresponding to such features represents a single sample ( in this case , one of the samples 7 ) .However , for the purpose of the present description , both the samples and their representations are referred to as the \" samples \" .", "label": "", "metadata": {}, "score": "60.564983"}
{"text": "A - inverted .v u . di - elect cons .U # # EQU00003 # # .[ 0051 ] Here the symbol c represents a function that indicates the class index ( label ) associated with a point ( known in advance only for labeled training points ) .", "label": "", "metadata": {}, "score": "60.60845"}
{"text": "The split functions and parameter combinations are applied 412 to the data elements .This acts to cluster the data elements .The split function and parameter values which provided an optimal \" split \" of the training data are selected 414 and stored at the split node .", "label": "", "metadata": {}, "score": "60.61291"}
{"text": "Consider the set of all points to be one large partition .While any partition P contains two points with conflicting labels : Partition P into smaller partitions .For each partition P : Assign the same label to all of the points in P. .", "label": "", "metadata": {}, "score": "60.817017"}
{"text": "For example , if a taxonomy contains the nodes News and Sports Cars , learning a classifier for the concept - rich node News , an obviously harder task , will require a lot more data than the node Sports Cars .", "label": "", "metadata": {}, "score": "60.878963"}
{"text": "In 2006 , the online movie company Netflix held the first \" Netflix Prize \" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10 % .", "label": "", "metadata": {}, "score": "60.910904"}
{"text": "The objective pertaining to prediction confidence 178 biases toward selecting samples for which the classifier 3 is most uncertain regarding its predicted classification label 4 ( such as boundary samples ) .For example , in the binaryclassification example discussed above , the bias is toward samples whose soft classification label is close to 0.5 .", "label": "", "metadata": {}, "score": "60.97342"}
{"text": "11 is a flow chart of an exemplary method for training classifiers in multiple categories through an active learning system , the method executable by the computer 110 having a processor .To execute the method , the computer 110 , at block 1110 , trains an initial set of m binary one - versus - all classifiers , one for each category in a taxonomy , on a labeled dataset of examples stored in a database coupled with the computer .", "label": "", "metadata": {}, "score": "61.034355"}
{"text": "In this method , a datum is represented as a linear combination of basis functions , and the coefficients are assumed to be sparse .Let x be a d -dimensional datum , D be a d by n matrix , where each column of D represents a basis function .", "label": "", "metadata": {}, "score": "61.142258"}
{"text": "This is in contrast to binary classification problems where one deals with two classes , e.g. , to detect spam ( class-1 ) versus non - spam ( class-2 ) email .Multiclass problems may be single label or multilabel .In single label problems , an example can belong to only one category .", "label": "", "metadata": {}, "score": "61.20005"}
{"text": "For example , two points may be given a distance of zero if they belong to the same cluster or infinity if not .The local distances may be defined as symmetric Mahalanobis distances . sub . ) is the covariance associated with the leaf reached by the point v i .", "label": "", "metadata": {}, "score": "61.423737"}
{"text": "For example , the unlabeled observations 106 may be digital images in the case of an image processing application .Digital images include two and higher dimensional images such as medical volumes , depth images , videos and other types of images .", "label": "", "metadata": {}, "score": "61.432625"}
{"text": "[ 0049 ] With further reference to FIG .7 , FIG .7 depicts a hypothetical ordering of the examples in U according to their informativeness with respect to all m binary classifiers c j into which the multiclass problem has been decomposed .", "label": "", "metadata": {}, "score": "61.46353"}
{"text": "If your model occupies a lot of memory , this memory should be freed .This method should not discard any parameters that the user has set .The \" serialize \" method should marshal your model into a DOM tree .", "label": "", "metadata": {}, "score": "61.51647"}
{"text": "[0010 ]FIG .2 is an exemplary taxonomy in hierarchical flow chart format depicting a multilabel problem in which examples may be assigned to multiple categories .[ 0011 ] FIG .3 is an exemplary graph depicting a line representing a trained classifier , and showing scores of labeled examples .", "label": "", "metadata": {}, "score": "61.561775"}
{"text": "Spam filtering is an example of classification , where the inputs are email ( or other ) messages and the classes are \" spam \" and \" not spam \" .In regression , also a supervised problem , the outputs are continuous rather than discrete .", "label": "", "metadata": {}, "score": "61.639008"}
{"text": "In the example in FIG .3 the trees are binary in that each internal node has two outgoing edges .However , this is not essential ; trees may be used where the internal nodes have one or more outgoing edge .", "label": "", "metadata": {}, "score": "61.758034"}
{"text": "Performing just twenty iterations -- as performed in the experiments disclosed herein -- would require editorial resources for labeling twenty thousand examples , which would be infeasible in most real - world applications .BRIEF DESCRIPTION OF THE DRAWINGS .[0008 ] The system and method may be better understood with reference to the following drawings and description .", "label": "", "metadata": {}, "score": "61.768845"}
{"text": "In FIG .4 , the \" x \" has been marked to represent other unlabeled examples from the population .Suppose the editors tell the system 100 that they can label 4 examples .Using active learning , the system 100 can determine that the four most - informative examples are the ones circled with black circles .", "label": "", "metadata": {}, "score": "61.77748"}
{"text": "Semi - Supervised Support Vector Machines ( S3VMs ) are an appealing method for using unlabeled data in classification : their objective function favors decision boundaries which do not cut clusters .However their main problem is that the optimization problem is non - convex and has many local minima , which often results in suboptimal performances .", "label": "", "metadata": {}, "score": "61.807552"}
{"text": "FIG .1 is a block diagram illustrating a system 1 according to a representative embodiment of the present invention .In production use , unlabeled samples 2 are input into an automated classifier 3 that then outputs a corresponding classprediction 4 for each such sample .", "label": "", "metadata": {}, "score": "61.90125"}
{"text": "In this case , GlobalMinConf did perform comparatively and even slightly better than LocalMinConf and IPMCAL .The performance of all methods , however , was rather poor with all of them having APRC lower than 0.4 compared to 0.5 APRC for the training set size in Table 1 .", "label": "", "metadata": {}, "score": "61.9314"}
{"text": "Accordingly , the embodiments are not to be restricted except in light of the attached claims and their equivalents .", "label": "", "metadata": {}, "score": "61.94584"}
{"text": "[ 0037 ]Unlabeled training data is received 400 and a number of trees for the decision forest is selected 402 .The number of trees may be preconfigured by an operator for example .The choice of the number of trees to use depends on the application domain .", "label": "", "metadata": {}, "score": "62.1227"}
{"text": "In some embodiments , the user interface can also receive input indicating which sample / s are inherently difficult to classify , or even an indication that some should be removed from considerationentirely .Upon completion of step 18 , processing returns to step 14 to select additional samples from training set 45 and repeat the foregoing process .", "label": "", "metadata": {}, "score": "62.19481"}
{"text": "0067 ] Those skilled in the art will realize that storage devices utilized to store program instructions can be distributed across a network .For example , a remote computer may store an example of the process described as software .A local or terminal computer may access the remote computer and download a part or all of the software to run the program .", "label": "", "metadata": {}, "score": "62.407658"}
{"text": "This decision 416 may be made based on an amount of information gain at step 414 .If that gain is less than a threshold then the node may be set as a leaf node 418 .If the depth of the tree is at a threshold then the node may also be set as a leaf node .", "label": "", "metadata": {}, "score": "62.55775"}
{"text": "This is a daunting task , requiring thousands of editorial work hours .[ 0025 ] The present disclosure implements an integer programming approach for a multiclass active learning system that reduces the required editorial resources by an order of magnitude , achieving the same classification accuracy as the naive active learning approach mentioned above .", "label": "", "metadata": {}, "score": "62.73518"}
{"text": "Accordingly , in advertising , a search engine may deal with multiple categories and that have multiple labels as well .The search engine usually also labels its advertisements , especially those served on behalf of sponsors or affiliates , through human editorial labeling , whether by editors of the search engine or by the sponsors or affiliates .", "label": "", "metadata": {}, "score": "62.74028"}
{"text": "[0044 ] Multiclass Active Learning .[0045 ] As mentioned , most popular approaches to active learning are not scalable for the task of large - scale multiclass active learning .On the other hand , methods that are specifically designed for such problems follow a greedy approach in which the active learning method attempts to improve the worst - performing binary classification task first .", "label": "", "metadata": {}, "score": "63.0233"}
{"text": "Propagated signals may be present in a tangible storage media , but propagated signals per se are not examples of tangible storage media .The software can be suitable for execution on a parallel processor or a serial processor such that the method steps may be carried out in any suitable order , or simultaneously .", "label": "", "metadata": {}, "score": "63.03534"}
{"text": "[0044 ] The supervised term I j s may be calculated as : . di - elect cons .The entropy H of a set of labeled training points may be calculated as .[ 0046 ] Where c is a ground truth class label of a training point .", "label": "", "metadata": {}, "score": "63.064766"}
{"text": "0048 ] A working assumption that clusters of training data at the split nodes may be described using multi - variate Gaussian probability distributions may be made .In that case , the unsupervised information gain I j u may be calculated as : . di - elect cons .", "label": "", "metadata": {}, "score": "63.081673"}
{"text": "However , in the present embodiments it is applied to samples in both the training set 45 ( disregarding the assigned labels ) and the prediction set 171 .That is , in certain embodiments of the invention prediction - confidence objective 178 is limited to selecting samples only from the prediction set 171 ( i.e. , unlabeled samples 2 ) .", "label": "", "metadata": {}, "score": "63.229595"}
{"text": "Therefore , the system 100 periodically samples new examples and uses them to improve the multiclass classifiers .Which examples should be sampled and labeled can be determined using active learning ( AL ) .[ 0035 ]FIG .4 is the graph of FIG .", "label": "", "metadata": {}, "score": "63.25883"}
{"text": "System Environment .Generally speaking , except where clearly indicated otherwise , all of the systems , methods and techniques described herein can be practiced with the use of one or more programmable general - purpose computing devices .However , in somecases the process steps initially are stored in RAM or ROM .", "label": "", "metadata": {}, "score": "63.28719"}
{"text": "Vision ( ICCV ) , Rio De Janeiro , 2007 . \" ...Linear Discriminant Analysis ( LDA ) has been a popular method for extracting features which preserve class separability .The projection vectors are commonly obtained by maximizing the between class covariance and simultaneously minimizing the within class covariance .", "label": "", "metadata": {}, "score": "63.42905"}
{"text": "Mathematically , sparse dictionary learning means solving where r is sparse .Generally speaking , n is assumed to be larger than d to allow the freedom for a sparse representation .Learning a dictionary along with sparse representations is strongly NP - hard and also difficult to solve approximately .", "label": "", "metadata": {}, "score": "63.59878"}
{"text": "[0085 ] We used two evaluation metrics that capture the improvement of the overall multi - class problem , as follows .[ 0086 ] Average Area Under the Precision Recall Curve : Using the standard definitions of precision and recall for each binary categorization problem , we obtain the corresponding precision - versus - recall curve .", "label": "", "metadata": {}, "score": "63.625763"}
{"text": "Note that for the presented settings , LocalMinConf and IPMCAL outperform GlobalMinConf consistently .TABLE -US-00002 TABLE 2 Performance across different datasets .APRC Macro - F1 Ex .At each iteration , we are allowed to label fewer examples than the total number of classes .", "label": "", "metadata": {}, "score": "63.8348"}
{"text": "unt of attention .Two well known algorithms are Transductive SVM ( TSVM ) [ 23 ] and Co - Training .All these algorithms considered the problem of classification , either transductive or inductive .In this paper , ... . by Jason Weston , Fr\u00e9d\u00e9ric Ratle - International Conference on Machine Learning , 2008 . \" ...", "label": "", "metadata": {}, "score": "63.841232"}
{"text": "Compared to other algorithms minimizing the same objective function , our continuation method often leads to lower test errors . thod to train S 3 VMs .But how good is the S 3 VM itself when compared to other semisupervised approaches ?", "label": "", "metadata": {}, "score": "63.88286"}
{"text": "0073 ] It will be understood that the above description is given by way of example only and that various modifications may be made by those skilled in the art .The above specification , examples and data provide a complete description of the structure and use of exemplary embodiments .", "label": "", "metadata": {}, "score": "63.97117"}
{"text": "The dataset tester 122 may be employed once the classifiers have been trained to test the classifiers on an unlabeled dataset in the indexed examples database 104 or on some other database of unlabeled examples .The computer 110 may include one or more computing devices to provide sufficient processing power to perform the active learning and testing methods as disclosed herein .", "label": "", "metadata": {}, "score": "64.03077"}
{"text": "The first step is to make a class that inherits from one of : GTransducer , GSupervisedLearner , or GIncrementalLearner .GTransducer is the most general base class .If your new algorithm makes predictions as a batch without producing an internal model , then GTransducer is the appropriate base class to use .", "label": "", "metadata": {}, "score": "64.04424"}
{"text": "For example , in a binary classification system an e - mailmessage that has been labeled as reporting a problem with a user 's PC is assigned a negative label with respect to a category of reporting a problem with a server .", "label": "", "metadata": {}, "score": "64.10973"}
{"text": "Depending uponthe particular embodiments , the grouping or ordering is done spatially and/or temporally .It is noted that although users 44 and 57 generally are described above as being human beings , either or both can comprise automated processes , either in whole or in part .", "label": "", "metadata": {}, "score": "64.44505"}
{"text": "In representative embodiments , the samples are presented in an ordered manner , e.g. , sorted by a determined degree of likelihood that the samples are amember of a particular class .Finally , in step 138 the training set 45 is modified based on the labels received in step 137 , the classifier 3 is retrained based on the modified training set 45 , and at least some of the samples 2 and 7 are reprocessed using classifier 3 .", "label": "", "metadata": {}, "score": "64.58312"}
{"text": "The system 100 , therefore , chooses values for q that are at most an order of magnitude or two larger than k. This strategy ensures that the system does not relax the pool size too much , so the data points in the pool are still relatively informative and at the same time also appear in the relaxed pools for multiple classes .", "label": "", "metadata": {}, "score": "64.77302"}
{"text": "As mentioned above the transducer is arranged to carry out label propagation .For example , this is achieved by implementing the following minimization .c ( v u ) c ( arg min v l .di - elect cons .", "label": "", "metadata": {}, "score": "64.78697"}
{"text": "v u \u03b5U. A generic geodesic distance D is defined as .di - elect cons .That is , a geodesic distance D between a labeled and an unlabeled point is lowest sum of local distances along a geodesic path between the labeled and the unlabeled point .", "label": "", "metadata": {}, "score": "64.82486"}
{"text": "Max flow min cut partitioning schemes are very popular for this purpose .Agglomerative transduction can be thought of as bottom - up transduction .It is a semi - supervised extension of agglomerative clustering .It is typically performed as follows : .", "label": "", "metadata": {}, "score": "65.23253"}
{"text": "[ 0030 ] FIG .2 is an example of data input to a semi - supervised random decision forest and how that data may be labeled using a transducer and an inducer .Input data points in a two - dimensional features space are illustrated at graph 200 .", "label": "", "metadata": {}, "score": "65.308075"}
{"text": "The rule may be expressed in words as the probability of label value c given unlabeled test input point v. The probability associated with the whole forest ( that the class label will be a specified value given a particular input point ) is obtained by aggregating the tree posterior probabilities such as by taking their average .", "label": "", "metadata": {}, "score": "65.48512"}
{"text": "The method for testing the existing multiclass classifiers from here on includes the integer optimization program the same as in the MCAL case , except that it is executed for each q i .Further in contrast with the MCAL case , the test set sampling is usually only performed once ( for each bucket ) and not iteratively as with the MCAL methods .", "label": "", "metadata": {}, "score": "65.52443"}
{"text": "[26 ] Shortly after the prize was awarded , Netflix realized that viewers ' ratings were not the best indicators of their viewing patterns ( \" everything is a recommendation \" ) and they changed their recommendation engine accordingly .[ 27 ] .", "label": "", "metadata": {}, "score": "65.562485"}
{"text": "0054 ]The labels produced by the transducer are different for each tree .When considering the whole forest this yields uncertainty in the newly obtained labels .This gives a probabilistic transductive output .For example , each datum is a member of only a single cluster in a given tree .", "label": "", "metadata": {}, "score": "65.59703"}
{"text": "FIELD OF THE INVENTION .The present invention pertains to machine learning and is particularly applicable to systems , methods and techniques for retraining a machine - learning classifier using re - labeled training samples .BACKGROUND .A great deal of attention has been given to automated machine - learning techniques .", "label": "", "metadata": {}, "score": "65.723885"}
{"text": "2 ) .One example is the effect that changing the assigned classification label 8 ( e.g. , tothe label predicted by module 52 ) would have on classifier 3 .This consideration is discussed in more detail below .Similarly , in certain embodiments comparison / selection module 53 also takes into account other inputs 54 ( e.g. , relating to theunlabeled samples 2 and their currently predicted labels 4 ) , which also are discussed in more detail below .", "label": "", "metadata": {}, "score": "65.76474"}
{"text": "The user 44 then clicks on one of theradio buttons , thereby designating the appropriate label for the sample 7 .More accurately , the training set 45 typically includes some representation of the samples 7 , rather than the actual samples themselves , together with the samples ' assigned labels .", "label": "", "metadata": {}, "score": "66.056786"}
{"text": "It is helpful that editors are asked multilabel questions per example x i .For instance , after labeling , the system 100 determines the full vector y i , which is a more expensive operation than the assignment of binary labels during a binary active learning procedure .", "label": "", "metadata": {}, "score": "66.18121"}
{"text": "Such cleaning reduced the number of classes and the number of data points in each set .Table 1 shows the final numbers .TABLE -US-00001 TABLE 1 Data sets used in the evaluation .Dataset Train set Active Pool Test set Classes Reuters 21578 2282 5385 2973 55 Reuters RCV1- 7070 16079 781265 92 V2 DMOZ 168812 675975 844971 367 User Queries 188655 437188 625101 917 .", "label": "", "metadata": {}, "score": "66.198685"}
{"text": "The browser code is operable to cause the browser to detect a selection of the graphical indicator , and display the interface along with the information displayed on the web page in response to the selection of the graphical indicator .The advertisement and the additional information conveyed via the interface are submitted by an advertiser during an advertisement submission time .", "label": "", "metadata": {}, "score": "66.34783"}
{"text": "[0038 ]FIG .6 is an exemplary flow diagram of a method for sampling examples with the integer programming - based solver 120 , to reduce the number of examples that need to be editorially labeled during active learning .", "label": "", "metadata": {}, "score": "66.774536"}
{"text": "A boosting based approach to reduce per - example misclassification cost has been discussed in this ICML'99 paper .Applications on cost - sensitive credit card fraud detection has been discussed in IEEE Intelligent System paper .We have two published papers using ensemble for anomaly detection .", "label": "", "metadata": {}, "score": "66.929085"}
{"text": "1 is a block diagram of an exemplary system 100 for training classifiers in multiple categories through active learning .The system 100 may include one or more data centers 102 including massive storage of indexed examples , which may be stored in an indexed examples database 104 .", "label": "", "metadata": {}, "score": "67.07846"}
{"text": "The observations are unlabeled in the sense that there is no ground truth data available for the observations .For example , in the case of images of landscapes it is not known what classes of landscape the individual images belong to ( such as cityscape , seascape , country scene ) .", "label": "", "metadata": {}, "score": "67.18853"}
{"text": "157 - 168 .Springer - Verlag , 2005 . cited by other .International Conference in Machine Learning , Morgan Kauffman , San Francisco , CA , 2001 . cited by other .Provided are systems , methods and techniques for machine learning .", "label": "", "metadata": {}, "score": "67.19321"}
{"text": "0062 ] All variables x i and \u03be i are binary and problem ( 4 ) remains a standard binary optimization problem .In our evaluation , we treat all classes equally and set all c i equal to one million .", "label": "", "metadata": {}, "score": "67.34262"}
{"text": "The \" trainSparse \" method performs batch training , using a sparse matrix instead of a dense matrix .( Typically , this is done by first calling \" beginIncrementalLearning \" , and then for each row in the sparse matrix , convert the row to a dense vector , and call trainIncremental . )", "label": "", "metadata": {}, "score": "67.38034"}
{"text": "This helps Yahoo !appropriately target users with ads .Based on this search activity , it is reasonable to conclude that the user is interested in \" jewelry \" or in \" parenting and children , \" and therefore should probably be qualified for behavioral targeting within these respective categories .", "label": "", "metadata": {}, "score": "67.39726"}
{"text": "Similarly , let x 3 and x 2 be the two most - informative data points for binary classifier c 2 .Data point x 2 is not the most informative one for classifier c 1 so if we label it , we will have potentially less improvement along c 1 than if we had added x 1 to the active set .", "label": "", "metadata": {}, "score": "67.446915"}
{"text": "Here , we consider the case in which the number of classes is larger than the number of total examples allowed for labeling per iteration .We now show that a simple modification of problem ( 3 ) can cope with the insufficient labeling resources problem . max z i \u03be A ^ z - C \u03be s .", "label": "", "metadata": {}, "score": "67.49992"}
{"text": "The paper by Xiaoxiao Shi , Wei Fan , and Jiangtao Ren can be found here , and the software and dataset ( synthetic and landmine ) written and prepared by Xiaoxiao Shi can be found here , and the 20 Newsgroup data can be found here .", "label": "", "metadata": {}, "score": "67.89191"}
{"text": "Interactive image segmentation typically involves a user making one or more brush strokes on a foreground and/or background region of an image .The image elements identified by the brush strokes are labeled training data and the remaining image elements are unlabeled training data .", "label": "", "metadata": {}, "score": "67.89409"}
{"text": "Thus , to the maximum extent allowed by law , the scope of the present embodiments are to be determined by the broadest permissible interpretation of the following claims and their equivalents , and shall not be restricted or limited by the foregoing detailed description .", "label": "", "metadata": {}, "score": "67.93637"}
{"text": "[ 0072 ] The term ' comprising ' is used herein to mean including the method blocks or elements identified , but that such blocks or elements do not comprise an exclusive list and a method or apparatus may contain additional blocks or elements .", "label": "", "metadata": {}, "score": "68.00862"}
{"text": "z A ^ z s .t .[ 0057 ] where we have set A to be the transposed vector of all A j , and A is the matrix of all indicator variables a ij .[0058 ]Though an exact solution to the integer optimization problem of problem ( 3 ) is NP - hard to resolve , there are different approximation techniques that can solve it both effectively and efficiently .", "label": "", "metadata": {}, "score": "68.01117"}
{"text": "The considerations pertaining to prediction confidence 178 and variety 179 pertain to selection of samples from the prediction set 171 , and information pertaining to such considerations generally is included within the information designated asother inputs 54 in system 40 ( shown in FIG .", "label": "", "metadata": {}, "score": "68.120575"}
{"text": "For example , in one embodiment an automated process ( which is moreprocessor - intensive than the production classifier ) or a lower - level worker functions as user 44 , while an actual human being or a higher - level employee , respectively , functions as user 57 .", "label": "", "metadata": {}, "score": "68.3457"}
{"text": "The components in the drawings are not necessarily to scale , emphasis instead being placed upon illustrating the principles of the present disclosure .In the drawings , like referenced numerals designate corresponding parts throughout the different views .[0009 ] FIG .", "label": "", "metadata": {}, "score": "68.41215"}
{"text": "[ 0104 ] FIG .12 is a flow chart of a second exemplary method , executable for each active learning iteration of the method of FIG .11 , the method executable by the computer 110 having a processor .To execute the method , the computer 110 , at block 1210 , selects a relaxed pool , q , of potentially most - informative examples from which to select examples to label for each of the m binary classifiers .", "label": "", "metadata": {}, "score": "68.46832"}
{"text": "At block 1130 , the computer orders the sampled unlabeled pages in order of informativeness for each classifier .At block 1140 , the computer determines a minimum subset of the unlabeled examples that are most informative for a maximum number of the classifiers to form an active set for learning .", "label": "", "metadata": {}, "score": "68.55427"}
{"text": "Thus , within the IPMCAL framework , at each iteration , the system 100 tries to identify data points like x 2 that contribute to the improvement of a maximum number of binary classifiers .The system 100 still chooses k data points per class but we can now optimally select them to make sure that as many of them as possible appear in multiple active sets A i ( t ) .", "label": "", "metadata": {}, "score": "68.648575"}
{"text": "7 illustrates a further example in which an incoming e - mail message 100 is being classified in the same manner as described above in connection with FIG .6 , with labels 101 - 105 corresponding to labels 91 - 95 , respectively .", "label": "", "metadata": {}, "score": "69.11177"}
{"text": "While we made use of a stopwords list , we did not use any stemming .Unigram features were then obtained from the processed text and for all data sets , we removed those features that appeared in less than five ( 5 ) documents .", "label": "", "metadata": {}, "score": "69.1333"}
{"text": "[0081 ] DMOZ : The DMOZ data comes from the Open Directory Project ( dmoz.org ) in which human editors maintain a taxonomy for web pages .The data set consists of URLs annotated with short , human - written snippets describing their content and placed in one of 367 categories .", "label": "", "metadata": {}, "score": "69.587585"}
{"text": "The selected samples 7 are labeled via interface module 43 in order to generate the training set 45 , which includes the set of samples and their assigned labels .More preferably , module 43 provides a user interface which allows a user 44 todesignate an appropriate label for each presented training sample 7 .", "label": "", "metadata": {}, "score": "69.620224"}
{"text": "In step 16 , the samples 77 are submitted to the user 57 ( which can be the same as user 44 , or a different person orautomated process ) for confirmation / re - labeling .For this purpose , a user interface module 55 ( shown in FIG .", "label": "", "metadata": {}, "score": "69.94026"}
{"text": "7 that x 1 and x i are informative only for class c 1 and the integer programming solver 120 selects one of them in the final solution .The solution will have the same value for the objective function regardless of which one is included .", "label": "", "metadata": {}, "score": "70.33533"}
{"text": "For instance , the training / testing computer 110 may be coupled with the data centers 102 through a network 114 including any number of networked switching devices .The active learner 118 may be coupled with the integer programming sampler 120 to solve an integer optimization problem on an unlabeled dataset from which the computer 110 wants to efficiently sample examples for labeling .", "label": "", "metadata": {}, "score": "70.36049"}
{"text": "A Mahalanobis distance is a scale - invariant distance measure based on correlations between variables which gauges similarity of an unknown sample set to a known one .It is not essential to use Mahalanobis local distances .Other distance measures may be used .", "label": "", "metadata": {}, "score": "70.73491"}
{"text": "0065 ] Finding the Size of the Relaxed Pool .[ 0066 ]An important question that still remains to be discussed is how to select q , the size of the relaxed informative pool .Given a validation set , one can use it to determine the appropriate value of q. Validation sets , though , are not always readily available and may require additional labeling resources .", "label": "", "metadata": {}, "score": "71.12224"}
{"text": "The paper by Erheng Zhong , Sihong Xie , Wei Fan , Jiangtao Ren , Jing Peng and Kun Zhang can be found in here , the PowerPoint presentation can be found here and the code and dataset is available from here .", "label": "", "metadata": {}, "score": "71.139404"}
{"text": "We used six editors to finish the project within only a couple of weeks .[ 0098 ] Testing Existing Multiclass Classifiers .[0099 ] Often to deploy classifiers in production , the system 100 needs to demonstrate that it meets certain performance requirements .", "label": "", "metadata": {}, "score": "71.2679"}
{"text": "[ 0019 ]FIG .1 is a schematic diagram of a machine learning system comprising a semi - supervised random decision forest 100 .A random decision forest is semi - supervised where it is trained using both labeled observations 104 and unlabeled observations 106 .", "label": "", "metadata": {}, "score": "71.31122"}
{"text": "If the computer 100 considers m\u02dc1000 and q\u02dc100 , the method results with at most a hundred thousand variables even for very large multiclass problems .In reality , because many of the variables appear in multiple constraints , which is the very property the proposed algorithm exploits , the actual number of variables is far less than qm .", "label": "", "metadata": {}, "score": "71.46434"}
{"text": "[ 0032 ] For most classification problems , editors manually label some examples .For instance , the editors may look at the 100 - 200 most popular queries every day and label them with the appropriate categories .Similarly , for pages , the editors may assign different articles or stories to different categories in the Yahoo !", "label": "", "metadata": {}, "score": "71.64639"}
{"text": "[ 0064 ] Once we have a solution of problem ( 3 ) , while it will contain many variables that appear across multiple constraints , it will still have some variables that participate in a single constraint .These are the data points that are most informative for just one of the m binary classifiers on an individual basis .", "label": "", "metadata": {}, "score": "71.83545"}
{"text": "Other similar methods , such as Maximum Marginal Hyperplane , choose data with the largest .Tradeoff methods choose a mix of the smallest and largest s. .Another active learning method , that typically learns a data set with fewer examples than Minimum Marginal Hyperplane but is more computationally intensive and only works for discrete classifiers is Maximum Curiosity [ 2 ] .", "label": "", "metadata": {}, "score": "72.64734"}
{"text": "[ 0051 ] For every example x j in the active pool U , we now introduce a selector variable z j , where : .We further introduce the binary indicator constants a ij , where : . [ 0053 ] Note that a ij are known since the relaxed pool set has already been selected according to some informativeness criterion .", "label": "", "metadata": {}, "score": "72.84508"}
{"text": "With a very large active pool size , the system 100 needs to ensure that the above procedure does not select q on the order of one thousand or more .This will make problem ( 3 ) infeasible even with relaxation techniques for the state - of - the - art integer programming solvers .", "label": "", "metadata": {}, "score": "73.28811"}
{"text": "In one representative embodiment , the presumption is absoluteso that the subject sample 7 is never again resubmitted .Alternatively , the previously assigned classification label 8 for the training sample 7 could have been contradicted by user 57 .The processing at this point preferably depends upon the particular embodiment , more preferably , on the presumptionsthat have been set by the users .", "label": "", "metadata": {}, "score": "73.30453"}
{"text": "[0038 ]A tree from the forest is selected 404 and the process moves 406 to the root node of that tree .Data elements are selected 406 from the training set .For example , where the training set comprises images the data elements may be image elements of an image .", "label": "", "metadata": {}, "score": "73.58687"}
{"text": "However , it should be understood that any other computing or other type of device instead may be used , such as a device utilizing any combination of electronic , optical , biological and chemical processing .Additional Considerations .In certain instances , the foregoing description refers to clicking or double - clicking on user - interface buttons , dragging user - interface items , or otherwise entering commands or information via a particular user - interface mechanism and/or in aparticular manner .", "label": "", "metadata": {}, "score": "73.981636"}
{"text": "Referring back to FIG .2 , in step 12 the classifier 3 is trained , e.g. , using training module 5 ( shown also in FIG .3 ) .Any of a variety of different ( e.g. , conventional ) training techniques can be used by training module 5 .", "label": "", "metadata": {}, "score": "74.31691"}
{"text": "At block 1230 , the computer provides as a second constraint that at least k examples be sampled for each binary classifier .At block 1240 , the computer provides an objective function that selects as the active set the examples that improve classification accuracy for the maximum number of the classifiers .", "label": "", "metadata": {}, "score": "74.345795"}
{"text": "DETAILED DESCRIPTION .[0017 ] The detailed description provided below in connection with the appended drawings is intended as a description of the present examples and is not intended to represent the only forms in which the present example may be constructed or utilized .", "label": "", "metadata": {}, "score": "74.36853"}
{"text": "[ 0041 ] Because of the large size of the sampled set of unlabeled examples , the editors can not be asked to label them all .[ 0042 ] The system 100 first orders the examples according to their informativeness for the individual classifiers .", "label": "", "metadata": {}, "score": "75.362045"}
{"text": "0058 ]With reference to FIG .7 an example of process at the inducer is described .Labels assigned by the transducer are accessed 700 and the inducer counts 702 the examples of each label / class arriving at each leaf .", "label": "", "metadata": {}, "score": "75.56565"}
{"text": "The training / testing computer 110 may include , but not be limited to , an active learner 118 , an integer programming - based solver 120 , a dataset tester 122 , and a trained classifiers database 124 .[ 0027 ] The training / testing computer 110 may be coupled with or integrated within the data centers 102 .", "label": "", "metadata": {}, "score": "76.02643"}
{"text": "Of course , if you do n't plan to use any operations that require serialization , you can implement this method the wimpy way : .The GSupervisedLearner class implements the \" transduceInner \" method for you , so you do not need to implement it .", "label": "", "metadata": {}, "score": "76.095795"}
{"text": "The way in which the selected training samples 77 are presented to the user 57 by labeling interface module 55 preferably is different in different embodiments of the invention .Some examples are illustrated in FIGS .5 - 7 .Specifically , FIG .", "label": "", "metadata": {}, "score": "76.11651"}
{"text": "[ 8 ] This definition is notable for its defining machine learning in fundamentally operational rather than cognitive terms , thus following Alan Turing 's proposal in his paper \" Computing Machinery and Intelligence \" that the question \" Can machines think ? \" be replaced with the question \" Can machines do what we ( as thinking entities ) can do ? \" [ 9 ] .", "label": "", "metadata": {}, "score": "76.29204"}
{"text": "In either event , the task of assigning classification labels generally will be easier for the user 57 .FIG .6 illustrates a non - binary example in which an incoming e - mail message 90 is being labeled according to the category of subject matter ( in this case , particular types of hardware ) to which it most closely pertains .", "label": "", "metadata": {}, "score": "76.31406"}
{"text": "The communication interface 1336 network may enable communications via any number of communication standards , such as 802.11 , 802.17 , 802.20 , WiMax , cellular telephone standards , or other communication standards .[ 0113 ]Accordingly , the method and system may be realized in hardware , software , or a combination of hardware and software .", "label": "", "metadata": {}, "score": "76.592964"}
{"text": "[ 0034 ] The labeled sets are usually not sufficient to build accurate multiclass classifiers .On one hand , what has been labeled by the editors so far is not a uniform sample of all the population : all pages on the web , all queries that people issue , or all news published in Taiwan , etc .", "label": "", "metadata": {}, "score": "76.910126"}
{"text": "Then a new datum is associated with the class such that it 's best sparsely represented by the corresponding dictionary .Sparse dictionary learning has also been applied in image de - noising .The key idea is that a clean image patch can be sparsely represented by an image dictionary , but the noise can not .", "label": "", "metadata": {}, "score": "76.9106"}
{"text": "Nodes are divided into internal ( or split ) nodes and terminal ( or leaf ) nodes .In FIG .3 the internal nodes and the root nodes are represented using ovals and the terminal nodes are represented using rectangles .", "label": "", "metadata": {}, "score": "77.329"}
{"text": "This acknowledges that software can be a valuable , separately tradable commodity .It is intended to encompass software , which runs on or controls \" dumb \" or standard hardware , to carry out the desired functions .It is also intended to encompass software which \" describes \" or defines the configuration of hardware , such as HDL ( hardware description language ) software , as is used for designing silicon chips , or for configuring universal programmable chips , to carry out desired functions .", "label": "", "metadata": {}, "score": "77.45779"}
{"text": "First , the previously assigned classification label 8 for the training sample 77 could have been confirmed by user 57 .In that case , the training sample 77 preferably is returned to training set 45 with the same classification label 8 .", "label": "", "metadata": {}, "score": "77.66661"}
{"text": "The remaining documents were split at 30%-70 % to obtain the initial labeled set L(0 ) and the active pool U , respectively .[ 0080 ] Reuters RCV1-V2 : This is another publicly - available data set consisting of 804,414 news articles from the Reuters news agency classified into 103 categories .", "label": "", "metadata": {}, "score": "77.79786"}
{"text": "0115 ]As shown above , the system serving advertisements and interfaces that convey additional information related to the advertisement .For example , the system generates browser code operable by a browser to cause the browser to display a web page of information that includes an advertisement .", "label": "", "metadata": {}, "score": "78.550964"}
{"text": "FIG .4 is the graph of FIG .3 , showing examples as x that are unlabeled and as a circle x that an algorithm determines would be the most informative for the classifier to have labeled .[ 0013 ] FIG .", "label": "", "metadata": {}, "score": "79.57258"}
{"text": "The input / output controller 816 may also output data to devices other than the display device , e.g. a locally connected printing device .[ 0064 ] The term ' computer ' or ' computing - based device ' is used herein to refer to any device with processing capability such that it can execute instructions .", "label": "", "metadata": {}, "score": "80.85172"}
{"text": "And , at block 1260 , the computer solves the integer optimization problem with an integer programming solver . [0105 ] FIG .13 illustrates a general computer system 1300 , which may represent the data center 102 , the training / testing computer 110 , or any other computing devices referenced herein or that may be executed by the system 100 .", "label": "", "metadata": {}, "score": "80.940575"}
{"text": "Manifold learning algorithms attempt to do so under the constraint that the learned representation is low - dimensional .Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse ( has many zeros ) .Multilinear subspace learning algorithms aim to learn low - dimensional representations directly from tensor representations for multidimensional data , without reshaping them into ( high - dimensional ) vectors .", "label": "", "metadata": {}, "score": "81.17964"}
{"text": "The function or classifier 300 assigns a score to each example .For instance , the scores may be probabilities that indicate how certain the classifier is of accuracy , wherein the farther from the line 300 an example is labeled along a linear scale , the more certain is the classifier .", "label": "", "metadata": {}, "score": "82.63263"}
{"text": "Below is demonstrated that the present method allows such requirements to be incorporated intuitively into the active learning process , thereby ensuring the improvement of the associated classifiers .[ 0047 ] Integer Programming Formulation .[0048 ] For brevity , the proposed Integer Programming approach for Multiclass Active Learning may be referred to herein as IPMCAL .", "label": "", "metadata": {}, "score": "82.92831"}
{"text": "[ 0078 ] We evaluated IPMCAL and the competing approaches on four data sets with diverse number of categories and with variable training and active pool sizes .We now provide some details about each of these data sets .[0079 ] Reuters 21578 : This is a publicly - available data set consisting of 12,902 articles in 118 classes .", "label": "", "metadata": {}, "score": "83.05946"}
{"text": "[ 0040 ] Once the current node is set as a split node the split function and parameters selected at step 414 are applied to the training data elements to give a binary test 422 .The process then recursively executes 424 blocks 410 to 422 for each subset of the training data elements directed to the respective child node .", "label": "", "metadata": {}, "score": "83.229675"}
{"text": "0031 ] FIG .3 is a schematic diagram of a random decision forest ( suitable for use as a semi - supervised random decision forest ) comprising three random decision trees 300 , 302 , 304 .Two or more trees may be used ; three are shown in this example for clarity .", "label": "", "metadata": {}, "score": "83.554504"}
{"text": "The input / output controller 816 is also arranged to receive and process input from one or more devices , such as a user input device 820 ( e.g. a mouse , keyboard , camera , microphone or other sensor ) .", "label": "", "metadata": {}, "score": "88.12578"}
{"text": "[0062 ]The computing - based device 800 also comprises an input / output controller 816 arranged to output display information to a display device 818 which may be separate from or integral to the computing - based device 800 .", "label": "", "metadata": {}, "score": "89.059456"}
{"text": "The machine - readable medium may selectively be , but not limited to , an electronic , magnetic , optical , electromagnetic , infrared , or semiconductor system , apparatus , device , or propagation medium .[ 0111 ]Additionally , the computer system 1300 may include an input device 1325 , such as a keyboard or mouse , configured for a user to interact with any of the components of system 1300 .", "label": "", "metadata": {}, "score": "89.51703"}
{"text": "As defined herein , computer storage media does not include communication media .Therefore , a computer storage medium should not be interpreted to be a propagating signal per se .Propagated signals may be present in a computer storage media , but propagated signals per se are not examples of computer storage media .", "label": "", "metadata": {}, "score": "89.58078"}
{"text": "The computer system 1300 may operate as a stand - alone device or may be connected , e.g. , using the network 114 , to other computer systems or peripheral devices . [ 0106 ] In a networked deployment , the computer system 1300 may operate in the capacity of a server or as a client - user computer in a server - client user network environment , or as a peer computer system in a peer - to - peer ( or distributed ) network environment .", "label": "", "metadata": {}, "score": "89.60774"}
{"text": "[ 0040 ] At block 610 , the computer(s ) 110 may uniformly sample a very large number of unlabeled examples creating a uniformly - sampled set 614 of unlabeled examples .This unlabeled set may include , e.g. , 10 million pages from Yahoo ! 's index 104 containing over three billion pages .", "label": "", "metadata": {}, "score": "89.85345"}
{"text": "Such media include , by way of example , magneticdisks , magnetic tape , optically readable media such as CD ROMs and DVD ROMs , or semiconductor memory such as PCMCIA cards , various types of memory cards , USB memory devices , etc .", "label": "", "metadata": {}, "score": "89.92488"}
{"text": "In the present example , the label 82 indicating a valid e - mailmessage has been pre - selected ( e.g. , by interface module 55 ) , and the user 57 only needs to click on the radio button for spam label 81 if the user 57 disagrees with this pre - selection .", "label": "", "metadata": {}, "score": "90.87214"}
{"text": "5 .This means that the initial classifier does not separate the data well in that it makes errors and using the newly - labeled examples helps train a new , better classifier , which is depicted as the dotted line 500 in FIG .", "label": "", "metadata": {}, "score": "91.19377"}
{"text": "Computer storage media , such as memory 812 , includes volatile and non - volatile , removable and non - removable media implemented in any method or technology for storage of information such as computer readable instructions , data structures , program modules or other data .", "label": "", "metadata": {}, "score": "93.22737"}
{"text": "The instructions 1302 may reside completely , or at least partially , within the memory 1304 and/or within the processor 1308 during execution by the computer system 1300 .Accordingly , the databases 104 and 124 described above in FIG .1 may be stored in the memory 1304 and/or the disk unit 1315 .", "label": "", "metadata": {}, "score": "93.95691"}
{"text": "Once again , a label ( in this case , label 93 ) has been pre - selected , based either on the previously assigned classification label or the predicted classification label fore - mail message 90 .Accordingly , the user 57 only needs to click on the appropriate radio button 91 , 92 , 94 or 95 if he or she disagrees with the pre - selection .", "label": "", "metadata": {}, "score": "95.76112"}
{"text": "[ 0107 ] The computer system 1300 may include a memory 1304 on a bus 1320 for communicating information .Code operable to cause the computer system to perform any of the acts or operations described herein may be stored in the memory 1304 .", "label": "", "metadata": {}, "score": "104.43065"}
{"text": "[0109 ] The computer system 1300 may also include a disk or optical drive unit 1315 .The disk drive unit 1315 may include a computer - readable medium 1340 in which one or more sets of instructions 1302 , e.g. , software , can be embedded .", "label": "", "metadata": {}, "score": "104.95845"}
{"text": "The display 1330 may act as an interface for the user to see the functioning of the processor 1308 , or specifically as an interface with the software stored in the memory 1304 or the drive unit 1315 .[ 0112 ] The computer system 1300 may include a communication interface 1336 that enables communications via the communications network 114 .", "label": "", "metadata": {}, "score": "108.37645"}
{"text": "[ 0108 ] The computer system 1300 may include a processor 1308 , such as a central processing unit ( CPU ) and/or a graphics processing unit ( GPU ) .The processor 1308 may include one or more general processors , digital signal processors , application specific integrated circuits , field programmable gate arrays , digital circuits , optical circuits , analog circuits , combinations thereof , or other now known or later - developed devices for analyzing and processing data .", "label": "", "metadata": {}, "score": "115.29088"}
