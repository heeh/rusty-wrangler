{"text": "This paper presents a new Markovian sequence model , closely related to HMMs , that allows observations to be represented as arbitrary overlapping features ( such as word , capitalization , formatting , part - of - speech ) , and defines the conditional probability of state sequences given observation sequences .", "label": "", "metadata": {}, "score": "25.571945"}
{"text": "Furthermore , we do not have simple frequency cut - offs .Parsimonious language m .. \" ...Language modeling is the art of determining the probability of a sequence of words .This is useful in a large variety of areas including speech recognition , optical character recognition , handwriting recognition , machine translation , and spelling correction ( Church , 1988 ; Brown et al . , 1990 ; Hull , 1 ... \" .", "label": "", "metadata": {}, "score": "29.438622"}
{"text": "Furthermore , we do not have simple frequency cut - offs .Parsimonious language m .. \" ...Language modeling is the art of determining the probability of a sequence of words .This is useful in a large variety of areas including speech recognition , optical character recognition , handwriting recognition , machine translation , and spelling correction ( Church , 1988 ; Brown et al . , 1990 ; Hull , 1 ... \" .", "label": "", "metadata": {}, "score": "29.438622"}
{"text": "We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .In addition , we introduce two novel smoothing techniques , one a variation of Jelinek - Mercer smoothing and one a very simple linear interpolation technique , both of which outperform existing methods . \" ...", "label": "", "metadata": {}, "score": "32.402702"}
{"text": "The following is a 1961 Times article about the device .Introduction of HMMs to speech .IBM was first to introduce hidden Markov models ( HMMs ) to the world of speech recognition .Although Rabiner 's tutorial 's on HMMs are more widely cited the IBM papers were first .", "label": "", "metadata": {}, "score": "33.31411"}
{"text": "This is useful in a large variety of areas including speech recognition , optical character recognition , handwriting recognition , machine translation , and spelling correction ( Church , 1988 ; Brown et al . , 1990 ; Hull , 1 ... \" .", "label": "", "metadata": {}, "score": "33.519165"}
{"text": "The baseline language model in t ..Tools . \" ...We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .", "label": "", "metadata": {}, "score": "33.916473"}
{"text": "Error analysis suggested the strategy of combining , which led to the implementation of methods that merged the outputs of syllable - based recognition systems with the phone - oriented baseline system at the frame level , the syllable level and the whole - utterance level .", "label": "", "metadata": {}, "score": "34.04032"}
{"text": "We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .In addition , we introduce two novel smoothing techniques , one a variation of Jelinek - Mercer smoothing and one a very simple linear interpolation technique , both of which outperform existing methods . ... he differences in performance seem to be less when cutoffs are used .", "label": "", "metadata": {}, "score": "34.098526"}
{"text": "We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .In addition , we introduce two novel smoothing techniques , one a variation of Jelinek - Mercer smoothing and one a very simple linear interpolation technique , both of which outperform existing methods . ... he differences in performance seem to be less when cutoffs are used .", "label": "", "metadata": {}, "score": "34.098526"}
{"text": "\"Principles of Lexical Language Modeling for Speech Recognition .\" Advances in Speech Signal Processing , S. Furui and J. Sondhi , Eds .M. Dekker Publishers , New York , NY 1991 .Pp.651 - 700 . F. Jelinek and J.D. Lafferty . \"", "label": "", "metadata": {}, "score": "34.317287"}
{"text": "\"A Maximum Likelihood Approach to Continuous Speech Recognition . \"IEEE Journal of Pattern Analysis and Machine Intelligence ( 1983 ) ; and Readings in Speech Recognition , A. Waibel , K.F. Lee , Eds .Morgan Kaufmann Publishers , San Mateo , CA 1990 .", "label": "", "metadata": {}, "score": "34.476715"}
{"text": "This paper describes a number of log - linear parsing models for an automatically extracted lexicalized grammar .The models are \" full \" parsing models in the sense that probabilities are defined for complete parses , rather than for independent events derived by decomposing the parse tree .", "label": "", "metadata": {}, "score": "34.894714"}
{"text": "This paper describes a number of log - linear parsing models for an automatically extracted lexicalized grammar .The models are \" full \" parsing models in the sense that probabilities are defined for complete parses , rather than for independent events derived by decomposing the parse tree .", "label": "", "metadata": {}, "score": "34.894714"}
{"text": "[ 2 ] Maximum likelihood approach to continuous speech recognition .LR Bahl , F Jelinek , RL Mercer , IEEE Transactions on Pattern Analysis and Machine Intelligence 5:22 , 179 - 190 , 1983 .[ 3 ] F. Jelinek , \" Continuous Speech Recognition by Statistical Methods \" , IEEE Proceedings ( Invited Paper ) , April 1976 , Vol . 64 , No . 4 , pp .", "label": "", "metadata": {}, "score": "35.687126"}
{"text": "This formulation requires statistical models of the speech production process .In this paper , we describe a number of statistical models for use in speech recognition .We give special attention to determining the parameters for such models from sparse data .", "label": "", "metadata": {}, "score": "35.97332"}
{"text": "The most commonly used language models are very simple ( e.g. a Katz - smoothed trigram model ) .There are many improvements over this simple model however , including caching , clustering , higherorder n - grams , skipping models , and sentence - mixture models , all of which we will describe below .", "label": "", "metadata": {}, "score": "36.979053"}
{"text": "The most commonly used language models are very simple ( e.g. a Katz - smoothed trigram model ) .There are many improvements over this simple model however , including caching , clustering , higherorder n - grams , skipping models , and sentence - mixture models , all of which we will describe below .", "label": "", "metadata": {}, "score": "36.979053"}
{"text": "The most commonly used language models are very simple ( e.g. a Katz - smoothed trigram model ) .There are many improvements over this simple model however , including caching , clustering , higherorder n - grams , skipping models , and sentence - mixture models , all of which we will describe below .", "label": "", "metadata": {}, "score": "36.979053"}
{"text": "Among sequence labeling tasks in language processing , shallow parsing has received much attention , with the development of standard evaluati ... \" .Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position .", "label": "", "metadata": {}, "score": "37.03394"}
{"text": "[5 ] Jelinek , F. , Mercer , R.L. , 1980 .Interpolated estimation of Markov source parameters from sparse data .In : Gelsema , E.S. , Kanal , L.N. ( Eds . ) , Pattern Recognition in Practice .", "label": "", "metadata": {}, "score": "37.108932"}
{"text": "The work described in this thesis explored the utility of information collected over syllable - related time - scales .The first approach involved integrating syllable segmentation information into the speech recognition process .The addition of acoustically - based syllable onset estimates [ 184 ] resulted in a 10 % relative reduction in word - error rate .", "label": "", "metadata": {}, "score": "37.134323"}
{"text": "We present positive experimental results on the segmentation of FAQ 's . \" ...This paper proposes the use of maximum entropy techniques for text classification .Maximum entropy is a probability distribution estimation technique widely used for a variety of natural language tasks , such as language modeling , part - of - speech tagging , and text segmentation .", "label": "", "metadata": {}, "score": "37.198006"}
{"text": "In this work we use standard full - forward decoding for the PHONEBOOK experiments and N - best decoding for the broad class experiments .The N - b ... .The popular press has reported slightly lower results for commercial systems .", "label": "", "metadata": {}, "score": "37.984215"}
{"text": "Finally , Dr. Jelinek is the author of two books , Probabilistic Information Theory and the recently published Statistical Methods for Speech Recognition .Selected Publications . F. Jelinek and A. Emami . \"A Neural Syntactic Language Model \" , Machine Learning , Vol .", "label": "", "metadata": {}, "score": "38.19425"}
{"text": "Some of the large margin classifier work was inspired by Povey 's work .Below are some of the older papers as well as selected papers from Povey 's IBM work .[ 12 ] Nadas , A. : A decision - theoretic formulation of a training problem in speech recognition and a comparison of training by unconditional versus conditional maximum likelihood .", "label": "", "metadata": {}, "score": "38.3736"}
{"text": "In noisy conditions , the mismatch between corrupted speech signals and models trained on clean speech may cause the decoder to produce word matches with unrealistic durations .This paper presents a simple way to incorporate word duration constraints by unrolling HMMs to form a lattice where word duration probabilities can be applied directly to state transitions .", "label": "", "metadata": {}, "score": "38.429947"}
{"text": "The Hidden Markov Model ( HMMs ) is one of the most successful modeling approaches for acoustic events in speech recognition , and more recently it has proven useful for several problems in biological sequence analysis .Although the HMM is good at capturing the temporal nature of processes such as spee ... \" .", "label": "", "metadata": {}, "score": "38.441742"}
{"text": "Probabilistic CKY parsing .Lexical Semantics .Word sense disambiguation , collocations and lexical acquisition from large text corpora .Word relations ( similarity , hyponymy , etc . ) , distributional semantics , distributional models of meaning .Computational Semantics .", "label": "", "metadata": {}, "score": "38.542603"}
{"text": "This paper proposes the use of maximum entropy techniques for text classification .Maximum entropy is a probability distribution estimation technique widely used for a variety of natural language tasks , such as language modeling , part - of - speech tagging , and text segmentation .", "label": "", "metadata": {}, "score": "38.719383"}
{"text": "The toolkit supports creation and evaluation of a variety of language model types based on N - gram statistics , as well as several related tasks , such as statistical tagging and manipulation of N - best lists and word lattices .", "label": "", "metadata": {}, "score": "39.066475"}
{"text": "The toolkit supports creation and evaluation of a variety of language model types based on N - gram statistics , as well as several related tasks , such as statistical tagging and manipulation of N - best lists and word lattices .", "label": "", "metadata": {}, "score": "39.066475"}
{"text": "Language typology and divergences .Rule - based models : direct translation , transfer systems , and interlingua systems .Statistical models : word - based , phrase - based and syntax - based .Synchronous context free grammars , synchronous tree substitution .", "label": "", "metadata": {}, "score": "39.32943"}
{"text": "Although the HMM is good at capturing the temporal nature of processes such as speech , it has a very limited capacity for recognizing complex patterns involving more than first order dependencies in the observed data sequences .This is due to the first order state process and the assumption of state conditional independence between observations .", "label": "", "metadata": {}, "score": "39.66488"}
{"text": "Spatial relation recognition .Course material .Handbooks Daniel Jurafsky and James H. Martin , Speech and Language Processing , Prentice - Hall , 2006 ( 2nd edition ) .Christopher D. Manning and Hinrich Sch\u00fctze , Foundations of Statistical Natural Language Processing , MIT Press , 1999 .", "label": "", "metadata": {}, "score": "39.73329"}
{"text": "( Amsterdam : Elsevier Science B.V. , 1996 ) , 242 - 246 . F. Jelinek . \"Direct Parsing of Text , \" Image Models ( and their Speech Model Cousins ) , edited by S. E. Levinson and L. Shepp .", "label": "", "metadata": {}, "score": "40.044388"}
{"text": "Two New Approaches to Language Modeling , \" Speech Recognition & Coding , edited by A.J. Rubio Ayuso and J.M Lopez Soler ( Berlin : Springer - Verlag , 1995 ) , 226 - 239 . F. Jelinek . \"Training and Search Methods for Speech Recognition , \" National Academy of Sciences Colloquium on Human - Machine Communication By Voice , edited by David B. Roe and J.G. Wilpon ( National Academy of Sciences Press , 1994 ) .", "label": "", "metadata": {}, "score": "40.202606"}
{"text": "This new chapter introduces two sequence models : HMMs and MEMMs .It gives the details of Hidden Markov Models , including Forward , Viterbi , and EM .It then introduces MaxEnt models , begining with linear regression , followed by logistic regression , then the extension to MaxEnt , and finally the MEMM and the Viterbi intuition .", "label": "", "metadata": {}, "score": "40.394936"}
{"text": "Advanced Syntactic and Semantic Parsing .Lexicalized PCFGs , word classes , parsing with limited supervision , latent variable grammars .Induction of grammars , inside - outside algorithm .Temporal and Spatial Recognition .Temporal expression recognition , temporal normalization , TimeBank .", "label": "", "metadata": {}, "score": "40.52024"}
{"text": "A maximum likelihood approach to continuous speech recognition .IEEE Trans . on Pattern Analysis and Machine Intelligence , PAMI-5(2 ) , 179 - -190 . S. Della Pietra , V. Della Pietra , J. Gillett , J. Lafferty , H. Printz , and L. Ures ( 1994 ) .", "label": "", "metadata": {}, "score": "40.71384"}
{"text": "An introduction to probability theory and its applications , volume 1 .John Wiley & Sons .F. Jelinek and R. Mercer ( 1980 ) .Interpolated estimation of markov source parameters from sparse data .In Proceedings , Workshop on Pattern Recognition in Practice .", "label": "", "metadata": {}, "score": "41.063377"}
{"text": "Probabilistic language models , n - grams , generative models , unsupervised and semi - supervised models , smoothed estimation .Expectation Maximization ( EM ) techniques .Hidden Markov Models ( HMMs ) , Viterbi algorithm , forward - backward algorithm .", "label": "", "metadata": {}, "score": "41.34613"}
{"text": "P. Brown , S. Della Pietra , V. Della Pietra , and R. Mercer ( 1991 ) .The mathematics of statistical machine translation : parameter estimation .Computational Linguistics , 19(2 ) , 263 - -311 .B. Merialdo .Tagging text with a probabilistic model ( 1990 ) .", "label": "", "metadata": {}, "score": "41.779556"}
{"text": "In Proeedings of the Second International Symposium on Grammatical Inference . A.Dempster , N. Laird , and D. Rubin ( 1977 ) .Maximum likelihood from incomplete data via the em algorithm .Journal of the Royal Statistical Society , 39(B ) , 1 - -38 .", "label": "", "metadata": {}, "score": "41.884407"}
{"text": "A new evaluation section covering human evaluation and Bleu has also been added , as well as sections on SYSTRAN and more details on cross - linguistic divergences .( top ) \" ...Incorporating the concept of the syllable into speech recognition may improve recognition accuracy through the integration of information over syllable - length time spans .", "label": "", "metadata": {}, "score": "41.900192"}
{"text": "However , in many domains labels are highly interdependent .This paper explores multilabel conditional random field ( CRF ) classification models that directly parameterize label co - occurrences in multi - label classification .Experiments show that the models outperform their singlelabel counterparts on standard text corpora .", "label": "", "metadata": {}, "score": "42.08632"}
{"text": "Background Reading .L. Bahl , P. Brown , P. de Souza , and R. Mercer ( 1989 ) .A tree - based statistical language model for natural language speech recognition .IEEE Transactions on Acoustics , Speech , and Signal Processing , 37(7 ) .", "label": "", "metadata": {}, "score": "42.173355"}
{"text": "F. Jelinek . \"Stochastic Analysis of Structured Language Modeling , \" Mathematical Foundations of Speech and Language Processing , edited by M. Johnson , S. Khudanpur , M. Ostendorf and R. Rosenfeld .( New York : Springer - Verlag , 2004 ) , 37 - 72 . F. Jelinek and C. Chelba . \"", "label": "", "metadata": {}, "score": "42.411674"}
{"text": "We show here how to train a conditional random field to achieve performance as good as any reported base noun - phrase chunking method on the CoNLL task , and better than any reported single model .Improved training methods based on modern optimization algorithms were critical in achieving these results .", "label": "", "metadata": {}, "score": "42.957527"}
{"text": "\" The Development of an Experimental Discrete Dictation Recognizer . \"Proceedings IEEE , 73:11 ( 1985 ) pp1616- 1624 , Nov. 1985 ; and Readings in Speech Recognition , A. Waibel , K.F. Lee , Eds . , Morgan Kaufmann Publishers , San Mateo , CA ( 1990 ) : 587 - 595 .", "label": "", "metadata": {}, "score": "43.19024"}
{"text": "Constraint - based reasoning , integer linear programming , column generation algorithms .Alignment Algorithms for Machine Translation .Sentence alignment , word alignment , phrase alignment , tree alignment .Alignment algorithms : Expectation Maximization algorithm , IBM models , HMM alignment , discriminative models .", "label": "", "metadata": {}, "score": "43.2539"}
{"text": "ICASSP , 2005 . \" ...Typical systems for large vocabulary conversational speech recognition ( LVCSR ) have been trained on a few hundred hours of carefully transcribed acoustic training data .This paper describes an LVCSR system for the conversational telephone speech ( CTS ) task trained on more than 2000 hours of data for ... \" .", "label": "", "metadata": {}, "score": "43.294476"}
{"text": "ICASSP , 2005 . \" ...Typical systems for large vocabulary conversational speech recognition ( LVCSR ) have been trained on a few hundred hours of carefully transcribed acoustic training data .This paper describes an LVCSR system for the conversational telephone speech ( CTS ) task trained on more than 2000 hours of data for ... \" .", "label": "", "metadata": {}, "score": "43.294476"}
{"text": "Cross - entropy and speech recognition In this section , we briefly examine how the performance of a language model meas ... . by Andreas Stolcke - IN PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING ( ICSLP 2002 , 2002 . \" ...", "label": "", "metadata": {}, "score": "43.379005"}
{"text": "Cross - entropy and speech recognition In this section , we briefly examine how the performance of a language model meas ... . by Andreas Stolcke - IN PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING ( ICSLP 2002 , 2002 . \" ...", "label": "", "metadata": {}, "score": "43.379005"}
{"text": "All this work will be performed on the basis of a database containing prompted ( read ) speech and ( simulated ) natural requests to information service .This report describes the initial steps that were required to set up a reasonable baseline system and a good research and evaluation framework .", "label": "", "metadata": {}, "score": "44.48369"}
{"text": "[ 1 ] L.R. Bahl , J. Cocke , F. Jelinek and J. Raviv , \" Optimal decoding of linear codes for minimizing symbol error rate , \" IEEE Trans .Inform Theory , vol IT-20 , pp .248 - 287 , March 1974 .", "label": "", "metadata": {}, "score": "44.513702"}
{"text": "This new significantly - expanded speech recognition chapter gives a complete introduction to HMM - based speech recognition , including extraction of MFCC features , Gaussian Mixture Model acoustic models , and embedded training .( top ) .Chapter 10 : Speech Recognition : Advanced Topics ( New Chapter ) .", "label": "", "metadata": {}, "score": "44.52391"}
{"text": "It includes much more information on modern dialogue systems , including VoiceXML , confirmation and clarification dialogues , the information - state model , markov decision processes , and other current approaches to dialogue agents .( top ) .Chapter 25 : Machine Translation .", "label": "", "metadata": {}, "score": "44.57303"}
{"text": "Firstly , explicit duration modelling is exploited to combat the corruption of acoustic data which often causes the decoder to produce word matches with unrealistic durations .Secondly , it is argued that the top - down information in recognition models may be insufficient to mediate the speech . by David Gelbart , John Bryant , Andreas Stolcke , Robert Porzel , Manja Baudis , Nelson Morgan . \" ...", "label": "", "metadata": {}, "score": "44.835686"}
{"text": "Tools . \" ...We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .", "label": "", "metadata": {}, "score": "44.982246"}
{"text": "Discriminative training of acoustic and language models .The discriminative training craze for acoustic modeling started with Maximum Mutual Information ( MMI ) in Peter Brown 's thesis .It was then continued at IBM and followed up with a paper on the extended Baum Welch algorithm that gives a recipe to optimize rational functions satisfying certain constraints .", "label": "", "metadata": {}, "score": "45.104893"}
{"text": "In noisy conditions , the mismatch between corrupted speech signals and models trained on clean speech may cause the decoder to produce word matches with unrealistic durations .This paper presents a simple way to incorporate word duration constraints ... \" .", "label": "", "metadata": {}, "score": "45.26844"}
{"text": "This paper describes an LVCSR system for the conversational telephone speech ( CTS ) task trained on more than 2000 hours of data for which only approximate transcriptions were available .The challenges of dealing which such a large data set and the accuracy improvements over the small baseline system are discussed .", "label": "", "metadata": {}, "score": "45.402794"}
{"text": "This paper describes an LVCSR system for the conversational telephone speech ( CTS ) task trained on more than 2000 hours of data for which only approximate transcriptions were available .The challenges of dealing which such a large data set and the accuracy improvements over the small baseline system are discussed .", "label": "", "metadata": {}, "score": "45.402794"}
{"text": "From 1972 to 1993 he headed the large Continuous Speech Recognition group of the IBM T.J. Watson Research Center .There he pioneered with his colleagues the statistical methods that are the basis of current state - of - the art speech recognizers .", "label": "", "metadata": {}, "score": "45.44178"}
{"text": "In this report , we discuss the initial issues addressed in a research project aiming at the development of an advanced natural speech recognition system for the automatic processing of telephone directory requests .This multi - faceted project involves ( 1 ) text processing ( labeling and tagging ) of a l ... \" .", "label": "", "metadata": {}, "score": "45.962883"}
{"text": "Because they do not exploit dependencies between labels , such techniques are only well - suited to problems in which categories are independen ... \" .Common approaches to multi - label classification learn independent classifiers for each category , and employ ranking or thresholding schemes for classification .", "label": "", "metadata": {}, "score": "46.11927"}
{"text": "A key component of the parsing system , for both training and testing , is a Maximum Entropy supertagger which assigns CCG lexical categories to words in a sentence .The supertagger makes the discriminative training feasible , and also leads to a highly efficient parser .", "label": "", "metadata": {}, "score": "46.466316"}
{"text": "To illustrate the usefulness of the methods described , we review a number of decoding results that have been obtained with them .CITATION .Lalit R. Bahl , Frederick Jelinek , Robert L. Mercer , \" A Maximum Likelihood Approach to Continuous Speech Recognition \" , IEEE Transactions on Pattern Analysis & Machine Intelligence , vol.5 , no . 2 , pp .", "label": "", "metadata": {}, "score": "46.72535"}
{"text": "The system incorporates two new kinds of acoustic model : triphone models conditioned on speaking rate , and an explicit joint model of within - word phone durations .We also obtained an unusually large improvement from modeling crossword pronunciation variants in \" multiword \" vocabulary items .", "label": "", "metadata": {}, "score": "46.885246"}
{"text": "The system incorporates two new kinds of acoustic model : triphone models conditioned on speaking rate , and an explicit joint model of within - word phone durations .We also obtained an unusually large improvement from modeling crossword pronunciation variants in \" multiword \" vocabulary items .", "label": "", "metadata": {}, "score": "46.885246"}
{"text": "..the interpolation weights are estimated by using the EM algorithm .4.4 Language model pruning Our system can produce an SLM given a memory constraint .The basic idea is to remove as many useless probabilities as possible without increasing the perplexity .", "label": "", "metadata": {}, "score": "46.913986"}
{"text": "..the interpolation weights are estimated by using the EM algorithm .4.4 Language model pruning Our system can produce an SLM given a memory constraint .The basic idea is to remove as many useless probabilities as possible without increasing the perplexity .", "label": "", "metadata": {}, "score": "46.913986"}
{"text": "[ 4 ] Bahl , L.R. , Brown , P.F. , deSouza , P.V. , Mercer , R.L. , Nahamoo , D. , 1991 .A fast algorithm for deleted interpolation .In : Proc .Europ .Conf .Speech Comm .", "label": "", "metadata": {}, "score": "46.953995"}
{"text": "The scheme can represent any standard n - gram model and is easily combined with existing model reduction techniques such as entropy - pruning .We demonstrate the space - savings of the scheme via machine translation experiments within a distributed language modeling framework .", "label": "", "metadata": {}, "score": "46.959457"}
{"text": "The scheme can represent any standard n - gram model and is easily combined with existing model reduction techniques such as entropy - pruning .We demonstrate the space - savings of the scheme via machine translation experiments within a distributed language modeling framework .", "label": "", "metadata": {}, "score": "46.959457"}
{"text": "There are more details about the formal descriptions of finite - state transducers , many bugs are fixed , and two new sections are added relating to words and subwords .The first new section is on word and sentence tokenization , including algorithms for English as well as the maxmatch algorithm for Chinese word segmentation .", "label": "", "metadata": {}, "score": "47.023323"}
{"text": "Inducing Features of Random Fields .Technical Report CMU - CS95 - 144 , School of Computer Science , Carnegie - Mellon University .Statistical Machine Translation .The work on maximum entropy and parsing led to the first purely statistically based translation system .", "label": "", "metadata": {}, "score": "47.26237"}
{"text": "Hidden Markov models ( HMMs ) are a powerful probabilistic tool for modeling sequential data , and have been applied with success to many text - related tasks , such as part - of - speech tagging , text segmentation and information extraction .", "label": "", "metadata": {}, "score": "47.61005"}
{"text": "Hidden Markov models ( HMMs ) are a powerful probabilistic tool for modeling sequential data , and have been applied with success to many text - related tasks , such as part - of - speech tagging , text segmentation and information extraction .", "label": "", "metadata": {}, "score": "47.61005"}
{"text": "Combining the two frameworks in a sensible way can therefore lead to a more powerful model with better classification abilities .The overall aim of this work has been to develop a probabilistic hybrid of hidden Markov models and neural networks and ... . by Yoshihiko Gotoh , Steve Renals , Gethin Williams - IN PROCEEDINGS OF ICASSP-99 , VOL .", "label": "", "metadata": {}, "score": "47.8488"}
{"text": "Howe ...Tools . \" ...We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .", "label": "", "metadata": {}, "score": "48.362114"}
{"text": "Utterance - level mean and variance normalization of the features and MLP output calculation is performed by ICSI 's FFWD - NORM tool .Source code for all these tools is available free for research use . 2.2 Language Modeling and Decodi ... . \" ...", "label": "", "metadata": {}, "score": "48.594193"}
{"text": "Named Entity Recognition and Semantic Role Labeling .Discriminative supervised learning models : loglinear models , maximum entropy Markov models , conditional random fields , BIO - tagging and chunking .PropBank , FrameNet .Discourse Analysis .Noun phrase coreference resolution .", "label": "", "metadata": {}, "score": "49.39623"}
{"text": "Chapter 7 : Phonetics ( Formerly parts of Chapters 4 , 5 , and 7 ) .This chapter is an introduction to articulatory and acoustic phonetics for speech processing , as well as foundational tools like the ARPAbet , wavefile formats , phonetic dictionaries , and PRAAT .", "label": "", "metadata": {}, "score": "49.614082"}
{"text": "[ 8 ] P. Brown , J. Cocke , S. Della Pietra , V. Della Pietra , F. Jelinek , R. Mercer , P. Roossin , A statistical approach to language translation , Proceedings of the 12th conference on Computational linguistics , p.71 - 76 , August 22 - 27 , 1988 .", "label": "", "metadata": {}, "score": "49.81295"}
{"text": "Sources of training data suitable for language modeling of conversational speech are limited .In this paper , we show how training data can be supplemented with text from the web filtered to match the style and/or topic of the target recognition task , but also that it is possible to get bigger perfor ... \" .", "label": "", "metadata": {}, "score": "49.83329"}
{"text": "Sources of training data suitable for language modeling of conversational speech are limited .In this paper , we show how training data can be supplemented with text from the web filtered to match the style and/or topic of the target recognition task , but also that it is possible to get bigger perfor ... \" .", "label": "", "metadata": {}, "score": "49.83329"}
{"text": "Finally , I want to express my thanks for the financial support I received from the SmartKom and SmartWeb projects as well as from Infineon .There are five parts to the ISOLET corpus , and thus five different ways to divide the corpus into four training parts and one testing part .", "label": "", "metadata": {}, "score": "49.933456"}
{"text": "We propose a succinct randomized language model which employs a perfect hash function to encode fingerprints of n - grams and their associated probabilities , backoff weights , or other parameters .The scheme can represent any standard n - gram model and is easily combined with existing model reduction te ... \" .", "label": "", "metadata": {}, "score": "49.9614"}
{"text": "We propose a succinct randomized language model which employs a perfect hash function to encode fingerprints of n - grams and their associated probabilities , backoff weights , or other parameters .The scheme can represent any standard n - gram model and is easily combined with existing model reduction te ... \" .", "label": "", "metadata": {}, "score": "49.9614"}
{"text": "As such , ... \" .We systematically investigate a new approach to estimating the parameters of language models for information retrieval , called parsimonious language models .Parsimonious language models explicitly address the relation between levels of language models that are typically used for smoothing .", "label": "", "metadata": {}, "score": "50.096863"}
{"text": "As such , ... \" .We systematically investigate a new approach to estimating the parameters of language models for information retrieval , called parsimonious language models .Parsimonious language models explicitly address the relation between levels of language models that are typically used for smoothing .", "label": "", "metadata": {}, "score": "50.096863"}
{"text": "Finally , we applied a generalized ROVER algorithm to combine the N - best hypotheses from several systems based on different acoustic models .ts had been optimized for perplexity on prior evaluation data .Lattice expansion used an unpruned , trigram backoff LM ( 4.8 M bigrams , 11.5 M trigrams ) constructed in the same fashion .", "label": "", "metadata": {}, "score": "50.335228"}
{"text": "Finally , we applied a generalized ROVER algorithm to combine the N - best hypotheses from several systems based on different acoustic models .ts had been optimized for perplexity on prior evaluation data .Lattice expansion used an unpruned , trigram backoff LM ( 4.8 M bigrams , 11.5 M trigrams ) constructed in the same fashion .", "label": "", "metadata": {}, "score": "50.335228"}
{"text": "14 , No . 4 , ( October 2000 ) , 283 - 332 . F. Jelinek and G. Potamianos . \"A Study of n - gram and Decision Tree Letter Language Models , \" Speech Communication , Vol .24 , No . 3 , ( Elsevier , Holland , June 1998 ) , 171 - 192 . F. Jelinek .", "label": "", "metadata": {}, "score": "50.64943"}
{"text": ".. consisting of a term and a label , as well as a feature for each triplet consisting of a term and two labels .Awards .In August of 1998 , Dr. Jelinek received the IEEE Information Theory Society Golden Jubilee Paper Award for a paper titled \" Optimal Decoding of Linear Codes for Minimizing Symbol Error Rate , \" ( See reference below ) .", "label": "", "metadata": {}, "score": "51.214737"}
{"text": "We describe SRI 's large vocabulary conversational speech recognition system as used in the March 2000 NIST Hub-5E evaluation .The system performs four recognition passes : ( 1 ) bigram recognition with phone - loop - adapted , within - word triphone acoustic models , ( 2 ) lattice generation with transcription - m ... \" .", "label": "", "metadata": {}, "score": "51.299587"}
{"text": "We describe SRI 's large vocabulary conversational speech recognition system as used in the March 2000 NIST Hub-5E evaluation .The system performs four recognition passes : ( 1 ) bigram recognition with phone - loop - adapted , within - word triphone acoustic models , ( 2 ) lattice generation with transcription - m ... \" .", "label": "", "metadata": {}, "score": "51.299587"}
{"text": "We apply parsimonious models at three stages of the retrieval process:1 ) at indexing time ; 2 ) at search time ; 3 ) at feedback time .Experimental results show that we are able to build models that are significantly smaller than standard models , but that still perform at least as well as the standard approaches . .", "label": "", "metadata": {}, "score": "51.40629"}
{"text": "We apply parsimonious models at three stages of the retrieval process:1 ) at indexing time ; 2 ) at search time ; 3 ) at feedback time .Experimental results show that we are able to build models that are significantly smaller than standard models , but that still perform at least as well as the standard approaches . .", "label": "", "metadata": {}, "score": "51.40629"}
{"text": "Nonetheless , the ex ... \" .Incorporating the concept of the syllable into speech recognition may improve recognition accuracy through the integration of information over syllable - length time spans .Evidence from psychoacoustics and phonology suggests that humans use the syllable as a basic perceptual unit .", "label": "", "metadata": {}, "score": "51.44767"}
{"text": "[16 ] Povey , D. , Kanevsky , D. , Kingsbury , B. , Ramabhadran , B. , Saon , G. , Visweswariah , K.:Boosted MMI for model and feature - space discriminative training .In : Proceedings of the IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP-08 ) , Las Vegas , NV ( 2008 ) .", "label": "", "metadata": {}, "score": "51.93315"}
{"text": "Chapter 5 : Part - of - Speech Tagging ( Formerly Chapter 8) .The main change to this revised chapter is a greatly expanded , and hence self - contained , description of bigram and trigram HMM part - of - speech tagging , including Viterbi decoding and deleted interpolation smoothing .", "label": "", "metadata": {}, "score": "52.36351"}
{"text": "Chapter 20 : Computational Lexical Semantics ( New Chapter ; Parts of old Chs .15 , 16 and 17 ) .The focus of this new chapter is on computing with word meanings .The three main topics are word sense disambiguation , computing relations between words ( similarity , hyponymy , etc . ) , and semantic role labeling .", "label": "", "metadata": {}, "score": "52.413273"}
{"text": "Tables do this by employing layout patterns to efficiently indicate fields and records in two - dimensional form . ...When the training labels make the state sequence unambiguous ( as they often do in practice ) , the likelihood function in exponential models such as CRFs is convex , so there are no local maxima , and t .. \" ...", "label": "", "metadata": {}, "score": "52.443924"}
{"text": "319 .L.R. Bahl , J. Cocke , F. Jelinek , J. Raviv .\"Optimal Decoding of Linear Codes for Minimizing Symbol Error Rate . \"IEEE Transactions on Information Theory , IT-20 , pp .284 - 287 , March 1974 . F. Jelinek . \"", "label": "", "metadata": {}, "score": "52.8621"}
{"text": "Computational Linguistics 17:3(1991 ) : 315 - 323 .P.F. Brown , J. Cocke , S. Della Pietra , V. Della Pietra , F. Jelinek , J. Lafferty , R.L. Mercer , P. Roossin . \"A Statistical Approach to Machine Translation . \"", "label": "", "metadata": {}, "score": "53.5045"}
{"text": "[ 11 ] A. Berger , P. Brown , S. Della Pietra , V. Della Pietra , J. Gillett , J. Lafferty , H. Printz , L. Ures ( 1994 ) .The Candide system for machine translation .ARPA Workshop on Speech and Natural Language .", "label": "", "metadata": {}, "score": "53.855362"}
{"text": "..The ability to approxima ... . by Djoerd Hiemstra , Stephen Robertson , Hugo Zaragoza - In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , 2004 . \" ...We systematically investigate a new approach to estimating the parameters of language models for information retrieval , called parsimonious language models .", "label": "", "metadata": {}, "score": "54.0104"}
{"text": "..The ability to approxima ... . by Djoerd Hiemstra , Stephen Robertson , Hugo Zaragoza - In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , 2004 . \" ...We systematically investigate a new approach to estimating the parameters of language models for information retrieval , called parsimonious language models .", "label": "", "metadata": {}, "score": "54.0104"}
{"text": "Experiments on connected - digit recognition show that when using explicit duration constraints the decoder generates word matches with more reasonable durations , and word error rates are significantly reduced across a broad range of noise conditions .t paths .Therefore it is not possible to correctly apply duration penalties .", "label": "", "metadata": {}, "score": "54.595207"}
{"text": "rpus .Combining several N - grams can produce a model with a very large number of parameters , which is costly in decoding .In such cases N - grams are typically pruned .The same pruning parameters were applied to all models in our experiments .", "label": "", "metadata": {}, "score": "54.79924"}
{"text": "rpus .Combining several N - grams can produce a model with a very large number of parameters , which is costly in decoding .In such cases N - grams are typically pruned .The same pruning parameters were applied to all models in our experiments .", "label": "", "metadata": {}, "score": "54.79924"}
{"text": "We discuss the underlying linguistic phenomena , i.e. , the linguistic features , which are relevant to the task , and learn how to design suitable rule based and machine learning models .We link to current applications in real world settings .", "label": "", "metadata": {}, "score": "55.237995"}
{"text": "Overall increasing the training data size from 360h to 2200h and optimising the training procedure reduced the word error rate on the DARPA / NIST 2003 eval set by about 20 % relative . ...n - grams are trained on different text corpora and then interpolated together with the interpolation weights optimised on a development test set .", "label": "", "metadata": {}, "score": "55.297863"}
{"text": "Overall increasing the training data size from 360h to 2200h and optimising the training procedure reduced the word error rate on the DARPA / NIST 2003 eval set by about 20 % relative . ...n - grams are trained on different text corpora and then interpolated together with the interpolation weights optimised on a development test set .", "label": "", "metadata": {}, "score": "55.297863"}
{"text": "A statistical approach to machine translation .Computational Linguistics , 16 , 79 - -85 .P. Brown , S. Della Pietra , V. Della Pietra , and R. Mercer ( 1991 ) .A statistical approach to sense disambiguation in machine translation .", "label": "", "metadata": {}, "score": "55.50351"}
{"text": "We provide an overview of our approach to NE tagged language model ( LM ) generation together with results of the application of such a LM to the task of out - of - vocabulary ( OOV ) word reduction in large vocabulary speech recognition .", "label": "", "metadata": {}, "score": "55.636425"}
{"text": "Part - of - Speech tagsets , often used linguistic categories .Part - of - Speech tagging : rule based and probabilistic models .Phrase chunking : rule based and probabilistic models .Rule - based and Statistical Parsing and Chunking .", "label": "", "metadata": {}, "score": "55.791588"}
{"text": "Thus , given training sentences S1 , . . ., Sm , gold - standard dependency structures , \u03c01 , . . ., \u03c0m , and the definition of the probability of a dependency structure ( 13 ) , the ob ... . \" ...", "label": "", "metadata": {}, "score": "55.918545"}
{"text": "Other changes in this chapter include expanded descriptions of unknown word modeling and part - of - speech tagging in other languages , and many bug fixes .Finally , we 've moved this chapter earlier in the book .( top ) .", "label": "", "metadata": {}, "score": "56.10002"}
{"text": "Documents often contain tables in order to communicate densely packed , multi - dimensional information .Tables do this by employing layout pa ... \" .The ability to find tables and extract information from them is a necessary component of data mining , question answering , and other information retrieval tasks .", "label": "", "metadata": {}, "score": "56.124863"}
{"text": "We compare models which use all CCG derivations , including nonstandard derivations , with normal - form models .The performances of the two models are comparable and the results are competitive with existing wide - coverage CCG parsers . ...", "label": "", "metadata": {}, "score": "56.162643"}
{"text": "It has been proposed that this human ability is governed by Auditory Scene Analysis ( ASA ) processes , in which a sound mixture is segregated into perceptual packages , called ' streams ' , by a combination of bottom - up and top - down processing .", "label": "", "metadata": {}, "score": "56.1864"}
{"text": "The primary focus is on supervised machine learning approaches to these topics .The coverage on mostly finite - state methods ( FASTUS ) has been moved from the old Ch .15 to here .( top ) .Chapter 23 : Question Answering and Summarization ( Mostly new ; Parts of old 17 and 20 ) .", "label": "", "metadata": {}, "score": "56.398376"}
{"text": "We introduce Named Entity ( NE ) Language Modelling , a stochastic finite state machine approach to identifying both words and NE categories from a stream of spoken data .We provide an overview of our approach to NE tagged language model ( LM ) generation together with results of the application of such ... \" .", "label": "", "metadata": {}, "score": "56.501663"}
{"text": "Finally , there is a significant new section on discourse segmentation ( including TextTiling ) .( top ) .Chapter 22 : Information Extraction ( New chapter : Parts of old 15 ) .This new chapter surveys current approaches to information extraction .", "label": "", "metadata": {}, "score": "56.73055"}
{"text": "The baseline language model in t .. Aims .The course introduces students into the principles and methods of natural language processing , blending traditional and current approaches .The course additionally aims at an understanding of the underlying computational properties of natural language and of current research directions .", "label": "", "metadata": {}, "score": "57.06884"}
{"text": "[ 13 ] L.R. Bahl , P.F. Brown , P.V. de Souza , R.L. Mercer ( 1986 ) .Maximum Mutual Information Estimation of Hidden Markov Model Parameters for Speech Recognition , Proc .ICASSP 86 , pp .49 - 52 , Tokyo .", "label": "", "metadata": {}, "score": "57.35741"}
{"text": "IEEE Proceedings 64:4(1976 ) : 532 - 556 . F. Jelinek .\"Tree Encoding of Memoryless Time - Discrete Sources with Fidelity Criterion . \"IEEE Transactions on Information Theory IT-15:5(1969 ) pp584 - 590 ; and Key Papers in the Development of Information Theory , D. Slepian , Ed . , IEEE Press , New York , NY 1974 . F. Jelinek . \"", "label": "", "metadata": {}, "score": "57.480637"}
{"text": "This chapter still covers basic notions surrounding meaning representation languages .It now has better coverage of model - theoretic semantics for meaning representations , and a new section on Description Logics and their role as a basis for OWL and its role in the Semantic Web .", "label": "", "metadata": {}, "score": "57.76828"}
{"text": "( top ) .Chapter 21 : Computational Discourse .This rewritten chapter includes a number of updates to the first edition .The anaphora resolution section is updated to include modern log - linear methods , and a section on the more general problem of coreference is also included .", "label": "", "metadata": {}, "score": "58.34755"}
{"text": "Constraints on the distribution , derived from labeled training data , inform the technique where to be minimally non - uniform .The maximum entropy formulation has a unique solution which can be found by the improved iterative scaling algorithm .In this paper , maximum entropy is used for text classification by estimating the conditional distribution of the class variable given the document .", "label": "", "metadata": {}, "score": "58.36702"}
{"text": "Publication Date : April 1976 .Pioneering Innovations in Language Modeling .The first successful language modeling smoothing algorithm , deleted interpolation , was invented at IBM .IBM also had the first application of the Maximum Entropy Principle to language modeling .", "label": "", "metadata": {}, "score": "58.369003"}
{"text": "Listeners are remarkably adept at recognising speech in natural multisource environments , while most Automatic Speech Recognition ( ASR ) technology fails in these conditions .It has been proposed that this human ability is governed by Auditory Scene Analysis ( ASA ) processes , in which a sound mixture ... \" .", "label": "", "metadata": {}, "score": "59.377487"}
{"text": "What is Natural Language Processing ( NLP ) ?Ambiguity and uncertainty in language .Introduction to the different analysis levels of NLP .Applications of NLP .Evaluation in NLP : Precision , recall , F - score , x - fold cross - validation , gold standards , good practices in NLP experiments , BLEU .", "label": "", "metadata": {}, "score": "59.381695"}
{"text": "Applying SLM techniques like trigram language models to Chinese is challenging because ( 1 ) there is no standard definition of words in Chinese ; ( 2 ) word boundaries are not marked by spaces ; and ( 3 ) there is a dearth of training data .", "label": "", "metadata": {}, "score": "59.67496"}
{"text": "Applying SLM techniques like trigram language models to Chinese is challenging because ( 1 ) there is no standard definition of words in Chinese ; ( 2 ) word boundaries are not marked by spaces ; and ( 3 ) there is a dearth of training data .", "label": "", "metadata": {}, "score": "59.67496"}
{"text": "He is also interested in novel methods of automatic parsing , of text understanding , and of machine translation .Dr. Jelinek was an Instructor at MIT ( 1959 - 1962 ) , a Visiting Lecturer at Harvard University ( 1962 ) , and a Professor of Electrical Engineering at Cornell University ( 1962 - 1974 ) .", "label": "", "metadata": {}, "score": "59.685555"}
{"text": "14 ] Gopalakrishnan , P.S. Kanevsky , D. Nadas , A. Nahamoo , D. An inequality for rational functions with applications to some statistical estimation problems , IEEE Trans .Inform .Theo . 37 ( 1 ) , pp .107 - 113 , 1991 .", "label": "", "metadata": {}, "score": "60.41057"}
{"text": "It is entirely possible that two techniques that work well separately will not work well together , and , as we will show , even possible that some techniques will work better together than either one does by itself .In this ... . \" ...", "label": "", "metadata": {}, "score": "60.446785"}
{"text": "It is entirely possible that two techniques that work well separately will not work well together , and , as we will show , even possible that some techniques will work better together than either one does by itself .In this ... . \" ...", "label": "", "metadata": {}, "score": "60.446785"}
{"text": "It is entirely possible that two techniques that work well separately will not work well together , and , as we will show , even possible that some techniques will work better together than either one does by itself .In this ... . \" ...", "label": "", "metadata": {}, "score": "60.446785"}
{"text": "( top ) .Chapter 19 : Lexical Semantics ( Formerly 16 ) .This chapter still covers the basics of lexical semantics , including sense relations , semantic roles , and primitive decomposition .The treatment of semantic roles has been updated , as has the coverage of WordNet , and new sections added for PropBank and FrameNet .", "label": "", "metadata": {}, "score": "61.025463"}
{"text": "In Proceedings of the ARPA Conference on Human Language Technology . A. Berger , S. Della Pietra , and V. Della Pietra ( 1996 ) .A maximum entropy approach to natural language processing .Computational Linguistics , 22(1 ) , 39 - -71 .", "label": "", "metadata": {}, "score": "61.05379"}
{"text": "Applying SLM techniques like trigram language models to Chinese is challenging because ( 1 ) there is no standard definition of words in Chinese ; ( 2 ) word boundaries are not marked by spaces ; and ( 3 ) there is a de ... \" .", "label": "", "metadata": {}, "score": "61.19872"}
{"text": "Applying SLM techniques like trigram language models to Chinese is challenging because ( 1 ) there is no standard definition of words in Chinese ; ( 2 ) word boundaries are not marked by spaces ; and ( 3 ) there is a de ... \" .", "label": "", "metadata": {}, "score": "61.19872"}
{"text": "This chapter describes the English - language SMARTKOM - Mobile system and related research .We explain the work required to support a second language in SMARTKOM and the design of the English speech recognizer .We then discuss research carried out on signal processing methods for robust speech recognition and on language analysis using the Embodied Construction Grammar formalism .", "label": "", "metadata": {}, "score": "61.26786"}
{"text": "This chapter describes the English - language SMARTKOM - Mobile system and related research .We explain the work required to support a second language in SMARTKOM and the design of the English speech recognizer .We then discuss research carried out on signal processing methods for robust speech ... \" .", "label": "", "metadata": {}, "score": "61.26788"}
{"text": "SRILM is freely available for noncommercial purposes .The toolkit supports creation ... \" .SRILM is a collection of C++ libraries , executable programs , and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications .", "label": "", "metadata": {}, "score": "61.843143"}
{"text": "SRILM is freely available for noncommercial purposes .The toolkit supports creation ... \" .SRILM is a collection of C++ libraries , executable programs , and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications .", "label": "", "metadata": {}, "score": "61.843143"}
{"text": "Regular languages , regular grammars , and finite state automata , and their limitations .Context free grammars and their limitations .Weak Generative Capacity and Strong Generative Capacity .Tree substitution grammars .Tree adjoining grammars .Finite State Transducers and their use in morphology .", "label": "", "metadata": {}, "score": "61.899117"}
{"text": "Much future work remains , but the re ... . \" ...Conditional Random Fields ( CRFs ) are undirected graphical models , a special case of which correspond to conditionally - trained finite state machines .A key advantage of CRFs is their great flexibility to include a wide variety of arbitrary , non - independent features of the input .", "label": "", "metadata": {}, "score": "61.9115"}
{"text": "Chapter 14 : Statistical Parsing ( Formerly 12 ) .This statistical parsing chapter has been extensively revised .It now covers PCFGs , probabilistic CKY parsing , parent annotations , the Collins parser , and touches on advanced topics such as discriminative reranking and parsing for language modeling .", "label": "", "metadata": {}, "score": "62.439564"}
{"text": "Dan Povey did this work for his dissertation before coming to IBM .At IBM he came up with new discriminative features ( feature space MPE ( fMPE ) ) as well as an altogether better criteria : boosted MMI ( bMMI ) .", "label": "", "metadata": {}, "score": "62.507492"}
{"text": "Context free grammars : constituency , Chomsky normal form , chart parsing ( Earley algorithm ) , top - down and bottom - up parsing , Cocke - Younger - Kasami ( CYK ) algorithm , efficiency .Limitations of context free grammars , implementing feature constraints .", "label": "", "metadata": {}, "score": "64.26276"}
{"text": "Chapter 12 : Formal Grammars of English ( Formerly 9 ) .This chapter still focuses on CFGs for English and includes a revamped and somewhat expanded grammar for the ATIS domain .New and expanded sections cover : treebanks with a focus on the Penn Treebank , searching treebanks with tgrep and tgrep2 , heads and head - finding rules , dependency grammars , Categorial grammar , and grammars for spoken language processing .", "label": "", "metadata": {}, "score": "65.22443"}
{"text": "( top ) .Chapter 11 : Computational Phonology ( Formerly parts of Chapters 4 , 5 , and 7 ) .This chapter is a brief introduction to computational phonology , including phonological and morphological learning , finite - state models , OT , and Stochastic OT .", "label": "", "metadata": {}, "score": "65.273506"}
{"text": "I am very grateful for the constant personal support I received from my family and friends .Intellectually my work owes a deep debt to my advisor , Nelson Morgan , as well as Alexey Tsymbal , David Opitz , and Aldebaro Klautau .", "label": "", "metadata": {}, "score": "66.34056"}
{"text": "Dr. Jelinek has recently been elected to the National Academy of Engineering ( NAE ) .The formal induction ceremony will take place in October 2006 .He is being recognized for his contributions to statistical language processing with applications to automatic speech recognition .", "label": "", "metadata": {}, "score": "67.111916"}
{"text": "381 - 397 .[ 6 ] [ Berger et al . , 1996 ] Adam Berger , Stephen A. Della Pietra , and Vincent J. Della Pietra.1996 .A Maximum Entropy Approach to Natural Language Processing .Computational Linguistics , 22 ( 1):39 - 71 .", "label": "", "metadata": {}, "score": "68.89635"}
{"text": "Group Name .Tab navigation .Here are some of the top contributions to speech recognition from IBM .The papers listed have been cited more than 10,000 citations times combined .First speech recognition application .In the early 1960s , IBM developed and demonstrated the Shoebox -- a forerunner of today 's voice recognition systems .", "label": "", "metadata": {}, "score": "69.29715"}
{"text": "A ' fragment ' is a spectro - temporal region where energy from a single sound source dominates .SFD employs techniques developed from knowledge about the auditory system to identify fragments .A decoding process using statistical speech models is applied to the fragment representation to simultaneously identify speech evidence and recognise speech .", "label": "", "metadata": {}, "score": "69.36134"}
{"text": "( top ) .Chapter 4 : N - grams ( Formerly Chapter 6 ) .This updated language model chapter has had a complete overhaul .This draft includes more examples , a more complete description of Good - Turing , expanded sections on practical issues like perplexity and evaluation , language modeling toolkits , including ARPA format , and an overview of modern methods like interpolated Kneser - Ney .", "label": "", "metadata": {}, "score": "69.47371"}
{"text": "Chapter 18 : Computational Semantics ( Formerly 15 ) .This chapter covers compositional approaches to semantic analysis at the sentence level .The primary focus is on rule - to - rule approaches based on lambda - expressions .It also now has new coverage of unification - based approaches to computational semantics .", "label": "", "metadata": {}, "score": "69.542946"}
{"text": "IEEE Transactions on Information Theory IT-15:1 ( 1969 ) .A highly selective MT bibliography . A. Berger , P. Brown , S. Della Pietra , V Della Pietra , J. Lafferty , H. Printz , and L. Ures ( 1994 ) .", "label": "", "metadata": {}, "score": "70.16025"}
{"text": "Also in 2008 , Dr. Jelinek was awarded the Statutary Prize of the city of Kladno ( Czech Republic ) for his contributions to science .Research .He is interested in Speech Recognition , statistical methods of natural language processing , and information theory .", "label": "", "metadata": {}, "score": "70.34761"}
{"text": "Conditional Random Fields ( CRFs ) are undirected graphical models , a special case of which correspond to conditionally - trained finite state machines .A key advantage of CRFs is their great flexibility to include a wide variety of arbitrary , non - independent features of the input .", "label": "", "metadata": {}, "score": "71.22967"}
{"text": "Chapter 13 : Syntactic Parsing ( Formerly 10 ) .The focus of this chapter is still on parsing with CFGs .It now includes sections on CKY , Earley and agenda - based ( chart ) parsing .In addition , there is a new section on partial parsing with a focus on machine learning based base - phrase chunking and the use of IOB tags .", "label": "", "metadata": {}, "score": "71.625786"}
{"text": "A brief introduction to the necessary background material from information retrieval is also included .The chapter includes factoid question answering , single document summarization , generic multiple document summarization , and query - focused summarization .( top ) .Chapter 24 : Dialog and Conversational Agents ( Formerly 19 ) .", "label": "", "metadata": {}, "score": "71.70909"}
{"text": "The lexicalized grammar formalism used is Combinatory Categorial Grammar ( CCG ) , and the grammar is automatically extracted from CCGbank , a CCG version of the Penn Treebank .The combination of discriminative training and an automatically extracted grammar leads to a significant memory requirement ( over 20 GB ) , which is satisfied using a parallel implementation of the BFGS optimisation algorithm running on a Beowulf cluster .", "label": "", "metadata": {}, "score": "73.40485"}
{"text": "This paper describes and evaluates log - linear parsing models for Combinatory Categorial Grammar ( CCG ) .A parallel implementation of the L - BFGS optimisation algorithm is described , which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation .", "label": "", "metadata": {}, "score": "74.87599"}
{"text": "This paper describes and evaluates log - linear parsing models for Combinatory Categorial Grammar ( CCG ) .A parallel implementation of the L - BFGS optimisation algorithm is described , which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation .", "label": "", "metadata": {}, "score": "74.87599"}
{"text": "+ recent articles : e.g. , of the proceedings of the Meetings of the Association for Computational Linguistics .", "label": "", "metadata": {}, "score": "79.33794"}
{"text": "We also describe an example of the direct tagging of spoken data with NE categories . \" ... personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page .", "label": "", "metadata": {}, "score": "79.78027"}
{"text": "In November 2001 , Dr. Jelinek was awarded an Honorary Doctorate of Mathematical Physical Disciplines awarded by the Academic Senate of Charles University of Prague .Click here to read the text of the citation ( scroll to the bottom for the English translation ) .", "label": "", "metadata": {}, "score": "84.55538"}
{"text": "IBM is not responsible for , and does not validate or confirm , the correctness or accuracy of any user content posted .IBM does not endorse any user content .User content does not represent the views or opinions of IBM .", "label": "", "metadata": {}, "score": "96.08516"}
{"text": "personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page .To copy otherwise , to republish , to post on servers or to redistribute to lists , requires prior specific permission .", "label": "", "metadata": {}, "score": "107.686714"}
