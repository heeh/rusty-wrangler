{"text": "The application of the designed algorithms to the traditional supervised evaluation tasks is also explored .To address the language - bias problem in most of the existing metrics , tunable parameters are assigned to different subfactors .The experiment on WMT11 and WMT12 corpora shows our designed algorithms yields the highest average correlation score on eight language pairs as compared to the state - of - the - art reference - aware metrics METEOR , BLEU , and TER .", "label": "", "metadata": {}, "score": "32.066856"}
{"text": ", 2004 ; Banerjee and Lavie , 2005 ; Lavie and Agarwal , 2007 ) have described the details underlying the metric and have extensively compared its performance with Bleu and several other MT evaluation metrics .\" [ Show abstract ] [ Hide abstract ] ABSTRACT : We describe our submission to the NIST Met- rics for Machine Translation Challenge consist- ing of 4 metrics - two versions of meteor , m - bleu and m - ter .", "label": "", "metadata": {}, "score": "32.444473"}
{"text": "That is followed by de- scriptino of m - bleu and m - ter , enhanced ver- sions of two other widely used metrics bleu and ter , which extend the exact word match- ing used in these metrics with the flexible matching based on stemming and Wordnet in Meteor .", "label": "", "metadata": {}, "score": "33.530643"}
{"text": "METEOR :An automatic metric for MT evaluation with improved correlation with human judgments .In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization , pages 65 - 72 , Ann Arbor , Michigan .", "label": "", "metadata": {}, "score": "33.87629"}
{"text": "BLEU and NIST .[ 12 ] .METEOR also includes some other features not found in other metrics , such as synonymy matching , where instead of matching only on the exact word form , the metric also matches on synonyms .", "label": "", "metadata": {}, "score": "34.437225"}
{"text": "Publications .Meteor ( current version , including paraphrase tables and X - ray ) : .Michael Denkowski and Alon Lavie , \" Meteor Universal : Language Specific Translation Evaluation for Any Target Language \" , Proceedings of the EACL 2014 Workshop on Statistical Machine Translation , 2014 [ PDF ] [ bib ] .", "label": "", "metadata": {}, "score": "34.622246"}
{"text": "This is due to the fact that METEOR employs many external materials including stemming , synonyms vocabulary , and paraphrasing resources , etc .However , to make the evaluation model concise , our method nLEPOR only uses the surface words and hLEPOR only uses the combination of surface words and POS sequences .", "label": "", "metadata": {}, "score": "36.440987"}
{"text": "Johns Hopkins University , Baltimore .Banerjee S , Lavie A ( 2005 )METEOR : an automatic metric for MT evaluation with improved correlation with human judgments .In : Proceedings of the workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization , Ann Arbor , Michigan , pp 65 - 72 .", "label": "", "metadata": {}, "score": "37.83703"}
{"text": "Abhaya Agarwal and Alon Lavie , \" METEOR , M - BLEU and M - TER : Evaluation Metrics for High - Correlation with Human Rankings of Machine Translation Output \" , Proceedings of the ACL 2008 Workshop on Statistical Machine Translation , 2008 [ PDF ] .", "label": "", "metadata": {}, "score": "38.746628"}
{"text": "The METEOR metric is designed to address some of the deficiencies inherent in the BLEU metric .The metric is based on the weighted harmonic mean of unigram precision and unigram recall .The metric was designed after research by Lavie ( 2004 ) into the significance of recall in evaluation metrics .", "label": "", "metadata": {}, "score": "39.24298"}
{"text": "Meteor is implemented in pure Java and requires no installation or dependencies to score MT output .On average , hypotheses are scored at a rate of 500 segments per second per CPU core .Meteor consistently demonstrates high correlation with human judgments in independent evaluations such as EMNLP WMT 2011 and NIST Metrics MATR 2010 .", "label": "", "metadata": {}, "score": "39.942036"}
{"text": "METEOR : an automatic metric for MT evaluation with high levels of correlation with human judgments .In : ACL 2007 : Proceedings of the second workshop on statistical machine translation , Prague , Czech Republic , pp 228 - 231 .", "label": "", "metadata": {}, "score": "40.72355"}
{"text": "An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments \" , Proceedings of the ACL 2007 Workshop on Statistical Machine Translation , 2007 [ PDF ] .Satanjeev Banerjee and Alon Lavie , \" METEOR :An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments \" , Proceedings of the ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization , 2005 [ PDF ] Tools . by Ding Liu , Daniel Gildea - In Proceedings of the HLT / NAACL-2007 , 2007 . \" ...", "label": "", "metadata": {}, "score": "41.40002"}
{"text": "We further improve performance by combining indi - vidual evaluation metrics using maximum correlation training , which is shown to be better than the classification - based frame - work .Such metrics were shown to have better fluency evaluation performance than metrics based on n - grams such BLEU and NIST ( Doddington , 2002 ) . \" ...", "label": "", "metadata": {}, "score": "42.62054"}
{"text": "We further improve the evaluation performance by combining the individual metrics using maximum correlation training , which is shown to be better than the classification - based framework .Such metrics were shown to have better fluency evaluation performance than metrics based on n - grams such BLEU and NIST ( Doddington , 2002 ) . \" ...", "label": "", "metadata": {}, "score": "44.28835"}
{"text": "The concept of METEOR is rather different from that of the above metrics in that all the other metrics relied on unigram precision ( the two SL and TL identical word strings ) only , while METEOR gives more weight to unigram precision and unigram recall .", "label": "", "metadata": {}, "score": "44.57846"}
{"text": "METEOR scores machine translation hypotheses by aligning them to one or more reference translations . \" \" meteor , initially proposed and released in 2004 ( Lavie et al ., 2004 ) was explicitly designed to improve correlation with human judgments of MT quality at the segment level .", "label": "", "metadata": {}, "score": "44.719273"}
{"text": "Open source tool pp .441 - 450 .International Association for Machine Translation .Open source tool .Association for Computational Linguistics .Online paper pp .414 - 421 About .The Meteor automatic evaluation metric scores machine translation hypotheses by aligning them to one or more reference translations .", "label": "", "metadata": {}, "score": "45.482437"}
{"text": "The commonly used automatic evaluation metrics include BLEU [ 5 ] , METEOR [ 6 ] , TER [ 7 ] and AMBER [ 8 ] , etc .However , most of the automatic MT evaluation metrics are referenceaware , which means they tend to employ different approaches to calculate the closeness between the hypothesis translations offered by MT systems and the reference translations provided by professional translators .", "label": "", "metadata": {}, "score": "45.65842"}
{"text": "Differing provisions from the publisher 's actual policy or licence agreement may be applicable .\" Their experiment also revealed that the enhancement of both BLEU and NIST is correlated to human evaluation .To overcome the weaknesses of the above mentioned metrics , a new metric was proposed by Lavie et al .", "label": "", "metadata": {}, "score": "46.107716"}
{"text": "The study concluded that , \" highly reliable assessments can be made of the quality of human and machine translations \" .[ 4 ] .As part of the Human Language Technologies Program , the Advanced Research Projects Agency ( ARPA ) created a methodology to evaluate machine translation systems , and continues to perform evaluations based on this methodology .", "label": "", "metadata": {}, "score": "46.121773"}
{"text": "We carry out our analysis using datasets from the Document Understanding Conferences , studying not only the impact of these features on automatic summarizers , but also their role in human summarization .Our research shows that a frequency based summarizer can achieve performance comparable to that of state - of - the - art systems , but only with a good composition function ; context sensitivity improves performance and significantly reduces repetition . .", "label": "", "metadata": {}, "score": "46.496323"}
{"text": "We carry out our analysis using datasets from the Document Understanding Conferences , studying not only the impact of these features on automatic summarizers , but also their role in human summarization .Our research shows that a frequency based summarizer can achieve performance comparable to that of state - of - the - art systems , but only with a good composition function ; context sensitivity improves performance and significantly reduces repetition . .", "label": "", "metadata": {}, "score": "46.496323"}
{"text": "Michael Denkowski and Alon Lavie , \" Meteor 1.3 : Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems \" , Proceedings of the EMNLP 2011Workshop on Statistical Machine Translation , 2011 [PDF ] [ bib ] .", "label": "", "metadata": {}, "score": "47.31865"}
{"text": "It has been shown to correlate highly with human judgments of quality at the corpus level .[ 9 ] .BLEU uses a modified form of precision to compare a candidate translation against multiple reference translations .The metric modifies simple precision since machine translation systems have been known to generate more words than appear in a reference text .", "label": "", "metadata": {}, "score": "48.0935"}
{"text": "Finally , the metric must be general , that is it should work with different text domains , in a wide range of scenarios and MT tasks .The aim of this subsection is to give an overview of the state of the art in automatic metrics for evaluating machine translation .", "label": "", "metadata": {}, "score": "49.015785"}
{"text": "The detailed results of WMT13 Metrics Task is introduced in the paper .[ 16 ] .^ While the metrics are described as for the evaluation of machine translation , in practice they may also be used to measure the quality of human translation .", "label": "", "metadata": {}, "score": "49.07218"}
{"text": "In this paper , the authors propose an unsupervised MT evaluation metric using universal Part - of - Speech tagset without relying on reference translations .The authors also explore the performances of the designed metric on traditional supervised evaluation tasks .", "label": "", "metadata": {}, "score": "49.562675"}
{"text": "This paper investigates the automatic evaluation of text coherence for machine - generated texts .We introduce a fully - automatic , linguistically rich model of local coherence that correlates with human judgments .Our modeling approach relies on shallow text properties and is relatively inexpensive .", "label": "", "metadata": {}, "score": "49.78795"}
{"text": "This paper investigates the automatic evaluation of text coherence for machine - generated texts .We introduce a fully - automatic , linguistically rich model of local coherence that correlates with human judgments .Our modeling approach relies on shallow text properties and is relatively inexpensive .", "label": "", "metadata": {}, "score": "49.78795"}
{"text": "This paper investigates the automatic evaluation of text coherence for machine - generated texts .We introduce a fully - automatic , linguistically rich model of local coherence that correlates with human judgments .Our modeling approach relies on shallow text properties and is relatively inexpensive .", "label": "", "metadata": {}, "score": "49.78795"}
{"text": "This paper investigates the automatic evaluation of text coherence for machine - generated texts .We introduce a fully - automatic , linguistically rich model of local coherence that correlates with human judgments .Our modeling approach relies on shallow text properties and is relatively inexpensive .", "label": "", "metadata": {}, "score": "49.78795"}
{"text": "( 2003 ) point out that , \" Any MT evaluation measure is less reliable on shorter translations \" , and show that increasing the amount of data improves the reliability of a metric .However , they add that \" ... reliability on shorter texts , as short as one sentence or even one phrase , is highly desirable because a reliable MT evaluation measure can greatly accelerate exploratory data analysis \" .", "label": "", "metadata": {}, "score": "49.897198"}
{"text": "We submitted the results for Task 1.1 ( sentence - level quality estimation ) , Task 1.2 ( system selection ) and Task 2 ( word - level quality estimation ) ... \" .This paper is to introduce our participation in the WMT13 shared tasks on Quality Estimation for machine translation without using reference translations .", "label": "", "metadata": {}, "score": "50.48987"}
{"text": "By aligning both t ... \" .We propose three new features for MT evaluation : source - sentence constrained n - gram precision , source - sentence re - ordering metrics , and discriminative un - igram precision , as well as a method of learning linear feature weights to directly maximize correlation with human judg - ments .", "label": "", "metadata": {}, "score": "50.53971"}
{"text": "The Quality & MT Relationship Machine Translation & Quality .NIST Fluency Usability GTM F - Measure Productivity TER Adequacy BLEU Acceptability METEOR Language Task Machine Translation & Quality .Who needs to measure Quality ?Who needs to measure Quality ?", "label": "", "metadata": {}, "score": "50.627693"}
{"text": "We show that significantly better correla ... \" .Recent research has shown that a balanced harmonic mean ( F1 measure ) of unigram precision and recall outperforms the widely used BLEU and NIST metrics for Machine Translation evaluation in terms of correlation with human judgments of translation quality .", "label": "", "metadata": {}, "score": "50.632366"}
{"text": "We show that significantly better correla ... \" .Recent research has shown that a balanced harmonic mean ( F1 measure ) of unigram precision and recall outperforms the widely used BLEU and NIST metrics for Machine Translation evaluation in terms of correlation with human judgments of translation quality .", "label": "", "metadata": {}, "score": "50.632366"}
{"text": "527 - 549 , 2006 .G. Lembersky , N. Ordan , and S. Wintner , \" Language Models for Machine Translation - Original vs. Translated Texts , \" J. Computational Linguistics , Volume 38 , Number 4 , 2012 .K. Papineni , S. Roukos , T. Ward , and W.-J. Zhu , \" BLEU : A Method for Auto - matic Evaluation of Machine Translation , \" In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , ACL Press , 2002 , pp . 311 - 318 .", "label": "", "metadata": {}, "score": "51.074368"}
{"text": "By aligning both the ... \" .We propose three new features for MT evaluation : source - sentence constrained n - gram precision , source - sentence reordering metrics , and discriminative unigram precision , as well as a method of learning linear feature weights to directly maximize correlation with human judgments .", "label": "", "metadata": {}, "score": "51.212006"}
{"text": "By aligning both the ... \" .We propose three new features for MT evaluation : source - sentence constrained n - gram precision , source - sentence reordering metrics , and discriminative unigram precision , as well as a method of learning linear feature weights to directly maximize correlation with human judgments .", "label": "", "metadata": {}, "score": "51.212006"}
{"text": "[ 1 ] The reason why it is such a poor predictor of quality is reasonably intuitive .A round - trip translation is not testing one system , but two systems : the language pair of the engine for translating into the target language , and the language pair translating back from the target language .", "label": "", "metadata": {}, "score": "51.787758"}
{"text": "Our experiments and their results are described in section 4 .Future directions and extensions of this work are discussed in section 5 . 2 Evaluation Metrics The metrics used in our evaluations , in addition to BLEU and NIST , are based on explicit word - to - word matches between the translation being evaluated and each of one or more reference translations .", "label": "", "metadata": {}, "score": "51.934364"}
{"text": "In this paper we first pre ... \" .Automatically produced texts ( e.g. translations or summaries ) are usually evaluated with n - gram based measures such as BLEU or ROUGE , while the wide set of more sophisticated measures that have been proposed in the last years remains largely ignored for practical purposes .", "label": "", "metadata": {}, "score": "52.055504"}
{"text": "BLEU was one of the first metrics to report high correlation with human judgments of quality .The metric is currently one of the most popular in the field .The central idea behind the metric is that \" the closer a machine translation is to a professional human translation , the better it is \" .", "label": "", "metadata": {}, "score": "52.06772"}
{"text": "This method is tested on DUC 2003 , 2004 , and 2005 systems . ... dividual pieces of information , which may not be used by humans for reference summaries .The question becomes what level of granularity is appropriate for automatic summary content comparison .", "label": "", "metadata": {}, "score": "52.185707"}
{"text": "The experiments also show that the latest proposed metrics ( e.g. AMBER ) achieve higher correlation score than the earlier ones ( e.g. BLEU ) .TABLE IV .The number of participated MT systems that offer the output translations is shown in Table 5 for each language pair .", "label": "", "metadata": {}, "score": "52.20215"}
{"text": "Our source - sentence c ... \" .We propose three new features for MT evaluation : source - sentence constrained n - gram precision , source - sentence reordering metrics , and discriminative unigram precision , as well as a method of learning linear feature weights to directly maximize correlation with human judgments .", "label": "", "metadata": {}, "score": "52.50518"}
{"text": "M. Felice and L. Specia , \" Linguistic Features for Quality Estimation , \" Proceedings of the 7th Workshop on Statistical Machine Translation , ACL Press , 2012 , pp .96 - 103 .S. Petrov , D. Das , and R. McDonald , \" A Universal Part - of - Speech Tagset , \" Proceedings of the Eight International Conference on Language Resources and Evaluation ( LREC'12 ) , European Language Resources Association ( ELRA ) Press .", "label": "", "metadata": {}, "score": "52.97145"}
{"text": "We show that this method correlates better with human judgments than any other automated procedure to date , and overcomes the subjectivity / variability problems of manual methods that require humans t ... \" .In this paper we introduce Basic Elements , a new way of automating the evaluation of text summaries .", "label": "", "metadata": {}, "score": "53.063644"}
{"text": "We show that this method correlates better with human judgments than any other automated procedure to date , and overcomes the subjectivity / variability problems of manual methods that require humans t ... \" .In this paper we introduce Basic Elements , a new way of automating the evaluation of text summaries .", "label": "", "metadata": {}, "score": "53.063644"}
{"text": "A number of sentences were used as a testing dataset to evaluate both engines .Human translations by three bilingual speakers were used as a gold standard .A simple evaluation metric was proposed to calculate the translation accuracy of verb - noun collocations .", "label": "", "metadata": {}, "score": "53.16564"}
{"text": "In the unsupervised design , the metric is measured on the source and target POS sequences instead of the surface words .In the exploration of its usage in the traditional supervised MT evaluations , we measure the score on the target translations ( system outputs ) and reference translations , i.e. measuring on the surface words .", "label": "", "metadata": {}, "score": "53.80038"}
{"text": "Previous work by Lin and Hovy [ 9 ] has shown that a recall - based automatic metric for evaluating summaries outperforms the BLEU metric on that task .We describe the metrics used in our evaluation in Section 2 .We also discuss certain characteristics of the BLEU and NIST metrics that may account for the advantage of metrics based on unigram recall .", "label": "", "metadata": {}, "score": "53.950966"}
{"text": "While this may seem unexpected , since BLEU and NIST focus on n - gram precision and disregard recall , our experiments show that correlation with human judgments is highest when almost all of the weight is assigned to recall .We also show that stemming is significantly beneficial not just to simpler unigram precision and recall based metrics , but also to BLEU and NIST . 1 Introduction Automatic Metrics for machine translation ( MT ) evaluation have been receiv- ing significant attention in the past two years , since IBM 's BLEU metric was proposed and made available [ 1].", "label": "", "metadata": {}, "score": "54.184906"}
{"text": "This paper investigates these concerns in the context of using regression to develop metrics for evaluating machine - translated sentences .We track a learned metric 's reliability across a 5 year period to measure the extent to which the learned metric can evaluate sentences produced by other systems .", "label": "", "metadata": {}, "score": "54.652306"}
{"text": "It is the similar approach for the source sentence .( 17 ) and ( 18 ) , where represents the number of matched n - gram chunks .The n - gram precision ( and recall ) is calculated on sentence - level not corpus - level used in BLEU ( ) .", "label": "", "metadata": {}, "score": "54.66161"}
{"text": "Coughlin ( 2003 ) reports that comparing the candidate text against a single reference translation does not adversely affect the correlation of metrics when working in a restricted domain text .Even if a metric correlates well with human judgment in one study on one corpus , this successful correlation may not carry over to another corpus .", "label": "", "metadata": {}, "score": "54.935265"}
{"text": "In this paper , we present a comparison between the widely used BLEU and NIST metrics , and a set of easily computable metrics based on unigram precision and recall .Using several empirical evaluation methods that have been proposed .Page 2 . in the recent literature as concrete means to assess the level of correlation of au- tomatic metrics and human judgments , we show that higher correlations can be obtained with fairly simple and straightforward metrics .", "label": "", "metadata": {}, "score": "55.278404"}
{"text": "As mentioned previously , the language - bias problem is presented in the evaluation results of BLEU metric .BLEU yields the highest Spearman rank correlation score 0.99 on FR - EN ; however it never achieves the highest average correlation score on any translation direction , due to the very low correlation scores on RU - EN , EN - DE , EN - ES , and ENRU corpora .", "label": "", "metadata": {}, "score": "55.407036"}
{"text": "Witten IH , Frank E ( 2005 )Data mining : practical machine learning tools and techniques , 2nd edn .Morgan Kaufmann , San Francisco .Ye Y , Zhou M , Lin C - Y ( 2007 ) Sentence level machine translation evaluation as a ranking .", "label": "", "metadata": {}, "score": "55.56508"}
{"text": "The usual approach for automatic summarization is sentence extraction , where key sentences from the input documents are selected based on a suite of features .While word frequency often is used as a feature in summarization , its impact on system performance has not been isolated .", "label": "", "metadata": {}, "score": "55.683952"}
{"text": "The usual approach for automatic summarization is sentence extraction , where key sentences from the input documents are selected based on a suite of features .While word frequency often is used as a feature in summarization , its impact on system performance has not been isolated .", "label": "", "metadata": {}, "score": "55.683952"}
{"text": "The usual approach for automatic summarization is sentence extraction , where key sentences from the input documents are selected based on a suite of features .While word frequency often is used as a feature in summarization , its impact on system performance has not been isolated .", "label": "", "metadata": {}, "score": "55.683952"}
{"text": "The usual approach for automatic summarization is sentence extraction , where key sentences from the input documents are selected based on a suite of features .While word frequency often is used as a feature in summarization , its impact on system performance has not been isolated .", "label": "", "metadata": {}, "score": "55.683952"}
{"text": "In this paper we describe a framework in which summary evaluation measures can be instantiated and compared , and we implement a specific evaluation method using very small units of content , called Basic Elements , that address some of the shortcomings of ngrams .", "label": "", "metadata": {}, "score": "55.708076"}
{"text": "Banerjee et al .( 2005 ) highlight five attributes that a good automatic metric must possess ; correlation , sensitivity , consistency , reliability and generality .Any good metric must correlate highly with human judgment , it must be consistent , giving similar results to the same MT system on similar text .", "label": "", "metadata": {}, "score": "56.21193"}
{"text": "In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics ( ACL ) , pages 311 - 318 , Philadelphia , PA , July .Doddington , George .Automatic Evaluation of Machine Translation Quality Using N - gram Co - Occurrence Statistics .", "label": "", "metadata": {}, "score": "56.735344"}
{"text": "Full - text preview . cmu.edu Abstract .Recent research has shown that a balanced harmonic mean ( F1 measure ) of unigram precision and recall outperforms the widely used BLEU and NIST metrics for Machine Translation evaluation in terms of correlation with human judgments of translation quality .", "label": "", "metadata": {}, "score": "56.799183"}
{"text": "It was originally used for measuring the performance of speech recognition systems , but is also used in the evaluation of machine translation .The metric is based on the calculation of the number of words that differ between a piece of machine translated text and a reference translation .", "label": "", "metadata": {}, "score": "56.871826"}
{"text": "Doddington , G. ( 2002 ) \" Automatic evaluation of machine translation quality using n - gram cooccurrence statistics \" .Proceedings of the Human Language Technology Conference ( HLT ) , San Diego , CA pp .128 - 132 .", "label": "", "metadata": {}, "score": "56.8886"}
{"text": "Many MT methods and automatic MT systems were proposed in the past years [ 2][3][4].Traditionally , people use the human evaluation approaches for the quality estimation of MT systems , such as the adequacy and fluency criteria .However , the human evaluation is expensive and time consuming .", "label": "", "metadata": {}, "score": "57.141697"}
{"text": "III .RELATED WORKS As mentioned previously , the traditional evaluation metrics tend to estimate quality of the automatic MT output by measuring its closeness with the reference translations .Gamon et al .[ 18 ] conduct a research about reference - free MT evaluation approaches also at sentence level , which work utilizes the linear and nonlinear combinations of language model and SVM classifier to find the badly translated sentences .", "label": "", "metadata": {}, "score": "57.226788"}
{"text": "In Task 1.1 , we used an enhanced version of BLEU metric without using reference translations to evaluate the translation quality .In Task 1.2 , we utilized a probability model Na\u00efve Bayes ( NB ) as a classification algorithm with the features borrowed from the traditional evaluation metrics .", "label": "", "metadata": {}, "score": "57.46257"}
{"text": "This paper will propose an automatic evaluation approach for English - to - German translation by calculating the similarity between source and hypothesis translations without the using of reference translation .Furthermore , the potential usage of the proposed evaluation algorithms in the traditional reference - aware MT evaluation tasks will also be explored .", "label": "", "metadata": {}, "score": "57.47959"}
{"text": "160 - 167 .J. B. Mari\u00f1 R. E. Banchs , J. M. Crego , A. de Gispert , P. Lambert , J. o , A. Fonollosa , and M. R. Costa - juss\u00e0 \" N - gram based machine , translation , \" Computational Linguistics , Vol .", "label": "", "metadata": {}, "score": "57.50103"}
{"text": "59 - 63 .G. Doddington , \" Automatic Evaluation of Machine Translation Quality using N - gram Co - occurrence Statistics , \" In Proceedings of the 2nd International Conference on Human Lan - guage Technology Research ( HLT-02 ) , Morgan Kaufmann Publishers Inc. Press , 2002 , pp .", "label": "", "metadata": {}, "score": "57.682312"}
{"text": "65 - 72 .B. Chen and R. Kuhn , \" Amber : A modi - fied bleu , enhanced ranking metric , \" In Proceed - ings of the Sixth Workshop on Statistical Machine translation of the Association for Computational Linguistics , ACL Press , 2011 , pp 71 - 77 . Y. Zhang , S. Vogel , and A. Waibel , \" Interpreting BLEU / NIST scores : How much im - provement do we need to have a better system ? , \" In Proc . of LREC , ELRA Press , 2004 , Lisbon .", "label": "", "metadata": {}, "score": "57.74778"}
{"text": "We will introduce the sub - factors in the formula step by step .This approach is different with BLEU metric which assigns penalty for the shorter sentence compared with the human reference translation .Parameter and specify the length of candidate sentence ( hypothesis ) and source sentence respectively .", "label": "", "metadata": {}, "score": "57.757015"}
{"text": "The edit categories include the insertion , deletion , substitution of single words and the shifts of word chunks .TER uses the strict matching of words and word order , e.g. mis - capitalization is also counted as an edit .", "label": "", "metadata": {}, "score": "57.93004"}
{"text": "These visualizations allow easy comparison of MT systems or system configurations and facilitate in - depth performance analysis by examination of underlying Meteor alignments .Final output is in PDF form with intermediate TeX and Gnuplot files preserved for inclusion in reports or presentations .", "label": "", "metadata": {}, "score": "57.976196"}
{"text": "Measuring systems based on adequacy and fluency , along with informativeness is now the standard methodology for the ARPA evaluation program .[5 ] .In the context of this article , a metric is a measurement .A metric that evaluates machine translation output represents the quality of the output .", "label": "", "metadata": {}, "score": "58.41146"}
{"text": "After this , we formalize and verify empirically a set of properties that every text evaluation measure based on similarity to human - produced references satisfies .These properties imply that corroborating system improvements with additional measures always increases the overall reliability of the evaluation process .", "label": "", "metadata": {}, "score": "58.596817"}
{"text": "However , the evaluation systems and actually tell different results on the quality of the three MT systems .The evaluation system gives very high score 0.90 on system and similar low scores 0.35 and 0.40 respectively on and systems .On the other hand , the evaluation system yields similar scores 0.46 , 0.35 and 0.42 respectively on the three MT systems , which means that all the three automatic MT systems have low translation quality .", "label": "", "metadata": {}, "score": "58.617893"}
{"text": "As a contrast and simplified version , Zhang et al .[ 9 ] proposes modified BLEU metric using the arithmetic mean of the n - gram precision .B. TER Metric TER [ 7 ] means translation edit rate , which is designed at sentence - level to calculate the amount of work needed to correct the hypothesis translation according to the closest reference translation ( assuming there are several reference translations ) .", "label": "", "metadata": {}, "score": "58.645252"}
{"text": "Papineni K , Roukos S , Ward T , Zhu W - J ( 2002 ) Bleu : a method for automatic evaluation of machine translation .In : 40th annual meeting of the Association for Computational Linguistics ( ACL-2002 ) , Philadelphia , PA , pp 311 - 318 .", "label": "", "metadata": {}, "score": "58.813522"}
{"text": "[ 10 ] .The NIST metric is based on the BLEU metric , but with some alterations .Where BLEU simply calculates n - gram precision adding equal weight to each one , NIST also calculates how informative a particular n - gram is .", "label": "", "metadata": {}, "score": "58.92209"}
{"text": "In the training period , the parameter values of ( weight on recall ) and ( weight on precision ) are tuned to 1 and 9 respectively , which is different with the reference - aware metric METEOR ( more weight on recall ) .", "label": "", "metadata": {}, "score": "58.99615"}
{"text": "Therefore , any metric must assign quality scores so they correlate with human judgment of quality .That is , a metric should score highly translations that humans score highly , and give low scores to those humans give low scores .", "label": "", "metadata": {}, "score": "59.08609"}
{"text": "Using the IBM model one and the information of morphemes , lexicon probabilities , Part - of - Speech , etc . , Popovi\u0107 et al .[ 22 ] also introduces an unsupervised evaluation method , and show that the most promising setting comes from the IBM-1 scores calculated on morphemes and POS-4gram .", "label": "", "metadata": {}, "score": "59.12555"}
{"text": "Several other automatic metrics for MT evaluation have been proposed since the early 1990s .The utility and attractiveness of automatic metrics for MT evaluation has been widely recognized by the MT community .Evaluating an MT system using such automatic metrics is much faster , easier and cheaper compared to human evaluations , which require trained bilingual evaluators .", "label": "", "metadata": {}, "score": "59.42125"}
{"text": "Each document contains 3,003 sentences of source English or translated German .To avoid the over fitting problem , the WMT20112 corpora are used as training data and WMT20123 corpora are used for the testing .This paper conducts the experiments on the simplified version ( unigram precision and recall ) of the metric .", "label": "", "metadata": {}, "score": "60.200897"}
{"text": "Both of the two metrics are trained on WMT11 corpora .The tuned parameters of hLEPOR are shown in Table 7 , using default values for EN - RU and RU - EN .TABLE VII .THE NUMBER OF EFFECTIVE MT SYSTEMS IN WMT13 Number of evaluated MT systems Year WMT13 other - to - English CS - EN 12 DE - EN 23 TABLE IX .", "label": "", "metadata": {}, "score": "60.232536"}
{"text": "First , a learned metric is more reliable for translations that are similar to its training examples ; this calls into question whether it is as effective in evaluating translations from systems that are not its contemporaries .Second , metrics trained from different sets of training examples may exhibit variations in their evaluations .", "label": "", "metadata": {}, "score": "60.79934"}
{"text": "In : HLT - NAACL 06Statistical machine translation , Proceedings of the workshop , New York City , pp 86 - 93 .Owczarzak K , Van Genabith J , Way A ( 2007 ) Dependency - based automatic evaluation for machine translation .", "label": "", "metadata": {}, "score": "60.83749"}
{"text": "119 - 131 , 2013 . A. L.-F. M. Gamon , A. Aue , and M. Smets , \" Sentence - level MT evaluation without refer - ence translations : Beyond language modeling , \" Proceedings of EAMT , EAMT Press , 2005 .", "label": "", "metadata": {}, "score": "60.938816"}
{"text": "386 - 393 .Chin - Yew Lin and Eduard Hovy .Automatic Evaluation of Summaries Using N - gram Co - occurrence Statistics .In Proceedings of HLT - NAACL 2003 .Edmonton , Canada .May 2003 .pp .", "label": "", "metadata": {}, "score": "61.08455"}
{"text": "B. Wong , and C. Kit , \" ATEC : auto - matic evaluation of machine translation via word choice and word order , \" Mach Translat , 23:141- 155 , 2009 . A. L.-F. A. L.-F. A L.-F. Han , D. F. Wong , L. S. Chao , L. He , S. Li and L. Zhu , \" Phrase Tagset Mapping for French and English Treebanks and Its Application in Machine Translation Evaluation , \" Lecture Notes in Computer Science ( LNCS ) , Vol .", "label": "", "metadata": {}, "score": "61.101547"}
{"text": "In this work we study the theoretical and empirical properties of various global inference algorithms for multi - document summarization .We start by defining a general framework and proving that inference in it is NP - hard .We then present three algorithms : The first is a greedy approximate ... \" .", "label": "", "metadata": {}, "score": "61.178913"}
{"text": "On the other hand , the Pearson correlation coefficient uses the absolute scores yielded by the automatic MT evaluation systems as shown in Eq .22 , without the preconverting into rank values .IX .Last , this paper also makes a contribution on the complementary POS tagset mapping between German and English in the light of 12 universal tags .", "label": "", "metadata": {}, "score": "61.205704"}
{"text": "The question becomes what level of granularity is appropriate for automatic summary content comparison .It uses ngrams of various lengths , a total of 17 different par ... . \" ...Topic - focused multi - document summarization aims to produce a summary biased to a given topic or user profile .", "label": "", "metadata": {}, "score": "61.228607"}
{"text": "We show that signican tly better correlations can be achieved by placing more weight on recall than on precision .While this may seem unexpected , since BLEU and NIST focus on n - gram precision and disregard recall , our experiments show that correlation with human judgments is highest when almost all of the weight is assigned to recall .", "label": "", "metadata": {}, "score": "61.260838"}
{"text": "Mach Learn 20(3 ) : 273 - 297 .Doddington G ( 2002 )Automatic evaluation of machine translation quality using n - gram co - occurrence statistics .In : Proceedings of the second conference on human language technology ( HLT-2002 ) , San Diego , California , pp 128 - 132 .", "label": "", "metadata": {}, "score": "61.284397"}
{"text": "[ 11 ] For example , if the bigram \" on the \" correctly matches , it receives lower weight than the correct matching of bigram \" interesting calculations , \" as this is less likely to occur .NIST also differs from BLEU in its calculation of the brevity penalty , insofar as small variations in translation length do not impact the overall score as much .", "label": "", "metadata": {}, "score": "61.290318"}
{"text": "115 - 132 .C. Callison - Burch , P. Koehn , C. Monz , M. Post , R. Soricut , and L. Specia , \" Findings of the 2012 Workshop on Statistical Machine Translation , \" In Proceedings of the Seventh Workshop on Statistical Machine Translation , ACL Press , 2012 , pp .", "label": "", "metadata": {}, "score": "61.584007"}
{"text": "Segment and system level metric scores are calculated based on the alignments between hypothesis - reference pairs .The metric includes several free parameters that are tuned to emulate various human judgment tasks including WMT ranking and NIST adequacy .The current version also includes a tuning configuration for use with MERT and MIRA .", "label": "", "metadata": {}, "score": "61.671776"}
{"text": "In this paper we describe ... \" .As part of evaluating a summary automatically , it is usual to determine how much of the contents of one or more human - produced ' ideal ' summaries it contains .Past automated methods such as ROUGE compare using fixed word ngrams , which are not ideal for a variety of reasons .", "label": "", "metadata": {}, "score": "61.845596"}
{"text": "Syntactic features for evaluation of machine translation .In : Intrinsic and extrinsic evaluation measures for machine translation and/or summarization , Proceedings of the ACL-05 workshop , Ann Arbor , MI , pp 25 - 32 .Liu D , Gildea D ( 2006 ) Stochastic iterative alignment for machine translation evaluation .", "label": "", "metadata": {}, "score": "61.864983"}
{"text": "Hastie T , Tibshirani R , Friedman J ( 2001 )The elements of statistical learning .Springer - Verlag , New York .Hovy E , King M , Popescu - Belis A ( 2002 )Principles of context - based machine translation evaluation .", "label": "", "metadata": {}, "score": "61.90388"}
{"text": "Universal POS chunk matching example for bigram precision and recall .The variable means n - gram Position difference Penalty that is designed for the different order of successful matched POS in source and hypothesis sentence .The position difference factor has been proved to be helpful for the MT evaluation in the research work of [ 13].", "label": "", "metadata": {}, "score": "61.94771"}
{"text": "The aim of the training stage is to achieve higher correlation with human judgments .The experiments on WMT11 corpora show that yields the best correlation scores on the language pairs of CS - EN , ES - EN , EN - CS and EN - ES , which contributes to the highest average score 0.77 on all the eight language pairs .", "label": "", "metadata": {}, "score": "61.998207"}
{"text": "The evaluation results in Table 9 and Table 10 show that the evaluation on the language pairs with English as the source language ( English - to - other ) is the main challenge at system - level performance .Fortunately , the designed .", "label": "", "metadata": {}, "score": "62.17543"}
{"text": "In : Intrinsic and extrinsic evaluation measures for machine translation and/or summarization , Proceedings of the ACL-05 workshop , Ann Arbor , MI , pp 57 - 64 .Snover M , Dorr B , Schwartz R , Micciulla L , Makhoul J ( 2006 )", "label": "", "metadata": {}, "score": "62.202423"}
{"text": "While this may seem unexpected , since BLEU and NIST focus on n - gram precision and disregard recall , our experiments show that correlation with human judgments is highest when almost all of the weight is assigned to recall .We also show that stemming is significantly beneficial not just to simpler unigram precision and recall based metrics , but also to BLEU and NIST . .", "label": "", "metadata": {}, "score": "62.422256"}
{"text": "While this may seem unexpected , since BLEU and NIST focus on n - gram precision and disregard recall , our experiments show that correlation with human judgments is highest when almost all of the weight is assigned to recall .We also show that stemming is significantly beneficial not just to simpler unigram precision and recall based metrics , but also to BLEU and NIST . .", "label": "", "metadata": {}, "score": "62.422256"}
{"text": "The human judges were specially trained for the purpose .The evaluation study compared an MT system translating from Russian into English with human translators , on two variables .The variables studied were \" intelligibility \" and \" fidelity \" .", "label": "", "metadata": {}, "score": "62.454384"}
{"text": "Presentation at DARPA / TIDES 2003 MT Workshop .NIST , Gathersberg , MD .July 2003 .Bo Pang , Kevin Knight and Daniel Marcu .Syntax - based Alignment of Multiple Translations : Extracting Paraphrases and Generating New Sentences .", "label": "", "metadata": {}, "score": "62.500004"}
{"text": "A metric that only works for text in a specific domain is useful , but less useful than one that works across many domains - because creating a new metric for every new evaluation or domain is undesirable .Another important factor in the usefulness of an evaluation metric is to have good correlation , even when working with small amounts of data , that is candidate sentences and reference translations .", "label": "", "metadata": {}, "score": "62.803444"}
{"text": "This study and its results may help to shed some light on the problem and to develop new methods to improve Arabic verb noun collocability in the output translation of current machine translation engines .\" ORdering ) is an automatic evaluation metric for the machine translation output .", "label": "", "metadata": {}, "score": "63.068718"}
{"text": "This paper employs the -gram method into the matching process , which means that the potential POS candidate will be assigned higher priority if it has neighbor matching .The nearest matching will be accepted as a backup choice if there are both neighbor matching or there is no other matched POS around the potential pairs .", "label": "", "metadata": {}, "score": "63.096924"}
{"text": "Kulesza A , Shieber SM ( 2004 )A learning approach to improving sentence - level MT evaluation .In : TMI-2004 : Proceedings of the tenth conference on theoretical and methodological issues in machine translation , Baltimore , MD , pp 75 - 84 .", "label": "", "metadata": {}, "score": "63.44803"}
{"text": "This is demonstrated on DUC 2005 peer systems and . by Aminul Islam , Diana Inkpen - ACM Transactions on Knowledge Discovery from Data ( TKDD , 2008 . \" ...We present a method for measuring the semantic similarity of texts using a corpus - based measure of semantic word similarity and a normalized and modified version of the Longest Common Subsequence ( LCS ) string matching algorithm .", "label": "", "metadata": {}, "score": "63.601288"}
{"text": "This is demonstrated on DUC 2005 peer systems and . by Aminul Islam , Diana Inkpen - ACM Transactions on Knowledge Discovery from Data ( TKDD , 2008 . \" ...We present a method for measuring the semantic similarity of texts using a corpus - based measure of semantic word similarity and a normalized and modified version of the Longest Common Subsequence ( LCS ) string matching algorithm .", "label": "", "metadata": {}, "score": "63.601288"}
{"text": "[ 23 ] use the cross - lingual textual entailment to push semantics into the MT evaluation without using reference translations , which work mainly focuses on the adequacy estimation .Avramidis [ 24 ] performs an automatic sentence - level ranking of multiple machine translations using the features of verbs , nouns , sentences , subordinate clauses and punctuation occurrences to derive the adequacy information .", "label": "", "metadata": {}, "score": "63.789577"}
{"text": "The manifold - ranking process can naturally make full use of both the relati ... \" .Topic - focused multi - document summarization aims to produce a summary biased to a given topic or user profile .This paper presents a novel extractive approach based on manifold - ranking of sentences to this summarization task .", "label": "", "metadata": {}, "score": "63.939316"}
{"text": "We present a method for measuring the semantic similarity of texts using a corpus - based measure of semantic word similarity and a normalized and modified version of the Longest Common Subsequence ( LCS ) string matching algorithm .Existing methods for computing text similarity have focused mainly on either large documents or individual words .", "label": "", "metadata": {}, "score": "63.974854"}
{"text": "We present a method for measuring the semantic similarity of texts using a corpus - based measure of semantic word similarity and a normalized and modified version of the Longest Common Subsequence ( LCS ) string matching algorithm .Existing methods for computing text similarity have focused mainly on either large documents or individual words .", "label": "", "metadata": {}, "score": "63.974854"}
{"text": "313 - 330 , 1993 .W. Skut , B. Krenn , T. Brants , and H. Uszkoreit , \" An annotation scheme for free word order languages , \" In Proceedings of Applied Natural Language Processing , ACL Press , 1997 .", "label": "", "metadata": {}, "score": "64.09485"}
{"text": "In this work we study the theoretical and empirical properties of various global inference algorithms for multi - document summarization .We start by defining a general framework and proving that inference in it is NP - hard .We then present three algorithms : The first is a greedy approximate method , the second a dynamic programming approach based on solutions to the knapsack problem , and the third is an exact algorithm that uses an Integer Linear Programming formulation of the problem .", "label": "", "metadata": {}, "score": "64.13336"}
{"text": "In : 10th EAMT conference , practical applications of machine translation , proceedings , Budapest , Hungary , pp 103 - 111 .Gim\u00e9nez J , M\u00e0rquez L ( 2008 )A smorgasbord of features for automatic MT evaluation .In : ACL-08 : HLT third workshop on statistical machine translation , Columbus , Ohio , pp 195 - 198 .", "label": "", "metadata": {}, "score": "64.22752"}
{"text": "Tools . by Eduard Hovy , Chin - yew Lin , Liang Zhou , Junichi Fukumoto - In Proceedings of the Fifth Conference on Language Resources and Evaluation ( LREC , 2006 . \" ...As part of evaluating a summary automatically , it is usual to determine how much of the contents of one or more human - produced ' ideal ' summaries it contains .", "label": "", "metadata": {}, "score": "64.419075"}
{"text": "( 2006 ) .Church , K. and Hovy , E. ( 1993 ) \" Good Applications for Crummy Machine Translation \" .Machine Translation , 8 pp .239 - 258 .Coughlin , D. ( 2003 ) \" Correlating Automated and Human Assessments of Machine Translation Quality \" in MT Summit IX , New Orleans , USA pp .", "label": "", "metadata": {}, "score": "64.4908"}
{"text": "We start by defining a general framework and proving that inference in it is NP - hard .We then present three algorithms : The first is a greedy approximate ... \" .Abstract .In this work we study the theoretical and empirical properties of various global inference algorithms for multi - document summarization .", "label": "", "metadata": {}, "score": "64.698814"}
{"text": "International Conference on Computational Linguistics and 44th Annual Meeting of the ACL , ACL Press , 2006 , pp .433 - 440 .[ 31 ] M. Mach\u00e1\u010dek and O. Bojar , \" Results of the WMT13 Metrics Shared Task , \" Proceedings of the Eighth Workshop on Statistical Machine Transla - tion , ACL Press , 2013 , pp .", "label": "", "metadata": {}, "score": "64.7133"}
{"text": "No . 1 . L. Specia and J. Gimenez , \" Combining Confidence Estimation and Reference - based Met - rics for Segment - level MT Evaluation , \" Proceedings of The Ninth Conference of the Association for Machine Translation in the Americas , AMTA Press , 2010 .", "label": "", "metadata": {}, "score": "65.07599"}
{"text": "The measure of evaluation for metrics is correlation with human judgment .This is generally done at two levels , at the sentence level , where scores are calculated by the metric for a set of translated sentences , and then correlated against human judgment for the same sentences .", "label": "", "metadata": {}, "score": "65.07852"}
{"text": "Short Papers : pp .61 - 63 .Joseph P. Turian , Luke Shen and I. Dan Melamed .Evaluation of Machine Translation and its Evaluation .In Proceedings of MT Summit IX .New Orleans , LA .Sept. 2003 .", "label": "", "metadata": {}, "score": "65.264824"}
{"text": "The idea of quality panel evaluation was to submit translations to a panel of expert native English speakers who were professional translators and get them to evaluate them .The evaluations were done on the basis of a metric , modelled on a standard US government metric used to rate human translations .", "label": "", "metadata": {}, "score": "65.315025"}
{"text": "Then we calculate the hLEPOR score on the extracted POS sequences , i.e. , the closeness of the corresponding POS tags between hypothesis sentence and reference sentence .The final score is the combination of the two sub - scores and .", "label": "", "metadata": {}, "score": "65.38779"}
{"text": "These results support the use of heterogeneous measures in order to consolidate text evaluation results .The lexical measure BLEU has been criticized in many ways .Some drawbacks of BLEU are the lack of interpretability ( Turian et al . , 2003a ) , the fa ... . by Aaron Li - feng Han , Lidia S. Chao , Yi Lu , Liangye He , Derek F. Wong , Junwen Xing . \" ...", "label": "", "metadata": {}, "score": "65.3934"}
{"text": "Various methods for the evaluation for machine translation have been employed .This article focuses on the evaluation of the output of machine translation , rather than on performance or usability evaluation .A typical way for lay people to assess machine translation quality is to translate from a source language to a target language and back to the source language with the same engine .", "label": "", "metadata": {}, "score": "65.530556"}
{"text": "In addition , we report the ROUGE-2 and ROUGE - SU4 m .. \" ...This paper describes a method for language independent extractive summarization that relies on iterative graph - based ranking algorithms .Through evaluations performed on a single - document summarization task for English and Portuguese , we show that the method performs equally well regardless of the l ... \" .", "label": "", "metadata": {}, "score": "65.59617"}
{"text": "In addition , we report the ROUGE-2 and ROUGE - SU4 m .. \" ...This paper describes a method for language independent extractive summarization that relies on iterative graph - based ranking algorithms .Through evaluations performed on a single - document summarization task for English and Portuguese , we show that the method performs equally well regardless of the l ... \" .", "label": "", "metadata": {}, "score": "65.59617"}
{"text": "Past automated methods such as ROUGE compare using fixed word ngrams , which are not ideal for a variety of reasons .In this paper we describe ... \" .As part of evaluating a summary automatically , it is usual to determine how much of the contents of one or more human - produced ' ideal ' summaries it contains .", "label": "", "metadata": {}, "score": "65.742615"}
{"text": "References .Albrecht JS , Hwa R ( 2007a )A re - examination of machine learning approaches for sentence - level MT evaluation .In : ACL 2007 Proceedings of the 45th annual meeting of the Association for Computational Linguistics , Prague , Czech Republic , pp 880 - 887 .", "label": "", "metadata": {}, "score": "65.87184"}
{"text": "Turian , J. , Shen , L. and Melamed , I. D. ( 2003 ) \" Evaluation of Machine Translation and its Evaluation \" .Proceedings of the MT Summit IX , New Orleans , USA , 2003 pp .386 - 393 .", "label": "", "metadata": {}, "score": "65.981674"}
{"text": "The parameter is designed to adjust the weights of different n - gram performances such as unigram , bigram , trigram , four - gram , etc . , which is different with the weight assignment in BLEU where each weight is equal as 1/N. In our model , higher weight value is designed for the high level n - gram .", "label": "", "metadata": {}, "score": "66.03572"}
{"text": "The study showed that the variables were highly correlated when the human judgment was averaged per sentence .The variation among raters was small , but the researchers recommended that at the very least , three or four raters should be used .", "label": "", "metadata": {}, "score": "66.04922"}
{"text": "Figures for correlation at the sentence level are rarely reported , although Banerjee et al .( 2005 ) do give correlation figures that show that , at least for their metric , sentence level correlation is substantially worse than corpus level correlation .", "label": "", "metadata": {}, "score": "66.06322"}
{"text": "Technical report natural language engineering workshop final report .Johns Hopkins University , Baltimore .Carbonell JG , Cullingford RE , Gershman AV ( 1981 ) Steps toward knowledge - based machine translation .IEEE Trans Pattern Anal Mach Intell 3(4 ) : 376 - 392 CrossRef .", "label": "", "metadata": {}, "score": "66.2586"}
{"text": "Automatic MT evaluation consists in comparing the MT system output to one or more human reference translations .Human scores ( manual evaluation ) are assigned according to the adequacy , the fluency or the informativeness of the translated text .In automatic evaluation , the fluency and adequacy of MT output can be measured by n - gram analysis .", "label": "", "metadata": {}, "score": "66.4258"}
{"text": "In : AMTA 2006 : Proceedings of the 7th conference of the Association for Machine Translation in the Americas , visions of the future of machine translation , Cambridge , MA , pp 223 - 231 .Tillmann C , Vogel S , Ney H , Sawaf H , Zubiaga A ( 1997 )", "label": "", "metadata": {}, "score": "66.719864"}
{"text": "Details of the programme can be found in White et al .( 1994 ) and White ( 1995 ) .The evaluation programme involved testing several systems based on different theoretical approaches ; statistical , rule - based and human - assisted .", "label": "", "metadata": {}, "score": "66.723816"}
{"text": "To address this problem , they proposed the Human - targeted TER ( HTER ) to consider the semantic equivalence , which is achieved by employing human annotators to generate a new targeted reference .However , HTER is very expensive due to that it requires around 3 to 7 minutes per sentence for a human to annotate , which means that it is more like a human judgment metric instead of an automatic one . D. AMBER Metric AMBER [ 8 ] declares a modified version of BLEU .", "label": "", "metadata": {}, "score": "66.841354"}
{"text": "A survey of current paradigms in machine translation .Adv Comput 49 : 2 - 68 .Duh K ( 2008 )Ranking vs. regression in machine translation evaluation .In : Proceedings of the third workshop on statistical machine translation , Columbus , Ohio , pp 191 - 194 .", "label": "", "metadata": {}, "score": "66.99021"}
{"text": "Training a sentence - level machine translation confidence measure .In : Proceedings of the international conference on language resources and evaluation ( LREC-2004 ) , Lisbon , Portugal , pp 825 - 828 .Riezler S , Maxwell JT III ( 2005 )", "label": "", "metadata": {}, "score": "67.02294"}
{"text": "We further improve performance by combining individual evaluation metrics using maximum correlation training , which is shown to be better than the classification - based framework .Such metrics were Ding Liu and Daniel Gildea Department of Computer Science University of Rochester Rochester , NY 14627 shown to have better fluency evaluation performance than metrics based on n - g ... Machine Translation .", "label": "", "metadata": {}, "score": "67.08534"}
{"text": "However , the quality panel evaluation was very difficult to set up logistically , as it necessitated having a number of experts together in one place for a week or more , and furthermore for them to reach consensus .This method was also abandoned .", "label": "", "metadata": {}, "score": "67.096436"}
{"text": "This paper presents a novel extractive approach based on manifold - ranking of sentences to this summarization task .The manifold - ranking process can naturally make full use of both the relationships among all the sentences in the documents and the relationships between the given topic and the sentences .", "label": "", "metadata": {}, "score": "67.12331"}
{"text": "While this does not allow us to simultaneously match different portions of the translation with different references , it supports the use of recall as a component in scoring each possible match .In the second case , we stem both translation and references prior to matching and then require identity on stems .", "label": "", "metadata": {}, "score": "67.16092"}
{"text": "Springer - Verlag , New York .Blatz J , Fitzgerald E , Foster G , Gandrabur S , Goutte C , Kulesza A , Sanchis A , Ueffing N ( 2003 ) Confidence estimation for machine translation .Technical report natural language engineering workshop final report .", "label": "", "metadata": {}, "score": "67.19023"}
{"text": "1994 ) .Fluency refers to the degree to which the system output is well - formed according the target language 's grammar .Adequacy refers to the degree to which the output communicates the information present in the reference translation .", "label": "", "metadata": {}, "score": "67.305336"}
{"text": "Through evaluations performed on a single - document summarization task for English and Portuguese , we show that the method performs equally well regardless of the language .Moreover , we show how a metasummarizer relying on a layered application of techniques for single - document summarization can be turned into an effective method for multi - document summarization . by Xiaojin Zhu , Andrew B. Goldberg , Jurgen Van , Gael David Andrzejewski - Physics Laboratory - University of Washington , 2007 . \" ...", "label": "", "metadata": {}, "score": "67.556885"}
{"text": "Through evaluations performed on a single - document summarization task for English and Portuguese , we show that the method performs equally well regardless of the language .Moreover , we show how a metasummarizer relying on a layered application of techniques for single - document summarization can be turned into an effective method for multi - document summarization . by Xiaojin Zhu , Andrew B. Goldberg , Jurgen Van , Gael David Andrzejewski - Physics Laboratory - University of Washington , 2007 . \" ...", "label": "", "metadata": {}, "score": "67.556885"}
{"text": "A machine learning approach to the automatic evaluation of machine translation .In : Association for Computational Linguistics , 39th annual meeting and 10th conference of the European chapter , proceedings of the conference , Toulouse , France , pp 148 - 155 .", "label": "", "metadata": {}, "score": "67.70094"}
{"text": "Since there is no absolute translation for a given text , the challenge of the machine translation evaluation is to provide an objective and economic assessment .Given the difficulty of the task , most of the translation quality assessments were based on human judgement in the history of MT evaluation .", "label": "", "metadata": {}, "score": "67.716385"}
{"text": "Furthermore , it represents an important \" real world \" scenario where summaries are generated in order to be displayed on small screens , such as mobile devices .This global inference problem is typica ... .by Eduard Hovy , Chin - yew Lin , Liang Zhou - Proceedings of DUC-2005 , 2005 . \" ...", "label": "", "metadata": {}, "score": "67.71771"}
{"text": "Furthermore , it represents an important \" real world \" scenario where summaries are generated in order to be displayed on small screens , such as mobile devices .This global inference problem is typica ... .by Eduard Hovy , Chin - yew Lin , Liang Zhou - Proceedings of DUC-2005 , 2005 . \" ...", "label": "", "metadata": {}, "score": "67.71771"}
{"text": "White J. S. , O'Connel T. A. and O'Maraf ( 1994 ) .The arpa mt evaluation methodologies : evolution , lessons , and future approaches .In Proceedings of the First Conference of the Association for Machine Translation in the Americas , Columbia , Maryland , USA .", "label": "", "metadata": {}, "score": "67.86443"}
{"text": "The methods were ; comprehension evaluation , quality panel evaluation , and evaluation based on adequacy and fluency .Comprehension evaluation aimed to directly compare systems based on the results from multiple choice comprehension tests , as in Church et al .", "label": "", "metadata": {}, "score": "67.96939"}
{"text": "Topic - focused multi - document summarization aims to produce a summary biased to a given topic or user profile .This paper presents a novel extractive approach based on manifold - ranking of sentences to this summarization task .The manifold - ranking process can naturally make full use of both the relati ... \" .", "label": "", "metadata": {}, "score": "68.14952"}
{"text": "The training experiments on the past WMT corpora showed that the designed methods of this paper yielded promising results especially the statistical models of CRF and NB .The official results show that our CRF model achieved the highest F - score 0.8297 in binary classification of Task 2 . \" ...", "label": "", "metadata": {}, "score": "68.227356"}
{"text": "In the WMT13 shared tasks , both Pearson correlation coefficient and the Spearman rank correlation coefficient are used as the evaluation criteria .So we list the official results in Table 9 and Table 10 respectively by using Spearman rank correlation and Pearson correlation criteria .", "label": "", "metadata": {}, "score": "68.32904"}
{"text": "The judges were presented with a segment , and asked to rate it for two variables , adequacy and fluency .Adequacy is a rating of how much information is transferred between the original and the translation , and fluency is a rating of how good the English is .", "label": "", "metadata": {}, "score": "68.84929"}
{"text": "Lin C.-Y. , Cao G. , Gao J. , Nie J.-Y. An information - theoretic approach to automatic evaluation of summaries .In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics , p.463 - 470 , New York , New York .", "label": "", "metadata": {}, "score": "68.90234"}
{"text": "Firstly , the designed incomprehensive factors result in language - bias problem , which means they perform well on some special language pairs but weak on other language pairs .Secondly , they tend to use no linguistic features or too many linguistic features , of which no usage of linguistic feature draws a lot of criticism from the linguists and too many linguistic features make the model weak in repeatability .", "label": "", "metadata": {}, "score": "69.30731"}
{"text": "C. van Rijsbergen .Information Retrieval .Butterworths .London , England .2nd Edition .Deborah Coughlin .Correlating Automated and Human Assessments of Machine Translation Quality .In Proceedings of MT Summit IX .New Orleans , LA .Sept. 2003 .", "label": "", "metadata": {}, "score": "69.361046"}
{"text": "Furthermore , the automatic evaluation metrics can be used to II .It first computes the n - gram matches sentence by sentence , then adds the clipped n - gram counts for all the candidate sentences and divides by the number of candidate n - gram in the test corpus .", "label": "", "metadata": {}, "score": "69.39891"}
{"text": "Specifically , for each of the 30 DUC 2003 Task 2 documen ...Tools . by Eduard Hovy , Chin - yew Lin , Liang Zhou , Junichi Fukumoto - In Proceedings of the Fifth Conference on Language Resources and Evaluation ( LREC , 2006 . \" ...", "label": "", "metadata": {}, "score": "69.5154"}
{"text": "The macro evaluation , also called total evaluation enables comparison of the performance of two translation systems or two versions of the same system .The micro evaluation , also known as detailed evaluation seeks to assess the improvability of the translation system .", "label": "", "metadata": {}, "score": "69.591515"}
{"text": "The Significance of Recall in Automatic Metrics for MT Evaluation .DOI : 10.1007/978 - 3 - 540 - 30194 - 3_16 Conference : Machine Translation : From Real Users to Research , 6th Conference of the Association for Machine Translation in the Americas , AMTA 2004 , Washington , DC , USA , September 28-October 2 , 2004 , Proceedings .", "label": "", "metadata": {}, "score": "69.59337"}
{"text": "Paraphrasing for automatic evaluation .In : HLT - NAACL 2006 Human language technology conference of the North American chapter of the Association for Computational Linguistics , New York , NY , pp 455 - 462 .Koehn P ( 2004 ) Statistical significance tests for machine translation evaluation .", "label": "", "metadata": {}, "score": "69.72082"}
{"text": "In Proceedings of the Second International Conference on Language Resources and Evaluation ( LREC-2000 ) .Athens , Greece .pp .39 - 45 .Gregor Leusch , Nicola Ueffing and Herman Ney .String - to - String Distance Measure with Applications to Machine Translation Evaluation .", "label": "", "metadata": {}, "score": "69.79539"}
{"text": "Secondly , it is designed to combine the performances on words and POS together , and the final score is the combination of them .( 23 ) where is the harmonic mean of precision and recall as mentioned above .( 24 )", "label": "", "metadata": {}, "score": "70.22798"}
{"text": "We further improve performance by combining individual evaluation metrics using maximum correlation training , which is shown to be better than the classification - based framework .Such metrics were Ding Liu and Daniel Gildea Department of Computer Science University of Rochester Rochester , NY 14627 shown to have better fluency evaluation performance than metrics based on n - g ... . \" ...", "label": "", "metadata": {}, "score": "70.27624"}
{"text": "The metric is also includes a stemmer , which lemmatises words and matches on the lemmatised forms .The implementation of the metric is modular insofar as the algorithms that match words are implemented as modules , and new modules that implement different matching strategies may easily be added .", "label": "", "metadata": {}, "score": "70.60457"}
{"text": "Finally , we present an alternative formulation of metric training in which the features are based on comparisons against pseudo - references in order to reduce the demand on human produced resources .Our results confirm that regression is a useful approach for developing new metrics for MT evaluation at the sentence level .", "label": "", "metadata": {}, "score": "70.677444"}
{"text": "San Diego , CA . pp .128 - 132 .K.-Y. Su , M.-W. Wu , and J.-S. Chang .A New Quantitative Quality Measure for Machine Translation Systems .In Proceedings of the fifteenth International Conference on Computational Linguistics ( COLING-92 ) .", "label": "", "metadata": {}, "score": "70.99742"}
{"text": "Rich source - side context for statistical machine translation .In : ACL-08 : HLT third workshop on statistical machine translation , Columbus , Ohio , pp 9 - 17 .Goldberg Y , Elhadad M ( 2007 )SVM model tampering and anchored learning : a case study in Hebrew NP chunking .", "label": "", "metadata": {}, "score": "71.09883"}
{"text": "In this section we will introduce an enhanced version of the proposed metric , which is called as hLEPOR ( harmonic mean of enhanced Length Penalty , Precision , n - gram Position difference Penalty and Recall ) .There are two contributions of this enhanced model .", "label": "", "metadata": {}, "score": "71.10832"}
{"text": "In : HLT - NAACL 2003 : conference combining human language technology conference series and the North American chapter of the Association for Computational Linguistics series , companion volume , Edmonton , Canada , pp 61 - 63 .Owczarzak K , Groves D , Van Genabith J , Way A ( 2006 )", "label": "", "metadata": {}, "score": "71.1978"}
{"text": "We then present three algorithms : The first is a greedy approximate method , the second a dynamic programming approach based on solutions to the knapsack problem , and the third is an exact algorithm that uses an Integer Linear Programming formulation of the problem .", "label": "", "metadata": {}, "score": "71.32803"}
{"text": "In the second example , the text translated back into English is perfect , but the Portuguese translation is meaningless .While round - trip translation may be useful to generate a \" surplus of fun , \" [ 2 ] the methodology is deficient for serious study of machine translation quality .", "label": "", "metadata": {}, "score": "72.19503"}
{"text": "Somers , H. , Gaspari , F. and Ana Ni\u00f1o ( 2006 ) \" Detecting Inappropriate Use of Free Online Machine Translation by Language Students - A Special Case of Plagiarism Detection \" .Proceedings of the 11th Annual Conference of the European Association of Machine Translation , Oslo University ( Norway ) pp .", "label": "", "metadata": {}, "score": "72.37175"}
{"text": "IV .DESIGNED APPROACH To reduce the expensive reference translations provided by human labor and some external resources such as synonyms , this work employs the universal part - of - speech ( POS ) tagset containing 12 universal tags proposed by [ 27].", "label": "", "metadata": {}, "score": "72.39494"}
{"text": "It provides eight kinds of preparations on the corpus including whether the words are tokenized or not , extracting the stem , prefix and suffix on the words , and splitting the words into several parts with different ratios .Advanced version of AMBER was introduced in [ 11].", "label": "", "metadata": {}, "score": "73.23386"}
{"text": "In : Proceedings of the 5th European conference on speech communication and technology ( EuroSpeech'97 ) , Rhodes , Greece , pp 2667 - 2670 .Uchimoto K , Kotani K , Zhang Y , Isahara H ( 2007 )Automatic evaluation of machine translation based on rate of accomplishment of sub - goals .", "label": "", "metadata": {}, "score": "73.62587"}
{"text": "Lavie , A. , Sagae , K. and Jayaraman , S. ( 2004 ) \" The Significance of Recall in Automatic Metrics for MT Evaluation \" in Proceedings of AMTA 2004 , Washington DC .September 2004 .Papineni , K. , Roukos , S. , Ward , T. , and Zhu , W. J. ( 2002 ) .", "label": "", "metadata": {}, "score": "73.86772"}
{"text": "EXPERIMENTS A. Unsupervised Performances In the unsupervised MT evaluation , this paper uses the English - to - German machine translation corpora ( produced by around twenty English - to - German MT systems ) from UseReference ?Yes Yes Yes No", "label": "", "metadata": {}, "score": "74.205154"}
{"text": "The performance of simplified variant will be tested using the reference translations .VI .EVALUATING THE EVALUATION METHOD The conventional method to evaluate the quality of different automatic MT evaluation metrics is to calculate their correlation scores with human judgments .", "label": "", "metadata": {}, "score": "74.30953"}
{"text": "That is , the top items should be different from each other in order to have a broad coverage of the whole item set .Many natural language processing tasks can benefit from such diversity ranki ... \" .We introduce a novel ranking algorithm called GRASSHOPPER , which ranks items with an emphasis on diversity .", "label": "", "metadata": {}, "score": "74.317825"}
{"text": "That is , the top items should be different from each other in order to have a broad coverage of the whole item set .Many natural language processing tasks can benefit from such diversity ranki ... \" .We introduce a novel ranking algorithm called GRASSHOPPER , which ranks items with an emphasis on diversity .", "label": "", "metadata": {}, "score": "74.317825"}
{"text": "Then the greedy algorithm is employed to impose diversity penalty on each sentence .The summary is produced by choosing the sentences with both high biased information richness and high information novelty .Experiments on DUC2003 and DUC2005 are performed and the ROUGE evaluation results show that the proposed approach can significantly outperform existing approaches of the top performing systems in DUC tasks and baseline approaches . \" ... Abstract .", "label": "", "metadata": {}, "score": "74.58942"}
{"text": "Our results demonstrate that certain models capture complementary aspects of coherence and thus can be combined to improve performance . by Alon Lavie , Kenji Sagae , Shyamsundar Jayaraman - In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas ( AMTA-2004 , 2004 . \" ...", "label": "", "metadata": {}, "score": "74.59976"}
{"text": "Our results demonstrate that certain models capture complementary aspects of coherence and thus can be combined to improve performance . by Alon Lavie , Kenji Sagae , Shyamsundar Jayaraman - In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas ( AMTA-2004 , 2004 . \" ...", "label": "", "metadata": {}, "score": "74.59976"}
{"text": "Proceedings of the 1stConference of the Association for Machine Translation in the Americas .Columbia , MD pp .193 - 205 .White , J. ( 1995 ) \" Approaches to Black Box MT Evaluation \" .Proceedings of MT Summit V .", "label": "", "metadata": {}, "score": "74.65213"}
{"text": "Google scored a verb - noun collocation value of 0.75 ( 3 % higher than Bing ) with a trend estimation ranging between 0.63 and 0.85 .The results also showed that , in most cases , the Arabic translation output of both engines produced a one verb synonym which did not collocate with the different nouns in the testing data sentences .", "label": "", "metadata": {}, "score": "74.74477"}
{"text": "Liu D , Gildea D ( 2007 )Source - language features and maximum correlation training for machine translation evaluation .In : NAACL HLT 2007 Human language technologies 2007 : the conference of the North American chapter of the Association for Computational Linguistics , Rochester , NY , pp 41 - 48 .", "label": "", "metadata": {}, "score": "75.30992"}
{"text": "New Orleans , LA .Sept. 2003 .pp .240 - 247 .I. Dan Melamed , R. Green and J. Turian .Precision and Recall of Machine Translation .In Proceedings of HLT - NAACL 2003 .Edmonton , Canada .", "label": "", "metadata": {}, "score": "75.426056"}
{"text": "Intelligibility was measured without reference to the original , while fidelity was measured indirectly .The translated sentence was presented , and after reading it and absorbing the content , the original sentence was presented .The judges were asked to rate the original sentence on informativeness .", "label": "", "metadata": {}, "score": "75.46596"}
{"text": "In : ACL 2007 Proceedings of the 45th annual meeting of the Association for Computational Linguistics , Prague , Czech Republic , pp 296 - 303 .Al - Onaizan Y , Curin J , Jahr M , Knight K , Lafferty J , Melamed ID , Och F - J , Purdy D , Smith NA , Yarowsky D ( 1999 ) Statistical machine translation .", "label": "", "metadata": {}, "score": "75.48244"}
{"text": "pp .433- 439 . Y. Akiba , K. Imamura , and E. Sumita .Using Multiple Edit Distances to Automatically Rank Machine Translation Output .In Proceedings of MT Summit VIII .Santiago de Compostela , Spain .pp .15 - 20 . S. Niessen , F. J. Och , G. Leusch , and H. Ney .", "label": "", "metadata": {}, "score": "75.685524"}
{"text": "Lita LV , Rogati M , Lavie A ( 2005 ) Blanc : learning evaluation metrics for MT .In : HLT / EMNLP 2005 Human language technology conference and conference on empirical methods in natural language processing , Vancouver , British Columbia , Canada , pp 740 - 747 .", "label": "", "metadata": {}, "score": "75.70155"}
{"text": "Critical study of methods for evaluating the quality of machine translation .Rapport technique Final report BR 19142 , Brussels : Bureau Marcel van Dijk .Abstract .Machine learning offers a systematic framework for developing metrics that use multiple criteria to assess the quality of machine translation ( MT ) .", "label": "", "metadata": {}, "score": "77.02864"}
{"text": "CDER : efficient MT evaluation using block movements .In : EACL-2006 , 11th conference of the European chapter of the Association for Computational Linguistics , proceedings of the conference , Trento , Italy , pp 241 - 248 .Lin C - Y , Och FJ ( 2004a )", "label": "", "metadata": {}, "score": "77.04041"}
{"text": "Fidelity was a measure of how much information the translated sentence retained compared to the original , and was measured on a scale of 0 - 9 .Each point on the scale was associated with a textual description .For example , 3 on the intelligibility scale was described as \" Generally unintelligible ; it tends to read like nonsense but , with a considerable amount of reflection and study , one can at least hypothesize the idea intended by the sentence \" .", "label": "", "metadata": {}, "score": "77.239395"}
{"text": "After all of the complementary mapping , the universal POS tagset alignment for German Negra Treebank is shown in Table 1 with the boldface POS as the added ones .B. Calculation Algorithms The designed calculation algorithms of this paper are LEPOR series .", "label": "", "metadata": {}, "score": "77.27734"}
{"text": "The texts chosen were a set of articles in English on the subject of financial news .These articles were translated by professional translators into a series of language pairs , and then translated back into English using the machine translation systems .", "label": "", "metadata": {}, "score": "77.36026"}
{"text": "The document - level \u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305 is calculated as the arithmetic mean value of each sentence - level score in the document .On the other hand , the document - level \u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305 is calculated as the product of three document - level variables , and the document - level variable value is the corresponding arithmetic mean score of each sentence .", "label": "", "metadata": {}, "score": "77.524475"}
{"text": "One of the most difficult things in Machine Translation is the evaluation of a proposed system .The problem with language is that language has some degree of ambiguity which makes it hard to run an objective evaluation .For example , with Machine Translation one problem is that there is not only one good translation for a given source text .", "label": "", "metadata": {}, "score": "78.301926"}
{"text": "[ 13 ] An enhanced version of LEPOR metric , hLEPOR , is introduced in the paper .[14 ] hLEPOR utilizes the harmonic mean to combine the sub - factors of the designed metric .Furthermore , they design a set of parameters to tune the weights of the sub - factors according to different language pairs .", "label": "", "metadata": {}, "score": "78.35878"}
{"text": "63 - 70 .Bradley Efron and Robert Tibshirani .Bootstrap Methods for Standard Er- rors , Confidence Intervals , and Other Measures of Statistical Accuracy .Statistical Science , 1(1 ) .pp .54 - 77 .George Doddington .", "label": "", "metadata": {}, "score": "78.53436"}
{"text": "Many natural language processing tasks can benefit from such diversity ranking .Our algorithm is based on random walks in an absorbing Markov chain .We turn ranked items into absorbing states , which effectively prevents redundant items from receiving a high rank .", "label": "", "metadata": {}, "score": "78.5573"}
{"text": "Many natural language processing tasks can benefit from such diversity ranking .Our algorithm is based on random walks in an absorbing Markov chain .We turn ranked items into absorbing states , which effectively prevents redundant items from receiving a high rank .", "label": "", "metadata": {}, "score": "78.5573"}
{"text": "The ranking score is obtained for each sentence in the manifold - ranking process to denote the biased information richness of the sentence .Then the greedy algorithm is employed to impose diversity penalty on each sentence .The summary is produced by choosing the sentences with both high biased information richness and high information novelty .", "label": "", "metadata": {}, "score": "78.952255"}
{"text": "KOUS PTKA VAFIN FM $ ( NN PDS PTKANT VAIMP ITJ $ , NNE PIAT PTKNEG VAINF TRUNC $ .The issues between the traditional supervised MT evaluations and the latest unsupervised MT evaluations are discussed in the work of [ 21].", "label": "", "metadata": {}, "score": "79.60575"}
{"text": "Unsupervised Quality Estimation Model for English to German Translation and Its Application in Extensive Supervised Evaluation .Unsupervised Quality Estimation Model for English to German Translation and Its Application in Extensive Supervised Evaluation Aaron L.-F. The conventional MT evaluation methods tend to calculate the similarity between hypothesis translations offered by automatic translation systems and reference translations offered by professional translators .", "label": "", "metadata": {}, "score": "79.73682"}
{"text": "Joachims T ( 1999 )Making large - scale SVM learning practical .In : Sch\u00f6elkopf B , Burges C , Smola A(eds ) Advances in kernel methods - support vector learning .MIT Press , Cambridge , pp 169 - 184 .", "label": "", "metadata": {}, "score": "80.00682"}
{"text": "It is generally defined by the syntactic or morphological behavior of the lexical item in question .For a simple example , \" there is a big bag \" and \" there is a large bag \" could be the same expression , \" big \" and \" large \" having the same POS as adjective .", "label": "", "metadata": {}, "score": "80.38592"}
{"text": "They serve to illustrate the effectiveness of stemming in MT evaluation .Page 10 .Acknowledgments This research was funded in part by NSF grant number IIS-0121631 .References 1 .Papineni , Kishore , Salim Roukos , Todd Ward , and Wei - Jing Zhu .", "label": "", "metadata": {}, "score": "82.24313"}
{"text": "The proposed method can be exploited in a variety of applications involving textual knowledge representation and knowledge discovery .Evaluation results on two different data sets show that our method outperforms several competing methods . by Mirella Lapata - In the Intl .", "label": "", "metadata": {}, "score": "83.20915"}
{"text": "The proposed method can be exploited in a variety of applications involving textual knowledge representation and knowledge discovery .Evaluation results on two different data sets show that our method outperforms several competing methods . by Mirella Lapata - In the Intl .", "label": "", "metadata": {}, "score": "83.20915"}
{"text": "So \" PWS \" is classified into the PRON ( pronoun ) category in the universal POS tags .The German POS \" PRF \" has a similar function with English POS \" RP \" and \" TO \" , which means Particle and to respectively , labeling the German word sich ( itself ) .", "label": "", "metadata": {}, "score": "83.67182"}
{"text": "Generally , is selected as 4 , and uniform weight is assigned as .It puts more weight on recall ( R ) compared with precision ( P ) .The matching process involves computationally expensive word alignment due to the external tools for stemming or synonym matches .", "label": "", "metadata": {}, "score": "83.90971"}
{"text": "APPENDIX This appendix offers the POS tags and their occurrences number in one of WMT2012 German document containing 3,003 sentences , parsed by Berkeley parser for German language that is trained on German Negra Treebank .Number of different POS tags in the document is : 55 .", "label": "", "metadata": {}, "score": "83.95137"}
{"text": "VIII .DISCUSSION The Spearman rank correlation coefficient is commonly used in the WMT shared tasks as the special case of Pearson correlation coefficient applied to ranks .However , there is some information that can not be reflected by using Spearman rank correlation coefficient instead of Pearson correlation coefficient .", "label": "", "metadata": {}, "score": "84.31834"}
{"text": "Employing the confidence estimation features and a learning mechanism trained on human .TABLE I. COMPLEMENTARY GERMAN POS MAPPINGS FOR UNIVERSAL POS TAGSET Language ADJ ADP ADV CONJ DET NOUN NUM PRON PRT VERB X .English JJ IN RB CC DT NN CD PRP POS MD FW # Penn Treebank JJR RBR EX NNP PRP$ RP VB LS $ [ 28 ] JJS RBS PDT NNPS WP TO VBD SYM \" WRB WDT NNS WP$ VBG UH .", "label": "", "metadata": {}, "score": "84.37175"}
{"text": "Impersonations , Chinese Whispers and Fun with Machine Translation on the Internet \" in Proceedings of the 11th Annual Conference of the European Association of Machine Translation .Graham , Y. and T. Baldwin .( 2014 ) \" Testing for Significance of Increased Correlation with Human Judgment \" .", "label": "", "metadata": {}, "score": "84.60664"}
{"text": "Secondly , the Pearson correlation coefficient information is introduced as below .Given a sample of paired data ( X , Y ) as , , the Pearson correlation coefficient is : ( 22 ) ( ) where and specify the arithmetical means of discrete random variable X and Y respectively .", "label": "", "metadata": {}, "score": "84.65608"}
{"text": "Specifically , for each of the 30 DUC 2003 Task 2 documen ...", "label": "", "metadata": {}, "score": "87.713455"}
{"text": "y assigning more weight to recall than to precision .In fact , our experiments show that the best correlations are achieved when recall is assigned almost all the weight .Our results show that this is also the case for evaluation of MT .", "label": "", "metadata": {}, "score": "87.73478"}
{"text": "y assigning more weight to recall than to precision .In fact , our experiments show that the best correlations are achieved when recall is assigned almost all the weight .Our results show that this is also the case for evaluation of MT .", "label": "", "metadata": {}, "score": "87.73478"}
{"text": "Selezioni questo collegamento per guardare il nostro Home Page .Translated back .Selections this connection in order to watch our Home Page .Original text .Tit for tat .Translated .Melharuco para o tat .Translated back .Tit for tat .", "label": "", "metadata": {}, "score": "87.791435"}
{"text": "So the value of and equals 3/4 and 3/5 respectively .( 13 ) ( 17 ) ( 14 )The parameter means the length of hypothesis sentence , and as the matched POS position number in hypothesis and source sentence respectively .", "label": "", "metadata": {}, "score": "88.042114"}
{"text": "So , firstly this paper conducts a complementary mapping work for German Negra POS tagset and extends the mapped POS tags to 57 tags .This paper classifies the omissive German POS tags according to the English POS tagset classification since that the English PennTreebank 45 POS tags are completely mapped by the universal POS tagset , as shown in Table 1 .", "label": "", "metadata": {}, "score": "88.3582"}
{"text": "ADV ( adverb ) category in the 12 universal POS tags .The German POS \" PIDAT \" has a similar function with English POS \" PDT \" which means Predeterminer labeling the German word jedem ( each ) , beide ( both ) , meisten ( most ) , etc .", "label": "", "metadata": {}, "score": "88.869354"}
{"text": "The source language is used as pseudo reference .We will also test this method by calculating the correlation coefficient of this approach with the human judgments in the experiment .Petrov et al .[ 27 ] describe that the English PennTreebank [ 28 ] has 45 tags and German Negra [ 29 ] has 54 tags .", "label": "", "metadata": {}, "score": "89.46987"}
{"text": "TABLE III .Yes Yes Yes No No 0.25 0.22 0.18 0.34 0.33 Testing result of the proposed evaluation approaches on the WMT2012 corpora is shown in Table 3 with the same parameter values obtained as in training , also compared with the state - of - the - art evaluation metrics .", "label": "", "metadata": {}, "score": "89.70034"}
{"text": "Edmonton , Canada .May 2003 .pp .102 - 109 .Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .", "label": "", "metadata": {}, "score": "92.32838"}
{"text": "ALPAC ( 1966 ) \" Languages and machines : computers in translation and linguistics \" .A report by the Automatic Language Processing Advisory Committee , Division of Behavioral Sciences , National Academy of Sciences , National Research Council .Washington , D.C. : National Academy of Sciences , National Research Council , 1966 .", "label": "", "metadata": {}, "score": "93.43624"}
{"text": "In :ACL-04 : 42nd annual meeting of the Association for Computational Linguistics , Barcelona , Spain , pp 605 - 612 .Lin C - Y , Och FJ ( 2004b )ORANGE : a method for evaluating automatic evaluation metrics for machine translation .", "label": "", "metadata": {}, "score": "93.81135"}
{"text": "The German POS \" PROAV \" has a similar function with English POS \" RB \" which means Adverb labeling the German word Dadurch ( thereby ) , dabei ( there ) , etc .So this paper classifies \" PWAV \" and \" PROAV \" into the .", "label": "", "metadata": {}, "score": "94.49623"}
{"text": "Secondly , the experiments using multireferences will be considered .Thirdly , how to handle the MT evaluation from the aspect of semantic similarity will be further explored .X. POS POS Frequency 213 PIS 824 $ , 4590 PPER 1827 $ .", "label": "", "metadata": {}, "score": "98.777176"}
