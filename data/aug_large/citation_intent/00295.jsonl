{"text": "B. Vlez , R. Wiess , M. Sheldon , and D. Gifford .Fast and effective query refinement .In Proc . of 20th Annual International ACM - SIGIR Conference on Research and Development in Information Retrieval , pages 6 - 15 , 1997 .", "label": "", "metadata": {}, "score": "83.31042"}
{"text": "We compute precision , recall and -score for each name in the data set and average the results over the dataset .For each person in our data set , let us denote the cluster that belongs to by .Moreover , we use to denote the affiliation of person , e.g. , ' ' Tiger Woods \" ' ' Tennis Player \" .", "label": "", "metadata": {}, "score": "87.103424"}
{"text": "Resnik and Smith [ 33 ] extracted bilingual sentences from the Web to create a parallel corpora for machine translation .Turney [ 38 ] defined a point - wise mutual information ( PMI - IR ) measure using the number of hits returned by a Web search engine to recognize synonyms .", "label": "", "metadata": {}, "score": "89.088135"}
{"text": "The word pairs are rated on a scale from ( no similarity ) to ( perfect synonymy ) .Miller - Charles ' data set is a subset of Rubenstein - Goodenough 's [ 35 ] original data set of word pairs .", "label": "", "metadata": {}, "score": "89.76087"}
{"text": "( 2008 ) and Blunsom et al .( 2008 ) use a translation hypergraph to represent search space .The difference is that their hypergraphs are specificall ... . by Matthias Huck , Stephan Peitz , Markus Freitag , Hermann Ney - In Proceedings of the 16th Annual Conference of the European Association for Machine Translation ( EAMT , 2012 . \" ...", "label": "", "metadata": {}, "score": "90.328384"}
{"text": "Therein , denotes the conjunction query P AND Q .Given the scale and noise in Web data , it is possible that two words may appear on some pages purely accidentally .In order to reduce the adverse effects attributable to random co - occurrences , we set the WebJaccard coefficient to zero if the page count for the query is less than a threshold 5 .", "label": "", "metadata": {}, "score": "90.46677"}
{"text": "Therefore , no guarantee exists that all the information we need to measure semantic similarity between a given pair of words is contained in the top - ranking snippets .This paper proposes a method that considers both page counts and lexico - syntactic patterns extracted from snippets , thereby overcoming the problems described above .", "label": "", "metadata": {}, "score": "91.24246"}
{"text": "They did not evaluate their method in terms of similarities among named entities .Lin [ 17 ] defined the similarity between two concepts as the information that is in common to both concepts and the information contained in each individual concept .", "label": "", "metadata": {}, "score": "93.83227"}
{"text": "Finally , function CountFreq counts the frequency of each pattern we extracted .The procedure described above yields a set of patterns with their frequencies in text snippets obtained from a search engine .It considers the words that fall between and as well as words that precede and succeeds .", "label": "", "metadata": {}, "score": "94.28742"}
{"text": "For that purpose , we employ the following procedure .First , we run the pattern extraction algorithm described in Figure 3 with a non - synonymous set of word - pairs and count the frequency of the extracted patterns .We then use a test of statistical significance to evaluate the probable applicability of a pattern as an indicator of synonymy .", "label": "", "metadata": {}, "score": "94.72558"}
{"text": "Here , is the set of names selected from the open directory project .Therefore , in our evaluations .Experimental results are shown in Table 6 .The proposed method shows the highest entity clustering accuracy in Table 6 with a statistically significant ( Tukey HSD ) F score of .", "label": "", "metadata": {}, "score": "94.84996"}
{"text": "Matsuo et al . , [ 19 ] proposed the use of Web hits for extracting communities on the Web .They measured the association between two personal names using the overlap ( Simpson ) coefficient , which is calculated based on the number of Web hits for each individual name and their conjunction ( i.e. , AND query of the two names ) .", "label": "", "metadata": {}, "score": "95.08307"}
{"text": "Therefore , the proposed method can be applied in many tasks where such taxonomies do not exist or are not up - to - date .We employed the proposed method in community mining and entity disambiguation experiments .Results of our experiments indicate that the proposed method can robustly capture semantic similarity between named entities .", "label": "", "metadata": {}, "score": "95.36339"}
{"text": "Considering the number of new senses constantly being associated to the existing words on the Web , it is costly , if not impossible to maintain sense tagged dictionaries to cover all senses .Contextual Hypothesis for Sense [ 37 ] states that the context in which a word appears can be used to determine its sense .", "label": "", "metadata": {}, "score": "95.51865"}
{"text": "The system configuration was identical for all language pairs ( with a few additional components for the German - English language pairs ) .This paper describes the configuration of the systems , plus novel contributions to Moses including truecasing , more efficient decoding methods , and a framework to specify reordering constraints . ... discarding ( 252ms / word for 62.5 % seach accuracy ) .", "label": "", "metadata": {}, "score": "95.647285"}
{"text": "Li et al ., [ 41 ] combined structural semantic information from a lexical taxonomy and information content from a corpus in a nonlinear model .They proposed a similarity measure that uses shortest path length , depth and local density in a taxonomy .", "label": "", "metadata": {}, "score": "95.86978"}
{"text": "This observation is confirmed by the experimental results in their paper which reports zero similarity scores for many pairs of words in the Miller and Charles [ 24 ] dataset .3 Method . 1 Outline .We propose a method which integrates both page counts and snippets to measure semantic similarity between a given pair of words .", "label": "", "metadata": {}, "score": "96.17456"}
{"text": "This page presents a list of the knowledge resources used by systems that have participated in the last RTE challenges .The first table lists the publicly available resources , the second one lists unpublished resources .Both tables are sortable by Resource name , type , author and number of users .", "label": "", "metadata": {}, "score": "96.46388"}
{"text": "To balance performance and speed , we prune the search space in several ways .First , beam thresh222old \u03b2 , items with a score w .. \" ...We present a machine translation framework that can incorporate arbitrary features of both input and output sentences .", "label": "", "metadata": {}, "score": "96.96974"}
{"text": "For polysemous nouns we selected the synonyms for the dominant sense .The pattern extraction algorithm described in Figure 3 yields unique patterns .Of those patterns , occur less than times .It is impossible to train a classifier with such numerous sparse patterns .", "label": "", "metadata": {}, "score": "97.008026"}
{"text": "Manually compiled taxonomies such as WordNet 3 and large text corpora have been used in previous works on semantic similarity [ 16 , 31 , 13 , 17 ] .Regarding the Web as a live corpus has become an active research topic recently .", "label": "", "metadata": {}, "score": "97.037186"}
{"text": "To create a set of non - synonymous word - pairs , we select two nouns from WordNet arbitrarily .If the selected two nouns do not appear in any WordNet synset then we select them as a non - synonymous word - pair .", "label": "", "metadata": {}, "score": "97.04953"}
{"text": "We propose a robust semantic similarity measure that uses the information available on the Web to measure similarity between words or entities .The proposed method exploits page counts and text snippets returned by a Web search engine .We define various similarity scores for two given words P and Q , using the page counts for the queries P , Q and P AND Q .", "label": "", "metadata": {}, "score": "97.33986"}
{"text": "In subsequent iterations , group average agglomerative clustering process , merges the two clusters with highest correlation .Correlation , between two clusters and is defined as the following , .Here , is the merger of the two clusters and .", "label": "", "metadata": {}, "score": "97.392365"}
{"text": "He defined the similarity between two concepts and in the taxonomy as the maximum of the information content of all concepts that subsume both and .Then the similarity between two words is defined as the maximum of the similarity between any concepts that the words belong to .", "label": "", "metadata": {}, "score": "97.54094"}
{"text": "To the best of our knowledge , this is the first attempt to combine both WordNet synsets and Web content to leverage a robust semantic similarity measure .The remainder of the paper is organized as follows .In section 2 we discuss previous works related to semantic similarity measures .", "label": "", "metadata": {}, "score": "97.595894"}
{"text": "Section 4 compares the proposed method against previous Web - based semantic similarity measures and several baselines on a benchmark data set .In order to evaluate the ability of the proposed method in capturing semantic similarity between real - world entities , we apply it in a community mining task .", "label": "", "metadata": {}, "score": "98.01712"}
{"text": "The probability of finding better patterns increases with the number of processed snippets .That fact enables us to represent each pair of words with a rich feature vector , resulting in better performance .We used synonymous word pairs extracted from WordNet synsets as positive training examples and automatically generated non - synonymous word pairs as negative training examples to train a two - class support vector machine in section 3.4 .", "label": "", "metadata": {}, "score": "98.08374"}
{"text": "Proposed method reports the best results among all the baselines compared in Table 7 .However , the experiment needs to be carried out on a much larger data set of ambiguous entities in order to obtain any statistical guarantees .In this paper , we proposed a measure that uses both page counts and snippets to robustly calculate semantic similarity between two given words or named entities .", "label": "", "metadata": {}, "score": "98.84423"}
{"text": "We modify four popular co - occurrence measures ; Jaccard , Overlap ( Simpson ) , Dice , and PMI ( Point - wise mutual information ) , to compute semantic similarity using page counts .For the remainder of this paper we use the notation to denote the page counts for the query in a search engine .", "label": "", "metadata": {}, "score": "98.94"}
{"text": "It contains more than 2 million entities and 20 million facts about these entities .The facts have been automatically extracted from Wikipedia and unified with WordNet .DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web .", "label": "", "metadata": {}, "score": "99.14707"}
{"text": "Disambiguating personal names on the web using automatically extracted key phrases .In Proc . of the 17th European Conference on Artificial Intelligence , pages 553 - 557 , 2006 .Y. Matsuo , J. Mori , M. Hamasaki , K. Ishida , T. Nishimura , H. Takeda , K. Hasida , and M. Ishizuka .", "label": "", "metadata": {}, "score": "99.362305"}
{"text": "al , [ 20 ] used a similar approach to measure the similarity between words and apply their method in a graph - based word clustering algorithm .Given a taxonomy of concepts , a straightforward method to calculate similarity between two words ( concepts ) is to find the length of the shortest path connecting the two words in the taxonomy [ 30 ] .", "label": "", "metadata": {}, "score": "99.4996"}
{"text": "Unlike the WordNet based methods , proposed method requires no a hierarchical taxonomy of concepts or sense - tagged definitions of words .Therefore , in principle the proposed method could be used to calculate semantic similarity between named entities , etc , which are not listed in WordNet or other manually compiled thesauri .", "label": "", "metadata": {}, "score": "99.57181"}
{"text": "We compare different feature sets for the discriminative reordering model and investigate combinations with three types of non - lexicalized reordering rules which are ad ... \" .In this paper , we propose novel extensions of hierarchical phrase - based systems with a discriminative lexicalized reordering model .", "label": "", "metadata": {}, "score": "99.78003"}
{"text": "The latter were taken mainly from VerbNet .Extends a resource described in ( Bar - Haim et al . , AAAI-07 ) Knowledge resources have shown their relevance for applied semantic inference , and are extensively used by applied inference systems , such as those developed within the Textual Entailment framework .", "label": "", "metadata": {}, "score": "99.806694"}
{"text": "Large - scale experiments show that our model performs well on long distance reordering , and outperforms the stateof - the - art constituency - to - string model ( +1.47 BLEU on average ) and hierarchical phrasebased model ( +0.46 BLEU on average ) on two Chinese - English NIST test sets without resort to phrases or parse forest .", "label": "", "metadata": {}, "score": "99.9193"}
{"text": "For each pair of names in our data set , we measure their similarity using the proposed method and baselines .We use group - average agglomerative hierarchical clustering ( GAAC ) to cluster the names in our dataset into five clusters .", "label": "", "metadata": {}, "score": "100.34587"}
{"text": "Moreover , given the scale and noise in the Web , some words might occur arbitrarily , i.e. by random chance , on some pages .For those reasons , page counts alone are unreliable when measuring semantic similarity .Snippets , a brief window of text extracted by a search engine around the query term in a document , provide useful information regarding the local context of the query term .", "label": "", "metadata": {}, "score": "100.442215"}
{"text": "We select synonymous word - pairs ( positive training examples ) from WordNet synsets 4 .Non - synonymous word - pairs ( negative training examples ) are automatically created using a random shuffling technique .We convert the output of SVM into a posterior probability .", "label": "", "metadata": {}, "score": "100.453094"}
{"text": "Knowledge resources have shown their relevance for applied semantic inference , and are extensively used by applied inference systems , such as those developed within the Textual Entailment framework .This page presents a list of the knowledge resources used by systems that have participated in the last RTE challenges .", "label": "", "metadata": {}, "score": "100.46184"}
{"text": "Improving automatic query expansion .In Proc . of 21st Annual International ACM - SIGIR Conference on Research and Development in Information Retrieval , pages 206 - 214 , 1998 .P. Resnik .Semantic similarity in a taxonomy : An information based measure and its application to problems of ambiguity in natural language .", "label": "", "metadata": {}, "score": "100.68622"}
{"text": "First , page count analyses ignore the position of a word in a page .Therefore , even though two words appear in a page , they might not be related .Secondly , page count of a polysemous word ( a word with multiple senses ) might contain a combination of all its senses .", "label": "", "metadata": {}, "score": "100.79128"}
{"text": "Interested people may turn to authors to obtain further information .Ontology containing geographic terms and two kinds of relations : the directional part - of relation , and the equal relation for synonyms and abbreviations of the same geographic area ( e.g the United Kingdom , the UK , Great Britain , etc . ) .", "label": "", "metadata": {}, "score": "101.187096"}
{"text": "Interested people may turn to authors to obtain further information .Ontology containing geographic terms and two kinds of relations : the directional part - of relation , and the equal relation for synonyms and abbreviations of the same geographic area ( e.g the United Kingdom , the UK , Great Britain , etc . ) .", "label": "", "metadata": {}, "score": "101.187096"}
{"text": "Interested people may turn to authors to obtain further information .Ontology containing geographic terms and two kinds of relations : the directional part - of relation , and the equal relation for synonyms and abbreviations of the same geographic area ( e.g the United Kingdom , the UK , Great Britain , etc . ) .", "label": "", "metadata": {}, "score": "101.187096"}
{"text": "Interested people may turn to authors to obtain further information .Ontology containing geographic terms and two kinds of relations : the directional part - of relation , and the equal relation for synonyms and abbreviations of the same geographic area ( e.g the United Kingdom , the UK , Great Britain , etc . ) .", "label": "", "metadata": {}, "score": "101.187096"}
{"text": "Interested people may turn to authors to obtain further information .Ontology containing geographic terms and two kinds of relations : the directional part - of relation , and the equal relation for synonyms and abbreviations of the same geographic area ( e.g the United Kingdom , the UK , Great Britain , etc . ) .", "label": "", "metadata": {}, "score": "101.187096"}
{"text": "Light - weight and extensible ontology .It contains more than 2 million entities and 20 million facts about these entities .The facts have been automatically extracted from Wikipedia and unified with WordNet .DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web .", "label": "", "metadata": {}, "score": "101.19957"}
{"text": "Light - weight and extensible ontology .It contains more than 2 million entities and 20 million facts about these entities .The facts have been automatically extracted from Wikipedia and unified with WordNet .DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web .", "label": "", "metadata": {}, "score": "101.19957"}
{"text": "Light - weight and extensible ontology .It contains more than 2 million entities and 20 million facts about these entities .The facts have been automatically extracted from Wikipedia and unified with WordNet .DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web .", "label": "", "metadata": {}, "score": "101.19957"}
{"text": "Light - weight and extensible ontology .It contains more than 2 million entities and 20 million facts about these entities .The facts have been automatically extracted from Wikipedia and unified with WordNet .DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web .", "label": "", "metadata": {}, "score": "101.19957"}
{"text": "Light - weight and extensible ontology .It contains more than 2 million entities and 20 million facts about these entities .The facts have been automatically extracted from Wikipedia and unified with WordNet .DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web .", "label": "", "metadata": {}, "score": "101.19957"}
{"text": "Light - weight and extensible ontology .It contains more than 2 million entities and 20 million facts about these entities .The facts have been automatically extracted from Wikipedia and unified with WordNet .DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web .", "label": "", "metadata": {}, "score": "101.19957"}
{"text": "Light - weight and extensible ontology .It contains more than 2 million entities and 20 million facts about these entities .The facts have been automatically extracted from Wikipedia and unified with WordNet .DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web .", "label": "", "metadata": {}, "score": "101.19957"}
{"text": "Results are shown in Table 4 .All figures , except those for the Miller - Charles ratings , are normalized into values in range for ease of comparison .Pearson 's correlation coefficient is invariant against a linear transformation .Proposed method earns the highest correlation of in our experiments .", "label": "", "metadata": {}, "score": "101.216194"}
{"text": "It is inspired by cube pruning ( Chiang , 2007 ; Huang and Chiang , 2007 ) in its computation of non - local features dynamically using scored k - best lists , but also maintains additional residual quantities used in calculating approximate marginals .", "label": "", "metadata": {}, "score": "101.32717"}
{"text": "Probabilities in Eq .4 are estimated according to the maximum likelihood principle .To calculate PMI accurately using Eq .4 , we must know , the number of documents indexed by the search engine .Although estimating the number of documents indexed by a search engine [ 2 ] is an interesting task itself , it is beyond the scope of this work .", "label": "", "metadata": {}, "score": "101.33453"}
{"text": "We use sigmoid functions to convert this uncalibrated distance into a calibrated posterior probability ( see [ 29 ] for a detailed discussion on this topic ) .4 Experiments .We conduct two sets of experiments to evaluate the proposed semantic similarity measure .", "label": "", "metadata": {}, "score": "101.49068"}
{"text": "Then they count the occurrences of word in the snippets for word and the occurrences of word in the snippets for word .These values are combined nonlinearly to compute the similarity between and .This method depends heavily on the search engine 's ranking algorithm .", "label": "", "metadata": {}, "score": "101.55391"}
{"text": "Replacing the query words by wildcards X and Y we can form the pattern X is a Y from the example given above .However , in some cases the words that indicate the semantic relationship do not fall between the query words .", "label": "", "metadata": {}, "score": "101.72586"}
{"text": "This is a resource of directional distributional term - similarity rules ( mostly lexical entailment rules ) automatically extracted using the inclusion relation as described in ( Kotlerman et.al . , ACL-09 ) .The following table lists the unpublished resources used by RTE participants .", "label": "", "metadata": {}, "score": "102.583786"}
{"text": "This is a resource of directional distributional term - similarity rules ( mostly lexical entailment rules ) automatically extracted using the inclusion relation as described in ( Kotlerman et.al . , ACL-09 ) .The following table lists the unpublished resources used by RTE participants .", "label": "", "metadata": {}, "score": "102.583786"}
{"text": "This is a resource of directional distributional term - similarity rules ( mostly lexical entailment rules ) automatically extracted using the inclusion relation as described in ( Kotlerman et.al . , ACL-09 ) .The following table lists the unpublished resources used by RTE participants .", "label": "", "metadata": {}, "score": "102.583786"}
{"text": "This is a resource of directional distributional term - similarity rules ( mostly lexical entailment rules ) automatically extracted using the inclusion relation as described in ( Kotlerman et.al . , ACL-09 ) .The following table lists the unpublished resources used by RTE participants .", "label": "", "metadata": {}, "score": "102.583786"}
{"text": "This is a resource of directional distributional term - similarity rules ( mostly lexical entailment rules ) automatically extracted using the inclusion relation as described in ( Kotlerman et.al . , ACL-09 ) .The following table lists the unpublished resources used by RTE participants .", "label": "", "metadata": {}, "score": "102.583786"}
{"text": "Co- occurrence of the word pairs in RTE3 and RTE4 using Normalized Google Distance ( Cilibrasi and Vitanyi , 2004 ) .The word pairs are all the possible combinations of content words in T and H. In practice , we used Yahoo ! as the search engine .", "label": "", "metadata": {}, "score": "102.77595"}
{"text": "Co- occurrence of the word pairs in RTE3 and RTE4 using Normalized Google Distance ( Cilibrasi and Vitanyi , 2004 ) .The word pairs are all the possible combinations of content words in T and H. In practice , we used Yahoo ! as the search engine .", "label": "", "metadata": {}, "score": "102.77595"}
{"text": "Co- occurrence of the word pairs in RTE3 and RTE4 using Normalized Google Distance ( Cilibrasi and Vitanyi , 2004 ) .The word pairs are all the possible combinations of content words in T and H. In practice , we used Yahoo ! as the search engine .", "label": "", "metadata": {}, "score": "102.77595"}
{"text": "Co- occurrence of the word pairs in RTE3 and RTE4 using Normalized Google Distance ( Cilibrasi and Vitanyi , 2004 ) .The word pairs are all the possible combinations of content words in T and H. In practice , we used Yahoo ! as the search engine .", "label": "", "metadata": {}, "score": "102.77595"}
{"text": "Co- occurrence of the word pairs in RTE3 and RTE4 using Normalized Google Distance ( Cilibrasi and Vitanyi , 2004 ) .The word pairs are all the possible combinations of content words in T and H. In practice , we used Yahoo ! as the search engine .", "label": "", "metadata": {}, "score": "102.77595"}
{"text": "Co- occurrence of the word pairs in RTE3 and RTE4 using Normalized Google Distance ( Cilibrasi and Vitanyi , 2004 ) .The word pairs are all the possible combinations of content words in T and H. In practice , we used Yahoo ! as the search engine .", "label": "", "metadata": {}, "score": "102.77595"}
{"text": "Co- occurrence of the word pairs in RTE3 and RTE4 using Normalized Google Distance ( Cilibrasi and Vitanyi , 2004 ) .The word pairs are all the possible combinations of content words in T and H. In practice , we used Yahoo ! as the search engine .", "label": "", "metadata": {}, "score": "102.77595"}
{"text": "Table 5 presents a comparison of the proposed method to the WordNet - based methods .The proposed method outperforms simple WordNet - based approaches such as Edge - counting and Information Content measures .It is comparable with Lin ( 1998 ) [", "label": "", "metadata": {}, "score": "102.83516"}
{"text": "The resource is the result of the application of a learning algorithm for inducing semantic taxonomies from parsed text .The algorithm automatically acquires items of world knowledge , and uses these to produce significantly enhanced versions of WordNet ( up to 40,000 synsets more ) .", "label": "", "metadata": {}, "score": "102.8958"}
{"text": "4 Integrating Patterns and Page Counts .In section 3.2 we defined four similarity scores using page counts .Section 3.3 described a lexico - syntactic pattern extraction algorithm and ranked the patterns according to their ability to express synonymy .In this section we describe leverage of a robust semantic similarity measure through integration of all the similarity scores and patterns described in previous sections .", "label": "", "metadata": {}, "score": "102.89798"}
{"text": "WebDice has the highest kernel weight followed by a series of pattern - based features .It is noteworthy that the pattern features in Table 2 agree with intuition .Lexical patterns ( e.g. , X or Y , X and Y are , X of Y ) as well as syntax patterns ( e.g. , bracketing , comma usage ) are extracted by our method .", "label": "", "metadata": {}, "score": "103.31498"}
{"text": "We computed the correlation with the Miller - Charles ratings for different numbers of snippets to investigate the effect of the number of snippets used to extract patterns upon the semantic similarity measure .The experimental results are presented in Figure 6 .", "label": "", "metadata": {}, "score": "103.38005"}
{"text": "Community Mining .Measuring semantic similarity between named entities is vital in many applications such as query expansion [ 36 ] , entity disambiguation ( e.g. namesake disambiguation ) and community mining [ 19 ] .Because most named entities are not covered by WordNet , similarity measures that are based on WordNet can not be used directly in these tasks .", "label": "", "metadata": {}, "score": "103.46187"}
{"text": "Our contributions in this paper are two fold : .We propose an automatically extracted lexico - syntactic patterns - based approach to compute semantic similarity using text snippets obtained from a Web search engine .We integrate different web - based similarity measures using WordNet synsets and support vector machines to create a robust semantic similarity measure .", "label": "", "metadata": {}, "score": "103.81841"}
{"text": "The following table lists the unpublished resources used by RTE participants .Some of them have been developed by Users themselves specifically for RTE .Interested people may turn to authors to obtain further information .Ontology containing geographic terms and two kinds of relations : the directional part - of relation , and the equal relation for synonyms and abbreviations of the same geographic area ( e.g the United Kingdom , the UK , Great Britain , etc . ) .", "label": "", "metadata": {}, "score": "103.841866"}
{"text": "The following table lists the unpublished resources used by RTE participants .Some of them have been developed by Users themselves specifically for RTE .Interested people may turn to authors to obtain further information .Ontology containing geographic terms and two kinds of relations : the directional part - of relation , and the equal relation for synonyms and abbreviations of the same geographic area ( e.g the United Kingdom , the UK , Great Britain , etc . ) .", "label": "", "metadata": {}, "score": "103.841866"}
{"text": "The following table lists the unpublished resources used by RTE participants .Some of them have been developed by Users themselves specifically for RTE .Interested people may turn to authors to obtain further information .Ontology containing geographic terms and two kinds of relations : the directional part - of relation , and the equal relation for synonyms and abbreviations of the same geographic area ( e.g the United Kingdom , the UK , Great Britain , etc . ) .", "label": "", "metadata": {}, "score": "103.841866"}
{"text": "In Proc . of 15th International World Wide Web Conference , 2006 .D. McCarthy , R. Koeling , J. Weeds , and J. Carroll .Finding predominant word senses in untagged text .In Proceedings of the 42nd Meeting of the Association for Computational Linguistics ( ACL'04 ) , pages 279 - 286 , 2004 .", "label": "", "metadata": {}, "score": "103.85058"}
{"text": "Therefore , one model can share translations and even derivations with other models .Comparable to the state - of - the - art system combination technique , joint decoding achieves an absolute improvement of 1.5 BLEU points over individual decoding . ... thm ( Och , 2003 ) to take the evaluation metric into account .", "label": "", "metadata": {}, "score": "104.0593"}
{"text": "We illustrate this w ... \" .We present a unified view of many translation algorithms that synthesizes work on deductive parsing , semiring parsing , and efficient approximate search algorithms .This gives rise to clean analyses and compact descriptions that can serve as the basis for modular implementations .", "label": "", "metadata": {}, "score": "104.14949"}
{"text": "We introduce cube summing , a technique that permits dynamic programming algorithms for summing over structures ( like the forward and inside algorithms ) to be extended with non - local features that violate the classical structural independence assumptions .It is inspired by cube pruning ( Chiang , 2007 ; ... \" .", "label": "", "metadata": {}, "score": "104.364624"}
{"text": "We analyze the behavior of the proposed measure with the number of patterns used as features , the number of snippets used to extract the patterns , and the size of the training dataset .Secondly , we apply the proposed measure in two real - world applications : community mining and entity disambiguation .", "label": "", "metadata": {}, "score": "104.369995"}
{"text": "2 Page - count - based Similarity Scores .Page counts for the query P AND Q , can be considered as an approximation of co - occurrence of two words ( or multi - word phrases ) and on the Web .", "label": "", "metadata": {}, "score": "104.475845"}
{"text": "In such cases , only the shortest path between any two senses of the words is considered for calculating similarity .A problem that is frequently acknowledged with this approach is that it relies on the notion that all links in the taxonomy represent a uniform distance .", "label": "", "metadata": {}, "score": "104.48983"}
{"text": "Our results show that augmenting a state - ofthe - art phrase - based system with this dependency language model leads to significant improvements in TER ( 0.92 % ) and BLEU ( 0.45 % ) scores on five NIST Chinese - English evaluation test sets .", "label": "", "metadata": {}, "score": "104.51841"}
{"text": "Some selected patterns are shown in Table 2 .Before we proceed to the integration of patterns and page - counts - based similarity scores , it is necessary to introduce some constraints to the development of semantic similarity measures .Evidence from psychological experiments suggest that semantic similarity can be context - dependent and even asymmetric [ 39 , 22 ] .", "label": "", "metadata": {}, "score": "104.92299"}
{"text": "Semantic similarity between two queries is then defined as the inner product between the corresponding centroid vectors .They did not compare their similarity measure with taxonomy - based similarity measures .Chen et al ., [ 6 ] proposed a double - checking model using text snippets returned by a Web search engine to compute semantic similarity between words .", "label": "", "metadata": {}, "score": "104.95215"}
{"text": "We trained a linear kernel SVM with top pattern features ( ranked according to their values ) and calculated the correlation coefficient against the Miller - Charles benchmark dataset .Results of the experiment are shown in Figure 5 .In Figure 5 a steep improvement of correlation with the number of top - ranking patterns is appearent ; it reaches a maximum at features .", "label": "", "metadata": {}, "score": "104.953735"}
{"text": "The latter were taken mainly from VerbNet .Extends a resource described in ( Bar - Haim et al . , AAAI-07 )", "label": "", "metadata": {}, "score": "105.02021"}
{"text": "Our pattern extraction algorithm is illustrated in Figure 3 .Given a set of synonymous word - pairs , GetSnippets function returns a list of text snippets for the query ' 'A \" AND ' ' B \" for each word - pair in .", "label": "", "metadata": {}, "score": "105.18635"}
{"text": "- occurrence of the word pairs in RTE3 and RTE4 using Normalized Google Distance ( Cilibrasi and Vitanyi , 2004 ) .The word pairs are all the possible combinations of content words in T and H. In practice , we used Yahoo ! as the search engine .", "label": "", "metadata": {}, "score": "105.37355"}
{"text": "- occurrence of the word pairs in RTE3 and RTE4 using Normalized Google Distance ( Cilibrasi and Vitanyi , 2004 ) .The word pairs are all the possible combinations of content words in T and H. In practice , we used Yahoo ! as the search engine .", "label": "", "metadata": {}, "score": "105.37355"}
{"text": "We then train a two - class support vector machine with the labelled feature vectors .Once we have trained an SVM using synonymous and non - synonymous word pairs , we can use it to compute the semantic similarity between two given words .", "label": "", "metadata": {}, "score": "105.37848"}
{"text": "For example , apple is frequently associated with computers on the Web .However , this sense of apple is not listed in most general - purpose thesauri or dictionaries .A user who searches for apple on the Web , may be interested in this sense of apple and not apple as a fruit .", "label": "", "metadata": {}, "score": "105.45958"}
{"text": "These different similarity scores are integrated using support vector machines , to leverage a robust semantic similarity measure .Experimental results on Miller - Charles benchmark dataset show that the proposed measure outperforms all the existing web - based semantic similarity measures by a wide margin , achieving a correlation coefficient of .", "label": "", "metadata": {}, "score": "105.55069"}
{"text": "Let us assume the set of patterns selected based on their values in section 3.3 to be .Then , the function SelectPatterns selects the -grams from which appear in .In , we normalize the count of each pattern by diving it from the total number of counts of the observed patterns .", "label": "", "metadata": {}, "score": "105.70815"}
{"text": "The latter were taken mainly from VerbNet .Extends a resource described in ( Bar - Haim et al . , AAAI-07 )The word pairs are all the possible combinations of content words in T and H. thai - sbobet . com . sbo . ] as the search engine .", "label": "", "metadata": {}, "score": "105.82187"}
{"text": "We then describe an automatic lexico - syntactic pattern extraction algorithm in section 3.3 .We rank the patterns extracted by our algorithm according to their ability to express semantic similarity .We use two - class support vector machines ( SVMs ) to find the optimal combination of page counts - based similarity scores and top - ranking patterns .", "label": "", "metadata": {}, "score": "106.1068"}
{"text": "Manually maintaining an up - to - date taxonomy of named entities is costly , if not impossible .The proposed semantic similarity measure is appealing for these applications because it does not require pre - compiled taxonomies .In order to evaluate the performance of the proposed measure in capturing the semantic similarity between named - entities , we set up a community mining task .", "label": "", "metadata": {}, "score": "106.17759"}
{"text": "Similar approximate inference techniques support efficient parameter estimation with hidden variables .We use the decoder to conduct controlled experiments on a German - to - English translation task , to compare lexical phrase , syntax , and combined models , and to measure effects of various restrictions on nonisomorphism . ... algorithms , including decoders ( Koehn et al . , 2003 ; Yamada and Knight , 2001 ) .", "label": "", "metadata": {}, "score": "106.24249"}
{"text": "If there exist a previous query that is semantically related to the current query , then it can be suggested either to the user or internally used by the search engine to modify the original query .Semantic similarity measures have been used in Semantic Web related applications such as automatic annotation of Web pages [ 7 ] , community mining [ 23 , 19 ] , and keyword extraction for inter - entity relation representation [ 26 ] .", "label": "", "metadata": {}, "score": "106.653885"}
{"text": "2 Related Work .Semantic similarity measures are important in many Web - related tasks .In query expansion [ 5 , 25 , 40 ] a user query is modified using synonymous words to improve the relevancy of the search .", "label": "", "metadata": {}, "score": "106.68113"}
{"text": "Similarity measure proposed by Sahami et al .[36 ] is placed third , reflecting a correlation of .This method use only those snippets when calculating semantic similarity .Among the four page - counts - based measures , WebPMI garners the highest correlation ( ) .", "label": "", "metadata": {}, "score": "106.78293"}
{"text": "Only positive examples and negative examples are necessary to leverage the proposed method , which is efficient and scalable because it only processes the snippets ( no downloading of Web pages is necessary ) for the top ranking results by Google .", "label": "", "metadata": {}, "score": "106.98056"}
{"text": "For each extracted pattern , we create a contingency table , as shown in Table 1 using its frequency in snippets for synonymous word pairs and in snippets for non - synonymous word pairs .In Table 1 , denotes the total frequency of all patterns in snippets for synonymous word pairs ( ) and is the same in snippets for non - synonymous word pairs ( ) .", "label": "", "metadata": {}, "score": "107.11714"}
{"text": "We present a machine translation framework that can incorporate arbitrary features of both input and output sentences .The core of the approach is a novel decoder based on lattice parsing with quasisynchronous grammar ( Smith and Eisner , 2006 ) , a syntactic formalism that does not require source and target trees to be isomorphic .", "label": "", "metadata": {}, "score": "107.37163"}
{"text": "3 Extracting Lexico - Syntactic Patterns from Snippets .Text snippets are returned by search engines alongside with the search results .They provide valuable information regarding the local context of a word .We extract lexico - syntactic patterns that indicate various aspects of semantic similarity .", "label": "", "metadata": {}, "score": "107.831795"}
{"text": "We integrated page - counts - based similarity scores with lexico syntactic patterns using support vector machines .Training data were automatically generated using WordNet synsets .Proposed method outperformed all the baselines including previously proposed Web - based semantic similarity measures on a benchmark dataset .", "label": "", "metadata": {}, "score": "107.91855"}
{"text": ", [ 36 ] measured semantic similarity between two queries using snippets returned for those queries by a search engine .For each query , they collect snippets from a search engine and represent each snippet as a TF - IDF - weighted term vector .", "label": "", "metadata": {}, "score": "107.97993"}
{"text": "Both tables are sortable by Resource name , type , author and number of users .RTE Participants are encouraged to add information about all kind of knowledge resources used , from standard existing resources ( e.g. WordNet ) to knowledge collections created for specific purposes , which can be made available to the community .", "label": "", "metadata": {}, "score": "108.1653"}
{"text": "We terminate GAAC process when exactly five clusters are formed .We adopt this clustering method with different semantic similarity measures to compare their accuracy in clustering people who belong to the same community .We employed the B - CUBED metric [ 1 ] to evaluate the clustering results .", "label": "", "metadata": {}, "score": "108.47506"}
{"text": "Contents .In order to help the research , all the participants are invited to contribute , sharing their own resources with the RTE community .Making the resources available to be used by other systems has several advantages .On the one hand , it helps improve the TE technology ; on the other hand , it offers an opportunity to further test and evaluate the resource .", "label": "", "metadata": {}, "score": "108.510666"}
{"text": "Here , the phrase is a indicates a semantic relationship between cricket and sport .Many such phrases indicate semantic relationships .For example , also known as , is a , part of , is an example of all indicate semantic relations of different types .", "label": "", "metadata": {}, "score": "109.382866"}
{"text": "The latter were taken mainly from VerbNet .Extends a resource described in ( Bar - Haim et al . , AAAI-07 ) Revision as of 06:28 , 20 June 2010 .Knowledge resources have shown their relevance for applied semantic inference , and are extensively used by applied inference systems , such as those developed within the Textual Entailment framework .", "label": "", "metadata": {}, "score": "109.81654"}
{"text": "Considering that the patterns are ranked according to their ability to express semantic similarity and the majority of patterns are sparse , we selected only the top ranking patterns for the remaining experiments .Features with the highest linear kernel weights are shown in Table 2 alongside their values .", "label": "", "metadata": {}, "score": "109.92228"}
{"text": ".. the input rule tables are sorted . 3.3Decoders Hierarchical translation .Pruning settings can be configured flexibly ... . \" ... Current SMT systems usually decode with single translation models and can not benefit from the strengths of other models in decoding phase .", "label": "", "metadata": {}, "score": "109.94844"}
{"text": "Recent syntactic extensions of statistical translation models work with a synchronous context - free or tree - substitution grammar extracted from an automatically parsed parallel corpus .The decoders accompanying these extensions typically exceed quadratic time complexity .This paper extends the Direct ... \" .", "label": "", "metadata": {}, "score": "109.986244"}
{"text": "However , experimental results investigating the effects of asymmetry reports that the average difference in ratings for a word pair is less than percent [ 22 ] .In this work , we assume semantic similarity to be symmetric .This is in line with previous work on semantic similarity described in section 2 .", "label": "", "metadata": {}, "score": "110.17817"}
{"text": "Here , the phrase is the largest indicates a hypernymic relationship between the Jaguar and the cat .Phrases such as also known as , is a , part of , is an example of all indicate various semantic relations .Such indicative phrases have been applied to numerous tasks with good results , such as hyponym extraction [ 12 ] and fact extraction [ 27 ] .", "label": "", "metadata": {}, "score": "110.31673"}
{"text": "Processing snippets is also efficient as it obviates the trouble of downloading web pages , which might be time consuming depending on the size of the pages .However , a widely acknowledged drawback of using snippets is that , because of the huge scale of the web and the large number of documents in the result set , only those snippets for the top - ranking results for a query can be processed efficiently .", "label": "", "metadata": {}, "score": "110.397705"}
{"text": "Categories & Subject Descriptors .H.3.3Information SystemsInformation Search and Retrieval Algorithms semantic similarity , Web mining .General terms .Algorithms .Keywords .The study of semantic similarity between words has long been an integral part of information retrieval and natural language processing .", "label": "", "metadata": {}, "score": "110.49692"}
{"text": "The latter were taken mainly from VerbNet .Extends a resource described in ( Bar - Haim et al . , AAAI-07 ) RTE Knowledge Resources .Knowledge resources have shown their relevance for applied semantic inference , and are extensively used by applied inference systems , such as those developed within the Textual Entailment framework .", "label": "", "metadata": {}, "score": "110.675705"}
{"text": "The latter were taken mainly from VerbNet .Extends a resource described in ( Bar - Haim et al . , AAAI-07 ) RTE Knowledge Resources .Knowledge resources have shown their relevance for applied semantic inference , and are extensively used by applied inference systems , such as those developed within the Textual Entailment framework .", "label": "", "metadata": {}, "score": "110.675705"}
{"text": "The latter were taken mainly from VerbNet .Extends a resource described in ( Bar - Haim et al . , AAAI-07 ) RTE Knowledge Resources .Knowledge resources have shown their relevance for applied semantic inference , and are extensively used by applied inference systems , such as those developed within the Textual Entailment framework .", "label": "", "metadata": {}, "score": "110.675705"}
{"text": "The latter were taken mainly from VerbNet .Extends a resource described in ( Bar - Haim et al . , AAAI-07 ) Revision as of 06:30 , 20 June 2010 .Knowledge resources have shown their relevance for applied semantic inference , and are extensively used by applied inference systems , such as those developed within the Textual Entailment framework .", "label": "", "metadata": {}, "score": "110.78258"}
{"text": "We append similarity scores calculated using page counts in section 3.2 to create the final feature vector for the word - pair .This procedure yields a dimensional ( page - counts based similarity scores and lexico - syntactic patterns ) feature vector .", "label": "", "metadata": {}, "score": "111.02494"}
{"text": "DIRT ( Discovery of Inference Rules from Text ) is both an algorithm and a resulting knowledge collection .The DIRT knowledge collection is the output of the DIRT algorithm over a 1 GB set of newspaper text .A Categorial - Variation Database ( or Catvar ) is a database of clusters of uninflected words ( lexemes ) and their categorial ( i.e. part - of - speech ) variants .", "label": "", "metadata": {}, "score": "111.1142"}
{"text": "The first table lists the publicly available resources , the second one lists unpublished resources .Both tables are sortable by Resource name , type , author and number of users .RTE Participants are encouraged to add information about all kind of knowledge resources used , from standard existing resources ( e.g. WordNet ) to knowledge collections created for specific purposes , which can be made available to the community .", "label": "", "metadata": {}, "score": "112.254715"}
{"text": "The first table lists the publicly available resources , the second one lists unpublished resources .Both tables are sortable by Resource name , type , author and number of users .RTE Participants are encouraged to add information about all kind of knowledge resources used , from standard existing resources ( e.g. WordNet ) to knowledge collections created for specific purposes , which can be made available to the community .", "label": "", "metadata": {}, "score": "112.254715"}
{"text": "The first table lists the publicly available resources , the second one lists unpublished resources .Both tables are sortable by Resource name , type , author and number of users .RTE Participants are encouraged to add information about all kind of knowledge resources used , from standard existing resources ( e.g. WordNet ) to knowledge collections created for specific purposes , which can be made available to the community .", "label": "", "metadata": {}, "score": "112.254715"}
{"text": "The first table lists the publicly available resources , the second one lists unpublished resources .Both tables are sortable by Resource name , type , author and number of users .RTE Participants are encouraged to add information about all kind of knowledge resources used , from standard existing resources ( e.g. WordNet ) to knowledge collections created for specific purposes , which can be made available to the community .", "label": "", "metadata": {}, "score": "112.254715"}
{"text": "The first table lists the publicly available resources , the second one lists unpublished resources .Both tables are sortable by Resource name , type , author and number of users .RTE Participants are encouraged to add information about all kind of knowledge resources used , from standard existing resources ( e.g. WordNet ) to knowledge collections created for specific purposes , which can be made available to the community .", "label": "", "metadata": {}, "score": "112.254715"}
{"text": "The first table lists the publicly available resources , the second one lists unpublished resources .Both tables are sortable by Resource name , type , author and number of users .RTE Participants are encouraged to add information about all kind of knowledge resources used , from standard existing resources ( e.g. WordNet ) to knowledge collections created for specific purposes , which can be made available to the community .", "label": "", "metadata": {}, "score": "112.254715"}
{"text": "Ranking of snippets , ( hence the value of ) , depends directly upon the search engine 's specifications .A search engine considers various factors such as novelty , authority , link structure , user preferences when ranking search results .", "label": "", "metadata": {}, "score": "112.6198"}
{"text": "Lowest similarity is reported for cord and smile 7 .Our reimplementation of Co - occurrence Double Checking ( CODC ) measure [ 6 ] indicates the second - best correlation of .The CODC measure is defined as , .Therein , denotes the number of occurrences of in the top - ranking snippets for the query in Google . is the page count for query . is a constant in CODC model and it is set to according to Chen et al .", "label": "", "metadata": {}, "score": "112.63145"}
{"text": "We define the semantic similarity between and as the posterior probability that feature vector belongs to the synonymous - words ( positive ) class .Being a large - margin classifier , the output of an SVM is the distance from the decision hyper - plane .", "label": "", "metadata": {}, "score": "112.6741"}
{"text": "The rules cover generic syntactic phenomena such as appositions , conjunctions , passive , relative clause , etc .( Bar - Haim et al . , AAAI-07 ) .A manually - composed collection of entailment rules which detect predicates whose polarity is negative ( e.g. did n't dance ) or unknown ( e.g. plans to dance ) .", "label": "", "metadata": {}, "score": "112.830505"}
{"text": "The rules cover generic syntactic phenomena such as appositions , conjunctions , passive , relative clause , etc .( Bar - Haim et al . , AAAI-07 ) .A manually - composed collection of entailment rules which detect predicates whose polarity is negative ( e.g. did n't dance ) or unknown ( e.g. plans to dance ) .", "label": "", "metadata": {}, "score": "112.830505"}
{"text": "The rules cover generic syntactic phenomena such as appositions , conjunctions , passive , relative clause , etc .( Bar - Haim et al . , AAAI-07 ) .A manually - composed collection of entailment rules which detect predicates whose polarity is negative ( e.g. did n't dance ) or unknown ( e.g. plans to dance ) .", "label": "", "metadata": {}, "score": "112.830505"}
{"text": "The rules cover generic syntactic phenomena such as appositions , conjunctions , passive , relative clause , etc .( Bar - Haim et al . , AAAI-07 ) .A manually - composed collection of entailment rules which detect predicates whose polarity is negative ( e.g. did n't dance ) or unknown ( e.g. plans to dance ) .", "label": "", "metadata": {}, "score": "112.830505"}
{"text": "The rules cover generic syntactic phenomena such as appositions , conjunctions , passive , relative clause , etc .( Bar - Haim et al . , AAAI-07 ) .A manually - composed collection of entailment rules which detect predicates whose polarity is negative ( e.g. did n't dance ) or unknown ( e.g. plans to dance ) .", "label": "", "metadata": {}, "score": "112.830505"}
{"text": "The rules cover generic syntactic phenomena such as appositions , conjunctions , passive , relative clause , etc .( Bar - Haim et al . , AAAI-07 ) .A manually - composed collection of entailment rules which detect predicates whose polarity is negative ( e.g. did n't dance ) or unknown ( e.g. plans to dance ) .", "label": "", "metadata": {}, "score": "112.830505"}
{"text": "The rules cover generic syntactic phenomena such as appositions , conjunctions , passive , relative clause , etc .( Bar - Haim et al . , AAAI-07 ) .A manually - composed collection of entailment rules which detect predicates whose polarity is negative ( e.g. did n't dance ) or unknown ( e.g. plans to dance ) .", "label": "", "metadata": {}, "score": "112.830505"}
{"text": "The rules cover generic syntactic phenomena such as appositions , conjunctions , passive , relative clause , etc .( Bar - Haim et al . , AAAI-07 ) .A manually - composed collection of entailment rules which detect predicates whose polarity is negative ( e.g. did n't dance ) or unknown ( e.g. plans to dance ) .", "label": "", "metadata": {}, "score": "112.830505"}
{"text": "First , we query Google for '' \" AND '' \" and collect snippets .Then we replace the query words and with two wildcards and , respectively in each snippet .Function GetNgrams extracts -grams for and from the snippets .", "label": "", "metadata": {}, "score": "113.035385"}
{"text": "Experimental results are summarized in Figure 7 .Maximum correlation coefficient of is achieved with positive training examples and negative training examples .Moreover , Figure 7 reveals that correlation does not improve beyond positive and negative training examples .Therefore , we can conclude that examples are sufficient to leverage the proposed semantic similarity measure .", "label": "", "metadata": {}, "score": "113.50714"}
{"text": "Alongside translation features extracted from the derived parse tree , we explore syntactic features extracted from the incremental derivation process .Our empirical experiments show that our model significantly outperforms the state - of - the art DTM2 system . ...e forced to employ small language models compared to what is usually used in phrase - based systems .", "label": "", "metadata": {}, "score": "113.82251"}
{"text": "Extraction of about 8 million lexical reference rules from the text body ( first sentence ) and from metadata ( links , redirects , parentheses ) of Wikipedia .Provides better performance than other automatically constructed resources and comparable performance to WordNet .", "label": "", "metadata": {}, "score": "113.87427"}
{"text": "Extraction of about 8 million lexical reference rules from the text body ( first sentence ) and from metadata ( links , redirects , parentheses ) of Wikipedia .Provides better performance than other automatically constructed resources and comparable performance to WordNet .", "label": "", "metadata": {}, "score": "113.87427"}
{"text": "Extraction of about 8 million lexical reference rules from the text body ( first sentence ) and from metadata ( links , redirects , parentheses ) of Wikipedia .Provides better performance than other automatically constructed resources and comparable performance to WordNet .", "label": "", "metadata": {}, "score": "113.87427"}
{"text": "Extraction of about 8 million lexical reference rules from the text body ( first sentence ) and from metadata ( links , redirects , parentheses ) of Wikipedia .Provides better performance than other automatically constructed resources and comparable performance to WordNet .", "label": "", "metadata": {}, "score": "113.87427"}
{"text": "Extraction of about 8 million lexical reference rules from the text body ( first sentence ) and from metadata ( links , redirects , parentheses ) of Wikipedia .Provides better performance than other automatically constructed resources and comparable performance to WordNet .", "label": "", "metadata": {}, "score": "113.87427"}
{"text": "Extraction of about 8 million lexical reference rules from the text body ( first sentence ) and from metadata ( links , redirects , parentheses ) of Wikipedia .Provides better performance than other automatically constructed resources and comparable performance to WordNet .", "label": "", "metadata": {}, "score": "113.87427"}
{"text": "Extraction of about 8 million lexical reference rules from the text body ( first sentence ) and from metadata ( links , redirects , parentheses ) of Wikipedia .Provides better performance than other automatically constructed resources and comparable performance to WordNet .", "label": "", "metadata": {}, "score": "113.87427"}
{"text": "Extraction of about 8 million lexical reference rules from the text body ( first sentence ) and from metadata ( links , redirects , parentheses ) of Wikipedia .Provides better performance than other automatically constructed resources and comparable performance to WordNet .", "label": "", "metadata": {}, "score": "113.87427"}
{"text": "When non - local features are included , cube summing does not reduce to any semiring , but is compatible with generic techniques for solving dynamic programming equations . ... tructures ( like the forward and inside algorithms ) to be extended with non - local features that violate the classical structural independence assumptions .", "label": "", "metadata": {}, "score": "113.8907"}
{"text": "Dependency structure , as a first step towards semantics , is believed to be helpful to improve translation quality .However , previous works on dependency structure based models typically resort to insertion operations to complete translations , which make it difficult to specify ordering information i ... \" .", "label": "", "metadata": {}, "score": "114.41774"}
{"text": "Page counts and snippets are two useful information sources provided by most Web search engines .Page count of a query is the number of pages that contain the query words 1 .The more than times more numerous page counts for ' ' apple ' ' AND ' ' computer ' ' indicate that apple is more semantically similar to computer than is banana .", "label": "", "metadata": {}, "score": "114.94722"}
{"text": "3.2 Framework to Specify R .. \" ...We present Jane 2 , an open source toolkit supporting both the phrase - based and the hierarchical phrase - based paradigm for statistical machine translation .It is implemented in C++ and provides efficient decoding algorithms and data structures .", "label": "", "metadata": {}, "score": "115.537704"}
{"text": "The decoders accompanying these extensions typically exceed quadratic time complexity .This paper extends the Direct Translation Model 2 ( DTM2 ) with syntax while maintaining linear - time decoding .We employ a linear - time parsing algorithm based on an eager , incremental interpretation of Combinatory Categorial Grammar ( CCG ) .", "label": "", "metadata": {}, "score": "115.67174"}
{"text": "An approch for measuring semantic similarity between words using multiple information sources .IEEE Transactions on Knowledge and Data Engineering , 15(4):871 - 882 , 2003 .Tools . \" ...We present a unified view of many translation algorithms that synthesizes work on deductive parsing , semiring parsing , and efficient approximate search algorithms .", "label": "", "metadata": {}, "score": "116.91813"}
{"text": "Let us assume these wildcards to be X and Y .For each snippet in the set of snippets returned by GetSnippets , function GetNgrams extract word -grams for and .We select -grams which contain exactly one X and one Y .", "label": "", "metadata": {}, "score": "116.9241"}
{"text": "Manually maintaining thesauri to capture these new words and senses is costly if not impossible .We propose an automatic method to measure semantic similarity between words or entities using Web search engines .Because of the vastly numerous documents and the high growth rate of the Web , it is difficult to analyze each document separately and directly .", "label": "", "metadata": {}, "score": "117.19564"}
{"text": "Ablation test are meant to help better understand the relevance of the knowledge resources used by RTE systems , and evaluate the contribution of each of them to the systems ' performances .In fact , comparing the results achieved in the ablation tests to those obtained by the systems as a whole allows assessing the contribution given by each single resource .", "label": "", "metadata": {}, "score": "117.506775"}
{"text": "All extensions are evaluated in standard hierarchical setups as well as in setups where the hierarchical recursion depth is restricted . ... task .2 6.1 Experimental Setup We employ the freely available hierarchical translation toolkit Jane ( Vilar et al . , 2010 ) to set up our systems .", "label": "", "metadata": {}, "score": "117.658936"}
{"text": "DIRT ( Discovery of Inference Rules from Text ) is both an algorithm and a resulting knowledge collection .The DIRT knowledge collection is the output of the DIRT algorithm over a 1 GB set of newspaper text .Co- occurrence of the word pairs in RTE3 and RTE4 using Normalized Google Distance ( Cilibrasi and Vitanyi , 2004 ) .", "label": "", "metadata": {}, "score": "118.22852"}
{"text": "DIRT ( Discovery of Inference Rules from Text ) is both an algorithm and a resulting knowledge collection .The DIRT knowledge collection is the output of the DIRT algorithm over a 1 GB set of newspaper text .Co- occurrence of the word pairs in RTE3 and RTE4 using Normalized Google Distance ( Cilibrasi and Vitanyi , 2004 ) .", "label": "", "metadata": {}, "score": "118.22852"}
{"text": "DIRT ( Discovery of Inference Rules from Text ) is both an algorithm and a resulting knowledge collection .The DIRT knowledge collection is the output of the DIRT algorithm over a 1 GB set of newspaper text .Co- occurrence of the word pairs in RTE3 and RTE4 using Normalized Google Distance ( Cilibrasi and Vitanyi , 2004 ) .", "label": "", "metadata": {}, "score": "118.22852"}
{"text": "DIRT ( Discovery of Inference Rules from Text ) is both an algorithm and a resulting knowledge collection .The DIRT knowledge collection is the output of the DIRT algorithm over a 1 GB set of newspaper text .Co- occurrence of the word pairs in RTE3 and RTE4 using Normalized Google Distance ( Cilibrasi and Vitanyi , 2004 ) .", "label": "", "metadata": {}, "score": "118.22852"}
{"text": "DIRT ( Discovery of Inference Rules from Text ) is both an algorithm and a resulting knowledge collection .The DIRT knowledge collection is the output of the DIRT algorithm over a 1 GB set of newspaper text .Co- occurrence of the word pairs in RTE3 and RTE4 using Normalized Google Distance ( Cilibrasi and Vitanyi , 2004 ) .", "label": "", "metadata": {}, "score": "118.22852"}
{"text": "DIRT ( Discovery of Inference Rules from Text ) is both an algorithm and a resulting knowledge collection .The DIRT knowledge collection is the output of the DIRT algorithm over a 1 GB set of newspaper text .Co- occurrence of the word pairs in RTE3 and RTE4 using Normalized Google Distance ( Cilibrasi and Vitanyi , 2004 ) .", "label": "", "metadata": {}, "score": "118.22852"}
{"text": "DIRT ( Discovery of Inference Rules from Text ) is both an algorithm and a resulting knowledge collection .The DIRT knowledge collection is the output of the DIRT algorithm over a 1 GB set of newspaper text .Co- occurrence of the word pairs in RTE3 and RTE4 using Normalized Google Distance ( Cilibrasi and Vitanyi , 2004 ) .", "label": "", "metadata": {}, "score": "118.22852"}
{"text": "[36 ] 's snippet - based similarity measure , WebJaccard , WebDice and WebOverlap measures yield similar clustering accuracies .For example , Jaguar is a cat , a car brand and also an operating system for computers .A user who searches for Jaguar on the Web , may be interested in either one of these different senses of Jaguar .", "label": "", "metadata": {}, "score": "118.431404"}
{"text": "We present Jane 2 , an open source toolkit supporting both the phrase - based and the hierarchical phrase - based paradigm for statistical machine translation .It is implemented in C++ and provides efficient decoding algorithms and data structures .This work focuses on the description of its phrase - based functionality .", "label": "", "metadata": {}, "score": "118.443275"}
{"text": "Extension of WordNet based on the exploitation of the information contained in WordNet definitional glosses : the glosses are syntactically parsed , transformed into logic forms and content words are semantically disambiguated .The Extended Wordnet is an ongoing project .The resource is the result of the application of a learning algorithm for inducing semantic taxonomies from parsed text .", "label": "", "metadata": {}, "score": "118.51558"}
{"text": "Extension of WordNet based on the exploitation of the information contained in WordNet definitional glosses : the glosses are syntactically parsed , transformed into logic forms and content words are semantically disambiguated .The Extended Wordnet is an ongoing project .The resource is the result of the application of a learning algorithm for inducing semantic taxonomies from parsed text .", "label": "", "metadata": {}, "score": "118.51558"}
{"text": "Extension of WordNet based on the exploitation of the information contained in WordNet definitional glosses : the glosses are syntactically parsed , transformed into logic forms and content words are semantically disambiguated .The Extended Wordnet is an ongoing project .The resource is the result of the application of a learning algorithm for inducing semantic taxonomies from parsed text .", "label": "", "metadata": {}, "score": "118.51558"}
{"text": "Extension of WordNet based on the exploitation of the information contained in WordNet definitional glosses : the glosses are syntactically parsed , transformed into logic forms and content words are semantically disambiguated .The Extended Wordnet is an ongoing project .The resource is the result of the application of a learning algorithm for inducing semantic taxonomies from parsed text .", "label": "", "metadata": {}, "score": "118.51558"}
{"text": "Extension of WordNet based on the exploitation of the information contained in WordNet definitional glosses : the glosses are syntactically parsed , transformed into logic forms and content words are semantically disambiguated .The Extended Wordnet is an ongoing project .The resource is the result of the application of a learning algorithm for inducing semantic taxonomies from parsed text .", "label": "", "metadata": {}, "score": "118.51558"}
{"text": "Extension of WordNet based on the exploitation of the information contained in WordNet definitional glosses : the glosses are syntactically parsed , transformed into logic forms and content words are semantically disambiguated .The Extended Wordnet is an ongoing project .The resource is the result of the application of a learning algorithm for inducing semantic taxonomies from parsed text .", "label": "", "metadata": {}, "score": "118.51558"}
{"text": "Extension of WordNet based on the exploitation of the information contained in WordNet definitional glosses : the glosses are syntactically parsed , transformed into logic forms and content words are semantically disambiguated .The Extended Wordnet is an ongoing project .The resource is the result of the application of a learning algorithm for inducing semantic taxonomies from parsed text .", "label": "", "metadata": {}, "score": "118.51558"}
{"text": "The word pairs are all the possible combinations of content words in T and H. thai - sbobet . com . sbobet . ] as the search engine .The word pairs are all the possible combinations of content words in T and H. Latest revision as of 04:18 , 25 June 2012 .", "label": "", "metadata": {}, "score": "118.786575"}
{"text": "In Formula 15 denotes the number of words in snippet .We used different semantic similarity measures for in Formula 15 and employed the group average agglomerative clustering explained in section 4.7 .We manually analyzed the snippets for queries Java ( 3 senses : programming language , Island , coffee ) and Jaguar ( 3 senses : cat , car , operating system ) and computed precision , recall and F - score for the clusters created by the algorithm .", "label": "", "metadata": {}, "score": "118.83771"}
{"text": "Although the framework is drawn from parsing and applied to translation , it is applicable to many dynamic programming problems arising in natural language processing and other areas .They show that MST parsing is almost as accurate as cubic - time dependency parsing in the case of English , and that it is more accurate with free word order languages .", "label": "", "metadata": {}, "score": "119.14388"}
{"text": "Roget 's Thesaurus is a widely - used English thesaurus , created by Dr. Peter Mark Roget in 1805 .The original edition had 15,000 words , and each new edition has been larger .The electronic edition ( version 1.02 ) is made available by University of Chicago .", "label": "", "metadata": {}, "score": "119.36785"}
{"text": "For example , Google returns as the page count for ' ' car ' ' AND ' ' automobile ' ' , whereas the same is for ' ' car ' ' AND ' ' apple ' ' .Although , automobile is more semantically similar to car than apple is , page counts for query ' ' car ' ' AND ' ' apple ' ' are more than four times greater than those for the query ' ' car ' ' and ' ' automobile ' ' .", "label": "", "metadata": {}, "score": "119.89709"}
{"text": "CODC measure reports zero similarity scores for many word - pairs in the benchmark .One reason for this sparsity in CODC measure is that even though two words in a pair are semantically similar , we might not always find among the top snippets for ( and vice versa ) .", "label": "", "metadata": {}, "score": "120.814705"}
{"text": "Our joint decoder draws connections among multiple models b ... \" .Current SMT systems usually decode with single translation models and can not benefit from the strengths of other models in decoding phase .We instead propose joint decoding , a method that combines multiple translation models in one decoder .", "label": "", "metadata": {}, "score": "121.9261"}
{"text": "Peter Mark Roget ( Electronic version distributed by University of Chicago ) .Roget 's Thesaurus is a widely - used English thesaurus , created by Dr. Peter Mark Roget in 1805 .The original edition had 15,000 words , and each new edition has been larger .", "label": "", "metadata": {}, "score": "122.69501"}
{"text": "Peter Mark Roget ( Electronic version distributed by University of Chicago ) .Roget 's Thesaurus is a widely - used English thesaurus , created by Dr. Peter Mark Roget in 1805 .The original edition had 15,000 words , and each new edition has been larger .", "label": "", "metadata": {}, "score": "122.69501"}
{"text": "Peter Mark Roget ( Electronic version distributed by University of Chicago ) .Roget 's Thesaurus is a widely - used English thesaurus , created by Dr. Peter Mark Roget in 1805 .The original edition had 15,000 words , and each new edition has been larger .", "label": "", "metadata": {}, "score": "122.69501"}
{"text": "Peter Mark Roget ( Electronic version distributed by University of Chicago ) .Roget 's Thesaurus is a widely - used English thesaurus , created by Dr. Peter Mark Roget in 1805 .The original edition had 15,000 words , and each new edition has been larger .", "label": "", "metadata": {}, "score": "122.69501"}
{"text": "Peter Mark Roget ( Electronic version distributed by University of Chicago ) .Roget 's Thesaurus is a widely - used English thesaurus , created by Dr. Peter Mark Roget in 1805 .The original edition had 15,000 words , and each new edition has been larger .", "label": "", "metadata": {}, "score": "122.69501"}
{"text": "Peter Mark Roget ( Electronic version distributed by University of Chicago ) .Roget 's Thesaurus is a widely - used English thesaurus , created by Dr. Peter Mark Roget in 1805 .The original edition had 15,000 words , and each new edition has been larger .", "label": "", "metadata": {}, "score": "122.69501"}
{"text": "Peter Mark Roget ( Electronic version distributed by University of Chicago ) .Roget 's Thesaurus is a widely - used English thesaurus , created by Dr. Peter Mark Roget in 1805 .The original edition had 15,000 words , and each new edition has been larger .", "label": "", "metadata": {}, "score": "122.69501"}
{"text": "Human Language Technology Research Institute , University of Texas at Dallas .Extension of WordNet based on the exploitation of the information contained in WordNet definitional glosses : the glosses are syntactically parsed , transformed into logic forms and content words are semantically disambiguated .", "label": "", "metadata": {}, "score": "124.35286"}
{"text": "Forced alignment phrase training can considerably reduce rule table size while learning the translation scores in a more principled manner .Word class language models can be used to integrate longer context with a reduced vocabulary size .Rule table interpolation is applicable for different tasks , e.g. domain adaptation .", "label": "", "metadata": {}, "score": "124.992165"}
{"text": "Best performance is achieved with the linear kernel .Second best is the Radial Basis Functions ( RBFs ) , which reports a correlation coefficient of .For the rest of the experiments in this paper we use the linear kernel .", "label": "", "metadata": {}, "score": "125.35603"}
{"text": "However , previous works on dependency structure based models typically resort to insertion operations to complete translations , which make it difficult to specify ordering information in translation rules .In our model of this paper , we handle this problem by directly specifying the ordering information in head - dependents rules which represent the source side as head - dependents relations and the target side as strings .", "label": "", "metadata": {}, "score": "126.38416"}
{"text": "Whereas , a Web page on Jaguar the cat , is likely to contain information about other types of cats and animals .In this section , we utilize the clustering algorithm described in section 4.7 to cluster the top snippets returned by Google for two ambiguous entities Jaguar and Java .", "label": "", "metadata": {}, "score": "126.61197"}
{"text": "Here , the relationship between Toyota and Nissan is that they are both car manufacturers .Identifying the exact set of words that convey the semantic relationship between two entities is a difficult problem which requires deeper semantic analysis .However , such an analysis is not feasible considering the numerous ill - formed sentences we need to process on the Web .", "label": "", "metadata": {}, "score": "126.61776"}
{"text": "RTE6 - Call for Resources In order to help the research , all the participants are invited to contribute , sharing their own resources with the RTE community .Making the resources available to be used by other systems has several advantages .", "label": "", "metadata": {}, "score": "126.79314"}
{"text": "National Institute of Advanced Industrial Science & Technology Sotokanda , 1 - 18 - 13 , Tokyo , Japan .Mitsuru Ishizuka .The University of Tokyo Hongo , 7 - 3 - 1 , Tokyo , Japan .Copyright is held by the World Wide Web Conference Committee ( IW3C2 ) .", "label": "", "metadata": {}, "score": "127.73276"}
{"text": "Edinburgh University participated in the WMT 2009 shared task using the Moses phrase - based statistical machine translation decoder , building systems for all language pairs .The system configuration was identical for all language pairs ( with a few additional components for the German - English language ... \" .", "label": "", "metadata": {}, "score": "128.24353"}
{"text": "WWW 2007 , May 8 - 12 , 2007 , Banff , Alberta , Canada .ACM 2007 .ABSTRACT .Semantic similarity measures play important roles in information retrieval and Natural Language Processing .Previous work in semantic web - related applications such as community mining , relation extraction , automatic meta data extraction have used various semantic similarity measures .", "label": "", "metadata": {}, "score": "129.65434"}
