{"text": "There are a number of other aspects of the LSH algorithm and its use that we are currently investigating .One important aspect is the issue of data reduction and scaling .In this study , we only considered a simplistic approach to data reduction as a means of reducing the dimensionality of the descriptor space .", "label": "", "metadata": {}, "score": "26.268116"}
{"text": "Clearly , by combining both questions , one may gain an understanding of which compounds to focus on during either the compound acquisition or compound selection steps .The LSH algorithm provides a robust approach to answering both of the above questions .", "label": "", "metadata": {}, "score": "26.914433"}
{"text": "Then , LSH can be made to self - tune the other parameters .Because of the high accuracy of our LSH - based frame- work , as demonstrated in the previous sections , we can also conclude that LSH will be at least as accurate as any other standard kNN - based methods for classification .", "label": "", "metadata": {}, "score": "29.215303"}
{"text": "LSH uses random projections to hash nearby points to nearby bins .It guarantees sublinear time for approximate nearest - neighbor queries .We demon- strate the efficacy of this tool in fast outlier detection .The results indicate that the LSH algorithm can help speed up nearest - neighbor queries by 2 orders of magnitude compared to the naive kNN algorithm .", "label": "", "metadata": {}, "score": "29.845688"}
{"text": "In our implementation of LSH the optimal parameters of the data structure are determined by a pilot learning procedure , and the partitions are data driven .As an application , the performance of mode and k - means based textons are compared in a texture classification study .", "label": "", "metadata": {}, "score": "29.904932"}
{"text": "Experimental results show that secure image retrieval can achieve comparable retrieval performance to conventional image retrieval techniques without revealing information about image content .This work enriches the area of secure information retrieval and can find applications in secure online services for images and videos .", "label": "", "metadata": {}, "score": "30.439335"}
{"text": "Future work will examine how the density of a descriptor space affects the running time of the LSH algorithm .In this study , we have only considered one sample application of outlier detection .To demonstrate the effective- ness of the method , future work will involve a more rigorous comparison of the LSH algorithm for outlier detection with other well - known outlier detection techniques as well as a more comprehensive application to other large data sets .", "label": "", "metadata": {}, "score": "30.911287"}
{"text": "This paper has proposed a novel unsupervised hashing method based on manifold learning theories , which can maximize the total variance of the hash codes while preserving the local structure of the training data .Two algorithms , MVH - CG and MVH - A , have been proposed to solve the derived optimization problem .", "label": "", "metadata": {}, "score": "31.163157"}
{"text": "Then , we compare the mean query time by dividing the total query time by the number of queries .We also feel the need to use another metric called the normalized query time and to compare the running times of LSH with that of traditional kNN .", "label": "", "metadata": {}, "score": "31.675793"}
{"text": "Bridging the cognitive gap in image retrieval has been an active research direction in recent years .Existing solutions typically require a large volume of training data that could be difficult to obtain in practice .In this paper , we propose a data - driven approach that uses Web images and their surrounding textual annotations as the source of training data to bridge the cognitive gap .", "label": "", "metadata": {}, "score": "32.09765"}
{"text": "First , we are able to overcome the natural LSH barrier .Second , this result shows that what \" practitioners \" have been doing for some time ( namely , data - dependent space partitioning ) can give advantage in theory , too .", "label": "", "metadata": {}, "score": "32.328247"}
{"text": "The idea is to maximize the total variance of the hash codes while preserving the local structure of the training data .To solve the derived optimization problem , we propose a column generation algorithm , which directly learns the binary - valued hash functions .", "label": "", "metadata": {}, "score": "32.559547"}
{"text": "These algorithms can drastically reduce the computational time needed for retrieving similar items , at the cost of a small probability of failing to find the absolute closest match .LSH has impacted fields as diverse as computer vision , databases , information retrieval , data mining , machine learning , and signal processing .", "label": "", "metadata": {}, "score": "32.960785"}
{"text": "To address this problem , we present a statistical performance model of Multi - probe LSH , a state - of - the - art variance of LSH .Our model can accurately predict the average search quality and latency given a small sample dataset .", "label": "", "metadata": {}, "score": "33.162033"}
{"text": "As described in Algorithm 2 , in this experiment , we first reduce the 69,000 training samples into .Experiments .In this section , we evaluate the proposed hashing algorithm on the large - scale image datasets MNIST and CIFAR-10 .", "label": "", "metadata": {}, "score": "33.572052"}
{"text": "In this paper , we introduce a data mining framework built on top of a recently developed fast approximate nearest - neighbor - finding algorithm(2 ) called locality - sensitive hashing ( LSH ) that can be used to mine huge chemical spaces in a scalable fashion using very modest computational resources .", "label": "", "metadata": {}, "score": "33.83677"}
{"text": "The training process of MVH - A is faster than MVH - CG , but the anchor representation of MVH - A may degrade the retrieval performance of the resulted hash codes .Experimental results on large - scale image datasets show that , in the case of image retrieval , the proposed algorithms are consistently superior to the state - of - the - art unsupervised methods such as PCAH , SH , and STH and outperform AGH with relatively longer codes .", "label": "", "metadata": {}, "score": "33.920856"}
{"text": "It should be noted that , for a few queries or a one - time analysis of a large combinatorial library , this approach does not offer significant advantages over the traditional kNN .However , if a library is to be repeatedly queried , the high speed exhibited by the LSH algorithm makes it an attractive approach .", "label": "", "metadata": {}, "score": "33.94501"}
{"text": "Furthermore , when viewed as a data - partitioning procedure , the LSH algorithm lends itself to easy parallelization of nearest - neighbor classification or regression .Full - text .The core of a VS informatics pipeline includes several data mining algorithms that work on huge databases of chemical compounds containing millions of molecular structures and their associated data .", "label": "", "metadata": {}, "score": "34.053528"}
{"text": "In this paper , we introduce a data mining framework built on top of a recently developed fast approximate nearest - neighbor- finding algorithm2called locality - sensitive hashing ( LSH ) that can be used to mine huge chemical spaces in a scalable fashion using very modest computational resources .", "label": "", "metadata": {}, "score": "34.07537"}
{"text": "Current methods generally assume that the data to be hashed comes from a multidimensional vector space , and require that the underlying embedding of the data be explicitly known and computable .For example , LSH relies on random projections with input vectors ; spectral hashing ( Weiss et al .", "label": "", "metadata": {}, "score": "34.36754"}
{"text": "Furthermore , when viewed as a data - partitioning procedure , the LSH algorithm lends itself to easy parallelization of nearest - neighbor classification or regression .In VS , one typically handles huge virtual chemical libraries containing millions of small molecules .", "label": "", "metadata": {}, "score": "34.538666"}
{"text": "The concept behind LSH is very simple , and the tool can be designed quite efficiently .Also , LSH requires minimal parameter tuning .The main parameter required is the error or approximation factor that we are willing to tolerate .", "label": "", "metadata": {}, "score": "34.686962"}
{"text": "We also observed ( in Figure 5 ) a monotonically increasing relationship between the logarithms of the mean query time and that of the number of neighbors that LSH found .One possible explanation for this observation is that , in order to yield several neighbors , it is the memory access that becomes the bottleneck .", "label": "", "metadata": {}, "score": "34.86774"}
{"text": "We have presented the LSH algorithm as a framework for working with large chemical data sets , and our results indicate that it provides an attractive alternative to traditional kNN algorithms in terms of time efficiency as well as flexibility .ACKNOWLEDGMENT We would like to thank Prof. P. Indyk and A. Andoni for providing us with the C++ source code for the LSH implementation .", "label": "", "metadata": {}, "score": "34.956764"}
{"text": "Fast solutions to the approximate NNS problem include Locality Sensitive Hashing ( LSH ) based techniques , which need storage polynomial in n with exponent greater than 1 , and query time sublinear , but still polynomial in n , wherenisthe size of the database .", "label": "", "metadata": {}, "score": "34.975323"}
{"text": "Our main technical contribution is to formulate the random projections necessary for LSH in kernel space .Our construction relies on an appropriate use of the central limit theorem , which allows us to approximate a random vector using items from our database .", "label": "", "metadata": {}, "score": "35.216457"}
{"text": "The performance of LSH i .. \" ...Neighborhood graphs are gaining popularity as a concise data representation in machine learning .However , naive graph construction by pairwise distance calculation takes O(n 2 ) runtime for n data points and this is prohibitively slow for millions of data points .", "label": "", "metadata": {}, "score": "35.366596"}
{"text": "Without using any training data , LSH and its variances can map close data samples to similar binary codes , and it is theoretically guaranteed that the original metrics are asymptotically preserved in the Hamming space as the code length increases .", "label": "", "metadata": {}, "score": "35.66223"}
{"text": "The LSH algorithm allows us to explore a variety of such spaces in a rapid fashion , which would not be feasible using the traditional kNN algorithm .As mentioned previously , another important use of the LSH algorithm would be to use it as a data partitioning scheme , whereby large libraries are divided into smaller chunks which can then be analyzed in detail by using classification or clustering techniques .", "label": "", "metadata": {}, "score": "35.87127"}
{"text": "We present a novel locality - sensitive hashing scheme for the decision version of the approximate nearest - neighbor problem in $ l_p^d$ for $ p \\in ( 0,2]$ , based on $ p$-stable distributions .Moreover , unlike earlier schemes , our LSH scheme works directly on points in $ R^d$ without embeddings .", "label": "", "metadata": {}, "score": "36.08013"}
{"text": "Although Locality - Sensitive Hashing ( LSH ) is a promising approach to similarity search in high - dimensional spaces , it has not been considered practical partly because its search quality is sensitive to several parameters that are quite data dependent .", "label": "", "metadata": {}, "score": "36.214355"}
{"text": "Exploring Chemical Space .As mentioned previ- ously , the LSH algorithm can be used to explore a descriptor space for a set of molecules .We investigated the use of the LSH algorithm to detect outlying molecules in the Kazius data set .", "label": "", "metadata": {}, "score": "36.677982"}
{"text": "By performing this construction appropriately , the algorithm can be applied entirely in kernel space , and can also be applied efficiently over very large data sets .Once we have computed the hash functions , we use standard LSH techniques to retrieve nearest neighbors of a query to the database in sublinear time .", "label": "", "metadata": {}, "score": "37.1896"}
{"text": "NN ) .( ii )To address the out - of - example extension difficulty , we propose a column generation - based solution of the derived optimization problem , named MVH - CG .As the size of training data increases , the construction of neighborhood graphs become infeasible .", "label": "", "metadata": {}, "score": "37.399937"}
{"text": "As with the Kazius data set , using radii between 10 % and 50 % of the maximum pairwise distances resulted in the LSH algorithm performing with 100 % accuracy .In the case of the NCI-3D data set , we find a similar situation .", "label": "", "metadata": {}, "score": "37.475548"}
{"text": "First , let us recall , how the standard LSH data structure works .We start from a -sensitive family and then consider the following simple \" tensoring \" operation : we sample functions from independently and then we hash a point into a tuple .", "label": "", "metadata": {}, "score": "37.580605"}
{"text": "Locality - sensitive hashing ( LSH ) reduces the dimensionality of high - dimensional data .LSH hashes input items so that similar items map to the same \" buckets \" with high probability ( the number of buckets being much smaller than the universe of possible input items ) .", "label": "", "metadata": {}, "score": "37.620132"}
{"text": "We have implemented the multi - probe LSH method and evaluated the implementation with two different high - dimensional datasets .Our evaluation shows that the multi - probe LSH method substantially improves upon previously proposed methods in both space and time efficiency .", "label": "", "metadata": {}, "score": "37.89437"}
{"text": "In this paper , we use LSH - based nearest - neighbor queries to find outliers quickly .Locality - Sensitive Hashing .If they are distant , they should collide with small probability .Thus , we have the following definitions .", "label": "", "metadata": {}, "score": "38.003082"}
{"text": "[ 1 ] Locality - sensitive hashing has much in common with data clustering and nearest neighbor search .An LSH family [ 1 ] [ 2 ] [ 3 ] is defined for a metric space , a threshold and an approximation factor .", "label": "", "metadata": {}, "score": "38.068604"}
{"text": "Thus , if we desire a coarse level of approximation , LSH can guarantee sublinear run times .Exploring Chemical Space .An important problem in the design and analysis of chemical libraries is the determination of which regions of the library are under- represented .", "label": "", "metadata": {}, "score": "38.324387"}
{"text": "This is a serious concern for many applications , in the fields of vision and learning , where we often deal with very high dimensional data .Algorithms for solving the approximate nearest - neighbor problem are known , that are relatively efficient .", "label": "", "metadata": {}, "score": "38.41472"}
{"text": "On the other hand , in LSH , we need to do a few dot products ( depending on the parameters that dictate the number of hash functions chosen ) per query .Effects of the Data Set Dimension .Theoretically , the LSH algorithm is linear in the number of dimensions of the data set .", "label": "", "metadata": {}, "score": "38.776405"}
{"text": "It is clear that , for both the original and reduced descriptor pools , the mean query time for the LSH algorithm is significantly lower compared to the kNN algorithm .We were also interested in understanding how the radius affects the number of nearest neighbors detected for a given point .", "label": "", "metadata": {}, "score": "39.06436"}
{"text": "This paper addresses the problem of image retrieval from an encrypted database , where data confidentiality is preserved both in the storage and retrieval process .The paper focuses on image feature protection techniques which enable similarity comparison among protected features .", "label": "", "metadata": {}, "score": "39.089653"}
{"text": "Experiments on large - scale image datasets demonstrate that the proposed method outperforms state - of - the - art hashing methods in many cases .Introduction .Nearest neighbor search is a fundamental problem in many applications concerned with information retrieval , including content - based multimedia retrieval [ 1 - 3 ] , object and scene recognition [ 4 ] , and image matching [ 5 ] .", "label": "", "metadata": {}, "score": "39.17017"}
{"text": "Our main goal is to build a knowledge base associated with as many concepts as possible for large scale object recognition studies .A second goal is supporting the building of more accurate text ... \" .We propose a new method for automated large scale gath - ering of Web images relevant to speci\u00afed concepts .", "label": "", "metadata": {}, "score": "39.262566"}
{"text": "In the case of the LSH algorithm , each query is associated with a set of nearest neighbors .Thus , for each query point , we considered the nearest neighbor in the set that was closest to the query point .", "label": "", "metadata": {}, "score": "39.29796"}
{"text": "However , we also considered radii between 10 % and 50 % of the maximum pairwise distance , which resulted in up to 90 % of the data set being considered as nearest neighbors for a given query point .At these radii , the nearest neighbors detected by the LSH algorithm were identical to those detected by the exact algorithm .", "label": "", "metadata": {}, "score": "39.335392"}
{"text": "Now let us show how to build a similar two - level data structure , which achieves somewhat better parameters .First , we apply the LSH family for with , but only partially .Namely , we choose a constant parameter and such that the collision probabilities are as follows : . at distance ; . at distance ; . at distance .", "label": "", "metadata": {}, "score": "39.33607"}
{"text": "The above discussion indicates that , in general , the use of a reduced descriptor pool provides some advantages in terms of mean query times and the mean number of nearest neighbors detected per query point .However , given that using the original descriptor pool results in mean query times that are generally 1 order of magnitude faster compared to those of the kNN algorithm , is there any advantage in performing descriptor reduction ?", "label": "", "metadata": {}, "score": "39.36457"}
{"text": "We then evaluated the mean and standard deviations of the distances from each query point to its nearest neighbor as detected by both algorithms .The results of this calculation for the Kazius data set are summarized in Table 3 .The results for the LSH algorithm were obtained at a radius of 0.06 % of the maximum pairwise distance in the data set .", "label": "", "metadata": {}, "score": "39.63389"}
{"text": "Although there exist solutions for these problems , they boil down to a linear scan when the space is intrinsically high - dimensional , as is the case in m ... \" .We introduce a new probabilistic proximity search algorithm for range and K - nearest neighbor ( K - NN ) searching in both coordinate and metric spaces .", "label": "", "metadata": {}, "score": "39.85264"}
{"text": "To reliably evaluate the proposed approaches , we create a moderate scale image set with region - level ground truth .The experimental results show that ( i ) spatial context constraints indeed help for accurate region annotation , ( ii ) the approaches combining the merits of discriminative learning and context constraints perform best , ( iii ) image retrieval can benefit from accurate regionlevel annotation . .", "label": "", "metadata": {}, "score": "39.89702"}
{"text": "To guarantee high search quality , the LSH scheme needs a rather large number of hash tables .This entails a large space require - ment , and in the distributed setting , with each query requir - ing a network call per hash bucket look up , this also entails a big network load .", "label": "", "metadata": {}, "score": "39.944946"}
{"text": "Texture feature estimation requires a finite neighborhood which limits the spatial resolutio ... \" .We propose an image segmentation algorithm that is based on spatially adaptive color and texture features .The features are first developed independently , and then combined to obtain an overall segmentation .", "label": "", "metadata": {}, "score": "39.98015"}
{"text": "This , for example , renders the K - NN approach to classification rather slow in large databases .Our novel idea is to predict closeness between elements according to how they order their distances towards a distinguished set of anchor objects .", "label": "", "metadata": {}, "score": "40.128723"}
{"text": "A second goal is supporting the building of more accurate text - based indexes for Web images .In our method , good quality candidate sets of images for each keyword are gathered as a function of analysis of the surrounding HTML text .", "label": "", "metadata": {}, "score": "40.22956"}
{"text": "This paper proposes a new indexing scheme called multi - probe LSH that overcomes this drawback .Multi - probe LSH is built on the well - known LSH technique , but it intelligently probes multiple buckets that are likely to contain query results in a hash table .", "label": "", "metadata": {}, "score": "40.326694"}
{"text": "The detail is described in [ 16 , 17].In the selection s .. \" ...Although Locality - Sensitive Hashing ( LSH ) is a promising approach to similarity search in high - dimensional spaces , it has not been considered practical partly because its search quality is sensitive to several parameters that are quite data dependent .", "label": "", "metadata": {}, "score": "40.57332"}
{"text": "This paper presents a randomized algorithm to embed a set of features into a single high - dimensional vector to simplify the feature - set matching problem .The main idea is to project feature vectors into an auxiliary space using locality sensitive hashing and to represent a set of features as a histogram in the auxiliary space .", "label": "", "metadata": {}, "score": "40.621758"}
{"text": "It is well - known that , even for the nearest - neighbor classifier , the error is at most twice the Bayesian classification error .Hence , our framework can be useful for the classifica- tion of large data sets as it will , at most , add a factor of 0.94 to any other classifier based on kNN .", "label": "", "metadata": {}, "score": "40.662334"}
{"text": "However , due to the complexity of computing the exact minimum cost matching , previous algorithms could only run efficiently when using a limited number of features per shape , and could not scale to perform retrievals from large databases .We present a contour matching algorithm that quickly computes the minimum weight matching between sets of descriptive local features using a recently introduced low - distortion embedding of the Earth Mover 's Distance ( EMD ) into a normed space .", "label": "", "metadata": {}, "score": "40.793594"}
{"text": "It will be shown that the query time for a single point is extremely small .Thus , a simple algorithm for obtaining outliers is to run the LSH algorithm for each point and for increasing radii iteratively and check whether the number of points exceeds a chosen sparsity threshold .", "label": "", "metadata": {}, "score": "40.90706"}
{"text": "In some cases , we proceed directly to clusters and do not directly determine the distances .We show how a hierarchical clustering can be read directly from one pass through the data .We offer insights also on practical implications of precision of data measurement .", "label": "", "metadata": {}, "score": "40.94377"}
{"text": "The preprocessing time needed per site is observed to match the query time .The data structure can be viewed as an application of a \" kd - tree \" approach in the metric space setting , using Voronoi regions of a subset in place of axis - aligned boxes . ...", "label": "", "metadata": {}, "score": "40.959248"}
{"text": "Conf .Acoust .Speech Signal Processing ( ICASSP , 2009 . \" ...This paper addresses the problem of image retrieval from an encrypted database , where data confidentiality is preserved both in the storage and retrieval process .The paper focuses on image feature protection techniques which enable similarity comparison among protected features .", "label": "", "metadata": {}, "score": "41.04901"}
{"text": "This is at most 2 - 3 min for the largest data sets we have tried ( the NCI-3D data set with about 250 000 compounds ) .kNN does not need to do this step .In fact , LSH excels when there is a need for a large number of nearest - neighbor queries .", "label": "", "metadata": {}, "score": "41.222748"}
{"text": "Our approach can be broadly divided into two steps .First , we may optionally want to transform the feature vector into a suitable space .Then , the data is projected on random lines using a new algorithm called LSH2based on the recently developed idea of random projections.13Second , we use these hash functions to detect the approximate nearest neighbors in the high - dimensional chemical descriptor space .", "label": "", "metadata": {}, "score": "41.941673"}
{"text": "Note that , in Figure 4 , the scales are all logarithmic .This shows that our metric is reasonable .Using this metric , it is possible to show that the LSH algorithm is about 4 orders of magnitude faster than the traditional kNN for the data sets we have considered .", "label": "", "metadata": {}, "score": "42.004753"}
{"text": "Although substantial progress has been made du ... \" .As the commonly used representation of a feature - rich data object has evolved from a single feature vector to a set of feature vectors , a key challenge in building a content - based search engine for feature - rich data is to match feature - sets efficiently .", "label": "", "metadata": {}, "score": "42.16578"}
{"text": "The main idea behind our approach is to construct a random hyperplane hash function , as in standard LSH , but to perform computations purely in kernel space .The construction is based on the central limit theorem , which will compute an approximate random vector using items from the database .", "label": "", "metadata": {}, "score": "42.214657"}
{"text": "Consider an LSH family .The algorithm has two main parameters : the width parameter k and the number of hash tables L .In the first step , we define a new family of hash functions g , where each function g is obtained by concatenating k functions from , i.e. , .", "label": "", "metadata": {}, "score": "42.21557"}
{"text": "To introduce this remarkably efficient algorithm to continuous domains such as images , signals and texts , we employ a random projection method to convert vectors to strings .Theoretical results are presented to elucidate the trade - off between approximation quality and computation time .", "label": "", "metadata": {}, "score": "42.25911"}
{"text": "Data - dependent hashing methods , instead , take advantage of the available data to learn more impact codes for specific tasks , leading to the recent endeavors in hashing .For instance , PCA hashing ( PCAH ) [ 11 ] generates linear hash functions through PCA projections of the training data and is suggested for producing more impact codes rather than random projections .", "label": "", "metadata": {}, "score": "42.29229"}
{"text": "For complex and high - dimensional problems such as pose estimation , the number of required examples and the computational complexity rapidly become prohibitively high .We describe a new algorithm that learns a set of hashing functions that efficiently index examples in a way relevant to a particular estimation task , thus essentially embedding the parameter similarity in a Hamming space .", "label": "", "metadata": {}, "score": "42.414375"}
{"text": "For machine learning applications there may , however , be some good news about such spatial access methods : when doing a k - NN classification we usually do not need to explicitly find the k - nearest neighbors .Instead we only need to know how many neighbors are from the positive class or , in some cases , which class is in the majority among the k - nearest neighbors .", "label": "", "metadata": {}, "score": "42.5291"}
{"text": "This paper gives a data structure for this problem ; the data structure is built using the distan ... \" .Given a set S of n sites ( points ) , and a distance measure d , the nearest neighbor searching problem is to build a data structure so that given a query point q , the site nearest to q can be found quickly .", "label": "", "metadata": {}, "score": "42.551144"}
{"text": "On this dataset , we can see that MVH - A yields rising performance as the number of bits increases .It outperforms all its competitors from .Figure 7 : Precision - recall curves for competing methods on the CIFAR-10 dataset for different code lengths .", "label": "", "metadata": {}, "score": "42.712303"}
{"text": "We can then use this core subroutine to solve several interesting problems including clustering , classifica- tion , outlier detection , and chemical diversity analysis , among several others .In this paper , we focus on the outlier detection problem .", "label": "", "metadata": {}, "score": "42.732933"}
{"text": "Similarity indices for high - dimensional data are very desirable for building content - based search systems for featurerich data such as audio , images , videos , and other sensor data .Recently , locality sensitive hashing ( LSH ) and its variations have been proposed as indexing techniques for approximate similarity search .", "label": "", "metadata": {}, "score": "42.858643"}
{"text": "We evaluated the proposed approach under three different task settings , i.e. content - based image search , image object recognition and near - duplicate video clip detection .The experimental results show that the proposed approach is indeed effective and flexible .", "label": "", "metadata": {}, "score": "43.027912"}
{"text": "In this work , we present an LSH - based technique for performing fast similarity searches over arbitrary kernel functions .The problem is as follows : given a kernel function and a database of n objects , how can we quickly find the most similar item to a query object in terms of the kernel function ?", "label": "", "metadata": {}, "score": "43.04887"}
{"text": "For realizing it , visual knowledge on various kinds of scenes is required .Then , we propose gathering visual knowledge on real world scenes for generic image classification from the World Wide Web .Our system gathers a large number of images from the Web automatically and makes use of them as training images for generic image classification .", "label": "", "metadata": {}, "score": "43.069412"}
{"text": "Due to the recent spread of digital imaging devices , the demand for image recognition of various kinds of real world scenes becomes greater .For realizing it , v ... \" .In this paper , we describe a generic image classification system with an automatic knowledge acquisition mechanism from the World Wide Web .", "label": "", "metadata": {}, "score": "43.14704"}
{"text": "Since the user 's goal behind top - k queries is to identify one or a few relevant and novel data items , it is intriguing to use approximate variants of TA to reduce run - time costs .This paper introduces a family of approximate top - k algorithms based on probabilistic arguments .", "label": "", "metadata": {}, "score": "43.164066"}
{"text": "But the constraint set is not sparse any more in t - SNE .Weinberger and Saul [ 14 ] , instead , proposed to maximize the variance of the embedded codes .That is , the mapping codes will be pulled apart as far as possible subject to the .", "label": "", "metadata": {}, "score": "43.394882"}
{"text": "In addition , even if a large library is analyzed once , any algorithm that makes multiple passes over the data ( such as k - means ) would benefit from an LSH - based preprocessing step .It is important to note that fingerprints can be used to rapidly determine which compounds in a library are similar to a query compound .", "label": "", "metadata": {}, "score": "43.50879"}
{"text": "On the other hand , since the outputs of MVH - CG are a set of binary - valued functions , we can learn the hash functions on the anchor set and then apply them to any unseen data items directly .", "label": "", "metadata": {}, "score": "43.53523"}
{"text": "Typically the hashing results appear qualitatively similar to ( or match exactly ) the linear scan results .We can see quantitatively how the results of the nearest neighbors extracted from KLSH compare to the linear scan nearest neighbors in the above plot .", "label": "", "metadata": {}, "score": "43.53595"}
{"text": "We also see that our MVH methods perform better at larger . , which confirms the observation in Figure 4 .PCAH performs worst in this case since it simply generates the hash hyperplanes by linear projects , which can not capture the nonlinear similarity information behind the training data .", "label": "", "metadata": {}, "score": "43.654865"}
{"text": "As a result , we only need to run the MVH - CG algorithm on the anchor set , and then apply the learnt binary - valued functions to hash the whole dataset .The anchor version of MVH ( referred as MVH - A ) is summarized in Algorithm 2 .", "label": "", "metadata": {}, "score": "43.880096"}
{"text": "n LSH family H to start with .Such families are known for a variety of metric spaces , including the Hamming distance , the Earth Mover Distance , and the Jaccard measure [ 10]. \" ...In this paper , we propose GAD ( General Activity Detection ) for fast clustering on large scale data .", "label": "", "metadata": {}, "score": "44.073822"}
{"text": "An appealing alternative is provided by state - of - the - art approximate NN search algorithms , which greatly improve on the running time and space complexity of the exact NN search .We analyze the accuracy loss incurred by the classification rule based on approximate nearest neighbor , and report empirical results for a variety of distributions .", "label": "", "metadata": {}, "score": "44.09478"}
{"text": ".. any methods of region segmentation have been proposed so far . byJunqing Chen , Thrasyvoulos Pappas , Mojsilovic , B. Rogowitz - In Proceedings on International Conference on Image Processing ( ICIP , 2002 . \" ...We propose an image segmentation algorithm that is based on spatially adaptive color and texture features .", "label": "", "metadata": {}, "score": "44.10066"}
{"text": "The resultant curves , termed R - NN curves , appear to follow a logistic model for any given descriptor space , which we justify theoretically for the 2D case .The method can be applied to data sets of arbitrary dimensions .", "label": "", "metadata": {}, "score": "44.12033"}
{"text": "The incremental aspect also provides significant benefits in situations when the number of desired neighbors is unknown in advance .Furthermore , our algorithm is at least as efficient as existing k - nearest neighbor algorithms , in terms of the number of distance computations and index node accesses .", "label": "", "metadata": {}, "score": "44.153786"}
{"text": "This is a problematic limitation , given that many recent successful vision results employ kernel functions for which the underlying embedding is known only implicitly ( i.e. , only the kernel function is computable ) .It is thus far impossible to apply LSH and its variants to search data with a number of powerful kernels --- including many kernels designed specifically for image comparisons , as well as some basic well - used functions like a Gaussian RBF .", "label": "", "metadata": {}, "score": "44.42935"}
{"text": "The image - gathering module gathers images related to given class keywords from the Web automatically .The learning module extracts image features from gathered images and associates them with each class .The image classification module classifies an unknown image into one of the classes corresponding to the class keywords by using the association between image features and classes .", "label": "", "metadata": {}, "score": "44.46277"}
{"text": "MVH - CG is consistently superior to MVH - A as more data are used in the learning process .Yet , MVH - A also catches up with AGH at .We then plot the precision - recall curves for the compared methods in Figure 5 .", "label": "", "metadata": {}, "score": "44.491264"}
{"text": "( 3 ) GAD based algorithms to handle the \" large clusters \" problem which appears in many large scale clustering applications .Two existing activity detection algorithms GT and CGAUTC are special cases under the framework .The most important contribution of our work is that the framework is the general solution to exploit activity detection for fast clustering in both exact and approximate senarios , and our proposed algorithms within the framework can achieve very high speed .", "label": "", "metadata": {}, "score": "44.554253"}
{"text": "In this paper we conduct a relatively complete study on how to exploit spatial context constraints for automated image region annotation .We present a straightforward method to regularize the segmented regions into 2D lattice layout , so that simple grid - structure graphical models can be employed to ... \" .", "label": "", "metadata": {}, "score": "44.65105"}
{"text": "Alternatively , spectral hashing ( SH ) [ 12 ] and self - taught hashing ( STH ) generate hash codes from the low - energy spectrums of data neighborhood graphs to seek nonlinear data representations .The difficulty is how to compute the code of an unseen item , which is known as the problem of out - of - sample extension .", "label": "", "metadata": {}, "score": "44.84401"}
{"text": "With the explosive growth of the data volume in modern applications such as web search and multimedia retrieval , hashing is becoming increasingly important for efficient nearest neighbor ( similar item ) search .Recently , a number of data - dependent methods have been developed , reflecting the great potential of learning for hashing .", "label": "", "metadata": {}, "score": "44.95955"}
{"text": "And it turns out that one can indeed exploit this dependency on data to get a slightly improved LSH family .Namely , we are able to achieve for , which by a simple embedding of into -squared gives for ( in particular , Hamming distance over the hypercube ) .", "label": "", "metadata": {}, "score": "44.98442"}
{"text": "We finish with a discussion of the role of unbalancing in metric space searching , and how it permits trading memory space for construction time . 1 Introduction The problem of proximity searching has received much attention in recent times , due to an increasing interest in manipulating and retrieving the more and more common multimedia data .", "label": "", "metadata": {}, "score": "45.077946"}
{"text": "The particular learned kernel we used has no simple , explicit feature embedding ( see the paper for details ) but the linear scan retrieval results are significantly better than the baseline Euclidean distance , thus providing another example where KLSH is useful for retrieval .", "label": "", "metadata": {}, "score": "45.251328"}
{"text": "This makes it especially difficult for mining dense regions of chemical space .We are willing to trade off accuracy for speed , and we desire approximate methods with coarse - grain guarantees on the accuracy .A guaranteed o(n ) method for a kNN search can help speed up several data mining applications , as we show in this paper .", "label": "", "metadata": {}, "score": "45.25181"}
{"text": "In this paper we present a sequential spectral learning approach to multi - view hashing where a hash function is sequentially determined by solving the successive maximization of local variances subject to decorrelation constraints .We compute multi - view local variances by \u03b1 - averaging view - specific distance matrices such that the best averaged distance matrix is determined by minimizing its \u03b1 - divergence from view - specific distance matrices .", "label": "", "metadata": {}, "score": "45.26679"}
{"text": "As in the case of the other data sets , the reduced data set leads to a higher value of C hNNfor larger radii .In this case , given the large number of nearest neighbors per query point for both the original and reduced descriptor pools , descriptor reduction is not necessary if all we require is a large set of nearest neighbors for further investigation .", "label": "", "metadata": {}, "score": "45.34719"}
{"text": "Thus , we considered a metric termed the normalized mean count of nearest neighbors per query point and investigated its variation with radius .This metric is defined as where Nt and Nq are the number of points in the training and query sets , respectively , and NNN , i is the number of nearest neighbors for the ith query point .", "label": "", "metadata": {}, "score": "45.35698"}
{"text": "Summary .We have shown that hashing can be performed over arbitrary kernels to yield significant speed - ups for similarity searches with little loss in accuracy .In experiments , we have applied KLSH over several kernels , and over several domains : .", "label": "", "metadata": {}, "score": "45.73796"}
{"text": "The main difficulty is how to generate hash codes for unseen points , which is known as out - of - sample extension problem .For this reason , [ 15 ] has to use the Nystr\u00f6m method [ 16 ] to learn eigenfunctions for a kernel matrix .", "label": "", "metadata": {}, "score": "45.814293"}
{"text": "The learned model is then applied to identify which images are visually relevant to the concept implied by the keyword .Implicitly , which regions or the images are relevant is also determined .Our experiments reveal that the new method performs much better than Google Image Search and a sim - ple method based on more standard content based image retrieval methods . . ..", "label": "", "metadata": {}, "score": "45.83702"}
{"text": "For vision applications , this complexity is amplified by the fact that often the most effective representations are high - dimensional or structured , and best known distance functions can require considerable computation to compare a single pair of objects .To make large - scale search practical , vision researchers have recently explored approximate similarity search techniques , most notably locality - sensitive hashing ( Indyk and Motwani 1998 , Charikar 2002 ) , where a predictable loss in accuracy is sacrificed in order to allow fast queries even for high - dimensional inputs .", "label": "", "metadata": {}, "score": "46.036304"}
{"text": "For SH , one reason may be that it attempts to keep all pairwise relationships during the mapping .Studies in dimensionality reduction point out that .NN kernel ( used in MVH - CG ) can analyze data that lies on a low - dimensional submanifold more faithfully than a predefined global kernel ( used in SH ) [ 23 ] .", "label": "", "metadata": {}, "score": "46.098557"}
{"text": "There are two parameters to tune LSH .Given a family H of hash functions as defined above , the LSH algorithm chooses k of them and concatenates them to amplify the gap between p1and p2 .Thus , for a point V , g(V ) )", "label": "", "metadata": {}, "score": "46.18688"}
{"text": "Inf .Model . , Vol .46 , No . 1 , 2006 331 .Page 12 . lies in a dense region of the descriptor space or in a more sparse region of the space , given a precalculated LSH data structure .", "label": "", "metadata": {}, "score": "46.25537"}
{"text": "In comparison with the entropy - based LSH method , to achieve the same search quality , multi - probe LSH uses less query time and 5 to 8 times fewer number of hash tables .m the query point is at most 1 + \u025b times the exact nearest neighbor 's distance .", "label": "", "metadata": {}, "score": "46.3151"}
{"text": "Hashing can be categorized into data - independent and data - dependent methods .The performance of LSH is not satisfactory when short binary codes are used [ 6].Datadependent hashing methods learn b ..Tools . by Keiji Yanai , Kobus Barnard - In Proc . of ACM SIGMM International Workshop on Multimedia Information Retrieval , 2005 . \" ...", "label": "", "metadata": {}, "score": "46.61149"}
{"text": "While this improves the LSH space re - quirement , it does not help with ( and in fact worsens ) the search network efficiency , as now each query offset requires a network call .In this paper , focusing on the Euclidian space under l2 norm and building up on Entropy LSH , we pro - pose the distributed Layered LSH scheme , and prove that it exponentially decreases the network cost , while maintaining a good load balance between different machines .", "label": "", "metadata": {}, "score": "46.64832"}
{"text": "From Table 3 , it is clear that the statistics of the 1-NN distances are identical for both the kNN and LSH algorithms .In general , performing a descriptor reduction leads to better results when using the kNN algorithm for classification or regression .", "label": "", "metadata": {}, "score": "46.702934"}
{"text": "Another aspect that was not considered in this study was the issue of scaling .In geometric terms , scaling the data set will shrink the extents of the descriptor space .This implies that the density of the space will increase .", "label": "", "metadata": {}, "score": "46.733986"}
{"text": "Here , our goal is to introduce a framework based on near neighbors and demonstrate its use in exploring the chemical spaces , as discussed later in this section .Execution Times .We were concerned with how fast our framework would be in comparison with a standard kNN routine with a large number of nearest neighbors .", "label": "", "metadata": {}, "score": "46.772224"}
{"text": "Some of the technical content of this talk will also be presented in a poster by Liu , Moore and Gray at the main NIPS conference poster session .Stefan Schaal University of Southern California .Approximate Nearest Neighbor Regression in Very High Dimensions As an alternative to fast nearest neighbor search methods , training data can also be on - line incorporated in appropriate sufficient statistics and adaptive data structures , such that nearest neighbor predictions can be accelerated by orders of magnitude .", "label": "", "metadata": {}, "score": "46.936317"}
{"text": "Scalable approaches to object recognition should be sublinear in the number of categories .We combine the idea of representative shape contexts ( Mori , Belongie & Malik ) with that of locality sensitive hashing ( Indyk & Motwani ) to achieve this objective .", "label": "", "metadata": {}, "score": "47.009274"}
{"text": "The underlying idea is to segment the input image several times , each time focussing on a different salient part of the image and to subsequently merge all obtained results into one composite segmentation .We identify salient parts of the image by applying affinity propagation clustering to efficiently calculated local color and texture models .", "label": "", "metadata": {}, "score": "47.078575"}
{"text": "Conf .Very Large Data Bases . \" ...Similarity indices for high - dimensional data are very desirable for building content - based search systems for featurerich data such as audio , images , videos , and other sensor data .", "label": "", "metadata": {}, "score": "47.185"}
{"text": "In this talk we will discuss algorithms and empirical results in accelerating K - NN classifiers in high dimensions without resorting to approximation .Spatial structures such as ball - trees and metric trees have frequently used as high dimensional indexes for finding the exact set of k - nearest neighbors .", "label": "", "metadata": {}, "score": "47.20426"}
{"text": "M. Raginsky and S. Lazebnik , \" Locality - sensitive binary codes from shift - invariant kernels , \" Advances in Neural Information Processing Systems , vol .View at Google Scholar .J. Wang , S. Kumar , and S. Chang , \" Semi - supervised hashing for large scale search , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "47.34977"}
{"text": "We will show cases from very high dimensional real - world datasets in which finding the k - NN using ball - trees is slower than a conventional linear scan , but performing k - NN classification with these new methods is more than one magnitude faster than a linear scan .", "label": "", "metadata": {}, "score": "47.37825"}
{"text": "Learning similarity metrics for vision problems Although a number of distance / similarity functions have been proposed for the k - nearest - neighbor algorithms , they have several limitations , including ( 1 ) the ad hoc nature , ( 2 ) demand for expert knowledge , etc .", "label": "", "metadata": {}, "score": "47.495552"}
{"text": "Talks : .Vassilis Athitsos Boston University .Approximate Nearest Neighbor Retrieval Using Euclidean Embeddings Joint work with Stan Sclaroff .This presentation describes a general framework for improving the efficiency of approximate nearest neighbor retrieval in spaces with computationally expensive distance measures .", "label": "", "metadata": {}, "score": "47.514572"}
{"text": "Another important advantage of our method is the ability to handle diverse types of similarities according to actual task requirements , including both feature similarities and semantic similarities like label consistency .We evaluate our method using both vector and non - vector data sets at a large scale up to 1 million samples .", "label": "", "metadata": {}, "score": "47.624428"}
{"text": "The natural question that arises from the above algorithm is how to vary the query radius .A naive way is to increase it linearly .A better approach is to use a technique similar to a binary search , which will take log Dm steps of LSH , where Dm is the maximum distance specified .", "label": "", "metadata": {}, "score": "47.691044"}
{"text": "The adaptive probing method addresses the problem that even though the average performance is tuned for optimal , the variance of the performance is extremely high .We experimented with three different datasets including audio , images and 3D shapes to evaluate our methods .", "label": "", "metadata": {}, "score": "47.77903"}
{"text": "M. Datar , N. Immorlica , P. Indyk , and V. S. Mirrokni , \" Locality - sensitive hashing scheme based on p -stable distributions , \" in Proceedings of the 20th Annual Symposium on Computational Geometry ( SCG ' 04 ) , pp .", "label": "", "metadata": {}, "score": "47.975464"}
{"text": "Anchor Hashing Using MVH .To solve the hashing problem more efficiently , Liu et al .[15 ] proposed to represent a data point by a set of anchors , which are the cluster centers obtained by running K - means on the whole ( or a random selected small subsample of the ) database .", "label": "", "metadata": {}, "score": "48.02633"}
{"text": "L. Saul , K. Weinberger , J. Ham , F. Sha , and D. Lee , \" Spectral methods for dimensionality reduction , \" Semisupervised Learn , pp .293 - 308 , 2006 .View at Google Scholar Approximate Nearest Neighbors Methods for Learning and Vision .", "label": "", "metadata": {}, "score": "48.217865"}
{"text": "It shows how great gains in efficiency can be enjoyed at low result - quality penalties .Further , KLEE affords the query - initiating peer the flexibility to trade - off result quality and expected performance and to trade - off the number of communication phases engaged during query execution versus network bandwidth performance .", "label": "", "metadata": {}, "score": "48.249115"}
{"text": "We present extensive experiments comparing our method against state - of - the - art exact and approximate techniques , both in synthetic and real , metric and non - metric databases , measuring both CPU time and distance computations .The experiments demonstrate that our technique almost always improves upon the performance of alternative techniques , in some cases by a wide margin . \" ...", "label": "", "metadata": {}, "score": "48.324768"}
{"text": "But as we can see , we drop every fixed point with low probability .The most basic but essential task in image search is the \" nearest neighbor \" problem : to take a query image and accurately find the examples that are most similar to it within a large database .", "label": "", "metadata": {}, "score": "48.353035"}
{"text": "An answer to this type of question would allow the users of a library to understand what types of compounds can be acquired to better represent a region of chemical space .The question p)1/sX , can be reversed by asking which compounds lie in the denser regions of chemical space covered by the library .", "label": "", "metadata": {}, "score": "48.427963"}
{"text": "Informally speaking , the closer two points are , the larger probability of their collision is .Let us construct a simple LSH family for hypercube , equipped with Hamming distance .We set , where .It is easy to check that this family is -sensitive .", "label": "", "metadata": {}, "score": "48.474365"}
{"text": "Our final goal is to have a single modular framework for the most common tasks in the mining of huge chemical libraries that can be integrated with different informatics pipelines for virtual screening .In this paper , we present a scalable framework for classification and clustering large chemical libraries with the help of recently developed geometric techniques such as random projections that give approximate answers but are guaranteed to run fast .", "label": "", "metadata": {}, "score": "48.602077"}
{"text": "We can find outliers very quickly in this case , and we have manually validated and confirmed the outliers .METHODS We now present a very high - level view of our approach .Assume that each chemical compound is described by a set or a feature vector of chemical descriptors .", "label": "", "metadata": {}, "score": "48.65184"}
{"text": "In this paper , we develop a new hashing algorithm to create efficient codes for large scale data of general formats with any kernel function , including kernels on vectors , graphs , sequences , sets and so on .Starting with the idea analogous to spectral hashing , novel formulations and solutions are proposed such that a kernel based hash function can be explicitly represented and optimized , and directly applied to compute compact hash codes for new samples of general formats .", "label": "", "metadata": {}, "score": "48.80743"}
{"text": "Or say , why the first level hashing is inevitable ?In my current understanding , the first level hashing is used to generate clusters with small bounded and rounded diameter and introduce randomness .Are there any other purposes ?( 2 ) why drop points before the second level hashing ?", "label": "", "metadata": {}, "score": "48.851486"}
{"text": "Some attempts have been made to cir- cumvent this bound by using statistical techniques such as the KS test14or cell - based methods.8We will show , in this paper , that outlier detection can be done by analyzing the nearest - neighbor space around a compound .", "label": "", "metadata": {}, "score": "48.92887"}
{"text": "( Key , Value ) based distributed frameworks , such as MapRe - duce , Memcached , and Twitter Storm are gaining increas - ingly widespread use in applications that process large amounts of data .One important example application is large scale similarity search , for which Locality Sensitive Hashing ( LSH ) has emerged as the method of choice , specially when the data is high - dimensional .", "label": "", "metadata": {}, "score": "49.036385"}
{"text": "Here , we focus on locality sensitive hashing techniques that are most relevant to our work .Locality sensitive hashing ( LSH ) , introduced by Indyk and Motwani , is the best - known index ... . \" ...Abstract The metric space model abstracts many proximity search problems , from nearest - neighborclassifiers to textual and multimedia information retrieval .", "label": "", "metadata": {}, "score": "49.051163"}
{"text": "It has been shown that it compares favorably against alternative data structures in spaces of high dimension or queries with low selectivity .Its main drawbacks are : costly construction ... \" .The Spatial Approximation Tree ( sa - tree ) is a recently proposed data structure for searching in metric spaces .", "label": "", "metadata": {}, "score": "49.094494"}
{"text": "Recently , hashing has become a popular method to address this issue in terms of storage and speed .Seminal work of hashing , such as locality - sensitive hashing ( LSH ) [ 6 ] , focuses on using random projection to generate random binary codes in the Hamming space .", "label": "", "metadata": {}, "score": "49.117615"}
{"text": "The underlying idea is to segment the input image several times , each time focussing on a different salient part of the image and to subsequently merge all obtained results into one composite segmentation .We identify salient parts of ... \" .", "label": "", "metadata": {}, "score": "49.12082"}
{"text": "The first is based on mapping to a low - dimensionalvector space ( making use of data structures such as the R - tree ) , while the second directly indexes the objects based on distances ( making use of data structures such as the M - tree ) .", "label": "", "metadata": {}, "score": "49.270035"}
{"text": "A near linear storage space solution was proposed by Panigrahy [ 32 ] which has space requirement of \u00d5(n ) and but a larger query time \u00d5(n2.09/c ) using entropy based techniques along with using the LS ... . by Wenjun Lu , Avinash L. Varna , Ashwin Swaminathan , Min Wu - in Proc .", "label": "", "metadata": {}, "score": "49.275112"}
{"text": "Existing methods for handling simi ... \" .Similarity search is a very important operation in multimedia databases and other database applications involving complex objects , and involves finding objects in a data set S similar to a query object q , based on some distance measure d , usually a distance metric .", "label": "", "metadata": {}, "score": "49.329987"}
{"text": "However , indexes lose their efficiency as the intrinsicdata dime ... \" .Abstract The metric space model abstracts many proximity search problems , from nearest - neighborclassifiers to textual and multimedia information retrieval .In this context , an index is a data structure that speeds up proximity queries .", "label": "", "metadata": {}, "score": "49.42386"}
{"text": "The most popular technique , k - means clustering , however , has two inherent limitations : the clusters are constrained to be spherically symmetric and their number has to be known a priori .In nonparametric clustering methods , like the one based on mean shift , these limitations are eliminated but the amount of computation becomes prohibitively large as the dimension of the space increases .", "label": "", "metadata": {}, "score": "49.529816"}
{"text": "Benefits of the RNN algorithm include its inherent applicability to multi - class problems , its ability to learn a vast class of functions , and its ability to avoid overfitting through the use of capacity control .This work presents the algorithm and an wmpirical study focusing visualizable two - dimensional toy problems , which demonstrates that improvement in generalization performance is attainable with RNN .", "label": "", "metadata": {}, "score": "49.60763"}
{"text": "One of the easiest ways to construct an LSH family is by bit sampling .[ 3 ] This approach works for the Hamming distance over d - dimensional vectors .Here , the family of hash functions is simply the family of all the projections of points on one of the coordinates , i.e. , , where is the th coordinate of .", "label": "", "metadata": {}, "score": "49.607903"}
{"text": "For small angles ( not too close to orthogonal ) , is a pretty good approximation to .The random projection method of LSH due to Moses Charikar [ 4 ] called SimHash ( also sometimes called arccos [ 17 ] ) is designed to approximate the cosine distance between vectors .", "label": "", "metadata": {}, "score": "49.641533"}
{"text": "For n points , the traditional kNN will take O(dn2 ) , whereas LSH will take O(dnF+1log n ) , which is clearly o(n2 ) or , in other words , less than O(n2 ) .Though we use approximate nearest neighbors , we do not lose accuracy in finding sparse regions .", "label": "", "metadata": {}, "score": "49.738583"}
{"text": "For example , it is well- known that the asymptotic error of a nearest - neighbor classifier is at most twice that of Bayesian classification.5 Another important data mining problem is finding the sparse regions in a chemical space defined by a set of molecular descriptors .", "label": "", "metadata": {}, "score": "49.796227"}
{"text": "For practical scenarios , the speedup is around 2 - 3 orders of magnitude .We show similar speedups in Figure 4 .For the reduced descriptor set , the speedups were not as great .Thus , LSH seems to be relatively faster for higher dimensionalities Figure 2 . neighbors detected by the LSH algorithm versus the radius for the NCI - AIDS data set using the original and reduced descriptor pools .", "label": "", "metadata": {}, "score": "49.925137"}
{"text": "The radii are reported as a percentage of the maximum pairwise distance in the data using the original descriptor pool .For the Kazius data set , the mean query time for the reduced descriptor pool is generally smaller than that for the original descriptor pool .", "label": "", "metadata": {}, "score": "49.987343"}
{"text": "We also explore the use of query expansion based on the constructed image thesaurus for improving image retrieval performance .e images so that we can relate high - level semantics to low - level features to bridge the cognitive gap .", "label": "", "metadata": {}, "score": "50.06404"}
{"text": "However , by viewing this problem from a different angle and removing the constraint of Hamming space , it can be seen as a variation of the traditional dimensionality reduction problem .Meanwhile , Liu et al .[ 15 ] recently proposed a scalable graph - based hashing method , named anchor graph hashing ( AGH ) .", "label": "", "metadata": {}, "score": "50.121513"}
{"text": "Differing provisions from the publisher 's actual policy or licence agreement may be applicable . \"As noted in subsection III , there is extensive literature in this area , e.g. [ 16].While random projection per se will not guarantee a bijection of best match in original and in lower dimensional spaces , our use of projection here is effectively a hashing method .", "label": "", "metadata": {}, "score": "50.131134"}
{"text": "For both cases , the query set consisted of 200 observations and the remainder were placed in the training set .The radii are reported as a percentage of the maximum pairwise distance in the original descriptor pool .328 J. Chem .", "label": "", "metadata": {}, "score": "50.473022"}
{"text": "We combine a previously proposed adaptive clustering algorithm for color segmentation with a simple but effective texture segmentation approach to obtain an overall image segmentation .Our focus is in the domain of photographic images with an essentially unlimited range of topics .", "label": "", "metadata": {}, "score": "50.4924"}
{"text": "However , in both cases , the mean query times are 1 order of magnitude and , for smaller radii , 2 orders of magnitude faster compared to the kNN algorithm ( k ) 200 ) .Figure 8 displays the results for the NCI- 3D data set , and it is clear that the behavior is similar to that of the NCI - AIDS data set .", "label": "", "metadata": {}, "score": "50.569546"}
{"text": "Our primary metric for comparison is the mean query time .For radii that yielded around 200 neighbors , the mean query time was 0.004 s. On the other hand , kNN took 0.665 s per query .We computed a modest 200 nearest neighbors per query .", "label": "", "metadata": {}, "score": "50.683083"}
{"text": "In recent years , some data structures have been found that have provable properties under fairly weak restrictions on the metric space , such as a sphere - packing bound .These data structures have relatives that can be seen experimentally to have attractive practical properties : ease of implementation , low space , O(log n ) performance in low dimension , and graceful degradation to brute force .", "label": "", "metadata": {}, "score": "51.01502"}
{"text": "Plot of mean query times versus the size of the data set plotted on a log - log scale using both original and reduced descriptor pools .For each data set , the first 200 observations were taken as the query set and the remainder placed in the training set .", "label": "", "metadata": {}, "score": "51.164753"}
{"text": "It should be noted that , unlike the previous experiments , we considered the whole data set as the query set rather than taking a subset of the whole data set .We considered both the original pool of 142 descriptors and the reduced pool of 20 descriptors .", "label": "", "metadata": {}, "score": "51.25567"}
{"text": "This fusion framework has been successfully applied on the Berkeley image database .The experiments reported in this paper demonstrate that the proposed method is efficient in terms of visual evaluation and quantitative performance measures and performs well compared to the best existing state - of - the - art segmentation methods recently proposed in the literature .", "label": "", "metadata": {}, "score": "51.332462"}
{"text": "We show how this framework can be applied in both classes of similarity search methods , by defining a suitable search hierarchy for a number of different indexing structures .Armed with an appropriate search hierarchy , our algorithm thus performs incremental similarity search , wherein the result objects are reported one by one in order of similarity to a query object , with as little effort as possible expended to produce each new result object .", "label": "", "metadata": {}, "score": "51.365364"}
{"text": "For comparison , the mean query times for the kNN algorithm using the original and reduced descriptor pools are plotted .The kNN algorithm was run with k ) 200 , as described before .It is clear that , by using the reduced descriptor pool , the mean query time for the kNN algorithm increases by nearly 1 order of magnitude .", "label": "", "metadata": {}, "score": "51.38949"}
{"text": "Numerical experiments on Caltech-256 , CIFAR-20 , and NUS - WIDE datasets confirm the high performance of our method , in comparison to single - view spectral hashing as well as existing multi - view hashing methods . ... cts are indexed by binary codes with small Hamming distances .", "label": "", "metadata": {}, "score": "51.6845"}
{"text": "We present a straightforward method to regularize the segmented regions into 2D lattice layout , so that simple grid - structure graphical models can be employed to characterize the spatial dependencies .We show how to represent the spatial context constraints in various graphical models and also present the related learning and inference algorithms .", "label": "", "metadata": {}, "score": "51.766567"}
{"text": "Comb .Chem .High Throughput Screening 2004 , 7 , 259 - 269 .( 4 ) Xu , H. ; Agrafiotis , D. Nearest Neighbor Search in General Metric Spaces Using a Tree Data Structure with a Simple Heuristic .", "label": "", "metadata": {}, "score": "51.852272"}
{"text": "Full - text \u00b7 Article \u00b7 Nov 2011 \u00b7 P - Adic Numbers Ultrametric Analysis and Applications .\" Traditionally this approach has a running time that is quadratic with the number of points in the dataset , though this can be improved by use of data structures such as kd - trees .", "label": "", "metadata": {}, "score": "51.952477"}
{"text": "Flickr Scene Recognition .We performed a similar experiment with a set of Flickr images containing tourist photos from a set of landmarks .Here , we applied a chi - squared kernel on top of SIFT features for the nearest neighbor search .", "label": "", "metadata": {}, "score": "52.115387"}
{"text": "Clustering is a useful preprocessing step before the actual classification .Also , classification can be considered to be a clustering problem , for example , if we want to partition the data set into k unique classes of activities .Thus , we would like to have a method that can be used for classification and clustering at the same time .", "label": "", "metadata": {}, "score": "52.2262"}
{"text": "The precision and the efficiency of the developed methods are experimentally evaluated based on a large Web corpus and a structured data collection . ... ition for stopping the algorithm is a conservative TA - style test .In this context , probabilistic estimators for selection cutoff values have been developed by ... . \" ...", "label": "", "metadata": {}, "score": "52.290043"}
{"text": "Experiments demonstrate that the resulting algorithm , which we call Parameter - Sensitive Hashing , can rapidly and accurately estimate the articulated pose of human figures or hands using a very large database .Ilan Shimshoni Technion .Adaptive Mean Shift Based Clustering in High Dimensions Joint work with Bogdan Georgescu and Peter Meer .", "label": "", "metadata": {}, "score": "52.414932"}
{"text": "Unless mentioned otherwise , we assume that descriptors are real - valued .There are various other kinds of descriptors as well,9and our techniques may also apply to them with some extra embedding steps .Thus , given a chemical space , we would like to identify sparse regions where there are a few active compounds .", "label": "", "metadata": {}, "score": "52.41812"}
{"text": "From these plots , one may conclude that large descriptor pools do not Table 3 .For comparison , the summary statistics for the nearest neighbor distances obtained by the 1-NN algorithm are also presented .SCALABLE PARTITIONING OF CHEMICAL SPACES J. Chem .", "label": "", "metadata": {}, "score": "52.43322"}
{"text": "7 thoughts on \" Beyond Locality - Sensitive Hashing \" .Nice !Also , is there any lower bound or intuition of lower bound with this new , weaker definition of LSH ?And what part of your approach ( if it 's obvious , I apologize ) actually hinges on the new definition ( restricting to the set $ P$ ) ?", "label": "", "metadata": {}, "score": "52.486214"}
{"text": "For both cases , the query set consisted of 200 observations and the remainder were placed in the training set .The radii are reported as a percentage of the maximum pairwise distance in the original descriptor pool .Figure 10 .", "label": "", "metadata": {}, "score": "52.533844"}
{"text": "Hashing is much faster than alternative methods because it avoids the pairwise comparisons required for partitioning and clas- sification .The method especially excels when the data set size is very large .As an application of this framework , we study a small problem in diversity analysis , that is , the problem of outlier detection .", "label": "", "metadata": {}, "score": "52.5597"}
{"text": "That said , in practice the method is robust to the number of database objects chosen for the construction of the random vectors , and behaves comparably to standard LSH on non - kernelized data .80 Million Tiny Images .We ran KLSH over the 80 million images in the Tiny Image data set .", "label": "", "metadata": {}, "score": "52.65841"}
{"text": "Abstract .Learning to hash involves learning hash functions from a set of images for embedding high - dimensional visual descriptors into a similarity - preserving low - dimensional Hamming space .Most of existing methods resort to a single representation of images , that is , only one type of visual descri ... \" .", "label": "", "metadata": {}, "score": "52.664288"}
{"text": "But now given this \" bounded buckets \" condition , we can utilize the better family designed above !Namely , we hash every bucket using our new family to achieve the following probabilities : . at distance ; . at distance .", "label": "", "metadata": {}, "score": "52.693287"}
{"text": "In addition , Weinberger and Saul [ 14 ] proved that if we add a small number of edges over the .NNs in the constraint , which is also a problem in the research of manifold learning .Various methods have been proposed in the literature to overcome this problem .", "label": "", "metadata": {}, "score": "52.717194"}
{"text": "10 , pp . 1 - 41 , 2009 .View at Google Scholar .K. Weinberger and L. Saul , \" Unsupervised learning of image manifolds by semidefinite programming , \" International Journal of Computer Vision , vol .70 , no . 1 , pp .", "label": "", "metadata": {}, "score": "52.765057"}
{"text": "We demonstrate the high quality of our method on the well - known Berkeley segmentation database .Furthermore we show that our method can be used to provide good spatial support for recognition frameworks . by Xin - jing Wang , Wei - ying Ma , Xing Li - in Image Retrieval .", "label": "", "metadata": {}, "score": "52.78682"}
{"text": "We applied our method on the Caltech-101 for object recognition , as there have been several recent kernel functions for images that have shown very good performance for object recognition , but have unknown or very complex featureembeddings .This data set also allowed us to test how changes in parameters affect hashing results .", "label": "", "metadata": {}, "score": "52.799225"}
{"text": "Inf .Comput .Sci .( 5 ) Cover , T. ; Hart , P. Nearest Neighbor Pattern Classification .IEEE Trans .Inf .Theory 1967 , 13 , 21 - 7 .( 6 ) Pearlman , R. ; Smith , K. Metric Validation And The Receptor - Relevant Subspace Concept .", "label": "", "metadata": {}, "score": "52.964325"}
{"text": "Inf .Comput .Sci .( 7 ) Pearlman , R. ; Smith , K. Novel Software Tools For Chemical Diversity .Perspect .Drug DiscoVery Des .( 8) Schnur , D. Design and Diversity Analysis of Large Combinatorial Libraries Using Cell - Based Methods .", "label": "", "metadata": {}, "score": "52.965584"}
{"text": "Neighborhood graphs are gaining popularity as a concise data representation in machine learning .However , naive graph construction by pairwise distance calculation takes O(n 2 ) runtime for n data points and this is prohibitively slow for millions of data points .", "label": "", "metadata": {}, "score": "53.205776"}
{"text": "It is normally difficult to optimize them simultaneously , and the fact that the former one is a group of functions , even adds to the difficulty of solving the problem .We can use the column generation ( CG ) technique to find an approximate solution of it iteratively .", "label": "", "metadata": {}, "score": "53.232407"}
{"text": "The top left image in each set is the query .The remainder of the top row shows the top nearest neighbor using a linear scan ( with the Gaussian kernel ) and the second row shows the nearest neighbor using KLSH .", "label": "", "metadata": {}, "score": "53.249252"}
{"text": "We performed a descriptor reduc- tion using an identical test with a cutoff set to 0.7 and a correlation test with a cutoff set to 0.6 .The sizes of the reduced descriptor pools for the three data sets are sum- marized in Table 1 .", "label": "", "metadata": {}, "score": "53.2678"}
{"text": "In the information theory literature , it arises as the problem of building a vector quantization encoder [ LBG80 , GN93].In the pattern recognition ( or statistics or learning theory ) literature , it m ..We present KLEE , a novel algorithmic framework for distributed top - k queries , designed for high performance and flexibility .", "label": "", "metadata": {}, "score": "53.560146"}
{"text": "The chamfer distance is used as a measure of distance between binary edge images of hands .The computational complexity of the chamfer distance can make it impractical for large databases , but the presented framework is used to achieve efficient retrieval with negligible loss of accuracy .", "label": "", "metadata": {}, "score": "53.719185"}
{"text": "Indyk et al.2showed that s - stable distributions can be used to construct such families of locality - sensitive hash functions .An s - stable distribution is defined as follows .Definition 3 .We chop the real line into equal - width segments of appropriate size and assign hash values to vectorson the basis of which segment they project onto .", "label": "", "metadata": {}, "score": "53.74504"}
{"text": "Due to independence , the collision probabilities multiply , and we get .Then we argue as before and conclude that we can achieve .After carefully optimizing all the parameters , we achieve , in fact , .Then we go further , and consider a multi - level scheme with several distance scales .", "label": "", "metadata": {}, "score": "53.771324"}
{"text": "Such scales are not uncommon in chemical libraries .Our research is based on several goals .First , we want a hierarchical scheme where we can trade off speed for accuracy , thus empowering the user .Next , we want our scheme to be independent of the semantics and the choice of the features we choose to describe the data set .", "label": "", "metadata": {}, "score": "53.825054"}
{"text": "Thus , the core algorithms for the scalable mining of large chemical data sets can be based on a quick solution to the kNN problem .Thus , we would like to have a fast kNN routine as the core of a data mining framework .", "label": "", "metadata": {}, "score": "53.97242"}
{"text": "For very large chemical libraries , the number of neighbors might be very large .For a large number of neighbors , the time taken to retrieve the data points might require several memory accesses .It is well - known that , for processing huge data sets , memory access time dominates the processing time , and it is a few orders of magnitude larger than the latter .", "label": "", "metadata": {}, "score": "53.998688"}
{"text": "For comparison , the mean query times for the traditional kNN with k ) 200 are also plotted .SCALABLE PARTITIONING OF CHEMICAL SPACES J. Chem .Inf .Model . , Vol .46 , No . 1 , 2006 325 .", "label": "", "metadata": {}, "score": "54.087326"}
{"text": "Also , L such groups of hash functions are chosen , indepen- dently and uniformly at random , ( i.e. , g1-gL ) to reduce the error .During preprocessing , each point V is hashed by the L function 's buckets and stored in the bucket given by each of the gi(V ) 's .", "label": "", "metadata": {}, "score": "54.168232"}
{"text": "In this paper we present a simple index called list of clusters ( LC ) , which is based on a compact partitioning of the data set .The LC is shown to require little space , to be suitable both for main and secondary memory implementations , and most importantly , to be very resistant to the intrinsic dimensionality of the data set .", "label": "", "metadata": {}, "score": "54.28854"}
{"text": "66 - 78 , 2012 .View at Google Scholar . A. Gionis , P. Indyk , R. Motwani , et al . , \" Similarity search in high dimensions via hashing , \" in Proceedings of the International Conference on Very Large Data Bases , pp .", "label": "", "metadata": {}, "score": "54.328205"}
{"text": "..The distance distortion under both L1 and L2 distance metrics can be made arbitr ... . by Saehoon Kim , Seungjin Choi - in Proceedings of the IEEE International Conference on Data Mining ( ICDM , 2011 . \" ...Abstract - Hashing refers to methods for embedding highdimensional data into a similarity - preserving low - dimensional Hamming space such that similar objects are indexed by binary codes whose Hamming distances are small .", "label": "", "metadata": {}, "score": "54.381805"}
{"text": "The digest identifying each message should not vary significantly for changes that can be produced automatically .TLSH is locality - sensitive hashing algorithm designed for a range of security and digital forensic applications .[ 15 ] The goal of TLSH is to generate a hash digest of document such that if two digests have a low distance between them , then it is likely that the messages are similar to each other .", "label": "", "metadata": {}, "score": "54.419563"}
{"text": "Such a family is called -sensitive .Alternatively [ 4 ] it is defined with respect to a universe of items U that have a similarity function .An LSH scheme is a family of hash functions H coupled with a probability distribution D over the functions such that a function chosen according to D satisfies the property that for any .", "label": "", "metadata": {}, "score": "54.43373"}
{"text": "STH addresses this problem in another way .By viewing the binary codes of the training data as pseudo - labels , it learns the hash functions via an extra pseudo - supervised learning stage .Nevertheless , learning errors in the self - taught stage may collapse the manifold structure of the learning data as illustrated in Figure 1 .", "label": "", "metadata": {}, "score": "54.703537"}
{"text": "Abstract - This paper presents a novel segmentation approach based on a Markov random field ( MRF ) fusion model which aims at combining several segmentation results associated with simpler clustering models in order to achieve a more reliable and accurate segmentation result .", "label": "", "metadata": {}, "score": "54.762276"}
{"text": "Abstract - This paper presents a novel segmentation approach based on a Markov random field ( MRF ) fusion model which aims at combining several segmentation results associated with simpler clustering models in order to achieve a more reliable and accurate segmentation result .", "label": "", "metadata": {}, "score": "54.762276"}
{"text": "The CIFAR-10 dataset consists of 60,000 images of 10 classes , which means that there are 6,000 samples for each class .In our experiments , we use the original 784-dimension pixel representation for the MNIST dataset and a 512-dimension GIST [ 24 ] feature for the CIFAR-10 dataset .", "label": "", "metadata": {}, "score": "54.76357"}
{"text": "Note that most of the aforementioned data mining prob- lems do not have guaranteed optimal exact solutions as they are mostly NP - Hard problems,15and we need to design either good heuristics or approximation algorithms tuned for the particular data set that needs to be mined .", "label": "", "metadata": {}, "score": "54.80926"}
{"text": "Scalable similarity search is the core of many large scale learning or data mining applications .Recently , many research results demonstrate that one promising approach is creating compact and efficient hash codes that preserve data similarity .By efficient , we refer to the low correlation ( and thus low redundancy ) among generated codes .", "label": "", "metadata": {}, "score": "54.81822"}
{"text": "20104307110002 , Hunan Provincial Innovation Foundation For Postgraduate under no .CX2010B028 , Fund of Innovation in Graduate School of NUDT under nos .B100603 and B120605 .References .M. S. Lew , N. Sebe , C. Djeraba , and R. Jain , \" Content - based multimedia information retrieval : state of the art and challenges , \" ACM Transactions on Multimedia Computing , Communications and Applications , vol .", "label": "", "metadata": {}, "score": "54.82627"}
{"text": "( 2 ) Approximate GAD a ... \" .In this paper , we propose GAD ( General Activity Detection ) for fast clustering on large scale data .Within this framework we design a set of algorithms for different scenarios : ( 1 ) Exact GAD algorithm E - GAD , which is much faster than K - Means and gets the same clustering result .", "label": "", "metadata": {}, "score": "54.995438"}
{"text": "We also present a method to numerically characterize the R - NN curves thus allowing identification of outliers in a single plot .Full - text \u00b7 Article \u00b7 Jul 2006 \u00b7 Journal of Chemical Information and Modeling", "label": "", "metadata": {}, "score": "55.012184"}
{"text": "For both cases , the query set consisted of 200 observations and the remainder were placed in the training set .The radii are reported as a percentage of the maximum pairwise distance in the original descriptor pool .Figure 11 .", "label": "", "metadata": {}, "score": "55.042328"}
{"text": "Traditionally , this type of feature selection has involved removing descrip- tors which are identical for a certain percentage of the data set ( identical test ) and removing correlated descriptors ( correlation test ) .Though the former task is quite rapid , the latter can be time - consuming for larger data sets .", "label": "", "metadata": {}, "score": "55.08879"}
{"text": "A thorough study of the related work is beyond the scope of this paper .A brief survey can be found in a standard book on pattern classification10,11 or in a recent clustering paper12and its references for clustering very large data sets and streams .", "label": "", "metadata": {}, "score": "55.111984"}
{"text": "From now on , assume that our metric is .The first ingredient is an LSH family that simplifies and improves upon for the case , when all data points and queries lie in a ball of radius .This scheme has strong parallels with an SDP rounding scheme of David Karger , Rajeev Motwani and Madhu Sudan .", "label": "", "metadata": {}, "score": "55.33496"}
{"text": "Abstract - Hashing seeks an embedding of high - dimensional objects into a similarity - preserving low - dimensional Hamming space such that similar objects are indexed by binary codes with small Hamming distances .A variety of hashing methods have been developed , but most of them resort to a single view ( r ... \" .", "label": "", "metadata": {}, "score": "55.475716"}
{"text": "There are some limitations to the method .The random vector constructed by the KLSH routine is only approximately random ; general bounds on the central limit theorem are unknown , so it is not clear how many database objects are required to get a sufficiently random vector for hashing .", "label": "", "metadata": {}, "score": "55.557705"}
{"text": "Inf .Model . , Vol .46 , No . 1 , 2006 DUTTA ET AL . .Page 13 . is effectively a linear scan .The use of the LSH algorithm allows one to perform such a search in sublinear time and also allows one to employ a variety of continuous descriptors to determine similarity .", "label": "", "metadata": {}, "score": "55.670216"}
{"text": "The original pool of descriptors for each data set was reduced in two steps .First , identical testing was carried out , whereby descriptors which were constant for more than 70 % of the observations were discarded .Next , a correlation test was carried out in which the pairwise correlation coefficient was evaluated and , for pairs having a correlation greater than 0.6 , one member of the pair was randomly selected and discarded .", "label": "", "metadata": {}, "score": "55.676453"}
{"text": "However , most of the singletons detected do lie away from the bulk of the plot .As before , points that appear to lie in or close to the bulk are more isolated when other pairs of components are considered .", "label": "", "metadata": {}, "score": "55.70221"}
{"text": "[ pdf ] .Also see the following related papers , which apply LSH to learned Mahalanobis metrics : .Fast Similarity Search for Learned Metrics Brian Kulis , Prateek Jain , & Kristen Grauman IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "55.74201"}
{"text": "Model . , Vol .46 , No . 1 , 2006 DUTTA ET AL . .Page 9 . to reach 100 % accuracy .In Figure 3 , we see that , at 0.0035 % of the maximum pairwise distance , the data set using the reduced descriptor pool produces results with 99 % accuracy .", "label": "", "metadata": {}, "score": "55.84422"}
{"text": "the algorithm succeeds in finding a point within distance from q ( if there exists a point within distance R ) with probability at least ; .For a fixed approximation ratio and probabilities and , one can set and , where .", "label": "", "metadata": {}, "score": "55.870407"}
{"text": "Mayur Datar Stanford University .Locality - Sensitive Hashing Scheme Based on p - Stable Distributions Joint work with Nicole Immorlica , Piotr Indyk and Vahab Mirrokni .The Nearest - Neighbor ( NN ) search problem has been extensively studied in the fields of computational geometry and algorithms , resulting in the discovery of many algorithms .", "label": "", "metadata": {}, "score": "55.914658"}
{"text": "For a fixed the hash function is given by .Other construction methods for hash functions have been proposed to better fit the data .[19 ] In particular k - means hash functions are better in practice than projection - based hash functions , but without any theoretical guarantee .", "label": "", "metadata": {}, "score": "55.934242"}
{"text": "( 12 ) Guha , S. ; Meyerson , A. ; Mishra , N. ; Motwani , R. ; O'Callaghan , L. Clustering Data Streams : Theory and Practice .IEEE Trans .Knowl- edge Data Eng .( 13 ) Gionis , A. ; Indyk , P. ; Motwani , R. Similarity Search in High Dimensions via Hashing .", "label": "", "metadata": {}, "score": "55.986282"}
{"text": "Segmentation is done by minimizing a convex energy functional based on weighted total variation leading to a global optimal solution .Each salient region provides an accurate figure / ground segmentation highlighting different parts of the image .These highly redundant results are combined into one composite segmentation by analyzing local segmentation certainty .", "label": "", "metadata": {}, "score": "56.106003"}
{"text": "We then take the MNIST dataset as an example to evaluate the influence of the parameters .The MNIST dataset consists of 70,000 images of handwritten digits divided into 10 classes of . of MVH - A based on the MNIST dataset .", "label": "", "metadata": {}, "score": "56.124256"}
{"text": "For each point x in the buckets , if the distance between q and x is within the query distance , we output this as the nearest neighbor .Thus , the parameters k and L are crucial .It has been shown2,13that k ) log1/p2n and L ) nF , where F ) ( log 1/p1)/(log 1/p2 ) , and this ensures locality - sensitive properties .", "label": "", "metadata": {}, "score": "56.208572"}
{"text": "Each image is represented by a 512-dimension GIST [ 24 ] feature and then hashed by MVH - A. MVH - CG is not run since decision stump on the whole dataset is expensive .The bottom row of Figure 6 shows the returning list of the example query , where the first 8 results are correct and the last 2 are false positives .", "label": "", "metadata": {}, "score": "56.226044"}
{"text": "The performance of the comparison methods is measured by Mean Average Precision ( MAP ) or precision - recall curves for Hamming ranking .Results on the MNIST Dataset .We report the experimental results based on MAP for Hamming ranking with code length from 32 to 128 bits in Figure 4(b ) .", "label": "", "metadata": {}, "score": "56.27044"}
{"text": "To analyze this simple data structure , we observe that the average number of \" outliers \" ( points at distance more than ) we encounter is at most one due to the choice of .On the other hand , for any near neighbor ( within distance at most ) we find it with probability at least , so , to boost it to constant , we build independent hash tables .", "label": "", "metadata": {}, "score": "56.274925"}
{"text": "Considering the variation of C hNNfor the Kazius data set ( Figure 9 ) , we see that the count is consistently larger when the reduced descriptor pool is used .This is a useful feature since it allows one to use a smaller radius but still obtain a sufficient number of nearest neighbors for a given query point .", "label": "", "metadata": {}, "score": "56.382896"}
{"text": "Their manipulation poses new challenges to classifiers and function approximators .The well - known k - nearest neighbor ( knn ) classifier is a favorite candidate for this task for being simple enough and well understood .One of the main obstacles , however , of using this classifier for massive data classification is its linear complexity to find a set of k neighbors for a given query . .", "label": "", "metadata": {}, "score": "56.435806"}
{"text": "The structure is able to speed up nearest neighbor searching in a variety of settings , for example : points in low - dimensional or structured Euclidean space , strings under Hamming and edit distance , and bit vector data from an OCR application .", "label": "", "metadata": {}, "score": "56.522945"}
{"text": "For example , outliers would take \u03a9(n2 ) time .This method can be speeded up using spatial data structures such as kd trees , metric trees , and their variants.4Typically such algorithms take O(log n ) per query , on average , for certain distributions of chemical spaces such as the uniform distribution .", "label": "", "metadata": {}, "score": "56.538986"}
{"text": "We demonstrate our shape matching method on databases of 10,000 images of human figures and 60,000 images of handwritten digits . \" ...Scalable similarity search is the core of many large scale learning or data mining applications .Recently , many research results demonstrate that one promising approach is creating compact and efficient hash codes that preserve data similarity .", "label": "", "metadata": {}, "score": "56.556686"}
{"text": "In comparison , using the traditional kNN algorithm , these singleton or near - singleton compounds will always have a set of nearest neighbors .In this case , determining whether a compound exists in a sparse region of the space would require an analysis of the distribution of nearest - neighbor distances , which is clearly time - consuming .", "label": "", "metadata": {}, "score": "56.59406"}
{"text": "Corresponding points on objects in a category will have similar shape contexts ; we regard these as instances of the same \" shapeme \" ( cf . phoneme ) .We compute shape contexts at a few randomly chosen ( \" representative \" ) points on a query image , and find their nearest neighbors in a stored set of shape contexts for a set of sample points on object models .", "label": "", "metadata": {}, "score": "56.623398"}
{"text": "For the NCI data sets , we obtained the maximum and mean distances by a random sampling procedure rather than a full analysis of the complete pairwise distance matrix .As a result , R simply called the precompiled kNN routine , and consequently , none of the timings include the time taken to load the data into memory .", "label": "", "metadata": {}, "score": "56.81574"}
{"text": "Recently semi - supervised learning approach was introduced in hashing where pairwise constraints ( mustlink and cannot - link ) using labeled data are leveraged while unlabeled data are used for regularization to avoid over - fitting .The resulting method is referred to as semi - supervised discriminant hashing ( SSDH ) .", "label": "", "metadata": {}, "score": "56.911594"}
{"text": "These compounds and their nearest neighbors would con- stitute an isolated cluster of compounds in a relatively unoccupied region of the chemical space .Depending on the rate of change of the count of nearest neighbors for a given query point , it is possible to understand the density of the region containing the point in question in a qualitative manner .", "label": "", "metadata": {}, "score": "56.944763"}
{"text": "However , the speedup , in practice , can even be 3 - 4 orders of magnitude , as shown in the figures .Note that the total execution time has not been used for our comparisons for the following reasons .", "label": "", "metadata": {}, "score": "56.983185"}
{"text": "For example , we may want to classify new molecules into classes such as inactive or active .Another interesting problem is to quickly identify the sparse regions of the chemical descriptor space for new leads .Mining such huge virtual libraries efficiently and accurately is an important and challenging problem .", "label": "", "metadata": {}, "score": "57.14245"}
{"text": "Science 2004 , 303 , 1813 - 1818 .( 2 ) Datar , M. ; Immorlica , N. ; Indyk , P. ; Mirrokni , V. S. Locality - sensitive hashing scheme based on p - stable distributions .In SCG ' 04 : Proceed- ings of the twentieth annual symposium on Computational geometry ; ACM Press : New York , 2004 .", "label": "", "metadata": {}, "score": "57.167763"}
{"text": "We also tested the perfor- mance on other machines including Dothan laptops and Intel P4 desktop processors , and the results were similar .RESULTS 3.1 .Accuracy .First , we validate the accuracy of our framework .For this purpose , we focused on the smaller data sets , the Kazius and the NCI - AIDS .", "label": "", "metadata": {}, "score": "57.188744"}
{"text": "11 , pp .1958 - 1970 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .C. Strecha , A. Bronstein , M. Bronstein , and P. Fua , \" LDAHash : improved matching with smaller descriptors , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "57.209473"}
{"text": "This question was resolved in 2011 by Ryan O'Donnell , Yi Wu and Yuan Zhou : they showed a lower bound for and for matching the upper bounds .Thus , the above simple LSH family for the hypercube is in fact , optimal !", "label": "", "metadata": {}, "score": "57.24122"}
{"text": "Learning to hash involves learning hash functions from a set of images for embedding high - dimensional visual descriptors into a similarity - preserving low - dimensional Hamming space .Most of existing methods resort to a single representation of images , that is , only one type of visual descriptors is used to learn a hash function to assign binary codes to images .", "label": "", "metadata": {}, "score": "57.247032"}
{"text": "We put forward our algorithms and present the main results on several large - scale image datasets in the next sections .Methodology .Notation .The following notations will be used throughout this paper : ( i ) a bold lower - case letter ( .", "label": "", "metadata": {}, "score": "57.269165"}
{"text": "10 , pp .2197 - 2219 , 2004 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . D. Zhang , J. Wang , D. Cai , and J. Lu , \" Self - taught hashing for fast similarity search , \" in Proceedings of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR ' 10 ) , pp .", "label": "", "metadata": {}, "score": "57.269707"}
{"text": "Katabi initiated a new approach to network design using an explicit Control Protocol ( XCP ) that minimizes network congestion and maximizes utilization efficiency .Her research addressed a strategic technological problem of Internet growth , which requires extreme scalability and robustness .", "label": "", "metadata": {}, "score": "57.35642"}
{"text": "Our initial investigation focused on nearest neighbor detection using the Euclidean metric .\" [ Show abstract ] [ Hide abstract ] ABSTRACT : Some of the latest trends in cheminformatics , computation , and the world wide web are reviewed with predictions of how these are likely to impact the field of cheminformatics in the next five years .", "label": "", "metadata": {}, "score": "57.380623"}
{"text": "These sets are manually selected , and each of these sets contains images that are visually similar .The average number of regions per image is 7.16 .We extract from each region a feature vector of 14-dimensions , including 9 dimensions of color moment information ... . \" ...", "label": "", "metadata": {}, "score": "57.62438"}
{"text": "In the second step , we solve the nearest neighbor problem in the normed space .We illustrate this approach by providing theoretical and experimental data for the problem of embedding Earth - Mover Distance into the L_1 norm .Jitendra Malik University of California , Berkley .", "label": "", "metadata": {}, "score": "57.760033"}
{"text": "Model . , Vol .46 , No . 1 , 2006 329 .Page 10 .necessarily affect the ability of the algorithm to detect singleton observations .That is , descriptor reduction is not a requirement .Table 4 summarizes the mean distance from the singletons annotated in Figure 13 to all the other members of the data set , using the original and reduced descriptor pools .", "label": "", "metadata": {}, "score": "57.81894"}
{"text": "( 14 ) Agrafiotis , D. A Constant Time Algorithm for Estimating the Diversity of Large Chemical Libraries .J. Chem .Inf .Comput .Sci .( 15 ) Garey , R. ; Johnson , D. Computers and Intractibility ; W. H. Freeman : New York , 1979 .", "label": "", "metadata": {}, "score": "57.970867"}
{"text": "By using a metric index , a knn classifier can afford massive classification tasks at reasonable time costs .Proximity searching has applications in a vast number of fields , apart from classification ... . by Gonzalo Navarro , Nora Reyes - In Proceedings of the 9th International Symposium on String Processing and Information Retrieval ( SPIRE 2002 ) , LNCS 2476 , 2002 . \" ...", "label": "", "metadata": {}, "score": "58.090103"}
{"text": "Figure 6 .Plot of the mean query time versus the radius for a 200- observation query set taken from the Kazius data set .The radii are reported as a percentage of the maximum pairwise distance in the data set , and the results are shown for both the original and reduced descriptor pools .", "label": "", "metadata": {}, "score": "58.238804"}
{"text": "In the case of the reduced pool of descriptors for the same data set , the algorithm detected 15 singleton observations for the same radius .With decreasing radii , the number of singletons detected was observed to increase significantly .This behavior is not surprising as the use of a smaller radius necessarily reduces the number of nearest neighbors .", "label": "", "metadata": {}, "score": "58.24478"}
{"text": "Let us denote this family by .Now we choose to have the following collision probabilities : . at distance ; . at distance .( actually , we can not set to achieve these probabilities exactly , since must be integer , that 's exactly why we need the condition ) .", "label": "", "metadata": {}, "score": "58.263035"}
{"text": "Consider a Figure 5 .Plot of the mean query time versus the normalized mean count of nearest neighbors per query point .Note that both axes are logarithmic , indicating that the mean query time increases expo- nentially with increasing number of nearest neighbors detected .", "label": "", "metadata": {}, "score": "58.328312"}
{"text": "Virtual screening ( VS ) has become a preferred tool to augment high - throughput screening(1 ) and determine new leads in the drug discovery process .The core of a VS informatics pipeline includes several data mining algorithms that work on huge databases of chemical compounds containing millions of molecular structures and their associated data .", "label": "", "metadata": {}, "score": "58.40969"}
{"text": "Communications of the ACM 51 ( 1 ) : 117 - 122 .doi : 10.1145/1327452.1327494 .^ Gorman , James , and James R. Curran . \"Scaling distributional similarity to large corpora .\" Proceedings of the 21stInternational Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics .", "label": "", "metadata": {}, "score": "58.4151"}
{"text": "In fact , this method also works around the best known lower bounds in the cell probe model for the query time using a data structure near linear in the size of the data base .TCAMs are high performance associative memories widely used in networking applications such as address lookups and access control lists .", "label": "", "metadata": {}, "score": "58.514706"}
{"text": "This non - parametric measure allows us to easily derive an appealing fusion model of label fields , easily expressed as a Gibbs distribution , or as a nonstationary MRF model defined on a complete graph .Concretely , this Gibbs energy model encodes the set of binary constraints , in terms of pairs of pixel labels , provided by each segmentation results to be fused .", "label": "", "metadata": {}, "score": "58.549976"}
{"text": "Numerical experiments on image datasets demonstrate the useful behavior of our deep multi - view hashing ( DMVH ) , compared to recently - proposed multi - modal deep network as well as existing shallow models of hashing .Keywords - deep learning ; harmonium ; hashing ; multi - view learning ; restricted Boltzmann machines ; I. . ...", "label": "", "metadata": {}, "score": "58.63261"}
{"text": "How do you implement such a \" refusal \" ?You first hash all the points in your $ P$ and compute the corresponding medium - level \" ball \" , and then on a point query do the first layer and then check whether it landed within the ball ( and only proceed if so ) ?", "label": "", "metadata": {}, "score": "58.63749"}
{"text": "We can also measure how the accuracy of a k - nearest neighbor classifier with KLSH approaches the accuracy of a linear scan k - NN classifier on this data set .The above plot shows that , as epsilon decreases , the hashing accuracy approaches the linear scan accuracy .", "label": "", "metadata": {}, "score": "58.68817"}
{"text": "Not quite .The catch is that the definition of LSH families is actually too strong .The real property that is used in the ANN data structure is the following : for every pair of points we have . if , then ; . if , then .", "label": "", "metadata": {}, "score": "58.739704"}
{"text": "Plot showing the percentage of correct R nearest Figure 3 . neighbors detected by the LSH algorithm versus the radius for the NCI-3D data set using the original and reduced descriptor pools .The radii are reported as a percentage of the maximum pairwise distance in the original descriptor pool .", "label": "", "metadata": {}, "score": "58.766647"}
{"text": "Figures 1 - 3 summarize the percentage of nearest neighbors that the LSH algorithm detected versus the radius compared to the nearest neighbors detected by the exact algorithm using a linear scanning procedure .In the case of the Kazius data set ( Figure 1 ) , we see that using the reduced descriptor set results in a consistent increase in the percentage of nearest neighbors that the LSH algorithm detects correctly .", "label": "", "metadata": {}, "score": "58.786137"}
{"text": "Spatiotemporal objects are subsequently formed by associating the already formed spatial regions using their low - level features .A different approach is to use motion information to perform motion p .. by Wei Dong , Zhe Wang , Moses Charikar , Kai Li - in ACM Multimedia , 2008 . \" ...", "label": "", "metadata": {}, "score": "58.803253"}
{"text": "The algorithm then constructs L hash tables , each corresponding to a different randomly chosen hash function g .In the preprocessing step we hash all n points from the data set S into each of the L hash tables .Given that the resulting hash tables have only n non - zero entries , one can reduce the amount of memory used per each hash table to using standard hash functions .", "label": "", "metadata": {}, "score": "58.91858"}
{"text": "Nearest neighbor search ( NNS ) algorithms are often used to retrieve similar entries , given a query .While there exist efficie ... \" .Similarity search methods are widely used as kernels in various data mining and machine learning applications including those in computational biology , web search / clustering .", "label": "", "metadata": {}, "score": "59.23785"}
{"text": "Our accuracy results were consistent across radii and the number of nearest neighbors for different radii .It is important to note that the results for the smaller radii are more critical than those at higher radii .In all of the data sets we analyzed , the accuracy dipped and went up back again as we increased the radii because a Table 1 .", "label": "", "metadata": {}, "score": "59.258194"}
{"text": "Approximate distances allow efficient identification of a small set of candidates , on which the exact distance measure is used to identify the nearest neighbors .This framework is applied in a system that estimates hand pose in an image by retrieving the most similar matches in a database of over 100,000 hand images .", "label": "", "metadata": {}, "score": "59.579315"}
{"text": "IEEE ICME 2004 , 2004 . \" ...Bridging the cognitive gap in image retrieval has been an active research direction in recent years .Existing solutions typically require a large volume of training data that could be difficult to obtain in practice .", "label": "", "metadata": {}, "score": "59.74661"}
{"text": "The best known general - purpose algorithm for evaluating top - k queries is Fagin 's threshold algorithm ( TA ) .Since the user 's goal behind top - k queries is to i ... \" .Top - k queries based on ranking elements of multidimensional datasets are a fundamental building block for many kinds of information discovery .", "label": "", "metadata": {}, "score": "59.84471"}
{"text": "To overcome this obstacle one can consider the approximate near neighbor search problem ( ANN ) .Now in addition to and we are also given an approximation parameter .The goal is given a query report a point from within distance from , provided that the neighborhood of radius is not empty .", "label": "", "metadata": {}, "score": "59.84896"}
{"text": "For the NCI data sets , the mean and maximum pairwise distances were obtained by randomly selecting 10 % of the data set four times .Figure 1 . neighbors detected by the LSH algorithm versus the radius for the Kazius data set using the original and reduced descriptor pools .", "label": "", "metadata": {}, "score": "59.898754"}
{"text": "As for dependency on data , we \" refuse \" to hash a point on the second level , if it is too far from the corresponding center .For the ANN application it is fine , since we will fail to find a near neighbor anyway .", "label": "", "metadata": {}, "score": "59.972584"}
{"text": "Model . , Vol .46 , No . 1 , 2006 323 .Page 4 . structures contained 3D coordinates which were washed and optimized using MOE using the MMF94 force field with a 0.1 \u00c5 tolerance .No property was available for this data set .", "label": "", "metadata": {}, "score": "60.048634"}
{"text": "But the generalized eigenfunctions are derived only for the Laplacian eigenmaps embedding and their performance may decline rapidly when their number increases .In summary , the main contributions of this work can be described as follows .( i ) Inspired by MVU , we propose maximum variance hashing ( MVH ) , which directly embeds the high - dimensional data into a specified low - dimensional Hamming space and preserve the geometric properties of local neighborhoods .", "label": "", "metadata": {}, "score": "60.151356"}
{"text": "More generally , in the same paper it was proved that one can achieve for the case of norms for ( via an embedding by William Johnson and Gideon Schechtman ) .In 2006 Alexandr Andoni and Piotr Indyk proved that one can achieve for the norm .", "label": "", "metadata": {}, "score": "60.172905"}
{"text": "Shape contexts are high - dimensional vectors ; finding nearest neighbors is done using locality sensitive hashing .We will show results on recognizing vehicles in range scans of scenes using a database of more than 50 vehicles .Andrew Moore Carnegie Mellon University .", "label": "", "metadata": {}, "score": "60.20627"}
{"text": "Abstract - Hashing refers to methods for embedding highdimensional data into a similarity - preserving low - dimensional Hamming space such that similar objects are indexed by binary codes whose Hamming distances are small .Learning hash functions from data has recently been recognized as a promising approach to approximate nearest neighbor search for highdimensional data .", "label": "", "metadata": {}, "score": "60.292667"}
{"text": "Figure 5 : Precision - recall curves for competing methods on the MNIST dataset for different code lengths .Results on the CIFAR-10 Dataset .The CIFAR-10 dataset is a manually labeled subset of the well - known 80 million tiny images dataset [ 4 ] .", "label": "", "metadata": {}, "score": "60.406567"}
{"text": "Suppose we have a -sensitive hash family for the metric we want to solve ANN for .Moreover , assume that we can sample and evaluate a function from relatively quickly , store it efficiently , and that .Then , one can solve ANN for this metric with space roughly and query time , where .", "label": "", "metadata": {}, "score": "60.586193"}
{"text": "When other principal components are considered , these points do indeed lie in sparse regions of those plots .We observed a similar situation when we considered the reduced descriptor pool of 20 descriptors .Figure 13 displays a plot of the first principal component versus the second principal component ( which together explain 99.99 % of the total variance in this data set ) .", "label": "", "metadata": {}, "score": "60.595543"}
{"text": "Kazius , J. ; McGuire , R. ; Bursi , R. Derivation and Validation of Toxicophores for Mutagenicity Prediction .J. Med .Chem .( 17 ) Ames , B. N. ; McCann , H. ; Yamasaki , E. Methods for detecting carcinogens and mutagens with the Salmonella / mammalian - microsome mutagenicity test .", "label": "", "metadata": {}, "score": "60.733353"}
{"text": "The resultant plot provides one view of data with respect to the components being plotted .Figure 12 shows a plot of the first principal component versus the second principal component , which together explained 99.97 % of the total variance in the Kazius data set when using 142 descriptors .", "label": "", "metadata": {}, "score": "60.789284"}
{"text": "Inf .Comput .Sci .( 9 ) Gasteiger , J. Chemoinformatics , A Textbook ; John Wiley & Sons : Weinheim , Germany , 2003 .( 10 )Duda , R. ; Hart , P. Pattern Classification , 2nd ed . ; Wiley - Inter- science : Hoboken , NJ ; 1998 .", "label": "", "metadata": {}, "score": "60.882954"}
{"text": "e , we consider it as tree rather than mountain since trees cover on the mountain .Because of the integrated seed growing mechanisms , each region yielded by JSEG is spatially connected , which makes it convenient to describe the spatial relationships among regions .", "label": "", "metadata": {}, "score": "61.095642"}
{"text": "Artificial neural networks ( ANN ) were used to learn the similarity metrics .The input was attributes of the two data points , and the desired output is the similarity ( or distance ) between them .Other conventional functions , such as the Euclidean distance , were used for comparison .", "label": "", "metadata": {}, "score": "61.281315"}
{"text": "It also gives some areas of current research .The embedding from the original space to the new space can be computationally prohibitive in the dimensionality d is quite large ( above 50 ) .A research question is when can the combination of embed ... . by Kenneth L. Clarkson - In Nearest - Neighbor Methods for Learning and Vision : Theory and Practice , 2006 . \" ...", "label": "", "metadata": {}, "score": "61.393867"}
{"text": "Different classes could represent different levels of activity or could represent different types of compounds .It is common to have the class labels represent disjointed sets of compounds of a given database .These classification techniques are often based on algorithms to partition the data set and then correctly assign a class label to each of the members of the cluster or partition .", "label": "", "metadata": {}, "score": "61.512733"}
{"text": "Table 4 . 330 J. Chem .Inf .Model . , Vol .46 , No . 1 , 2006 DUTTA ET AL . .Page 11 .small radii , which will produce a large number of singletons .Our experiments indicate that , for the Kazius data set , using radii beyond 5 % of the maximum pairwise distance results in the detection of identical singletons , which correspond to the most isolated points in the descriptor space .", "label": "", "metadata": {}, "score": "61.90322"}
{"text": "Our accuracy was at least 94 % .The accuracy results for the Kazius data set are shown in Figure 1 .In Figure 2 , we plot the accuracy results for the NCI - AIDS data set , while Figure 3 compares the accuracy for the NCI- 3D data set .", "label": "", "metadata": {}, "score": "62.067673"}
{"text": "A number of hybrid descriptors were also evaluated .These included charged polar surface areas ( using the Gasteiger - Marsilli charges32,33 ) as well as the topological polar surface34area descriptor .The octanol - water partition coefficient was also evaluated .", "label": "", "metadata": {}, "score": "62.19183"}
{"text": "( 34 )Ertl , P. ; Rohde , B. ; Selzer , P. Fast Calculation of Molecular Polar Surface Area as a Sum of Fragment Based Contributions and Its Application to the Prediction of Drug Transport Properties .J. Med .", "label": "", "metadata": {}, "score": "62.31584"}
{"text": "326 J. Chem .Inf .Model . , Vol .46 , No . 1 , 2006 DUTTA ET AL . .Page 7 . scenario when the number of dimensions is d )O(n ) .For a traditional kNN , each distance calculation now takes O(d ) )", "label": "", "metadata": {}, "score": "62.36786"}
{"text": "( typically less than 128 ) iterations in practice .Moreover , if it is a nondeterministic model here , column generation can produce new hash functions even after satisfying all constraints .We , therefore , do not mention the convergence in Algorithm 1 . )", "label": "", "metadata": {}, "score": "62.429585"}
{"text": "[ 12 ] Approximate min - wise independence differs from the property by at most a fixed \u03b5 .[ 13 ] .Nilsimsa is an anti - spam focused locality - sensitive hashing algorithm .[14 ] The goal of Nilsimsa is to generate a hash digest of an email message such that the digests of two similar messages are similar to each other .", "label": "", "metadata": {}, "score": "62.4868"}
{"text": "We also calculated the average Tanimoto similarity between the two outliers noted above with the remainder of the data set , using MACCS fingerprints .However , in both cases , the average similarity was not significantly different from the average Tanimoto similarity for the whole data set .", "label": "", "metadata": {}, "score": "62.65811"}
{"text": "A variety of hashing methods have been developed , but most of them resort to a single view ( representation ) of data .However , objects are often described by multiple representations .For instance , images are described by a few different visual descriptors ( such as SIFT , GIST , and HOG ) , so it is desirable to incorporate multiple representations into hashing , leading to multi - view hashing .", "label": "", "metadata": {}, "score": "62.728348"}
{"text": ".. ith computation and storage .In some early works , spatial partitions of the feature space via various tree structures[1 , 2 ] have been extensively studied .Despite good results f .. by Rajendra Shinde , Ashish Goel , Debojyoti Dutta , Pankaj Gupta - In SIGMOD , 2010 . \" ...", "label": "", "metadata": {}, "score": "62.788174"}
{"text": "( 2007 ) , \" Google news personalization : scalable online collaborative filtering \" , Proceedings of the 16th international conference on World Wide Web ( ACM ) , doi : 10.1145/1242572.1242610 .^ Koga , Hisashi , Tetsuo Ishibashi , and Toshinori Watanabe ( 2007 ) , \" Fast agglomerative hierarchical clustering algorithm using Locality - Sensitive Hashing \" , Knowledge and Information Systems 12 ( 1 ) : 25 - 53 , doi : 10.1007/s10115 - 006 - 0027 - 5 .", "label": "", "metadata": {}, "score": "62.84597"}
{"text": "The experimental results are quite promising .They showed that the machine - learned similarity functions can work better than human - designed functions , besides their advantages of reducing human intervention .Darrin P. Lewis Columbia University .Regularized Nearest Neighbor Joint work with William S. Noble .", "label": "", "metadata": {}, "score": "62.888847"}
{"text": "12 , pp .2143 - -2157 , 2009 .[ pdf ] .Fast Image Search for Learned Metrics Prateek Jain , Brian Kulis , & Kristen Grauman In .Proc .IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , 2008 .", "label": "", "metadata": {}, "score": "62.92375"}
{"text": "It has been established that a min - wise independent family of permutations is at least of size .[ 10 ] and that this bound is tight .[ 11 ] .Because min - wise independent families are too big for practical applications , two variant notions of min - wise independence are introduced : restricted min - wise independent permutations families , and approximate min - wise independent families .", "label": "", "metadata": {}, "score": "63.258636"}
{"text": "Such analysis is also related to preprocessing the data set using clustering and classification based on kNN .\u00a7 These authors contributed equally to this paper .321 J. Chem .Inf .Model .2006 , 46 , 321 - 333 10.1021/ci050403o CCC : $ 33.50 \u00a9 2006 American Chemical Society Published on Web 11/08/2005 .", "label": "", "metadata": {}, "score": "63.48396"}
{"text": "Thus , we chose k ) 200 arbitrarily .Also , from our data sets , we chose the first 200 compounds arbitrarily as the query set .We tried other schemes ( including using the whole training set as the query set ) , and the results were similar .", "label": "", "metadata": {}, "score": "63.657425"}
{"text": "If one insists on having near - linear ( in ) memory and being subexponential in the dimension , then the only known technique for ANN is locality - sensitive hashing .Let us give some definitions .Say a hash family on a metric space is -sensitive , if for every two points . if , then ; . if , then .", "label": "", "metadata": {}, "score": "63.72297"}
{"text": "View at Google Scholar .B. Kulis and K. Grauman , \" Kernelized locality - sensitive hashing , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .34 , no .6 , pp .1092 - 1104 , 2012 .", "label": "", "metadata": {}, "score": "63.832825"}
{"text": "The LSH family satisfies the following conditions for any two points , using a function which is chosen uniformly at random : . if , then ( i.e. , p and q collide ) with probability at least , . if , then with probability at most .", "label": "", "metadata": {}, "score": "63.990135"}
{"text": "View at Google Scholar .W. Liu , J. Wang , S. Kumar , and S. Chang , \" Hashing with graphs , \" in Proceedings of the International Conference on Machine Learning , 2011 . Y. Bengio , O. Delalleau , N. Le Roux , J. F. Paiement , P. Vincent , and M. Ouimet , \" Learning eigenfunctions links spectral embedding and kernel PCA , \" Neural Computation , vol .", "label": "", "metadata": {}, "score": "64.07175"}
{"text": "( 2010 ) , \" RAPID detection of gene - gene interactions in genome - wide association studies \" , Bioinformatics 26 ( 22 ) : 2856 - 2862 , doi : 10.1093/bioinformatics / btq529 .^ Alexandr Andoni ; Indyk , P. ( 2008 ) .", "label": "", "metadata": {}, "score": "64.0733"}
{"text": "Figure 8 .Plot of the mean query time versus the radius for a 200- observation query set taken from the NCI-3D data set .The radii are reported as a percentage of the maximum pairwise distance in the data set , and the results are shown for both the original and reduced descriptor pools .", "label": "", "metadata": {}, "score": "64.09329"}
{"text": "Observe that the accuracy has not dropped with the size of the data set .In fact , the minimum accuracy for the NCI-3D data set is 95 % , which is more than that of the smaller NCI - AIDS data set .", "label": "", "metadata": {}, "score": "64.14746"}
{"text": "Figure 7 .Plot of the mean query time versus the radius for a 200- observation query set taken from the NCI - AIDS data set .The radii are reported as a percentage of the maximum pairwise distance in the data set , and the results are shown for both the original and reduced descriptor pools .", "label": "", "metadata": {}, "score": "64.18943"}
{"text": "To create an OR - construction , we define a new family of hash functions g , where each function g is constructed from k random functions from .We then say that for a hash function , if and only if for one or more values of i .", "label": "", "metadata": {}, "score": "64.24187"}
{"text": "View at Scopus .B. Kulis , P. Jain , and K. Grauman , \" Fast similarity search for learned metrics , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .31 , no .12 , pp .", "label": "", "metadata": {}, "score": "64.294876"}
{"text": "Though this discussion has focused on varying the LSH radius to explore chemical space , the low mean query times exhibited by the LSH algorithm make it an attractive tool to determine whether a new query point Figure 14 .Structure of molecule 3904 .", "label": "", "metadata": {}, "score": "64.43239"}
{"text": "For each g considered , it retrieves the data points that are hashed into the same bucket as q .The process is stopped as soon as a point within distance from q is found .Given the parameters k and L , the algorithm has the following performance guarantees : . preprocessing time : , where t is the time to evaluate a function on an input point p ; .", "label": "", "metadata": {}, "score": "64.57358"}
{"text": "Res .( 19 ) Voigt , J. ; Bienfait , B. ; Wang , S. ; Nicklaus , M. Comparison of the Open NCI Database with Seven Large Chemical Structural Databases .J. Chem .Inf .Comput .Sci .", "label": "", "metadata": {}, "score": "64.63223"}
{"text": "As shown in Figures 10 and 11 , there is no significant difference in the nearest - neighbor count at the lower radii .However , after a certain point , in both cases , the mean nearest - neighbor count increases significantly with the radius for the reduced descriptor pools .", "label": "", "metadata": {}, "score": "64.97418"}
{"text": "Points marked in blue represent singleton observations detected by the LSH algorithm with the radius set to 0.1 % of the maximum pairwise distance in the data set .Figure 13 .Plot of the first versus second principal component of the Kazius data set , using 20 descriptors .", "label": "", "metadata": {}, "score": "64.98433"}
{"text": "( 1 ) Your intuition is correct : the outer level is used to achieve the low - diameter condition ( it is not used for introducing randomness though ) .( 2 ) We drop points that we can not handle .", "label": "", "metadata": {}, "score": "65.139465"}
{"text": "For object recognition with Caltech 101 dataset , our method runs 25 times faster to achieve the same precision as Pyramid Matching Kernel , the state - of - the - art feature - set matching method . ... purpose images .", "label": "", "metadata": {}, "score": "65.23174"}
{"text": "[ 1 ] .To create an AND - construction , we define a new family of hash functions g , where each function g is constructed from k random functions from .We then say that for a hash function , if and only if all for .", "label": "", "metadata": {}, "score": "65.378654"}
{"text": "Using this data structure , one can perform approximate nearest - neighbor searches very quickly , in sublinear time .We validate the accuracy and performance of our framework on three real data sets of sizes ranging from 4337 to 249 071 molecules .", "label": "", "metadata": {}, "score": "65.38578"}
{"text": "Using this data structure , one can perform approximate nearest - neighbor searches very quickly , in sublinear time .We validate the accuracy and performance of our framework on three real data sets of sizes ranging from 4337 to 249 071 molecules .", "label": "", "metadata": {}, "score": "65.38578"}
{"text": "Struct.-Act .Relat .Pharmacol . , Chem .Biol .( 29 ) Kier , L. ; Hall , L. ; Murray , W. Molecular Connectivity I : Relationship to local anasthesia .J. Pharm .Sci .( 30 ) Kier , L. ; Hall , L. Molecular ConnectiVity in Structure ActiVity Analysis ; John Wiley & Sons : Hertfordshire , England , 1986 .", "label": "", "metadata": {}, "score": "65.407684"}
{"text": "One problem that one encounters a lot in machine learning , databases and other areas is the near neighbor search problem ( NN ) .Given a set of points in a -dimensional space and a threshold the goal is to build a data structure that given a query reports any point from within distance at most from .", "label": "", "metadata": {}, "score": "65.93728"}
{"text": "MVH - CG can maintain the manifold of the Swiss roll in some sense .SH and STH fail to preserve the manifold .Indeed , all these mentioned data - dependent methods aim at hashing the high - dimensional features of the training data with low - dimensional binary codes while preserving the underlying data structure .", "label": "", "metadata": {}, "score": "66.0513"}
{"text": "Similar to the well - known expectation - maximization ( EM ) algorithm , column generation has a two - step iteration framework , where one set of variables are treated as constant in each step .The aim of column generation is to reduce the gap between the primal and the dual solutions iteratively .", "label": "", "metadata": {}, "score": "66.18026"}
{"text": "Model . , Vol .46 , No . 1 , 2006 333 .Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .", "label": "", "metadata": {}, "score": "66.379974"}
{"text": "J. Pharm .Sci .( 32 ) Gasteiger , J. ; Marsili , M. A New Model for Calculating Atomic Charges in Molecules .Tetrahedron Lett .( 33 ) Gasteiger , J. ; Marsili , M. Iterative partial equalization of orbital elektronegativity - a rapid access to atomic charges .", "label": "", "metadata": {}, "score": "66.6494"}
{"text": "Indyk is a principal investigator at the MIT Computer Science and Artificial Intelligence Lab ( CSAIL ) , a member of the Theory of Computation Group at CSAIL , and a professor in the MIT Department of Electrical Engineering and Computer Science .", "label": "", "metadata": {}, "score": "66.71625"}
{"text": "12 , pp .2393 - 2406 , 2012 .View at Google Scholar .Y. Weiss , A. Torralba , and R. Fergus , \" Spectral hashing , \" Advances in Neural Information Processing Systems , 2008 .View at Google Scholar .", "label": "", "metadata": {}, "score": "66.93791"}
{"text": "Its performance , however , declines rapidly as .The performance of PCAH and STH , similar to AGH , also drops down with longer bit lengths .By contrast , MVH - A and MVH - CG consistently improve their performance as code length grows .", "label": "", "metadata": {}, "score": "66.988785"}
{"text": "The main parameter of interest is epsilon , a parameter from standard LSH which trades off speed for accuracy .Local Patch Indexing with the Photo Tourism Data Set .Finally , we applied KLSH over a data set of 100,000 image patches from the Photo Tourism data set .", "label": "", "metadata": {}, "score": "67.09639"}
{"text": "Nearest Neighbors in Metric Spaces Many approaches to nearest neighbor searching exploit specific properties of the problem instance , such as a setting in Euclidean space , or in low dimension .We can also attack the search problem armed only with the distance function and its triangle inequality .", "label": "", "metadata": {}, "score": "67.33925"}
{"text": "John Fisher MIT .On the Risk of the Approximate Nearest Neighbor Classifier Joint work with Gregory Shakhnarovich and Trevor Darrell .The nearest neighbor ( NN ) classification rule has excellent asymptotic performance , and is known to have good accuracy on finite samples .", "label": "", "metadata": {}, "score": "67.85222"}
{"text": "Further- more , the two outliers that were highlighted by the LSH algorithm would contain a number of features in common with smaller molecules .As a result , the fingerprints would be expected to contain a number of bits that would be set to 1 in both the outliers as well as in the nonoutliers .", "label": "", "metadata": {}, "score": "67.93242"}
{"text": "Struct.-Act .Relat .Pharmacol . , Chem .Biol .( 27 ) Kier , L. Shape indexes for orders one and three from molecular graphs .Quant .Struct.-Act .Relat .Pharmacol . , Chem .Biol .( 28 ) Kier , L. Distinguishing atom differences in a molecular graph index .", "label": "", "metadata": {}, "score": "68.0408"}
{"text": "Furthermore , the singletons detected were the same .For radii beyond 5 % of the maximum pairwise distance , the algorithm detected only two singletons .These correspond to points 3904 and 1861 in Figure 13 , in which it is clear that these two points lie far from the bulk of the data set .", "label": "", "metadata": {}, "score": "68.050545"}
{"text": "As with the other data sets , the accuracy obtained using the reduced pool of descriptors is consistently higher .Interestingly , in the case of this data set , one does not need to use a very large radius Figure 9 .", "label": "", "metadata": {}, "score": "68.33957"}
{"text": "Plot showing the percentage of correct R nearest 324 J. Chem .Inf .Model . , Vol .46 , No . 1 , 2006 DUTTA ET AL . .Page 5 . smaller radius forces the algorithm to probe denser regions .", "label": "", "metadata": {}, "score": "68.419945"}
{"text": "Examples of such metrics include Hausdorff metric or Earth - Mover Distance .In such cases , the nearest neighbor problem becomes much harder to solve .One approach to solving such problem is to design algorithms for general metric spaces ( as in Ken Clarkson 's talk ) .", "label": "", "metadata": {}, "score": "68.571556"}
{"text": "Given an input vector v and a hyperplane defined by r , we let .That is , depending on which side of the hyperplane v lies .Each possible choice of r defines a single function .Let H be the set of all such functions and let D be the uniform distribution once again .", "label": "", "metadata": {}, "score": "68.58193"}
{"text": "[ Show abstract ] [ Hide abstract ] ABSTRACT : Libraries of chemical structures are used in a variety of cheminformatics tasks such as virtual screening and QSAR modeling and are generally characterized using molecular descriptors .When working with libraries it is useful to understand the distribution of compounds in the space defined by a set of descriptors .", "label": "", "metadata": {}, "score": "68.64395"}
{"text": "Our evaluation employed real - world and synthetic large , web - data collections , and query benchmarks .Our experimental results show that KLEE can achieve major performance gains in terms of network bandwidth , query response times , and much lighter peer loads , all with small errors in result precision and other result - quality measures . by Qin Lv , William Josephson , Zhe Wang , Moses Charikar , Kai Li - in Proc . 33rd", "label": "", "metadata": {}, "score": "69.47868"}
{"text": "Phys .Lett .( 25 ) Kier , L. ; Hall , L. Molecular ConnectiVity in Chemistry and Drug Research ; Academic Press : New York , 1976 .( 26 ) Kier , L. A shape index from molecular graphs .", "label": "", "metadata": {}, "score": "69.50954"}
{"text": "Her scheme is the first protocol to achieve both goals simultaneously without imposing excessive per - flow overhead on Internet routers .The design separated the efficiency and fairness policies of congestion control , which delivered the highest possible application performance over a broad range of network infrastructure . \" ...", "label": "", "metadata": {}, "score": "69.58667"}
{"text": "Inf .Model . , Vol .46 , No . 1 , 2006 DUTTA ET AL . .Page 3 .Definition 1 .That is , the probability of the collision of points q and V decreases with the distance between them .", "label": "", "metadata": {}, "score": "69.646576"}
{"text": "Thus , by successively increasing the radius in some fashion , discussed later , one may include an increasing number of approximate nearest neighbors .A side effect of this approach is that , while increasing the radius , certain compounds might have only one or two nearest neighbors , whereas others will have hundreds ( or even thousands ) .", "label": "", "metadata": {}, "score": "69.77899"}
{"text": "Keywords - Hashing , regularized discriminant analysis , semisupervised learning .I. . by Bahman Bahmani , Stanford Univesrity , Ashish Goel , Rajendra Shinde - In CIKM , 2012 . \" ...( Key , Value ) based distributed frameworks , such as MapRe - duce , Memcached , and Twitter Storm are gaining increas - ingly widespread use in applications that process large amounts of data .", "label": "", "metadata": {}, "score": "69.78943"}
{"text": "In this instance hashing produces only a single bit .Two vectors ' bits match with probability proportional to the cosine of the angle between them .The hash function [ 18 ] maps a d dimensional vector onto a set of integers .", "label": "", "metadata": {}, "score": "70.61121"}
{"text": "It is clear that , for the given components , the majority of singletons correspond to isolated compounds and com- pounds relatively distant from the bulk of the data set .However , a number of points lie near the bulk of the plot ( upper - right region ) .", "label": "", "metadata": {}, "score": "70.618904"}
{"text": "By successively increasing the radii for the LSH algorithm , we can focus on more isolated compounds .Thus , Figures 12 and 13 both highlight singletons detected at a specific radius .By successively increasing the radius , we can focus on points that are located in sparser regions of the descriptor space .", "label": "", "metadata": {}, "score": "70.62982"}
{"text": "We calculated topological and geometric descriptors , ignor- ing semiempirical electronic descriptors because of the long time required for calculation .Table 1 summarizes the number of compounds and the number of descriptors calculated for each data set .The topological descriptors included the Wiener path index,23the Zagreb index , Balaban 's J topologi- cal index,24,25Kier shape descriptors,26 - 28and the zeroth- and first - order ? indices.29 - 31In addition , a number of constitutional descriptors such as counts of heavy atoms , bonds , and hydrogen - bond donors and acceptors were also evaluated .", "label": "", "metadata": {}, "score": "70.85916"}
{"text": "View at Google Scholar .L. Luo , C. Shen , C. Zhang , and A. van den Hengel , \" Shape similarity analysis by self - tuning locally constrained mixeddiffusion , \" IEEE Transactions on Multimedia , 2013 .View at Google Scholar . A. Torralba , R. Fergus , and W. T. Freeman , \" 80 million tiny images : a large data set for nonparametric object and scene recognition , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "71.28851"}
{"text": "Structure of molecule 1861 .Figure 16 .Some representative structures from the dense region of the Kazius data set , obtained by performing a LSH computation using 0.01 % of the maximum pairwise distance .The values in parentheses indicate the number of nearest neighbors detected by the LSH algorithm at this radius .", "label": "", "metadata": {}, "score": "71.40937"}
{"text": "Its main drawbacks are : costly construction time , poor performance in low dimensional spaces or queries with high selectivity , and the fact of being a static data structure , that is , once built , one can not add or delete elements . \" ...", "label": "", "metadata": {}, "score": "71.79584"}
{"text": "( 22 ) MACCS Fingerprints ; MDL Information Systems Inc. : San Leandro , CA .( 23 ) Wiener , H. Structural Determination of Paraffin Boiling Points .J. Am .Chem .Soc .( 24 ) Balaban , A. Higly discriminating distance based topological index .", "label": "", "metadata": {}, "score": "72.17247"}
{"text": "We then noted , for each radius , which observations were regarded as singletons , that is , observations for which the nearest neighbor was itself .Our premise is that those observations which are regarded as singletons for successively increasing radii can be regarded as occupying sparse regions of the chemical space defined by the data set and descriptors used .", "label": "", "metadata": {}, "score": "72.24616"}
{"text": "This family has the following parameters : , .Suppose U is composed of subsets of some ground set of enumerable items S and the similarity function of interest is the Jaccard index J .If \u03c0 is a permutation on the indices of S , for let .", "label": "", "metadata": {}, "score": "72.595505"}
{"text": "Katabi has been honored as one of the recipients of the Grace Murray Hopper Award , which recognizes the outstanding young computer professionals of the year .According to the ACM , \" These innovators have made significant contributions that enable computer science to solve real world challenges .", "label": "", "metadata": {}, "score": "72.6063"}
{"text": "We are going to develop more efficient hashing method based on other manifold learning approaches .Acknowledgments .The authors gratefully acknowledge the kind help from the Academic Editor Shengyong Chen .This work was supported by the National Nature Science Foundation of China under NSFC nos .", "label": "", "metadata": {}, "score": "73.078964"}
{"text": "To test the approach , we considered three data sets .The first data set contained 4337 compounds and was described by Kazius et al.16The molecules in this data set were studied using the AMES test.17The second data set consisted of 42 689 compounds18,19from the NCI repository .", "label": "", "metadata": {}, "score": "73.59708"}
{"text": "View at Google Scholar \u00b7 View at Scopus .S. Chen , J. Zhang , Y. Li , and J. Zhang , \" A hierarchical model incorporating segmented regions and pixel descriptors for video background subtraction , \" IEEE Transactions on Industrial Informatics , vol . 8 , no . 1 , pp .", "label": "", "metadata": {}, "score": "74.08075"}
{"text": "Jonathan Goldstein Microsoft Research .Limitations on the performance and approximation of high dimensional nearest neighbor searches In this talk , I will discuss recent results concerning the properties of high dimensional nearest neighbor searches in metric spaces .These properties are closely tied to the weak law of large numbers and the central limit theorem , and point to inherent limitations on the performance of high dimensional searches .", "label": "", "metadata": {}, "score": "74.555466"}
{"text": "The results are illustrated in Figure 1 .It is shown that all three methods tend to keep the neighborhood relationship during the mapping .MVH - CG maps the Swiss roll into a cube and can maintain the submanifold of it in some sense .", "label": "", "metadata": {}, "score": "74.78671"}
{"text": "Some application to on - line learning in humanoid robotics will serve to illustrate the efficiency of the suggested methods .Greg Shakhnarovich MIT Computer Science and Artificial Intelligence Lab .Fast Example - Based Estimation with Parameter - Sensitive Hashing Joint work with Paul Viola and Trevor Darrell .", "label": "", "metadata": {}, "score": "76.109764"}
{"text": "We order the regions based o .. News + Events .Indyk and Katabi Win Top ACM honors .Press Contact .The Association for Computing Machinery ( ACM ) has announced that it is honoring Professor Piotr Indyk and Professor Dina Katabi for their innovations in computing technology .", "label": "", "metadata": {}, "score": "76.35708"}
{"text": "^ Gurmeet Singh , Manku ; Jain , Arvind ; Das Sarma , Anish ( 2007 ) , \" Detecting near - duplicates for web crawling \" , Proceedings of the 16th international conference on World Wide Web ( ACM ) .", "label": "", "metadata": {}, "score": "77.170586"}
{"text": "The structures were 2D and were converted to 3D and washed using MOE20with the MMFF94 force field with a 0.1 \u00c5 tolerance .However , a few molecules could not be converted successfully to 3D and , thus , were removed from the data set , resulting in a final data set consisting of 42 613 compounds .", "label": "", "metadata": {}, "score": "77.427414"}
{"text": "I. .As in [ 40 ] , [ 41 ] , all color images are normalized to have the longest side equals to 320 pixels .The segmentation results are then supersampled in order to obtain segmentation images with the origin ... . \" ...", "label": "", "metadata": {}, "score": "77.44863"}
{"text": "In the limiting radius ( i.e. , equal to the maximum pairwise radius ) , there will be no query point that will be a singleton .Thus , a possible strategy to look for isolated points is to consider relatively Figure 12 .", "label": "", "metadata": {}, "score": "77.8825"}
{"text": "It works on the basis that the sequence of restrict primal problems all have the same dual in which the most violated constraint indicates the steepest ascent direction of the dual .For ( 8 ) , the subproblem for generating the most violated constraint is as follows : . , we restrict it to be the decision stumps ( Since decision stump is a deterministic model , the column generation process will converge when all the constraints in the dual are satisfied , which means that no new hash function can be generated .", "label": "", "metadata": {}, "score": "79.54056"}
{"text": "Inf .Model . , Vol .46 , No . 1 , 2006 327 .Page 8 .Similar results were observed for the NCI-3D data set .In fact , for this data set , it was observed that , even for a radius equal to 0.0035 % of the maximum pairwise distance in the data set , the value of C hNNranged from 0.07 to 0.09 .", "label": "", "metadata": {}, "score": "80.60433"}
{"text": "Define the function family H to be the set of all such functions and let D be the uniform distribution .Given two sets the event that corresponds exactly to the event that the minimizer of \u03c0 over lies inside .As h was chosen uniformly at random , and define an LSH scheme for the Jaccard index .", "label": "", "metadata": {}, "score": "80.89165"}
{"text": "( 35 ) R : A Language and EnVironment for Statistical Computing ; R Foundation for Statistical Computing : Vienna , Austria , 2004 ; ISBN 3 - 900051 - 07 - 0 .CI050403O SCALABLE PARTITIONING OF CHEMICAL SPACES J. Chem .", "label": "", "metadata": {}, "score": "81.12883"}
{"text": "The structures are quite dis- tinct from the bulk of the data set , which consists of relatively smaller molecules .It is also interesting to note that molecule 3904 , which is the most outlying point in the principal component plots as well , in terms of Euclidean distance , is classified as a nonmutagen ( Table 4 ) .", "label": "", "metadata": {}, "score": "81.93204"}
{"text": "Finally , I will propose a new approach for evaluating nearest neighbor query processing strategies which takes into account data specific properties .Piotr Indyk MIT Computer Science and Artificial Intelligence Lab .Embedding Earth - Mover Distance into Normed Spaces Joint work with Nitin Thaper .", "label": "", "metadata": {}, "score": "82.91714"}
{"text": "In this section , we evaluate the hashing behavior of MVH - CG and the influence of the parameters .We first evaluate the hashing behavior of MVH - CG on a Swiss roll toy data .The Swiss roll is a 2D submanifold embedded in a 3D space and can be thought of as curling a piece of rectangular paper .", "label": "", "metadata": {}, "score": "94.68806"}
{"text": "Maximum Variance Hashing via Column Generation .Received 23 January 2013 ; Accepted 6 March 2013 .Academic Editor :Shengyong Chen .Copyright \u00a9 2013 Lei Luo et al .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "99.582306"}
