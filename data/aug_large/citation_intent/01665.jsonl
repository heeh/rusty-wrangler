{"text": "The parameters of the acoustic Markov models may be estimated from a known uttered training text by , for example , the Forward - Backward Algorithm .( See , for example , L. R. Bahl , et al . \"A Maximum Likelihood Approach to Continuous Speech Recognition . \"", "label": "", "metadata": {}, "score": "47.19174"}
{"text": "The score w_score for the reference template is the accumulated score of the winning path , including the noise frames , normalized by full path length , as standard in DTW .A speech score , called the Epa_Score , is the accumulated score of the portion of the winning path which refers to the speech frames only ( i.e. not including the noise frames ) , normalized by the length of the path which only compares speech frames .", "label": "", "metadata": {}, "score": "49.3479"}
{"text": "No .730,714 , filed on Jul. 16 , 1991 , entitled \" Past Algorithm for Deriving Acoustic Prototypes for Automatic Speech Recognition .\"Alternatively , all acoustic feature vectors generated by the utterance of a training text and which correspond to a given elementary model may be clustered by K - means Euclidean clustering or K - means Gaussian clustering , or both .", "label": "", "metadata": {}, "score": "49.615524"}
{"text": "No .730,714 , filed on Jul. 16 , 1991 , entitled \" Past Algorithm for Deriving Acoustic Prototypes for Automatic Speech Recognition .\"Alternatively , all acoustic feature vectors generated by the utterance of a training text and which correspond to a given elementary model may be clustered by K - means Euclidean clustering or K - means Gaussian clustering , or both .", "label": "", "metadata": {}, "score": "49.615524"}
{"text": "No .4,980,918 to Bahl et al entitled \" Speech Recognition System with Efficient Storage and Rapid Assembly of Phonological Graphs \" .Preferably , according to the present invention , for each frequency band i of the adapted feature vector signal X'(t ) at time t , the auditory model 62 calculates a new parameter E.sub.i ( t ) according to Equations 23 and 24 : . where . and where K.sub.1 , K.sub.2 , and K.sub.3 are fixed parameters of the auditory model .", "label": "", "metadata": {}, "score": "50.084206"}
{"text": "A speech recognition method as claimed in claim 19 , further comprising the steps of : . storing a source vocabulary of words in the source language ; . comparing each word in the source text with each word in the source vocabulary to identify each word in the source text which is not in the source vocabulary ; and .", "label": "", "metadata": {}, "score": "50.844597"}
{"text": "A speech recognition method as claimed in claim 19 , further comprising the steps of : . storing a source vocabulary of words in the source language ; . comparing each word in the source text with each word in the source vocabulary to identify each word in the source text which is not in the source vocabulary ; and .", "label": "", "metadata": {}, "score": "50.844597"}
{"text": "Such restriction of the possible paths from state to state within the model helps better model the sequential , ordered nature of acoustic events in a speech utterance .Such HMM architectures have proven effective for many speech recognition tasks .In particular , the algorithm considers a score , or probability , for the path back from point A , which represents the probability that the path lies through state 1 for all of the first five frames .", "label": "", "metadata": {}, "score": "51.273026"}
{"text": "A speech recognition system as claimed in claim 1 , further comprising : . means for storing a source vocabulary of words in the source language ; . means for comparing each word in the source text with each word in the source vocabulary to identify each word in the source text which is not in the source vocabulary ; and .", "label": "", "metadata": {}, "score": "51.653545"}
{"text": "A speech recognition system as claimed in claim 1 , further comprising : . means for storing a source vocabulary of words in the source language ; . means for comparing each word in the source text with each word in the source vocabulary to identify each word in the source text which is not in the source vocabulary ; and .", "label": "", "metadata": {}, "score": "51.653545"}
{"text": "The recognition task considers various word sequences in trying to determine the best match .For each word sequence that is considered by the recognition task , it computes an acoustic score and a language score .A language score indicates how likely the word sequence is , in the language , and is indicated by the term P(W ) in the above equation ( 10 ) .", "label": "", "metadata": {}, "score": "52.24583"}
{"text": "The recognition task considers various word sequences in trying to determine the best match .For each word sequence that is considered by the recognition task , it computes an acoustic score and a language score .A language score indicates how likely the word sequence is , in the language , and is indicated by the term P(W ) in the above equation ( 10 ) .", "label": "", "metadata": {}, "score": "52.24583"}
{"text": "The recognition task considers various word sequences in trying to determine the best match .For each word sequence that is considered by the recognition task , it computes an acoustic score and a language score .A language score indicates how likely the word sequence is , in the language , and is indicated by the term P(W ) in the above equation ( 10 ) .", "label": "", "metadata": {}, "score": "52.24583"}
{"text": "The score w_score for the reference template is the accumulated score of the winning path , including the noise frames , normalized by full path length , as standard in DTW .A speech score , called the Epd_Score , is the accumulated score of the portion of the winning path which refers to the speech frames only ( i.e. not including the noise frames ) , normalized by the length of the path which only compares speech frames .", "label": "", "metadata": {}, "score": "52.252823"}
{"text": "At the other extreme , the \" Delta Score \" can be set to such a value that the rejection rate of in - vocabulary words at noisy conditions will be similar to the rate at quiet conditions , at the expense of fewer rejections of out - of - vocabulary words .", "label": "", "metadata": {}, "score": "52.25324"}
{"text": "At the other extreme , the \" Delta Score \" can be set to such a value that the rejection rate of in - vocabulary words at noisy conditions will be similar to the rate at quiet conditions , at the expense of fewer rejections of out - of - vocabulary words .", "label": "", "metadata": {}, "score": "52.25324"}
{"text": "The Baum - Welch algorithm is preferred since it makes better use of training data .It is described in Huang et al . , Hidden Markov Models For Speech Recognition , Edinburgh University Press , 1990 , which is hereby incorporated by reference .", "label": "", "metadata": {}, "score": "52.55151"}
{"text": "The Baum - Welch algorithm is preferred since it makes better use of training data .It is described in Huang et al . , Hidden Markov Models For Speech Recognition , Edinburgh University Press , 1990 , which is hereby incorporated by reference .", "label": "", "metadata": {}, "score": "52.55151"}
{"text": "The Baum - Welch algorithm is preferred since it makes better use of training data .It is described in Huang et al . , Hidden Markov Models For Speech Recognition , Edinburgh University Press , 1990 , which is hereby incorporated by reference .", "label": "", "metadata": {}, "score": "52.55151"}
{"text": "When the acoustic models are Markov models , acoustic match scores may be obtained , for example , by the forward pass of the Forward - Backward Algorithm .( See , for example , L. R. Bahl , et al , March 1983 , cited above . )", "label": "", "metadata": {}, "score": "53.099403"}
{"text": "No .5,732,394 and in the article by Richard C. Rose and Douglas B. Paul , \" A Hidden Markov Model Based Keyword Recognition System \" , ICASSP ' 90 , 1990 , page 129 .[ 0029 ] .Even when such score - normalization methods are efficient to the extent that the variability due to the specific utterance is minimized , there still remains a problem due to the variability in the environment .", "label": "", "metadata": {}, "score": "53.667465"}
{"text": "5,732,394 and in the article by Richard C. Rose and Douglas B. Paul , \" A Hidden Markov Model Based Keyword Recognition System \" , ICASSP ' 90 , 1990 , page 129 .Even when such score - normalization methods are efficient to the extent that the variability due to the specific utterance is minimized , there still remains a problem due to the variability in the environment .", "label": "", "metadata": {}, "score": "53.6931"}
{"text": "5,732,394 and in the article by Richard C. Rose and Douglas B. Paul , \" A Hidden Markov Model Based Keyword Recognition System \" , ICASSP ' 90 , 1990 , page 129 .Even when such score - normalization methods are efficient to the extent that the variability due to the specific utterance is minimized , there still remains a problem due to the variability in the environment .", "label": "", "metadata": {}, "score": "53.6931"}
{"text": "A solution to the problem is to adapt the threshold to the acoustic conditions , e.g. , make the threshold a function of the signal to noise ratio , as in U.S. Pat .No . 5,778,342 .This solution requires the estimation of the noise from speech - free waveform segments , which , in turn requires knowledge of the speech end points , which are not known to a sufficient precision .", "label": "", "metadata": {}, "score": "53.696175"}
{"text": "No .4,748,670 by Lalit R. Bahl et al entitled \" Apparatus And Method For Determining A Likely Word Sequence From Labels Generated By An Acoustic Processor . \"FIG .2 is a block diagram of a portion of one example of a speech recognition system according to the invention .", "label": "", "metadata": {}, "score": "53.869507"}
{"text": "5 , a parametric representation for the data points in the deleted block is generated as shown in step 90 .In this case , the parametric representation is a mixture of Gaussians .This representation can be made using the Baum - Welch algorithm as described above .", "label": "", "metadata": {}, "score": "53.93592"}
{"text": "5 , a parametric representation for the data points in the deleted block is generated as shown in step 90 .In this case , the parametric representation is a mixture of Gaussians .This representation can be made using the Baum - Welch algorithm as described above .", "label": "", "metadata": {}, "score": "53.93592"}
{"text": "5 , a parametric representation for the data points in the deleted block is generated as shown in step 90 .In this case , the parametric representation is a mixture of Gaussians .This representation can be made using the Baum - Welch algorithm as described above .", "label": "", "metadata": {}, "score": "53.93592"}
{"text": "By contrast , the context - independent model is highly trainable thereby producing more robust estimates which are less detailed .The combination of these two models , weighed in the appropriate manner , can be used by the recognition engine to produce a more accurate acoustic probability score .", "label": "", "metadata": {}, "score": "54.221394"}
{"text": "By contrast , the context - independent model is highly trainable thereby producing more robust estimates which are less detailed .The combination of these two models , weighed in the appropriate manner , can be used by the recognition engine to produce a more accurate acoustic probability score .", "label": "", "metadata": {}, "score": "54.221394"}
{"text": "By contrast , the context - independent model is highly trainable thereby producing more robust estimates which are less detailed .The combination of these two models , weighed in the appropriate manner , can be used by the recognition engine to produce a more accurate acoustic probability score .", "label": "", "metadata": {}, "score": "54.221394"}
{"text": "64 , pp .1211 - 1222 , July / August 1986 .J. Kittler and J. Illingworth , \" Minimum error thresholding \" , Pattern Recognition , vol .19 , pp .41 - 47 , 1986 .The performance can be further improved by incorporating discriminative training .", "label": "", "metadata": {}, "score": "54.44587"}
{"text": "This parametric representation can be derived using the Baum - Welch algorithm on the training data in the deleted block .From this parametric representation , a prescribed number of data points is reconstructed using a random number generator with the mean and weight parameters , as shown in step 102 .", "label": "", "metadata": {}, "score": "54.768867"}
{"text": "This parametric representation can be derived using the Baum - Welch algorithm on the training data in the deleted block .From this parametric representation , a prescribed number of data points is reconstructed using a random number generator with the mean and weight parameters , as shown in step 102 .", "label": "", "metadata": {}, "score": "54.768867"}
{"text": "This parametric representation can be derived using the Baum - Welch algorithm on the training data in the deleted block .From this parametric representation , a prescribed number of data points is reconstructed using a random number generator with the mean and weight parameters , as shown in step 102 .", "label": "", "metadata": {}, "score": "54.768867"}
{"text": "Auditory model 62 may , for example , provide a model of how the human auditory system perceives sound signals .An example of an auditory model is described in U.S. Pat .No .4,980,918 to Bahl et al entitled \" Speech Recognition System with Efficient Storage and Rapid Assembly of Phonological Graphs \" .", "label": "", "metadata": {}, "score": "54.94879"}
{"text": "This mixture prevents the possibility of an abnormally small or large average score .The a priori average score , shown in FIG .8A to which reference is now briefly made , is a function of a segmental SNR SegSNR , where this function is determined by simulation over a large data base .", "label": "", "metadata": {}, "score": "54.987488"}
{"text": "No .5,778,342 to Erell et al . .[ 0016 ] .The problem of inaccurate endpoints has been less covered in the art .No .5,732,394 to Nakadai et al . .[0017 ] .In normal DTW , a sequence of spectral parameters from the speech start to end point is stored as an input speech pattern .", "label": "", "metadata": {}, "score": "55.065727"}
{"text": "8A to which reference is now briefly made , is a function of a segmental SNR SegSNR , where this function is determined by simulation over a large data base .FIG .8A shows that the a priori average score falls as the segmental SNR increases .", "label": "", "metadata": {}, "score": "55.37448"}
{"text": "8A to which reference is now briefly made , is a function of a segmental SNR SegSNR , where this function is determined by simulation over a large data base .FIG .8A shows that the a priori average score falls as the segmental SNR increases .", "label": "", "metadata": {}, "score": "55.37448"}
{"text": "In the speech recognition system and method according to the present invention , information about the source sentence being translated is used to estimate the probability that each speech hypothesis would be uttered .This probability is estimated with the aid of a translation model .", "label": "", "metadata": {}, "score": "55.80079"}
{"text": "In the speech recognition system and method according to the present invention , information about the source sentence being translated is used to estimate the probability that each speech hypothesis would be uttered .This probability is estimated with the aid of a translation model .", "label": "", "metadata": {}, "score": "55.80079"}
{"text": "These models are later used during speech recognition .The recognition system compares the features of an unknown utterance with stored model parameters to determine the best match .The best matching model is then output from the recognition system as the result .", "label": "", "metadata": {}, "score": "55.864773"}
{"text": "5,732,394 , there is a higher computation load since several DTW matches are performed for each pair of test and reference patterns , instead of one .[ 0021 ] .[ 0022 ] .The approach in these publications is that of a single - stage HMM - based system , running in real - time on the input speech , without a VAD .", "label": "", "metadata": {}, "score": "56.239655"}
{"text": "The models may be context - independent or context - dependent .The models may be built up from submodels of phonemes .Context - independent acoustic Markov models may be produced , for example , by the method described in U.S. Pat .", "label": "", "metadata": {}, "score": "56.242283"}
{"text": "The corresponding word is the word recognized by the algorithm .Recognition accuracy can be significantly improved by incorporating the N - best search to be described hereinafter .A suitable global N - best hypotheses algorithm is described in F. K. Soong and E - F. Huang , \" A tree - trellis based fast search for finding the n best sentence hypotheses in continuous speech recognition , \" in Proc .", "label": "", "metadata": {}, "score": "56.246525"}
{"text": "One technique is described in U.S. Pat .No .5,778,342 to Erell et al . .The problem of inaccurate endpoints has been less covered in the art .No .5,732,394 to Nakadai et al . .In normal DTW , a sequence of spectral parameters from the speech start to end point is stored as an input speech pattern .", "label": "", "metadata": {}, "score": "56.494034"}
{"text": "It is desired to minimize the number of training utterances to a small number ( 1 - 3 ) , for which it is known in the art that a dynamic time warping ( DTW ) matching algorithm works better than a hidden marlcov model ( HMM ) algorithm ; . [ 0006 ] .", "label": "", "metadata": {}, "score": "56.686195"}
{"text": "Thus , the latter is part of the model parameter , and the former is not ( at least in practical sense ) .This nested Viterbi algorithm is a crucial part of the training and recognition process .This nested structure is clearly shown in FIG .", "label": "", "metadata": {}, "score": "56.839973"}
{"text": "Again , the idea is to avoid the use of endpoints in the DTW matching by using noise - templates that are augmented to the speech templates and matching the whole utterance to the concatenated templates .[ 0025 ] .No details are given in the Landell et al .", "label": "", "metadata": {}, "score": "56.903175"}
{"text": "Then , the signal beyond the endpoints will not contain the noise burst , and the SNR estimator will overestimate the SNR , leading to a badly - adapted rejection threshold .[ 0031 ] .Another source of score - variability occurs in speaker dependent systems which allow the user to register either one word or two connected words .", "label": "", "metadata": {}, "score": "56.929543"}
{"text": "This greatly increases the computation and degrades the effectiveness for this hypotheses search approach , which was intended to improve recognition rate by imposing postprocess on those hypotheses and find the correct hypotheses .That is the advantage of the approaches described above .", "label": "", "metadata": {}, "score": "56.941387"}
{"text": "The recognition is speaker dependent , where the reference templates are created from speech utterances , spoken by the user in a designated \" training session \" ; .It is desired to minimize the number of training utterances to a small number ( 1 - 3 ) , for which it is known in the art that a dynamic time warping ( DTW ) matching algorithm works better than a hidden markov model ( HMM ) algorithm ; .", "label": "", "metadata": {}, "score": "56.997932"}
{"text": "Speech recognition system for natural language translation US 5293584 A .Abstract .A speech recognition system displays a source text of one or more words in a source language .The system has an acoustic processor for generating a sequence of coded representations of an utterance to be recognized .", "label": "", "metadata": {}, "score": "57.01255"}
{"text": "However , in speaker dependent recognition systems with user - created vocabularies , there might be only a few words in the vocabulary and therefore , the average score to all the other templates is not a good representative of a general speech template .", "label": "", "metadata": {}, "score": "57.108948"}
{"text": "A speech recognition method is claimed in claim 25 , characterized in that : . each word in the source text bas a spelling comprising one or more letters , each letter being upper case or being lower case ; . the method further comprises the step of identifying each word in the source text which has an upper case first letter ; and .", "label": "", "metadata": {}, "score": "57.29412"}
{"text": "A speech recognition method is claimed in claim 25 , characterized in that : . each word in the source text bas a spelling comprising one or more letters , each letter being upper case or being lower case ; . the method further comprises the step of identifying each word in the source text which has an upper case first letter ; and .", "label": "", "metadata": {}, "score": "57.29412"}
{"text": "The twenty dimension adapted feature vector signal X'(t ) from the adaptive labeler 60 is preferably provided to an auditory model 62 .Auditory model 62 may , for example , provide a model of how the human auditory system perceives sound signals .", "label": "", "metadata": {}, "score": "57.46376"}
{"text": "In particular , the alignment algorithm operates to determine the best path back from any point based upon a score for each path considered , as will be described generally with respect to FIG .4 .As used herein , a point is a frame and state location in the lattice 400 .", "label": "", "metadata": {}, "score": "57.569794"}
{"text": "[0067 ] .The dependence on the average score to all the other templates is by way of normalization , i.e. the Epd_Score has to be significantly smaller than the average score .In that sense , the average score acts as a model of general speech , similar to that used in state of the art HMM systems .", "label": "", "metadata": {}, "score": "57.758163"}
{"text": "This is performed using the graph of FIG .1A , to which reference is now briefly made .The frames of the input speech pattern are placed on the X axis and those of the current reference pattern are placed on the Y axis .", "label": "", "metadata": {}, "score": "57.911423"}
{"text": "This is performed using the graph of FIG .1A , to which reference is now briefly made .The frames of the input speech pattern are placed on the X axis and those of the current reference pattern are placed on the Y axis .", "label": "", "metadata": {}, "score": "57.911423"}
{"text": "( a ) from F - E extension : .( b ) from F - C extension : .Through the process mentioned above , the predicted CPSs of all the possible extensions was determined ( at F , the extensions are F - E and F - C ) .", "label": "", "metadata": {}, "score": "57.93047"}
{"text": "For this purpose , the average score is computed as a mixture of an a priori value , determined by simulations , and the actual average score .This mixture prevents the possibility of an abnormally small or large average score .", "label": "", "metadata": {}, "score": "57.945362"}
{"text": "For this purpose , the average score is computed as a mixture of an a priori value , determined by simulations , and the actual average score .This mixture prevents the possibility of an abnormally small or large average score .", "label": "", "metadata": {}, "score": "57.945362"}
{"text": "8A shows that the a priori average score falls as the segmental SNR increases .[0068 ] .The segmental SNR is the signal to noise ratio of the test utterance between the DTW - derived endpoints as determined from the match to the best template Best_Template .", "label": "", "metadata": {}, "score": "57.988956"}
{"text": "Note that this procedure is only required when there is no initial estimates of PHMMs available to start the training process .References .O. E. Agazzi and S. Kuo , \" Pseuod two - dimensional hidden markov models for document recognition , \" AT&T Technical Journal , vol .", "label": "", "metadata": {}, "score": "58.087116"}
{"text": "The system has limited fast - access memory , so that it is impossible to run DTW matching against all reference templates , in real - time and in a word - spotting manner .Therefore a two - stage processing is required , where the first stage is a voice activity detector ( VAD ) , and the second stage is a DTW matcher .", "label": "", "metadata": {}, "score": "58.10324"}
{"text": "( This statement is true even through the DTW scoring normalizes the accumulated score by the DTW path length , which is longer for two - word utterances than for one - word . )This creates a problem for the rejection mechanism , since two - word utterances are rejected more than one - word utterances .", "label": "", "metadata": {}, "score": "58.13643"}
{"text": "( This statement is true even through the DTW scoring normalizes the accumulated score by the DTW path length , which is longer for two - word utterances than for one - word . )This creates a problem for the rejection mechanism , since two - word utterances are rejected more than one - word utterances .", "label": "", "metadata": {}, "score": "58.13643"}
{"text": "( This statement is true even through the DTW scoring normalizes the accumulated score by the DTW path length , which is longer for two - word utterances than for one - word . )This creates a problem for the rejection mechanism , since two - word utterances are rejected more than one - word utterances .", "label": "", "metadata": {}, "score": "58.13643"}
{"text": "After calculating the scores , the path transition type producing the highest score for each state is saved as the path into each state of frame five .FIG .9 further illustrates the alignment algorithm that takes place during recognition between the features vectors of the utterance and the states of the stored models generated during training .", "label": "", "metadata": {}, "score": "58.182106"}
{"text": "The recognition algorithm is essentially a mapping of the speech signal , a continuous - time signal , to a set of reference patterns representing the phonetic and phonological descriptions of speech previously obtained from training data .In order to perform this mapping , signal processing techniques such as fast fourier transforms ( FFT ) , linear predictive coding ( LPC ) , or filter banks are applied to a digital form of the speech signal to extract an appropriate parametric representation of the speech signal .", "label": "", "metadata": {}, "score": "58.24115"}
{"text": "The recognition algorithm is essentially a mapping of the speech signal , a continuous - time signal , to a set of reference patterns representing the phonetic and phonological descriptions of speech previously obtained from training data .In order to perform this mapping , signal processing techniques such as fast fourier transforms ( FFT ) , linear predictive coding ( LPC ) , or filter banks are applied to a digital form of the speech signal to extract an appropriate parametric representation of the speech signal .", "label": "", "metadata": {}, "score": "58.24115"}
{"text": "The recognition algorithm is essentially a mapping of the speech signal , a continuous - time signal , to a set of reference patterns representing the phonetic and phonological descriptions of speech previously obtained from training data .In order to perform this mapping , signal processing techniques such as fast fourier transforms ( FFT ) , linear predictive coding ( LPC ) , or filter banks are applied to a digital form of the speech signal to extract an appropriate parametric representation of the speech signal .", "label": "", "metadata": {}, "score": "58.24115"}
{"text": "As a result , the recognition performance has been significantly enhanced , as will be described in greater detail hereinafter .Pursuant to the enhanced N - best hypotheses algorithm , the N - best hypotheses are obtained by first executing the nested Viterbi forward search , and then followed by a backward search .", "label": "", "metadata": {}, "score": "58.24166"}
{"text": "8 is a flow diagram of the speech recognition method used in the system of FIG .1 . DETAILED DESCRIPTION OF THE INVENTION .The speech recognition system of the claimed invention receives an input speech utterance , in the form of a continuous signal , and generates the most likely linguistic expression that corresponds to the utterance .", "label": "", "metadata": {}, "score": "58.33306"}
{"text": "8 is a flow diagram of the speech recognition method used in the system of FIG .1 . DETAILED DESCRIPTION OF THE INVENTION .The speech recognition system of the claimed invention receives an input speech utterance , in the form of a continuous signal , and generates the most likely linguistic expression that corresponds to the utterance .", "label": "", "metadata": {}, "score": "58.33306"}
{"text": "8 is a flow diagram of the speech recognition method used in the system of FIG .1 . DETAILED DESCRIPTION OF THE INVENTION .The speech recognition system of the claimed invention receives an input speech utterance , in the form of a continuous signal , and generates the most likely linguistic expression that corresponds to the utterance .", "label": "", "metadata": {}, "score": "58.33306"}
{"text": "The noise vector N(t ) is updated according to the formula .After noise cancellation , the feature vector F'(t ) is normalized to adjust for variations in the loudness of the input speech by short term mean normalization processor 58 .", "label": "", "metadata": {}, "score": "58.380566"}
{"text": "A speech recognition system as claimed in claim 7 , characterized in that : . each word in the source text has a spelling comprising one or more letters , each letter being upper case or being lower case ; .the system further comprises means for identifying each word in the source text which has an upper case first letter ; and .", "label": "", "metadata": {}, "score": "58.422558"}
{"text": "A speech recognition system as claimed in claim 7 , characterized in that : . each word in the source text has a spelling comprising one or more letters , each letter being upper case or being lower case ; .the system further comprises means for identifying each word in the source text which has an upper case first letter ; and .", "label": "", "metadata": {}, "score": "58.422558"}
{"text": "Another prior art method that also uses concatenated noise - speech - noise models for a DTW - based system is proposed in the article by B. Patrick Landell , Robert E. Wohlford and Lawrence G. Bahler entitled \" Improved Speech Recognition in Noise \" , ICASSP 86 , TOKYO , 1986 , pages 749 - 751 .", "label": "", "metadata": {}, "score": "58.447968"}
{"text": "5,732,394 to Nakadai et al . .In normal DTW , a sequence of spectral parameters from the speech start to end point is stored as an input speech pattern .The DTW operation matches the unknown speech pattern with the content of each reference template and calculates a distance measure between them .", "label": "", "metadata": {}, "score": "58.49791"}
{"text": "The method according to . claim 1 further comprising : .Modifying a template for use in a speech recognition system comprising : . a peak energy estimator adapted to estimate the peak of a reference template and a widened token ; . an adjuster adapted to adjust said level - raised reference template by adding to it noise qualities of said widened token .", "label": "", "metadata": {}, "score": "58.624477"}
{"text": "A computer system for matching an input speech utterance to a linguistic expression , comprising : . a model sequence generator which provides select sequences of context - dependent acoustic models representing a plurality of linguistic expressions likely to match the input speech utterance ; . a comparator to determine the sequence which best matches the input speech utterance , the sequence representing the linguistic expression .", "label": "", "metadata": {}, "score": "58.63396"}
{"text": "A computer system for matching an input speech utterance to a linguistic expression , comprising : . a model sequence generator which provides select sequences of context - dependent acoustic models representing a plurality of linguistic expressions likely to match the input speech utterance ; . a comparator to determine the sequence which best matches the input speech utterance , the sequence representing the linguistic expression .", "label": "", "metadata": {}, "score": "58.63396"}
{"text": "A computer system for matching an input speech utterance to a linguistic expression , comprising : . a model sequence generator which provides select sequences of context - dependent acoustic models representing a plurality of linguistic expressions likely to match the input speech utterance ; . a comparator to determine the sequence which best matches the input speech utterance , the sequence representing the linguistic expression .", "label": "", "metadata": {}, "score": "58.63396"}
{"text": "The approach in these publications is that of a single - stage HMM - based system , running in real - time on the input speech , without a VAD .To deal with the noise segments , the HMM model of the word is concatenated on both ends with HMM model of the noise , to form a composite model of the whole utterance .", "label": "", "metadata": {}, "score": "58.678665"}
{"text": "The system has limited fast - access memory , so that it is impossible to run DTW matching against all reference templates , in realtime and in a word - spotting manner .Therefore a two - stage processing is required , where the first stage is a voice activity detector ( VAD ) , and the second stage is a DTW matcher .", "label": "", "metadata": {}, "score": "58.787136"}
{"text": "In step 138 , the method recognizes the input speech utterance as the word sequence having the highest combined recognition score .In step 130 , the acoustic score can be determined as described above in accord with equation ( 11 ) where the output probability is computed as described above in equation ( 12 ) .", "label": "", "metadata": {}, "score": "58.807304"}
{"text": "In step 138 , the method recognizes the input speech utterance as the word sequence having the highest combined recognition score .In step 130 , the acoustic score can be determined as described above in accord with equation ( 11 ) where the output probability is computed as described above in equation ( 12 ) .", "label": "", "metadata": {}, "score": "58.80732"}
{"text": "In step 138 , the method recognizes the input speech utterance as the word sequence having the highest combined recognition score .In step 130 , the acoustic score can be determined as described above in accord with equation ( 11 ) where the output probability is computed as described above in equation ( 12 ) .", "label": "", "metadata": {}, "score": "58.80732"}
{"text": "Used in conjunction with an N - best approach , ( to be described hereinafter ) it can also give better word candidates , especially for those cases where the best candidate is not the correct one .It can also greatly reduce the search size for proper higher - level postprocessing ( e.g. dictionary check ) .", "label": "", "metadata": {}, "score": "58.810482"}
{"text": "At the end of all the iterations , the average value of the weighting factor is calculated and used in the recognition phase .FIGS .3 - 6 illustrate the steps used in computing the weighting factors .Referring to FIG .", "label": "", "metadata": {}, "score": "58.889847"}
{"text": "At the end of all the iterations , the average value of the weighting factor is calculated and used in the recognition phase .FIGS .3 - 6 illustrate the steps used in computing the weighting factors .Referring to FIG .", "label": "", "metadata": {}, "score": "58.889847"}
{"text": "At the end of all the iterations , the average value of the weighting factor is calculated and used in the recognition phase .FIGS .3 - 6 illustrate the steps used in computing the weighting factors .Referring to FIG .", "label": "", "metadata": {}, "score": "58.889847"}
{"text": "8 .[ 0076 ] .It will be appreciated by persons skilled in the art that the present invention is not limited by what has been particularly shown and described herein above .Rather the scope of the invention is defined by the claims that follow : A speech recognition system includes a token builder , a noise estimator , a template padder , a gain and noise adapter and a dynamic time warping ( DTW ) unit .", "label": "", "metadata": {}, "score": "58.991882"}
{"text": "If the Examiner requires the book during prosecution of this application , Applicants will provide a copy of it .A frame synchronous Network Search Algorithm for Connected Word Recognition by C. Lee et al . , IEEE trans .Acouts .", "label": "", "metadata": {}, "score": "59.039433"}
{"text": "Alternatively , the weighting factor can be estimated from a parametric representation of the data points or from randomly - generated data points generated from a parametric representation of the data points .The recognition engine receives an input speech utterance and generates candidate word sequences which will most likely match the feature vectors of the input speech utterance .", "label": "", "metadata": {}, "score": "59.040432"}
{"text": "Alternatively , the weighting factor can be estimated from a parametric representation of the data points or from randomly - generated data points generated from a parametric representation of the data points .The recognition engine receives an input speech utterance and generates candidate word sequences which will most likely match the feature vectors of the input speech utterance .", "label": "", "metadata": {}, "score": "59.040432"}
{"text": "Alternatively , the weighting factor can be estimated from a parametric representation of the data points or from randomly - generated data points generated from a parametric representation of the data points .The recognition engine receives an input speech utterance and generates candidate word sequences which will most likely match the feature vectors of the input speech utterance .", "label": "", "metadata": {}, "score": "59.040432"}
{"text": "ASSP 37 , pp .1649 1658 , Nov. 1989 .A Segmental k Means Training Procedure for Connected Word Recognition Based on Whole Word Reference Patterns , L. Rabiner , et al . , AT&T Technical Journal , vol .65 , pp .", "label": "", "metadata": {}, "score": "59.089226"}
{"text": "Here , select extension E , assuming that CPS(E ) .Since one goal of character recognition systems is to find the top N \" word \" hypotheses for high - level grammar or dictionary checks , no two paths in the stack should lead to the same \" word \" hypothesis , specially under limited stack space .", "label": "", "metadata": {}, "score": "59.37405"}
{"text": "In that sense , the average score acts as a model of general speech , similar to that used in state of the art HMM systems .If the vocabulary of templates is large enough , the average score is a good representative of a score to a general speech template .", "label": "", "metadata": {}, "score": "59.496765"}
{"text": "Remember that at this time , the path score is still equal to the accumulated likelihood scores in the Viterbi search .No modification is performed yet .These paths will then develop backward towards their initial nodes ( i.e. from right to left , towards frame 0 ) .", "label": "", "metadata": {}, "score": "59.560883"}
{"text": "This inaccurate noise estimate leads to recognition errors .Another prior art method that also uses concatenated noise - speech - noise models for a DTW - based system is proposed in the article by B. Patrick Landell , Robert E. Wohlford and Lawrence G. Bahler entitled \" improved Speech Recognition in Noise \" , ICASSP 86 , TOKYO , 1986 , pages 749 - 751 .", "label": "", "metadata": {}, "score": "59.594738"}
{"text": "This produces a more accurate computation at the expense of increasing the training time as well as the amount of storage needed by the training engine to perform the computations .In some instances , it may be more advantageous to generate a parametric representation of the data points in the deleted block that correspond and to use the appropriate parameters instead .", "label": "", "metadata": {}, "score": "59.610558"}
{"text": "This produces a more accurate computation at the expense of increasing the training time as well as the amount of storage needed by the training engine to perform the computations .In some instances , it may be more advantageous to generate a parametric representation of the data points in the deleted block that correspond and to use the appropriate parameters instead .", "label": "", "metadata": {}, "score": "59.610558"}
{"text": "This produces a more accurate computation at the expense of increasing the training time as well as the amount of storage needed by the training engine to perform the computations .In some instances , it may be more advantageous to generate a parametric representation of the data points in the deleted block that correspond and to use the appropriate parameters instead .", "label": "", "metadata": {}, "score": "59.610558"}
{"text": "The padding and adaptation operations are shown in FIGS .5 A and SB to which reference is now briefly made .FIG .5A shows the signal 71 represented by a noiseless template with blank frames 72 ( having no signal therein ) on either end of signal 71 .", "label": "", "metadata": {}, "score": "59.614388"}
{"text": "the step of generating speech hypotheses generates one or more speech hypotheses solely from words in the set of candidate words .A speech recognition method as claimed in claim 19 , characterized in that the acoustic match score comprises an estimate of the probability of occurrence of the sequence of coded representations of the utterance given the occurrence of the speech hypothesis .", "label": "", "metadata": {}, "score": "59.671654"}
{"text": "the step of generating speech hypotheses generates one or more speech hypotheses solely from words in the set of candidate words .A speech recognition method as claimed in claim 19 , characterized in that the acoustic match score comprises an estimate of the probability of occurrence of the sequence of coded representations of the utterance given the occurrence of the speech hypothesis .", "label": "", "metadata": {}, "score": "59.671654"}
{"text": "In this way , duration penalty could be incorporated in the forward search , instead of as a postprocess .Our experiments show that this incorporation greatly improve the recognition performance .In N best approach , it can also give better word candidates , especially for those cases where the best candidate is not the correct one .", "label": "", "metadata": {}, "score": "59.717087"}
{"text": "A set of one or more speech hypotheses , each comprising one or more words from the target language , are produced .Each speech hypothesis is modeled with an acoustic model .An acoustic match score for each speech hypothesis comprises an estimate of the closeness of a match between the acoustic model of the speech hypothesis and the sequence of coded representations of the utterance .", "label": "", "metadata": {}, "score": "59.898792"}
{"text": "It should also compare them with the path coming in from the same model and same level , i.e. a path that might have been existing in this level started at some previous frame .The best incoming path among them will be picked as the origin of the new path .", "label": "", "metadata": {}, "score": "59.988598"}
{"text": "Use terminal nodes with the top M scores to build up the path stack ( block 1103 ) .All the paths are now null paths and only one node a terminal node .Enter a LOOP to develop the best first path ( 1105 ) .", "label": "", "metadata": {}, "score": "60.038208"}
{"text": "For each utterance , an alignment algorithm , such as a Viterbi algorithm , is used to make an assignment of the frames of an utterance , such as U 1 , to the states of the model at step 604 .", "label": "", "metadata": {}, "score": "60.078243"}
{"text": "Thus , for example , prototype vector signals P5 , P1 , P2 , P4 , and P3 could have been assigned rank scores of \" 1 \" , \" 2 \" , \" 3 \" , \" 3 \" and \" 3 \" , respectively .", "label": "", "metadata": {}, "score": "60.090294"}
{"text": "Thus , for example , prototype vector signals P5 , P1 , P2 , P4 , and P3 could have been assigned rank scores of \" 1 \" , \" 2 \" , \" 3 \" , \" 3 \" and \" 3 \" , respectively .", "label": "", "metadata": {}, "score": "60.090294"}
{"text": "This concept is discussed below .Once we get the initial models , or any updated models during this iterated k - mean process , the models are used to get the ( super)state sequence segmentation through optimal path back - tracing .", "label": "", "metadata": {}, "score": "60.260742"}
{"text": "This results in images with connected neighboring character .The parameter range for set 2 is ( 70 - 79 , 0.9 - 1.0 ) , and set 3 is ( 80 - 89 , 1.0 - 1.1 ) .The algorithms we compared are binary , gray - level , and gray - level with N - best search , all with duration constrain .", "label": "", "metadata": {}, "score": "60.264137"}
{"text": "77 , pp .257 - 286 , February 1989 .The Viterbi algorithm has also been used in text recognition , as described in U. S. patent application Ser .No .07/813,225 assigned to the assignee of the present patent application mentioned above .", "label": "", "metadata": {}, "score": "60.27686"}
{"text": "The source text comprises one or more words in a source language .An acoustic processor generates a sequence of coded representations of an utterance to be recognized .The utterance comprising a series of one or more words in a target language different from the source language .", "label": "", "metadata": {}, "score": "60.349648"}
{"text": "The source text comprises one or more words in a source language .An acoustic processor generates a sequence of coded representations of an utterance to be recognized .The utterance comprising a series of one or more words in a target language different from the source language .", "label": "", "metadata": {}, "score": "60.349648"}
{"text": "Speech Signal Processing , Vol .38 , pp .1870 - 1878 , November 1990 .It has also been realized that many problems in speech recognition have their counterparts in optical character recognition ( OCR ) .Thus , stochastic modeling and dynamic programming have also been employed for this purpose .", "label": "", "metadata": {}, "score": "60.377728"}
{"text": "To deal with the noise segments , the HMM model of the word is concatenated on both ends with HMM model of the noise , to form a composite model of the whole utterance .This occurs because the word endpoints are not determined prior to the recognition and therefore , the noise can not be estimated from speech - free segments .", "label": "", "metadata": {}, "score": "60.551422"}
{"text": "i. e. the Epd_Score has to be significantly smaller than the average score .In that sense , the average score acts as a model of general speech , similar to that used in state of the art HMM systems .If the vocabulary of templates is large enough , the average score is a good representative of a score to a general speech template .", "label": "", "metadata": {}, "score": "60.72915"}
{"text": "determining the sequence which best matches the input speech utterance , the sequence representing the linguistic expression .A method as in claim 1 where each acoustic model is a continuous density hidden Markov model .A method as in claim 1 wherein the step of determining the output probability further comprises the step of weighing the less - detailed model and more - detailed model output probabilities with separate weighting factors when combined .", "label": "", "metadata": {}, "score": "60.936752"}
{"text": "determining the sequence which best matches the input speech utterance , the sequence representing the linguistic expression .A method as in claim 1 where each acoustic model is a continuous density hidden Markov model .A method as in claim 1 wherein the step of determining the output probability further comprises the step of weighing the less - detailed model and more - detailed model output probabilities with separate weighting factors when combined .", "label": "", "metadata": {}, "score": "60.936752"}
{"text": "A sequence of these feature vectors is mapped to the set of reference patterns which identify linguistic units , words and/or sentences contained in the speech signal .Often , the speech signal does not exactly match the stored reference patterns .", "label": "", "metadata": {}, "score": "60.98619"}
{"text": "A sequence of these feature vectors is mapped to the set of reference patterns which identify linguistic units , words and/or sentences contained in the speech signal .Often , the speech signal does not exactly match the stored reference patterns .", "label": "", "metadata": {}, "score": "60.98619"}
{"text": "A sequence of these feature vectors is mapped to the set of reference patterns which identify linguistic units , words and/or sentences contained in the speech signal .Often , the speech signal does not exactly match the stored reference patterns .", "label": "", "metadata": {}, "score": "60.98619"}
{"text": "It will be appreciated by persons skilled in the art that the present invention is not limited by what has been particularly shown and described herein above .Rather the scope of the invention is defined by the claims that follow : A speech recognition system displays a source text of one or more words in a source language .", "label": "", "metadata": {}, "score": "61.02477"}
{"text": "The stored acoustic models may be , for example , Markov models or other dynamic programming type models .The parameters of the acoustic Markov models may be estimated from a known uttered training text by , for example , the Forward - Backward Algorithm .", "label": "", "metadata": {}, "score": "61.08419"}
{"text": "No .673,189 , filed Mar. 22 , 1991 .Bahl , L. R. , et al . \"Vector Quantization Procedure For Speech Recognition Systems Using Discrete Parameter Phoneme - Based Markov Word Models . \"IBM Technical Disclosure Bulletin , vol .", "label": "", "metadata": {}, "score": "61.12671"}
{"text": "More sophisticated language models based on probabilistic decision trees , stochastic context - free grammars , and automatically discovered classes of words have also been used .While statistical language models which use no knowledge or information about the actual utterance to be recognized are useful in scoring speech hypotheses in a speech recognition system , the best scoring speech hypotheses do not always correctly identify the corresponding utterances to be recognized .", "label": "", "metadata": {}, "score": "61.18773"}
{"text": "More sophisticated language models based on probabilistic decision trees , stochastic context - free grammars , and automatically discovered classes of words have also been used .While statistical language models which use no knowledge or information about the actual utterance to be recognized are useful in scoring speech hypotheses in a speech recognition system , the best scoring speech hypotheses do not always correctly identify the corresponding utterances to be recognized .", "label": "", "metadata": {}, "score": "61.18773"}
{"text": "This is usually done by setting a threshold to the recognition score ( e. g. , the DTW or HMM score ) , i. e. , the recognition result is accepted only if the score is significant enough relative to the threshold .", "label": "", "metadata": {}, "score": "61.212986"}
{"text": "The basic information we need from this process is the link information from the current node ( superstate or state ) to one node before along the optimal path .The exact usage of the information obtained in this process for training and recognition will be discussed in detail in the following sections .", "label": "", "metadata": {}, "score": "61.21993"}
{"text": "This arrangement puts no constraints on how many frames at the edges of the widened test can be aligned against the noise frames .[0065 ] .The score w_score for the reference template is the accumulated score of the winning path , including the noise frames , normalized by full path length , as standard in DTW .", "label": "", "metadata": {}, "score": "61.377296"}
{"text": "Another source of score - variability occurs in speaker dependent systems which allow the user to register either one word or two connected words .For example , in Voice Activated Dialing by name , a user may register either a first name , last name , or a full name .", "label": "", "metadata": {}, "score": "61.41153"}
{"text": "Another source of score - variability occurs in speaker dependent systems which allow the user to register either one word or two connected words .For example , in Voice Activated Dialing by name , a user may register either a first name , last name , or a full name .", "label": "", "metadata": {}, "score": "61.41153"}
{"text": "With reference to FIG .7 , the following process is repeated until all the M paths in the main path stack 1000 have been subjected to the process .The process starts by removing the top path 1010 from the path stack 1000 .", "label": "", "metadata": {}, "score": "61.435074"}
{"text": "It is understood that a back - tracing path should start from a terminal node .For each terminal node of each character model at every level , a null path is constructed with only a terminal node in it , along with its final accumulated path score obtained after the Viterbi search .", "label": "", "metadata": {}, "score": "61.456623"}
{"text": "( b ) scanning the document to create a gray - level pixel map for each of a plurality of unknown text elements ; and .( c ) for each unknown text element : . determining the identity of said unknown text element as the known text element associated with the stochastic model for which said probability is highest .", "label": "", "metadata": {}, "score": "61.50023"}
{"text": "A speech recognition method as claimed in claim 25 , characterized in that the step of generating an acoustic model comprises : . storing a plurality of acoustic letter models ; and .generating an acoustic model of a word by replacing each letter in the spelling of the word with an acoustic letter model corresponding to the letter .", "label": "", "metadata": {}, "score": "61.51574"}
{"text": "A speech recognition method as claimed in claim 25 , characterized in that the step of generating an acoustic model comprises : . storing a plurality of acoustic letter models ; and .generating an acoustic model of a word by replacing each letter in the spelling of the word with an acoustic letter model corresponding to the letter .", "label": "", "metadata": {}, "score": "61.51574"}
{"text": "A speech recognition system as claimed in claim 7 , characterized in that the means for generating an acoustic model comprises : . means for storing a plurality of acoustic letter models ; and .means for generating an acoustic model of a word by replacing each letter in the spelling of the word with an acoustic letter model corresponding to the letter .", "label": "", "metadata": {}, "score": "61.552147"}
{"text": "A speech recognition system as claimed in claim 7 , characterized in that the means for generating an acoustic model comprises : . means for storing a plurality of acoustic letter models ; and .means for generating an acoustic model of a word by replacing each letter in the spelling of the word with an acoustic letter model corresponding to the letter .", "label": "", "metadata": {}, "score": "61.552147"}
{"text": "Still referring to FIG .1 , the speech recognition system further comprises a translation match score generator 26 for generating a translation match score for each speech hypothesis .Each translation match score comprises an estimate of the probability of occurrence of the speech hypothesis given the occurrence of the source text .", "label": "", "metadata": {}, "score": "61.596096"}
{"text": "No . 673,810 , filed on Mar. 22 , 1991 entitled \" Speaker - Independent Label Coding Apparatus \" .Bahl , L. R. et al .Apparatus and Method For Grouping Utterances of a Phoneme Into Context Dependent Categories Based on Sound Similarity For Automatic Speech Recognition .", "label": "", "metadata": {}, "score": "61.61402"}
{"text": "Abstract .A speech recognition system includes a token builder , a noise estimator , a template padder , a gain and noise adapter and a dynamic time warping ( DTW ) unit .The token builder produces a widened test token representing an input test utterance and at least one frame before and after the input test utterance .", "label": "", "metadata": {}, "score": "61.633636"}
{"text": "The normalized score is defined as the ratio of the best score Best_Score to the average score Av_Score corrected by two corrections , one for signal to noise ( SNRcorr ) and one for length ( length_corr ) , as follows : .", "label": "", "metadata": {}, "score": "61.696243"}
{"text": "According to the algorithm structure , after the nested Viterbi search , the optimal path will show : a ) at superstate level , where are the boundaries ( in terms of column index ) of each superstate and character .b ) at state level , where are the boundaries ( in terms of pixel index ) of each state .", "label": "", "metadata": {}, "score": "61.713356"}
{"text": "The parameters SNR 1 and SNR 2 are determined experimentally from the large speech database .The parameter \" Delta Score \" is left as an adjustable parameter , to be tailored to a specific application .For example , if delta_score is set to zero , there is no SNR correction at all .", "label": "", "metadata": {}, "score": "61.797962"}
{"text": "This method indeed eliminates some of the errors due to inaccurate endpoints .However , the relaxed - endpoint solutions have several disadvantages .Other disadvantages of the relaxed - endpoint methods are specific to the method .For example , in the article by Shallom , it is necessary to normalize , for each point on the DTW grid , the DTW accumulated score by the path length , since the relaxation of the beginning point allows now for multiple paths of different lengths .", "label": "", "metadata": {}, "score": "62.038696"}
{"text": "The padding and adaptation operations are shown in FIGS .5A and 5B to which reference is now briefly made .FIG .5A shows the signal 71 represented by a noiseless template with blank frames 72 ( having no signal therein ) on either end of signal 71 .", "label": "", "metadata": {}, "score": "62.057404"}
{"text": "The result is provided to the decision unit 62 which then determines which comparison was the best , by some measure .Reference is now made to FIG .6 which details the operation of an exemplary noise and peak energy estimator 68 .", "label": "", "metadata": {}, "score": "62.094604"}
{"text": "The result is provided to the decision unit 62 which then determines which comparison was the best , by some measure .Reference is now made to FIG .6 which details the operation of an exemplary noise and peak energy estimator 68 .", "label": "", "metadata": {}, "score": "62.094604"}
{"text": "The output probability associated with a state is determined by weighing the output probabilities of the context - dependent and context - independent states in accordance with a weighting factor .The weighting factor indicates the robustness of the output probability associated with each state of each model , especially in predicting unseen speech utterances .", "label": "", "metadata": {}, "score": "62.10023"}
{"text": "The output probability associated with a state is determined by weighing the output probabilities of the context - dependent and context - independent states in accordance with a weighting factor .The weighting factor indicates the robustness of the output probability associated with each state of each model , especially in predicting unseen speech utterances .", "label": "", "metadata": {}, "score": "62.10023"}
{"text": "Thus , the new modified output probability function of a context - dependent state is a combination of the output probability functions of both models weighed in accordance with the robustness of the estimates .Accordingly , in the preferred embodiment , deleted interpolation is used to smooth the probability space rather than the parameter space .", "label": "", "metadata": {}, "score": "62.179184"}
{"text": "Thus , the new modified output probability function of a context - dependent state is a combination of the output probability functions of both models weighed in accordance with the robustness of the estimates .Accordingly , in the preferred embodiment , deleted interpolation is used to smooth the probability space rather than the parameter space .", "label": "", "metadata": {}, "score": "62.179184"}
{"text": "Thus , the new modified output probability function of a context - dependent state is a combination of the output probability functions of both models weighed in accordance with the robustness of the estimates .Accordingly , in the preferred embodiment , deleted interpolation is used to smooth the probability space rather than the parameter space .", "label": "", "metadata": {}, "score": "62.179184"}
{"text": "A feature vector is matched to a codeword using a distortion measure .The feature vector is replaced by the index of the codeword having the smallest distortion measure .The recognition problem is reduced to computing the discrete output probability of an observed speech signal as a table look - up operation which requires minimal computation .", "label": "", "metadata": {}, "score": "62.210056"}
{"text": "A feature vector is matched to a codeword using a distortion measure .The feature vector is replaced by the index of the codeword having the smallest distortion measure .The recognition problem is reduced to computing the discrete output probability of an observed speech signal as a table look - up operation which requires minimal computation .", "label": "", "metadata": {}, "score": "62.210056"}
{"text": "A feature vector is matched to a codeword using a distortion measure .The feature vector is replaced by the index of the codeword having the smallest distortion measure .The recognition problem is reduced to computing the discrete output probability of an observed speech signal as a table look - up operation which requires minimal computation .", "label": "", "metadata": {}, "score": "62.210056"}
{"text": "By imposing postprocessing functions on these hypotheses , the recognition rate is greatly improved .The effectiveness of the aforementioned approaches is enhanced by employing duration constraints in the search processes .These and other aspects of the invention will become apparent from the drawings and detailed description .", "label": "", "metadata": {}, "score": "62.22872"}
{"text": "9 is performed for each word , or model , in the stored vocabulary , and the best output score is output as the matched word .The processor in step 906 calculates the best path to state i at frame t , described in greater detail in the description of FIG .", "label": "", "metadata": {}, "score": "62.274105"}
{"text": "The parameters SNR1 and SNR2 are determined experimentally from the large speech database .The parameter \" Delta Score \" is left as an adjustable parameter , to be tailored to a specific application .For example , if delta_score is set to zero , there is no SNR correction at all .", "label": "", "metadata": {}, "score": "62.305885"}
{"text": "However , in the relaxed - endpoint solution , shown in FIG .1B to which reference is now made , the DTW paths are not constrained to start or end at the exact endpoints of the test and reference utterances .", "label": "", "metadata": {}, "score": "62.324974"}
{"text": "Furthermore , documents in gray - level may be scanned and processed with much lower resolution than in binary without sacrificing the performance .This can also significantly increase the processing speed .A method of determining the identity of unknown text elements in a document comprising the following steps : .", "label": "", "metadata": {}, "score": "62.345272"}
{"text": "Preferably , in this embodiment , the speech hypothesis generator 16 generates one or more speech hypotheses solely from words in the set of candidate words from candidate word generator 20 .Returning to FIG .1 , the speech recognition system further comprises an acoustic model generator 18 for generating an acoustic model for each speech hypothesis generated by the speech hypothesis generator 16 .", "label": "", "metadata": {}, "score": "62.491062"}
{"text": "Bahl , L. R. et al .\" Fast Algorithm for Deriving Acoustic Prototypes for Automatic Speech Recognition . \" U.S. patent application Ser .No .730,714 Filed on Jul. 16 , 1991 .Bahl , L. R. , et al . \"", "label": "", "metadata": {}, "score": "62.501778"}
{"text": "No .468,546 , filed Jan. 23 , 1990 .Bahl , L. R. et al .Fast Algorithm for Deriving Acoustic Prototypes for Automatic Speech Recognition .U.S. patent application Ser .No .730,714 Filed on Jul. 16 , 1991 .", "label": "", "metadata": {}, "score": "62.52616"}
{"text": "The proportional nature of the penalty assigned allows the algorithm to be used even with models using a minimal number of training utterances .The penalty is linearly proportional to the distance from the upper and lower transition thresholds .However , other functional relations could be used , and applicants have successfully implemented other functions such as the square of the distance .", "label": "", "metadata": {}, "score": "62.596786"}
{"text": "( See , for example , \" Vector Quantization Procedure For Speech Recognition Systems Using Discrete Parameter Phoneme - Based Markov Word Models \" by L. R. Bahl , et al , IBM Technical Disclosure Bulletin , Volume 32 , No . 7 , December 1989 , pages 320 and 321 . )", "label": "", "metadata": {}, "score": "62.60135"}
{"text": "Speech recognition in noisy environments is a well studied , yet difficult task .One such task is characterized by the following parameters : .The recognition is speaker dependent , where the reference templates are created from speech utterances , spoken by the user in a designated \" training session \" ; .", "label": "", "metadata": {}, "score": "62.603542"}
{"text": "A method of assigning a penalty to a score in a voice recognition system comprising the steps of : . generating at least one of : . a lower threshold for the number of frames assigned to at least one state of at least one model ; . and an upper threshold for the number of frames assigned to at least one state of at least one model ; and . assigning at least one of : .", "label": "", "metadata": {}, "score": "62.813267"}
{"text": "No .4,829,577 issued to Kuroda et al . on Apr. 9 , 1989 .The portions of U.S. Pat .No .4,829,577 that describe the forward - backward algorithm are hereby incorporated by reference .However , this invention is not limited to this particular training algorithm , others may be utilized .", "label": "", "metadata": {}, "score": "62.822876"}
{"text": "No .4,829,577 issued to Kuroda et al . on Apr. 9 , 1989 .The portions of U.S. Pat .No .4,829,577 that describe the forward - backward algorithm are hereby incorporated by reference .However , this invention is not limited to this particular training algorithm , others may be utilized .", "label": "", "metadata": {}, "score": "62.822876"}
{"text": "No .4,829,577 issued to Kuroda et al . on Apr. 9 , 1989 .The portions of U.S. Pat .No .4,829,577 that describe the forward - backward algorithm are hereby incorporated by reference .However , this invention is not limited to this particular training algorithm , others may be utilized .", "label": "", "metadata": {}, "score": "62.822876"}
{"text": "FIG .2 is a circuit schematic in block diagram form illustrating a voice recognition system in the device according to FIG .1 .FIG .3 is an illustration of a left - right Hidden Markov Model with two associated speech utterances segmented into frames .", "label": "", "metadata": {}, "score": "62.87192"}
{"text": "6 is a diagram showing an illustrative state and superstate segmentation of a gray - level textual image ; .FIG .7 is a graph showing the character recognition rates of various preferred embodiments disclosed herein for three different character testing sets ; .", "label": "", "metadata": {}, "score": "62.945644"}
{"text": "For this reason some modem recognition systems break the Markov assumption and assign state transition penalties which are related to the duration of a state .In particular , it is known to simply bound the state duration to a minimum and maximum that are estimated during the training process .", "label": "", "metadata": {}, "score": "63.00167"}
{"text": "No .5,778,342 for the case where the features are autocorrelation function ( ACF ) .For the present invention , this transformation is : . where : .Other transformations can be employed for where the features are filterbank energies , as is discussed in the article \" Noise Masking in a Transform Domain \" , by B. A. Mellor and A. P. Varga , ICASSP ' 93 , 1993 , pp .", "label": "", "metadata": {}, "score": "63.008553"}
{"text": "No .5,778,342 for the case where the features are autocorrelation function ( ACF ) .For the present invention , this transformation is : . where : .Other transformations can be employed for where the features are filterbank energies , as is discussed in the article \" Noise Masking in a Transform Domain \" , by B. A. Mellor and A. P. Varga , ICASSP ' 93 , 1993 , pp .", "label": "", "metadata": {}, "score": "63.008553"}
{"text": "1A , to which reference is now briefly made .The frames of the input speech pattern are placed on the X axis and those of the current reference pattern are placed on the Y axis .A path is made through the graph , starting at the lower left comer and ending at the upper right comer , where the corners are defined as the endpoints of the test and reference utterances .", "label": "", "metadata": {}, "score": "63.041164"}
{"text": "This is usually done by setting a threshold to the recognition score ( e.g. , the DTW or IMM score ) , i.e. , the recognition result is accepted oily if the score is significant enough relative to the threshold .It is generally difficult to achieve efficient rejection of out - of - vocabulary or mispronounced utterances , without sacrificing also some rejection of in - vocabulary , well - pronounced utterances .", "label": "", "metadata": {}, "score": "63.09088"}
{"text": "Specifically , the average score is defined as : . [0073 ] .where w1 is a weight for the a_priori av_score .[ 0074 ] .The SNR correction SNR_corr is a piecewise linear function of segmental SNR , controlled by parameters .", "label": "", "metadata": {}, "score": "63.143795"}
{"text": "A more detailed description of the operation of the training engine 20 will be discussed below .The dictionary 24 contains a pronunciation of each word in terms of phonemes .For example , a dictionary entry for \" add \" might be \" /AE DD/. \" After the initial training phase , switching block 18 is switched to transmit the feature vectors to recognition engine 34 .", "label": "", "metadata": {}, "score": "63.182426"}
{"text": "A more detailed description of the operation of the training engine 20 will be discussed below .The dictionary 24 contains a pronunciation of each word in terms of phonemes .For example , a dictionary entry for \" add \" might be \" /AE DD/. \" After the initial training phase , switching block 18 is switched to transmit the feature vectors to recognition engine 34 .", "label": "", "metadata": {}, "score": "63.182426"}
{"text": "A more detailed description of the operation of the training engine 20 will be discussed below .The dictionary 24 contains a pronunciation of each word in terms of phonemes .For example , a dictionary entry for \" add \" might be \" /AE DD/. \" After the initial training phase , switching block 18 is switched to transmit the feature vectors to recognition engine 34 .", "label": "", "metadata": {}, "score": "63.182426"}
{"text": "If the last frame was processed , as indicated in step 912 , the score for the last state of the model is output at block 914 and is then compared to the scores of all other models in the vocabulary .", "label": "", "metadata": {}, "score": "63.20362"}
{"text": "The reason is because we now know exactly what \" word \" is for each training image , which implies that choosing among outlet points at beginning of each level is now not necessary .All we have to do is then to cascade character models , according to their order in the word , to form a word model .", "label": "", "metadata": {}, "score": "63.22091"}
{"text": "The main purpose of back - tracing is to get frame by frame recovery of the optimal path in the superstate level , plus a pixel by pixel recovery of the optimal path in state level within each node of the superstate level optimal path .", "label": "", "metadata": {}, "score": "63.275658"}
{"text": "The article of .claim 14 , wherein the instructions when executed further result in : . providing a modified reference template having a peak energy level substantially equal to the difference between the peak energy level and the average noise energy level of the widened token .", "label": "", "metadata": {}, "score": "63.27735"}
{"text": "The article of .claim 14 , wherein the instructions when executed further result in : . providing a modified reference template having a peak energy level substantially equal to the difference between the peak energy level and the average noise energy level of the widened token .", "label": "", "metadata": {}, "score": "63.27735"}
{"text": "[ 0003 ] .Speech recognition in noisy environments is a well studied , yet difficult task .One such task is characterized by the following parameters : . [ 0004 ] .The recognition is speaker dependent , where the reference templates are created from speech utterances , spoken by the user in a designated \" training session \" ; .", "label": "", "metadata": {}, "score": "63.303764"}
{"text": "A speech recognition system includes a token builder , a noise estimator , a template padder , a gain and noise adapter and a dynamic time warping ( DTW ) unit .The token builder produces a widened test token representing an input test utterance and at least one frame before and after the input test utterance .", "label": "", "metadata": {}, "score": "63.315098"}
{"text": "All the others are the same for both a binary and a gray - level system .More improvement could be expected if more of such gray - level features are added .An alternate embodiment for applying duration penalty to acquired images is to use the original global N - best hypotheses search , without duration consideration in either forward or backward search .", "label": "", "metadata": {}, "score": "63.315342"}
{"text": "An initial node is just the opposite : it is the first node ( superstate ) at the first frame of the observation for a path .Obviously , if we trace back a path starting from a terminal node , it will end up at the initial node .", "label": "", "metadata": {}, "score": "63.406963"}
{"text": "According to the original Viterbi structure , the best path to G from F should be an extension of the best path into F. But this is not necessarily the case when duration penalty is added at the transition of each state .", "label": "", "metadata": {}, "score": "63.423042"}
{"text": "To find b j ( O i ) , the likelihood of generating column i by superstate j , we should first find the best match between the 1D HMM within superstate j , and the pixels of column i. This is exactly where the state level Viterbi search fits in , as shown in lower part of FIG .", "label": "", "metadata": {}, "score": "63.46945"}
{"text": "[ 0011 ] .Two difficulties imposed by the noise in the recognition phase are : . [ 0012 ] .Mismatch in the acoustics between the training and recognition phases ; and .[ 0013 ] .Inaccurate VAD estimates of the word endpoints in the recognition phase .", "label": "", "metadata": {}, "score": "63.53279"}
{"text": "Using state duration information in the determination of transition probabilities breaks the Markov process assumption , but typically yields better recognition results .More complex systems having large amounts of training data can accurately model state transition probabilities as a function of the state duration .", "label": "", "metadata": {}, "score": "63.606205"}
{"text": "The above detailed invention improves the recognition capability of a speech recognition system by utilizing multiple continuous density output probabilities corresponding to the same speech event in different contexts .This improves the mapping of the feature vectors to the hidden Markov models since it improves the model 's performance in predicting speech events that the model was not trained with .", "label": "", "metadata": {}, "score": "63.617096"}
{"text": "The above detailed invention improves the recognition capability of a speech recognition system by utilizing multiple continuous density output probabilities corresponding to the same speech event in different contexts .This improves the mapping of the feature vectors to the hidden Markov models since it improves the model 's performance in predicting speech events that the model was not trained with .", "label": "", "metadata": {}, "score": "63.617096"}
{"text": "The above detailed invention improves the recognition capability of a speech recognition system by utilizing multiple continuous density output probabilities corresponding to the same speech event in different contexts .This improves the mapping of the feature vectors to the hidden Markov models since it improves the model 's performance in predicting speech events that the model was not trained with .", "label": "", "metadata": {}, "score": "63.617107"}
{"text": "The normalized score is defined as the ratio of the best score Best_Score to the average score Av_Score corrected by two corrections , one for signal to noise ( SNR_corr ) and one for length ( length_corr ) , as follows : .", "label": "", "metadata": {}, "score": "63.63062"}
{"text": "find the state of the previous pixel according to the best path ( block 1509 ) .LOOP CONTROL ( block 1511 ) .LOOP CONTROL ( block 1513 ) .Training .We use the segmental k - means training procedure as described in reference 10 ! to train our model .", "label": "", "metadata": {}, "score": "63.727562"}
{"text": "BACKGROUND OF THE INVENTION .The invention relates to automatic speech recognition .More specifically , the invention relates to automatic speech recognition of an utterance in a target language of a translation of a source text in a source language different from the target language .", "label": "", "metadata": {}, "score": "63.74285"}
{"text": "Also , because of the normalization , the standard DTW solution for the best matching path is in fact not optimal .For example , in U.S. Pat .No .5,732,394 , there is a higher computation load since several DTW matches are performed for each pair of test and reference patterns , instead of one .", "label": "", "metadata": {}, "score": "63.765602"}
{"text": "Also , because of the normalization , the standard DTW solution for the best matching path is in fact not optimal .For example , in U.S. Pat .No .5,732,394 , there is a higher computation load since several DTW matches are performed for each pair of test and reference patterns , instead of one .", "label": "", "metadata": {}, "score": "63.765602"}
{"text": "This creates a problem for the rejection mechanism .Suppose that the rejection threshold on the normalization score is set to an optimal compromise between rejection of out - of - vocabulary words and misdetection of in - vocabulary words for quiet conditions .", "label": "", "metadata": {}, "score": "63.777824"}
{"text": "This creates a problem for the rejection mechanism .Suppose that the rejection threshold on the normalization score is set to an optimal compromise between rejection of out - of - vocabulary words and misdetection of in - vocabulary words for quiet conditions .", "label": "", "metadata": {}, "score": "63.777824"}
{"text": "This creates a problem for the rejection mechanism .Suppose that the rejection threshold on the normalization score is set to an optimal compromise between rejection of out - of - vocabulary words and misdetection of in - vocabulary words for quiet conditions .", "label": "", "metadata": {}, "score": "63.777824"}
{"text": "This maximization problem can be solved using a modified version of the well - known Bayes formula which is described mathematically as : .P(X / W ) is the probability that the input speech signal X matches the word string W , and is referred to as the acoustic score . P(W ) is the probability that the word string W will occur , and is referred to as the language score .", "label": "", "metadata": {}, "score": "63.789047"}
{"text": "This maximization problem can be solved using a modified version of the well - known Bayes formula which is described mathematically as : .P(X / W ) is the probability that the input speech signal X matches the word string W , and is referred to as the acoustic score . P(W ) is the probability that the word string W will occur , and is referred to as the language score .", "label": "", "metadata": {}, "score": "63.789047"}
{"text": "This maximization problem can be solved using a modified version of the well - known Bayes formula which is described mathematically as : .P(X / W ) is the probability that the input speech signal X matches the word string W , and is referred to as the acoustic score . P(W ) is the probability that the word string W will occur , and is referred to as the language score .", "label": "", "metadata": {}, "score": "63.789047"}
{"text": "A speech recognition system includes a token builder , a noise estimator , a template padder , a gain and noise adapter and a dynamic time warping ( DTW ) unit .The token builder produces a widened test token representing an input test utterance and at least one frame before and after the input test utterance ....", "label": "", "metadata": {}, "score": "63.80114"}
{"text": "In step 608 , the processor determines if the model has converged by observing the change in the model parameters .Convergence occurs when aligning the sample utterances to the state model produces less than a predetermined amount of change in the state model .", "label": "", "metadata": {}, "score": "63.84507"}
{"text": "FIG .13 is an extension of FIG .12 by adding one more dimension for accommodating all possible character models , and building levels to search for the optimal model combination .An outlet point of model \" a \" at frame 4 is concatenated by another match of model \" a \" at level 2 , from frame 5 and up .", "label": "", "metadata": {}, "score": "63.86774"}
{"text": "Each possible model representation may be conceptualized as an hypothesis .An N - best hypothesis search is performed in combination with a duration constraint .Parameters for the models are generated by training routines .A technique termed the N - best hypotheses search may be advantageously utilized in conjunction with the approach developed above .", "label": "", "metadata": {}, "score": "63.875725"}
{"text": "8A , 8 B and 8 C are graphical illustrations of an a priori average score , SNR correction and length correction curves , useful in understanding the operation of the present invention .DETAILED DESCRIPTION OF THE PRESENT INVENTION .Reference is now made to FIG .", "label": "", "metadata": {}, "score": "63.971725"}
{"text": "8A , 8 B and 8 C are graphical illustrations of an a priori average score , SNR correction and length correction curves , useful in understanding the operation of the present invention .DETAILED DESCRIPTION OF THE PRESENT INVENTION .Reference is now made to FIG .", "label": "", "metadata": {}, "score": "63.971725"}
{"text": "Abstract .A method and system for achieving an improved recognition accuracy in speech recognition systems which utilize continuous density hidden Markov models to represent phonetic units of speech present in spoken speech utterances is provided .An acoustic score which reflects the likelihood that a speech utterance matches a modeled linguistic expression is dependent on the output probability associated with the states of the hidden Markov model .", "label": "", "metadata": {}, "score": "64.04823"}
{"text": "Abstract .A method and system for achieving an improved recognition accuracy in speech recognition systems which utilize continuous density hidden Markov models to represent phonetic units of speech present in spoken speech utterances is provided .An acoustic score which reflects the likelihood that a speech utterance matches a modeled linguistic expression is dependent on the output probability associated with the states of the hidden Markov model .", "label": "", "metadata": {}, "score": "64.04823"}
{"text": "It will be appreciated by persons skilled in the art that the present invention is not limited by what has been particularly shown and described herein above .Rather the scope of the invention is defined by the claims that follow : A speech recognition system includes a token builder , a noise estimator , a template padder , a gain and noise adapter and a dynamic time warping ( DTW ) unit .", "label": "", "metadata": {}, "score": "64.07644"}
{"text": "However , wherever the comparison is to a reference noise frame , there are no duration constraints .This means that the path can move only horizontally to the right an unlimited number of frames or diagonally to the right one frame and up one frame .", "label": "", "metadata": {}, "score": "64.08449"}
{"text": "However , wherever the comparison is to a reference noise frame , there are no duration constraints .This means that the path can move only horizontally to the right an unlimited number of frames or diagonally to the right one frame and up one frame .", "label": "", "metadata": {}, "score": "64.08449"}
{"text": "Specifically , the endpoints are those test - utterance frames on the DTW path to the best template Best_Template , where the path enters the first ( or exits the last ) reference speech frames .The segmental SNR SegSNR is the average log - energy of the test utterance between the DTW - derived endpoints less the average log - energy of the test utterance outside the endpoints .", "label": "", "metadata": {}, "score": "64.114204"}
{"text": "Specifically , the endpoints are those test - utterance frames on the DTW path to the best template Best_Template , where the path enters the first ( or exits the last ) reference speech frames .The segmental SNR SegSNR is the average log - energy of the test utterance between the DTW - derived endpoints less the average log - energy of the test utterance outside the endpoints .", "label": "", "metadata": {}, "score": "64.114204"}
{"text": "This is a Continuation of application Ser .No .08/278,733 filed Jul. 22 , 1994 , now abandoned .TECHNICAL FIELD .This invention relates to optical text recognition and more specifically to robust methods of recognizing characters or words regardless of impediments such as poor document reproduction .", "label": "", "metadata": {}, "score": "64.201294"}
{"text": "1 .The complete set of feature vectors is referred to as the \" training data .\" The use of LCP cepstral analysis to model speed signals is well known in the art of speech recognition systems .In step 46 , senone and HMM data structures are generated .", "label": "", "metadata": {}, "score": "64.21499"}
{"text": "1 .The complete set of feature vectors is referred to as the \" training data .\" The use of LCP cepstral analysis to model speed signals is well known in the art of speech recognition systems .In step 46 , senone and HMM data structures are generated .", "label": "", "metadata": {}, "score": "64.21499"}
{"text": "1 .The complete set of feature vectors is referred to as the \" training data .\" The use of LCP cepstral analysis to model speed signals is well known in the art of speech recognition systems .In step 46 , senone and HMM data structures are generated .", "label": "", "metadata": {}, "score": "64.21499"}
{"text": "[ 0018 ] .However , in the relaxed - endpoint solution , shown in FIG .1B to which reference is now made , the DTW paths are not constrained to start or end at the exact endpoints of the test and reference utterances .", "label": "", "metadata": {}, "score": "64.25167"}
{"text": "Since the best incoming path might be different for each node , this penalty is updated together with other path information every time a backward one arc extension is made .Consider the paths in FIG .8 as an example .", "label": "", "metadata": {}, "score": "64.268234"}
{"text": "If this non - completed path has only one incoming path available , then no path splitting is possible .One arc extension is performed and the extended path is then inserted back into path stack .FIG .8 is a small section of the path map obtained from forward Viterbi search , which is also used for backward tree search .", "label": "", "metadata": {}, "score": "64.29457"}
{"text": "The environmental noise has both stationary and non - stationary components ; and .[0010 ] .The system has limited fast - access memory , so that it is impossible to run DTW matching against all reference templates , in real - time and in a word - spotting manner .", "label": "", "metadata": {}, "score": "64.408035"}
{"text": "The summary of this level building algorithm with nested Viterbi search using PHMM is as follows : .LOOP over all columns , from left to right ( block 1403 ) .LOOP over all levels , from 0 to MAX - LEVEL ( block 1405 ) .", "label": "", "metadata": {}, "score": "64.42613"}
{"text": "Methods that are known in the alt for improving the rejection capability of HMM systems include mostly the usage of a \" general speech \" template ( these are discussed in the previously mentioned article by Raman , in U.S. Pat .", "label": "", "metadata": {}, "score": "64.443924"}
{"text": "the speech hypothesis generator generates one or more speech hypotheses solely from words in the set of candidate words .A speech recognition system as claimed in claim 1 , characterized in that the acoustic match score comprises an estimate of the probability of occurrence of the sequence of coded representations of the utterance given the occurrence of the speech hypothesis .", "label": "", "metadata": {}, "score": "64.52849"}
{"text": "the speech hypothesis generator generates one or more speech hypotheses solely from words in the set of candidate words .A speech recognition system as claimed in claim 1 , characterized in that the acoustic match score comprises an estimate of the probability of occurrence of the sequence of coded representations of the utterance given the occurrence of the speech hypothesis .", "label": "", "metadata": {}, "score": "64.52849"}
{"text": "The initial models for the training process were built from two images per character , with parameter ( 50 , 0.8 ) and ( 20 , 0.2 ) , respectively .The correlation coefficient method to perform the initial segmentation for each of the matches as described previously .", "label": "", "metadata": {}, "score": "64.57977"}
{"text": "3 , in the determination of the new value of \u03bb new , ( step 74 ) .The flow sequence remains as shown in FIG .3 .Referring to FIG .6 , in step 100 , a parametric representation is generated for the data points in the deleted block .", "label": "", "metadata": {}, "score": "64.60562"}
{"text": "3 , in the determination of the new value of \u03bb new , ( step 74 ) .The flow sequence remains as shown in FIG .3 .Referring to FIG .6 , in step 100 , a parametric representation is generated for the data points in the deleted block .", "label": "", "metadata": {}, "score": "64.60562"}
{"text": "3 , in the determination of the new value of \u03bb new , ( step 74 ) .The flow sequence remains as shown in FIG .3 .Referring to FIG .6 , in step 100 , a parametric representation is generated for the data points in the deleted block .", "label": "", "metadata": {}, "score": "64.60562"}
{"text": "For example , the number of misdetections of in - vocabulary words will significantly increase .It may be desired in this case to relax the threshold , thereby to reduce the number of misdetections of in - vocabulary words , even at the expense of less rejection of out - of - vocabulary words .", "label": "", "metadata": {}, "score": "64.61509"}
{"text": "For example , the number of misdetections of in - vocabulary words will significantly increase .It may be desired in this case to relax the threshold , thereby to reduce the number of misdetections of in - vocabulary words , even at the expense of less rejection of out - of - vocabulary words .", "label": "", "metadata": {}, "score": "64.61509"}
{"text": "For example , the number of misdetections of in - vocabulary words will significantly increase .It may be desired in this case to relax the threshold , thereby to reduce the number of misdetections of in - vocabulary words , even at the expense of less rejection of out - of - vocabulary words .", "label": "", "metadata": {}, "score": "64.61509"}
{"text": "The length correction length_corr is a piecewise linear function , shown in FIG .8C to which reference is now briefly made , of test - utterance length , controlled by parameters .The parameters are determined experimentally from the large speech database used for determining all of FIG .", "label": "", "metadata": {}, "score": "64.63561"}
{"text": "Language models use knowledge of the structure and semantics of a language in predicting the likelihood of the occurrence of a word considering the words that have been previously uttered .The language model can be a bigram language model where the language score is based on the probability of one word being followed by a particular second word .", "label": "", "metadata": {}, "score": "64.65367"}
{"text": "Language models use knowledge of the structure and semantics of a language in predicting the likelihood of the occurrence of a word considering the words that have been previously uttered .The language model can be a bigram language model where the language score is based on the probability of one word being followed by a particular second word .", "label": "", "metadata": {}, "score": "64.65367"}
{"text": "Language models use knowledge of the structure and semantics of a language in predicting the likelihood of the occurrence of a word considering the words that have been previously uttered .The language model can be a bigram language model where the language score is based on the probability of one word being followed by a particular second word .", "label": "", "metadata": {}, "score": "64.65367"}
{"text": "The language score takes into consideration the syntax and semantics of the language , such as the grammar of the language , and indicates whether the sequence of words corresponding to the sequence of phonemes form a grammatically - correct linguistic expression .", "label": "", "metadata": {}, "score": "64.67492"}
{"text": "The language score takes into consideration the syntax and semantics of the language , such as the grammar of the language , and indicates whether the sequence of words corresponding to the sequence of phonemes form a grammatically - correct linguistic expression .", "label": "", "metadata": {}, "score": "64.67492"}
{"text": "The language score takes into consideration the syntax and semantics of the language , such as the grammar of the language , and indicates whether the sequence of words corresponding to the sequence of phonemes form a grammatically - correct linguistic expression .", "label": "", "metadata": {}, "score": "64.67492"}
{"text": "Abstract .A voice recognition system ( 204 , 206 , 207 , 208 ) assigns a penalty to a score in a voice recognition system .The system generates a lower threshold for the number of frames assigned to at least one state of at least one model and an upper threshold for the number of frames assigned to at least one state of at least one model .", "label": "", "metadata": {}, "score": "64.69263"}
{"text": "[ 0040]FIGS .8A , 8B and 8 C are graphical illustrations of an a priori average score , SNR correction and length correction curves , useful in understanding the operation of the present invention .DETAILED DESCRIPTION OF THE PRESENT INVENTION .", "label": "", "metadata": {}, "score": "64.809364"}
{"text": "The parameter \" Delta Score \" is left as an adjustable parameter , to be tailored to a specific application .For example , if delta_score is set to zero , there is no SNR correction at all .In this case , the rejection of out - of - vocabulary words at noisy conditions will be as effective as in quiet conditions , at the expense of a dramatic increase in rejections of in - vocabulary words .", "label": "", "metadata": {}, "score": "64.9119"}
{"text": "7 , to which reference is now made , illustrates the DTW grid and a sample path .The first and last reference frames ( on the Y axis ) are \" noise \" frames , estimated from the low - energy frames of the test utterance and the widened token is on the X axis .", "label": "", "metadata": {}, "score": "64.958466"}
{"text": "7 , to which reference is now made , illustrates the DTW grid and a sample path .The first and last reference frames ( on the Y axis ) are \" noise \" frames , estimated from the low - energy frames of the test utterance and the widened token is on the X axis .", "label": "", "metadata": {}, "score": "64.958466"}
{"text": "7 , to which reference is now made , illustrates the DTW grid and a sample path .The first and last reference frames ( on the Y axis ) are \" noise\"frames , estimated from the low - energy frames of the test utterance and the widened token is on the X axis .", "label": "", "metadata": {}, "score": "64.958466"}
{"text": "However , it should be noted that this invention is not restricted to a speech recognition system .Any application which requires the matching of a speech utterance to a linguistic expression can utilize the claimed invention .The speech utterance can be any form of acoustic data , such as but not limited to , sounds , speech waveforms , and the like .", "label": "", "metadata": {}, "score": "65.093506"}
{"text": "However , it should be noted that this invention is not restricted to a speech recognition system .Any application which requires the matching of a speech utterance to a linguistic expression can utilize the claimed invention .The speech utterance can be any form of acoustic data , such as but not limited to , sounds , speech waveforms , and the like .", "label": "", "metadata": {}, "score": "65.093506"}
{"text": "However , it should be noted that this invention is not restricted to a speech recognition system .Any application which requires the matching of a speech utterance to a linguistic expression can utilize the claimed invention .The speech utterance can be any form of acoustic data , such as but not limited to , sounds , speech waveforms , and the like .", "label": "", "metadata": {}, "score": "65.093506"}
{"text": "Often there is an insufficient amount of training data to accurately estimate the output probability function .To account for this problem , context - independent and context - dependent models are constructed for a predetermined set of phonemes .The output probabilities of the context - independent model are then interpolated with the output probabilities of the context - dependent model .", "label": "", "metadata": {}, "score": "65.094444"}
{"text": "Often there is an insufficient amount of training data to accurately estimate the output probability function .To account for this problem , context - independent and context - dependent models are constructed for a predetermined set of phonemes .The output probabilities of the context - independent model are then interpolated with the output probabilities of the context - dependent model .", "label": "", "metadata": {}, "score": "65.094444"}
{"text": "Often there is an insufficient amount of training data to accurately estimate the output probability function .To account for this problem , context - independent and context - dependent models are constructed for a predetermined set of phonemes .The output probabilities of the context - independent model are then interpolated with the output probabilities of the context - dependent model .", "label": "", "metadata": {}, "score": "65.094444"}
{"text": "Other information included are : ( super)state transition counts , and character and ( super)state duration records .We keep repeating this procedure until all training images ( tokens ) has been segmented .A reestimation process is then initiated to get a new set of models .", "label": "", "metadata": {}, "score": "65.13826"}
{"text": "The combined recognition score can be determined in accord with the modified Bayes formula as denoted above in equation ( 10 ) .The combined recognition score consists of an acoustic score and a language score .The acoustic score is determined in step 130 , the language score is determined in step 132 , and the combined score is computed in step 134 .", "label": "", "metadata": {}, "score": "65.14495"}
{"text": "The combined recognition score can be determined in accord with the modified Bayes formula as denoted above in equation ( 10 ) .The combined recognition score consists of an acoustic score and a language score .The acoustic score is determined in step 130 , the language score is determined in step 132 , and the combined score is computed in step 134 .", "label": "", "metadata": {}, "score": "65.14495"}
{"text": "The combined recognition score can be determined in accord with the modified Bayes formula as denoted above in equation ( 10 ) .The combined recognition score consists of an acoustic score and a language score .The acoustic score is determined in step 130 , the language score is determined in step 132 , and the combined score is computed in step 134 .", "label": "", "metadata": {}, "score": "65.144966"}
{"text": "Preferably , the initial value can be an estimated guess as 0.8 .In step 68 , the process iterates K times .At each iteration , one block of data is selected as the deleted block , and the selected deleted block is one that was not chosen previously , step 70 .", "label": "", "metadata": {}, "score": "65.176605"}
{"text": "Preferably , the initial value can be an estimated guess as 0.8 .In step 68 , the process iterates K times .At each iteration , one block of data is selected as the deleted block , and the selected deleted block is one that was not chosen previously , step 70 .", "label": "", "metadata": {}, "score": "65.176605"}
{"text": "Preferably , the initial value can be an estimated guess as 0.8 .In step 68 , the process iterates K times .At each iteration , one block of data is selected as the deleted block , and the selected deleted block is one that was not chosen previously , step 70 .", "label": "", "metadata": {}, "score": "65.176605"}
{"text": "16 , pp .842 848 , Aug. 1994 .Minimum Error Thresholding by J. Kittler et al . , Pattern Recognition , vol .19 , pp .41 47 , 1986 .Pseudo two dimensional hidden markov models for document recognition by O. E. Agazzi et al . , AT&T Technical Journal , vol .", "label": "", "metadata": {}, "score": "65.210304"}
{"text": "National Academy of the Sciences , 1966 . )In one approach to speech recognition , speech hypotheses are scored using two probability models .One model is a language model which estimates the probability that the speech hypothesis would be uttered , but which uses no knowledge or information about the actual utterance to be recognized .", "label": "", "metadata": {}, "score": "65.22995"}
{"text": "National Academy of the Sciences , 1966 . )In one approach to speech recognition , speech hypotheses are scored using two probability models .One model is a language model which estimates the probability that the speech hypothesis would be uttered , but which uses no knowledge or information about the actual utterance to be recognized .", "label": "", "metadata": {}, "score": "65.22995"}
{"text": "A system as in claim 17 , further comprising : . a training device to receive an amount of training data of speech utterances and to estimate the output probability for each state of each acoustic model with the amount of training data ; and .", "label": "", "metadata": {}, "score": "65.23554"}
{"text": "A system as in claim 17 , further comprising : . a training device to receive an amount of training data of speech utterances and to estimate the output probability for each state of each acoustic model with the amount of training data ; and .", "label": "", "metadata": {}, "score": "65.23554"}
{"text": "A system as in claim 17 , further comprising : . a training device to receive an amount of training data of speech utterances and to estimate the output probability for each state of each acoustic model with the amount of training data ; and .", "label": "", "metadata": {}, "score": "65.23554"}
{"text": "The final step in the training loop is the test for convergence as a criterion to stop the iteration .We compare the fitness of the alignment between the new model and the previous one by comparing their optimal path likelihood .", "label": "", "metadata": {}, "score": "65.246765"}
{"text": "In addition , other lexical knowledge such as syntax and grammatical rules can be employed to create the language model .Methods for creating and using language models are well - known in the art and are described in more detail in the Huang et al .", "label": "", "metadata": {}, "score": "65.26638"}
{"text": "In addition , other lexical knowledge such as syntax and grammatical rules can be employed to create the language model .Methods for creating and using language models are well - known in the art and are described in more detail in the Huang et al .", "label": "", "metadata": {}, "score": "65.26638"}
{"text": "In addition , other lexical knowledge such as syntax and grammatical rules can be employed to create the language model .Methods for creating and using language models are well - known in the art and are described in more detail in the Huang et al .", "label": "", "metadata": {}, "score": "65.26639"}
{"text": "The frame rate is generally between 10 and 30 ms , and will be exemplified herein as 20 ms in duration .A large number of different features are known for use in voice recognition systems .Generally speaking , a training algorithm uses the features extracted from the sampled speech of one or more utterances of a word or phrase to generate parameters for a model of that word or phrase .", "label": "", "metadata": {}, "score": "65.29431"}
{"text": "3 ) .[ 0053 ] .Referring back to FIG .3 , the gain and noise adapter 70 uses the noise feature and the test and reference peak energies R 0 t and R 0r , respectively , to adapt the padded , noiseless templates .", "label": "", "metadata": {}, "score": "65.30183"}
{"text": "If it was determined that the state duration was greater than the minimum threshold , the adjustment to the score is adding 0 .The resulting adjusted score from step 1013 or 1016 is stored as indicated in step 1017 .The self loop score will be calculated as indicated in block 1018 .", "label": "", "metadata": {}, "score": "65.327385"}
{"text": "The state transition probabilities for both a transition and a self loop , for each state , will be generated .The upper and lower duration limits stored for the states of each model are used when performing the alignment algorithm .", "label": "", "metadata": {}, "score": "65.43593"}
{"text": "A plot of these penalties are shown in FIG .8 .The penalty factors are figures selected to make the penalty proportional to is other values in the equation for generating a path score .In summary , the probability of each state after the first frame is determined by comparing the probability of a transition into that state from the previous state ( a state change ) to the probability of a self - transition ( a self - loop ) .", "label": "", "metadata": {}, "score": "65.44352"}
{"text": "No penalty is assigned for out - of - state transitions after the lower threshold has been met , and no penalty is assigned for same - state transitions until the maximum threshold has been exceeded .The performance of speech recognition systems is improved with only a modest increase in the memory size and millions of instructions per second ( MIPS ) requirements .", "label": "", "metadata": {}, "score": "65.639145"}
{"text": "No penalty is assigned for out - of - state transitions after the lower threshold has been met , and no penalty is assigned for same - state transitions until the maximum threshold has been exceeded .The performance of speech recognition systems is improved with only a modest increase in the memory size and millions of instructions per second ( MIPS ) requirements .", "label": "", "metadata": {}, "score": "65.639145"}
{"text": "In the preferred embodiment , the statistical representation is a mean of frames from utterance U 1 and utterance U 2 .Thus , state 1 is initially set to the mean of frames F 11 and F 12 of utterance U 1 and frames F 21 and F 22 of utterance U 2 .", "label": "", "metadata": {}, "score": "65.64114"}
{"text": "3 is a block diagram illustration of a speech recognizer , constructed and operative in accordance with a preferred embodiment of the present invention ; .FIG .4 is a graphical illustration of the energy in a test utterance , useful in understanding the operation of the present invention ; .", "label": "", "metadata": {}, "score": "65.66388"}
{"text": "These two problems lead to recognition errors .[ 0015 ] .There are many techniques known in the art to deal with the acoustic mismatch problem .A good review can be found in Jean - Claude Junqua and Jean - Paul Haton , Robustness in Automatic Speech Recognition , Kluwer Academic Publishers , 1996 .", "label": "", "metadata": {}, "score": "65.667404"}
{"text": "77 , pp .257 286 , Feb. 1989 .Automatic Recognition of Keywords in Unconstrained Speech UsingHidden Markov Models , J. Wilpon et al . , IEEE Trans .Acoust .Speech Signal Processing , vol .38 , pp .", "label": "", "metadata": {}, "score": "65.717316"}
{"text": "A method as in claim 1 where each acoustic model is a continuous density hidden Markov model .A method as in claim 1 wherein the step of determining the output probability further comprises the step of weighing the less - detailed model and more - detailed model output probabilities with separate weighting factors when combined .", "label": "", "metadata": {}, "score": "65.73985"}
{"text": "The analog electric signal corresponding to the speech utterance is transmitted to analog - to - digital ( A / D ) converter 14 , which converts the analog signal to a sequence of digital samples .The digital samples are then transmitted to a feature extractor 16 which extracts a parametric representation of the digitized input speech signal .", "label": "", "metadata": {}, "score": "65.75864"}
{"text": "The analog electric signal corresponding to the speech utterance is transmitted to analog - to - digital ( A / D ) converter 14 , which converts the analog signal to a sequence of digital samples .The digital samples are then transmitted to a feature extractor 16 which extracts a parametric representation of the digitized input speech signal .", "label": "", "metadata": {}, "score": "65.75864"}
{"text": "The analog electric signal corresponding to the speech utterance is transmitted to analog - to - digital ( A / D ) converter 14 , which converts the analog signal to a sequence of digital samples .The digital samples are then transmitted to a feature extractor 16 which extracts a parametric representation of the digitized input speech signal .", "label": "", "metadata": {}, "score": "65.75864"}
{"text": "The k - mean training procedure can then operate on a larger training set to get the final converged models .The training set is generated in a similar manner as the training set for the initial models .The noise and blurring range are now ( 50 - 59 , 0.8 - 0.7 ) , with about 10 samples per character .", "label": "", "metadata": {}, "score": "65.76788"}
{"text": "Reference is now made to FIG .3 which illustrates the system of the present invention .The system comprises a feature extractor 50 , a feature buffer 52 , a voice activity detector ( VAD ) 54 , a template database 56 , two feature transformers 58 A and 58 B , a comparison unit 60 and a decision unit 62 .", "label": "", "metadata": {}, "score": "65.772705"}
{"text": "A method in a computer readable storage medium for recognizing an input speech utterance , said method comprising the steps of : . providing a context - independent continuous density hidden Markov model for the plurality of context - dependent continuous density hidden Markov models representing the same phonetic unit of speech ; . providing a plurality of sequences of the context - dependent models , each sequence representing a linguistic expression ; . utilizing the acoustic probability to recognize the linguistic expression which closely matches the input speech utterance .", "label": "", "metadata": {}, "score": "65.77309"}
{"text": "A method in a computer readable storage medium for recognizing an input speech utterance , said method comprising the steps of : . providing a context - independent continuous density hidden Markov model for the plurality of context - dependent continuous density hidden Markov models representing the same phonetic unit of speech ; . providing a plurality of sequences of the context - dependent models , each sequence representing a linguistic expression ; . utilizing the acoustic probability to recognize the linguistic expression which closely matches the input speech utterance .", "label": "", "metadata": {}, "score": "65.77309"}
{"text": "A method in a computer readable storage medium for recognizing an input speech utterance , said method comprising the steps of : . providing a context - independent continuous density hidden Markov model for the plurality of context - dependent continuous density hidden Markov models representing the same phonetic unit of speech ; . providing a plurality of sequences of the context - dependent models , each sequence representing a linguistic expression ; . utilizing the acoustic probability to recognize the linguistic expression which closely matches the input speech utterance .", "label": "", "metadata": {}, "score": "65.77309"}
{"text": "( See , L. R. Bahl et al , March 1983 , cited above . )The parameters of the model are : . a bucketing scheme c which assigns word pairs t 1 t 2 small number of classes ; .", "label": "", "metadata": {}, "score": "65.80194"}
{"text": "With such a large number of hypotheses , it is not feasible to generate all possible hypotheses .Therefore , preferably , the hypothesis generator does not generate all possible hypotheses for the utterance to be recognized .No .4,748,670 by Lalit R. Bahl et al entitled \" Apparatus And Method For Determining A Likely Word Sequence From Labels Generated By An Acoustic Processor . \"", "label": "", "metadata": {}, "score": "65.84775"}
{"text": "The buffer 52 stores the features of each frame in frame order , maintaining a history of the frames for a predetermined length of time .The voice activity detector 54 can be any suitable detector , such as the one in the G729B dated November 1996 silence compression scheme , which can determine the frames at which speech begins and ends .", "label": "", "metadata": {}, "score": "65.85522"}
{"text": "However , it will be understood by those skilled in the art that various changes may be made therein without departing from the spirit and scope of the invention .\" Fundamentals of Speech Recognition \" , L. Rabiner and B. Juang , PTR , Prentice Hall , 1993 .", "label": "", "metadata": {}, "score": "65.90547"}
{"text": "The buffer 52 stores the features of each frame in frame order , maintaining a history of the frames for a predetermined length of time .The voice activity detector 54 can be any suitable detector , such as the one in the G729B silence compression scheme dated November 1996 , which can determine the frames at which speech begins and ends .", "label": "", "metadata": {}, "score": "65.92392"}
{"text": "( b ) comparing the first portion to a plurality of stochastic models , wherein the stochastic models are two - dimensional hidden Markov models , to identify a closely - matching two - dimensional hidden Markov model from the plurality of two - dimensional hidden Markov models that most closely approximates the first portion .", "label": "", "metadata": {}, "score": "65.97011"}
{"text": "The set of candidate words consists solely of words in the target language which are partial or full translations of words in the source text .One or more speech hypotheses are generated solely from words in the set of candidate words .", "label": "", "metadata": {}, "score": "66.05571"}
{"text": "The set of candidate words consists solely of words in the target language which are partial or full translations of words in the source text .One or more speech hypotheses are generated solely from words in the set of candidate words .", "label": "", "metadata": {}, "score": "66.05571"}
{"text": "Other rules in such a system specify the dropping of accents from letters , or the modification of verbal endings .The comparator 34 may also identify words in the source text that begin with an uppercase letter , but do not appear in the source language vocabulary , and place them into the target language vocabulary .", "label": "", "metadata": {}, "score": "66.13649"}
{"text": "Other rules in such a system specify the dropping of accents from letters , or the modification of verbal endings .The comparator 34 may also identify words in the source text that begin with an uppercase letter , but do not appear in the source language vocabulary , and place them into the target language vocabulary .", "label": "", "metadata": {}, "score": "66.13649"}
{"text": "No . 5,778,342 .This solution requires the estimation of the noise from speech - free waveform segments , which , in turn requires knowledge of the speech end points , which are not known to a sufficient precision .For example , if the interfering noise is a short burst that is partially overlapping with the speech , the burst may have been erroneously identified by the VAD as part of the speech .", "label": "", "metadata": {}, "score": "66.176605"}
{"text": "No . 5,778,342 .This solution requires the estimation of the noise from speech - free waveform segments , which , in turn requires knowledge of the speech end points , which are not known to a sufficient precision .For example , if the interfering noise is a short burst that is partially overlapping with the speech , the burst may have been erroneously identified by the VAD as part of the speech .", "label": "", "metadata": {}, "score": "66.176605"}
{"text": "Otherwise , the whole segmentation - estimation loop will be repeated .Only at this following loop , the newly generated models will be used for the segmentation procedure .Initial Estimation of PHMMs .As we have already mentioned , the number of superstates and states for each character model is determined by their topology .", "label": "", "metadata": {}, "score": "66.19329"}
{"text": "A method in a computer system for matching an input speech utterance to a linguistic expression , the method comprising the steps of : . for each of select sequences of more - detailed acoustic models , determining how close the input speech utterance matches the sequence , the matching further comprising the step of : . for each state of the select sequence of more - detailed acoustic models , determining an accumulative output probability as a combination of the output probability of the state and a same state of the less - detailed acoustic model representing the same phonetic unit ; and .", "label": "", "metadata": {}, "score": "66.281784"}
{"text": "HMM recognition systems allocate frames of the utterance to states of the HMM .The frame - to - state allocation that produces the largest probability , or score , is selected as the best match .One problem with HMMs is that they assume an exponential distribution for the duration of a state .", "label": "", "metadata": {}, "score": "66.28203"}
{"text": "Returning to FIG .1 , the speech recognition system further comprises an acoustic model generator 18 for generating an acoustic model for each speech hypothesis generated by the speech hypothesis generator 16 .The acoustic model generator 18 forms an acoustic model of a speech hypothesis by substituting , for each word in the speech hypothesis , an acoustic model of the word from a set of stored acoustic models .", "label": "", "metadata": {}, "score": "66.2872"}
{"text": "The word image to be recognized here in accordance with the invention will typically be part of a larger body of textual and graphical material on a page .For the purposes of describing the invention , it will be assumed that the page has been optically scanned and that a word image to be recognized has been captured as a sequence of pixels generated by a conventional gray - level raster scan .", "label": "", "metadata": {}, "score": "66.29508"}
{"text": "Once this segmentation information is obtained , we will be able to know exactly which model , superstate , and state does each pixel belong to .We then group pixel observation vectors according to this information .These observation vectors are all included in the \" information for PHMM K \" database , as seen in FIG .", "label": "", "metadata": {}, "score": "66.340706"}
{"text": "Similar to the rule for selecting incoming path for each node in the Viterbi algorithm , the rule for selecting an outlet to continue the path is : .For a path to be started at superstate O of model i at level j+1 and frame k+1 , the best outlet point should have the minimum accumulated likelihood score among the last superstate of all model , at level j and frame k. That is , we first find out which character model should the optimal path coming from .", "label": "", "metadata": {}, "score": "66.35876"}
{"text": "As shown in FIG .13 , when a Viterbi search path went through one level , it means that a match between a section of frames and a HMM has been found .Outlet points , as shown in FIG .", "label": "", "metadata": {}, "score": "66.36536"}
{"text": "3 ) .Referring back to FIG .3 , the gain and noise adapter 70 uses the noise feature and the test and reference peak energies R 0 t and R 0r , respectively , to adapt the padded , noiseless templates .", "label": "", "metadata": {}, "score": "66.40044"}
{"text": "3 ) .Referring back to FIG .3 , the gain and noise adapter 70 uses the noise feature and the test and reference peak energies R 0 t and R 0r , respectively , to adapt the padded , noiseless templates .", "label": "", "metadata": {}, "score": "66.40044"}
{"text": "If , now at F , one tries to make an one - arc - extension backward , a comparison between F - C extension ( path 2 ) and F - E extension ( path 1 ) is necessary .Upon consideration of FIG .", "label": "", "metadata": {}, "score": "66.41435"}
{"text": "\" The recognition problem is one of finding the state sequence having the highest probability of matching the feature vectors representing the input speech signal .Primarily , this search process involves enumerating every possible state sequence that has been modeled and determining the probability that the state sequence matches the input speech signal .", "label": "", "metadata": {}, "score": "66.4716"}
{"text": "\" The recognition problem is one of finding the state sequence having the highest probability of matching the feature vectors representing the input speech signal .Primarily , this search process involves enumerating every possible state sequence that has been modeled and determining the probability that the state sequence matches the input speech signal .", "label": "", "metadata": {}, "score": "66.4716"}
{"text": "\" The recognition problem is one of finding the state sequence having the highest probability of matching the feature vectors representing the input speech signal .Primarily , this search process involves enumerating every possible state sequence that has been modeled and determining the probability that the state sequence matches the input speech signal .", "label": "", "metadata": {}, "score": "66.4716"}
{"text": "Those skilled in the art will recognize that lattice 400 ( FIG .4 ) shows all paths back from states 1 through 5 for 8 frames .An additional limitation is that frames must be allocated to either the same state as a previous frame or the state immediately after the state of the previous frame ( no states can be skipped ) .", "label": "", "metadata": {}, "score": "66.47499"}
{"text": "In other embodiments , various approximations are made to simplify the computation of this sum .One such approximation is # # EQU2 # # Here T.cndot .sup.k denotes the set of target sequences that begin with T and contain k additional words , n is a parameter specifying the maximum allowed number of additional words , and . alpha . is a special generic target word .", "label": "", "metadata": {}, "score": "66.52717"}
{"text": "( S n , T n ) using a procedure that is explained in detail in the above mentioned patent .Briefly , this procedure works as follows .The probability of the aligned sentence pairs is a computable function of the parameter values .", "label": "", "metadata": {}, "score": "66.619385"}
{"text": "However , the relaxed - endpoint solutions have several disadvantages .Other disadvantages of the relaxed - endpoint methods are specific to the method .For example , in the article by Shallom , it is necessary to normalize , for each point on the DTW grid , the DTW accumulated score by the path length , since the relaxation of the beginning point allows now for multiple paths of different lengths .", "label": "", "metadata": {}, "score": "66.64418"}
{"text": "For some applications an array of backpointers is also maintained so that the actual state sequence can be reconstructed .For simplicity , the term \" state \" as used herein throughout may refer to a state and/or a superstate .Typically , such state will be state N. However , cases are possible in which the maximum probability after the last observation is associated with a state other than state N. In some applications of HMMs , such as speech processing and character recognition , it is preferred in such cases to use the probability calculated for state N after the last observation as the probability for the HMM , not the maximum probability .", "label": "", "metadata": {}, "score": "66.90905"}
{"text": "In one study , it was found that the efficiency of a human translator who dictates a translation in one language corresponding to source text in another language , is greater than the efficiency of a human translator who writes or types a translation .", "label": "", "metadata": {}, "score": "66.92385"}
{"text": "This is performed by forming all possible linguistic expressions that have been modeled and calculating the probability that the expression matches the sequence of feature vectors .Since a linguistic expression is composed of a sequence of phonemes , the determination can involve calculating the likelihood that the phonemes forming the expression match the feature vectors and that the expression is likely to occur ( i.e. , grammatically correct ) .", "label": "", "metadata": {}, "score": "66.96939"}
{"text": "This is performed by forming all possible linguistic expressions that have been modeled and calculating the probability that the expression matches the sequence of feature vectors .Since a linguistic expression is composed of a sequence of phonemes , the determination can involve calculating the likelihood that the phonemes forming the expression match the feature vectors and that the expression is likely to occur ( i.e. , grammatically correct ) .", "label": "", "metadata": {}, "score": "66.96939"}
{"text": "This is performed by forming all possible linguistic expressions that have been modeled and calculating the probability that the expression matches the sequence of feature vectors .Since a linguistic expression is composed of a sequence of phonemes , the determination can involve calculating the likelihood that the phonemes forming the expression match the feature vectors and that the expression is likely to occur ( i.e. , grammatically correct ) .", "label": "", "metadata": {}, "score": "66.96939"}
{"text": "An acoustic score for the word sequence under the constraints of each senone alignment is computed .The acoustic score for the word sequence is the best acoustic score over all possible senone alignments .Mathematically , this can be expressed as : . where .", "label": "", "metadata": {}, "score": "67.05336"}
{"text": "An acoustic score for the word sequence under the constraints of each senone alignment is computed .The acoustic score for the word sequence is the best acoustic score over all possible senone alignments .Mathematically , this can be expressed as : . where .", "label": "", "metadata": {}, "score": "67.05336"}
{"text": "An acoustic score for the word sequence under the constraints of each senone alignment is computed .The acoustic score for the word sequence is the best acoustic score over all possible senone alignments .Mathematically , this can be expressed as : . where .", "label": "", "metadata": {}, "score": "67.05336"}
{"text": "Context - independent acoustic Markov models may be produced , for example , by the method described in U.S. Pat .No .4,759,068 entitled \" Constructing Markov Models of Words From Multiple Utterances , \" or by any other known method of generating acoustic word models .", "label": "", "metadata": {}, "score": "67.11635"}
{"text": "In one embodiment , this combined score is computed as a sum over all complete sentence T ' in T.cndot .: # # EQU1 # # .The probability P(T ' ) is obtained from the language match score generator , and the probability P(S.vertline .", "label": "", "metadata": {}, "score": "67.17797"}
{"text": "4 graphs the first autocorrelation coefficient per frame and indicates the endpoints indicated by the voice activity detector 54 .A standard token is defined as the set of frames between the endpoints .In accordance with a preferred embodiment of the present invention , wide token builder 66 additionally takes X frames from either side of a standard token , where X is typically 8 .", "label": "", "metadata": {}, "score": "67.20177"}
{"text": "The system comprises a feature extractor 50 , a feature buffer 52 , a voice activity detector ( VAD ) 54 , a template database 56 , two feature transformers 58 A and 58 B , a comparison unit 60 and a decision unit 62 .", "label": "", "metadata": {}, "score": "67.23207"}
{"text": "The system comprises a feature extractor 50 , a feature buffer 52 , a voice activity detector ( VAD ) 54 , a template database 56 , two feature transformers 58 A and 58 B , a comparison unit 60 and a decision unit 62 .", "label": "", "metadata": {}, "score": "67.23207"}
{"text": "A Maximum Likelihood Approach to Continuous Speech Recognition . \"IEEE Transactions on Pattern Analysis and Machine Intelligence , Volume PAMI-5 , No . 2 , pages 179 - 190 , March 1983 . )The models may be context - independent or context - dependent .", "label": "", "metadata": {}, "score": "67.23256"}
{"text": "The fundamental structure of Viterbi decoding algorithm can be found in many literatures , such as references 3 ! and 4 ! FIG .12 shows three paths among others obtained after a completed Viterbi search with PHMM .This Viterbi search is performed between a complete word image ( with N frames / columns ) and a PHMM with four superstates .", "label": "", "metadata": {}, "score": "67.28186"}
{"text": "The Government has certain rights in this invention .Claims .We claim : .A speech recognition system comprising : . means for displaying a source text comprising one or more words in a source language ; . an acoustic processor : for generating a sequence of coded representations of an utterance to be recognized , said utterance comprising one or more words in a target language different from the source language ; . means for generating a set of one or more speech hypotheses , each speech hypothesis comprising one or more words from the target language ; . means for generating an acoustic model of each speech hypothesis ; . means for generating an acoustic match score for each speech hypothesis , each acoustic match score comprising an estimate of the closeness of a match between the acoustic model of the speech hypothesis and the sequence of coded representations of the utterance ; . means for generating a translation match score for each speech hypothesis , each translation match score comprising an estimate of the probability of occurrence of the speech hypothesis given the occurrence , of the source text ; . means for generating a hypothesis score for each hypothesis , each hypothesis score comprising a combination of the acoustic match score and the translation match score for the hypothesis ; . means for storing a subset of one or more speech hypotheses , from the set of speech hypotheses , having the best hypothesis scores ; and . means for outputting at least one word of one or more of the speech hypotheses in the subset of speech hypotheses having the best hypothesis scores .", "label": "", "metadata": {}, "score": "67.300064"}
{"text": "The remaining discussion will be made with respect to a system using five states for any utterance , regardless of its length .As previously mentioned the purpose of training is to generate an HMM for the utterance .This is illustrated in FIG .", "label": "", "metadata": {}, "score": "67.312775"}
{"text": "In other words , in the superstate level Viterbi search , every time we need b i ( O j ) , we will initiate a nested state level Viterbi search to find this likelihood score .That is the reason why we called it a nested Viterbi search .", "label": "", "metadata": {}, "score": "67.32315"}
{"text": "In operation , the feature extractor 50 extracts features , such as autocorrelation coefficients or filterbank energies , of each frame of the input signal and provides these to the voice activity detector 54 and the buffer 52 .The buffer 52 stores the features of each frame in frame order , maintaining a history of the frames for a predetermined length of time .", "label": "", "metadata": {}, "score": "67.36508"}
{"text": "The score for the state at frame t is stored as indicated in step 908 .At step 920 , the transition type is checked to see if a self loop was the best path to state i. If the self loop was the best path , then a state duration counter is incremented at step 924 .", "label": "", "metadata": {}, "score": "67.37904"}
{"text": "When path enters C , the penalty to be paid for state 1 , according to the best incoming path at C is : f 1 ( 3 ) ( for C , B , A ) ; .The final duration correction for going into C is .", "label": "", "metadata": {}, "score": "67.38462"}
{"text": "4,759,068 entitled \" Constructing Markov Models of Words From Multiple Utterances , \" or by any other known method of generating acoustic word models .For context - dependent acoustic Markov word models , the context can be , for example , manually or automatically selected .", "label": "", "metadata": {}, "score": "67.40292"}
{"text": "FIG .8 depicts the operation of the speech recognition method .Referring to FIG .8 , the method commences by receiving an input speech utterance , ( step 122 ) , which is converted to feature vectors ( step 124 ) , which was previously detailed above with reference to FIG .", "label": "", "metadata": {}, "score": "67.40947"}
{"text": "FIG .8 depicts the operation of the speech recognition method .Referring to FIG .8 , the method commences by receiving an input speech utterance , ( step 122 ) , which is converted to feature vectors ( step 124 ) , which was previously detailed above with reference to FIG .", "label": "", "metadata": {}, "score": "67.40947"}
{"text": "FIG .8 depicts the operation of the speech recognition method .Referring to FIG .8 , the method commences by receiving an input speech utterance , ( step 122 ) , which is converted to feature vectors ( step 124 ) , which was previously detailed above with reference to FIG .", "label": "", "metadata": {}, "score": "67.4095"}
{"text": "The present invention includes the method performed by the system . modifying a reference template for use in speech recognition by matching a gain and a noise of the reference template according to a peak energy level and an average noise energy level of a widened token .", "label": "", "metadata": {}, "score": "67.43271"}
{"text": "The present invention includes the method performed by the system . modifying a reference template for use in speech recognition by matching a gain and a noise of the reference template according to a peak energy level and an average noise energy level of a widened token .", "label": "", "metadata": {}, "score": "67.43271"}
{"text": "Predetermined patterns consisting of a set of phonemes and their associated left and right phonemic contexts are selected to be modeled by the context - dependent HMM .These selected patterns represent the most frequently occurring phonemes and the most frequently occurring contexts of these phonemes .", "label": "", "metadata": {}, "score": "67.456924"}
{"text": "Predetermined patterns consisting of a set of phonemes and their associated left and right phonemic contexts are selected to be modeled by the context - dependent HMM .These selected patterns represent the most frequently occurring phonemes and the most frequently occurring contexts of these phonemes .", "label": "", "metadata": {}, "score": "67.456924"}
{"text": "Predetermined patterns consisting of a set of phonemes and their associated left and right phonemic contexts are selected to be modeled by the context - dependent HMM .These selected patterns represent the most frequently occurring phonemes and the most frequently occurring contexts of these phonemes .", "label": "", "metadata": {}, "score": "67.456924"}
{"text": "3 is a block diagram illustration of a speech recognizer , constructed and operative in accordance with some embodiments of the present invention ; .FIG .4 is a graphical illustration of the energy in a test utterance , useful in understanding the operation of the present invention ; .", "label": "", "metadata": {}, "score": "67.48344"}
{"text": "A Maximum Likelihood Approach to Continuous Speech Recognition .IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .PAMI 5 , No . 2 , pp .179 190 , Mar. 1983 .Bahl , L. R. , et al .", "label": "", "metadata": {}, "score": "67.52447"}
{"text": "If you are looking to hire a patent attorney , you 've come to the right place .Protect your idea and hire a patent lawyer .A speech recognition system displays a source text of one or more words in a source language .", "label": "", "metadata": {}, "score": "67.567764"}
{"text": "No . 4,933,976 .If desired , the gain adaptation can be performed separately , or not at all .The noise adapted DTW unit 60 compares a widened test utterance , which includes both speech and noise from both sides , with a noise adapted template .", "label": "", "metadata": {}, "score": "67.58761"}
{"text": "Mathematically this is stated as : .[ 0050 ] .The noise energy R 0n is then determined from the resultant noise feature .[ 0051 ] .The peak energy estimation is determined in a similar manner but considering the frames with the highest energy .", "label": "", "metadata": {}, "score": "67.66579"}
{"text": "One method of automatically selecting context is described in U.S. patent application Ser .No .468,546 , filed Jan. 23 , 1990 , entitled \" Apparatus and Method For Grouping Utterances of a Phoneme Into Context - Dependent Categories Based on Sound - Similarity For Automatic Speech Recognition . \"", "label": "", "metadata": {}, "score": "67.67177"}
{"text": "Training may be accomplished using representative samples ( tokens ) of the word images in the form such images are likely to take in the text to be recognized ( step 352 ) .An observation sequence ( pixel map ) is created for each token , and the Viterbi algorithm is then used for segmentation of the observation sequence with respect to the model ( step 353 ) .", "label": "", "metadata": {}, "score": "67.72577"}
{"text": "The utterance comprises a series of one or more words in a target language different from the source language .A set of one or more speech hypotheses , each comprising one or more words from the target language , are produced .", "label": "", "metadata": {}, "score": "67.7298"}
{"text": "The character duration penalty is added on top of the superstate duration penalty at the transition out of the last superstate of any character models .In this way , duration penalty is incorporated in the forward search , instead of as a postprocess .", "label": "", "metadata": {}, "score": "67.75339"}
{"text": "The lower state duration threshold for each state is then stored , as indicated in step 708 .These lower and upper thresholds are stored in memory 110 along with the HMM parameters which can be used in the voice recognition process as described in greater detail herein below .", "label": "", "metadata": {}, "score": "67.754425"}
{"text": "An Information Theoretic Approach To The Automatic Determination Of Phonemic Baseforms . \" Proceedings of the 1984 IEEE Inter - Conference on Acoustics , Speech , and Signal Processing , vol .3 , pp .42.5.1 - 42.5.4 , Mar. 1984 .", "label": "", "metadata": {}, "score": "67.76278"}
{"text": "The number of superstates and states for each character model is determined by image topology .First , original columns are employed as initial blocks of the image , and then the two neighboring blocks which carry the highest cross correlation coefficient are grouped together to become one bigger block .", "label": "", "metadata": {}, "score": "67.7699"}
{"text": "What level , model , and superstate does this previous node belong to ?If the back - tracing is for training , we need to know exactly where does each model , superstate , state start and end , with precise pixel and column index , plus all the observation vectors generated by each state .", "label": "", "metadata": {}, "score": "67.79992"}
{"text": "The same principles also apply to superstates .The presence of noise influences the probability of observing pixels having specific gray levels .The observations for a text element are based on the pixels .One kind of observation is based upon gray - level characterizations of pixels .", "label": "", "metadata": {}, "score": "67.811844"}
{"text": "The binary approach uses feature vectors with thresholded pixel value ( 0 or 1 ) as the first component to replace the gray - level one .The threshold algorithm used is the same algorithm used to extract the 3rd component of the feature vector , as described above .", "label": "", "metadata": {}, "score": "67.84152"}
{"text": "Here we only use FIG .11 to show the correspondence when using a 1D HMM to model a string of pixels .In FIG .1 , we have an extension of the pixel string in FIG .11 into a 2D image .", "label": "", "metadata": {}, "score": "67.84648"}
{"text": "No .468,546 , filed Jan. 23 , 1990 , entitled \" Apparatus and Method For Grouping Utterances of a Phoneme Into Context - Dependent Categories Based on Sound - Similarity For Automatic Speech Recognition . \"An acoustic match score generator 24 generates an acoustic match score for each speech hypothesis .", "label": "", "metadata": {}, "score": "67.87486"}
{"text": "As to the computation , for the discrete case here , table look - up was employed to find the observation probability , so whether there are 2 or 200 levels , the number of levels does not make much difference .", "label": "", "metadata": {}, "score": "67.886505"}
{"text": "These alternatives provide a coarse approximation of the data points but has the advantage of computational efficiency .FIGS .5 and 6 depict these alternate embodiments for the calculation of the weighting factors .FIG .5 depicts the first alternate embodiment .", "label": "", "metadata": {}, "score": "67.90122"}
{"text": "These alternatives provide a coarse approximation of the data points but has the advantage of computational efficiency .FIGS .5 and 6 depict these alternate embodiments for the calculation of the weighting factors .FIG .5 depicts the first alternate embodiment .", "label": "", "metadata": {}, "score": "67.90122"}
{"text": "These alternatives provide a coarse approximation of the data points but has the advantage of computational efficiency .FIGS .5 and 6 depict these alternate embodiments for the calculation of the weighting factors .FIG .5 depicts the first alternate embodiment .", "label": "", "metadata": {}, "score": "67.90122"}
{"text": "LOOP over all states of the 1D model ( block 1419 ) .find the best incoming path towards this state ( block 1421 ) .find the likelihood score between this state and the pixel observation ( block 1423 ) .", "label": "", "metadata": {}, "score": "68.0361"}
{"text": "At the start of the main routine , the observations for the first column are addressed ( step 300 ) .Then the subroutine ( steps 310 - 318 ) is performed using such observations .where N equals the number of states , \u03c0 i equals the probability of being in state i initially , b i equals the probablity of a given observation in state i , and O 1 equals the actual initial observation .", "label": "", "metadata": {}, "score": "68.040924"}
{"text": "Probabilistic models and statistical techniques have been used with more success in predicting the intended message than techniques that seek an exact match .One such technique is Hidden Markov Models ( HMMs ) .These techniques are more adept for speech recognition since they determine the reference pattern that will more likely match the speech signal rather than finding an exact match .", "label": "", "metadata": {}, "score": "68.149284"}
{"text": "Probabilistic models and statistical techniques have been used with more success in predicting the intended message than techniques that seek an exact match .One such technique is Hidden Markov Models ( HMMs ) .These techniques are more adept for speech recognition since they determine the reference pattern that will more likely match the speech signal rather than finding an exact match .", "label": "", "metadata": {}, "score": "68.149284"}
{"text": "Probabilistic models and statistical techniques have been used with more success in predicting the intended message than techniques that seek an exact match .One such technique is Hidden Markov Models ( HMMs ) .These techniques are more adept for speech recognition since they determine the reference pattern that will more likely match the speech signal rather than finding an exact match .", "label": "", "metadata": {}, "score": "68.149284"}
{"text": "Such PHMMs are thus treated as nested one - dimensional models rather than truly two - dimensional .FIG .2 is a flow chart showing the operation of a computer programmed to compare a pixel map for a word image with a set of pseudo two - dimensional HMMs in accordance with the invention , in which the superstates represent vertical slices .", "label": "", "metadata": {}, "score": "68.19718"}
{"text": "The context - independent models will be based on the selected phonemes and modeled within whatever phonemic context appears in the training data .Similarly , the training data will provide the estimates for the parameters of the context - independent models .", "label": "", "metadata": {}, "score": "68.264"}
{"text": "The context - independent models will be based on the selected phonemes and modeled within whatever phonemic context appears in the training data .Similarly , the training data will provide the estimates for the parameters of the context - independent models .", "label": "", "metadata": {}, "score": "68.264"}
{"text": "The context - independent models will be based on the selected phonemes and modeled within whatever phonemic context appears in the training data .Similarly , the training data will provide the estimates for the parameters of the context - independent models .", "label": "", "metadata": {}, "score": "68.264"}
{"text": "This is from the expected traversal through C , B , and A , according to the path map .Corrected Duration Penalty ( CDP ) : .In this term , the forward and backward information is combined to determine the correct duration penalty at each extension .", "label": "", "metadata": {}, "score": "68.30112"}
{"text": "6 assumes that tie features of each frame are autocorrelation coefficients , where the first coefficient is labeled R 0 and indicates the energy level in the frame .To determine the noise structure , only the frames of the wide token having the lowest energy are considered and they are used to compute an average noise feature .", "label": "", "metadata": {}, "score": "68.3041"}
{"text": "5A and 5B to which reference is now briefly made .FIG .5A shows the signal 71 represented by a noiseless template with blank frames 72 ( having no signal therein ) on either end of signal 71 .After noise and gain adaptation , the clean signal 71 becomes noisy signal 74 and the blank frames 72 become noisy frames 76 .", "label": "", "metadata": {}, "score": "68.46683"}
{"text": "Once the training data has been generated and stored in the appropriate storage locations , the recognition system is ready to execute .The primary task of the speech recognition system is to detect the linguistic message which is embodied in the input speech signal .", "label": "", "metadata": {}, "score": "68.47741"}
{"text": "Once the training data has been generated and stored in the appropriate storage locations , the recognition system is ready to execute .The primary task of the speech recognition system is to detect the linguistic message which is embodied in the input speech signal .", "label": "", "metadata": {}, "score": "68.47741"}
{"text": "Once the training data has been generated and stored in the appropriate storage locations , the recognition system is ready to execute .The primary task of the speech recognition system is to detect the linguistic message which is embodied in the input speech signal .", "label": "", "metadata": {}, "score": "68.47741"}
{"text": "Mismatch in the acoustics between the training and recognition phases ; and .Inaccurate VAD estimates of the word endpoints in the recognition phase .These two problems lead to recognition errors .There are many techniques known in the art to deal with the acoustic mismatch problem .", "label": "", "metadata": {}, "score": "68.50554"}
{"text": "Mismatch in the acoustics between the training and recognition phases ; and .Inaccurate VAD estimates of the word endpoints in the recognition phase .These two problems lead to recognition errors .There are many techniques known in the art to deal with the acoustic mismatch problem .", "label": "", "metadata": {}, "score": "68.50554"}
{"text": "It starts at step 602 , where an initial estimate of the model parameters is made .For example , with reference to FIG .3 , initially , state 1 ( S 1 ) is formed from frames F 11 and F 12 of utterance U 1 , and frames F 21 and F 22 of utterance 2 .", "label": "", "metadata": {}, "score": "68.50841"}
{"text": "Description .TECHNICAL FIELD .The present invention relates to computer speech recognition , and more particularly , to a computer speech recognition system that utilizes continuous hidden Markov models .BACKGROUND OF THE INVENTION .The area of speech recognition is challenged by the need to produce a speaker - independent continuous speech recognition system which has a minimal recognition error rate .", "label": "", "metadata": {}, "score": "68.51961"}
{"text": "Description .TECHNICAL FIELD .The present invention relates to computer speech recognition , and more particularly , to a computer speech recognition system that utilizes continuous hidden Markov models .BACKGROUND OF THE INVENTION .The area of speech recognition is challenged by the need to produce a speaker - independent continuous speech recognition system which has a minimal recognition error rate .", "label": "", "metadata": {}, "score": "68.51961"}
{"text": "Description .TECHNICAL FIELD .The present invention relates to computer speech recognition , and more particularly , to a computer speech recognition system that utilizes continuous hidden Markov models .BACKGROUND OF THE INVENTION .The area of speech recognition is challenged by the need to produce a speaker - independent continuous speech recognition system which has a minimal recognition error rate .", "label": "", "metadata": {}, "score": "68.51961"}
{"text": "The structure of the hidden Markov models and senones is formed and estimates of the parameters for these data structures are calculated from the training data .The weighting factors are then determined through the technique of deleted interpolation .Referring to FIG .", "label": "", "metadata": {}, "score": "68.53859"}
{"text": "The structure of the hidden Markov models and senones is formed and estimates of the parameters for these data structures are calculated from the training data .The weighting factors are then determined through the technique of deleted interpolation .Referring to FIG .", "label": "", "metadata": {}, "score": "68.53859"}
{"text": "The structure of the hidden Markov models and senones is formed and estimates of the parameters for these data structures are calculated from the training data .The weighting factors are then determined through the technique of deleted interpolation .Referring to FIG .", "label": "", "metadata": {}, "score": "68.53859"}
{"text": "Stochastic models , such as hidden Markov models ( HMMs ) , together with dynamic programming techniques , have been widely used in the field of speech recognition .For example , see the article entitled \" Automatic Recognition of Keywords in Unconstrained Speech Using Hidden Markov Models \" by J. Wilpon , L. Rabiner , C. Lee and E. Goldman , IEEE Trans .", "label": "", "metadata": {}, "score": "68.54425"}
{"text": "LOOP over all superstate , from 0 to N ( block 1409 ) .find the best incoming path towards this superstate ( block 1411 ) .find the likelihood score between this superstate ( 1D model ) and the column ( block 1413 ) .", "label": "", "metadata": {}, "score": "68.57167"}
{"text": "1 shows a pixel map 100 of the character \" h \" together with a corresponding state diagram 200 for a pseudo two - dimensional HMM for such character , in accordance with the invention .The model is termed a \" top - down , left - right \" model because it depicts the sequence of states resulting from a vertical gray - scale raster scan of the pixel map .", "label": "", "metadata": {}, "score": "68.64521"}
{"text": "Each speech hypothesis comprises one or more words from the target language .An acoustic model generator produces an acoustic model of each speech hypothesis .An acoustic match score generator produces an acoustic match score for each speech hypothesis .Each acoustic match score comprises an estimate of the closeness of a match between the acoustic model of the speech hypothesis and the sequence of coded representations of the utterance produced by the acoustic processor .", "label": "", "metadata": {}, "score": "68.64818"}
{"text": "Each speech hypothesis comprises one or more words from the target language .An acoustic model generator produces an acoustic model of each speech hypothesis .An acoustic match score generator produces an acoustic match score for each speech hypothesis .Each acoustic match score comprises an estimate of the closeness of a match between the acoustic model of the speech hypothesis and the sequence of coded representations of the utterance produced by the acoustic processor .", "label": "", "metadata": {}, "score": "68.64818"}
{"text": "FIG .6 is a flow diagram of a second alternate embodiment for calculating a new value for lambda as used in the system of FIG .3 .FIGS .7A and 7B depict an example of the hidden Markov models and senone structures associated with a phoneme .", "label": "", "metadata": {}, "score": "68.72772"}
{"text": "FIG .6 is a flow diagram of a second alternate embodiment for calculating a new value for lambda as used in the system of FIG .3 .FIGS .7A and 7B depict an example of the hidden Markov models and senone structures associated with a phoneme .", "label": "", "metadata": {}, "score": "68.72772"}
{"text": "FIG .6 is a flow diagram of a second alternate embodiment for calculating a new value for lambda as used in the system of FIG .3 .FIGS .7A and 7B depict an example of the hidden Markov models and senone structures associated with a phoneme .", "label": "", "metadata": {}, "score": "68.72772"}
{"text": "This is specially important for gray - level images with noise and blur , for it is difficult to make meaningful , efficient , and consistent segmentation ( grouping ) for different characters and samples by manual operation .For each character , the procedure is repeated for a few sample images .", "label": "", "metadata": {}, "score": "68.72958"}
{"text": "Statistical representations of the other frames are also generated .The second state S 2 is the mean of the values of frames F 13 and F 14 of utterance U 1 and frames F 23 and F 24 of utterance U 2 .", "label": "", "metadata": {}, "score": "68.755646"}
{"text": "For example , the number of final blocks may be decided by inspecting character topology , and given before the grouping process starts .After those vertical blocks of columns are formed , one then examines each vertical block to group rows within the block by using the same process as was done for columns .", "label": "", "metadata": {}, "score": "68.77315"}
{"text": "2 is a block diagram of a portion of one example of a speech recognition system according to the invention .In this embodiment , the speech recognition system further comprises a candidate word generator 20 for generating a set of candidate words consisting solely of words in the target language which are partial or full translations of words in the source text .", "label": "", "metadata": {}, "score": "68.78607"}
{"text": "Those skilled in the art will recognize that the routine illustrated in FIG .10 is called for each frame and state for purposes of selectively applying the penalty to each state of each frame .During recognition , the size of the penalty assigned is proportional to the amount by which the duration is short of the lower threshold or has exceeded the maximum threshold .", "label": "", "metadata": {}, "score": "68.78984"}
{"text": "If the state duration is greater than the upper threshold , the processor 110 calculates a proportional penalty , as indicated in step 1024 .The penalty is added to the score to produce an adjusted score as indicated in step 1026 .", "label": "", "metadata": {}, "score": "68.8474"}
{"text": "It is noted that this operation produces the test peak energy R 0 t of the widened test token .A similar operation is performed , typically off - line , for each reference template , producing the reference peak energies R or .", "label": "", "metadata": {}, "score": "68.84881"}
{"text": "1 , a superstate transition happens when we moves from column to column ( from left to right in our case ) .If we assume S k is the superstate for column k , then a ij could be defined as .", "label": "", "metadata": {}, "score": "68.866516"}
{"text": "The first fifty eigenvectors of the resulting matrix form the rotation matrix .( See , for example , \" Vector Quantization Procedure For Speech Recognition Systems Using Discrete Parameter Phoneme - Based Markov Word Models \" by L. R. Bahl , et al , IBM Technical Disclosure Bulletin , Volume 32 , No . 7 , December 1989 , pages 320 and 321 . )", "label": "", "metadata": {}, "score": "68.89868"}
{"text": "The duration constrain is first imposed in the forward Viterbi search as added penalty , then is combined with the path map obtained in the forward pass to do the duration penalty correction in the backward search .Experimental results have proven the superiority of the system in the form of greatly improved recognition rate , and robustness over widely changed applications .", "label": "", "metadata": {}, "score": "68.93399"}
{"text": "No . 673,810 , filed on Mar. 22 , 1991 entitled \" Speaker - Independent Label Coding Apparatus \" .A voice recognition system ( 204 , 206 , 207 , 208 ) assigns a penalty to a score in a voice recognition system .", "label": "", "metadata": {}, "score": "68.94042"}
{"text": "8 is a diagram setting forth the data structure flow for a path stack ; .FIG .9 is a diagram showing the topology of a backward tree search ; .FIG .10 is a flow chart setting forth a preferred method for processing the path stack of FIG .", "label": "", "metadata": {}, "score": "68.95177"}
{"text": "The problem is difficult because of the high variability in the values of the best - match scores .Methods that are known in the art for improving the rejection capability of HMM systems include mostly the usage of a \" general speech \" template ( these are discussed in the previously mentioned article by Raman , in U.S. Pat .", "label": "", "metadata": {}, "score": "69.04719"}
{"text": "ICASSP ' 93 Vol .II , pp . 311 - 314 , 1993 .In the preferred embodiment , a HMM can be used to model the speech unit of a phoneme .The HMM can also be referred to as an acoustic model .", "label": "", "metadata": {}, "score": "69.07161"}
{"text": "ICASSP ' 93 Vol .II , pp . 311 - 314 , 1993 .In the preferred embodiment , a HMM can be used to model the speech unit of a phoneme .The HMM can also be referred to as an acoustic model .", "label": "", "metadata": {}, "score": "69.07161"}
{"text": "ICASSP ' 93 Vol .II , pp . 311 - 314 , 1993 .In the preferred embodiment , a HMM can be used to model the speech unit of a phoneme .The HMM can also be referred to as an acoustic model .", "label": "", "metadata": {}, "score": "69.07161"}
{"text": "Wherever the path compares a test token frame against a non - noise frame , the present invention performs the standard DTW operation using standard alignment constraints .However , wherever the comparison is to a reference noise frame , there are no duration constraints .", "label": "", "metadata": {}, "score": "69.09662"}
{"text": "Connected and Degraded Text Recognition Using Hidden Markov Model by C. Bose et al . , Proc . of the 11th Int .Conf . on Pattern Recognition , 1992 .Connected and Degraded Text Recognition Using Planar Hidden Markov Models , by O. E. Agazzi et al . , in Proc . of ICASSP 93 , pp .", "label": "", "metadata": {}, "score": "69.10066"}
{"text": "When such additional information is included , each observation can be a vector quantity representing several different components .An example of such an observation vector will be given below .The pseudo two - dimensional hidden Markov model utilized by a preferred embodiment described herein employs a concept referred to as the duration probability of a character .", "label": "", "metadata": {}, "score": "69.11456"}
{"text": "The Incorporation of Duration Penalty in PHMM .Under the nested Viterbi search structure , if we want to incorporate duration penalty as part of the path search process , the likelihood should be updated along the path at each node as follows ( reference 9 ! ) : # # EQU13 # # where b i ( b O .", "label": "", "metadata": {}, "score": "69.14801"}
{"text": "In our application , N is determined by the topology of the image .For example in FIG .1 , by grouping similar columns together , a PHMM with three superstates should be sufficient to characterize the character .D : Duration probability for character .", "label": "", "metadata": {}, "score": "69.16379"}
{"text": "These parameters are estimated using the same technique as described above in reference to the estimation of the parameters of the HMMs in the training phase ( ie . , Baum - Welch algorithm ) .Next in step 74 , a new value , \u03bb new , is computed .", "label": "", "metadata": {}, "score": "69.192024"}
{"text": "These parameters are estimated using the same technique as described above in reference to the estimation of the parameters of the HMMs in the training phase ( ie . , Baum - Welch algorithm ) .Next in step 74 , a new value , \u03bb new , is computed .", "label": "", "metadata": {}, "score": "69.192024"}
{"text": "These parameters are estimated using the same technique as described above in reference to the estimation of the parameters of the HMMs in the training phase ( ie . , Baum - Welch algorithm ) .Next in step 74 , a new value , \u03bb new , is computed .", "label": "", "metadata": {}, "score": "69.192024"}
{"text": "This gives two approach a fair comparison .The results of the experiment are shown in FIG .10 .Upon examining FIG .10 , it is apparent that : .The gray level approach improves the performance of the OCR by more than 2 % in the similarly degraded data set as the training set .", "label": "", "metadata": {}, "score": "69.1935"}
{"text": "An acoustic model of a word is then produced by replacing each letter in the spelling of the word with an acoustic letter model corresponding to the letter .BRIEF DESCRIPTION OF THE DRAWING .FIG .1 is a block diagram of an example of a speech recognition system according to the invention .", "label": "", "metadata": {}, "score": "69.200905"}
{"text": "An acoustic model of a word is then produced by replacing each letter in the spelling of the word with an acoustic letter model corresponding to the letter .BRIEF DESCRIPTION OF THE DRAWING .FIG .1 is a block diagram of an example of a speech recognition system according to the invention .", "label": "", "metadata": {}, "score": "69.200905"}
{"text": "FIG .6 is a block diagram illustration of a noise and peak energy estimator forming part of the system of FIG .3 ; .FIG .7 is a graphical illustration of the noise adapted DTW operation of the present invention ; and .", "label": "", "metadata": {}, "score": "69.24275"}
{"text": "FIG .6 is a block diagram illustration of a noise and peak energy estimator forming part of the system of FIG .3 ; .FIG .7 is a graphical illustration of the noise adapted DTW operation of the present invention ; and .", "label": "", "metadata": {}, "score": "69.24275"}
{"text": "Where D 1,i is the number of frames assigned to state i from utterance 1 , and D 2,i , is the number of frames assigned to state i from utterance 2 .Those skilled in the art will recognize that more than two utterances can be used .", "label": "", "metadata": {}, "score": "69.275986"}
{"text": "6 assumes that the features of each frame are autocorrelation coefficients , where the first coefficient is labeled R 0 and indicates the energy level in the frame .To determine the noise structure , only the frames of the wide token having the lowest energy are considered and they are used to compute an average noise feature .", "label": "", "metadata": {}, "score": "69.30028"}
{"text": "0070 ] .The best score is the value of Epd_Score for the best template Best_Template .[ 0071 ] .The average score is based on two values : a_priori_av_score(SegSNR ) and Sum_Score .Thus , the best template and the other templates uttering the same word as the best template are not used to generate Sum_Score .", "label": "", "metadata": {}, "score": "69.37053"}
{"text": "For Landell et al . 's system , which was designed for an air force cockpit where the noise is fairly constant , this might be sufficient .However , with variable noise such as encountered during , for example , regular use of mobile phones , this past estimate can be inaccurate and can lead to recognition errors .", "label": "", "metadata": {}, "score": "69.45494"}
{"text": "For Landell et al . 's system , which was designed for an air force cockpit where the noise is fairly constant , this might be sufficient .However , with variable noise such as encountered during , for example , regular use of mobile phones , this past estimate can be inaccurate and can lead to recognition errors .", "label": "", "metadata": {}, "score": "69.45494"}
{"text": "12 - 19 , June 1990 , which combines a forward Viterbi - based trellis search , and a backward tree search .According to a preferred embodiment disclosed herein , the algorithm disclosed by Soong and Huang is enhanced as follows , to provide an improved hypothesis search algorithm .", "label": "", "metadata": {}, "score": "69.56898"}
{"text": "16 .The strong point of this training procedure is that it is unsupervised .Throughout the whole process , the goal is to use the current model to do path finding , or ( super)state sequence segmentation , and then use the segmentation information to do model parameter estimation .", "label": "", "metadata": {}, "score": "69.5799"}
{"text": "This can also significantly increase the processing speed .A character is represented by a pseudo two - dimensional HMM having a number of superstates , with each superstate having at least one state .In a preferred embodiment , the superstates represent vertical slices through the word image .", "label": "", "metadata": {}, "score": "69.59598"}
{"text": "Context - independent and context - dependent continuous density hidden Markov models are generated for each phonetic unit .The output probability associated with a state is determined by weighing the output probabilities of the context - dependent and context - independent states in accordance with a weighting factor .", "label": "", "metadata": {}, "score": "69.66754"}
{"text": "The whole stack is examined for duplication whenever there is a path ( re)insertion .Among all the paths with a duplicated \" word \" , only the best path is retained and all the other paths are removed .Starting from the beginning of the stack , there is usually only one duplication to be compared and removed .", "label": "", "metadata": {}, "score": "69.68839"}
{"text": "Note that there is no B parameters for each model at superstate level .The reason will be clear from the following discussion .Feature Extraction .The observation vector O xy for each pixel located at ( x , y ) has three components : .", "label": "", "metadata": {}, "score": "69.731674"}
{"text": "The \" fast \" match examines at least a portion of every word in the target vocabulary to find a number of words which are good possibilities for extending the candidate word strings .The fast match estimates the closeness of a match between an acoustic fast match model of a word and a portion of the sequence of coded representations of the utterance .", "label": "", "metadata": {}, "score": "69.76001"}
{"text": "No details are given in the Landell et al .article for how the noise templates are constructed and how to implement the DTW matching against the concatenated noise - speech - noise templates .Also , the Landell et al .", "label": "", "metadata": {}, "score": "69.81726"}
{"text": "No details are given in the Landell et al .article for how the noise templates are constructed and how to implement the DTW matching against the concatenated noise - speech - noise templates .Also , the Landell et al .", "label": "", "metadata": {}, "score": "69.81726"}
{"text": "15 ) : .LOOP find the level , model , and superstate of the previous column from the best path ( block 1503 ) .Begin back - tracing of the optimal path within the previous column ( block 1505 ) .", "label": "", "metadata": {}, "score": "69.81732"}
{"text": "Assume as before that path ABCFG is preferred over path ADEFG in forward stage .This implies that , during the forward stage , the accumulated likelihood score at C should be better ( smaller ) than at E ( so that F chose C at that time ) .", "label": "", "metadata": {}, "score": "69.881134"}
{"text": "A speech recognition system as claimed in claim 17 , characterized in that the means for measuring the value of at least one feature of an utterance comprises a microphone .A speech recognition method comprising : . displaying a source text comprising one or more words in a source language ; . generating a sequence of coded representations of an utterance to be recognized , said utterance comprising one or more words in a target language different from the source language ; . generating a set of one or more speech hypotheses , each speech hypothesis comprising one or more words from the target language ; . generating an acoustic model of each speech hypothesis ; . generating an acoustic match score for each speech hypothesis , each acoustic match score comprising an estimate of the closeness of a match between the acoustic model of the speech hypothesis and the sequence of coded representations of the utterance ; . generating a translation match score for each speech hypothesis , each translation match score comprising an estimate of the probability of occurrence of the speech hypothesis given the occurrence of the source text ; . generating a hypothesis score for each hypothesis , each hypothesis score comprising a combination of the acoustic match score and the translation match score for the hypothesis ; . storing a subset of one or more speech hypotheses , from the set of speech hypotheses , having the best hypothesis scores ; and . outputting at least one word of one or more of the speech hypotheses in the subset of speech hypotheses having the best hypothesis scores .", "label": "", "metadata": {}, "score": "69.90637"}
{"text": "A speech recognition system as claimed in claim 17 , characterized in that the means for measuring the value of at least one feature of an utterance comprises a microphone .A speech recognition method comprising : . displaying a source text comprising one or more words in a source language ; . generating a sequence of coded representations of an utterance to be recognized , said utterance comprising one or more words in a target language different from the source language ; . generating a set of one or more speech hypotheses , each speech hypothesis comprising one or more words from the target language ; . generating an acoustic model of each speech hypothesis ; . generating an acoustic match score for each speech hypothesis , each acoustic match score comprising an estimate of the closeness of a match between the acoustic model of the speech hypothesis and the sequence of coded representations of the utterance ; . generating a translation match score for each speech hypothesis , each translation match score comprising an estimate of the probability of occurrence of the speech hypothesis given the occurrence of the source text ; . generating a hypothesis score for each hypothesis , each hypothesis score comprising a combination of the acoustic match score and the translation match score for the hypothesis ; . storing a subset of one or more speech hypotheses , from the set of speech hypotheses , having the best hypothesis scores ; and . outputting at least one word of one or more of the speech hypotheses in the subset of speech hypotheses having the best hypothesis scores .", "label": "", "metadata": {}, "score": "69.90637"}
{"text": "5A is a graphical illustration of a test utterance and two extra , blank frames , useful in understanding the operation of the present invention ; .FIG .5B is a graphical illustration of a noise adapted version of the signal of FIG .", "label": "", "metadata": {}, "score": "69.95865"}
{"text": "5A is a graphical illustration of a test utterance and two extra , blank frames , useful in understanding the operation of the present invention ; .FIG .5B is a graphical illustration of a noise adapted version of the signal of FIG .", "label": "", "metadata": {}, "score": "69.95865"}
{"text": "The noise cancellation processor 52 adapts to changing noise levels by periodically updating the noise vector N(t ) whenever the prior feature vector F(t-1 ) is identified as noise or silence .The noise vector N(t ) is updated according to the formula .", "label": "", "metadata": {}, "score": "69.991714"}
{"text": "A HMM can represent a particular phonetic unit of speech , such as a phoneme or word .Associated with each state is an output probability indicating the likelihood that the state matches a feature vector .For each transition , there is an associated transition probability indicating the likelihood of following the transition .", "label": "", "metadata": {}, "score": "70.07922"}
{"text": "A HMM can represent a particular phonetic unit of speech , such as a phoneme or word .Associated with each state is an output probability indicating the likelihood that the state matches a feature vector .For each transition , there is an associated transition probability indicating the likelihood of following the transition .", "label": "", "metadata": {}, "score": "70.07922"}
{"text": "A HMM can represent a particular phonetic unit of speech , such as a phoneme or word .Associated with each state is an output probability indicating the likelihood that the state matches a feature vector .For each transition , there is an associated transition probability indicating the likelihood of following the transition .", "label": "", "metadata": {}, "score": "70.07922"}
{"text": "For this reason , one does not impose a penalty for a given state until a change - of - state transition happens .But after the forward state transition is completed , one can see from FIG .8 that the cost of passing through F - G will be different for ABCFG and ADEFG .", "label": "", "metadata": {}, "score": "70.22327"}
{"text": "The present invention provides a method for recognizing connected and degraded text embedded in a gray - scale image .In accordance with the invention , pseudo two - dimensional hidden Markov models ( PHMMs ) are used to represent characters .", "label": "", "metadata": {}, "score": "70.235535"}
{"text": "In this invention , we assume no skipping state and/or superstate is possible .The Viterbi algorithm is a well - known dynamic programming method that can be used for calculating the probability that a particular HMM is a match for a given sequence of observations .", "label": "", "metadata": {}, "score": "70.23692"}
{"text": "The value of d for a given state is determined from the array \u03c8 by counting the number of self - transitions in the state or superstate .Under the nested Viterbi search structure , duration penalty is incorporated as part of the path search process , according to a preferred embodiment described herein .", "label": "", "metadata": {}, "score": "70.28082"}
{"text": "[ 0027 ] .In all speech recognition applications , e.g. , in voice - dialing by name , it is very important to reject utterances that are either not hi the vocabulary , or are so badly pronounced that they yield erroneous recognition .", "label": "", "metadata": {}, "score": "70.312836"}
{"text": "The additional external frames are utilized to overcome any errors made by the voice activity detector 54 , in particular those errors arising from inaccurate VAD estimates of endpoints .[0044 ] .Referring back to FIG .3 , the wide token is provided both to feature transformer 58 A and to the noise and peak energy estimator 68 .", "label": "", "metadata": {}, "score": "70.31352"}
{"text": "Briefly , this procedure works as follows .The probability of the aligned sentence pairs is a computable function of the parameter values .The goal of the procedure it to find parameter values which locally maximize this function .This is accomplished iteratively .", "label": "", "metadata": {}, "score": "70.41077"}
{"text": "For example , Dmin tweak and Dmax tweak may be 0.1 .Those skilled in that art will recognize that the actual values may vary without deviating from the invention .FIG .5 illustrates the left - right , no skip HMM .", "label": "", "metadata": {}, "score": "70.41482"}
{"text": "60 - 72 , September / October 1993 .F. K. Soong and E.-F. Huang , \" A tree - trellis based fast search for finding the n best sentence hypotheses in continuous speech recognition \" , in Proc .DARPA Speech and Natural Language Workshop , pp .", "label": "", "metadata": {}, "score": "70.436264"}
{"text": "word translation probabilities p.sub.5 ( s.vertline.t ) for source words s and target words t satisfying # # EQU7 # # . alignment probabilities P.sub.6 ( i.vertline.j , l ) satisfying # # EQU8 # # .Values for these parameters can be determined from a large quantity of aligned source - target sentence pairs ( S.sup.l , T.sup.l ) . . .", "label": "", "metadata": {}, "score": "70.46492"}
{"text": "5A is a graphical illustration of a test utterance and two extra , blank frames , useful in understanding the operation of the present invention ; .[ 0037 ] .[ 0037]FIG .5B is a graphical illustration of a noise adapted version of the signal of FIG .", "label": "", "metadata": {}, "score": "70.4673"}
{"text": "No .5,778,342 for the case where the features are autocorrelation function ( ACF ) .For the present invention , this transformation is : .[ 0057 ] .[0058 ] .[ 0059 ] .[ 0060 ] .", "label": "", "metadata": {}, "score": "70.47717"}
{"text": "In the prior art , language match scores and conditional translation match scores are combined only when the words of S are generated from the words T and no other words .In contrast , the combined match score generator must estimate a combined score when S is generated from the words of T together with some additional unspecified words .", "label": "", "metadata": {}, "score": "70.523735"}
{"text": "The N - best hypotheses approach greatly boosts the performance of the gray - level system , not only in significantly improved accuracy rate , but also in the increased robustness of the system .The value of this experiment is to show that a great performance improvement can be achieved by getting the best duration - corrected candidate , without any postprocessing being required .", "label": "", "metadata": {}, "score": "70.533165"}
{"text": "Most of the words found in the hypotheses list , i.e. those that are considered top competitive to the correct one , are mostly combination of characters from the confusion set .In summary , the utilization of gray - scale imaging in combination with pseudo 2-dimensional Markov modeling provides significant improvement over binary systems , especially for degraded and connected word images .", "label": "", "metadata": {}, "score": "70.55737"}
{"text": "A typical method of determining convergence is to compare the overall likelihoods from step 355 for successive iterations .If the difference is small , indicating that the parameters have not changed significantly during the last iteration , then convergence can be assumed .", "label": "", "metadata": {}, "score": "70.600174"}
{"text": "Thus , in step 79 , all points in the deleted block that correspond to sen SD are found using the model generated in step 48 and forced alignment .In step 80 , the process iterates for each data point x i in the deleted block that is aligned with sen SD .", "label": "", "metadata": {}, "score": "70.66436"}
{"text": "Thus , in step 79 , all points in the deleted block that correspond to sen SD are found using the model generated in step 48 and forced alignment .In step 80 , the process iterates for each data point x i in the deleted block that is aligned with sen SD .", "label": "", "metadata": {}, "score": "70.66436"}
{"text": "Thus , in step 79 , all points in the deleted block that correspond to sen SD are found using the model generated in step 48 and forced alignment .In step 80 , the process iterates for each data point x i in the deleted block that is aligned with sen SD .", "label": "", "metadata": {}, "score": "70.66436"}
{"text": "4 is a block diagram of an example of an acoustic processor for a speech recognition system according to the invention .FIG .5 is a block diagram of an example of an acoustic feature value measure for an acoustic processor for a speech recognition system according to the invention .", "label": "", "metadata": {}, "score": "70.68571"}
{"text": "4 is a block diagram of an example of an acoustic processor for a speech recognition system according to the invention .FIG .5 is a block diagram of an example of an acoustic feature value measure for an acoustic processor for a speech recognition system according to the invention .", "label": "", "metadata": {}, "score": "70.68571"}
{"text": "The acoustic model generator 18 generates an acoustic model of at least one word in the source text which is not in the source vocabulary .In one embodiment of the invention , this comparator may operate according to a set of rules that describe the manner in which letters in the source language should be rewritten when translated into the target language .", "label": "", "metadata": {}, "score": "70.71536"}
{"text": "The acoustic model generator 18 generates an acoustic model of at least one word in the source text which is not in the source vocabulary .In one embodiment of the invention , this comparator may operate according to a set of rules that describe the manner in which letters in the source language should be rewritten when translated into the target language .", "label": "", "metadata": {}, "score": "70.71536"}
{"text": "Description .FIELD OF THE INVENTION .The present invention relates to speech recognition generally and to speaker dependent recognition in the presence of noise , in particular .BACKGROUND OF THE INVENTION .Speech recognition in noisy environments is a well studied , but yet difficult task .", "label": "", "metadata": {}, "score": "70.79382"}
{"text": "The whole process will be repeated until finally the parameter set converges within certain criteria , and a HMM is thus built for each character .Before the training process begins , we should have an initial PHMM set to start the iteration .", "label": "", "metadata": {}, "score": "70.816895"}
{"text": "An acoustic match score for each speech hypothesis comprises an estimate of the closeness of a match between the acoustic model of the speech hypothesis and the sequence of coded representations of the utterance .A translation match score for each speech hypothesis comprises an estimate of the probability of occurrence of the speech hypothesis given the occurrence of the source text .", "label": "", "metadata": {}, "score": "70.93954"}
{"text": "Note that this procedure is only required when there is no initial estimates of PHMMs available to start the training process .Next , the parameters for the model are initialized ( step 351 ) .This can be done by using parameters derived above from the sample used to create the structure .", "label": "", "metadata": {}, "score": "71.01229"}
{"text": "Back - Tracing .In our forward ( left to right ) Viterbi search process , at every node in superstate level , we record all the information necessary for future use .The necessary information might be different for training and recognition purpose .", "label": "", "metadata": {}, "score": "71.02773"}
{"text": "The fast match estimates the closeness of a match between an acoustic fast match model of a word and a portion of the sequence of coded representations of the utterance .The \" detailed \" match examines only those words which the \" fast \" match determines to be good possibilities for extending the candidate word strings .", "label": "", "metadata": {}, "score": "71.15714"}
{"text": "This is accomplished iteratively .At each step of the iteration , the parameter values are updated according to the formulas : # # EQU9 # # Here the sum on n runs over all source - target sentence pairs ( S n , T n ) .", "label": "", "metadata": {}, "score": "71.23141"}
{"text": "( a )For F - E extension : no state transition occurs .The predicted duration for state 2 : .How long has this path been staying in state 2 : 2 ( i.e. G , F ) .How much longer will it be staying in state 2 if through E : according to the best path ADEF from the forward pass , it 's 2 ( i.e. E , D ) .", "label": "", "metadata": {}, "score": "71.23839"}
{"text": "Speech Signal Processing , Vol .ASSP-37 , pp .1649 - 1658 , November 1989 .L. R. Rabiner , B. H. Juang , S. E. Levinson , and M. M. Sondhi , \" Recognition of isolated digits using hidden markov models with continuous mixture densities \" , AT&T Tech .", "label": "", "metadata": {}, "score": "71.251785"}
{"text": "In such cases , references to \" columns \" can be replaced with references to \" rows \" .Many of the calculations used in HMMs involve multiple calculations of products of probabilities .It is usually more convenient , and preferred , to convert such calculations to logarithmic form .", "label": "", "metadata": {}, "score": "71.359726"}
{"text": "FIG .6 is a high level flow chart illustrating the training algorithm .FIG .7 is a flow chart illustrating the setting of an upper and lower duration threshold .FIG .8 illustrates the penalty applied to one state of one model .", "label": "", "metadata": {}, "score": "71.40202"}
{"text": "Preferably , there are two blocks of data .However , the invention is not limited to this number of blocks , others may be used dependent on the constraints of training data storage and training time .A weighting factor is calculated for each context - dependent senone ( step 62 ) by first finding sense , which is the context - independent senone that corresponds to sen SD ( ie .", "label": "", "metadata": {}, "score": "71.41504"}
{"text": "Preferably , there are two blocks of data .However , the invention is not limited to this number of blocks , others may be used dependent on the constraints of training data storage and training time .A weighting factor is calculated for each context - dependent senone ( step 62 ) by first finding sense , which is the context - independent senone that corresponds to sen SD ( ie .", "label": "", "metadata": {}, "score": "71.41504"}
{"text": "Preferably , there are two blocks of data .However , the invention is not limited to this number of blocks , others may be used dependent on the constraints of training data storage and training time .A weighting factor is calculated for each context - dependent senone ( step 62 ) by first finding sense , which is the context - independent senone that corresponds to sen SD ( ie .", "label": "", "metadata": {}, "score": "71.41504"}
{"text": "First of all , modern optical scanners provide a gray - scale output .Second , many documents are gray - level in nature .For example , the printed text might be embedded in a gray - level or color picture , or a textured background .", "label": "", "metadata": {}, "score": "71.51935"}
{"text": "No . 4,933,976 . [0062 ] .If desired , the gain adaptation can be performed separately , or not at all .[ 0063 ] .The noise adapted DTW unit 60 compares a widened test utterance , which includes both speech and noise from both sides , with a noise adapted template .", "label": "", "metadata": {}, "score": "71.54088"}
{"text": "In computing the acoustic score for a given word sequence , the recognition task considers various senone alignments .A senone alignment is a mapping from the sequence of acoustic feature vectors to senones which assigns a unique senone to each acoustic feature vector .", "label": "", "metadata": {}, "score": "71.54357"}
{"text": "In computing the acoustic score for a given word sequence , the recognition task considers various senone alignments .A senone alignment is a mapping from the sequence of acoustic feature vectors to senones which assigns a unique senone to each acoustic feature vector .", "label": "", "metadata": {}, "score": "71.54357"}
{"text": "In computing the acoustic score for a given word sequence , the recognition task considers various senone alignments .A senone alignment is a mapping from the sequence of acoustic feature vectors to senones which assigns a unique senone to each acoustic feature vector .", "label": "", "metadata": {}, "score": "71.54357"}
{"text": "Therefore , the recognized word is the word that comes out directly from the best in stack , right after the backward search with duration correction is completed .No postprocessing is imposed .The first hypothesis with the word that could be found in the dictionary will be the output .", "label": "", "metadata": {}, "score": "71.62837"}
{"text": "( c ) comparing the first portion to a plurality of two - dimensional hidden Markov models to identify a closely - matching two - dimensional hidden Markov model from the plurality of two - dimensional hidden Markov models that most closely approximates the first portion ; . wherein the two - dimensional hidden Markov models represent textual elements .", "label": "", "metadata": {}, "score": "71.687355"}
{"text": "The resulting gray level value of the pixel ( from 0 to 255 ) is then quantized into 100 levels .The second component of O xy is the relative position of each pixel in the columns .It is assumed that , after layout analysis of the document , the position of the baseline and the difference between topline and baseline are known .", "label": "", "metadata": {}, "score": "71.72556"}
{"text": "[ 0034 ] .[ 0034]FIG .3 is a block diagram illustration of a speech recognizer , constructed and operative in accordance with some embodiments of the present invention ; .[ 0035 ] .[ 0035]FIG .4 is a graphical illustration of the energy in a test utterance , useful in understanding the operation of the present invention ; . [ 0036 ] .", "label": "", "metadata": {}, "score": "71.77522"}
{"text": "If this path has more than one possible extension , split the partial path into two : a first path having the best one - arc extension and a second path having the remaining one - arc extensions .Otherwise , do a one - arc extension ( block 1109 ) .", "label": "", "metadata": {}, "score": "71.782135"}
{"text": "The apparatus of .claim 6 , wherein the gain and noise adapter is able to provide a modified reference template having peak energy level substantially equivalent to the difference between the peak energy level and the average noise energy level of the widened token .", "label": "", "metadata": {}, "score": "71.81261"}
{"text": "The apparatus of .claim 6 , wherein the gain and noise adapter is able to provide a modified reference template having peak energy level substantially equivalent to the difference between the peak energy level and the average noise energy level of the widened token .", "label": "", "metadata": {}, "score": "71.81261"}
{"text": "A system as in claim 20 wherein the training device further comprises : . a parametric generator to produce a parametric representation of the training data ; . a data generator to generate a set of data points from the parametric representation ; and .", "label": "", "metadata": {}, "score": "71.82249"}
{"text": "A system as in claim 20 wherein the training device further comprises : . a parametric generator to produce a parametric representation of the training data ; . a data generator to generate a set of data points from the parametric representation ; and .", "label": "", "metadata": {}, "score": "71.82249"}
{"text": "A system as in claim 20 wherein the training device further comprises : . a parametric generator to produce a parametric representation of the training data ; . a data generator to generate a set of data points from the parametric representation ; and .", "label": "", "metadata": {}, "score": "71.82249"}
{"text": "FIG .4 , to which reference is now briefly made , illustrates the data stored in buffer 52 .Specifically , FIG .4 graphs the first autocorrelation coefficient per frame and indicates the endpoints indicated by the voice activity detector 54 .", "label": "", "metadata": {}, "score": "71.9164"}
{"text": "FIG .4 , to which reference is now briefly made , illustrates the data stored in buffer 52 .Specifically , FIG .4 graphs the first autocorrelation coefficient per frame and indicates the endpoints indicated by the voice activity detector 54 .", "label": "", "metadata": {}, "score": "71.9164"}
{"text": "( b )For F - C extension : state transition occurs .Correction for the state just finished : .The penalty currently paid in the path according to the forward path information is the same as case 1 above ( note that we still are considering only one path at F up to now ) : f 2 ( 2 ) ; .", "label": "", "metadata": {}, "score": "71.91705"}
{"text": "When put as a string , they can also be used to represent a portion of a complete path .For example , ABCFG represents part of path 2 in FIG .8 .f x ( y ) represents the duration penalty for staying y units long in superstate x. The penalty is the minus of the log of duration probability .", "label": "", "metadata": {}, "score": "71.970764"}
{"text": "Also p.sub.9 ( k.vertline.T ) is the probability of the set of all complete sentences which begin with T and contain k additional words .In one embodiment this probability is estimated as # # EQU4 # # where q is an estimate of the unigram probability of the end - of - sentence marker .", "label": "", "metadata": {}, "score": "72.00463"}
{"text": "[0048 ] .Reference is now made to FIG .6 which details the operation of an exemplary noise and peak energy estimator 68 .Any suitable energy and noise structure estimate can be utilized ; the method shown in FIG .", "label": "", "metadata": {}, "score": "72.00882"}
{"text": "18 - 23 are flowcharts setting forth procedures used for initial model estimation ; .FIGS .24 - 26 are flowcharts setting forth procedures used for character recognition ; FIG .27 is a flowchart setting forth a procedure for path track initialization ; .", "label": "", "metadata": {}, "score": "72.112724"}
{"text": "This process is repeated for the entire input speech signal and results in a sequence of feature vectors which is transmitted to a data processor 38 .Data processor 38 can be any conventional computer , such as a desktop personal computer .", "label": "", "metadata": {}, "score": "72.11498"}
{"text": "This process is repeated for the entire input speech signal and results in a sequence of feature vectors which is transmitted to a data processor 38 .Data processor 38 can be any conventional computer , such as a desktop personal computer .", "label": "", "metadata": {}, "score": "72.11498"}
{"text": "This process is repeated for the entire input speech signal and results in a sequence of feature vectors which is transmitted to a data processor 38 .Data processor 38 can be any conventional computer , such as a desktop personal computer .", "label": "", "metadata": {}, "score": "72.11498"}
{"text": "Each acoustic match score comprises an estimate of the closeness of a match between the acoustic model of the speech hypothesis and the sequence of coded representations of the utterance .When the acoustic models are Markov models , acoustic match scores may be obtained , for example , by the forward pass of the Forward - Backward Algorithm .", "label": "", "metadata": {}, "score": "72.18629"}
{"text": "1 , the speech recognition system comprises a hypothesis score generator 28 for generating a hypothesis score for each hypothesis .Each hypothesis score comprises a combination of the acoustic match score and the translation match score for the hypothesis .The speech recognition system further comprises a storage device 30 for storing a subset of one or more speech hypotheses , from the set of speech hypotheses , having the best hypothesis scores .", "label": "", "metadata": {}, "score": "72.21802"}
{"text": "1 , the speech recognition system comprises a hypothesis score generator 28 for generating a hypothesis score for each hypothesis .Each hypothesis score comprises a combination of the acoustic match score and the translation match score for the hypothesis .The speech recognition system further comprises a storage device 30 for storing a subset of one or more speech hypotheses , from the set of speech hypotheses , having the best hypothesis scores .", "label": "", "metadata": {}, "score": "72.21802"}
{"text": "The length normalization introduces an extra computation load that does not exist in standard DTW .Also , because of the normalization , the standard DTW solution for the best matching path is in fact not optimal .For example , in U.S. Pat .", "label": "", "metadata": {}, "score": "72.241325"}
{"text": "This can be represented mathematically as : # # EQU8 # # In step 108 , the sum of these contributions is formed for all the data points in the set .At the completion of the iteration through all the data points in the set , the average of all of the contributions is returned as the value of \u03bb new , ( step 110 ) .", "label": "", "metadata": {}, "score": "72.2444"}
{"text": "This can be represented mathematically as : # # EQU8 # # In step 108 , the sum of these contributions is formed for all the data points in the set .At the completion of the iteration through all the data points in the set , the average of all of the contributions is returned as the value of \u03bb new , ( step 110 ) .", "label": "", "metadata": {}, "score": "72.2444"}
{"text": "This can be represented mathematically as : # # EQU8 # # In step 108 , the sum of these contributions is formed for all the data points in the set .At the completion of the iteration through all the data points in the set , the average of all of the contributions is returned as the value of \u03bb new , ( step 110 ) .", "label": "", "metadata": {}, "score": "72.2444"}
{"text": "A HMM may represent a phoneme , and a sequence of HMMs may represent words or sentences composed of phonemes .Continuous density probability distribution functions , such as a mixture of Gaussian probability distribution functions , can be utilized to represent the output probability of a state , since they are more accurate at modeling a speech signal .", "label": "", "metadata": {}, "score": "72.25259"}
{"text": "A HMM may represent a phoneme , and a sequence of HMMs may represent words or sentences composed of phonemes .Continuous density probability distribution functions , such as a mixture of Gaussian probability distribution functions , can be utilized to represent the output probability of a state , since they are more accurate at modeling a speech signal .", "label": "", "metadata": {}, "score": "72.25259"}
{"text": "A HMM may represent a phoneme , and a sequence of HMMs may represent words or sentences composed of phonemes .Continuous density probability distribution functions , such as a mixture of Gaussian probability distribution functions , can be utilized to represent the output probability of a state , since they are more accurate at modeling a speech signal .", "label": "", "metadata": {}, "score": "72.25259"}
{"text": "wherein the step of determining an acoustic probability further comprises the step of weighing the output probability of the state of the context - dependent model and the state of the context - independent model based on the weighting factor .A method as in claim 13 wherein the step of providing a weighting factor further comprises the step of deriving the weighting factor from an application of deleted interpolation to the amount of training data .", "label": "", "metadata": {}, "score": "72.35544"}
{"text": "wherein the step of determining an acoustic probability further comprises the step of weighing the output probability of the state of the context - dependent model and the state of the context - independent model based on the weighting factor .A method as in claim 13 wherein the step of providing a weighting factor further comprises the step of deriving the weighting factor from an application of deleted interpolation to the amount of training data .", "label": "", "metadata": {}, "score": "72.35544"}
{"text": "wherein the step of determining an acoustic probability further comprises the step of weighing the output probability of the state of the context - dependent model and the state of the context - independent model based on the weighting factor .A method as in claim 13 wherein the step of providing a weighting factor further comprises the step of deriving the weighting factor from an application of deleted interpolation to the amount of training data .", "label": "", "metadata": {}, "score": "72.35544"}
{"text": "No .09/226,535 filed on Jan. 6 , 1999 , now U.S. Pat .No . 6,466,906 Oct. 15,2002 .FIELD OF THE INVENTION .The present invention relates to speech recognition generally and to speaker dependent recognition in the presence of noise , in particular .", "label": "", "metadata": {}, "score": "72.40128"}
{"text": "The training phase is relatively noise - free , whereas the recognition needs to cope with additive environmental noise ; .The environmental noise is unknown to the system prior to the instant the user pushes a push to talk ( PTT ) button and starts speaking ; .", "label": "", "metadata": {}, "score": "72.41699"}
{"text": "If the top path 1010 has already been subjected to the process described in this paragraph , put the top path 1010 back into the path stack 1000 and take the next path 1020 in the path stack 1000 .For the first non - completed path , the process described herein ( block 1050 ) is used to split the path into two separate paths : a first path 1080 with the best one arc ( backward ) extension and a second path 1090 with all remaining possible extensions .", "label": "", "metadata": {}, "score": "72.43599"}
{"text": "The average score is based on two values : a_priori_av_score(SegSNR ) and Sum_Score .Thus , the best template and the other templates uttering the same word as the best template are not used to generate Sum_Score .Specifically , the average score is defined as : . where w 1 is a weight for the a priori av_score .", "label": "", "metadata": {}, "score": "72.47417"}
{"text": "If desired , the gain adaptation can be performed separately , or not at all .The noise adapted DTW unit 60 compares a widened test utterance , which includes both speech and noise from both sides , with a noise adapted template .", "label": "", "metadata": {}, "score": "72.5358"}
{"text": "[ 0026 ] .Also , the Landell et al .article assumes that the noise acoustic features can be estimated from past observations of the noise , from before the speaker pushed the PTT button .For Landell et al . 's system , which was designed for an air force cockpit where the noise is fairly constant , this might be sufficient .", "label": "", "metadata": {}, "score": "72.55091"}
{"text": "Most HMM - based speech recognition systems are based on discrete HMMs utilizing vector quantization .A discrete HMM has a finite set of output symbols and the transition and output probabilities are based on discrete probability distribution functions ( pdfs ) .", "label": "", "metadata": {}, "score": "72.589066"}
{"text": "Most HMM - based speech recognition systems are based on discrete HMMs utilizing vector quantization .A discrete HMM has a finite set of output symbols and the transition and output probabilities are based on discrete probability distribution functions ( pdfs ) .", "label": "", "metadata": {}, "score": "72.589066"}
{"text": "Most HMM - based speech recognition systems are based on discrete HMMs utilizing vector quantization .A discrete HMM has a finite set of output symbols and the transition and output probabilities are based on discrete probability distribution functions ( pdfs ) .", "label": "", "metadata": {}, "score": "72.589066"}
{"text": "Alternatively , it may be a bank of twenty band pass filters .The twenty - one dimension vector signals produced by spectrum analyzer 50 may be adapted to remove background noise by an adaptive noise cancellation processor 52 .Noise cancellation processor 52 subtracts a noise vector N(t ) from the feature vector F(t ) input into the noise cancellation processor to produce an output feature vector F'(t ) .", "label": "", "metadata": {}, "score": "72.59875"}
{"text": "IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .PAMI-5 , No . 2 , pp .179 - 190 , Mar. 1983 .Bahl , L. R. , et al .\" Speaker - Independent Label Coding Apparatus \" .", "label": "", "metadata": {}, "score": "72.603134"}
{"text": "FIG .3 is a block diagram of a portion of an example of a speech recognition system according to the invention .In this embodiment of the invention , the system comprises a source vocabulary store 33 for storing the source language vocabulary .", "label": "", "metadata": {}, "score": "72.62588"}
{"text": "FIG .3 is a block diagram of a portion of an example of a speech recognition system according to the invention .In this embodiment of the invention , the system comprises a source vocabulary store 33 for storing the source language vocabulary .", "label": "", "metadata": {}, "score": "72.62588"}
{"text": "The method as defined in claim 1 , wherein upper and lower thresholds are generated for each state of each model stored in the voice recognition system .The method as defined in claim 4 , wherein the thresholds for each state of each model are applied when the allocation assignment algorithm is applied to each state for each model .", "label": "", "metadata": {}, "score": "72.69298"}
{"text": "means for displaying a source text comprising one or more words in a source language ; . an acoustic processor : for generating a sequence of coded representations of an utterance to be recognized , said utterance comprising one or more words in a target language different from the source language ; . means for generating a set of one or more speech hypotheses , each speech hypothesis comprising one or more words from the target language ; . means for generating an acoustic model of each speech hypothesis ; . means for generating an acoustic match score for each speech hypothesis , each acoustic match score comprising an estimate of the closeness of a match between the acoustic model of the speech hypothesis and the sequence of coded representations of the utterance ; . means for generating a translation match score for each speech hypothesis , each translation match score comprising an estimate of the probability of occurrence of the speech hypothesis given the occurrence , of the source text ; . means for generating a hypothesis score for each hypothesis , each hypothesis score comprising a combination of the acoustic match score and the translation match score for the hypothesis ; . means for storing a subset of one or more speech hypotheses , from the set of speech hypotheses , having the best hypothesis scores ; and . means for outputting at least one word of one or more of the speech hypotheses in the subset of speech hypotheses having the best hypothesis scores .", "label": "", "metadata": {}, "score": "72.69414"}
{"text": "[ 0023 ] .This occurs because the word endpoints are not determined prior to the recognition and therefore , the noise can not be estimated from speech - free segments .This inaccurate noise estimate leads to recognition errors .[ 0024 ] .", "label": "", "metadata": {}, "score": "72.76918"}
{"text": "A hypothesis score for each hypothesis comprises a combination of the acoustic match score and the translation match score .At least one word of one or more speech hypot .This invention was made with Government support under Contract Number N00014 - 91-C-0135 awarded by the office of Naval Research .", "label": "", "metadata": {}, "score": "72.77895"}
{"text": "A method as in claim 8 wherein the step of providing a weighting factor further comprises the steps of : . producing a parametric representation of the training data ; . providing a set of data points from the parametric representation of the training data , the data points representing the training data ; and .", "label": "", "metadata": {}, "score": "72.828415"}
{"text": "A method as in claim 8 wherein the step of providing a weighting factor further comprises the steps of : . producing a parametric representation of the training data ; . providing a set of data points from the parametric representation of the training data , the data points representing the training data ; and .", "label": "", "metadata": {}, "score": "72.828415"}
{"text": "A method as in claim 8 wherein the step of providing a weighting factor further comprises the steps of : . producing a parametric representation of the training data ; . providing a set of data points from the parametric representation of the training data , the data points representing the training data ; and .", "label": "", "metadata": {}, "score": "72.828415"}
{"text": "Other transformations can be employed for where the features are filterbank energies , as is discussed in the article \" Noise Masking in a Transform Domain \" , by B. A. Mellor and A. P. Varga , ICASSP ' 93 , 1993 , pp .", "label": "", "metadata": {}, "score": "72.83864"}
{"text": "The task of the conditional translation match score generator is to compute a conditional translation score P(S.vertline .T ) of a sequence S of source given a sequence T of target words .In one embodiment of a conditional translation match score generator , the probability of S given T is computed as # # EQU5 # # .", "label": "", "metadata": {}, "score": "72.88861"}
{"text": "L. Wang and T. Pavlidis , \" Direct gray - scale extraction of features for character recognition \" , IEEE Trans .on PAMI , vol .15 , October 1993 .C. Lee and L. Rabiner , \" A frame - synchronous network search algorithm for connected word recognition \" , IEEE Trans .", "label": "", "metadata": {}, "score": "72.88966"}
{"text": "Referring to FIG .1 , the speech recognition system comprises a display 10 for displaying a source text .The source text comprises one or more words in a source language , such as French .The source text may be provided to the display by , for example , a source text input device 12 such as a computer system .", "label": "", "metadata": {}, "score": "73.015335"}
{"text": "Referring to FIG .1 , the speech recognition system comprises a display 10 for displaying a source text .The source text comprises one or more words in a source language , such as French .The source text may be provided to the display by , for example , a source text input device 12 such as a computer system .", "label": "", "metadata": {}, "score": "73.015335"}
{"text": "FIG .1 shows a pixel map of a character together with a corresponding superstate and state diagram for a pseudo two - dimensional HMM ; .FIG .2 is a flow chart showing the operation of a computer programmed to compare a pixel map for a word image with pseudo two - dimensional HMMs in accordance with the invention ; .", "label": "", "metadata": {}, "score": "73.114716"}
{"text": "The phrases to be recognized are isolated words ; .The training phase is relatively noise - free , whereas the recognition needs to cope with additive environmental noise ; .The environmental noise is unknown to the system prior to the instant the user pushes a push to talk ( PTT ) button and starts speaking ; .", "label": "", "metadata": {}, "score": "73.12772"}
{"text": "This is specially important for gray level images with noise and blur , for its difficulty in make meaningful , efficient , and consistent segmentation ( grouping ) for different characters and samples by manual operation .For each character , the procedure is repeated for a few sample images .", "label": "", "metadata": {}, "score": "73.1334"}
{"text": "Automatic Determination of Pronunciation of Words From Their Spellings . \"IBM Technical Disclosure Bulletin , Volume 32 , No .10B , March 1990 , pages 19 - 23 ; and J. M. Lucassen , et al . \"An Information Theoretic Approach To The Automatic Determination of Phonemic Baseforms . \" Proceedings of the 1984 IEEE International Conference on Acoustic and Signal Processing , Vol . 3 , pages 42.5.1 - 42.5.4 , March 1984 . )", "label": "", "metadata": {}, "score": "73.15021"}
{"text": "Automatic Determination of Pronunciation of Words From Their Spellings . \"IBM Technical Disclosure Bulletin , Volume 32 , No .10B , March 1990 , pages 19 - 23 ; and J. M. Lucassen , et al . \"An Information Theoretic Approach To The Automatic Determination of Phonemic Baseforms . \" Proceedings of the 1984 IEEE International Conference on Acoustic and Signal Processing , Vol . 3 , pages 42.5.1 - 42.5.4 , March 1984 . )", "label": "", "metadata": {}, "score": "73.15021"}
{"text": "This application is a continuation application of U.S. patent application Ser .No .09/226,535 filed on Jan. 6 , 1999 .FIELD OF THE INVENTION .[0002 ] .The present invention relates to speech recognition generally and to speaker dependent recognition in the presence of noise , in particular .", "label": "", "metadata": {}, "score": "73.20087"}
{"text": "Although it is possible to quantitize continuous signals through codewords , there may be serious degradation associated with such quantization resulting in poor recognition accuracy .Recognition systems utilizing continuous density HMMs do not suffer from the inaccuracy associated with quantization distortion .", "label": "", "metadata": {}, "score": "73.32144"}
{"text": "Although it is possible to quantitize continuous signals through codewords , there may be serious degradation associated with such quantization resulting in poor recognition accuracy .Recognition systems utilizing continuous density HMMs do not suffer from the inaccuracy associated with quantization distortion .", "label": "", "metadata": {}, "score": "73.32144"}
{"text": "Although it is possible to quantitize continuous signals through codewords , there may be serious degradation associated with such quantization resulting in poor recognition accuracy .Recognition systems utilizing continuous density HMMs do not suffer from the inaccuracy associated with quantization distortion .", "label": "", "metadata": {}, "score": "73.32144"}
{"text": "The recognition engine determines which senone / state alignment best matches the feature vectors by utilizing an acoustic and language probability score .The acoustic probability score represents the likelihood that the senone alignment corresponds to the feature vectors and the language probability score indicates the likelihood of the utterance corresponding to the senone alignment occurring in the language .", "label": "", "metadata": {}, "score": "73.40418"}
{"text": "The recognition engine determines which senone / state alignment best matches the feature vectors by utilizing an acoustic and language probability score .The acoustic probability score represents the likelihood that the senone alignment corresponds to the feature vectors and the language probability score indicates the likelihood of the utterance corresponding to the senone alignment occurring in the language .", "label": "", "metadata": {}, "score": "73.40418"}
{"text": "The recognition engine determines which senone / state alignment best matches the feature vectors by utilizing an acoustic and language probability score .The acoustic probability score represents the likelihood that the senone alignment corresponds to the feature vectors and the language probability score indicates the likelihood of the utterance corresponding to the senone alignment occurring in the language .", "label": "", "metadata": {}, "score": "73.40418"}
{"text": "9 is a flow chart illustrating the voice recognition system .FIG .10 is a flow chart illustrating the setting of a penalty to a score in the voice recognition system .DETAILED DESCRIPTION OF THE DRAWINGS .This efficient method assigns a transition penalty to an out of state transition score if a lower threshold number of frames assigned to that state ( the state duration ) has not been met .", "label": "", "metadata": {}, "score": "73.44692"}
{"text": "The twenty - first dimension of the feature vector F'(t ) , representing the total amplitude or total power , is discarded .Each component i of the normalized feature vector X(t ) at time t may , for example , be given by the equation .", "label": "", "metadata": {}, "score": "73.501526"}
{"text": "Preferably , the feature extractor 16 performs spectral analysis to generate a sequence of feature vectors , each of which contains coefficients representing a spectra of the input speech signal .Methods for performing the spectral analysis are well - known in the art of signal processing and can include fast Fourier transforms ( FFT ) , linear predictive coding ( LPC ) , and cepstral coefficients , all of which can be utilized by feature extractor 16 .", "label": "", "metadata": {}, "score": "73.52885"}
{"text": "Preferably , the feature extractor 16 performs spectral analysis to generate a sequence of feature vectors , each of which contains coefficients representing a spectra of the input speech signal .Methods for performing the spectral analysis are well - known in the art of signal processing and can include fast Fourier transforms ( FFT ) , linear predictive coding ( LPC ) , and cepstral coefficients , all of which can be utilized by feature extractor 16 .", "label": "", "metadata": {}, "score": "73.52885"}
{"text": "Preferably , the feature extractor 16 performs spectral analysis to generate a sequence of feature vectors , each of which contains coefficients representing a spectra of the input speech signal .Methods for performing the spectral analysis are well - known in the art of signal processing and can include fast Fourier transforms ( FFT ) , linear predictive coding ( LPC ) , and cepstral coefficients , all of which can be utilized by feature extractor 16 .", "label": "", "metadata": {}, "score": "73.52885"}
{"text": "6 shows the final grouping ( cutting ) for image \" b \" .As can be seen in this figure , the character \" b \" can be represented by a PHMM with four superstates , and 3 , 3 , 5 , 3 states within each superstate , respectively .", "label": "", "metadata": {}, "score": "73.532104"}
{"text": "6 shows the final grouping ( cutting ) for image \" b \" .As can be seen in this figure , the character \" b \" can be represented by a PHMM with four superstates , and 3 , 3 , 5 , 3 states within each superstate , respectively .", "label": "", "metadata": {}, "score": "73.532104"}
{"text": "5 for a definition of the topline 500 and the baseline 502 ) .The actual value of the second component is # # EQU2 # # It is then quantized into 50 levels .Obviously , the value could be negative for part of g , p , q , . . .", "label": "", "metadata": {}, "score": "73.61563"}
{"text": "8 , the growth of the backward path is constituted by sequential backward arc extension from terminal node T headed towards the initial node I. When doing the backward one - arc extension of the partial path , duration penalty is re - evaluated to reflect the exact duration fitness .", "label": "", "metadata": {}, "score": "73.69707"}
{"text": "Spectral analysis may be performed every ten milliseconds to divide the input speech signal into a feature vector which represents twenty - five milliseconds of the utterance .However , this invention is not limited to using feature vectors that represent twenty - five milliseconds of the utterance .", "label": "", "metadata": {}, "score": "73.70412"}
{"text": "Spectral analysis may be performed every ten milliseconds to divide the input speech signal into a feature vector which represents twenty - five milliseconds of the utterance .However , this invention is not limited to using feature vectors that represent twenty - five milliseconds of the utterance .", "label": "", "metadata": {}, "score": "73.70412"}
{"text": "Spectral analysis may be performed every ten milliseconds to divide the input speech signal into a feature vector which represents twenty - five milliseconds of the utterance .However , this invention is not limited to using feature vectors that represent twenty - five milliseconds of the utterance .", "label": "", "metadata": {}, "score": "73.70412"}
{"text": "For example , the linguistic expression can be used as input into another program or processor for further processing or may be stored .FIGS . 2 - 6 are flow charts that illustrate the steps performed in the training phase of the system where the parameters of the HMMs and senones are estimated and the weighting factors are calculated .", "label": "", "metadata": {}, "score": "73.70756"}
{"text": "For example , the linguistic expression can be used as input into another program or processor for further processing or may be stored .FIGS . 2 - 6 are flow charts that illustrate the steps performed in the training phase of the system where the parameters of the HMMs and senones are estimated and the weighting factors are calculated .", "label": "", "metadata": {}, "score": "73.70756"}
{"text": "For example , the linguistic expression can be used as input into another program or processor for further processing or may be stored .FIGS . 2 - 6 are flow charts that illustrate the steps performed in the training phase of the system where the parameters of the HMMs and senones are estimated and the weighting factors are calculated .", "label": "", "metadata": {}, "score": "73.70756"}
{"text": "In most conditions the novel state duration method gives significantly improved recognition results over both having no state duration penalties and using the HMM / BSD technique .The HMM technique of the prior art having a bounded , hard state duration , did not work well with a recognition system using a small number , such as two , training utterances .", "label": "", "metadata": {}, "score": "73.79472"}
{"text": "It is noted that this operation produces the test peak energy R 0 t of the widened test token .A similar operation is performed , typically off - line , for each reference template , producing the reference peak energies R 0r .", "label": "", "metadata": {}, "score": "73.79504"}
{"text": "It is noted that this operation produces the test peak energy R 0 t of the widened test token .A similar operation is performed , typically off - line , for each reference template , producing the reference peak energies R 0r .", "label": "", "metadata": {}, "score": "73.79504"}
{"text": "The \" detailed \" acoustic match score estimates the closeness of a match between an acoustic detailed match model of a word and the sequence of coded representations of the utterance .Still referring to FIG .1 , the speech recognition system further comprises a translation match score generator 26 for generating a translation match score for each speech hypothesis .", "label": "", "metadata": {}, "score": "73.89913"}
{"text": "The average score is based on two values : a_priori_av_score(SegSNR ) and Sum_Score .Thus , the best template and the other templates uttering the same word as the best template are not used to generate Sum_Score .Specifically , the average score is defined as : . where w1 is a weight for the a_priori av_score .", "label": "", "metadata": {}, "score": "73.89928"}
{"text": "The number sequence underneath the character image represents the possible superstate transition sequence to generate those corresponding columns as their observations .In other words , superstates generates columns as their observation units .Within each superstate , states of the 1D HMM corresponds to those pixels within each columns as in FIG .", "label": "", "metadata": {}, "score": "73.93161"}
{"text": "[ 0047 ] .Referring back to FIG .3 , the wide and noise / gain adapted templates are provided to feature transformer 58 B for converting to the cepstral features required by the DTW unit 60 .The latter compares the representation of the wide token with the representation of each wide and noise / gain adapted template and provides a score for each comparison .", "label": "", "metadata": {}, "score": "74.00184"}
{"text": "In one embodiment this probability is estimated as # # EQU4 # # where q is an estimate of the unigram probability of the end - of - sentence marker .The conditional translation match score generator will now be described .", "label": "", "metadata": {}, "score": "74.01933"}
{"text": "No .468,546 , filed Jan. 23 , 1990 .Bahl , L. R. , et al . \"Automatic Determination of Pronunciation of Words From Their Spellings . \"IBM Technical Disclosure Bulletin , vol .32 , No .10B Mar. 1990 , pp .", "label": "", "metadata": {}, "score": "74.06396"}
{"text": "The normalization constants . lambda . sub.5 and . lambda . sub.6 are chosen so that the updated quantities are conditional probabilities .( See , U.S. patent application Ser .No .736,278 , filed on Jul. 25 , 1991 , by Peter F. Brown et al entitled \" Method and System For Natural Language Translation . \" )", "label": "", "metadata": {}, "score": "74.11597"}
{"text": "Degraded gray - scale document recognition using pseudo two - dimensional hidden Markov models and N - best hypotheses US 5754695 A .Abstract .The present invention provides a method for recognizing connected and degraded text embedded in a gray - scale image .", "label": "", "metadata": {}, "score": "74.1839"}
{"text": "Conf on Pattern Recognition , 1992 , U. S. patent application Ser .No .07/813,225 , filed Dec. 23 , 1991 , and U. S. patent application Ser .No .07/981,028 filed Nov. 24 , 1992 .State - of - the - art OCR algorithms are generally based on bi - level images ( black characters on white background ) .", "label": "", "metadata": {}, "score": "74.241066"}
{"text": "For detailed description of the PHMM , please refer to reference 6 !A PHMM can be completely specified by a set of parameters with the following description .For the clearness of the expression , we omit the model index here , with an understanding that different PHMM will have the same set of parameters with different values .", "label": "", "metadata": {}, "score": "74.24349"}
{"text": "The segmental SNR SegSNR is the average log - energy of the test utterance between the DTW - derived endpoints less the average log - energy of the test utterance outside the endpoints .[ 0069 ] .The accept / reject algorithm accepts the best match result if the normalized score Norm_Score is less than a threshold level Th .", "label": "", "metadata": {}, "score": "74.299324"}
{"text": "For example , the updated estimates of parameters \u03c0 i , a ij and b i would be : # # EQU6 # # .After all the tokens have been processed ( step 356 ) , if the parameters have converged , that is , such overall measure has reached essentially its maximum value , the training procedure is complete ( step 357 ) .", "label": "", "metadata": {}, "score": "74.302864"}
{"text": "O xy thus has five distinct component values .Pseudo two - dimensional HMMs can be thought of as elastic templates because they allow for distortion in two dimensions , thus significantly increasing the robustness of the model .For example , vertical distortion , as often occurs in facsimile transmission , can be accommodated by the methods of the invention .", "label": "", "metadata": {}, "score": "74.3612"}
{"text": "The method of .claim 3 wherein adjusting comprises : . adding the noise qualities of the widened token to a blank frame of the modified reference template ; and . adding the noise qualities of the widened token to a speech frame of the modified reference template .", "label": "", "metadata": {}, "score": "74.36592"}
{"text": "The method of .claim 3 wherein adjusting comprises : . adding the noise qualities of the widened token to a blank frame of the modified reference template ; and . adding the noise qualities of the widened token to a speech frame of the modified reference template .", "label": "", "metadata": {}, "score": "74.36592"}
{"text": "One technique is described in U.S. Pat .No .5,778,342 to Erell et al . .The problem of inaccurate endpoints has been less covered in the art .One solution was given in the form of relaxed - endpoint DTW and is described in the following : .", "label": "", "metadata": {}, "score": "74.374916"}
{"text": "Clean up word duplicated paths in the stack ( block 1113 ) .Loop back to block 1105 until all M paths are \" completed \" ( block 1115 ) .The output is the top N completed paths ( block 1117 ) .", "label": "", "metadata": {}, "score": "74.37607"}
{"text": "in the logarithmic domain , where F ' .The normalized twenty dimension feature vector X(t ) may be further processed by an adaptive labeler 60 to adapt to variations in pronunciation of speech sounds .An adapted twenty dimension feature vector X'(t ) is generated by subtracting a twenty dimension adaptation vector A(t ) from the twenty dimension feature vector X(t ) provided to the input of the adaptive labeler 60 .", "label": "", "metadata": {}, "score": "74.43004"}
{"text": "Other methods known in the art can be used to estimate the parameters for HMMs ; for example , the Baum - Welch reestimation formulas and the generalized probabilistic descent method .In a conventional HMM , the probability of remaining in a state for a duration di ( the probability density ) is an exponential function expressed as . sup .", "label": "", "metadata": {}, "score": "74.549355"}
{"text": "( a ) subsampling the binary document to generate a gray - scale image by mapping binary n\u00d7n space to gray - scale m\u00d7m space , wherein m is greater than n , and m and n represent the number of dimensions in a multidimensional space ; .", "label": "", "metadata": {}, "score": "74.57593"}
{"text": "3 for definition of topline and baseline ) .The actual value of the second component is # # EQU11 # # It is then quantized into 50 levels .Obviously , the value could be negative for part of g , p , q , the location of each pixel in the column regardless of the point size of the printed characters .", "label": "", "metadata": {}, "score": "74.64342"}
{"text": "N is typically 10 .Mathematically this is stated as : .The noise energy R 0n is then determined from the resultant noise feature .The peak energy estimation is determined in a similar manner but considering the frames with the highest energy .", "label": "", "metadata": {}, "score": "74.662285"}
{"text": "An adapted twenty dimension feature vector X'(t ) is generated by subtracting a twenty dimension adaptation vector A(t ) from the twenty dimension feature vector X(t ) provided to the input of the adaptive labeler 60 .The adaptation vector A(t ) at time t may , for example , be given by the formula .", "label": "", "metadata": {}, "score": "74.67187"}
{"text": "14 is a flowchart setting forth procedures for a superstate level Viterbit search ; .FIG .15 is a flowchart setting forth a procedure for backtracking a path ; .FIGS .16 and 17 are flowcharts setting forth various training procedures ; .", "label": "", "metadata": {}, "score": "74.768364"}
{"text": "The terminal node of a path is located at the last superstate of a model , at the last frame of the observation .According to this definition , we will have one terminal node for every model at every level of the structure , as long as it has been visited by paths .", "label": "", "metadata": {}, "score": "74.77095"}
{"text": "The above - described techniques have been experimentally verified .In the experiments , all images in the database were computer - simulated word images .The images were based upon a clean word image to which Gaussian noise , with standard deviation \u03c3 n as the parameter , was added .", "label": "", "metadata": {}, "score": "74.86978"}
{"text": "The resulting adjusted score from either step 1022 or step 1026 is stored as indicated in step 1027 .The processor 108 then selects one of a self loop or a transition depending on which transition type had the higher adjusted score as indicated in step 1028 .", "label": "", "metadata": {}, "score": "74.925735"}
{"text": "The higher of the two probabilities is chosen as the winner .This probability is then added to the observation probability o i ( \u0192 m ) , where \u0192 m is a vector of m features , and that probability is stored as C i ( m+1 ) .", "label": "", "metadata": {}, "score": "75.007355"}
{"text": "28 is a flowchart setting forth a procedure for obtaining a first noncompleted path from the path stack ; .FIG .29 is a flowchart illustrating a procedure for path splitting ; and .FIG .30 is a flowchart setting forth a procedure for estimation of all possible backward one - arc extensions .", "label": "", "metadata": {}, "score": "75.06652"}
{"text": "The most preferable alternative is to use an intermediate value that compromises between the two contradicting demands .[0075 ] .The length correction length colt is a piecewise linear function , shown in FIG .8C to which reference is now briefly made , of test - utterance length , controlled by parameters .", "label": "", "metadata": {}, "score": "75.08428"}
{"text": "13 as an example , there are 2 outlets available at level 1 , one at model \" a \" , the other at \" c \" , both at frame 4 .This path then brings the possible recognition of \" a \" from frame 0 to 4 , and another \" a \" from 5 and up .", "label": "", "metadata": {}, "score": "75.16363"}
{"text": "It is another Viterbi search between 1D HMM of the 2nd superstate and the pixels within the first column .It gives the likelihood of this column generated by this superstate ( or by this 1D HMM ) .This optimal path likelihood score of the state level Viterbi search is exactly the likelihood score b 2 ( O 1 ) needed in the previous equation .", "label": "", "metadata": {}, "score": "75.20091"}
{"text": "L. Rabiner , \" A tutorial on hidden markov models and selected applications in speech recognition \" , Proceedings of the IEEE , Vol .77 , pp .257 - 286 , February 1989 .L. Rabiner and B. Juang , Fundamental of Speech Recognition , Prentice Hall , 1993 .", "label": "", "metadata": {}, "score": "75.241455"}
{"text": "a combined score generator which uses the language match score and the conditional translation match score to produce an estimate of a joint probability P(S , T.cndot . )The combined match score generator will now be described .In the prior art , language match scores and conditional translation match scores are combined only when the words of S are generated from the words T and no other words .", "label": "", "metadata": {}, "score": "75.27634"}
{"text": "3 is a flow chart showing the procedure for creating a pseudo two - dimensional HMM ; .FIG .4 is a diagram of a convolution kernel used to produce an observation vector ; .FIG .5 is a diagram showing the topline and the baseline for an illustrative textual image ; .", "label": "", "metadata": {}, "score": "75.33358"}
{"text": "60 72 , Sep./Oct .1993 .Recognition of Isolated Digits Using Hidden markov Models with Continuous Mixture Densities by L. R. Rabiner et al . , AT&T Tech .J. vol .64 , pp .1211 1222 , Jul./Aug .", "label": "", "metadata": {}, "score": "75.436066"}
{"text": "Two types of HMMs can be utilized .A context - dependent HMM can be used to model a phoneme with its left and right phonemic contexts .This type of model captures the contextual dependencies which are usually present in word modeling .", "label": "", "metadata": {}, "score": "75.43891"}
{"text": "Two types of HMMs can be utilized .A context - dependent HMM can be used to model a phoneme with its left and right phonemic contexts .This type of model captures the contextual dependencies which are usually present in word modeling .", "label": "", "metadata": {}, "score": "75.43891"}
{"text": "Two types of HMMs can be utilized .A context - dependent HMM can be used to model a phoneme with its left and right phonemic contexts .This type of model captures the contextual dependencies which are usually present in word modeling .", "label": "", "metadata": {}, "score": "75.43891"}
{"text": "In other words , the kernel of FIG .4 effectively convolves an entire image comprised of an array of pixels .The purpose of convolution is to reduce the effect of randomness on the grey level value caused by noise .", "label": "", "metadata": {}, "score": "75.52762"}
{"text": "and where K 1 , K 2 , and K 3 are fixed parameters of the auditory model .For each centisecond time interval , the output of the auditory model 62 is a modified twenty dimension feature vector signal .This feature vector is augmented by a twenty - first dimension having a value equal to the square root of the sum of the squares of the values of the other twenty dimensions .", "label": "", "metadata": {}, "score": "75.53104"}
{"text": "LOOP CONTROL ( block 1427 ) .LOOP CONTROL ( block 1429 ) .build up superstate level path extension if applicable ( block 1431 ) .LOOP CONTROL ( block 1433 ) .LOOP CONTROL ( block 1435 ) .LOOP CONTROL ( block 1437 ) .", "label": "", "metadata": {}, "score": "75.535446"}
{"text": "It is typically the case that two - word utterances have more variability in their pronunciation ( e.g. the duration of the pause in between may vary significantly ) , so that the DTW or HMM matching scores typically differ than the ones encountered with one - word utterances .", "label": "", "metadata": {}, "score": "75.5877"}
{"text": "It is typically the case that two - word utterances have more variability in their pronunciation ( e. g. the duration of the pause in between may vary significantly ) , so that the DTW or HMM matching scores typically differ than the ones encountered with one - word utterances .", "label": "", "metadata": {}, "score": "75.5877"}
{"text": "FIG .11 is a state sequence diagram showing the correspondence between a 1-dimensional HMM and a string of pixels ; .FIG .12 is a path mpa of a nest Viterbi search ; .FIG .13 sets forth the data structures generated by a level building algorithm which uses a nest Viterbi search ; .", "label": "", "metadata": {}, "score": "75.65114"}
{"text": "A speech recognition method as claimed in claim 35 , characterized in that the means for measuring the value of at least one feature of in utterance comprises a microphone .Description .BACKGROUND OF THE INVENTION .The invention relates to automatic speech recognition .", "label": "", "metadata": {}, "score": "75.658264"}
{"text": "1B to which reference is now made , the DTW paths are not constrained to start or end at the exact endpoints of the test and reference utterances .Instead , paths can start or end within a given range ( delta and Qmax_delta ) of the corners .", "label": "", "metadata": {}, "score": "75.659454"}
{"text": "2 is a block diagram of a portion of another example of a speech recognition system according to the invention .FIG .3 is a block diagram of a portion of another example of a speech recognition system according to the invention .", "label": "", "metadata": {}, "score": "75.66307"}
{"text": "2 is a block diagram of a portion of another example of a speech recognition system according to the invention .FIG .3 is a block diagram of a portion of another example of a speech recognition system according to the invention .", "label": "", "metadata": {}, "score": "75.66307"}
{"text": "( Although a speaker may try to pronounce words as concatenated sequences of phones , the speaker 's articulator can not move instantaneously to produce unaffected phones .As a result , a phone is strongly inverted by the phone that precedes it and the phone that follows it in a word .", "label": "", "metadata": {}, "score": "75.78473"}
{"text": "( Although a speaker may try to pronounce words as concatenated sequences of phones , the speaker 's articulator can not move instantaneously to produce unaffected phones .As a result , a phone is strongly inverted by the phone that precedes it and the phone that follows it in a word .", "label": "", "metadata": {}, "score": "75.78473"}
{"text": "( Although a speaker may try to pronounce words as concatenated sequences of phones , the speaker 's articulator can not move instantaneously to produce unaffected phones .As a result , a phone is strongly inverted by the phone that precedes it and the phone that follows it in a word .", "label": "", "metadata": {}, "score": "75.78473"}
{"text": "The probability of a same - state transition is determined as the sum of the current state 's cumulative probability with the probability of a self - transition as : .C i ( m ) + Ps i ( d i ) .", "label": "", "metadata": {}, "score": "75.78793"}
{"text": "The utterance comprises , for example , a series of one or more words in a target language , such as English , different from the source language .A speech hypothesis generator 16 generates a set of one or more speech hypotheses .", "label": "", "metadata": {}, "score": "75.833534"}
{"text": "The utterance comprises , for example , a series of one or more words in a target language , such as English , different from the source language .A speech hypothesis generator 16 generates a set of one or more speech hypotheses .", "label": "", "metadata": {}, "score": "75.833534"}
{"text": "The penalty this path has already been paying at F for state 2 : according to the originally preferred path 2 : f 2 ( 2 ) ( for staying in F , G ) .The correction for duration penalty should be : .", "label": "", "metadata": {}, "score": "75.86162"}
{"text": "Finally , after the last row , the ultimate likelihood score that the HMM represents the text element being compared is the last probability evaluated for the final superstate ( step 307 ) .Again , an optional postprocessing step ( step 308 ) is indicated for refinements to be explained .", "label": "", "metadata": {}, "score": "75.89206"}
{"text": "Utterance U 1 represents the signal stored the first time that a speaker says a particular word during training .Utterance U 2 represents the signal the second time a speaker says a particular word during training .In the illustrated example , utterance U 1 is of a different length than utterance U 2 .", "label": "", "metadata": {}, "score": "75.925865"}
{"text": "A system as in claim 20 wherein the weighting factor is derived by applying a deleted interpolation technique to the amount of the training data .A system as in claim 20 wherein the training device further comprises a parametric generator to generate a parametric representation of the training data ; and .", "label": "", "metadata": {}, "score": "75.97669"}
{"text": "A system as in claim 20 wherein the weighting factor is derived by applying a deleted interpolation technique to the amount of the training data .A system as in claim 20 wherein the training device further comprises a parametric generator to generate a parametric representation of the training data ; and .", "label": "", "metadata": {}, "score": "75.97669"}
{"text": "A system as in claim 20 wherein the weighting factor is derived by applying a deleted interpolation technique to the amount of the training data .A system as in claim 20 wherein the training device further comprises a parametric generator to generate a parametric representation of the training data ; and .", "label": "", "metadata": {}, "score": "75.97669"}
{"text": "( See , U.S. patent application Ser .No .736,278 , filed on Jul. 25 , 1991 , by Peter F. Brown et al entitled \" Method and System For Natural Language Translation . \" )The language match score generator will now be described .", "label": "", "metadata": {}, "score": "76.1103"}
{"text": "The role of the translation match score generator is to compute a translation match score Score(S , T.cndot . ) that a finite sequence S of source words is the translation of a sequence of target words beginning with the finite sequence T. Here and in the following , T.cndot . will denote the set of all complete target sentences that begin with the sequence of target words T. A complete sentence is a sequence that ends in a special end - of - sentence marker .", "label": "", "metadata": {}, "score": "76.16792"}
{"text": "Therefore , the parameters set ( \u03c3 n , \u03c3 b ) specifies how much the image quality has deteriorated .Some normalization process may be employed in generating the image database , such that the drifting of the means of gray - level for the background and foreground pixels can be limited .", "label": "", "metadata": {}, "score": "76.18776"}
{"text": "Each word in the source text has a spelling comprising one or more letters .Each letter is either upper case or lower case .The acoustic model generator produces an acoustic model of each word in the source text which is not in the source vocabulary , and which has an upper case first letter .", "label": "", "metadata": {}, "score": "76.21647"}
{"text": "Each word in the source text has a spelling comprising one or more letters .Each letter is either upper case or lower case .The acoustic model generator produces an acoustic model of each word in the source text which is not in the source vocabulary , and which has an upper case first letter .", "label": "", "metadata": {}, "score": "76.21647"}
{"text": "The method of claim 2 wherein said superstates represent substantially vertical slices through the text elements represented by said hidden Markov models and each comparing step comprises the steps of : .( a ) combining each of a plurality of respective columns in said pixel map with a corresponding superstate ; .", "label": "", "metadata": {}, "score": "76.23537"}
{"text": "The Viterbi search progresses from left to right , until the last frame is finished .The overall likelihood of the image being generated by the PHMM is obtained .The Level Building Algorithm for PHMM .For connected character recognition , a word image has to be matched with all possible combinations of PHMMs from all characters .", "label": "", "metadata": {}, "score": "76.276825"}
{"text": "deriving the weighting factor from an application of deleted interpolation to the parametric representation of the training data .A method as in claim 13 wherein the step of providing a weighting factor further comprises the steps of : . producing a parametric representation of the training data ; . generating a set of data points from the parametric representation of the training data ; and .", "label": "", "metadata": {}, "score": "76.52715"}
{"text": "deriving the weighting factor from an application of deleted interpolation to the parametric representation of the training data .A method as in claim 13 wherein the step of providing a weighting factor further comprises the steps of : . producing a parametric representation of the training data ; . generating a set of data points from the parametric representation of the training data ; and .", "label": "", "metadata": {}, "score": "76.52715"}
{"text": "deriving the weighting factor from an application of deleted interpolation to the parametric representation of the training data .A method as in claim 13 wherein the step of providing a weighting factor further comprises the steps of : . producing a parametric representation of the training data ; . generating a set of data points from the parametric representation of the training data ; and .", "label": "", "metadata": {}, "score": "76.52715"}
{"text": "The N - best hypotheses approach has proven to be very efficient in improving the recognition performance .More improvement is expected with larger N. This further confirm its superiority in efficiency .The gray - level approach brings big performance improvement with almost no computation increase and small memory space increase .", "label": "", "metadata": {}, "score": "76.54286"}
{"text": "A method as in claim 8 wherein the step of providing a weighting factor further comprises the step of generating the weighting factor by using a deleted interpolation technique on the amount of training data .A method as in claim 8 wherein the step of providing a weighting factor further comprises the steps of : . producing a parametric representation of the training data ; and .", "label": "", "metadata": {}, "score": "76.57631"}
{"text": "A method as in claim 8 wherein the step of providing a weighting factor further comprises the step of generating the weighting factor by using a deleted interpolation technique on the amount of training data .A method as in claim 8 wherein the step of providing a weighting factor further comprises the steps of : . producing a parametric representation of the training data ; and .", "label": "", "metadata": {}, "score": "76.57631"}
{"text": "A method as in claim 8 wherein the step of providing a weighting factor further comprises the step of generating the weighting factor by using a deleted interpolation technique on the amount of training data .A method as in claim 8 wherein the step of providing a weighting factor further comprises the steps of : . producing a parametric representation of the training data ; and .", "label": "", "metadata": {}, "score": "76.57631"}
{"text": "The probability P(T ) of occurrence of the target word sequence may be approximated by the product of n - gram probabilities for all n - grams in each string .That is , the probability of a sequence of words may be approximated by the product of the conditional probabilities of each word in the string , given the occurrence of the n-1 words ( or absence of words ) preceding each word .", "label": "", "metadata": {}, "score": "76.608986"}
{"text": "claim 7 wherein the gain and noise adapter is able to add to the modified reference template noise qualities of the widened token .The apparatus of .claim 6 , wherein the peak energy estimator comprises a peak energy averager to average energy levels of high energy frames of said widened token .", "label": "", "metadata": {}, "score": "76.61461"}
{"text": "claim 7 wherein the gain and noise adapter is able to add to the modified reference template noise qualities of the widened token .The apparatus of .claim 6 , wherein the peak energy estimator comprises a peak energy averager to average energy levels of high energy frames of said widened token .", "label": "", "metadata": {}, "score": "76.61461"}
{"text": "For example , the invention may be used to recognize an utterance in English of a translation of a sentence in French .In one study , it was found that the efficiency of a human translator who dictates a translation in one language corresponding to source text in another language , is greater than the efficiency of a human translator who writes or types a translation .", "label": "", "metadata": {}, "score": "76.64204"}
{"text": "When the backward path reaches node F , the \" current state \" is state 2 , and a state transition out of this state is expected after F , according to the forward path map .Therefore , DP should be f 2 ( 2 ) , for staying over G , F in state 2 .", "label": "", "metadata": {}, "score": "76.65463"}
{"text": "Briefly , the technique of deleted interpolation partitions the training data into two distinct sets .One set is used to estimate the parameters of the model and a second set is used to determine the weighting factor which indicates how well the output pdf can predict unseen training data .", "label": "", "metadata": {}, "score": "76.68619"}
{"text": "Briefly , the technique of deleted interpolation partitions the training data into two distinct sets .One set is used to estimate the parameters of the model and a second set is used to determine the weighting factor which indicates how well the output pdf can predict unseen training data .", "label": "", "metadata": {}, "score": "76.68619"}
{"text": "Briefly , the technique of deleted interpolation partitions the training data into two distinct sets .One set is used to estimate the parameters of the model and a second set is used to determine the weighting factor which indicates how well the output pdf can predict unseen training data .", "label": "", "metadata": {}, "score": "76.68619"}
{"text": "A speech recognition method as claimed in claim 35 , characterized in that the means for measuring the value of at least one feature of in utterance comprises a microphone .Description .This invention was made with Government support under Contract Number N00014 - 91-C-0135 awarded by the office of Naval Research .", "label": "", "metadata": {}, "score": "76.73065"}
{"text": "Each model 's robustness is related to the amount of training data used to estimate its parameters which also enables it to predict data not present in the training data .The combination of the two models provides a more robust estimate benefiting from the training of both models .", "label": "", "metadata": {}, "score": "76.75557"}
{"text": "Each model 's robustness is related to the amount of training data used to estimate its parameters which also enables it to predict data not present in the training data .The combination of the two models provides a more robust estimate benefiting from the training of both models .", "label": "", "metadata": {}, "score": "76.75557"}
{"text": "Each model 's robustness is related to the amount of training data used to estimate its parameters which also enables it to predict data not present in the training data .The combination of the two models provides a more robust estimate benefiting from the training of both models .", "label": "", "metadata": {}, "score": "76.75557"}
{"text": "3 until the process converges and the current average value \u03bb new is stored in the lambda table 26 for the particular context - dependent senone .In the second alternative embodiment of the calculation of the weighting factors , a select number of data points are used , which are randomly generated from a parametric representation for the senone .", "label": "", "metadata": {}, "score": "76.79254"}
{"text": "3 until the process converges and the current average value \u03bb new is stored in the lambda table 26 for the particular context - dependent senone .In the second alternative embodiment of the calculation of the weighting factors , a select number of data points are used , which are randomly generated from a parametric representation for the senone .", "label": "", "metadata": {}, "score": "76.79254"}
{"text": "3 until the process converges and the current average value \u03bb new is stored in the lambda table 26 for the particular context - dependent senone .In the second alternative embodiment of the calculation of the weighting factors , a select number of data points are used , which are randomly generated from a parametric representation for the senone .", "label": "", "metadata": {}, "score": "76.79254"}
{"text": "3 , at the completion of the K iterations , the process proceeds to calculate the average value for \u03bb new in step 76 in accord with equation ( 3 ) above .The process continues as described above with reference to FIG .", "label": "", "metadata": {}, "score": "76.83104"}
{"text": "3 , at the completion of the K iterations , the process proceeds to calculate the average value for \u03bb new in step 76 in accord with equation ( 3 ) above .The process continues as described above with reference to FIG .", "label": "", "metadata": {}, "score": "76.83104"}
{"text": "3 , at the completion of the K iterations , the process proceeds to calculate the average value for \u03bb new in step 76 in accord with equation ( 3 ) above .The process continues as described above with reference to FIG .", "label": "", "metadata": {}, "score": "76.83104"}
{"text": "[ 0028 ] .It is generally difficult to achieve efficient rejection of out - of - vocabulary or mispronounced utterances , without sacrificing also some rejection of in - vocabulary , well - pronounced utterances .The problem is difficult because of the high variability in the values of the best - match scores .", "label": "", "metadata": {}, "score": "76.90472"}
{"text": "After a model has been created and stored in memory 208 , the processor 108 calculates the upper duration threshold for each state of the model as indicated in same 702 .The upper duration threshold calculated for each state is stored as indicated in step 704 .", "label": "", "metadata": {}, "score": "76.9698"}
{"text": "In the first two cases , the utterance contains one word , whereas in the second case it contains two words .It is typically the case that two - word utterances have more variability in their pronunciation ( e.g. the duration of the pause in between may vary significantly ) , so that the DTW or HMM matching scores typically differ than the ones encountered with one - word utterances .", "label": "", "metadata": {}, "score": "77.02699"}
{"text": "A speech recognition system as claimed in claim 1 , characterized in that the output means comprises a loudspeaker .A speech recognition system as claimed in claim 1 , characterized in that the output means comprises a speech synthesizer .A speech recognition system as claimed in claim 1 , characterized in that the means for storing speech hypotheses comprises readable computer memory .", "label": "", "metadata": {}, "score": "77.06094"}
{"text": "A speech recognition system as claimed in claim 1 , characterized in that the output means comprises a loudspeaker .A speech recognition system as claimed in claim 1 , characterized in that the output means comprises a speech synthesizer .A speech recognition system as claimed in claim 1 , characterized in that the means for storing speech hypotheses comprises readable computer memory .", "label": "", "metadata": {}, "score": "77.06094"}
{"text": "6 as an example .In our experiment , we use the first strategy , with the number of final blocks decided by inspecting characters topology , and given before grouping process starts .After those vertical blocks of columns are formed , we then go to each vertical block to group rows within the block by using the same process as we did for columns .", "label": "", "metadata": {}, "score": "77.08061"}
{"text": "Once convergence is achieved , .the lower and upper state duration thresholds Dmin i and Dmax i for every state must be computed , as indicated in step 610 .FIG .7 illustrates in greater detail the calculation of the duration thresholds involved at step 610 ( FIG .", "label": "", "metadata": {}, "score": "77.17502"}
{"text": "If the self loop had the higher adjusted score , then the self loop transition type will be stored in step 1030 .Those skilled in that art will recognize that the out of state transition penalty will be applied to a state skip score just as it is applied to the step score if state skips are allowed .", "label": "", "metadata": {}, "score": "77.18238"}
{"text": "S ) , while in another embodiment the translation match score is an estimate of a joint probability P(S , T.cndot . )In the latter embodiment , the translation match score generator includes three components : . a language match score generator which computes an estimate P(T ) of the prior probability of a target word sequence T ; . a conditional translation match score generator which computes an estimate P(S.vertline .", "label": "", "metadata": {}, "score": "77.258194"}
{"text": "The resulting gray level value of the pixel ( from 0 to 255 ) is then quantized into 100 levels .The second component is the relative position of each pixel in the columns .We assume that after layout analysis of the document , the position of baseline and the difference between topline and baseline are known .", "label": "", "metadata": {}, "score": "77.3949"}
{"text": "Although the present invention finds particular application in portable wireless devices such as cellular radiotelephones , the invention could be applied to any device employing speech recognition , including pagers , electronic organizers , computers , and telephony equipment .The invention should be limited only by the following claims .", "label": "", "metadata": {}, "score": "77.44566"}
{"text": "From the source text and from the translations , candidate word generator 20 generates a set of candidate words consisting solely of words in the target language which are partial or full translations of words in the source text .The set of candidate words is provided to speech hypothesis generator 16 .", "label": "", "metadata": {}, "score": "77.44849"}
{"text": "For example , this gray - level information may be incorporated into the optical scanner output due to the blurring effect caused by point spread function of the scanner , or if the scanning process is done in a badly - illuminated environment .", "label": "", "metadata": {}, "score": "77.4796"}
{"text": "Hidden Markov Model Based Optical Character Recognition in the Presence of Deterministic Transformations , by O. E. Agazzi et al . , Pattern Recognition , vol .26 , No . 12 , 1993 .Keyword Spotting in Poorly Printed Documents Using Pseudo 2d Hidden Markov Models , by S. Kuo et al . , IEEE Trans .", "label": "", "metadata": {}, "score": "77.50359"}
{"text": "The length correction length colt is a piecewise linear function , shown in FIG .8C to which reference is now briefly made , of test - utterance length , controlled by parameters .The parameters are determined experimentally from the large speech database used for determining all of FIG .", "label": "", "metadata": {}, "score": "77.513565"}
{"text": "Direct gray scale extraction of features for character recognition , by L. Wang et al . , IEEE Trans .on PAMI , vol .15 , Oct. 1993 .Document Image Analysis by S. Srihari et al . , Proceedings of the 8th International Conference on Pattern Recognition , Paris , Oct. 1986 .", "label": "", "metadata": {}, "score": "77.523994"}
{"text": "320 - 321 .Brown , P. F. et al . \"Method and Apparatus For Translating A Series of Words From One Language to Another \" .U.S. patent application Ser .No .736,278 , Filed on Jul. 25 , 1991 .", "label": "", "metadata": {}, "score": "77.552414"}
{"text": "A lower and upper state duration threshold is stored for each state of each HMM in memory 208 .Thus , every model stored in memory has a respective upper and lower threshold for each state .However , this represents only a modest increase in memory requirements and gives a considerable improvement in recognition performance .", "label": "", "metadata": {}, "score": "77.64285"}
{"text": "The translation match score generator 26 will now be described .A complete sentence is a sequence that ends in a special end - of - sentence marker .In the latter embodiment , the translation match score generator includes three components : . a language match score generator which computes an estimate P(T ) of the prior probability of a target word sequence T ; .", "label": "", "metadata": {}, "score": "77.65454"}
{"text": "In step 48 of FIG .2 , the parameters for the senone , context - dependent HMM , and the context - independent HMM are estimated .The training phase of a HMM consists of estimating these parameters using the training data , a text of the speech 22 , and a dictionary of phonemic spellings of words 24 .", "label": "", "metadata": {}, "score": "77.75929"}
{"text": "In step 48 of FIG .2 , the parameters for the senone , context - dependent HMM , and the context - independent HMM are estimated .The training phase of a HMM consists of estimating these parameters using the training data , a text of the speech 22 , and a dictionary of phonemic spellings of words 24 .", "label": "", "metadata": {}, "score": "77.75929"}
{"text": "In step 48 of FIG .2 , the parameters for the senone , context - dependent HMM , and the context - independent HMM are estimated .The training phase of a HMM consists of estimating these parameters using the training data , a text of the speech 22 , and a dictionary of phonemic spellings of words 24 .", "label": "", "metadata": {}, "score": "77.75929"}
{"text": "A detailed description of the method employed by the recognition engine 34 is presented below with reference to FIG .8 .The language model storage 22 may specify a grammar .In the preferred embodiment , the linguistic expression which is generated from recognition engine 34 is displayed to an output device 36 , such as a conventional printer , computer monitor , or the like .", "label": "", "metadata": {}, "score": "77.80396"}
{"text": "A detailed description of the method employed by the recognition engine 34 is presented below with reference to FIG .8 .The language model storage 22 may specify a grammar .In the preferred embodiment , the linguistic expression which is generated from recognition engine 34 is displayed to an output device 36 , such as a conventional printer , computer monitor , or the like .", "label": "", "metadata": {}, "score": "77.80396"}
{"text": "A detailed description of the method employed by the recognition engine 34 is presented below with reference to FIG .8 .The language model storage 22 may specify a grammar .In the preferred embodiment , the linguistic expression which is generated from recognition engine 34 is displayed to an output device 36 , such as a conventional printer , computer monitor , or the like .", "label": "", "metadata": {}, "score": "77.80396"}
{"text": "In step 126 , the method performs steps 128 - 136 for each word sequence that can represent the input speech utterance .The word sequence can consist of a variety of different senone alignments , where each senone alignment corresponds to a sequence of HMM states .", "label": "", "metadata": {}, "score": "77.831955"}
{"text": "In step 126 , the method performs steps 128 - 136 for each word sequence that can represent the input speech utterance .The word sequence can consist of a variety of different senone alignments , where each senone alignment corresponds to a sequence of HMM states .", "label": "", "metadata": {}, "score": "77.831985"}
{"text": "In step 126 , the method performs steps 128 - 136 for each word sequence that can represent the input speech utterance .The word sequence can consist of a variety of different senone alignments , where each senone alignment corresponds to a sequence of HMM states .", "label": "", "metadata": {}, "score": "77.831985"}
{"text": "During training , if the Viterbi algorithm is used , each feature vector in the training data can be identified with a specific senone .This mapping or vectors with senone is known as \" forced alignment .\" A value of \u03bb new is determined for each of the K iterations .", "label": "", "metadata": {}, "score": "77.87198"}
{"text": "During training , if the Viterbi algorithm is used , each feature vector in the training data can be identified with a specific senone .This mapping or vectors with senone is known as \" forced alignment .\" A value of \u03bb new is determined for each of the K iterations .", "label": "", "metadata": {}, "score": "77.87198"}
{"text": "During training , if the Viterbi algorithm is used , each feature vector in the training data can be identified with a specific senone .This mapping or vectors with senone is known as \" forced alignment .\" A value of \u03bb new is determined for each of the K iterations .", "label": "", "metadata": {}, "score": "77.87198"}
{"text": "65 , pp .21 - 36 , May 1986 .FIG .3 is a flowchart showing the procedure for creating a pseudo two - dimensional HMM .First , the structure of the model , that is , the number of superstates and states in each superstate , is determined ( step 350 ) .", "label": "", "metadata": {}, "score": "78.03262"}
{"text": "The duration probability may be found through training by calculating histogram of the length of the duration of states , superstates , and characters .The histogram for states is in units of pixels , and for superstates and characters are in columns .", "label": "", "metadata": {}, "score": "78.04202"}
{"text": "The front - most node of this backward partial path , i.e. the left most node of it , is here called \" front node \" .The backward development of the path is \" completed \" when the front node is equal to the initial node .", "label": "", "metadata": {}, "score": "78.07923"}
{"text": "Calling the minimum duration of state i Dmin i and the maximum duration Dmax i , the penalty Po i ( d i ) assigned for an out - of - state - transition is : . where p o is the out - of - state transition penalty factor .", "label": "", "metadata": {}, "score": "78.09523"}
{"text": "Thus there is one feature vector per frame .The processor 108 uses the features for speech recognition 206 or training 207 .In training , the feature vectors of the utterance are used to create templates in the form of HMMs which are stored in memory 208 .", "label": "", "metadata": {}, "score": "78.100624"}
{"text": "[ 0049 ] .To determine the noise structure , only the frames of the wide token having the lowest energy are considered and they are used to compute an average noise feature .To find the frames with the lowest energy , the frames are sorted by increasing energy value R 0 ( step 80 ) and the N frames having the lowest energy values R 0 are stored in order based on their energy values , from lowest to highest .", "label": "", "metadata": {}, "score": "78.12867"}
{"text": "The gain level is typically also changed to match that of the wide token .Referring back to FIG .3 , the wide and noise / gain adapted templates are provided to feature transformer 58 B for converting to the cepstral features required by the DTW unit 60 .", "label": "", "metadata": {}, "score": "78.16015"}
{"text": "The gain level is typically also changed to match that of the wide token .Referring back to FIG .3 , the wide and noise / gain adapted templates are provided to feature transformer 58 B for converting to the cepstral features required by the DTW unit 60 .", "label": "", "metadata": {}, "score": "78.16015"}
{"text": "[0007 ] .The training phase is relatively noise - free , whereas the recognition needs to cope with additive environmental noise ; . [0008 ] .The environmental noise is unknown to the system prior to the instant the user pushes a push to talk ( PTT ) button and starts speaking ; .", "label": "", "metadata": {}, "score": "78.36816"}
{"text": "The output probabilities associated with like states corresponding to the same modeled phoneme are clustered forming senones .A weighting factor for each context - dependent senone which indicates the robustness of the output probability in predicting unseen data is also generated .", "label": "", "metadata": {}, "score": "78.40097"}
{"text": "The output probabilities associated with like states corresponding to the same modeled phoneme are clustered forming senones .A weighting factor for each context - dependent senone which indicates the robustness of the output probability in predicting unseen data is also generated .", "label": "", "metadata": {}, "score": "78.40097"}
{"text": "The output probabilities associated with like states corresponding to the same modeled phoneme are clustered forming senones .A weighting factor for each context - dependent senone which indicates the robustness of the output probability in predicting unseen data is also generated .", "label": "", "metadata": {}, "score": "78.40097"}
{"text": "( b ) for F - C extension : .The final PPS update equation should be : .( a ) for F - E extension : .( b ) for F - C extension : .As mentioned before , the complete path score CPS is composed of the partial backward path score ( PPS ) and the score of the best incoming path into the front node , ( FPS ) which is the optimal forward partial path score recorded in forward Viterbi pass .", "label": "", "metadata": {}, "score": "78.44491"}
{"text": "N is typically 10 .Mathematically this is stated as : .The noise energy Ron is then determined from the resultant noise feature .The peak energy estimation is determined in a similar manner but considering the frames with the highest energy .", "label": "", "metadata": {}, "score": "78.49348"}
{"text": "Method and system for speech recognition using continuous density hidden Markov models US 5937384 A .Abstract .A method and system for achieving an improved recognition accuracy in speech recognition systems which utilize continuous density hidden Markov models to represent phonetic units of speech present in spoken speech utterances is provided .", "label": "", "metadata": {}, "score": "78.68228"}
{"text": "The calculation is derived through an iterative process , step 64 , which converges when the difference between the new value of \u03bb , denoted as \u03bb new , meets a certain threshold .The process commences by selecting an initial value for \u03bb , step 66 .", "label": "", "metadata": {}, "score": "78.72"}
{"text": "The calculation is derived through an iterative process , step 64 , which converges when the difference between the new value of \u03bb , denoted as \u03bb new , meets a certain threshold .The process commences by selecting an initial value for \u03bb , step 66 .", "label": "", "metadata": {}, "score": "78.72"}
{"text": "The calculation is derived through an iterative process , step 64 , which converges when the difference between the new value of \u03bb , denoted as \u03bb new , meets a certain threshold .The process commences by selecting an initial value for \u03bb , step 66 .", "label": "", "metadata": {}, "score": "78.72"}
{"text": "The output pdf is estimated with training data and will closely predict data which resembles the training data .However , it is impossible to estimate the output pdf with training data that represents every possible input speech utterance , or with sufficient training data for it to predict all unseen data correctly .", "label": "", "metadata": {}, "score": "78.82637"}
{"text": "The output pdf is estimated with training data and will closely predict data which resembles the training data .However , it is impossible to estimate the output pdf with training data that represents every possible input speech utterance , or with sufficient training data for it to predict all unseen data correctly .", "label": "", "metadata": {}, "score": "78.82637"}
{"text": "The output pdf is estimated with training data and will closely predict data which resembles the training data .However , it is impossible to estimate the output pdf with training data that represents every possible input speech utterance , or with sufficient training data for it to predict all unseen data correctly .", "label": "", "metadata": {}, "score": "78.82637"}
{"text": "The histogram for states is in units of pixels , and for superstates and characters are in columns .As one might notice in the equation , the duration penalty is added only when a transition occurs between different states , superstates or characters .", "label": "", "metadata": {}, "score": "78.88237"}
{"text": "The weightings of the kernel were shown in FIG .4 .In another words , we use this kernel to convolve with the whole image .The purpose is to reduce the effect of randomness on the grey level value caused by noise .", "label": "", "metadata": {}, "score": "78.92183"}
{"text": "This method indeed eliminates some of the errors due to inaccurate endpoints .[ 0019 ] .However , the relaxed - endpoint solutions have several disadvantages .[ 0020 ] .Other disadvantages of the relaxed - endpoint methods are specific to the method .", "label": "", "metadata": {}, "score": "78.980194"}
{"text": "Such pixel sequences for known characters are used to \" train \" HMMs representing those characters .As will be explained in more detail below , the pixel sequence for an unknown character string is then compared with the models to determine the probability that the models represent such unknown strings .", "label": "", "metadata": {}, "score": "79.038"}
{"text": "It is an object of the invention the provide a speech recognition system which has an improved language model for increasing the accuracy of speech recognition .It is another object of the invention the provide a speech recognition system which estimates the probability of occurrence of each speech hypothesis using additional knowledge or information about the actual utterance to be recognized .", "label": "", "metadata": {}, "score": "79.08357"}
{"text": "It is an object of the invention the provide a speech recognition system which has an improved language model for increasing the accuracy of speech recognition .It is another object of the invention the provide a speech recognition system which estimates the probability of occurrence of each speech hypothesis using additional knowledge or information about the actual utterance to be recognized .", "label": "", "metadata": {}, "score": "79.08357"}
{"text": "3 , the decision unit 62 determines the best match based on the DTh scores w_score of the whole path where the best template Best_Template is defined as the one for which the score is minimal .However , the best template may not provide a good result .", "label": "", "metadata": {}, "score": "79.14911"}
{"text": "The token builder produces a widened test token representing an input test utterance and at least one frame before and after the input test utterance .The noise estimator estimates noise qualities of the widened test token .The template padder pads each of a plurality of reference templates with at least one blank frame either the beginning or end of the reference template .", "label": "", "metadata": {}, "score": "79.151054"}
{"text": "7A illustrates an example of a context - independent HMM structure for the phoneme /aa/ 114 .The context - independent HMM includes three states , denoted as state 1 ( 111 ) , state 2 ( 112 ) , and state 3 ( 113 ) .", "label": "", "metadata": {}, "score": "79.16443"}
{"text": "7A illustrates an example of a context - independent HMM structure for the phoneme /aa/ 114 .The context - independent HMM includes three states , denoted as state 1 ( 111 ) , state 2 ( 112 ) , and state 3 ( 113 ) .", "label": "", "metadata": {}, "score": "79.16443"}
{"text": "7A illustrates an example of a context - independent HMM structure for the phoneme /aa/ 114 .The context - independent HMM includes three states , denoted as state 1 ( 111 ) , state 2 ( 112 ) , and state 3 ( 113 ) .", "label": "", "metadata": {}, "score": "79.16443"}
{"text": "The prototype vector signal P1 has the second best prototype match score with the feature vector signal at time t1 , and therefore is associated with the second - rank score of \" 2 \" .Similarly , for the feature vector signal at time t1 , prototype vector signals P2 , P4 , and P3 are ranked \" 3 \" , \" 4 \" and \" 5 \" respectively .", "label": "", "metadata": {}, "score": "79.165726"}
{"text": "The prototype vector signal P1 has the second best prototype match score with the feature vector signal at time t1 , and therefore is associated with the second - rank score of \" 2 \" .Similarly , for the feature vector signal at time t1 , prototype vector signals P2 , P4 , and P3 are ranked \" 3 \" , \" 4 \" and \" 5 \" respectively .", "label": "", "metadata": {}, "score": "79.165726"}
{"text": "FIG .1 is a block diagram of a speech recognition system employed in the preferred embodiment .FIG .2 is a flow diagram of a training method used in the system of FIG .1 .FIG .3 is flow diagram of the method for calculating weighing factors used in the system of FIG .", "label": "", "metadata": {}, "score": "79.16811"}
{"text": "FIG .1 is a block diagram of a speech recognition system employed in the preferred embodiment .FIG .2 is a flow diagram of a training method used in the system of FIG .1 .FIG .3 is flow diagram of the method for calculating weighing factors used in the system of FIG .", "label": "", "metadata": {}, "score": "79.16811"}
{"text": "FIG .1 is a block diagram of a speech recognition system employed in the preferred embodiment .FIG .2 is a flow diagram of a training method used in the system of FIG .1 .FIG .3 is flow diagram of the method for calculating weighing factors used in the system of FIG .", "label": "", "metadata": {}, "score": "79.16811"}
{"text": "The parameters of the model are : . conditional frequency distributions f.sub.3 ( t.sub.3 .vertline.t.sub.1 t.sub.2 ) , f.sub.2 ( t.sub.3 . vertline.t.sub.2 ) , f.sub.1 ( t.sub.3 ) , for target words t.sub.1,t.sub.2,t.sub.3 ; . a bucketing scheme c which assigns word pairs t.sub.1 t.sub.2 small number of classes ; . non - negative interpolation functions . lambda .", "label": "", "metadata": {}, "score": "79.17964"}
{"text": "The state S 4 is the mean of frames F 17 and F 18 of utterance U 1 and frames F 27 , F 28 and F 29 of utterance U 2 .The above frame allocation is provided as an example of how the frames may be initially allocated to states and how a statistical representation of a state could be formed .", "label": "", "metadata": {}, "score": "79.30753"}
{"text": "A speech recognition method as claimed in claim 19 , characterized in that the output means comprises a loudspeaker .A speech recognition method as claimed in claim 19 , characterized in that the output means comprises a speech synthesizer .A speech recognition method as claimed in claim 19 , characterized in that the means for storing speech hypotheses comprises readable computer memory .", "label": "", "metadata": {}, "score": "79.31778"}
{"text": "A speech recognition method as claimed in claim 19 , characterized in that the output means comprises a loudspeaker .A speech recognition method as claimed in claim 19 , characterized in that the output means comprises a speech synthesizer .A speech recognition method as claimed in claim 19 , characterized in that the means for storing speech hypotheses comprises readable computer memory .", "label": "", "metadata": {}, "score": "79.31778"}
{"text": "26 , no .12 , 1993 .S. Kuo and 0 .Agazzi , \" Keyword spotting in poorly printed documents using pseudo 2d hidden Markov models \" , IEEE Trans .on PAMI , in press .O. E. Agazzi , S. Kuo , E1 Levin , and R. Pieraccini , \" Connected and degraded text recognition using planar hidden markov models \" , in Proc . of ICASSP'93 , pp .", "label": "", "metadata": {}, "score": "79.42156"}
{"text": "Backward Partial Path Score Update in One Arc Extension : # # EQU9 # # Complete Path Score Update : . where PPS(E ) is the backward Partial Path likelihood Score with front node E , CPS(E ) is the Complete Path Score updated up to E. The forward optimal path is recorded during the forward trellis search These two paths are joined at node E. In other words , when started at the terminal node , all CPS scores are the same as those final likelihood scores obtained from the forward Viterbi search .", "label": "", "metadata": {}, "score": "79.47661"}
{"text": "A higher number of data points improves the accuracy of the \u03bb new at the cost of greater computational requirements .A suitable number of reconstructed data points per mixture is 100 .In step 104 , steps 106 and 108 are performed for each data point in the set , step 104 .", "label": "", "metadata": {}, "score": "79.524"}
{"text": "A higher number of data points improves the accuracy of the \u03bb new at the cost of greater computational requirements .A suitable number of reconstructed data points per mixture is 100 .In step 104 , steps 106 and 108 are performed for each data point in the set , step 104 .", "label": "", "metadata": {}, "score": "79.524"}
{"text": "A higher number of data points improves the accuracy of the \u03bb new at the cost of greater computational requirements .A suitable number of reconstructed data points per mixture is 100 .In step 104 , steps 106 and 108 are performed for each data point in the set , step 104 .", "label": "", "metadata": {}, "score": "79.524"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .The present invention will be understood and appreciated more fully from the following detailed description taken in conjunction with the appended drawings in which : .FIGS .1A and 1B and 2 are graphical illustrations of three different , prior art dynamic time warping ( DTW ) operations ; .", "label": "", "metadata": {}, "score": "79.5977"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .The present invention will be understood and appreciated more fully from the following detailed description taken in conjunction with the appended drawings in which : .FIGS .1A and 1B and 2 are graphical illustrations of three different , prior art dynamic time warping ( DTW ) operations ; .", "label": "", "metadata": {}, "score": "79.5977"}
{"text": "This usually occurs due to insufficient training data .The robustness of the distribution increases with the use of more training data to estimate output pdf .One way to reduce this problem is to utilize several HMMs which model the same phenomes at several levels of detail .", "label": "", "metadata": {}, "score": "79.704216"}
{"text": "This usually occurs due to insufficient training data .The robustness of the distribution increases with the use of more training data to estimate output pdf .One way to reduce this problem is to utilize several HMMs which model the same phenomes at several levels of detail .", "label": "", "metadata": {}, "score": "79.704216"}
{"text": "This usually occurs due to insufficient training data .The robustness of the distribution increases with the use of more training data to estimate output pdf .One way to reduce this problem is to utilize several HMMs which model the same phenomes at several levels of detail .", "label": "", "metadata": {}, "score": "79.704216"}
{"text": "Description .FIELD OF THE INVENTION .The present invention pertains to voice recognition systems , and more particularly to system assigning probabilities in a state allocation algorithm .BACKGROUND OF THE INVENTION .Speaker dependent speech recognition systems use a feature extraction algorithm to perform signal processing on a frame of the input speech and output feature vectors representing each frame .", "label": "", "metadata": {}, "score": "79.71662"}
{"text": "[ 0038]FIG .6 is a block diagram illustration of a noise and peak energy estimator forming part of the system of FIG .3 ; .[ 0039 ] .[ 0039]FIG .7 is a graphical illustration of the noise adapted DTW operation of the present invention ; and .", "label": "", "metadata": {}, "score": "79.79931"}
{"text": "The combination is done based on the ability to predict data not seen during training .A robust output pdf which is more adept at predicting unseen data will receive a higher weight while a poorly estimated output pdf will receive a lower weight in the combined output pdf .", "label": "", "metadata": {}, "score": "79.851944"}
{"text": "The combination is done based on the ability to predict data not seen during training .A robust output pdf which is more adept at predicting unseen data will receive a higher weight while a poorly estimated output pdf will receive a lower weight in the combined output pdf .", "label": "", "metadata": {}, "score": "79.851944"}
{"text": "The combination is done based on the ability to predict data not seen during training .A robust output pdf which is more adept at predicting unseen data will receive a higher weight while a poorly estimated output pdf will receive a lower weight in the combined output pdf .", "label": "", "metadata": {}, "score": "79.851944"}
{"text": "A method as in claim 5 wherein the step of providing a plurality of context - dependent senones further comprises the steps of : . training the context - dependent senones from an amount of training data representing speech utterances ; . providing a weighting factor for each context - dependent senone representing the amount of training data used to estimate the senone ; and .", "label": "", "metadata": {}, "score": "79.903305"}
{"text": "A method as in claim 5 wherein the step of providing a plurality of context - dependent senones further comprises the steps of : . training the context - dependent senones from an amount of training data representing speech utterances ; . providing a weighting factor for each context - dependent senone representing the amount of training data used to estimate the senone ; and .", "label": "", "metadata": {}, "score": "79.903305"}
{"text": "A method as in claim 5 wherein the step of providing a plurality of context - dependent senones further comprises the steps of : . training the context - dependent senones from an amount of training data representing speech utterances ; . providing a weighting factor for each context - dependent senone representing the amount of training data used to estimate the senone ; and .", "label": "", "metadata": {}, "score": "79.903305"}
{"text": "In the hypothetical example , the feature vector signals and the prototype vector signal are shown as having one dimension only , with only one parameter value for that dimension .In practice , however , the feature vector signals and prototype vector signals may have , for example , fifty dimensions , where each dimension has two parameter values .", "label": "", "metadata": {}, "score": "79.91224"}
{"text": "In the hypothetical example , the feature vector signals and the prototype vector signal are shown as having one dimension only , with only one parameter value for that dimension .In practice , however , the feature vector signals and prototype vector signals may have , for example , fifty dimensions , where each dimension has two parameter values .", "label": "", "metadata": {}, "score": "79.91224"}
{"text": "claim 11 wherein the gain and noise adapter is able to add to the modified reference template noise qualities of the widened token .The apparatus of . claim 10 , wherein the peak energy estimator comprises a peak energy averager to average energy levels of high energy frames of said widened token .", "label": "", "metadata": {}, "score": "79.917786"}
{"text": "claim 11 wherein the gain and noise adapter is able to add to the modified reference template noise qualities of the widened token .The apparatus of . claim 10 , wherein the peak energy estimator comprises a peak energy averager to average energy levels of high energy frames of said widened token .", "label": "", "metadata": {}, "score": "79.917786"}
{"text": "Thus it can be seen that an improved method of calculating a state transition is disclosed .This efficient method assigns a transition penalty to an out of state transition score if a lower threshold number of frames assigned to that state ( the state duration ) has not been met .", "label": "", "metadata": {}, "score": "79.93821"}
{"text": "The noise and peak energy estimator 68 determines the noise structure and peak energy levels in the wide token .This is provided to the gain and noise adapter 70 in order to provide the noiseless templates with a noise structure and gain level similar to that found in the wide token .", "label": "", "metadata": {}, "score": "79.98395"}
{"text": "The noise and peak energy estimator 68 determines the noise structure and peak energy levels in the wide token .This is provided to the gain and noise adapter 70 in order to provide the noiseless templates with a noise structure and gain level similar to that found in the wide token .", "label": "", "metadata": {}, "score": "79.98395"}
{"text": "The output probability analysis utilizes the output probabilities of both the context - dependent and context - independent senones by weighing each output probability as a function of the weighting factor .The output probability having the more robust estimate will dominate the analysis thereby improving the output probability analysis .", "label": "", "metadata": {}, "score": "80.170944"}
{"text": "The output probability analysis utilizes the output probabilities of both the context - dependent and context - independent senones by weighing each output probability as a function of the weighting factor .The output probability having the more robust estimate will dominate the analysis thereby improving the output probability analysis .", "label": "", "metadata": {}, "score": "80.170944"}
{"text": "The output probability analysis utilizes the output probabilities of both the context - dependent and context - independent senones by weighing each output probability as a function of the weighting factor .The output probability having the more robust estimate will dominate the analysis thereby improving the output probability analysis .", "label": "", "metadata": {}, "score": "80.170944"}
{"text": "One example of an acoustic feature value measure is shown in FIG .5 .The measuring means includes a microphone 44 for generating an analog electrical signal corresponding to the utterance .The analog electrical signal from microphone 44 is converted to a digital electrical signal by analog to digital converter 46 .", "label": "", "metadata": {}, "score": "80.32631"}
{"text": "The sum of the contributions for all data points computed thus far are totaled in step 84 .At the completion of the iteration when all the data points in the deleted block that are aligned with sen SD have been processed , the average of the contributions is computed , \u03bb new , step 86 in accord with equation ( 2 ) above .", "label": "", "metadata": {}, "score": "80.35019"}
{"text": "The sum of the contributions for all data points computed thus far are totaled in step 84 .At the completion of the iteration when all the data points in the deleted block that are aligned with sen SD have been processed , the average of the contributions is computed , \u03bb new , step 86 in accord with equation ( 2 ) above .", "label": "", "metadata": {}, "score": "80.35019"}
{"text": "The sum of the contributions for all data points computed thus far are totaled in step 84 .At the completion of the iteration when all the data points in the deleted block that are aligned with sen SD have been processed , the average of the contributions is computed , \u03bb new , step 86 in accord with equation ( 2 ) above .", "label": "", "metadata": {}, "score": "80.35019"}
{"text": "The present invention pertains to a speech recognition system which improves the modeling of the speech signal to continuous density HMM corresponding to a linguistic expression .In the preferred embodiment , the recognition system utilizes a context - independent and several context - dependent HMMs to represent the speech unit of a phoneme in different contextual patterns .", "label": "", "metadata": {}, "score": "80.668915"}
{"text": "The present invention pertains to a speech recognition system which improves the modeling of the speech signal to continuous density HMM corresponding to a linguistic expression .In the preferred embodiment , the recognition system utilizes a context - independent and several context - dependent HMMs to represent the speech unit of a phoneme in different contextual patterns .", "label": "", "metadata": {}, "score": "80.668915"}
{"text": "The present invention pertains to a speech recognition system which improves the modeling of the speech signal to continuous density HMM corresponding to a linguistic expression .In the preferred embodiment , the recognition system utilizes a context - independent and several context - dependent HMMs to represent the speech unit of a phoneme in different contextual patterns .", "label": "", "metadata": {}, "score": "80.668915"}
{"text": "Accordingly the phoneme - based continuous density HMM used in the preferred embodiment can be characterized by the following mathematical definition : .( 1 ) N , the number of states in the model : preferably , three states are employed .", "label": "", "metadata": {}, "score": "80.671036"}
{"text": "Accordingly the phoneme - based continuous density HMM used in the preferred embodiment can be characterized by the following mathematical definition : .( 1 ) N , the number of states in the model : preferably , three states are employed .", "label": "", "metadata": {}, "score": "80.671036"}
{"text": "Accordingly the phoneme - based continuous density HMM used in the preferred embodiment can be characterized by the following mathematical definition : .( 1 ) N , the number of states in the model : preferably , three states are employed .", "label": "", "metadata": {}, "score": "80.671036"}
{"text": "Modeling individual words requires a longer training period and additional storage to store the associated parameters .This is feasible for small vocabulary systems but impractical for those which utilize large vocabularies .However , this invention is not limited to phoneme - based HMMs .", "label": "", "metadata": {}, "score": "80.76819"}
{"text": "Modeling individual words requires a longer training period and additional storage to store the associated parameters .This is feasible for small vocabulary systems but impractical for those which utilize large vocabularies .However , this invention is not limited to phoneme - based HMMs .", "label": "", "metadata": {}, "score": "80.76819"}
{"text": "Modeling individual words requires a longer training period and additional storage to store the associated parameters .This is feasible for small vocabulary systems but impractical for those which utilize large vocabularies .However , this invention is not limited to phoneme - based HMMs .", "label": "", "metadata": {}, "score": "80.76819"}
{"text": "Proceedings of the 1984 IEEE Inter Conference on Acoustics , Speech , and Signal Processing , vol .3 , pp .42.5.1 42.5.4 , Mar. 1984 .Easy To Use Patents Search & Patent Lawyer Directory .At Patents you can conduct a Patent Search , File a Patent Application , find a Patent Attorney , or search available technology through our Patent Exchange .", "label": "", "metadata": {}, "score": "80.771774"}
{"text": "Referring back to FIG .3 , the wide token is provided both to feature transformer 58 A and to the noise and peak energy estimator 68 .The feature transformer 58 A transforms the features of the wide token to the cepstral features required by the DTW unit 60 .", "label": "", "metadata": {}, "score": "80.83026"}
{"text": "Referring back to FIG .3 , the wide token is provided both to feature transformer 58 A and to the noise and peak energy estimator 68 .The feature transformer 58 A transforms the features of the wide token to the cepstral features required by the DTW unit 60 .", "label": "", "metadata": {}, "score": "80.83026"}
{"text": "1 illustrates a speech recognition system 10 which can be used to implement the recognition and training processes in accordance with the preferred embodiment of the invention .The speech recognition system 10 contains an input device 12 , such as but not limited to a microphone , which receives an input speech utterance and generates a corresponding analog electric signal .", "label": "", "metadata": {}, "score": "80.84418"}
{"text": "1 illustrates a speech recognition system 10 which can be used to implement the recognition and training processes in accordance with the preferred embodiment of the invention .The speech recognition system 10 contains an input device 12 , such as but not limited to a microphone , which receives an input speech utterance and generates a corresponding analog electric signal .", "label": "", "metadata": {}, "score": "80.84418"}
{"text": "1 illustrates a speech recognition system 10 which can be used to implement the recognition and training processes in accordance with the preferred embodiment of the invention .The speech recognition system 10 contains an input device 12 , such as but not limited to a microphone , which receives an input speech utterance and generates a corresponding analog electric signal .", "label": "", "metadata": {}, "score": "80.84418"}
{"text": "Prototype stores 54 and 56 may be electronic computer memory of the types discussed above .The prototype vectors in prototype store 38 may be obtained , for example , by clustering feature vector signals from a training set into a plurality of clusters , and then calculating the mean and standard deviation for each cluster to form the parameter values of the prototype vector .", "label": "", "metadata": {}, "score": "80.93385"}
{"text": "Prototype stores 54 and 56 may be electronic computer memory of the types discussed above .The prototype vectors in prototype store 38 may be obtained , for example , by clustering feature vector signals from a training set into a plurality of clusters , and then calculating the mean and standard deviation for each cluster to form the parameter values of the prototype vector .", "label": "", "metadata": {}, "score": "80.93385"}
{"text": "An acoustic feature value measure 36 is provided for measuring the value of at least one feature of an utterance over each of a series of successive time intervals to produce a series of feature vector signals representing the feature values .", "label": "", "metadata": {}, "score": "81.07686"}
{"text": "An acoustic feature value measure 36 is provided for measuring the value of at least one feature of an utterance over each of a series of successive time intervals to produce a series of feature vector signals representing the feature values .", "label": "", "metadata": {}, "score": "81.07686"}
{"text": "Each utterance is segmented into frames of feature vectors .The frames may be 20 ms in length , for example .A feature vector may be generated in any conventional manner .For example , a feature vector may comprise cepstral and delta - cepstral features that are generated from the output of A / D converter 202 ( FIG .", "label": "", "metadata": {}, "score": "81.21527"}
{"text": "Optical recognition of text is often hampered by the use of characters with different sizes , blurred documents , distorted characters and other impediments .Accordingly , it is desirable to improve recognition methods to be robust regardless of such impediments .", "label": "", "metadata": {}, "score": "81.230064"}
{"text": "Observation vectors for the gray - scale image are produced from pixel maps obtained by gray - scale optical scanning .Three components are employed to characterize a pixel : a convoluted , quantized gray - level component , a pixel relative position component , and a pixel major stroke direction component .", "label": "", "metadata": {}, "score": "81.24233"}
{"text": "In step 98 , the final sum resulting from step 96 is stored as the value of \u03bb new for the current sen SD and the deleted block .Referring to FIG .3 , at the completion of the K iterations , the process proceeds to calculate the average value for \u03bb new in step 76 in accord with equation ( 3 ) above .", "label": "", "metadata": {}, "score": "81.31967"}
{"text": "In step 98 , the final sum resulting from step 96 is stored as the value of \u03bb new for the current sen SD and the deleted block .Referring to FIG .3 , at the completion of the K iterations , the process proceeds to calculate the average value for \u03bb new in step 76 in accord with equation ( 3 ) above .", "label": "", "metadata": {}, "score": "81.31967"}
{"text": "In step 98 , the final sum resulting from step 96 is stored as the value of \u03bb new for the current sen SD and the deleted block .Referring to FIG .3 , at the completion of the K iterations , the process proceeds to calculate the average value for \u03bb new in step 76 in accord with equation ( 3 ) above .", "label": "", "metadata": {}, "score": "81.31967"}
{"text": "4 is a diagram illustrating the traceback lattice associated with all of the possible state transitions in a left - right model with no skip transitions allowed .FIG .5 illustrates a left to right , no skip HMM corresponding to FIG .", "label": "", "metadata": {}, "score": "81.49936"}
{"text": "In this embodiment , the several context - dependent HMMs provide more - detailed acoustic models and each context - independent HMM provides a less - detailed acoustic model .A weighting factor , \u03bb , for each senone corresponding to a context - dependent state which was computed previously in the training phase is used to indicate the weight each senone is given .", "label": "", "metadata": {}, "score": "81.52913"}
{"text": "In this embodiment , the several context - dependent HMMs provide more - detailed acoustic models and each context - independent HMM provides a less - detailed acoustic model .A weighting factor , \u03bb , for each senone corresponding to a context - dependent state which was computed previously in the training phase is used to indicate the weight each senone is given .", "label": "", "metadata": {}, "score": "81.52913"}
{"text": "In this embodiment , the several context - dependent HMMs provide more - detailed acoustic models and each context - independent HMM provides a less - detailed acoustic model .A weighting factor , \u03bb , for each senone corresponding to a context - dependent state which was computed previously in the training phase is used to indicate the weight each senone is given .", "label": "", "metadata": {}, "score": "81.529144"}
{"text": "P(A ) represents the state transition probability for the senone sequence sd i . . .sd n .P(x i /sd i ) represents the probability that feature version x i matches context - dependent senone sd i .This represents the likelihood that the feature vector , x , matches the senone , sd , which corresponds to a context - dependent HMM state .", "label": "", "metadata": {}, "score": "81.67652"}
{"text": "P(A ) represents the state transition probability for the senone sequence sd i . . .sd n .P(x i /sd i ) represents the probability that feature version x i matches context - dependent senone sd i .This represents the likelihood that the feature vector , x , matches the senone , sd , which corresponds to a context - dependent HMM state .", "label": "", "metadata": {}, "score": "81.67652"}
{"text": "P(A ) represents the state transition probability for the senone sequence sd i . . .sd n .P(x i /sd i ) represents the probability that feature version x i matches context - dependent senone sd i .This represents the likelihood that the feature vector , x , matches the senone , sd , which corresponds to a context - dependent HMM state .", "label": "", "metadata": {}, "score": "81.67652"}
{"text": "However because the distribution of state durations in actual practice is not likely to result in an exponential probability density , significant improvement in performance can be achieved by introducing more elaborate state duration densities into the HMMs .Following is one way to include duration .", "label": "", "metadata": {}, "score": "81.85693"}
{"text": "Normalization processor 58 normalizes the twenty - one dimension feature vector F'(t ) to produce a twenty dimension normalized feature vector X(t ) .The twenty - first dimension of the feature vector F'(t ) , representing the total amplitude or total power , is discarded .", "label": "", "metadata": {}, "score": "81.91315"}
{"text": "10 .The processor 108 determines in step 1012 whether the state dwell time of the previous state has already exceeded the min threshold as indicated in decision block 1012 .If it less than the lower threshold , the processor calculates a proportional penalty , as indicated in step 1014 .", "label": "", "metadata": {}, "score": "82.19203"}
{"text": "7B shows an example of corresponding context - dependent HMMs for the phoneme /aa/. In FIG .7B there are five context - dependent models which model the phoneme /aa/ in five distinct phonemic contexts ( 115 - 119 ) .The senones are classified within like states in the different HMMs .", "label": "", "metadata": {}, "score": "82.31037"}
{"text": "7B shows an example of corresponding context - dependent HMMs for the phoneme /aa/. In FIG .7B there are five context - dependent models which model the phoneme /aa/ in five distinct phonemic contexts ( 115 - 119 ) .The senones are classified within like states in the different HMMs .", "label": "", "metadata": {}, "score": "82.31037"}
{"text": "7B shows an example of corresponding context - dependent HMMs for the phoneme /aa/. In FIG .7B there are five context - dependent models which model the phoneme /aa/ in five distinct phonemic contexts ( 115 - 119 ) .The senones are classified within like states in the different HMMs .", "label": "", "metadata": {}, "score": "82.31037"}
{"text": "Huang , Xuedong et al . , An Overview of the SPHINX II Speech Recognition System , Proceedings of ARPA Human Language Technology Workshop ; 1993 ; pp .1 6 .Lee , Kai Fu et al . , Automatic Speech Recognition The Development of the SPHINX System , Kluwer Academic Publishers ; 1989 ; pp .", "label": "", "metadata": {}, "score": "82.32849"}
{"text": "Huang , Xuedong et al . , An Overview of the SPHINX II Speech Recognition System , Proceedings of ARPA Human Language Technology Workshop ; 1993 ; pp .1 6 .Lee , Kai Fu et al . , Automatic Speech Recognition The Development of the SPHINX System , Kluwer Academic Publishers ; 1989 ; pp .", "label": "", "metadata": {}, "score": "82.32851"}
{"text": "Huang , Xuedong et al . , An Overview of the SPHINX II Speech Recognition System , Proceedings of ARPA Human Language Technology Workshop ; 1993 ; pp .1 6 .Lee , Kai Fu et al . , Automatic Speech Recognition The Development of the SPHINX System , Kluwer Academic Publishers ; 1989 ; pp .", "label": "", "metadata": {}, "score": "82.32851"}
{"text": "( 2 ) M , the number of mixtures in the output pdf .The number M of mixture - components is typically , anywhere from 1 to 50 ; and .c k is the weight for the kth mixture component in state i. .", "label": "", "metadata": {}, "score": "82.33215"}
{"text": "( 2 ) M , the number of mixtures in the output pdf .The number M of mixture - components is typically , anywhere from 1 to 50 ; and .c k is the weight for the kth mixture component in state i. .", "label": "", "metadata": {}, "score": "82.33215"}
{"text": "( 2 ) M , the number of mixtures in the output pdf .The number M of mixture - components is typically , anywhere from 1 to 50 ; and .c k is the weight for the kth mixture component in state i. .", "label": "", "metadata": {}, "score": "82.33215"}
{"text": "Vector Quantization Procedure For Speech Recognition Systems Using Discrete Parameter Phoneme Based Markov Word Models .IBM Technical Disclosure Bulletin , vol .32 , No . 7 , Dec. 1989 , pp .320 321 .Lucassen , J. M. et al .", "label": "", "metadata": {}, "score": "82.3402"}
{"text": "The template padder pads each of a plurality of reference templates with at least one blank frame either the beginning or end of the reference template .The gain and noise adapter adapts each padded reference template with the noise and gain qualities thereby producing adapted reference templates having noise frames wherever a blank frame was originally placed and noise adapted speech where speech exists .", "label": "", "metadata": {}, "score": "82.48172"}
{"text": "The template padder pads each of a plurality of reference templates with at least one blank frame either the beginning or end of the reference template .The gain and noise adapter adapts each padded reference template with the noise and gain qualities thereby producing adapted reference templates having noise frames wherever a blank frame was originally placed and noise adapted speech where speech exists .", "label": "", "metadata": {}, "score": "82.48172"}
{"text": "The DTW unit performs a noise adapted DTW operation comparing the widened token with one of the noise adapted reference templates .A method for modifying a template for use in speech recognition comprising : .The method according to . claim 1 further comprising adjusting said at least one level - raised reference template by adding to it noise qualities of said widened token .", "label": "", "metadata": {}, "score": "82.72136"}
{"text": "The system may output the best match , a set of the best matches , or optionally , no match .Memory 208 is preferably a non - volatile memory portion of memory 110 ( FIG .1 ) , and may for example be an EEPROM or flash ROM .", "label": "", "metadata": {}, "score": "82.864105"}
{"text": "Step 92 iterates for each mixture and determines the contribution of the context - dependent output probability over the overall probability for the mixture having the corresponding mean and weight parameters .For mixture components , this is represented mathematically as : # # EQU7 # # .", "label": "", "metadata": {}, "score": "83.00503"}
{"text": "Step 92 iterates for each mixture and determines the contribution of the context - dependent output probability over the overall probability for the mixture having the corresponding mean and weight parameters .For mixture components , this is represented mathematically as : # # EQU7 # # .", "label": "", "metadata": {}, "score": "83.00503"}
{"text": "Step 92 iterates for each mixture and determines the contribution of the context - dependent output probability over the overall probability for the mixture having the corresponding mean and weight parameters .For mixture components , this is represented mathematically as : # # EQU7 # # .", "label": "", "metadata": {}, "score": "83.00503"}
{"text": "The out of state transition penalty is proportional to the number of frames that the dwell time is below the lower threshold .A self loop penalty is applied to a self loop score if the upper threshold number of frames assigned to a state has been exceeded .", "label": "", "metadata": {}, "score": "83.01646"}
{"text": "The task of the language match score generator is to compute a score P(T ) for a finite sequence T of target words .The probability P(T ) of occurrence of the target word sequence may be approximated by the product of n - gram probabilities for all n - grams in each string .", "label": "", "metadata": {}, "score": "83.028595"}
{"text": "The acoustic match score may comprise , for example , an estimate of the probability of occurrence of the sequence of coded representations of the utterance given the occurrence of the speech hypothesis .The hypothesis score may then comprise the product of the acoustic match score multiplied by the translation match score .", "label": "", "metadata": {}, "score": "83.04541"}
{"text": "The acoustic match score may comprise , for example , an estimate of the probability of occurrence of the sequence of coded representations of the utterance given the occurrence of the speech hypothesis .The hypothesis score may then comprise the product of the acoustic match score multiplied by the translation match score .", "label": "", "metadata": {}, "score": "83.04541"}
{"text": "each state , the duration probability D 1 j for the state .An observation vector O xy may be defined for each pixel at coordinates ( x , y ) .The observation vector O xy has three components .The kernel applies weighting factors to pixels as shown in FIG .", "label": "", "metadata": {}, "score": "83.339134"}
{"text": "The call processor , 108 , performs feature extraction 204 on the processed digital signal representation of the analog signal output by microphone 114 and produces a set of feature vectors representative of the user utterance .A feature vector is produced for each short time analysis window .", "label": "", "metadata": {}, "score": "83.34662"}
{"text": "The transformations from one type of features to another type are well known and , therefore , will not be further discussed herein .[ 0045 ] .The noise and peak energy estimator 68 determines the noise structure and peak energy levels in the wide token .", "label": "", "metadata": {}, "score": "83.53391"}
{"text": "Here l is the length of S , m is the length of T , S i is the i th word of S , and T j is the j th word of T. The parameters of the model are : .", "label": "", "metadata": {}, "score": "83.57675"}
{"text": "3 , the decision unit 62 determines the best match based on the DTW scores w_score of the whole path where the best template Best_Template is defined as the one for which the score is minimal .However , the best template may not provide a good result .", "label": "", "metadata": {}, "score": "83.659065"}
{"text": "Each translation match score comprises an estimate of the probability of occurrence of the speech hypothesis given the occurrence of the source text .A hypothesis score generator produces a hypothesis score for each hypothesis .Each hypothesis score comprises a combination of the acoustic match score and the translation match score for the hypothesis .", "label": "", "metadata": {}, "score": "83.67854"}
{"text": "Each translation match score comprises an estimate of the probability of occurrence of the speech hypothesis given the occurrence of the source text .A hypothesis score generator produces a hypothesis score for each hypothesis .Each hypothesis score comprises a combination of the acoustic match score and the translation match score for the hypothesis .", "label": "", "metadata": {}, "score": "83.67854"}
{"text": "Preferably , the rank score processor 42 associates a rank score with all prototype vector signals for each feature vector signal .Each rank score represents the estimated closeness of the associated prototype vector signal to the feature vector signal relative to the estimated closeness of all other prototype vector signals to the feature vector signal .", "label": "", "metadata": {}, "score": "83.72948"}
{"text": "Preferably , the rank score processor 42 associates a rank score with all prototype vector signals for each feature vector signal .Each rank score represents the estimated closeness of the associated prototype vector signal to the feature vector signal relative to the estimated closeness of all other prototype vector signals to the feature vector signal .", "label": "", "metadata": {}, "score": "83.72948"}
{"text": "For a sentence of , for example , 10 words out of a target language vocabulary of 20,000 words , there are 20,000 . times.10 .sup.43 possible hypotheses .With such a large number of hypotheses , it is not feasible to generate all possible hypotheses .", "label": "", "metadata": {}, "score": "83.93839"}
{"text": "The output device 32 may be , for example , a display , such as a cathode ray tube or liquid crystal display , a printer , a loudspeaker , or a speech synthesizer .FIG .4 is a block diagram of an example of an acoustic processor 14 ( FIG .", "label": "", "metadata": {}, "score": "84.014465"}
{"text": "The output device 32 may be , for example , a display , such as a cathode ray tube or liquid crystal display , a printer , a loudspeaker , or a speech synthesizer .FIG .4 is a block diagram of an example of an acoustic processor 14 ( FIG .", "label": "", "metadata": {}, "score": "84.014465"}
{"text": "A method as in claim 5 wherein the step of providing a plurality of context - dependent senones further comprises the step of training the context - dependent senones from an amount of training data representing speech utterances ; . wherein the step of providing a context - independent senone further comprises the step of training the context - independent senones from the amount of training data ; and .", "label": "", "metadata": {}, "score": "84.02266"}
{"text": "A method as in claim 5 wherein the step of providing a plurality of context - dependent senones further comprises the step of training the context - dependent senones from an amount of training data representing speech utterances ; . wherein the step of providing a context - independent senone further comprises the step of training the context - independent senones from the amount of training data ; and .", "label": "", "metadata": {}, "score": "84.02266"}
{"text": "A method as in claim 5 wherein the step of providing a plurality of context - dependent senones further comprises the step of training the context - dependent senones from an amount of training data representing speech utterances ; . wherein the step of providing a context - independent senone further comprises the step of training the context - independent senones from the amount of training data ; and .", "label": "", "metadata": {}, "score": "84.02266"}
{"text": "In step 910 , the processor 108 makes a decision as to whether the last state was reached .If not , the state counter i is incremented as indicated in step 916 and the processor returns to step 906 .Otherwise , the processor determines in step 912 whether the last frame was processed , as indicated in step 912 .", "label": "", "metadata": {}, "score": "84.055916"}
{"text": "A Tree Trellis Based Fast Search for Finding the N Best Sentence Hypothesis in Continuous Speech Recognition by F. K. Soong et al . , Proc .DARPA Speech and Natural Language Workshop , pp .12 19 .A tree trellis based fast search for finding the n best sentence hypothesis in continuous speech recognition F. K. Soong et al . , Proc .", "label": "", "metadata": {}, "score": "84.08833"}
{"text": "claim 3 further comprising : . performing a noise adapted dynamic time warping ( DTW ) operation including comparing the widened token to the noise adapted reference template .An apparatus comprising : . a peak energy estimator able to estimate a peak energy level of a reference template and a peak energy level of a widened token ; and .", "label": "", "metadata": {}, "score": "84.13451"}
{"text": "claim 3 further comprising : . performing a noise adapted dynamic time warping ( DTW ) operation including comparing the widened token to the noise adapted reference template .An apparatus comprising : . a peak energy estimator able to estimate a peak energy level of a reference template and a peak energy level of a widened token ; and .", "label": "", "metadata": {}, "score": "84.13451"}
{"text": "Statistical language models exploit the fact that not all word sequences occur naturally with equal probability .One simple model is the trigram model of English , in which it is assumed that the probability that a word will be spoken depends only on the previous two words that have been spoken .", "label": "", "metadata": {}, "score": "84.179855"}
{"text": "Statistical language models exploit the fact that not all word sequences occur naturally with equal probability .One simple model is the trigram model of English , in which it is assumed that the probability that a word will be spoken depends only on the previous two words that have been spoken .", "label": "", "metadata": {}, "score": "84.179855"}
{"text": "Many proper names are missing from even large vocabularies and yet are often translated directly from one language to another with no change in spelling .In one embodiment of the invention , the acoustic model generator 18 generates an acoustic model of a word by replacing each letter in the spelling of the word with an acoustic letter model , from an acoustic letter model store 35 , corresponding to the letter .", "label": "", "metadata": {}, "score": "84.309784"}
{"text": "Many proper names are missing from even large vocabularies and yet are often translated directly from one language to another with no change in spelling .In one embodiment of the invention , the acoustic model generator 18 generates an acoustic model of a word by replacing each letter in the spelling of the word with an acoustic letter model , from an acoustic letter model store 35 , corresponding to the letter .", "label": "", "metadata": {}, "score": "84.309784"}
{"text": "wherein the step of determining the output probability further comprises the step of weighing the less - detailed model and more - detailed model output probabilities relative to the amount of training data used to train each acoustic model . providing a plurality of context - dependent senones ; . providing a context - independent senone associated with the plurality context - dependent senones representing a same position of the linguistic expression ; . providing a linguistic expression likely to match the input speech utterance ; . utilizing the output probabilities to determine the likelihood that the input speech utterance matches the linguistic expression .", "label": "", "metadata": {}, "score": "84.33281"}
{"text": "wherein the step of determining the output probability further comprises the step of weighing the less - detailed model and more - detailed model output probabilities relative to the amount of training data used to train each acoustic model . providing a plurality of context - dependent senones ; . providing a context - independent senone associated with the plurality context - dependent senones representing a same position of the linguistic expression ; . providing a linguistic expression likely to match the input speech utterance ; . utilizing the output probabilities to determine the likelihood that the input speech utterance matches the linguistic expression .", "label": "", "metadata": {}, "score": "84.33281"}
{"text": "wherein the step of determining the output probability further comprises the step of weighing the less - detailed model and more - detailed model output probabilities relative to the amount of training data used to train each acoustic model . providing a plurality of context - dependent senones ; . providing a context - independent senone associated with the plurality context - dependent senones representing a same position of the linguistic expression ; . providing a linguistic expression likely to match the input speech utterance ; . utilizing the output probabilities to determine the likelihood that the input speech utterance matches the linguistic expression .", "label": "", "metadata": {}, "score": "84.33281"}
{"text": "When the process converges for a particular context - dependent senone , the current value of \u03bb new is stored in lambda table 26 for the particular context - dependent senone .FIG .4 depicts a flowchart of the steps used in computing the new value for the weighting factor , \u03bb new in accord with equations ( 2 ) and ( 3 ) above .", "label": "", "metadata": {}, "score": "84.371994"}
{"text": "When the process converges for a particular context - dependent senone , the current value of \u03bb new is stored in lambda table 26 for the particular context - dependent senone .FIG .4 depicts a flowchart of the steps used in computing the new value for the weighting factor , \u03bb new in accord with equations ( 2 ) and ( 3 ) above .", "label": "", "metadata": {}, "score": "84.371994"}
{"text": "When the process converges for a particular context - dependent senone , the current value of \u03bb new is stored in lambda table 26 for the particular context - dependent senone .FIG .4 depicts a flowchart of the steps used in computing the new value for the weighting factor , \u03bb new in accord with equations ( 2 ) and ( 3 ) above .", "label": "", "metadata": {}, "score": "84.371994"}
{"text": "A device 100 , in which the invention can be advantageously employed is disclosed in FIG .1 .The illustrated radiotelephone includes a transmitter 102 and a receiver 104 coupled to an antenna 106 .The transmitter 102 and receiver 104 are coupled to a call processor 108 , which performs call processing functions .", "label": "", "metadata": {}, "score": "84.54442"}
{"text": "The conditional probabilities may be determined empirically by examining large bodies of text .For example , the conditional probability f(W.sub.z . vertline . W.sub.x W.sub.y ) of word W.sub.z given the occurrence of the string W.sub.x W.sub.y may be estimated from the equation # # EQU10 # # .", "label": "", "metadata": {}, "score": "84.68341"}
{"text": "Accordingly , the penalties may produce erroneous results .Consequently there is a need for an improved system of using state duration information to generate transition penalties in a system having minimal training information .BRIEF DESCRIPTION OF THE DRAWINGS .FIG .", "label": "", "metadata": {}, "score": "84.7449"}
{"text": "The method of claim 5 wherein said adjusting step further comprises : . during step ( c ) , ( i ) adjusting the probability for the final superstate by extracting duration constraints based on self - transition probabilities , and ( ii ) inserting duration constraints based on the mean durations and standard deviations for each superstate .", "label": "", "metadata": {}, "score": "84.77452"}
{"text": "This way only the more plausible words , i.e. , words with similar number of characters and/or similar topology , would be found .This is especially important when working on poorly - printed document images .Pseudo Two - Dimensional HMM with Duration Constraints .", "label": "", "metadata": {}, "score": "84.81023"}
{"text": "The spectrum analyzer 50 may be , for example , a fast Fourier transform processor .Alternatively , it may be a bank of twenty band pass filters .The twenty - one dimension vector signals produced by spectrum analyzer 50 may be adapted to remove background noise by an adaptive noise cancellation processor 52 .", "label": "", "metadata": {}, "score": "84.86801"}
{"text": "5 .The measuring means includes a microphone 44 for generating an analog electrical signal corresponding to the utterance .The analog electrical signal from microphone 44 is converted to a digital electrical signal by analog to digital converter 46 .For this purpose , the analog signal may be sampled , for example , at a rate of twenty kilohertz by the analog to digital converter 46 .", "label": "", "metadata": {}, "score": "84.937645"}
{"text": "As mentioned briefly above , the stored vocabulary words in memory 208 are created in a training mode .For example , stored vocabulary words are each initially derived from two training signals , utterances U 1 and U 2 ( FIG .", "label": "", "metadata": {}, "score": "84.98064"}
{"text": "The method of claim 3 further comprising : .The method of claim 8 , further comprising : . after step ( c ) , readjusting said adjusted probability by substituting said overall adjustment parameter for each preliminary adjustment parameter .The method of claim 7 further including the steps of : .", "label": "", "metadata": {}, "score": "85.01612"}
{"text": "A window generator 48 obtains , for example , a twenty millisecond duration sample of the digital signal from analog to digital converter 46 every ten milliseconds ( one centisecond ) .Each twenty millisecond sample of the digital signal is analyzed by spectrum analyzer 50 in order to obtain the amplitude of the digital signal sample in each of , for example , twenty frequency bands .", "label": "", "metadata": {}, "score": "85.211395"}
{"text": "The method of claim 3 wherein step ( c ) uses the probabilities determined in step ( b ) to calculate N probabilities for the final superstate , the N probabilities representing N best hypotheses of text elements for the final superstate .", "label": "", "metadata": {}, "score": "85.42042"}
{"text": "claim 14 , wherein the instructions when executed further result in : . providing a noise adapted reference template by adjusting the modified reference template with noise qualities of the widened token .The article of .claim 16 , wherein the instructions when executed further result in : . adding the noise qualities to a blank frame of the modified reference template ; and . adding the noise qualities to a speech frame of the modified reference template .", "label": "", "metadata": {}, "score": "85.44773"}
{"text": "claim 14 , wherein the instructions when executed further result in : . providing a noise adapted reference template by adjusting the modified reference template with noise qualities of the widened token .The article of .claim 16 , wherein the instructions when executed further result in : . adding the noise qualities to a blank frame of the modified reference template ; and . adding the noise qualities to a speech frame of the modified reference template .", "label": "", "metadata": {}, "score": "85.44773"}
{"text": "IBM Technical Disclosure Bulletin , vol .32 , No .10B Mar. 1990 , pp .19 23 .Bahl , L. R. , et al .Speaker Independent Label Coding Apparatus .U.S. patent application Ser .No .673,189 , filed Mar. 22 , 1991 .", "label": "", "metadata": {}, "score": "85.508255"}
{"text": "That is , characters are mistaken for other ones with similar topology , and these characters got more confusable when images are noisy and blurry .If discriminative training is used to train the models with this competitive ( confusable ) set , it could greatly reduce the error rate of recognition .", "label": "", "metadata": {}, "score": "85.509796"}
{"text": "An optional postprocessing step ( step 317 ) is indicated , which can be used for various refinements to the method of the invention , as will be described below .If there are still more superstates ( step 318 ) , the state parameters of the next superstate are addressed ( step 319 ) and steps 311 - 318 are repeated for the current column and the next superstate .", "label": "", "metadata": {}, "score": "85.62193"}
{"text": "Each prototype vector signal has at least one parameter value and has a unique identification value .Table 2 shows a hypothetical example of five prototype vectors signals having one parameter value each , and having identification values P1 , P2 , P3 , P4 , and P5 , respectively .", "label": "", "metadata": {}, "score": "85.8309"}
{"text": "Each prototype vector signal has at least one parameter value and has a unique identification value .Table 2 shows a hypothetical example of five prototype vectors signals having one parameter value each , and having identification values P1 , P2 , P3 , P4 , and P5 , respectively .", "label": "", "metadata": {}, "score": "85.8309"}
{"text": "However , this invention is not constrained to this particular limitation .Mixtures of other well - known continuous density functions can be used , such as the Laplacian and K 0 -type density functions .Further , to capture similarity between states of different context - dependent phonemes and to increase the amount of training data available for each senone , the output distributions of like states of different context - dependent phonetic HMM models for the same context - independent phone are clustered together forming senones .", "label": "", "metadata": {}, "score": "85.841705"}
{"text": "However , this invention is not constrained to this particular limitation .Mixtures of other well - known continuous density functions can be used , such as the Laplacian and K 0 -type density functions .Further , to capture similarity between states of different context - dependent phonemes and to increase the amount of training data available for each senone , the output distributions of like states of different context - dependent phonetic HMM models for the same context - independent phone are clustered together forming senones .", "label": "", "metadata": {}, "score": "85.841705"}
{"text": "However , this invention is not constrained to this particular limitation .Mixtures of other well - known continuous density functions can be used , such as the Laplacian and K 0 -type density functions .Further , to capture similarity between states of different context - dependent phonemes and to increase the amount of training data available for each senone , the output distributions of like states of different context - dependent phonetic HMM models for the same context - independent phone are clustered together forming senones .", "label": "", "metadata": {}, "score": "85.841705"}
{"text": "At least one word of one or more speech hypot This invention was made with Government support under Contract Number N00014 - 91-C-0135 awarded by the office of Naval Research .The Government has certain rights in this invention .", "label": "", "metadata": {}, "score": "85.99122"}
{"text": "FIG .4 is a flow diagram of the preferred embodiment for calculating a new value for lambda as used in the system of FIG .3 .FIG .5 is a flow diagram of a first alternate embodiment for calculating a new value for lambda as used in the system of FIG .", "label": "", "metadata": {}, "score": "86.10963"}
{"text": "FIG .4 is a flow diagram of the preferred embodiment for calculating a new value for lambda as used in the system of FIG .3 .FIG .5 is a flow diagram of a first alternate embodiment for calculating a new value for lambda as used in the system of FIG .", "label": "", "metadata": {}, "score": "86.10963"}
{"text": "FIG .4 is a flow diagram of the preferred embodiment for calculating a new value for lambda as used in the system of FIG .3 .FIG .5 is a flow diagram of a first alternate embodiment for calculating a new value for lambda as used in the system of FIG .", "label": "", "metadata": {}, "score": "86.10963"}
{"text": "a dynamic time warping ( DTW ) unit to compare between the widened token and the reference template .The apparatus of . claim 10 , wherein the gain and noise adapter is able to provide a modified reference template having a peak energy level substantially equivalent to the difference between the peak energy level and the average noise energy level of the widened token .", "label": "", "metadata": {}, "score": "86.31691"}
{"text": "a dynamic time warping ( DTW ) unit to compare between the widened token and the reference template .The apparatus of . claim 10 , wherein the gain and noise adapter is able to provide a modified reference template having a peak energy level substantially equivalent to the difference between the peak energy level and the average noise energy level of the widened token .", "label": "", "metadata": {}, "score": "86.31691"}
{"text": "However , continuous density HMMs require a considerable amount of training data and require a longer recognition computation which has deterred their use in most commercial speech recognition systems .Accordingly , a significant problem in continuous speech recognition systems has been the use of continuous density HMMs for achieving high recognition accuracy .", "label": "", "metadata": {}, "score": "86.53715"}
{"text": "However , continuous density HMMs require a considerable amount of training data and require a longer recognition computation which has deterred their use in most commercial speech recognition systems .Accordingly , a significant problem in continuous speech recognition systems has been the use of continuous density HMMs for achieving high recognition accuracy .", "label": "", "metadata": {}, "score": "86.53715"}
{"text": "However , continuous density HMMs require a considerable amount of training data and require a longer recognition computation which has deterred their use in most commercial speech recognition systems .Accordingly , a significant problem in continuous speech recognition systems has been the use of continuous density HMMs for achieving high recognition accuracy .", "label": "", "metadata": {}, "score": "86.53715"}
{"text": "Three components are employed to characterize a pixel : a convolved , quantized gray - level component , a pixel relative position component , and a pixel major stroke direction component .These components are organized as an observation vector , which is continuous in nature , invariant in different font sizes , and flexible for use in various quantization processes .", "label": "", "metadata": {}, "score": "86.62564"}
{"text": "The sum of all the probabilities of transition from a state to another state or back to the same state must equal 1 .For example , a transition probability is associated with each transition from state 210 indicated by arrows 230 , 231 and 232 .", "label": "", "metadata": {}, "score": "86.69512"}
{"text": "Switching block 18 can be implemented in hardware or software .However , the speech recognition system is not limited to executing on a data processor .Other types of executable mediums can be used , such as but not limited to , a computer readable storage medium which can be a memory device , compact disc , or floppy disk .", "label": "", "metadata": {}, "score": "86.71796"}
{"text": "Switching block 18 can be implemented in hardware or software .However , the speech recognition system is not limited to executing on a data processor .Other types of executable mediums can be used , such as but not limited to , a computer readable storage medium which can be a memory device , compact disc , or floppy disk .", "label": "", "metadata": {}, "score": "86.71796"}
{"text": "Switching block 18 can be implemented in hardware or software .However , the speech recognition system is not limited to executing on a data processor .Other types of executable mediums can be used , such as but not limited to , a computer readable storage medium which can be a memory device , compact disc , or floppy disk .", "label": "", "metadata": {}, "score": "86.71796"}
{"text": "For example , referring again to pixel map 100 in FIG .1 , the two adjacent columns in region 112 each have two transitions and the same number of black and white pixels , corresponding to a superstate having three states .", "label": "", "metadata": {}, "score": "86.730415"}
{"text": "[ 0046 ] .In accordance with a preferred embodiment of the present invention , the noiseless templates are padded with one blank frame oil either end prior to adapting them with the appropriate gain level and noise structure ; this is performed in the template padder 64 .", "label": "", "metadata": {}, "score": "86.77609"}
{"text": "A speech recognition method as claimed in claim 28 , characterized in that the display comprises a cathode ray tube .A speech recognition method as claimed in claim 28 , characterized in that the display comprises a liquid crystal display , .", "label": "", "metadata": {}, "score": "86.867584"}
{"text": "A speech recognition method as claimed in claim 28 , characterized in that the display comprises a cathode ray tube .A speech recognition method as claimed in claim 28 , characterized in that the display comprises a liquid crystal display , .", "label": "", "metadata": {}, "score": "86.867584"}
{"text": "claim 1 , wherein modifying a reference template comprises : . modifying the reference template to provide a modified reference template having a peak energy level of substantially equivalent to the difference between the peak energy level and the average noise energy level of the widened token .", "label": "", "metadata": {}, "score": "87.28555"}
{"text": "claim 1 , wherein modifying a reference template comprises : . modifying the reference template to provide a modified reference template having a peak energy level of substantially equivalent to the difference between the peak energy level and the average noise energy level of the widened token .", "label": "", "metadata": {}, "score": "87.28555"}
{"text": "A speech recognition system as claimed in claim 10 , characterized in that the display comprises a cathode ray tube .A speech recognition system as claimed in claim 10 , characterized in that the display comprises a liquid crystal display .", "label": "", "metadata": {}, "score": "87.34779"}
{"text": "A speech recognition system as claimed in claim 10 , characterized in that the display comprises a cathode ray tube .A speech recognition system as claimed in claim 10 , characterized in that the display comprises a liquid crystal display .", "label": "", "metadata": {}, "score": "87.34779"}
{"text": "A comparison processor 40 compares the closeness of the feature value of each feature vector signal to the parameter values of the prototype vector signals to obtain prototype match scores for each feature vector signal and each prototype vector signal .Table 3 illustrates a hypothetical example of prototype match scores for the feature vector signals of Table 1 , and the prototype vector signals of Table 2 .", "label": "", "metadata": {}, "score": "87.39061"}
{"text": "A comparison processor 40 compares the closeness of the feature value of each feature vector signal to the parameter values of the prototype vector signals to obtain prototype match scores for each feature vector signal and each prototype vector signal .Table 3 illustrates a hypothetical example of prototype match scores for the feature vector signals of Table 1 , and the prototype vector signals of Table 2 .", "label": "", "metadata": {}, "score": "87.39061"}
{"text": "[ 0066 ] .Referring back to FIG .3 , the decision unit 62 determines the best match based on the DTW scores w_score of the whole path where the best template Best_Template is defined as the one for which the score is minimal .", "label": "", "metadata": {}, "score": "87.4383"}
{"text": "Tom Claes and Dirk Van Compemolle , \" SNR - Normalization for Robust Speech Recognition \" , ICASSP 96,1996 , pages 331 - 334 ; .Vijay Raman an d Vidhya Ramanujam , \" Robustness Issues and Solution s in Speech Recognition Based Telephony Services \" , ICASSP 97,1997 , pages 1523 - 1526 ; and .", "label": "", "metadata": {}, "score": "87.977875"}
{"text": "In addition , a weighting factor for each context - dependent senone is calculated and stored in lambda table storage 26 for use by the recognition engine 34 .The lambda table storage 26 holds lambda values indexed by context - dependent HMMs .", "label": "", "metadata": {}, "score": "88.0311"}
{"text": "In addition , a weighting factor for each context - dependent senone is calculated and stored in lambda table storage 26 for use by the recognition engine 34 .The lambda table storage 26 holds lambda values indexed by context - dependent HMMs .", "label": "", "metadata": {}, "score": "88.0311"}
{"text": "In addition , a weighting factor for each context - dependent senone is calculated and stored in lambda table storage 26 for use by the recognition engine 34 .The lambda table storage 26 holds lambda values indexed by context - dependent HMMs .", "label": "", "metadata": {}, "score": "88.0311"}
{"text": "Tullken , Application of the Grey Box Approach to Parameter Estimation in Physicochemical Models ( IEEE , 1991 , Proceedings of the 3oth Conference on Decision Control , Brighton , England , Dec. 1991 ) .Free format text : TERMINATION AND RELEASE OF SECURITY INTEREST IN PATENT RIGHTS;ASSIGNOR : JPMORGAN CHASE BANK , N.A. ( FORMERLY KNOWN AS THE CHASE MANHATTAN BANK ) , AS ADMINISTRATIVE AGENT;REEL / FRAME:018584/0446 A method and system for achieving an improved recognition accuracy in speech recognition systems which utilize continuous density hidden Markov models to represent phonetic units of speech present in spoken speech utterances is provided .", "label": "", "metadata": {}, "score": "88.16933"}
{"text": "Permitted transitions between superstates are denoted by arrows such as 230 and 231 .Arrows such as 232 indicate that a transition can occur from a superstate to itself .Similarly , transitions between states are denoted by arrows such as 240 and 241 and transitions to the same state by arrows such as 242 .", "label": "", "metadata": {}, "score": "88.62068"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .The foregoing and other features and advantages of the invention will be apparent from the following more particular description of the preferred embodiment of the invention , as illustrated in the accompanying drawings in which like reference characters refer to the same elements throughout the different views .", "label": "", "metadata": {}, "score": "89.7899"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .The foregoing and other features and advantages of the invention will be apparent from the following more particular description of the preferred embodiment of the invention , as illustrated in the accompanying drawings in which like reference characters refer to the same elements throughout the different views .", "label": "", "metadata": {}, "score": "89.7899"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .The foregoing and other features and advantages of the invention will be apparent from the following more particular description of the preferred embodiment of the invention , as illustrated in the accompanying drawings in which like reference characters refer to the same elements throughout the different views .", "label": "", "metadata": {}, "score": "89.7899"}
{"text": "This feature indicates the location of each pixel in the column regardless of the point size of the printed characters .In this way , the feature could be more robust in different applications .The third component value of O xy is the direction of the major stroke in which the pixel resides .", "label": "", "metadata": {}, "score": "89.89582"}
{"text": "Current Duration Penalty ( DP ) : .The path keeps record of the penalty it is paying for the current state .The penalty calculation is based on how long the path has been staying in this state , plus how long it will be staying before change to another state .", "label": "", "metadata": {}, "score": "89.98207"}
{"text": "In the example used herein , and as illustrated in FIGS .3 - 5 , the HMM has five states that are used regardless of the length of the utterance .Those skilled in the art will recognize that any number of states may be used , and it is envisioned that more than ten states will be employed for each utterance .", "label": "", "metadata": {}, "score": "90.25096"}
{"text": "for each state having a lower state transition threshold , assigning a proportional out of state transition penalty to an out of state transition if the dwell time has not reached the lower state transition threshold ; .for each state having an upper state transition threshold , assigning a proportional self loop penalty if some maximum number of frames assigned to that state has been exceeded .", "label": "", "metadata": {}, "score": "90.70264"}
{"text": "The process of matching a feature vector to a phoneme then entails matching a feature vector to the senones associated with the states of a HMM representing the phoneme .Thus , the linguistic expression can be composed of senones corresponding to states of a sequence of HMMs .", "label": "", "metadata": {}, "score": "90.74298"}
{"text": "The process of matching a feature vector to a phoneme then entails matching a feature vector to the senones associated with the states of a HMM representing the phoneme .Thus , the linguistic expression can be composed of senones corresponding to states of a sequence of HMMs .", "label": "", "metadata": {}, "score": "90.74298"}
{"text": "The process of matching a feature vector to a phoneme then entails matching a feature vector to the senones associated with the states of a HMM representing the phoneme .Thus , the linguistic expression can be composed of senones corresponding to states of a sequence of HMMs .", "label": "", "metadata": {}, "score": "90.74298"}
{"text": "The count n.sub.xy is the number of occurrence ; of the bigram W.sub.x W.sub.y in the training text .The values of the coefficients . lambda .sub.1 , . lambda . sub.2 , . lambda . sub.3 , and . lambda . sub.4 in equations [ 10 ] and [ 15 ] may be estimated by the deleted interpolation method .", "label": "", "metadata": {}, "score": "90.773315"}
{"text": "Lee , Kai Fu , Context Dependent Phonetic Hidden Markov Models for Speaker Independent Continuous Speech Recognition , IEEE Transactions on Acoustics , Speech and Signal Processing ; Apr. 1990 ; pp .347 362 .The present invention provides a method for recognizing connected and degraded text embedded in a gray - scale image .", "label": "", "metadata": {}, "score": "91.21867"}
{"text": "claim 14 , wherein the instructions when executed further result in : . performing a noise adapted dynamic time warping ( DTW ) operation by comparing between the widened token and the noise adapted reference template .Description .REFERENCE APPLICATION .", "label": "", "metadata": {}, "score": "91.46872"}
{"text": "claim 14 , wherein the instructions when executed further result in : . performing a noise adapted dynamic time warping ( DTW ) operation by comparing between the widened token and the noise adapted reference template .Description .REFERENCE APPLICATION .", "label": "", "metadata": {}, "score": "91.46872"}
{"text": "In equations [ 11]-[14 ] the count n xyz is the number of occurrences of the trigram W x W y W z in a large body of training text .The count n xy is the number of occurrence ; of the bigram W x W y in the training text .", "label": "", "metadata": {}, "score": "91.73987"}
{"text": "As the amount of training data for the context - dependent models gets large , \u03bb will approach 1.0 and the output pdf will be heavily weighed .With a small amount of training data for the context - dependent model , \u03bb will approach 0.0 and the output pdf will be weighed less .", "label": "", "metadata": {}, "score": "92.0432"}
{"text": "As the amount of training data for the context - dependent models gets large , \u03bb will approach 1.0 and the output pdf will be heavily weighed .With a small amount of training data for the context - dependent model , \u03bb will approach 0.0 and the output pdf will be weighed less .", "label": "", "metadata": {}, "score": "92.0432"}
{"text": "As the amount of training data for the context - dependent models gets large , \u03bb will approach 1.0 and the output pdf will be heavily weighed .With a small amount of training data for the context - dependent model , \u03bb will approach 0.0 and the output pdf will be weighed less .", "label": "", "metadata": {}, "score": "92.0432"}
{"text": "Training engine 20 utilizes the feature vectors to estimate the parameters of the HMMs which will represent the phonemes present in the training data and to compute a set of weighting factors for use by the recognition engine 34 .A more detailed description of the method employed by training engine 20 is presented below with reference to FIGS . 2 - 6 .", "label": "", "metadata": {}, "score": "92.36378"}
{"text": "Training engine 20 utilizes the feature vectors to estimate the parameters of the HMMs which will represent the phonemes present in the training data and to compute a set of weighting factors for use by the recognition engine 34 .A more detailed description of the method employed by training engine 20 is presented below with reference to FIGS . 2 - 6 .", "label": "", "metadata": {}, "score": "92.36378"}
{"text": "Training engine 20 utilizes the feature vectors to estimate the parameters of the HMMs which will represent the phonemes present in the training data and to compute a set of weighting factors for use by the recognition engine 34 .A more detailed description of the method employed by training engine 20 is presented below with reference to FIGS . 2 - 6 .", "label": "", "metadata": {}, "score": "92.36378"}
{"text": "This CDP information is calculated and stored in the node every time it is reached by a backward search path .Notice that CDPs of the same node could be different when reached by different backward search paths .Other Updates : all probabilities are in minus log scale .", "label": "", "metadata": {}, "score": "92.40772"}
{"text": "The third component value is the direction of the major stroke in which the pixel resides .We threshold the whole image ( the threshold algorithm is explained in the experimental setup ) .This component thus has five distinct values .", "label": "", "metadata": {}, "score": "93.08292"}
{"text": "Each twenty millisecond sample of the digital signal is analyzed by spectrum analyzer 50 in order to obtain the amplitude of the digital signal sample in each of , for example , twenty frequency bands .Preferably , spectrum analyzer 50 also generates a twenty - first dimension signal representing the total amplitude or total power of the twenty millisecond digital signal sample .", "label": "", "metadata": {}, "score": "93.205"}
{"text": "The candidate word generator 20 receives the source text from source text input device 12 , and receives translations of each word in the source text from a source - text translation store 22 .From the source text and from the translations , candidate word generator 20 generates a set of candidate words consisting solely of words in the target language which are partial or full translations of words in the source text .", "label": "", "metadata": {}, "score": "93.76128"}
{"text": "In accordance with a preferred embodiment of the present invention , wide token builder 66 additionally takes X frames from either side of a standard token , where X is typically 8 .Thus , the wide token comprises X frames before the VAD begin point through X frames after the VAD end point .", "label": "", "metadata": {}, "score": "94.42038"}
{"text": "In accordance with a preferred embodiment of the present invention , wide token builder 66 additionally takes X frames from either side of a standard token , where X is typically 8 .Thus , the wide token comprises X frames before the VAD begin point through X frames after the VAD end point .", "label": "", "metadata": {}, "score": "94.42038"}
{"text": "In other words , only transition into next superstate or to itself is possible .For the jth superstate , \u03bb j consists of : .N j , the number of states for the 1D HMM within superstate j. Similar to N , N j is also determined by the topology of the character .", "label": "", "metadata": {}, "score": "94.50306"}
{"text": "Comment : Because of the voluminous nature of this book , applicants have not included it with this information disclosure statement but have elected to cite the reference .If the Examiner requires the book during prosecution of this application , Applicants will provide a copy of it .", "label": "", "metadata": {}, "score": "94.58199"}
{"text": "TABLE 1______________________________________time t1 t2 t3 t4 t5Feature Value 0.18 0.52 0.96 0.61 0.84 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .", "label": "", "metadata": {}, "score": "95.17257"}
{"text": "In step 50 of FIG .2 , weighting or interpolation factors for each content - dependent senone are generated and are denoted by the mathematical symbol , \u03bb .The weighting factors will be used to interpolate the output probabilities of the context - independent HMM with the output probabilities of the context - dependent HMM .", "label": "", "metadata": {}, "score": "95.21996"}
{"text": "In step 50 of FIG .2 , weighting or interpolation factors for each content - dependent senone are generated and are denoted by the mathematical symbol , \u03bb .The weighting factors will be used to interpolate the output probabilities of the context - independent HMM with the output probabilities of the context - dependent HMM .", "label": "", "metadata": {}, "score": "95.21996"}
{"text": "In step 50 of FIG .2 , weighting or interpolation factors for each content - dependent senone are generated and are denoted by the mathematical symbol , \u03bb .The weighting factors will be used to interpolate the output probabilities of the context - independent HMM with the output probabilities of the context - dependent HMM .", "label": "", "metadata": {}, "score": "95.21996"}
{"text": "19 , p. 41- 47 ( 1986 ) .For each pixel , the length of strokes ( in terms of continuous black pixels ) is computed in four directions : 0 \u00b0 , 45 \u00b0 , 90 \u00b0 , 135 \u00b0 , and the longest stroke is selected which passes the pixel as its major stroke direction .", "label": "", "metadata": {}, "score": "95.53969"}
{"text": "The memory 122 can be implemented using RAM , EEPROM , ROM , flash ROM , or the like , or a combination of two or more of these memory types .With reference to FIG .2 , the audio signals received by microphone 114 are converted to digital signals in an analog - to - digital converter 202 of audio circuit 112 .", "label": "", "metadata": {}, "score": "95.589165"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .[ 0032 ] .The present invention will be understood and appreciated more fully from the following detailed description taken in conjunction with the appended drawings in which : .[ 0033 ] .[ 0033]FIGS .", "label": "", "metadata": {}, "score": "95.671616"}
{"text": "The output distributions for each context - dependent state are clustered forming senones which are stored in senone table storage 30 .The senone table storage 30 , in general , holds senones for both context - dependent and context - independent HMMs .", "label": "", "metadata": {}, "score": "95.6781"}
{"text": "The output distributions for each context - dependent state are clustered forming senones which are stored in senone table storage 30 .The senone table storage 30 , in general , holds senones for both context - dependent and context - independent HMMs .", "label": "", "metadata": {}, "score": "95.6781"}
{"text": "The output distributions for each context - dependent state are clustered forming senones which are stored in senone table storage 30 .The senone table storage 30 , in general , holds senones for both context - dependent and context - independent HMMs .", "label": "", "metadata": {}, "score": "95.6781"}
{"text": "Nonetheless , this still results in a very robust models set , as will become evident from the experimental results .Three testing sets were employed to evaluate the performance of the techniques described herein .The size of each set is about 200 words , with more than 1,000 characters , and length varying from 2 to 12 characters .", "label": "", "metadata": {}, "score": "95.975845"}
{"text": "The call processor 108 is coupled to a memory 110 .Memory 110 contains RAM , electronically erasable programmable read only memory ( EEPROM ) , read only memory ( ROM ) , flash ROM , or the like , or a combination of two or more of these memory types .", "label": "", "metadata": {}, "score": "96.04761"}
{"text": "Lee , Kai Fu , Context Dependent Phonetic Hidden Markov Models for Speaker Independent Continuous Speech Recognition , IEEE Transactions on Acoustics , Speech and Signal Processing ; Apr. 1990 ; pp .347 362 .", "label": "", "metadata": {}, "score": "96.71623"}
{"text": "12 19 , Jun. 1990 .Comment : At the present time , a copy of this reference is unavailable to us .As soon as we are in receipt of it we will make the same available to the United States Patent & Trademark Office .", "label": "", "metadata": {}, "score": "96.77544"}
{"text": "TABLE 1 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ time t1 t2 t3 t4 t5 Feature Value 0.18 0.52 0.96 0.61 0.84 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .", "label": "", "metadata": {}, "score": "96.79501"}
{"text": "The ROM can be provided to store the device operating programs .An audio circuit 112 provides digitized signals from a microphone 114 to all processor 108 .The audio circuit 112 drives speaker 116 responsive to digital signals from the call processor 108 .", "label": "", "metadata": {}, "score": "96.88898"}
{"text": "Five \" superstates \" 210 - 214 are shown in state diagram 200 .Such superstates correspond respectively to the five vertical regions 110 - 114 in pixel map 100 .Note that all the columns of pixels in a given one of regions 110 - 114 are the same number of pixels .", "label": "", "metadata": {}, "score": "97.914764"}
{"text": "The rotation matrix used in rotator 66 may be obtained , for example , by classifying into M classes a set of 189 dimension spliced vectors obtained during a training session .The inverse of the covariance matrix for all of the spliced vectors in the training set is multiplied by the within - sample covariance matrix for all of the spliced vectors in all M classes .", "label": "", "metadata": {}, "score": "98.868454"}
{"text": "\" Pseudo \" in the sense that it is not a fully connected 2D network , but , still flexible enough to represent 2D word images .We call the big ovals the \" superstates \" .Each superstate is a 1D HMM itself .", "label": "", "metadata": {}, "score": "99.20206"}
{"text": "The acoustic match score generator 24 preferably generates two types of acoustic match scores : ( 1 ) a relatively fast , relatively less accurate acoustic match score , and ( 2 ) a relatively slow , relatively more accurate \" detailed \" acoustic : match score .", "label": "", "metadata": {}, "score": "99.25108"}
{"text": "For example , superstate 211 includes three states 220 - 222 representing the white , black and white pixels respectively in the rows of region 111 .However , it is to be understood that \" white pixels \" and \" black pixels \" are described for purposes of illustration .", "label": "", "metadata": {}, "score": "99.34148"}
{"text": "These endpoints are provided to the wide token builder 66 which extracts a wide token from the buffer 52 .[ 0043 ] .[0043]FIG .4 , to which reference is now briefly made , illustrates the data stored in buffer 52 .", "label": "", "metadata": {}, "score": "100.41562"}
{"text": "This feature vector is augmented by a twenty - first dimension having a value equal to the square root of the sum of the squares of the values of the other twenty dimensions .For each centisecond time interval , a concatenator 64 preferably concatenates nine twenty - one dimension feature vectors representing the one current centisecond time interval , the four preceding centisecond time intervals , and the four following centisecond time intervals to form a single spliced vector of 189 dimensions .", "label": "", "metadata": {}, "score": "100.42319"}
{"text": "TABLE 5 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ Prototype Vector Rank Scores ( alternative)______________________________________time t1 t2 t3 t4 t5Prototype VectorIdentification ValueP1 2 1 3 3 3P2 3 1 3 1 3P3 3 3 1 3 2P4 3 3 2 2 1P5 1 3 3 3 3 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .", "label": "", "metadata": {}, "score": "101.56275"}
{"text": "Senones are classified within like states ( e.g. , state 1 ) for each type of model ( e.g. , context - dependent vs. context - independent ) corresponding to the same phoneme .In this example , the context - independent HMM has senones 10 , 55 , and 125 corresponding to states 1 , 2 and 3 respectively .", "label": "", "metadata": {}, "score": "101.927444"}
{"text": "Senones are classified within like states ( e.g. , state 1 ) for each type of model ( e.g. , context - dependent vs. context - independent ) corresponding to the same phoneme .In this example , the context - independent HMM has senones 10 , 55 , and 125 corresponding to states 1 , 2 and 3 respectively .", "label": "", "metadata": {}, "score": "101.927444"}
{"text": "Senones are classified within like states ( e.g. , state 1 ) for each type of model ( e.g. , context - dependent vs. context - independent ) corresponding to the same phoneme .In this example , the context - independent HMM has senones 10 , 55 , and 125 corresponding to states 1 , 2 and 3 respectively .", "label": "", "metadata": {}, "score": "101.927444"}
{"text": "When \u03bb is small ( approaches 0.0 ) , the context - independent senone dominates .where .\u03bb is the weighting factor between 0 and 1 for senone sd ; .x is the feature vector ; . sd d is the senone associated with a state of a context - dependent HMM ; . sd i is the senone associated with the corresponding state of a context - independent HMM ; .", "label": "", "metadata": {}, "score": "103.270676"}
{"text": "When \u03bb is small ( approaches 0.0 ) , the context - independent senone dominates .where .\u03bb is the weighting factor between 0 and 1 for senone sd ; .x is the feature vector ; . sd d is the senone associated with a state of a context - dependent HMM ; . sd i is the senone associated with the corresponding state of a context - independent HMM ; .", "label": "", "metadata": {}, "score": "103.27072"}
{"text": "When \u03bb is small ( approaches 0.0 ) , the context - independent senone dominates .where .\u03bb is the weighting factor between 0 and 1 for senone sd ; .x is the feature vector ; . sd d is the senone associated with a state of a context - dependent HMM ; . sd i is the senone associated with the corresponding state of a context - independent HMM ; .", "label": "", "metadata": {}, "score": "103.27072"}
{"text": "National Academy of the Sciences , Washington , D.C. , Publication 1416 , 1966 .Primary Examiner : Fleming ; Michael R. Assistant Examiner : Hofiz ; Tarif R. Attorney , Agent or Firm : .Schechter ; Marc D. Government Interests .", "label": "", "metadata": {}, "score": "104.24513"}
{"text": "TABLE 4 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ Prototype Vector Rank Scores _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ time t1 t2 t3 t4 t5 Prototype Vector Identification Value P1 2 1 4 3 4 P2 3 1 3 1 3 P3 5 5 1 4 2 P4 4 3 2 2 1 P5 1 4 5 5 5 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .", "label": "", "metadata": {}, "score": "104.606606"}
{"text": "Lee , Kai Fu , Context Dependent Phonetic Hidden Markov Models for Speaker Independent Continuous Speech Recognition , IEEE Transactions on Acoustics , Speech and Signal Processing ; Apr. 1990 ; pp .347 362 .A method and system for achieving an improved recognition accuracy in speech recognition systems which utilize continuous density hidden Markov models to represent phonetic units of speech present in spoken speech utterances is provided .", "label": "", "metadata": {}, "score": "104.63599"}
{"text": "TABLE 4 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ Prototype Vector Rank Scores______________________________________time t1 t2 t3 t4 t5Prototype VectorIdentification ValueP1 2 1 4 3 4P2 3 1 3 1 3P3 5 5 1 4 2P4 4 3 2 2 1P5 1 4 5 5 5 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .", "label": "", "metadata": {}, "score": "107.32045"}
{"text": "The display processor is optional if additional processor support is desired for the device 100 .In particular , the display processor 120 provides display control signals to the display 126 and receives inputs from keys 124 .The display processor 120 can be implemented using a microprocessor , a microcontroller , a digital signal processor , a programmable logic unit , a combination thereof , or the like .", "label": "", "metadata": {}, "score": "113.01915"}
{"text": "Each 189 dimension spliced vector is preferably multiplied in a rotator 66 by a rotation matrix to rotate the spliced vector and to reduce the spliced vector to fifty dimensions .The rotation matrix used in rotator 66 may be obtained , for example , by classifying into M classes a set of 189 dimension spliced vectors obtained during a training session .", "label": "", "metadata": {}, "score": "114.29779"}
