{"text": "[ 0047]FIG .5 is a flowchart illustrating a method creating compact linguistic data .The method uses a word - list containing word frequency information to produce compact linguistic data , and includes word prefix indexing and statistical character substitution .", "label": "", "metadata": {}, "score": "32.3256"}
{"text": "In order to train the models , we took advantage of the Semcor ( release 1.6 ) .Moreover , we chose to apply a special treatment on words fulfilling two constraints : .To be i ) one of the most words to be disambiguated in the all - words task , ii ) one of the words to be disambiguated in the lexical sample task .", "label": "", "metadata": {}, "score": "34.218872"}
{"text": "At step 903 , if the size of the linguistic data is smaller than a configured maximum size , then the method continues at step 901 .Otherwise , the method concludes with the step 904 of creating a list of the words in the word - list without the suffixes contained in the inflection table .", "label": "", "metadata": {}, "score": "35.298515"}
{"text": "At step 903 , if the size of the linguistic data is smaller than a configured maximum size , then the method continues at step 901 .Otherwise , the method concludes with the step 904 of creating a list of the words in the word - list without the suffixes contained in the inflection table .", "label": "", "metadata": {}, "score": "35.298515"}
{"text": "The data is preprocessed : SGML tags are eliminated , the text is tokenized , part of speech tagged and Named Entities are identified .2 ) Compound concepts are identified : we determine the maximum sequence of words which form a compound word defined in WordNet .", "label": "", "metadata": {}, "score": "35.64209"}
{"text": "0096]FIG .9 is a flowchart illustrating a method of inflection analysis .The system and method of creating compact linguistic data may alternatively include the method of inflection analysis , in which both a list of words that have frequencies higher than a minimum specified frequency and an inflection table are created .", "label": "", "metadata": {}, "score": "35.869904"}
{"text": "Likewise , if only one possible sense tag was observed for any POS - level lemma analysis , then this unambiguous sense tag was also returned .keywords : word sense disambiguation , morphological analysis , part - of - speech tagging , highly inflected languages .", "label": "", "metadata": {}, "score": "36.599556"}
{"text": "Frequencies of words appearing in a corpus are calculated .Each unique character in the words is mapped to a character index , and characters in the words are replaced with the character indexes .System and method of creating and using compact linguistic data US 7809553 B2 .", "label": "", "metadata": {}, "score": "36.743935"}
{"text": "The inflection table is created based on statistical suffix analysis , and encapsulates the linguistic rules for word creation in the language of the corpus .The inflection tables make it possible to produce more than one word using the basic word forms stored in the inflection table , ensuring that more words are covered by the linguistic data , while the basic word - list remains compact .", "label": "", "metadata": {}, "score": "36.8245"}
{"text": "Based on the SEMCOR corpus , semantic word experts are trained for the multi - sense words .The word experts combine different types of learning algorithms , viz .memory - based learning ( TiMBL ) and rule induction ( Ripper ) , which take as input different knowledge sources : . - The input for the memory - based learner is a feature vector consisting of the target word and lemma and three words to its right and left , along with a more fine - grained part - of - speech .", "label": "", "metadata": {}, "score": "36.876873"}
{"text": "In order to cope with the lack of training corpus , we have introduced rough semantic features as Wordnet coarse Semantic Classes ( SC ) in the question set .This multi - level view of the context improves dramatically the coverage of SCT on various productions .", "label": "", "metadata": {}, "score": "36.916862"}
{"text": "The learning word - list includes an updated frequencies table that contains words with updated frequency and a new words table that contains new words .Both of these tables include words which are encoded as in the base linguistic data , using the same character - mapping 608 ( .", "label": "", "metadata": {}, "score": "36.992493"}
{"text": "The learning word - list includes tables for frequency modification and for the addition of new words .FIG .8 is a flowchart illustrating a method of frequency modification .The method proceeds on the assumption that the base linguistic data , which is the linguistic data compiled as described above before any learning data is gathered , has correct frequency information in general .", "label": "", "metadata": {}, "score": "36.99937"}
{"text": "6 ) , and inserted into the beginning of the new words table .Finally , the frequencies and sorting index of the learning word - list table are updated .FIG .9 is a flowchart illustrating a method of inflection analysis .", "label": "", "metadata": {}, "score": "37.17061"}
{"text": "9 .[0070 ] .The linguistic data also contains word definition tables 614 .A word definition table stores words from a single word group and frequencies associated with the words , and can be either simple or complex .", "label": "", "metadata": {}, "score": "37.281982"}
{"text": "System and method of creating and using compact linguistic data US 20040006455 A1 .Zusammenfassung .A system and method of creating and using compact linguistic data are provided .Frequencies of words appearing in a corpus are calculated .Each unique character in the words is mapped to a character index , and characters in the words are replaced with the character indexes .", "label": "", "metadata": {}, "score": "37.6231"}
{"text": "The examples from WordNet 1.7 , in which the words from the synsets they correspond to are disambiguated .2 ) SemCor - which was translated such that it points to senses from WordNet 1.7 .3 )A large additional set of sense tagged word - word pairs , generated from the pairs created at step 1 and 2 , based on a set of heuristics .", "label": "", "metadata": {}, "score": "37.690926"}
{"text": "It can be broken up into two distinctive phases .The first phase consists in identifying the coarse Semantic Classes ( SC ) related to each word in the test .For this purpose , we have applied the Viterbi algorithm , using a trisem model , in order to select the most likely path in the SC graph built for each sentence .", "label": "", "metadata": {}, "score": "37.948055"}
{"text": "These suffixes do not always match the existing counterparts in the grammar of the given language , but rather the suffix finding is based on the number of occurrences of suffixes in the word - list .The method continues with the step 902 of updating the inflection table with the suffixes found in the previous step .", "label": "", "metadata": {}, "score": "38.093475"}
{"text": "No information from WordNet is utilized by this system .This system uses a filter to perform feature identification prior to learning .All bigrams ( two word sequences ) that meet the following criteria form a set of candidate features : .", "label": "", "metadata": {}, "score": "38.162422"}
{"text": "3 is a table showing results of an experiment utilizing the system of .FIG .1 .FIG .4 is a block diagram of a system for building a translation lexicon according to another embodiment .FIG .5 is a suffix tree .", "label": "", "metadata": {}, "score": "38.40587"}
{"text": "The method continues with a step of creating a character - mapping table for encoding the words in the word - list by replacing characters in the words with associated character indexes contained in the character - mapping table .The method continues with a step of separating the words in the word - list into groups , wherein words in each group have a common prefix .", "label": "", "metadata": {}, "score": "38.54697"}
{"text": "The inflection table stores word suffixes which may be used in word definitions .A method of inflection analysis in illustrated in .FIG .9 .The linguistic data also contains word definition tables 614 .A word definition table stores words from a single word group and frequencies associated with the words , and can be either simple or complex .", "label": "", "metadata": {}, "score": "38.710194"}
{"text": "This vector can be translated into the other language , since we already know the translations of the seed words are already known .The lexicon builder 125 can search for the best matching context vector in the target language , and decide upon the corresponding word to construct a word mapping .", "label": "", "metadata": {}, "score": "38.819633"}
{"text": "The inflection table and the list of words without suffixes can then be encoded as described above in reference to FIG .5 .When the method of inflection analysis is used , the resulting compact linguistic data as illustrated in FIG .", "label": "", "metadata": {}, "score": "39.00911"}
{"text": "From an unannotated Italian web - mined corpus ( approx 6 M words ) , we selected all the sentences containing these words ( the words v ) .We considered the contexts containing the word as being representative for the corresponding sense with the appropriate weight , and we used a Bayes similarity - based model to run an adaptive clustering model , in a k - means fashion .", "label": "", "metadata": {}, "score": "39.249588"}
{"text": "This system uses a filter to perform feature identification prior to learning .All bigrams ( two word sequences ) that meet the following criteria form a set of candidate features : .The training examples are converted into feature vectors , where each feature represents whether a candidate feature occurs in the context of a specific training example .", "label": "", "metadata": {}, "score": "39.292236"}
{"text": "When the method ends , all of the source files which comprise the corpus have been filtered .[ 0039 ] .[ 0039]FIG .4 is flowchart illustrating a method of word frequency calculation .The method utilizes the filtered - words files that were produced by the method illustrated in FIG .", "label": "", "metadata": {}, "score": "39.382835"}
{"text": "In accordance with the present invention , a method is provided for performing part of speech tagging for content - word pairs in a natural language text processing system .Content - word pairs are first identified in a large corpus of text used for training purposes .", "label": "", "metadata": {}, "score": "39.439957"}
{"text": "[ 0092 ] .The learning word - list includes an updated frequencies table that contains words with updated frequency and a new words table that contains new words .Both of these tables include words which are encoded as in the base linguistic data , using the same character - mapping 608 ( FIG .", "label": "", "metadata": {}, "score": "39.476307"}
{"text": "The common prefixes for words in each word definition table are removed .A method of creating compact linguistic data is also provided .The method begin with a step of creating a word - list comprising a plurality of words occurring most frequently in a corpus .", "label": "", "metadata": {}, "score": "39.579506"}
{"text": "FIG .3 shows results of the English - German implementation .\" Entries \" indicate the number of correct lexicon entries that were added to a seed lexicon of 1337 identically spelled words , and \" Corpus \" indicates how well the resulting translation lexicon performs compared to the actual word - level translations in a parallel corpus .", "label": "", "metadata": {}, "score": "39.76752"}
{"text": "The inflection tables make it possible to produce more than one word using the basic word forms stored in the inflection table , ensuring that more words are covered by the linguistic data , while the basic word - list remains compact .", "label": "", "metadata": {}, "score": "39.778763"}
{"text": "No information from WordNet is utilized by this system .This system uses a filter to perform feature identification prior to learning .Two different kinds of bigrams are identified as candidate features .The first is a consecutive two word sequence that meets the following criteria : .", "label": "", "metadata": {}, "score": "39.791542"}
{"text": "The method continues with the step 504 of separating the words in the wordlist into groups .Words in each group have a common prefix of a given length and are sorted by frequency .Words are initially grouped by prefixes that are two characters long .", "label": "", "metadata": {}, "score": "39.917038"}
{"text": "Semantic domain information : using a semantic hierarchy linked to WordNet 1.6 .The domain weigths depends on the words of the context , the number of senses of these words and their distribution on the context .Multiwords have been preprocessed separately in a previous task .", "label": "", "metadata": {}, "score": "40.01817"}
{"text": "Subsequently , NODE was used on the Senseval-2 training data ( which had not otherwise been used ) .NODE definitions were then automatically mapped into WordNet , so that results could be compared with the use of WordNet on the training data .", "label": "", "metadata": {}, "score": "40.031616"}
{"text": "A system of creating compact linguistic data is provided .The system comprises a corpus and linguistic data analyzer .The linguistic data analyzer calculates frequencies of words appearing in the corpus .The linguistic data analyzer also maps each unique character in the words to a character index , and replaces each character in the words with the character index to which the character is mapped .", "label": "", "metadata": {}, "score": "40.089317"}
{"text": "The method concludes with step 514 .At this step , the linguistic data resulting from the method has been stored in the tables that have been created .The data tables , including the character - mapping table , the substitution table , the offset table and the word definition tables , are stored in an output file .", "label": "", "metadata": {}, "score": "40.352104"}
{"text": "[ 0084 ] .[0084]FIG . 8 is a flowchart illustrating a method of frequency modification .The method proceeds on the assumption that the base linguistic data , which is the linguistic data compiled as described above before any learning data is gathered , has correct frequency information in general .", "label": "", "metadata": {}, "score": "40.542877"}
{"text": "These relationships were approximated using heuristic patterns over base noun phrase bracketed sentences ( Florian and Ngai , 2001 ) .Additional features included parts - of - speech and lemmas in all syntactic positions , extracted using a Brill - style POS tagger and morphological analysis based on Yarowsky and Wicentowski , 2000 ) .", "label": "", "metadata": {}, "score": "40.574097"}
{"text": "A new measure of the fixed or variable nature of such word pairs is created and used to classify word pairs as either noun - verb , adjective - noun , or verb - noun .A method for performing thematic part - of - speech tagging for collocations having content - word pairs in a natural language text processing system comprising the steps of : . identifying collocations of content - word pairs in a large corpus of text ; . calculating , for each of said collocation content - word pair identified , a variability factor which is a measure of variability of said collocation content - word pairs occurring in said text ; . storing said collocation content word pairs and associated variability factors in a collocation database ; and .", "label": "", "metadata": {}, "score": "40.61258"}
{"text": "The method begin with a step of creating a word - list comprising a plurality of words occurring most frequently in a corpus .The method continues with a step of sorting the words in the wordlist alphabetically .The method continues with a step of creating a character - mapping table for encoding the words in the word - list by replacing characters in the words with associated character indexes contained in the character - mapping table .", "label": "", "metadata": {}, "score": "40.698215"}
{"text": "The basic design is similar to CL Research 's system for Senseval-1 ; however , many of the disambiguation routines were not able to be reimplemented by the submission date .For Senseval-2 , the implemented routines included special routines for examining multiword units and examining contextual clues ( both specific word , Lesk - style use of definition content words , and subject matter analyses ) ; syntactic constraints have not yet been employed .", "label": "", "metadata": {}, "score": "40.764694"}
{"text": "Additional features included parts - of - speech and lemmas in all syntactic positions , extracted using JHU - developed algorithms based on minimally supervised learning ( including Yarowsky and Wicentowski , 2000 ; Cucerzan and Yarowsky , 2000 ) .The output of each subsystem was merged by a classifier combination algorithm using weighted and thresholded voting and score combination .", "label": "", "metadata": {}, "score": "40.848938"}
{"text": "Thanks to previous experiments , we noticed that for some words or even some synsets , the information useful for disambiguation purpose are present in variable window sizes around the term to be disambiguated .In order to select automatically the appropriate window size , we have designed a mixed approach combining the SCT and a long - range similarity measure ( like in document retrieval ) , in some particular cases .", "label": "", "metadata": {}, "score": "40.977516"}
{"text": "FIG .2 ) , and is the output of the method illustrated by .FIG .5 .The format allows linguistic data to be stored with or without word frequency information .When the linguistic data includes frequency information , learning capabilities , which are described below , can be implemented , and the data can be used to predict input entered with a reduced keyboard .", "label": "", "metadata": {}, "score": "40.980495"}
{"text": "There were a collection of first level word sense classifiers , mainly using Naive Bayes methods , but also including vector space , n - gram , and KNN classifiers , and implementing a range of windowing , distance weighting , and smoothing techniques .", "label": "", "metadata": {}, "score": "41.482574"}
{"text": "The words are grouped by common prefixes , and each prefix is mapped to location information for the group of words which start with the prefix .wherein the compact linguistic data includes the unique characters , the character indexes , the substitution indexes , the location information , the groups of words and the frequencies of the words .", "label": "", "metadata": {}, "score": "41.49734"}
{"text": "The system is based on Yarowsky 's decision list .It sorts the features according to the log - likelihood value and chooses the sense of the feature with the highest value .Features occurring only once were pruned .In the case of the English all - words task , Semcor 1.6 was used for training , via a automatically produced WordNet 1.6 - 1.7 map .", "label": "", "metadata": {}, "score": "41.62269"}
{"text": "These assignments are then used as seeds for a bootstrapping algorithm ( a la Yarowsky , 1995 ) which disambiguates the whole corpus .The result for the lexicographer is a number of \" Sense - Sketches \" , showing significant patterns for the individual senses of the word , while for automatic WSD we have a decision list of clues for sense disambiguation , consisting of grammatical relation patterns , words - in - context , and n - grams .", "label": "", "metadata": {}, "score": "41.68917"}
{"text": "The parameter smoothing is performed by recasting the WordNet hierarchy as Bayesian networks , in a processed called Semantic Backoff .The sentential structure is then used to construct another Bayesian network , quantitated via the parameters established earlier .WSD is performed by the Maximum A Posteriori estimation , using the Join Tree inferencing algorithm .", "label": "", "metadata": {}, "score": "41.7939"}
{"text": "Note that the parallel corpus belongs to a different domain than the comparable corpus .Also the parallel corpus is extremely small .For low density languages , such a corpus can be built manually .When given as input the comparable corpora described above and the bilingual lexicon of 6,900 entries , the algorithm 1000 found 33,926 parallel sequences , with length between three and seven words .", "label": "", "metadata": {}, "score": "41.965492"}
{"text": "The method continues with the step 502 of creating a character - mapping table .The character - mapping table is used to encode words in a subsequent step of the method .When encoding is performed , the characters in the original words are replaced with the character indexes of those characters in the character - mapping table .", "label": "", "metadata": {}, "score": "42.028194"}
{"text": "With the aid of the matrix we have enriched sense descriptions .We have added the information of the five first hyponyms where possible and also we have used the matrix to filter the context of the words to be disambiguated .", "label": "", "metadata": {}, "score": "42.04731"}
{"text": "For the few words which are still ambiguous at this point , we assign the most frequent sense from WordNet .( An initial , simpler version of this algorithm is described in \" An Iterative Approach to Word Sense Disambiguation \" , Mihalcea & Moldovan , in Proceedings of Flairs-2000 ) .", "label": "", "metadata": {}, "score": "42.080765"}
{"text": "[ 0046 ] .The method illustrated in FIG .4 allows even very large corpora to be processed by a single computer .The resulting word - list contains up to a predefined limited number of most frequently occurring words in the corpus , and the absolute frequencies associated with the words .", "label": "", "metadata": {}, "score": "42.08851"}
{"text": "A system and method of creating and using compact linguistic data are provided .Frequencies of words appearing in a corpus are calculated .Each unique character in the words is mapped to a character index , and characters in the words are replaced with the character indexes .", "label": "", "metadata": {}, "score": "42.111084"}
{"text": "4 is flowchart illustrating a method of word frequency calculation .The method utilizes the filtered - words files that were produced by the method illustrated in .FIG .3 .The words from the filtered - words file are loaded into a word - tree .", "label": "", "metadata": {}, "score": "42.19172"}
{"text": "With the aid of the matrix we have enriched sense descriptions and also we have used it to filter the context of the words to be disambiguated taking into account the part of speech involved .As back - off strategies we have used the same one discarding the frequency filter and for the few words left the first sense .", "label": "", "metadata": {}, "score": "42.293404"}
{"text": "With the aid of the matrix we have enriched sense descriptions and also we have used it to filter the context of the words to be disambiguated taking into account the part of speech involved .As back - off strategies we have used the same one discarding the frequency filter and for the few words left the first sense .", "label": "", "metadata": {}, "score": "42.293404"}
{"text": "Previous experiments [ Loupy , 2000 ] using the Semcor have shown some good results for word SC affectation ( about 90 % ) .For this reason , we could expect to achieve results at least as good as those obtained with an approach based on unisem model .", "label": "", "metadata": {}, "score": "42.314846"}
{"text": "The linguistic data analyzer also maps sequences of characters that appear in the words to substitution indexes , and replaces each sequence of characters in each word with the substitution index to which the sequence of characters are mapped .The linguistic data analyzer also arranges the words into groups where each group contains words that start with a common prefix , and maps each prefix to location information for the group of words which start with the prefix .", "label": "", "metadata": {}, "score": "42.34485"}
{"text": "In order to filter out such cases , the system 400 uses two simple heuristics : length and word content .The translation candidate must also be an open - class word .The algorithm 1000 for learning translations of unknown words is summarized in .", "label": "", "metadata": {}, "score": "42.427902"}
{"text": "The selected word is added to the learning word - list .The method continues with step 804 of obtaining the word with the maximum frequency of words in the prediction list that was presented to the user .The words in the prediction list and their corresponding frequencies may have been obtained from the word definition tables in base linguistic data , or from the learning word - list .", "label": "", "metadata": {}, "score": "42.483326"}
{"text": "From these , some high - quality lexical entries can be learned , but there will always be many words that are missing .These may be learned using the described methods .FIG .4 shows a system 400 for building a translation lexicon according to another embodiment .", "label": "", "metadata": {}, "score": "42.507996"}
{"text": "the method further comprises steps of : . creating a plurality of relative frequencies by applying a normalization function to the absolute frequencies stored in the word - list ; . creating frequency sets associated with the groups of words ; and .", "label": "", "metadata": {}, "score": "42.544018"}
{"text": "8 is a flowchart illustrating a method of frequency modification ; and .FIG .9 is a flowchart illustrating a method of inflection analysis .DETAILED DESCRIPTION .A system and method of creating and using compact linguistic data , based on word prefix indexing with statistical character substitution , is provided .", "label": "", "metadata": {}, "score": "42.551525"}
{"text": "6 ) as are used by the base linguistic data .Each learning word - list table also includes indexes for the beginnings of words in the table , frequency information associated with the words in the table , and a sorting index that specifies the alphabetically sorted order of the words .", "label": "", "metadata": {}, "score": "42.585144"}
{"text": "a text output device that displays the predicted word ; . wherein the compact linguistic data structure comprises : . a plurality of word definition tables for storing the words , each word definition table storing each of the words included in one of the groups and storing a plurality of frequencies associated with the words ; and .", "label": "", "metadata": {}, "score": "42.683212"}
{"text": "The words are grouped by common prefixes , and each prefix is mapped to location information for the group of words which start with the prefix .a linguistic data analyzer , wherein the linguistic data analyzer comprises instructions that , when executed by the processor , cause the processor to : . wherein the processor uses a structure to calculate the frequencies of the words appearing in the corpus such that more frequently occurring words are located at inner nodes of the structure and less frequently occurring words are located at outer nodes of the structure ; and .", "label": "", "metadata": {}, "score": "42.698524"}
{"text": "The linguistic data 100 is based on word prefix indexing with statistical character substitution , and is described in more detail below .The text input logic unit 102 may , for example , be implemented by computer instructions which are executed by a computer processor that is contained in a mobile device .", "label": "", "metadata": {}, "score": "42.76875"}
{"text": "The linguistic data analyzer also arranges the words into groups where each group contains words that start with a common prefix , and maps each prefix to location information for the group of words which start with the prefix .The compact linguistic data includes the unique characters , the character indexes , the substitution indexes , the location information , the groups of words and the frequencies of the words .", "label": "", "metadata": {}, "score": "42.939682"}
{"text": "In a GST of a set of strings , each path from the root to a leaf represents a suffix in one or more strings from the set .FIG .6 shows the GST 600 for a corpus of two sentences .", "label": "", "metadata": {}, "score": "42.99714"}
{"text": "In another embodiment , a system may align text segments in comparable , non - parallel corpora , matching strings in the corpora , and using the matched strings to build a parallel corpus .The system may build a Bilingual Suffix Tree ( BST ) and traverse edges of the BST to identify matched strings .", "label": "", "metadata": {}, "score": "43.0091"}
{"text": "In one approach , the context vector for each word in the lexicon may consist of co - occurrence counts in respect to a number of peripheral tokens ( basically , the most frequent words ) .These counts may be collected for each position in an n - word window around the word in focus .", "label": "", "metadata": {}, "score": "43.121395"}
{"text": "10 is psuedocode describing an algorithm for learning translations of unknown words .DETAILED DESCRIPTION .FIG .1 shows a system 100 for building a translation lexicon 105 according to an embodiment .The system may use non - parallel monolingual corpora 110 , 115 in two languages to automatically generate one - to - one mapping of words in the two languages .", "label": "", "metadata": {}, "score": "43.168594"}
{"text": "Description : This system takes a supervised learning approach to word sense disambiguation , where a Naive Bayesian classifier is learned from sense - tagged training examples .No information from WordNet is utilized by this system .The Naive Bayesian classifier is based on a set of features that consists of unigrams ( one word sequences ) that are identified in a filtering set prior to learning .", "label": "", "metadata": {}, "score": "43.306538"}
{"text": "For every subsystem the features included not only bag - of - words in a fixed context window and contextual n - grams , but also a rich variety of syntactic features including subjects , objects and objects of prepositions of verbs and several modification relationships for nouns and adjectives .", "label": "", "metadata": {}, "score": "43.316612"}
{"text": "The method then ends with step 812 of deleting the word with maximum frequency obtained at step 804 from the learning word - list .The following paragraphs are examples of the method illustrated in .FIG .8 .Each example assumes that the user enters a three - character prefix .", "label": "", "metadata": {}, "score": "43.454163"}
{"text": "We have detected multiword terms as present in WordNet .According to the file cntlist we have established a filter , discarding in the first heuristic the senses that have not appeared more than 10 % in the wordnet files .We have made a relevance matrix between words with the data from 3200 books in English from the Gutenberg Project ( Click here ) .", "label": "", "metadata": {}, "score": "43.476234"}
{"text": "The next step is to identify unknown target language words that are surrounded by aligned substrings .The source language word that corresponds to the \" well - aligned \" unknown is considered to be a possible translation .A suffix tree stores in linear space all suffixes of a given string .", "label": "", "metadata": {}, "score": "43.612793"}
{"text": "Assoc . for Computational Linguistics , Morristown , NJ .A system and method of creating and using compact linguistic data are provided .Frequencies of words appearing in a corpus are calculated .Each unique character in the words is mapped to a character index , and characters in the words are replaced with the character indexes .", "label": "", "metadata": {}, "score": "43.63159"}
{"text": "6 ) and substitution tables 610 ( .FIG .6 ) as are used by the base linguistic data .Each learning word - list table also includes indexes for the beginnings of words in the table , frequency information associated with the words in the table , and a sorting index that specifies the alphabetically sorted order of the words .", "label": "", "metadata": {}, "score": "43.658028"}
{"text": "For each word in the test data , a \" domain vector \" is built considering a text window around the target word .Then , the resulting domain vector is compared with the domain vectors previously acquired for each word sense from the training data , and the most similar one is selected .", "label": "", "metadata": {}, "score": "43.765648"}
{"text": "With the aid of the matrix we have enriched sense descriptions .We have added the information of the five first hyponyms where possible .We have added the training information to the definitions .Also we have used the matrix to filter the context of the words to be disambiguated .", "label": "", "metadata": {}, "score": "43.849277"}
{"text": "A second memory - based learner is fed with a co - occurence vector consisting of possible desambiguating keywords above a predefined threshold of one sentence to the right and one to the left of the focus .This vector further contains the sense words available in the WordNet definitions .", "label": "", "metadata": {}, "score": "43.87333"}
{"text": "The fact that n has outgoing edge e indicates there is a mismatch on the subsequent words of those two sequences .Thus , in order to extract all aligned substrings , the system 400 traverses the BST on edges labeled with word pairs , and extract all paths that end either at the leaves or at nodes that have outgoing edges labeled only with source language words .", "label": "", "metadata": {}, "score": "43.904945"}
{"text": "The method then ends with step 812 of deleting the word with maximum frequency obtained at step 804 from the learning wordlist .[ 0088 ] .The following paragraphs are examples of the method illustrated in FIG .8 .Each example assumes that the user enters a three - character prefix .", "label": "", "metadata": {}, "score": "43.981743"}
{"text": "Methods for the calculation of frequency and creation of a word - list are described in .FIG .3 and .FIG .4 .Once a word - list has been derived from the corpus 200 , the word - list is used to create the linguistic data 204 .", "label": "", "metadata": {}, "score": "44.008957"}
{"text": "The filter obtained at step 304 is then applied at step 306 in order to remove words which are not part of the corpus , but rather are part of format definitions .For example , an XML filter removes mark - up tags from the text read from the file .", "label": "", "metadata": {}, "score": "44.09147"}
{"text": "The filter obtained at step 304 is then applied at step 306 in order to remove words which are not part of the corpus , but rather are part of format definitions .For example , an XML filter removes mark - up tags from the text read from the file .", "label": "", "metadata": {}, "score": "44.09147"}
{"text": "The words from the filtered - words file are loaded into a word - tree .The word - tree is an effective structure to store unique words and their frequencies using minimal memory .The tree is organized such that words that occur frequently in the filtered - words files are located in the inner nodes of the tree , and words that occur less frequently are located in the leaves of the tree .", "label": "", "metadata": {}, "score": "44.14778"}
{"text": "A distance function based on the WordNet noun hierarchy forms the core of the WSD algorithm .We assign a weight to every node in WordNet ; this is proportional to the number of descendants of the node .By combining appropriate weights , we can derive the ' distance ' between any two noun senses .", "label": "", "metadata": {}, "score": "44.441162"}
{"text": "The three systems are different in the number of primitives used in the MTD as well as in the semantic distance matrix .They could either be a little over 4,000 or less than 500 .keywords : descriptive - semantic - primitives ; machine - tractable - dictionary ; sense - tagging ; semantic - disambiguation .", "label": "", "metadata": {}, "score": "44.547302"}
{"text": "[ 0053 ] .The method continues with step 508 .In order to reduce the amount of data required to store the words in the word - list , the character sequences that occur most frequently in the words are replaced with substitution indexes .", "label": "", "metadata": {}, "score": "44.55207"}
{"text": "[ 0022]FIG .9 is a flowchart illustrating a method of inflection analysis .DETAILED DESCRIPTION .[ 0023 ] .A system and method of creating and using compact linguistic data , based on word prefix indexing with statistical character substitution , is provided .", "label": "", "metadata": {}, "score": "44.57484"}
{"text": "In this case , a text input logic unit retrieves predictions from the linguistic data that start with any of the combinations of characters that correspond to the prefix entered by the user .[ 0082 ] .The format also allows for easy modification of the words ' frequencies , to conform to individual user 's text input habits .", "label": "", "metadata": {}, "score": "44.62394"}
{"text": "This technique causes some loss of accuracy , but this is acceptable for the purpose of text input prediction , and results in a smaller storage requirement for frequency information .The method continues with step 508 .In order to reduce the amount of data required to store the words in the word - list , the character sequences that occur most frequently in the words are replaced with substitution indexes .", "label": "", "metadata": {}, "score": "44.688507"}
{"text": "The step of suffix finding is based on an iterative search of suffixes of decreasing length , starting with suffixes that are six characters long and ending with suffixes that are two characters long .These suffixes do not always match the existing counterparts in the grammar of the given language , but rather the suffix finding is based on the number of occurrences of suffixes in the word - list .", "label": "", "metadata": {}, "score": "44.74515"}
{"text": "We can improve these results by using heuristic : pass through the candidate phrases through a part - of - speech filter which only allows those patterns that are likely to be phrases .Juteson and Katz have introduced a part - of - speech filter that uses the patterns adjective - noun , noun - noun , etc to decide which pattern should be allowed to let through .", "label": "", "metadata": {}, "score": "44.952644"}
{"text": "This energy is minimized using simulated annealing .( See Preiss 2001 for a more detailed explanation . )This method is combined with an anaphora resolution algorithm ( Kennedy and Boguraev 1996 ) .Resolving pronouns and replacing them in the text with their antecedents leads to the repetition of some words that are likely to be a ' ' topic stamp ' ' ( Boguraev et al .", "label": "", "metadata": {}, "score": "45.07717"}
{"text": "The method of .claim 20 , wherein the step of storing comprises writing the character - mapping table , the substitution table , the word definition tables , and the offset table to a computer - readable file .The method of step 20 , wherein : . the step of creating the word - list comprises steps of : . filtering the corpus into a plurality of filtered words ; . calculating absolute frequencies associated with the filtered words ; and .", "label": "", "metadata": {}, "score": "45.200706"}
{"text": "Frequencies of each variant in the WSJ corpus are shown .For example , joint venture takes 3 variants totaling 4300 instances , out of which 4288 are concentrated in 2 patterns , which in effect ( stripping the plural \" S \" suffix ) , are a single pattern .", "label": "", "metadata": {}, "score": "45.22863"}
{"text": "2 shows a flowchart describing a method 200 for building a translation lexicon from non - parallel corpora .A word comparator 120 may be used to collect pairs of identical words ( block 205 ) .In the English German implementation described above , 977 identical words were found .", "label": "", "metadata": {}, "score": "45.363655"}
{"text": "The first step ( i.e. word domain disambiguation ) considers , for each word , a text window of about 100 words .A score is computed which takes into account the domains of the words within the window as well as their distance from the target word .", "label": "", "metadata": {}, "score": "45.404488"}
{"text": "4 .Once a word - list has been derived from the corpus 200 , the word - list is used to create the linguistic data 204 .The linguistic data 204 includes the unique characters , the character indexes , the substitution indexes , the location information , the groups of words and the frequencies of the words .", "label": "", "metadata": {}, "score": "45.480957"}
{"text": "9 ( ii ) shows the reverse BST 910 , and in bold , the path we are interested in .When d and y are surrounded by aligned sequences , we hypothesize that they are translations of each other .For a pair of words from the two corpora , we use the terms \" right alignment \" and \" left alignment \" to refer to the aligned sequences that precede and respectively succeed the two words in each corpus .", "label": "", "metadata": {}, "score": "45.513527"}
{"text": "2 is a block diagram of a system of creating compact linguistic data ; .FIG .3 is flowchart illustrating a method of filtering source files ; .FIG .4 is flowchart illustrating a method of word frequency calculation ; .", "label": "", "metadata": {}, "score": "45.57849"}
{"text": "Note that here verbs precede propositions .Calzolari and Bindi uses mutual information for extracting lexical information from an Italian corpus .They used a measure , dispersion , to show how the second word is distributed within the window under consideration .", "label": "", "metadata": {}, "score": "45.697697"}
{"text": "With system design facilitating analysis of the contribution of different types of information , further implementation ( using Senseval-1 data ) will allow some useful assessments of the importance of various lexical information .keywords : dictionary definitions , sense inventory mapping , context assessment , multiword units .", "label": "", "metadata": {}, "score": "45.74971"}
{"text": "The words are organized into groups , each group containing words that have a common prefix .Each word definition table stores each of the words included in one of the groups .The compact linguistic data structure further comprises an offset table for locating the word definition tables .", "label": "", "metadata": {}, "score": "45.797718"}
{"text": "[ 0083 ] .Learning capabilities include the modification of frequency information for words , and the addition of words to the linguistic data .Both operations are based on similar processes of adding the words and corresponding frequency information into a learning word - list .", "label": "", "metadata": {}, "score": "45.929405"}
{"text": "The method continues with the step 506 of producing a frequency set for each group of words .In order to reduce the amount of space required to store frequency information , only the maximum frequency of words in each group is retained with full precision .", "label": "", "metadata": {}, "score": "45.955067"}
{"text": "5 is a flowchart illustrating a method creating compact linguistic data .The method uses a word - list containing word frequency information to produce compact linguistic data , and includes word prefix indexing and statistical character substitution .The method beings at step 500 , where the word - list is read from an output file that was produced by a method of word frequency calculation such as the method illustrated in .", "label": "", "metadata": {}, "score": "45.977165"}
{"text": "According to the file cntlist we have established a filter , discarding in the first heuristic the senses that have not appeared more than 10 % in the wordnet files .We have made a relevance matrix between words with the data from 3200 books in English from the Gutenberg Project ( Click here ) .", "label": "", "metadata": {}, "score": "45.985493"}
{"text": "According to the file cntlist we have established a filter , discarding in the first heuristic the senses that have not appeared more than 10 % in the wordnet files .We have made a relevance matrix between words with the data from 3200 books in English from the Gutenberg Project ( Click here ) .", "label": "", "metadata": {}, "score": "45.985493"}
{"text": "According to the file cntlist we have established a filter , discarding in the first heuristic the senses that have not appeared more than 10 % in the wordnet files .We have made a relevance matrix between words with the data from 3200 books in English from the Gutenberg Project ( Click here ) .", "label": "", "metadata": {}, "score": "45.985493"}
{"text": "[ 0033]FIG .3 is flowchart illustrating a method of filtering source files .The source files contain text which comprises a corpus .The filtering method is the first step in calculating the frequency of words in the corpus .[ 0034 ] .", "label": "", "metadata": {}, "score": "45.99366"}
{"text": "6 ) and the substitution table 610 ( FIG .6 ) , and inserted into the beginning of the new words table .Finally , the frequencies and sorting index of the learning word - list table are updated .[ 0096 ] .", "label": "", "metadata": {}, "score": "45.99984"}
{"text": "[ 0009 ] .Existing solutions for storage of linguistic data used for text input and processing typically rely on hash tables , trees , linguistic databases or plain word lists .The number of words covered by these linguistic data formats is limited to the words which have been stored .", "label": "", "metadata": {}, "score": "46.034508"}
{"text": "However , a sufficiently large tagged corpus is not available .To acquire an adequate database of collocations , the full 85-million WSJ corpus is needed .It is necessary to infer the nature of combinations from indirect corpus - based statistics as shown below .", "label": "", "metadata": {}, "score": "46.052223"}
{"text": "[ 0012 ] .A compact linguistic data structure for a plurality of words is also provided .The words are organized into groups , each group containing words that have a common prefix .Each word definition table stores each of the words included in one of the groups .", "label": "", "metadata": {}, "score": "46.06098"}
{"text": "We can also provide the complete mapping between WordNet 1.5 and 1.6 versions ( see this site ) .Description : The lexical sample task in SENSEVAL-2 for Swedish consisted of 40 lemmas ; ( 145 senses , 304 sub - senses ) .", "label": "", "metadata": {}, "score": "46.208755"}
{"text": "an exception table for storing words which are associated with highest frequencies of words in the group contained in the word definition table , . wherein the common prefixes of the words in the local word definition tables are removed .The compact linguistic data structure of .", "label": "", "metadata": {}, "score": "46.499588"}
{"text": "Once collocation database 116 is in place the actual tagging process may begin .An input device 122 is used for entering text to be tagged .The text is first processed by local context tagger 124 which uses local context rules to tag words .", "label": "", "metadata": {}, "score": "46.58275"}
{"text": "A method for building a translation lexicon from non - parallel corpora by a machine translation system , the method comprising : . generating a seed lexicon by the machine translation system , the seed lexicon including identically spelled words ; and .", "label": "", "metadata": {}, "score": "46.655334"}
{"text": "The words in the word definition tables 614 ( FIG .6 ) then do not include the suffixes that are included in the inflection table , but rather contain references to the suffixes in the inflection table .The space saved by using the inflection table for each suffix stored is the number of occurrences of the suffix , multiplied by the length of the suffix .", "label": "", "metadata": {}, "score": "46.717438"}
{"text": "The method begins with the step 400 of reading a filtered - words file .The method continues with the step 402 of reading a word from the filter - words file and adding it into the word - tree , if the word is not already in the word - tree .", "label": "", "metadata": {}, "score": "46.75813"}
{"text": "For evaluation , we distribute test documents with marked target words .Participants is required to assign one or more sense IDs to each target word , optionally with associated probabilities .Test documents will take the form of newspaper articles annotated with UDC codes .", "label": "", "metadata": {}, "score": "46.892563"}
{"text": "4 .The words in the word - list are then sorted alphabetically .The method continues with step 501 of normalizing the absolute frequencies in the word - list .Each absolute frequency is replaced by a relative frequency .Absolute frequencies are mapped to relative frequencies by applying a function , which may be specified by a user .", "label": "", "metadata": {}, "score": "46.96234"}
{"text": "If there are , then the method continues at step 400 .Otherwise , the method continues at step 412 .At step 412 , a word - list which stores words which have been added to the word - tree and their frequencies are written to an output file .", "label": "", "metadata": {}, "score": "46.968945"}
{"text": "The method ends with a step of storing the character - mapping table , the substitution table , the word definition tables , and the offset table .BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1 is a block diagram of a system in which linguistic data is used for text input prediction ; .", "label": "", "metadata": {}, "score": "46.985184"}
{"text": "The lexicon was generated the 2001 - 04 - 24 from GLDB / SDB ( Click here ) 8,718 annotated instances were provided as training material and 1,527 unannotated instances were provided for testing .URL containing additional information ( optional ) : Kokkinakis D. , Jarborg J. and Cederholm Y. ( May , 2001 ) , Swedish SENSEVAL ; a Developers ' Perspective .", "label": "", "metadata": {}, "score": "46.98584"}
{"text": "The algorithm follows two steps : first a domain is chosen for a word ( among those allowed by the word senses in wordnet ) ; then a sense , among those belonging to the preferred domain , is selected .The first step ( i.e. word domain disambiguation ) is based on a similarity function among semantic domains , which was trained on an English corpus .", "label": "", "metadata": {}, "score": "47.05507"}
{"text": "The lexicon builder 125 extracts potential translation word pairs based on one or more clues .These clues may include similar spelling , similar context , preserving word similarity , and word frequency .When words are adopted into another language , their spelling might change slightly in a manner that can not be simply generalized in a rule , e.g. , \" website \" and \" Webseite .", "label": "", "metadata": {}, "score": "47.35313"}
{"text": "Accordingly , the word comparator 120 may restrict the word length to be able to increase the accuracy of the collected word pairs .For instance , by relying only on words at least of length six , 622 word pairs were collected with 96 % accuracy .", "label": "", "metadata": {}, "score": "47.459015"}
{"text": "The subsystems included decision lists ( Yarowsky , 2000 ) , transformation - based error - driven learning ( Brill , 1995 ; Florian and Ngai , 2001 ) , cosine - based vector models , decision stumps and two feature - enhanced naive Bayes systems ( one trained on words , one trained on lemmas ) .", "label": "", "metadata": {}, "score": "47.517612"}
{"text": "FIG .6 .The method begins with the step 900 of finding a configured number of words that occur most frequently in the word - list , based on the absolute frequency of the words .The method continues with the step 901 of finding suffixes of the frequently occurring words .", "label": "", "metadata": {}, "score": "47.56711"}
{"text": "This is the main step of the algorithm and it disambiguates all ambiguous instances which have not been previously disambiguated .We use an instance based learning algorithm and a large pool of features that are actively selected .The learner is trained on the training data provided and then applied on the test instances .", "label": "", "metadata": {}, "score": "47.588238"}
{"text": "The text input logic unit 102 then uses the text output device 106 to present the user with predictions of words that the user has started to enter .The predictions are the most probable complete words that start with prefixes entered as text by the user , and are retrieved by the text input logic unit 102 from the linguistic data 100 .", "label": "", "metadata": {}, "score": "47.631813"}
{"text": "For SENSEVAL-2 , the dataset was divided in two parts .The training set consisted of 76 books ( about 115.000 words ) .The test set consisted of the remaining 26 books ( about 38.000 words ) .Lexicon used was WordNet 1.7 ; instances were mostly from the British Natinal Corpus with some from the Wall Street Journal .", "label": "", "metadata": {}, "score": "47.64297"}
{"text": "The selectional preferences are acquired for subject and direct object slots .For each slot , the verb and argument head training data is obtained from grammatical relations automatically extracted from parses of the BNC .These are then used to populate the verb and noun WordNet hypernym hierarchy with frequencies .", "label": "", "metadata": {}, "score": "47.83715"}
{"text": "The present algorithm tags both light cases as a noun .The example below illustrates the use of a fixed and a variable collocation in context , and motivates the need for thematic analysis .In this small sample , 8 out of 35 cases ( the ones marked \" - \" ) can not be resolved reliably by using local context only .", "label": "", "metadata": {}, "score": "47.88686"}
{"text": "The method continues with the step 308 of extracting the words from the data resulting from step 306 , and writing the extracted words to a filtered - words file at step 310 .[ 0038 ] .If it is determined at step 312 that there are more source files to filter , then the method continues at step 300 .", "label": "", "metadata": {}, "score": "47.89676"}
{"text": "0048 ] .The method beings at step 500 , where the word - list is read from an output file that was produced by a method of word frequency calculation such as the method illustrated in FIG .4 .The words in the word - list are then sorted alphabetically .", "label": "", "metadata": {}, "score": "47.91219"}
{"text": "The method continues with step 501 of normalizing the absolute frequencies in the word - list .Each absolute frequency is replaced by a relative frequency .Absolute frequencies are mapped to relative frequencies by applying a function , which may be specified by a user .", "label": "", "metadata": {}, "score": "47.91811"}
{"text": "Words are added and deleted from the tree in a fashion that assures that the tree remains balanced .[ 0040 ] .The method begins with the step 400 of reading a filtered - words file .The method continues with the step 402 of reading a word from the filter - words file and adding it into the word - tree , if the word is not already in the word - tree .", "label": "", "metadata": {}, "score": "47.926952"}
{"text": "The focus of the present invention is on how to exploit the preferences encountered in corpus analysis using thematic analysis ( analysis of word relationships ) .Referring now to FIG .1 , there is shown a corpus of text 112 to be used for training .", "label": "", "metadata": {}, "score": "47.933178"}
{"text": "These are conditioned on verb classes , so only the noun data which has occurred in the relationship specified by the slot with a member of the verb class is used .For selectional preference acquisition , the members of a verb class include direct members and hyponyms which have 10 senses or less and have occurred in the BNC with a frequency of 20 or more .", "label": "", "metadata": {}, "score": "47.952393"}
{"text": "These are conditioned on verb classes , so only the noun data which has occurred in the relationship specified by the slot with a member of the verb class is used .For selectional preference acquisition , the members of a verb class include direct members and hyponyms which have 10 senses or less and have occurred in the BNC with a frequency of 20 or more .", "label": "", "metadata": {}, "score": "47.952393"}
{"text": "These are conditioned on verb classes , so only the noun data which has occurred in the relationship specified by the slot with a member of the verb class is used .For selectional preference acquisition , the members of a verb class include direct members and hyponyms which have 10 senses or less and have occurred in the BNC with a frequency of 20 or more .", "label": "", "metadata": {}, "score": "47.952393"}
{"text": "Verbs , adjectives , adverbs and other part of speech may be handled in a similar way .They might also provide useful context information that is beneficial to building a noun lexicon .These methods may be also useful given a different starting point .", "label": "", "metadata": {}, "score": "47.95988"}
{"text": "[ 0095 ] .In order to add or delete words from one of the learning word - list tables , the alphabet in the character - mapping table 608 ( FIG .6 ) is updated if it does n't contain the characters that appear in words to be added .", "label": "", "metadata": {}, "score": "48.05568"}
{"text": "FIG .5 .When the method of inflection analysis is used , the resulting compact linguistic data as illustrated in .FIG .6 also includes the inflection table .The words in the word definition tables 614 ( .FIG .", "label": "", "metadata": {}, "score": "48.125877"}
{"text": "The text output device 106 may be a graphical component presented on the screen of a mobile device or computer .[ 0026 ] .The linguistic data 100 is based on word prefix indexing with statistical character substitution , and is described in more detail below .", "label": "", "metadata": {}, "score": "48.145252"}
{"text": "FIG .4 allows even very large corpora to be processed by a single computer .The resulting word - list contains up to a predefined limited number of most frequently occurring words in the corpus , and the absolute frequencies associated with the words .", "label": "", "metadata": {}, "score": "48.232796"}
{"text": "A technique for injecting corpus - based preference into syntactic text parsing is provided .Specifically , the problem of tagging content - word pairs by part - of - speech is solved by using thematic analysis .A technique for injecting corpus - based preference into syntactic text parsing is provided .", "label": "", "metadata": {}, "score": "48.36893"}
{"text": "An apparatus comprising : . a lexicon builder operative to be executed to expand the seed lexicon by identifying possible translations of words in the first and second corpora using one or more clues .The apparatus of .claim 24 , wherein the lexicon builder is configured to use the identically spelled words in the seed lexicon as accurate translations .", "label": "", "metadata": {}, "score": "48.397438"}
{"text": "6 is a block diagram of a format of compact linguistic data ; .[ 0020 ] .[ 0020]FIG .7 is a block diagram of a complex word definition table ; . [ 0021 ] .[ 0021]FIG .8 is a flowchart illustrating a method of frequency modification ; and .", "label": "", "metadata": {}, "score": "48.443844"}
{"text": "The test documents are annotated with morphological information ( word segmentation , POS tag , reading and base form , all automatically annotated ) for all words .For each target word , the ID of the sense in the TM best approximating that usage must be submitted .", "label": "", "metadata": {}, "score": "48.623962"}
{"text": "Similar examples are defined as sentences which share the head word and several words around it .Given a sentence , our system outputs the head word in the closest cluster to the given sentence .The closest cluster is selected based on machine learning systems such as SVM .", "label": "", "metadata": {}, "score": "48.667274"}
{"text": "The learning word - list tables follow sequentially one after the other , with the updated frequencies table appearing first .[0093 ] .If the learning word - list tables reach a maximum - defined length , then the oldest words from the tables are deleted in order to make room for new entries in the tables .", "label": "", "metadata": {}, "score": "48.679565"}
{"text": "The information used to represent examples is : . -Local information : using part - of - speech and lemmas of words placed in a 7-word window around the target word .- Topic information : treating open - class words as a bag of words .", "label": "", "metadata": {}, "score": "48.785973"}
{"text": "Adding words to and deleting words from a learning word - list table are performed by creating the byte sequence representing the updated table and simultaneously writing the byte sequence into an output stream .After the update is complete , the updated data is reread .", "label": "", "metadata": {}, "score": "48.866455"}
{"text": "To avoid this , the system 400 appends an end - of - string marker \" $ \" that appears nowhere else in the string .For clarity , the drawings only show the $ marker when necessary .Each monolingual corpus given as input to the system 400 may be divided into a set of sentences .", "label": "", "metadata": {}, "score": "48.896553"}
{"text": "The selectional preferences are acquired for subject and direct object slots .For each slot , the verb and argument head training data is obtained from grammatical relations automatically extracted from parses of the BNC produced by a shallow parser .These are then used to populate the verb and noun WordNet hypernym hierarchy with frequencies .", "label": "", "metadata": {}, "score": "48.923203"}
{"text": "The selectional preferences are acquired for subject and direct object slots .For each slot , the verb and argument head training data is obtained from grammatical relations automatically extracted from parses of the BNC produced by a shallow parser .These are then used to populate the verb and noun WordNet hypernym hierarchy with frequencies .", "label": "", "metadata": {}, "score": "48.923203"}
{"text": "The system of .claim 3 , wherein the keyboard is a reduced keyboard .The system of . claim 2 , wherein the user interface and the text input logic unit are implemented on a mobile communication device .A compact linguistic data structure for a plurality of words , wherein the words are organized into groups , each group containing words that have a common prefix , the compact linguistic data structure comprising : . an alphabet comprised of each unique character in the words ; . a character - mapping table for mapping each character in the alphabet to a character index ; . a substitution table for mapping sequences of characters from the words to substitution indexes ; . a plurality of word definition tables for storing the words , each word definition table storing each of the words included in one of the groups ; and .", "label": "", "metadata": {}, "score": "48.926693"}
{"text": "10 .An advantage of the algorithm over previous approaches is that we do not provide as input to the algorithm a list of unknown words .Instead , the system automatically learns from the corpus both the unknown words and their translation , upon discovery of appropriate context alignments .", "label": "", "metadata": {}, "score": "49.030525"}
{"text": "The offset table is used to locate the byte sequences that comprise the encoded words for a particular group that start with a common prefix .The method concludes with step 514 .At this step , the linguistic data resulting from the method has been stored in the tables that have been created .", "label": "", "metadata": {}, "score": "49.048393"}
{"text": "There are many approaches to find collocations in a text corpus .The important ones are : .Frequency 2 .Mean and Variance 3 .Hypothesis Testing 4 .Mutual Information .Frequency .Frequency is the simplest method for finding collocations in a text corpus .", "label": "", "metadata": {}, "score": "49.060646"}
{"text": "Words in each group have a common prefix of a given length and are sorted by frequency .Words are initially grouped by prefixes that are two characters long .If there are more than 256 words that start with the same two - character prefix , then additional separation will be performed with longer prefixes .", "label": "", "metadata": {}, "score": "49.189476"}
{"text": "keywords : Trisem Model , Semantic Classes , Semantic Classification Tree . organisation : University of Maryland , College Park , Linguistics Department & UMIACS , USA .Task / s : English all words .Did you use any training data provided in an automatic training procedure ?", "label": "", "metadata": {}, "score": "49.20111"}
{"text": "claim 12 , further comprising adding the matching scores .The method of . claim 13 , further comprising weighting the matching scores .A non - transitory computer readable medium having embodied thereon a program , the program being executable by a processor for performing a method for building a translation lexicon from non - parallel corpora , the method comprising : . generating a seed lexicon by the machine translation system , the seed lexicon including identically spelled words ; and .", "label": "", "metadata": {}, "score": "49.28084"}
{"text": "In these cases , the fine - grained output of the Czech lemmatizer was ignored ( in both training and test ) and a generic lexical sample sense classifier was trained on the training data ( see JHU_Swedish for further details of this ) .", "label": "", "metadata": {}, "score": "49.360687"}
{"text": "applying transformation rules to words in the first corpora to form transformed words ; and . comparing said transformed words to words in the second corpora .The non - transitory computer readable medium of . claim 15 , wherein said one or more clues includes similar spelling .", "label": "", "metadata": {}, "score": "49.484985"}
{"text": "Based on these alignments , the system 400 may generate a parallel corpus 420 and identify translations 425 of words from the source language which are not in the lexicon .For example , consider the following two sentences where the only unknown French word is \" raison \" : .", "label": "", "metadata": {}, "score": "49.542324"}
{"text": "Statistical data gathered during the method of creating compact linguistic data may optionally be stored at step 514 .FIG .6 is a block diagram of a format of compact linguistic data .The primary objective of the data format is to preserve the simplicity of interpretation of the linguistic data , while minimizing memory use and the number of computer instructions required to create and interpret the data .", "label": "", "metadata": {}, "score": "49.547997"}
{"text": "The translated vector can be compared to other vectors in the second language .The lexicon builder 125 may perform a greedy search for the best matching similarity vectors and add the corresponding words to the lexicon .Another clue is based on the assumption that in comparable corpora , the same concepts should occur with similar frequencies .", "label": "", "metadata": {}, "score": "49.60965"}
{"text": "Description : This system takes a supervised learning approach to word sense disambiguation , where three Naive Bayesian classifiers are induced from sense - tagged training examples .A weighted vote is taken among these to assign senses to test examples .", "label": "", "metadata": {}, "score": "49.697258"}
{"text": "5 is a flowchart illustrating a method creating compact linguistic data ; .FIG .6 is a block diagram of a format of compact linguistic data ; .FIG .7 is a block diagram of a complex word definition table ; .", "label": "", "metadata": {}, "score": "49.773228"}
{"text": "We also use Mainichi Simbun newspaper articles for 1994 as additional training data .The Basque task consists of lexical samples for 40 words .There will be approx .The samples would comprise 5 sentences centered in the target word , taken from a newspaper corpus , and , if there is interest , the whole documents will be available via internet .", "label": "", "metadata": {}, "score": "49.799038"}
{"text": "Such a processor is trained to exploit properties of the corpus itself , highlights regularities , identifies thematic relations , and in general , feeds digested text into the unification parser .Consider the following WSJ , ( Aug. 19 , 1987 ) paragraph processed by a preprocessor : .", "label": "", "metadata": {}, "score": "49.860542"}
{"text": "The lexicon provided has been created specifically for the task and it consists of a definition for each sense linked to the Spanish version of EuroWordNet and , thus , to the English WordNet 1.5 , the syntactic category and , sometimes , examples and synonyms are also provided .", "label": "", "metadata": {}, "score": "49.904144"}
{"text": "0099 ] .The method continues with the step 902 of updating the inflection table with the suffixes found in the previous step .The first time step 902 is performed , the inflection table is created before it is updated .", "label": "", "metadata": {}, "score": "49.912674"}
{"text": "[ 0050 ] .The method continues with the step 502 of creating a character - mapping table .The character - mapping table is used to encode words in a subsequent step of the method .When encoding is performed , the characters in the original words are replaced with the character indexes of those characters in the character - mapping table .", "label": "", "metadata": {}, "score": "49.99934"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1 is a block diagram of a system for building a translation lexicon according to an embodiment .FIG .2 is a flowchart describing a method for building a translation lexicon from non - parallel corpora .", "label": "", "metadata": {}, "score": "50.02543"}
{"text": "For each occurrence of a target word , the counts may be collected over how often certain context words occur in the two positions directly ahead of the target word and the two following positions .The counts may be collected separately for each position and then entered into a context vector with a dimension for each context word in each position .", "label": "", "metadata": {}, "score": "50.045174"}
{"text": "A hotword can be retrieved quickly using the hotword table 700 and the exception table 702 , instead of performing a search of the local word definition tables 708 to find the hotword .[0081 ] .The format of linguistic data described above enables determination of word predictions very quickly , using a minimal amount of memory .", "label": "", "metadata": {}, "score": "50.080532"}
{"text": "The features for a verb w are : .The system computes the probablity of each sense for a test instance based on the maximum entropy model , filters out senses using the satellites , and outputs the senses that have probability within a factor of .80 of the highest probablity sense .", "label": "", "metadata": {}, "score": "50.089546"}
{"text": "The extractions of all parallel phrases and of the translations took about 2 hours each .The experiments were run on a Linux \u00ae system 400 with an Intel \u00ae Pentium \u00ae 3 processor of 866 Mhz .A number of embodiments have been described .", "label": "", "metadata": {}, "score": "50.10859"}
{"text": "The senses of the repeated words are tied together , thus making any change of sense more noticeable ( in the resulting energy value ) .This leads to a more accurate disambiguation of topic words which in turn lead to an increased disambiguation accuracy of other nouns .", "label": "", "metadata": {}, "score": "50.165405"}
{"text": "keywords : Classification Information model , local context , topical context , bigram context .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "50.16584"}
{"text": "3 is flowchart illustrating a method of filtering source files .The source files contain text which comprises a corpus .The filtering method is the first step in calculating the frequency of words in the corpus .The method begins with the step 300 of reading the contents a source file .", "label": "", "metadata": {}, "score": "50.268456"}
{"text": "The selected word is added to the learning word - list .[ 0086 ] .The method continues with step 804 of obtaining the word with the maximum frequency of words in the prediction list that was presented to the user .", "label": "", "metadata": {}, "score": "50.347557"}
{"text": "Assoc . for Computational Linguistics , Morristown , NJ , 160 - 167 .Taskar , B. , et al . , \" A Discriminative Matching Approach to Word Alignment , \" In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing ( Vancouver , BC , Canada , Oct. 6 - 8 , 2005 ) .", "label": "", "metadata": {}, "score": "50.36848"}
{"text": "Hence , the size of the offset table 612 is based on the length of the alphabet .[ 0069 ] .An inflection table , not shown , may optionally be included in the linguistic data .The inflection table stores word suffixes which may be used in word definitions .", "label": "", "metadata": {}, "score": "50.37098"}
{"text": "[ 0042 ] .At step 406 , the word - tree is shrunk so that it no longer exceeds the size limit .The tree is shrunk by deleting the least - frequently used words from the tree , which are located in the leaf nodes .", "label": "", "metadata": {}, "score": "50.39468"}
{"text": "The tree is organized such that words that occur frequently in the filtered - words files are located in the inner nodes of the tree , and words that occur less frequently are located in the leaves of the tree .Each node of the tree contains a unique word and the word 's absolute frequency .", "label": "", "metadata": {}, "score": "50.398956"}
{"text": "The disambiguated version of the Wordnet dictionary was then used in handling the test data .In the sense - tagging of the test sentences , all nonheads ( words that ddi not require tagging ) were removed from each sentence , leaving only the heads ( words to be sense - tagged ) behind .", "label": "", "metadata": {}, "score": "50.412003"}
{"text": "In disambiguation , these features are combined according to a voting scheme .In the learning phase we consider features that have a total frequency above a set threshold ( 3 ) , and calculate for each of these : . 1 ) the relative frequency of each sense for the feature , 2 ) a probability measure , using a Student - t distribution to test the hypothesis , that the observed distribution for a feature is significantly different from the overall distribution of senses in the training data .", "label": "", "metadata": {}, "score": "50.47484"}
{"text": "The learning word - list tables follow sequentially one after the other , with the updated frequencies table appearing first .If the learning word - list tables reach a maximum - defined length , then the oldest words from the tables are deleted in order to make room for new entries in the tables .", "label": "", "metadata": {}, "score": "50.538467"}
{"text": "The number of words covered by these linguistic data formats is limited to the words which have been stored .The linguistic data which is used in existing text input prediction systems is typically derived from a body of language , either text or speech , known as a corpus .", "label": "", "metadata": {}, "score": "50.55333"}
{"text": "The senses are hierarchically organized , and the definition , synonyms and examples are provided for each sense , among other lexicographical data .Due to the complex structure of the dictionary , a flat list of word senses and multiword terms is also provided .", "label": "", "metadata": {}, "score": "50.589973"}
{"text": "In order to reduce the amount of space required to store frequency information , only the maximum frequency of words in each group is retained with full precision .The frequency of each other word is retained as a percentage of the maximum frequency of words in its group .", "label": "", "metadata": {}, "score": "50.681084"}
{"text": "Description ( 250 words max ) : Our Word Sense Disambiguation ( WSD ) System was entered for the English all words task in Senseval 2001 .The system makes use of the WordNet 1.7 hierarchy and an anaphora resolution algorithm to assign precisely one sense to each noun from the text .", "label": "", "metadata": {}, "score": "50.700397"}
{"text": "It extracts a basic feature set : . i ) local features for english : bigrams and trigrams around the target word , consisting on lemmas or word forms or parts of speech .Also a bag of lemmas constructed using the content words in a + /- 4 word window around the target . ii ) local features for Basque : being Basque an agglutinative language , part of the syntactic information is in the inflectional suffixes .", "label": "", "metadata": {}, "score": "50.7004"}
{"text": "The second and third columns present the collocation and its frequency .The fourth and fifth column present the stemmed collocation and its frequency .The sixth column presents the mutual information score ( MIS ) .The MIS is calculated by dividing the number of occurrences of the collocation by the number of times each individual word in the collocation occurs alone .", "label": "", "metadata": {}, "score": "50.734238"}
{"text": "[ 0014 ] .[ 0014]FIG . 1 is a block diagram of a system in which linguistic data is used for text input prediction ; .[ 0015 ] .[ 0015]FIG .2 is a block diagram of a system of creating compact linguistic data ; .", "label": "", "metadata": {}, "score": "50.817856"}
{"text": "It requires a parallel corpus that is sentence aligned and a sense inventory for the language that needs to be sense tagged .It produces both corpora tagged with sense IDs from the sense inventory .Basically , the system assumes that the parallel corpus is token aligned .", "label": "", "metadata": {}, "score": "50.832108"}
{"text": "As a result , the absolute value of the frequencies for this group of words will be modified using the weight assigned to the group , so that this group of words will have frequencies that are different they would have otherwise had .", "label": "", "metadata": {}, "score": "50.851357"}
{"text": "As a result , the absolute value of the frequencies for this group of words will be modified using the weight assigned to the group , so that this group of words will have frequencies that are different they would have otherwise had .", "label": "", "metadata": {}, "score": "50.851357"}
{"text": "It 's quite fast , is network accessible , atomic operations make locking unnecessary , supports sortable and sliceable list structures , and is very easy to configure .Why build a NLTK FreqDist on Redis .Building a NLTK FreqDist on top of Redis allows you to create a ProbDist , which in turn can be used for classification .", "label": "", "metadata": {}, "score": "50.894707"}
{"text": "The three classifiers are a bagged J48 decision tree , a Naive Bayesian classifier , and a nearest neighbor classifier ( IBk ) .This system uses a filter to perform feature identification prior to learning .All non - consecutive bigrams ( that may include zero , one , or two intervening words that are ingored ) and that meet the following criteria form a set of candidate features : .", "label": "", "metadata": {}, "score": "50.908447"}
{"text": "The choice of combination method and the parameters of weighted voting and the features and weights of the loglinear model were chosen by crossvalidation on the training data .The first level and combined classifiers simply reported a single most likely sense choice for each test word .", "label": "", "metadata": {}, "score": "50.988007"}
{"text": "Tags P and U have not been used .There is no special treatment for multiword detection .keywords : Supervised learning , decision lists , agglutinative languages .Did you use any training data provided in an automatic training procedure ?", "label": "", "metadata": {}, "score": "51.03163"}
{"text": "The method continues with a step of creating a substitution table for encoding the words in the groups by replacing character sequences in the words in the groups with substitution indexes that are mapped to the character sequences by the substitution table .", "label": "", "metadata": {}, "score": "51.09298"}
{"text": "To do this effectively , we 'll modify the previous code so that we can use an arbitrary feature extractor function that takes the words in a file and returns the feature dictionary .As before , we 'll use these features to train a Naive Bayes Classifier .", "label": "", "metadata": {}, "score": "51.119366"}
{"text": "To do this effectively , we 'll modify the previous code so that we can use an arbitrary feature extractor function that takes the words in a file and returns the feature dictionary .As before , we 'll use these features to train a Naive Bayes Classifier .", "label": "", "metadata": {}, "score": "51.119366"}
{"text": "To do this effectively , we 'll modify the previous code so that we can use an arbitrary feature extractor function that takes the words in a file and returns the feature dictionary .As before , we 'll use these features to train a Naive Bayes Classifier .", "label": "", "metadata": {}, "score": "51.119366"}
{"text": "To do this effectively , we 'll modify the previous code so that we can use an arbitrary feature extractor function that takes the words in a file and returns the feature dictionary .As before , we 'll use these features to train a Naive Bayes Classifier .", "label": "", "metadata": {}, "score": "51.119366"}
{"text": "The correctness of word mappings acquired in this fashion may depend highly on word length .While identical three - letter words were only translations of each other 60 % of the time , this was true for 98 % of ten - letter words .", "label": "", "metadata": {}, "score": "51.1874"}
{"text": "The predictions are the most probable complete words that start with prefixes entered as text by the user , and are retrieved by the text input logic unit 102 from the linguistic data 100 .The user may then select one of the predictions using the text input device 104 .", "label": "", "metadata": {}, "score": "51.289486"}
{"text": "Description : This system takes a supervised learning approach to word sense disambiguation , where three different classifiers are induced from sense - tagged training examples .Each classifier is based on the same feature set .A weighted vote is taken among these classifiers to assign senses to test examples .", "label": "", "metadata": {}, "score": "51.320007"}
{"text": "The JHU - English system differed slightly from our other lexical sample systems in its treatment of phrasal senses .Because phrasal compounds such as verb - particle pairs were explicitly marked , both in training and test data , this additional provided information was used as follows : If a phrasal compound was marked in the data , then only compound senses ( e.g. verb - particle ) were considered .", "label": "", "metadata": {}, "score": "51.39001"}
{"text": "Fox , H. , \" Phrasal Cohesion and Statistical Machine Translation \" Proceedings of the Conference on Empirical Methods in Natural Language Processing , Philadelphia , Jul. 2002 , pp .304 - 311 .Association for Computational Linguistics .Franz Josef Och , Hermann Ney : \" Improved Statistical Alignment Models \" ACLOO :", "label": "", "metadata": {}, "score": "51.395554"}
{"text": "For each of the common prefixes , the offset table contains a location of the word definition table which stores words starting with the common prefix .The common prefixes for words in each word definition table are removed .[ 0013 ] .", "label": "", "metadata": {}, "score": "51.445763"}
{"text": "All other frequencies are stored as percentages of the maximum frequency .The encoded words are sorted by frequency .However , if learning capabilities are applied , as described below , then the initial sorting is no longer valid , and the encoded words may need to be resorted .", "label": "", "metadata": {}, "score": "51.49594"}
{"text": "FIG .5 shows the suffix tree 500 of string xyzyxzy .Note that if a suffix of a string is also a prefix of another suffix ( as would be the case for suffix zy of string xyzyxzy ) , a proper suffix tree can not be built for the string .", "label": "", "metadata": {}, "score": "51.62349"}
{"text": "[ 0016]FIG .3 is flowchart illustrating a method of filtering source files ; .[0017 ] .[ 0017]FIG .4 is flowchart illustrating a method of word frequency calculation ; .[ 0018 ] .[ 0018]FIG .5 is a flowchart illustrating a method creating compact linguistic data ; . [ 0019 ] .", "label": "", "metadata": {}, "score": "51.628883"}
{"text": "The features used in decision lists are content words and part - of - speech tags in a window .In Iwanami Kokugo Jiten ( a Japanese dictionary ) , sense inventory of this task , word sense descriptions contain some example sentences .", "label": "", "metadata": {}, "score": "51.730045"}
{"text": "Linguistic data in the format is produced by the linguistic data analyzer 202 ( FIG .2 ) , and is the output of the method illustrated by FIG .5 .[ 0060 ] .The format allows linguistic data to be stored with or without word frequency information .", "label": "", "metadata": {}, "score": "51.733284"}
{"text": "Each Naive Bayesian classifier is based on a different set of features that are identified in a filtering step prior to learning .The first feature set is based on bigrams ( two word sequences ) that meet the following criteria : .", "label": "", "metadata": {}, "score": "51.73442"}
{"text": "A complex table is used to define words which are grouped by prefixes of greater lengths .[ 0071 ] .Words in the definition tables 614 are encoded using the character - mapping table 608 and the substitution table 610 .", "label": "", "metadata": {}, "score": "51.755783"}
{"text": "Checking for the noun - verb case is symmetrical ( in step 2 .b ) .The threshold is different for each suffix and should be determined experimentally ( initial threshold can be taken as 0.75 .Notice that local - context rules override corpus preference .", "label": "", "metadata": {}, "score": "51.876427"}
{"text": "Upon completion of thematic analysis tagging , the text is in condition to be passed to parser 130 which performs parsing on the tagged text .The algorithm yields incorrect results in two problematic cases .The first is ambiguous thematic relations which are collocations that entertain both subject - verb and verb - object relations , i.e. , selling - companies ( as in \" the company sold its subsidiary . . .", "label": "", "metadata": {}, "score": "52.04046"}
{"text": "First , the lexicon builder 125 searches for the highest score for any word pair .This is added to the lexicon ( block 230 ) , and word pairs that include either the German and English word are dropped from further search .", "label": "", "metadata": {}, "score": "52.07464"}
{"text": "Abstract .A machine translation system may use non - parallel monolingual corpora to generate a translation lexicon .The system may identify identically spelled words in the two corpora , and use them as a seed lexicon .The system may use various clues , e.g. , context and frequency , to identify and score other possible translation pairs , using the seed lexicon as a basis .", "label": "", "metadata": {}, "score": "52.104584"}
{"text": "However , when tested on a real corpus , ( i.e. , Wall Street Journal ( WSJ ) news stories ) , this mechanism collapses .For a typical well - behaved 33-word sentence it produces hundreds of candidate interpretations .", "label": "", "metadata": {}, "score": "52.132534"}
{"text": "The features used in the model is outputs of morphological and syntactic analysis .We used a hybrid model of support vector machine and simple Bayes for learning .Did you use any training data provided in an automatic training procedure ?", "label": "", "metadata": {}, "score": "52.200447"}
{"text": "The relevance score between a feature and a sense is proportion to the conditional probability of the feature given the sense .The equations that we used are in the following web site .keywords : Classification Information model , local context , topical context , bigram context .", "label": "", "metadata": {}, "score": "52.23515"}
{"text": "keywords : classifier combination , word sense disambiguation , bayes classifiers , cosine similarity , decision lists , transformation based learning .name : David Yarowsky , Silviu Cucerzan , Radu Florian , Charles Schafer and Richard Wicentowski . jhu.edu .organisation : Computer Science Department and Center for Language and Speech Processing , Johns Hopkins University .", "label": "", "metadata": {}, "score": "52.247635"}
{"text": "FIG .1 is a block diagram of a system in which linguistic data is used for text input prediction .The system includes linguistic data 100 , a text input logic unit 102 , and a user interface 103 .The system can be implemented on any computing device requiring text input , but is especially suited for embedded devices with a slow CPU and significant RAM and ROM limitations , such as a mobile communication device .", "label": "", "metadata": {}, "score": "52.438435"}
{"text": "0073 ] .A simple word definition table contains the encoded words of a group , and the frequencies associated with the words .The frequencies are normalized by applying a normalization function which converts the frequencies so that their values are within a predetermined range .", "label": "", "metadata": {}, "score": "52.444046"}
{"text": "The memory - based learning ( TiMBL ) was used .The input for the learner was a feature vector consisting of 100 features .The training data was taken i)from the syntactic examples in the dictionary and ii)the training corpus .", "label": "", "metadata": {}, "score": "52.472004"}
{"text": "After the update is complete , the updated data is reread .The process of writing into an output stream occurs every time words are added or deleted from the learning word - list .In order to add or delete words from one of the learning word - list tables , the alphabet in the character - mapping table 608 ( .", "label": "", "metadata": {}, "score": "52.61977"}
{"text": "The mobile communication device of .A mobile communication device , comprising : . a text input device that receives a text input ; . a computer - readable medium encoded with a compact linguistic data structure for a plurality of words , wherein the words are organized into groups , each group containing words that have a common prefix ; . a text input logic program that uses the compact linguistic data structure to predict a word from the text input ; and .", "label": "", "metadata": {}, "score": "52.68731"}
{"text": "Here 's the baseline feature extractor for bag of words feature selection .def word_feats(words ) : return dict([(word , True ) for word in words ] ) evaluate_classifier(word_feats ) .The results are the same as in the previous articles , but I 've included them here for reference : .", "label": "", "metadata": {}, "score": "52.73611"}
{"text": "Here 's the baseline feature extractor for bag of words feature selection .def word_feats(words ) : return dict([(word , True ) for word in words ] ) evaluate_classifier(word_feats ) .The results are the same as in the previous articles , but I 've included them here for reference : .", "label": "", "metadata": {}, "score": "52.73611"}
{"text": "Here 's the baseline feature extractor for bag of words feature selection .def word_feats(words ) : return dict([(word , True ) for word in words ] ) evaluate_classifier(word_feats ) .The results are the same as in the previous articles , but I 've included them here for reference : .", "label": "", "metadata": {}, "score": "52.73611"}
{"text": "Here 's the baseline feature extractor for bag of words feature selection .def word_feats(words ) : return dict([(word , True ) for word in words ] ) evaluate_classifier(word_feats ) .The results are the same as in the previous articles , but I 've included them here for reference : .", "label": "", "metadata": {}, "score": "52.73611"}
{"text": "0010 ] .The linguistic data which is used in existing text input prediction systems is typically derived from a body of language , either text or speech , known as a corpus .There are national corpora of hundreds of millions of words and there are also corpora which are constructed for particular purposes .", "label": "", "metadata": {}, "score": "52.739113"}
{"text": "The feature vectors are the input to the J48 learning algorithm , the Weka implementation of the C4.5 decision tree learner .The decision tree learner is \" bagged \" .The training examples are sampled ten times ( with replacement ) and a decision tree is learned for each sample .", "label": "", "metadata": {}, "score": "52.758423"}
{"text": "Given an initial bilingual lexicon 405 and two texts 410 , 415 in each of the languages , the system 400 may identify parts of the texts which can be aligned ( i.e. , are mutual translations of each other according to the lexicon ) .", "label": "", "metadata": {}, "score": "52.794865"}
{"text": "The Naive Bayesian classifier is based on a set of features that consists of unigrams ( one word sequences ) that are identified in a filtering set prior to learning .These features must meet the following criteria : . 1 ) occur 5 or more times and 2 ) are not found on the stop - list .", "label": "", "metadata": {}, "score": "52.858162"}
{"text": "The third feature set is based on bigrams that may include one intervening word that is ignored and that meet the following criteria : .A Naive Bayesian classifier is learned based on each feature set .When presented with a test example , each classifier assigns a probability to each possible sense .", "label": "", "metadata": {}, "score": "52.863525"}
{"text": "Thus , when more than 90 % of the phrases are concentrated in a single pattern , it is classified as a fixed adjective - noun ( or noun - noun ) phrase .Otherwise , it is classified as a noun - verb ( or verb - noun ) thematic relation .", "label": "", "metadata": {}, "score": "52.905243"}
{"text": "Such sequences are learned from sense - tagged data using transformation - based learning .The system as such consists of a PWE compiler which translates PWE specifications into Horn clause formulas ( similar to a DCG compiler ) .The rest is just a matter of performing Prolog - style constructive proofs .", "label": "", "metadata": {}, "score": "52.947304"}
{"text": "All non - consecutive bigrams ( that may include zero , one , or two intervening words that are ingored ) and that meet the following criteria form a set of candidate features : .The training examples are converted into feature vectors , where each feature represents whether a candidate feature occurs in the context of a specific training example .", "label": "", "metadata": {}, "score": "52.95285"}
{"text": "6 ) is updated if it does n't contain the characters that appear in words to be added .Words to be added are then encoded using the character - mapping table 608 ( .FIG .6 ) and the substitution table 610 ( .", "label": "", "metadata": {}, "score": "53.100212"}
{"text": "In this model , each branch of the tree either continues on to a new pair of branches , or stops , and at each branching you use a classifier to determine which branch to take .Share this : .NLTK - Trainer ( available github and bitbucket ) was created to make it as easy as possible to train NLTK text classifiers .", "label": "", "metadata": {}, "score": "53.113792"}
{"text": "keywords : context vector , cosine similarity , semantic feature , syntactic relation .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "53.17079"}
{"text": "Because no sense - tagged training data was used , it was not possible to know the lexical priors or even majority sense for each word .Thus baseline performance is significantly less than for other lexical - sample tasks .keywords : unsupervised word sense disambiguation , bayes similarity , adaptive clustering , unsupervised learning .", "label": "", "metadata": {}, "score": "53.18159"}
{"text": "keywords : selectional preferences , grammatical relations , one sense per discourse , anaphor resolution .organisation : Dept . of Computer Science , University of California Los Angeles .Task / s : English all words .Did you use any training data provided in an automatic training procedure ?", "label": "", "metadata": {}, "score": "53.184635"}
{"text": "Once we have those numbers , we can score words with the BigramAssocMeasures.chi_sq function , then sort the words by score and take the top 10000 .We then put these words into a set , and use a set membership test in our feature selection function to select only those words that appear in the set .", "label": "", "metadata": {}, "score": "53.194923"}
{"text": "Once we have those numbers , we can score words with the BigramAssocMeasures.chi_sq function , then sort the words by score and take the top 10000 .We then put these words into a set , and use a set membership test in our feature selection function to select only those words that appear in the set .", "label": "", "metadata": {}, "score": "53.194923"}
{"text": "Once we have those numbers , we can score words with the BigramAssocMeasures.chi_sq function , then sort the words by score and take the top 10000 .We then put these words into a set , and use a set membership test in our feature selection function to select only those words that appear in the set .", "label": "", "metadata": {}, "score": "53.194923"}
{"text": "Once we have those numbers , we can score words with the BigramAssocMeasures.chi_sq function , then sort the words by score and take the top 10000 .We then put these words into a set , and use a set membership test in our feature selection function to select only those words that appear in the set .", "label": "", "metadata": {}, "score": "53.194923"}
{"text": "Feature Scoring .As I 've shown previously , eliminating low information features can have significant positive effects .Below is a table showing the accuracy of each algorithm at different score levels , using the option --min_score SCORE ( and keeping the --ngrams 1 2 option to get bigram features ) .", "label": "", "metadata": {}, "score": "53.26085"}
{"text": "The SklearnClassifier provides a general interface to text classification with scikit - learn .While scikit - learn is still pre-1.0 , it is rapidly becoming one of the most popular machine learning toolkits , and provides more advanced feature extraction methods for classification .", "label": "", "metadata": {}, "score": "53.314747"}
{"text": "Description : The sussex system2ospd was applied to the all - words task , but processed only the plain text , ignoring the supplied bracketings .The system parses the text to identify subject / verb and verb / direct object relationships , and the nouns and verbs involved are disambiguated using class - based selectional preferences acquired from unsupervised training .", "label": "", "metadata": {}, "score": "53.36814"}
{"text": "I did not include the most informative features since they did not change .nbest(score_fn , n ) return dict([(ngram , True ) for ngram in itertools.chain(words , bigrams ) ] ) evaluate_classifier(bigram_word_feats ) .After some experimentation , I found that using the 200 best bigrams from each file produced great results : .", "label": "", "metadata": {}, "score": "53.40305"}
{"text": "I did not include the most informative features since they did not change .nbest(score_fn , n ) return dict([(ngram , True ) for ngram in itertools.chain(words , bigrams ) ] ) evaluate_classifier(bigram_word_feats ) .After some experimentation , I found that using the 200 best bigrams from each file produced great results : .", "label": "", "metadata": {}, "score": "53.40305"}
{"text": "I did not include the most informative features since they did not change .nbest(score_fn , n ) return dict([(ngram , True ) for ngram in itertools.chain(words , bigrams ) ] ) evaluate_classifier(bigram_word_feats ) .After some experimentation , I found that using the 200 best bigrams from each file produced great results : .", "label": "", "metadata": {}, "score": "53.40305"}
{"text": "I did not include the most informative features since they did not change .nbest(score_fn , n ) return dict([(ngram , True ) for ngram in itertools.chain(words , bigrams ) ] ) evaluate_classifier(bigram_word_feats ) .After some experimentation , I found that using the 200 best bigrams from each file produced great results : .", "label": "", "metadata": {}, "score": "53.40305"}
{"text": "The prefixes used to collect words into groups are removed from the words themselves .As a result , each word is represented by a byte sequence , which includes all the data required to find the original word , given its prefix .", "label": "", "metadata": {}, "score": "53.45018"}
{"text": "FIG .9 .FIG .9 ( i ) shows a branch 905 of the BST corresponding to the comparable corpus in the same figure .The path defined by the bold edges shows that sequences xyz and abc are aligned , and diverge ( i.e. , have a mismatch ) at characters y and d , respectively .", "label": "", "metadata": {}, "score": "53.49737"}
{"text": "While it works great if you know exactly what you 're looking for , I worry that new / interested users will have a harder time getting started .New Corpora .Since the 0.9.9 release , a number of new corpora and corpus readers have been added : .", "label": "", "metadata": {}, "score": "53.50195"}
{"text": "For example , in an implementation , an English - German translation lexicon was generated from a 1990 - 1992 Wall Street Journal corpus on the English side and a 1995 - 1996 German news wire ( DPA ) on the German side .", "label": "", "metadata": {}, "score": "53.56138"}
{"text": "The character - mapping 608 table contains the alphabet being used for this word - list , and maps each character in the alphabet to a character index .The alphabet consists of each unique character used in words in the word - list .", "label": "", "metadata": {}, "score": "53.574814"}
{"text": "Bannard , C. and Callison - Burch , C. , \" Paraphrasing with Bilingual Parallel Corpora , \" In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics ( Ann Arbor , MI , Jun. 25 - 30 , 2005 ) .", "label": "", "metadata": {}, "score": "53.685642"}
{"text": "Vector comparison is done by adding all absolute differences of all components .Alternatively , the lexicon builder 125 may count how often another word occurs in the same sentence as the target word .The counts may then be normalized by a using the tf / idf method , which is often used in information retrieval .", "label": "", "metadata": {}, "score": "53.715397"}
{"text": "Scikit - Learn .If you have n't yet tried using scikit - learn for text classification , then I hope this article convinces you that it 's worth learning .NLTK 's SklearnClassifier makes the process much easier , since you do n't have to convert feature dictionaries to numpy arrays yourself , or keep track of all known features .", "label": "", "metadata": {}, "score": "53.82605"}
{"text": "Upper case words may optionally be marked with an additional special character .The special character is stored in the character - mapping table 608 , extending the alphabet with an additional character not used in the language of the words in the word - list .", "label": "", "metadata": {}, "score": "53.83256"}
{"text": "Upper case words may optionally be marked with an additional special character .The special character is stored in the character - mapping table 608 , extending the alphabet with an additional character not used in the language of the words in the word - list .", "label": "", "metadata": {}, "score": "53.83256"}
{"text": "Existing statistical taggers which rely on bigrams or trigram , but which do not employ thematic analysis of individual collocations fare poorly on this linguistic aspect .A database of collocations must be put in place in order to perform thematic analysis .", "label": "", "metadata": {}, "score": "53.9234"}
{"text": "If frequency information is not included , then words which are less than three characters long are not included , since they will not be useful for predicting user input .[ 0061 ] .The format defines the structure of a computer file which contains a header 602 followed by a number of tables .", "label": "", "metadata": {}, "score": "53.98676"}
{"text": "This indication would become stronger if , for example , the sequences following d and y in the two corpora would also be aligned .One way to verify this is to reverse both strings , build a BST for the reversed corpora ( a reverse BST ) , and look for a common path that diverges at the same d and y. .", "label": "", "metadata": {}, "score": "54.037575"}
{"text": "High Information Feature Selection .Using the same evaluate_classifier method as in the previous post on classifying with bigrams , I got the following results using the 10000 most informative words : .The accuracy is over 20 % higher when using only the best 10000 words and pos precision has increased almost 24 % while neg recall improved over 40 % .", "label": "", "metadata": {}, "score": "54.0531"}
{"text": "High Information Feature Selection .Using the same evaluate_classifier method as in the previous post on classifying with bigrams , I got the following results using the 10000 most informative words : .The accuracy is over 20 % higher when using only the best 10000 words and pos precision has increased almost 24 % while neg recall improved over 40 % .", "label": "", "metadata": {}, "score": "54.0531"}
{"text": "High Information Feature Selection .Using the same evaluate_classifier method as in the previous post on classifying with bigrams , I got the following results using the 10000 most informative words : .The accuracy is over 20 % higher when using only the best 10000 words and pos precision has increased almost 24 % while neg recall improved over 40 % .", "label": "", "metadata": {}, "score": "54.0531"}
{"text": "High Information Feature Selection .Using the same evaluate_classifier method as in the previous post on classifying with bigrams , I got the following results using the 10000 most informative words : .The accuracy is over 20 % higher when using only the best 10000 words and pos precision has increased almost 24 % while neg recall improved over 40 % .", "label": "", "metadata": {}, "score": "54.0531"}
{"text": "Each classifier is based on the same feature set .A weighted vote is taken among these classifiers to assign senses to test examples .No information from WordNet is utilized by this system .The three classifiers are a bagged J48 decision tree , a Naive Bayesian classifier , and a nearest neighbor classifier ( IBk ) .", "label": "", "metadata": {}, "score": "54.087692"}
{"text": "For empty groups , the offset is equal to the next non - empty offset .Each offset also specifies whether the word definition table located at the offset in the file is simple or complex , as described below .Hence , the size of the offset table 612 is based on the length of the alphabet .", "label": "", "metadata": {}, "score": "54.129833"}
{"text": "The tagging was only done with those features , getting a precision close to 0.85 at the cost of losing coverage .keywords : Supervised learning , decision lists , agglutinative languages , feature selection .organisation : Dept of Computer and Information Science , Link\u00f6ping University .", "label": "", "metadata": {}, "score": "54.13524"}
{"text": "Share this : .If you liked the NLTK demos , then you 'll love the text processing APIs .They provide all the functionality of the demos , plus a little bit more , and return results in JSON .Requests can contain up to 10,000 characters , instead of the 1,000 character limit on the demos , and you can do up to 100 calls per day .", "label": "", "metadata": {}, "score": "54.15172"}
{"text": "The lexicon builder 125 may combine different clues by adding up the matching scores .The scores can be weighted .For example , when using the spelling clue in combination with others , it may be useful to define a cutoff .", "label": "", "metadata": {}, "score": "54.176224"}
{"text": "A weighted vote is taken among these to assign senses to test examples .No information from WordNet is utilized by this system .Each Naive Bayesian classifier is based on a different set of features that are identified in a filtering step prior to learning .", "label": "", "metadata": {}, "score": "54.198326"}
{"text": "6 ) , except that it only contains characters are included in words that are in local word definition tables 708 .The local character - mapping table 704 maps each character in the words in the local word definition tables 708 to a local character index .", "label": "", "metadata": {}, "score": "54.21581"}
{"text": "The word definition tables store the frequency sets calculated at step 506 and the encoded words produced at 510 .The method continues with step 512 of creating an offset table .The offset table contains byte sequences that represent the groups of words .", "label": "", "metadata": {}, "score": "54.23831"}
{"text": "To find the highest information features , we need to calculate information gain for each word .Information gain for classification is a measure of how common a feature is in a particular class compared to how common it is in all other classes .", "label": "", "metadata": {}, "score": "54.376213"}
{"text": "To find the highest information features , we need to calculate information gain for each word .Information gain for classification is a measure of how common a feature is in a particular class compared to how common it is in all other classes .", "label": "", "metadata": {}, "score": "54.376213"}
{"text": "To find the highest information features , we need to calculate information gain for each word .Information gain for classification is a measure of how common a feature is in a particular class compared to how common it is in all other classes .", "label": "", "metadata": {}, "score": "54.376213"}
{"text": "To find the highest information features , we need to calculate information gain for each word .Information gain for classification is a measure of how common a feature is in a particular class compared to how common it is in all other classes .", "label": "", "metadata": {}, "score": "54.376213"}
{"text": "The availability of monolingual corpora has been enhanced greatly due to the digital revolution and widespread use of the World Wide Web .Methods for processing such resources can therefore greatly benefit the field .SUMMARY .In an embodiment , a system may be able to build a translation lexicon from comparable , non - parallel corpora .", "label": "", "metadata": {}, "score": "54.438354"}
{"text": "keywords : Bayesian networks , semantic distance and density , parameter smoothing , MAP .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "54.474377"}
{"text": "One application of linguistic data is to facilitate text entry by predicting word completions based on the first characters of a word that are entered by a user .Given a set of predictions that are retrieved from the linguistic data , the user may select one of the predictions , and thus not have to enter the remaining characters in the word .", "label": "", "metadata": {}, "score": "54.480278"}
{"text": "For example , the left alignment xyzabc , the right alignment xzy - acb and the words y and d in .FIG .9 ( iii ) make up a context alignment 915 .Given a comparable corpus , this procedure will yield many context alignments which correspond to incorrect translations , such as that between the words \" canadien \" and \" previous \" : . tout canadien serieux .", "label": "", "metadata": {}, "score": "54.522125"}
{"text": "The space saved by using the inflection table for each suffix stored is the number of occurrences of the suffix , multiplied by the length of the suffix .The above description relates to one example of the present invention .Many variations will be apparent to those knowledgeable in the field , and such variations are within the scope of the application .", "label": "", "metadata": {}, "score": "54.574455"}
{"text": "( meaning \" untagged \" ) .When local context tagging is completed , the text is next processed by thematic analysis tagger 126 .Thematic analysis tagger 126 uses database 116 to tag the word - pairs left untagged by tagger 124 .", "label": "", "metadata": {}, "score": "54.702637"}
{"text": "The method continues at step 404 , where it is determined if the number of nodes in the tree exceeds a predefined limit , which may be specified in a properties file .If the size of the word - tree does not exceed the limit , then the method continues at step 408 .", "label": "", "metadata": {}, "score": "54.720295"}
{"text": "When the user starts to type a word using a reduced keyboard , the word prefix is ambiguous , since each key on a reduced keyboard is mapped to multiple characters .In this case , a text input logic unit retrieves predictions from the linguistic data that start with any of the combinations of characters that correspond to the prefix entered by the user .", "label": "", "metadata": {}, "score": "54.812126"}
{"text": "keywords : semantic domains , domain driven disambiguation , similarity .Did you use any training data provided in an automatic training procedure ?Yes .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "54.911827"}
{"text": "The method of .claim 1 wherein said expanding comprises using the identically spelled words in the seed lexicon as accurate translations .The method of .claim 1 , further comprising : . identifying substantially identical words in the first and second corpora ; and . adding said substantially identical words to the seed lexicon .", "label": "", "metadata": {}, "score": "54.936344"}
{"text": "Only 149,556 words in this text is manually annotated for sense .The text is also annotated with morphological information ( word segmentation , POS tag and base form and reading , all manually post - edited ) for all words .", "label": "", "metadata": {}, "score": "54.96186"}
{"text": "keywords : supervised learning , Naive Bayesian classifier , bag of words .URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .Did you use any training data provided in an automatic training procedure ?", "label": "", "metadata": {}, "score": "54.971592"}
{"text": "The decision tree learner performs its own feature selection based on the gain ratio , which measures how well a feature partitions the training examples into senses .This is the same approach as taken in duluth3 for English .The only difference is in the stop list .", "label": "", "metadata": {}, "score": "54.97967"}
{"text": "The second feature set is based on unigrams ( one word sequences ) that meet the following criteria : . 1 ) occur 5 or more times and 2 ) are not found on the stop - list .The third feature set is based on bigrams that may include one intervening word that is ignored and that meet the following criteria : .", "label": "", "metadata": {}, "score": "54.99775"}
{"text": "Where the text input device 104 is a reduced keyboard , the text input logic unit 102 also disambiguates individual keystrokes that are received from the reduced keyboard , presenting the user with the most probable characters based on words in the linguistic data 100 .", "label": "", "metadata": {}, "score": "55.049652"}
{"text": "Where the text input device 104 is a reduced keyboard , the text input logic unit 102 also disambiguates individual keystrokes that are received from the reduced keyboard , presenting the user with the most probable characters based on words in the linguistic data 100 .", "label": "", "metadata": {}, "score": "55.049652"}
{"text": "a ) for each feature , keep t - values that are above the 95 % signifcance level , and single out the sense , that has the highest t - value , and give the vote to this sense .b ) order the senses according to vote , .", "label": "", "metadata": {}, "score": "55.06764"}
{"text": "The method of .claim 22 , further comprising steps of : . adding a word selected by a user from a list of predicted words to a learning word - list , wherein the list of predictions contains words retrieved from the word definition tables and the learning word - list ; . obtaining a word associated with a maximum frequency of words in the list of predicted words ; . determining whether the word associated with the maximum frequency was obtained from the word definition tables ; . associating the selected word with a frequency equal to the maximum frequency plus one where the word associated with the maximum frequency was obtained from the word definition tables ; . associating the selected word with a frequency equal to the maximum frequency where the word associated with the maximum frequency was not obtained from the word definition tables ; and .", "label": "", "metadata": {}, "score": "55.080757"}
{"text": "claim 3 , wherein the keyboard is a reduced keyboard .The system of . claim 2 , wherein the user interface and the text input logic unit are implemented on a mobile communication device .The system of . claim 2 , wherein the text input logic unit selects one of the groups of words as the plurality of predicted words .", "label": "", "metadata": {}, "score": "55.146137"}
{"text": "The system also found translations for thirty unknown French words .Of these , nine were correct , which means a precision of 30 % .For each of the two corpora , building the monolingual GST took only 1.5 minutes .", "label": "", "metadata": {}, "score": "55.1589"}
{"text": "This is loosely based on the NAACL-00 paper \" A Simple Approach to Building Ensembles of Naive Bayesian Classifiers for Word Sense Disambiguation \" by Ted Pedersen .This is the same approach as taken in duluth1 for English .The only difference is in the stop list .", "label": "", "metadata": {}, "score": "55.223774"}
{"text": "0058 ] .Statistical data gathered during the method of creating compact linguistic data may optionally be stored at step 514 .[ 0059 ] .[ 0059]FIG .6 is a block diagram of a format of compact linguistic data .", "label": "", "metadata": {}, "score": "55.331146"}
{"text": "( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Y .Description : UST is an unsupervised system for word sense tagging .", "label": "", "metadata": {}, "score": "55.417297"}
{"text": "2000 ) amount of word experts makes it also easy to parallelise the training process .For the classification of a given test item , it is first checked whether a word expert is available .If so , the best performing algorithm on the train set is applied with its optimal parameter settings to classify the item .", "label": "", "metadata": {}, "score": "55.42015"}
{"text": "FIG .7 shows two corpora 705 , 710 , a bilingual lexicon 715 , and the corresponding BST 720 .Edges drawn with dotted lines mark ends of alignment paths through the tree .Their labels are ( unaligned ) continuations of the source language substrings from the respective paths .", "label": "", "metadata": {}, "score": "55.475197"}
{"text": "If the monolingual corpora are somewhat comparable , it can be assumed that a word that occurs in a certain context should have a translation that occurs in a similar context .The context may be defined by the frequencies of context words in surrounding positions .", "label": "", "metadata": {}, "score": "55.488525"}
{"text": "The rule induction method takes as input both context information and all possible keywords within the context of three sentences .Both memory - based learners are cross - validated to determine the optimal parameter settings for each word expert .On these combined classifier outputs and the WordNet most frequent sense , majority voting and weighted voting are performed .", "label": "", "metadata": {}, "score": "55.514282"}
{"text": "6 .[0097 ] .The method begins with the step 900 of finding a configured number of words that occur most frequently in the word - list , based on the absolute frequency of the words .[ 0098 ] .", "label": "", "metadata": {}, "score": "55.52076"}
{"text": "( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ? :No .Description : We have used tokenized , lemmatized , stripped the stop words out of the contexts and detected person names and numbers .", "label": "", "metadata": {}, "score": "55.574562"}
{"text": "Also , the most frequent semantic tags of the whole available context .We chose the ib1 algorithm with weigheted overlap metric + gain ration weighting , parameters that gave best results in a similar exercise for Swedish conducted in the past .", "label": "", "metadata": {}, "score": "55.62609"}
{"text": "When presented with a test example , each classifier assigns a probability to each possible sense .These probabilities are summed and sense with the largest value is assigned .This is loosely based on the NAACL-00 paper \" A Simple Approach to Building Ensembles of Naive Bayesian Classifiers for Word Sense Disambiguation \" by Ted Pedersen . keywords : supervised learning , Naive Bayesian classifier , ensemble .", "label": "", "metadata": {}, "score": "55.759705"}
{"text": "At step 406 , the word - tree is shrunk so that it no longer exceeds the size limit .The tree is shrunk by deleting the least - frequently used words from the tree , which are located in the leaf nodes .", "label": "", "metadata": {}, "score": "55.78151"}
{"text": "The similarity is calculated by considering an agreement between Japanese examples in TM and an input sentence .Otherwise , our system returns a translation as follows : .TM entries are classified according to the English head word after reinforced by a bilingual corpus .", "label": "", "metadata": {}, "score": "55.816643"}
{"text": "[ 0041 ] .The method continues at step 404 , where it is determined if the number of nodes in the tree exceeds a predefined limit , which may be specified in a properties file .If the size of the word - tree does not exceed the limit , then the method continues at step 408 .", "label": "", "metadata": {}, "score": "55.837254"}
{"text": "Note that it 's significant bigrams that enhance effectiveness .I tried using nltk.util.bigrams to include all bigrams , and the results were only a few points above baseline .This points to the idea that including only significant features can improve accuracy compared to using all features .", "label": "", "metadata": {}, "score": "55.85657"}
{"text": "Note that it 's significant bigrams that enhance effectiveness .I tried using nltk.util.bigrams to include all bigrams , and the results were only a few points above baseline .This points to the idea that including only significant features can improve accuracy compared to using all features .", "label": "", "metadata": {}, "score": "55.85657"}
{"text": "Note that it 's significant bigrams that enhance effectiveness .I tried using nltk.util.bigrams to include all bigrams , and the results were only a few points above baseline .This points to the idea that including only significant features can improve accuracy compared to using all features .", "label": "", "metadata": {}, "score": "55.85657"}
{"text": "Note that it 's significant bigrams that enhance effectiveness .I tried using nltk.util.bigrams to include all bigrams , and the results were only a few points above baseline .This points to the idea that including only significant features can improve accuracy compared to using all features .", "label": "", "metadata": {}, "score": "55.85657"}
{"text": "There were 29 nouns and 15 adjectives , with between 70 and 455 instances per word ( total instances : 7567 , divided 2:1 between training data and test data ) .Inter - tagger - agreement was 85.5 % .( verb data is not covered here as that was prepared by Martha Palmer and colleagues at UPenn ) .", "label": "", "metadata": {}, "score": "55.869377"}
{"text": "The common prefixes of words in the local word definition tables 708 are removed .In addition to containing local word definition tables 708 , each word definition table includes a local offset table 706 , which is used to locate each of the local word definition tables 708 .", "label": "", "metadata": {}, "score": "55.93651"}
{"text": "keywords : supervised learning , ensemble .URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .Did you use any training data provided in an automatic training procedure ?", "label": "", "metadata": {}, "score": "56.002594"}
{"text": "The method of .claim 1 , wherein said identifying comprises identifying cognates .The method of .claim 1 , wherein said identifying comprises identifying word pairs having a minimum longest common subsequence ratio .The method of .claim 1 , wherein said one or more clues includes similar context .", "label": "", "metadata": {}, "score": "56.00496"}
{"text": "Each word in these texts is manually annotated with its appropriate sense , by six persons who all processed a different part of the data .The sense inventory is roughly based on a Dutch children 's dictionary .Sense tags are non - hierarchical .", "label": "", "metadata": {}, "score": "56.02008"}
{"text": "an offset table for locating the word definition tables , wherein for each of the common prefixes , the offset table contains a location of the word definition table which stores words starting with the common prefix ; . a plurality of frequencies associated with the words , wherein the word definition tables store the frequencies ; . wherein at least one of the word definition tables comprises : . a local offset table for locating each of the local word definition tables ; . a local character - mapping table for mapping each character in the words in the local word definition tables to a local character index ; . a hotword table for storing the location in the local word definition tables of words which are associated with highest frequencies of words in the group contained in the word definition table ; and .", "label": "", "metadata": {}, "score": "56.096825"}
{"text": "The assumption is that if words clustered together based on their translations then the relevant senses will get higher weights given the appropriate similarity measure .keywords : parallel corpora , token alignments , WordNet , information - theoretic similarity measure . organisation : TALP Research Center - Technical University of Catalonia .", "label": "", "metadata": {}, "score": "56.153866"}
{"text": "The system 400 may use a suffix tree data structure in order to identify the alignments .The suffix tree of a string uniquely encodes all the suffixes of that string ( and thus , implicitly , all its substrings too ) .", "label": "", "metadata": {}, "score": "56.184002"}
{"text": "Tag Archives : bigrams .NLTK - Trainer ( available github and bitbucket ) was created to make it as easy as possible to train NLTK text classifiers .The train_classifiers.py script provides a command - line interface for training & evaluating classifiers , with a number of options for customizing text feature extraction and classifier training ( run python train_classifier.py --help for a complete list of options ) .", "label": "", "metadata": {}, "score": "56.226616"}
{"text": "A topical context consists of the following features for all open - class words within a window .Word : an open - class word The window size of + -1 sentences was empirically chosen .A bigram context consists of the following features for all word pairs within a window .", "label": "", "metadata": {}, "score": "56.270065"}
{"text": "This system is identical to duluth2 , except that it relies on a different feature set .No information from WordNet is utilized by this system .This system uses a filter to perform feature identification prior to learning .Two different kinds of bigrams are identified as candidate features .", "label": "", "metadata": {}, "score": "56.270264"}
{"text": "[ 0090 ] .[ 0091 ] .The following is an example of the method of FIG .8 where the three - character prefix is entered using a reduced keyboard .The reduced keyboard includes a key for entering \" a \" , \" b \" or \" c \" , a key for entering \" n \" or \" o \" , and a key for entering \" w \" , \" x \" , or \" y \" .", "label": "", "metadata": {}, "score": "56.29071"}
{"text": "The only difference is that morphological information in the evaluation data was corrected automatically .We learned the transformation rules like Brill 's POS tagger to correct morphological information .keywords : decision list , correcting morphological information , transformation rules .", "label": "", "metadata": {}, "score": "56.318382"}
{"text": "Such unigrams form a set of features .The training examples are converted into feature vectors , where each feature represents whether or not a unigram occurs in the context of a specific training example .These features vectors are used to make the estimates of the parameters of the Naive Bayesian classifier .", "label": "", "metadata": {}, "score": "56.33041"}
{"text": "We also ended up with a different top 10 most informative features .This is due to train_classifier.py choosing slightly different training instances than the code in the previous articles .But the results are still basically the same .Filtering Stopwords .", "label": "", "metadata": {}, "score": "56.43257"}
{"text": "We also ended up with a different top 10 most informative features .This is due to train_classifier.py choosing slightly different training instances than the code in the previous articles .But the results are still basically the same .Filtering Stopwords .", "label": "", "metadata": {}, "score": "56.43257"}
{"text": "We also ended up with a different top 10 most informative features .This is due to train_classifier.py choosing slightly different training instances than the code in the previous articles .But the results are still basically the same .Filtering Stopwords .", "label": "", "metadata": {}, "score": "56.43257"}
{"text": "Given a set of predictions that are retrieved from the linguistic data , the user may select one of the predictions , and thus not have to enter the remaining characters in the word .The prediction of user input is especially useful when included in a mobile device , since such devices typically have input devices , including keyboards , that are constrained in size , Input prediction minimizes the number of keystrokes required to enter words on such devices .", "label": "", "metadata": {}, "score": "56.43753"}
{"text": "The --sequential argument also recognizes the letter a , which will insert an AffixTagger into the backoff chain .If you do not specify the --affix argument , then it will include one AffixTagger with a 3-character suffix .However , you can change this by specifying one or more --affix N options , where N should be a positive number for prefixes , and a negative number for suffixes .", "label": "", "metadata": {}, "score": "56.445953"}
{"text": "claim 16 , wherein the computer - readable file is embodied in a computer - readable medium .A method of creating compact linguistic data , comprising steps of : . creating a word - list comprising a plurality of words occurring most frequently in a corpus ; . sorting the words in the word - list alphabetically ; . creating a character - mapping table which maps characters in the words to character indexes ; . separating the words in the word - list into groups , wherein words in each group have a common prefix ; . creating a substitution table which maps character sequences in the words to substitution indexes ; . creating word definition tables and storing the encoded words in the word definition tables ; . creating an offset table for locating groups of encoded words ; and .", "label": "", "metadata": {}, "score": "56.453"}
{"text": "Bag of Words Feature Extraction .All of the NLTK classifiers work with featstructs , which can be simple dictionaries mapping a feature name to a feature value .For text , we 'll use a simplified bag of words model where every word is feature name with a value of True .", "label": "", "metadata": {}, "score": "56.474007"}
{"text": "[0085 ] .The method starts with the step 802 of adding a user - selected word 800 to the learning word - list .The user - selected word 800 is the word selected by the user from the list of predicted words offered that begin with a word prefix entered by the user .", "label": "", "metadata": {}, "score": "56.523132"}
{"text": "The method starts with the step 802 of adding a user - selected word 800 to the learning word - list .The user - selected word 800 is the word selected by the user from the list of predicted words offered that begin with a word prefix entered by the user .", "label": "", "metadata": {}, "score": "56.539635"}
{"text": "Fortunately , only a small percent of the cases ( in newspaper stories ) depend on global reading .The third type of due is corpus analysis and is described in R. Beckwith , \" Wordnet : A Lexical Database Organized in Psycholinguistic Principles \" in Lexical Acquisition : Exploiting On - Line Dictionary to Build a Lexicon , Lawrence Erlbaum Assoc . , 1991 .", "label": "", "metadata": {}, "score": "56.58001"}
{"text": "Such bigrams must meet the following criteria : .This is the same approach as taken in duluth5 for English .The only difference is in the stop list .This is loosely based on the NAACL-01 paper \" A Decision Tree of Bigrams is an Accurate Predictor of Word Sense \" by Ted Pedersen . keywords : supervised learning , decision tree of bigrams , bagging .", "label": "", "metadata": {}, "score": "56.590874"}
{"text": "The compact linguistic data structure of .claim 6 , further comprising an inflection table for storing suffixes of the words , wherein the words stored in the word definition tables refer to the suffixes stored in the inflection table .The compact linguistic data structure of .", "label": "", "metadata": {}, "score": "56.59795"}
{"text": "N .Description : Our systems are descriptive - semantic - primitive - based , general - domain systems that do not require training or supervision .There are three components to the systems : a machine tool level machine - tractable dictionary ( MTD ) , a semantic distance matrix of the primitives , and a semantic tagger that uses a simple summation algorithm .", "label": "", "metadata": {}, "score": "56.608658"}
{"text": "The training examples are converted into feature vectors , where each feature represents whether or not a unigram occurs in the context of a specific training example .These features vectors are used to make the estimates of the parameters of the Naive Bayesian classifier .", "label": "", "metadata": {}, "score": "56.672108"}
{"text": "Each complex word definition table also includes a local character - mapping table 704 .This table is functionally the same as the character - mapping table 608 ( .FIG .6 ) , except that it only contains characters are included in words that are in local word definition tables 708 .", "label": "", "metadata": {}, "score": "56.701572"}
{"text": "Features were collected in 7-word window around the target word , and decision list method was used for learning .Did you use any training data provided in an automatic training procedure ?N .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "56.70277"}
{"text": "Share this : .Say you want to build a frequency distribution of many thousands of samples with the following characteristics : . fast to build .The only solution I know that meets those requirements is Redis .NLTK 's FreqDist is not persistent , shelve is far too slow , BerkeleyDB is not network accessible ( and is generally a PITA to manage ) , and AFAIK there 's no other key - value store that makes sliceable lists really easy to create & access .", "label": "", "metadata": {}, "score": "56.71882"}
{"text": "If the lexicon is probabilistic , each matching between two words will be weighted by the corresponding translation probability .The paths in the resulting bilingual tree will also have weights associated with them , defined as the product of the matching probabilities of the words along the path .", "label": "", "metadata": {}, "score": "56.801975"}
{"text": "Description : For a target word , accumulate all WordNet 1.7 examples ( stuff in quotes ) that are related to the word 's plausible synsets and any of their related synsets .( Related synsets include all ancestors for parent relations , immediate children for child relations , and transitive closure within a relation for all other relations . )", "label": "", "metadata": {}, "score": "56.810448"}
{"text": "0105 ] .Also , the methods illustrated in FIGS .3 , 4 , 5 , 8 and 9 may contain fewer , more or different steps than those that are shown .For example , although the methods describe using computer files to store final and intermediate results of the methods , the results could also be stored in computer memory such as RAM or Flash memory modules .", "label": "", "metadata": {}, "score": "56.845367"}
{"text": "In this case , the sense tag selected would be the sense tag ( of those tags which could apply to \" art \" ) most frequently assigned to the other words with the same part of speech in the same section of text .", "label": "", "metadata": {}, "score": "56.8564"}
{"text": "Description : The CL Research disambiguation system is part of the DIMAP dictionary software , which has been designed to use any full dictionary as the basis for disambiguation .Senseval-2 results were generated using WordNet , but also using the New Oxford Dictionary of English ( NODE ) .", "label": "", "metadata": {}, "score": "56.85801"}
{"text": "Marcu , Daniel , \" Building Up Rhetorical Structure Trees , \" 1996 , Proc . of the National Conference on Artificial Intelligence and Innovative Applications of Artificial Intelligence Conference , vol .2 , pp .1069 - 1074 .Och , F. , \" Minimum Error Rate Training in Statistical Machine Translation , \" In Proceedings of the 41st Annual Meeting on Assoc . for Computational Linguistics - vol . 1 ( Sapporo , Japan , Jul. 7 - 12 , 2003 ) .", "label": "", "metadata": {}, "score": "56.880768"}
{"text": "The --double - metaphone algorithm comes from metaphone.py , while all the other phonetic algorithm have been copied from the advas project ( which appears to be abandoned ) .I created these options after discussions with Michael D Healy about Twitter Linguistics , in which he explained the prevalence of regional spelling variations .", "label": "", "metadata": {}, "score": "56.90001"}
{"text": "keywords : word experts , word sense disambiguation as deduction , supervised learning , transformation - based learning .Did you use any training data provided in an automatic training procedure ?N .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "56.962914"}
{"text": "This variability factor is a measure of the variability of the form that the particular word pair takes in the training text .A database of all of the content - word pairs and their associated variability factors is created for use by a program which performs tagging of a body of text .", "label": "", "metadata": {}, "score": "57.043457"}
{"text": "The decision tree learner performs its own feature selection based on the gain ratio , which measures how well a feature partitions the training examples into senses .The bagging process for each decision tree is as described in duluth2 , and the features used as the basis for each decision tree are the same as in duluth1 .", "label": "", "metadata": {}, "score": "57.13936"}
{"text": "The method continues with a step of encoding the words in the groups into byte sequences using the character - mapping table and the substitution table .The method continues with a step of creating word definition tables and storing the encoded words in the word definition tables .", "label": "", "metadata": {}, "score": "57.170753"}
{"text": "keywords : word sense disambiguation , morphological analysis , classifier combination .name : David Yarowsky , Silviu Cucerzan , Radu Florian , Charles Schafer and Richard Wicentowski . jhu.edu .organisation : Computer Science Department and Center for Language and Speech Processing , Johns Hopkins University .", "label": "", "metadata": {}, "score": "57.17498"}
{"text": "No .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ? :No .Description : We have used tokenized , lemmatized , stripped the stop words out of the contexts and detected person names and numbers .", "label": "", "metadata": {}, "score": "57.31526"}
{"text": "The method of claim 1 wherein said high variability factors exceed 0.75 and said low variability factors are less than or equal to 0.75 .The method of claim 1 comprising the additional step of using local context analysis to tag collocation content word pairs before using said collocation database .", "label": "", "metadata": {}, "score": "57.4598"}
{"text": "( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : The JHU SENSEVAL-2 system for the lexical sample tasks consists of 6 diverse supervised learning subsystems integrated via classifier combination .", "label": "", "metadata": {}, "score": "57.532146"}
{"text": "claim 6 , wherein the selected group of words is selected based on the frequency of the group of words .The system of . claim 2 , wherein the text input logic unit is configured to update the frequencies of the words based on whether the predicted word is input by a device user .", "label": "", "metadata": {}, "score": "57.564034"}
{"text": "As will be appreciated by those skilled in the art , characters are represented in computer systems by sequences of bits .The words in the word definition tables 614 are separated by characters with the most significant bit set .If a character has its most significant bit set , then it is the last character in a word .", "label": "", "metadata": {}, "score": "57.61743"}
{"text": "In contrast , VF ( joint - venture ) is 1.00 .A list of the first 38 content - word pairs encountered in a test corpus is shown below .The frequency of each collocation P in the corpus relative to its stem frequency is shown .", "label": "", "metadata": {}, "score": "57.656475"}
{"text": "claim 1 , wherein said identifying comprises : . identifying a plurality of context words ; and . identifying a frequency of context words in an n - word window around a target word .The method of .claim 9 , further comprising generating a context vector .", "label": "", "metadata": {}, "score": "57.67188"}
{"text": "Building the suffix tree of a string takes time and space linear in the length of the string .Building a GST for a set of strings takes time and space linear in the sum of the lengths of all strings in the set .", "label": "", "metadata": {}, "score": "57.703484"}
{"text": "0054 ] .The method continues with step 510 of encoding the word groups into byte sequences using the character - mapping table and the substitution table , as described above .The prefixes used to collect words into groups are removed from the words themselves .", "label": "", "metadata": {}, "score": "57.80568"}
{"text": "No .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ? :Yes .Description : We have used tokenized , lemmatized , stripped the stop words out of the contexts and detected person names and numbers .", "label": "", "metadata": {}, "score": "57.84114"}
{"text": "The compact linguistic data structure of .claim 6 , further comprising a plurality of frequencies associated with the words , wherein the word definition tables store the frequencies .The compact linguistic data structure of .The compact linguistic data structure of .", "label": "", "metadata": {}, "score": "57.880394"}
{"text": "Second method disambiguates all ambiguous verbs and adjectives instances in the test data .This algorithm implements a supervised learning method ( Maximum Entropy Probability Models ) consisting of the estimation of functions for classifying word senses by learning on a training data provided and then applied on the test instances .", "label": "", "metadata": {}, "score": "57.927773"}
{"text": "The second is a non - consecutive two word sequence , where there may be zero or one intervening word that is ignored .Such bigrams must meet the following criteria : .The process of converting the training examples into feature vectors , bagging the decision tree , and making sense assignments is identical to duluth2 .", "label": "", "metadata": {}, "score": "57.989388"}
{"text": "For example , blocks in the flowcharts may be skipped or performed out of order and still produce desirable results .Also , the heuristics described herein may be combined with the alignment method described herein .Accordingly , other embodiments are within the scope of the following claims .", "label": "", "metadata": {}, "score": "58.05495"}
{"text": "A topical context consists of the following features for all open - class morphemes within a window .Morpheme : an open - class morpheme The window size of all sentences was empirically chosen .A bigram context consists of the following features for all word pairs within a window .", "label": "", "metadata": {}, "score": "58.09591"}
{"text": "[ 0005 ] .The growing use of mobile devices and different types of embedded systems challenges the developers and manufacturers of these devices to create products that require minimal memory usage , yet perform well .A key element of these products is the user interface , which typically enables a user to enter text which is processed by the product .", "label": "", "metadata": {}, "score": "58.162292"}
{"text": "claim 1 , wherein said identifying comprises identifying frequencies of occurrence of words in the first and second first corpora .The method of .claim 1 , further comprising : generating matching scores for each of a plurality of clues .", "label": "", "metadata": {}, "score": "58.17885"}
{"text": "[ 0055 ] .The method continues with step 511 of creating word definition tables .The word definition tables store the frequency sets calculated at step 506 and the encoded words produced at 510 .[ 0056 ] .The method continues with step 512 of creating an offset table .", "label": "", "metadata": {}, "score": "58.334137"}
{"text": "For empty groups , the offset is equal to the next non - empty offset .Each offset also specifies whether the word definition table located at the offset in the file is simple or complex , as described below .[", "label": "", "metadata": {}, "score": "58.430874"}
{"text": "Procedure of sense disambiguation .( 1 ) Filter out senses using the satellite features .( 2 ) Disambiguate word sense using the classification information model .Classification information model(CIM ) CIM disambiguates word sense considering the discrimination score(DS ) of features .", "label": "", "metadata": {}, "score": "58.462746"}
{"text": "Parser 130 is standard text parsing software which accepts as input the marked up text as processed according to the method just described .If a word pair is a collocation ( e.g. , holding companies ) , and one of the two words is tagged \" ? ? \" then generate the S - stripped version ( i.e. , holding company ) , and the affix - stripped version ( i.e. , hold company ) .", "label": "", "metadata": {}, "score": "58.489662"}
{"text": "an exception table for storing words which are associated with highest frequencies of words in the group contained in the word definition table , . wherein the common prefixes of the words in the local word definition tables are removed .The mobile communication device of . claim 10 , further comprising a name table , the name table comprising a name for identifying the words .", "label": "", "metadata": {}, "score": "58.50036"}
{"text": "Each offset also indicates whether the table that is referred to by the offset is a complex or simple word definition table .[0079 ] .Each complex word definition table also includes a local character - mapping table 704 .", "label": "", "metadata": {}, "score": "58.512817"}
{"text": "Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : The system selects the most similar TM entry based on the cosine similarity between context vectors , which were constructed from semantic features and syntactic relations of neighboring words of the target word .", "label": "", "metadata": {}, "score": "58.611244"}
{"text": "The use of bagging and a stop list is new for Senseval .This is the same approach as taken in duluth2 for English .The only difference is in the stop list .keywords : supervised learning , decision tree of bigrams , bagging .", "label": "", "metadata": {}, "score": "58.616142"}
{"text": "The method of .claim 22 , where the step of calculating absolute frequencies associated with the filtered words comprises steps of : . adding the filtered words to the word - tree , the adding step for each filtered word comprising the steps of : . determining whether the filtered word is in the word - tree ; . adding the filtered word and its associated absolute frequency to the word - tree where the filtered word is not in the word - tree ; . incrementing the absolute frequency associated with the filtered word where the filtered word is in the word - tree ; . determining whether the number of nodes in the word - tree exceeds a predefined limit ; and . shrinking the word - tree where the number of nodes in the word - tree exceeds the predefined limit , wherein the step of shrinking comprises the step of deleting leaf nodes which contain words associated with lowest absolute frequencies .", "label": "", "metadata": {}, "score": "58.67005"}
{"text": "The second is interference between coinciding collocations such as : market - experience and marketing - experience , or ship - agent and shipping - agent .Fortunately , these cases are very infrequent .Adjectives and nouns are difficult to distinguish in raw corpus ( unless they are marked as such lexically ) .", "label": "", "metadata": {}, "score": "58.807346"}
{"text": "The non - transitory computer readable medium of .claim 15 wherein said expanding comprises using the identically spelled words in the seed lexicon as accurate translations .The non - transitory computer readable medium of . claim 15 , further comprising : . identifying substantially identical words in the first and second corpora ; and . adding said substantially identical words to the seed lexicon .", "label": "", "metadata": {}, "score": "58.821648"}
{"text": "keywords : memory - based learning , rule induction , classifier combination , word experts .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "58.880676"}
{"text": "The user 's habits , confirmed by the input choices he or she makes when presented with word prediction alternatives , are learned by the text input logic unit and stored in tables including those described below .Learning capabilities include the modification of frequency information for words , and the addition of words to the linguistic data .", "label": "", "metadata": {}, "score": "58.96897"}
{"text": "( a )If neither collocation is found , then do nothing .( b ) if only affix - stripped collocation is found , or . if VF ( variability factor ) is smaller than threshold , then tag first word a verb and the second word a noun ; .", "label": "", "metadata": {}, "score": "59.06934"}
{"text": "Rather than learned a bagged decision tree ( as duluth10 does ) this system simply learns a decision stump , a one node decision tree .The features used are the same as duluth10 .This system provides a baseline that can be used to compare the benefits of learning an entire decision tree ( duluth10 ) versus identifying a single node tree ( duluthB ) .", "label": "", "metadata": {}, "score": "59.159393"}
{"text": "Franz Josef Och , Hermann Ney : \" Improved Statistical Alignment Models \" ACLOO :Proc . of the 38th Annual Meeting of the Association for Computational Lingustics , ' Online !Oct. 2 - 6 , 2000 , pp .440 - 447 , XP002279144", "label": "", "metadata": {}, "score": "59.160965"}
{"text": "So maybe otherwise neutral or meaningless words are being placed in the pos class because the classifier does n't know what else to do .If this is the case , then the metrics should improve if we eliminate the neutral or meaningless words from the featuresets , and only classify using sentiment rich words .", "label": "", "metadata": {}, "score": "59.261707"}
{"text": "A table is located using the index information found at the table 's entry in the index table 604 .The index table 604 is followed by the name table 606 .The name table 606 contains a name which identifies the word - list .", "label": "", "metadata": {}, "score": "59.284595"}
{"text": "int means word counts are used , so if a word occurs twice , it gets the number 2 as its feature value ( whereas with bow it would still get a 1 ) .And tfidf means the TfidfTransformer is used to produce a floating point number that measures the importance of a word , using the tf - idf algorithm .", "label": "", "metadata": {}, "score": "59.300735"}
{"text": "The compact linguistic data structure of .The compact linguistic data structure of .claim 7 , further comprising : .The compact linguistic data structure of .claim 6 , wherein the character - mapping table , the substitution table , the word definition tables , and the offset table are contained in a computer - readable file .", "label": "", "metadata": {}, "score": "59.31094"}
{"text": "The only difference is in the stop list .This system is motivated by the relative success of decision stumps as reported in the NAACL-01 paper \" A Decision Tree of Bigrams is an Accurate Predictor of Word Sense \" by Ted Pedersen . keywords : supervised learning , decision stumps .", "label": "", "metadata": {}, "score": "59.31551"}
{"text": "Classification produces a confidence score for every sense ; our answers include just the highest scoring sense for each instance .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "59.323494"}
{"text": "( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : SST is a supervised word sense tagger .This tagger uses support vector machine ( SVM ) learning to build classifiers from the training data .", "label": "", "metadata": {}, "score": "59.40445"}
{"text": "Classification information model(CIM ) CIM disambiguates word sense considering the discrimination score(DS ) of features .The DS of a feature is the sum of relevance scores between the feature and each sense .The relevance score between a feature and a sense is proportion to the conditional probability of the feature given the sense .", "label": "", "metadata": {}, "score": "59.407036"}
{"text": "Each word is associated with candidate parts of speech , and almost all words . are ambiguous .The tagger 's task is to resolve the ambiguity .A program can bring to bear 3 types of clues in resolving part - of - speech ambiguity .", "label": "", "metadata": {}, "score": "59.4199"}
{"text": "Priority information is used to assign relative importance to the linguistic data when multiple files containing linguistic data are used by a text input logic unit .The header 602 also indicates whether the file includes frequency information .The header 602 is followed by the index table 604 .", "label": "", "metadata": {}, "score": "59.43247"}
{"text": "The apparatus of 26 , the apparatus comprising : . an alignment module operative to be executed to align text segments in two nonparallel corpora , the corpora including a source language corpus and a target language corpus .The apparatus of .", "label": "", "metadata": {}, "score": "59.444458"}
{"text": "Each of those systems outputs probabilities for each sense when presented with a test example , so all of these are summed together and the sense with the maximum probability is assigned to a test example .keywords : supervised learning , ensemble .", "label": "", "metadata": {}, "score": "59.49062"}
{"text": "[0075 ] .[ 0075]FIG .7 is a block diagram of a complex word definition table .The complex word definition table is recursive , in that it contains local word definition tables 708 , each of which is a simple or complex word definition table as described above .", "label": "", "metadata": {}, "score": "59.50762"}
{"text": "If it is determined at step 312 that there are more source files to filter , then the method continues at step 300 .Otherwise , the method ends at step 314 .When the method ends , all of the source files which comprise the corpus have been filtered .", "label": "", "metadata": {}, "score": "59.591866"}
{"text": "( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : This system takes a supervised learning approach to word sense disambiguation , where a decision tree is induced from sense - tagged training examples .", "label": "", "metadata": {}, "score": "59.610634"}
{"text": "For example , if the word - list contains 520 words with the prefix \" co \" , then this group will be separated into groups with prefixes \" com \" , \" con \" , and so on .[ 0052 ] .", "label": "", "metadata": {}, "score": "59.652534"}
{"text": "Notice that morphological information in the training data is post - edited , but not in the evaluation data , so participants may ignore morphological information in the evaluation data .The number of target words is 100 , 50 nouns and 50 verbs .", "label": "", "metadata": {}, "score": "59.785736"}
{"text": "And the results for a stopword filtered bag of words are : . accuracy : 0.726 pos precision : 0.649867374005 pos recall : 0.98 neg precision : 0.959349593496 neg recall : 0.472 .Accuracy went down .2 % , and pos precision and neg recall dropped as well !", "label": "", "metadata": {}, "score": "59.802643"}
{"text": "And the results for a stopword filtered bag of words are : . accuracy : 0.726 pos precision : 0.649867374005 pos recall : 0.98 neg precision : 0.959349593496 neg recall : 0.472 .Accuracy went down .2 % , and pos precision and neg recall dropped as well !", "label": "", "metadata": {}, "score": "59.802643"}
{"text": "And the results for a stopword filtered bag of words are : . accuracy : 0.726 pos precision : 0.649867374005 pos recall : 0.98 neg precision : 0.959349593496 neg recall : 0.472 .Accuracy went down .2 % , and pos precision and neg recall dropped as well !", "label": "", "metadata": {}, "score": "59.802643"}
{"text": "And the results for a stopword filtered bag of words are : . accuracy : 0.726 pos precision : 0.649867374005 pos recall : 0.98 neg precision : 0.959349593496 neg recall : 0.472 .Accuracy went down .2 % , and pos precision and neg recall dropped as well !", "label": "", "metadata": {}, "score": "59.802643"}
{"text": "Without bigrams , precision and recall are less balanced .But the differences may depend on your particular data , so do n't assume these observations are always true .Improving Feature Selection .The big lesson here is that improving feature selection will improve your classifier .", "label": "", "metadata": {}, "score": "59.847725"}
{"text": "Without bigrams , precision and recall are less balanced .But the differences may depend on your particular data , so do n't assume these observations are always true .Improving Feature Selection .The big lesson here is that improving feature selection will improve your classifier .", "label": "", "metadata": {}, "score": "59.847725"}
{"text": "Without bigrams , precision and recall are less balanced .But the differences may depend on your particular data , so do n't assume these observations are always true .Improving Feature Selection .The big lesson here is that improving feature selection will improve your classifier .", "label": "", "metadata": {}, "score": "59.847725"}
{"text": "Without bigrams , precision and recall are less balanced .But the differences may depend on your particular data , so do n't assume these observations are always true .Improving Feature Selection .The big lesson here is that improving feature selection will improve your classifier .", "label": "", "metadata": {}, "score": "59.847725"}
{"text": "The growing use of mobile devices and different types of embedded systems challenges the developers and manufacturers of these devices to create products that require minimal memory usage , yet perform well .A key element of these products is the user interface , which typically enables a user to enter text which is processed by the product .", "label": "", "metadata": {}, "score": "59.888096"}
{"text": "This table enables the identification of the start of a byte sequences that represents a particular word group .The offset table is used to locate the byte sequences that comprise the encoded words for a particular group that start with a common prefix .", "label": "", "metadata": {}, "score": "59.95194"}
{"text": "FIG .8 where the three - character prefix is entered using a reduced keyboard .The reduced keyboard includes a key for entering \" a \" , \" b \" or \" c \" , a key for entering \" n \" or \" o \" , and a key for entering \" w \" , \" xx \" , or \" y \" .", "label": "", "metadata": {}, "score": "59.962856"}
{"text": "[ 0024 ] .[ 0024]FIG .1 is a block diagram of a system in which linguistic data is used for text input prediction .The system includes linguistic data 100 , a text input logic unit 102 , and a user interface 103 .", "label": "", "metadata": {}, "score": "59.97113"}
{"text": "Internally , RedisFreqDist also stores a set of all the samples under the key _ _ samples _ _ for efficient lookup and sorting .Here 's some example code for using it .For more info , checkout the wiki , or read the code .", "label": "", "metadata": {}, "score": "59.97708"}
{"text": "keywords : machine translation system , translation dictionary , semantic analysis .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "60.040863"}
{"text": "The sense with the highest probability is assigned to the test example .This system implements a standard benchmark , the Naive Bayesian classifier based on a bag of words feature set .This is the same approach as taken in duluth4 for English .", "label": "", "metadata": {}, "score": "60.055027"}
{"text": "ALGORITHM --fraction 0.75 .For int features , the option --value - type int was used , and for tfidf features , the options --value - type float --tfidf were used .This was with NLTK 2.0.3 and sklearn 0.12.1 .Only BernoulliNB & MultinomialNB got a modest boost in accuracy , putting them on - par with the rest of the algorithms .", "label": "", "metadata": {}, "score": "60.127808"}
{"text": "The above description relates to one example of the present invention .Many variations will be apparent to those knowledgeable in the field , and such variations are within the scope of the application .[ 0103 ] .For example , while the language used in most of the examples is English , the system and method provided creates compact linguistic data for any alphabetical language .", "label": "", "metadata": {}, "score": "60.137333"}
{"text": "This also goes against what I said at the end of the article on high information feature selection : . bigrams do n't matter much when using only high information words .In fact , bigrams can make a huge difference , but you ca n't restrict them to just 200 significant collocations .", "label": "", "metadata": {}, "score": "60.19675"}
{"text": "This also goes against what I said at the end of the article on high information feature selection : . bigrams do n't matter much when using only high information words .In fact , bigrams can make a huge difference , but you ca n't restrict them to just 200 significant collocations .", "label": "", "metadata": {}, "score": "60.19675"}
{"text": "This also goes against what I said at the end of the article on high information feature selection : . bigrams do n't matter much when using only high information words .In fact , bigrams can make a huge difference , but you ca n't restrict them to just 200 significant collocations .", "label": "", "metadata": {}, "score": "60.19675"}
{"text": "URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .Did you use any training data provided in an automatic training procedure ?YES .", "label": "", "metadata": {}, "score": "60.207035"}
{"text": "URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .Did you use any training data provided in an automatic training procedure ?YES .", "label": "", "metadata": {}, "score": "60.207035"}
{"text": "URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .Did you use any training data provided in an automatic training procedure ?YES .", "label": "", "metadata": {}, "score": "60.207035"}
{"text": "URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .Did you use any training data provided in an automatic training procedure ?YES .", "label": "", "metadata": {}, "score": "60.207035"}
{"text": "Still , these words , often called \" cognates , \" maintain a very similar spelling .This can be defined as differing in very few letters .This measurement can be formalized as the number of letters common in sequence between the two words , divided by the length of the longer word .", "label": "", "metadata": {}, "score": "60.211693"}
{"text": "However , if learning capabilities are applied , as described below , then the initial sorting is no longer valid , and the encoded words may need to be resorted .As will be appreciated by those skilled in the art , characters are represented in computer systems by sequences of bits .", "label": "", "metadata": {}, "score": "60.258835"}
{"text": "Description : Because no training data was provided for Italian , the JHU_Italian system was unsupervised , exploiting hierarchical cluster models induced from the Italian WordNet .Because several relationship types ( e.g. hypernymy ) are represented in the Italian WordNet , each relationship type was arbitrarily assigned a generic weight indicating a rough semantic similarity implied by that relationship ( e.g. synonymy received a 0.5 , while meronomy received a 10 ) .", "label": "", "metadata": {}, "score": "60.536427"}
{"text": "But just selecting the most frequently occurring bigrams ( sequence of two adjacent words ) does not always yield the better results .Following are a few bigrams resulted from an experiment . of the in the to the as a has been New York is a for a .", "label": "", "metadata": {}, "score": "60.569458"}
{"text": "Share this : .When your classification model has hundreds or thousands of features , as is the case for text categorization , it 's a good bet that many ( if not most ) of the features are low information .", "label": "", "metadata": {}, "score": "60.6"}
{"text": "Share this : .When your classification model has hundreds or thousands of features , as is the case for text categorization , it 's a good bet that many ( if not most ) of the features are low information .", "label": "", "metadata": {}, "score": "60.6"}
{"text": "5 .The linguistic data 204 produced by the linguistic data analyzer 202 illustrated in FIG .6 . [ 0032 ] .The absolute frequency of a certain group of words found in the corpus 200 may alternatively be modified by separating this group to a different file and assigning a custom weight to this file .", "label": "", "metadata": {}, "score": "60.636086"}
{"text": "0067 ] .The substitution table 610 is followed by the offset table 612 .This table is used to locate a word definition table , described below , based on the common prefix of words in the word definition table to be located .", "label": "", "metadata": {}, "score": "60.66779"}
{"text": "This is based on the NAACL-01 paper \" A Decision Tree of Bigrams is an Accurate Predictor of Word Sense \" by Ted Pedersen .The use of bagging and a stop list is new for Senseval .keywords : supervised learning , decision tree of bigrams , bagging .", "label": "", "metadata": {}, "score": "60.679253"}
{"text": "The NLTK metrics module provides functions for calculating all three metrics mentioned above .But to do so , you need to build 2 sets for each classification label : a reference set of correct values , and a test set of observed values .", "label": "", "metadata": {}, "score": "60.72766"}
{"text": "The features used in the model is outputs of morphological and syntactic analysis .We used simple Bayes for learning .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "60.761864"}
{"text": "Hierarchical classification is an obscure but simple concept .The idea is that you arrange two or more classifiers in a hierarchy such that the classifiers lower in the hierarchy are only used if a higher classifier returns an appropriate result .", "label": "", "metadata": {}, "score": "60.80662"}
{"text": "The meanings of these multi - word expressions can not be broken down into the set of meanings of the individual words in the expressions .Multi - word expressions cover idiomatic expressions ( in de steek laten , aan de hand zijn ) , sayings and proverbs ( Boontje komt om zijn loontje ) and strong collocations ( derde wereld , klavertje vier ) .", "label": "", "metadata": {}, "score": "60.84039"}
{"text": "Description : This system takes a supervised learning approach to word sense disambiguation , where the results of the systems duluth1 , duluth2 , duluth3 , duluth4 , duluth5 , duluthA , and duluthB are combined into an ensemble .Each of those systems outputs probabilities for each sense when presented with a test example , so all of these are summed together and the sense with the maximum probability is assigned to a test example .", "label": "", "metadata": {}, "score": "60.86384"}
{"text": "The learning algorithm is support vector machine .keywords : support vector machine , morphological analysis , syntactic analysis .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "61.02466"}
{"text": "After the source file is read , the method continues with the step 302 of performing substitution of text from the file according to user preferences , which may be stored in a properties file .The user preferences specify regular expressions which are applied to the text in order to substitute invalid or unwanted characters .", "label": "", "metadata": {}, "score": "61.09691"}
{"text": "Alternatively , submissions can take the form of actual target word translations , or translations of phrases or sentences including each target word .In this case , translation experts are asked to judge whether the supplied translation is appropriate or not .", "label": "", "metadata": {}, "score": "61.099873"}
{"text": "claim 16 , wherein the computer - readable file further comprises a header , the header comprising format , version and priority information .The compact linguistic data structure of .claim 16 , further comprising an index table , the index table containing indexes into the computer - readable file for locating the tables in the computer - readable file .", "label": "", "metadata": {}, "score": "61.11198"}
{"text": "Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : Given an input sentence , our system returns an entry number in TM or a translation of head word ( or phrase ) .", "label": "", "metadata": {}, "score": "61.133934"}
{"text": "Words in simple local word definition tables are encoded by replacing characters in the words with the local character indexes .A complex word definition table also contains a hotword table 700 and an exception table 702 .Hotwords are the words associated with the highest frequencies in the group contained in the complex word definition table .", "label": "", "metadata": {}, "score": "61.17917"}
{"text": "This information is stored in a substitution table .The substitution table is indexed , so that each n - gram is mapped to a substitution index .The words can then be compacted by replacing each n - gram with its substitution index in the substitution table each time the n - gram appears in a word .", "label": "", "metadata": {}, "score": "61.240955"}
{"text": "This information is stored in a substitution table .The substitution table is indexed , so that each n - gram is mapped to a substitution index .The words can then be compacted by replacing each n - gram with its substitution index in the substitution table each time the n - gram appears in a word .", "label": "", "metadata": {}, "score": "61.240955"}
{"text": "Y ( SemCor ) .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : The semantic disambiguation of a word is performed based on its relations with the preceding and succeeding words .", "label": "", "metadata": {}, "score": "61.250473"}
{"text": "Several words on the left and on the right of the head word in a given sentence , and their POS assigned by the morphological analyzer JUMAN .N - grams including the head word in a given sentence .English head word in the cluster where the longest string in a given sentence is found , and the length .", "label": "", "metadata": {}, "score": "61.25586"}
{"text": "I think NLTK 's ideal role is be a standard interface between corpora and NLP algorithms .There are many different corpus formats , and every algorithm has its own data structure requirements , so providing common abstract interfaces to connect these together is very powerful .", "label": "", "metadata": {}, "score": "61.28273"}
{"text": "But how well do they work ?Below is a table showing both the accuracy & F - measure of many of these algorithms using different feature extraction methods .Unlike the standard NLTK classifiers , sklearn classifiers are designed for handling numeric features .", "label": "", "metadata": {}, "score": "61.296276"}
{"text": "[0002 ] .Field of the Invention .[ 0003 ] .The present invention relates in general to linguistic data , and in particular to storage and use of the linguistic data for text processing and text input .[ 0004 ] .", "label": "", "metadata": {}, "score": "61.302242"}
{"text": "( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : .Our approach for the all - words disambiguation task is based on statistical models .", "label": "", "metadata": {}, "score": "61.30616"}
{"text": "Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : The system selects the appropriate translation record based on the character - bigram - based similarity , as follows .", "label": "", "metadata": {}, "score": "61.38426"}
{"text": "The method continues with a step of creating word definition tables and storing the encoded words in the word definition tables .The method continues with a step of creating an offset table for locating groups of encoded words .The method ends with a step of storing the character - mapping table , the substitution table , the word definition tables , and the offset table .", "label": "", "metadata": {}, "score": "61.437164"}
{"text": "No .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ? :Yes .Description : The system uses \" semantic domains \" ( e.g. Medicine , Sport , Architecture ) associated to wordnet synsets .", "label": "", "metadata": {}, "score": "61.504547"}
{"text": "101 , No . 61 , 9 pages ( in Japanese language ) .A machine translation system may use non - parallel monolingual corpora to generate a translation lexicon .The system may identify identically spelled words in the two corpora , and use them as a seed lexicon .", "label": "", "metadata": {}, "score": "61.557755"}
{"text": "For each position , the tokens are compared in frequency and the frequency count is replaced by the frequency rank , e.g. , the most frequent token count is replaced with 1 and the least frequent by n. R . a .", "label": "", "metadata": {}, "score": "61.6572"}
{"text": "i .b .i . )n .n .The result is a matrix with similarity scores between all German words , and a second matrix with similarity scores between all English words .For a new word , the lexicon builder 125 may look up its similarity scores to seed words , thus creating a similarity vector .", "label": "", "metadata": {}, "score": "61.687653"}
{"text": "The non - transitory computer readable medium of . claim 15 , wherein said identifying comprises identifying word pairs having a minimum longest common subsequence ratio .The non - transitory computer readable medium of . claim 15 , wherein said one or more clues includes similar context .", "label": "", "metadata": {}, "score": "61.711998"}
{"text": "Rather than learned a bagged decision tree ( as duluth5 does ) this system simply learns a decision stump , a one node decision tree .The features used are the same as duluth5 .This system provides a baseline that can be used to compare the benefits of learning an entire decision tree ( duluth5 ) versus identifying a single node tree ( duluthB ) .", "label": "", "metadata": {}, "score": "61.918106"}
{"text": "For each pair of inputs , calculate the string similarity by way of Dice 's coefficient over character bigrams 3 .For each input and translation record combination , calculate the maximum \" linked similarity \" via each other input , as the product of the input - input and input - translation record similarities 4 .", "label": "", "metadata": {}, "score": "61.984364"}
{"text": "6 is a Generalized Suffix Tree ( GST ) .FIG .7 is a Bilingual Suffix Tree ( BST ) .FIG .8 is a portion of a BST showing example alignments .FIG .9 are portions of a BST describing left and right alignments .", "label": "", "metadata": {}, "score": "61.989395"}
{"text": "For German to English , this includes replacing the letters k and z by c and changing the ending -t\u00e4t to -ty .Both these rules can be observed in the word pair Elektrizitat and electricity .The lexicon builder 125 may utilize these rules to expand the seed lexicon .", "label": "", "metadata": {}, "score": "62.11734"}
{"text": "This time , instead of measuring accuracy , we 'll collect reference values and observed values for each label ( pos or neg ) , then use those sets to calculate the precision , recall , and F - measure of the naive bayes classifier .", "label": "", "metadata": {}, "score": "62.188934"}
{"text": "( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : This probabilistic WSD system uses synonym permutations to form n - grams , then queries AltaVista for word counts as the basis for establishing the probabilities .", "label": "", "metadata": {}, "score": "62.200317"}
{"text": "Below , I 'll show you how to use it to ( mostly ) replicate the results shown in my previous articles on text classification .You should checkout or download nltk - trainer if you want to run the examples yourself .", "label": "", "metadata": {}, "score": "62.275"}
{"text": "The order of the --affix arguments is the order in which each AffixTagger will be trained and inserted into the backoff chain .The default training options are a maximum of 200 rules with a minimum score of 2 , but you can change that with the --max_rules and --min_score arguments .", "label": "", "metadata": {}, "score": "62.275368"}
{"text": "Yes , Redis is still fairly alpha , so I would n't use it for critical systems .But I 've had very few issues so far , especially compared to dealing with BerkeleyDB .I highly recommend it for your non - critical computational needs Redis has been quite stable for a while now , and many sites are using it successfully in production Tag Archives : classification .", "label": "", "metadata": {}, "score": "62.323853"}
{"text": "As shown in text classification with stopwords and collocations , filtering stopwords reduces accuracy .A helpful comment by Pierre explained that adverbs and determiners that start with \" wh \" can be valuable features , and removing them is what causes the dip in accuracy .", "label": "", "metadata": {}, "score": "62.337437"}
{"text": "As shown in text classification with stopwords and collocations , filtering stopwords reduces accuracy .A helpful comment by Pierre explained that adverbs and determiners that start with \" wh \" can be valuable features , and removing them is what causes the dip in accuracy .", "label": "", "metadata": {}, "score": "62.337437"}
{"text": "As shown in text classification with stopwords and collocations , filtering stopwords reduces accuracy .A helpful comment by Pierre explained that adverbs and determiners that start with \" wh \" can be valuable features , and removing them is what causes the dip in accuracy .", "label": "", "metadata": {}, "score": "62.337437"}
{"text": "We used a hybrid model of two kinds of support vector machines and two kinds of simple Bayes for learning .Did you use any training data provided in an automatic training procedure ?YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "62.402187"}
{"text": "So , no test example is tagged with one of these labels .keywords : hierarchical LazyBoosting , semantic domain attributes , multiword preprocessing , AdaBoost .MH .Did you use any training data provided in an automatic training procedure ?", "label": "", "metadata": {}, "score": "62.450043"}
{"text": "There 's two options that allow you to restrict which words are used by their information gain : . --max_feats 10000 will use the 10,000 most informative words , and discard the rest .--min_score 3 will use all words whose score is at least 3 , and discard any words with a lower score .", "label": "", "metadata": {}, "score": "62.476257"}
{"text": "There 's two options that allow you to restrict which words are used by their information gain : . --max_feats 10000 will use the 10,000 most informative words , and discard the rest .--min_score 3 will use all words whose score is at least 3 , and discard any words with a lower score .", "label": "", "metadata": {}, "score": "62.476257"}
{"text": "There 's two options that allow you to restrict which words are used by their information gain : . --max_feats 10000 will use the 10,000 most informative words , and discard the rest .--min_score 3 will use all words whose score is at least 3 , and discard any words with a lower score .", "label": "", "metadata": {}, "score": "62.476257"}
{"text": "organisation : Computer Science Department and Center for Language and Speech Processing , Johns Hopkins University .Task / s : Spanish lexical choice , Swedish lexical choice , Basque lexical choice .Did you use any training data provided in an automatic training procedure ?", "label": "", "metadata": {}, "score": "62.5122"}
{"text": "The format defines the structure of a computer file which contains a header 602 followed by a number of tables .The header 602 contains a signature including a magic number , which is a number identifying the format of the file .", "label": "", "metadata": {}, "score": "62.52376"}
{"text": "YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : This system takes a supervised learning approach to word sense disambiguation , where a decision tree is induced from sense - tagged training examples and then used to assign senses to the test examples .", "label": "", "metadata": {}, "score": "62.54119"}
{"text": "The patterns are validated on the training data , and we keep only those which are 100 % accurate .The patterns are then applied on the test data .Only a few instances can be disambiguated this way , but with high confidence : previous experiments have shown that high accuracy is obtained with this procedure .", "label": "", "metadata": {}, "score": "62.591393"}
{"text": "The TM contains , for each Japanese head word , a list of typical Japanese expressions ( phrases / sentences ) involving the head word and an English translation for each .Each pair is treated as a distinct sense and has a unique \" sense ID \" .", "label": "", "metadata": {}, "score": "62.59845"}
{"text": "Wide co - occurrences include all of the words in each instance whereas local collocation uses a window consisting of the 3 tokens immediately before and after the word to be tagged ( i.e. features include left_wd3 , left_wd2 , left_wd1 , right_wd1 , right_wd2 , right_wd3 ) .", "label": "", "metadata": {}, "score": "62.663277"}
{"text": "Also , the methods illustrated in .FIGS .3 , 4 , 5 , 8 and 9 may contain fewer , more or different steps than those that are shown .For example , although the methods describe using computer files to store final and intermediate results of the methods , the results could also be stored in computer memory such as RAM or Flash memory modules .", "label": "", "metadata": {}, "score": "62.724037"}
{"text": "And the ability to create sliceable lists allows you to make sorted indexes for paging thru your samples .Here 's some more concrete use cases for persistent frequency distributions : .RedisFreqDist .I put the code I 've been using to build frequency distributions over large sets of words up at BitBucket . probablity.py contains RedisFreqDist , which works just like the NTLK FreqDist , except it stores samples and frequencies as keys and values in Redis .", "label": "", "metadata": {}, "score": "62.86404"}
{"text": "For accuracy evaluation , we can use nltk.classify.util.accuracy with the test set as the gold standard .Training and Testing the Naive Bayes Classifier .And the output is : .As you can see , the 10 most informative features are , for the most part , highly descriptive adjectives .", "label": "", "metadata": {}, "score": "63.025005"}
{"text": "0044 ] .Step 410 determines whether there are any remaining filtered - words files to process .If there are , then the method continues at step 400 .Otherwise , the method continues at step 412 .[ 0045 ] .", "label": "", "metadata": {}, "score": "63.04908"}
{"text": "FIG .8 shows some alignments we can extract from the BST in .FIG .7 , a portion of which is shown in .FIG .8 .As can be seen in .For alignment extraction , we are interested in edges of the third type , because they mark ends of alignments .", "label": "", "metadata": {}, "score": "63.098274"}
{"text": "One of the best metrics for information gain is chi square .NLTK includes this in the BigramAssocMeasures class in the metrics package .To use it , first we need to calculate a few frequencies for each word : its overall frequency and its frequency within each class .", "label": "", "metadata": {}, "score": "63.166832"}
{"text": "One of the best metrics for information gain is chi square .NLTK includes this in the BigramAssocMeasures class in the metrics package .To use it , first we need to calculate a few frequencies for each word : its overall frequency and its frequency within each class .", "label": "", "metadata": {}, "score": "63.166832"}
{"text": "One of the best metrics for information gain is chi square .NLTK includes this in the BigramAssocMeasures class in the metrics package .To use it , first we need to calculate a few frequencies for each word : its overall frequency and its frequency within each class .", "label": "", "metadata": {}, "score": "63.166832"}
{"text": "One of the best metrics for information gain is chi square .NLTK includes this in the BigramAssocMeasures class in the metrics package .To use it , first we need to calculate a few frequencies for each word : its overall frequency and its frequency within each class .", "label": "", "metadata": {}, "score": "63.166832"}
{"text": "Score the match quality of each example .IIT2 reduces the effect of mismatches distant from the target word over IIT1 .IIT3 ( All Word only ) restricts senses of context words to the \" best \" sense for words to the left of the target word before beginning the example match .", "label": "", "metadata": {}, "score": "63.191338"}
{"text": "The frequencies are normalized by applying a normalization function which converts the frequencies so that their values are within a predetermined range .Only the maximum frequency of words in the group is stored with full precision in the table .All other frequencies are stored as percentages of the maximum frequency .", "label": "", "metadata": {}, "score": "63.21367"}
{"text": "If that pattern of distance is relatively predictable then we have evidence for a collocation of variable phrases like cement ... relations .Hypothesis Testing .Hypothesis testing is used to check whether two words co - occur more often than a chance .", "label": "", "metadata": {}, "score": "63.217148"}
{"text": "Did you use any training data provided in an automatic training procedure ?Y .Description : .The TALP system can be defined as Hierarchical LazyBoosting .It works as Yarowsky 's hierarchical decision lists , but using LazyBoosting instead of Decision Lists .", "label": "", "metadata": {}, "score": "63.21746"}
{"text": "0008 ] .Input prediction is also useful when text is entered using a reduced keyboard .A reduced keyboard has fewer keys than characters that can be entered , thus keystroke combinations are ambiguous .A system that uses linguistic data for input prediction allows the user to easily resolve such ambiguities .", "label": "", "metadata": {}, "score": "63.255238"}
{"text": "Each of the clues provides a matching score between two words ( block 220 ) , e.g. , a German word and an English word .The likelihood of these two words being actual translations of each other may correlate to these scores .", "label": "", "metadata": {}, "score": "63.294735"}
{"text": "7 is a block diagram of a complex word definition table .The complex word definition table is recursive , in that it contains local word definition tables 708 , each of which is a simple or complex word definition table as described above .", "label": "", "metadata": {}, "score": "63.341976"}
{"text": "The local word definition tables 708 define words that are grouped by higher order n - gram prefixes .The common prefixes of words in the local word definition tables 708 are removed .[ 0077 ] .[ 0078 ] .", "label": "", "metadata": {}, "score": "63.349274"}
{"text": "System name : usm_english_tagger , usm_english_tagger2 , usm_english_tagger3 .Your contact details .Did you use any training data provided in an automatic training procedure ?No .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "63.45571"}
{"text": "60/393,903 , filed on Jul. 3 , 2002 .BACKGROUND .Field of the Invention .The present invention relates in general to linguistic data , and in particular to storage and use of the linguistic data for text processing and text input .", "label": "", "metadata": {}, "score": "63.4982"}
{"text": "The header 602 also indicates whether the file includes frequency information .[ 0063 ] .The header 602 is followed by the index table 604 .The index table 604 contains indexes in the file to the remaining tables which are defined below , and also allows for additional tables to be added .", "label": "", "metadata": {}, "score": "63.692448"}
{"text": "Description : Word senses are defined according to the Iwanami Kokugo Jiten , a Japanese dictionary published by Iwanami Shoten .The Iwanami Kokugo Jiten is distributed to all participants .For each sense in the dictionary , a corresponding sense ID and morphological information ( word segmentation , POS tag , base form and reading , all manually post - edited ) will be supplied .", "label": "", "metadata": {}, "score": "63.90967"}
{"text": "The Hansard Corpus includes parallel texts in English and Canadian French , drawn from official records of the proceedings of the Canadian Parliament .A small bilingual lexicon of 6,900 entries was built using 5,000 sentences pairs ( 150,000 words for each language ) .", "label": "", "metadata": {}, "score": "64.04532"}
{"text": "A local context consists of the following features for all words within a window .Morpheme_Position : a morpheme and its position .Morpheme_POS : a morpheme and its part - of - speech .POS_Position : the part - of - speech and position of a morpheme Eojeol_Position : an \" eojeol \" and its position .", "label": "", "metadata": {}, "score": "64.0563"}
{"text": "claim 3 , wherein said identifying substantially identical words comprises .applying transformation rules to words in the first corpora to form transformed words ; and . comparing said transformed words to words in the second corpora .The method of .", "label": "", "metadata": {}, "score": "64.07952"}
{"text": "Compounds form a bridge between collocations and idioms as they are quite invariable but need not be semantically opaque .Flexible Word Pairs : Flexible word pairs include collocations between subject and verb , or verb and object .Any number of intervening words may occur between the words of the collocation .", "label": "", "metadata": {}, "score": "64.190674"}
{"text": "When presented with a test example , each classifier outputs a probability for each possible sense .These are summed and the sense with the maximum probability is assigned to a test example .This is the same approach as taken in duluthA for English .", "label": "", "metadata": {}, "score": "64.44627"}
{"text": "For all lemmas exhibiting only one sense in the training data , this sense was returned .Likewise , if there was insufficient data for word - specific training ( the sum of the minority sense examples for the word in training data was below a threshold ) the majority sense in training was returned for all instances of that lemma .", "label": "", "metadata": {}, "score": "64.448105"}
{"text": "Consider the following 2 cases where local context dominates : . the preferred stock raised .he expressed concern about .The words the and he dictate that preferred and expressed are adjective and verb respectively .This kind of inference , due to its local nature , is captured and propagated by the preprocessor .", "label": "", "metadata": {}, "score": "64.47969"}
{"text": "( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : This system takes a supervised learning approach to word sense disambiguation , where a decision stump is induced from sense - tagged training examples .", "label": "", "metadata": {}, "score": "64.51742"}
{"text": "These are summed and the sense with the maximum probability is assigned to a test example .keywords : supervised learning , ensemble .URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .", "label": "", "metadata": {}, "score": "64.75581"}
{"text": "Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : Because of the importance of morphological analysis in a highly inflected language such as Czech , a part - of - speech tagger and lemmatizer kindly provided by Jan Hajic of Charles University was first applied to the data .", "label": "", "metadata": {}, "score": "64.93306"}
{"text": "A method for creating the linguistic data 204 is described in .FIG .5 , The linguistic data 204 produced by the linguistic data analyzer 202 illustrated in .FIG .6 .The absolute frequency of a certain group of words found in the corpus 200 may alternatively be modified by separating this group to a different file and assigning a custom weight to this file .", "label": "", "metadata": {}, "score": "65.0072"}
{"text": "When presented with a test example , each decision tree outputs probabilities for each possible sense .These probabilities are summed and the sense with the maximum value is assigned to the test example .No information from WordNet is utilized by this system .", "label": "", "metadata": {}, "score": "65.11725"}
{"text": "When presented with a test example , each decision tree outputs probabilities for each possible sense .These probabilities are summed and the sense with the maximum value is assigned to the test example .No information from WordNet is utilized by this system .", "label": "", "metadata": {}, "score": "65.11725"}
{"text": "When your classification model has hundreds or thousands of features , as is the case for text categorization , it 's a good bet that many ( if not most ) of the features are low information .These are features that are common across all classes , and therefore contribute little information to the classification process .", "label": "", "metadata": {}, "score": "65.25541"}
{"text": "When your classification model has hundreds or thousands of features , as is the case for text categorization , it 's a good bet that many ( if not most ) of the features are low information .These are features that are common across all classes , and therefore contribute little information to the classification process .", "label": "", "metadata": {}, "score": "65.25541"}
{"text": "The disadvantage of this is that we are often forced to make difficult , if not impossible , decisions in distingishing between senses .Also as we do not use the training data , we have no knowledge of the relative frequencies of the different senses .", "label": "", "metadata": {}, "score": "65.33697"}
{"text": "The accuracy is a bit lower than shown in the article on eliminating low information features , most likely due to the slightly different training & testing instances .Using --min_score 3 instead increases accuracy a little bit : .Bigram Features .", "label": "", "metadata": {}, "score": "65.35011"}
{"text": "The accuracy is a bit lower than shown in the article on eliminating low information features , most likely due to the slightly different training & testing instances .Using --min_score 3 instead increases accuracy a little bit : .Bigram Features .", "label": "", "metadata": {}, "score": "65.35011"}
{"text": "The accuracy is a bit lower than shown in the article on eliminating low information features , most likely due to the slightly different training & testing instances .Using --min_score 3 instead increases accuracy a little bit : .Bigram Features .", "label": "", "metadata": {}, "score": "65.35011"}
{"text": "If this is the case , then these metrics should improve if we also train on multiple words , a topic I 'll explore in a future article .Another possibility is the abundance of naturally neutral words , the kind of words that are devoid of sentiment .", "label": "", "metadata": {}, "score": "65.362564"}
{"text": "organisation : Computer Science Department and Center for Language and Speech Processing , Johns Hopkins University .Task / s : English Lexical choice .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "65.44357"}
{"text": "If it is determined at step 806 that the word with maximum frequency was obtained from the word definition tables , then the method continues at step 808 , and the user - selected word 800 is assigned a frequency equal to the maximum frequency plus one .", "label": "", "metadata": {}, "score": "65.46582"}
{"text": "[ 0064 ] .The index table 604 is followed by the name table 606 .The name table 606 contains a name which identifies the word - list .[0065 ] .The name table 606 is followed by the character - mapping table 608 .", "label": "", "metadata": {}, "score": "65.48437"}
{"text": "The decision tree learner is \" bagged \" .The training examples are sampled ten times ( with replacement ) and a decision tree is learned for each sample .Each test example is assigned a sense based on a vote taken from among the learned trees .", "label": "", "metadata": {}, "score": "65.56862"}
{"text": "The main idea of boosting algorithms is to combine many simple and moderately accurate hypotheses ( weak classifiers ) into a single , highly accurate classifier .More specifically , LazyBoosting is a simple modification of AdaBoost .MH algorithm , wich consists in reducing the feature space that is explored whenever a weak classifier is learnt .", "label": "", "metadata": {}, "score": "65.6087"}
{"text": "The method then continues with the step 304 of obtaining a filter corresponding to the type indicated by the file extension of the source file .For example , if the file extension is \" .xml \" , it is assumed that the file contains an extensible Markup Language ( XML ) document , so an XML filter is obtained .", "label": "", "metadata": {}, "score": "65.77427"}
{"text": "This is different than finding significant collocations , as all bigrams are considered using the nltk.util.bigrams function .Combining --bigrams with --min_score 3 gives us the highest accuracy yet , 97 % !Of course , the \" Bourne bias \" is still present with the ( ' matt ' , ' damon ' ) bigram , but you ca n't argue with the numbers .", "label": "", "metadata": {}, "score": "65.95425"}
{"text": "This is different than finding significant collocations , as all bigrams are considered using the nltk.util.bigrams function .Combining --bigrams with --min_score 3 gives us the highest accuracy yet , 97 % !Of course , the \" Bourne bias \" is still present with the ( ' matt ' , ' damon ' ) bigram , but you ca n't argue with the numbers .", "label": "", "metadata": {}, "score": "65.95425"}
{"text": "This is different than finding significant collocations , as all bigrams are considered using the nltk.util.bigrams function .Combining --bigrams with --min_score 3 gives us the highest accuracy yet , 97 % !Of course , the \" Bourne bias \" is still present with the ( ' matt ' , ' damon ' ) bigram , but you ca n't argue with the numbers .", "label": "", "metadata": {}, "score": "65.95425"}
{"text": "YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : This system takes a supervised learning approach to word sense disambiguation , where a Naive Bayesian classifier is learned from sense - tagged training examples .", "label": "", "metadata": {}, "score": "65.95462"}
{"text": "So , consider how well each example matches the target context .Align the context target word to the synset word in the example .For each example word ( working out from the synset word ) find the closet word in the target context that is related under WordNet relations .", "label": "", "metadata": {}, "score": "65.96684"}
{"text": "0007 ] .The prediction of user input is especially useful when included in a mobile device , since such devices typically have input devices , including keyboards , that are constrained in size .Input prediction minimizes the number of keystrokes required to enter words on such devices .", "label": "", "metadata": {}, "score": "66.076935"}
{"text": "In another embodiment of the present invention , a mutual information score is used to control which word pairs occurring in the training text are to be stored in the database .BRIEF DESCRIPTION OF THE DRAWING .The sole FIGURE is a schematic diagram which shows the elements of the present invention .", "label": "", "metadata": {}, "score": "66.11093"}
{"text": "URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .email : tpederse@d.umn.edu organisation : University of Minnesota Duluth .Task : Spanish Lexical Sample .", "label": "", "metadata": {}, "score": "66.15213"}
{"text": "N .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Yes .Description : Sussex - sel was applied to the all - words task , but processed only the plain text , ignoring the supplied bracketings .", "label": "", "metadata": {}, "score": "66.16806"}
{"text": "claim 1 , further comprising : . a user interface , comprising : . a text input device ; and .a text output device ; and .a text input logic unit , . wherein the text input logic unit receives a text prefix from the text input device , retrieves a plurality of predicted words from the compact linguistic data that start with the prefix , and displays the predicted words using the text output device .", "label": "", "metadata": {}, "score": "66.22023"}
{"text": "The alphabet consists of each unique character used in words in the word - list .[ 0066 ] .The character - mapping table 608 is followed by the substitution table 610 .The substitution table 610 contains a bi - gram substitution table , followed by a table for each group of higher - order n - grams which are defined , such as tri - grams , four - grams , and so on .", "label": "", "metadata": {}, "score": "66.30555"}
{"text": "URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .email : tpederse@d.umn.edu organisation : University of Minnesota Duluth .Task : English Lexical Sample .", "label": "", "metadata": {}, "score": "66.50995"}
{"text": "Two strings ( i.e. , sequences of words ) match if the corresponding words are translations of each other according to a bilingual lexicon .In order to perform the matching operation , all paths that correspond to an exhaustive traversal of one of the trees ( the source tree ) are traversed in the other ( the target tree ) , until a mismatch occurs .", "label": "", "metadata": {}, "score": "66.51057"}
{"text": "Semantic Collocations : They are lexically restricted word pairs , for which only a subset of the synonym of the collocator can be used in the same lexical context .Collocations are also categorized into compounds and flexible word pairs .Compounds : Compounds include word pairs that occur consecutively in language and typically are immutable in function .", "label": "", "metadata": {}, "score": "66.53586"}
{"text": "The substitution table 610 is followed by the offset table 612 .This table is used to locate a word definition table , described below , based on the common prefix of words in the word definition table to be located .", "label": "", "metadata": {}, "score": "66.60168"}
{"text": "YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : This system takes a supervised learning approach to word sense disambiguation , where a decision stump is induced from sense - tagged training examples .", "label": "", "metadata": {}, "score": "66.676025"}
{"text": "Parse each input and translation record , and generate a \" case frame \" for each 2 .Return the translation record for which the most case slot matches are produced , breaking ties according to the overall quality of match .", "label": "", "metadata": {}, "score": "67.12746"}
{"text": "Church and Hanks made use of mutual information ( MI ) to evaluate the correlation between a pair of words .They take a window size of five words for co - occurrence and conduct experiments based on a corpus .They were able to extract interesting pairs of related words such as doctors and nurses doctors and treating .", "label": "", "metadata": {}, "score": "67.17323"}
{"text": "A complex table is used to define words which are grouped by prefixes of greater lengths .Words in the definition tables 614 are encoded using the character - mapping table 608 and the substitution table 610 .The characters in the words are replaced with the corresponding character indexes from the character - mapping table 608 , and the n - grams that are in the substitution table 610 are replaced in the words with their corresponding substitution indexes in the substitution table 610 .", "label": "", "metadata": {}, "score": "67.200096"}
{"text": "Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : We used SVM , PCA and ICA for learning .", "label": "", "metadata": {}, "score": "67.23831"}
{"text": "( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : This supervised system is a variation of ehu - dlist - all .", "label": "", "metadata": {}, "score": "67.24784"}
{"text": "Due to cultural exchange , a large number of words that originate in one language may be adopted by others .Recently , this phenomenon can be seen with words such as \" Internet U or \" Aids u. \" These terms may be adopted verbatim or changed by well - established rules .", "label": "", "metadata": {}, "score": "67.461334"}
{"text": "Notice that fixed collocations are easily distinguishable from thematic relations .The smallest VF of a fixed collocation has a VF of 0.86 ( finance specialist ) ; the largest VF of a thematic relation is 0.56 ( produce concrete ) .", "label": "", "metadata": {}, "score": "67.623764"}
{"text": "It shows that collocations play a key role in understanding sentences .Collocations are recursive so collocational phrase may contain more than two words .Types of Collocations .Grammatical Collocations : Contain prepositions , including paired syntactic categories , such as verb+preposition ( e.g come to , put on ) , adjective+preposition ( e.g. afraid that , fond of ) , and noun+preposition ( e.g by accident , witness to ) .", "label": "", "metadata": {}, "score": "67.95561"}
{"text": "BACKGROUND OF THE INVENTION .Sentences in a typical newspaper story include idioms , ellipses , and ungrammatical constructs .Since authentic language defies text - book grammar , the basic parsing paradigm must be tuned to the nature of the text under analysis .", "label": "", "metadata": {}, "score": "67.99196"}
{"text": "Since the offset table 612 uniquely maps each bi - gram prefix in the alphabet to a location in the file that defines words that start with that prefix , the prefixes do not need to be retained , and thus are removed from the word definitions .", "label": "", "metadata": {}, "score": "68.07086"}
{"text": "It is usually composed of the word 's lemma and a sense circumscription of one or two words , often using a related term ( drogen_nat , \" dry_wet \" ) or a reference of the grammatical category ( fiets_N , fietsen_V ) .", "label": "", "metadata": {}, "score": "68.101425"}
{"text": "Here 's the full code I used to get these results , with an explanation below .inc(word.lower ( ) ) label_word_fd['pos'].inc(word.lower ( ) ) for word in movie_reviews .inc(word.lower ( ) ) label_word_fd['neg'].", "label": "", "metadata": {}, "score": "68.138794"}
{"text": "Here 's the full code I used to get these results , with an explanation below .inc(word.lower ( ) ) label_word_fd['pos'].inc(word.lower ( ) ) for word in movie_reviews .inc(word.lower ( ) ) label_word_fd['neg'].", "label": "", "metadata": {}, "score": "68.138794"}
{"text": "Here 's the full code I used to get these results , with an explanation below .inc(word.lower ( ) ) label_word_fd['pos'].inc(word.lower ( ) ) for word in movie_reviews .inc(word.lower ( ) ) label_word_fd['neg'].", "label": "", "metadata": {}, "score": "68.138794"}
{"text": "Here 's the full code I used to get these results , with an explanation below .inc(word.lower ( ) ) label_word_fd['pos'].inc(word.lower ( ) ) for word in movie_reviews .inc(word.lower ( ) ) label_word_fd['neg'].", "label": "", "metadata": {}, "score": "68.138794"}
{"text": "For the assistant , the length of the interaction with the system ranged from 3 to 25 mins , with an average under 15 mins .Due to time constraints we only managed to submit results for nouns within the deadline .", "label": "", "metadata": {}, "score": "68.248535"}
{"text": "Signficant Bigrams .This shows that bigrams do n't matter much when using only high information words .In this case , the best way to evaluate the difference between including bigrams or not is to look at precision and recall .", "label": "", "metadata": {}, "score": "68.57934"}
{"text": "Signficant Bigrams .This shows that bigrams do n't matter much when using only high information words .In this case , the best way to evaluate the difference between including bigrams or not is to look at precision and recall .", "label": "", "metadata": {}, "score": "68.57934"}
{"text": "Signficant Bigrams .This shows that bigrams do n't matter much when using only high information words .In this case , the best way to evaluate the difference between including bigrams or not is to look at precision and recall .", "label": "", "metadata": {}, "score": "68.57934"}
{"text": "Signficant Bigrams .This shows that bigrams do n't matter much when using only high information words .In this case , the best way to evaluate the difference between including bigrams or not is to look at precision and recall .", "label": "", "metadata": {}, "score": "68.57934"}
{"text": "No language resources were used .keywords : character bigram , Dice 's coefficient , input - input similarity .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "68.60659"}
{"text": "Feature .A local context consists of the following features for all words within a window .Word_Position : a word and its position Word_POS : a word and its part - of - speech .POS_position : the part - of - speech and position of a word .", "label": "", "metadata": {}, "score": "68.872"}
{"text": "( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Yes .Description : Sussex - sel - ospd - ana parses the plain text to identify subject / verb and verb / direct object relationships , and the nouns and verbs involved are disambiguated using class - based selectional preferences acquired from unsupervised training .", "label": "", "metadata": {}, "score": "68.87735"}
{"text": "Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description ( 250 words max ) : We used a machine learning technique for constructing the WSD system .", "label": "", "metadata": {}, "score": "68.93956"}
{"text": "[ 0080 ] .A complex word definition table also contains a hotword table 700 and an exception table 702 .Hotwords are the words associated with the highest frequencies in the group contained in the complex word definition table .The hotword table 700 contains indexes of hotwords that are located in local word definition tables 708 that are simple word definition tables .", "label": "", "metadata": {}, "score": "68.94279"}
{"text": "Description : The WASPS - Workbench is a browser - based tool which integrates lexicography and automatic WSD for the benefit of both parties .The user enters a word to be analyzed and the Workbench calculates a \" Word - Sketch \" : a page of statistically significant collocation patterns for that word ( currently based on the BNC ) .", "label": "", "metadata": {}, "score": "68.970604"}
{"text": "xml \" , it is assumed that the file contains an eXtensible Markup Language ( XML ) document , so an XML filter is obtained Similarly , if the file extension is \" .html \" , then a HyperText Markup Language ( HTML ) filter is obtained , and if the file extension is \" . txt \" , then a text filter is obtained .", "label": "", "metadata": {}, "score": "69.03648"}
{"text": "A reduced keyboard has fewer keys than characters that can be entered , thus keystroke combinations are ambiguous .A system that uses linguistic data for input prediction allows the user to easily resolve such ambiguities .Linguistic data can also be used to disambiguate individual keystrokes that are entered using a reduced keyboard .", "label": "", "metadata": {}, "score": "69.075806"}
{"text": "( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : We used a machine learning technique for constructing the WSD system .", "label": "", "metadata": {}, "score": "69.24083"}
{"text": "The movie_reviews corpus can then be found under the corpora subdirectory .Training a Naive Bayes Classifier .python train_classifier.py --algorithm NaiveBayes --instances files --fraction 0.75 --show - most - informative 10 --no - pickle movie_reviews .Here 's an explanation of each option : . --instances files : this says that each file is treated as an individual instance , so that each feature set will contain word : True for each word in a file .", "label": "", "metadata": {}, "score": "69.30305"}
{"text": "0062 ] .The header 602 contains a signature including a magic number , which is a number identifying the format of the file .The header 602 also contains information which specifies the version and priority of the linguistic data contained in the file .", "label": "", "metadata": {}, "score": "69.3155"}
{"text": "A tagger trained with any of these phonetic features will be an instance of nltk_trainer . tagging.taggers.PhoneticClassifierBasedPOSTagger , which means nltk_trainer must be included in your PYTHONPATH in order to load & use the tagger .The simplest way to do this is to install nltk - trainer using python setup.py install .", "label": "", "metadata": {}, "score": "69.38974"}
{"text": "It may be noun phrase like large house , verbal phrase like pick up , idioms , cliches or technical terms .Collocations are characterized by limited compositionality , that is , it is difficult to predict the meaning of collocation from the meaning of its parts .", "label": "", "metadata": {}, "score": "69.46059"}
{"text": "keywords : Semantic Classification Tree , Semantic Classes , Cosine Similarity Measure .name : Marc El - Beze email : marc.elbeze@lia.univ-avignon.fr organisation : Laboratoire Informatique d'Avignon .Task / s ( e.g. English all words ) : English all words .", "label": "", "metadata": {}, "score": "69.52841"}
{"text": "; \" and .\" It is for this reason that the party has proposed . . . \" .Since \" Ce est pour cette \" can be aligned with \" It is for this \" and \" que le \" with \" that the , \" it is a reasonable assumption that \" raison \" can be translated by \" reason .", "label": "", "metadata": {}, "score": "69.684906"}
{"text": "N N .Chief .Executive .A N .next . year .A N . real . estate .A N .Where A stands for adjective , N for noun and P for propositon .Note that the bigrams , last week and last year can not be regarded as non - compositional phrases .", "label": "", "metadata": {}, "score": "70.273056"}
{"text": "NLTK - Trainer ( available github and bitbucket ) was created to make it as easy as possible to train NLTK text classifiers .The train_classifiers.py script provides a command - line interface for training & evaluating classifiers , with a number of options for customizing text feature extraction and classifier training ( run python train_classifier.py --help for a complete list of options ) .", "label": "", "metadata": {}, "score": "70.297195"}
{"text": "An example of a national corpus is the English language .SUMMARY .[ 0011 ] .A system of creating compact linguistic data is provided .The system comprises a corpus and linguistic data analyzer .The linguistic data analyzer calculates frequencies of words appearing in the corpus .", "label": "", "metadata": {}, "score": "70.471855"}
{"text": "Nearly every file that is pos is correctly identified as such , with 98 % recall .This means very few false negatives in the pos class .But , a file given a pos classification is only 65 % likely to be correct .", "label": "", "metadata": {}, "score": "70.56293"}
{"text": "The sense with the highest probability is assigned to the test example .This system implements a standard benchmark , the Naive Bayesian classifier based on a bag of words feature set .keywords : supervised learning , Naive Bayesian classifier , bag of words .", "label": "", "metadata": {}, "score": "70.575485"}
{"text": "[ 0043 ] .Step 408 determines whether there are any filtered words left in the filtered - words file .If there are , then the method continues at step 402 .If there are no filtered words left , then the method continues at step 410 .", "label": "", "metadata": {}, "score": "70.64076"}
{"text": "Description : The task for Spanish is a ' lexical sample ' for 40 words ( 18 nouns , 13 verbs and 9 adjectives ) .The items chosen can only belong to one of the syntactic categories and the sentences have been chosen to illustrated it .", "label": "", "metadata": {}, "score": "70.6431"}
{"text": "--show - most - informative 10 : show the 10 most informative words .--no - pickle : the default is to store a pickled classifier , but this option lets us do evaluation without pickling the classifier .If you cd into the nltk - trainer directory and the run the above command , your output should look like this : .", "label": "", "metadata": {}, "score": "70.77858"}
{"text": "--show - most - informative 10 : show the 10 most informative words .--no - pickle : the default is to store a pickled classifier , but this option lets us do evaluation without pickling the classifier .If you cd into the nltk - trainer directory and the run the above command , your output should look like this : .", "label": "", "metadata": {}, "score": "70.77858"}
{"text": "--show - most - informative 10 : show the 10 most informative words .--no - pickle : the default is to store a pickled classifier , but this option lets us do evaluation without pickling the classifier .If you cd into the nltk - trainer directory and the run the above command , your output should look like this : .", "label": "", "metadata": {}, "score": "70.77858"}
{"text": "A lexicon builder 125 may expand the seed lexicon into the larger translation lexicon 105 by applying rules based on clues which indicate probable translations .The lexicon builder 125 may use seed lexicon to bootstrap these methods , using the word pairs in the seed lexicon as correct translations .", "label": "", "metadata": {}, "score": "70.797844"}
{"text": "The analysis of the corpus 200 by the linguistic data analyzer 202 includes the calculation of the absolute frequency of the unique words appearing in the corpus 200 .Methods for the calculation of frequency and creation of a word - list are described in FIG .", "label": "", "metadata": {}, "score": "70.9716"}
{"text": "While specific embodiments of the invention have been illustrated and described herein , it is realized that modifications and changes will occur to those skilled in the art .It is therefore to be understood that the appended claims are intended to cover all such modifications and changes as fall within the true spirit and scope of the invention .", "label": "", "metadata": {}, "score": "71.027534"}
{"text": "Gildea , D. , \" Loosely Tree - based Alignment for Machine Translation , \" In Proceedings of the 41st Annual Meeting on Assoc . for Computational Linguistics - vol . 1 ( Sapporo , Japan , Jul. 7 - 12 , 2003 ) .", "label": "", "metadata": {}, "score": "71.08194"}
{"text": "Gildea , D. , \" Loosely Tree - based Alignment for Machine Translation , \" In Proceedings of the 41st Annual Meeting on Assoc . for Computational Linguistics - vol . 1 ( Sapporo , Japan , Jul. 7 - 12 , 2003 ) .", "label": "", "metadata": {}, "score": "71.08194"}
{"text": "The statistical models may be trained on parallel corpora .Parallel corpora contain large amounts of text in one language along with their translation in another .Unfortunately , such corpora are available only in limited amounts and cover only in specific genres ( Canadian politics , Hong Kong laws , etc ) .", "label": "", "metadata": {}, "score": "71.544495"}
{"text": "Precision and recall can be combined to produce a single metric known as F - measure , which is the weighted harmonic mean of precision and recall .I find F - measure to be about as useful as accuracy .Or in other words , compared to precision & recall , F - measure is mostly useless , as you 'll see below .", "label": "", "metadata": {}, "score": "71.711365"}
{"text": "( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?N .Description : A commercial MT system was utilized as it was .", "label": "", "metadata": {}, "score": "71.84547"}
{"text": "name : David Yarowsky , Silviu Cucerzan , Radu Florian , Charles Schafer and Richard Wicentowski . jhu.edu .organisation : Computer Science Department and Center for Language and Speech Processing , Johns Hopkins University .Task / s : Czech all words .", "label": "", "metadata": {}, "score": "72.05708"}
{"text": "claim 1 , wherein for each group , only the maximum frequency , which is the highest frequency value in the group , is retained with full precision , and the frequencies of words with less than the maximum frequency are retained as a percentage of the maximum frequency .", "label": "", "metadata": {}, "score": "72.09082"}
{"text": "Step 408 determines whether there are any filtered words left in the filtered - words file .If there are , then the method continues at step 402 .If there are no filtered words left , then the method continues at step 410 .", "label": "", "metadata": {}, "score": "72.099915"}
{"text": "Tasks : English lexical sample ( official ) , Spanish lexical sample ( official ) , Swedish lexical sample ( official ) , Basque lexical sample ( unofficial ) .Did you use any training data provided in an automatic training procedure ?", "label": "", "metadata": {}, "score": "72.27321"}
{"text": "Stopwords are words that are generally considered useless .Most search engines ignore these words because they are so common that including them would greatly increase the size of the index without improving precision or recall .NLTK comes with a stopwords corpus that includes a list of 128 english stopwords .", "label": "", "metadata": {}, "score": "72.37699"}
{"text": "Stopwords are words that are generally considered useless .Most search engines ignore these words because they are so common that including them would greatly increase the size of the index without improving precision or recall .NLTK comes with a stopwords corpus that includes a list of 128 english stopwords .", "label": "", "metadata": {}, "score": "72.37699"}
{"text": "Stopwords are words that are generally considered useless .Most search engines ignore these words because they are so common that including them would greatly increase the size of the index without improving precision or recall .NLTK comes with a stopwords corpus that includes a list of 128 english stopwords .", "label": "", "metadata": {}, "score": "72.37699"}
{"text": "Stopwords are words that are generally considered useless .Most search engines ignore these words because they are so common that including them would greatly increase the size of the index without improving precision or recall .NLTK comes with a stopwords corpus that includes a list of 128 english stopwords .", "label": "", "metadata": {}, "score": "72.37699"}
{"text": "0030]FIG .2 is a block diagram of a system of creating compact linguistic data .The linguistic data analyzer 202 creates linguistic data 204 , described in detail below , by analyzing the corpus 200 of a natural language , such as English or French .", "label": "", "metadata": {}, "score": "72.51908"}
{"text": "Training Set vs Test Set and Accuracy .The movie reviews corpus has 1000 positive files and 1000 negative files .We 'll use 3/4 of them as the training set , and the rest as the test set .This gives us 1500 training instances and 500 test instances .", "label": "", "metadata": {}, "score": "72.5341"}
{"text": "Perhaps these words refer to important plot points or character development that signify a good movie .Whatever the case , with simple assumptions and very little code we 're able to get almost 73 % accuracy .This is somewhat near human accuracy , as apparently people agree on sentiment only around 80 % of the time .", "label": "", "metadata": {}, "score": "72.56928"}
{"text": "No .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ? :Yes .Description : The system tries to exploit the idea of \" Domain Driven Disambiguation \" .", "label": "", "metadata": {}, "score": "72.70914"}
{"text": "For example , the context examples containing the verb \" dress down \" are separated from the examples containing only \" dress \" .Words which are monosemous are eliminated at this step , as well as words which can be tagged as proper nouns ( if they are tagged as such by the part of speech tagger and if they have a role identified by the Named Entity recognizer ) .", "label": "", "metadata": {}, "score": "72.83601"}
{"text": "keywords : selectional preferences , grammatical relations , one sense per discourse .organisation : The University of Sussex and the University of Sheffield .Task / s : English All Words .Did you use any training data provided in an automatic training procedure ?", "label": "", "metadata": {}, "score": "72.97171"}
{"text": "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 1 . and expressed / VB concern / NN about2 .", "label": "", "metadata": {}, "score": "73.108185"}
{"text": "System is created and developed by Kaarel Kaljurand .keywords : wordnet - based , conceptual density .URL containing additional information : Click here - in Estonian , sorry !Did you use any training data provided in an automatic training procedure ?", "label": "", "metadata": {}, "score": "73.70875"}
{"text": "To run the code , we need to make sure everything is setup for training .The most important thing is installing the NLTK data ( and of course , you 'll need to install NLTK as well ) .In this case , we need the movie_reviews corpus , which you can download / install by running sudo python -m nltk.downloader movie_reviews .", "label": "", "metadata": {}, "score": "73.7679"}
{"text": "html \" , then a HyperText Markup Language ( HTML ) filter is obtained , and if the file extension is \" . txt \" , then a text filter is obtained .Other file extensions may also be mapped to additional filters .", "label": "", "metadata": {}, "score": "74.00154"}
{"text": "The user preferences specify regular expressions which are applied to the text in order to substitute invalid or unwanted characters .The method then continues with the step 304 of obtaining a filter corresponding to the type indicated by the file extension of the source file .", "label": "", "metadata": {}, "score": "74.259796"}
{"text": "There are two words in between in the third sentence .Moreover the words between cement and relations may vary and the distance between the two words also can vary .But there is a regularity in the patterns so that we can determine that cement is the right verb to use for this situation .", "label": "", "metadata": {}, "score": "74.47415"}
{"text": "If none of these sequences were found , we would consider the word \" art \" in isolation .If there was just one sense in the sense index for \" art \" , that sense would be returned , if there were no sense , then \" sense unknown \" would be returned .", "label": "", "metadata": {}, "score": "74.529106"}
{"text": "name : Marc El - Beze email : marc.elbeze@lia.univ-avignon.fr organisation : Laboratoire Informatique d'Avignon .Did you use any training data provided in an automatic training procedure ?Yes .Description ( 250 words max ) : The approach used in the Senseval-2 campaign is based on a multi - level view of the context .", "label": "", "metadata": {}, "score": "74.62372"}
{"text": "2 is a block diagram of a system of creating compact linguistic data .The linguistic data analyzer 202 creates linguistic data 204 , described in detail below , by analyzing the corpus 200 of a natural language , such as English or French .", "label": "", "metadata": {}, "score": "74.713585"}
{"text": "It 's ok to throw away data if that data is not adding value .And it 's especially recommended when that data is actually making your model worse .Improving feature extraction can often have a significant positive impact on classifier accuracy ( and precision and recall ) .", "label": "", "metadata": {}, "score": "74.76077"}
{"text": "It 's ok to throw away data if that data is not adding value .And it 's especially recommended when that data is actually making your model worse .Improving feature extraction can often have a significant positive impact on classifier accuracy ( and precision and recall ) .", "label": "", "metadata": {}, "score": "74.76077"}
{"text": "It 's ok to throw away data if that data is not adding value .And it 's especially recommended when that data is actually making your model worse .Improving feature extraction can often have a significant positive impact on classifier accuracy ( and precision and recall ) .", "label": "", "metadata": {}, "score": "74.76077"}
{"text": "It 's ok to throw away data if that data is not adding value .And it 's especially recommended when that data is actually making your model worse .Improving feature extraction can often have a significant positive impact on classifier accuracy ( and precision and recall ) .", "label": "", "metadata": {}, "score": "74.76077"}
{"text": "Coinciding with the github move , the documentation was updated to use Sphinx , the same documentation generator used by Python and many other projects .While I personally like Sphinx and restructured text ( which I used to write this post ) , I 'm not thrilled with the results .", "label": "", "metadata": {}, "score": "74.931335"}
{"text": "The system of .claim 1 , further comprising : . a user interface , comprising : . a text input device ; and .a text output device ; and .a text input logic unit , .The system of . claim 2 , wherein the text input device is a keyboard .", "label": "", "metadata": {}, "score": "74.94006"}
{"text": "If a character has its most significant bit set , then it is the last character in a word .The character is then treated as if its most significant bit were not set for the purpose of determining the value of the character , so that the most significant bit does not affect the value of the character .", "label": "", "metadata": {}, "score": "75.249054"}
{"text": "Precision and Recall for Positive and Negative Reviews .I found the results quite interesting : . pos precision : 0.651595744681 pos recall : 0.98 pos F - measure : 0.782747603834 neg precision : 0.959677419355 neg recall : 0.476 neg F - measure : 0.636363636364 .", "label": "", "metadata": {}, "score": "75.442825"}
{"text": "Description .CROSS - REFERENCE TO RELATED APPLICATIONS .This application is a Continuation of U.S. patent application Ser .No .10/401,124 , filed on Mar. 26 , 2003 , now U.S. Pat .No .7,620,538 , which claims priority to U.S. Provisional Application Ser .", "label": "", "metadata": {}, "score": "75.47557"}
{"text": "probablity.py also includes ConditionalRedisFreqDist for creating ConditionalProbDists .Lists .For creating lists of samples , that very much depends on your use case , but here 's some example code for doing so .r is a redis object , key is the index key for storing the list , and samples is assumed to be a sorted list .", "label": "", "metadata": {}, "score": "75.56996"}
{"text": "The exception table 702 stores hotwords that are located in local word definition tables 708 that are complex word definition tables .A hotword can be retrieved quickly using the hotword table 700 and the exception table 702 , instead of performing a search of the local word definition tables 708 to find the hotword .", "label": "", "metadata": {}, "score": "75.90794"}
{"text": "You should checkout or download nltk - trainer if you want to run the examples yourself .NLTK Movie Reviews Corpus .To run the code , we need to make sure everything is setup for training .The most important thing is installing the NLTK data ( and of course , you 'll need to install NLTK as well ) .", "label": "", "metadata": {}, "score": "76.14281"}
{"text": "You should checkout or download nltk - trainer if you want to run the examples yourself .NLTK Movie Reviews Corpus .To run the code , we need to make sure everything is setup for training .The most important thing is installing the NLTK data ( and of course , you 'll need to install NLTK as well ) .", "label": "", "metadata": {}, "score": "76.14281"}
{"text": "This is what NLTK already does best , and I hope that becomes even more true in the future .You can rearrange ubt any way you want to change the order of the taggers ( though ubt is generally the most accurate order ) .", "label": "", "metadata": {}, "score": "76.670204"}
{"text": "The company / nn adjourned / vb its annual / jj meeting / nn May 12 to allow / vb time / nn for negotiations and expressed / vb concern / nn ab out future / jj actions / nn by preferred / jj holders / nn .", "label": "", "metadata": {}, "score": "76.7145"}
{"text": "claim 22 , wherein the step of filtering the corpus comprises the steps of : . reading a plurality of computer files , each file having an extension which indicates a type of the file ; . applying regular expressions to text contained in the files to substitute invalid or unwanted characters in the text ; . obtaining a filter for each file , the filter corresponding to the type indicated by the extension ; and .", "label": "", "metadata": {}, "score": "76.9577"}
{"text": "Description .CROSS - REFERENCE TO RELATED APPLICATION .This is a continuation of U.S. patent application Ser .No .10/289,656 , filed on Nov. 7 , 2002 , now U.S. Pat .No . 7,269,548 which claims priority from U.S. Provisional Application Ser .", "label": "", "metadata": {}, "score": "77.04314"}
{"text": "The constructs expressed concern and spokesman said must be tagged verb - object and noun - verb respectively .Preferred stock , on the other hand , must be identified and tagged as a fixed adjective - noun construct .The complex scope of the pre - processing task is best illustrated by the input to the preprocessor shown below .", "label": "", "metadata": {}, "score": "77.243805"}
{"text": "Any file that is identified as neg is 96 % likely to be correct ( high precision ) .This means very few false positives for the neg class .But many files that are neg are incorrectly classified .Low recall causes 52 % false negatives for the neg label .", "label": "", "metadata": {}, "score": "77.24449"}
{"text": "For example , the Unicode character 0x3600 can be represented as 10 if it is located at index 10 in the character - mapping table .The location of a character in the character - mapping table is not significant , and is based on the order that characters appear in the given word - list .", "label": "", "metadata": {}, "score": "78.200386"}
{"text": "For example , the Unicode character 0x3600 can be represented as 10 if it is located at index 10 in the character - mapping table .The location of a character in the character - mapping table is not significant , and is based on the order that characters appear in the given word - list .", "label": "", "metadata": {}, "score": "78.200386"}
{"text": "Classifier Precision .Precision measures the exactness of a classifier .A higher precision means less false positives , while a lower precision means more false positives .This is often at odds with recall , as an easy way to improve precision is to decrease recall .", "label": "", "metadata": {}, "score": "78.29303"}
{"text": "The subjectivity classifier is first , and determines whether the text is objective or subjective .If the text is objective , then a label of neutral is returned , and the polarity classifier is not used .However , if the text is subjective ( or polar ) , then the polarity classifier is used to determine if the text is positive or negative .", "label": "", "metadata": {}, "score": "78.42084"}
{"text": "\" The lexicon builder 125 may measure the spelling similarity between every German and English word , and sort possible word pairs accordingly .This may be done in a greedy fashion , i.e. , once a word is assigned to a word pair , the lexicon builder 125 does not look for another match .", "label": "", "metadata": {}, "score": "78.42238"}
{"text": "The substitution table 610 contains a bi - gram substitution table , followed by a table for each group of higher - order n - grams which are defined , such as tri - grams , four - grams , and so on .", "label": "", "metadata": {}, "score": "78.67149"}
{"text": "This command will ensure that the movie_reviews corpus is downloaded and/or located in an NLTK data directory , such as /usr / share / nltk_data on Linux , or C:\\nltk_data on Windows .The movie_reviews corpus can then be found under the corpora subdirectory .", "label": "", "metadata": {}, "score": "78.927795"}
{"text": "This command will ensure that the movie_reviews corpus is downloaded and/or located in an NLTK data directory , such as /usr / share / nltk_data on Linux , or C:\\nltk_data on Windows .The movie_reviews corpus can then be found under the corpora subdirectory .", "label": "", "metadata": {}, "score": "78.927795"}
{"text": "Description : We searched for collocations within a window of one word only .For instance , if the word to be disambiguated was \" art \" and the sentence was the \" the pop art collection is owned by ... \" , we would have three sequences : 1 . pop art , 2 . pop art collection , 3 . art collection .", "label": "", "metadata": {}, "score": "78.935135"}
{"text": "BACKGROUND .Machine translation ( MT ) concerns the automatic translation of natural language sentences from a first language ( e.g. , French ) into another language ( e.g. , English ) .Systems that perform MT techniques are said to \" decode \" the source language into the target language .", "label": "", "metadata": {}, "score": "79.41916"}
{"text": "Eliminating low information features gives your model clarity by removing noisy data .It can save you from overfitting and the curse of dimensionality .When you use only the higher information features , you can increase performance while also decreasing the size of the model , which results in less memory usage along with faster training and classification .", "label": "", "metadata": {}, "score": "79.49786"}
{"text": "Eliminating low information features gives your model clarity by removing noisy data .It can save you from overfitting and the curse of dimensionality .When you use only the higher information features , you can increase performance while also decreasing the size of the model , which results in less memory usage along with faster training and classification .", "label": "", "metadata": {}, "score": "79.49786"}
{"text": "The text input device 104 is any device that enables text entry , such a QWERTY , AZERTY or Dvorak keyboard , or a reduced keyboard .The user interface 103 also includes a text output device 106 , which displays text to a user .", "label": "", "metadata": {}, "score": "79.67186"}
{"text": "The text input logic unit 102 may , for example , be implemented by computer instructions which are executed by a computer processor that is contained in a mobile device .[ 0028 ] .The text input logic unit 102 receives text that is entered by a user using the text input device 104 .", "label": "", "metadata": {}, "score": "79.83593"}
{"text": "Beschreibung .CROSS - REFERENCE TO RELATED APPLICATION .[ 0001 ] .This application claims priority from United States Provisional Application S / No .60/393,903 , filed on Jul. 3 , 2002 .The complete disclosure of the provisional application , including drawings , is hereby incorporated into this application by reference .", "label": "", "metadata": {}, "score": "79.92958"}
{"text": "60/368,070 , filed on Mar. 26 , 2002 , and U.S. Provisional Application Ser .No .60/368,447 , filed on Mar. 27 , 2002 , the disclosures of which are incorporated by reference .ORIGIN OF INVENTION .The research and development described in this application were supported by Defense Advanced Research Project Agency ( DARPA ) under grant number N66001 - 00 - 1 - 8914 .", "label": "", "metadata": {}, "score": "79.95439"}
{"text": "python train_classifier.py --algorithm NaiveBayes --instances files --fraction 0.75 --show - most - informative 10 --no - pickle movie_reviews .Here 's an explanation of each option : . --instances files : this says that each file is treated as an individual instance , so that each feature set will contain word : True for each word in a file .", "label": "", "metadata": {}, "score": "80.11271"}
{"text": "python train_classifier.py --algorithm NaiveBayes --instances files --fraction 0.75 --show - most - informative 10 --no - pickle movie_reviews .Here 's an explanation of each option : . --instances files : this says that each file is treated as an individual instance , so that each feature set will contain word : True for each word in a file .", "label": "", "metadata": {}, "score": "80.11271"}
{"text": "Mutual Information .By the chain rule for entropy , .Therefore , . where H represents entropy and X and Y are random variables .This difference is called the mutual information between X and Y. It is the amount of information one random variable contains about another .", "label": "", "metadata": {}, "score": "80.13467"}
{"text": "NLTK has moved development and hosting to github , replacing google code and SVN .The primary motivation is to make new development easier , and already a Python 3 branch is under active development .I think this is great , since github makes forking & pull requests quite easy , and it 's become the de - facto \" social coding \" site .", "label": "", "metadata": {}, "score": "80.62482"}
{"text": "Did you use any training data provided in an automatic training procedure ?Yes .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "81.033936"}
{"text": "Did you use any training data provided in an automatic training procedure ?YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "81.033936"}
{"text": "Did you use any training data provided in an automatic training procedure ?YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "81.033936"}
{"text": "Did you use any training data provided in an automatic training procedure ?YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "81.033936"}
{"text": "Did you use any training data provided in an automatic training procedure ?YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "81.033936"}
{"text": "Did you use any training data provided in an automatic training procedure ?YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "81.033936"}
{"text": "Did you use any training data provided in an automatic training procedure ?YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "81.033936"}
{"text": "Did you use any training data provided in an automatic training procedure ?Yes .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "81.033936"}
{"text": "Did you use any training data provided in an automatic training procedure ?Yes .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "81.033936"}
{"text": "If you have your own theories to explain the results , or ideas on how to improve precision and recall , please share in the comments .Collocation or lexical collocation means two or more words co - occur in a sentence more frequently than by chance .", "label": "", "metadata": {}, "score": "81.44061"}
{"text": "However , they span different time periods and have a different orientation : the World Street Journal covers mostly business news , the German news wire mostly German politics .The system 100 may use clues to find translations of words in the monolingual corpora .", "label": "", "metadata": {}, "score": "81.45354"}
{"text": "Individually they are harmless , but in aggregate , low information features can decrease performance .Eliminating low information features gives your model clarity by removing noisy data .It can save you from overfitting and the curse of dimensionality .When you use only the higher information features , you can increase performance while also decreasing the size of the model , which results in less memory usage along with faster training and classification .", "label": "", "metadata": {}, "score": "81.65422"}
{"text": "Individually they are harmless , but in aggregate , low information features can decrease performance .Eliminating low information features gives your model clarity by removing noisy data .It can save you from overfitting and the curse of dimensionality .When you use only the higher information features , you can increase performance while also decreasing the size of the model , which results in less memory usage along with faster training and classification .", "label": "", "metadata": {}, "score": "81.65422"}
{"text": "An example of a purpose - specific corpus is one comprised of recordings of car drivers speaking to a simulation of a voice - operated control system that recognizes spoken commands .An example of a national corpus is the English language .", "label": "", "metadata": {}, "score": "83.12047"}
{"text": "Did you use any training data provided in an automatic training procedure ?N .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "84.3982"}
{"text": "Did you use any training data provided in an automatic training procedure ?N .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "84.3982"}
{"text": "Did you use any training data provided in an automatic training procedure ?N .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "84.3982"}
{"text": "Did you use any training data provided in an automatic training procedure ?N .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "84.3982"}
{"text": "Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "84.67818"}
{"text": "Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "84.67818"}
{"text": "Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "84.67818"}
{"text": "Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "84.67818"}
{"text": "Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "84.67818"}
{"text": "Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "84.67818"}
{"text": "Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "84.67818"}
{"text": "[ 0025 ] .The user interface 103 includes a text input device 104 , which allows a user to enter text into the system .The text input device 104 is any device that enables text entry , such a QWERTY , AZERTY or Dvorak keyboard , or a reduced keyboard .", "label": "", "metadata": {}, "score": "84.851395"}
{"text": "korea.ac.kr .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "85.51918"}
{"text": "Recall measures the completeness , or sensitivity , of a classifier .Higher recall means less false negatives , while lower recall means more false negatives .Improving recall can often decrease precision because it gets increasingly harder to be precise as the sample space increases .", "label": "", "metadata": {}, "score": "87.183716"}
{"text": "python train_tagger.py treebank --sequential '' --classifier NaiveBayes .If you do want to backoff to a sequential tagger , be sure to specify a cutoff probability , like so : . python train_tagger.py treebank --sequential ubt --classifier NaiveBayes --cutoff_prob 0.4 .Any of the NLTK classification algorithms can be used for the --classifier argument , such as Maxent or MEGAM , and every algorithm other than NaiveBayes has specific training options that can be customized .", "label": "", "metadata": {}, "score": "87.23068"}
{"text": "For example , the presence of the word \" magnificent \" in a movie review is a strong indicator that the review is positive .That makes \" magnificent \" a high information word .Notice that the most informative features above did not change .", "label": "", "metadata": {}, "score": "87.4301"}
{"text": "For example , the presence of the word \" magnificent \" in a movie review is a strong indicator that the review is positive .That makes \" magnificent \" a high information word .Notice that the most informative features above did not change .", "label": "", "metadata": {}, "score": "87.4301"}
{"text": "For example , the presence of the word \" magnificent \" in a movie review is a strong indicator that the review is positive .That makes \" magnificent \" a high information word .Notice that the most informative features above did not change .", "label": "", "metadata": {}, "score": "87.4301"}
{"text": "For example , the presence of the word \" magnificent \" in a movie review is a strong indicator that the review is positive .That makes \" magnificent \" a high information word .Notice that the most informative features above did not change .", "label": "", "metadata": {}, "score": "87.4301"}
{"text": "If you 'd like to do more , please fill out this survey to let me know what your needs are .If you like it , please share it .If you want to see more , leave a comment below .", "label": "", "metadata": {}, "score": "89.19268"}
{"text": "Global - sentence constraints are shown by the following two examples : . and preferred stock sold yesterday was . . . .2 .In case 1 , a main verb is found ( i.e. , was ) , and preferred is taken as an adjective ; in case 2 , a main verb is not found , and therefore expressed itself is taken as the main verb .", "label": "", "metadata": {}, "score": "89.655075"}
{"text": "But despite this chuckle - worthy result .accuracy is up almost 9 % .pos precision has increased over 10 % with only 4 % drop in recall .neg recall has increased over 21 % with just under 4 % drop in precision .", "label": "", "metadata": {}, "score": "90.17143"}
{"text": "But despite this chuckle - worthy result .accuracy is up almost 9 % .pos precision has increased over 10 % with only 4 % drop in recall .neg recall has increased over 21 % with just under 4 % drop in precision .", "label": "", "metadata": {}, "score": "90.17143"}
{"text": "But despite this chuckle - worthy result .accuracy is up almost 9 % .pos precision has increased over 10 % with only 4 % drop in recall .neg recall has increased over 21 % with just under 4 % drop in precision .", "label": "", "metadata": {}, "score": "90.17143"}
{"text": "But despite this chuckle - worthy result .accuracy is up almost 9 % .pos precision has increased over 10 % with only 4 % drop in recall .neg recall has increased over 21 % with just under 4 % drop in precision .", "label": "", "metadata": {}, "score": "90.17143"}
{"text": "There 's no insight to be gained from having it , and we would n't lose any knowledge if it was taken away .Improving Results with Better Feature Selection .One possible explanation for the above results is that people use normally positives words in negative reviews , but the word is preceded by \" not \" ( or some other negative word ) , such as \" not great \" .", "label": "", "metadata": {}, "score": "90.38605"}
{"text": "Mean and Variance .Collocations are not always of fixed phrases .Consider the following sentences .Obama cemented relations with the East .Jill cemented his relations with Jack .John cemented Mark Taylor 's relations with Tom .While cement and relations occur together frequently cement does not follow relations immediately always .", "label": "", "metadata": {}, "score": "90.575455"}
{"text": "Collocational windows are used to capture bigrams at a specific distance .Let us use a three word collocational window for the following sentence .Plane crashed as climate worsened .We get the following bigrams .Plane crashed plane as plane climate . crashed as crashed climate crashed worsened as climate as worsened . climate worsened .", "label": "", "metadata": {}, "score": "93.42836"}
{"text": "email : Yvonne.Canning@sunderland.ac.uk , Michael.Oakes@sunderland.ac.uk , John.Tait@sunderland.ac.uk .organisation : The University of Sunderland .Did you use any training data provided in an automatic training procedure ?N .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?", "label": "", "metadata": {}, "score": "94.166725"}
{"text": "Uppsala , Sweden ; Click here", "label": "", "metadata": {}, "score": "101.03038"}
{"text": "He is known for his fair and square dealings and everybody trusts his work .Here fair and square means honest but if we take the individual words though the word fair gives somewhat closer meaning as it means just the word square confuses us .", "label": "", "metadata": {}, "score": "106.30951"}
{"text": "The best word matches may be collected in a greedy fashion .Another clue is based on the assumption that pairs of words that are similar in one language should have translations that are similar in the other language .For instance , Wednesday is similar to Thursday as Mittwoch is similar to Donnerstag .", "label": "", "metadata": {}, "score": "122.85074"}
