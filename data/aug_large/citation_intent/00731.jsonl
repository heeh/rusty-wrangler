{"text": "In contrast , the MDP - HMM is robust to these effects and correctly calls no copy number alterations in the region .The explanation for the improved performance of the MDP - HMM for this application is ex- plained by the QQ - plots in Figure 8(d - f ) .", "label": "", "metadata": {}, "score": "31.687052"}
{"text": "The HMMs were trained on 180 hours of Hub5 training data .The system used a 27k vocabulary that covered all words in the acoustic training data .The core of the pronunciation dictionary was based on the 1993 LIMSI WSJ lexicon , but used a large number of additions along with various changes .", "label": "", "metadata": {}, "score": "35.340996"}
{"text": "The exact update rule for the mixture weights is not too important for the decision - tree tied - state mixture Gaussian HMMs used in the experiments reported here , since the Gaussian means and variances play a much larger role in discrimination .", "label": "", "metadata": {}, "score": "35.672276"}
{"text": "The exact update rule for the mixture weights is not too important for the decision - tree tied - state mixture Gaussian HMMs used in the experiments reported here , since the Gaussian means and variances play a much larger role in discrimination .", "label": "", "metadata": {}, "score": "35.672276"}
{"text": "The use of MMIE training was a key contributer to the performance of the CU - HTK March 2000 Hub5 evaluation system .The model parameters in HMM based speech recognition systems are normally estimated using Maximum Likelihood Estimation ( MLE ) .", "label": "", "metadata": {}, "score": "36.393982"}
{"text": "The calculation of the denominator terms directly is computationally very expensive and so , in this work and as suggested in [ 9 ] , word lattices are used to approximate the denominator model .The first step is to generate word - level lattices , normally using an MLE - trained HMM system and a bigram LM appropriate for the training set .", "label": "", "metadata": {}, "score": "36.528908"}
{"text": "The calculation of the denominator terms directly is computationally very expensive and so , in this work and as suggested in [ 9 ] , word lattices are used to approximate the denominator model .The first step is to generate word - level lattices , normally using an MLE - trained HMM system and a bigram LM appropriate for the training set .", "label": "", "metadata": {}, "score": "36.528908"}
{"text": "The flexibility of the HMM structure together with the general Dirichlet error distribution suggest that the approach will have many potential applications , particularly for long time - series such as the copy number data analysed in this paper .The results in our genomic example are very promising , and we are already investigating further genetic applications of this work .", "label": "", "metadata": {}, "score": "36.62193"}
{"text": "Previously , using an implementation from JHU , the technique was investigated using various training set sizes and levels of model complexity [ 7 ] .It was found that while consistent improvements were obtained , the improvement in WER was reduced when features such as VTLN and MLLR adaptation were included in the system .", "label": "", "metadata": {}, "score": "36.717323"}
{"text": "By extendingthe standard forward - backward(BaumWelch ) algorithm , we derive an efficient procedure for estimating the model parameters from unlabeled data .We then use the trained model for automatic hierarchical parsing of observation sequences .We describe two applications of our model and its parameter estimation procedure .", "label": "", "metadata": {}, "score": "36.742302"}
{"text": "In this paper we shall essentially generalise the approach of this paper to our HMM - MDP context .Furthermore , we shall introduce a further innovation using the slice sampler construction of Walker ( 2007 ) .The paper in structured as follows .", "label": "", "metadata": {}, "score": "36.793907"}
{"text": "This is probably because while MLE training gives equal weight to all training utterances , MMIE training effectively gives greater weight to those training set utterances with low sentence posterior probabilities for the correct utterance .MMIE was also used to train quinphone HMMs .", "label": "", "metadata": {}, "score": "37.498253"}
{"text": "Details of the March 2000 CU - HTK Hub5 system can be found in [ 5 ] .The experiments investigated the effect of different training set and HMM set sizes and types ; the use of acoustic likelihood scaling and unigram LMs in training and any possible interactions between MMIE training and maximum likelihood linear regression - based adaptation .", "label": "", "metadata": {}, "score": "38.264812"}
{"text": "Details of the March 2000 CU - HTK Hub5 system can be found in [ 5 ] .The experiments investigated the effect of different training set and HMM set sizes and types ; the use of acoustic likelihood scaling and unigram LMs in training and any possible interactions between MMIE training and maximum likelihood linear regression - based adaptation .", "label": "", "metadata": {}, "score": "38.264812"}
{"text": "A further approximately 3-fold increase in the amount of training data only brings a further 1.6 % absolute reduction in WER .The model parameters in HMM based speech recognition systems are normally estimated using Maximum Likelihood Estimation ( MLE ) .", "label": "", "metadata": {}, "score": "38.494232"}
{"text": "This study presents a region where de novo deletions appear to be involved in the etiology of oralclefts , although the underlying biological mechanisms are still unknown .As HMMs readily accomodate multiple data sequences , the observation that copy number can be estimated from genotyping arrays [ 29 ] led to the development of several HMMs that jointly model copy number and genotypes at SNPs [ 30 - 37].", "label": "", "metadata": {}, "score": "38.685284"}
{"text": "The implementation of the combined architecture and training scheme is described in detail .The networks are evaluated in a hybrid HMM / ANN system for phoneme recognition on the TIMIT database , and for word recognition on the WAXHOLM database .", "label": "", "metadata": {}, "score": "39.020576"}
{"text": "The implementation of the combined architecture and training scheme is described in detail .The networks are evaluated in a hybrid HMM / ANN system for phoneme recognition on the TIMIT database , and for word recognition on the WAXHOLM database .", "label": "", "metadata": {}, "score": "39.020576"}
{"text": "However as discussed in [ 16 ] this can be approximated by using a word lattice which is generated once to constrain the number of word sequences considered .This lattice - based framework can be used to generate the necessary statistics to apply the Extended - Baum Welch ( EBW ) algorithm [ 5 , 13 , 16 ] to iteratively update the model parameters .", "label": "", "metadata": {}, "score": "39.02264"}
{"text": "Informally , we would like to make moves in the high probability region of HMM configurations and then use the residuals to fit the MDP component .And third , we would like the algorithm to require as little human intervention as possible ( hence avoid having to tune algorithmic parameters ) .", "label": "", "metadata": {}, "score": "39.136814"}
{"text": "Page 17 .We see that the G - HMM and R - HMM fails to capture the behaviour of the data in the tails and that the distribution of the data also appears to be asymmetric .Both of these features are pathological for the G - HMM and , though the R - HMM can compensate for heavy tails , it inherently assumes symmetry that is not present in the data .", "label": "", "metadata": {}, "score": "39.191406"}
{"text": "Conditional random fields ( Lafferty et al . , 2001 ) are quite effective at sequence labeling tasks like shallow parsing ( Sha and Pereira , 2003 ) and namedentity extraction ( McCallum and Li , 2003 ) .CRFs are log - linear , allowing the incorporation of arbitrary features into the model .", "label": "", "metadata": {}, "score": "39.372368"}
{"text": "Conditional random fields ( Lafferty et al . , 2001 ) are quite effective at sequence labeling tasks like shallow parsing ( Sha and Pereira , 2003 ) and namedentity extraction ( McCallum and Li , 2003 ) .CRFs are log - linear , allowing the incorporation of arbitrary features into the model .", "label": "", "metadata": {}, "score": "39.372368"}
{"text": "In this way , more confusable data is generated which improves generalisation .An unigram LM for MMIE training is investigated in this paper .When combining the likelihoods from an HMM - based acoustic model and the LM it is usual to scale the LM log probability .", "label": "", "metadata": {}, "score": "39.677917"}
{"text": "In this way , more confusable data is generated which improves generalisation .An unigram LM for MMIE training is investigated in this paper .When combining the likelihoods from an HMM - based acoustic model and the LM it is usual to scale the LM log probability .", "label": "", "metadata": {}, "score": "39.677917"}
{"text": "We want the computational methodology for HMM - MDP to meet 5 .Page 7 .The model we introduce in Section 2 is targeted to uncover structural changes in long time series ( T can be of O(105 ) ) .", "label": "", "metadata": {}, "score": "40.064724"}
{"text": "The training set lattices regenerated after a single MMIE iteration gave a WER of 16.8 % and a LWER of 3.2 % , showing that the technique is very effective in reducing training set error .However , it was found that these regenerated lattices were no better to use in subsequent training iterations and so all further work used just the initially generated word lattices .", "label": "", "metadata": {}, "score": "40.14922"}
{"text": "The training set lattices regenerated after a single MMIE iteration gave a WER of 16.8 % and a LWER of 3.2 % , showing that the technique is very effective in reducing training set error .However , it was found that these regenerated lattices were no better to use in subsequent training iterations and so all further work used just the initially generated word lattices .", "label": "", "metadata": {}, "score": "40.14922"}
{"text": "First the details of the MMIE objective function are introduced .Then the lattice - based framework used for a compact encoding of alternative hypotheses is described along with the Extended Baum - Welch ( EBW ) algorithm for updating model parameters .", "label": "", "metadata": {}, "score": "40.443684"}
{"text": "First the details of the MMIE objective function are introduced .Then the lattice - based framework used for a compact encoding of alternative hypotheses is described along with the Extended Baum - Welch ( EBW ) algorithm for updating model parameters .", "label": "", "metadata": {}, "score": "40.443684"}
{"text": "Furthermore , it is important to enhance the discrimination of the acoustic models without overly relying on the language model to resolve difficulties .Therefore as suggested in [ 15 ] a unigram language model was used during MMIE training which also improves generalisation performance [ 19 ] .", "label": "", "metadata": {}, "score": "40.54731"}
{"text": "This section gives an overview of the complete system as used in the March 2000 evaluation .The system operates in multiple passes through the data : initial passes are used to generate word lattices and then these lattices are rescored using four different sets of adapted acoustic models .", "label": "", "metadata": {}, "score": "40.638664"}
{"text": "Initial passes were used for test - data warp factor selection , gender determination and finding an initial word string for unsupervised mean and variance maximum likelihood linear regression ( MLLR ) adaptation [ 8 , 3 ] .Word - level lattices were then created using adapted triphone HMMs and a bigram model which were expanded to included the full 4-gram and class model probabilities .", "label": "", "metadata": {}, "score": "40.773865"}
{"text": "The HMMs are constructed using decision - tree based state - clustering and both triphone and quinphone models can be used .All experiments here used gender independent HMM sets .The pronunciation dictionary used in the experiments discussed below was for either a 27k vocabulary ( as used in [ 4 ] ) or a 54k vocabulary and the core of this dictionary is based on the LIMSI 1993 WSJ lexicon .", "label": "", "metadata": {}, "score": "41.172203"}
{"text": "The HMMs are constructed using decision - tree based state - clustering and both triphone and quinphone models can be used .All experiments here used gender independent HMM sets .The pronunciation dictionary used in the experiments discussed below was for either a 27k vocabulary ( as used in [ 4 ] ) or a 54k vocabulary and the core of this dictionary is based on the LIMSI 1993 WSJ lexicon .", "label": "", "metadata": {}, "score": "41.172203"}
{"text": "Access to the published version may require a subscription .Page 2 .Our approach uses a Mixture of Dirichlet Process ( MDP ) model for the unknown sampling distri- bution ( likelihood ) for the observations arising in each state and a computationally efficient data augmentation scheme to aid inference .", "label": "", "metadata": {}, "score": "41.528465"}
{"text": "If words were not seen in the training data a uniform distribution over all pronunciation variants is assumed .However , this straight - forward implementation only brought moderate improvements in WER .The dictionaries in the HTK system explicitly contain silence models as part of a pronunciation .", "label": "", "metadata": {}, "score": "41.633427"}
{"text": "FORTRAN 77 and MATLAB code are available on request by the authors .3.2 Comparison with alternative schemes There are other Gibbs sampling schemes which can be used to fit the HMM - MDP to the observed data .They are based on alternative parametrisations of the DPP .", "label": "", "metadata": {}, "score": "41.706856"}
{"text": "So the M - estimator is much faster to train .Sutton and McCallum ( 2005 ) present approximate methods that keep a discriminative objective while avoiding full inference .We see M - estimation as a particularly promising method in settings where p ..", "label": "", "metadata": {}, "score": "41.814346"}
{"text": "Page 12 .4.1Comparison of MDP posterior sampling schemes We first carry out a comparison of different schemes for performing the \" MDP update \" .We consider this part of the simulation algorithm separately since it can be used in various contexts which involve posterior simulation of stick - breaking processes .", "label": "", "metadata": {}, "score": "41.819954"}
{"text": "This allows for example for the forward - backward sampling and marginal likelihood sampling of state transition paths in an HMM .Our work here is motivated by the problem of analysing of genomic copy number variation in mammalian genomes ( Colella et al . , 2007 ) .", "label": "", "metadata": {}, "score": "42.08288"}
{"text": "In developing our methodology therefore , we have paid close attention to ensure that methods scale well with the size of the data .Moreover , our approach gives good MCMC mixing properties and needs little or no algorithm tuning .This work constructed a marginal algorithm where the DP itself is analytically integrated out ( see also Liu , 1996 ; Green & Richardson , 2001 ; Jain & Neal , 2004 ) .", "label": "", "metadata": {}, "score": "42.349945"}
{"text": "This document describes Algorithm::BaumWelch version 0.0.2 .SYNOPSIS .# In VOID - context prints formated results to STDOUT .# In LIST - context returns references to the predicted transition & emission matrices and the starting parameters .DESCRIPTION .The Baum - Welch algorithm is used to compute the parameters ( transition and emission probabilities ) of an Hidden Markov Model ( HMM ) .", "label": "", "metadata": {}, "score": "42.52999"}
{"text": "As shown in [ 19 ] the gains from MLLR adaptation are as great for MMIE models as for MLE trained models .Hence the primary acoustic models used in the March 2000 CU - HTK evaluation system used gender - independent MMIE trained HMMs .", "label": "", "metadata": {}, "score": "42.566944"}
{"text": "The effect of extending the training set to the 68 hour h5train00sub set [ 5 ] was investigated next using an HMM system with 6165 speech states and 12 Gaussians / state .Tests were performed on both the eval97sub and the 1998 evaluation set ( eval98 ) .", "label": "", "metadata": {}, "score": "42.61222"}
{"text": "The effect of extending the training set to the 68 hour h5train00sub set [ 5 ] was investigated next using an HMM system with 6165 speech states and 12 Gaussians / state .Tests were performed on both the eval97sub and the 1998 evaluation set ( eval98 ) .", "label": "", "metadata": {}, "score": "42.61222"}
{"text": "All the above results used models that were not adapted to the particular conversation side using maximum likelihood linear regression ( MLLR ) [ 2 ] .To measure MLLR adaptation performance , MMIE and MLE models ( with data weighting ) were used in a full - decode of the test data , i.e. not rescoring lattices , with a 4-gram language model .", "label": "", "metadata": {}, "score": "42.632156"}
{"text": "All the above results used models that were not adapted to the particular conversation side using maximum likelihood linear regression ( MLLR ) [ 2 ] .To measure MLLR adaptation performance , MMIE and MLE models ( with data weighting ) were used in a full - decode of the test data , i.e. not rescoring lattices , with a 4-gram language model .", "label": "", "metadata": {}, "score": "42.632156"}
{"text": "This paper has discussed the use of discriminative training for large vocabulary HMM - based speech recognition for a training set size and level of task difficulty not previously attempted .It has been shown that 2 - 3 % absolute reductions in word error rates can be obtained for the transcription of conversational telephone speech .", "label": "", "metadata": {}, "score": "42.662388"}
{"text": "This paper has discussed the use of discriminative training for large vocabulary HMM - based speech recognition for a training set size and level of task difficulty not previously attempted .It has been shown that 2 - 3 % absolute reductions in word error rates can be obtained for the transcription of conversational telephone speech .", "label": "", "metadata": {}, "score": "42.662388"}
{"text": "Additionally , the computational times for all algorithms grow linearly with T , the size of the data1 .The simulation experiment suggests that the retrospective MCMC is mixing faster than the other algorithms , and that the block Gibbs sampler ( with or without label - switching moves ) is more efficient than the slice Gibbs sampler .", "label": "", "metadata": {}, "score": "42.766403"}
{"text": "The model parameters in HMM based speech recognition systems are normally estimated using Maximum Likelihood Estimation ( MLE ) .If certain conditions hold , including model correctness , then MLE can be shown to be optimal .However , when estimating the parameters of HMM - based speech recognisers , the true data source is not an HMM and therefore other training objective functions , in particular those that involve discriminative training , are of interest .", "label": "", "metadata": {}, "score": "42.816444"}
{"text": "Table 3 shows word error rates using triphone HMMs trained on h5train00 .These experiments required the generation of numerator and denominator lattices for each of the 267,611 training segments .It was found that two iterations of MMIE re - estimation gave the best test - set performance [ 19 ] .", "label": "", "metadata": {}, "score": "42.82509"}
{"text": "These experiments help tease apart the contributions of rich features and discriminative training , which are shown to be more than additive . ...e model .So the M - estimator is much faster to train .Sutton and McCallum ( 2005 ) present approximate inference methods that aim to keep a discriminative objective while avoiding the full inference required by maximum conditional likelihood .", "label": "", "metadata": {}, "score": "42.88509"}
{"text": "Furthermore , other product partition models such as CART or changepoint models could similarly be investigated within this MDP error structure context .Further work will explore these extensions .References Andersson , R. , Bruder , C. E. G. , Piotrowski , A. , Menzel , U. , Nord , H. , Sandgren , J. , Hvidsten , T. R. , de Sthl , T. D. , Dumanski , J. P. & Komorowski , J. ( 2008 ) .", "label": "", "metadata": {}, "score": "43.009895"}
{"text": "We describe a novel approach , contrastive estimation .We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efficient .Applied to a sequence labeling problem - POS tagging given a tagging dictionary and unlabeled text - contrastive estimation outperforms EM ( with the same feature set ) , is more robust to degradations of the dictionary , and can largely recover by modeling additional features . ... each iteration , solve a supervised problem ( see Tab . \" ...", "label": "", "metadata": {}, "score": "43.64667"}
{"text": "In the experiments reported here , trigram LMs are used unless otherwise stated .Initially we investigated MMIE training using the 18 hour BBN - defined Minitrain corpus with an HMM set using 3088 speech states and 12 Gaussian / state HMMs , which were our best MLE trained models .", "label": "", "metadata": {}, "score": "43.711357"}
{"text": "In the experiments reported here , trigram LMs are used unless otherwise stated .Initially we investigated MMIE training using the 18 hour BBN - defined Minitrain corpus with an HMM set using 3088 speech states and 12 Gaussian / state HMMs , which were our best MLE trained models .", "label": "", "metadata": {}, "score": "43.711357"}
{"text": "However , the inclusion of the MLE system outputs gives a 0.2 % WER absolute improvement .The final error rate from the system ( 25.4 % ) was lowest in the evaluation by a statistically significant margin .A further run on eval98 was performed to investigate the effect of using a combined MMIE / MLE system .", "label": "", "metadata": {}, "score": "43.78222"}
{"text": "This is because the probes on the microarray are designed to target specific genomic sequences and , if a mutation occurs in the target sequence , the probes will be unable to bind to the DNA .The G - HMM and R - HMM are highly sensitive to the outlier measurements caused by SNPs in this region .", "label": "", "metadata": {}, "score": "43.8743"}
{"text": "However , when estimating the parameters of HMM - based speech recognisers , the true data source is not an HMM and therefore other training objective functions , in particular those that involve discriminative training , are of interest .During MLE training , model parameters are adjusted to increase the likelihood of the word strings corresponding to the training utterances without taking account of the probability of other possible word strings .", "label": "", "metadata": {}, "score": "43.90442"}
{"text": "In our implementation , these phone marked lattices also encode the LM probabilities used in MMIE training which again may be different to the LM used to generate the original word - level lattices .This stage typically took about 2xRT to generate triphone - marked lattices for the experiments in Section 6 , although the speed of this process could be considerably increased .", "label": "", "metadata": {}, "score": "44.16397"}
{"text": "In our implementation , these phone marked lattices also encode the LM probabilities used in MMIE training which again may be different to the LM used to generate the original word - level lattices .This stage typically took about 2xRT to generate triphone - marked lattices for the experiments in Section 6 , although the speed of this process could be considerably increased .", "label": "", "metadata": {}, "score": "44.16397"}
{"text": "Additionally , it is particularly appropriate 7 .Page 9 .Simulation experiments with these algorithms are provided in Section 4.1 .We first review briefly the algorithms of Papaspiliopoulos & Roberts ( 2008 ) and Walker ( 2007 ) .The retrospective algorithm works with the parametrisation of the MDP model in terms of ( k , v , z ) ( see the discussion in Section 2 ) .", "label": "", "metadata": {}, "score": "44.498672"}
{"text": "Rabiner [ 1](1 .html ) defined three main problems for HMM models : .The Evaluation problem can be efficiently solved using the Forward algorithm .The Decoding problem can be efficiently solved using the Viterbi algorithm .The Learning problem can be efficiently solved using the Baum - Welch algorithm .", "label": "", "metadata": {}, "score": "44.755234"}
{"text": "Central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectations to be efficiently approximated in a gradient - based procedure .We c ... \" .We demonstrate that log - linear grammars with latent variables can be practically trained using discriminative methods .", "label": "", "metadata": {}, "score": "44.764084"}
{"text": "We com - pare its performance and training time to an HMM , a CRF , an MEMM , and pseudolike - lihood on a shallow parsing task .These ex - periments help tease apart the contributions of rich features and discriminative training , which are shown to be more than additive . ...", "label": "", "metadata": {}, "score": "44.893543"}
{"text": "In our implementation , the full variance transform was computed after standard mean and variance maximum likelihood linear regression ( MLLR ) .Typically a WER reduction of 0.5 % to 0.8 % was obtained .However as a side effect , we found that there were reduced benefits from multiple iterations of MLLR when used with a full variance transform .", "label": "", "metadata": {}, "score": "45.156662"}
{"text": "The use of the MSU Swb1 training transcriptions for language modelling purposes raised certain issues .First , the average sentence length was 11.3 words compared to 9.5 words on the LDC transcripts that we previously used .This has the effect that LMs trained on the MSU transcripts have a higher test - set perplexity which is mainly due to the reduced probability of the sentence - end symbol .", "label": "", "metadata": {}, "score": "45.28068"}
{"text": "( 2006 ) using the MDP - HMM and a standard HMM with Gaussian observations ( G - HMM ) .The data set consists of approximately 84,000 probes measurements from a DNA sample derived from a tumour generated in a mouse model of liver cancer compared to normal ( non - tumour )", "label": "", "metadata": {}, "score": "45.459427"}
{"text": "While MMIE is very effective at reducing training set error a key issue is generalisation to test data .It is very important that the confusable data generated during training ( as found from the posterior distribution of state occupancy for the recognition lattice ) is representative to ensure good generalisation .", "label": "", "metadata": {}, "score": "45.603935"}
{"text": "We compare L1 and L2 regularization and show that L1 regularization is superior , requiring fewer iterations to converge , and yielding sparser solutions .On full - scale treebank parsing experiments , the discriminative latent models outperform both the comparable generative latent models as well as the discriminative non - latent baselines . ... , object NPs , and so on .", "label": "", "metadata": {}, "score": "45.608246"}
{"text": "The rest of the paper is arranged as follows .First an overview of the 1998 HTK system is given .This is followed by a description of the data sets used in the experiments and then by sections that discuss each of the major new features of the system .", "label": "", "metadata": {}, "score": "45.630707"}
{"text": "Table 6 lists the evaluation system performance on the March 2000 evaluation set .The performance on eval00 gives a similar per stage improvement to that obtained for eval98 .However the absolute WER levels are reduced by about 10 % absolute .", "label": "", "metadata": {}, "score": "45.67895"}
{"text": "In contrast to MLE , discriminative training schemes , such as Maximum Mutual Information Estimation ( MMIE ) which is the focus of this paper , take account of possible competing word hypotheses and try and reduce the probability of incorrect hypotheses .", "label": "", "metadata": {}, "score": "45.947136"}
{"text": "We describe a new loss function , due to Jeon and Lin ( 2006 ) , for estimating structured log - linear models on arbitrary features .The loss function can be seen as a ( generative ) alternative to maximum likelihood estimation with an interesting information - theoretic interpretation , and it is statistical ... \" .", "label": "", "metadata": {}, "score": "45.966484"}
{"text": "The average error of the created triphones is lower in the filter and cepstmm domains than for formants .This is explained to be caused by limitations in the Analysis - bySynthesis formant tracking algorithm .A small improvement with the proposed technique is achieved for all representations in the task of reordering N - best sentence recognition candidate lists .", "label": "", "metadata": {}, "score": "45.966522"}
{"text": "The average error of the created triphones is lower in the filter and cepstmm domains than for formants .This is explained to be caused by limitations in the Analysis - bySynthesis formant tracking algorithm .A small improvement with the proposed technique is achieved for all representations in the task of reordering N - best sentence recognition candidate lists .", "label": "", "metadata": {}, "score": "45.966522"}
{"text": "We describe a novel training criterion for probabilistic grammar induction models , contrastive estimation [ Smith and Eisner , 2005 ] , which can be interpreted as exploiting implicit negative evidence and includes a wide class of likelihood - based objective functions .", "label": "", "metadata": {}, "score": "46.0344"}
{"text": "We describe a novel training criterion for probabilistic grammar induction models , contrastive estimation [ Smith and Eisner , 2005 ] , which can be interpreted as exploiting implicit negative evidence and includes a wide class of likelihood - based objective functions .", "label": "", "metadata": {}, "score": "46.0344"}
{"text": "These lattices were expanded expanded to contain language model probabilities generated by the interpolation of the word 4-gram and the class trigram .Subsequent passes rescored these lattices and operated in two branches : a branch using GI MMIE trained models ( branch ' ' a ' ' ) and a branch using GD , soft - tied , MLE models ( branch ' ' b ' ' ) .", "label": "", "metadata": {}, "score": "46.08699"}
{"text": "This paper describes a lattice - based framework for maximum mutual information estimation ( MMIE ) of HMM parameters which has been used to train HMM systems for conversational telephone speech transcription using up to 265 hours of training data .These experiments represent the largest - scale application of discriminative training techniques for speech recognition of which the authors are aware , and have led to significant reductions in word error rate for both triphone and quinphone HMMs compared to our best models trained using maximum likelihood estimation .", "label": "", "metadata": {}, "score": "46.401596"}
{"text": "Page 15 .In terms of updating the hidden states s , the use of forward - backward sampling gives a distinct ad- vantage over the local updates .This replicates previous findings by Scott ( 2002 ) who showed that forward - backward Gibbs sampling for Hidden Markov Models mix faster than using local updates as it is difficult to move from one configuration of s to another configuration of entirely different structure using local updates only .", "label": "", "metadata": {}, "score": "46.404034"}
{"text": "We have considered two methods of improving generalisation that both increase the amount of confusable data processed during training : weaker language models and acoustic model scaling .In [ 8 ] it was shown that improved test - set performance could be obtained using a unigram LM during MMIE training , even though a bigram or trigram was used during recognition .", "label": "", "metadata": {}, "score": "46.43429"}
{"text": "We have considered two methods of improving generalisation that both increase the amount of confusable data processed during training : weaker language models and acoustic model scaling .In [ 8 ] it was shown that improved test - set performance could be obtained using a unigram LM during MMIE training , even though a bigram or trigram was used during recognition .", "label": "", "metadata": {}, "score": "46.43429"}
{"text": "A range of new features have been added to the HTK system used in the 1998 Hub5 evaluation , and the changes taken together have resulted in an 11 % relative decrease in word error rate on the 1998 evaluation test set .", "label": "", "metadata": {}, "score": "46.630676"}
{"text": "The method uses novel MCMC methodology which combines recent retrospective sampling methods with the use of slice sampler variables .The methodology is computationally efficient , both in terms of MCMC mixing properties , and robustness to the length of the time series being investigated .", "label": "", "metadata": {}, "score": "46.746075"}
{"text": "In these models different levels of the hierarchy correspond to structures on different length scales in the text .In the second application we demonstrate how HHMMs can b ..We consider the development of Bayesian Nonparametric methods for product partition models such as Hidden Markov Models and change point models .", "label": "", "metadata": {}, "score": "46.76624"}
{"text": "This study mainly focuses on the automatic evaluation of mispronunciations at a phonetic level .We describe a new database we have collected for this work .Then we report the reliability of several phonetic scores to locate automatically segmental problems in student utterances . .", "label": "", "metadata": {}, "score": "46.866554"}
{"text": "This study mainly focuses on the automatic evaluation of mispronunciations at a phonetic level .We describe a new database we have collected for this work .Then we report the reliability of several phonetic scores to locate automatically segmental problems in student utterances . .", "label": "", "metadata": {}, "score": "46.866554"}
{"text": "the user during the enrolment call , and changing prompt texts to fit the application .StarLite was developed during the WAXHOLM project , where it was used for a medium - size vocabulary recognition task .It has later also been used in the Gulan ( Sj\u00f6lander & Gustafson , 1997 ) and the Au ... . \" ...", "label": "", "metadata": {}, "score": "46.97114"}
{"text": "the user during the enrolment call , and changing prompt texts to fit the application .StarLite was developed during the WAXHOLM project , where it was used for a medium - size vocabulary recognition task .It has later also been used in the Gulan ( Sj\u00f6lander & Gustafson , 1997 ) and the Au ... . \" ...", "label": "", "metadata": {}, "score": "46.97114"}
{"text": "This section develops an appropriate methodology and shows that it achieves these three goals .Further empirical evidence is provided in Section 4.2 .The methodology we develop has two im- portant by - products which have interest outside the scope of this paper .", "label": "", "metadata": {}, "score": "47.09217"}
{"text": "Page 18 .A Bayesian semiparametric model for random - effects meta - analysis .J. Am .Statist .Assoc .100 , 242 - 51 .Cappe , . , Moulines , E. & Ryden , T. ( 2005 ) .", "label": "", "metadata": {}, "score": "47.352783"}
{"text": "This thesis is about estimating probabilistic models to uncover useful hidden structure in data ; specifically , we address the problem of discovering syntactic structure in natural language text .We present three new parameter estimation techniques that generalize the standard approach , maximum likelihood estimation , in different ways .", "label": "", "metadata": {}, "score": "47.425873"}
{"text": "Skewed deterministic annealing locally maximizes likelihood using a cautious parameter search strategy that starts with an easier optimization problem than likelihood , and iteratively moves to harder problems , culminating in likelihood .Structural annealing is similar , but starts with a heavy bias toward simple syntactic structures and gradually relaxes the bias .", "label": "", "metadata": {}, "score": "47.441536"}
{"text": "The results in Table 2 and again show the best performance after two MMIE iterations .Furthermore the gain over the MLE system is 1.7 % absolute if a bigram LM is used and 1.9 % absolute if a unigram LM is used : the 6 Gaussian per state MMIE - trained HMM set now slightly outperforms the 12 Gaussian system .", "label": "", "metadata": {}, "score": "47.45703"}
{"text": "The results in Table 2 and again show the best performance after two MMIE iterations .Furthermore the gain over the MLE system is 1.7 % absolute if a bigram LM is used and 1.9 % absolute if a unigram LM is used : the 6 Gaussian per state MMIE - trained HMM set now slightly outperforms the 12 Gaussian system .", "label": "", "metadata": {}, "score": "47.45703"}
{"text": "Using the G - HMM is able to identify this known copy number variant , however , it also detects many additional copy number variants on this chromosome most of which must be false positives .( c )The R - HMM reduces the number of false positives but ( d ) the MDP - HMM identifies only the known copy number variant and no other copy number alterations on this chromosome .", "label": "", "metadata": {}, "score": "47.489525"}
{"text": "Table 2 shows the number of clustered speech states and the number of Gaussians per state for each of these systems as well as word error rates on eval97sub .An initial 3.5-fold increase in the amount of training data results in a 4.6 % absolute reduction in word error rate ( WER ) .", "label": "", "metadata": {}, "score": "47.546112"}
{"text": "We describe a new loss function , due to Jeon and Lin ( 2006 ) , for estimating structured log - linear models on arbitrary features .The loss function can be seen as a ( generative ) al - ternative to maximum likelihood estimation with an interesting information - theoretic in - terpretation , and it is statistically consis - tent .", "label": "", "metadata": {}, "score": "47.63731"}
{"text": "The easiest way to simulate from this constrained distribution is by single site Gibbs sampling of the vjs .This single - site Gibbs sampling tends to be slowly mixing and deteriorating with T. Our method updates u and v in a single block , by first updating v from its marginal ( with respect to u ) according to ( 3 ) and consequently u conditionally on v as described above .", "label": "", "metadata": {}, "score": "47.795525"}
{"text": "However , these measurements are extremely sensitive to variations in DNA quality , DNA quantity and instrumental noise and this has lead to the development of a number of statistical methods for data analysis .However , many data sets contain non - Gaussian noise distributions on the measurements , as pointed out in Hu et al .", "label": "", "metadata": {}, "score": "47.918404"}
{"text": "The bigram 1-best hypotheses had a 24.6 % word error rate ( WER ) and a Lattice WER ( LWER ) of 6.2 % .The Minitrain 12 Gaussian / state results given in Table 1 compare acoustic and language model scaling for several iterations of MMIE training on the eval97sub test set ( a subset of the 1997 Hub5 evaluation ) .", "label": "", "metadata": {}, "score": "47.95575"}
{"text": "The bigram 1-best hypotheses had a 24.6 % word error rate ( WER ) and a Lattice WER ( LWER ) of 6.2 % .The Minitrain 12 Gaussian / state results given in Table 1 compare acoustic and language model scaling for several iterations of MMIE training on the eval97sub test set ( a subset of the 1997 Hub5 evaluation ) .", "label": "", "metadata": {}, "score": "47.95575"}
{"text": "This paper describes the new system features and gives the results of each processing stage for both the 1998 and 2000 evaluation sets .The transcription of conversational telephone speech is one of the most challenging tasks for speech recognition technology with state - of - the - art systems yielding high word error rates .", "label": "", "metadata": {}, "score": "47.956104"}
{"text": "In contrast to MLE , discriminative training schemes , such as Maximum Mutual Information Estimation ( MMIE ) take account of possible competing word hypotheses and try to reduce the probability of incorrect hypotheses .The objective function to maximise in MMIE is the posterior probability of the true word transcriptions given the training data .", "label": "", "metadata": {}, "score": "48.157032"}
{"text": "Stage P2 used MMIE GI triphones to generate the transcriptions for unsupervised test - set MLLR adaptation [ 8 , 3 ] with a 4-gram LM .A global transform ( A ' ' global transform ' ' denotes one transform for speech and a separate transform for silence . ) for the means ( block - diagonal ) and variances ( diagonal ) was computed for each side .", "label": "", "metadata": {}, "score": "48.279583"}
{"text": "The last four lines in the hierarchy identify P with the Dirichlet process prior ( DPP ) with base measure H\u03b8and the concentration parameter \u03b1 .Such mixture models are known as mixtures of Dirichlet process ( MDP ) .The conditional distribution of y given the HMM state is specified as a mixture 4 .", "label": "", "metadata": {}, "score": "48.49237"}
{"text": "In these equations , the and are sums of data and squared data respectively , weighted by occupancy , for mixture component of state , and the Gaussian occupancies ( summed over time ) are .The superscripts and refer to the model corresponding to the correct word sequence , and the recognition model for all word sequences , respectively .", "label": "", "metadata": {}, "score": "48.6416"}
{"text": "In these equations , the and are sums of data and squared data respectively , weighted by occupancy , for mixture component of state , and the Gaussian occupancies ( summed over time ) are .The superscripts and refer to the model corresponding to the correct word sequence , and the recognition model for all word sequences , respectively .", "label": "", "metadata": {}, "score": "48.6416"}
{"text": "The results of MMIE trained quinphones on the eval97sub set are shown in Table 5 .Note that these experiments , unlike all previous ones reported here , include pronunciation probabilities .As with the MMIE training runs discussed above , the largest WER reduction ( 2.1 % absolute ) comes after two iterations of training .", "label": "", "metadata": {}, "score": "48.714447"}
{"text": "The results of MMIE trained quinphones on the eval97sub set are shown in Table 5 .Note that these experiments , unlike all previous ones reported here , include pronunciation probabilities .As with the MMIE training runs discussed above , the largest WER reduction ( 2.1 % absolute ) comes after two iterations of training .", "label": "", "metadata": {}, "score": "48.714447"}
{"text": "It is straightforward ( a single line change in the code ) to allow more general stick - breaking priors , as for example the two - parameter Poisson-Dirichlet process ( Ishwaran & James , 2001 ) .It is also simple to allow the joint analysis of various series simultaneously using a hierarchical model .", "label": "", "metadata": {}, "score": "48.860085"}
{"text": "While MMIE training often greatly reduces training set error from an MLE baseline , the reduction in error rate on an independent test set is normally much less , i.e. , compared to MLE , the generalisation performance is poorer .Furthermore , as with all statistical modelling approaches , the more complex the model , the poorer the generalisation .", "label": "", "metadata": {}, "score": "48.907265"}
{"text": "While MMIE training often greatly reduces training set error from an MLE baseline , the reduction in error rate on an independent test set is normally much less , i.e. , compared to MLE , the generalisation performance is poorer .Furthermore , as with all statistical modelling approaches , the more complex the model , the poorer the generalisation .", "label": "", "metadata": {}, "score": "48.907265"}
{"text": "The HTK system used in the 1998 Hub5 evaluation served as the basis for development .In this section a short overview of its features is given ( see [ 6 ] for details ) .The system uses perceptual linear prediction cepstral coefficients derived from a mel - scale filterbank ( MF - PLP ) [ 18 ] covering the frequency range from 125Hz to 3.8kHz .", "label": "", "metadata": {}, "score": "48.9964"}
{"text": "The results in Table 3 show that again the peak improvement comes after two iterations , but there is an even larger reduction in WER : 2.3 % absolute on eval97sub and 1.9 % absolute on eval98 .The word error rate for the 1-best hypothesis from the original bigram word lattices measured on 10 % of the training data was 27.4 % .", "label": "", "metadata": {}, "score": "48.998116"}
{"text": "The results in Table 3 show that again the peak improvement comes after two iterations , but there is an even larger reduction in WER : 2.3 % absolute on eval97sub and 1.9 % absolute on eval98 .The word error rate for the 1-best hypothesis from the original bigram word lattices measured on 10 % of the training data was 27.4 % .", "label": "", "metadata": {}, "score": "48.998116"}
{"text": "For training observation sequences with corresponding transcriptions , the MMIE objective function is given by .where is the composite model corresponding to the word sequence and is the probability of this sequence as determined by the language model .The summation in the denominator of ( 1 ) is taken over all possible word sequences allowed in the task and it can be replaced by .", "label": "", "metadata": {}, "score": "49.222473"}
{"text": "For training observation sequences with corresponding transcriptions , the MMIE objective function is given by .where is the composite model corresponding to the word sequence and is the probability of this sequence as determined by the language model .The summation in the denominator of ( 1 ) is taken over all possible word sequences allowed in the task and it can be replaced by .", "label": "", "metadata": {}, "score": "49.222473"}
{"text": "495656581383351 E - 34 0 .465660634827592 E - 33 2 .8603899591778015 E - 36 .14 / 03 / 22 09 : 52 : 21 INFO driver .MahoutDriver : Program took 180 ms ( Minutes : 0 .The model trained with the input set now is in the file ' hmm - model ' , which we can use to build a predicted sequence .", "label": "", "metadata": {}, "score": "49.522793"}
{"text": "The output of the respective branches served as the adaptation supervision to stage P5a / P5b .These were as P4a / P4b but were based on quinphone acoustic models .Finally for the MMIE branch only , a pass with two MLLR transforms was run ( P6a ) .", "label": "", "metadata": {}, "score": "49.59736"}
{"text": "The methodology is computationally efficient , both in terms of MCMC mixing properties , and robustness to the length of the time series be- ing investigated .Moreover , the method is easy to implement requiring little or no user - interaction .", "label": "", "metadata": {}, "score": "49.647827"}
{"text": "We made a number of changes to these manual corrections and also automatically removed more than 30 hours of silence data at segment boundaries .An important feature of the MSU transcripts is the full - word transcription of false starts and mispronunciations .", "label": "", "metadata": {}, "score": "49.679085"}
{"text": "J. Am .Statist .Assoc 90 577 - 88 .Gelfand , A. & Kottas , A. ( 2003 ) .Bayesian semiparametric regression for median residual life .Scand .J. Statist .Green , P. & Richardson , S. ( 2001 ) .", "label": "", "metadata": {}, "score": "49.910954"}
{"text": "The alternative ( and in principle more flexible ) methodology is the conditional method , which does not require analytical integration of the DP .This approach was suggested in Ishwaran & Zarepour ( 2000 ) ; Ishwaran & James ( 2001 , 2003 ) where finite - dimensional truncations are employed to circumvent the impossibe task of storing the entire Dirichlet process state ( which would require infinite storage capacity ) .", "label": "", "metadata": {}, "score": "49.927563"}
{"text": "Furthermore while the overall performance of the system is significantly enhanced by the use of MMIE models , the complete pure MLE system achieves a 36.8 % WER on eval98 .This paper has discussed the substantial improvements in system performance that have been made to our Hub5 transcription system since the 1998 evaluation .", "label": "", "metadata": {}, "score": "49.986076"}
{"text": "pdf . Y. , Jordan , processes .M. , to Beal , appear M. & Blei , D. ( 2006 ) .Assoc . , Hierarchical availablein J.Amer .Statist.fromWalker , S. ( 2007 ) .Sampling the dirichlet mixture model with slices .", "label": "", "metadata": {}, "score": "50.02458"}
{"text": "J. Comp .Graph .Statist .Mouse genomic representational oligonucleotide microarray analysis : detection of copy number vari- ations in normal and tumor specimens .Proc Natl Acad Sci U S A 103 11234 - 11239 .Liu , J. S. ( 1996 ) .", "label": "", "metadata": {}, "score": "50.036236"}
{"text": "If the value set is too large then training is very slow ( but stable ) and if it is too small the updates may not increase the objective function on each iteration .A useful lower bound on is the value which ensures that all variances remain positive .", "label": "", "metadata": {}, "score": "50.18924"}
{"text": "If the value set is too large then training is very slow ( but stable ) and if it is too small the updates may not increase the objective function on each iteration .A useful lower bound on is the value which ensures that all variances remain positive .", "label": "", "metadata": {}, "score": "50.18924"}
{"text": "This has been enabled by microarray technology that has enabled copy number variation across the genome to be routinely profiled using array comparative genomic hybridisation ( aCGH )meth- ods .These technologies allow DNA copy number to be measure at millions of genomic locations simultaneously allowing copy number variants to be mapped with high resolution .", "label": "", "metadata": {}, "score": "50.231846"}
{"text": "Finally all dictionary probabilities are renormalised so that the pronunciation for each word which has the highest probability is set to one .During recognition the ( log ) pronunciation probabilities are scaled by the same factor as used for the language model .", "label": "", "metadata": {}, "score": "50.425163"}
{"text": "Table 5 gives results for each processing stage for the 1998 evaluation set .The large difference ( 6.8 % absolute in WER ) between the P1 and P2 results is due to the combined effects of VTLN , MMIE models on the new training set , the larger vocabulary and a 4-gram LM .", "label": "", "metadata": {}, "score": "50.802658"}
{"text": "This data sets shows a copy number gain ( duplication ) and a copy number loss ( deletion ) which are characterised by relative upward and downward shifts in the log intensity ratio respectively .et al .( 2007 ) ) .", "label": "", "metadata": {}, "score": "50.80878"}
{"text": "All training and simulation software used is made freely available by the author , and detailed information about the software and the training process is given in an Appendix . ... the HTK Toolkit ( Young et al , 1999 ) with speech from the Swedish tele109Speech , Music and Hearing phone speech database SpeechDat ( Elenius , 2000 ) .", "label": "", "metadata": {}, "score": "50.812588"}
{"text": "All training and simulation software used is made freely available by the author , and detailed information about the software and the training process is given in an Appendix . ... the HTK Toolkit ( Young et al , 1999 ) with speech from the Swedish tele109Speech , Music and Hearing phone speech database SpeechDat ( Elenius , 2000 ) .", "label": "", "metadata": {}, "score": "50.812588"}
{"text": "This is clearly undesirable .Although we have only given a heuristic argument , a formal proof is feasible .This determines the cost for each likelihood calculation , and since to carry out the 4 Simulation experiments In this section , we compare rival MCMC schemes as described in Section 3 .", "label": "", "metadata": {}, "score": "51.040016"}
{"text": "Table 5 : % WER and normalised cross entropy ( NCE ) values on eval98 for all stages of the evaluation system .The final system output is a combination of P4a , P4b , P6a and P5b . ''no FV ' ' denotes system output without full variance transform . ''", "label": "", "metadata": {}, "score": "51.071693"}
{"text": "According to the terminology of Section 1 we deal with a conditional method for MDP posterior simulation since the random measure ( z , v ) is imputed and explicitly updated .The algorithm we propose is a synthesis of the retrospective Markov chain Monte Carlo algorithm of Papaspiliopoulos & Roberts ( 2008 ) and the slice Gibbs sampler of Walker ( 2007 ) .", "label": "", "metadata": {}, "score": "51.09624"}
{"text": "For large vocabulary tasks , especially on large datasets there are two main problems : generalisation to unseen data in order to increase test - set performance over MLE ; and providing a viable computation framework to estimate confusable hypotheses and perform parameter estimation .", "label": "", "metadata": {}, "score": "51.098656"}
{"text": "Section 3.2 discusses a variety of possible alternative schemes and argues why they would lead to failings in some of the three requirements we have specified .The rest of the Section 3.1 Block Gibbs sampling for HMM - MDP We will sample from the joint posterior distribution of ( s , u , v , z , k , \u03b1 ) by block Gibbs sampling according to the following conditional distributions : 1 .", "label": "", "metadata": {}, "score": "51.215885"}
{"text": "An Investigation of Frame Discrimination for Continuous Speech Recognition .[ ps ] [ pdf ] Technical Report CUED / F - INFENG / TR.332 , Cambridge University Engineering Dept .Abstract : .This paper describes a lattice - based framework for maximum mutual information estimation ( MMIE ) of HMM parameters which has been used to train HMM systems for conversational telephone speech transcription using up to 265 hours of training data .", "label": "", "metadata": {}, "score": "51.359997"}
{"text": "An alternative to LM scaling is to multiply the acoustic model log likelihood values by the inverse of the LM scale factor ( acoustic model scaling ) .If language model scaling is used , one particular state - sequence tends to dominate the likelihood at any point in time and hence dominates any sums using path likelihoods .", "label": "", "metadata": {}, "score": "51.36835"}
{"text": "An alternative to LM scaling is to multiply the acoustic model log likelihood values by the inverse of the LM scale factor ( acoustic model scaling ) .If language model scaling is used , one particular state - sequence tends to dominate the likelihood at any point in time and hence dominates any sums using path likelihoods .", "label": "", "metadata": {}, "score": "51.36835"}
{"text": "While this architecture results in a complex overall system , this section also reports the results of each of the stages .This allows the performance of many system variants at different levels of complexity to be assessed .The VTLN acoustic models used in the system were either triphones ( 6165 speech states/16 Gaussians per state ) or quinphones ( 9640 states/16 Gaussians per state ) trained on h5train00 .", "label": "", "metadata": {}, "score": "51.42039"}
{"text": "Working as in Appendix 1 , it is easy to see that a version of Proposition 1 still holds , 9 .Page 11 .Therefore , the likelihoods associated with each HMM state are not directly computable due to the infinite summation .", "label": "", "metadata": {}, "score": "51.434177"}
{"text": "Page 21 .We show autocorrelation plots which correspond to three different functions in the parameter space : the number of clusters ( left ) , the deviance ( middle ) and zk3(right ) .Page 22 .( a ) lepto 1000 , ( b ) bimod 1000 and ( c ) trimod 1000 .", "label": "", "metadata": {}, "score": "51.58631"}
{"text": "The goal is to recover high - score sentence hypotheses that may have been pruned halfway during the forward search due to the delayed use of LM .For the application of Hong Kong stock information inquiry , the proposed technique shows a noticeable performance improvement .", "label": "", "metadata": {}, "score": "51.6028"}
{"text": "The goal is to recover high - score sentence hypotheses that may have been pruned halfway during the forward search due to the delayed use of LM .For the application of Hong Kong stock information inquiry , the proposed technique shows a noticeable performance improvement .", "label": "", "metadata": {}, "score": "51.6028"}
{"text": "For this work , two methods were investigated : the use of acoustic scaling and a weakened language model .Normally the language model probability and the acoustic model likelihoods are combined by scaling the language model log probabilities .This situation leads to a very large dynamic range in the combined likelihoods and a very sharp posterior distribution in the denominator of ( 1 ) .", "label": "", "metadata": {}, "score": "51.67277"}
{"text": "The most successful scheme used three separate dictionary entries for each real pronunciation which differed by the word - end silence type : a no silence version ; adding a short pause preserving cross - word context ; and a general silence model altering context .", "label": "", "metadata": {}, "score": "51.915108"}
{"text": "Bioinformatics Khojasteh , M. , Lam , W. L. , Ng , K. analysisa robust 22 e431-e439 .URL Stjernqvist , S. , Rydn , T. , Skld , M. & Staaf , J. ( 2007 ) .markov modelling of array cgh copy number data .", "label": "", "metadata": {}, "score": "51.951797"}
{"text": "Thus , we consider each type of update separately .Page 8 .This is facilitated by the following key result which is proved in Appendix 1 .Proposition 1 .This relates to the fact that the number of new components generated by the Dirichlet process grows logarithmically with the size of the data ( Antoniak , 1974 ) .", "label": "", "metadata": {}, "score": "52.05318"}
{"text": "All run times are measured on an Intel Pentium III running at 550MHz .The second step is to generate phone - marked lattices which label each word lattice arc with a phone / model sequence and the Viterbi segmentation points .", "label": "", "metadata": {}, "score": "52.07202"}
{"text": "All run times are measured on an Intel Pentium III running at 550MHz .The second step is to generate phone - marked lattices which label each word lattice arc with a phone / model sequence and the Viterbi segmentation points .", "label": "", "metadata": {}, "score": "52.07202"}
{"text": "Indeed , this is illustrated in the simulation study of Section 4.2 .A competing conditional algorithm is obtained by integrating out u from the model and working with the parametrisation of the DPP in terms of ( k , v , z ) as in Papaspiliopoulos & Roberts ( 2008 ) .", "label": "", "metadata": {}, "score": "52.12211"}
{"text": "While they contain the most - likely paths , a significant part of the ' ' tail ' ' of the overall posterior distribution is missing .To compensate for this a decision tree was trained to map the estimates to confidence scores .", "label": "", "metadata": {}, "score": "52.312763"}
{"text": "Hence , we integrate out the global allocation variables k in the update of the local allocation variables s. As a result the algorithm does not get trapped in secondary modes which correspond to mis - classification of consecutive data to Dirichlet mixture components .", "label": "", "metadata": {}, "score": "52.328346"}
{"text": "Figure 2 summarizes the comparison between the competing approaches .We show autocorrela- tion plots for three different functions in the parameter space : the number of clusters , the deviance of the fit ( see Papaspiliopoulos & Roberts ( 2008 ) for its calculation ) and zk3 .", "label": "", "metadata": {}, "score": "52.445023"}
{"text": "Cepstral mean subtraction and variance normalisation are performed for each conversation side .Vocal tract length normalisation ( VTLN ) was applied in both training and test .The acoustic modelling used cross - word triphone and quinphone hidden Markov models ( HMMs ) trained using conventional maximum likelihood estimation .", "label": "", "metadata": {}, "score": "52.634018"}
{"text": "The implementation we have used is rather different to the one in [ 16 ] and does a full forward - backward pass constrained by ( a margin around ) the phone boundary times that make up each lattice arc .Furthermore the smoothing constant in the EBW equations is computed on a per - Gaussian basis for fast convergence and a novel weight update formulation used .", "label": "", "metadata": {}, "score": "52.675034"}
{"text": "Mixture Gaussian distributions for each tied state were then trained using sentence - level Baum - Welch estimation and iterative mixture splitting [ 20 ] .After gender independent ( GI ) models had been trained , a final training iteration using gender - specific training data and updating only the means and mixture weights was performed to estimate gender dependent ( GD ) model sets .", "label": "", "metadata": {}, "score": "53.135063"}
{"text": "Discriminative schemes have been widely used in small vocabulary recognition tasks , where the relatively small number of competing hypotheses makes training viable .For large vocabulary tasks , especially on large datasets there are two main problems : generalisation to unseen data in order to increase test - set performance over MLE ; and providing a viable computation framework to estimate confusable hypotheses and perform parameter estimation .", "label": "", "metadata": {}, "score": "53.211086"}
{"text": "Sets of experiments for evaluating the techniques on conversational telephone speech transcription are presented that show how MMIE training can be successfully applied over a range of training set sizes ; the effect of methods to improve generalisation ; and the interaction of MMIE with maximum - likelihood adaptation .", "label": "", "metadata": {}, "score": "53.305717"}
{"text": "Sets of experiments for evaluating the techniques on conversational telephone speech transcription are presented that show how MMIE training can be successfully applied over a range of training set sizes ; the effect of methods to improve generalisation ; and the interaction of MMIE with maximum - likelihood adaptation .", "label": "", "metadata": {}, "score": "53.305717"}
{"text": "The update of k is done as in the slice Gibbs sampler , and the update of z as described earlier .When a gamma prior is used for \u03b1 , its conditional distribution given k and marginal with respect to the rest is a mixture of gamma distributions and can be simulated as described in Escobar & West ( 1995 ) .", "label": "", "metadata": {}, "score": "53.60059"}
{"text": "It should be noted that optimisation of ( 1 ) requires the maximisation of the numerator term , which is identical to the MLE objective function , while simultaneously minimising the denominator term .3 EXTENDED BAUM - WELCH ALGORITHM .The most effective method to optimise the MMIE objective function for large data and model sets is the Extended Baum - Welch ( EBW ) algorithm [ 3 ] as applied to Gaussian mixture HMMs [ 6 ] .", "label": "", "metadata": {}, "score": "53.63435"}
{"text": "It should be noted that optimisation of ( 1 ) requires the maximisation of the numerator term , which is identical to the MLE objective function , while simultaneously minimising the denominator term .3 EXTENDED BAUM - WELCH ALGORITHM .The most effective method to optimise the MMIE objective function for large data and model sets is the Extended Baum - Welch ( EBW ) algorithm [ 3 ] as applied to Gaussian mixture HMMs [ 6 ] .", "label": "", "metadata": {}, "score": "53.63435"}
{"text": "The R - HMM provides much more conservative and realistic estimates of the number of putative copy number variants in the tumour , however , the MDP - HMM identifies the known deletion only whilst the R - HMM still produces many additional copy number variants whose existence can not be confirmed .", "label": "", "metadata": {}, "score": "53.645428"}
{"text": "The adapted models were then used for a second full - decode pass .The results in Table 6 show that the MMIE models are 2.1 % absolute better than the MLE models without MLLR , and 2.2 % better with MLLR .", "label": "", "metadata": {}, "score": "53.713985"}
{"text": "The adapted models were then used for a second full - decode pass .The results in Table 6 show that the MMIE models are 2.1 % absolute better than the MLE models without MLLR , and 2.2 % better with MLLR .", "label": "", "metadata": {}, "score": "53.713985"}
{"text": "We use circular binary segmentation to segment the minimum distance and maximum a posteriori estimation to infer de novo CNVs from the segmented genome .Compared to PennCNV on simulated data , MinimumDistance identifies fewer false positives on average and is comparable to PennCNV with respect to false negatives .", "label": "", "metadata": {}, "score": "53.835"}
{"text": "A Post - Processing System to Yield Reduced Word Error Rates : Recogniser Output Voting Error Reduction ( ROVER ) .Proc .IEEE Workshop on Automatic Speech Recognition and Understanding , pp .347 - 354 , Santa Barbara .P.S. Gopalakrishnan , D. Kanevsky , A. Nadas & D. Nahamoo ( 1991 ) .", "label": "", "metadata": {}, "score": "54.03936"}
{"text": "Here we consider Bayesian nonparametric extensions where the sampling density ( likelihood ) within a state or partition is given by a Mixture of Dirichlet Process ( Antoniak , 1974 ; Escobar , 1988 ) .Conventional constructions of the MDP make inference extremely challenging computationally due to the joint dependence structure induced on the observations .", "label": "", "metadata": {}, "score": "54.26097"}
{"text": "5.2Posterior Inference We analysed the mouse ROMA dataset using the MDP - HMM and two additional HMM - based models .The first is a standard HMM model with Gaussian distributed observations ( that we should 14 .Page 16 .These two latter models are representative of currently available HMM - based methods for analysing aCGH datasets and can be considered to be special cases of the more general MDP - HMM .", "label": "", "metadata": {}, "score": "54.402885"}
{"text": "5.3 Results Figure 6 shows the analysis of Chromosome 5 for the mouse tumour .The G - HMM , R - HMM and MDP - HMM are both able to identify a deletion found previously in ( Lakshmi et al . , 2006 ) , however , the G - HMM also identifies many other putative copy number variants .", "label": "", "metadata": {}, "score": "55.124172"}
{"text": "Sankhy\u00af a , A , 65 , 577 - 92 .Page 19 .Markov chain Monte Carlo in approximate Dirichlet and beta two - parameter process hierarchical models .Biometrika 87 , 371 - 90 .Jain , S. & Neal , R. M. ( 2004 ) .", "label": "", "metadata": {}, "score": "55.250187"}
{"text": "On other test sets improvements greater than 1 % absolute have also been obtained and size of the gains is found to be fairly independent of the complexity of the underlying system .A side - dependent block - full variance ( FV ) transformation [ 4 ] , , of the form was investigated .", "label": "", "metadata": {}, "score": "55.40943"}
{"text": "ICASSP'98 , pp .177 - 180 , Seattle .S.J. Young , J.J. Odell & P.C. Woodland ( 1994 ) .Tree - Based State Tying for High Accuracy Acoustic Modelling .[ ps ] [ pdf ] Proc .1994 ARPA Human Language Technology Workshop , pp .", "label": "", "metadata": {}, "score": "55.606255"}
{"text": "So far we have considered conditional algorithms , which impute and update the random measure ( z , v ) ( using retrospective sampling ) .We first argue why we prefer conditional methods in this context .The marginal methods integrate out analytically the random weights w from the model and update the rest of the variables .", "label": "", "metadata": {}, "score": "55.609356"}
{"text": "A path through the graph is found by choosing one of the alternatives from each confusion set .By picking the word with the highest posterior from each set the sentence hypothesis with the lowest overall expected word error rate can be found .", "label": "", "metadata": {}, "score": "55.741924"}
{"text": "Bioinformatics 24 751- 758 .Antoniak , C. E. ( 1974 ) .Mixtures of Dirichlet processes with applications to bayesian nonpara- metric problems .Ann .Statist .Barry , D. & Hartigan , J. A. ( 1992 ) .Product partition models for change point models .", "label": "", "metadata": {}, "score": "55.77278"}
{"text": "Discriminative methods for parsing are not new .However , most discrim ... . \" ...This thesis is about estimating probabilistic models to uncover useful hidden structure in data ; specifically , we address the problem of discovering syntactic structure in natural language text .", "label": "", "metadata": {}, "score": "55.803402"}
{"text": "The HMM - MDP model is defined in Section 2 while the corresponding computa- tional methodology is described in Section 3 .The different models and methods are tested and compared in Section 4 on various simulated data sets .The genomic copy number variation analysis is presented in Section 5 , and brief conclusions are given in Section 6 . 1.1 Motivating Application The development of the Bayesian nonparametric HMM reported here was motivated by on - going work by two of the authors in the analysis of genomic copy number variation ( CNV ) ( see Colella 2 .", "label": "", "metadata": {}, "score": "55.81509"}
{"text": "In preliminary experiments , it was found that the convergence speed could be further improved if was set on a per - Gaussian level , i.e. a Gaussian specific was used .It was set at the maximum of i ) twice the value necessary to ensure positive variance updates for all dimensions of the Gaussian ; or ii ) the denominator occupancy .", "label": "", "metadata": {}, "score": "55.89833"}
{"text": "In preliminary experiments , it was found that the convergence speed could be further improved if was set on a per - Gaussian level , i.e. a Gaussian specific was used .It was set at the maximum of i ) twice the value necessary to ensure positive variance updates for all dimensions of the Gaussian ; or ii ) the denominator occupancy .", "label": "", "metadata": {}, "score": "55.89833"}
{"text": "It was again found that there is a fairly consistent 1 % absolute reduction in WER from confusion networks .A contrast ( not shown in the table ) showed that on P2 the use of MMIE models had given a 2.1 % absolute reduction in WER over the corresponding MLE models .", "label": "", "metadata": {}, "score": "55.92137"}
{"text": "Computationally , MinimumDistance provides a nearly 8-fold increase in speed relative to the joint HMM in a study of oral cleft trios .Conclusions Our results indicate that batch effects and genomic waves are important considerations for case - parent studies of de novo CNV , and that the minimum distance is an effective statistic for reducing technical variation contributing to false de novo discoveries .", "label": "", "metadata": {}, "score": "55.94692"}
{"text": "Baum , L. E. ( 1966 ) .Statistical inference for probabilistic functions of finite state space markov chains .Annals of Mathematical Statistics 37 1554 - 1563 .B.Dunson , D. ( 2005 ) .Bayesian semiparametric isotonic regression for count data .", "label": "", "metadata": {}, "score": "56.040184"}
{"text": "Among the leading array - based methods for discovery of de novo CNVs in case - parent trios is the joint hidden Markov model ( HMM ) implemented in the PennCNV software .However , the computational demands of the joint HMM are substantial and the extent to which false positive identifications occur in case - parent trios has not been well described .", "label": "", "metadata": {}, "score": "56.092926"}
{"text": "Biohmm : a heterogeneous hid-Bioinformatics 22 1144 - 1146 .URL Muliere , P. & Tardella , L. ( 1998 ) .Approximating distributions of random functionals of Ferguson - Dirichlet priors .Can .J. Statist .M\u00a8 uller , P. , Erkanli , A. & West , M. ( 1996 ) .", "label": "", "metadata": {}, "score": "56.114265"}
{"text": "Three different training sets were used during the course of development : the 18 hour Minitrain set defined by BBN which gives a fast turnaround ; the full 265 hour training set ( h5train00 ) for the the March 2000 system and a subset of h5train00 denoted h5train00sub .", "label": "", "metadata": {}, "score": "56.287067"}
{"text": "The model and the computational methodology we introduce , extend straightforwardly to the more general class of stick - breaking priors for P , obtained by generalising the beta distribution on the final stage of the hierarchy .3Simulation methodology Our primary computational target is the exploration of the posterior distribution of ( s , u , v , z , k , \u03b1 ) by Markov chain Monte Carlo .", "label": "", "metadata": {}, "score": "56.423634"}
{"text": "For a given model set a single Gaussian per state version was created .For each speech state in the single Gaussian system , the nearest two other states were found using a log - overlap distance metric [ 14 ] , which calculates the distance between two Gaussians as the area of overlap of the two probability density functions .", "label": "", "metadata": {}, "score": "56.652092"}
{"text": "It should be emphasised that the MMIE models were all gender independent while the MLE VTLN models were all gender dependent using soft - tying .All the acoustic models used Call Home weighting .9.2 Word List & Language Models .", "label": "", "metadata": {}, "score": "56.719982"}
{"text": "So far , soft - tying has only been used with MLE training , although the technique could also be applied to MMIE trained models .The pronunciation dictionary used in this task contains on average 1.1 to 1.2 pronunciations per word .", "label": "", "metadata": {}, "score": "56.823303"}
{"text": "The increased flexibility of the MDP - HMM allows this asymmetry to be capture and explains why the MDP - HMM is able to give far more accurate predictions for copy number alteration .( Red ) MDP - HMM , ( Green ) R - HMM and ( Blue ) G - HMM .", "label": "", "metadata": {}, "score": "56.88807"}
{"text": "Simulation from the conditional distributions of v and z is particularly easy .On the other hand , simulation from the conditional distribution of k is more involved .The slice Gibbs sampler of Walker ( 2007 ) parametrises in terms of ( k , u , v , z ) .", "label": "", "metadata": {}, "score": "56.905167"}
{"text": "Page 13 .Additional simulation parameters are detailed in Table 1 . 4.2.3Posterior Inference We applied five different Gibbs sampling approaches .Page 14 . otherwise leave ( st , kt ) unchanged .We also analysed the datasets using two variations of both the Slice and Block Gibbs Sampling approaches .", "label": "", "metadata": {}, "score": "56.946705"}
{"text": "As a consequence , 3 .Page 5 .As genomic technologies evolve from being pure research tools to diagnostic devices , more robust techniques are required .Bayesian nonparametrics offers an attractive solution to these problems and lead us to investigate the models we describe here .", "label": "", "metadata": {}, "score": "56.984"}
{"text": "This paper presents new methods for training large neural networks for phoneme probability estimation .An architecture combining time - delay windows and recurrent connections is used to capture the important dynamic information of the speech signal .Because the number of connections in a fully connected recurrent network grows super - linear with the number of hidden units , schemes for sparse connection and connection pruning are explored .", "label": "", "metadata": {}, "score": "57.166977"}
{"text": "This paper presents new methods for training large neural networks for phoneme probability estimation .An architecture combining time - delay windows and recurrent connections is used to capture the important dynamic information of the speech signal .Because the number of connections in a fully connected recurrent network grows super - linear with the number of hidden units , schemes for sparse connection and connection pruning are explored .", "label": "", "metadata": {}, "score": "57.166977"}
{"text": "CRiSM technical report 07 - 05 .Hu , J. , Gao , J.-B. , Cao , Y. , Bottinger , E. & Zhang , W. ( 2007 ) .Exploiting noise in array cgh data to improve detection of dna copy number change .", "label": "", "metadata": {}, "score": "57.1799"}
{"text": "Biometrika 83 , 67 - 79 .M\u00a8 uller , P. , Rosner , G. L. , De Iorio , M. & MacEachern , S. ( 2005 ) .A nonparametric Bayesian model for inference in related longitudinal studies .Appl .", "label": "", "metadata": {}, "score": "57.201324"}
{"text": "Various hidden Markov model ( HMM ) implementations integrate the log R ratios and B allele frequencies to infer copy number [ 12 - 19].Copy number estimation is challenging , in part , due to technical artifacts that contribute to false positives .", "label": "", "metadata": {}, "score": "57.262474"}
{"text": "Hence , we expect an overall computational cost O(T logT ) for the exact simulation of the hidden Markov chain in this non - parametric setup .Additionally , note that we only need partial information MDP update Conditionally on a realisation of s , we have an MDP model .", "label": "", "metadata": {}, "score": "57.263557"}
{"text": "The loss function can be seen as a ( generative ) alternative to maximum likelihood estimation with an interesting information - theoretic interpretation , and it is statistically consistent .It is substantially faster than maximum ( conditional ) likelihood estimation of conditional random fields ( Lafferty et al . , 2001 ; an order of magnitude or more ) .", "label": "", "metadata": {}, "score": "57.42194"}
{"text": "The estimates of the word posterior probabilities encoded in the confusion networks can be used directly as confidence scores ( which are essentially word - level posteriors ) , but they tend to be over - estimates of the true posteriors .", "label": "", "metadata": {}, "score": "57.732403"}
{"text": "Scand .J. Statist .Griffin , J. & Steel , M. J. F. ( 2004 ) .Semiparametric bayesian inference for stochastic frontier models .J. Econometrics 123 121 - 152 .Griffin , J. & Steel , M. J. F. ( 2007 ) .", "label": "", "metadata": {}, "score": "57.94713"}
{"text": "There is a signif- icant amount of correlation in the samples of s from the samplers employing local Gibbs updates compared to the samplers using forward - backward sampling .Page 24 .( a ) lepto 1000 , ( b ) bimod 1000 and ( c ) trimod 1000 .", "label": "", "metadata": {}, "score": "58.12346"}
{"text": "First P1 ( GI non - VTLN MLE triphones , trigram LM , 27k dictionary ) , generated an initial transcription .This P1 pass is identical to the 1998 P1 setup [ 6 ] .The P1 output was used solely for VTLN warp - factor generation and assignment of a gender label for each test conversation side .", "label": "", "metadata": {}, "score": "58.156754"}
{"text": "There is also a short description of the preliminary speech recogniser used in the project . ... better conception of Olga 's intended functionality , it was decided to include a preliminary speech recognition facility .This is a software only continuous speech recognition engine with different modes for the phonetic pattern matching .", "label": "", "metadata": {}, "score": "58.323566"}
{"text": "There is also a short description of the preliminary speech recogniser used in the project . ... better conception of Olga 's intended functionality , it was decided to include a preliminary speech recognition facility .This is a software only continuous speech recognition engine with different modes for the phonetic pattern matching .", "label": "", "metadata": {}, "score": "58.323566"}
{"text": "Therefore acoustic model scaling tends to increase the confusable data set in training by broadening the posterior distribution of state occupation that is used in the EBW update equations .This increase in confusable data also leads to improved generalisation performance .", "label": "", "metadata": {}, "score": "58.360626"}
{"text": "Therefore acoustic model scaling tends to increase the confusable data set in training by broadening the posterior distribution of state occupation that is used in the EBW update equations .This increase in confusable data also leads to improved generalisation performance .", "label": "", "metadata": {}, "score": "58.360626"}
{"text": "It can be seen that there is an improvement in WER of 2.6 % absolute on eval97sub and 2.7 % on eval98 .Data weighting gives a further small improvement , although interestingly , data weighting for MLE reduces the WER by 0.7 % absolute on eval97sub .", "label": "", "metadata": {}, "score": "58.40949"}
{"text": "It can be seen that there is an improvement in WER of 2.6 % absolute on eval97sub and 2.7 % on eval98 .Data weighting gives a further small improvement , although interestingly , data weighting for MLE reduces the WER by 0.7 % absolute on eval97sub .", "label": "", "metadata": {}, "score": "58.40949"}
{"text": "These were constructed by training separate models for transcriptions of the Hub5 acoustic training data and for Broadcast News data and then merging the resultant language models to effectively interpolate the component N - grams .The word - level 4-grams used were smoothed with a class - based trigram model using automatically derived classes [ 12 ] .", "label": "", "metadata": {}, "score": "58.42401"}
{"text": "For simplicity , and without compromising the comparison , we take stto be constant in time .We design the simulation study according to Papaspiliopoulos & Roberts ( 2008 ) , where the retrospective MCMC is compared with various other ( marginal ) algorithms .", "label": "", "metadata": {}, "score": "58.72309"}
{"text": "Thus the complete soft - tied system has the same number of Gaussians as the original system and three times as many mixture weights per state .After this revised structure has been created all system parameters are re - estimated .", "label": "", "metadata": {}, "score": "59.243263"}
{"text": "17 ] was then applied using quinphone models and confidence scores estimated using an N - best homogeneity measure for both the triphone and quinphone output .The final stage combined these two transcriptions using the ROVER program [ 2 ] .", "label": "", "metadata": {}, "score": "59.41501"}
{"text": "To build a Hidden Markov Model and use it to build some predictions , try a simple example like this : .Create an input file to train the model .Here we have a sequence drawn from the set of states 0 , 1 , 2 , and 3 , separated by space characters .", "label": "", "metadata": {}, "score": "59.433083"}
{"text": "The good performance on smaller training sets led us to investigate MMIE training using all the available Hub5 data : the 265 hour h5train00 set .The h5train00 set contains 267,611 segments and numerator and denominator word level lattices were created for each trained segment , and from these , phone - marked lattices were generated .", "label": "", "metadata": {}, "score": "59.468575"}
{"text": "The good performance on smaller training sets led us to investigate MMIE training using all the available Hub5 data : the 265 hour h5train00 set .The h5train00 set contains 267,611 segments and numerator and denominator word level lattices were created for each trained segment , and from these , phone - marked lattices were generated .", "label": "", "metadata": {}, "score": "59.468575"}
{"text": "Results Our analysis of the oral cleft trios reveals that genomic waves represent a substantial source of false positive identifications in the joint HMM , despite a wave - correction implementation in PennCNV .In addition , the noise of low - level summaries of relative copy number ( log R ratios ) is strongly associated with batch and correlated with the frequency of de novo CNV calls .", "label": "", "metadata": {}, "score": "59.47226"}
{"text": "Pruning is performed by using the phone - marked lattice segmentation points extended by a short - period in each direction ( typically 50ms at both the start and end of each phone ) .The search was also optimised as far as possible by combining redundantly repeated models which occur in the phone - marked lattice .", "label": "", "metadata": {}, "score": "59.495213"}
{"text": "Pruning is performed by using the phone - marked lattice segmentation points extended by a short - period in each direction ( typically 50ms at both the start and end of each phone ) .The search was also optimised as far as possible by combining redundantly repeated models which occur in the phone - marked lattice .", "label": "", "metadata": {}, "score": "59.495213"}
{"text": "$ $ MAHOUT_HOME / bin / mahout baumwelch - i hmm - input - o hmm - model - nh 3 - no 4 - e .0001 - m 1000 .Output like the following should appear in the console .", "label": "", "metadata": {}, "score": "59.500786"}
{"text": "Ann .Statist .MacEachern , S. & M\u00a8 uller , P. ( 1998 ) .Estimating mixture of Dirichlet process models .J. Comp .Graph .Statist .Marioni , J. C. , Thorne , N. P. & Tavar , S. ( 2006 ) .", "label": "", "metadata": {}, "score": "59.512794"}
{"text": "Quintana , F. & Iglesias , P. ( 2003 ) .Bayesian clustering and product partition models .J. Roy .Statist .Soc .B 65 , 557 - 574 .Rodriguez , A. , B.Dunson , D. & Gelfand , A. E. ( 2008 ) .", "label": "", "metadata": {}, "score": "59.902294"}
{"text": "The pure MLE system ( MLE models in P2/P3 and MLE lattices ) performs 2.1 % absolute poorer than the MMIE system on P2 .Comparing the performance of MLE models in P4b , they are 0.7 % poorer than in the eval setup ( MLE models with MMIE lattices and adaptation supervision ) without confusion networks but only 0.3 % poorer with confusion networks .", "label": "", "metadata": {}, "score": "59.918762"}
{"text": "A laboratory ASR system has been developed with which on - line trials ( user studies ) have been conducted in order to investigate and evaluate the idea of using state - of - the - art phoneme recognition as a \" heating aid \" .", "label": "", "metadata": {}, "score": "59.929012"}
{"text": "A laboratory ASR system has been developed with which on - line trials ( user studies ) have been conducted in order to investigate and evaluate the idea of using state - of - the - art phoneme recognition as a \" heating aid \" .", "label": "", "metadata": {}, "score": "59.929012"}
{"text": "Neal , R. ( 2000 ) .Markov chain sampling : Methods for Dirichlet process mixture models .J. Comp .Graph .Statist .Papaspiliopoulos , O. & Roberts , G. O. ( 2008 ) .Retrospective markov chain monte carlo for dirichlet process hierarchical models .", "label": "", "metadata": {}, "score": "60.134876"}
{"text": "The full system operates in multiple passes , using more complex acoustic and language models and unsupervised adaptation in later passes .Incoming speech is parameterised into cepstral coefficients and their first and second derivatives to form a 39 dimensional vector every 10ms .", "label": "", "metadata": {}, "score": "60.144897"}
{"text": "The full system operates in multiple passes , using more complex acoustic and language models and unsupervised adaptation in later passes .Incoming speech is parameterised into cepstral coefficients and their first and second derivatives to form a 39 dimensional vector every 10ms .", "label": "", "metadata": {}, "score": "60.144897"}
{"text": "P.S. Gopalakrishnan , D. Kanevsky , A. N\u00e1das & D. Nahamoo ( 1991 ) .An Inequality for Rational Functions with Applications to Some Statistical Estimation Problems .IEEE Trans .Information Theory , Vol .37 , pp .107 - 113 . D.", "label": "", "metadata": {}, "score": "60.20137"}
{"text": "P.S. Gopalakrishnan , D. Kanevsky , A. N\u00e1das & D. Nahamoo ( 1991 ) .An Inequality for Rational Functions with Applications to Some Statistical Estimation Problems .IEEE Trans .Information Theory , Vol .37 , pp .107 - 113 . D.", "label": "", "metadata": {}, "score": "60.20137"}
{"text": "We apply our methodology to the analysis of genomic copy number variation .Full - text .This paper is made available online in accordance with publisher policies .Please scroll down to view the document itself .Please refer to the repository record for this item and our policy information available from the repository home page for further information .", "label": "", "metadata": {}, "score": "60.28867"}
{"text": "For each link in a particular word lattice ( from standard decoding ) a posterior probability is estimated using the forward - backward algorithm .The lattice with these posteriors is then transformed into a linear graph , or confusion network ( CN ) , using a link clustering procedure [ 11 ] .", "label": "", "metadata": {}, "score": "60.583054"}
{"text": "Should the software prove defective , you assume the cost of all necessary servicing , repair , or correction .Module Install Instructions .To install Algorithm::BaumWelch , simply copy and paste either of the commands in to your terminal Abstract : .", "label": "", "metadata": {}, "score": "60.596783"}
{"text": "This gave a new word list with 54,537 words where most of the pronunciations were already available in our broadcast news ( Hub4 ) dictionary .The 54k wordlist reduced the out - of - vocabulary ( OOV ) rate on eval98 from 0.94 % to 0.38 % .", "label": "", "metadata": {}, "score": "60.988823"}
{"text": "We describe a natural alternative for training se - quence labeling models , based on MIRA ( Margin In - fused Relaxed Algorithm ) .In addition , we describe a novel method for performing Viterbi - like decoding .We test MIRA and contrast it with other training algo - rithms and contrast our decoding algorithm ... \" .", "label": "", "metadata": {}, "score": "61.05922"}
{"text": "alga required the development of a system with components from many different fields : multimodal interfaces , dialogue management , speech recognition , speech synthesis , graphics , animation , facilities for direct manipulation and database handling .To integrate all knowledge sources alga is implemented with separate modules communicaring with a central dialogue interaction manager .", "label": "", "metadata": {}, "score": "61.344772"}
{"text": "alga required the development of a system with components from many different fields : multimodal interfaces , dialogue management , speech recognition , speech synthesis , graphics , animation , facilities for direct manipulation and database handling .To integrate all knowledge sources alga is implemented with separate modules communicaring with a central dialogue interaction manager .", "label": "", "metadata": {}, "score": "61.344772"}
{"text": "Conclusions This is the first study to characterize CNPs in ARIC and the first genome - wide analysis of CNPs and uric acid .Our findings suggests a novel , non - coding regulatory mechanism for SLC2A9-mediated modulation of serum uric acid , and detail a bioinformatic approach for assessing the contribution of CNPs to heritable traits in large population - based studies where technical sources of variation are substantial .", "label": "", "metadata": {}, "score": "61.441612"}
{"text": "f the backward search is only a very small fraction of the forward search . \" ...This paper presents new methods for training large neural networks for phoneme probability estimation .An architecture combining time - delay windows and recurrent connections is used to capture the important dynamic information of the speech signal .", "label": "", "metadata": {}, "score": "61.67585"}
{"text": "f the backward search is only a very small fraction of the forward search . \" ...This paper presents new methods for training large neural networks for phoneme probability estimation .An architecture combining time - delay windows and recurrent connections is used to capture the important dynamic information of the speech signal .", "label": "", "metadata": {}, "score": "61.67585"}
{"text": "During the last couple of years , researchers around the world have given considerable attention to the problem of extracting semantic information from television and re - representing it in a form suitable for automatic indexing and searching .Detailed content information in a television broadcast is typically found in teletext subtitles ( if such are available ) , text embedded in the video image , and in the spoken dialogue .", "label": "", "metadata": {}, "score": "61.946396"}
{"text": "During the last couple of years , researchers around the world have given considerable attention to the problem of extracting semantic information from television and re - representing it in a form suitable for automatic indexing and searching .Detailed content information in a television broadcast is typically found in teletext subtitles ( if such are available ) , text embedded in the video image , and in the spoken dialogue .", "label": "", "metadata": {}, "score": "61.946396"}
{"text": "The slice Gibbs sampler proceeds by Gibbs sampling of k , u , v , and z according to their full conditional distributions .Therefore , direct simulation from this distribution is difficult .Hence this distribution can be simulated by the inverse CDF method by computing a number of 8 .", "label": "", "metadata": {}, "score": "61.955406"}
{"text": "Journal of the American Statistical Association 103 1131 - 1144 .Page 20 .Bayesian Methods for Hidden Markov Models : Recursive Computing in the 21st Century .Journal of the American Statistical Association 97 337 - 351 .S. P. , Xuan , X. , P. ( 2006 ) .", "label": "", "metadata": {}, "score": "62.309387"}
{"text": "The inherently delayed use of lan ... \" .Tree - trellis forward - backward algorithm has been widely used for N - best search in continuous speech recognition .The inherently delayed use of language model in the lexical tree structure leads to inefficient pruning and the partial - path scores recorded is an underestimated heuristic score .", "label": "", "metadata": {}, "score": "62.591156"}
{"text": "The inherently delayed use of lan ... \" .Tree - trellis forward - backward algorithm has been widely used for N - best search in continuous speech recognition .The inherently delayed use of language model in the lexical tree structure leads to inefficient pruning and the partial - path scores recorded is an underestimated heuristic score .", "label": "", "metadata": {}, "score": "62.591156"}
{"text": "CE is a natural fit for log - linear models , which can include arbitrary features but for which EM is computationally difficult .We show that , using the same features , log - linear dependency grammar models trained using CE can drastically outperform EMtrained generative models on the task of matching human linguistic annotations ( the MATCHLIN - GUIST task ) .", "label": "", "metadata": {}, "score": "62.796623"}
{"text": "The representation of the DPP in terms of only k , v and z ( that is where u is marginalised out ) is well known and has been used in hierarchical modelling among others by Ishwaran & James ( 2001 ) ; Papaspiliopoulos & Roberts ( 2008 ) .", "label": "", "metadata": {}, "score": "62.799683"}
{"text": "Since the CU - HTK Hub5 system can use quinphone models , we investigated MMIE quinphone training using h5train00 .The decision tree state clustering process for quinphones includes questions regarding phone context and word - boundaries .The baseline quinphone system uses 9640 speech states and 16 Gaussians / state .", "label": "", "metadata": {}, "score": "63.053967"}
{"text": "Since the CU - HTK Hub5 system can use quinphone models , we investigated MMIE quinphone training using h5train00 .The decision tree state clustering process for quinphones includes questions regarding phone context and word - boundaries .The baseline quinphone system uses 9640 speech states and 16 Gaussians / state .", "label": "", "metadata": {}, "score": "63.053967"}
{"text": "We also experimented with data - weighting with this setup during MMIE training .The rationale for this is that while the test data sets contain equal amounts of Switchboard and CHE data , the training set is not balanced .Therefore we gave a 3x higher weighting to CHE data during training .", "label": "", "metadata": {}, "score": "63.225014"}
{"text": "We also experimented with data - weighting with this setup during MMIE training .The rationale for this is that while the test data sets contain equal amounts of Switchboard and CHE data , the training set is not balanced .Therefore we gave a 3x higher weighting to CHE data during training .", "label": "", "metadata": {}, "score": "63.225014"}
{"text": "The mixing of the Block Gibbs Sampler is considerably better than the Slice Sampler .The Block Gibbs Sampler appears able to explore different modes in the posterior distribution of v for each of the three datasets whereas the Slice Sampler tends to get fixated to one mode .", "label": "", "metadata": {}, "score": "63.252567"}
{"text": "We denote these as the Slice Samplers and Block Gibbs Samplers with forward - backward updates .For all the sampling methods , we generated 20,000 sweeps ( one sweep being equivalent to an update of all T allocation and state variables ) and discarded the first 10,000 as burn - in .", "label": "", "metadata": {}, "score": "63.44465"}
{"text": "The h5train00sub set was chosen to include all the speakers from Swb1 in h5train00 as well as a subset of the available CHE sides .Furthermore results are given for the March 2000 evaluation data set , eval00 , which has 40 sides of Swb1 and 40 CHE sides .", "label": "", "metadata": {}, "score": "63.516064"}
{"text": "Confusion networks were shown to consistently improve word error rates and yield improved confidence scores .On the 1998 evaluation set a relative reduction in word error rate of 11 % was obtained .The system presented here gave the lowest word error rate in the March 2000 Hub5E evaluation .", "label": "", "metadata": {}, "score": "63.56122"}
{"text": "Ishwaran , H. & James , L. ( 2001 ) .Gibbs sampling methods for stick - breaking priors .J. Am .Statist .Assoc .96 , 161 - 73 .Ishwaran , H. & James , L. F. ( 2003 ) .", "label": "", "metadata": {}, "score": "63.704002"}
{"text": "This thesis investigates the idea of using phoneme recognition as a telephone communication aid aimed at heating - impaired people .A laboratory ASR system has been developed with which on - line trials ( user studies ) have been conducted in order to investigate and evaluate the idea of using state - of - th ... \" .", "label": "", "metadata": {}, "score": "63.79647"}
{"text": "This thesis investigates the idea of using phoneme recognition as a telephone communication aid aimed at heating - impaired people .A laboratory ASR system has been developed with which on - line trials ( user studies ) have been conducted in order to investigate and evaluate the idea of using state - of - th ... \" .", "label": "", "metadata": {}, "score": "63.79647"}
{"text": "A technique for predicting triphones by concatenation of diphone or monophone models is studied .The models are connected using linear interpolation between endpoints of piece - wise linear parameter trajectories .Three types of spectral representation are compared : formants , filter amplitudes and cepstmm coefficients .", "label": "", "metadata": {}, "score": "64.35182"}
{"text": "A technique for predicting triphones by concatenation of diphone or monophone models is studied .The models are connected using linear interpolation between endpoints of piece - wise linear parameter trajectories .Three types of spectral representation are compared : formants , filter amplitudes and cepstmm coefficients .", "label": "", "metadata": {}, "score": "64.35182"}
{"text": "In Figure 7 we show a region of Chromosome 3 from the mouse tumour that contains a region consisting of a cluster of mutations known as single nucleotide polymorphisms ( SNPs ) ( as shown in ( Lakshmi et al . , 2006 ) ) .", "label": "", "metadata": {}, "score": "64.75551"}
{"text": "Estimation Using an Online Algorit ... . by Noah A. Smith , Douglas Vail , John D. Lafferty , Noah A. Smith , Douglas L. Vail , John D. Lafferty . \" ...We describe a new loss function , due to Jeon and Lin ( 2006 ) , for estimating structured log - linear models on arbitrary features .", "label": "", "metadata": {}, "score": "65.045815"}
{"text": "T ?The second equality follows from the conditional independence of the yt 's and the kt 's given the conditioning variables .We exploit the product structure to exchange the order of the summation and the product to obtain the third equality .", "label": "", "metadata": {}, "score": "65.19285"}
{"text": "The use of quinphone models instead of triphone models gives a further gain of 0.9 % for both branches .Whereas the second adaptation stage with two speech transforms for the quinphone MMIE models brings 0.5 % , after obtaining CN output the difference is only 0.2 % .", "label": "", "metadata": {}, "score": "65.45658"}
{"text": "Nucleic Acids Res 35 2013 - 2025 .Devroye , L. ( 1986 ) .Non - Uniform Random Variate Generation .Springer - Verlag .Dunson , D. B. , Pillai , N. & Park , J. H. ( 2007 ) .", "label": "", "metadata": {}, "score": "65.69932"}
{"text": "Share .OpenURL .Abstract .We introduce , analyze and demonstrate a recursive hierarchical generalization of the widely used hidden Markov models , which we name Hierarchical Hidden Markov Models ( HHMM ) .Our model is motivated by the complex multi - scale structure which appears in many natural sequences , particularly in language , handwriting and speech .", "label": "", "metadata": {}, "score": "65.79054"}
{"text": "This contribution presents part of the work initiated at CTT on the development of speech technology to assist non - native speakers in learning Swedish .This study mainly focuses on the automatic evaluation of mispronunciations at a phonetic level .We describe a new database we have collected for thi ... \" .", "label": "", "metadata": {}, "score": "65.82612"}
{"text": "This contribution presents part of the work initiated at CTT on the development of speech technology to assist non - native speakers in learning Swedish .This study mainly focuses on the automatic evaluation of mispronunciations at a phonetic level .We describe a new database we have collected for thi ... \" .", "label": "", "metadata": {}, "score": "65.82612"}
{"text": "Bigram , trigram and 4-gram LMs were trained on each data set ( LDC Hub5 , MSU Hub5 , BN ) and merged to form an effective 3-way interpolation .Furthermore , as described in [ 6 ] a class - based trigram model using 400 automatically generated word classes [ 12 , 9 ] was built to smooth the merged 4-gram language model by a further interpolation step to form the language model used in lattice rescoring .", "label": "", "metadata": {}, "score": "66.20511"}
{"text": "where is the composite model corresponding to the word sequence and is the probability of this sequence as determined by the language model .The summation in the denominator of ( 1 ) is taken over all possible word sequences allowed in the task and it can be replaced by .", "label": "", "metadata": {}, "score": "66.61169"}
{"text": "Springer .Colella , S. , Yau , C. , Taylor , J. M. , Mirza , G. , Butler , H. , Clouston , P. , Bassett , A. S. , Seller , A. , Holmes , C. C. & Ragoussis , J. ( 2007 ) .", "label": "", "metadata": {}, "score": "67.085556"}
{"text": "An Investigation of Frame Discrimination for Continuous Speech Recognition .[ ps ] [ pdf ] Technical Report CUED / F - INFENG / TR.332 , Cambridge University Engineering Dept .Algorithm - BaumWelch - v0.0.2 .NAME .Algorithm::BaumWelch - Baum - Welch Algorithm for Hidden Markov Chain parameter estimation .", "label": "", "metadata": {}, "score": "68.99831"}
{"text": "Note that conditionally on w the pairs ( kt , ut ) are independent over t. The marginal for ktimplied from this joint distribution is clearly ( 2 ) .Expression ( 1 ) follows from a standard representation of an arbitrary random variable k with density p as a marginal of a pair ( k , u ) uniformly distributed under the curve", "label": "", "metadata": {}, "score": "69.12534"}
{"text": "( a )The region indicated ( red ) contains a 24 .Page 26 .Chromosome 3 .( a )The region indicated ( red ) contains no copy number alterations but contains SNPs that can disrupt the binding of probes on the microarray ( Lakshmi et al . , 2006 ) .", "label": "", "metadata": {}, "score": "69.26727"}
{"text": "This section describes a series of MMIE training experiments using the Cambridge University HTK ( CU - HTK ) system for the transcription of conversational telephone data from the Switchboard and Call Home English corpora ( ' ' Hub5 ' ' data ) .", "label": "", "metadata": {}, "score": "69.27625"}
{"text": "This section describes a series of MMIE training experiments using the Cambridge University HTK ( CU - HTK ) system for the transcription of conversational telephone data from the Switchboard and Call Home English corpora ( ' ' Hub5 ' ' data ) .", "label": "", "metadata": {}, "score": "69.27625"}
{"text": "The reason why we prefer the augmented representation in terms of ( k , v , z , u ) to the marginal representation in terms of ( k , v , z ) will be fully appreciated in Section 3 .", "label": "", "metadata": {}, "score": "69.3654"}
{"text": "IEEE Trans .Information Theory , Vol .37 , pp .107 - 113 .T.R. Niesler , E.W.D. Whittaker & P.C. Woodland ( 1998 ) .Comparison of Part - Of - Speech and Automatically Derived Category - Based Language Models for Speech Recognition .", "label": "", "metadata": {}, "score": "69.48395"}
{"text": "Hidden Markov Models .Introduction and Usage .Formal Definition .A Hidden Markov Model ( HMM ) is a statistical model of a process consisting of two ( in our case discrete ) random variables O and Y , which change their state sequentially .", "label": "", "metadata": {}, "score": "69.587265"}
{"text": "( Red ) Slice sampler with local updates , ( Green )Block Gibbs Sampler with local updates , ( Blue ) Slice sampler with forward- backward updates and ( Black )Block Gibbs Sampler with forward - backward updates .Page 25 . confirmed deletion ( Lakshmi et al . , 2006 ) .", "label": "", "metadata": {}, "score": "69.92326"}
{"text": "The Cambridge University HTK ( CU - HTK ) Hub5 system has been developed over several years .This paper describes changes to the September 1998 Hub5 evaluation system [ 6 ] made while developing the March 2000 system .All of these features made a significant contribution to the word error rate improvements of the complete system .", "label": "", "metadata": {}, "score": "70.41191"}
{"text": "In addition , we describe a novel method for performing Viterbi - like decoding .We test MIRA and contrast it with other training algo - rithms and contrast our decoding algorithm with the vanilla Viterbi algorithm . ces of the transition and emission features .", "label": "", "metadata": {}, "score": "72.41414"}
{"text": "PennCNV[43 ] is based on a hidden Markov model ( HMM ) , jointly modeling the unknown copy number states in all three trio members .We identified a genome - wide significant 62 kilo base ( kb ) non - coding region on chromosome 7p14.1where de novo deletions occur more frequently among oral cleft cases than controls .", "label": "", "metadata": {}, "score": "73.09401"}
{"text": "Section 5 gives an interpretation of y , s , m , n and \u03a0 in the context of the ROMA experiment for the study of copy number variation in the genome .In that context there exists reliable prior knowledge which allows us to treat n , \u03a0 and m as known .", "label": "", "metadata": {}, "score": "73.329926"}
{"text": "This is an 11 % reduction in WER relative to the CU - HTK evaluation result obtained on the same data set in 1998 ( 39.5 % ) .Note that confusion network output consistently improves performance by about 1 % absolute and that combination of the 4 outputs using confusion network combination ( CNC ) is 0.4 % absolute better than using the ROVER approach .", "label": "", "metadata": {}, "score": "74.030266"}
{"text": "The table also shows the effect of giving a factor of three weighting to the CHE training data ( The test set is balanced across Switchboard and Call Home data but the training set is n't and so data weighting attempts to partially correct this imbalance ) .", "label": "", "metadata": {}, "score": "74.09122"}
{"text": "The results of using soft - tied ( ST ) triphone and quinphone systems on eval98 is shown in Table 4 when data weighting is used .We have found that the use of acoustic data weighting reduces the beneficial effect of soft - tying .", "label": "", "metadata": {}, "score": "74.42175"}
{"text": "It can be shown that increasing M with T at any rate is sufficient to ensure that A(M , T , z , w ) is bounded away from 0 almost surely ( with respect to the prior measure on ( w , z ) ) .", "label": "", "metadata": {}, "score": "75.04584"}
{"text": "The Hub5 acoustic training data is from two corpora : Switchboard-1 ( Swb1 ) and Call Home English ( CHE ) .The 180 hour training set used for training the 1998 HTK system used various sources of Swbd1 transcriptions and turn - level segmentations .", "label": "", "metadata": {}, "score": "75.6208"}
{"text": "This means , that the state change probability of Y only depends on its current state and does not change in time .O does not have a Markov Property , but its state probability depends statically on the current state of Y. .", "label": "", "metadata": {}, "score": "76.80532"}
{"text": "Tools . by Jonas Beskow , Kjell Elenius , Scott Mcglashan - In Proceedings of Eurospeech & apos;97 , 1997 . \" ...The object of the Olga project is to develop an interactive 3D animated talking agent .A futuristic application scenario is interactive digital TV , where the alga agent would guide naive users through the various services available on the network .", "label": "", "metadata": {}, "score": "77.09607"}
{"text": "Previously the ROVER technique introduced in [ 2 ] had been used to combine the 1-best output of multiple systems .Confusion network combination ( CNC ) can be seen as a generalisation of ROVER to confusion networks , i.e. it uses the competing word hypotheses and their posteriors encoded in the confusion sets instead of only considering the most likely word hypothesised by each system .", "label": "", "metadata": {}, "score": "77.299484"}
{"text": "SEE ALSO .Algorithm::Viterbi .DEPENDENCIES . 'WARNING .BUGS AND LIMITATIONS .Let me know .AUTHOR .LICENCE AND COPYRIGHT .All rights reserved .This module is free software ; you can redistribute it and/or modify it under the same terms as Perl itself .", "label": "", "metadata": {}, "score": "77.59785"}
{"text": "News shows and other television broadcasts carry vast amounts of information , but generally in forms that are not reachable through conventional information retri ... \" .With the ever - increasing flow of information , the need for computer - automated tools for handling information becomes greater and greater .", "label": "", "metadata": {}, "score": "77.83724"}
{"text": "News shows and other television broadcasts carry vast amounts of information , but generally in forms that are not reachable through conventional information retri ... \" .With the ever - increasing flow of information , the need for computer - automated tools for handling information becomes greater and greater .", "label": "", "metadata": {}, "score": "77.83724"}
{"text": "Journal of the Royal Statistical Society , Series B 69 163 - 183 .Escobar , M. ( 1988 ) .estimation of the distribution of the means .PhD Dissertation , Department of Statistics , Yale University .Estimating the means of several normal populations by nonparametric Escobar , M. D. & West , M. ( 1995 ) .", "label": "", "metadata": {}, "score": "78.72144"}
{"text": "Phoneme bigrams were trained on phonetic transcriptions of Swedish newspaper text .Speaker adaptat ... . \" ...A technique for predicting triphones by concatenation of diphone or monophone models is studied .The models are connected using linear interpolation between endpoints of piece - wise linear parameter trajectories .", "label": "", "metadata": {}, "score": "78.876205"}
{"text": "Phoneme bigrams were trained on phonetic transcriptions of Swedish newspaper text .Speaker adaptat ... . \" ...A technique for predicting triphones by concatenation of diphone or monophone models is studied .The models are connected using linear interpolation between endpoints of piece - wise linear parameter trajectories .", "label": "", "metadata": {}, "score": "78.876205"}
{"text": "Statist .Sim .Comput .Appendix 1 : proof of Proposition 1 Proposition 1 follows directly from the following result which shows that the data y conditionally on ( s , z , v , u ) are independent even when the allocation variables k are integrated out .", "label": "", "metadata": {}, "score": "79.65306"}
{"text": "Serum urate is highly heritable , yet association studies of single nucleotide polymorphisms ( SNPs ) and serum uric acid explain a small fraction of the heritability .Whether copy number polymorphisms ( CNPs ) contribute to uric acid levels is unknown .", "label": "", "metadata": {}, "score": "80.14818"}
{"text": "A reliable filter system for use in e.g. Scandinavia , where only a few of the television broadcasts are teletext - subtitled , must take advantage of information in all three forms , or modalities .The presented report contains a survey of current research projects in this area and a theoretical design of a modular , multi - modal , content - based television filter , based on findings in the . \" ...", "label": "", "metadata": {}, "score": "80.199905"}
{"text": "A reliable filter system for use in e.g. Scandinavia , where only a few of the television broadcasts are teletext - subtitled , must take advantage of information in all three forms , or modalities .The presented report contains a survey of current research projects in this area and a theoretical design of a modular , multi - modal , content - based television filter , based on findings in the . \" ...", "label": "", "metadata": {}, "score": "80.199905"}
{"text": "Page 27 .( a , d ) Chromosome 3 , ( b , e ) Chromosome 5 , ( c , f ) Chromosome 9 .The empirical distribution of the ROMA data appears to be heavy - tailed and asymmetric .", "label": "", "metadata": {}, "score": "80.25719"}
{"text": "This work was in part supported by GCHQ .Gunnar Evermann has studentships from the EPSRC and the Cambridge European Trust , and Dan Povey holds a studentship from the Schiff Foundation .The authors are grateful to Thomas Niesler and Ed Whittaker for their help in building the class - based language models .", "label": "", "metadata": {}, "score": "80.97378"}
{"text": "5659361683006626 E - 251 Transition matrix : .098919959130616 E - 5 0 .1147850399214744 E - 4 .404648706054873 E - 37 0 .01786289571088 E - 11 0 .7715625453610858 Emission matrix : . 0536163836449762 E - 39 2 . 1414087769942127 E - 7 1 .", "label": "", "metadata": {}, "score": "85.52634"}
{"text": "The object of the Olga project is to develop an interactive 3D animated talking agent .A futuristic application scenario is interactive digital TV , where the alga agent would guide naive users through the various services available on the network .", "label": "", "metadata": {}, "score": "86.52464"}
{"text": "The object of the Olga project is to develop an interactive 3D animated talking agent .A futuristic application scenario is interactive digital TV , where the alga agent would guide naive users through the various services available on the network .", "label": "", "metadata": {}, "score": "86.52464"}
{"text": "The object of the Olga project is to develop an interactive 3D animated talking agent .A futuristic application scenario is interactive digital TV , where the alga agent would guide naive users through the various services available on the network .", "label": "", "metadata": {}, "score": "86.52464"}
{"text": "( Black )Marginal Gibbs Sampler , ( Green ) Slice Sampler using local updates , ( Red ) Block Gibbs Sampler using local updates , ( Blue ) Slice Sampler using forward - backward updates and ( Purple ) Block Gibbs Sampler using forward - backward updates .", "label": "", "metadata": {}, "score": "88.59616"}
{"text": "In diploid organisms , such as humans , somatic cells normally contain two copies of each gene , one inherited from each parent .However , abnormalities during the process of DNA replication and synthesis can lead to the loss or gain of DNA fragments , leading to variable gene copy numbers that may initiate or promote disease conditions .", "label": "", "metadata": {}, "score": "90.15355"}
{"text": "Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .Publisher conditions are provided by RoMEO .Differing provisions from the publisher 's actual policy or licence agreement may be applicable .", "label": "", "metadata": {}, "score": "91.51781"}
{"text": "DISCLAIMER OF WARRANTY .Because this software is licensed free of charge , there is no warranty for the software , to the extent permitted by applicable law .Except when otherwise stated in writing the copyright holders and/or other parties provide the software \" as is \" without warranty of any kind , either expressed or implied , including , but not limited to , the implied warranties of merchantability and fitness for a particular purpose .", "label": "", "metadata": {}, "score": "93.767975"}
