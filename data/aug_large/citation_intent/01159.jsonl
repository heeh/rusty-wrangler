{"text": "[ Joachims , 1999a].The algorithm has scalable memory requirements and can handle problems with many thousands of support vectors efficiently .Support Vector Machines in general , and SVMLight specifically , represent some of the best - performing Machine Learning approaches in domains such as text categorization , image recognition , bioinformatics string processing , and others .", "label": "", "metadata": {}, "score": "46.64991"}
{"text": "This module implements a perl interface to Thorsten Joachims ' SVMLight package : .SVMLight is an implementation of Vapnik 's Support Vector Machine [ Vapnik , 1995 ] for the problem of pattern recognition , for the problem of regression , and for the problem of learning a ranking function .", "label": "", "metadata": {}, "score": "49.228188"}
{"text": "An SVM regularization constant can be selected to provide a modest correction that limits the amount that weights are modified .[0075 ]In addition to using SVM 's to tune the collection of phrases globally , they can alternatively be used to tune each individual feature by rescaling the particular feature .", "label": "", "metadata": {}, "score": "50.120995"}
{"text": "[ 0029 ] The system generates 108 a linear document classifier using the filtered and weighted resulting phrases .In particular , the generated document classifier can be a linear classifier having the filtered candidate phrases as input phrases , each having an assigned weight .", "label": "", "metadata": {}, "score": "50.781384"}
{"text": "Supervised improvements can be used to correct weights assigned to phrase candidates for use in a classifier , for example , using a perceptron or support vector machine technique described in greater detail below .In particular , the supervised improvement can identify and correct for systematic errors .", "label": "", "metadata": {}, "score": "50.851254"}
{"text": "Filtering Candidate Phrases .[ 0050 ] Phrase candidates can be pre - filtered before assigning weights in order to improve performance of weight assignment .One technique uses a Web search combined with a classifier of pages on a given topic .", "label": "", "metadata": {}, "score": "50.95732"}
{"text": "When the CombinedKernel between two examples is # evaluated it computes the corresponding linear combination of kernels according to their weights .# We then show how to create an MKLClassifier that trains an SVM and learns the optimal # weighting of kernels ( w.r.t . a given norm q ) at the same time .", "label": "", "metadata": {}, "score": "50.984093"}
{"text": "In some implementations , the technique is modified to exclude input phrases in the linear classifier from being provided as outputs to the system .Thus , the system can use the classifier to identify new candidate phrases .The system uses the phrases generated by training the linear classifier as candidate phrases of a document classifier .", "label": "", "metadata": {}, "score": "51.062973"}
{"text": "[ 0021 ] Before addressing the particulars of the present principles , the following notations are used throughout . denotes an underlying word ( unigram ) dictionary and S denotes the set of all finite length sequences of words from .It is in this space that a standard classifier such as linear perceptron or support vector machine can be applied .", "label": "", "metadata": {}, "score": "52.52005"}
{"text": "[ 0072 ] The system uses 518 the rated documents as input for a particular supervised improvement technique , e.g. , SVM or perceptron .More specifically , a value is assigned to each phrase candidate occurring in each document .This value is then input to the supervised improvement technique .", "label": "", "metadata": {}, "score": "52.754852"}
{"text": "It # computes the chi - squared distance between sets of histograms .It is a very # useful distance in image recognition ( used to detect objects ) .The preprocessor # LogPlusOne adds one to a dense real - valued vector and takes the logarithm of # each component of it .", "label": "", "metadata": {}, "score": "53.101612"}
{"text": "The weight learning module 612 initializes the weights used to embed the document and adjusts those weights based on learning training documents and on the output of the classifier 610 . [0048 ]Having described preferred embodiments of a system and method ( which are intended to be illustrative and not limiting ) , it is noted that modifications and variations can be made by persons skilled in the art in light of the above teachings .", "label": "", "metadata": {}, "score": "53.311546"}
{"text": "How the supervised and unsupervised methods are applied , as well as how many are scheduled , can depend on the application , quality of the initial classifier , and available resources .[0079 ] The iterative process reduces noise in the phrase weights .", "label": "", "metadata": {}, "score": "53.568382"}
{"text": "For example , scores of other classifiers ( e.g. , image classifiers ) or site scores as described above can be used as input features .Additionally , a simple score of \" 1 if belonging to a known set of documents of this topic , 0 else \" can be used as an input feature .", "label": "", "metadata": {}, "score": "53.795036"}
{"text": "6 , a system for document classification 600 is shown .The system includes a processor 602 and memory 604 , which store and process input text to produce classifications .The memory 604 furthermore stores training sets of text to be used by weight learning module 612 .", "label": "", "metadata": {}, "score": "54.008446"}
{"text": "However , the complexity of modeling n - grams grows exponentially with the dictionary size .[0008 ] Despite the simplicity and relative success of document classification using n - gram features , previous models disregard all the spatial and ordering information of the n - grams -- such information is important for text data .", "label": "", "metadata": {}, "score": "54.15103"}
{"text": "6 is a diagram of a system for document classification according to the present principles .DETAILED DESCRIPTION OF PREFERRED EMBODIMENTS .[ 0019 ]The present principles provide a unified deep learning framework for using high - order n - grams and for exploring spatial information .", "label": "", "metadata": {}, "score": "54.1697"}
{"text": "The main difference to the binary # classification version of MKL is that we can use more than two values as labels , when training # the classifier .# Finally , the example shows how to classify with a trained MKLMultiClass classifier .", "label": "", "metadata": {}, "score": "55.07196"}
{"text": "The main difference to the binary # classification version of MKL is that we can use more than two values as labels , when training # the classifier .# Finally , the example shows how to classify with a trained MKLMultiClass classifier .", "label": "", "metadata": {}, "score": "55.07196"}
{"text": "The main difference to the binary # classification version of MKL is that we can use more than two values as labels , when training # the classifier .# Finally , the example shows how to classify with a trained MKLMultiClass classifier .", "label": "", "metadata": {}, "score": "55.07196"}
{"text": "The main difference to the binary # classification version of MKL is that we can use more than two values as labels , when training # the classifier .# Finally , the example shows how to classify with a trained MKLMultiClass classifier .", "label": "", "metadata": {}, "score": "55.07196"}
{"text": "The main difference to the binary # classification version of MKL is that we can use more than two values as labels , when training # the classifier .# Finally , the example shows how to classify with a trained MKLMultiClass classifier .", "label": "", "metadata": {}, "score": "55.07196"}
{"text": "[ 0074 ]In some other implementations , support vector machines are used to correct feature weights .Support vector machines ( \" SVM 's \" ) are a set of related supervised learning methods used for classification and regression .A generated linear classifier can be input as one feature in the SVM .", "label": "", "metadata": {}, "score": "55.184982"}
{"text": "First , a supervised embedding mechanism is provided to directly model n - grams in a low - dimensional latent semantic space .Then , to achieve a fine - grained model of target documents , a controller module is trained by spatially re - weighting the sub - parts of each document .", "label": "", "metadata": {}, "score": "55.578514"}
{"text": "%We then show how to create an MKLMultiClass classifier that trains an SVM and learns the optimal % weighting of kernels ( w.r.t . a given norm q ) at the same time .The main difference to the binary % classification version of MKL is that we can use more than two values as labels , when training % the classifier . %", "label": "", "metadata": {}, "score": "55.67192"}
{"text": "Conversely , various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination .In certain circumstances , multitasking and parallel processing may be advantageous .[ 0101 ] Particular embodiments of the subject matter described in this specification have been described .", "label": "", "metadata": {}, "score": "55.691605"}
{"text": "Consequently , a supervised correction step can be included to eliminate such phrases .[ 0080 ] Following such a correction , one or more new iterations can be performed either supervised or unsupervised .In some implementations , this co - occurrence technique is combined with techniques using existing classifiers in order to generate a weighted collection of phrase candidates for use in classifying documents .", "label": "", "metadata": {}, "score": "55.78016"}
{"text": "[0068 ] In some implementations , some phrase candidates that occur more often on documents of a given topic will also occur in clusters of features that are correlated because they also appear on documents of a different , but related topic .", "label": "", "metadata": {}, "score": "56.210365"}
{"text": "Each n - gram has a corresponding feature vector that characterizes the n - gram .[ 0023 ] Block 106 uses factors to weight the n - gram vectors according to where each n - gram is located within the document .", "label": "", "metadata": {}, "score": "56.46135"}
{"text": "In some implementations , when training the SVM , the system uses documents having a common approximate length .[ 0077 ] In some implementations , instead of , or in addition to , the regularization constant for the SVM , a multiplier is applied to the SVM correction ( e.g. , 0.6 ) to ensure that the SVM does not over fit the training data .", "label": "", "metadata": {}, "score": "56.545586"}
{"text": "Large amounts of data are used and can improve all phrase candidate weights , but can miss clusters of semantically related erroneous phrase candidate features .[ 0064 ] When not performing supervised improvement and given the initial classifier , the system assigns 508 a score to each phrase candidate .", "label": "", "metadata": {}, "score": "56.585865"}
{"text": "Therefore , a monotonic regression can be applied to the function p(s , -n ) to learn the function p from labeled examples .[ 0033 ]The resulting document classifier can be applied to a set of documents in order to identify those documents of the collection which belong to a particular topic targeted by the classifier .", "label": "", "metadata": {}, "score": "56.62791"}
{"text": "This example shows how to compute the Eucledian Distance using a sparse encoding ./data / fm_train_real ./data / fm_test_real . /examples / documented / octave_modular / distance_tanimoto_modular .m . %An approach as applied below , which shows the processing of input data % from a file becomes a crucial factor for writing your own sample applications .", "label": "", "metadata": {}, "score": "56.648144"}
{"text": "[ 0041 ] In some alternative implementations , Web site scores are used to generate candidate phrases from a collection of Web documents .The system receives a collection of Web sites .The system assigns to each of the Web sites a probability of belonging to a certain topic based on a monotonic regression combining of scores of different site classifiers .", "label": "", "metadata": {}, "score": "56.787388"}
{"text": "In some implementations , the system performs the assignment of weights and the filtering 104 concurrently or as part of a single technique .For example , an assigned weight of zero effectively filters out a given candidate phrase .Example techniques for filtering and assigning weights to candidate phrases are described in greater detail below .", "label": "", "metadata": {}, "score": "57.003536"}
{"text": "Other embodiments of this aspect include corresponding systems , apparatus , and computer program products .[ 0011 ] These and other embodiments can optionally include one or more of the following features .The method further includes determining whether multiple iterations are to be performed ; and when additional iterations are to be performed , using a previously generated linear classifiers as the initial classifier for the next iteration .", "label": "", "metadata": {}, "score": "57.041687"}
{"text": "Example techniques for generating candidate phrases ( for example , from a collection of documents , Web pages scores , or from search queries ) are described in greater detail below .[ 0026 ] The system filters 104 the received collection of candidate phrases .", "label": "", "metadata": {}, "score": "57.276516"}
{"text": "However , in general , it is possible for n - grams to overlap or to be substrings of each other .This can degrade performance since it effectively provide additional weight on longer n - grams that are more likely to have substrings among the collection of phrase candidates .", "label": "", "metadata": {}, "score": "57.333855"}
{"text": "[ 0056 ] Once a collection of candidate phrases is received or generated , each candidate phrase is assigned a weight for use with a classifier .In some implementations , an existing initial classifier is used to assign weights to phrase candidates .", "label": "", "metadata": {}, "score": "57.509647"}
{"text": "In some implementations , the classifier is adjusted to correct weights assigned to the phrases in the classifier .For example , weights can need to be corrected when a systematic error allows some phrases to be part of the classifier that are not associated with the topic of interest to the classifier .", "label": "", "metadata": {}, "score": "58.093494"}
{"text": "It # computes the chi - squared distance between sets of histograms .It is a very # useful distance in image recognition ( used to detect objects ) .The preprocessor # PruneVarSubMean substracts the mean from each feature and removes features that # have zero variance .", "label": "", "metadata": {}, "score": "58.159264"}
{"text": "[ 0006 ] These and other embodiments can optionally include one or more of the following features .Generating phrase candidates further includes extracting n+k - grams from a collection of documents as extracted candidate phrases .The candidate phrases exclude the seed phrases .", "label": "", "metadata": {}, "score": "58.277336"}
{"text": "4 is a block / flow diagram of a training procedure that produces weights for document embedding according to the present principles .[0017 ]FIG .5 is a block / flow diagram of an alternative embodiment of weighting n - grams according to the present principles .", "label": "", "metadata": {}, "score": "58.38174"}
{"text": "The resulting queries are used as candidate phrases for the topic associated with the particular Web site .[ 0043 ] In some other implementations , phrase candidates are generated using expectation regularization .FIG .3 shows a flowchart of an example method 300 of generating candidate phrases using expectation regularization .", "label": "", "metadata": {}, "score": "58.39405"}
{"text": "The one - class classifier is # typically used to estimate the support of a high - dimesnional distribution .# For more details see e.g. # B. Schoelkopf et al .Estimating the support of a high - dimensional # distribution .", "label": "", "metadata": {}, "score": "58.69732"}
{"text": "The one - class classifier is # typically used to estimate the support of a high - dimesnional distribution .# For more details see e.g. # B. Schoelkopf et al .Estimating the support of a high - dimensional # distribution .", "label": "", "metadata": {}, "score": "58.69732"}
{"text": "The output of the supervised improvement technique is a weight correction for one or more of the phrase candidates .[0073 ]The system can use a correction step during supervised improvement , for example , to remove erroneous phrases from the collection of candidate phrases or modify weights to generate correct classification results .", "label": "", "metadata": {}, "score": "58.71592"}
{"text": "Support vector machines ( SVMs ) were trained with the selected sequence features to predict siRNA potency .The SVM learning algorithm has been applied to a variety of biological problems for pattern classification , and may have superior generalization power with the ability to avoid overfitting [ 18 ] .", "label": "", "metadata": {}, "score": "58.817326"}
{"text": "# # It was used in # # K. Tsuda , M. Kawanabe , G. Raetsch , S. Sonnenburg , and K.R. Mueller .A new # discriminative kernel from probabilistic models .Neural Computation , # 14:2397 - 2414 , 2002 .", "label": "", "metadata": {}, "score": "58.855694"}
{"text": "It # computes the chi - squared distance between sets of histograms .It is a very # useful distance in image recognition ( used to detect objects ) .The preprocessor # NormOne , normalizes vectors to have norm 1 .", "label": "", "metadata": {}, "score": "58.892742"}
{"text": "It # computes the chi - squared distance between sets of histograms .It is a very # useful distance in image recognition ( used to detect objects ) .The preprocessor # NormOne , normalizes vectors to have norm 1 .", "label": "", "metadata": {}, "score": "58.892742"}
{"text": "It # computes the chi - squared distance between sets of histograms .It is a very # useful distance in image recognition ( used to detect objects ) .The preprocessor # NormOne , normalizes vectors to have norm 1 .", "label": "", "metadata": {}, "score": "58.892742"}
{"text": "# In this example a two - class support vector machine classifier is trained on a # DNA splice - site detection data set and the trained classifier is used to predict # labels on test set .# # For more details on the SVM^light see # T. Joachims .", "label": "", "metadata": {}, "score": "58.90238"}
{"text": "[ 0033 ] With the goal of classifying the whole document into certain classes , the whole - document may be represented based on its included n - grams .The structured spatial patterns in natural language text could not be captured by the unstructured \" bag \" assumption of BOW .", "label": "", "metadata": {}, "score": "58.903008"}
{"text": "In particular , human raters can be used to manually rate the sampled documents as belonging to the given topic or not ( e.g. , a score of 1 or 0 ) .One or more raters can be used for each document .", "label": "", "metadata": {}, "score": "58.951504"}
{"text": "The network communications module 618 includes various components for establishing and maintaining network connections ( e.g. , software for implementing communication protocols , such as TCP / IP , HTTP , Ethernet , etc . ) .[ 0089 ] The phrase generator 620 and document classifier 622 provide various software components for performing the various functions for generating candidate phrases for use in training a document classifier and classifying documents as belonging to a topic as described with respect to FIGS . 1 - 5 .", "label": "", "metadata": {}, "score": "59.220993"}
{"text": "[0037 ]Referring now to FIG .5 , an alternative embodiment for modeling the spatial evidence in a document is shown .This method models longer sequence patterns .Considering the fact that text documents have variable length , each document is uniformly split into K equal subparts according to an original linear ordering in block 502 .", "label": "", "metadata": {}, "score": "59.223225"}
{"text": "The model may later be re - loaded using the read_model ( ) method .The model is written using SVMLight 's write_model ( ) C function , so it will be fully compatible with SVMLight command - line tools like svm_classify .", "label": "", "metadata": {}, "score": "59.336426"}
{"text": "While there are many possibilities for the aggregation function , a mean value function provides a good summarization of the document 's sentiment in this latent space .One can also use a maximizing function that selects the maximum value along each latent dimension .", "label": "", "metadata": {}, "score": "59.43017"}
{"text": "The application further claims priority to provisional application Ser .No .61/647,012 , filed on May 15 , 2012 , incorporated herein by reference .BACKGROUND .[0002 ] 1 .Technical Field .[ 0003 ] The present invention relates to document classification and , more particularly , to document classification using supervised weighted n - gram embedding .", "label": "", "metadata": {}, "score": "59.59328"}
{"text": "# # The distance initialized by two data sets ( the same data set as shown in the # first call ) and norm ' k ' controls the processing of the given data points , # where a pairwise distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "59.66514"}
{"text": "# # The distance initialized by two data sets ( the same data set as shown in the # first call ) and norm ' k ' controls the processing of the given data points , # where a pairwise distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "59.66514"}
{"text": "# # The distance initialized by two data sets ( the same data set as shown in the # first call ) and norm ' k ' controls the processing of the given data points , # where a pairwise distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "59.66514"}
{"text": "[0081 ] When no additional iterations are performed , the system generates 524 a classifier using the phrase candidates and assigned weights from the last iteration .In some implementations , the number and type of iterations is fixed by a specified schedule .", "label": "", "metadata": {}, "score": "59.66765"}
{"text": "To validate the important features selected by the RFs , support vector machines ( SVMs ) were trained with these features for siRNA classification .Classifier performance was evaluated using a fivefold cross - validation approach .As shown in Table 3 , the SVM classifier , named RF_Features , achieved 70.71 % overall accuracy with 73.94 % sensitivity and 66.08 % specificity .", "label": "", "metadata": {}, "score": "59.677948"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .[ 0016 ] FIG .1 shows a flowchart of an example method of classifying a document .[0017 ]FIG .2 shows a flowchart of an example method of generating candidate phrases using n+k - grams .", "label": "", "metadata": {}, "score": "59.683144"}
{"text": "[ 0082 ] Once the linear classifier has been generated for the identified and weighted phrase candidates , the resulting phrases candidates are to classify documents as belonging to a particular topic .For example , documents can be input to the classifier and decisions output as to whether or not the document belongs to the topic classified by the classifier .", "label": "", "metadata": {}, "score": "59.975433"}
{"text": "The phrase candidates are ordered 510 by the score s and the weights are assigned 512 f(s ) with a monotonic function f. .A threshold is used to decide whether a given phrase has a particular assigned weight value ( e.g. , whether to give the phrase a weight of 2 or 3 based on the output score of the initial classifier ) .", "label": "", "metadata": {}, "score": "60.11014"}
{"text": "To completely capturing such relationships would require full semantic understanding , which is beyond the current state of technology .SUMMARY .[ 0011 ] These and other features and advantages will become apparent from the following detailed description of illustrative embodiments thereof , which is to be read in connection with the accompanying drawings .", "label": "", "metadata": {}, "score": "60.185764"}
{"text": "In particular , using scores generated from phrase candidates only having positive weights can bias the classification of topicality to longer length documents .However , whether a given topic is affected by document length and to what degree can vary .", "label": "", "metadata": {}, "score": "60.328625"}
{"text": "In word embedding , because individual words carry significant semantic information , a mapping of each word is projected into a real - valued vector space in block 202 .Specifically , each word w j \u03b5 is embedded into an m - dimensional feature space using a lookup table LT E ( . ) defined as .", "label": "", "metadata": {}, "score": "60.33272"}
{"text": "Also the concantation operation keeps the original linear ordering between subsequences that are useful for the classification of a document .[ 0039 ] Document classification in block 110 may be performed using a linear projection function .Given the document embedding d x , and C candidate classes , W represents a linear projection matrix which projects the embedding representation into a vector with size C. The output vector from this classifier layer is , .", "label": "", "metadata": {}, "score": "60.342445"}
{"text": "The system also receives 304 an estimate for a fraction of all documents belonging to a given topic .Again , human experts can provide the estimates .The system uses 306 the received seed phrases and estimates as input along with a small set of labeled documents and a larger set of unlabeled documents to train a linear classifier that reproduces these fractions as closely as possible .", "label": "", "metadata": {}, "score": "60.49023"}
{"text": "3 shows a flowchart of an example method of generating candidate phrases using expectation regularization .[ 0019 ]FIG .4 shows a flowchart of an example method of filtering candidate phrases using web search .[ 0020 ] FIG .", "label": "", "metadata": {}, "score": "60.801125"}
{"text": "[ 0035 ]FIG .2 shows a flowchart of an example method 200 of generating candidate phrases using n+k - grams .For convenience , the method 200 is described with respect to a system that performs the method 200 .", "label": "", "metadata": {}, "score": "61.218987"}
{"text": "Consequently , the definition of w can be modified to select lower weights and/or to remove features with low weight in order to avoid misclassification of documents .[ 0069 ] Supervised Improvement .[0070 ] When the system determines 506 that supervised improvement is performed , the system samples 514 documents of a specific approximate length .", "label": "", "metadata": {}, "score": "61.244873"}
{"text": "It % computes the chi - squared distance between sets of histograms .It is a very % useful distance in image recognition ( used to detect objects ) .The preprocessor % PruneVarSubMean substracts the mean from each feature and removes features that % have zero variance .", "label": "", "metadata": {}, "score": "61.248577"}
{"text": "The Algorithm::SVMLight perl interface is copyright ( C ) 2005 - 2008 Thomson Legal & Regulatory , and written by Ken Williams .It is free software ; you can redistribute it and/or modify it under the same terms as perl itself .", "label": "", "metadata": {}, "score": "61.27613"}
{"text": "# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance ( extended Jaccard coefficient ) matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "61.292206"}
{"text": "FIG .2 is a block / flow diagram of a method for embedding n - grams in a latent space .[ 0015 ] FIG .3 is a block / flow diagram of a method for weighting n - grams to model a document according to the present principles .", "label": "", "metadata": {}, "score": "61.331787"}
{"text": "As a result , the SVM has to \" pay more \" to change the weight assigned to the phrase , i.e. a change of that weight counts more towards the penalty introduced with the regularization .One way to identify phrases that have a high level of confidence is to identify those already observed in a number of documents in previous steps ( e.g. , in previous iterations ) .", "label": "", "metadata": {}, "score": "61.51568"}
{"text": "It is a very # useful distance in image recognition ( used to detect objects ) .The preprocessor # LogPlusOne adds one to a dense real - valued vector and takes the logarithm of # each component of it .It is most useful in situations where the inputs are # counts : When one compares differences of small counts any difference may matter # a lot , while small differences in large counts do n't .", "label": "", "metadata": {}, "score": "61.60683"}
{"text": "It is a very # useful distance in image recognition ( used to detect objects ) .The preprocessor # LogPlusOne adds one to a dense real - valued vector and takes the logarithm of # each component of it .It is most useful in situations where the inputs are # counts : When one compares differences of small counts any difference may matter # a lot , while small differences in large counts do n't .", "label": "", "metadata": {}, "score": "61.60683"}
{"text": "It is a very # useful distance in image recognition ( used to detect objects ) .The preprocessor # LogPlusOne adds one to a dense real - valued vector and takes the logarithm of # each component of it .It is most useful in situations where the inputs are # counts : When one compares differences of small counts any difference may matter # a lot , while small differences in large counts do n't .", "label": "", "metadata": {}, "score": "61.60683"}
{"text": "It is a very # useful distance in image recognition ( used to detect objects ) .The preprocessor # LogPlusOne adds one to a dense real - valued vector and takes the logarithm of # each component of it .It is most useful in situations where the inputs are # counts : When one compares differences of small counts any difference may matter # a lot , while small differences in large counts do n't .", "label": "", "metadata": {}, "score": "61.60683"}
{"text": "As above , equation 6 may be used to calculate the sub - part embedding representations p k T , with the representation of the whole - document being defined as , . which is a vector having size KM .The document vector is built from the concantation of the embedding vectors from its K subsequences in block 512 .", "label": "", "metadata": {}, "score": "61.769424"}
{"text": "The DASVM internally computes a custom linear term # ( for the underlying quadratic program of the dual formulation of the SVM ) # based on the support vectors of the source SVM and the training examples # of the target SVM .", "label": "", "metadata": {}, "score": "61.846558"}
{"text": "The DASVM internally computes a custom linear term # ( for the underlying quadratic program of the dual formulation of the SVM ) # based on the support vectors of the source SVM and the training examples # of the target SVM .", "label": "", "metadata": {}, "score": "61.846558"}
{"text": "The DASVM internally computes a custom linear term # ( for the underlying quadratic program of the dual formulation of the SVM ) # based on the support vectors of the source SVM and the training examples # of the target SVM .", "label": "", "metadata": {}, "score": "61.846558"}
{"text": "The one - class classifier is % typically used to estimate the support of a high - dimesnional distribution .%For more details see e.g. % B. Schoelkopf et al .Estimating the support of a high - dimensional % distribution .", "label": "", "metadata": {}, "score": "61.848946"}
{"text": "A deep neural network learns the parameters of the latent space , the article modeling layers , and the classifier jointly in one end - to - end discriminative framework .[ 0020 ] Compared to BON strategy using feature selection or the n - gram embedding based method , an advantage of the present principles is that , the whole - document modeling using the controller module provides a spatial weighting scheme for different subparts using the supervised classification signals .", "label": "", "metadata": {}, "score": "61.97715"}
{"text": "[ 0024 ] The system receives 102 a collection of candidate phrases .The collection of candidate phrases can be received from an external source of pre - generated candidate phrases or , alternatively , the candidate phrases can be generated by the system .", "label": "", "metadata": {}, "score": "62.17473"}
{"text": "Block 112 uses the results of the classification to \" backpropagate \" and train the weights and document embeddings used in blocks 106 and 108 .[ 0024 ] Embodiments described herein may be entirely hardware , entirely software or including both hardware and software elements .", "label": "", "metadata": {}, "score": "62.313095"}
{"text": "# In this example a two - class linear support vector machine classifier is trained # on a toy data set and the trained classifier is used to predict labels of test # examples .# # For more details on the OCAS solver see # V. Franc , S. Sonnenburg .", "label": "", "metadata": {}, "score": "62.363747"}
{"text": "# In this example a two - class linear support vector machine classifier is trained # on a toy data set and the trained classifier is used to predict labels of test # examples .# # For more details on the OCAS solver see # V. Franc , S. Sonnenburg .", "label": "", "metadata": {}, "score": "62.363747"}
{"text": "# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .# Each column of the matrices corresponds to one data point .# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance ( extended Jaccard coefficient ) matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "62.407066"}
{"text": "# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .# Each column of the matrices corresponds to one data point .# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance ( extended Jaccard coefficient ) matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "62.407066"}
{"text": "The method of claim 1 , wherein embedding the input text in the latent space is calculated as a weighted sum over the embedded n - grams in the input text .The method of claim 1 , wherein a weight of each n - gram is modeled as an output of a nonlinear function using a mixture model on a relative position of the n - gram in the input text .", "label": "", "metadata": {}, "score": "62.47512"}
{"text": "Additionally , the URLs of the results can lead the human expert to identify additional keywords to a list commonly used in URLs of the topic .These can then be used to automatically identify a larger collection of potential keywords and phrase candidates .", "label": "", "metadata": {}, "score": "62.573204"}
{"text": "This can be useful for model inspection , to see which features are having the greatest impact on decision - making .The first element ( position 0 ) of the array will be the threshold b , and the rest of the elements will be the weights themselves .", "label": "", "metadata": {}, "score": "62.636246"}
{"text": "Referring to FIG .3 , a method for reweighting n - grams to model a document is shown .The weights for each n - gram \u03b3 j \u03b5x are learned based on the n - gram 's position in the text , as determined in block 302 .", "label": "", "metadata": {}, "score": "62.637596"}
{"text": "4 shows a flowchart of an example method 400 of filtering candidate phrases using Web search .For convenience , the method 400 is described with respect to a system that performs the method 400 .[0052 ] The system receives 402 a collection of candidate phrases .", "label": "", "metadata": {}, "score": "62.984467"}
{"text": "Block 108 uses the weighted vectors to obtain a fixed - dimension representation of the document , a process called \" embedding . \"The resulting vector representation of the text document is used in block 110 to classify the document according to a predetermined classification system .", "label": "", "metadata": {}, "score": "62.989708"}
{"text": "While there are many possibilities to combine latent n - grams into a document embedding vector , an averaging strategy is described herein .Formally , the document representation is defined as : .In other words , d x is the centroid of the vector associated with n - grams of the document x. Using sentiment classification as a test case , the sentiment polarity of a document is intuitively related to the aggregated semantic or polarity of all its n - grams .", "label": "", "metadata": {}, "score": "63.023987"}
{"text": "The document is classified as belonging to the topic in question if the sum of the values for all of the phrases occurring in the document exceeds a specified threshold .SUMMARY .[0004 ] This specification describes technologies relating to constructing text classifiers .", "label": "", "metadata": {}, "score": "63.07341"}
{"text": "In such a scenario , the system assigns a score of 1 to documents in the collection on the topic and a score of zero to other documents .[ 0061 ] For a given set of phrase candidates , it can be assumed that most of the phrase candidates actually belong to the topic in question or that it is known that some portion of the phrase candidates are actually indicative of the topic .", "label": "", "metadata": {}, "score": "63.25997"}
{"text": "# # For more details on the MPD solver see # Kienzle , W. and B. Sch\u00f6lkopf : Training Support Vector Machines with Multiple # Equality Constraints .Machine Learning : ECML 2005 , 182 - 193 .( Eds . )", "label": "", "metadata": {}, "score": "63.2906"}
{"text": "# # For more details on the MPD solver see # Kienzle , W. and B. Sch\u00f6lkopf : Training Support Vector Machines with Multiple # Equality Constraints .Machine Learning : ECML 2005 , 182 - 193 .( Eds . )", "label": "", "metadata": {}, "score": "63.2906"}
{"text": "# # For more details on the MPD solver see # Kienzle , W. and B. Sch\u00f6lkopf : Training Support Vector Machines with Multiple # Equality Constraints .Machine Learning : ECML 2005 , 182 - 193 .( Eds . )", "label": "", "metadata": {}, "score": "63.2906"}
{"text": "0004 ] 2 .Description of the Related Art .[ 0005 ] The task of document classification is defined as automatic assignment of one or more categorical labels to a given document .Examples of document classification include topic categorization , sentiment analysis , and formality studies .", "label": "", "metadata": {}, "score": "63.665443"}
{"text": "The system receives 502 a collection of phrase candidates .The phrase candidates can be generated as described above or received as a pre - generated collection .The received phrase candidates are presumed to be associated with a given topic of interest .", "label": "", "metadata": {}, "score": "63.78467"}
{"text": "Here , E w j \u03b5R m is the embedding of the word w j in the dictionary and m denotes the target word embedding dimensionality .It is important to note that the parameters of E are automatically trained during the learning process using backpropagation , discussed below .", "label": "", "metadata": {}, "score": "63.869896"}
{"text": "Methods .Data .As described in the previous study [ 7 ] , a non - redundant set of experimentally evaluated siRNAs were collected from several published studies .For each siRNA , the relative level of target gene mRNA was the ratio of the remaining mRNA level after siRNA treatment to the wild - type control level .", "label": "", "metadata": {}, "score": "63.924736"}
{"text": "[0044 ] The system receives 302 a set of seed phrases with an estimate of a fraction of documents belonging to a given topic among all documents containing the feature .The set of seed phrases can be obtained from human experts .", "label": "", "metadata": {}, "score": "63.97197"}
{"text": "# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .# Each column of the matrices corresponds to one data point .# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "63.975784"}
{"text": "# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .# Each column of the matrices corresponds to one data point .# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "63.975784"}
{"text": "# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .# Each column of the matrices corresponds to one data point .# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "63.975784"}
{"text": "Those phrases are identified as candidate phrases .[ 0025 ] The candidate phrases can be n - grams of text extracted from the documents .In particular , a phrase can be defined as a specified sequence of words that have a particular semantic meaning when taken alone .", "label": "", "metadata": {}, "score": "64.10664"}
{"text": "You are responsible for obtaining an appropriate license for SVMLight if you intend to use Algorithm::SVMLight .In particular , please note that SVMLight \" is granted free of charge for research and education purposes .However you must obtain a license from the author to use it for commercial purposes . \"", "label": "", "metadata": {}, "score": "64.204"}
{"text": "m . %In this example a two - class linear support vector machine classifier is trained % on a toy data set and the trained classifier is used to predict labels of test % examples .As training algorithm the steepest descent subgradient algorithm is % used .", "label": "", "metadata": {}, "score": "64.2473"}
{"text": "\" [ 0003 ] Thus , it is useful to be able to classify documents ( e.g. , particular Web pages or Web sites as a whole ) as belonging to certain topics .One conventional technique for classifying documents is to use a linear classifier that uses the document text .", "label": "", "metadata": {}, "score": "64.42616"}
{"text": "# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "64.51459"}
{"text": "# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "64.51459"}
{"text": "0067 ] In this scenario an automatic procedure based on a \" Naive Bayes assumption \" can alternatively be used to assign weights automatically .The Naive Bayes assumption is that the occurrence of different phrase candidates are statistically independent given the topic of the document .", "label": "", "metadata": {}, "score": "64.74573"}
{"text": "The output of the learning process is a decision function that assigns a label to each pair ( s , n ) of score and document length .The decision function can be generated using various learning techniques , for example , adaptive boosting \" AdaBoost \" or support vector machines \" SVM 's \" .", "label": "", "metadata": {}, "score": "64.802505"}
{"text": "[0038 ]The system extracts 206 n+k - grams from each piece of text separately so that they do not cross piece boundaries .For each language , the system defines n ( the base order of an n+k - gram ) and a list of skip words .", "label": "", "metadata": {}, "score": "64.88219"}
{"text": "The preprocessor # PruneVarSubMean substracts the mean from each feature and removes features that # have zero variance ./data / fm_train_real ./data / fm_test_real . /examples / documented / octave_modular / classifier_domainadaptationsvm_modular .m . %In this example we demonstrate how to use SVMs in a domain adaptation % scenario .", "label": "", "metadata": {}, "score": "64.971504"}
{"text": "0085 ] This correction can be applied to all chains of n - grams X ik where X ik is a substring of X i ( k+1 ) by sorting the n - grams according to length and iteratively subtracting the weights of the shorter n - grams from all their parents .", "label": "", "metadata": {}, "score": "65.03644"}
{"text": "For a fixed number of trees in the forest , the larger importance score a variable has , the more important it is for classification .In addition , a z - score can be obtained by dividing the variable importance score by its standard error , and a statistical significance level may be assigned to the z - score assuming normality [ 17 ] .", "label": "", "metadata": {}, "score": "65.07325"}
{"text": "This projection of n - grams can be performed with a word embedding or with direct phrase embedding .Using a sparse vector representation , a unigram word w j can be described as a vector .[0031 ] Referring now to FIG .", "label": "", "metadata": {}, "score": "65.097694"}
{"text": "# For more details see e.g. # B. Schoelkopf et al .Estimating the support of a high - dimensional # distribution .Neural Computation , 13 , 2001 , 1443 - 1471 ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / classifier_mpdsvm_modular.py .", "label": "", "metadata": {}, "score": "65.1142"}
{"text": "# For more details see e.g. # B. Schoelkopf et al .Estimating the support of a high - dimensional # distribution .Neural Computation , 13 , 2001 , 1443 - 1471 ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / classifier_mpdsvm_modular.py .", "label": "", "metadata": {}, "score": "65.1142"}
{"text": "# For more details see e.g. # B. Schoelkopf et al .Estimating the support of a high - dimensional # distribution .Neural Computation , 13 , 2001 , 1443 - 1471 ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / classifier_mpdsvm_modular.py .", "label": "", "metadata": {}, "score": "65.1142"}
{"text": "Most of the previous studies selected some siRNA features based on empirical knowledge or simple statistical analysis ( e.g. , correlation analysis ) .More recently , Klingelhoefer et al .[ 13 ] used a stochastic logistic regression - based algorithm to identify relevant features associated with siRNA potency .", "label": "", "metadata": {}, "score": "65.12621"}
{"text": "0099 ] While this specification contains many specific implementation details , these should not be construed as limitations on the scope of any implementation or of what may be claimed , but rather as descriptions of features that may be specific to particular embodiments of particular implementations .", "label": "", "metadata": {}, "score": "65.24376"}
{"text": "[ 0006 ] Previous techniques applied to this task are either generative or discriminative supervised methods .Discriminative document classification techniques commonly rely on the so - called \" bag - of - words \" ( BoW ) representation that maps text articles of variable lengths into a fixed - dimensional vector space , parameterized by a finite vocabulary .", "label": "", "metadata": {}, "score": "65.27391"}
{"text": "# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .# Each column of the matrices corresponds to one data point .# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "65.40605"}
{"text": "# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .# Each column of the matrices corresponds to one data point .# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "65.40605"}
{"text": "# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .# Each column of the matrices corresponds to one data point .# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "65.40605"}
{"text": "# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .# Each column of the matrices corresponds to one data point .# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "65.40605"}
{"text": "# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .# Each column of the matrices corresponds to one data point .# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "65.40605"}
{"text": "# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .# Each column of the matrices corresponds to one data point .# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "65.40605"}
{"text": "# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .# Each column of the matrices corresponds to one data point .# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "65.40605"}
{"text": "# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .# Each column of the matrices corresponds to one data point .# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "65.40605"}
{"text": "# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .# Each column of the matrices corresponds to one data point .# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "65.40605"}
{"text": "# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .# Each column of the matrices corresponds to one data point .# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance ( dissimilarity ratio ) matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "65.4919"}
{"text": "# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .# Each column of the matrices corresponds to one data point .# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance ( dissimilarity ratio ) matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "65.4919"}
{"text": "# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .# Each column of the matrices corresponds to one data point .# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance ( dissimilarity ratio ) matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "65.4919"}
{"text": "In some implementations , the document text is further processed to drop any remaining punctuation , convert the text to lowercase , and remove text duplicates .[0037 ]For example , if the only punctuation character specified was a comma , the following HTML code : . would be broken into the following pieces of text : .", "label": "", "metadata": {}, "score": "65.49834"}
{"text": "Sentimental text binary classification predicts a binary sentiment , positive or negative , expressed by a given document , whereas news text categorization predicts the semantic class a news text piece belongs to .[ 0029 ] To overcome the dimensionality burdens that high - order models impose , the present principles project n - grams to a low - dimensional latent semantic space at block 104 using only unigram word dictionaries .", "label": "", "metadata": {}, "score": "65.53822"}
{"text": "[0012 ]The disclosure will provide details in the following description of preferred embodiments with reference to the following figures wherein : . [ 0013 ] FIG .1 is a block / flow diagram of a method for document classification according to the present principles .", "label": "", "metadata": {}, "score": "65.72705"}
{"text": "This is since even documents that do not belong to the particular topic have a certain probability of containing one or more of the candidate phrases for the topic .However , for longer documents there is a greater probability of such a phrase occurring than for shorter documents .", "label": "", "metadata": {}, "score": "65.8607"}
{"text": "0045 ] The system obtains 308 new candidate phrases from the output classifier .To obtain new phrases indicative of a given topic , the system examines those phrases of the output classifier having highest coefficients ( i.e. , the weights from the classifier where the sum for each phrase would be used when classifying documents based on the output classifier alone ) .", "label": "", "metadata": {}, "score": "65.922775"}
{"text": "Starting with each object being # assigned to its own cluster clusters are iteratively merged .Here the clusters # are merged that have the closest ( minimum distance , here set via the Euclidean # distance object ) two elements .", "label": "", "metadata": {}, "score": "66.18159"}
{"text": "100 , pp .5591 - 5596 ) ./data / fm_train_real . /examples / documented / python_modular / converter_isomap_modular.py .# In this example toy data is being processed using the Isomap algorithm # as described in # # Silva , V. D. , & Tenenbaum , J. B. ( 2003 ) .", "label": "", "metadata": {}, "score": "66.45359"}
{"text": "The computer - readable medium 612 further includes an operating system 616 ( e.g. , Mac OS \u00ae , Windows \u00ae , Linux , etc . ) , a network communication module 618 , a phrase generator 620 , and a document classifier 622 .", "label": "", "metadata": {}, "score": "66.752686"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / kernel_top_modular.py .# The class TOPFeatures implements TOP kernel features obtained from # two Hidden Markov models .# # It was used in # # K. Tsuda , M. Kawanabe , G. Raetsch , S. Sonnenburg , and K.R. Mueller .", "label": "", "metadata": {}, "score": "66.75401"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / kernel_top_modular.py .# The class TOPFeatures implements TOP kernel features obtained from # two Hidden Markov models .# # It was used in # # K. Tsuda , M. Kawanabe , G. Raetsch , S. Sonnenburg , and K.R. Mueller .", "label": "", "metadata": {}, "score": "66.75401"}
{"text": "[ 9 ] also constructed an SVM classifier to identify hyperfunctional siRNAs by using simple sequence and structural features such as base composition at each position , GC content , and secondary structure .Artificial neural networks ( ANNs ) have been trained for regression analysis of siRNA efficacy .", "label": "", "metadata": {}, "score": "66.99085"}
{"text": "Structure . /examples / documented / python_modular / structure_dynprog_modular.py .# In this example we use the dynamic progaramm implementation with a # gene finding specific model .The model and the training parameter # are stored in a file and are used to create a gene prediction on # some example sequence .", "label": "", "metadata": {}, "score": "66.99258"}
{"text": "Structure . /examples / documented / python_modular / structure_dynprog_modular.py .# In this example we use the dynamic progaramm implementation with a # gene finding specific model .The model and the training parameter # are stored in a file and are used to create a gene prediction on # some example sequence .", "label": "", "metadata": {}, "score": "66.99258"}
{"text": "Structure . /examples / documented / python_modular / structure_dynprog_modular.py .# In this example we use the dynamic progaramm implementation with a # gene finding specific model .The model and the training parameter # are stored in a file and are used to create a gene prediction on # some example sequence .", "label": "", "metadata": {}, "score": "66.99258"}
{"text": "It % computes the chi - squared distance between sets of histograms .It is a very % useful distance in image recognition ( used to detect objects ) .The preprocessor % NormOne , normalizes vectors to have norm 1 .", "label": "", "metadata": {}, "score": "67.10641"}
{"text": "The initial classifier is a classifier that assigns a score to a document such that the higher the score is , the more likely that the document belongs to the specified topic .If no initial classifier is given , the phrase candidates themselves can be used as a classifier by assigning each of them the weight 1 . [ 0060 ] The initial classifier can be a basic linear classifier constructed using a few phrase candidates ( e.g. , to provide a rough estimate of whether documents belong to a topic ) , or an older classifier that is to be improved .", "label": "", "metadata": {}, "score": "67.21852"}
{"text": "0053 ] The system specifies 408 a threshold on a number of off - topic results that a candidate phrase can yield ( e.g. , a number from 1 - 10 ) .The system then drops 410 candidate phrases that exceed the specified threshold .", "label": "", "metadata": {}, "score": "67.236244"}
{"text": "The method of claim 8 , wherein the weights are learned using a stochastic gradient descent .The method of claim 1 , wherein classifying includes applying a classification having three or more classes .The system of claim 11 , wherein the document embedding module is further configured to embed the input text in the latent space as a weighted sum over the embedded n - grams in the input text .", "label": "", "metadata": {}, "score": "67.35795"}
{"text": "# Each column of the matrices corresponds to one data point .# # The distance initialized by two data sets ( the same data set as shown in the # first call ) controls the processing of the given data points , where a pairwise # distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "67.36839"}
{"text": "[ 0032 ] Additionally , monotonic regression can be used in place of a decision function , e.g. , AdaBoost or SVM 's , in order to provide a probability that a document belongs to the topic and learned in one step .", "label": "", "metadata": {}, "score": "67.546234"}
{"text": "/data / fm_test_real . /examples / documented / r_modular / kernel_top_modular .R .# The class TOPFeatures implements TOP kernel features obtained from # two Hidden Markov models .# # It was used in # # K. Tsuda , M. Kawanabe , G. Raetsch , S. Sonnenburg , and K.R. Mueller .", "label": "", "metadata": {}, "score": "67.558395"}
{"text": "/data / fm_test_real . /examples / documented / r_modular / kernel_top_modular .R .# The class TOPFeatures implements TOP kernel features obtained from # two Hidden Markov models .# # It was used in # # K. Tsuda , M. Kawanabe , G. Raetsch , S. Sonnenburg , and K.R. Mueller .", "label": "", "metadata": {}, "score": "67.558395"}
{"text": "In the previous study by S trom [ 7 ] , a similar dataset of siRNAs was used to construct SVM and genetic programming ( GP ) classifiers .However , it is not straightforward to compare RF_Features with the other existing models .", "label": "", "metadata": {}, "score": "67.568756"}
{"text": "Thus , if a feature often occurs alone on a document , the feature may not be a good phrase .In some implementations , where a part of the collection has a higher confidence than other parts , then only the portion with a high confidence is used .", "label": "", "metadata": {}, "score": "67.58362"}
{"text": "In some implementations , long documents and short documents are considered separately or excluded since they may require different weighting thresholds .[ 0066 ] In some implementations , the initial classifier assigns a score of 1 for a document classified as belonging to the topic T and 0 otherwise ( i.e. , a binary classifier ) .", "label": "", "metadata": {}, "score": "67.61018"}
{"text": "( M+1)\u00d71 maps the vector .[ j N x , p \u03b3 j ] # # EQU00007 # # .The resulting weight value q j for phrase \u03b3 j considers not only the spatial evidence , but also the n - gram itself .", "label": "", "metadata": {}, "score": "67.65495"}
{"text": "Thus , the RF algorithm can handle many redundant features and avoid model overfitting .It has been shown that RFs outperform AdaBoost ensembles on noisy datasets , and can perform well on data with many weak input variables [ 17 ] .", "label": "", "metadata": {}, "score": "67.67041"}
{"text": "The system of claim 11 , wherein the weight of an n - gram is modeled as a function of both a relative position of the n - gram in the document and an embedding representation of the n - gram .", "label": "", "metadata": {}, "score": "67.67446"}
{"text": "If the model has not yet been trained , or if the kernel type is not linear , an exception will be thrown .Returns the number of features known to this model .This is because after training , an instance could be passed to the predict ( ) method with real values for these previously unseen features .", "label": "", "metadata": {}, "score": "67.68259"}
{"text": "Making large - scale SVM learning practical .In Advances in Kernel # Methods -- Support Vector Learning , pages 169 - 184 .MIT Press , Cambridge , MA USA , 1999 .# # For more details on the Weighted Degree kernel see # G. Raetsch , S.Sonnenburg , and B. Schoelkopf .", "label": "", "metadata": {}, "score": "67.734726"}
{"text": "Making large - scale SVM learning practical .In Advances in Kernel # Methods -- Support Vector Learning , pages 169 - 184 .MIT Press , Cambridge , MA USA , 1999 .# # For more details on the Weighted Degree kernel see # G. Raetsch , S.Sonnenburg , and B. Schoelkopf .", "label": "", "metadata": {}, "score": "67.734726"}
{"text": "Making large - scale SVM learning practical .In Advances in Kernel # Methods -- Support Vector Learning , pages 169 - 184 .MIT Press , Cambridge , MA USA , 1999 .# # For more details on the Weighted Degree kernel see # G. Raetsch , S.Sonnenburg , and B. Schoelkopf .", "label": "", "metadata": {}, "score": "67.734726"}
{"text": "Making large - scale SVM learning practical .In Advances in Kernel # Methods -- Support Vector Learning , pages 169 - 184 .MIT Press , Cambridge , MA USA , 1999 .# # For more details on the Weighted Degree kernel see # G. Raetsch , S.Sonnenburg , and B. Schoelkopf .", "label": "", "metadata": {}, "score": "67.734726"}
{"text": "In k - means clustering one tries to partition n observations into k # clusters in which each observation belongs to the cluster with the nearest mean . # The algorithm class constructor takes the number of clusters and a distance to # be used as input .", "label": "", "metadata": {}, "score": "67.85797"}
{"text": "# This is an example for the initialization of a combined kernel , which is a weighted sum of # in this case three kernels on real valued data .The sub - kernel weights are all set to 1 ./data / fm_train_real .", "label": "", "metadata": {}, "score": "67.86511"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CCosineDistance.html . # # Obviously , using the Cosine distance is not limited to this showcase # example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "67.92679"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CCosineDistance.html . # # Obviously , using the Cosine distance is not limited to this showcase # example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "67.92679"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CCosineDistance.html . # # Obviously , using the Cosine distance is not limited to this showcase # example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "67.92679"}
{"text": "obtain_from_char(charfeat , order-1 , order , gap , reverse ) ; wordfeats_test .add_preproc(preproc ) ; wordfeats_test . set_observations(wordfeats_test ) ; neg_clone . set_a(feats_train . /examples / documented / octave_modular / kernel_weighted_comm_word_string_modular .m . % % These 16bit integers correspond to k - mers .", "label": "", "metadata": {}, "score": "68.01407"}
{"text": "Having thus described aspects of the invention , with the details and particularity required by the patent laws , what is claimed and desired protected by Letters Patent is set forth in the appended claims .# In this example a multi - class support vector machine is trained on a toy data # set and the trained classifier is then used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "68.025406"}
{"text": "It was shown that the boosted GP classifier outperformed support vector machines ( SVMs ) trained on the same dataset .Ladunga [ 8 ] trained SVMs for siRNA classification based on biophysical signatures of free energy , target site accessibility and dinucleotide characteristics .", "label": "", "metadata": {}, "score": "68.09187"}
{"text": "Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages .Accurate classifiers for particular topics can be generated with only small amounts of human labeled data .[ 0015 ] The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below .", "label": "", "metadata": {}, "score": "68.09541"}
{"text": "When it 's convenient for you to organize the data in this manner , you may see speed improvements .When using a ranking SVM , it is possible to customize the cost of ranking each pair of instances incorrectly by supplying a custom Perl callback function .", "label": "", "metadata": {}, "score": "68.274956"}
{"text": "First , we create a number of base kernels and features . %The base kernels are then subsequently added to a CombinedKernel , which % contains a weight for each kernel and encapsulates the base kernels % from the training procedure .", "label": "", "metadata": {}, "score": "68.320946"}
{"text": "The \" bag - of - unigrams \" is the most common form of BoW representation that utilizes a word dictionary as its vocabulary .[0007 ] Some classification attempts have employed short phrases as being more effective than single words ( unigrams ) for the task .", "label": "", "metadata": {}, "score": "68.33824"}
{"text": "In addition to overall accuracy , sensitivity and specificity , Matthews correlation coefficient ( MCC ) is also commonly used as a measure of the quality of binary classifications [ 20 ] .MCC measures the correlation between predictions and the actual class labels .", "label": "", "metadata": {}, "score": "68.39455"}
{"text": "In particular , using statistical methods to generate candidate phrases , there are some number of phrases that are erroneously added to the collection of candidate phrases .Removing the erroneous phrases from the collection of candidate phrases can improve classifier results .", "label": "", "metadata": {}, "score": "68.43772"}
{"text": "Specifically , a latent embedding of document x in the latent space is defined as : .The weight of every \u03b3 j is modeled as a scalar q j using the following mixture model .The spatial re - weighting attempts to capture longer \" trends \" within each document and preserves spatial information for phrases within a document .", "label": "", "metadata": {}, "score": "68.56213"}
{"text": "Alternatively , in some implementations , any n - grams can be sued without being defined as phrases .Typically , n - gram phrases of order greater than 1 can provide higher quality candidate phrases .In some implementations , the order of n is fixed .", "label": "", "metadata": {}, "score": "68.65614"}
{"text": "The class TOPFeatures implements TOP kernel features obtained from % two Hidden Markov models . % % It was used in % % K. Tsuda , M. Kawanabe , G. Raetsch , S. Sonnenburg , and K.R. Mueller .A new % discriminative kernel from probabilistic models .", "label": "", "metadata": {}, "score": "68.731155"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CJensenMetric.html .# # Obviously , using the Jensen - Shannon distance / divergence is not limited to # this showcase example .", "label": "", "metadata": {}, "score": "68.85533"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CJensenMetric.html .# # Obviously , using the Jensen - Shannon distance / divergence is not limited to # this showcase example .", "label": "", "metadata": {}, "score": "68.85533"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CJensenMetric.html .# # Obviously , using the Jensen - Shannon distance / divergence is not limited to # this showcase example .", "label": "", "metadata": {}, "score": "68.85533"}
{"text": "Further details of expectation regularization techniques are described in Gideon S. Mann and Andrew McCallum , \" Simple , Robust , Scalable Semi - supervised Learning via Expectation Regularization \" in Proceedings of the 24 th International Conference on Machine Learning , Corvallis , Oreg . , 2007 .", "label": "", "metadata": {}, "score": "68.86558"}
{"text": "After calling the constructor of the HMM class specifying # the number of states and transitions the model is trained .Via the Baum - Welch # algorithm the optimal transition and emission probabilities are estimated .The # best path , i.e. the path with highest probability given the model can then be # calculated using get_best_path_state .", "label": "", "metadata": {}, "score": "68.86764"}
{"text": "After calling the constructor of the HMM class specifying # the number of states and transitions the model is trained .Via the Baum - Welch # algorithm the optimal transition and emission probabilities are estimated .The # best path , i.e. the path with highest probability given the model can then be # calculated using get_best_path_state .", "label": "", "metadata": {}, "score": "68.86764"}
{"text": "After calling the constructor of the HMM class specifying # the number of states and transitions the model is trained .Via the Baum - Welch # algorithm the optimal transition and emission probabilities are estimated .The # best path , i.e. the path with highest probability given the model can then be # calculated using get_best_path_state .", "label": "", "metadata": {}, "score": "68.86764"}
{"text": "1 , a diagram showing a method for document classification is provided .Block 102 inputs the raw text of the document .As noted above , this document may be a text sequence of any length broken into a set of unigrams separated by some appropriate token ( e.g. , a space or punctuation ) .", "label": "", "metadata": {}, "score": "68.87274"}
{"text": "The output features from the classifier will all be phrases .The non - phrase features are added as input to recognize more documents as belonging to a topic .In some implementations , all input features ( phrases included ) are discarded such that only newly generated phrases are used ( e.g. , if their coefficients / weights are above a threshold level ) .", "label": "", "metadata": {}, "score": "68.94085"}
{"text": "An approach as applied below , which shows the processing of input data % from a file becomes a crucial factor for writing your own sample applications .%This approach is just one example of what can be done using the distance % functions provided by shogun .", "label": "", "metadata": {}, "score": "68.94545"}
{"text": "An approach as applied below , which shows the processing of input data % from a file becomes a crucial factor for writing your own sample applications .%This approach is just one example of what can be done using the distance % functions provided by shogun .", "label": "", "metadata": {}, "score": "68.94545"}
{"text": "An approach as applied below , which shows the processing of input data % from a file becomes a crucial factor for writing your own sample applications .%This approach is just one example of what can be done using the distance % functions provided by shogun .", "label": "", "metadata": {}, "score": "68.94545"}
{"text": "An approach as applied below , which shows the processing of input data % from a file becomes a crucial factor for writing your own sample applications .%This approach is just one example of what can be done using the distance % functions provided by shogun .", "label": "", "metadata": {}, "score": "68.94545"}
{"text": "/data / fm_test_dna ./data / label_train_dna .set_observations(wordfeats_test ) neg_clone . /examples / documented / python_modular / kernel_weighted_comm_word_string_modular.py .# # These 16bit integers correspond to k - mers .To applicable in this kernel they # need to be sorted ( e.g. via the SortWordString pre - processor ) .", "label": "", "metadata": {}, "score": "68.96208"}
{"text": "Joachims T : Making large scale SVM learning practical .In Advances in Kernel Methods - Support Vector Learning .Edited by : Scholkopf B , Burges C , Sola A. MIT Press , Cambridge ; 1999 .Baldi P , Brunak S , Chauvin Y , Andersen CAF , Nielsen H : Assessing the accuracy of prediction algorithms for classification : an overview .", "label": "", "metadata": {}, "score": "69.04023"}
{"text": "# We currently support reading and writing compressed files using # LZO , GZIP , BZIP2 and LZMA .Furthermore , we demonstrate how to extract # compressed streams on - the - fly in order to fit data sets into # memory that would be too large , otherwise .", "label": "", "metadata": {}, "score": "69.049805"}
{"text": "Alternatively , candidate phrases can be automatically generated from a collection of documents known to belong to a topic of interest ( e.g. , labeled by human raters ) and a collection of documents known not to belong to the topic of interest .", "label": "", "metadata": {}, "score": "69.07328"}
{"text": "Via the Baum - Welch # algorithm the optimal transition and emission probabilities are estimated .The # best path , i.e. the path with highest probability given the model can then be # calculated using get_best_path_state ./data / fm_train_cube . /examples / documented / r_modular / distribution_linearhmm_modular .", "label": "", "metadata": {}, "score": "69.14911"}
{"text": "Via the Baum - Welch # algorithm the optimal transition and emission probabilities are estimated .The # best path , i.e. the path with highest probability given the model can then be # calculated using get_best_path_state ./data / fm_train_cube . /examples / documented / r_modular / distribution_linearhmm_modular .", "label": "", "metadata": {}, "score": "69.14911"}
{"text": "Abstract : .Methods , systems , and apparatus , including computer program products , for constructing text classifiers .Claims : .The method of claim 1 , where generating phrase candidates further comprises : extracting n+k - grams from a collection of documents as extracted candidate phrases .", "label": "", "metadata": {}, "score": "69.22469"}
{"text": "Here the clusters % are merged that have the closest ( minimum distance , here set via the Euclidean % distance object ) two elements ./data / fm_train_real . /examples / documented / octave_modular / clustering_kmeans_modular .m . %In this example the k - means clustering method is used to cluster a given toy % data set .", "label": "", "metadata": {}, "score": "69.25923"}
{"text": "0054 ]In some implementations , the classifier is tuned for high recall to maximize the identification of good candidate phrases at the cost of allowing some bad phrase candidates to pass through .Consequently , the classifier can classify Web pages based on the URL of a search result .", "label": "", "metadata": {}, "score": "69.29387"}
{"text": "[ 0098 ] The computing system can include clients and servers .A client and server are generally remote from each other and typically interact through a communication network .The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client - server relationship to each other .", "label": "", "metadata": {}, "score": "69.298416"}
{"text": "[0093 ] The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output .The processes and logic flows can also be performed by , and apparatus can also be implemented as , special purpose logic circuitry , e.g. , an FPGA ( field programmable gate array ) or an ASIC ( application - specific integrated circuit ) .", "label": "", "metadata": {}, "score": "69.32452"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CTanimotoDistance.html .# # Obviously , using the Tanimoto distance / coefficient is not limited to # this showcase example .", "label": "", "metadata": {}, "score": "69.44975"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CTanimotoDistance.html .# # Obviously , using the Tanimoto distance / coefficient is not limited to # this showcase example .", "label": "", "metadata": {}, "score": "69.44975"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CTanimotoDistance.html .# # Obviously , using the Tanimoto distance / coefficient is not limited to # this showcase example .", "label": "", "metadata": {}, "score": "69.44975"}
{"text": "# # First , you need to determine what type your data will be , because this # will determine the distance function you can use .# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .", "label": "", "metadata": {}, "score": "69.62938"}
{"text": "# # First , you need to determine what type your data will be , because this # will determine the distance function you can use .# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .", "label": "", "metadata": {}, "score": "69.62938"}
{"text": "# # First , you need to determine what type your data will be , because this # will determine the distance function you can use .# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .", "label": "", "metadata": {}, "score": "69.62938"}
{"text": "# # First , you need to determine what type your data will be , because this # will determine the distance function you can use .# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .", "label": "", "metadata": {}, "score": "69.62938"}
{"text": "# # First , you need to determine what type your data will be , because this # will determine the distance function you can use .# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .", "label": "", "metadata": {}, "score": "69.62938"}
{"text": "# # First , you need to determine what type your data will be , because this # will determine the distance function you can use .# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .", "label": "", "metadata": {}, "score": "69.62938"}
{"text": "# # First , you need to determine what type your data will be , because this # will determine the distance function you can use .# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .", "label": "", "metadata": {}, "score": "69.62938"}
{"text": "# # First , you need to determine what type your data will be , because this # will determine the distance function you can use .# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .", "label": "", "metadata": {}, "score": "69.62938"}
{"text": "# # First , you need to determine what type your data will be , because this # will determine the distance function you can use .# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .", "label": "", "metadata": {}, "score": "69.62938"}
{"text": "# # First , you need to determine what type your data will be , because this # will determine the distance function you can use .# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .", "label": "", "metadata": {}, "score": "69.62938"}
{"text": "# # First , you need to determine what type your data will be , because this # will determine the distance function you can use .# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .", "label": "", "metadata": {}, "score": "69.62938"}
{"text": "# # First , you need to determine what type your data will be , because this # will determine the distance function you can use .# # This example loads two stored matrices of real values from different # files and initializes the matrices to ' RealFeatures ' .", "label": "", "metadata": {}, "score": "69.62938"}
{"text": "Although more siRNA data have recently become available and will be used in the future study , our approach was first tested on this relatively small dataset so that our findings could be compared with the previously published results .Sequence features .", "label": "", "metadata": {}, "score": "69.68261"}
{"text": "If a document is misclassified , the system adjusts the weight of all of the phrases up or down until the document is correctly classified .Testing a small set of known documents one or more times adjusts the weights such that more of the labeled documents are correctly classified by the classifier .", "label": "", "metadata": {}, "score": "69.690475"}
{"text": "This is particularly useful , when working # with genomic data , where storing all explicitly copied strings in memory # quickly becomes infeasible .In addition to a sliding window ( of a particular # length ) over all position , we also support defining a custom position # list . /examples / documented / python_modular / features_string_ulong_modular.py .", "label": "", "metadata": {}, "score": "69.83432"}
{"text": "This is particularly useful , when working # with genomic data , where storing all explicitly copied strings in memory # quickly becomes infeasible .In addition to a sliding window ( of a particular # length ) over all position , we also support defining a custom position # list . /examples / documented / python_modular / features_string_ulong_modular.py .", "label": "", "metadata": {}, "score": "69.83432"}
{"text": "This is particularly useful , when working # with genomic data , where storing all explicitly copied strings in memory # quickly becomes infeasible .In addition to a sliding window ( of a particular # length ) over all position , we also support defining a custom position # list . /examples / documented / python_modular / features_string_ulong_modular.py .", "label": "", "metadata": {}, "score": "69.83432"}
{"text": "The n - gram phrases that tend to be particularly indicated of the given topic tend to appear early in this list .The system extracts the top n - grams as candidate phrases .For example , the top 1000 n - grams can be used as candidate phrases .", "label": "", "metadata": {}, "score": "69.83621"}
{"text": "5 shows a flowchart of an example method 500 of using a collection of phrase candidates to iteratively generate a classifier .For convenience , the method 500 is described with respect to a system that performs the method 500 .[", "label": "", "metadata": {}, "score": "69.919586"}
{"text": "The system of claim 11 , wherein the document embedding module is further configured to form the embedded representation of each subpart by calculating a weighted sum over the n - grams in the subpart .The system of claim 11 , wherein the weight learning module is further configured to learn weights for each n - gram by optimizing over a set of training documents with known class labels .", "label": "", "metadata": {}, "score": "69.92781"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CEuclidianDistance.html .# # Obviously , using the Euclidian distance is not limited to this showcase # example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "70.02402"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CEuclidianDistance.html .# # Obviously , using the Euclidian distance is not limited to this showcase # example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "70.02402"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CEuclidianDistance.html .# # Obviously , using the Euclidian distance is not limited to this showcase # example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "70.02402"}
{"text": "To be applicable in this kernel the mapped k - mers have to be sorted .# This is done using the SortWordString preprocessor , which sorts the indivual # strings in ascending order .The kernel function basically uses the algorithm in # the unix \" comm \" command ( hence the name ) .", "label": "", "metadata": {}, "score": "70.03009"}
{"text": "To be applicable in this kernel the mapped k - mers have to be sorted .# This is done using the SortWordString preprocessor , which sorts the indivual # strings in ascending order .The kernel function basically uses the algorithm in # the unix \" comm \" command ( hence the name ) .", "label": "", "metadata": {}, "score": "70.03009"}
{"text": "To be applicable in this kernel the mapped k - mers have to be sorted .# This is done using the SortWordString preprocessor , which sorts the indivual # strings in ascending order .The kernel function basically uses the algorithm in # the unix \" comm \" command ( hence the name ) .", "label": "", "metadata": {}, "score": "70.03009"}
{"text": "Distance . /examples / documented / octave_modular / distance_braycurtis_modular .m . %An approach as applied below , which shows the processing of input data % from a file becomes a crucial factor for writing your own sample applications .%", "label": "", "metadata": {}, "score": "70.08315"}
{"text": "We used a fivefold cross - validation approach to evaluate the performance of SVM classifiers .Positive and negative instances were distributed randomly into five folds .In each of the five iterative steps , four of the five folds were used to train a classifier , and then the classifier was evaluated using the holdout fold ( test data ) .", "label": "", "metadata": {}, "score": "70.16502"}
{"text": "X l ( x i , y i ) # # EQU00011 # # .Stochastic gradient descent ( SGD ) may be used to optimize the above loss .SGD optimization method is scalable and proven to rival the performance of batch - mode gradient descent methods when dealing with large - scale datasets .", "label": "", "metadata": {}, "score": "70.2045"}
{"text": "To be applicable in this kernel the mapped k - mers have to be sorted .# This is done using the SortUlongString preprocessor , which sorts the indivual # strings in ascending order .The kernel function basically uses the algorithm in # the unix \" comm \" command ( hence the name ) .", "label": "", "metadata": {}, "score": "70.269424"}
{"text": "To be applicable in this kernel the mapped k - mers have to be sorted .# This is done using the SortUlongString preprocessor , which sorts the indivual # strings in ascending order .The kernel function basically uses the algorithm in # the unix \" comm \" command ( hence the name ) .", "label": "", "metadata": {}, "score": "70.269424"}
{"text": "To be applicable in this kernel the mapped k - mers have to be sorted .# This is done using the SortUlongString preprocessor , which sorts the indivual # strings in ascending order .The kernel function basically uses the algorithm in # the unix \" comm \" command ( hence the name ) .", "label": "", "metadata": {}, "score": "70.269424"}
{"text": "There are 16 features , one for the overall G / C content and 15 for local G / C contents .With a sliding window size of five nucleotides , local G / C contents are calculated for all the possible windows along a 19-nucleotide siRNA sequence .", "label": "", "metadata": {}, "score": "70.28061"}
{"text": "PubMed View Article .Breiman L : Random forests .Machine Learning 2001 , 45 : 5 - 32 .View Article .Noble WS : Support vector machine applications in computational biology .In Kernel Methods in Computational Biology .Edited by : Scholkopf B , Tsuda K , Vert JP .", "label": "", "metadata": {}, "score": "70.5139"}
{"text": "m . %In this example a multi - class support vector machine is trained on a toy data % set and the trained classifier is then used to predict labels of test % examples .The training algorithm is based on BSVM formulation ( L2-soft margin % and the bias added to the objective function ) which is solved by the Improved % Mitchell - Demyanov - Malozemov algorithm .", "label": "", "metadata": {}, "score": "70.61882"}
{"text": "The free energy of secondary structure was calculated by using the RNAfold program in the Vienna RNA package [ 16 ] .Random forests for feature selection .The RF algorithm uses a combination of independent decision trees to model data and measure variable importance [ 17 ] .", "label": "", "metadata": {}, "score": "70.66258"}
{"text": "/data / fm_train_real ./data / fm_test_real .get_labels ( ) ; . /examples / documented / octave_modular / classifier_svmocas_modular .m . %In this example a two - class linear support vector machine classifier is trained % on a toy data set and the trained classifier is used to predict labels of test % examples . % % For more details on the OCAS solver see % V. Franc , S. Sonnenburg .", "label": "", "metadata": {}, "score": "70.70405"}
{"text": "# An Introduction to Locally Linear Embedding .Available from , 290(5500 ) , 2323 - 2326 ./data / fm_train_real . /examples / documented / python_modular / converter_localtangentspacealignment_modular.py . # In this example toy data is being processed using the Local Tangent Space # Alignment ( LTSA ) algorithms as described in # # Zhang , Z. , & Zha , H. ( 2002 ) .", "label": "", "metadata": {}, "score": "70.74492"}
{"text": "0036 ] The system receives 202 a document from a collection of documents from which phrase candidates are to be extracted .The system breaks 204 the content of the document into pieces of text that are likely to constitute logical units .", "label": "", "metadata": {}, "score": "70.77906"}
{"text": "# # For more details on the SVM^light see # T. Joachims .Making large - scale SVM learning practical .In Advances in Kernel # Methods -- Support Vector Learning , pages 169 - 184 .MIT Press , Cambridge , MA USA , 1999 .", "label": "", "metadata": {}, "score": "70.793655"}
{"text": "# # For more details on the SVM^light see # T. Joachims .Making large - scale SVM learning practical .In Advances in Kernel # Methods -- Support Vector Learning , pages 169 - 184 .MIT Press , Cambridge , MA USA , 1999 .", "label": "", "metadata": {}, "score": "70.793655"}
{"text": "# # For more details on the SVM^light see # T. Joachims .Making large - scale SVM learning practical .In Advances in Kernel # Methods -- Support Vector Learning , pages 169 - 184 .MIT Press , Cambridge , MA USA , 1999 .", "label": "", "metadata": {}, "score": "70.793655"}
{"text": "It should return a real number indicating the cost .By default , SVMLight will use an internal C function assigning a cost of the average of the costfactor s for the two instances .After a sufficient number of instances have been added to your model , call train ( ) in order to actually learn the underlying discriminative Machine Learning model .", "label": "", "metadata": {}, "score": "70.79521"}
{"text": "The method of claim 12 , where performing additional iterations includes performing one or more iterations without supervised improvement and one or more iterations with supervised improvement .The system of claim 15 , where generating phrase candidates further comprises : extracting n+k - grams from a collection of documents as extracted candidate phrases .", "label": "", "metadata": {}, "score": "70.91089"}
{"text": "It is a very # useful distance in image recognition ( used to detect objects ) .The preprocessor # NormOne , normalizes vectors to have norm 1 ./data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / preprocessor_prunevarsubmean_modular .", "label": "", "metadata": {}, "score": "70.95407"}
{"text": "It is a very # useful distance in image recognition ( used to detect objects ) .The preprocessor # NormOne , normalizes vectors to have norm 1 ./data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / preprocessor_prunevarsubmean_modular .", "label": "", "metadata": {}, "score": "70.95407"}
{"text": "Strom P : Predicting the efficacy of short oligonucleotides in antisense and RNAi experiments with boosted genetic programming .Bioinformatics 2004 , 20 : 3055 - 3063 .View Article .Ladunga I : More complete gene silencing by fewer siRNAs : transparent optimized design and biophysical signature .", "label": "", "metadata": {}, "score": "70.96817"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular / kernel_oligo_string_modular.py .# This is an example initializing the oligo string kernel which takes distances # between matching oligos ( k - mers ) into account via a gaussian .", "label": "", "metadata": {}, "score": "71.075325"}
{"text": "In this example a two - class linear support vector machine classifier is trained % on a toy data set and the trained classifier is used to predict labels of test % examples . % % For more details on the SGD solver see % L. Bottou , O. Bousquet .", "label": "", "metadata": {}, "score": "71.12062"}
{"text": "/data / fm_train_real ./data / fm_test_real .get_labels ( ) ; . /examples / documented / octave_modular / classifier_mpdsvm_modular .m . %In this example a two - class support vector machine classifier is trained on a % toy data set and the trained classifier is used to predict labels of test % examples . % % For more details on the MPD solver see % Kienzle , W. and B. Sch\u00f6lkopf : Training Support Vector Machines with Multiple % Equality Constraints .", "label": "", "metadata": {}, "score": "71.17857"}
{"text": "SIAM ./data / fm_train_real . /examples / documented / python_modular / converter_multidimensionalscaling_modular.py .# In this example toy data is being processed using the multidimensional # scaling as described on p.261 ( Section 12.1 ) of # # Borg , I. , & Groenen , P. J. F. ( 2005 ) .", "label": "", "metadata": {}, "score": "71.17909"}
{"text": "/data / fm_test_cube . /examples / documented / r_modular / kernel_weighted_comm_word_string_modular .R .# # These 16bit integers correspond to k - mers .To applicable in this kernel they # need to be sorted ( e.g. via the SortWordString pre - processor ) .", "label": "", "metadata": {}, "score": "71.22012"}
{"text": "/data / fm_test_cube . /examples / documented / r_modular / kernel_weighted_comm_word_string_modular .R .# # These 16bit integers correspond to k - mers .To applicable in this kernel they # need to be sorted ( e.g. via the SortWordString pre - processor ) .", "label": "", "metadata": {}, "score": "71.22012"}
{"text": "In Figure 2 , the SVM output for each test instance is plotted against the level of gene expression inhibition .The result further suggests that the classifier RF_Features has learned some important siRNA patterns related to the efficacy of gene silencing .", "label": "", "metadata": {}, "score": "71.25542"}
{"text": "Each RF with 1000 trees selected the top 20 features based on the z - score of variable importance .Some of the common features selected by the RFs were then identified for siRNA classification .The use of multiple RFs might increase the reliability for identifying relevant features .", "label": "", "metadata": {}, "score": "71.27691"}
{"text": "Note also that this % is an unpublished work which was predecessor of the OCAS solver ( see % classifier_svmocas ) ./data / label_train_twoclass ./data / fm_train_real ./data / fm_test_real .get_labels ( ) ; . /examples / documented / octave_modular / classifier_svmlight_modular . m . %", "label": "", "metadata": {}, "score": "71.31137"}
{"text": "The preprocessor % LogPlusOne adds one to a dense real - valued vector and takes the logarithm of % each component of it .It is most useful in situations where the inputs are % counts : When one compares differences of small counts any difference may matter % a lot , while small differences in large counts do n't .", "label": "", "metadata": {}, "score": "71.318665"}
{"text": "These domains are assumed to be % different but related enough to transfer information between them . %Thus , we first train an SVM on the source domain and then subsequently % pass this previously trained SVM object to the DASVM , that we train % on the target domain .", "label": "", "metadata": {}, "score": "71.357056"}
{"text": "/data / fm_train_real . /examples / documented / python_modular /preprocessor_kernelpca_modular.py .# In this example toy data is being processed using the kernel PCA algorithm # as described in # # Sch\u00c3\u00b6lkopf , B. , Smola , A. J. , & Muller , K. R. ( 1999 ) .", "label": "", "metadata": {}, "score": "71.387314"}
{"text": "We have developed a new machine learning approach for predicting siRNA potency based on random forests and support vector machines .Since there were many potential features for siRNA classification , random forests were used for feature selection based on variable importance scores .", "label": "", "metadata": {}, "score": "71.39568"}
{"text": "/data / fm_test_dna .Mkl . /examples / documented / octave_modular / mkl_multiclass_modular .m . %In this example we show how to perform Multiple Kernel Learning ( MKL ) % with the modular interface for multi - class classification .", "label": "", "metadata": {}, "score": "71.398796"}
{"text": "[ 0091 ] The term \" data processing apparatus \" encompasses all apparatus , devices , and machines for processing data , including by way of example a programmable processor , a computer , or multiple processors or computers .A computer program does not necessarily correspond to a file in a file system .", "label": "", "metadata": {}, "score": "71.43823"}
{"text": "In this example a hidden markov model with 3 states and 6 transitions is trained % on a string data set .After calling the constructor of the HMM class specifying % the number of states and transitions the model is trained .", "label": "", "metadata": {}, "score": "71.45508"}
{"text": "Thus , the features selected by the RFs can be used to construct relatively accurate SVM models for predicting siRNA potency .For performance comparison , SVM classifiers were also constructed using all the 120 features or only the 19 siRNA sequence features ( Table 1 ) .", "label": "", "metadata": {}, "score": "71.56818"}
{"text": "/data / fm_test_dna . /examples / documented / octave_modular / kernel_weighteddegreestring_modular .m . %The Weighted Degree String kernel . % % The WD kernel of order d compares two sequences X and % Y of length L by summing all contributions of k - mer matches of % lengths k in 1 ... d , weighted by coefficients beta_k . is the indicator function % which evaluates to 1 when its argument is true and to 0 % otherwise .", "label": "", "metadata": {}, "score": "71.58365"}
{"text": "/data / fm_test_real . /examples / documented / octave_modular / kernel_linear_string_modular .m . %This is an example for the initialization of a linear kernel on string data .The % strings are all of the same length and consist of the characters ' ACGT ' corresponding % to the DNA - alphabet .", "label": "", "metadata": {}, "score": "71.63049"}
{"text": "( Eds . )/data / label_train_twoclass ./data / fm_train_real ./data / fm_test_real .get_labels ( ) ; . /examples / documented / octave_modular / classifier_perceptron_modular . m .The Perceptron algorithm works by % iteratively passing though the training examples and applying the update rule on % those examples which are misclassified by the current classifier .", "label": "", "metadata": {}, "score": "71.66568"}
{"text": "/data / DynProg_example_py .set_plif_names(all_names ) # pm .reshape(8,1 ) .T ) dyn.precompute_content_values ( ) dyn.init_mod_words_array(data_dict['model'].mod_words . features dyn.set_observation_matrix(features ) dyn.set_content_type_array(data_dict['seg_path']. astype(numpy.float64 ) ) dyn.best_path_set_segment_loss(data_dict['loss'].Tests . /examples / documented / python_modular / tests_check_commwordkernel_memleak_modular.py .", "label": "", "metadata": {}, "score": "71.66742"}
{"text": "# # For more details on the OCAS solver see # V. Franc , S. Sonnenburg .Optimized Cutting Plane Algorithm for Large - Scale Risk # Minimization .The Journal of Machine Learning Research , vol .10 , # pp .", "label": "", "metadata": {}, "score": "71.9154"}
{"text": "# # For more details on the OCAS solver see # V. Franc , S. Sonnenburg .Optimized Cutting Plane Algorithm for Large - Scale Risk # Minimization .The Journal of Machine Learning Research , vol .10 , # pp .", "label": "", "metadata": {}, "score": "71.9154"}
{"text": "# # For more details on the OCAS solver see # V. Franc , S. Sonnenburg .Optimized Cutting Plane Algorithm for Large - Scale Risk # Minimization .The Journal of Machine Learning Research , vol .10 , # pp .", "label": "", "metadata": {}, "score": "71.9154"}
{"text": "0048 ] In some implementations , identified phrase candidates are positive features , that is the features are indicative that the document belongs to the given topic .However , the phrase candidates can also include negative features that indicate that the document does not belong to the topic ( e.g. , by assigning negative weights to the negative features ) .", "label": "", "metadata": {}, "score": "72.00124"}
{"text": "[ 10 ] developed the BIOPREDsi model with nucleotide sequence information to predict the inhibitory activity of siRNAs , and used the ANN model to design a human siRNA library .Shabalina et al .[ 11 ] performed thermodynamic and correlation analyses on a set of siRNAs , and constructed an ANN model with three parameters characterizing siRNA sequences .", "label": "", "metadata": {}, "score": "72.092255"}
{"text": "The system receives query logs and associated Web sites ( e.g. , Web sites identified as responsive to the respective queries .For example , the system can obtain a top 100 queries that result in the Web site being identified as a top search result .", "label": "", "metadata": {}, "score": "72.1656"}
{"text": "The method of claim 6 , where scores for documents obtained from other classifiers are used as additional input to train the linear classifier .The method of claim 10 , further comprising : determining whether multiple iterations are to be performed ; and when additional iterations are to be performed , using a previously generated linear classifiers as the initial classifier for the next iteration .", "label": "", "metadata": {}, "score": "72.28048"}
{"text": "/data / fm_test_real . /examples / documented / octave_modular / kernel_gaussian_shift_modular .m . %An experimental kernel inspired by the WeightedDegreePositionStringKernel and the Gaussian kernel . %The idea is to shift the dimensions of the input vectors against eachother . ' shift_step ' is the step % size of the shifts and max_shift is the maximal shift .", "label": "", "metadata": {}, "score": "72.316025"}
{"text": "The example also shows how to % retrieve parameters ( vector w and bias b ) ) of the trained linear classifier . % % For more details on the SVMLIN solver see % V. Sindhwani , S.S. Keerthi .Newton Methods for Fast Solution of Semi - supervised % Linear SVMs .", "label": "", "metadata": {}, "score": "72.31999"}
{"text": "The optimal hyperplane maximizes the separation margin between the two classes of training data , and is defined by a fraction of the input data instances ( called support vectors ) close to the hyperplane .The distance measurement between the data points in the high - dimensional space is defined by the kernel function .", "label": "", "metadata": {}, "score": "72.38841"}
{"text": "The delta rule is derived through gradient descent which tries to optimize the parameters by minimizing the error ( loss ) in the output of each single - layer module .It can be seen that : . and the first factor on the right can be recursively calculated : . are Jacobian matrices .", "label": "", "metadata": {}, "score": "72.44628"}
{"text": "# In this example a two - class linear support vector machine classifier is trained # on a toy data set and the trained classifier is used to predict labels of test # examples .As training algorithm the steepest descent subgradient algorithm is # used .", "label": "", "metadata": {}, "score": "72.46356"}
{"text": "# The kernel used is the Chi2 kernel which operates on real - valued vectors .It # computes the chi - squared distance between sets of histograms .It is a very # useful distance in image recognition ( used to detect objects ) .", "label": "", "metadata": {}, "score": "72.47752"}
{"text": "# The kernel used is the Chi2 kernel which operates on real - valued vectors .It # computes the chi - squared distance between sets of histograms .It is a very # useful distance in image recognition ( used to detect objects ) .", "label": "", "metadata": {}, "score": "72.47752"}
{"text": "# # For more details on the SGD solver see # L. Bottou , O. Bousquet .The tradeoff of large scale learning .In NIPS 20 .MIT # Press ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass .", "label": "", "metadata": {}, "score": "72.50336"}
{"text": "# # For more details on the SGD solver see # L. Bottou , O. Bousquet .The tradeoff of large scale learning .In NIPS 20 .MIT # Press ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass .", "label": "", "metadata": {}, "score": "72.50336"}
{"text": "Bioinformatics , 21:369 - 377 , June 2005 ./data / label_train_dna ./data / fm_train_dna ./data / fm_test_dna .get_labels ( ) ; else disp('No support for SVMLight available . ' ) end . /examples / documented / octave_modular / classifier_svmlin_modular . m . %", "label": "", "metadata": {}, "score": "72.57527"}
{"text": "The method of claim 5 , wherein forming the embedded representation of each subpart includes calculating a weighted sum over the n - grams in the subpart .The method of claim 1 , wherein embedding the input text in the latent space comprises calculating a weighted sum over the n - grams in the input text .", "label": "", "metadata": {}, "score": "72.65103"}
{"text": "The default is a cost of 1.0 ; to assign a different cost , pass a cost_factor parameter with the desired value .When using a ranking SVM , you may also pass a query_id parameter , whose integer value will identify the group of instances in which this instance belongs for ranking purposes .", "label": "", "metadata": {}, "score": "72.75651"}
{"text": "distance_geodesic_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .# This approach is just one example of what can be done using the distance # functions provided by shogun .", "label": "", "metadata": {}, "score": "72.859344"}
{"text": "distance_geodesic_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .# This approach is just one example of what can be done using the distance # functions provided by shogun .", "label": "", "metadata": {}, "score": "72.859344"}
{"text": "distance_geodesic_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .# This approach is just one example of what can be done using the distance # functions provided by shogun .", "label": "", "metadata": {}, "score": "72.859344"}
{"text": "[ 0086 ] FIG .6 illustrates an example system architecture 600 .The system architecture 600 is capable of performing operations for constructing text classifiers .These components exchange communications and data using one or more buses 614 ( e.g. , EISA , PCI , PCI Express , etc . ) .", "label": "", "metadata": {}, "score": "72.87017"}
{"text": "The system issues 404 the candidate phrases as queries to a search engine .For example , each candidate phrase can be specified in quotes and then submitted to the search engine .In some implementations , the system further specifies a language for the results .", "label": "", "metadata": {}, "score": "72.897194"}
{"text": "% Each column of the matrices corresponds to one data point . % % The distance initialized by two data sets ( the same data set as shown in the % first call ) controls the processing of the given data points , where a pairwise % distance ( extended Jaccard coefficient ) matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "72.93457"}
{"text": "The tradeoff of large scale learning .In NIPS 20 .MIT # Press ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . io.set_loglevel(0 ) svm.train ( ) svm.set_features(feats_test ) svm.classify ( ) .Clustering . /examples / documented / python_modular / clustering_hierarchical_modular.py .", "label": "", "metadata": {}, "score": "72.98606"}
{"text": "[ 12 ] constructed the simple linear model DSIR using the LASSO procedure for siRNA efficacy prediction with basic sequence features .The above - mentioned previous studies suggest that many siRNA features of sequence composition , thermodynamic stability and secondary structure are related to the effectiveness of gene silencing .", "label": "", "metadata": {}, "score": "73.07689"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / kernel_gaussian_shift_modular.py .# An experimental kernel inspired by the WeightedDegreePositionStringKernel and the Gaussian kernel .# The idea is to shift the dimensions of the input vectors against eachother . ' shift_step ' is the step # size of the shifts and max_shift is the maximal shift .", "label": "", "metadata": {}, "score": "73.104454"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / kernel_gaussian_shift_modular.py .# An experimental kernel inspired by the WeightedDegreePositionStringKernel and the Gaussian kernel .# The idea is to shift the dimensions of the input vectors against eachother . ' shift_step ' is the step # size of the shifts and max_shift is the maximal shift .", "label": "", "metadata": {}, "score": "73.104454"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / kernel_gaussian_shift_modular.py .# An experimental kernel inspired by the WeightedDegreePositionStringKernel and the Gaussian kernel .# The idea is to shift the dimensions of the input vectors against eachother . ' shift_step ' is the step # size of the shifts and max_shift is the maximal shift .", "label": "", "metadata": {}, "score": "73.104454"}
{"text": "The the label of both the train and the test data are # fetched via svr.classify ( ) .get_labels ( ) .# # For more details on the SVM^light see # T. Joachims .Making large - scale SVM learning practical .", "label": "", "metadata": {}, "score": "73.29576"}
{"text": "The the label of both the train and the test data are # fetched via svr.classify ( ) .get_labels ( ) .# # For more details on the SVM^light see # T. Joachims .Making large - scale SVM learning practical .", "label": "", "metadata": {}, "score": "73.29576"}
{"text": "The the label of both the train and the test data are # fetched via svr.classify ( ) .get_labels ( ) .# # For more details on the SVM^light see # T. Joachims .Making large - scale SVM learning practical .", "label": "", "metadata": {}, "score": "73.29576"}
{"text": "In this example a support vector regression algorithm is trained on a % real - valued toy data set .The underlying library used for the SVR training is % SVM^light .The the label of both the train and the test data are % fetched via svr.classify ( ) .", "label": "", "metadata": {}, "score": "73.35886"}
{"text": "Use this method when you already have your features represented as integers .The $ label parameter must be a number ( typically 1 or -1 ) , and the @indices and @values arrays must be parallel arrays of indices and their corresponding values .", "label": "", "metadata": {}, "score": "73.43441"}
{"text": "The n - gram vectors p.sub.\u03b3 j are each multiplied by their respective combination parameters q j in block 306 , and combined in block 308 to form the document vector d x .[ 0036 ] An alternative embodiment for modeling the weight of every \u03b3 j is to use both the position evidence and the content of this n - gram .", "label": "", "metadata": {}, "score": "73.54381"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_svmlight_batch_linadd_modular.py .# In this example a two - class support vector machine classifier is trained on a # DNA splice - site detection data set and the trained classifier is used to predict # labels on test set .", "label": "", "metadata": {}, "score": "73.5609"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_svmlight_batch_linadd_modular.py .# In this example a two - class support vector machine classifier is trained on a # DNA splice - site detection data set and the trained classifier is used to predict # labels on test set .", "label": "", "metadata": {}, "score": "73.5609"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_svmlight_batch_linadd_modular.py .# In this example a two - class support vector machine classifier is trained on a # DNA splice - site detection data set and the trained classifier is used to predict # labels on test set .", "label": "", "metadata": {}, "score": "73.5609"}
{"text": "The results suggest that there may be some redundant or correlated information in the full feature set , and the RF - based feature selection can be used to improve classifier performance .The Seq_Features classifier ( constructed using the 19 siRNA sequence features ) showed significantly worse performance than RF_Features ( Table 3 ) , suggesting that the selected composition features contain some important information for siRNA classification .", "label": "", "metadata": {}, "score": "73.680336"}
{"text": "For example , in some implementations , the sampled documents have a length of approximately 1000 words ( e.g. , 750 to 1250 words ) .The number of sampled documents can also vary .In some implementations , 2000 documents are sampled .", "label": "", "metadata": {}, "score": "73.741455"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CMinkowskiMetric.html .# # Obviously , using the Minkowski metric is not limited to this showcase # example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "73.74982"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CMinkowskiMetric.html .# # Obviously , using the Minkowski metric is not limited to this showcase # example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "73.74982"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CMinkowskiMetric.html .# # Obviously , using the Minkowski metric is not limited to this showcase # example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "73.74982"}
{"text": "Extensions and applications .October ./data / fm_train_real . /examples / documented / python_modular /preprocessor_kernelpca_modular.py .# In this example toy data is being processed using the kernel PCA algorithm # as described in # # Sch\u00c3\u00b6lkopf , B. , Smola , A. J. , & Muller , K. R. ( 1999 ) .", "label": "", "metadata": {}, "score": "73.80264"}
{"text": "These instances are called the out - of - bag ( oob ) data for the tree .At each node of the tree , m variables out of all the n input variables ( m \" n ) are randomly selected , and the tree node is split using the selected m variables .", "label": "", "metadata": {}, "score": "73.80315"}
{"text": "# Trains an inhomogeneous Markov chain of order 3 on a DNA string data set .Due to # the structure of the Markov chain it is very similar to a HMM with just one # chain of connected hidden states - that is why we termed this linear HMM .", "label": "", "metadata": {}, "score": "73.81476"}
{"text": "# Trains an inhomogeneous Markov chain of order 3 on a DNA string data set .Due to # the structure of the Markov chain it is very similar to a HMM with just one # chain of connected hidden states - that is why we termed this linear HMM .", "label": "", "metadata": {}, "score": "73.81476"}
{"text": "# Trains an inhomogeneous Markov chain of order 3 on a DNA string data set .Due to # the structure of the Markov chain it is very similar to a HMM with just one # chain of connected hidden states - that is why we termed this linear HMM .", "label": "", "metadata": {}, "score": "73.81476"}
{"text": "# Trains an inhomogeneous Markov chain of order 3 on a DNA string data set .Due to # the structure of the Markov chain it is very similar to a HMM with just one # chain of connected hidden states - that is why we termed this linear HMM .", "label": "", "metadata": {}, "score": "73.81476"}
{"text": "# Trains an inhomogeneous Markov chain of order 3 on a DNA string data set .Due to # the structure of the Markov chain it is very similar to a HMM with just one # chain of connected hidden states - that is why we termed this linear HMM .", "label": "", "metadata": {}, "score": "73.81476"}
{"text": "The system of claim 20 , where scores for documents obtained from other classifiers are used as additional input to train the linear classifier .The system of claim 24 , further configured to perform operations comprising : determining whether multiple iterations are to be performed ; and when additional iterations are to be performed , using a previously generated linear classifiers as the initial classifier for the next iteration .", "label": "", "metadata": {}, "score": "73.84764"}
{"text": "% % The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .% % For more details see doc / classshogun_1_1CCosineDistance.html . % % Obviously , using the Cosine distance is not limited to this showcase % example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "73.876816"}
{"text": "The % best path , i.e. the path with highest probability given the model can then be % calculated using get_best_path_state . /examples / documented / octave_modular / distribution_linearhmm_modular .m . %Trains an inhomogeneous Markov chain of order 3 on a DNA string data set .", "label": "", "metadata": {}, "score": "73.94136"}
{"text": "% Each column of the matrices corresponds to one data point . % % The distance initialized by two data sets ( the same data set as shown in the % first call ) and norm ' k ' controls the processing of the given data points , % where a pairwise distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "73.94232"}
{"text": "If you like add_instance_i ( ) , I 've got a predict_i ( )I bet you 'll just love .An alternative to calling add_instance_i ( ) for each instance is to organize a collection of training data into SVMLight 's standard \" example_file \" format , then call this read_instances ( ) method to import the data .", "label": "", "metadata": {}, "score": "73.95042"}
{"text": "/examples / documented / python_modular / features_string_word_modular.py .# This example demonstrates how to encode string # features efficiently by creating a more compactly encoded # bit - string from StringCharFeatures . # Here , this is done in junks of 16bit ( word ) .", "label": "", "metadata": {}, "score": "74.0438"}
{"text": "/examples / documented / python_modular / features_string_word_modular.py .# This example demonstrates how to encode string # features efficiently by creating a more compactly encoded # bit - string from StringCharFeatures . # Here , this is done in junks of 16bit ( word ) .", "label": "", "metadata": {}, "score": "74.0438"}
{"text": "/examples / documented / python_modular / features_string_word_modular.py .# This example demonstrates how to encode string # features efficiently by creating a more compactly encoded # bit - string from StringCharFeatures . # Here , this is done in junks of 16bit ( word ) .", "label": "", "metadata": {}, "score": "74.0438"}
{"text": "Nat Biotechnol 2005 , 23 : 995 - 1001 .PubMed View Article .Shabalina SA , Spiridonov AN , Ogurtsov AY : Computational mocels with thermodynamic and composition features improve siRNA design .BMC Bioinformatics 2006 , 7 : 65 .", "label": "", "metadata": {}, "score": "74.07311"}
{"text": "add_preproc(preproc ) ; feats_test ./examples / documented / octave_modular / distance_minkowski_modular .m . %An approach as applied below , which shows the processing of input data % from a file becomes a crucial factor for writing your own sample applications .", "label": "", "metadata": {}, "score": "74.08582"}
{"text": "In Advances in Kernel # Methods -- Support Vector Learning , pages 169 - 184 .MIT Press , Cambridge , MA USA , 1999 .# # For more details on the Weighted Degree kernel see # G. Raetsch , S.Sonnenburg , and B. Schoelkopf .", "label": "", "metadata": {}, "score": "74.11073"}
{"text": "# # EQU00017 # # .Block 414 weights . by a factor \u03bb and subtracts the weighted value from the parameter \u03b8 i , storing the updated parameter .Block 416 multiplies the accumulator by . and stores the value as the new accumulator .", "label": "", "metadata": {}, "score": "74.13043"}
{"text": "In GMM clustering one tries to partition n observations into k # clusters in which each observation belongs to the cluster defined by a Gaussian # distribution , whose probability of generating the observation is highest .# The algorithm class constructor takes the number of clusters and a type of # covariance as input .", "label": "", "metadata": {}, "score": "74.14161"}
{"text": "In GMM clustering one tries to partition n observations into k # clusters in which each observation belongs to the cluster defined by a Gaussian # distribution , whose probability of generating the observation is highest .# The algorithm class constructor takes the number of clusters and a type of # covariance as input .", "label": "", "metadata": {}, "score": "74.14161"}
{"text": "m . %This example initializes the locality improved string kernel .The locality improved string % kernel is defined on sequences of the same length and inspects letters matching at % corresponding positions in both sequences .The kernel sums over all matches in windows of % length l and takes this sum to the power of ' inner_degree ' .", "label": "", "metadata": {}, "score": "74.16344"}
{"text": "# The base kernels are then subsequently added to a CombinedKernel , which # contains a weight for each kernel and encapsulates the base kernels # from the training procedure .When the CombinedKernel between two examples is # evaluated it computes the corresponding linear combination of kernels according to their weights .", "label": "", "metadata": {}, "score": "74.22578"}
{"text": "# The base kernels are then subsequently added to a CombinedKernel , which # contains a weight for each kernel and encapsulates the base kernels # from the training procedure .When the CombinedKernel between two examples is # evaluated it computes the corresponding linear combination of kernels according to their weights .", "label": "", "metadata": {}, "score": "74.22578"}
{"text": "# The base kernels are then subsequently added to a CombinedKernel , which # contains a weight for each kernel and encapsulates the base kernels # from the training procedure .When the CombinedKernel between two examples is # evaluated it computes the corresponding linear combination of kernels according to their weights .", "label": "", "metadata": {}, "score": "74.22578"}
{"text": "# The base kernels are then subsequently added to a CombinedKernel , which # contains a weight for each kernel and encapsulates the base kernels # from the training procedure .When the CombinedKernel between two examples is # evaluated it computes the corresponding linear combination of kernels according to their weights .", "label": "", "metadata": {}, "score": "74.22578"}
{"text": "# The base kernels are then subsequently added to a CombinedKernel , which # contains a weight for each kernel and encapsulates the base kernels # from the training procedure .When the CombinedKernel between two examples is # evaluated it computes the corresponding linear combination of kernels according to their weights .", "label": "", "metadata": {}, "score": "74.22578"}
{"text": "# The base kernels are then subsequently added to a CombinedKernel , which # contains a weight for each kernel and encapsulates the base kernels # from the training procedure .When the CombinedKernel between two examples is # evaluated it computes the corresponding linear combination of kernels according to their weights .", "label": "", "metadata": {}, "score": "74.22578"}
{"text": "# The base kernels are then subsequently added to a CombinedKernel , which # contains a weight for each kernel and encapsulates the base kernels # from the training procedure .When the CombinedKernel between two examples is # evaluated it computes the corresponding linear combination of kernels according to their weights .", "label": "", "metadata": {}, "score": "74.22578"}
{"text": "The unbiased linear rule is trained .# # Note that this solver often does not converges because the steepest descent # subgradient algorithm is oversensitive to rounding errors .Note also that this # is an unpublished work which was predecessor of the OCAS solver ( see # classifier_svmocas ) .", "label": "", "metadata": {}, "score": "74.284996"}
{"text": "The unbiased linear rule is trained .# # Note that this solver often does not converges because the steepest descent # subgradient algorithm is oversensitive to rounding errors .Note also that this # is an unpublished work which was predecessor of the OCAS solver ( see # classifier_svmocas ) .", "label": "", "metadata": {}, "score": "74.284996"}
{"text": "The unbiased linear rule is trained .# # Note that this solver often does not converges because the steepest descent # subgradient algorithm is oversensitive to rounding errors .Note also that this # is an unpublished work which was predecessor of the OCAS solver ( see # classifier_svmocas ) .", "label": "", "metadata": {}, "score": "74.284996"}
{"text": "The tradeoff of large scale learning .In NIPS 20 .MIT # Press ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . io.set_loglevel(0 ) svm.train ( ) svm.set_features(feats_test ) svm.apply ( ) .Clustering . /examples / documented / python_modular / clustering_gmm_modular.py .", "label": "", "metadata": {}, "score": "74.33989"}
{"text": "The tradeoff of large scale learning .In NIPS 20 .MIT # Press ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . io.set_loglevel(0 ) svm.train ( ) svm.set_features(feats_test ) svm.apply ( ) .Clustering . /examples / documented / python_modular / clustering_gmm_modular.py .", "label": "", "metadata": {}, "score": "74.33989"}
{"text": "# The PolyMatchString kernel sums over the matches of two stings of the same length and # takes the sum to the power of ' degree ' .The strings consist of the characters ' ACGT ' corresponding # to the DNA - alphabet .", "label": "", "metadata": {}, "score": "74.37178"}
{"text": "# The training method used in this example is Expectation - Maximization .It takes # minimum covariance , maximum iterations and minimum log - likelihood change as # input .After training one can cluster observations by selecting the most # likely cluster to have generated the observation .", "label": "", "metadata": {}, "score": "74.37446"}
{"text": "# The training method used in this example is Expectation - Maximization .It takes # minimum covariance , maximum iterations and minimum log - likelihood change as # input .After training one can cluster observations by selecting the most # likely cluster to have generated the observation .", "label": "", "metadata": {}, "score": "74.37446"}
{"text": "Vert J - P , Foveau N , Lajaunie C , Vandenbrouck Y : An accurate and interpretable model for siRNA efficacy prediction .BMC Bioinformatics 2006 , 7 : 520 .PubMed View Article .Klingelhoefer JW , Moutsianas L , Holmes C : Approximate Bayesian feature selection on a large meta - dataset offers novel insights on factors that effect siRNA potency .", "label": "", "metadata": {}, "score": "74.40231"}
{"text": "% This example loads two stored matrices of real values from different % files and initializes the matrices to ' RealFeatures ' .% Each column of the matrices corresponds to one data point . % % The distance initialized by two data sets ( the same data set as shown in the % first call ) controls the processing of the given data points , where a pairwise % distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "74.5392"}
{"text": "% This example loads two stored matrices of real values from different % files and initializes the matrices to ' RealFeatures ' .% Each column of the matrices corresponds to one data point . % % The distance initialized by two data sets ( the same data set as shown in the % first call ) controls the processing of the given data points , where a pairwise % distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "74.5392"}
{"text": "% This example loads two stored matrices of real values from different % files and initializes the matrices to ' RealFeatures ' .% Each column of the matrices corresponds to one data point . % % The distance initialized by two data sets ( the same data set as shown in the % first call ) controls the processing of the given data points , where a pairwise % distance matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "74.5392"}
{"text": "m . %This is an example for the initialization of the local alignment kernel on % DNA sequences , where each column of the matrices of type char corresponds to % one training / test example ./data / fm_train_dna .", "label": "", "metadata": {}, "score": "74.54525"}
{"text": "% % The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .% % For more details see doc / classshogun_1_1CEuclidianDistance.html . % % Obviously , using the Euclidian distance is not limited to this showcase % example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "74.549576"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_oligo_string_modular .R .# This is an example initializing the oligo string kernel which takes distances # between matching oligos ( k - mers ) into account via a gaussian .", "label": "", "metadata": {}, "score": "74.55747"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_oligo_string_modular .R .# This is an example initializing the oligo string kernel which takes distances # between matching oligos ( k - mers ) into account via a gaussian .", "label": "", "metadata": {}, "score": "74.55747"}
{"text": "/data / fm_test_real . /examples / documented / r_modular / kernel_gaussian_shift_modular .R . # An experimental kernel inspired by the WeightedDegreePositionStringKernel and the Gaussian kernel .# The idea is to shift the dimensions of the input vectors against eachother . ' shift_step ' is the step # size of the shifts and max_shift is the maximal shift .", "label": "", "metadata": {}, "score": "74.59391"}
{"text": "/data / fm_test_real . /examples / documented / r_modular / kernel_gaussian_shift_modular .R . # An experimental kernel inspired by the WeightedDegreePositionStringKernel and the Gaussian kernel .# The idea is to shift the dimensions of the input vectors against eachother . ' shift_step ' is the step # size of the shifts and max_shift is the maximal shift .", "label": "", "metadata": {}, "score": "74.59391"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / r_modular / classifier_svmlight_modular .R .# In this example a two - class support vector machine classifier is trained on a # DNA splice - site detection data set and the trained classifier is used to predict # labels on test set .", "label": "", "metadata": {}, "score": "74.61879"}
{"text": "Although these findings provided important insight into RNA interference , the empirical rules were often derived from relatively small datasets , and thus might not cover all the relevant features affecting siRNA potency .With the accumulation of siRNA data , machine learning methods have been developed for both classification and regression analysis of siRNA potency .", "label": "", "metadata": {}, "score": "74.66021"}
{"text": "% This example loads two stored matrices of real values from different % files and initializes the matrices to ' RealFeatures ' .% Each column of the matrices corresponds to one data point . % % The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "74.696014"}
{"text": "% This example loads two stored matrices of real values from different % files and initializes the matrices to ' RealFeatures ' .% Each column of the matrices corresponds to one data point . % % The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "74.696014"}
{"text": "% This example loads two stored matrices of real values from different % files and initializes the matrices to ' RealFeatures ' .% Each column of the matrices corresponds to one data point . % % The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "74.696014"}
{"text": "p .su-b.j T , E w j +1 T , . . .E w j + n-1 T ] T ) , ( 2 ) .The h ( ) function is not limited to the hyperbolic tangent , but may instead be any appropriate function that converts an unbounded range into a range from -1 to 1 .", "label": "", "metadata": {}, "score": "74.86227"}
{"text": "m . %An approach as applied below , which shows the processing of input data % from a file becomes a crucial factor for writing your own sample applications .%This approach is just one example of what can be done using the distance % functions provided by shogun .", "label": "", "metadata": {}, "score": "74.88702"}
{"text": "m . %An approach as applied below , which shows the processing of input data % from a file becomes a crucial factor for writing your own sample applications .%This approach is just one example of what can be done using the distance % functions provided by shogun .", "label": "", "metadata": {}, "score": "74.88702"}
{"text": "m . %This is an example for the initialization of the diag - kernel . %The diag kernel has all kernel matrix entries but those on % the main diagonal set to zero ./data / fm_train_real ./data / fm_test_real . /examples / documented / octave_modular / kernel_distance_modular .", "label": "", "metadata": {}, "score": "74.90472"}
{"text": "However , a computer need not have such devices .The processor and the memory can be supplemented by , or incorporated in , special purpose logic circuitry .The components of the system can be interconnected by any form or medium of digital data communication , e.g. , a communication network .", "label": "", "metadata": {}, "score": "74.940315"}
{"text": "All training instances share the same attribute - space ; if an attribute is unspecified for a certain instance , it is equivalent to specifying a value of zero .Typically you can save a lot of memory ( and potentially training time ) by omitting zero - valued attributes .", "label": "", "metadata": {}, "score": "74.987976"}
{"text": "Available from , 290(5500 ) , 2323 - 2326 ./data / fm_train_real . /examples / documented / python_modular / preprocessor_localtangentspacealignment_modular.py . # In this example toy data is being processed using the Local Tangent Space # Alignment ( LTSA ) algorithms as described in # # Zhang , Z. , & Zha , H. ( 2002 ) .", "label": "", "metadata": {}, "score": "75.004005"}
{"text": "The system of claim 26 , where performing additional iterations includes performing one or more iterations without supervised improvement and one or more iterations with supervised improvement .Description : .BACKGROUND .[ 0001 ] This specification relates to constructing text classifiers .", "label": "", "metadata": {}, "score": "75.026054"}
{"text": "In this study , random forests ( RFs ) were constructed to select important sequence features for predicting siRNA potency .RF - based variable importance measures were previously used in microarray expression data analyses to select a relatively small set of informative genes for disease / sample classification [ 14 , 15 ] .", "label": "", "metadata": {}, "score": "75.05318"}
{"text": "# Science , 14 , 585 - 591 .MIT Press ./data / fm_train_real . /examples / documented / python_modular /preprocessor_locallylinearembedding_modular.py .# In this example toy data is being preprocessed using the Locally Linear Embedding ( LLE ) # algorithm as described in # # Saul , L. K. , Ave , P. , Park , F. , & Roweis , S. T. ( 2001 ) .", "label": "", "metadata": {}, "score": "75.14212"}
{"text": "To facilitate gene functional studies , we have developed a new machine learning method to predict siRNA potency based on random forests and support vector machines .Since there were many potential sequence features , random forests were used to select the most relevant features affecting gene expression inhibition .", "label": "", "metadata": {}, "score": "75.14404"}
{"text": "distance_cosine_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .# This approach is just one example of what can be done using the distance # functions provided by shogun .", "label": "", "metadata": {}, "score": "75.21211"}
{"text": "distance_cosine_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .# This approach is just one example of what can be done using the distance # functions provided by shogun .", "label": "", "metadata": {}, "score": "75.21211"}
{"text": "distance_cosine_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .# This approach is just one example of what can be done using the distance # functions provided by shogun .", "label": "", "metadata": {}, "score": "75.21211"}
{"text": "Each column of the matrices of type char corresponds to % one training / test example ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / octave_modular / kernel_sigmoid_modular .m . %The standard Sigmoid kernel computed on dense real valued features .", "label": "", "metadata": {}, "score": "75.30357"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_perceptron_modular.py .The Perceptron algorithm works by # iteratively passing though the training examples and applying the update rule on # those examples which are misclassified by the current classifier .", "label": "", "metadata": {}, "score": "75.328476"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_perceptron_modular.py .The Perceptron algorithm works by # iteratively passing though the training examples and applying the update rule on # those examples which are misclassified by the current classifier .", "label": "", "metadata": {}, "score": "75.328476"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_perceptron_modular.py .The Perceptron algorithm works by # iteratively passing though the training examples and applying the update rule on # those examples which are misclassified by the current classifier .", "label": "", "metadata": {}, "score": "75.328476"}
{"text": "# Finally , the example shows how to classify with a trained MKLClassifier ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . shape # fm_test_real . /examples / documented / python_modular / mkl_multiclass_modular.py . # In this example we show how to perform Multiple Kernel Learning ( MKL ) # with the modular interface for multi - class classification .", "label": "", "metadata": {}, "score": "75.35246"}
{"text": "# Finally , the example shows how to classify with a trained MKLClassifier ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . shape # fm_test_real . /examples / documented / python_modular / mkl_multiclass_modular.py . # In this example we show how to perform Multiple Kernel Learning ( MKL ) # with the modular interface for multi - class classification .", "label": "", "metadata": {}, "score": "75.35246"}
{"text": "[ 0021 ] FIG .6 illustrates an example system architecture .[ 0022 ]Like reference numbers and designations in the various drawings indicate like elements .DETAILED DESCRIPTION .[ 0023 ] FIG .1 shows a flowchart of an example method 100 of classifying a document .", "label": "", "metadata": {}, "score": "75.4469"}
{"text": "# In this example a kernelized version of ridge regression ( KRR ) is trained on a # real - valued data set .The labels of both the train and the test # data can be fetched via krr.classify ( ) .", "label": "", "metadata": {}, "score": "75.478485"}
{"text": "# In this example a kernelized version of ridge regression ( KRR ) is trained on a # real - valued data set .The labels of both the train and the test # data can be fetched via krr.classify ( ) .", "label": "", "metadata": {}, "score": "75.478485"}
{"text": "# In this example a kernelized version of ridge regression ( KRR ) is trained on a # real - valued data set .The labels of both the train and the test # data can be fetched via krr.classify ( ) .", "label": "", "metadata": {}, "score": "75.478485"}
{"text": "# This is an example for the initialization of a linear kernel on string data .The # strings are all of the same length and consist of the characters ' ACGT ' corresponding # to the DNA - alphabet .Each column of the matrices of type char corresponds to # one training / test example .", "label": "", "metadata": {}, "score": "75.62829"}
{"text": "# This is an example for the initialization of a linear kernel on string data .The # strings are all of the same length and consist of the characters ' ACGT ' corresponding # to the DNA - alphabet .Each column of the matrices of type char corresponds to # one training / test example .", "label": "", "metadata": {}, "score": "75.62829"}
{"text": "PubMed View Article .Wang X - W , Wang X - H , Varma RK , Beauchamp L , Magdaleno S , Sendera TJ : Selection of hyperfunctional siRNAs with improved potency and specificity .Nucleic Acids Res 2009 , 37 : e152 .", "label": "", "metadata": {}, "score": "75.72482"}
{"text": "# In this example a multi - class support vector machine is trained on a toy data # set and the trained classifier is then used to predict labels of test # examples .The training algorithm is based on BSVM formulation ( L2-soft margin # and the bias added to the objective function ) which is solved by the Improved # Mitchell - Demyanov - Malozemov algorithm .", "label": "", "metadata": {}, "score": "75.8306"}
{"text": "If you want to train the model only once and save it for later re - use in a different context , see the write_model ( ) and read_model ( ) methods .After train ( ) has been called , the model may be applied to previously - unseen combinations of attributes .", "label": "", "metadata": {}, "score": "75.87551"}
{"text": "R .# This example initializes the locality improved string kernel .The locality improved string # kernel is defined on sequences of the same length and inspects letters matching at # corresponding positions in both sequences .The kernel sums over all matches in windows of # length l and takes this sum to the power of ' inner_degree ' .", "label": "", "metadata": {}, "score": "75.88649"}
{"text": "R .# This example initializes the locality improved string kernel .The locality improved string # kernel is defined on sequences of the same length and inspects letters matching at # corresponding positions in both sequences .The kernel sums over all matches in windows of # length l and takes this sum to the power of ' inner_degree ' .", "label": "", "metadata": {}, "score": "75.88649"}
{"text": "# # Note that this solver often does not converges because the steepest descent # subgradient algorithm is oversensitive to rounding errors .Note also that this # is an unpublished work which was predecessor of the OCAS solver ( see # classifier_svmocas ) .", "label": "", "metadata": {}, "score": "76.03662"}
{"text": "A smaller \u03b3 value makes the decision boundary smoother .Another parameter for SVM training is the regularization factor C , which controls the trade - off between low training error and large margin [ 19 ] .Different values for the \u03b3 and C parameters have been tested in this study to optimize the classifier performance .", "label": "", "metadata": {}, "score": "76.15794"}
{"text": "/examples / documented / python_modular /features_simple_modular.py . # This example shows how to encode features that live in various vector spaces # using the appropriate shogun objects .We demonstrate how to use # three types of features : ByteFeatures ( small integer values ) , # LongIntFeatures ( large integer values ) and finally RealFeatures # ( real - valued vectors ) .", "label": "", "metadata": {}, "score": "76.203766"}
{"text": "/examples / documented / python_modular /features_simple_modular.py . # This example shows how to encode features that live in various vector spaces # using the appropriate shogun objects .We demonstrate how to use # three types of features : ByteFeatures ( small integer values ) , # LongIntFeatures ( large integer values ) and finally RealFeatures # ( real - valued vectors ) .", "label": "", "metadata": {}, "score": "76.203766"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_comm_ulong_string_modular .R .# This is an example for the initialization of the CommUlongString - kernel .For efficient computing a preprocessor is used # that extracts and sorts all k - mers .", "label": "", "metadata": {}, "score": "76.49693"}
{"text": "# This is an example for the initialization of a linear kernel on raw byte # data . /examples / documented / r_modular / kernel_linear_modular .R .# This is an example for the initialization of a linear kernel on real valued # data using scaling factor 1.2 .", "label": "", "metadata": {}, "score": "76.511536"}
{"text": "# This approach is just one example of what can be done using the distance # functions provided by shogun .# # First , you need to determine what type your data will be , because this # will determine the distance function you can use .", "label": "", "metadata": {}, "score": "76.64513"}
{"text": "The distance used in this example is Euclidean distance .# After training one can fetch the result of clustering by obtaining the cluster # centers and their radiuses ./data / fm_train_real .Converter . /examples / documented / python_modular / converter_diffusionmaps_modular.py .", "label": "", "metadata": {}, "score": "76.68077"}
{"text": "Each column of the matrices of type char corresponds to # one training / test example ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_polymatchword_modular .R .# The PolyMatchWordString kernel is defined on strings of equal length .", "label": "", "metadata": {}, "score": "76.72479"}
{"text": "Each column of the matrices of type char corresponds to # one training / test example ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_polymatchword_modular .R .# The PolyMatchWordString kernel is defined on strings of equal length .", "label": "", "metadata": {}, "score": "76.72479"}
{"text": "The average values of raw scores and z - scores of variable importance are shown together with the feature 's correlation with siRNA efficacy ( inhibition of target gene expression ) .Interestingly , the efficacy of gene silencing appears to be significantly affected by nucleotide dimer and trimer compositions of siRNA sequence .", "label": "", "metadata": {}, "score": "76.752625"}
{"text": "In particular , for two n - grams i and j where X i is a substring of X j , the score for the two n - grams will be independently calculated such that the score is w(x i ) + w(x j ) .", "label": "", "metadata": {}, "score": "76.78694"}
{"text": "For efficient computing a preprocessor is used % that extracts and sorts all k - mers .If ' use_sign ' is set to one each k - mere is counted % only once ./data / fm_train_dna ./data / fm_test_dna .", "label": "", "metadata": {}, "score": "77.20271"}
{"text": "[ 0025 ] Embodiments may include a computer program product accessible from a computer - usable or computer - readable medium providing program code for use by or in connection with a computer or any instruction execution system .A computer - usable or computer readable medium may include any apparatus that stores , communicates , propagates , or transports the program for use by or in connection with the instruction execution system , apparatus , or device .", "label": "", "metadata": {}, "score": "77.334335"}
{"text": "/data / fm_test_real ./data / label_train_multiclass . /examples / documented / python_modular / classifier_libsvmoneclass_modular.py .# In this example a one - class support vector machine classifier is trained on a # toy data set .The training algorithm finds a hyperplane in the RKHS which # separates the training data from the origin .", "label": "", "metadata": {}, "score": "77.40585"}
{"text": "/data / fm_test_real ./data / label_train_multiclass . /examples / documented / python_modular / classifier_libsvmoneclass_modular.py .# In this example a one - class support vector machine classifier is trained on a # toy data set .The training algorithm finds a hyperplane in the RKHS which # separates the training data from the origin .", "label": "", "metadata": {}, "score": "77.40585"}
{"text": "/data / fm_test_real ./data / label_train_multiclass . /examples / documented / python_modular / classifier_libsvmoneclass_modular.py .# In this example a one - class support vector machine classifier is trained on a # toy data set .The training algorithm finds a hyperplane in the RKHS which # separates the training data from the origin .", "label": "", "metadata": {}, "score": "77.40585"}
{"text": "# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .# This approach is just one example of what can be done using the distance # functions provided by shogun .", "label": "", "metadata": {}, "score": "77.42287"}
{"text": "# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .# This approach is just one example of what can be done using the distance # functions provided by shogun .", "label": "", "metadata": {}, "score": "77.42287"}
{"text": "# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .# This approach is just one example of what can be done using the distance # functions provided by shogun .", "label": "", "metadata": {}, "score": "77.42287"}
{"text": "/data / fm_train_real . /examples / documented / python_modular / kernel_fisher_modular.py .# The class FKFeatures implements Fischer kernel features obtained from # two Hidden Markov models .# # It was used in # # K. Tsuda , M. Kawanabe , G. Raetsch , S. Sonnenburg , and K.R. Mueller .", "label": "", "metadata": {}, "score": "77.43794"}
{"text": "Here the clusters # are merged that have the closest ( minimum distance , here set via the Euclidean # distance object ) two elements ./data / fm_train_real . /examples / documented / python_modular / clustering_kmeans_modular.py . # In this example the k - means clustering method is used to cluster a given toy # data set .", "label": "", "metadata": {}, "score": "77.45717"}
{"text": "Here the clusters # are merged that have the closest ( minimum distance , here set via the Euclidean # distance object ) two elements ./data / fm_train_real . /examples / documented / python_modular / clustering_kmeans_modular.py . # In this example the k - means clustering method is used to cluster a given toy # data set .", "label": "", "metadata": {}, "score": "77.45717"}
{"text": "% % The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .% % For more details see doc / classshogun_1_1CMinkowskiMetric.html . % % Obviously , using the Minkowski metric is not limited to this showcase % example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "77.53117"}
{"text": "Making large - scale SVM learning practical .In Advances in Kernel % Methods -- Support Vector Learning , pages 169 - 184 .MIT Press , Cambridge , MA USA , 1999 . % % For more details on the Weighted Degree kernel see % G. Raetsch , S.Sonnenburg , and B. Schoelkopf .", "label": "", "metadata": {}, "score": "77.537094"}
{"text": "% Each column of the matrices corresponds to one data point . % % The distance initialized by two data sets ( the same data set as shown in the % first call ) controls the processing of the given data points , where a pairwise % distance ( dissimilarity ratio ) matrix is computed by ' get_distance_matrix ' .", "label": "", "metadata": {}, "score": "77.58707"}
{"text": "In this example a two - class support vector machine classifier is trained on a % toy data set and the trained classifier is used to predict labels of test % examples .The example also shows how to retrieve the % support vectors from the train SVM model .", "label": "", "metadata": {}, "score": "77.75012"}
{"text": "Here , we assume that we have two problem domains , one with # an abundance of training data ( source domain ) and one with only a few # training examples ( target domain ) .These domains are assumed to be # different but related enough to transfer information between them .", "label": "", "metadata": {}, "score": "77.766846"}
{"text": "Here , we assume that we have two problem domains , one with # an abundance of training data ( source domain ) and one with only a few # training examples ( target domain ) .These domains are assumed to be # different but related enough to transfer information between them .", "label": "", "metadata": {}, "score": "77.766846"}
{"text": "Here , we assume that we have two problem domains , one with # an abundance of training data ( source domain ) and one with only a few # training examples ( target domain ) .These domains are assumed to be # different but related enough to transfer information between them .", "label": "", "metadata": {}, "score": "77.766846"}
{"text": "[ 0042 ] Table 1 .[0043 ] Each parameter listed in Table 1 corresponds to a parameter from one of the above equations .[0044 ] For a layer f i , i\u03b5[1,T ] , the derivative . is used for updating its parameter set \u03b8 i uses the delta rule .", "label": "", "metadata": {}, "score": "77.849304"}
{"text": "The classifier RF_Features appears to be slightly more accurate than All_Features .Both RF_Features and All_Features are significantly better than Seq_Features .To further evaluate the classifier RF_Features , we examined the SVM output used to predict siRNA potency .The classifier was constructed using siRNA instances with binary labels ( potent or non - potent ) .", "label": "", "metadata": {}, "score": "77.910065"}
{"text": "The sign of the returned label ( positive or negative ) indicates whether the new instance is considered a positive or negative instance , and the magnitude of the label corresponds in some way to the confidence with which the model is making that assertion .", "label": "", "metadata": {}, "score": "77.916245"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_fisher_modular.py .# The class FKFeatures implements Fischer kernel features obtained from # two Hidden Markov models .# # It was used in # # K. Tsuda , M. Kawanabe , G. Raetsch , S. Sonnenburg , and K.R. Mueller .", "label": "", "metadata": {}, "score": "77.9382"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_fisher_modular.py .# The class FKFeatures implements Fischer kernel features obtained from # two Hidden Markov models .# # It was used in # # K. Tsuda , M. Kawanabe , G. Raetsch , S. Sonnenburg , and K.R. Mueller .", "label": "", "metadata": {}, "score": "77.9382"}
{"text": "add_preproc(preproc ) ; feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) ; feats_test .add_preproc(preproc ) ; feats_test ./examples / documented / octave_modular / kernel_oligo_string_modular .m . %This is an example initializing the oligo string kernel which takes distances % between matching oligos ( k - mers ) into account via a gaussian .", "label": "", "metadata": {}, "score": "77.961945"}
{"text": "The third group has 16 features representing the frequencies of all possible dinucleotides ( e.g. , AG , UC , etc ) .The fourth feature group consists of 64 frequencies of all possible trinucleotides ( e.g. , CAG , UCC , etc ) .", "label": "", "metadata": {}, "score": "77.97783"}
{"text": "R .# This is an example for the initialization of the local alignment kernel on # DNA sequences , where each column of the matrices of type char corresponds to # one training / test example ./data / fm_train_dna .", "label": "", "metadata": {}, "score": "78.058"}
{"text": "R .# This is an example for the initialization of the local alignment kernel on # DNA sequences , where each column of the matrices of type char corresponds to # one training / test example ./data / fm_train_dna .", "label": "", "metadata": {}, "score": "78.058"}
{"text": "# In this example a kernel matrix is computed for a given real - valued data set .# The kernel used is the Chi2 kernel which operates on real - valued vectors .It # computes the chi - squared distance between sets of histograms .", "label": "", "metadata": {}, "score": "78.146286"}
{"text": "# In this example a kernel matrix is computed for a given real - valued data set .# The kernel used is the Chi2 kernel which operates on real - valued vectors .It # computes the chi - squared distance between sets of histograms .", "label": "", "metadata": {}, "score": "78.146286"}
{"text": "arg max i . di - elect cons .# # EQU00009 # # .[ 0040 ] The last layer is measurement of how different the predicted class of a document compares to its true class label : . di - elect cons .", "label": "", "metadata": {}, "score": "78.28646"}
{"text": "# In this example toy data is being preprocessed using the Hessian Locally Linear Embedding algorithm # as described in # # Donoho , D. , & Grimes , C. ( 2003 ) .# Hessian eigenmaps : new tools for nonlinear dimensionality reduction .", "label": "", "metadata": {}, "score": "78.29176"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_libsvm_minimal_modular.py .# In this example a two - class support vector machine classifier is trained on a # 2-dimensional randomly generated data set and the trained classifier is used to # predict labels of test examples . /examples / documented / python_modular / classifier_libsvm_modular.py .", "label": "", "metadata": {}, "score": "78.38753"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_libsvm_minimal_modular.py .# In this example a two - class support vector machine classifier is trained on a # 2-dimensional randomly generated data set and the trained classifier is used to # predict labels of test examples . /examples / documented / python_modular / classifier_libsvm_modular.py .", "label": "", "metadata": {}, "score": "78.38753"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_libsvm_minimal_modular.py .# In this example a two - class support vector machine classifier is trained on a # 2-dimensional randomly generated data set and the trained classifier is used to # predict labels of test examples . /examples / documented / python_modular / classifier_libsvm_modular.py .", "label": "", "metadata": {}, "score": "78.38753"}
{"text": "m . %In this example a two - class support vector machine classifier is trained on a % toy data set and the trained classifier is then used to predict labels of test % examples ./data / label_train_twoclass ./data / fm_train_real .", "label": "", "metadata": {}, "score": "78.39581"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CChebyshewMetric.html .# # Obviously , using the Chebyshew distance is not limited to this showcase # example .", "label": "", "metadata": {}, "score": "78.39702"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CChebyshewMetric.html .# # Obviously , using the Chebyshew distance is not limited to this showcase # example .", "label": "", "metadata": {}, "score": "78.39702"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CChebyshewMetric.html .# # Obviously , using the Chebyshew distance is not limited to this showcase # example .", "label": "", "metadata": {}, "score": "78.39702"}
{"text": "/data / fm_test_real . /examples / documented / r_modular / kernel_linear_string_modular .R .# This is an example for the initialization of a linear kernel on string data .The # strings are all of the same length and consist of the characters ' ACGT ' corresponding # to the DNA - alphabet .", "label": "", "metadata": {}, "score": "78.42394"}
{"text": "/data / fm_test_real . /examples / documented / r_modular / kernel_linear_string_modular .R .# This is an example for the initialization of a linear kernel on string data .The # strings are all of the same length and consist of the characters ' ACGT ' corresponding # to the DNA - alphabet .", "label": "", "metadata": {}, "score": "78.42394"}
{"text": "If not , block 406 randomly samples a data point and label in the text document .Block 408 calculates the loss l based on the randomly sampled data point and label .[ 0046 ] Block 410 begins a loop by initializing an iterator index i to zero and an accumulator variable to one .", "label": "", "metadata": {}, "score": "78.54668"}
{"text": "PubMed View Article .Jiang H , Deng Y , Chen H , Tao L , Sha Q , Chen J , Tsai C , Zhang S : Joint analysis of two microarray gene - expression data sets to select lung adenocarcinoma marker genes .", "label": "", "metadata": {}, "score": "78.62378"}
{"text": "Bioinformatics , 21:369 - 377 , June 2005 ./data / fm_train_dna ./data / fm_test_dna ./data / label_train_dna ./examples / documented / python_modular / classifier_svmlin_modular.py .# In this example a two - class linear support vector machine classifier ( SVM ) is # trained on a toy data set and the trained classifier is used to predict labels # of test examples .", "label": "", "metadata": {}, "score": "78.64966"}
{"text": "Bioinformatics , 21:369 - 377 , June 2005 ./data / fm_train_dna ./data / fm_test_dna ./data / label_train_dna ./examples / documented / python_modular / classifier_svmlin_modular.py .# In this example a two - class linear support vector machine classifier ( SVM ) is # trained on a toy data set and the trained classifier is used to predict labels # of test examples .", "label": "", "metadata": {}, "score": "78.64966"}
{"text": "Bioinformatics , 21:369 - 377 , June 2005 ./data / fm_train_dna ./data / fm_test_dna ./data / label_train_dna ./examples / documented / python_modular / classifier_svmlin_modular.py .# In this example a two - class linear support vector machine classifier ( SVM ) is # trained on a toy data set and the trained classifier is used to predict labels # of test examples .", "label": "", "metadata": {}, "score": "78.64966"}
{"text": "/data / fm_train_real ./data / fm_test_real .get_labels ( ) ; . /examples / documented / octave_modular / classifier_liblinear_modular .m . %In this example a two - class linear support vector machine classifier is trained % on a toy data set and the trained classifier is then used to predict labels of % test examples .", "label": "", "metadata": {}, "score": "78.71089"}
{"text": "PubMed View Article .Patzel V , Rutz S , Dietrich I , Koberle C , Scheffold A , Kaufmann SH : Design of siRNAs producing unstructured guide - RNAs results in improved RNA interference efficiency .Nat Biotechnol 2005 , 23 : 1440 - 1444 .", "label": "", "metadata": {}, "score": "78.83476"}
{"text": "% % The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .% % For more details see doc / classshogun_1_1CTanimotoDistance.html . % % Obviously , using the Tanimoto distance / coefficient is not limited to % this showcase example .", "label": "", "metadata": {}, "score": "78.87041"}
{"text": "/data / fm_test_real ./data / label_train_twoclass .get_labels ( ) return lda , lda.apply ( ) ./examples / documented / python_modular / classifier_liblinear_modular.py . # In this example a two - class linear support vector machine classifier is trained # on a toy data set and the trained classifier is then used to predict labels of # test examples .", "label": "", "metadata": {}, "score": "78.93309"}
{"text": "/data / fm_test_real ./data / label_train_twoclass .get_labels ( ) return lda , lda.apply ( ) ./examples / documented / python_modular / classifier_liblinear_modular.py . # In this example a two - class linear support vector machine classifier is trained # on a toy data set and the trained classifier is then used to predict labels of # test examples .", "label": "", "metadata": {}, "score": "78.93309"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_salzberg_word_string_modular.py . # The SalzbergWordString kernel implements the Salzberg kernel .# # It is described in # # Engineering Support Vector Machine Kernels That Recognize Translation Initiation Sites # A. Zien , G.Raetsch , S. Mika , B. Schoelkopf , T. Lengauer , K.-R. /data / fm_train_dna .", "label": "", "metadata": {}, "score": "79.01691"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CChiSquareDistance.html .# # Obviously , using the ChiSquare distance is not limited to this showcase # example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "79.07419"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CChiSquareDistance.html .# # Obviously , using the ChiSquare distance is not limited to this showcase # example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "79.07419"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CChiSquareDistance.html .# # Obviously , using the ChiSquare distance is not limited to this showcase # example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "79.07419"}
{"text": "The # strings are all of the same length and consist of the characters ' ACGT ' corresponding # to the DNA - alphabet .Each column of the matrices of type char corresponds to # one training / test example .", "label": "", "metadata": {}, "score": "79.11488"}
{"text": "# ROC curve ( as matrix ) and auROC ( area under ROC ) is returned ./data / label_train_twoclass .Features . /examples / documented / python_modular / features_io_modular.py .# This example shows how to read and write plain ascii files , binary files and # hdf5 datasets .", "label": "", "metadata": {}, "score": "79.35638"}
{"text": "/data / fm_test_real .add_preproc(preproc ) ; feats_train .apply_preproc ( ) ; feats_test .add_preproc(preproc ) ; feats_test ./examples / documented / octave_modular / preproc_prunevarsubmean_modular .m . %In this example a kernel matrix is computed for a given real - valued data set . %", "label": "", "metadata": {}, "score": "79.39371"}
{"text": "The strings consist of the characters ' ACGT ' corresponding % to the DNA - alphabet .Each column of the matrices of type char corresponds to % one training / test example ./data / fm_train_dna ./data / fm_test_dna .", "label": "", "metadata": {}, "score": "79.39752"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular / kernel_poly_modular.py .# This example initializes the polynomial kernel with real data .# If variable ' inhomogene ' is ' True ' +1 is added to the scalar product # before taking it to the power of ' degree ' .", "label": "", "metadata": {}, "score": "79.42474"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / octave_modular / kernel_combined_modular .m . %This is an example for the initialization of a combined kernel , which is a weighted sum of % in this case three kernels on real valued data .", "label": "", "metadata": {}, "score": "79.50551"}
{"text": "/data / fm_test_real . /examples / documented / r_modular / kernel_combined_modular .R .# This is an example for the initialization of a combined kernel , which is a weighted sum of # in this case three kernels on real valued data .", "label": "", "metadata": {}, "score": "79.507545"}
{"text": "The oligo string kernel is % implemented for the DNA - alphabet ' ACGT ' ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / octave_modular / kernel_poly_match_word_string .m . %This is an example for the initialization of the PolyMatchString kernel on string data . %", "label": "", "metadata": {}, "score": "79.54918"}
{"text": "% % First , you need to determine what type your data will be , because this % will determine the distance function you can use . %% This example loads two stored matrices of real values from different % files and initializes the matrices to ' RealFeatures ' .", "label": "", "metadata": {}, "score": "79.5497"}
{"text": "The Journal of Machine Learning Research , vol .10 , % pp .2157 - -2192 .October 2009 ./data / label_train_twoclass ./data / fm_train_real ./data / fm_test_real .get_labels ( ) ; . /examples / documented / octave_modular / classifier_svmsgd_modular .", "label": "", "metadata": {}, "score": "79.558136"}
{"text": "R .# This example shows usage of a k - nearest neighbor ( KNN ) classification rule on # a toy data set .Finally , the KNN rule is applied to predict # labels of test examples ./data / fm_train_real .", "label": "", "metadata": {}, "score": "79.657776"}
{"text": "Each column of the matrices of type char corresponds to # one training / test example ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular / kernel_poly_modular.py .# This example initializes the polynomial kernel with real data .", "label": "", "metadata": {}, "score": "79.662506"}
{"text": "Each column of the matrices of type char corresponds to # one training / test example ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular / kernel_poly_modular.py .# This example initializes the polynomial kernel with real data .", "label": "", "metadata": {}, "score": "79.662506"}
{"text": "This approach is just one example of what can be done using the distance % functions provided by shogun .% % First , you need to determine what type your data will be , because this % will determine the distance function you can use . %", "label": "", "metadata": {}, "score": "79.74025"}
{"text": "This approach is just one example of what can be done using the distance % functions provided by shogun .% % First , you need to determine what type your data will be , because this % will determine the distance function you can use . %", "label": "", "metadata": {}, "score": "79.74025"}
{"text": "This approach is just one example of what can be done using the distance % functions provided by shogun .% % First , you need to determine what type your data will be , because this % will determine the distance function you can use . %", "label": "", "metadata": {}, "score": "79.74025"}
{"text": "Furthermore , we demonstrate how to extract # compressed streams on - the - fly in order to fit data sets into # memory that would be too large , otherwise .str \" , True ) # print \" uncompressed strings \" , f2.get_features ( ) # print # load compressed data and uncompress on load # snappy - not stable yet ? !", "label": "", "metadata": {}, "score": "80.00734"}
{"text": "Furthermore , we demonstrate how to extract # compressed streams on - the - fly in order to fit data sets into # memory that would be too large , otherwise .str \" , True ) # print \" uncompressed strings \" , f2.get_features ( ) # print # load compressed data and uncompress on load # snappy - not stable yet ? !", "label": "", "metadata": {}, "score": "80.00734"}
{"text": "/data / fm_test_real ./data / label_train_twoclass .get_labels ( ) return lda , lda.classify ( ) ./examples / documented / python_modular / classifier_liblinear_modular.py . # In this example a two - class linear support vector machine classifier is trained # on a toy data set and the trained classifier is then used to predict labels of # test examples .", "label": "", "metadata": {}, "score": "80.02079"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_knn_modular.py .# This example shows usage of a k - nearest neighbor ( KNN ) classification rule on # a toy data set .Finally , the KNN rule is applied to predict # labels of test examples .", "label": "", "metadata": {}, "score": "80.03945"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_knn_modular.py .# This example shows usage of a k - nearest neighbor ( KNN ) classification rule on # a toy data set .Finally , the KNN rule is applied to predict # labels of test examples .", "label": "", "metadata": {}, "score": "80.03945"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_knn_modular.py .# This example shows usage of a k - nearest neighbor ( KNN ) classification rule on # a toy data set .Finally , the KNN rule is applied to predict # labels of test examples .", "label": "", "metadata": {}, "score": "80.03945"}
{"text": "[ 0027 ] Network adapters may also be coupled to the system to enable the data processing system to become coupled to other data processing systems or remote printers or storage devices through intervening private or public networks .Modems , cable modem and Ethernet cards are just a few of the currently available types of network adapters .", "label": "", "metadata": {}, "score": "80.083786"}
{"text": "The system of claim 11 , wherein the classifier is configured to apply a classification having three or more classes .Description : .RELATED APPLICATION INFORMATION .[ 0001 ] This application claims priority to provisional application Ser .No .", "label": "", "metadata": {}, "score": "80.195816"}
{"text": "[0045 ] Referring now to FIG .4 , a training procedure is shown that sets the weights described above .Block 402 initializes the parameters \u03b8 i for the associated functions f i .Decision block 404 marks the determination of whether the parameters have converged .", "label": "", "metadata": {}, "score": "80.2317"}
{"text": "obtain_from_char(charfeat , order-1 , order , gap , reverse ) ; feats_test .add_preproc(preproc ) ; feats_test ./examples / documented / octave_modular / kernel_comm_word_string_modular .m . %This is an example for the initialization of the CommWordString - kernel ( aka % Spectrum or n - gram kernel ; its name is derived from the unix command comm ) .", "label": "", "metadata": {}, "score": "80.24095"}
{"text": "Bioinformatics , 21:369 - 377 , June 2005 ./data / fm_train_dna ./data / fm_test_dna ./data / label_train_dna ./examples / documented / r_modular / classifier_svmlin_modular .R .# In this example a two - class linear support vector machine classifier ( SVM ) is # trained on a toy data set and the trained classifier is used to predict labels # of test examples .", "label": "", "metadata": {}, "score": "80.27753"}
{"text": "Bioinformatics , 21:369 - 377 , June 2005 ./data / fm_train_dna ./data / fm_test_dna ./data / label_train_dna ./examples / documented / r_modular / classifier_svmlin_modular .R .# In this example a two - class linear support vector machine classifier ( SVM ) is # trained on a toy data set and the trained classifier is used to predict labels # of test examples .", "label": "", "metadata": {}, "score": "80.27753"}
{"text": "For each iteration , the process returns to the determining of whether to perform supervised improvement .Thus , some iterations can use supervised improvement while others do not .One example iterative schedule can begin with two iterations of the unsupervised method to remove any clearly erroneous features , then perform one supervised iteration to remove any clusters of semantically related wrong features .", "label": "", "metadata": {}, "score": "80.36451"}
{"text": "This is an example for the initialization of a linear kernel on raw byte % data ./data / fm_train_byte ./data / fm_test_byte . /examples / documented / octave_modular / kernel_linear_modular .m . %This is an example for the initialization of a linear kernel on real valued % data using scaling factor 1.2 .", "label": "", "metadata": {}, "score": "80.39384"}
{"text": "Consistent with the previous findings , both the overall G / C% and the 5 ' terminal G / C% ( first 5 bases ) of the antisense strand were selected by the RFs in this study .It was previously shown that the frequency of U , but not G or GG , was positively correlated with siRNA efficacy [ 8 ] .", "label": "", "metadata": {}, "score": "80.42819"}
{"text": "The solver stops if the # relative duality gap falls below 1e-5 .# # For more details on the used SVM solver see # V.Franc : Optimization Algorithms for Kernel Methods .Research report .# CTU - CMP-2005 - 22 .", "label": "", "metadata": {}, "score": "80.50196"}
{"text": "The solver stops if the # relative duality gap falls below 1e-5 .# # For more details on the used SVM solver see # V.Franc : Optimization Algorithms for Kernel Methods .Research report .# CTU - CMP-2005 - 22 .", "label": "", "metadata": {}, "score": "80.50196"}
{"text": "The solver stops if the # relative duality gap falls below 1e-5 .# # For more details on the used SVM solver see # V.Franc : Optimization Algorithms for Kernel Methods .Research report .# CTU - CMP-2005 - 22 .", "label": "", "metadata": {}, "score": "80.50196"}
{"text": "/data / fm_train_dna .Evaluation . /examples / documented / python_modular / evaluation_contingencytableevaluation_modular.py . # In this example various ( accuracy , error rate , . measures are being computed # for the pair of ground truth toy data and random data .", "label": "", "metadata": {}, "score": "80.51613"}
{"text": "/data / fm_train_dna .Evaluation . /examples / documented / python_modular / evaluation_contingencytableevaluation_modular.py . # In this example various ( accuracy , error rate , . measures are being computed # for the pair of ground truth toy data and random data .", "label": "", "metadata": {}, "score": "80.51613"}
{"text": "get_labels ( ) ; . /examples / documented / octave_modular / classifier_knn_modular .m . %This example shows usage of a k - nearest neighbor ( KNN ) classification rule on % a toy data set .Finally , the KNN rule is applied to predict % labels of test examples .", "label": "", "metadata": {}, "score": "80.54735"}
{"text": "/data / DynProg_example_py .set_plif_names(all_names ) # pm .reshape(8,1 ) .T ) dyn.precompute_content_values ( ) dyn.init_mod_words_array(data_dict['model'].mod_words . features dyn.set_observation_matrix(features ) dyn.set_content_type_array(data_dict['seg_path']. astype(numpy.float64 ) ) dyn.best_path_set_segment_loss(data_dict['loss'].Tests . /examples / documented / python_modular / tests_check_commwordkernel_memleak_modular.py . /examples / documented / python_modular / classifier_averaged_perceptron_modular.py .", "label": "", "metadata": {}, "score": "80.599915"}
{"text": "/data / DynProg_example_py .set_plif_names(all_names ) # pm .reshape(8,1 ) .T ) dyn.precompute_content_values ( ) dyn.init_mod_words_array(data_dict['model'].mod_words . features dyn.set_observation_matrix(features ) dyn.set_content_type_array(data_dict['seg_path']. astype(numpy.float64 ) ) dyn.best_path_set_segment_loss(data_dict['loss'].Tests . /examples / documented / python_modular / tests_check_commwordkernel_memleak_modular.py . /examples / documented / python_modular / classifier_averaged_perceptron_modular.py .", "label": "", "metadata": {}, "score": "80.599915"}
{"text": "R .# In this example the Histogram algorithm object computes a histogram over all # 16bit unsigned integers in the features ./data / fm_train_dna . /examples / documented / r_modular / distribution_hmm_modular .R .# In this example a hidden markov model with 3 states and 6 transitions is trained # on a string data set .", "label": "", "metadata": {}, "score": "80.60803"}
{"text": "R .# In this example the Histogram algorithm object computes a histogram over all # 16bit unsigned integers in the features ./data / fm_train_dna . /examples / documented / r_modular / distribution_hmm_modular .R .# In this example a hidden markov model with 3 states and 6 transitions is trained # on a string data set .", "label": "", "metadata": {}, "score": "80.60803"}
{"text": "These features can be particularly fast # in linear SVM solvers . /examples / documented / python_modular / features_string_sliding_window_modular.py .# In this example , we demonstrate how to obtain string features # by using a sliding window in a memory - efficient way .", "label": "", "metadata": {}, "score": "80.63005"}
{"text": "These features can be particularly fast # in linear SVM solvers . /examples / documented / python_modular / features_string_sliding_window_modular.py .# In this example , we demonstrate how to obtain string features # by using a sliding window in a memory - efficient way .", "label": "", "metadata": {}, "score": "80.63005"}
{"text": "These features can be particularly fast # in linear SVM solvers . /examples / documented / python_modular / features_string_sliding_window_modular.py .# In this example , we demonstrate how to obtain string features # by using a sliding window in a memory - efficient way .", "label": "", "metadata": {}, "score": "80.63005"}
{"text": "The preprocessor # PruneVarSubMean substracts the mean from each feature and removes features that # have zero variance ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / classifier_custom_kernel_modular.py .# This example shows how to use a custom defined kernel function for training a # two class Support Vector Machine ( SVM ) classifier on a randomly generated # examples . /examples / documented / python_modular / classifier_domainadaptationsvm_modular.py .", "label": "", "metadata": {}, "score": "80.645485"}
{"text": "/data / fm_test_real .add_preproc(preproc ) feats_train .apply_preproc ( ) feats_test .add_preproc(preproc ) feats_test ./examples / documented / python_modular / preproc_prunevarsubmean_modular.py .# In this example a kernel matrix is computed for a given real - valued data set .", "label": "", "metadata": {}, "score": "80.67082"}
{"text": "# In this example the multiquadric kernel is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_oligo_string_modular.py .# This is an example initializing the oligo string kernel which takes distances # between matching oligos ( k - mers ) into account via a gaussian .", "label": "", "metadata": {}, "score": "80.67723"}
{"text": "# In this example the multiquadric kernel is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_oligo_string_modular.py .# This is an example initializing the oligo string kernel which takes distances # between matching oligos ( k - mers ) into account via a gaussian .", "label": "", "metadata": {}, "score": "80.67723"}
{"text": "# This is an example for the initialization of the diag - kernel .# The diag kernel has all kernel matrix entries but those on # the main diagonal set to zero ./data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / kernel_distance_modular .", "label": "", "metadata": {}, "score": "80.71048"}
{"text": "# This is an example for the initialization of the diag - kernel .# The diag kernel has all kernel matrix entries but those on # the main diagonal set to zero ./data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / kernel_distance_modular .", "label": "", "metadata": {}, "score": "80.71048"}
{"text": "Some of the important features in Table 2 were previously shown to be related to the efficacy of gene silencing .Very high G / C contents were found to have a negative effect on siRNA efficacy [ 9 , 13 ] .", "label": "", "metadata": {}, "score": "80.722"}
{"text": "/data / fm_train_real ./data / fm_test_real .add_preprocessor(preproc ) feats_train .apply_preprocessor ( ) feats_test .add_preprocessor(preproc ) feats_test ./examples / documented / python_modular /preprocessor_normone_modular.py .# In this example a kernel matrix is computed for a given real - valued data set .", "label": "", "metadata": {}, "score": "80.78476"}
{"text": "For example , the actions recited in the claims can be performed in a different order and still achieve desirable results .As one example , the processes depicted in the accompanying figures do not necessarily require the particular order shown , or sequential order , to achieve desirable results .", "label": "", "metadata": {}, "score": "80.80997"}
{"text": "The training algorithm is based on BSVM formulation ( L2-soft margin # and the bias added to the objective function ) which is solved by the Improved # Mitchell - Demyanov - Malozemov algorithm .The solver stops if the # relative duality gap falls below 1e-5 .", "label": "", "metadata": {}, "score": "80.829185"}
{"text": "/data / label_train_dna ./examples / documented / python_modular / classifier_svmlight_linear_term_modular.py .# This example demonstrates how to train an SVMLight classifier # using a custom linear term .This is used in the class DASVM that # pre - computes this linear term using a previously trained SVM . /examples / documented / python_modular / classifier_svmlight_modular.py . # In this example a two - class support vector machine classifier is trained on a # DNA splice - site detection data set and the trained classifier is used to predict # labels on test set .", "label": "", "metadata": {}, "score": "80.87755"}
{"text": "/data / label_train_dna ./examples / documented / python_modular / classifier_svmlight_linear_term_modular.py .# This example demonstrates how to train an SVMLight classifier # using a custom linear term .This is used in the class DASVM that # pre - computes this linear term using a previously trained SVM . /examples / documented / python_modular / classifier_svmlight_modular.py . # In this example a two - class support vector machine classifier is trained on a # DNA splice - site detection data set and the trained classifier is used to predict # labels on test set .", "label": "", "metadata": {}, "score": "80.87755"}
{"text": "/data / label_train_dna ./examples / documented / python_modular / classifier_svmlight_linear_term_modular.py .# This example demonstrates how to train an SVMLight classifier # using a custom linear term .This is used in the class DASVM that # pre - computes this linear term using a previously trained SVM . /examples / documented / python_modular / classifier_svmlight_modular.py . # In this example a two - class support vector machine classifier is trained on a # DNA splice - site detection data set and the trained classifier is used to predict # labels on test set .", "label": "", "metadata": {}, "score": "80.87755"}
{"text": "# # For more details on the SVMLIN solver see # V. Sindhwani , S.S. Keerthi .Newton Methods for Fast Solution of Semi - supervised # Linear SVMs ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_svmocas_modular.py .", "label": "", "metadata": {}, "score": "80.89058"}
{"text": "# # For more details on the SVMLIN solver see # V. Sindhwani , S.S. Keerthi .Newton Methods for Fast Solution of Semi - supervised # Linear SVMs ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_svmocas_modular.py .", "label": "", "metadata": {}, "score": "80.89058"}
{"text": "# # For more details on the SVMLIN solver see # V. Sindhwani , S.S. Keerthi .Newton Methods for Fast Solution of Semi - supervised # Linear SVMs ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_svmocas_modular.py .", "label": "", "metadata": {}, "score": "80.89058"}
{"text": "# This is an example for the initialization of the diag - kernel .# The diag kernel has all kernel matrix entries but those on # the main diagonal set to zero . /examples / documented / python_modular / kernel_distance_modular.py .", "label": "", "metadata": {}, "score": "81.060394"}
{"text": "# This is an example for the initialization of the diag - kernel .# The diag kernel has all kernel matrix entries but those on # the main diagonal set to zero . /examples / documented / python_modular / kernel_distance_modular.py .", "label": "", "metadata": {}, "score": "81.060394"}
{"text": "# This is an example for the initialization of the diag - kernel .# The diag kernel has all kernel matrix entries but those on # the main diagonal set to zero . /examples / documented / python_modular / kernel_distance_modular.py .", "label": "", "metadata": {}, "score": "81.060394"}
{"text": "/data / fm_test_dna ./data / fm_test_real .add_preprocessor(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .add_preprocessor(preproc ) feats_test ./examples / documented / python_modular / distance_minkowski_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .", "label": "", "metadata": {}, "score": "81.0701"}
{"text": "/data / fm_test_dna ./data / fm_test_real .add_preprocessor(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .add_preprocessor(preproc ) feats_test ./examples / documented / python_modular / distance_minkowski_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .", "label": "", "metadata": {}, "score": "81.0701"}
{"text": "# The Weighted Degree String kernel .# # The WD kernel of order d compares two sequences X and # Y of length L by summing all contributions of k - mer matches of # lengths k in 1 ... d , weighted by coefficients beta_k . is the indicator function # which evaluates to 1 when its argument is true and to 0 # otherwise .", "label": "", "metadata": {}, "score": "81.07378"}
{"text": "# The Weighted Degree String kernel .# # The WD kernel of order d compares two sequences X and # Y of length L by summing all contributions of k - mer matches of # lengths k in 1 ... d , weighted by coefficients beta_k . is the indicator function # which evaluates to 1 when its argument is true and to 0 # otherwise .", "label": "", "metadata": {}, "score": "81.07378"}
{"text": "Several features of base composition and G / C content were also selected by the RFs .In addition , the nucleotide identity at the third position ( NT3 , from the 5 ' end ) of the antisense strand may be an important feature for siRNA classification .", "label": "", "metadata": {}, "score": "81.08205"}
{"text": "# ROC curve ( as matrix ) and auROC ( area under ROC ) is returned ./data / label_train_twoclass . /examples / documented / python_modular /evaluation_thresholds_modular.py .Features . /examples / documented / python_modular / features_io_modular.py .# This example shows how to read and write plain ascii files , binary files and # hdf5 datasets .", "label": "", "metadata": {}, "score": "81.392654"}
{"text": "Extensions and applications .October ./data / fm_train_real . /examples / documented / python_modular / converter_kernellocaltangentspacealignment_modular.py ./data / fm_train_real . /examples / documented / python_modular / converter_laplacianeigenmaps_modular.py .# In this example toy data is being processed using Laplacian Eigenmaps # algorithm as described in # # Belkin , M. , & Niyogi , P. ( 2002 ) .", "label": "", "metadata": {}, "score": "81.41245"}
{"text": "This is what this log # transformation controls for ./data / fm_train_real ./data / fm_test_real .add_preproc(preproc ) feats_train .apply_preproc ( ) feats_test .add_preproc(preproc ) feats_test ./examples / documented / python_modular / preproc_normone_modular.py . # In this example a kernel matrix is computed for a given real - valued data set .", "label": "", "metadata": {}, "score": "81.45093"}
{"text": "/data / fm_train_real ./data / fm_test_real .get_labels ( ) ; . /examples / documented / octave_modular / classifier_lda_modular .m . %In this example a two - class linear classifier based on the Linear Discriminant % Analysis ( LDA ) is trained on a toy data set and then the trained classifier is % used to predict test examples .", "label": "", "metadata": {}, "score": "81.64607"}
{"text": "R .The Perceptron algorithm works by # iteratively passing though the training examples and applying the update rule on # those examples which are misclassified by the current classifier ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / r_modular / classifier_svmlight_modular .", "label": "", "metadata": {}, "score": "81.66645"}
{"text": "/data / fm_test_real ./data / label_train_multiclass .Preprocessor . /examples / documented / r_modular / preprocessor_logplusone_modular .R .# In this example a kernel matrix is computed for a given real - valued data set .# The kernel used is the Chi2 kernel which operates on real - valued vectors .", "label": "", "metadata": {}, "score": "81.70712"}
{"text": "/data / fm_test_real ./data / label_train_multiclass .Preprocessor . /examples / documented / r_modular / preprocessor_logplusone_modular .R .# In this example a kernel matrix is computed for a given real - valued data set .# The kernel used is the Chi2 kernel which operates on real - valued vectors .", "label": "", "metadata": {}, "score": "81.70712"}
{"text": "If you want to use this mechanism , use the add_instance ( ) and predict ( ) methods .If not , use the add_instance_i ( ) ( or read_instances ( ) ) and predict_i ( ) methods .The following parameters can be set by using methods with their corresponding names - for instance , the maxiter parameter can be set by using set_maxiter($x ) , where $ x is the new desired value .", "label": "", "metadata": {}, "score": "81.7674"}
{"text": "The permuted oob instances as well as the original oob instances are then classified using the tree .The number of correct classifications on the original oob instances is subtracted by the number of predictions for the correct class on the permuted oob instances to calculate a raw score based on the tree .", "label": "", "metadata": {}, "score": "81.80191"}
{"text": "/data / fm_train_real ./data / fm_test_real .add_preprocessor(preproc ) feats_train .apply_preprocessor ( ) feats_test .add_preprocessor(preproc ) feats_test ./examples / documented / python_modular /preprocessor_multidimensionalscaling_modular.py .# In this example toy data is being processed using the multidimensional # scaling as described on p.261 ( Section 12.1 ) of # # Borg , I. , & Groenen , P. J. F. ( 2005 ) .", "label": "", "metadata": {}, "score": "81.8902"}
{"text": "The Journal of Machine Learning Research , vol .10 , # pp .2157 - -2192 .October 2009 ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / r_modular / classifier_svmsgd_modular .R .", "label": "", "metadata": {}, "score": "81.96984"}
{"text": "The Journal of Machine Learning Research , vol .10 , # pp .2157 - -2192 .October 2009 ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / r_modular / classifier_svmsgd_modular .R .", "label": "", "metadata": {}, "score": "81.96984"}
{"text": "# Proceedings of National Academy of Science ( Vol .100 , pp .5591 - 5596 ) ./data / fm_train_real . /examples / documented / python_modular /preprocessor_isomap_modular.py . # In this example toy data is being processed using the Isomap algorithm # as described in # # Silva , V. D. , & Tenenbaum , J. B. ( 2003 ) .", "label": "", "metadata": {}, "score": "82.01579"}
{"text": "Adds a training instance to the set of instances which will be used to train the model .An attributes parameter specifies a hash of attribute - value pairs for the instance , and a label parameter specifies the label .The label must be a number , and typically it should be 1 for positive training instances and -1 for negative training instances .", "label": "", "metadata": {}, "score": "82.32788"}
{"text": "% % For more details see doc / classshogun_1_1CGeodesicMetric.html . % % Obviously , using the Geodesic distance is not limited to this showcase % example ./data / fm_train_real ./data / fm_test_real . /examples / documented / octave_modular / distance_hammingword_modular .", "label": "", "metadata": {}, "score": "82.398254"}
{"text": "/data / fm_train_real ./data / fm_test_real .get_labels ( ) ; . /examples / documented / octave_modular / classifier_libsvm_minimal_modular .m . %In this example a two - class support vector machine classifier is trained on a % 2-dimensional randomly generated data set and the trained classifier is used to % predict labels of test examples . /examples / documented / octave_modular / classifier_libsvm_modular .", "label": "", "metadata": {}, "score": "82.48132"}
{"text": "If ' use_sign ' is set to one each k - mere is counted # only once ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_const_modular .R .# The constant kernel gives a trivial kernel matrix with all entries set to the same value # defined by the argument ' c ' .", "label": "", "metadata": {}, "score": "82.51346"}
{"text": "If ' use_sign ' is set to one each k - mere is counted # only once ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_const_modular .R .# The constant kernel gives a trivial kernel matrix with all entries set to the same value # defined by the argument ' c ' .", "label": "", "metadata": {}, "score": "82.51346"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / octave_modular / kernel_polymatchstring_modular .m . %This is an example for the initialization of the PolyMatchString kernel on string data . %The PolyMatchString kernel sums over the matches of two stings of the same length and % takes the sum to the power of ' degree ' .", "label": "", "metadata": {}, "score": "82.56468"}
{"text": "For this kernel the linadd speedups are implemented # ( though there is room for improvement here when a whole set of sequences is # ADDed ) using sorted lists ./data / fm_train_dna ./data / fm_test_dna .add_preprocessor(preproc ) feats_train .", "label": "", "metadata": {}, "score": "82.63745"}
{"text": "For this kernel the linadd speedups are implemented # ( though there is room for improvement here when a whole set of sequences is # ADDed ) using sorted lists ./data / fm_train_dna ./data / fm_test_dna .add_preprocessor(preproc ) feats_train .", "label": "", "metadata": {}, "score": "82.63745"}
{"text": "An n+k - gram starts and ends at a non - skip word , contains exactly n non - skip works and a variable number ( k ) of skip words that come in - between them .TABLE -US-00003 come and hear hear the sound sound of rain heavy drops plim plum .", "label": "", "metadata": {}, "score": "82.63847"}
{"text": "Interestingly , gene expression inhibition is significantly affected by nucleotide dimer and trimer compositions of siRNA sequence .Conclusions .The findings in this study should help design potent siRNAs for functional genomics , and might also provide further insights into the molecular mechanism of RNA interference .", "label": "", "metadata": {}, "score": "82.664665"}
{"text": "R .The Perceptron algorithm works by # iteratively passing though the training examples and applying the update rule on # those examples which are misclassified by the current classifier ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / r_modular / classifier_subgradientsvm_modular .", "label": "", "metadata": {}, "score": "82.84474"}
{"text": "/data / fm_train_real ./data / fm_test_real .add_preproc(preproc ) ; feats_train .apply_preproc ( ) ; feats_test .add_preproc(preproc ) ; feats_test ./examples / documented / octave_modular / preproc_normone_modular .m . %In this example a kernel matrix is computed for a given real - valued data set . %", "label": "", "metadata": {}, "score": "82.855774"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / preprocessor_normone_modular .R .# In this example a kernel matrix is computed for a given real - valued data set .# The kernel used is the Chi2 kernel which operates on real - valued vectors .", "label": "", "metadata": {}, "score": "82.858925"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / preprocessor_normone_modular .R .# In this example a kernel matrix is computed for a given real - valued data set .# The kernel used is the Chi2 kernel which operates on real - valued vectors .", "label": "", "metadata": {}, "score": "82.858925"}
{"text": "/data / fm_train_dna ./data / fm_test_dna ./data / fm_test_real .add_preprocessor(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .add_preprocessor(preproc ) feats_test ./examples / documented / python_modular / distance_jensen_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .", "label": "", "metadata": {}, "score": "82.92961"}
{"text": "/data / fm_train_dna ./data / fm_test_dna ./data / fm_test_real .add_preprocessor(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .add_preprocessor(preproc ) feats_test ./examples / documented / python_modular / distance_jensen_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .", "label": "", "metadata": {}, "score": "82.92961"}
{"text": "This example shows how to compute the Hamming Word Distance for string features ./data / fm_train_dna ./data / fm_test_dna .add_preproc(preproc ) ; feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) ; feats_test .add_preproc(preproc ) ; feats_test .", "label": "", "metadata": {}, "score": "82.97374"}
{"text": "% % The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .% % For more details see doc / classshogun_1_1CChiSquareDistance.html . % % Obviously , using the ChiSquare distance is not limited to this showcase % example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "83.01435"}
{"text": "Nevertheless , many of the other features in Table 2 , including the top three features ( UCC% , CAG% , and GAG% ) , have not been well documented in the literature .Thus , the findings in this study provide new insights into the rational design of potent siRNAs .", "label": "", "metadata": {}, "score": "83.06333"}
{"text": "/data / fm_train_real ./data / fm_test_real .get_labels ( ) ; else disp('No support for SVRLight available . ' ) end", "label": "", "metadata": {}, "score": "83.20274"}
{"text": "# # For more details on the SVMLIN solver see # V. Sindhwani , S.S. Keerthi .Newton Methods for Fast Solution of Semi - supervised # Linear SVMs ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / r_modular / classifier_svmocas_modular .", "label": "", "metadata": {}, "score": "83.30679"}
{"text": "# # For more details on the SVMLIN solver see # V. Sindhwani , S.S. Keerthi .Newton Methods for Fast Solution of Semi - supervised # Linear SVMs ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / r_modular / classifier_svmocas_modular .", "label": "", "metadata": {}, "score": "83.30679"}
{"text": "# In this example PRC ( Precision - Recall curve ) is being computed # for the pair of ground truth toy labels and random labels .# PRC curve ( as matrix ) and auPRC ( area under PRC ) is returned .", "label": "", "metadata": {}, "score": "83.32892"}
{"text": "# In this example PRC ( Precision - Recall curve ) is being computed # for the pair of ground truth toy labels and random labels .# PRC curve ( as matrix ) and auPRC ( area under PRC ) is returned .", "label": "", "metadata": {}, "score": "83.32892"}
{"text": "% % For more details on the SVM^light see % T. Joachims .Making large - scale SVM learning practical .In Advances in Kernel % Methods -- Support Vector Learning , pages 169 - 184 .MIT Press , Cambridge , MA USA , 1999 .", "label": "", "metadata": {}, "score": "83.44579"}
{"text": "The ROC curve is drawn by plotting the true positive rate ( i.e. , sensitivity ) against the false positive rate , which equals to ( 1 - specificity ) .In this work , the ROC curve has been generated by varying the output threshold of a classifier and plotting the true positive rate against false positive rate for each threshold value .", "label": "", "metadata": {}, "score": "83.53731"}
{"text": "/data / fm_test_dna .add_preproc(preproc ) ; feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) ; feats_test .add_preproc(preproc ) ; feats_test ./examples / documented / octave_modular / kernel_weighted_degree_position_string_modular .m . %The Weighted Degree Position String kernel ( Weighted Degree kernel with shifts ) .", "label": "", "metadata": {}, "score": "83.698395"}
{"text": "# # Note that TOP - features are computed on the fly , so to be effective feature # caching should be enabled . # # It inherits its functionality from CSimpleFeatures , which should be # consulted for further reference .", "label": "", "metadata": {}, "score": "83.70018"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / r_modular / classifier_knn_modular .R .# This example shows usage of a k - nearest neighbor ( KNN ) classification rule on # a toy data set .Finally , the KNN rule is applied to predict # labels of test examples .", "label": "", "metadata": {}, "score": "83.703705"}
{"text": "The oligo string kernel is # implemented for the DNA - alphabet ' ACGT ' ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_poly_modular .R .# This example initializes the polynomial kernel with real data .", "label": "", "metadata": {}, "score": "83.7444"}
{"text": "The oligo string kernel is # implemented for the DNA - alphabet ' ACGT ' ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_poly_modular .R .# This example initializes the polynomial kernel with real data .", "label": "", "metadata": {}, "score": "83.7444"}
{"text": "# This example demonstrates the use of the AUC Kernel ./data / fm_train_real ./data / label_train_twoclass . /examples / documented / python_modular / kernel_chi2_modular.py .# This is an example for the initialization of the chi2-kernel on real data , where # each column of the matrices corresponds to one training / test example .", "label": "", "metadata": {}, "score": "83.7638"}
{"text": "PubMed View Article .D\u00edaz - Uriarte R , Alvarez de Andr\u00e9s S : Gene selection and classification of microarray data using random forest .BMC Bioinformatics 2006 , 7 : 3 .PubMed View Article .Hofacker IL : Vienna RNA secondary structure server .", "label": "", "metadata": {}, "score": "83.812515"}
{"text": "The algorithm class constructor takes the number of clusters and a distance to % be used as input .The distance used in this example is Euclidean distance . %After training one can fetch the result of clustering by obtaining the cluster % centers and their radiuses .", "label": "", "metadata": {}, "score": "83.98004"}
{"text": "The strings in this example # consist of the characters ' ACGT ' corresponding to the DNA - alphabet .Each # column of the matrices of type char corresponds to one training / test example ./data / fm_train_word ./data / fm_test_word . /examples / documented / r_modular / kernel_sigmoid_modular .", "label": "", "metadata": {}, "score": "84.01384"}
{"text": "The strings in this example # consist of the characters ' ACGT ' corresponding to the DNA - alphabet .Each # column of the matrices of type char corresponds to one training / test example ./data / fm_train_word ./data / fm_test_word . /examples / documented / r_modular / kernel_sigmoid_modular .", "label": "", "metadata": {}, "score": "84.01384"}
{"text": "/data / fm_test_real . /examples / documented / octave_modular / distance_canberra_modular .m . %An approach as applied below , which shows the processing of input data % from a file becomes a crucial factor for writing your own sample applications .", "label": "", "metadata": {}, "score": "84.144745"}
{"text": "Declarations .Acknowledgements .Publication of this supplement was made possible with support from the International Society of Intelligent Biological Medicine ( ISIBM ) .We acknowledge the scientific contributions and inputs of Dr. Mary Qu Yang .References .Hannon GJ , Rossi JJ : Unlocking the potential of the human genome with RNA interference .", "label": "", "metadata": {}, "score": "84.24226"}
{"text": "For this kernel the linadd speedups are implemented # ( though there is room for improvement here when a whole set of sequences is # ADDed ) using sorted lists ./data / fm_train_dna ./data / fm_test_dna .add_preproc(preproc ) feats_train .", "label": "", "metadata": {}, "score": "84.29259"}
{"text": "The other composition features positively correlated with siRNA efficacy include CG% , AAG% , AUC% , GCG% , AAC% , UUU% , ACA% , UUC% , and CAA% ( Table 2 ) .Some trinucleotide or dinucleotide features show negative correlation with siRNA efficacy .", "label": "", "metadata": {}, "score": "84.36751"}
{"text": "obtain_from_char(charfeat , order-1 , order , gap , reverse ) ; feats_test .add_preproc(preproc ) ; feats_test ./examples / documented / octave_modular / kernel_poly_modular . m . %This example initializes the polynomial kernel with real data .% If variable ' inhomogene ' is ' True ' +1 is added to the scalar product % before taking it to the power of ' degree ' .", "label": "", "metadata": {}, "score": "84.40848"}
{"text": "SIAM ./data / fm_train_real . /examples / documented / python_modular / preprocessor_logplusone_modular.py .# In this example a kernel matrix is computed for a given real - valued data set .# The kernel used is the Chi2 kernel which operates on real - valued vectors .", "label": "", "metadata": {}, "score": "84.49801"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / r_modular / classifier_liblinear_modular .R .# In this example a two - class linear support vector machine classifier is trained # on a toy data set and the trained classifier is then used to predict labels of # test examples .", "label": "", "metadata": {}, "score": "84.5782"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / r_modular / classifier_liblinear_modular .R .# In this example a two - class linear support vector machine classifier is trained # on a toy data set and the trained classifier is then used to predict labels of # test examples .", "label": "", "metadata": {}, "score": "84.5782"}
{"text": "In this example a kernel matrix is computed for a given real - valued data set . %The kernel used is the Chi2 kernel which operates on real - valued vectors .It % computes the chi - squared distance between sets of histograms .", "label": "", "metadata": {}, "score": "84.613205"}
{"text": "This study examined 120 sequence features belonging to six groups ( Table 1 ) .The first group has 19 features , each of which is the nucleotide identity ( A , U , G or C ) of a sequence position in a siRNA .", "label": "", "metadata": {}, "score": "84.63505"}
{"text": "Springer .# # Before processing the landmark approximation is disabled ./data / fm_train_real . /examples / documented / python_modular /preprocessor_normone_modular.py .# In this example a kernel matrix is computed for a given real - valued data set .", "label": "", "metadata": {}, "score": "84.68267"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . shape # fm_test_real . /examples / documented / python_modular / mkl_multiclass_modular.py . # In this example we show how to perform Multiple Kernel Learning ( MKL ) # with the modular interface for multi - class classification .", "label": "", "metadata": {}, "score": "84.6986"}
{"text": "[ 0026 ] A data processing system suitable for storing and/or executing program code may include at least one processor coupled directly or indirectly to memory elements through a system bus .The memory elements can include local memory employed during actual execution of the program code , bulk storage , and cache memories which provide temporary storage of at least some program code to reduce the number of times code is retrieved from bulk storage during execution .", "label": "", "metadata": {}, "score": "84.78419"}
{"text": "# After training one can fetch the result of clustering by obtaining the cluster # centers and their radiuses ./data / fm_train_real .Distance . /examples / documented / python_modular / distance_braycurtis_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .", "label": "", "metadata": {}, "score": "84.811386"}
{"text": "MIT Press ./data / fm_train_real . /examples / documented / python_modular / preprocessor_laplacianeigenmaps_modular.py .# In this example toy data is being processed using Laplacian Eigenmaps # algorithm as described in # # Belkin , M. , & Niyogi , P. ( 2002 ) .", "label": "", "metadata": {}, "score": "85.04549"}
{"text": "/data / fm_train_real ./data / fm_test_real .get_labels ( ) ; . /examples / documented / octave_modular / classifier_libsvmoneclass_modular .m . %In this example a one - class support vector machine classifier is trained on a % toy data set .", "label": "", "metadata": {}, "score": "85.16018"}
{"text": "# The standard Sigmoid kernel computed on dense real valued features ./data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / kernel_simple_locality_improved_string_modular .R . # SimpleLocalityImprovedString kernel , is a ' simplified ' and better performing version of the Locality improved kernel .", "label": "", "metadata": {}, "score": "85.18428"}
{"text": "# The standard Sigmoid kernel computed on dense real valued features ./data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / kernel_simple_locality_improved_string_modular .R . # SimpleLocalityImprovedString kernel , is a ' simplified ' and better performing version of the Locality improved kernel .", "label": "", "metadata": {}, "score": "85.18428"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / kernel_combined_modular.py .# This is an example for the initialization of a combined kernel , which is a weighted sum of # in this case three kernels on real valued data .", "label": "", "metadata": {}, "score": "85.21798"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / kernel_combined_modular.py .# This is an example for the initialization of a combined kernel , which is a weighted sum of # in this case three kernels on real valued data .", "label": "", "metadata": {}, "score": "85.21798"}
{"text": "Since the ROC plot is a unit square , the maximum value of AUC is 1 , which is achieved by a perfect classifier .Weak classifiers have AUC values close to 0.5 .Results and discussion .Random forest - based selection of important features .", "label": "", "metadata": {}, "score": "85.224976"}
{"text": "# # For more details see doc / classshogun_1_1CGeodesicMetric.html . # # Obviously , using the Geodesic distance is not limited to this showcase # example ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / distance_hammingword_modular.py .", "label": "", "metadata": {}, "score": "85.2478"}
{"text": "# # For more details see doc / classshogun_1_1CGeodesicMetric.html . # # Obviously , using the Geodesic distance is not limited to this showcase # example ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / distance_hammingword_modular.py .", "label": "", "metadata": {}, "score": "85.2478"}
{"text": "# # For more details see doc / classshogun_1_1CGeodesicMetric.html . # # Obviously , using the Geodesic distance is not limited to this showcase # example ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / distance_hammingword_modular.py .", "label": "", "metadata": {}, "score": "85.2478"}
{"text": "/data / label_train_multiclass . /examples / documented / r_modular / classifier_lda_modular .R .# In this example a two - class linear classifier based on the Linear Discriminant # Analysis ( LDA ) is trained on a toy data set and then the trained classifier is # used to predict test examples .", "label": "", "metadata": {}, "score": "85.263275"}
{"text": "# Advances in Neural Information Processing Systems 15 , 15(Figure 2 ) , 721 - 728 .MIT Press ./data / fm_train_real . /examples / documented / python_modular /converter_kernellocallylinearembedding_modular.py . # In this example toy data is being processed using kernel extension # of the Locally Linear Embedding ( LLE ) algorithm as described in # # Kayo , O. ( 2006 ) .", "label": "", "metadata": {}, "score": "85.27933"}
{"text": "Features . /examples / documented / python_modular / features_io_modular.py .# This example shows how to read and write plain ascii files , binary files and # hdf5 datasets .# # Binary files use some custom native format and datasets can be read / written # from / to hdf5 files with arbitrary group / path .", "label": "", "metadata": {}, "score": "85.32639"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_comm_ulong_string_modular .R .# This is an example for the initialization of the CommUlongString - kernel .For efficient computing a preprocessor is used # that extracts and sorts all k - mers .", "label": "", "metadata": {}, "score": "85.41516"}
{"text": "# Advances in Neural Information Processing Systems 15 , 15(Figure 2 ) , 721 - 728 .MIT Press ./data / fm_train_real . /examples / documented / python_modular /preprocessor_kernellocallylinearembedding_modular.py . # In this example toy data is being processed using kernel extension # of the Locally Linear Embedding ( LLE ) algorithm as described in # # Kayo , O. ( 2006 ) .", "label": "", "metadata": {}, "score": "85.437195"}
{"text": "/data / fm_test_real ./data / label_train_multiclass . /examples / documented / r_modular / classifier_lda_modular .R .# In this example a two - class linear classifier based on the Linear Discriminant # Analysis ( LDA ) is trained on a toy data set and then the trained classifier is # used to predict test examples .", "label": "", "metadata": {}, "score": "85.52606"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / distance_tanimoto_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .# This approach is just one example of what can be done using the distance # functions provided by shogun .", "label": "", "metadata": {}, "score": "85.54642"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / distance_tanimoto_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .# This approach is just one example of what can be done using the distance # functions provided by shogun .", "label": "", "metadata": {}, "score": "85.54642"}
{"text": "/data / fm_test_real . /examples / documented / r_modular / kernel_custom_modular .R .Labels for the examples are given , a svm is trained and # the svm is used to classify the examples . /examples / documented / r_modular / kernel_diag_modular .", "label": "", "metadata": {}, "score": "85.74008"}
{"text": "/data / fm_test_real . /examples / documented / r_modular / kernel_custom_modular .R .Labels for the examples are given , a svm is trained and # the svm is used to classify the examples . /examples / documented / r_modular / kernel_diag_modular .", "label": "", "metadata": {}, "score": "85.74008"}
{"text": "% % For more details see doc / classshogun_1_1CJensenMetric.html . % % Obviously , using the Jensen - Shannon distance / divergence is not limited to % this showcase example ./data / fm_train_real ./data / fm_test_real . /examples / documented / octave_modular / distance_manhatten_modular . m . %", "label": "", "metadata": {}, "score": "85.891045"}
{"text": "/data / fm_test_real .add_preproc(preproc ) ; feats_train .apply_preproc ( ) ; feats_test .add_preproc(preproc ) ; feats_test .Regression . /examples / documented / octave_modular / regression_krr_modular .m . %In this example a kernelized version of ridge regression ( KRR ) is trained on a % real - valued data set .", "label": "", "metadata": {}, "score": "85.91897"}
{"text": "In Bottou , Leon and Chapelle , Olivier and DeCoste , Dennis and Weston , # Jason , editor , In Large Scale Kernel Machines , pages 73 - 103 , MIT Press , # Cambridge , MA ./data / fm_train_dna .", "label": "", "metadata": {}, "score": "85.95537"}
{"text": "In Bottou , Leon and Chapelle , Olivier and DeCoste , Dennis and Weston , # Jason , editor , In Large Scale Kernel Machines , pages 73 - 103 , MIT Press , # Cambridge , MA ./data / fm_train_dna .", "label": "", "metadata": {}, "score": "85.95537"}
{"text": "In Bottou , Leon and Chapelle , Olivier and DeCoste , Dennis and Weston , # Jason , editor , In Large Scale Kernel Machines , pages 73 - 103 , MIT Press , # Cambridge , MA ./data / fm_train_dna .", "label": "", "metadata": {}, "score": "85.95537"}
{"text": "# This is an example for the initialization of a linear kernel on raw byte # data ./data / fm_train_byte ./data / fm_test_byte . /examples / documented / r_modular / kernel_linear_modular .R .# This is an example for the initialization of a linear kernel on real valued # data using scaling factor 1.2 .", "label": "", "metadata": {}, "score": "85.98195"}
{"text": "ICML 2007 ./data / fm_train_real ./data / fm_test_real ./data / label_train_multiclass . set_tau(1e-3 ) svm.set_batch_mode(False ) # svm . /examples / documented / python_modular / classifier_lda_modular.py . # In this example a two - class linear classifier based on the Linear Discriminant # Analysis ( LDA ) is trained on a toy data set and then the trained classifier is # used to predict test examples .", "label": "", "metadata": {}, "score": "86.02791"}
{"text": "ICML 2007 ./data / fm_train_real ./data / fm_test_real ./data / label_train_multiclass . set_tau(1e-3 ) svm.set_batch_mode(False ) # svm . /examples / documented / python_modular / classifier_lda_modular.py . # In this example a two - class linear classifier based on the Linear Discriminant # Analysis ( LDA ) is trained on a toy data set and then the trained classifier is # used to predict test examples .", "label": "", "metadata": {}, "score": "86.02791"}
{"text": "Neural Computation , # 14:2397 - 2414 , 2002 .# # which also has the details .# # Note that FK - features are computed on the fly , so to be effective feature # caching should be enabled . # # It inherits its functionality from CSimpleFeatures , which should be # consulted for further reference .", "label": "", "metadata": {}, "score": "86.10523"}
{"text": "Neural Computation , # 14:2397 - 2414 , 2002 .# # which also has the details .# # Note that FK - features are computed on the fly , so to be effective feature # caching should be enabled . # # It inherits its functionality from CSimpleFeatures , which should be # consulted for further reference .", "label": "", "metadata": {}, "score": "86.10523"}
{"text": "Neural Computation , # 14:2397 - 2414 , 2002 .# # which also has the details .# # Note that FK - features are computed on the fly , so to be effective feature # caching should be enabled . # # It inherits its functionality from CSimpleFeatures , which should be # consulted for further reference .", "label": "", "metadata": {}, "score": "86.10523"}
{"text": "The Pearson 's correlation coefficients are -0.289 for CAG% and -0.305 for GAG% .The other composition features with negative effects on siRNA efficacy include GCA% , AUA% , CUG% , AG% , GG% , GGA% , and GGC% ( Table 2 ) .", "label": "", "metadata": {}, "score": "86.23091"}
{"text": "Some nucleotide motifs ( e.g. , UCC ) showed positive correlation with siRNA efficacy , whereas other motifs ( e.g. , GAG ) might have a negative effect on gene silencing .These important features were used to train support vector machines for predicting siRNA potency with relatively high accuracy .", "label": "", "metadata": {}, "score": "86.24654"}
{"text": "% % which also has the details . % % Note that TOP - features are computed on the fly , so to be effective feature % caching should be enabled . % % It inherits its functionality from CSimpleFeatures , which should be % consulted for further reference .", "label": "", "metadata": {}, "score": "86.30026"}
{"text": "Neural Computation , # 14:2397 - 2414 , 2002 .# # which also has the details .# # Note that TOP - features are computed on the fly , so to be effective feature # caching should be enabled . # # It inherits its functionality from CSimpleFeatures , which should be # consulted for further reference .", "label": "", "metadata": {}, "score": "86.31025"}
{"text": "Neural Computation , # 14:2397 - 2414 , 2002 .# # which also has the details .# # Note that TOP - features are computed on the fly , so to be effective feature # caching should be enabled . # # It inherits its functionality from CSimpleFeatures , which should be # consulted for further reference .", "label": "", "metadata": {}, "score": "86.31025"}
{"text": "Neural Computation , # 14:2397 - 2414 , 2002 .# # which also has the details .# # Note that TOP - features are computed on the fly , so to be effective feature # caching should be enabled . # # It inherits its functionality from CSimpleFeatures , which should be # consulted for further reference .", "label": "", "metadata": {}, "score": "86.31025"}
{"text": "Neural Computation , # 14:2397 - 2414 , 2002 .# # which also has the details .# # Note that TOP - features are computed on the fly , so to be effective feature # caching should be enabled . # # It inherits its functionality from CSimpleFeatures , which should be # consulted for further reference .", "label": "", "metadata": {}, "score": "86.31025"}
{"text": "/data / fm_test_dna ./data / fm_test_real .add_preproc(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .add_preproc(preproc ) feats_test ./examples / documented / python_modular / distance_minkowski_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .", "label": "", "metadata": {}, "score": "86.319176"}
{"text": "To applicable in this kernel they # need to be sorted ( e.g. via the SortWordString pre - processor ) .# # Note that this representation is especially tuned to small alphabets # ( like the 2-bit alphabet DNA ) , for which it enables spectrum kernels # of order 8 . # # For this kernel the linadd speedups are quite efficiently implemented using # direct maps .", "label": "", "metadata": {}, "score": "86.46165"}
{"text": "To applicable in this kernel they # need to be sorted ( e.g. via the SortWordString pre - processor ) .# # Note that this representation is especially tuned to small alphabets # ( like the 2-bit alphabet DNA ) , for which it enables spectrum kernels # of order 8 . # # For this kernel the linadd speedups are quite efficiently implemented using # direct maps .", "label": "", "metadata": {}, "score": "86.46165"}
{"text": "/data / fm_test_real .Distribution . /examples / documented / octave_modular / distribution_histogram_modular .m . %In this example the Histogram algorithm object computes a histogram over all % 16bit unsigned integers in the features ./data / fm_train_dna . /examples / documented / octave_modular / distribution_hmm_modular .", "label": "", "metadata": {}, "score": "86.5544"}
{"text": "/data / fm_test_dna .add_preprocessor(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .add_preprocessor(preproc ) feats_test ./examples / documented / python_modular / kernel_weighted_degree_position_string_modular.py .# The Weighted Degree Position String kernel ( Weighted Degree kernel with shifts ) .", "label": "", "metadata": {}, "score": "86.699165"}
{"text": "/data / fm_test_dna .add_preprocessor(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .add_preprocessor(preproc ) feats_test ./examples / documented / python_modular / kernel_weighted_degree_position_string_modular.py .# The Weighted Degree Position String kernel ( Weighted Degree kernel with shifts ) .", "label": "", "metadata": {}, "score": "86.699165"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / octave_modular / kernel_linear_word_modular .m . %This is an example for the initialization of a linear kernel on word ( 2byte ) % data ./data / fm_train_word .", "label": "", "metadata": {}, "score": "86.756645"}
{"text": "% % The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .% % The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .% % For more details see doc / classshogun_1_1CBrayCurtisDistance.html . % % Obviously , using the Bray Curtis distance is not limited to this showcase % example .", "label": "", "metadata": {}, "score": "86.77486"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / distance_chisquare_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .# This approach is just one example of what can be done using the distance # functions provided by shogun .", "label": "", "metadata": {}, "score": "86.77922"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / distance_chisquare_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .# This approach is just one example of what can be done using the distance # functions provided by shogun .", "label": "", "metadata": {}, "score": "86.77922"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / distance_chisquare_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .# This approach is just one example of what can be done using the distance # functions provided by shogun .", "label": "", "metadata": {}, "score": "86.77922"}
{"text": "% % Note that this representation is especially tuned to small alphabets % ( like the 2-bit alphabet DNA ) , for which it enables spectrum kernels % of order 8 . % % For this kernel the linadd speedups are quite efficiently implemented using % direct maps .", "label": "", "metadata": {}, "score": "86.88373"}
{"text": "October 2009 ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_svmsgd_modular.py .# In this example a two - class linear support vector machine classifier is trained # on a toy data set and the trained classifier is used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "86.90771"}
{"text": "October 2009 ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_svmsgd_modular.py .# In this example a two - class linear support vector machine classifier is trained # on a toy data set and the trained classifier is used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "86.90771"}
{"text": "October 2009 ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_svmsgd_modular.py .# In this example a two - class linear support vector machine classifier is trained # on a toy data set and the trained classifier is used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "86.90771"}
{"text": "# In this example an squared euclidian distance is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / distance_sparseeuclidean_modular.py .# In this example a sparse euclidean distance is computed for sparse toy data .", "label": "", "metadata": {}, "score": "87.02725"}
{"text": "# In this example an squared euclidian distance is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / distance_sparseeuclidean_modular.py .# In this example a sparse euclidean distance is computed for sparse toy data .", "label": "", "metadata": {}, "score": "87.02725"}
{"text": "If ' use_sign ' is set to one each k - mere is counted # only once ./data / fm_train_dna ./data / fm_test_dna .add_preprocessor(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .", "label": "", "metadata": {}, "score": "87.333084"}
{"text": "If ' use_sign ' is set to one each k - mere is counted # only once ./data / fm_train_dna ./data / fm_test_dna .add_preprocessor(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .", "label": "", "metadata": {}, "score": "87.333084"}
{"text": "If ' use_sign ' is set to one each k - mere is counted # only once ./data / fm_train_dna ./data / fm_test_dna .add_preprocessor(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .", "label": "", "metadata": {}, "score": "87.333084"}
{"text": "If ' use_sign ' is set to one each k - mere is counted # only once ./data / fm_train_dna ./data / fm_test_dna .add_preprocessor(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .", "label": "", "metadata": {}, "score": "87.333084"}
{"text": "/data / label_train_twoclass . bin \" ) feats2.load(f ) # print \" diff binary \" , numpy.max(numpy.abs(feats2.get_feature_matrix ( ) .flatten()-fm_train_real . ascii \" ) feats2.load(f ) # print \" diff ascii \" , numpy.max(numpy.abs(feats2.get_feature_matrix ( ) .flatten()-fm_train_real .h5\",\"r\" , \" /data / labels \" )", "label": "", "metadata": {}, "score": "87.354996"}
{"text": "If not , processing returns to block 412 .If so , processing returns to block 404 to determine whether the updated parameters have converged .The overall loop may be performed any number of times , whether until convergence or until a maximum number of iterations has been reached .", "label": "", "metadata": {}, "score": "87.38819"}
{"text": "get_labels ( ) ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / regression_svrlight_modular.py .# In this example a support vector regression algorithm is trained on a # real - valued toy data set .", "label": "", "metadata": {}, "score": "87.43003"}
{"text": "get_labels ( ) ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / regression_svrlight_modular.py .# In this example a support vector regression algorithm is trained on a # real - valued toy data set .", "label": "", "metadata": {}, "score": "87.43003"}
{"text": "get_labels ( ) ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / regression_svrlight_modular.py .# In this example a support vector regression algorithm is trained on a # real - valued toy data set .", "label": "", "metadata": {}, "score": "87.43003"}
{"text": "ICML 2007 ./data / fm_train_real ./data / fm_test_real ./data / label_train_multiclass . set_tau(1e-3 ) # svm .set_batch_mode(False ) # svm . /examples / documented / python_modular / classifier_lda_modular.py . # In this example a two - class linear classifier based on the Linear Discriminant # Analysis ( LDA ) is trained on a toy data set and then the trained classifier is # used to predict test examples .", "label": "", "metadata": {}, "score": "87.43866"}
{"text": "Performing additional iterations includes performing one or more iterations without supervised improvement and one or more iterations with supervised improvement .Other embodiments of this aspect include corresponding systems , apparatus , and computer program products .Other embodiments of this aspect include corresponding systems , apparatus , and computer program products .", "label": "", "metadata": {}, "score": "87.485214"}
{"text": "# This is an example for the initialization of the chi2-kernel on real data , where # each column of the matrices corresponds to one training / test example ./data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / kernel_combined_modular .", "label": "", "metadata": {}, "score": "87.57814"}
{"text": "# This example demonstrates how to load string features from files .# We cover two cases : First , we show how to obtain StringCharFeatues # from a directory of text files ( particularly useful in computational biology ) # and second , we demonstrate how to load StringCharFeatues from one ( multi - line ) file . , \" features_string_char_modular . /examples / documented / python_modular / features_string_hashed_wd_modular.py .", "label": "", "metadata": {}, "score": "87.60585"}
{"text": "# This example demonstrates how to load string features from files .# We cover two cases : First , we show how to obtain StringCharFeatues # from a directory of text files ( particularly useful in computational biology ) # and second , we demonstrate how to load StringCharFeatues from one ( multi - line ) file . , \" features_string_char_modular . /examples / documented / python_modular / features_string_hashed_wd_modular.py .", "label": "", "metadata": {}, "score": "87.60585"}
{"text": "# This example demonstrates how to load string features from files .# We cover two cases : First , we show how to obtain StringCharFeatues # from a directory of text files ( particularly useful in computational biology ) # and second , we demonstrate how to load StringCharFeatues from one ( multi - line ) file . , \" features_string_char_modular . /examples / documented / python_modular / features_string_hashed_wd_modular.py .", "label": "", "metadata": {}, "score": "87.60585"}
{"text": "/data / fm_test_dna . /examples / documented / python_modular / kernel_poly_match_word_string_modular.py .# This is an example for the initialization of the PolyMatchString kernel on string data .# The PolyMatchString kernel sums over the matches of two stings of the same length and # takes the sum to the power of ' degree ' .", "label": "", "metadata": {}, "score": "87.65668"}
{"text": "/data / fm_test_dna . /examples / documented / python_modular / kernel_poly_match_word_string_modular.py .# This is an example for the initialization of the PolyMatchString kernel on string data .# The PolyMatchString kernel sums over the matches of two stings of the same length and # takes the sum to the power of ' degree ' .", "label": "", "metadata": {}, "score": "87.65668"}
{"text": "PubMed View Article .Ui - Tei K , Naito Y , Takahashi F , Haraguchi T , Ohki - Hamazaki H , Juni A , Ueda R : Guidelines for the selection of highly effective siRNA sequences for mammalian and chick RNA interference .", "label": "", "metadata": {}, "score": "87.77466"}
{"text": "/data / fm_train_dna ./data / fm_test_dna ./data / fm_test_real .add_preproc(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .add_preproc(preproc ) feats_test ./examples / documented / python_modular / distance_jensen_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .", "label": "", "metadata": {}, "score": "87.77946"}
{"text": "If ' use_normalization ' is # set to ' true ' then kernel matrix will be normalized by the square roots # of the diagonal entries ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular /kernel_power_modular.py . # In this example the power kernel is being computed for toy data .", "label": "", "metadata": {}, "score": "87.810074"}
{"text": "If ' use_normalization ' is # set to ' true ' then kernel matrix will be normalized by the square roots # of the diagonal entries ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular /kernel_power_modular.py . # In this example the power kernel is being computed for toy data .", "label": "", "metadata": {}, "score": "87.810074"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / octave_modular / distance_manhattenword_modular .m . %This example shows how to compute the Manahattan Distance for string features ./data / fm_train_dna ./data / fm_test_dna .add_preproc(preproc ) ; feats_train .", "label": "", "metadata": {}, "score": "87.81471"}
{"text": "The labels of both the train and the test data are % fetched via svr.classify ( ) .get_labels ( ) ./data / label_train_twoclass ./data / fm_train_real ./data / fm_test_real .get_labels ( ) ; . /examples / documented / octave_modular / regression_svrlight_modular .", "label": "", "metadata": {}, "score": "87.82108"}
{"text": "# This examples demonstrates how to encode real - valued features in Shogun , # using RealFeatures . /examples / documented / python_modular / features_snp_modular.py .# Creates features similar to the feature space of the SNP kernel .Useful when # working with linear methods .", "label": "", "metadata": {}, "score": "87.871155"}
{"text": "# This examples demonstrates how to encode real - valued features in Shogun , # using RealFeatures . /examples / documented / python_modular / features_snp_modular.py .# Creates features similar to the feature space of the SNP kernel .Useful when # working with linear methods .", "label": "", "metadata": {}, "score": "87.871155"}
{"text": "set_nth_cov(array([[0.3 , 0.1],[0.1 , 1.0 ] ] ) , 1 ) real_gmm .train(feat_train )est_gmm . /examples / documented / python_modular / clustering_hierarchical_modular.py .# In this example an agglomerative hierarchical single linkage clustering method # is used to cluster a given toy data set .", "label": "", "metadata": {}, "score": "87.93556"}
{"text": "set_nth_cov(array([[0.3 , 0.1],[0.1 , 1.0 ] ] ) , 1 ) real_gmm .train(feat_train )est_gmm . /examples / documented / python_modular / clustering_hierarchical_modular.py .# In this example an agglomerative hierarchical single linkage clustering method # is used to cluster a given toy data set .", "label": "", "metadata": {}, "score": "87.93556"}
{"text": "add_preprocessor(preproc ) feats_test ./examples / documented / python_modular / preprocessor_sortwordstring_modular.py .# In this example a kernel matrix is computed for a given string data set .The # CommWordString kernel is used to compute the spectrum kernel from strings that # have been mapped into unsigned 16bit integers .", "label": "", "metadata": {}, "score": "88.0144"}
{"text": "add_preprocessor(preproc ) feats_test ./examples / documented / python_modular / preprocessor_sortwordstring_modular.py .# In this example a kernel matrix is computed for a given string data set .The # CommWordString kernel is used to compute the spectrum kernel from strings that # have been mapped into unsigned 16bit integers .", "label": "", "metadata": {}, "score": "88.0144"}
{"text": "In NIPS 20 .MIT % Press ./data / label_train_twoclass ./data / fm_train_real ./data / fm_test_real .get_labels ( ) ; .Clustering . /examples / documented / octave_modular / clustering_hierarchical_modular .m . %In this example an agglomerative hierarchical single linkage clustering method % is used to cluster a given toy data set .", "label": "", "metadata": {}, "score": "88.023544"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / regression_libsvr_modular.py . # In this example a support vector regression algorithm is trained on a # real - valued toy data set .The underlying library used for the SVR training is # LIBSVM .", "label": "", "metadata": {}, "score": "88.036026"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / regression_libsvr_modular.py . # In this example a support vector regression algorithm is trained on a # real - valued toy data set .The underlying library used for the SVR training is # LIBSVM .", "label": "", "metadata": {}, "score": "88.036026"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / regression_libsvr_modular.py . # In this example a support vector regression algorithm is trained on a # real - valued toy data set .The underlying library used for the SVR training is # LIBSVM .", "label": "", "metadata": {}, "score": "88.036026"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / distance_sparseeuclidean_modular.py ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / distance_tanimoto_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .", "label": "", "metadata": {}, "score": "88.24863"}
{"text": "If ' use_normalization ' is # set to ' true ' then kernel matrix will be normalized by the square roots # of the diagonal entries ./data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / kernel_polymatchstring_modular .", "label": "", "metadata": {}, "score": "88.31164"}
{"text": "If ' use_normalization ' is # set to ' true ' then kernel matrix will be normalized by the square roots # of the diagonal entries ./data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / kernel_polymatchstring_modular .", "label": "", "metadata": {}, "score": "88.31164"}
{"text": "# This example demsonstrates how to encode sparse ( most entries zero ) , # real - valued features in shogun using SparseRealFeatures . /examples / documented / python_modular / features_string_char_compressed_modular.py .# This example demonstrates how to use compressed strings with shogun .", "label": "", "metadata": {}, "score": "88.44468"}
{"text": "# This example demsonstrates how to encode sparse ( most entries zero ) , # real - valued features in shogun using SparseRealFeatures . /examples / documented / python_modular / features_string_char_compressed_modular.py .# This example demonstrates how to use compressed strings with shogun .", "label": "", "metadata": {}, "score": "88.44468"}
{"text": "Generally , a processor will receive instructions and data from a read - only memory or a random access memory or both .The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data .", "label": "", "metadata": {}, "score": "88.453094"}
{"text": "If ' use_sign ' is set to one each k - mere is counted # only once ./data / fm_train_dna ./data / fm_test_dna .add_preproc(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .", "label": "", "metadata": {}, "score": "88.51853"}
{"text": "If ' use_sign ' is set to one each k - mere is counted # only once ./data / fm_train_dna ./data / fm_test_dna .add_preproc(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .", "label": "", "metadata": {}, "score": "88.51853"}
{"text": "add_preproc(preproc ) feats_test ./examples / documented / python_modular / preproc_sortwordstring_modular.py .# In this example a kernel matrix is computed for a given string data set .The # CommWordString kernel is used to compute the spectrum kernel from strings that # have been mapped into unsigned 16bit integers .", "label": "", "metadata": {}, "score": "88.519"}
{"text": "Effective siRNAs gave rise to lower levels of remaining gene expression .The dataset used in this study consisted of 165 positive instances and 115 negative instances .Each siRNA instance was a sequence of 19 nucleotides ( from 5 ' to 3 ' end ) representing the antisense strand of the target gene mRNA transcript .", "label": "", "metadata": {}, "score": "88.556366"}
{"text": "/data / fm_test_real ./data / label_train_multiclass . /examples / documented / python_modular / classifier_gpbtsvm_modular.py .# In this example a two - class support vector machine classifier is trained on a # toy data set and the trained classifier is then used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "88.60474"}
{"text": "/data / fm_train_real ./data / label_train_twoclass . bin \" ) feats2.load(f ) # print \" diff binary \" , numpy.max(numpy.abs(feats2.get_feature_matrix ( ) .flatten()-fm_train_real . ascii \" ) feats2.load(f ) # print \" diff ascii \" , numpy.max(numpy.abs(feats2.get_feature_matrix ( ) .", "label": "", "metadata": {}, "score": "88.74712"}
{"text": "/data / fm_train_real ./data / label_train_twoclass . bin \" ) feats2.load(f ) # print \" diff binary \" , numpy.max(numpy.abs(feats2.get_feature_matrix ( ) .flatten()-fm_train_real . ascii \" ) feats2.load(f ) # print \" diff ascii \" , numpy.max(numpy.abs(feats2.get_feature_matrix ( ) .", "label": "", "metadata": {}, "score": "88.74712"}
{"text": "MIT Press , Cambridge , MA USA , 1999 ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass .Serialization . /examples / documented / python_modular / serialization_complex_example.py .h5\\n \" # else : # print \" ERROR reading / writing .", "label": "", "metadata": {}, "score": "88.75896"}
{"text": "MIT Press , Cambridge , MA USA , 1999 ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass .Serialization . /examples / documented / python_modular / serialization_complex_example.py .h5\\n \" # else : # print \" ERROR reading / writing .", "label": "", "metadata": {}, "score": "88.75896"}
{"text": "MIT Press , Cambridge , MA USA , 1999 ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass .Serialization . /examples / documented / python_modular / serialization_complex_example.py .h5\\n \" # else : # print \" ERROR reading / writing .", "label": "", "metadata": {}, "score": "88.75896"}
{"text": "# This is an example for the initialization of the PolyMatchString kernel on string data .# The PolyMatchString kernel sums over the matches of two stings of the same length and # takes the sum to the power of ' degree ' .", "label": "", "metadata": {}, "score": "88.78459"}
{"text": "# This is an example for the initialization of the PolyMatchString kernel on string data .# The PolyMatchString kernel sums over the matches of two stings of the same length and # takes the sum to the power of ' degree ' .", "label": "", "metadata": {}, "score": "88.78459"}
{"text": "MIT Press ./data / fm_train_real . /examples / documented / python_modular / preprocessor_logplusone_modular.py .# In this example a kernel matrix is computed for a given real - valued data set .# The kernel used is the Chi2 kernel which operates on real - valued vectors .", "label": "", "metadata": {}, "score": "88.79149"}
{"text": "# This example demonstrates , how to encode features composed of 64bit Integers in Shogun # using LongIntFeatures ./examples / documented / python_modular /features_simple_modular.py . # This example shows how to encode features that live in various vector spaces # using the appropriate shogun objects .", "label": "", "metadata": {}, "score": "88.823616"}
{"text": "/data / fm_train_real ./data / fm_test_real .get_labels ( ) ; . /examples / documented / octave_modular / classifier_libsvmmulticlass_modular .m . %In this example a multi - class support vector machine classifier is trained on a % toy data set and the trained classifier is used to predict labels of test % examples .", "label": "", "metadata": {}, "score": "88.8286"}
{"text": "If ' use_sign ' is set to one each k - mere is counted % only once ./data / fm_train_dna ./data / fm_test_dna .add_preproc(preproc ) ; feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) ; feats_test .", "label": "", "metadata": {}, "score": "88.9577"}
{"text": "Springer .# # Before processing the landmark approximation is disabled ./data / fm_train_real .Distance . /examples / documented / python_modular / distance_braycurtis_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .", "label": "", "metadata": {}, "score": "89.078835"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CBrayCurtisDistance.html . # # Obviously , using the Bray Curtis distance is not limited to this showcase # example .", "label": "", "metadata": {}, "score": "89.090164"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CBrayCurtisDistance.html . # # Obviously , using the Bray Curtis distance is not limited to this showcase # example .", "label": "", "metadata": {}, "score": "89.090164"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CBrayCurtisDistance.html . # # Obviously , using the Bray Curtis distance is not limited to this showcase # example .", "label": "", "metadata": {}, "score": "89.090164"}
{"text": "# # For more details on the used SVM solver see # V.Franc : Optimization Algorithms for Kernel Methods .Research report .# CTU - CMP-2005 - 22 .CTU FEL Prague . # ftp://cmp.felk.cvut.cz / pub / cmp / articles / franc / Franc - PhD. pdf .", "label": "", "metadata": {}, "score": "89.214386"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . , 1000,1],[traindat , testdat , label_traindat,1 ./examples / documented / python_modular / classifier_subgradientsvm_modular.py .# In this example a two - class linear support vector machine classifier is trained # on a toy data set and the trained classifier is used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "89.29803"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . , 1000,1],[traindat , testdat , label_traindat,1 ./examples / documented / python_modular / classifier_subgradientsvm_modular.py .# In this example a two - class linear support vector machine classifier is trained # on a toy data set and the trained classifier is used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "89.29803"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . , 1000,1],[traindat , testdat , label_traindat,1 ./examples / documented / python_modular / classifier_subgradientsvm_modular.py .# In this example a two - class linear support vector machine classifier is trained # on a toy data set and the trained classifier is used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "89.29803"}
{"text": "/data / fm_test_real .Distribution . /examples / documented / python_modular / distribution_histogram_modular.py .# In this example the Histogram algorithm object computes a histogram over all # 16bit unsigned integers in the features ./data / fm_train_dna . /examples / documented / python_modular / distribution_hmm_modular.py .", "label": "", "metadata": {}, "score": "89.4176"}
{"text": "/data / fm_test_real .Distribution . /examples / documented / python_modular / distribution_histogram_modular.py .# In this example the Histogram algorithm object computes a histogram over all # 16bit unsigned integers in the features ./data / fm_train_dna . /examples / documented / python_modular / distribution_hmm_modular.py .", "label": "", "metadata": {}, "score": "89.4176"}
{"text": "/data / fm_test_real .Distribution . /examples / documented / python_modular / distribution_histogram_modular.py .# In this example the Histogram algorithm object computes a histogram over all # 16bit unsigned integers in the features ./data / fm_train_dna . /examples / documented / python_modular / distribution_hmm_modular.py .", "label": "", "metadata": {}, "score": "89.4176"}
{"text": "/data / fm_test_dna . /examples / documented / python_modular / kernel_weighted_degree_string_modular.py .# This examples shows how to create a Weighted Degree String Kernel from data # and how to compute the kernel matrix from the resulting object ./data / fm_train_dna .", "label": "", "metadata": {}, "score": "89.46203"}
{"text": "/data / fm_test_dna . /examples / documented / python_modular / kernel_weighted_degree_string_modular.py .# This examples shows how to create a Weighted Degree String Kernel from data # and how to compute the kernel matrix from the resulting object ./data / fm_train_dna .", "label": "", "metadata": {}, "score": "89.46203"}
{"text": "/data / fm_test_dna . /examples / documented / python_modular / kernel_weighted_degree_string_modular.py .# This examples shows how to create a Weighted Degree String Kernel from data # and how to compute the kernel matrix from the resulting object ./data / fm_train_dna .", "label": "", "metadata": {}, "score": "89.46203"}
{"text": "View Article .Copyright .\u00a9 Wang et al .2010 .This article is published under license to BioMed Central Ltd.Sign up to receive free email alerts when patent applications with chosen keywords are published SIGN UP .Abstract : .", "label": "", "metadata": {}, "score": "89.477844"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular / kernel_locality_improved_string_modular.py .# The LocalityImprovedString kernel is inspired by the polynomial kernel .# Comparing neighboring characters it puts emphasize on local features ./data / fm_train_dna .", "label": "", "metadata": {}, "score": "89.53409"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular / kernel_locality_improved_string_modular.py .# The LocalityImprovedString kernel is inspired by the polynomial kernel .# Comparing neighboring characters it puts emphasize on local features ./data / fm_train_dna .", "label": "", "metadata": {}, "score": "89.53409"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular / kernel_locality_improved_string_modular.py .# The LocalityImprovedString kernel is inspired by the polynomial kernel .# Comparing neighboring characters it puts emphasize on local features ./data / fm_train_dna .", "label": "", "metadata": {}, "score": "89.53409"}
{"text": "Since not all siRNAs are equally effective , siRNA design is one of the critical steps in gene silencing studies .Earlier experimental data have suggested several sets of empirical rules for designing potent siRNAs .The thermodynamic properties of siRNA duplexes were shown to affect target mRNA degradation [ 4 , 5 ] .", "label": "", "metadata": {}, "score": "89.57434"}
{"text": "/data / fm_test_real .add_preproc(preproc ) feats_train .apply_preproc ( ) feats_test .add_preproc(preproc ) feats_test ./examples / documented / python_modular /preproc_sortulongstring_modular.py . # In this example a kernel matrix is computed for a given string data set .The # CommUlongString kernel is used to compute the spectrum kernel from strings that # have been mapped into unsigned 64bit integers .", "label": "", "metadata": {}, "score": "89.6308"}
{"text": "/data / fm_test_real ./data / label_train_multiclass . /examples / documented / python_modular / classifier_larank_modular.py .# In this example a multi - class support vector machine classifier is trained on a # toy data set and the trained classifier is then used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "89.6405"}
{"text": "/data / fm_test_real ./data / label_train_multiclass . /examples / documented / python_modular / classifier_larank_modular.py .# In this example a multi - class support vector machine classifier is trained on a # toy data set and the trained classifier is then used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "89.6405"}
{"text": "/data / fm_test_real ./data / label_train_multiclass . /examples / documented / python_modular / classifier_larank_modular.py .# In this example a multi - class support vector machine classifier is trained on a # toy data set and the trained classifier is then used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "89.6405"}
{"text": "/examples / documented / python_modular /evaluation_multiclassaccuracy_modular.py . # In this example a multiclass accuracy is being computed for toy data labels # and toy data labels multiplied by two ./data / label_train_multiclass ./data / label_train_multiclass . /examples / documented / python_modular /", "label": "", "metadata": {}, "score": "89.645065"}
{"text": "/examples / documented / python_modular /evaluation_multiclassaccuracy_modular.py . # In this example a multiclass accuracy is being computed for toy data labels # and toy data labels multiplied by two ./data / label_train_multiclass ./data / label_train_multiclass . /examples / documented / python_modular /", "label": "", "metadata": {}, "score": "89.645065"}
{"text": "# In this example a two - class support vector machine classifier is trained on a # toy data set and the trained classifier is then used to predict labels of test # examples ./data / fm_train_real ./data / fm_test_real .", "label": "", "metadata": {}, "score": "89.71129"}
{"text": "The distance used in this example is Euclidean distance .# After training one can fetch the result of clustering by obtaining the cluster # centers and their radiuses ./data / fm_train_real .Distance . /examples / documented / python_modular / distance_braycurtis_modular.py .", "label": "", "metadata": {}, "score": "89.78178"}
{"text": "/data / fm_test_real ./data / label_train_multiclass .append_feature_obj(subkfeats_train ) feats_test .append_feature_obj(subkfeats_train ) feats_test .append_feature_obj(subkfeats_train ) feats_test .Preproc . /examples / documented / python_modular / preproc_logplusone_modular.py .# In this example a kernel matrix is computed for a given real - valued data set .", "label": "", "metadata": {}, "score": "90.169876"}
{"text": "The example also shows how to retrieve the # support vectors from the train SVM model ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_libsvmmulticlass_modular.py .# In this example a multi - class support vector machine classifier is trained on a # toy data set and the trained classifier is used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "90.37428"}
{"text": "The example also shows how to retrieve the # support vectors from the train SVM model ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_libsvmmulticlass_modular.py .# In this example a multi - class support vector machine classifier is trained on a # toy data set and the trained classifier is used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "90.37428"}
{"text": "The example also shows how to retrieve the # support vectors from the train SVM model ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / classifier_libsvmmulticlass_modular.py .# In this example a multi - class support vector machine classifier is trained on a # toy data set and the trained classifier is used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "90.37428"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_comm_word_string_modular .R .# This is an example for the initialization of the CommWordString - kernel ( aka # Spectrum or n - gram kernel ; its name is derived from the unix command comm ) .", "label": "", "metadata": {}, "score": "90.600555"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_comm_word_string_modular .R .# This is an example for the initialization of the CommWordString - kernel ( aka # Spectrum or n - gram kernel ; its name is derived from the unix command comm ) .", "label": "", "metadata": {}, "score": "90.600555"}
{"text": "PubMed View Article .Khvorova A , Reynolds A , Jayasena SD : Functional siRNAs and miRNAs exhibit strand bias .Cell 2003 , 115 : 209 - 216 .PubMed View Article .Schwarz DS , Hutvagner G , Du T , Xu Z , Aronin N , Zamore PD : Asymmetry in the assembly of the RNAi enzyme complex .", "label": "", "metadata": {}, "score": "90.6754"}
{"text": "/data / fm_train_dna ./data / fm_test_dna .add_preproc(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .add_preproc(preproc ) feats_test ./examples / documented / python_modular / kernel_weighted_degree_position_string_modular.py .# The Weighted Degree Position String kernel ( Weighted Degree kernel with shifts ) .", "label": "", "metadata": {}, "score": "90.87071"}
{"text": "h5\",\"r\" , \" /data / labels \" )lab2.load(f ) # print lab2.get_labels ( ) # clean up import os for f in [ ' fm_train_sparsereal . bin','fm_train_sparsereal . ascii ' , ' fm_train_real . bin','fm_train_real .h5','fm_train_real . ascii ' , ' label_train_real . h5 ' , ' label_train_twoclass .", "label": "", "metadata": {}, "score": "90.93202"}
{"text": "h5\",\"r\" , \" /data / labels \" )lab2.load(f ) # print lab2.get_labels ( ) # clean up import os for f in [ ' fm_train_sparsereal . bin','fm_train_sparsereal . ascii ' , ' fm_train_real . bin','fm_train_real .h5','fm_train_real . ascii ' , ' label_train_real . h5 ' , ' label_train_twoclass .", "label": "", "metadata": {}, "score": "90.93202"}
{"text": "Affiliated with .Affiliated with .Affiliated with .Abstract .Background .Short interfering RNAs ( siRNAs ) can be used to knockdown gene expression in functional genomics .For a target gene of interest , many siRNA molecules may be designed , whereas their efficiency of expression inhibition often varies .", "label": "", "metadata": {}, "score": "91.27105"}
{"text": "/examples / documented / octave_modular / kernel_const_modular .m . %The constant kernel gives a trivial kernel matrix with all entries set to the same value % defined by the argument ' c ' ./data / fm_train_real ./data / fm_test_real . /examples / documented / octave_modular / kernel_custom_modular . m .", "label": "", "metadata": {}, "score": "91.27826"}
{"text": "get_labels ( ) ./data / label_train_twoclass ./data / fm_train_real ./data / fm_test_real .get_labels ( ) ; . /examples / documented / octave_modular / regression_libsvr_modular .m . %In this example a support vector regression algorithm is trained on a % real - valued toy data set .", "label": "", "metadata": {}, "score": "91.36517"}
{"text": "str \" , True ) # print \" lzo strings \" , f2.get_features ( ) # print # # gzip f.save_compressed(\"foo_gzip . str \" , True ) # print \" gzip strings \" , f2.get_features ( ) # print # bzip2 f.save_compressed(\"foo_bzip2 .", "label": "", "metadata": {}, "score": "91.44289"}
{"text": "/examples / documented / python_modular /preprocessor_sortulongstring_modular.py .# In this example a kernel matrix is computed for a given string data set .The # CommUlongString kernel is used to compute the spectrum kernel from strings that # have been mapped into unsigned 64bit integers .", "label": "", "metadata": {}, "score": "91.53467"}
{"text": "/examples / documented / python_modular /preprocessor_sortulongstring_modular.py .# In this example a kernel matrix is computed for a given string data set .The # CommUlongString kernel is used to compute the spectrum kernel from strings that # have been mapped into unsigned 64bit integers .", "label": "", "metadata": {}, "score": "91.53467"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CCanberraMetric.html . # # Obviously , using the Canberra distance is not limited to this showcase # example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "91.5992"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CCanberraMetric.html . # # Obviously , using the Canberra distance is not limited to this showcase # example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "91.5992"}
{"text": "# # The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .# # For more details see doc / classshogun_1_1CCanberraMetric.html . # # Obviously , using the Canberra distance is not limited to this showcase # example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "91.5992"}
{"text": "/data / fm_test_real . /examples / documented / octave_modular / kernel_simple_locality_improved_string_modular . m . %SimpleLocalityImprovedString kernel , is a ' simplified ' and better performing version of the Locality improved kernel ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / octave_modular / kernel_sparsegaussian_modular .", "label": "", "metadata": {}, "score": "91.7024"}
{"text": "PubMed View Article .Swets JA : Measuring the accuracy of diagnostic systems .Science 1988 , 240 : 1285 - 1293 .PubMed View Article .Bradley AP : The use of the area under the ROC curve in the evaluation of machine learning algorithms .", "label": "", "metadata": {}, "score": "92.05542"}
{"text": "/data / fm_train_byte ./data / fm_test_byte . /examples / documented / python_modular / kernel_linear_modular.py .# This is an example for the initialization of a linear kernel on real valued # data using scaling factor 1.2 ./data / fm_train_real .", "label": "", "metadata": {}, "score": "92.236786"}
{"text": "/data / fm_train_byte ./data / fm_test_byte . /examples / documented / python_modular / kernel_linear_modular.py .# This is an example for the initialization of a linear kernel on real valued # data using scaling factor 1.2 ./data / fm_train_real .", "label": "", "metadata": {}, "score": "92.236786"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . , 1000,1],[traindat , testdat , label_traindat,1 ./examples / documented / python_modular / classifier_custom_kernel_modular.py .# This example shows how to use a custom defined kernel function for training a # two class Support Vector Machine ( SVM ) classifier on a randomly generated # examples . /examples / documented / python_modular / classifier_domainadaptationsvm_modular.py .", "label": "", "metadata": {}, "score": "92.2424"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . , 1000,1],[traindat , testdat , label_traindat,1 ./examples / documented / python_modular / classifier_custom_kernel_modular.py .# This example shows how to use a custom defined kernel function for training a # two class Support Vector Machine ( SVM ) classifier on a randomly generated # examples . /examples / documented / python_modular / classifier_domainadaptationsvm_modular.py .", "label": "", "metadata": {}, "score": "92.2424"}
{"text": "load_serializable(fstream ) check_status(status ) new_svm .load_serializable(fstream ) check_status(status ) new_svm .load_serializable(fstream ) check_status(status ) new_svm . /examples / documented / python_modular / serialization_svmlight_modular.py .# This example shows how to use boost serialization ( only available if the compile flag was enabled ) # to serialize / deserialize an SVMLight object .", "label": "", "metadata": {}, "score": "92.32564"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_linear_word_modular .R .# This is an example for the initialization of a linear kernel on word ( 2byte ) # data ./data / fm_train_word .", "label": "", "metadata": {}, "score": "92.40056"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_linear_word_modular .R .# This is an example for the initialization of a linear kernel on word ( 2byte ) # data ./data / fm_train_word .", "label": "", "metadata": {}, "score": "92.40056"}
{"text": "/examples / documented / python_modular / kernel_comm_word_string_modular.py .# This is an example for the initialization of the CommWordString - kernel ( aka # Spectrum or n - gram kernel ; its name is derived from the unix command comm ) .", "label": "", "metadata": {}, "score": "92.41611"}
{"text": "/examples / documented / python_modular / kernel_comm_word_string_modular.py .# This is an example for the initialization of the CommWordString - kernel ( aka # Spectrum or n - gram kernel ; its name is derived from the unix command comm ) .", "label": "", "metadata": {}, "score": "92.41611"}
{"text": "/examples / documented / python_modular / kernel_comm_word_string_modular.py .# This is an example for the initialization of the CommWordString - kernel ( aka # Spectrum or n - gram kernel ; its name is derived from the unix command comm ) .", "label": "", "metadata": {}, "score": "92.41611"}
{"text": "dump(kernel , file('kernel_obj .Mkl . /examples / documented / python_modular / mkl_binclass_modular.py . # In this example we show how to perform Multiple Kernel Learning ( MKL ) # with the modular interface .First , we create a number of base kernels .", "label": "", "metadata": {}, "score": "92.524124"}
{"text": "Machine Learning : ECML 2005 , 182 - 193 .( Eds . )/data / fm_train_real ./data / fm_test_real ./data / label_train_multiclass . /examples / documented / r_modular / classifier_multiclasslibsvm_modular .R ./data / fm_train_real ./data / fm_test_real .", "label": "", "metadata": {}, "score": "92.57109"}
{"text": "Machine Learning : ECML 2005 , 182 - 193 .( Eds . )/data / fm_train_real ./data / fm_test_real ./data / label_train_multiclass . /examples / documented / r_modular / classifier_multiclasslibsvm_modular .R ./data / fm_train_real ./data / fm_test_real .", "label": "", "metadata": {}, "score": "92.57109"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / classifier_mpdsvm_modular .R .# In this example a two - class support vector machine classifier is trained on a # toy data set and the trained classifier is used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "92.99827"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / classifier_mpdsvm_modular .R .# In this example a two - class support vector machine classifier is trained on a # toy data set and the trained classifier is used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "92.99827"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / r_modular / classifier_libsvmoneclass_modular .R .# In this example a one - class support vector machine classifier is trained on a # toy data set .", "label": "", "metadata": {}, "score": "93.004585"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / r_modular / classifier_libsvmoneclass_modular .R .# In this example a one - class support vector machine classifier is trained on a # toy data set .", "label": "", "metadata": {}, "score": "93.004585"}
{"text": "Streaming . /examples / documented / python_modular / streaming_vw_createcache_modular.py ./data / fm_train_sparsereal . dat ' ] ] def streaming_vw_createcache_modular(fname ) : # First creates a binary cache from an ascii data file .dat.cache input_file . set_write_to_cache(True ) # Tell VW that the file is in SVMLight format # Supported types are T_DENSE , T_SVMLIGHT and T_VW input_file . /examples / documented / python_modular / streaming_vw_modular.py .", "label": "", "metadata": {}, "score": "93.0446"}
{"text": "Streaming . /examples / documented / python_modular / streaming_vw_createcache_modular.py ./data / fm_train_sparsereal . dat ' ] ] def streaming_vw_createcache_modular(fname ) : # First creates a binary cache from an ascii data file .dat.cache input_file . set_write_to_cache(True ) # Tell VW that the file is in SVMLight format # Supported types are T_DENSE , T_SVMLIGHT and T_VW input_file . /examples / documented / python_modular / streaming_vw_modular.py .", "label": "", "metadata": {}, "score": "93.0446"}
{"text": "For example , a document with the phrase \" generally good \" at its beginning is very likely to be a positive sentiment .When the same phrase appears in the middle of another text , the document is less likely to be a positive comment .", "label": "", "metadata": {}, "score": "93.094246"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / r_modular / classifier_libsvm_modular .R .# In this example a two - class support vector machine classifier is trained on a # toy data set and the trained classifier is used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "93.22508"}
{"text": "/data / fm_test_real ./data / label_train_twoclass . /examples / documented / r_modular / classifier_libsvm_modular .R .# In this example a two - class support vector machine classifier is trained on a # toy data set and the trained classifier is used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "93.22508"}
{"text": "/data / label_train_dna ./examples / documented / python_modular / kernel_sigmoid_modular.py .# The standard Sigmoid kernel computed on dense real valued features ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_simple_locality_improved_string_modular.py .# SimpleLocalityImprovedString kernel , is a ' simplified ' and better performing version of the Locality improved kernel .", "label": "", "metadata": {}, "score": "93.7458"}
{"text": "For this kernel the linadd speedups # are quite efficiently implemented using direct maps ./data / fm_train_dna ./data / fm_test_dna .add_preprocessor(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .add_preprocessor(preproc ) feats_test .", "label": "", "metadata": {}, "score": "93.925095"}
{"text": "For this kernel the linadd speedups # are quite efficiently implemented using direct maps ./data / fm_train_dna ./data / fm_test_dna .add_preprocessor(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .add_preprocessor(preproc ) feats_test .", "label": "", "metadata": {}, "score": "93.925095"}
{"text": "% % The resulting distance matrix can be reaccessed by ' get_distance_matrix ' .% % For more details see doc / classshogun_1_1CCanberraMetric.html . % % Obviously , using the Canberra distance is not limited to this showcase % example ./data / fm_train_real .", "label": "", "metadata": {}, "score": "94.21402"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / label_train_multiclass . /examples / documented / python_modular / classifier_gmnpsvm_modular.py .# In this example a multi - class support vector machine is trained on a toy data # set and the trained classifier is then used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "94.344284"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / label_train_multiclass . /examples / documented / python_modular / classifier_gmnpsvm_modular.py .# In this example a multi - class support vector machine is trained on a toy data # set and the trained classifier is then used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "94.344284"}
{"text": "/data / snps . /examples / documented / python_modular / features_sparse_modular.py .# This example demsonstrates how to encode sparse ( most entries zero ) , # real - valued features in shogun using SparseRealFeatures . /examples / documented / python_modular / features_string_char_compressed_modular.py .", "label": "", "metadata": {}, "score": "94.48982"}
{"text": "/examples / documented / python_modular / kernel_const_modular.py .# The constant kernel gives a trivial kernel matrix with all entries set to the same value # defined by the argument ' c ' ./examples / documented / python_modular / kernel_custom_modular.py .", "label": "", "metadata": {}, "score": "94.594055"}
{"text": "/examples / documented / python_modular / kernel_const_modular.py .# The constant kernel gives a trivial kernel matrix with all entries set to the same value # defined by the argument ' c ' ./examples / documented / python_modular / kernel_custom_modular.py .", "label": "", "metadata": {}, "score": "94.594055"}
{"text": "/examples / documented / python_modular / kernel_const_modular.py .# The constant kernel gives a trivial kernel matrix with all entries set to the same value # defined by the argument ' c ' ./examples / documented / python_modular / kernel_custom_modular.py .", "label": "", "metadata": {}, "score": "94.594055"}
{"text": "/data / fm_test_dna . /examples / documented / python_modular / kernel_linear_word_modular.py .# This is an example for the initialization of a linear kernel on word ( 2byte ) # data ./data / fm_train_word ./data / fm_test_word . /examples / documented / python_modular / kernel_local_alignment_string_modular.py .", "label": "", "metadata": {}, "score": "94.7171"}
{"text": "/data / fm_test_dna . /examples / documented / python_modular / kernel_linear_word_modular.py .# This is an example for the initialization of a linear kernel on word ( 2byte ) # data ./data / fm_train_word ./data / fm_test_word . /examples / documented / python_modular / kernel_local_alignment_string_modular.py .", "label": "", "metadata": {}, "score": "94.7171"}
{"text": "/data / fm_test_dna . /examples / documented / python_modular / kernel_linear_word_modular.py .# This is an example for the initialization of a linear kernel on word ( 2byte ) # data ./data / fm_train_word ./data / fm_test_word . /examples / documented / python_modular / kernel_local_alignment_string_modular.py .", "label": "", "metadata": {}, "score": "94.7171"}
{"text": "m . %This example shows how to compute the Canberra Word Distance ./data / fm_train_dna ./data / fm_test_dna .add_preproc(preproc ) ; feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) ; feats_test .add_preproc(preproc ) ; feats_test .", "label": "", "metadata": {}, "score": "94.766205"}
{"text": "load_serializable(fstream ) check_status(status ) new_svm .load_serializable(fstream ) check_status(status ) new_svm .load_serializable(fstream ) check_status(status ) new_svm . /examples / documented / python_modular / serialization_matrix_modular.py ./examples / documented / python_modular / serialization_svmlight_modular.py .# This example shows how to use boost serialization ( only available if the compile flag was enabled ) # to serialize / deserialize an SVMLight object .", "label": "", "metadata": {}, "score": "94.782524"}
{"text": "load_serializable(fstream ) check_status(status ) new_svm .load_serializable(fstream ) check_status(status ) new_svm .load_serializable(fstream ) check_status(status ) new_svm . /examples / documented / python_modular / serialization_matrix_modular.py ./examples / documented / python_modular / serialization_svmlight_modular.py .# This example shows how to use boost serialization ( only available if the compile flag was enabled ) # to serialize / deserialize an SVMLight object .", "label": "", "metadata": {}, "score": "94.782524"}
{"text": "For this kernel the linadd speedups # are quite efficiently implemented using direct maps ./data / fm_train_dna ./data / fm_test_dna .add_preproc(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .add_preproc(preproc ) feats_test .", "label": "", "metadata": {}, "score": "95.18498"}
{"text": "The oligo string kernel is # implemented for the DNA - alphabet ' ACGT ' ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular / kernel_poly_match_string_modular.py .# In this example the poly match string kernel is being computed for toy data .", "label": "", "metadata": {}, "score": "95.41625"}
{"text": "The oligo string kernel is # implemented for the DNA - alphabet ' ACGT ' ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular / kernel_poly_match_string_modular.py .# In this example the poly match string kernel is being computed for toy data .", "label": "", "metadata": {}, "score": "95.41625"}
{"text": "str ' , ' foo_lzma . str ' , ' foo_lzo .str ' , ' foo_lzo . /examples / documented / python_modular / features_string_char_modular.py .# This example demonstrates how to encode ASCII - strings ( 255 symbols ) in shogun . /examples / documented / python_modular / features_string_file_char_modular.py .", "label": "", "metadata": {}, "score": "95.42056"}
{"text": "str ' , ' foo_lzma . str ' , ' foo_lzo .str ' , ' foo_lzo . /examples / documented / python_modular / features_string_char_modular.py .# This example demonstrates how to encode ASCII - strings ( 255 symbols ) in shogun . /examples / documented / python_modular / features_string_file_char_modular.py .", "label": "", "metadata": {}, "score": "95.42056"}
{"text": "str ' , ' foo_lzma . str ' , ' foo_lzo .str ' , ' foo_lzo . /examples / documented / python_modular / features_string_char_modular.py .# This example demonstrates how to encode ASCII - strings ( 255 symbols ) in shogun . /examples / documented / python_modular / features_string_file_char_modular.py .", "label": "", "metadata": {}, "score": "95.42056"}
{"text": "/data / fm_test_dna ./data / label_train_dna .set_observations(wordfeats_test ) neg_clone . set_a(feats_train . /examples / documented / python_modular / kernel_fixed_degree_string_modular.py ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular /kernel_gaussian_modular.py .# The well known Gaussian kernel ( swiss army knife for SVMs ) on dense real valued features .", "label": "", "metadata": {}, "score": "95.55432"}
{"text": "/data / fm_test_dna ./data / label_train_dna .set_observations(wordfeats_test ) neg_clone . set_a(feats_train . /examples / documented / python_modular / kernel_fixed_degree_string_modular.py ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular /kernel_gaussian_modular.py .# The well known Gaussian kernel ( swiss army knife for SVMs ) on dense real valued features .", "label": "", "metadata": {}, "score": "95.55432"}
{"text": "/data / fm_test_dna ./data / label_train_dna .set_observations(wordfeats_test ) neg_clone . set_a(feats_train . /examples / documented / python_modular / kernel_fixed_degree_string_modular.py ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular /kernel_gaussian_modular.py .# The well known Gaussian kernel ( swiss army knife for SVMs ) on dense real valued features .", "label": "", "metadata": {}, "score": "95.55432"}
{"text": "/data / fm_test_byte . /examples / documented / python_modular / kernel_linear_modular.py .# This is an example for the initialization of a linear kernel on real valued # data using scaling factor 1.2 ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_linear_string_modular.py .", "label": "", "metadata": {}, "score": "96.0094"}
{"text": "# Computes the standard linear kernel on sparse real valued features ./data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / kernel_sparsepoly_modular .R .# Computes the standard polynomial kernel on sparse real valued features .", "label": "", "metadata": {}, "score": "96.03119"}
{"text": "# Computes the standard linear kernel on sparse real valued features ./data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / kernel_sparsepoly_modular .R .# Computes the standard polynomial kernel on sparse real valued features .", "label": "", "metadata": {}, "score": "96.03119"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / kernel_combined_custom_poly_modular.py ./data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / python_modular / kernel_combined_modular.py .# This is an example for the initialization of a combined kernel , which is a weighted sum of # in this case three kernels on real valued data .", "label": "", "metadata": {}, "score": "96.20507"}
{"text": "/data / fm_test_real .ascii\",\"w \" ) kernel.save(f ) del f # clean up import os os.unlink(\"gaussian_test . ascii \" ) os.unlink(\"gaussian_train . /examples / documented / python_modular / kernel_linear_byte_modular.py .# This is an example for the initialization of a linear kernel on raw byte # data .", "label": "", "metadata": {}, "score": "96.416916"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / label_train_multiclass . /examples / documented / python_modular / classifier_gpbtsvm_modular.py .# In this example a two - class support vector machine classifier is trained on a # toy data set and the trained classifier is then used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "96.43257"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / label_train_multiclass . /examples / documented / python_modular / classifier_gpbtsvm_modular.py .# In this example a two - class support vector machine classifier is trained on a # toy data set and the trained classifier is then used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "96.43257"}
{"text": "# In this example the log kernel ( logarithm of the distance powered by degree plus one ) is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_match_word_string_modular.py ./data / fm_train_dna .", "label": "", "metadata": {}, "score": "96.45143"}
{"text": "# In this example the log kernel ( logarithm of the distance powered by degree plus one ) is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_match_word_string_modular.py ./data / fm_train_dna .", "label": "", "metadata": {}, "score": "96.45143"}
{"text": "/examples / documented / python_modular / features_simple_real_modular.py .# This examples demonstrates how to encode real - valued features in Shogun , # using RealFeatures . /examples / documented / python_modular / features_snp_modular.py .# Creates features similar to the feature space of the SNP kernel .", "label": "", "metadata": {}, "score": "96.4798"}
{"text": "/data / fm_test_dna . /examples / documented / r_modular / kernel_sparsegaussian_modular .R .# The well known Gaussian kernel ( swiss army knife for SVMs ) on sparse real valued features ./data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / kernel_sparselinear_modular .", "label": "", "metadata": {}, "score": "96.596466"}
{"text": "/data / fm_test_dna . /examples / documented / r_modular / kernel_sparsegaussian_modular .R .# The well known Gaussian kernel ( swiss army knife for SVMs ) on sparse real valued features ./data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / kernel_sparselinear_modular .", "label": "", "metadata": {}, "score": "96.596466"}
{"text": "Research report . % CTU - CMP-2005 - 22 .CTU FEL Prague . % ftp://cmp.felk.cvut.cz / pub / cmp / articles / franc / Franc - PhD. pdf ./data / label_train_multiclass ./data / fm_train_real ./data / fm_test_real .", "label": "", "metadata": {}, "score": "96.6588"}
{"text": "In several other studies [ 10 - 12 ] , regression models instead of classifiers were constructed for predicting siRNA efficacy .Correlation of SVM output with siRNA efficacy .The true positive ( TP ) and true negative ( TN ) predictions are shown in red circles , whereas the false positive ( FP ) and false negative ( FN ) predictions are shown in green triangles .", "label": "", "metadata": {}, "score": "96.78688"}
{"text": "The selected sequence features were used to construct SVM models for predicting siRNA potency .Our results suggest that siRNA potency is significantly affected by its nucleotide dimer and trimer compositions .Some nucleotide motifs such as UCC appear to be positively correlated with siRNA efficacy , whereas other motifs such as GAG may have a negative effect on gene expression inhibition .", "label": "", "metadata": {}, "score": "97.29407"}
{"text": "/data / fm_test_dna . /examples / documented / r_modular / kernel_wordmatch_modular .R ./data / fm_train_word ./data / fm_test_word .Mkl . /examples / documented / r_modular / mkl_multiclass_modular .R .# In this example we show how to perform Multiple Kernel Learning ( MKL ) # with the modular interface for multi - class classification .", "label": "", "metadata": {}, "score": "97.34895"}
{"text": "/data / fm_test_dna . /examples / documented / r_modular / kernel_wordmatch_modular .R ./data / fm_train_word ./data / fm_test_word .Mkl . /examples / documented / r_modular / mkl_multiclass_modular .R .# In this example we show how to perform Multiple Kernel Learning ( MKL ) # with the modular interface for multi - class classification .", "label": "", "metadata": {}, "score": "97.34895"}
{"text": "# This example shows how to compute the Canberra Word Distance ./data / fm_train_dna ./data / fm_test_dna .add_preprocessor(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .add_preprocessor(preproc ) feats_test ./examples / documented / python_modular / distance_chebyshew_modular.py . # An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .", "label": "", "metadata": {}, "score": "97.708466"}
{"text": "# This example shows how to compute the Canberra Word Distance ./data / fm_train_dna ./data / fm_test_dna .add_preprocessor(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .add_preprocessor(preproc ) feats_test ./examples / documented / python_modular / distance_chebyshew_modular.py . # An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .", "label": "", "metadata": {}, "score": "97.708466"}
{"text": "str \" , False ) # f2 .io.set_loglevel(MSG_DEBUG ) f2.add_preprocessor(DecompressCharString(LZO ) ) f2.enable_on_the_fly_preprocessing ( ) # print \" lzo strings \" , f2.get_features ( ) # print # clean up import os for f in [ ' foo_uncompressed .str ' , ' foo_snappy . str ' , ' foo_lzo .", "label": "", "metadata": {}, "score": "97.966034"}
{"text": "str \" , False ) # f2 .io.set_loglevel(MSG_DEBUG ) f2.add_preprocessor(DecompressCharString(LZO ) ) f2.enable_on_the_fly_preprocessing ( ) # print \" lzo strings \" , f2.get_features ( ) # print # clean up import os for f in [ ' foo_uncompressed .str ' , ' foo_snappy . str ' , ' foo_lzo .", "label": "", "metadata": {}, "score": "97.966034"}
{"text": "The oligo string kernel is # implemented for the DNA - alphabet ' ACGT ' ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular / kernel_poly_match_string_modular.py ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular / kernel_poly_match_word_string_modular.py .", "label": "", "metadata": {}, "score": "98.15057"}
{"text": "/data / fm_test_dna ./data / label_train_dna .set_observations(wordfeats_test ) neg_clone . /examples / documented / python_modular / kernel_tstudent_modular.py .# In this example the t - Student 's kernel is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_wave_modular.py .", "label": "", "metadata": {}, "score": "98.15372"}
{"text": "/data / fm_test_dna ./data / label_train_dna .set_observations(wordfeats_test ) neg_clone . /examples / documented / python_modular / kernel_tstudent_modular.py .# In this example the t - Student 's kernel is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_wave_modular.py .", "label": "", "metadata": {}, "score": "98.15372"}
{"text": "Finally , it can be used for prediction just as any other % SVM object ./data / label_train_dna ./data / fm_train_dna ./data / label_train_dna ./data / fm_train_dna ./data / fm_test_dna . set_features(fm_train_dna ) ; feats_test .set_features(fm_train_dna2 ) ; feats_test2 .", "label": "", "metadata": {}, "score": "98.51601"}
{"text": "% % For more details see doc / classshogun_1_1CChebyshewMetric.html .% % Obviously , using the Chebyshew distance is not limited to this showcase % example ./data / fm_train_real ./data / fm_test_real . /examples / documented / octave_modular / distance_chisquare_modular .", "label": "", "metadata": {}, "score": "98.716415"}
{"text": "# In this example the ANOVA kernel is being computed for toy data ./data / fm_train_real ./data / fm_test_real .get_num_vectors ( ) ) : for j in range(0,feats_train . /examples / documented / python_modular / kernel_auc_modular.py .# This example demonstrates the use of the AUC Kernel .", "label": "", "metadata": {}, "score": "98.7299"}
{"text": "# In this example the ANOVA kernel is being computed for toy data ./data / fm_train_real ./data / fm_test_real .get_num_vectors ( ) ) : for j in range(0,feats_train . /examples / documented / python_modular / kernel_auc_modular.py .# This example demonstrates the use of the AUC Kernel .", "label": "", "metadata": {}, "score": "98.7299"}
{"text": "Research report .# CTU - CMP-2005 - 22 .CTU FEL Prague . # ftp://cmp.felk.cvut.cz / pub / cmp / articles / franc / Franc - PhD. pdf ./data / fm_train_real ./data / fm_test_real ./data / label_train_multiclass . /examples / documented / r_modular / classifier_gpbtsvm_modular .", "label": "", "metadata": {}, "score": "99.22215"}
{"text": "svm_costratio transduction_posratio biased_hyperplane sharedslack svm_maxqpsize svm_newvarsinqp kernel_cache_size epsilon_crit epsilon_shrink svm_iter_to_shrink maxiter remove_inconsistent skip_final_opt_check compute_loo rho xa_depth predfile alphafile Kernel parameters : kernel_type poly_degree rbf_gamma coef_lin coef_const custom .For an explanation of these parameters , you may be interested in looking at the svm_common.h file in the SVMLight distribution .", "label": "", "metadata": {}, "score": "99.35997"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / label_train_multiclass . /examples / documented / r_modular / classifier_gpbtsvm_modular .R .# In this example a two - class support vector machine classifier is trained on a # toy data set and the trained classifier is then used to predict labels of test # examples .", "label": "", "metadata": {}, "score": "99.47245"}
{"text": "Kernel . /examples / documented / r_modular / kernel_auc_modular .R .# This example demonstrates the use of the AUC Kernel ./data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / kernel_chi2_modular .R .# This is an example for the initialization of the chi2-kernel on real data , where # each column of the matrices corresponds to one training / test example .", "label": "", "metadata": {}, "score": "99.751816"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / label_train_twoclass . /examples / documented / octave_modular / kernel_fixed_degree_string_modular .m ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / octave_modular / kernel_gaussian_modular .m . %The well known Gaussian kernel ( swiss army knife for SVMs ) on dense real valued features .", "label": "", "metadata": {}, "score": "99.88886"}
{"text": "str \" , False ) # f2 .io.set_loglevel(MSG_DEBUG ) f2.add_preproc(DecompressCharString(LZO ) ) f2.enable_on_the_fly_preprocessing ( ) # print \" lzo strings \" , f2.get_features ( ) # print # clean up import os for f in [ ' foo_uncompressed .str ' , ' foo_lzo .", "label": "", "metadata": {}, "score": "100.011314"}
{"text": "# Science , 14 , 585 - 591 .MIT Press ./data / fm_train_real . /examples / documented / python_modular / converter_linearlocaltangentspacealignment_modular.py ./data / fm_train_real . /examples / documented / python_modular / converter_localitypreservingprojections_modular.py ./data / fm_train_real . /examples / documented / python_modular / converter_locallylinearembedding_modular.py .", "label": "", "metadata": {}, "score": "100.5275"}
{"text": "/data / fm_test_real ./data / label_train_multiclass .append_feature_obj(subkfeats_train ) feats_test .append_feature_obj(subkfeats_train ) feats_test .append_feature_obj(subkfeats_train ) feats_test .Modelselection . /examples / documented / python_modular / modelselection_grid_search_linear_modular.py .# In this example simple crossvalidation model parameters selection is shown .", "label": "", "metadata": {}, "score": "100.532776"}
{"text": "/data / fm_test_real ./data / label_train_multiclass .append_feature_obj(subkfeats_train ) feats_test .append_feature_obj(subkfeats_train ) feats_test .append_feature_obj(subkfeats_train ) feats_test .Modelselection . /examples / documented / python_modular / modelselection_grid_search_linear_modular.py .# In this example simple crossvalidation model parameters selection is shown .", "label": "", "metadata": {}, "score": "100.532776"}
{"text": "# This example shows how to compute the Canberra Word Distance ./data / fm_train_dna ./data / fm_test_dna .add_preproc(preproc ) feats_train .obtain_from_char(charfeat , order-1 , order , gap , reverse ) feats_test .add_preproc(preproc ) feats_test ./examples / documented / python_modular / distance_chebyshew_modular.py . # An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .", "label": "", "metadata": {}, "score": "100.70002"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / distance_canberra_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .# This approach is just one example of what can be done using the distance # functions provided by shogun .", "label": "", "metadata": {}, "score": "100.74335"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / distance_canberra_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .# This approach is just one example of what can be done using the distance # functions provided by shogun .", "label": "", "metadata": {}, "score": "100.74335"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / distance_canberra_modular.py .# An approach as applied below , which shows the processing of input data # from a file becomes a crucial factor for writing your own sample applications .# This approach is just one example of what can be done using the distance # functions provided by shogun .", "label": "", "metadata": {}, "score": "100.74335"}
{"text": "/data / fm_test_dna . /examples / documented / python_modular / kernel_sparse_gaussian_modular.py .# This example demonstrates how to use the Gaussian Kernel with sparse features ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_sparse_linear_modular.py .", "label": "", "metadata": {}, "score": "101.53486"}
{"text": "The well known Gaussian kernel ( swiss army knife for SVMs ) on sparse real valued features ./data / fm_train_real ./data / fm_test_real . dat ' ) ; % sparse_gaussian - b0rked ? /examples / documented / octave_modular / kernel_sparselinear_modular .", "label": "", "metadata": {}, "score": "101.65961"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / kernel_fixed_degree_string_modular .R ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_gaussian_modular .R .# The well known Gaussian kernel ( swiss army knife for SVMs ) on dense real valued features .", "label": "", "metadata": {}, "score": "101.737755"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / kernel_fixed_degree_string_modular .R ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_gaussian_modular .R .# The well known Gaussian kernel ( swiss army knife for SVMs ) on dense real valued features .", "label": "", "metadata": {}, "score": "101.737755"}
{"text": "build_values(1 , 1 , R_EXP ) param_power_kernel .build_values(1 , 12 , R_LINEAR ) param_power_kernel_metric1 .build_values(1 , 2 , R_EXP ) param_gaussian_kernel . build_values(1 , 2 , R_EXP )param_ds_kernel .build_values(1 , 2 , R_EXP )param_ds_kernel .Preprocessor . /examples / documented / python_modular /", "label": "", "metadata": {}, "score": "101.856415"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / octave_modular / kernel_sparsepoly_modular .m . % Computes the standard polynomial kernel on sparse real valued features ./data / fm_train_real ./data / fm_test_real . /examples / documented / octave_modular / kernel_top_modular .", "label": "", "metadata": {}, "score": "101.85789"}
{"text": "/data / fm_test_real . /examples / documented / octave_modular / kernel_histogramword_modular .m . %The HistogramWordString computes the TOP kernel on inhomogeneous Markov Chains ./data / label_train_dna ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / octave_modular / kernel_linear_byte_modular .", "label": "", "metadata": {}, "score": "102.314606"}
{"text": "/data / fm_train_dna .Kernel . /examples / documented / octave_modular / kernel_auc_modular .m . %This example demonstrates the use of the AUC Kernel ./data / fm_train_real ./data / label_train_twoclass . /examples / documented / octave_modular / kernel_chi2_modular . m . %", "label": "", "metadata": {}, "score": "102.62935"}
{"text": "# This example demonstrates how to read and write data in the SVMLight Format # from Shogun . /examples / documented / python_modular / features_simple_byte_modular.py .# This example demonstrates how to encode small positive natural numbers # ( up to 255 ) in shogun using ByteFeatures . /examples / documented / python_modular / features_simple_longint_modular.py .", "label": "", "metadata": {}, "score": "103.56529"}
{"text": "# This example demonstrates how to read and write data in the SVMLight Format # from Shogun . /examples / documented / python_modular / features_simple_byte_modular.py .# This example demonstrates how to encode small positive natural numbers # ( up to 255 ) in shogun using ByteFeatures . /examples / documented / python_modular / features_simple_longint_modular.py .", "label": "", "metadata": {}, "score": "103.56529"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / distance_manhatten_modular.py . # This example shows how to compute the Manhatten Distance ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / distance_manhattenword_modular.py .# This example shows how to compute the Manahattan Distance for string features .", "label": "", "metadata": {}, "score": "103.75667"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / distance_manhatten_modular.py . # This example shows how to compute the Manhatten Distance ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / distance_manhattenword_modular.py .# This example shows how to compute the Manahattan Distance for string features .", "label": "", "metadata": {}, "score": "103.75667"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / distance_manhatten_modular.py . # This example shows how to compute the Manhatten Distance ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / distance_manhattenword_modular.py .# This example shows how to compute the Manahattan Distance for string features .", "label": "", "metadata": {}, "score": "103.75667"}
{"text": "build_values(1 , 1 , R_EXP ) param_power_kernel .build_values(1 , 12 , R_LINEAR ) param_power_kernel_metric1 .build_values(1 , 2 , R_EXP ) param_gaussian_kernel . build_values(1 , 2 , R_EXP )param_ds_kernel .build_values(1 , 2 , R_EXP )param_ds_kernel .Preprocessor . /examples / documented / python_modular / preprocessor_hessianlocallylinearembedding_modular.py . # In this example toy data is being preprocessed using the Hessian Locally Linear Embedding algorithm # as described in # # Donoho , D. , & Grimes , C. ( 2003 ) .", "label": "", "metadata": {}, "score": "103.835526"}
{"text": "/data / fm_test_dna ./data / label_train_dna ./examples / documented / python_modular / kernel_sigmoid_modular.py .# The standard Sigmoid kernel computed on dense real valued features ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_simple_locality_improved_string_modular.py .", "label": "", "metadata": {}, "score": "104.156586"}
{"text": "/data / fm_test_dna ./data / label_train_dna ./examples / documented / python_modular / kernel_sigmoid_modular.py .# The standard Sigmoid kernel computed on dense real valued features ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_simple_locality_improved_string_modular.py .", "label": "", "metadata": {}, "score": "104.156586"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_weighted_degree_position_string_modular .R .# The Weighted Degree Position String kernel ( Weighted Degree kernel with shifts ) ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_weighteddegreestring_modular .", "label": "", "metadata": {}, "score": "104.39639"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_weighted_degree_position_string_modular .R .# The Weighted Degree Position String kernel ( Weighted Degree kernel with shifts ) ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / r_modular / kernel_weighteddegreestring_modular .", "label": "", "metadata": {}, "score": "104.39639"}
{"text": "/data / label_train_multiclass ./data / fm_train_real ./data / fm_test_real .append_feature_obj(subkfeats_train ) ; feats_test .append_feature_obj(subkfeats_train ) ; feats_test .append_feature_obj(subkfeats_train ) ; feats_test .get_labels ( ) ; result .Preproc . /examples / documented / octave_modular / preproc_logplusone_modular .", "label": "", "metadata": {}, "score": "104.39836"}
{"text": "Kernel . /examples / documented / r_modular / kernel_auc_modular .R .# This example demonstrates the use of the AUC Kernel , which # can be used to maximize AUC instead of margin in SVMs ./data / fm_train_real ./data / fm_test_real . /examples / documented / r_modular / kernel_chi2_modular .", "label": "", "metadata": {}, "score": "104.67498"}
{"text": "dump(kernel , file('kernel_obj .Library . /examples / documented / python_modular / library_fisher2x3_modular.py .Mkl . /examples / documented / python_modular / mkl_binclass_modular.py . # In this example we show how to perform Multiple Kernel Learning ( MKL ) # with the modular interface .", "label": "", "metadata": {}, "score": "105.786705"}
{"text": "h5','fm_train_real . ascii ' , ' label_train_real . h5 ' , ' label_train_twoclass .ascii','label_train_twoclass . /examples / documented / python_modular / features_read_svmlight_format_modular.py .# This example demonstrates how to read and write data in the SVMLight Format # from Shogun . /examples / documented / python_modular / features_simple_byte_modular.py .", "label": "", "metadata": {}, "score": "105.9305"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_sparse_poly_modular.py .# This example shows how to use the polynomial kernel with sparse features ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_top_modular.py .", "label": "", "metadata": {}, "score": "106.548416"}
{"text": "load_compressed(\"foo_snappy .str \" , True ) # print \" snappy strings \" , f2.get_features ( ) # print # lzo f.save_compressed(\"foo_lzo .str \" , True ) # print \" lzo strings \" , f2.get_features ( ) # print # # gzip f.save_compressed(\"foo_gzip . str \" , True ) # print \" gzip strings \" , f2.get_features ( ) # print # bzip2 f.save_compressed(\"foo_bzip2 .", "label": "", "metadata": {}, "score": "106.83283"}
{"text": "load_compressed(\"foo_snappy .str \" , True ) # print \" snappy strings \" , f2.get_features ( ) # print # lzo f.save_compressed(\"foo_lzo .str \" , True ) # print \" lzo strings \" , f2.get_features ( ) # print # # gzip f.save_compressed(\"foo_gzip . str \" , True ) # print \" gzip strings \" , f2.get_features ( ) # print # bzip2 f.save_compressed(\"foo_bzip2 .", "label": "", "metadata": {}, "score": "106.83283"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_sparse_poly_modular.py .# This example shows how to use the polynomial kernel with sparse features ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular /", "label": "", "metadata": {}, "score": "107.28005"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_sparse_poly_modular.py .# This example shows how to use the polynomial kernel with sparse features ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular /", "label": "", "metadata": {}, "score": "107.28005"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / kernel_histogram_word_string_modular.py .# The HistogramWordString computes the TOP kernel on inhomogeneous Markov Chains ./data / fm_train_dna ./data / fm_test_dna ./data / label_train_dna ./examples / documented / python_modular / kernel_io_modular.py .", "label": "", "metadata": {}, "score": "107.312004"}
{"text": "/data / fm_test_real . /examples / documented / r_modular / kernel_histogramword_modular .R . # The HistogramWordString computes the TOP kernel on inhomogeneous Markov Chains ./data / fm_train_dna ./data / fm_test_dna ./data / label_train_dna ./examples / documented / r_modular / kernel_linear_byte_modular .", "label": "", "metadata": {}, "score": "107.41583"}
{"text": "/data / fm_test_real . /examples / documented / r_modular / kernel_histogramword_modular .R . # The HistogramWordString computes the TOP kernel on inhomogeneous Markov Chains ./data / fm_train_dna ./data / fm_test_dna ./data / label_train_dna ./examples / documented / r_modular / kernel_linear_byte_modular .", "label": "", "metadata": {}, "score": "107.41583"}
{"text": "# In this example the spherical kernel is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_spline_modular.py .# In this example the spline kernel is being computed for toy data .", "label": "", "metadata": {}, "score": "107.77075"}
{"text": "# In this example the spherical kernel is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_spline_modular.py .# In this example the spline kernel is being computed for toy data .", "label": "", "metadata": {}, "score": "107.77075"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular / kernel_sparse_gaussian_modular.py .# This example demonstrates how to use the Gaussian Kernel with sparse features ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_sparse_linear_modular.py .", "label": "", "metadata": {}, "score": "108.254974"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular / kernel_sparse_gaussian_modular.py .# This example demonstrates how to use the Gaussian Kernel with sparse features ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_sparse_linear_modular.py .", "label": "", "metadata": {}, "score": "108.254974"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / fm_train_dna ./data / fm_test_dna .append_feature_obj(subkfeats_train ) ; feats_test .append_feature_obj(subkfeats_train ) ; feats_test .append_feature_obj(subkfeats_train ) ; feats_test ./examples / documented / octave_modular / kernel_comm_ulong_string_modular .m . %", "label": "", "metadata": {}, "score": "108.58234"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / kernel_histogram_word_string_modular.py .# The HistogramWordString computes the TOP kernel on inhomogeneous Markov Chains ./data / fm_train_dna ./data / fm_test_dna ./data / label_train_dna ./examples / documented / python_modular / kernel_inversemultiquadric_modular.py . # In this example the inverse multiquadic kernel is being computed for toy data .", "label": "", "metadata": {}, "score": "108.968155"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / kernel_histogram_word_string_modular.py .# The HistogramWordString computes the TOP kernel on inhomogeneous Markov Chains ./data / fm_train_dna ./data / fm_test_dna ./data / label_train_dna ./examples / documented / python_modular / kernel_inversemultiquadric_modular.py . # In this example the inverse multiquadic kernel is being computed for toy data .", "label": "", "metadata": {}, "score": "108.968155"}
{"text": "/data / fm_train_dna ./data / fm_test_dna . /examples / documented / octave_modular / kernel_matchwordstring_modular .m . %The class MatchWordStringKernel computes a variant of the polynomial % kernel on strings of same length converted to a word alphabet ./data / fm_train_dna .", "label": "", "metadata": {}, "score": "110.469696"}
{"text": "/data / label_train_twoclass . /examples / documented / python_modular /kernel_cauchy_modular.py . # In this example the Cauchy kernel is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_chi2_modular.py .", "label": "", "metadata": {}, "score": "110.624794"}
{"text": "/data / label_train_twoclass . /examples / documented / python_modular /kernel_cauchy_modular.py . # In this example the Cauchy kernel is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_chi2_modular.py .", "label": "", "metadata": {}, "score": "110.624794"}
{"text": "dump(kernel , file('kernel_obj .Library . /examples / documented / python_modular / library_fisher2x3_modular.py . /examples / documented / python_modular / library_time.py .Mkl . /examples / documented / python_modular / mkl_binclass_modular.py . # In this example we show how to perform Multiple Kernel Learning ( MKL ) # with the modular interface .", "label": "", "metadata": {}, "score": "111.23991"}
{"text": "/data / fm_train_real . /examples / documented / python_modular /kernel_distantsegments_modular.py .# In this example the distant segments kernel is being computed for toy data ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular / kernel_exponential_modular.py .", "label": "", "metadata": {}, "score": "111.927"}
{"text": "/data / fm_train_real . /examples / documented / python_modular /kernel_distantsegments_modular.py .# In this example the distant segments kernel is being computed for toy data ./data / fm_train_dna ./data / fm_test_dna . /examples / documented / python_modular / kernel_exponential_modular.py .", "label": "", "metadata": {}, "score": "111.927"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / fm_train_dna ./data / fm_test_dna .append_feature_obj(subkfeats_train ) feats_test .append_feature_obj(subkfeats_train ) feats_test .append_feature_obj(subkfeats_train ) feats_test ./examples / documented / python_modular / kernel_comm_ulong_string_modular.py . # This is an example for the initialization of the CommUlongString - kernel .", "label": "", "metadata": {}, "score": "111.97139"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / fm_train_dna ./data / fm_test_dna .append_feature_obj(subkfeats_train ) feats_test .append_feature_obj(subkfeats_train ) feats_test .append_feature_obj(subkfeats_train ) feats_test ./examples / documented / python_modular / kernel_comm_ulong_string_modular.py . # This is an example for the initialization of the CommUlongString - kernel .", "label": "", "metadata": {}, "score": "111.97139"}
{"text": "/data / fm_train_real ./data / fm_test_real ./data / fm_train_dna ./data / fm_test_dna .append_feature_obj(subkfeats_train ) feats_test .append_feature_obj(subkfeats_train ) feats_test .append_feature_obj(subkfeats_train ) feats_test ./examples / documented / python_modular / kernel_comm_ulong_string_modular.py . # This is an example for the initialization of the CommUlongString - kernel .", "label": "", "metadata": {}, "score": "111.97139"}
{"text": "/data / fm_test_real . /examples / documented / python_modular /kernel_rationalquadratic_modular.py .# In this example the rational quadratic kernel is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_salzberg_word_string_modular.py . # The SalzbergWordString kernel implements the Salzberg kernel .", "label": "", "metadata": {}, "score": "113.0686"}
{"text": "/data / fm_test_real . /examples / documented / python_modular /kernel_rationalquadratic_modular.py .# In this example the rational quadratic kernel is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_salzberg_word_string_modular.py . # The SalzbergWordString kernel implements the Salzberg kernel .", "label": "", "metadata": {}, "score": "113.0686"}
{"text": "/data / fm_train_real ./data / fm_test_real .add_preprocessor(preproc ) feats_train .apply_preprocessor ( ) feats_test .add_preprocessor(preproc ) feats_test ./examples / documented / python_modular /preprocessor_randomfouriergausspreproc_modular.py ./data / fm_train_real ./data / fm_test_real .add_preprocessor(preproc ) feats_train .apply_preprocessor ( ) feats_test .", "label": "", "metadata": {}, "score": "113.538185"}
{"text": "/data / fm_train_real ./data / fm_test_real .add_preprocessor(preproc ) feats_train .apply_preprocessor ( ) feats_test .add_preprocessor(preproc ) feats_test ./examples / documented / python_modular /preprocessor_randomfouriergausspreproc_modular.py ./data / fm_train_real ./data / fm_test_real .add_preprocessor(preproc ) feats_train .apply_preprocessor ( ) feats_test .", "label": "", "metadata": {}, "score": "113.538185"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / kernel_io_modular.py ./data / fm_train_real ./data / fm_test_real .ascii\",\"w \" ) kernel.save(f ) del f # clean up import os os.unlink(\"gaussian_test . ascii \" ) os.unlink(\"gaussian_train . /examples / documented / python_modular / kernel_linear_byte_modular.py .", "label": "", "metadata": {}, "score": "115.92085"}
{"text": "/data / fm_test_real . /examples / documented / python_modular / kernel_io_modular.py ./data / fm_train_real ./data / fm_test_real .ascii\",\"w \" ) kernel.save(f ) del f # clean up import os os.unlink(\"gaussian_test . ascii \" ) os.unlink(\"gaussian_train . /examples / documented / python_modular / kernel_linear_byte_modular.py .", "label": "", "metadata": {}, "score": "115.92085"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_wavelet_modular.py .# In this example the wavelet kernel is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_weighted_comm_word_string_modular.py .", "label": "", "metadata": {}, "score": "118.44052"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_wavelet_modular.py .# In this example the wavelet kernel is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_weighted_comm_word_string_modular.py .", "label": "", "metadata": {}, "score": "118.44052"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_circular_modular.py .# In this example the circular kernel is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_combined_custom_poly_modular.py .", "label": "", "metadata": {}, "score": "120.509796"}
{"text": "/data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_circular_modular.py .# In this example the circular kernel is being computed for toy data ./data / fm_train_real ./data / fm_test_real . /examples / documented / python_modular / kernel_combined_custom_poly_modular.py .", "label": "", "metadata": {}, "score": "120.509796"}
{"text": "/data / fm_test_real .add_preprocessor(preprocessor ) feats_train .apply_preprocessor ( ) feats_test .add_preprocessor(preprocessor ) feats_test ./examples / documented / python_modular / preprocessor_pca_modular.py .# In this example toy data is being processed using the # Principal Component Analysis ./data / fm_train_real . /examples / documented / python_modular / preprocessor_prunevarsubmean_modular.py .", "label": "", "metadata": {}, "score": "124.541916"}
{"text": "/data / fm_test_real .add_preprocessor(preprocessor ) feats_train .apply_preprocessor ( ) feats_test .add_preprocessor(preprocessor ) feats_test ./examples / documented / python_modular / preprocessor_pca_modular.py .# In this example toy data is being processed using the # Principal Component Analysis ./data / fm_train_real . /examples / documented / python_modular / preprocessor_prunevarsubmean_modular.py .", "label": "", "metadata": {}, "score": "124.541916"}
{"text": "RNA interference ( RNAi ) is a post - transcriptional gene regulatory mechanism by which a double - stranded RNA ( dsRNA ) induces sequence - specific gene silencing [ 1 ] .In mammals and many other organisms , chemically synthesized siRNA molecules can be introduced into cells to knockdown the expression of a specific gene .", "label": "", "metadata": {}, "score": "126.63981"}
