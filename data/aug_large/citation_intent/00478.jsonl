{"text": "Based on their performance , we then define a new type of classifier .Experimental results show the resulting classifiers , learned using the proposed learning algorithms , are competitive with ( or superior to ) the best classifiers , based on both Bayesian networks and other formalisms , and that the computational time for learning and using these classifiers is relatively small .", "label": "", "metadata": {}, "score": "20.005287"}
{"text": "It is shown that the rates of convergence of classifiers depend on two parameters : the complexity of the class of candidate sets and the margin parameter .The main result of the paper concerns optimal aggregation of classifiers : we suggest a classifier that automatically adapts both to the complexity and to the margin , and attains the optimal fast rates , up to a logarithmic factor .", "label": "", "metadata": {}, "score": "23.185314"}
{"text": "This approach is simple in that RBMs are used directly to build a classifier , rather than as a stepping stone .Finally , we demonstrate how discriminative RBMs can also be successfully employed in a semi - supervised setting . .", "label": "", "metadata": {}, "score": "26.426662"}
{"text": "This demonstrates , at least empirically , that discriminatively structured generative models do not lose their ability to impute missing features .In Figure 3(b ) , we show for the same data set and experimental setup that the classification performance of a discriminatively parameterized NB classifier may be superior to a generatively parameterized NB model in the case of missing features .", "label": "", "metadata": {}, "score": "27.31308"}
{"text": "While the classification results are highly similar , our CG - based optimization is computationally up to orders of magnitude faster .Margin - optimized Bayesian network classifiers achieve classification performance comparable to support vector machines ( SVMs ) using a fewer number of parameters .", "label": "", "metadata": {}, "score": "27.414629"}
{"text": "The learning algorithm is applied to the original gene datasets as well as each newly obtained dataset containing only the selected genes , and in each case the final overall accuracy is measured .Figure 1 summarizes the learning accuracy of decision tree classifier on different feature sets .", "label": "", "metadata": {}, "score": "27.997337"}
{"text": "It is therefore essential to develop general approaches and robust methods that are able to overcome the limitation of the small number of training instances and reduce the influence of uncertainties so as to produce reliable classification results .The motivation for this study is to utilize robust ensemble methods that are less sensitive to the selection of genes and are capable of removing the uncertainties of gene expression data .", "label": "", "metadata": {}, "score": "28.657303"}
{"text": "That is , the base learners utilized in a robust ensemble classifier should be of high classification accuracy and avoid making coincident misclassification errors which in turn necessitate the diverse learners .Thus , a sample misclassified by a base learner will be corrected by others , so the fused outputs are more accurate than that of the best individual classifier [ 37 ] .", "label": "", "metadata": {}, "score": "29.237097"}
{"text": "Discriminative parameter learning produces a significantly better classification per- formance than ML parameter learning on the same clas- sifier structure .This is especially valid for cases where the structure of the underlying model is not optimized for classification [ 10 ] , i.e. NB and TAN - CMI .", "label": "", "metadata": {}, "score": "29.465462"}
{"text": "We compare two recently proposed frameworks for combining generative and discriminative probabilistic classifiers and apply them to semi - supervised classification .In both cases we explore the tradeoff between maximizing a discriminative likelihood of labeled data and a generative likelihood of labeled and unlabeled data .", "label": "", "metadata": {}, "score": "29.625374"}
{"text": "However , RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed - forward neural network classifiers , and are not considered as a standalone solution to classification problems .In this paper , we argue that RBMs provide a self - contained framework for deriving competitive non - linear classifiers .", "label": "", "metadata": {}, "score": "29.898067"}
{"text": "However , a necessary and sufficient condition for an ensemble to outperform its individual members is that the base classifiers should be accurate and diverse [ 18 ] .An accurate classifier is one that has an error rate of better than randomly guessing classes for new unseen samples .", "label": "", "metadata": {}, "score": "30.182522"}
{"text": "These classifiers consist of a directed acyclic graph and a set of conditional proba - bility densities , which in case of discrete - valued nodes can be repre - sented by conditional probability tables .In this paper , we investigate the effect of quantizing these conditional probabilities .", "label": "", "metadata": {}, "score": "30.529718"}
{"text": "There has therefore been increasing interest both in active discovery : to identify new classes quickly , and active learning : to train classifiers with minimal supervision .These goals occur together in practice and are intrinsically related because examples of each class are required to train a classifier .", "label": "", "metadata": {}, "score": "30.953644"}
{"text": "In all ensemble experiments a classification tree [ 16 ] was exploited as the base learner because it is sensitive to the changes in its training data .In order to provide a fair comparison , for all utilized ensemble techniques 100 trees are trained to constitute the corresponding ensemble classifiers .", "label": "", "metadata": {}, "score": "31.209122"}
{"text": "We provide here as a starting point two plausible explanations supported by theory and a simulation experiment ( in Additional File 2 ) .We note that prior research has established that linear decision functions capture very well the underlying distributions in microarray classification tasks [ 15 , 16 ] .", "label": "", "metadata": {}, "score": "31.757252"}
{"text": "The exact values of these parameters depend on both the complexity of the classification function and the number of genes in a microarray dataset .Therefore , in general , it is advisable to optimize these parameters by nested cross - validation that accounts for the variability of the random forest model ( e.g. , the selected parameter configuration is the one that performs best on average over multiple validation sample sets ) .", "label": "", "metadata": {}, "score": "31.86515"}
{"text": "Various strategies such as self - training , semi - supervised learning and multiple - instance learning have been proposed .However , these methods are either too adaptive , which causes drifting , or biased by a prior , which hinders incorporation of new ( orthogonal ) information .", "label": "", "metadata": {}, "score": "31.87783"}
{"text": "We compare discriminative and generative learning as typified by logistic regression and naive Bayes .We show , contrary to a widely held belief that discriminative classifiers are almost always to be preferred , that there can often be two distinct regimes of performance as the training set size is increased , one in which each algorithm does better .", "label": "", "metadata": {}, "score": "31.903"}
{"text": "These ensemble techniques have the advantage to alleviate the small sample size problem by averaging and incorporating over multiple classification models to reduce the potential for overfitting the training data [ 16 ] .In this way the training data set may be used in a more efficient way , which is critical to many bioinformatics applications with small sample size .", "label": "", "metadata": {}, "score": "31.95366"}
{"text": "Our results build on the work of Erhan et al .( 2009b ) , showing that unsupervised pre - training appears to play predominantly a regularization role in subsequent supervised training .However our results in an online setting , with a virtually unlimited data stream , point to a somewhat more nuanced interpretation of the roles of optimization and regularization in the unsupervised pre - training effect . .", "label": "", "metadata": {}, "score": "32.20544"}
{"text": "The statistics for both the correct and the competing model are solely collected on word lattices without the use of N - best lists .Thus , particularly for long utterances , the number of sentence alternatives taken into account is significantly larger compared to N - best lists .", "label": "", "metadata": {}, "score": "32.24467"}
{"text": "One line of research has been concerned with designing objective functions which incorporate both g .. by Salah Rifai , Yann N. Dauphin , Pascal Vincent , Yoshua Bengio , Xavier Muller . \" ...We combine three important ideas present in previous work for building classifiers : the semi - supervised hypothesis ( the input distribution contains information about the classifier ) , the unsupervised manifold hypothesis ( data density concentrates near low - dimensional manifolds ) , and the manifold hyp ... \" .", "label": "", "metadata": {}, "score": "32.42993"}
{"text": "Given that the number of optimal genes varies from dataset to dataset , and that SVMs are known to be fairly insensitive to a very large number of irrelevant genes , such application of SVMs likely biases down their performance .Second , a one - versus - one SVM algorithm was applied for the multicategory classification tasks , while it is has been shown that in microarray gene expression domain this method is inferior to other multicategory SVM methods , such as one - versus - rest [ 1 , 6 ] .", "label": "", "metadata": {}, "score": "32.433575"}
{"text": "In this paper a model for automatically learning bias is investigated .The central assumption of the model is that the learner is embedded within an environment of related learning tasks .Within such an environment the learner can sample from multiple tasks , and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment .", "label": "", "metadata": {}, "score": "32.502136"}
{"text": "In practice , it appeared to be difficult to define a single measure of diversity and even more difficult to relate that measure to the ensemble performance in a neat and expressive dependency .Here , to investigate the ability of the proposed ICA - based RotBoost ensemble to build accurate and diverse base learners efficiently , the pairwise diversity measure is utilized [ 40 ] .", "label": "", "metadata": {}, "score": "32.81072"}
{"text": "In this work , we introduce and justify this algorithm as a stochastic natural gradient descent method , i.e. a method which accounts for the information geometry in the parameter space of the statistical model .We show how this learning algorithm can be used to train probabilistic generative models by minimizing different discriminative loss functions , such as the negative conditional log - likelihood and the Hinge loss .", "label": "", "metadata": {}, "score": "33.316734"}
{"text": "We demonstrate that under challenging situations , this method has strong reacquisition ability and robustness to distracters in background . ... tice .However , generative methods often have better generalization performance when the size of training data is small .Specifically , a simple generative classifier ( n .. \" ...", "label": "", "metadata": {}, "score": "33.377155"}
{"text": "Specifically , if the generative linear decision function is not orthogonal to the coordinate axes , then a decision tree of infinite size is required to represent this function without error [ 17 ] .SVMs on the other hand use linear classifiers and thus can model such functions naturally , using a small number of free parameters ( i.e. , bounded by the available sample size ) .", "label": "", "metadata": {}, "score": "33.44281"}
{"text": "We provide results for a recently proposed maximum margin optimization approach based on convex relaxation [ 1].While the classification results are highly similar , our CG - based optimization is computationally up to orders of magnitude faster .Margin - optimized Bayesian network classifiers achieve classification performance comparable to support vector machines ( SVMs ) using fewer parameters .", "label": "", "metadata": {}, "score": "33.779922"}
{"text": "In contrast , the structure of Bayesian network classifiers naturally limits the number of parameters .A substantial difference is that SVMs determine the num- ber of support vectors automatically while in the case of Bayesian networks the number of parameters is given by the cardinality of the variables and the structure .", "label": "", "metadata": {}, "score": "34.03141"}
{"text": "The advantage of the classification approach is that it can make use of powerful machine learning algorithms in connection with arbitrary features from the sentence context .Typical features includ ... . \" ...Many evaluation issues for grammatical error detection have previously been overlooked , making it hard to draw meaningful comparisons between different approaches , even when they are evaluated on the same corpus .", "label": "", "metadata": {}, "score": "34.20014"}
{"text": "We first learn a standard PRM , and show that its performance is competitive with the best known techniques .We then define a hierarchical PRM , which extends standard PRMs by dynamically refining classes into hierarchies , which improves the expressiveness as well as the context sensitivity of the PRM .", "label": "", "metadata": {}, "score": "34.348206"}
{"text": "First , it provides a better probabilistic model of the data , which can better identify where the data concentrate in .-dimensional space .Second , it can find a not necessarily orthogonal basis , which may reconstruct the data better than PCA in the presence of noise .", "label": "", "metadata": {}, "score": "34.41288"}
{"text": "Next , we built a classification model with the best parameters on the original training set and applied this model to the original testing set .Details about the \" nested cross - validation \" procedure can be found in [ 19 , 20 ] .", "label": "", "metadata": {}, "score": "34.455803"}
{"text": "Following transformation , the axes are rotated optimally .Despite the conventional approach of choosing some directions for good discriminate capability , the rotation mainly contributes to the generation of diversity among the classifiers without weakening the individual classifiers .Thus , an acceptable trade - off between diversity and accuracy can be maintained simultaneously .", "label": "", "metadata": {}, "score": "34.740532"}
{"text": "This hypothesis underlies not only the strict semi - supervised setting where one has many more unlabeled examples at his disposal than labeled ones , but also the successful unsupervised pretraining ... \" ...We compare discriminative and generative learning as typified by logistic regression and naive Bayes .", "label": "", "metadata": {}, "score": "34.783173"}
{"text": "However , without further algorithmic developments its practical application seems to be limited to applications using only few training data and a low number of features .In contrast , the proposed method for maximum margin parameter learning can deal with large sets of training data efficiently and achieves comparable classification rates .", "label": "", "metadata": {}, "score": "34.791157"}
{"text": "Many evaluation issues for grammatical error detection have previously been overlooked , making it hard to draw meaningful comparisons between different approaches , even when they are evaluated on the same corpus .To begin with , the three - way contingency between a writer 's sentence , the annotator 's correction , and the system 's output makes evaluation more complex than in some other NLP tasks , which we address by presenting an intuitive evaluation scheme .", "label": "", "metadata": {}, "score": "34.982437"}
{"text": "either modifying the supervised objective function ( Barron , 1991 ) or explicitly imposing constraints on the parame626WHY DOES UNSUPERVISED PRE - TRAINING HELP DEEP LEARNING ?This type of initialization - as - regularization strategy has precedence in the neural networks literature , in the shape of the early stopping idea ( Sj\u00f6berg and Ljung , 1995 ; Amari et al . , 1997 ) , and i .. by Hugo Larochelle , Yoshua Bengio - In ICML ' 08 : Proceedings of the 25th international conference on Machine learning .", "label": "", "metadata": {}, "score": "35.016533"}
{"text": "Taskar et al .[ 3 ] observed that undirected graphical models can be efficiently trained to maximize the margin .More recently , Guo et al .[ 1 ] introduced the maximization of the margin to Bayesian networks using convex optimization .", "label": "", "metadata": {}, "score": "35.10543"}
{"text": "Empirically , we could not observe any advantage of regularization over early stopping in terms of achieved classification performance .Continuous features were discretized using recursive minimal entropy partitioning [ 38 ] where the quantiza- tion intervals were determined using only the training data .", "label": "", "metadata": {}, "score": "35.38604"}
{"text": "Because these are restrictive and unreliable , we apply cross - corpus evaluation to the task .We demonstrate the efficacy of lexical features , which had previo ... \" .Previous approaches to the task of native language identification ( Koppel et al . , 2005 ) have been limited to small , within - corpus evaluations .", "label": "", "metadata": {}, "score": "35.43141"}
{"text": "Gene selection is then performed on the training set , and the goodness of selected genes is assessed from the unseen test set [ 31 ] .However , due to the small number of instances in gene microarray datasets , such an approach can lead to unreliable results .", "label": "", "metadata": {}, "score": "35.55863"}
{"text": "We note that instead of early stopping also regularization of the parameters can be used to avoid over - training of the models .In any case , a weight measuring the trade - off between objective function and regularization term has to be determined by cross - validation .", "label": "", "metadata": {}, "score": "35.76635"}
{"text": "Developing active learning algorithms to optimise both rare class discovery and classification simultaneously is challenging because discovery and classification have conflicting requirements in query criteria .In this paper , we address these issues with two contributions : a unified active learning model to jointly discover new categories and learn to classify them by adapting query criteria online ; and a classifier combination algorithm that switches generative and discriminative classifiers as learning progresses .", "label": "", "metadata": {}, "score": "36.08623"}
{"text": "Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of autoencoder variants with impressive results being obtained in several areas , mostly on vision and language datasets .The best results obtained on supervised learning tasks ... \" .", "label": "", "metadata": {}, "score": "36.188793"}
{"text": "This is coincident with the observation previously given that the diversity usually conflicts with the accuracy of the base learners .However our experiments indicate that the ICA - based RotBoost ensemble could establish a proper balance between the diversity and the accuracy of the constituted base learners .", "label": "", "metadata": {}, "score": "36.202065"}
{"text": "Thus , it might result in a partial 1-tree ( forest ) over the attributes .This approach is computationally expensive since each time an edge is added , the scores for all O?N2 ? need to be re - evaluated due to the discriminative non-", "label": "", "metadata": {}, "score": "36.29565"}
{"text": "There are largely two kinds of transformation methods , that is , PCA and ICA .PCA projects the data into a new space spanned by the principal components [ 26 ] .In contrast to PCA , ICA decomposes an input dataset into components so that each component is statistically as independent from the others as possible .", "label": "", "metadata": {}, "score": "36.314743"}
{"text": "The generative model uses a number of low dimension linear subspaces to describe the appearance of the object .In order to reacquire an object , the generative model encodes all the appearance variations that have been seen .A discriminative classifier is implemented as an online support vector machine , which is trained to focus on recent appearance variations .", "label": "", "metadata": {}, "score": "36.320694"}
{"text": "The classification results of both maximum margin parameter learning approaches are almost iden- tical , whereas the computational requirements of our CG - based optimization are up to orders of magnitude lower .Margin - optimized Bayesian networks perform on par with SVMs in terms of classification rate , however the Bayesian network classifiers require fewer parame- ters than the SVM and can directly deal with missing features , a case where discriminative classifiers usually require imputation techniques .", "label": "", "metadata": {}, "score": "36.407608"}
{"text": "We also provide results for SVMs showing that margin - optimized Bayesian net- work classifiers are serious competitors - especially in cases where small - sized and probabilistic models are re- quired .Moreover , we show experiments demonstrating the ability of handling missing feature scenarios .", "label": "", "metadata": {}, "score": "36.41775"}
{"text": "results for this algorithm have only been demonstrated on small - scale experiments .Since then , different margin- based training algorithms have been proposed for hid- den Markov models in [ 6 ] , [ 7 ] and references therein .", "label": "", "metadata": {}, "score": "36.47783"}
{"text": "Inverse Multiple Instance Learning for Classifier Grids ( Sternig , Roth , Bischof ) .Recently , classifier grids have shown to be a considerable alternative for object detection from static cameras .However , one drawback of such approaches is drifting if an object is not moving over a long period of time .", "label": "", "metadata": {}, "score": "36.510323"}
{"text": "The experiment shows that the choice of RF parameters creates large variation in the classifier performance whereas the choice of the main SVM parameter has only minor effects on the error .In practical analysis of microarrays this means that finding the RFs with optimal error for the dataset may involve extensive model selection which in turn opens up the possibility for overfitting given the small sample sizes in validation datasets .", "label": "", "metadata": {}, "score": "36.76561"}
{"text": "We demonstrate our algorithm on two different representative tasks , \\ie , object detection and object tracking showing that we can handle typical problems considerable better than existing approaches .In addition , to demonstrate the stability and the robustness , we show long - term experiments for both tasks .", "label": "", "metadata": {}, "score": "36.821743"}
{"text": "A theoretical analysis of the sparsification method reveals its close affinity to kernel PCA , and a data - dependent loss bound is presented , quantifying the generalization performance of the KRLS algorithm .We demonstrate the performance and scaling properties of KRLS and compare it to a stateof -the - art Support Vector Regression algorithm , using both synthetic and real data .", "label": "", "metadata": {}, "score": "36.96027"}
{"text": "The parameters \u03b3 and r were set to default value 1 .Random forest classifiers .Random forests ( RF ) is a classification algorithm that uses an ensemble of unpruned decision trees , each of which is built on a bootstrap sample of the training data using a randomly selected subset of variables [ 2 ] .", "label": "", "metadata": {}, "score": "37.078606"}
{"text": "Because of the impact of different uncertainties together with the lack of labeled training samples , the conventional machine learning techniques face complicated challenges to develop reliable classification models .Quite often selecting only a few genes can discriminate a majority of training instances correctly [ 14 ] .", "label": "", "metadata": {}, "score": "37.134903"}
{"text": "However , SVMs outperform our discriminative Bayesian network classifiers on all data sets .For TIMIT-4/6 one reason might be that SVMs are applied to the continuous feature domain .In Table 4 we compare the model complexity , i.e. the number of parameters , between SVMs and the best performing Bayesian network classifier .", "label": "", "metadata": {}, "score": "37.16358"}
{"text": "Additionally , NB - CG - MM seems to be more robust to increasing number of missing features compared to NB - CG - CL .This can be attributed to the better generalization property of a margin - optimized classifier .", "label": "", "metadata": {}, "score": "37.2902"}
{"text": "So there was no consistent relationship between the classification accuracy and . was the optimum choice to establish a proper balance between the overall classification accuracy and the diversity of the based learners for most examined gene datasets .To find the most promising transformation method and preserve both diversity and accuracy of the base classifiers , we conduct experiments with two well - known transforms , that is , PCA and ICA .", "label": "", "metadata": {}, "score": "37.29164"}
{"text": "Training a model on error - tagged non - native text is expensive , as it requires large amounts of manually- ... . by Alla Rozovskaya , Kai - wei Chang , Mark Sammons , Dan Roth - In Proceedings of the Seventeenth Conference on Computational Natural Language Learning : Shared , 2013 . \" ...", "label": "", "metadata": {}, "score": "37.361877"}
{"text": "Statistical comparison among classifiers .When comparing two classifiers , it is important to assess whether the observed difference in classification performance is statistically significant or simply due to chance .We assessed significance of differences in classification performance in individual datasets or in all datasets on average using a non - parametric permutation test [ 29 ] based on the theory of [ 30 ] .", "label": "", "metadata": {}, "score": "37.371407"}
{"text": "New samples are classified according to the side of the hyperplane they belong to [ 22 ] .Many extensions of the basic SVM algorithm can handle multicategory data .The \" one - versus - rest \" SVM works better for multi - class microarray data [ 1 , 6 ] , so we adopted this method for the analysis of multicategory datasets in the present study .", "label": "", "metadata": {}, "score": "37.488487"}
{"text": "Many BN - learners , however , attempt to find the BN that maximizes a different objective function ( viz ., likelihood , rather than classification accuracy ) , typically by first learning an appropriate graphical structure , then finding the maximal likelihood parameters for that structure .", "label": "", "metadata": {}, "score": "37.506393"}
{"text": "We keep the sum - to - one constraints which maintain the proba- bilistic interpretation of the network .This has the par- ticular advantage that summing over missing variables is still possible ( as we show in this paper ) .", "label": "", "metadata": {}, "score": "37.51368"}
{"text": "There is no dataset where RFs outperform SVMs with statistically significant difference .The number of genes selected on average across 10 cross - validation training sets is provided in Table 3 .We note that in the present comparison we focus exclusively on classification performance and do not incorporate number of selected genes in the comparison metrics because there is no well - defined trade - off between number of selected genes and classification performance in the datasets studied .", "label": "", "metadata": {}, "score": "37.519005"}
{"text": "( 4 ) and ( 6 ) yield identical estimators in the setting where the ( language model ) probabilities p(Yn ) are held fixed . ...We present an efficient discriminative training procedure utilizing phone lattices .Different approaches to expediting lattice generation , statistics collection , and convergence were studied .", "label": "", "metadata": {}, "score": "37.53155"}
{"text": "Tools . \" ...We present a novel beam - search decoder for grammatical error correction .The decoder iteratively generates new hypothesis corrections from current hypotheses and scores them based on features of grammatical correctness and fluency .These features include scores from discriminative classifiers for sp ... \" .", "label": "", "metadata": {}, "score": "37.625935"}
{"text": "The decoder iteratively generates new hypothesis corrections from current hypotheses and scores them based on features of grammatical correctness and fluency .These features include scores from discriminative classifiers for specific error categories , such as articles and prepositions .Unlike all previous approaches , our method is able to perform correction of whole sentences with multiple and interacting errors while still taking advantage of powerful existing classifier approaches .", "label": "", "metadata": {}, "score": "37.80152"}
{"text": "The performance of this method is illustrated by several text classification problems for which a multinomial naive Bayes and a latent Dirichlet allocation based classifier are learned using different discriminative loss functions . a ) Discriminatively versus generatively optimized parameters : Here , we compare the classification performance of BNCs with MAP parameters and of BNCs with MM parameters over varying numbers of bits used for quantization .", "label": "", "metadata": {}, "score": "37.813988"}
{"text": "Furthermore , we compare the classification performance and the robustness of BNCs with generatively and discriminatively optimized parameters , i.e. parameters optimized for high data likelihood and parameters optimized for classification , with respect to parameter quantization .Generatively optimized parameters are more robust for very low bit - widths , i.e. less classifications change because of quantization .", "label": "", "metadata": {}, "score": "37.86035"}
{"text": "[ 32 ] E. Keogh and M. Pazzani , \" Learning augmented Bayesian classi- fiers : A comparison of distribution - based and classification - based approaches , \" in Workshop on Artificial Intelligence and Statistics , 1999 , pp .225 - 230 .", "label": "", "metadata": {}, "score": "37.928116"}
{"text": "The proposed method achieved the highest averaged generalization ability compared to its counterparts and denoted an acceptable level of diversity among the learners for majority of the analyzed benchmark datasets .J. Khan , J. S. Wei , M. Ringn\u00e9r et al . , \" Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks , \" Nature Medicine , vol .", "label": "", "metadata": {}, "score": "38.07202"}
{"text": "We show that maximizing the margin dominates the con- ditional likelihood approach with respect to classification performance for most cases .Furthermore , we provide results for maximum margin optimization using convex relaxation [ 1].We achieve highly similar classification rates , whereas our CG - based margin optimization is computationally dramatically less costly .", "label": "", "metadata": {}, "score": "38.21623"}
{"text": "We are interested in the problem of learning the dependency structure of a belief net , which involves a trade - off between simplicity and goodness of fit to the training data .We describe the results of an empirical comparison of three standard model selection criteria --- viz .", "label": "", "metadata": {}, "score": "38.22468"}
{"text": "In terms of generalization accuracy , ICA - based RotBoost ensemble in conjunction with fast correlation - based filter demonstrated superior average performance over all considered ensemble classifiers and is therefore recommended as an efficient classification technique for the prediction of new gene microarray class labels .", "label": "", "metadata": {}, "score": "38.295116"}
{"text": "We examine the algorithms both in the settings when no gene selection is performed and when several popular gene selection methods are used .To make our evaluation more relevant to practitioners , we focus not only on diagnostic datasets that are in general known to have strong predictive signals , but also include several outcome prediction datasets where the signals are weaker and larger gene sets are often required for optimal prediction .", "label": "", "metadata": {}, "score": "38.31636"}
{"text": "Active learning provides the link between the generative and discriminative model , in the sense that the former is selectively sampled such that the training process is guided towards the most informative samples of the latter .ets , and a pattern classification technique , e.g. neural networks , support vector machines , [ 13 , 21].", "label": "", "metadata": {}, "score": "38.40083"}
{"text": "In this paper we employ the Minimum Classification Error ( MCE ) criterion to optimize the parameters of the acoustic model of a large scale speech recognition system .The ... \" .Discriminative training criteria have been shown to consistently outperform maximum likelihood trained speech recognition systems .", "label": "", "metadata": {}, "score": "38.498497"}
{"text": "We unify f - divergences , Bregman divergences , surrogate regret bounds , proper scoring rules , cost curves , ROC - curves and statistical information .We do this by systematically studying integral and variational representations of these various objects and in so doing identify their primitives which all are related to cost - sensitive binary classification .", "label": "", "metadata": {}, "score": "38.504623"}
{"text": "Similarly , the SuperParent al- gorithm [ 32 ] is almost as efficient as OMI - CR achieving slightly lower classification performance [ 20].Once the structure has been determined discrimina- tive parameter learning is performed .This is either done using the proposed CG algorithm to maximize the margin , labeled as CG - MM ( see Section 3 ) , or the CL method ( see Section 2.2 ) .", "label": "", "metadata": {}, "score": "38.50798"}
{"text": "It is also necessary to point out that all these experiments have been accomplished on the best discriminative genes already selected by FCBF algorithm .It is well known that no algorithm can hold a general advantage in terms of generalization capability over another one across all possible classification tasks .", "label": "", "metadata": {}, "score": "38.533264"}
{"text": "There are two broad categories for feature selection algorithms , filter model or wrapper [ 28 ] .The filter model relies on general characteristics of the training data to choose best features without involving any learning algorithm .The wrapper models , on the contrary , depends on feature addition or deletion to compose subset features and uses an evaluation function with a predetermined learning algorithm to estimate the subset features .", "label": "", "metadata": {}, "score": "38.73101"}
{"text": "The parameters are initialized to the ML estimates for all discriminative parameter learning methods.4Similar as in [ 10 ] we use cross tuning to estimate the optimal number of iterations for the CG algorithm to avoid overfitting .Empirical results showed that the initialization of the Bayesian network to the ML estimates for MM or CL optimization performs better than pure random initialization .", "label": "", "metadata": {}, "score": "38.8462"}
{"text": "We demonstrate the efficacy of lexical features , which had previously been avoided due to the within - corpus topic confounds , and provide a detailed evaluation of various options , including a simple bias adaptation technique and a number of classifier algorithms .", "label": "", "metadata": {}, "score": "38.895866"}
{"text": "In contrast to SVMs , the used Bayesian network structures are probabilistic generative models - even when discriminatively learned .They might be preferred since it is easy to work with missing features , domain knowledge can be directly incorporated into the graph structure , and it is easy to work with structured data .", "label": "", "metadata": {}, "score": "39.069736"}
{"text": "Furthermore , we determine performance bounds that hold with a user specified confidence using quantization theory .Our results emphasize that only small bit - widths are necessary to achieve good classification rates .Affiliated with .Abstract .Background .Cancer diagnosis and clinical outcome prediction are among the most important emerging applications of gene expression microarray technology with several molecular signatures on their way toward clinical deployment .", "label": "", "metadata": {}, "score": "39.179245"}
{"text": "Classification can be considered as nonparametric estimation of sets , where the risk is defined by means of a specific distance between sets associated with misclassification error .It is shown that the rates of convergence of classifiers depend on two parameters : the complexity of the class of cand ... \" .", "label": "", "metadata": {}, "score": "39.384254"}
{"text": "We present experiments validating our analysis . by Gregory Druck Chris Pal , Xiaojin Zhu , Andrew Mccallum - In KDD , 2007 . \" ...We compare two recently proposed frameworks for combining generative and discriminative probabilistic classifiers and apply them to semi - supervised classification .", "label": "", "metadata": {}, "score": "39.43582"}
{"text": "We provide experimental results gained on speech and handwriting recognition that demonstrate the potential of the method .Recently , a promising direction has been explored with the development of margin - based methods for sequences ( Taskar et al ., 2004 ; Tsochantaridis et al .", "label": "", "metadata": {}, "score": "39.703674"}
{"text": "This in turn improves classification accuracy with predominant selected features .-correlation .Using the sorted feature list , redundant features are eliminated one by one in a descending order .The remaining feature subset thus contains the predominant features with zero redundant features in terms of .", "label": "", "metadata": {}, "score": "39.73456"}
{"text": "Cross - validation design .We used 10-fold cross - validation to estimate the performance of the classification algorithms .In order to optimize algorithm parameters , we used another \" nested \" loop of cross - validation by further splitting each of the 10 original training sets into smaller training sets and validation sets .", "label": "", "metadata": {}, "score": "39.77379"}
{"text": "In this paper we demonstrate that , ignoring computational constraints , it is possible to release synthetic databases that are useful for accurately answering large classes of queries while preserving differential privacy .Specifically , we give a mechanism that privately releases synthetic data usefu ... \" .", "label": "", "metadata": {}, "score": "39.885593"}
{"text": "Add to this tree the class node C and the edges from C to all attributes X1, ... ,XN .This generative structure learning method is abbreviated as CMI in the experiments .Page 6 . 6 4.2Greedy Discriminative Structure Learning This method proceeds as follows : a network is initialized to NB and at each iteration an edge is added that gives the largest improvement of the scoring function , while maintaining a partial 1-tree .", "label": "", "metadata": {}, "score": "39.9235"}
{"text": "Discriminative parameter learning significantly outperforms generative maximum likelihood estimation for naive Bayes and tree augmented naive Bayes structures on all considered data sets .Furthermore , maximizing the margin dominates the conditional likelihood approach in terms of classification performance in most cases .", "label": "", "metadata": {}, "score": "39.933228"}
{"text": "Rotation Forest is an ensemble classification approach which is built with a set of decision trees .For each tree , the bootstrap samples extracted from the original training set are adopted to construct a new training set .Then the feature set of the new training set is randomly split into some subsets , which are transformed individually .", "label": "", "metadata": {}, "score": "39.964348"}
{"text": ", likelihood , rather than classiifcation accuracy , typically by first using some model selection criterion to identify an appropriate graphical structure , then finding good parameters for that structure .We empirically compare these criteria against a variety of different correct BN structures , both real - world and synthetic , over a range of complexities .", "label": "", "metadata": {}, "score": "40.047203"}
{"text": "In experiments , we compare the classification performance of maximum margin parameter learning to conditional likelihood and maximum likelihood learning approaches .Discriminative parameter learning significantly outperforms generative maximum likelihood estimation for naive Bayes and tree augmented naive Bayes structures on all considered data sets .", "label": "", "metadata": {}, "score": "40.059845"}
{"text": "We introduce classifier grids in order to develop an adaptive but still robust real - time object detector for static cameras .Instead of using a sliding window for object detection we propose to train a separate classifier for each image location , obtaining a very specific object detector with a low false alarm rate .", "label": "", "metadata": {}, "score": "40.172295"}
{"text": "Convex problems are desirable in many cases as any local optimum is a global optimum .Collobert et al .[ 8 ] show that the optimization of non - convex loss functions in SVMs can lead to sparse solutions ( lower number of support vectors ) and accelerated training performance .", "label": "", "metadata": {}, "score": "40.23794"}
{"text": "We partly close this gap by analyzing reduced precision implementations of BNCs .In detail , we investigate the quantization of the parameters of BNCs with discrete valued nodes including the implications on the classification rate ( CR ) .We derive worst - case and probabilistic bounds on the CR for different bit - widths .", "label": "", "metadata": {}, "score": "40.286377"}
{"text": "The results show that the proposed BN and Bayes multi - net classifiers are competitive with ( or superior to ) the best known classifiers , based on both BN and other formalisms ; and that the computational time for learning and using these classifiers is relatively small .", "label": "", "metadata": {}, "score": "40.28898"}
{"text": "This paper provides algorithms that use an information - theoretic analysis to learn Bayesian network structures from data .Based on our three - phase learning framework , we develop efficient algorithms that can effectively learn belief networks , requiring only polynomial numbers of conditional independence ( CI ) tests in typical cases .", "label": "", "metadata": {}, "score": "40.354427"}
{"text": "The deep network learning algorithms that have been proposed recently and that we study in this paper can be seen as combining the i .. \" ... Statistical and computational concerns have motivated parameter estimators based on various forms of likelihood , e.g. , joint , conditional , and pseudolikelihood .", "label": "", "metadata": {}, "score": "40.362743"}
{"text": "A semi - supervised hybrid generative / discriminative method provides the best accuracy in 75 % of the experiments , and the multi - conditional learning hybrid approach achieves the highest overall mean accuracy across all tasks . ... combine generative and discriminative training .", "label": "", "metadata": {}, "score": "40.41263"}
{"text": "This paper takes the ' ' performance criteria ' ' seriously , and considers the challenge of computing the BN whose performance --- read ' ' accuracy over the distribution of queries ' ' --- is optimal .We show that many aspects of this learning task are more difficult than the corresponding subtasks in the standard model , and then present an important subclass of queries that greatly simplifies our learning task .", "label": "", "metadata": {}, "score": "40.419933"}
{"text": "Using full set of genes .The performance results of classification prior to gene selection are shown in Figure 1 and Table 1 .In total , SVMs nominally ( that is , not necessarily statistically significantly ) outperform RFs in 15 datasets , RFs nominally outperform SVMs in 4 datasets , and in 3 datasets algorithms perform the same .", "label": "", "metadata": {}, "score": "40.64174"}
{"text": "123 - 138 , 2008 .[40 ] A. W\u00a8 achter and L. Biegler , \" On the implementation of an interior- point filter line - search algorithm for large - scale nonlinear pro- gramming , \" Mathematical Programming , vol .", "label": "", "metadata": {}, "score": "40.65171"}
{"text": "RCI is an entropy - based measure that quantifies how much the uncertainty of a decision problem is reduced by a classifier relative to classifying using only the prior probabilities of each class .We note that both AUC and RCI are more discriminative than the accuracy metric ( also known as proportion of correct classifications ) and are not sensitive to unbalanced distributions [ 7 - 10 ] .", "label": "", "metadata": {}, "score": "40.662098"}
{"text": "Transformation Methods .As it was already mentioned , the purpose of rotation - based ensemble classifiers such as Rotation Forest and RotBoost is to increase the individual classifier performance and the diversity within the ensemble .Thus , a full feature set is obtained with all the transformed features for each considered tree in the ensemble .", "label": "", "metadata": {}, "score": "40.68608"}
{"text": "To be more specific , in a typical microarray dataset , there are thousands of gene features .Then if RotBoost ensemble classifier is applied to classify such dataset directly , a rotation matrix with thousands of dimensions is required for each tree , which greatly increases the computational complexity .", "label": "", "metadata": {}, "score": "40.7003"}
{"text": "Thanks to Jeff Bilmes for discussions and support in writing this paper .The conditional log likelihood given in Eq .( 2 ) can be optimized by a conjugate gradient algorithm using line- search in a similar manner as given in Section 3.2 .", "label": "", "metadata": {}, "score": "40.7138"}
{"text": "See subsection \" Statistical comparison among classifiers \" for the description of statistical test employed to compute reported p - values .P - values shown with boldface denote statistically significant differences between classification methods at the 0.05 \u03b1 level .According to the results in Figure 2 and Table 2 , in 17 datasets SVMs nominally outperform RFs , in 3 datasets RFs nominally outperform SVMs , and in 2 datasets algorithms perform the same .", "label": "", "metadata": {}, "score": "40.78524"}
{"text": "Specifically , we give a mechanism that privately releases synthetic data useful for answering a class of queries over a discrete domain with error that grows as a function of the size of the smallest net approximately representing the answers to that class of queries .", "label": "", "metadata": {}, "score": "40.84411"}
{"text": "The key feature of this problem is that there is n ... \" .We investigate the following problem : Given a set of documents of a particular topic or class # , and a large set # of mixed documents that contains documents from class # and other types of documents , identify the documents from class # in # .", "label": "", "metadata": {}, "score": "40.897728"}
{"text": "491 - 496 .T. Roos , H. Wettig , P. Gr\u00a8 unwald , P. Myllym\u00a8 aki , and H. Tirri , \" On discriminative Bayesian network classifiers and logistic regres- sion , \" Machine Learning , vol .59 , pp .", "label": "", "metadata": {}, "score": "40.98632"}
{"text": "[19 ] J. Pearl , Probabilistic reasoning in intelligent systems : Networks of plausible inference .Morgan Kaufmann , 1988 .[20 ] F. Pernkopf and J. Bilmes , \" Efficient heuristics for discriminative structure learning of Bayesian network classifiers , \" Journal of Machine Learning Research , vol .", "label": "", "metadata": {}, "score": "41.197796"}
{"text": "It may take several years before the precise reasons of differences in empirical error are thoroughly understood , and in the meantime the empirical advantages and disadvantages of methods should be noted first by practitioners .Data analysts should also be aware of a limitation of RFs imposed by its embedded random gene selection .", "label": "", "metadata": {}, "score": "41.296204"}
{"text": "Having captured the spot intensities , the obtained intensities undergo a normalization preprocessing stage to remove systematic errors within the data [ 2 ] .Early application of microarrays to the study of human disease conditions rapidly revealed their potential as a medical diagnostic tool [ 3 , 4 ] .", "label": "", "metadata": {}, "score": "41.32492"}
{"text": "Unlike previous discriminative frameworks for ASR , such as maximum mutual information and minimum classification error , our framework leads to a convex optimization , without any spurious local minima .The objective function for large margin training of CD - HMMs is defined over a parameter space of positive semidefinite matrices .", "label": "", "metadata": {}, "score": "41.480453"}
{"text": "This is realized by using an internal multi - class representation and modeling reliable and unreliable data in separate classes .Unreliable data is considered transient , hence we use highly adaptive learning parameters to adapt to fast changes in the scene while errors fade out fast .", "label": "", "metadata": {}, "score": "41.60545"}
{"text": "We also show that training on a large corpus and testing on a smaller one works well , but not vice versa .Finally , we show that system performance varies across proficiency scores . ... ently used in forensic linguistics .", "label": "", "metadata": {}, "score": "41.645313"}
{"text": "Specifying a SVMs classifier requires two parameters , that is , the kernel function and the regularization parameter .Table 4 : Mean classification accuracy of each classification method against 8 different gene datasets .Table 4 summarizes the mean classification accuracy of each classification method on the considered datasets .", "label": "", "metadata": {}, "score": "41.70201"}
{"text": "38 , no . 3 , pp . 1 - 10 , 2005 .[34 ] D. Grossman and P. Domingos , \" Learning Bayesian network classifiers by maximizing conditional likelihood , \" in Inter .Conf . of Machine Lerning ( ICML ) , 2004 , pp .", "label": "", "metadata": {}, "score": "41.734524"}
{"text": "38 ] U.Fayyad andK.Irani , continuous - valued attributes for classification learning , \" in Joint Conf . on Artificial Intelligence , 1993 , pp .1022 - 1027 .[ 39 ] F. Pernkopf , T. Van Pham , and J. Bilmes , \" Broad phonetic classi- fication using discriminative Bayesian networks , \" Speech Commu- nication , vol .", "label": "", "metadata": {}, "score": "41.782322"}
{"text": "Index Terms - Bayesian network classifier , discriminative learning , discriminative classifiers , large margin training , missing features , convex relaxation .One of the most successful discriminative classifiers , namely the support vector machine ( SVM ) , finds a de- cision boundary which maximizes the margin between samples of distinct classes resulting in good general- ization properties of the classifier .", "label": "", "metadata": {}, "score": "41.79221"}
{"text": "Springer , 2006 .[26 ] S. Acid , L. de Campos , and J. Castellano , \" Learning Bayesian network classifiers : Searching in a space of partially directed acyclic graphs , \" Machine Learning , vol .59 , pp .", "label": "", "metadata": {}, "score": "41.928123"}
{"text": "In Bagging , each base classifier is constructed on a bootstrap sample of the original training data , that is , a random sample of instances drawn with replacement and having the same size as the original training data .Ensemble classification is achieved by means of majority voting , where an unlabeled unseen data is assigned the class with the highest number of votes among the individual classifiers ' predictions [ 21 ] .", "label": "", "metadata": {}, "score": "41.977818"}
{"text": "We empirically observed similar results as for CG - based optimization , however the EBW requires a rational objective function which can not be guaranteed anymore .Similarly , we introduced maximum margin learning to Gaussian mixture models using the EBW algorithm [ 16].", "label": "", "metadata": {}, "score": "42.089573"}
{"text": "In particular , this holds for SVMs , logistic regression , and multi - layered perceptrons .Then the classification approach is applied on the completed data .In Figure 3(a ) , we present the classification performance of discriminative and generative structures using ML parameter learning on the MNIST data as-", "label": "", "metadata": {}, "score": "42.106094"}
{"text": "Our results suggest that AIC and cross - validation are both good criteria for avoiding overfitting , but MDL does not work well in this context .Bayesian belief nets ( BNs ) are often used for classification tasks , typically to return the most likely class label for a specified instance .", "label": "", "metadata": {}, "score": "42.213436"}
{"text": "To assess the efficiency of the RotBoost , other nonensemble / ensemble techniques including Decision Trees , Support Vector Machines , Rotation Forest , AdaBoost , and Bagging are also deployed .Experimental results have revealed that the combination of the fast correlation - based feature selection method with ICA - based RotBoost ensemble is highly effective for gene classification .", "label": "", "metadata": {}, "score": "42.26751"}
{"text": "[ 18 ] Y. LeCun , L. Bottou , Y. Bengio , and P. Haffner , \" Gradient - based learning applied to document recognition , \" Proceedings fo the IEEE , vol .86 , no .11 , pp .", "label": "", "metadata": {}, "score": "42.372707"}
{"text": "Discriminative models usually require mechanisms to first complete unknown feature values in the data - known as data imputation - and then applying the stan- dard classification approach to the completed data .kNN feature value imputation is slow and requires the train- ing data to be available during classification .", "label": "", "metadata": {}, "score": "42.463238"}
{"text": "( 2011 ) proposed translator - based error correction .This approach can handle all error types by converting the learner 's sentences into the correct ones .Although th ... . \" ...Previous work on automated error recognition and correction of texts written by learners of English as a Second Language has demonstrated experimentally that training classifiers on error - annotated ESL text generally outperforms training on native text alone and that adaptation of error correction m ... \" .", "label": "", "metadata": {}, "score": "42.490223"}
{"text": "Here , we address the gene classification issue using RotBoost ensemble methodology .This method is a combination of Rotation Forest and AdaBoost techniques which in turn preserve both desirable features of an ensemble architecture , that is , accuracy and diversity .", "label": "", "metadata": {}, "score": "42.51728"}
{"text": "We also show that it is not possible to release even simple classes of queries ( such as intervals and their generalizations ) over continuous domains with worst - case utility guarantees while preserving differential privacy .In response to this , we consider a relaxation of the utility guarantee and give a privacy preserving polynomial time algorithm that for any halfspace query will provide an answer that is accurate for some small perturbation of the query .", "label": "", "metadata": {}, "score": "42.55512"}
{"text": "In this workshop , researchers present papers from many subfields : tools for automated scoring of text and speech , intelligent tutoring , readability measures , use of corpora , grammatical error detection , and tools for teachers and test developers .", "label": "", "metadata": {}, "score": "42.591072"}
{"text": "In [ 1 ] , these constraints are relaxed to obtain a convex optimization problem .However , conditions on the graph structure are given , ensuring that the class posterior of the relaxed problem is unchanged in case of re - normalization [ 4 ] , [ 5].", "label": "", "metadata": {}, "score": "42.71689"}
{"text": "Specifically , we compare two popular frameworks , based on conditional maximum likelihood ( CML ) and minimum classification error ( MCE ) , to a new framework based on margin maximization .Unlike CML and MCE , our formulation of large margin training explicitly penalizes incorrect decodings by an amount proportional to the number of mislabeled hidden states .", "label": "", "metadata": {}, "score": "42.80709"}
{"text": "In particular , this is realized by adapting ideas from Multiple Instance Learning within a boosting framework .Since the set of positive samples is well defined , we apply this concept to the negative samples extracted from the scene : Inverse Multiple Instance Learning .", "label": "", "metadata": {}, "score": "42.91854"}
{"text": "The Relief family methods are especially attractive because they may be applied in all situations , have low bias , include interaction among features , and may capture local dependencies that other methods miss [ 35 ] .The CFS method is based on test theory concepts and relies on a set of heuristics to assess the adequacy of subsets of features .", "label": "", "metadata": {}, "score": "42.923443"}
{"text": "Yet due to the non - convexity of the optimization problem , previous works usually rely on severe approximations so that it is still an open problem .We propose a new learning algorithm that relies on non - convex optimization and bundle methods and allows tackling the original optimization problem as is .", "label": "", "metadata": {}, "score": "43.123356"}
{"text": "Some studies have been reported on the application of microarray gene expression data analysis for molecular cancer classification [ 5 , 6 ] .Usually , microarray classification process comprised of two successive steps , that is , feature selection and classification .", "label": "", "metadata": {}, "score": "43.141838"}
{"text": "The probability \u03b8j and ?Page 5 .( 4 ) after introducing the joint probabil- ity of Eq .These derivatives are further used in Eq .( 7 ) resulting in the required gradient for the CG algorithm .4STRUCTURE LEARNING This section provides three structure learning heuristics - one generative and two discriminative ones - used in the experiments in Section 5 .", "label": "", "metadata": {}, "score": "43.282722"}
{"text": "In this work , motivated by large margin classifiers in machine learning , we propose a novel method to estimate continuous density hidden Markov model ( CDHMM ) for speech recognition according to the principle of maximizing the minimum muti - class separation margin .", "label": "", "metadata": {}, "score": "43.349262"}
{"text": "In this work , motivated by large margin classifiers in machine learning , we propose a novel method to estimate continuous density hidden Markov model ( CDHMM ) for speech recognition according to the principle of maximizing the minimum muti - class separation margin .", "label": "", "metadata": {}, "score": "43.349262"}
{"text": "CG - based CL learning for Bayesian networks has been introduced in [ 10].Recently , we proposed to use the extended Baum - Welch ( EBW ) algorithm [ 11 ] for optimizing the CL of Bayesian network classifiers [ 12].", "label": "", "metadata": {}, "score": "43.452377"}
{"text": "Here , we utilized 8 publicly available benchmark datasets [ 32 ] .A brief overview of these datasets is summarized in Table 1 .Preprocessing is an important step for handling gene expression data .This includes two steps : filling missing values and normalization .", "label": "", "metadata": {}, "score": "43.55922"}
{"text": "Normalization is performed so that every gene expression has mean equal to 0 and variance equal to 1 .In summary , the 8 datasets had between 2 and 5 distinct diagnostic categories , 60 - 253 instances , and 2000 - 24481 genes .", "label": "", "metadata": {}, "score": "43.588444"}
{"text": "View Article PubMed .Vapnik VN : Statistical learning theory New York , Wiley 1998 .Fan RE , Chen PH , Lin CJ : Working set selection using second order information for training support vector machines .Journal of Machine Learning Research 2005 , 6 : 1918 .", "label": "", "metadata": {}, "score": "43.621475"}
{"text": "L. Kuncheva and J. Rodriguez , \" An experimental study on rotation forest ensembles , \" in Multiple Classifier Systems , vol .4472 of Lecture Notes on Computer Science , pp .459 - 468 , 2007 .View at Google Scholar .", "label": "", "metadata": {}, "score": "43.64617"}
{"text": "When the number of features becomes very large , the filter model is usually chosen due to its computational efficiency .Here , we utilize fast correlation - based filter ( FCBF ) as previous experiments [ 30 ] suggest that FCBF is an efficient and fast feature selection algorithm for classification of high dimensional data .", "label": "", "metadata": {}, "score": "43.709156"}
{"text": "Similarly to RFE and RFVS , we perform backward elimination by discarding 0.2 proportion of genes at each iteration .Backward elimination procedure based on univariate ranking of genes with Kruskal - Wallis one - way non - parametric ANOVA [ 1 ] ( denoted as \" KW \" ) : This procedure is applied similarly to the S2N method except for it uses different univariate ranking of genes .", "label": "", "metadata": {}, "score": "43.830734"}
{"text": "sdEM could be used for learning using large scale data sets due to its stochastic approximation nature and , as we will show , because it allows to compute the natural gradient of the loss function with no extra cost [ 3].", "label": "", "metadata": {}, "score": "43.83237"}
{"text": "In short , Segal showed that there exist some data distributions where maximal unpruned trees used in the random forests do not achieve as good performance as the trees with smaller number of splits and/or smaller node size .Thus , application of random forests in general requires careful tuning of the relevant classifier parameters .", "label": "", "metadata": {}, "score": "43.898384"}
{"text": "For margin optimization we introduced a conjugate gradient algorithm .In contrast to previous work on margin optimization in probabilistic models , we kept the sum - to - one constraint which maintains the probabilistic interpretation of the network , e.g. sum- mation over missing variables is still possible .", "label": "", "metadata": {}, "score": "43.905876"}
{"text": "Deep neural networks with many hidden layers , that are trained using new methods have been shown to outperform Gaussian mixture models on a variety of speech recognition benchmarks , sometimes by a large margin .This paper provides an overview of this progress and represents the shared views of four research groups who have had recent successes in using deep neural networks for acoustic modeling in speech recognition .", "label": "", "metadata": {}, "score": "44.252693"}
{"text": "This paper investigates the methods for learning predictive classifiers based on Bayesian belief networks ( BN ) -- primarily unrestricted Bayesian networks and Bayesian multi - nets .We present our algorithms for learning these classifiers , and discuss how these methods address the overfitting problem and provide a natural method for feature subset selection .", "label": "", "metadata": {}, "score": "44.278175"}
{"text": "[ 10 ] R. Greiner , X. Su , S. Shen , and W. Zhou , \" Structural extension to logistic regression : Discriminative parameter learning of belief net classifiers , \" Machine Learning , vol .59 , pp .297 - 322 , 2005 .", "label": "", "metadata": {}, "score": "44.294872"}
{"text": "His research interests include Bayesian networks , information theory in conjunction with graphical models and statistical pattern recognition .Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .", "label": "", "metadata": {}, "score": "44.307655"}
{"text": "33 , no .12 , pp .2451 - 2464 , Dec. 2011 .[21 ] J.-H. Xue and D.M. Titterington , \" Comment on \" Discriminative versus Generative Classifiers : A Comparison of Logistic Regression and Naive Bayes \" , \" Neural Processing Letters , vol .", "label": "", "metadata": {}, "score": "44.34094"}
{"text": "Menke J , Martinez TR : Using permutations instead of student 's t distribution for p - values in paired - difference algorithm comparisons .Proceedings of 2004 IEEE International Joint Conference on Neural Networks 2004 , 2 : 1331 - 1335 .", "label": "", "metadata": {}, "score": "44.351345"}
{"text": "query .This paper investigates the distribution of this response , shows that it is asymptotically normal , and derives closed - form expressions for its mean and asymptotic variance .We also provide empirical evidence showing that the error - bars computed from our estimates are fairly accurate in practice , over a wide range of belief net structures and queries .", "label": "", "metadata": {}, "score": "44.361248"}
{"text": "As suggested by a large body of literature to date , support vector machines can be considered \" best of class \" algorithms for classification of such data .Recent work , however , suggests that random forest classifiers may outperform support vector machines in this domain .", "label": "", "metadata": {}, "score": "44.65793"}
{"text": "Here , for the sake of comparison , we experiment with both PCA and ICA transformation methods and will report on their efficiency for our gene microarray classification task later on .Gene Selection .Available training data sets for classification of cancer types generally have a fairly small sample size compared to the number of genes involved .", "label": "", "metadata": {}, "score": "44.703453"}
{"text": "A major problem in machine learning is that of inductive bias : how to choose a learner 's hypothesis space so that it is large enough to contain a solution to the problem being learnt , yet small enough to ensure reliable generalization from reasonably - sized training sets .", "label": "", "metadata": {}, "score": "44.84495"}
{"text": "Statistical and computational concerns have motivated parameter estimators based on various forms of likelihood , e.g. , joint , conditional , and pseudolikelihood .In this paper , we present a unified framework for studying these estimators , which allows us to compare their relative ( statistical ) efficiencies .", "label": "", "metadata": {}, "score": "44.89353"}
{"text": "As it can be seen , the number of selected genes for each processed gene dataset is different and depends on the choice of a feature selection algorithm .It should be noted that both mRMR and GSNR algorithms provide an ordered list of the initial genes ( features ) according to the genes importance and discrimination power .", "label": "", "metadata": {}, "score": "44.90699"}
{"text": "However these works mainly deal with ... . \" ...In this paper we compare three frameworks for discriminative training of continuous - density hidden Markov models ( CD - HMMs ) .Specifically , we compare two popular frameworks , based on conditional maximum likelihood ( CML ) and minimum classification error ( MCE ) , to a new framework based on margin maximi ... \" .", "label": "", "metadata": {}, "score": "44.984406"}
{"text": "Most current speech recognition systems use hidden Markov models ( HMMs ) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input .", "label": "", "metadata": {}, "score": "45.134315"}
{"text": "Although we show that even higher accuracy is possible using crossvalidation , we present strong evidence calling into question the validity of cross - validation evaluation using the standard dataset . \" ...This study explores the efficacy of an approach to native language identification that utilizes grammatical , rhetorical , semantic , syntactic , and cohesive function categories comprised of key n - grams .", "label": "", "metadata": {}, "score": "45.152245"}
{"text": "Gene selection methods .Even though both SVM and RF classifiers are fairly insensitive to very large number of irrelevant genes , we applied the following widely used gene selection methods in order to further improve classification performance : .Random forest - based backward elimination procedure RFVS [ 5 ] : The RFVS procedure involves iteratively fitting RFs ( on the training data ) , and at each iteration building a random forest after discarding genes with the smallest importance values .", "label": "", "metadata": {}, "score": "45.17313"}
{"text": ".. lexity of function classes using the VC - dimension ( Vapnik & Chervonenkis , 1971 ) of the function class .For a class of function and a finite set , let be the restriction of to ( that is , the set of all possible -valued functions on the domain that can be obtained from the class ) .", "label": "", "metadata": {}, "score": "45.224106"}
{"text": "We also discuss I - smoothing which is a novel technique for smoothing dis - criminative training criteria using statistics for maximum likeli - hood estimation ( MLE ) .Experiments have been performed on the Switchboard / Call Home corpora of telephone conversations with up to 265 hours of training data .", "label": "", "metadata": {}, "score": "45.44588"}
{"text": "[ 12 ] F. Pernkopf and M. Wohlmayr , \" On discriminative parameter learning of Bayesian network classifiers , \" in European Conference on Machine Learning ( ECML ) , 2009 , pp .221 - 237 .[ 13 ] P. Woodland and D. Povey , \" Large scale discriminative training of hidden Markov models for speech recognition , \" Computer Speech and Language , vol .", "label": "", "metadata": {}, "score": "45.530907"}
{"text": "2323 - 2360 , 2010 .[21 ] N. Friedman , D. Geiger , and M. Goldszmidt , \" Bayesian network classifiers , \" Machine Learning , vol .29 , pp .131 - 163 , 1997 .[ 22 ] P. Domingos and M. Pazzani , \" On the optimality of the simple Bayesian classifier under zero - one loss , \" Machine Learning , vol .", "label": "", "metadata": {}, "score": "45.56224"}
{"text": "Similar observations are reported in [ 7 ] and in this article .In this paper , we introduce maximum margin ( MM ) parameter learning for Bayesian network classifiers us- ing a conjugate gradient ( CG ) method [ 9].", "label": "", "metadata": {}, "score": "45.57716"}
{"text": "25 - 47 , 2002 .[14 ] R. Schl\u00a8 uter , W. Macherey , M. B. , and H. Ney , \" Comparison of discriminative training criteria and optimization methods for speech recognition , \" Speech Communication , vol .", "label": "", "metadata": {}, "score": "45.60666"}
{"text": "The algorithm can alternatively be viewed as automatic step size selection for gradient ascent , where the amount of computation is traded off to guarantees that each step increases the likelihood .The tradeoff makes the algorithm computationally more feasible than the earlier conditional EM .", "label": "", "metadata": {}, "score": "45.69435"}
{"text": "Abstract .The gene microarray analysis and classification have demonstrated an effective way for the effective diagnosis of diseases and cancers .However , it has been also revealed that the basic classification techniques have intrinsic drawbacks in achieving accurate gene classification and cancer diagnosis .", "label": "", "metadata": {}, "score": "45.722584"}
{"text": "11 ] O. Gopalakrishnan , D. Kanevsky , A. N ' adas , and D. Nahamoo , \" An inequality for rational functions with applications to some statistical estimation problems , \" IEEE Transactions on Information Theory , vol .37 , no . 1 , pp .", "label": "", "metadata": {}, "score": "45.864746"}
{"text": "First we provide a short intro- duction to convex relaxation for margin maximization and give details on solving the convex problem for our data .Unfortunately , Guo et al .[ 1 ] only provided results on small - scale experiments , i.e. 50 samples and up to 36 features .", "label": "", "metadata": {}, "score": "45.877888"}
{"text": "Considering that the 8 gene microarray datasets include different characteristics in terms of number of samples , genes , classes and the type of the cancer to which these data is related to ; overall , ICA - based RotBoost classifier seems to be more effective than PCA - based RotBoost .", "label": "", "metadata": {}, "score": "45.955406"}
{"text": "Hence , any local minimum is also a global minimum .There are many possibilities to solve the optimization problem in Eq .Any minimization method allowing for a nonlinear objective function and nonlinear convex inequality constraints can be used in principle .", "label": "", "metadata": {}, "score": "45.960487"}
{"text": "Table 3 : Classification results obtained by RotBoost ensemble learning against typical 8 gene datasets in terms of PCA / ICA transformation methods .As it can be noted from Table 3 , in 5 cases out of 8 , ICA - based RotBoost learners could outperform their PCA - based counterparts in terms of higher classifications accuracies and lower standard deviations .", "label": "", "metadata": {}, "score": "46.04786"}
{"text": "Overall , for learning a k - tree structure , O?Nk+2 ? are necessary .In our experiments , we consider the CR score which is directly related to the empirical risk in [ 2].The CR is the discriminative criterion that , given suffi- cient training data , most directly evaluates the objective ( small error rate ) , while an alternative would be to use a convex upper - bound on the 0/1-loss function [ 35].", "label": "", "metadata": {}, "score": "46.119892"}
{"text": "Table 3 .Number of genes selected for each microarray dataset and gene selection method .Average number of genes selected over 10 cross - validation training sets .Discussion .The results presented in this paper illustrate that SVMs offer classification performance advantages compared to RFs in diagnostic and prognostic classification tasks based on microarray gene expression data .", "label": "", "metadata": {}, "score": "46.21833"}
{"text": "K. Kira and L. Rendell , \" A practical approach to feature selection , \" in Proceedings of the 9th International Workshop on Machine Learning , pp .249 - 256 , 1992 .C. Ding and H. Peng , \" Minimum redundancy feature selection from microarray gene expression data , \" in Proceedings of the 2nd IEEE Computational Systems Bioinformatics , pp .", "label": "", "metadata": {}, "score": "46.236786"}
{"text": "Differing provisions from the publisher 's actual policy or licence agreement may be applicable .The resulting algorithm , called stochastic discriminative EM ( sdEM ) , is an online - EM - type algorithm that can train generative probabilistic models belonging to the exponential family using a wide range of discriminative loss functions , such as the negative conditional log - likelihood or the Hinge loss .", "label": "", "metadata": {}, "score": "46.243587"}
{"text": "The quest for high performance classifiers with microarray gene expression and other \" omics \" data is ongoing .Random forests have appealing theoretical and practical characteristics , however our experiments show that currently they do not exhibit \" best of class \" performance .", "label": "", "metadata": {}, "score": "46.294098"}
{"text": "Explaining Naive Bayes Classifiers .Abstract Machine - learned classifiers are important components of many data mining and knowledge discovery systems .In several application domains , an explanation of the classifier 's reasoning is critical for the classifier 's acceptance by the end - user .", "label": "", "metadata": {}, "score": "46.63064"}
{"text": "1205 - 1224 , 2004 .View at Google Scholar .T. Li , C. Zhang , and M. Ogihara , \" A comparative study of feature selection and multiclass classfication methods for tissue classification based on gene expression , \" Bioinformatics , vol .", "label": "", "metadata": {}, "score": "46.732758"}
{"text": "In this work , we addressed RotBoost ensemble classification method to cope with gene microarray classification problems .This ensemble classifier method is a combination of Rotation Forest and AdaBoost techniques which in turn preserve both desirable features of an ensemble architecture , that is , accuracy and diversity .", "label": "", "metadata": {}, "score": "46.782234"}
{"text": "Recently , many applications for Restricted Boltzmann Machines ( RBMs ) have been developed for a large variety of learning problems .However , RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed - forward neural network classifiers , ... \" .", "label": "", "metadata": {}, "score": "46.929924"}
{"text": "Experimental results clearly show that the large margin HMMs consistently outperform the conventional HMM training methods .It has been consistently observed that the large margin training method yields significant recognition error rate reduction even on top of some popular discriminative training methods .", "label": "", "metadata": {}, "score": "46.93351"}
{"text": "Sparsity of the solution is achieved by a sequential sparsification process that admits into the kernel representation a new input sample only if its feature space image can not be suffciently well approximated by combining the images of previously admitted samples .", "label": "", "metadata": {}, "score": "46.952435"}
{"text": "Our MM parameter learn- ing keeps the sum - to - one constraint of the probability distributions .Therefore , we suggest , similarly to the generatively optimized models , to sum over the missing feature values .The interpretation of marginalizing over missing features is delicate since the discriminatively op- timized parameters might not have anything in common with consistently estimated probabilities ( such as e.g. maximum likelihood estimation ) .", "label": "", "metadata": {}, "score": "47.051315"}
{"text": "This representation learning algorithm can be stacked to yield a deep architecture , and we combine it with a domain knowledge - free version of the TangentProp algorithm to encourage the classifier to be insensitive to local directions changes along the manifold .", "label": "", "metadata": {}, "score": "47.05372"}
{"text": "In the last few years substantial interest has developed within the bioinformatics community in the random forest algorithm [ 2 ] for classification of microarray and other high - dimensional molecular data [ 3 - 5 ] .Recent work [ 5 ] reported an empirical evaluation of random forests in the cancer microarray gene expression domain and concluded that random forest classifiers have predictive performance comparable to that of the best performing alternatives ( including SVMs ) for classification of microarray gene expression data .", "label": "", "metadata": {}, "score": "47.091576"}
{"text": "Let ( Xi , Yi ) . by Jonathan Baxter - Journal of Artificial Intelligence Research , 2000 . \" ...A major problem in machine learning is that of inductive bias : how to choose a learner 's hypothesis space so that it is large enough to contain a solution to the problem being learnt , yet small enough to ensure reliable generalization from reasonably - sized training sets .", "label": "", "metadata": {}, "score": "47.255142"}
{"text": ".. ase of Gaussian distributions we arrive at the same update formulas .By using the approximation , the computational complexity in the case of HMMs is O(S 3 T 2 ) , where S is the number of hidden states and T the length of the time sequence .", "label": "", "metadata": {}, "score": "47.268623"}
{"text": "Large margin learning of Continuous Density HMMs with a partially labeled dataset has been extensively studied in the speech and handwriting recognition fields .Yet due to the non - convexity of the optimization problem , previous works usually rely on severe approximations so that it is still an open ... \" .", "label": "", "metadata": {}, "score": "47.339073"}
{"text": "We experiment with various feature types , varying quantities of error - corrected data , and generic versus L1-specific adaptation to typical errors using Na\u00efve Bayes ( NB ) classifiers and develop one model which maximizes precision .We report and discuss the results for 8 models , 5 trained on the HOO data and 3 ( partly ) on the full error - coded Cambridge Learner Corpus , from which the HOO data is drawn .", "label": "", "metadata": {}, "score": "47.39657"}
{"text": "( 2 ) Should we use ( i ) the entire training sample first to learn the best parameters and then to evaluate the models , versus ( ii ) only a partition for parameter estimation and another partition for evaluation ( cross - validation ) ?", "label": "", "metadata": {}, "score": "47.52116"}
{"text": "Dietterich TG : Ensemble methods in machine learning .Proceedings of the First International Workshop on Multiple Classifier Systems New York , NY , Springer - Verlag 2000 , 1 - 15 .View Article .Segal MR : Machine Learning Benchmarks and Random Forest Regression .", "label": "", "metadata": {}, "score": "47.59095"}
{"text": "Conclusion .We found that both on average and in the majority of microarray datasets , random forests are outperformed by support vector machines both in the settings when no gene selection is performed and when several popular gene selection methods are used .", "label": "", "metadata": {}, "score": "47.59651"}
{"text": "We also obtain rates of convergence in L\u00e9vy distance of empirical margin distribution to the true margin distribution uniformly over the classes of classifiers and prove the optimality of these rates . ...Proof . ...he sample may be found , which , however , leads to very poor generalization . \" ...", "label": "", "metadata": {}, "score": "47.663906"}
{"text": "The combination of MPE and I - smoothing gives an im - provement of 1 % over MMIE and a total reduction in WER of 4.8 % absolute over the original MLE system . rk along with the Extended BaumWelch ( EBW ) algorithm [ 3 , 5 ] for MMIE parameter estimation .", "label": "", "metadata": {}, "score": "47.741024"}
{"text": "This paper first formally specifies this task , and shows how it relates to logistic regression , which corresponds to finding the optimal CL parameters for a naive - bayes structure .After analyzing its inherent ( sample and computational ) complexity , we then present a general algorithm for this task , ELR , which applies to arbitrary BN structures and which works effectively even when the training data is only partial .", "label": "", "metadata": {}, "score": "48.097794"}
{"text": "The article system builds on the elements of the system described in ( Rozovskaya and Roth , 2010c ) .The remaining three models are all Na\u00efve Bayes classifiers trained on the Google Web 1 T 5-gram corpus ( henceforth , Google corpus , ( Brants and Franz , 2006 ) ) .", "label": "", "metadata": {}, "score": "48.156536"}
{"text": "# document , which makes traditional machine learning techniques inapplicable , as they all need labeled documents of both classes .We call this problem partially supervised classification .In this paper , we show that this problem can be posed as a constrained optimization problem and that under appropriate conditions , solutions to the constrained optimization problem will give good solutions to the partially supervised classification problem .", "label": "", "metadata": {}, "score": "48.180218"}
{"text": "An example of a TAN network is shown in Figure 1(b ) .A TAN network is typically initialized as a NB net- work and additional edges between attributes are de- termined through structure learning .An extension of the TAN network is to use a k - tree , i.e. each attribute can have a maximum of k attribute nodes as parents .", "label": "", "metadata": {}, "score": "48.441032"}
{"text": "The best results obtained on supervised learning tasks often involve an unsupervised learning component , usually in an unsupervised pre - training phase .The main question investigated here is the following : why does unsupervised pre - training work so well ?", "label": "", "metadata": {}, "score": "48.46514"}
{"text": "11 , pp .1341 - 1347 , 2003 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . Y. Freund and R. E. Schapire , \" A decision - theoretic generalization of on - line learning and an application to boosting , \" Journal of Computer and System Sciences , vol .", "label": "", "metadata": {}, "score": "48.50567"}
{"text": "R. Collobert , F. Siz , J. Weston , and L. Bottou , \" Trading convexity for scalability , \" in International Conference on Machine Learning ( ICML ) , 2006 , pp .201 - 208 .C. Bishop , Neural networks for pattern recognition .", "label": "", "metadata": {}, "score": "48.56385"}
{"text": "17 , pp .4963 - 4967 , 2002 .View at Google Scholar \u00b7 View at Scopus .X. Wang , M. J. Hessner , Y. Wu , N. Pati , and S. Ghosh , \" Quantitatative quality control in microarray experiments and the application in data filtering , normalization and false positive rate prediction , \" Bioinformatics , vol .", "label": "", "metadata": {}, "score": "48.655617"}
{"text": "M. P. S. Brown , W. N. Grundy , D. Lin et al . , \" Knowledge - based analysis of microarray gene expression data by using support vector machines , \" Proceedings of the National Academy of Sciences of the United States of America , vol .", "label": "", "metadata": {}, "score": "48.682983"}
{"text": "A straightforward , generative semi - supervised method is the expectation maxim ... .by Mark D. Reid , Robert C. Williamson - JOURNAL OF MACHINE LEARNING RESEARCH , 2009 . \" ...We unify f - divergences , Bregman divergences , surrogate regret bounds , proper scoring rules , cost curves , ROC - curves and statistical information .", "label": "", "metadata": {}, "score": "48.683285"}
{"text": ", likelihood , rather than classification accuracy --- typically by first learning an appropriate graphical structure , then finding the parameters for that structure that maximize the likelihood of the data .As these parameters may not maximize the classification accuracy , ' ' discriminative parameter learners ' ' follow the alternative approach of seeking the parameters that maximize conditional likelihood ( CL ) , over the distribution of instances the BN will have to classify .", "label": "", "metadata": {}, "score": "48.707542"}
{"text": "This in turn leads to less computational cost in experiments .From Table 2 , it is obvious that FCBF achieves the highest level of dimensionality reduction by selecting the least number of discriminative genes .This is consistent with the theoretical analysis about FCBF 's ability to identify and ignore redundant features .", "label": "", "metadata": {}, "score": "48.726833"}
{"text": "5 , pp .1155 - 1178 , 2007 .[ 30 ] W. Press , S. Teukolsky , W. Vetterling , and B. Flannery , Numerical recipes in C.Cambridge Univ .Press , 1992 .[ 31 ] T. Cover and J. Thomas , Elements of information theory .", "label": "", "metadata": {}, "score": "48.84665"}
{"text": "G. Heigold , T. Deselaers , R. Schl\u00a8 uter , and H. Ney , \" Modified MMI / MPE : A direct evaluation of the margin in speech recogni- tion , \" in International Conference on Machine Learning ( ICML ) , 2008 , pp .", "label": "", "metadata": {}, "score": "48.881622"}
{"text": "In our experiments , we choose four representative feature selection algorithms , that is , ReliefF , correlation - based filter selection ( CFS ) , minimum redundancy maximum relevance ( mRMR ) , and general signal to noise ratio ( GSNR ) in comparison with FCBF .", "label": "", "metadata": {}, "score": "48.974747"}
{"text": "The most popular ensemble methods utilize a base classification algorithm to differently permutated training sets .Examples of these techniques include AdaBoost , Bagging , Random Subspace , Random Forest , and Rotation Forest [ 19 ] .AdaBoost has become a very popular choice for its simplicity and adaptability [ 20 ] .", "label": "", "metadata": {}, "score": "49.00639"}
{"text": "We then present a general algorithm for this task , ELR , that applies to arbitrary BN structures and that works effectively even when given incomplete training data .Unfortunately , ELR is not guaranteed to find the parameters that optimize conditional likelihood ; moreover , even the optimal - CL parameters need not have minimal classification error .", "label": "", "metadata": {}, "score": "49.007477"}
{"text": "12 , pp .6745 - 6750 , 1999 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .R. Blanco , P. Larra\u00f1aga , I. Inza , and B. Sierra , \" Gene selection for cancer classification using wrapper approaches , \" International Journal of Pattern Recognition and Artificial Intelligence , vol .", "label": "", "metadata": {}, "score": "49.019547"}
{"text": "The maximum margin optimized Bayesian network structures achieve good classification performance .The feature that is most informative about C is selected first .The next node in the order is the node that is most informative about C conditioned on the first node .", "label": "", "metadata": {}, "score": "49.102028"}
{"text": "The mRMR criterion computes both the redundancy between features and the relevance of each feature .Redundancy is computed by the mutual information ( MI ) between pairs of features whereas relevance is measured by the MI between each feature and the class labels .", "label": "", "metadata": {}, "score": "49.201126"}
{"text": "Exp ... . by Maria - florina Balcan , Alina Beygelzimer , John Langford - In ICML , 2006 . \" ...We state and analyze the first active learning algorithm which works in the presence of arbitrary forms of noise .", "label": "", "metadata": {}, "score": "49.2201"}
{"text": "In this paper , we present our experiments on lightly supervised discriminative training with large amounts of broadcast news data for which only closed caption transcriptions are available ( TDT data ) .In particular , we use language models biased to the closedcaption transcripts to recognise the audio data , and the recognised transcripts are then used as the training transcriptions for acoustic model training .", "label": "", "metadata": {}, "score": "49.253616"}
{"text": "36 ] F. Pernkopf and M. Wohlmayr , \" Stochastic margin - based structure learning of Bayesian network classifiers , \" Laboratory of Signal Processing and Speech Communication , Graz University of Tech- nology , Tech .Rep. , 2011 .[ 37 ] F. Pernkopf and J. Bilmes , \" Order - based discriminative structure learning for Bayesian network classifiers , \" in International Sympo- sium on Artificial Intelligence and Mathematics , 2008 .", "label": "", "metadata": {}, "score": "49.366196"}
{"text": "A Bayesian net ( BN ) is more than a succinct way to encode a probabilistic distribution ; it also corresponds to a function used to answer queries .A BN can therefore be evaluated by the accuracy of the answers it returns .", "label": "", "metadata": {}, "score": "49.457546"}
{"text": "Conclusion .The primary contribution of the present work is that we conducted the most comprehensive comparative benchmarking of random forests and support vector machines to date , using 22 diagnostic and outcome prediction datasets .Our hypothesis that in previously reported work , research design limitations may have biased the comparison of classifiers in favour of random forests , was verified .", "label": "", "metadata": {}, "score": "49.466553"}
{"text": "50 - 66 . [17 ] L. Lamel , R. Kassel , and S. Seneff , \" Speech database development : Design and analysis of the acoustic - phonetic corpus , \" in DARPA Speech Recognition Workshop , Report No .", "label": "", "metadata": {}, "score": "49.498116"}
{"text": "10 , pp .906 - 914 , 2000 .View at Google Scholar \u00b7 View at Scopus .N. Friedman , M. Linial , I. Nachman , and D. Pe'er , \" Using Bayesian networks to analyze expression data , \" in Proceedings of the 4th Annual International Conference on Computational Molecular Biology ( RECOMB ' 00 ) , pp .", "label": "", "metadata": {}, "score": "49.51725"}
{"text": "103 - 130 , 1997 .[ 23 ] J. Bilmes , \" Dynamic Bayesian multinets , \" in 16th Inter .Conf . of Uncertainty in Artificial Intelligence ( UAI ) , 2000 , pp .38 - 45 .[ 24 ] R. Cowell , A. Dawid , S. Lauritzen , and D. Spiegelhalter , Proba- bilistic networks and expert systems .", "label": "", "metadata": {}, "score": "49.531334"}
{"text": "Maximizing ? where \u03ba parameterizes this loss function .Similar as in [ 29 ] we empirically identified typical values of \u03ba in the range between 0.01 and 0.5 .Tuning parameter \u03ba in the given range has a moderate impact on the performance ( as we show in experiments ) .", "label": "", "metadata": {}, "score": "49.561916"}
{"text": "Index Terms - speech recognition , discriminative training , MMI , MCE , large margin , phoneme recognition 1 . ...n more explicitly by rewriting eq .( 5 )The CML estimator in eq .( 6 ) \u03b8 p(Xn)p(Yn )", "label": "", "metadata": {}, "score": "49.61715"}
{"text": "P - values shown with boldface denote statistically significant differences between classification methods at the 0.05 \u03b1 level .Using gene selection .Six classification performance estimates have been produced for each classifier and dataset ( 5 estimates corresponding to various gene selection methods and one estimate corresponding to using no gene selection ) .", "label": "", "metadata": {}, "score": "49.73214"}
{"text": "These are combined in order to obtain a discriminative model .To enable to adapt to changing environments these classifiers are learned on - line ( i.e. , boosting ) .Continuously learning ( 24 hours a day , 7 days a week ) requires a stable system .", "label": "", "metadata": {}, "score": "49.780113"}
{"text": "( 6 )This function is differentiable and can be optimized by CG methods . 3.2CG Algorithm We use a conjugate gradient algorithm with line - search [ 30 ] which requires both the objective function ( 6 ) and its derivative .", "label": "", "metadata": {}, "score": "49.85173"}
{"text": "More details are given in [ 24 ] , [ 25 ] and references therein .Page 4 . 4 3 TER LEARNING DISCRIMINATIVE MARGIN - BASED PARAME-The proposed CG - based maximum margin learning al- gorithm is developed in the following sections . xexp(\u03b7f(x ) ) ] 1 \u03b7pa- \u0398. For a non - separable \u0398close to zero .", "label": "", "metadata": {}, "score": "50.13138"}
{"text": "Efron B , Tibshirani R : Improvements on cross - validation : the .632 + bootstrap method .Journal of the American Statistical Association 1997 , 92 : 548 - 560 .View Article .Hastie T , Tibshirani R , Friedman JH : The elements of statistical learning : data mining , inference , and prediction New York , Springer 2001 .", "label": "", "metadata": {}, "score": "50.21858"}
{"text": "[35 ] P. Bartlett , M. Jordan , and J. McAuliffe , \" Convexity , classification , and risk bounds , \" Journal of the American Statistical Association , vol .101 , no .473 , pp .138 - 156 , 2006 .", "label": "", "metadata": {}, "score": "50.27182"}
{"text": "Indeed , to verify the efficiency of the proposed method on gene - related data , 8 publically available gene microarray benchmark datasets are analyzed .To this end , we accomplish a comparative study of RotBoost efficiency against several other ensemble and single classifier systems including AdaBoost , Bagging , Rotation Forest single tree , and support vector machines ( SVMs ) .", "label": "", "metadata": {}, "score": "50.340515"}
{"text": "EBW offers an EM - like parameter update .Page 2 . 2 In fact , it is shown in [ 14 ] that the EBW algorithm resem- bles the gradient descent algorithm for discriminatively optimizing Gaussian mixtures using a particular step size choice in the gradient descent method .", "label": "", "metadata": {}, "score": "50.498543"}
{"text": "Introduction .Previous studies have shown that gene microarray data analysis is a powerful and revolutionary tool for biological and medical researches by allowing the simultaneous monitoring of the expression levels of tens of thousands of genes [ 1 ] .This is done by measuring the signal intensity of fluorescing molecules attached to DNA species that are bound to complementary strands of DNA localized to the surface of the microarray .", "label": "", "metadata": {}, "score": "50.544765"}
{"text": "It is worth to note that the feature selection is then carried out using only the training samples .Finally , the test error ( classification accuracy ) is estimated on the unseen test samples using ( 3 ) .Table 3 presents the RotBoost mean classification accuracy against the considered 8 gene datasets when transformation matrix is chosen to be either PCA or ICA where the values following \" \u00b1 \" denote the related standard deviations .", "label": "", "metadata": {}, "score": "50.55137"}
{"text": "We demonstrate the stability in a long - term experiment by running the system for a whole week , which shows a stable performance over time .In addition , we compare the proposed approach to state - of - the - art methods in the field of person and car detection .", "label": "", "metadata": {}, "score": "50.654915"}
{"text": "In our experiments we were not able to find a setting such that the achieved classification rate on the test set is larger than 90.90 % which is much smaller than the classification rate of CG - MM .The large computational requirements of CVX - MM are caused by the convex formulation in Eq .", "label": "", "metadata": {}, "score": "50.781242"}
{"text": "The NB network assumes that all the attributes are condition- ally independent given the class label .This means that , given C , any subset of X is independent of any other disjoint subset of X. As reported in the literature [ 21 ] , [ 22 ] , the performance of the NB classifier is surprisingly good even if the conditional independence assumption between attributes is unrealistic or even false in most of the data .", "label": "", "metadata": {}, "score": "50.852165"}
{"text": "Chen X , Zeng X , van Alphen D : Multi - class feature selection for texture classification .Pattern Recognition Letters 2006 , 27 : 1685 - 1691 .View Article .Science 1999 , 286 : 531 - 537 .", "label": "", "metadata": {}, "score": "50.906536"}
{"text": "Classification performance of SVMs and RFs with gene selection .The performance is estimated using area under ROC curve ( AUC ) for binary classification tasks and relative classifier information ( RCI ) for multicategory tasks .Table 2 .Comparison of classification performance of SVMs and RFs with gene selection .", "label": "", "metadata": {}, "score": "51.11199"}
{"text": "119 - 139 , 1997 .View at Google Scholar \u00b7 View at Scopus .C.-H. Yang , L.-Y. Chuang , and C.-H. Yang , \" IG - GA : a hybrid filter / wrapper method for feature selection of microarray data , \" Journal of Medical and Biological Engineering , vol .", "label": "", "metadata": {}, "score": "51.120583"}
{"text": "15 , pp .2429 - 2437 , 2004 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .I. Kononenko , \" Estimating attributes : analysis and extensions of RELIEF , \" in Proceedings of the European Conference on Machine Learning , pp .", "label": "", "metadata": {}, "score": "51.141106"}
{"text": "Russell Greiner , Xiaoyuan Su , Bin Shen , Wei Zhou Machine Learning Journal 59:3 , 2005 .Bayesian belief nets ( BNs ) are often used for classification tasks --- typically to return the most likely class label for each specified instance .", "label": "", "metadata": {}, "score": "51.226143"}
{"text": "[ 1 ] proposed to solve the maximum mar- gin parameter learning problem for Bayesian network classifiers by reformulating it as a convex optimization problem .Page 11 .The parameter B can be used to control the slack effect ( similar as parameter C in SVMs ) .", "label": "", "metadata": {}, "score": "51.25659"}
{"text": "To avoid the curse of dimensionality problem , gene selection plays a crucial role in DNA microarray analysis .Another important reason to reduce dimensionality is to help biologists to identify the underlying mechanism that relates gene expression to diseases .Indeed , the microarray data is associated with various uncertainties such as microarray data , gathering process which include fabrication , hybridization and image processing .", "label": "", "metadata": {}, "score": "51.35685"}
{"text": "The GSNR is a measure of the ratio between intergroup and intragroup variations .Higher GSNR values indicate higher discrimination power for the gene .GSNR selects .In order to reduce the computational complexity of the problem at hand and select the most informative genes , we run all 5 feature selection algorithms against each dataset and obtain the number of selected features for each algorithm .", "label": "", "metadata": {}, "score": "51.386894"}
{"text": "We show that A2 achieves an exponential improvement ... \" .We state and analyze the first active learning algorithm which works in the presence of arbitrary forms of noise .The algorithm , A2 ( for Agnostic Active ) , relies only upon the assumption that the samples are drawn i.i.d . from a fixed distribution .", "label": "", "metadata": {}, "score": "51.44345"}
{"text": "Unlike earlier work on max - margin Markov networks , our ap ... \" .We study the problem of parameter estimation in continuous density hidden Markov models ( CD - HMMs ) for automatic speech recognition ( ASR ) .As in support vector machines , we propose a learning algorithm based on the goal of margin maximization .", "label": "", "metadata": {}, "score": "51.471134"}
{"text": "Maximizing CL is tightly connected to minimizing the empirical risk .Unfortunately , CL does not decompose as ML does .Consequently , there is no closed - form solution .For the sake of completeness , we shortly sketch the CG algorithm for CL optimization in the Appendix .", "label": "", "metadata": {}, "score": "51.51219"}
{"text": "To cope with these challenges and to develop a more robust and accurate learning method , ensemble learning methodology is utilized .The datasets are first preprocessed , and then to reduce the computational complexity and select the most informative genes , FCBF is applied to these datasets .", "label": "", "metadata": {}, "score": "51.53192"}
{"text": "Ph .D.Thesis , Technischen Universit\u00e4t Berlin , School of Computer Science 1999 .Furey TS , Cristianini N , Duffy N , Bednarski DW , Schummer M , Haussler D : Support vector machine classification and validation of cancer tissue samples using microarray expression data .", "label": "", "metadata": {}, "score": "51.655514"}
{"text": "The data set is split into 8000 images for training and 3000 for testing .Each digit is represented as a 16 \u00d7 16 grayscale image , where again each pixel is considered as feature .The reason is that the final step of MFCC feature extraction involves a dis- crete cosine transform , i.e. the features are decorrelated .", "label": "", "metadata": {}, "score": "51.789604"}
{"text": "Statnikov A , Tsamardinos I , Dosbayev Y , Aliferis CF : GEMS : a system for automated cancer diagnosis and biomarker discovery from microarray gene expression data .Int J Med Inform 2005 , 74 : 491 - 503 .View Article PubMed .", "label": "", "metadata": {}, "score": "51.899395"}
{"text": "The structures considered are NB and TAN - CMI .[ Show abstract ] [ Hide abstract ] ABSTRACT : Bayesian network classifier ( BNCs ) are typically implemented on nowadays desktop computers .However , many real world applications require classifier implementation on embedded or low power systems .", "label": "", "metadata": {}, "score": "51.993412"}
{"text": "Fifth , no statistical comparison among classifiers has been performed .Furthermore , .632 + bootstrap is currently not developed for performance metrics other than proportion of correct classifications .We hypothesize that these apparent methodological biases of prior work have compromised its conclusions and the question of whether random forests indeed outperform SVMs for classification of microarray gene expression data is not convincingly answered .", "label": "", "metadata": {}, "score": "52.053238"}
{"text": "Handling missing features with NB classifiers is easy since we can simply neglect the conditional probability of the missing feature Zj in Eq .( 1 ) , i.e. the joint probability is the product of the available features only .Fig .", "label": "", "metadata": {}, "score": "52.082253"}
{"text": "F. Sha and L. Saul , \" Comparison of large margin training to other discriminative methods for phonetic recognition by hidden Markov models , \" in IEEE International Conference on Acoustics , Speech , and Signal Processing ( ICASSP ) , 2007 , pp .", "label": "", "metadata": {}, "score": "52.173634"}
{"text": "Further , we used the same data set partitioning for various learning algorithms .Speech frames are classified into either four or six classes using 110134 and 121629 samples , respectively .Each sample is represented by 20 mel- frequency cepstral coefficients ( MFCCs ) and wavelet- based features .", "label": "", "metadata": {}, "score": "52.283722"}
{"text": "Page 10 .10 0 0.10.2 0.30.4 0.5 \u03ba 0.60.70.8 0.91 88.6 88.8 89 89.2 89.4 Classification Performance [ % ] Fig .5 .NB classifiers outperform NB - ML in the case of up to 150 missing features .Furthermore , we present results for SVMs where first imputation methods are used to complete missing feature values in the data .", "label": "", "metadata": {}, "score": "52.350967"}
{"text": "Classification performance evaluation metrics .We used two classification performance metrics .For binary tasks , we used the area under the ROC curve ( AUC ) which was computed from continuous outputs of the classifiers ( distances from separating hyperplane for SVMs and outcome probabilities for RFs ) [ 8 ] .", "label": "", "metadata": {}, "score": "52.39315"}
{"text": "[28 ] P. Huber , \" Robust estimation of a location parameter , \" Annals of Statistics , vol .53 , pp .73 - 101 , 1964 .[29 ] O. Chapelle , \" Training a support vector machine in the primal , \" Neural Computation , vol .", "label": "", "metadata": {}, "score": "52.497154"}
{"text": "View Article .Sindhwani V , Bhattacharyya P , Rakshit S : Information Theoretic Feature Crediting in Multiclass Support Vector Machines .Proceedings of the First SIAM International Conference on Data Mining 2001 .Harrell FE Jr. , Lee KL , Mark DB : Multivariable prognostic models : issues in developing models , evaluating assumptions and adequacy , and measuring and reducing errors .", "label": "", "metadata": {}, "score": "52.55744"}
{"text": "The x - axis denotes the number of missing features .The curves are the average over 100 classifications of the test data with uniformly at random selected missing features .We use exactly the same missing features for each classifier .", "label": "", "metadata": {}, "score": "52.562424"}
{"text": "Figure 2 illustrates these histograms against the typical Lung cancer dataset .Figure 2 : Kappa error diagrams for the Lung dataset using different ensemble algorithms .We perform these experiments on all 8 gene datasets , and overall the ICA and PCA RotBoost methods perform best in terms of accuracy with average classification accuracy about 98.0 % and 96.3 % , respectively .", "label": "", "metadata": {}, "score": "52.57479"}
{"text": "We arbitrarily choose 5000 samples of each class to learn the classifier .This remote sensing application is in particular interesting for our classifiers since spectral bands might be missing or should be ne- glected due to atmospheric effects .For example radiation within the visible range should be neglected in case of clouds or darkness .", "label": "", "metadata": {}, "score": "52.616737"}
{"text": "We refer to this method as \" RFVS1 .\" Even though RFE was originally introduced as a method for binary classification problems , it can be trivially extended to multiclass case by using binary SVM models in \" one - versus - rest \" fashion ( e.g. , see [ 27 ] ) .", "label": "", "metadata": {}, "score": "52.718594"}
{"text": "Abstract .Visual tracking is a challenging problem , as an object may change its appearance due to viewpoint variations , illumination changes , and occlusion .Also , an object may leave the field of view and then reappear .In order to track and reacquire an unknown object with limited labeling data , we propose to learn these changes online and build a model that describes all seen appearance while tracking .", "label": "", "metadata": {}, "score": "52.770214"}
{"text": "The average performance of SVMs is 0.775 AUC and 0.860 RCI in binary and multicategory classification tasks , respectively .The average performance of RFs in the same tasks is 0.742 AUC and 0.803 RCI .Classification performance of SVMs and RFs without gene selection .", "label": "", "metadata": {}, "score": "52.871834"}
{"text": "The average CR over the six TIMIT-4/6 data sets is determined by weighting the CR of each data set with the number of samples in the test set .These values are accumulated and normalized by the total amount of samples in all test sets .", "label": "", "metadata": {}, "score": "52.92408"}
{"text": "html 9 .Page 9 . 9 TABLE 4 Model complexity for best Bayesian network ( BN ) and SVM .Classification performance on MNIST assuming missing features .The x - axis denotes the number of miss- ing features and the shaded regions correspond to the standard deviation over 100 classifications : ( a ) Different structure learning methods with generative parameter- ization ; ( b ) Different discriminative parameter learning methods on NB structure .", "label": "", "metadata": {}, "score": "53.05244"}
{"text": "Therefore , we limit the experiments to NB and TAN structures .Many other network topologies have been suggested in the past - a good overview is provided in [ 26].The tree - width of a graph is defined as the size ( i.e. number of variables ) of the largest clique of the moralized and triangulated directed graph minus one .", "label": "", "metadata": {}, "score": "53.080845"}
{"text": "Moreover , the improvements obtained with MCE turn out to be in the same order of magnitude as the performance gains obtained with the MWE criterion .den Markov Models in many state - of - the - art speech recognition systems .", "label": "", "metadata": {}, "score": "53.338318"}
{"text": "We used discriminatively trained CD - HMMs from all three frameworks to build phonetic recognizers on the TIMIT speech corpus .The different recognizers employed exactly the same acoustic front end and hidden state space , thus enabling us to isolate the effect of different cost functions , parameterizations , and numerical optimizations .", "label": "", "metadata": {}, "score": "53.60199"}
{"text": "[21 ] introduced the TAN classifier .A TAN is based on structural augmentations of the NB network : Additional edges are added between attributes .Each attribute may have at most one other attribute as an additional parent which means that the tree - width of the attribute induced sub - graph is unity2 , i.e. we have to learn a 1-tree over the attributes .", "label": "", "metadata": {}, "score": "53.70986"}
{"text": "IEEE INT .CONF .ACOUST . , SPEECH , SIGNAL PROCESS , 2004 . \" ...In this paper , we present our experiments on lightly supervised discriminative training with large amounts of broadcast news data for which only closed caption transcriptions are available ( TDT data ) .", "label": "", "metadata": {}, "score": "53.81499"}
{"text": "Methods .Microarray datasets and classification tasks .Gene expression microarray datasets used in the present work are described in Table 4 .All 22 datasets span the domain of cancer ; 14 datasets correspond to diagnostic tasks ( and denoted with prefix \" Dx \" ) and 8 are concerned with clinical outcome prediction ( and denoted with \" Px \" ) .", "label": "", "metadata": {}, "score": "53.90982"}
{"text": "INDEX TERMS .Uncertainty , Data models , Adaptation models , Training , Support vector machines , Entropy , Approximation methods , discriminative models , Active learning , rare class discovery , imbalanced learning , classification , generative models .CITATION .", "label": "", "metadata": {}, "score": "53.983887"}
{"text": "[ 2 ] [ 3 ] Wiley & Sons , 1998 .[ 4 ] [ 5 ] [ 6 ] [ 7 ] [ 8 ] [ 9 ] Springer Verlag , 1999 .[ 27 ] B. Sch\u00a8 olkopf and A. Smola , Learning with kernels : Support Vector Machines , regularization , optimization , and beyond .", "label": "", "metadata": {}, "score": "54.01658"}
{"text": "The datasets contain 50 - 308 samples and 2,000 - 24,188 variables ( genes ) after data preparatory steps described in [ 1 ] .Similarly , all prognostic datasets were obtained from the links given in the primary study for each dataset .", "label": "", "metadata": {}, "score": "54.028862"}
{"text": "For .Table 5 summarizes the kappa - error values for typical Lung cancer dataset with FCBF gene selection method in terms of the centroids of different ensembles .From this table it is clear that the ICA - based RotBoost provides the highest pairwise accuracy , and the second best accuracy is achieved by PCA - based RotBoost .", "label": "", "metadata": {}, "score": "54.123184"}
{"text": "We also discuss I - smoothing which is a novel technique for s ... \" .In this paper we introduce the Minimum Phone Error ( MPE ) and Minimum Word Error ( MWE ) criteria for the discriminative train - ing of HMM systems .", "label": "", "metadata": {}, "score": "54.3162"}
{"text": "Conf .Comput .Vision . \" ...Abstract .Visual tracking is a challenging problem , as an object may change its appearance due to viewpoint variations , illumination changes , and occlusion .Also , an object may leave the field of view and then reappear .", "label": "", "metadata": {}, "score": "54.39498"}
{"text": "View at Scopus .G. J. Gordon , R. V. Jensen , L.-L. Hsiao et al . , \" Translation of microarray data into clinically relevant cancer diagnostic tests using gene expression ratios in lung cancer and mesothelioma , \" Cancer Research , vol .", "label": "", "metadata": {}, "score": "54.41736"}
{"text": "TABLE 1 Classification results in [ % ] for MNIST data with standard deviation .Best parameter learning results for each structure are emphasized using bold font .Best parameter learning results for each structure are emphasized using bold font .Best results for each data set are emphasized using bold font . Discrimina-", "label": "", "metadata": {}, "score": "54.43387"}
{"text": "The experimental results demonstrate that using the proposed approach state - of - the - art detection results can by obtained , however , showing superior classification results in presence of non - moving objects .TransientBoost ( Sternig , Godec , Roth , Bischof ) .", "label": "", "metadata": {}, "score": "54.489838"}
{"text": "The microarray data measures the expressions of tens of thousands of genes , producing a feature vector that is high in dimensionality and that contains much irrelevant information .This dimensionality degrades classification performance .Moreover , datasets typically contain few samples for training ( e.g. , lung dataset [ 12 ] contains 12535 genes and only 181 samples ) , leading to the curse of dimensionality problem .", "label": "", "metadata": {}, "score": "54.49128"}
{"text": "23 - 28 , 2010 .View at Google Scholar \u00b7 View at Scopus .L. Yu and H. Liu , \" Efficient feature selection via analysis of relevance and redundancy , \" Journal of Machine Learning Research , vol .5 , no .", "label": "", "metadata": {}, "score": "54.52911"}
{"text": "25 - 57 , 2006 .[ 41 ] L. Biegler and V. Zavala , \" Large - scale nonlinear programming using IPOPT : An integrating framework for enterprise - wide dy- namic optimization , \" Computers & Chemical Engineering , vol .", "label": "", "metadata": {}, "score": "54.5411"}
{"text": "287 - 310 , 2001 .[ 15 ] F. Pernkopf and M. Wohlmayr , \" Maximum margin Bayesian network classifiers , \" Institute of Signal Processing and Speech Communication , Graz University of Technology , Tech .Rep. , 2010 .", "label": "", "metadata": {}, "score": "54.656116"}
{"text": "ExplainD applies to many widely used classifiers , including linear discriminants and many additive models .We demonstrate our ExplainD framework using implementations of naive Bayes , linear support vector machine , and logistic regression classifiers on example applications .We demonstrate the effectiveness of ExplainD in the context of a deployed web - based system ( Proteome Analyst ) and using a downloadable Python - based implementation .", "label": "", "metadata": {}, "score": "54.671505"}
{"text": "View Article PubMed .Breiman L : Random forests .Machine Learning 2001 , 45 : 5 - 32 .View Article .Wu B , Abbott T , Fishman D , McMurray W , Mor G , Stone K , Ward D , Williams K , Zhao H : Comparison of statistical methods for classification of ovarian cancer using mass spectrometry data .", "label": "", "metadata": {}, "score": "54.795174"}
{"text": "While OFE typically produces more accurate classifiers with GBN ( vs. NB ) , we show that ELR does not , when the training data is not sufficient for the GBN structure learner to produce a good model .Our empirical studies also suggest that the better the BN structure is , the less advantages ELR has over OFE , for classification purposes .", "label": "", "metadata": {}, "score": "54.972122"}
{"text": "Table 1 .Comparison of classification performance of SVMs and RFs without gene selection .The performance is estimated using area under ROC curve ( AUC ) for binary classification tasks and relative classifier information ( RCI ) for multicategory tasks .", "label": "", "metadata": {}, "score": "55.15789"}
{"text": "While we ... .by Fei Sha , Lawrence K. Saul - in Advances in Neural Information Processing Systems 19 , 2007 . \" ...We study the problem of parameter estimation in continuous density hidden Markov models ( CD - HMMs ) for automatic speech recognition ( ASR ) .", "label": "", "metadata": {}, "score": "55.182007"}
{"text": "The CR evaluation can be accelerated by techniques presented in [ 20 ] , [ 36].In the experiments this greedy heuristic is labeled as TAN- CR for 1-tree structures .Recently , the maximum margin score was introduced for discriminatively optimizing the structure of Bayesian network classifiers [ 36].", "label": "", "metadata": {}, "score": "55.20392"}
{"text": "The neighbors of a sample with missing features are determined by the Euclidean distance in the relevant subspace .In the special case where k equals the number of training instances M this method is identical to mean value imputation .As shown in Figure 6 , mean value imputation de-", "label": "", "metadata": {}, "score": "55.445114"}
{"text": "262 - 267 , 2000 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .T. S. Furey , N. Cristianini , N. Duffy , D. W. Bednarski , M. Schummer , and D. Haussler , \" Support vector machine classification and validation of cancer tissue samples using microarray expression data , \" Bioinformatics , vol .", "label": "", "metadata": {}, "score": "55.566956"}
{"text": "The online version of this article ( doi : 10 .1186/\u200b1471 - 2105 - 9 - 319 ) contains supplementary material , which is available to authorized users .Background .Gene expression microarrays are becoming increasingly promising for clinical decision support in the form of diagnosis and prediction of clinical outcomes of cancer and other complex diseases .", "label": "", "metadata": {}, "score": "55.87617"}
{"text": "The system consists of five components and targets five types of common grammatical mistakes made by English as Second Language writers .We describe our underlying approach , which relates to our previous work , and describe the novel aspects of the system in more detail .", "label": "", "metadata": {}, "score": "55.906174"}
{"text": "99 , no .10 , pp .6562 - 6566 , 2002 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus \" ...In this paper we introduce the Minimum Phone Error ( MPE ) and Minimum Word Error ( MWE ) criteria for the discriminative train - ing of HMM systems .", "label": "", "metadata": {}, "score": "56.008476"}
{"text": "10 , pp .1104 - 1125 , 2006 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . U.Alon , N. Barka , D. A. Notterman et al . , \" Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays , \" Proceedings of the National Academy of Sciences of the United States of America , vol .", "label": "", "metadata": {}, "score": "56.042114"}
{"text": "In the present paper we identify methodological biases of prior work comparing random forests and support vector machines and conduct a new rigorous evaluation of the two algorithms that corrects these limitations .Our experiments use 22 diagnostic and prognostic datasets and show that support vector machines outperform random forests , often by a large margin .", "label": "", "metadata": {}, "score": "56.13887"}
{"text": "Additionally , we perform analysis for margin - optimized tree augmented network ( TAN ) structures which outperform generatively optimized TAN structures in terms of CR and robustness .Full - text \u00b7 Article \u00b7 Jan 2014 \u00b7 IEEE Transactions on Pattern Analysis and Machine Intelligence .", "label": "", "metadata": {}, "score": "56.15985"}
{"text": "BMC Genomics 2006 , 7 : 278 .View Article PubMed .Hammer B , Gersmann K : A Note on the Universal Approximation Capability of Support Vector Machines .Neural Processing Letters 2003 , 17 : 43 - 53 .View Article .", "label": "", "metadata": {}, "score": "56.163303"}
{"text": "Firstly , we show this type of large margin HMM estimation problem can be formulated as a constrained minimax optimization problem .Secondly , by imposing different constraints to the minimax problem , we propose three solutions to the large margin HMM estimation problem , namely the iterative localized optimization method , the constrained joint optimization method and the semidefinite pro - gramming ( SDP ) method .", "label": "", "metadata": {}, "score": "56.177795"}
{"text": "Indeed , from individual accuracy values , we observe that for all the datasets except SRBCT , FCBF can highly increase the overall gene classification accuracy .On the other hand , CFS method achieves the second best classification accuracy , and both relief and GSNR accomplish more than 90 % average accuracy .", "label": "", "metadata": {}, "score": "56.254753"}
{"text": "374 - 386 , Feb. 2013 , doi:10.1109/TKDE.2011.231 .[5 ] T. Hospedales , J. Li , S. Gong , and T. Xiang , \" Identifying Rare and Subtle Behaviours : A Weakly Supervised Joint Topic Model , \" IEEE Trans .", "label": "", "metadata": {}, "score": "56.27894"}
{"text": "..The covering nu ...We present a maximum margin parameter learning algorithm for Bayesian network classifiers using a conjugate gradient ( CG ) method for optimization .In contrast to previous approaches , we maintain the normalization constraints on the parameters of the Bayesian network during optimization , i.e. , the probabilistic interpretation of the model is not lost .", "label": "", "metadata": {}, "score": "56.39476"}
{"text": "13 REFERENCES [ 1]Y. Guo , D. Wilkinson , and D. Schuurmans , \" Maximum margin Bayesian networks , \" in International Conference on Uncertainty in Artificial Intelligence ( UAI ) , 2005 , pp .233 - 242 .V. Vapnik , Statistical learning theory .", "label": "", "metadata": {}, "score": "56.40909"}
{"text": "It is applicable to hidden variable models where the distributions are from the exponential family .The algorithm can alternatively be viewed as automatic step size selection for ... \" .We introduce an expectation maximizationtype ( EM ) algorithm for maximum likelihood optimization of conditional densities .", "label": "", "metadata": {}, "score": "56.64067"}
{"text": "cided to use the large scale solver IPOPT [ 40 ] which shows good performance in several applications ( see e.g. [ 41]).10IPOPT applies an interior - point method [ 43 ] to solve the problem in ( 9 ) .It requires the objective func- tion , its gradient , the constraint functions , the Jacobian of the constraint functions , and the second derivative of the Lagrangian function .", "label": "", "metadata": {}, "score": "56.802277"}
{"text": "RotBoost offers a potential computational advantage over AdaBoost in that it has the ability to execute in parallel .In fact , each subensemble classifier formed by AdaBoost can be learned independently of the other ones .Pseudocode 1 illustrates this algorithm . . .", "label": "", "metadata": {}, "score": "57.041325"}
{"text": "Bagging also presents the second best diversity but low classification accuracy .Table 5 : Kappa error diagram for Lung dataset ( the centroids of ensembles ) .When outlining the kappa - error diagrams , the diagrams of different ensembles are greatly overlapping , and the distances between the centroids are small .", "label": "", "metadata": {}, "score": "57.279335"}
{"text": "BMC Bioinformatics 2006 , 7 : 3 .View Article PubMed .Rifkin R , Mukherjee S , Tamayo P , Ramaswamy S , Yeang CH , Angelo M , Reich M , Poggio T , Lander ES , Golub TR , Mesirov JP : An analytical method for multi - class molecular cancer classification .", "label": "", "metadata": {}, "score": "57.40104"}
{"text": "The x - axis denotes the number of missing features .We average the performances over 100 classifications of the test data with randomly miss- ing features .The standard deviation indicates that the resulting differences are significant for a moderate num- ber of missing features .", "label": "", "metadata": {}, "score": "57.494896"}
{"text": "We also give an efficient algorithm for releasing synthetic data for the class of interval queries and axis - aligned rectangles of constant dimension over discrete domains . \" ...Dedicated to A.V. Skorohod on his seventieth birthday We prove new probabilistic upper bounds on generalization error of complex classifiers that are combinations of simple classifiers .", "label": "", "metadata": {}, "score": "57.54715"}
{"text": "Support vector machine classifiers .Extensive applications literature in text categorization , image recognition and other fields also shows the excellent empirical performance of this classifier in many more domains .The underlying idea of SVM classifiers is to calculate a maximal margin hyperplane separating two classes of the data .", "label": "", "metadata": {}, "score": "57.66487"}
{"text": "We obtain competitive results for phonetic recognition on the TIMIT speech corpus .The learning algorithms in these frameworks optimize discriminative criteria that more closely track actual error rates , as opposed to the EM algorithm for maximum likelihood estimation .These algor ... . by Geoffrey Hinton , Li Deng , Dong Yu , George Dahl , Abdel - rahman Mohamed , Navdeep Jaitly , Vincent Vanhoucke , Patrick Nguyen , Tara Sainath , Brian Kingsbury .", "label": "", "metadata": {}, "score": "57.95121"}
{"text": "Our generative model captures prior knowledge about the pedestrian class in te ... \" .This paper presents a novel approach to pedestrian classification which involves utilizing the synthesized virtual samples of a learned generative model to enhance the classification performance of a discriminative model .", "label": "", "metadata": {}, "score": "58.219128"}
{"text": "R News 2002 , 2 : 18 - 22 .Guyon I , Weston J , Barnhill S , Vapnik V : Gene selection for cancer classification using support vector machines .Machine Learning 2002 , 46 : 389 - 422 .", "label": "", "metadata": {}, "score": "58.262173"}
{"text": "When neural nets were first used they were trained discriminatively and it was only recently that researchers showed that significant gains could be achieved by adding an initial stage of generative ... . by Wolfgang Macherey , Lars Haferkamp , Ralf Schl\u00fcter , Hermann Ney - Proceedings of Eurospeech , 2005 . \" ...", "label": "", "metadata": {}, "score": "58.293964"}
{"text": "The structure of the naive Bayes classifier represented as a Bayesian network is illustrated in Figure 1(a ) .( a ) C X1 X2 X3 XN ( b ) C X1 X2 X3 XN Fig . 1 .Bayesian Network : ( a ) NB , ( b ) TAN .", "label": "", "metadata": {}, "score": "58.391983"}
{"text": "It measures the information be- tween Xiand Xj in the context of C. In [ 21 ] , an algo- rithm for constructing TAN networks using this measure is provided .3 ) Transform the undirected 1-tree into a directed tree .", "label": "", "metadata": {}, "score": "58.443207"}
{"text": "Then , RotBoost was employed on the selected genes .Here , we experimented with 2 different transformation matrixes , that is , PCA and ICA towards RotBoost implementation .To assess the efficiency of RotBoost algorithm different ensemble / nonensemble techniques including Rotation Forest , AdaBoost , Bagging single tree , and SVMs were also deployed .", "label": "", "metadata": {}, "score": "58.533195"}
{"text": "The new viewpoint also illuminates existing algorithms : it provides a new derivation of Support Vector Machines in terms of divergences and relates Maximum Mean Discrepancy to Fisher Linear Discriminants . by Qian Yu , Thang Ba Dinh , G\u00e9rard Medioni - in Proc .", "label": "", "metadata": {}, "score": "58.553978"}
{"text": "and the results for which a significant difference with RotBoost was found are marked with a bullet or an open circle next to them .A bullet next to a result indicates that RotBoost is significantly better than the corresponding method .", "label": "", "metadata": {}, "score": "58.85658"}
{"text": "However , it provides more information to the classifier compared to simple summation over the missing feature values as shown for the NB - CG - MM case .Fig . 7 .Washington D.C.Mall : Classification results for NB - CG - MM ( summation over missing feature values ) , NB - CG - MM ( using kNN most frequent value imputation ) , and SVMs ( using kNN mean value imputation ) assuming missing features .", "label": "", "metadata": {}, "score": "59.092503"}
{"text": "In Section 3 , we introduce MM parameter learning .Section 4 summarizes a generative and two discriminative structure learning algorithms used in the experiments .In Section 5 , we present experimental re- sults for phonetic classification using the TIMIT speech 1 .", "label": "", "metadata": {}, "score": "59.160965"}
{"text": "This study explores the efficacy of an approach to native language identification that utilizes grammatical , rhetorical , semantic , syntactic , and cohesive function categories comprised of key n - grams .The study found that a model based on these categories of key n - grams was able to successfully predict the L1 of essays written in English by L2 learners from 11 different L1 backgrounds with an accuracy of 59 % .", "label": "", "metadata": {}, "score": "59.16892"}
{"text": "The used test sets were the same as described above .The runtime experiments were performed on a personal computer with 2.8 GHz CPUs , 16 GB of memory , not exploiting any ( multicore ) parallelization .Furthermore , we fixed the regularization parameter B to 1 for the MNIST data because of time reasons , but selected it using cross tuning for the TIMIT-4/6 and USPS data .", "label": "", "metadata": {}, "score": "59.55537"}
{"text": "e continuous w.r.t . \u03bd . 2 During the course of the proof , we will need several capacity concepts of function sets . by Yaakov Engel , Shie Mannor , Ron Meir - IEEE Transactions on Signal Processing , 2003 . \" ...", "label": "", "metadata": {}, "score": "59.91149"}
{"text": "Full - text .In contrast to previous approaches , we maintain the normalization constraints of the parameters of the Bayesian network during optimization , i.e. the probabilistic interpretation of the model is not lost .This enables to handle missing features in discriminatively optimized Bayesian networks .", "label": "", "metadata": {}, "score": "59.928967"}
{"text": "Return to Greiner 's home page Navigation .Sabine Sternig .Short CV .Sabine Sternig received her MSc Degree in Software Development and Business Management from the Graz University of Technology in December 2008 .She is currently a research assistant at the Institute for Computer Graphics and Vision where she is currently involved in the MASA Project .", "label": "", "metadata": {}, "score": "60.044098"}
{"text": "View Article PubMed .Lee JW , Lee JB , Park M , Song SH : An extensive comparison of recent classification tools applied to microarray data .Computational Statistics & Data Analysis 2005 , 48 : 869 - 885 .View Article .", "label": "", "metadata": {}, "score": "60.168278"}
{"text": "1373 - 1390 , 2004 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .S. Cho and H. Won , \" Machine learning in DNA microarray analysis for cancer classification , \" in Proceedings of the 1st Asia - Pacific Bioinformatics Conference on Bioinformatics , pp .", "label": "", "metadata": {}, "score": "60.548702"}
{"text": "6 , pp .673 - 679 , 2001 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . H. Bhaskar , D. C. Hoyle , and S. Singh , \" Machine learning in bioinformatics : a brief survey and recommendations for practitioners , \" Computers in Biology and Medicine , vol .", "label": "", "metadata": {}, "score": "60.571785"}
{"text": "CG - MM is implemented in MATLAB .Page 12 .12 TABLE 7 Runtimes in [ s ] for different data using a naive Bayes classifier ( B as in Table 6 ) .Parameter Learning CG - MM 833 113 391 168 87 202 241 108 Data MNIST USPS Ma+Fe-4 Ma-4 Fe-4 Ma+Fe-6 Ma-6 Fe-6 CVX - MM 54 hours 21 hours 1338 842 844 4566 3505 3002 formance of the proposed method in the conducted experiments .", "label": "", "metadata": {}, "score": "61.05957"}
{"text": "Bin Shen , Xiaoyuan Su , Russell Greiner , Petr Musilek , Corrine Cheng Proceedings of the Fifteenth IEEE International Conference on Tools with Artificial Intelligence ( ICTAI-03 ) , Sacramento , Nov 2002 .This is especially true when learning parameters for incorrect structures , such as Na ve Bayes ( NB ) .", "label": "", "metadata": {}, "score": "61.19217"}
{"text": "575 - 582 , 2009 .[42 ] P. Amestoy , I. Duff , J.-Y. L'Excellent , and J. Koster , \" MUMPS :A general purpose distributed memory sparse solver , \" in 5th International Workshop on Applied Parallel Computing .", "label": "", "metadata": {}, "score": "61.275085"}
{"text": "Furthermore , we applied various parameter learning algorithms on naive Bayes and generatively and discriminatively optimized TAN structures .Discriminative parameter learning sig- nificantly outperforms ML parameter estimation .Fur- thermore , maximizing the margin slightly improves the classification performance compared to CL parameter optimization in most cases .", "label": "", "metadata": {}, "score": "61.457756"}
{"text": "Dedicated to A.V. Skorohod on his seventieth birthday We prove new probabilistic upper bounds on generalization error of complex classifiers that are combinations of simple classifiers .Such combinations could be implemented by neural networks or by voting methods of combining the classifiers , such as boosting and bagging .", "label": "", "metadata": {}, "score": "61.72518"}
{"text": "Our Kernel - RLS ( KRLS ) algorithm performs linear regression in the feature space induced by a Mercer kernel , and can therefore be used to recursively construct the minimum mean squared -error regressor .Spars ... \" .We present a non - linear kernel - based version of the Recursive Least Squares ( RLS ) algorithm .", "label": "", "metadata": {}, "score": "61.905453"}
{"text": "When comb ... \" .We present an efficient discriminative training procedure utilizing phone lattices .Different approaches to expediting lattice generation , statistics collection , and convergence were studied .We also propose a new discriminative training criterion , namely , minimum phone frame error ( MPFE ) .", "label": "", "metadata": {}, "score": "61.952255"}
{"text": "To control the increasing traffic - flow on highways , monitoring of traffic is becoming more and more important .Existing incident detection systems shall be improved for the surveillance of highways .Starting from the existing methods and systems , the main goal of the proposed project is to improve and to adapt these systems in order to get more robust systems .", "label": "", "metadata": {}, "score": "61.967632"}
{"text": "Thus , the classifiers trained on the learner data make use of a discriminative model .Because the Google corpus does not contain complete sentences but only n - gram counts of length up to five , trai ... . \" ...", "label": "", "metadata": {}, "score": "62.048088"}
{"text": "In Section 2 , the framework of RotBoost is described in detail .In Section 3 , the experimental results and corresponding discussions are presented .Section 4 concludes the paper .Materials and Methods .The Description of RotBoost Ensemble Classification .", "label": "", "metadata": {}, "score": "62.05886"}
{"text": "View Article PubMed .Ling CX , Huang J , Zhang H : AUC : a statistically consistent and more discriminating measure than accuracy .Proceedings of the Eighteenth International Joint Conference of Artificial Intelligence ( IJCAI ) 2003 .Fawcett T : ROC Graphs : Notes and Practical Considerations for Researchers .", "label": "", "metadata": {}, "score": "62.155506"}
{"text": "Phone - lattice - based discriminative training gave around 8 % to 12 % relative word error rate ( WER ) reduction in SRI 's latest English Conversational Telephone Speech and Broadcast News transcription systems developed for DARPA 's EARS project .", "label": "", "metadata": {}, "score": "62.436584"}
{"text": "corpus [ 17 ] , for handwritten digit recognition using the MNIST [ 18 ] and USPS data sets , and for a remote sens- ing application .Furthermore , experiments for missing feature situations are reported in Section 5.1 and 5.2 .", "label": "", "metadata": {}, "score": "62.65232"}
{"text": "In case of a small size of X\u03a0j(i.e .N ) and of k a computational costly scoring function to find X\u03a0jcan be used .Basically , either the CL or the CR can be used as cost function to select the parents for learning a discriminative structure .", "label": "", "metadata": {}, "score": "63.066696"}
{"text": "In this paper , we describe the University of Illinois system that participated in the shared task .The system consists of five components and targets five types of common grammati ... \" .The CoNLL-2013 shared task focuses on correcting grammatical errors in essays written by non - native learners of English .", "label": "", "metadata": {}, "score": "63.092766"}
{"text": "The core of the system is based on the University of Illinois model that placed first in the CoNLL-2013 shared task .This baseline model has been improved and ex - panded for this year 's competition in sev - eral respects .", "label": "", "metadata": {}, "score": "63.233624"}
{"text": "The test was applied with 100,000 permutations and two - sided p - values were computed as described in [ 29 ] .Declarations .Acknowledgements .The work was in part supported by grant 2R56LM007948 - 04A1 .Electronic supplementary material .", "label": "", "metadata": {}, "score": "63.325893"}
{"text": "Journal of the American Statistical Association 2002 , 97 : 77 - 88 .View Article .Dupuy A , Simon RM :Critical review of published microarray studies for cancer outcome and guidelines on statistical analysis and reporting .J Natl Cancer Inst 2007 , 99 : 147 - 157 .", "label": "", "metadata": {}, "score": "63.389286"}
{"text": "More details about the features can be found in [ 39].MNIST Data : We present results for the handwritten digit MNIST data [ 18 ] which contains 60000 samples for training and 10000 digits for testing .We down - sample the gray - level images by a factor of two which results in a resolution of 14 \u00d7 14 pixels , i.e. 196 features .", "label": "", "metadata": {}, "score": "63.675354"}
{"text": "Copyright .\u00a9 Statnikov et al .2008 .This article is published under license to BioMed Central Ltd.Many of today 's artificial intelligence tools inherently involve probabilistic reasoning .Bayesian Belief Nets have become the standard tool for these applications .", "label": "", "metadata": {}, "score": "64.12312"}
{"text": "Conditional independencies among variables reduce the computational effort for exact inference on such a graph .The set of parameters which quantify the network is represented by \u0398. Each node Zj is rep- resented as a local conditional probability distribution given its parents Z\u03a0j .", "label": "", "metadata": {}, "score": "64.55617"}
{"text": "C. Zhang and J. Zhang [ 19 ] proposed a novel ensemble classifier generation method RotBoost through combining Rotation Forest and AdaBoost .In this new ensemble method , the base classifier in Rotation Forest algorithm is replaced with AdaBoost .The experimental results show that RotBoost performs better than either Rotation Forest or AdaBoost when using some non - microarray gene - related data sets from the UCI repository .", "label": "", "metadata": {}, "score": "64.926605"}
{"text": "Research in natural language processing ( NLP ) applications for education has continued to progress using innovative statistical and rule - based NLP methods , or most commonly , a combination of the two .As a community , we continue to improve existing capabilities and to identify and generate innovative ways to use NLP in applications for writing , reading , speaking , critical thinking , curriculum development , and assessment .", "label": "", "metadata": {}, "score": "66.02448"}
{"text": "We employed the state - of - the - art implementation of RF available in the R package randomForest [ 24 ] .This implementation is based on the original Fortran code authored by Leo Breiman , the inventor of RFs .", "label": "", "metadata": {}, "score": "66.1676"}
{"text": "Finally , Section 6 concludes the paper .G , \u0398 ?We use boldface capital letters , e.g. Z , to denote a set of random variables and correspondingly boldface lower case letters , e.g. z , denote a set of instantiations ( values ) .", "label": "", "metadata": {}, "score": "66.22226"}
{"text": "NLI is achieved by identifying patterns of language use that are common to a group of users of a particular second language ( L2 ; e.g. , English ) that share a native language ( L1 ) .Useful to the disc ... . \" ...", "label": "", "metadata": {}, "score": "66.230446"}
{"text": "The parameters are trained using ML learning .A parent is connected to Xj when CR is improved .Otherwise Xj entless ( except C ) .This might result in a partial 1-tree ( forest ) over the attributes .", "label": "", "metadata": {}, "score": "66.249115"}
{"text": "In a 5xRT broadcast news transcription system that includes adaptation , it is shown that reductions in word error rate ( WER ) in the range of 1 % absolute can be achieved .Finally , some experiments on training data selection are presented to compare different methods of \" filtering \" the transcripts . by Jarkko Saloj\u00e4rvi , Kai Puolam\u00e4ki , Samuel Kaski - Proceedings of the 22nd International Conference on Machine Learning ( ICML-2005 , 2005 . \" ...", "label": "", "metadata": {}, "score": "66.45068"}
{"text": "IPOPT is typically faster than the function fmincon of MATLAB for this type of optimization problem .Good classifiers do not require highly accurate solutions . better than those of CG - MM , while the proposed algo- rithm CG - MM12is up to orders of magnitude faster .", "label": "", "metadata": {}, "score": "68.42061"}
{"text": "The practical need for languageanalysis Tools . by Julien Mairal , Francis Bach , Jean Ponce , Guillermo Sapiro , Andrew Zisserman , Th\u00e8me Cog , Julien Mairal , Francis Bach , Jean Ponce , Guillermo Sapiro , Andrew Zisserman , \u00c9quipes - projets Willow , Ecole Normale Sup\u00e9rieure , 2008 . ... nary learning via class supervised simultaneous orthogonal matching pursuit ) .", "label": "", "metadata": {}, "score": "68.544914"}
{"text": "122 - 131 .[43 ] S. Boyd and L. Vandenberghe , Convex Optimization .University Press , March 2004 .\" Multi - intervaldiscretizaton of Springer-Cambridge Franz Pernkopf received his MSc ( Dipl .Ing . ) degree in Electrical Engineering at Graz Uni- versity of Technology , Austria , in summer 1999 .", "label": "", "metadata": {}, "score": "68.79847"}
{"text": "5.3.2 Experimental Comparison Table 6 and Table 7 show the classification rates and run- times for the different algorithms and datasets , respec- tively .The classification rates of CVX - MM are slightly 10 .We used IPOPT 3.9.2 in conjunction with MUMPS 4.9.2 [ 42 ] , a parallel sparse direct solver .", "label": "", "metadata": {}, "score": "69.21791"}
{"text": "The classification performances are shown in Table 5 .Remarkably , NB - CG - MM slightly outperforms SVMs in this experiment .The classification rate slightly improves ( a)(b ) Fig .4 . Washington D.C.Mall : ( a )", "label": "", "metadata": {}, "score": "69.31467"}
{"text": "Sebastian Tschiatschek received the BSc de- gree and MSc degree ( with distinction ) in Elec- trical Engineering at Graz University of Tech- nology ( TUG ) in 2007 and 2010 , respectively .He conducted his Master thesis during a one- year stay at ETH Z\u00a8 urich , Switzerland .", "label": "", "metadata": {}, "score": "70.09006"}
{"text": "The aim of the project MASA is movement and action sequence analysis in complex sport games .In particular we are interested in team handball .The main goal is the identification of team tactics , discrimination of successful from non - successful player / team behavior , anticipation of player / team behavior and determination of physical demands during the game .", "label": "", "metadata": {}, "score": "70.22823"}
{"text": "TABLE 5 Classification results in [ % ] for Washington D.C. Mall data with standard deviation .Best parameter learning result is emphasized using bold font .However , the impact is moderate .Note that the selection of \u03ba is based on the cross - validation performance on the training data .", "label": "", "metadata": {}, "score": "70.60906"}
{"text": "In 2002 he was awarded the Erwin Schr\u00a8 odinger Fellowship .He was a Research Associate in the Department of Electrical Engineering at the University of Wash- ington , Seattle , from 2004 to 2006 .Currently , he is Associate Professor at the Laboratory of Signal Processing and Speech Communication , Graz University of Technology , Austria .", "label": "", "metadata": {}, "score": "71.052216"}
{"text": "We present an alternative corpus to the ICLE which has been used in most work up until now .We believe that our corpus , TOEFL11 , is more suitable for the task of NLI and will allow researchers to better compare systems and results .", "label": "", "metadata": {}, "score": "71.45963"}
{"text": "Authors ' contributions .Conceived and designed the experiments : AS , LW , CFA .Performed the experiments : AS .Analyzed the results of experiments : AS , LW , CFA .Wrote the paper : AS , CFA .", "label": "", "metadata": {}, "score": "72.25064"}
{"text": "( PDF 287 KB ) .1471 - 2105 - 9 - 319-S2.pdf Additional file 2 : Simulation experiment demonstrating sensitivity of random forests to input parameters .( PDF 14 KB ) .1471 - 2105 - 9 - 319-S3.pdf Additional file 3 : Complete information about microarray datasets used in the study .", "label": "", "metadata": {}, "score": "73.09594"}
{"text": "For example , a French speaker learning English might write sent ... . \" ...Research in natural language processing ( NLP ) applications for education has continued to progress using innovative statistical and rule - based NLP methods , or most commonly , a combination of the two .", "label": "", "metadata": {}, "score": "73.13868"}
{"text": "Notice that the dataset collection used in this work contains all datasets from the prior comparison [ 5 ] .Breast cancer 5-year metastasis - free survival , metastasis within 5 years , germline BRCA1 mutation .Px - Yeoh .", "label": "", "metadata": {}, "score": "73.17051"}
{"text": "The CoNLL-2014 shared task is an ex - tension of last year 's shared task and fo - cuses on correcting grammatical errors in essays written by non - native learners of English .In this paper , we describe the Illinois - Columbia system that participated in the shared task .", "label": "", "metadata": {}, "score": "73.375145"}
{"text": "The CoNLL-2014 shared task is an ex - tension of last year 's shared task and fo - cuses on correcting grammatical errors in essays written by non - native learners of English .In this paper , we describe the Illinois - Columbia system that participated in the shared task .", "label": "", "metadata": {}, "score": "73.375145"}
{"text": "We present an alternative corpus to the ICLE which has been used in most work up until now .We believe that our corpus , TOEFL11 , is more suitable for the task of NLI and will allow researchers to better compare system ... \" .", "label": "", "metadata": {}, "score": "74.16452"}
{"text": "We discuss recommendations for best practices with regard to reporting the results of system evaluation for these cases , recommendations which depend upon making clear one 's assumptions and applications for error detection .By highlighting the problems with current error detection evaluation , the field will be better able to move forward . by Kenji Imamura , Kuniko Saito , Kugatsu Sadamitsu , Hitoshi Nishikawa - In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 388 - 392 , Jeju Island , Korea , 2012 .", "label": "", "metadata": {}, "score": "76.00729"}
{"text": "Authors ' Affiliations .Department of Biomedical Informatics , Vanderbilt University .Department of Biostatistics , Vanderbilt University .Department of Cancer Biology , Vanderbilt University .Department of Computer Science , Vanderbilt University .References .Statnikov A , Aliferis CF , Tsamardinos I , Hardin D , Levy S : A comprehensive evaluation of multicategory classification methods for microarray gene expression cancer diagnosis .", "label": "", "metadata": {}, "score": "76.134674"}
{"text": "An Efficient Ensemble Learning Method for Gene Microarray Classification .Department of Computer Engineering , Islamic Azad University , Dezful Branch , Dezful 313 , Iran .Received 30 April 2013 ; Accepted 12 July 2013 .Copyright \u00a9 2013 Alireza Osareh and Bita Shadgar .", "label": "", "metadata": {}, "score": "76.8219"}
{"text": "When compared with Rotation Forest , the statistically significant difference is favorable in 6 datasets , although the Rotation Forest could surpass the RotBoost when working on Breast and Ovarian datasets .Indeed , RotBoost is seen to outperform AdaBoost in most cases even though the advantage of RotBoost is not significant in 1 dataset and tie is occurred on the remaining 2 datasets .", "label": "", "metadata": {}, "score": "78.19126"}
{"text": "Page 14 .14 Michael Wohlmayr graduated from Graz Uni- versity of Technology in June 2007 .He con- ducted his Master thesis in collaboration with University of Crete , Greece .Since October 2007 , he is pursuing the PhD degree at the Signal Processing and Speech Communication Laboratory at Graz University of Technology .", "label": "", "metadata": {}, "score": "85.90522"}
{"text": "Washington D.C.Mall : Classification results for NB - ML , NB - CG - MM , NB - CG - CL , and SVMs ( using mean value imputation ) assuming missing features .Figure 7 shows kNN value imputation results for SVMs and NB - CG - MM .", "label": "", "metadata": {}, "score": "86.55632"}
