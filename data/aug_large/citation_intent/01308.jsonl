{"text": "We present an algorithm for computing n - gram probabilities from stochastic context - free grammars , a procedure that can alleviate some of the standard problems associated with n - grams ( estimation from sparse data , lack of linguistic structure , among others ) .", "label": "", "metadata": {}, "score": "26.93739"}
{"text": "Based on decision lists , the algorithm incorporates both local syntactic patterns and more distant collocational evidence , combining the strengths of decision trees , N - gram taggers an ... \" .This chapter presents a statistical decision procedure for lexical ambiguity resolution in text - to - speech synthesis .", "label": "", "metadata": {}, "score": "35.196068"}
{"text": "For ( non - length - dependent ) SCFGs it is well known how this can be achieved [ 11 , 12 ] : .If we are training from full - parse trees the relative frequencies with which the rules occur ( among all rules with the same left - hand side ) are a maximum - likelihood estimator .", "label": "", "metadata": {}, "score": "35.248474"}
{"text": "Approximating them with easily describable functions seems possible , but would potentially be difficult to automate and would not be covered by the considerations in Section 3 .Thus we decided to use uniform distributions here .Combining the above considerations length - dependent stochastic context - free grammars can be defined : .", "label": "", "metadata": {}, "score": "35.357925"}
{"text": "Tools . \" ...We present an algorithm for computing n - gram probabilities from stochastic context - free grammars , a procedure that can alleviate some of the standard problems associated with n - grams ( estimation from sparse data , lack of linguistic structure , among others ) .", "label": "", "metadata": {}, "score": "36.762558"}
{"text": "This implies that our scheme may be useful as a novel method for PCFG induction .1 Introduction Like its non - stochastic brethren , probabilistic parsing has been based upon context - free grammars ( CFGs ) , and for similar reasons : CFGs support a simple and efficien ... . ... that tag given the two previous tags . ) 8 What would correspond to a good improvement in the cross - entropy of the tag sequence ?", "label": "", "metadata": {}, "score": "39.204445"}
{"text": "We would like to thank the anonymous referees of the previous revisions for their helpful suggestions .References .Nussinov , R. ; Pieczenik , G. ; R'Griggs , J. ; Kleitmann , D.J. Algorithms for loop matchings .SIAM J. Appl .", "label": "", "metadata": {}, "score": "39.735664"}
{"text": "An N - Gram grammar is a representation of an N - th order Markov language model in which the probability of occurrence of a symbol is conditioned upon the prior occurrence of N-1 other symbols .N - Gram grammars are typically constructed from statistics obtained from a large corpus of text using the co - occurrences of words in the corpus to determine word sequence probabilities .", "label": "", "metadata": {}, "score": "40.419827"}
{"text": "We develop a language model using probabilistic context - free grammars ( PCFGs ) that is \" pseudo context - sensitive \" in that the probability that a non - terminal N expands using a rule r depends on N 's parent .", "label": "", "metadata": {}, "score": "40.593857"}
{"text": "We develop a language model using probabilistic context - free grammars ( PCFGs ) that is \" pseudo context - sensitive \" in that the probability that a non - terminal N expands using a rule r depends on N 's parent .", "label": "", "metadata": {}, "score": "40.593857"}
{"text": "We develop a language model using probabilistic context - free grammars ( PCFGs ) that is \" pseudo context - sensitive \" in that the probability that a non - terminal N expands using a rule r depends on N 's parent .", "label": "", "metadata": {}, "score": "40.593857"}
{"text": "This document defines syntax for representing N - Gram ( Markovian ) stochastic grammars within the W3C Speech Interface Framework .The use of stochastic N - Gram models has a long and successful history in the research community and is now more and more effecting commercial systems , as the market asks for more robust and flexible solutions .", "label": "", "metadata": {}, "score": "40.73066"}
{"text": "For this reason , cross - referencing between N - Gram models and CFGs is an important feature of the markup described below .Most former publicly available N - Gram grammar file formats use log probabilities to represent the word sequence probabilities .", "label": "", "metadata": {}, "score": "40.822235"}
{"text": "This implies that our scheme may be useful as a novel method for PCFG induction .1 Introduction Like its non - stochastic brethren , probabilistic parsing has been based upon context - free grammars ( CFGs ) , and for similar reasons : CFGs support a simple and efficien ... .", "label": "", "metadata": {}, "score": "41.040527"}
{"text": "The same holds for the changes to the training algorithms explained at the beginning of the following section .Estimating the Rule Probabilities .When given a set of training data of either full - parse trees of the grammar or words from the language generated by it , we want to train the grammar according to the maximum likelihood principle , i.e. , choose rule probabilities such that .", "label": "", "metadata": {}, "score": "42.02793"}
{"text": "To counter this the authors of [ 13 ] translated the concept of transition and emission probabilities from hidden Markov models to SCFGs .The emission probability then is the probability that a given b is a placeholder , e.g. u independent of the rule which introduced the b . G1 and G2 have been taken from [ 13 ] , G1 being the simplest grammar in the comparison and G2 , which originates from [ 3 ] , being the grammar which achieved the best results .", "label": "", "metadata": {}, "score": "42.91747"}
{"text": "Thus for this grammar each parse tree will have \" probability \" 1 and each word will have a \" probability \" equal to its degree of ambiguity .Both cases can be countered by adding the proper probability distributions to the model .", "label": "", "metadata": {}, "score": "43.226017"}
{"text": "YEAR .INFERENCE .VENUE .In Proceedings of the Twelfth National Conference on Artificial Intelligence Other Repositories / Bibliography .BibTeX .Share .OpenURL .Abstract .We present an algorithm for computing n - gram probabilities from stochastic context - free grammars , a procedure that can alleviate some of the standard problems associated with n - grams ( estimation from sparse data , lack of linguistic structure , among others ) .", "label": "", "metadata": {}, "score": "43.489124"}
{"text": "Since each of these factors is described by an unrestricted probability distribution on its defining domain , Theorem 1 from [ 11 ] applies , stating that relative frequencies are a maximum likelihood estimator for their probabilities .Theorem 4 .Then the inside - outside algorithm will converge to a set of rule probabilities that ( locally or globally ) maximises the likelihood of observing .", "label": "", "metadata": {}, "score": "43.546608"}
{"text": "This algorithm roughly works as follows : .Summing up these probabilities for all subwords it gets expected numbers of rule occurrences from which expected relative frequencies can be computed as in the case of given parse trees .By using these expected relative frequencies as a new estimate for the rule probabilities and iterating the procedure we are guaranteed to converge to a set of rule probabilities that gives a ( local ) maximum likelihood .", "label": "", "metadata": {}, "score": "43.817505"}
{"text": "Finally , we show how the tagger can be extended into a k - best tagger , where multiple tags can be assigned to words in some cases of uncertainty .Abstract .While symbolic parsers can be viewed as deduction systems , this view is less natural for probabilistic parsers .", "label": "", "metadata": {}, "score": "43.997375"}
{"text": "In this pa - per , we describe a number of extensions to this rule - based tagger .First , we describe a method for expressing lexical re - lations in tagging that stochastic taggers are currently unable to express .", "label": "", "metadata": {}, "score": "44.006393"}
{"text": "In the format presented here we depart from this tradition and represent the core statistical information with word sequence event counts .Motivations for using counts includes : . using counts leaves the decision on the actual implementation of the N - Gram model to the platform .", "label": "", "metadata": {}, "score": "44.088905"}
{"text": "The concepts and methods presented in Sections 2 - 4 can immediately be applied to any other application where SCFGs are used as a model , e.g. , natural language processing .From our limited insight in that field it appears possible that length - dependent grammars can successfully be applied there as well .", "label": "", "metadata": {}, "score": "44.503273"}
{"text": "Note 1 .For probabilistic Earley parsing ( [ 10 ] ) we have to regard that the length of the generated subword will only be known in the completion step .Thus for LSCFGs we have to multiply in the rule probability ( and the factor p \u03b1 , l ) in this step instead of the prediction step , as is usually done .", "label": "", "metadata": {}, "score": "44.92126"}
{"text": "Notably , R(prompt ) can include weights if desired .Next , for each rule , a new probability for each choice of the rule can be created by normalizing the counts .Then , the new probabilities for a rule can be interpolated with the original probabilities for the rule from the original grammar .", "label": "", "metadata": {}, "score": "44.998817"}
{"text": "A . i . )P .n .A . i .Theorem 3 .Then P is a maximum - likelihood estimator for the length - dependent rule - probabilities on G , R and Q .Proof .P .", "label": "", "metadata": {}, "score": "44.999695"}
{"text": "Determining the Most Probable Derivation .In order to find the most probable derivation for a given primary structure we decided to employ a probabilistic Earley parser , since it allows to use the grammars unmodified while the commonly used CYK algorithm requires the grammars to be transformed into Chomsky normal form .", "label": "", "metadata": {}, "score": "45.364445"}
{"text": "Stolcke , A. An Efficient Probabilistic Context - Free Parsing Algorithm that Computes Prefix Probabilities .Comput .Linguist .[ Google Scholar ] .Prescher , D. A Tutorial on the Expectation - Maximization Algorithm Including Maximum - Likelihood Estimation and EM Training of Probabilistic Context - Free Grammars .", "label": "", "metadata": {}, "score": "45.41675"}
{"text": "An ( ambiguous ) stochastic context - free grammar ( SCFG ) that generates the primary structures is chosen such that the derivation trees for a given primary structure uniquely correspond to the possible secondary structures .The probabilities of this grammar are then trained either from molecules with known secondary structures or by expectation maximization ( [ 3 ] ) .", "label": "", "metadata": {}, "score": "46.031605"}
{"text": "The parser also implements the scaling approach as described in my TACL'13 paper which speeds up parsing time and allows for parsing long sentences ( with restricted grammars ) .Features : ( a ) Code was restructured and rewritten to follow the flow of Stolcke 's algorithm ( see the method parse ( ) in parser .", "label": "", "metadata": {}, "score": "46.33808"}
{"text": "An N - Gram language model can be constructed from a linear interpolation of several models .For interpolated models , no common lexicon is defined .The platform is responsible for combining these lexica .Example : . 9.2 Log - Linear Interpolation .", "label": "", "metadata": {}, "score": "46.34135"}
{"text": "Observations and Dicussion .We did the training and prediction using a length - dependent version of the Earley - style - parser from [ 16 ] .The results of the benchmarks are listed in Table 1 .Looking at these results it is immediately apparent that the predictions from G1 are too bad to be of any use either without or with lengths .", "label": "", "metadata": {}, "score": "46.42778"}
{"text": "Semiring - based constraint satisfaction and optimization .Journal of the ACM 44(2):201 - 236 .MathSciNet MATH CrossRef .Caraballo , S. A. , and E. Charniak ( 1998 ) .New figures of merit for best - first probabilistic chart parsing .", "label": "", "metadata": {}, "score": "46.769157"}
{"text": "In addition , stochastic grammars can be used to represent concepts or semantics .This specification defines the mechanism for combining stochastic and structured ( in this case Context - Free ) grammars as well as methods for combined semantic definitions .", "label": "", "metadata": {}, "score": "46.896324"}
{"text": "In order to dene this grammatical model , which will be used on large - vocabulary complex tasks , a category - based SCFG and a probabilistic model of word distribution in the categories have been proposed .Methods for learning these stochastic models for complex tasks are described , and algorithms for computing the word transition probabilities are also presented .", "label": "", "metadata": {}, "score": "47.86209"}
{"text": "In G. Rozenberg and A. Salomaa ( Eds . ) , Handbook of Formal Languages , Vol . 2 : Linear Modelling : Background and Application , 61 - 100 .Berlin : Springer .CrossRef .Stolcke , A. ( 1995 ) .", "label": "", "metadata": {}, "score": "48.31138"}
{"text": "The N - Gram language model is usually derived from large training texts that share the same language characteristics as expected input .In contrast , N - Gram language models rely on the likelihood of sequences of words , such as word pairs ( in the case of bigrams ) or word triples ( in the case of trigrams ) and are therefore less restrictive .", "label": "", "metadata": {}, "score": "48.6196"}
{"text": "In the present paper we will however confine ourselves with grouping the lengths together in finitely many intervals , a rule having the same probability for lengths that are in the same interval .This allows for the probabilities to be stored as a vector and be retrieved in the algorithms without further computation .", "label": "", "metadata": {}, "score": "49.18245"}
{"text": "The method of .claim 1 wherein a probabilistically adjusted grammar is formed by adapting a static grammar modeling of responses the user might say at any time to the phrases in the response set to boost the probability of predicting phrases in the response set by the static grammar .", "label": "", "metadata": {}, "score": "49.820957"}
{"text": "In ACL 21 , 137 - 144 .Shieber , S. , Y. Schabes , and F. Pereira ( 1995 ) .Principles and implementation of deductive parsing .Journal of Logic Programming 24:3 - 36 .MathSciNet MATH CrossRef .Sikkel , K. , and A. Nijholt ( 1997 ) .", "label": "", "metadata": {}, "score": "50.05976"}
{"text": "Comput .Linguist .[ Google Scholar ] .Dowell , R.D. ; Eddy , S.R. Evaluation of several lightweight stochastic context - free grammars for RNA secondary structure prediction .BMC Bioinforma .[ Google Scholar ] .Nebel , M.E. Identifying Good Predictions of RNA Secondary Structure .", "label": "", "metadata": {}, "score": "50.305534"}
{"text": "If probabilistic context - free grammars are t ... \" .In this paper we describe a new algorithm that achieves the required computation in at most a constant times k3-steps .ven word string wlw2 ...Wk ?That is , which sequence of rewrite rules resulting in wlw2 ...", "label": "", "metadata": {}, "score": "50.465317"}
{"text": "Chappelier , J.-C. , and M. Rajman ( 1998 ) .A generalized CYK algorithm for parsing stochastic CFG .In First Workshop on Tabulation in Parsing and Deduction ( TAPD98 ) , 133 - 137 , Paris .Earley , J. ( 1970 ) .", "label": "", "metadata": {}, "score": "50.699173"}
{"text": "The word set may be determined using the strategy ( or technology ) used next by the speech recognition system , such as n - grams , grammars , or both .Then , the system boosts the n - grams in the n - gram model or the appropriate rules in the grammars in order to increase the likelihood of what the user is likely to say in their response to the prompt , by a preconfigured amount .", "label": "", "metadata": {}, "score": "50.857338"}
{"text": "The primary purpose of specifying a stochastic grammar format is to support large vocabulary and open vocabulary applications .In addition , stochastic grammars can be used to represent concepts or semantics .This specification defines the mechanism for combining stochastic and structured ( in this case Context - Free ) grammars as well as methods for combined semantic definitions .", "label": "", "metadata": {}, "score": "50.89186"}
{"text": "Formal Definitions .We assume the reader is familiar with basic terms of context - free grammars .An introduction can be found in ( [ 8 ] ) .Definition 1 .Words are generated as for usual context - free grammars , the product of the probabilities of the production rules used in a parse tree \u0394 provides its probability P ( \u0394 ) .", "label": "", "metadata": {}, "score": "50.931335"}
{"text": "This effectively increases the counts of these n - grams in the n - gram LM used by the speech recognition system in the next turn .It is also possible to make a separate \" mini LM \" out of one sentence , and weight this sentence using lambdas with the base LM .", "label": "", "metadata": {}, "score": "51.351067"}
{"text": "In practice such grammars have not been used , but in principle it is possible to parse and count small CFG phrase sequences in a corpus to generate event counts .The fourth and fifth examples are references to external named N - Gram grammars , the last being treated as a class or category grammar element ( cf .", "label": "", "metadata": {}, "score": "51.548927"}
{"text": "As we stated in Section 2 we implemented length - dependency such that we grouped the lengths into intervals , the rule probabilities changing only from one interval to the other but not within them .Since the influence a change in length has on the probabilities most likely depends on the relative change rather than the absolute one , we decided to make the intervals longer as the subwords considered get longer .", "label": "", "metadata": {}, "score": "51.573193"}
{"text": "Thus the most probable secondary structure ( derivation tree ) is computed as prediction ( [ 3 ] ) .Many other approaches as well as extensions and modifications of the ones mentioned above have been suggested over the years .A recent overview can be found in [ 4 ] .", "label": "", "metadata": {}, "score": "51.75572"}
{"text": "We will present this extension formally in Section 2 .In Sections 3 and 4 we show that existing training algorithms can easily be adapted to the new model without significant losses in performance .We have compared the prediction quality of the modified model with the conventional one for different grammars and sets of RNA .", "label": "", "metadata": {}, "score": "51.95798"}
{"text": "( b )Scaling approach to parse long sentences ( see my TACL'13 paper ) .With scaling , no log operations are required ( see the usage of util .Operator / ProbOperator / LogProbOperator ) .( c ) Rule probability estimation : inside - outside algorithm in the prefix parser context as described in Stolcke 's paper .", "label": "", "metadata": {}, "score": "52.098045"}
{"text": "Both frequencies were computed over the complete set ( instead of calculating individual scores for each molecule and taking the average of these ) .Data .In [ 13 ] Dowell and Eddy compared the prediction quality of several different grammars as well as some commonly used programs that predict RNA secondary structures by minimizing free energy .", "label": "", "metadata": {}, "score": "52.129"}
{"text": "b ) adapting static embedded grammars modeling of responses the user might say at any time is with phrases in the response set to boost the probability of predicting phrases in the response set by the static grammar to form probabilistically adjusted embedded grammars .", "label": "", "metadata": {}, "score": "52.2647"}
{"text": "In this grammar we can derive from each symbol A i exactly those words that can be derived from A in the original grammar and have length i .This reflects the distinction between choosing the symbols on the right - hand side and distributing the length amongst them introduced by our model .", "label": "", "metadata": {}, "score": "52.478455"}
{"text": "The following set of intervals yielded the best or close to the best results for all 4 grammars : .Since all structures in the benchmark sets are shorter than 500 bases the probabilities of the last interval did not influence the predictions .", "label": "", "metadata": {}, "score": "52.54525"}
{"text": "These results indicate that LSCFGs are indeed capable of giving better predictions than classic SCFGs .However further experiments will be needed to confirm these initial results on other data sets and determine good choices for grammar and length intervals .Possible Other Applications .", "label": "", "metadata": {}, "score": "53.226242"}
{"text": "In addition to the applications , extending the concept of context - free grammars also gives rise to interesting questions in the field of formal language theory .The most obvious of these questions is if adding in length - dependencies changes the class of languages that can be generated .", "label": "", "metadata": {}, "score": "53.554535"}
{"text": "Communications of the ACM 6(8):451 - 455 .Gallo , G. , G. Longo , S. Pallottino , and S. Nguyen .Directed hypergraphs and applications .Discrete Applied Mathematics 42:177 - 201 .MathSciNet MATH CrossRef .Gazdar , G. , and C. Mellish ( 1989 ) .", "label": "", "metadata": {}, "score": "53.648106"}
{"text": "Technical Report dbpubs/2002 - 16 , Stanford University .Knuth , D. E. ( 1977 ) .A generalization of Dijkstra 's algorithm .Information Processing Letters 6(1):1 - 5 .MathSciNet MATH CrossRef .Kupiec , J. ( 1991 ) .", "label": "", "metadata": {}, "score": "53.690033"}
{"text": "On these three sets we again did the training and prediction for the grammars G2 and G3 , them being the most likely candidates for future use .The results are listed in Table 2 .For each of the sets G3 with lengths performed best , backing our assumption .", "label": "", "metadata": {}, "score": "53.691525"}
{"text": "These systems rely heavily on domain - specific , handcrafted knowledge to handle the myriad syntactic , semantic , and pragmatic ambiguities that pervade virtually all aspects of sentence analysis .Not surprisingly , however , generating this knowledge for new domain ... . \" ...", "label": "", "metadata": {}, "score": "53.87529"}
{"text": "The method of .claim 22 , wherein the additional step is inducing new embedded grammars from the response set and interpolating with a static grammar of what responses the user might say at any time to form interpolated embedded grammars ; .", "label": "", "metadata": {}, "score": "53.98324"}
{"text": "MATH CrossRef .Jelinek , F , J. D. Lafferty , and R. L. Mercer ( 1992 ) .Basic methods of probabilistic context free grammars .In P. Laface and R. De Mori ( Eds . ) , Speech Recognition and Understanding : Recent Advances , Trends , and Applications , Vol .", "label": "", "metadata": {}, "score": "54.110054"}
{"text": "Klein , D. , and C. D. Manning ( 2001 b ) .Parsing with treebank grammars : Empirical bounds , theoretical models , and the structure of the Penn treebank .In ACL 39/EACL 10 , 330 - 337 .", "label": "", "metadata": {}, "score": "54.144234"}
{"text": "The data computed by R(prompt ) is then used to boost the speech recognition probabilities , regardless of the technology being used to decode the next utterance ( n - gram LM , grammars , and/or an n - gram LM with embedded grammars ) .", "label": "", "metadata": {}, "score": "54.16189"}
{"text": "In this paper , a new part - of - speech tagging method based on neural networks ( Net-7h . qger ) is presented and its performance is compared to that of a 11IvlM - tagger ( Cutting ct al . , 1992 ) anti a trigrambased tagger ( Kempe , 1993 ) .", "label": "", "metadata": {}, "score": "54.637444"}
{"text": "Each of these alternatives requires an additional reference mechanism .To be consistent with the CFG rule reference specification , Section 2.2 , this rule reference format may also be used in the N - Gram lexicon declaration ( cf .Section 5 ) .", "label": "", "metadata": {}, "score": "54.947594"}
{"text": "The method of .further wherein the interpolated n - gram language model has weights of the responses adjusted therein based upon previous responses to the prompt .The method of .claim 31 , wherein the interpolated n - gram language model is used for future responses .", "label": "", "metadata": {}, "score": "55.103554"}
{"text": "Other actions .Share .References .Baker , J. K. ( 1979 ) .Trainable grammars for speech recognition .In D. H. Klatt and J. J. Wolf ( Eds . ) , Speech Communication Papers for the 97th Meeting of the Acoustical Society of America , 547 - 550 .", "label": "", "metadata": {}, "score": "55.16941"}
{"text": "Experience shows that a context free grammar with reasonable complexity can never foresee all the different sentence patterns , users come up with in spontaneous speech input .This approach is therefore not sufficient for robust speech recognition / understanding tasks or free text input applications such as dictation .", "label": "", "metadata": {}, "score": "55.276443"}
{"text": "This document defines syntax for representing N - Gram ( Markovian ) stochastic grammars within the W3C Voice Browser Markup Language .The parent language for specification of a stochastic grammar is XML , however for efficiency some variance from strict XML syntax will be used .", "label": "", "metadata": {}, "score": "55.31653"}
{"text": "The absence of the name attribute indicates that the imported grammar , which must be an N - Gram grammar , will be treated as a contribution to the superior N - Gram grammar and added to the union of N - Gram event counts of the superior grammar .", "label": "", "metadata": {}, "score": "55.353447"}
{"text": "Conclusions .We introduced an extension to the concept of stochastic context - free grammars that allows the probabilities of the productions to depend on the length of the generated subword .Furthermore we showed that existing algorithms that work on stochastic context - free grammars like training algorithms or determining the most likely parse - tree can easily be adapted to the new concept without significantly affecting their run - time or memory consumption .", "label": "", "metadata": {}, "score": "55.40635"}
{"text": "An Efficient Probabilistic Context - Free Parsing Algorithm that Computes Prefix Probabilities .Computational Linguistics 21(2 ) , 165 - 201 .( b ) Roger Levy 's prefix parser .Expectation - based syntactic comprehension .Cognition 106(3):1126 - 1177 .", "label": "", "metadata": {}, "score": "55.68187"}
{"text": "This can simply be done by adding them to the items as an additional parameter .The modifications of scanner and predictor are straightforward .Since choosing the most probable sequence of steps for each partial derivation will lead to the most probable derivation overall , the completer maximises the overall probability by choosing the most probable alternative , whenever there are multiple possibilities for generating a subword .", "label": "", "metadata": {}, "score": "55.807373"}
{"text": "For a more detailed introduction of probabilistic Earley parsing as well as a proof of correctness and hints on efficient implementation see ( [ 10 ] ) .Application .In order to see if adding length - dependency actually improves the quality of the predictions of RNA secondary structures from stochastic context - free grammars , we used length - dependent and traditional versions of four different grammars to predict two sets of RNA molecules for which the correct secondary structure is already known .", "label": "", "metadata": {}, "score": "55.85508"}
{"text": "These data show that monolingual sense distinctions at most levels of granularity can be effectively captured by translations into some ... . ... her than verb tokens ( Dorr and Jones 1996a , 1996b ) .Theseld has narrowed down approaches , but only a little .", "label": "", "metadata": {}, "score": "55.857906"}
{"text": "n .A .A .P .n .A .A .For the second type note that there is at most 1 way to distribute length 0 among the symbols of \u03b1 ensuring that if A \u03b1 , 0 occurs in .", "label": "", "metadata": {}, "score": "55.88304"}
{"text": "Accordingly , reference should be made to the following claims , rather than to the foregoing specification , as indicating the scope of the invention .BibTeX .Share .OpenURL .Abstract .This paper describes a hybrid proposal to combine n - grams and Stochastic Context - Free Grammars ( SCFGs ) for language modeling .", "label": "", "metadata": {}, "score": "55.898876"}
{"text": "A grammar token example can be treated in similar manner .Then the pseudo - corpus has 2 instances of this \" token \" .Interpretation of string overlaps in the corpus is at the discretion of the N - Gram designer .", "label": "", "metadata": {}, "score": "56.130714"}
{"text": "The N - Gram declaration is defined with the tree element and requires a lexicon declaration .Following the example a complete declaration is : .A B C 3 5 1 1 2 2 2 2 1 1 3 1 2 2 2 1 1 1 2 1 3 1 3 1 .", "label": "", "metadata": {}, "score": "56.15432"}
{"text": "Wild , S. An Earley - style Parser for Solving the RNA - RNA Interaction Problem . B.Sc .Thesis , Kaiserslautern , Germany , 2010 .[ Google Scholar ] .Weinberg , F. Position - and - Length - Dependent Context - free Grammars .", "label": "", "metadata": {}, "score": "56.295197"}
{"text": "Importation declarations of inferior N - Gram grammars may be used to declare additional event counts to be added to the union of N - Gram event counts in the superior grammar .If desired , the entire superior N - Gram grammar may be constructed solely from imported grammars .", "label": "", "metadata": {}, "score": "56.64406"}
{"text": "A C 2,2 ; 1,1 ; 2,1 ; .Note that since this is a grammar of depth one it can easily be recognized and treated as a class .If desirable , non - uniform probability distribution can be assigned by defining the appropriate counts .", "label": "", "metadata": {}, "score": "56.68248"}
{"text": "Systems can incur extra run - time overhead , which could impact high call volume applications .In these cases , it would be preferable to have a single grammar that remains unchanged , and another having the probabilities of the rules to bias the grammar adjusted in favor of what the user is likely to say in response to the prompt .", "label": "", "metadata": {}, "score": "56.704453"}
{"text": "and G3 both extend G1 based on the observation that a secondary structure will be more stable if it contains longer runs of immediately nested base pairs .They differ in the approach taken to get this into the model .G4 has been taken from [ 15 ] .", "label": "", "metadata": {}, "score": "56.84541"}
{"text": "For example , LM(prompt ) , which is an LM that is based upon likely responses , can be created .Then LM(prompt ) can be interpolated with LM(base ) .FIG .1 is a flow chart illustrating a method 100 for biasing a speech recognizer based on prompt context in accordance with one embodiment of the present invention .", "label": "", "metadata": {}, "score": "57.150948"}
{"text": "B .i . )P .B .i . )B .A . and .i . q . else . , where w \u0394 denotes the word generated by \u0394. Then we find .L .T .", "label": "", "metadata": {}, "score": "57.244286"}
{"text": "This work has been motivated by two long term goals : to understand how humans learn language and to build programs that can understand language .Using a representation that makes the relevant features explicit is a prerequisite for successful learning and understanding .", "label": "", "metadata": {}, "score": "57.276646"}
{"text": "This work has been motivated by two long term goals : to understand how humans learn language and to build programs that can understand language .Using a representation that makes the relevant features explicit is a prerequisite for successful learning and understanding .", "label": "", "metadata": {}, "score": "57.276646"}
{"text": "Abstract .Furthermore , we show that the extended model is suited to improve the quality of predictions of RNA secondary structures .The extended model may also be applied to other fields where stochastic context - free grammars are used like natural language processing .", "label": "", "metadata": {}, "score": "57.345364"}
{"text": "For example : .In the first import example the general purpose Perl script ngram.pl can process any raw N - Gram event count file , such as mygrammar.g , to produce the proper XML formatted N - Gram declaration while trimming the file contents at the server .", "label": "", "metadata": {}, "score": "57.73781"}
{"text": "As such , regardless of the technology that is being used to decode the next utterance by the user , the present invention helps ensure that the utterance will correctly recognize the response from the user .The present invention may be used with an n - gram LM , grammars , and/or an n - gram LM with embedded grammars .", "label": "", "metadata": {}, "score": "57.993965"}
{"text": "Backoff weights may only be attached to non - leaf elements and are indicated by a leading colon .Backoff weights are computed by dividing the scaled integer by the backoff - scale .Distant or skip N - Grams are used to cover long - range dependencies with N - Gram models with a small N. This is done by introducing a gap of a certain length between a word and its history .", "label": "", "metadata": {}, "score": "58.066963"}
{"text": "For further information on stochastic language models , you are recommended to look at : . \"Speech and Language Processing : An introduction to Natural Language Processing , Computational Linguistics , and Speech Processing \" , Daniel Jurafsky & James H. Martin , published 2000 by Prentice - Hall .", "label": "", "metadata": {}, "score": "58.148212"}
{"text": "The procedure is fully implemented and has proved viable and useful in practice .Subsequent discussion of those proposals resulted in senseval , the first evaluation exercise for word sense disambiguation ( Kilgarriff and Palmer forthcoming ) .This article is a revised and extended version of our 1997 workshop paper , reviewing its observations and proposals and discussing them in light of the senseval exercise .", "label": "", "metadata": {}, "score": "58.264645"}
{"text": "The method of . claim 8 , wherein the probabilistically adjusted grammar is reset to the static grammar after the user cancels a prompt and resets the system to generate a new prompt .The method of .claim 1 wherein an interpolated n - gram language model is formed by inducing a language model from the response set and interpolating with a static n - gram language model of responses the user might say at any time .", "label": "", "metadata": {}, "score": "58.348698"}
{"text": "Following the example a complete declaration is : .A B C .Note that if pruning has been performed then the branching values must be recomputed accordingly .The depth of the tree is implied by the structure of the data .", "label": "", "metadata": {}, "score": "58.56351"}
{"text": "Then the real starting symbols will be conditioned only on the start symbol or histories starting with the start symbol .In principle , it is possible to import named N - Gram grammars into a CFG and vice versa .Yet another alternative is to import a named inferior N - Gram grammar into a superior N - Gram grammar .", "label": "", "metadata": {}, "score": "58.880966"}
{"text": "S .l .m . w .j .X .w .j .i .The item is considered to represent the partial derivation .X .w .j .i .Then the transitive closure with respect to the following operations is computed : .", "label": "", "metadata": {}, "score": "58.884575"}
{"text": "( f ) Handle grammars with high fan - out ( see Util .TrieSurprisal ) .( g ) Use integers for strings for speed .( h )Smoothing of rule probabilities for unknown words ( see parser .SmoothLexicon ) .", "label": "", "metadata": {}, "score": "59.0679"}
{"text": "The format of the N - Gram event count declaration deviates from the pure XML format because of the need for the efficiency of a compact representation .N - Gram grammars are generally quite large and would require very large file sizes , thus putting a burden on the communications network .", "label": "", "metadata": {}, "score": "59.208374"}
{"text": "The preconfigured amount may be changed dynamically as additional data is collected .By boosting the probabilities , the speech recognizer is more likely to recognize the user 's response to the prompt , thereby making the speech recognition system more accurate .", "label": "", "metadata": {}, "score": "59.281563"}
{"text": "In Proceedings of the Sixth International Workshop on Parsing Technologies .Pereira , F. , and S. M. Shieber ( 1987 ) .Prolog and Natural - Language Analysis .Stanford , CA : CSLI Publications .MATH .Pereira , F. C. , and D. H. Warren ( 1983 ) .", "label": "", "metadata": {}, "score": "59.31343"}
{"text": "counts are more robust in terms of manipulations of the tree .Backoff weights are eliminated from the required components of the format since these weights may be computed easily from the count data .Backoff weights may optionally be included as an addendum , to be described later .", "label": "", "metadata": {}, "score": "59.45875"}
{"text": "Generally , the 2 sets or probabilities , the original ones with 0.20 for each entry , were interpolated with the new probabilities computing by normalizing the weighted counts from processing the weighted prompts R(prompt ) with the grammar .Unlike an embodiment incorporating grammar induction , in this embodiment , a new grammar is created from everything in R(prompt ) that is a subset of the initial grammar .", "label": "", "metadata": {}, "score": "59.607563"}
{"text": "The corresponding CFG compatible references are shown .The first example shows a reference to a grammar .Generally starting tokens are determined by the monogram probabilities of the N - Gram model .The second example shows a reference to a grammar and a particular starting symbol .", "label": "", "metadata": {}, "score": "59.689194"}
{"text": "A method , a system , and an apparatus biasing a speech recognizer based on prompt context .The present invention is capable of analyzing the words used in the prompt given to the user .Then , a set of words the user is likely to say in response to the prompt is determined .", "label": "", "metadata": {}, "score": "59.985237"}
{"text": "To do this we first need to define the length of a rule application formally : .Definition 2 .We will use this idea , but in order to ensure that our modified versions of the training algorithms provide consistent grammars - like the original versions do for SCFGs - we need some technical additions to this idea .", "label": "", "metadata": {}, "score": "60.256706"}
{"text": "A mutual fund grammar or a language model is likely to be used in recognizing the user 's response .While such mechanisms reflect the probabilities that particular words will be spoken by the user , the probabilities are determined through an empirical study of a text corpus with little or no concern over the particular questions asked to obtain user responses .", "label": "", "metadata": {}, "score": "60.56752"}
{"text": "Note that if pruning has been performed then the branching values must be recomputed accordingly .The depth of the tree is implied by the structure of the data .Line breaks are significant in this format since the leaf branch counts have been elided .", "label": "", "metadata": {}, "score": "60.765724"}
{"text": "However it has been shown that this co - transcriptional folding has an effect on the resulting secondary structures ( e.g. , [ 6 , 7 ] ) .Since the simulation algorithms have the downside of being computationally expensive , it is desirable to add the effects of co - transcriptional folding into the traditional algorithms .", "label": "", "metadata": {}, "score": "61.07794"}
{"text": "We then compared these predicted structures to the structures from the database , computing two commonly used criteria to measure the quality : .Sensitivity ( also called recall ) : The relative frequency of correctly predicted base pairs among base pairs that appear in the correct structure .", "label": "", "metadata": {}, "score": "61.221027"}
{"text": "There may be additional difficulties that arise if the prompt developer is different from the grammar developer .However , by biasing the probabilities the of the specific fund names noted in the prompt within the general FUND grammar , many of these difficulties can be avoided .", "label": "", "metadata": {}, "score": "61.24656"}
{"text": "InsideOutside ) .( d ) Handling of dense and sparse grammars ( arrays vs lists , see parser .EarleyParserDense / EarleyParserSparse ) .( e ) Compute closure matrices efficiently in a way that avoids inverting large matrices as described in Stolcke 's paper ( see base .", "label": "", "metadata": {}, "score": "61.38914"}
{"text": "Aside from this consideration and the restrictions implied by Definition 5 ( consistency of a set of intervals ) there is no obvious criterion that helps with deciding on a set of intervals .Thus we created several different sets ranging from approximately 10 to approximately 100 intervals , evaluating a subset of the prediction data with each of them .", "label": "", "metadata": {}, "score": "61.608635"}
{"text": "In Proceedings of the Speech and Natural Language Workshop , 241 - 246 .DARPA .Mohri , M. ( 1997 ) .Finite - state transducers in language and speech processing .Computational Linguistics 23(4):269 - 311 .MathSciNet .Moore , R. C. ( 2000 ) .", "label": "", "metadata": {}, "score": "61.80553"}
{"text": "The method of . claim 13 , wherein the interpolated n - gram language model is reset to the static n - gram language model after the user cancels a prompt and resets the system to generate a new prompt .The method of .", "label": "", "metadata": {}, "score": "62.237442"}
{"text": "i . ) j .n .p .j . )P .n .A . i .P .i . w .T .i .R .n .w . else .Lemma 1 .Proof .", "label": "", "metadata": {}, "score": "62.641598"}
{"text": "claim 1 , wherein an interpolated grammar is formed by inducing a grammar from the response set and interpolating with a static grammar of what responses the user might say at any time .The method of .claim 3 , wherein the interpolated grammar has weights of the responses and grammars adjusted therein based upon previous responses to the prompt .", "label": "", "metadata": {}, "score": "62.671448"}
{"text": "claim 37 , wherein the additional step is inducing new embedded grammars from the response set and interpolating with a static grammar of what responses the user might say at any time to form interpolated embedded grammars ; .further wherein the adapted n - gram language model is reset to the static n - gram language model after the user cancels a prompt and resets the system to generate a new prompt .", "label": "", "metadata": {}, "score": "62.690865"}
{"text": "In contrast , a distant N - Gram with a gap of 1 provides counts for AB .D , BC .Distant N - Grams are stored in the same tree structure as regular N - Grams .Assuming that the ' gap ' always occurs between the current word and its history , only the length of the gap has to be specified .", "label": "", "metadata": {}, "score": "62.73791"}
{"text": "Importation of backoff weights is generally not useful since modification of the event counts generally alters the full set of backoff weights , which should be recomputed after all N - Gram event counts are compiled .An arbitrary number of importation rules may optionally be declared as follows : .", "label": "", "metadata": {}, "score": "62.861225"}
{"text": "The speech recognition system boosts the probabilities of the analyzed words in the word set by a preconfigured amount .The preconfigured amount is selected based on collected data .A method of biasing a speech recognition system at a lexical level based on prompt context comprising : . prompting a user for a response . determining a set of words the user is likely to use when responding to the prompt based on the analyzing step , wherein the word set is determined using a speech recognition system strategy including at least one of n - gram language models and grammars ; and . boosting probabilities of the determined set of words in a response from the user to the prompt by an adjustable predetermined amount such that a speech recognizer of the speech recognition system has an increased likelihood of recognizing the response from the user .", "label": "", "metadata": {}, "score": "62.894424"}
{"text": "\" When used in a grammar - based system , the present invention is capable of examining which grammar will be used for the next turn .As such , it is possible to keep the base grammar unchanged , but boost the probabilities for what the user is likely to say .", "label": "", "metadata": {}, "score": "62.894463"}
{"text": "The N - Gram Grammar declaration is consistent with the XML format of the structural grammar specification as described in Section 4.2 of that document .The document type definition for the N - Gram specification is given in Section 11 .", "label": "", "metadata": {}, "score": "63.402584"}
{"text": "However , in other embodiments , such as for dollar amounts , it would not be beneficial to generate new grammars dynamically .In a system using an n - gram LM with embedded grammars , there are additional complexities that arise other than those associated with only an n - gram LM system or a grammar system .", "label": "", "metadata": {}, "score": "63.974968"}
{"text": "The bias does not require the subsystem generating the prompt to provide any information for this to take place , which is what is usually done .The speech recognition system analyzes the words used in the prompt given to the user .", "label": "", "metadata": {}, "score": "64.04518"}
{"text": "There are many possible ways to combine N - Gram models and context free grammars within a single voice browser system such as .using an N - Gram model in the recognizer and a CFG in a ( separate ) understanding component . integrating special N - Gram rules at various levels in a CFG to allow for flexible input in specific context .", "label": "", "metadata": {}, "score": "64.23231"}
{"text": "The present invention provides a method to adapt the grammars of what has already been determined to be an allowable response .G(prompt ) and G(base ) then can be interpolated .As mentioned , R(prompt ) may be used to create a G(prompt ) , which is then rulewise interpolated with G(base ) .", "label": "", "metadata": {}, "score": "64.42269"}
{"text": "I . i . a .i .A . i .w .P .n .A . i . w . ) by induction on i .From this the result follows by Lemma 1 .For the first kind the construction of the bijection in Lemma 1 guarantees that .", "label": "", "metadata": {}, "score": "64.443146"}
{"text": "The N - Gram lexicon section consists of a single lexicon tag set containing lexical entries to define indices for the succeeding N - Gram event count rules .A lexical entry may contain a word symbol or rule reference .Rule references are always references to an external inferior grammar rule .", "label": "", "metadata": {}, "score": "64.46128"}
{"text": "expected ) number of occurrences for each rule globally we determine separate counts for each length interval and use these to compute separate ( expected ) relative frequencies .This works since each occurrence ( resp .probability of occurrence ) is tied to a specific subword and thus associated a length .", "label": "", "metadata": {}, "score": "64.62028"}
{"text": "Thus this case is implicitly included in the following definitions and proofs .Definition 6 . r .A .R .P .r . )f .r .T . ) s .A .R .f .", "label": "", "metadata": {}, "score": "64.64549"}
{"text": "B 2,5 ; 1,1,3 ; 2,1,2 ; 1,2 ; 2,1,2 ; 1,1,2 ; 2,1 ; .This is the XML document type definition for the N - Gram specification : .The following pure XML format is not required for compliance , but is suggested for those who prefer use a pure XML reader .", "label": "", "metadata": {}, "score": "64.72992"}
{"text": "Optionally , precomputed backoff weights may be declared ( cf .Section 7 ) , and optional distant or skip N - Grams may be declared ( cf .Section 8 ) .In the event that all optional sections of the grammar declaration are missing , the grammar is a null grammar equivalent to an epsilon - transition or zerogram model .", "label": "", "metadata": {}, "score": "64.79266"}
{"text": "The two main advantages of this ordering are the elimination of some redundancy , hence reducing file size , and more convenient ordering for stream processing and data loading .The file format consists lines of data tuples , each representing a branch and the succeeding node of the grammar tree .", "label": "", "metadata": {}, "score": "64.80534"}
{"text": "Technical Report CSL-80 - 12 , Xerox PARC , Palo Alto , CA .Klein , D. , and C. D. Manning ( 2001 a ) .An O(n 3 ) agenda - based chart parser for arbitrary probabilistic context - free grammars .", "label": "", "metadata": {}, "score": "64.8699"}
{"text": "n .A . i .P . a .i . w .T .i . w . else .The second restriction obviously only applies if lengths are to be grouped into intervals as we will do in this paper .", "label": "", "metadata": {}, "score": "64.90836"}
{"text": "In order to do so we took the tRNA database from [ 14 ] , filtered out all sequences with unidentified bases and split the remaining data into a training set of 1285 sequences and a benchmark set of 1284 sequences .", "label": "", "metadata": {}, "score": "65.05589"}
{"text": "claim 24 , wherein the interpolated n - gram language model is reset to the static n - gram language model after the user cancels a prompt and resets the system to generate a new prompt .The method of .claim 23 , wherein the interpolated embedded grammars have weights of the responses adjusted therein based upon previous responses to the prompt .", "label": "", "metadata": {}, "score": "65.29027"}
{"text": "[ Google Scholar ] .Zuker , M. ; Stiegler , P. Optimal computer folding of large RNA sequences using thermodynamics and auxiliary information .Nucleic Acids Res .[ Google Scholar ] .Knudsen , B. ; Hein , J. RNA secondary structure prediction using stochastic context - free grammars and evolutionary history .", "label": "", "metadata": {}, "score": "65.442"}
{"text": "Then , in step 110 , the system determines R(prompt ) .R(prompt ) is determined using the strategy ( or technology ) used next for the speech recognition system .As previously discussed , this can be a strategy or technique using n - grams , grammars , or both .", "label": "", "metadata": {}, "score": "65.478966"}
{"text": "claim 37 , wherein the additional step is inducing new embedded grammars from the response set and interpolating with a static grammar of what responses the user might say at any time to form interpolated embedded grammars ; .further wherein the adapted n - gram language model is reset to the static n - gram language model after the user responds to the prompt .", "label": "", "metadata": {}, "score": "65.87561"}
{"text": "Most recent research in trainable part of speech taggers has explored stochastic tagging .While these taggers obtain high accuracy , linguistic information is captured indirectly , typi - cally in tens of thousands of lexical and contextual probabili - ties .", "label": "", "metadata": {}, "score": "66.20467"}
{"text": "Most recent research in trainable part of speech taggers has explored stochastic tagging .While these taggers obtain high accuracy , linguistic information is captured indirectly , typi - cally in tens of thousands of lexical and contextual probabili - ties .", "label": "", "metadata": {}, "score": "66.20467"}
{"text": "claim 1 , wherein the speech recognition system strategy uses n - gram language models with embedded grammars for named entries .The method of . claim 21 , wherein the n - gram language models with embedded grammars strategy comprises : . creating an interpolated n - gram language model that is formed by interpolating the response set and with a static n - gram language model of responses the user might say at any time ; . analyzing responses in the response set to determine values mentioned for the embedded grammars ; and . creating new embedded grammars from values mentioned in the prompt using one additional step ; . wherein the additional step is selected from : . a ) inducing new embedded grammars from the response set and interpolating with a static grammar of what responses the user might say at any time to form interpolated embedded grammars ; and .", "label": "", "metadata": {}, "score": "66.23533"}
{"text": "This is the most conventional lexicon entry .The second example indicates a \" super - word \" or word phrase that is used when the co - occurrence of words is so frequent that they might as well be treated as a single word .", "label": "", "metadata": {}, "score": "66.47412"}
{"text": "B 2,5 ; 1,1,3 ; 2,1,2 ; ; 1,2 ; 2,1,2 ; 1,1,2 ; ; 2,1 ; .Semantic tags are written in XML format and appended to the appropriate N - Gram count declaration .Please note that further study is planned for semantic markup for N - Grams .", "label": "", "metadata": {}, "score": "66.73404"}
{"text": "Runtime .The considerations in Note 1 lead to the assumption that both versions should take about the same time on a given grammar .This was confirmed during our experiments , with none of the versions being consistently faster , i.e. , if there is a difference it was overshadowed by effects like system load .", "label": "", "metadata": {}, "score": "67.576416"}
{"text": "P .n .A . i .P .n .w . ) , where each of the \u03b2 contains at most one symbol B i contributing a factor of b i along with an arbitrary number of other symbols ( terminal or with smaller indices ) contributing factors of 0 or 1 by the induction hypothesis .", "label": "", "metadata": {}, "score": "67.6212"}
{"text": "Following the example a complete declaration is : .A B C 3,5 ; 1,1,2 ; 2,2,2 ; 1,1 ; 3,1 ; 2,2,2 ; 1,1,1 ; 2,1 ; 3,1 ; 3,1 ; .Intraline delimiters are commas and semi - colons are used to indicate the end of an N - Gram rule .", "label": "", "metadata": {}, "score": "67.68842"}
{"text": "Following the example a declaration of backoff weights is : .A B C 3,5 ; 1,1,2:0.543 ; 2,2,2:0.54 ; 1,1 ; 3,1 ; 2,2,2:0.54 ; 1,1,1:0.543 ; 2,1 ; 3,1 ; 3,1 ; .Weight delimiters are colons .Backoff weights may only be attached to non - leaf elements and are indicated by a leading colon .", "label": "", "metadata": {}, "score": "67.827194"}
{"text": "Therefore , it would be beneficial to ascertain the classes available to be used within the set of allowable responses to the prompt .The present invention attempts to ascertain these classes using a variety of different techniques .It may be possible for the system to determine these classes from the dialog context .", "label": "", "metadata": {}, "score": "67.954216"}
{"text": "2 is a flow chart illustrating a method for biasing a speech recognizer based on prompt context in accordance with another embodiment of the present invention .The method may begin in step 200 wherein the prompt generation module generates a prompt for the user .", "label": "", "metadata": {}, "score": "68.64562"}
{"text": "In addition , log - linear interpolation of models is possible .Therefore no additional special markup language is needed for the declaration of class grammars .Continuing the example of Section 6 , let us declare that \" A \" and \" C \" are equally probable members of a class named \" firstclass \" .", "label": "", "metadata": {}, "score": "68.64902"}
{"text": "claim 37 , wherein the additional step is inducing new embedded grammars from the response set and interpolating with a static grammar of what responses the user might say at any time to form interpolated embedded grammars ; .further wherein the interpolated embedded grammars are used for future responses .", "label": "", "metadata": {}, "score": "68.83553"}
{"text": "Suppose we have the pseudo - corpus \" A B A B C \" .Zerogram information represents the root node of a tree .In this case 5 token instances of 3 distinct token types were seen in the corpus .", "label": "", "metadata": {}, "score": "68.91874"}
{"text": "2005 , 33 , W605-W610 .[ Google Scholar ] .Boyle , J. ; Robillard , G.T. ; Kim , S. Sequential Folding of Transfer RNA .J. Mol .Biol .[ Google Scholar ] .Meyer , I. ; Miklos , I. Co - transcriptional folding is encoded within RNA genes .", "label": "", "metadata": {}, "score": "68.96155"}
{"text": "Studies have shown , however , that within the context of a conversational speech recognition system , users tend to vary their replies based upon the particular prompt to which the users are responding .More particularly , users tend to repeat words from the prompt when responding .", "label": "", "metadata": {}, "score": "68.972336"}
{"text": "In directed dialog systems like VoiceXML , the program that generates the prompt also returns the grammars used on the next turn to decode the prompt .With respect to grammar - based systems and development , the grammar developer may be different from the prompt developer thereby causing a disconnect with respect to incorporating feedback from the prompts to the grammars .", "label": "", "metadata": {}, "score": "68.98398"}
{"text": "Springer Verlag .Kasami , T. ( 1965 ) .An efficient recognition and syntax analysis algorithm for context - free languages .Technical Report AFCRL-65 - 758 , Air Force Cambridge Research Laboratory , Bedford , MA .Kay , M. ( 1980 ) .", "label": "", "metadata": {}, "score": "69.19466"}
{"text": "Following the word sequence data is a list of one or two integers representing the node branching factor and event count .Consistent with this tuple per line format , the first entry is a ' zerogram ' , a virtual null branch with successor node being the actual root of the grammar tree ( see pseudo - code example later ) .", "label": "", "metadata": {}, "score": "69.260666"}
{"text": "Concerning the other grammars we note that adding length - dependency significantly improved the results on tRNA while they became worse on the mixed set .A possible explanation for these results could be that the correct parameters for the folding are different for different types of RNA .", "label": "", "metadata": {}, "score": "69.36478"}
{"text": "4 is a flow chart illustrating a method for biasing a speech recognizer based on prompt context in accordance with another embodiment of the present invention .The method may begin in step 400 wherein the prompt generation module generates a prompt for the user .", "label": "", "metadata": {}, "score": "69.40336"}
{"text": "Lexical attraction is defined as the likelihood of such relations .I introduce a new class of probabilistic language models named lexical attraction models which can represent long distance relations between words and I formalize this new class of models using information theory .", "label": "", "metadata": {}, "score": "69.46284"}
{"text": "At some point in the near future it is expected that these documents will be unified to ensure consistency among the common components of the specifications .To simplify this unification this document also borrows from some of the CFG examples .", "label": "", "metadata": {}, "score": "69.59778"}
{"text": "Accordingly , in general , the present invention biases the speech recognition system by determining one or more words a user of the system is likely to say in response to a prompt .This set may be defined as \" R(prompt ) \" .", "label": "", "metadata": {}, "score": "69.598114"}
{"text": "Open vocabulary applications are easily supported with N - Gram grammars .This specification is influenced by a variety of preceding N - Gram grammar formats .This specification is not explicitly based on any particular preceding format .Concepts are similar but the syntax is largely original in this specification due to the XML parent language .", "label": "", "metadata": {}, "score": "69.612946"}
{"text": "Following the XML convention the language and variant are indicated by a \" xml : lang \" attribute on the root \" grammar \" element .[ importation declarations ] .[ lexicon declaration .[ N - Gram event counts .", "label": "", "metadata": {}, "score": "69.642914"}
{"text": "Thus we have to distinguish ( in the grammar ) between terminal symbols representing unpaired bases and terminal symbols being part of a base pair .In the above grammars the former are denoted by the symbol b while the latter are denoted by pairs a , \u00e2 .", "label": "", "metadata": {}, "score": "69.737915"}
{"text": "claim 37 , wherein the additional step is inducing new embedded grammars from the response set and interpolating with a static grammar of what responses the user might say at any time to form interpolated embedded grammars ; .further wherein the interpolated embedded grammars have weights of the responses adjusted therein based upon previous responses to the prompt .", "label": "", "metadata": {}, "score": "69.75488"}
{"text": "We give experimental results showing that , beginning with a high - performance PCFG , one can develop a pseudo PCSG that yields significant performance gains .Analysis shows that the benefits from the context - sensitive statistics are localized , suggesting that we can use them to extend the original PCFG .", "label": "", "metadata": {}, "score": "70.25397"}
{"text": "We give experimental results showing that , beginning with a high - performance PCFG , one can develop a pseudo PCSG that yields significant performance gains .Analysis shows that the benefits from the context - sensitive statistics are localized , suggesting that we can use them to extend the original PCFG .", "label": "", "metadata": {}, "score": "70.25397"}
{"text": "further wherein the probabilistically adjusted embedded grammars are reset to the static grammar after the user responds to the prompt .The method of .further wherein the probabilistic ally adjusted embedded grammars are reset to the static grammar after the user cancels a prompt and resets the system to generate a new prompt .", "label": "", "metadata": {}, "score": "70.29282"}
{"text": "claim 16 , wherein the adapted n - gram language model has weights of the adapted language model and static language model adjusted therein based upon previous responses to the prompt .The method of .claim 16 , wherein the adapted n - gram language model is used for future responses .", "label": "", "metadata": {}, "score": "70.65483"}
{"text": "This occurs because of the weights in R(prompt ) .Once normalized , in a second step , the probability of the choices in the rule are 0.66 for \" I would like to \" , 0.33 for \" we would like to \" , and 0.0 for the other 3 choices .", "label": "", "metadata": {}, "score": "70.7222"}
{"text": "3 is a flow chart illustrating a method for biasing a speech recognizer based on prompt context in accordance with another embodiment of the present invention .The method may begin in step 300 wherein the prompt generation module generates a prompt for the user .", "label": "", "metadata": {}, "score": "70.77645"}
{"text": "Then , in step 330 , the system determines rules within a static grammar or the n - grams in a static n - gram LM to be boosted or increased .In step 340 , the system interpolates the grammar rules and boosts probabilities of selected grammar rules such that when the user speaks , step 350 , the system has an increased likelihood of recognizing the speech in step 360 .", "label": "", "metadata": {}, "score": "70.85359"}
{"text": "The algorithm is applied to 7 major types of ambiguity where context can be used to choose a word 's pronunciation . ... versesHe covered the hull with lead ) .In general , we will resolve the part - of - speech ambiguitysrst , and then resolve the additional semantic ambiguity if present .", "label": "", "metadata": {}, "score": "70.99325"}
{"text": "For example , if a user is prompted for a fidelity fund that they would like to sell as part of a transfer , and two choices are provided , it is very likely that the user will pick one of the two .", "label": "", "metadata": {}, "score": "71.01735"}
{"text": "This may be particularly useful for class grammars where several alternative expressions with the same semantics should yield the same output , i.e. the semantic tag .If defined , semantic tags take precedence over other interpretations .Continuing our example , we declare the occurrence of \" B X \" and \" X B \" to be identical semantic events of type \" BX \" , where \" X \" represents an instance of class \" firstclass \" .", "label": "", "metadata": {}, "score": "71.17986"}
{"text": "Other pairs are less stable and hence less common ) , thus folding the molecule to a complex three - dimensional layout called the tertiary structure .As determining the tertiary structure is computationally complex , it has proven convenient to first search for the secondary structure , for which only a subset of the hydrogen bonds is considered , so that the molecule can be modeled as a planar graph .", "label": "", "metadata": {}, "score": "71.7851"}
{"text": "The lexicon , which is required if N - Gram counts are specified , contains index definition of symbols that may represent speech events ( i.e. words ) or references to other grammars or grammar rules ( cf .Section 5 ) .", "label": "", "metadata": {}, "score": "71.98556"}
{"text": "[ Google Scholar ] .Andersen , E.S. Prediction and design of DNA and RNA structures .New Biotechnol .[ Google Scholar ] .Xayaphoummine , A. ; Bucher , T. ; Isambert , H. Kinefold web server for RNA / DNA folding path and structure prediction including pseudoknots and knots .", "label": "", "metadata": {}, "score": "72.00064"}
{"text": "For example when starting transcription at the marked end the structure from Figure 1 would be denoted by the word .The oldest and most commonly used method for computing the secondary structure is to determine the structure with minimum free energy .", "label": "", "metadata": {}, "score": "72.19477"}
{"text": "further wherein the adapted n - gram language model is reset to the static n - gram language model after the user responds to the prompt .The method of .further wherein the adapted n - gram language model is reset to the static n - gram language model after the user cancels a prompt and resets the system to generate a new prompt .", "label": "", "metadata": {}, "score": "72.27571"}
{"text": "Note :In this tree format , we can only fall back from the distant trigram to a regular bigram , not to a distant bigram .Fallback to gap N - Grams would require a different ordering of the tree .", "label": "", "metadata": {}, "score": "72.29008"}
{"text": "The following gives , as we will show afterwards , a sufficient condition that they do .Definition 5 .Intuitively to satisfy this condition we may not group lengths i and j together if there is a rule that can lead to a word of length i but not to one of length j ( or vice versa ) .", "label": "", "metadata": {}, "score": "72.48532"}
{"text": "T .i . w .A .R .w .T .i . otherwise .and .i .P .i . )f .i .T . )T .Proof .We show that .", "label": "", "metadata": {}, "score": "72.662346"}
{"text": "At that point , the method may return to step 200 .The system also stores the prompt and replies to the prompt recognized by the system at 280 .Additionally , after speech recognition 260 , the interpolation weights may be adjusted based upon the recognized response .", "label": "", "metadata": {}, "score": "72.681625"}
{"text": "In addition to floating point format , a scaled integer format is supported .The element is modified to include a scale attribute as follows : .Following the example a scaled integer equivalent declaration of backoff weights is : .A B C 3,5 ; 1,1,2:543 ; 2,2,2:540 ; 1,1 ; 3,1 ; 2,2,2:540 ; 1,1,1:543 ; 2,1 ; 3,1 ; 3,1 ; .", "label": "", "metadata": {}, "score": "72.83399"}
{"text": "Specifications will be defined in lavender boxes and examples will be given in green boxes .Why N - Grams ?In simple speech recognition / speech understanding systems , the expected input sentences are often modeled by a strict grammar ( such as a CFG ) .", "label": "", "metadata": {}, "score": "72.90468"}
{"text": "In step 435 , grammar choices can be determined from embedded grammars mentioned or referenced in the prompts .The grammars can be interpolated or the probabilities of rules of one or more of the embedded grammars can be boosted in step 445 .", "label": "", "metadata": {}, "score": "73.096344"}
{"text": "Computational Linguistics 21:165- 202 .MathSciNet .Younger , D. H. ( 1967 ) .Recognition and parsing of context free languages in time n 3 .Information and Control 10:189 - 208 .MATH CrossRef Description : The Earleyx parser was originated from Roger Levy 's prefix parser , but has evolved significantly .", "label": "", "metadata": {}, "score": "73.15241"}
{"text": "2 is a flow chart illustrating a method for biasing a speech recognizer based on prompt context in accordance with another embodiment of the present invention .FIG .3 is a flow chart illustrating a method for biasing a speech recognizer based on prompt context in accordance with yet another embodiment of the present invention .", "label": "", "metadata": {}, "score": "73.659134"}
{"text": "claim 37 , wherein the additional step is inducing new embedded grammars from the response set and interpolating with a static grammar of what responses the user might say at any time to form interpolated embedded grammars ; .further wherein the interpolated embedded grammars are reset to the static grammar after the user cancels a prompt and resets the system to generate a new prompt .", "label": "", "metadata": {}, "score": "74.16808"}
{"text": "The present invention increases efficiency of speech recognition systems while still maintaining accuracy .Many users of speech - based user interfaces will vary their response based upon the prompt given to them .This variance affects conversational speech recognition systems based upon an n - gram language model ( \" LM \" ) .", "label": "", "metadata": {}, "score": "74.20554"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .There are shown in the drawings , embodiments which are presently preferred , it being understood , however , that the invention is not limited to the precise arrangements and instrumentalities shown .FIG .1 is a flow chart illustrating a method for biasing a speech recognizer based on prompt context in accordance with the inventive arrangements disclosed herein .", "label": "", "metadata": {}, "score": "74.80509"}
{"text": "Text corpora which are tagged with part - o[-speech information are useful in many areas of linguistic research .In this paper , a new part - of - speech tagging method based on neural networks ( Net-7h . qger ) is presented and its performance is compared to that of a 11IvlM - tagger ( Cutting ct al . , 1992 ) anti ... \" .", "label": "", "metadata": {}, "score": "74.82211"}
{"text": "At that point , the method may return to step 400 .The system also stores the prompt and replies to the prompt recognized by the system at 480 .Additionally , after speech recognition 460 , the interpolation weights may be adjusted based upon the recognized response .", "label": "", "metadata": {}, "score": "74.91908"}
{"text": "claim 11 , wherein the interpolated n - gram language model has weights of the induced language model and static language model adjusted therein based upon previous responses to the prompt .The method of .claim 11 , wherein the interpolated n - gram language model is used for future responses .", "label": "", "metadata": {}, "score": "75.36741"}
{"text": "claim 37 , wherein the additional step is inducing new embedded grammars from the response set and interpolating with a static grammar of what responses the user might say at any time to form interpolated embedded grammars ; .further wherein the interpolated embedded grammars are reset to the static grammar after the user responds to the prompt .", "label": "", "metadata": {}, "score": "75.44208"}
{"text": "The method of .claim 23 , wherein the interpolated n - gram language model is used for future responses .The method of .claim 24 , wherein the interpolated n - gram language model is reset to the static n - gram language model after the user responds to the prompt .", "label": "", "metadata": {}, "score": "75.65067"}
{"text": "Following the example a complete declaration is : . B .Naming a node replaces the normal syntactic output with the semantic tag name .Hence , input string \" A B A B C \" will now yield the interpretation \" BX BX X(C ) \" indicating the occurrence of two semantic events \" BX \" followed by an instance of member \" C \" of class \" X \" .", "label": "", "metadata": {}, "score": "75.75488"}
{"text": "A single optional grammar declaration is allowed in the XML grammar document .This grammar declaration may be imported into a parent N - Gram or CFG declaration and may in turn import other N - Gram or CFG declarations as described by import rules ( cf .", "label": "", "metadata": {}, "score": "76.25404"}
{"text": "At that point , the speech recognition may return to step 300 .The system also stores the prompt and replies to the prompt from the user at 380 .Additionally , after speech recognition 360 , the interpolation weights may be adjusted based upon the user 's response .", "label": "", "metadata": {}, "score": "76.40909"}
{"text": "This document has been produced as part of the W3C Voice Browser Activity , following the procedures set out for the W3C Process .The authors of this document are members of the Voice Browser Working Group ( W3C Members only ) .", "label": "", "metadata": {}, "score": "76.646095"}
{"text": "The present invention may be realized in hardware , software , or a combination of hardware and software .The present invention may be realized in a centralized fashion in one computer system , or in a distributed fashion where different elements are spread across several interconnected computer systems .", "label": "", "metadata": {}, "score": "77.18074"}
{"text": "4 is a flow chart illustrating a method for biasing a speech recognizer based on prompt context in accordance with still another embodiment of the present invention .DETAILED DESCRIPTION OF THE INVENTION .The present invention provides a method , a system , and an apparatus for biasing a speech recognition system based on prompt context .", "label": "", "metadata": {}, "score": "77.34167"}
{"text": "Since this value is always zero at the leaves , this information can simply be deleted .The token types can be super - word tokens or even grammar instances .For example , if \" A B \" is a token then the pseudo - corpus would appear to consist of 3 tokens total and there would be 2 token types .", "label": "", "metadata": {}, "score": "77.42482"}
{"text": "Due to co - transcriptional folding one would expect that the probability of two bases being paired depends on how far the bases are apart , and the probability of a part of the molecule forming a specific motif depends on how large the part of the molecule is .", "label": "", "metadata": {}, "score": "77.73654"}
{"text": "the size of a motif ) is just the size of the subword that results from the rule application introducing the base pair as first and last symbol ( resp .starting building of the motif ) , assuming such a rule application exists .", "label": "", "metadata": {}, "score": "78.17749"}
{"text": "Wokingham , England : Addison - Wesley .Goodman , J. ( 1998 ) .Parsing inside - out .PhD thesis , Harvard University .Graham , S. L. , M. A. Harrison , and W. L. Ruzzo .An improved context - free recognizer .", "label": "", "metadata": {}, "score": "78.685104"}
{"text": "Biol .[ Google Scholar ] .Harrison , M.A. Introduction to Formal Language Theory ; Addison - Wesley : Boston , MA , USA , 1978 .[ Google Scholar ] .Durbin , R. ; Eddy , S.R. ; Krogh , A. ; Mitchison , G. Biological Sequence Analysis ; Cambridge University Press : Cambridge , UK , 1998 .", "label": "", "metadata": {}, "score": "79.400116"}
{"text": "A typical combination of hardware and software may be a general purpose computer system with a computer program that , when being loaded and executed , controls the computer system such that it carries out the methods described herein .The present invention also may be embedded in a computer program product , which comprises all the features enabling the implementation of the methods described herein , and which when loaded in a computer system is able to carry out these methods .", "label": "", "metadata": {}, "score": "79.56165"}
{"text": "SUMMARY OF THE INVENTION .The present invention provides a method , a system , and an apparatus for biasing a speech recognizer based on prompt context .More specifically , the present invention is capable of biasing the speech recognizer 's speech grammars and/or language models to increase the recognition accuracy for the likely responses to a prompt .", "label": "", "metadata": {}, "score": "79.66048"}
{"text": "Their training set consists of 139 each large and small subunit rRNAs , the benchmark dataset contains 225 RNase Ps , 81 SRPs and 97 tmRNAs .Since it contains different types of RNA we will refer to this set as the mixed set for the remainder of this article .", "label": "", "metadata": {}, "score": "80.00224"}
{"text": "Figure 1 .Example of an RNA secondary structure .Letters represent bases , the colored band marks the phosphordiester bonds , short edges mark hydrogen bonds .( The different colors only serve to identify the corresponding parts in the formal language represantation below . )", "label": "", "metadata": {}, "score": "80.172935"}
{"text": "In this embodiment , R(prompt ) is a grammar and the probabilities computed by this grammar , rather than being used to boost the probabilities of a base grammar , are instead used to replace the probabilities .In one embodiment , the present invention boosts the probabilities of the current grammar .", "label": "", "metadata": {}, "score": "81.08438"}
{"text": "The availability of on - line corpora is rapidly changing the field of natural language processing ( NLP ) from one dominated by theoretical models of often very specific linguistic phenomena to one guided by computational models that simultaneously account for a wide variety of phenomena that occur in real - world text .", "label": "", "metadata": {}, "score": "81.09581"}
{"text": "l .c .l . where c \u03b1 , l is the number of different assignments of lengths to the symbols of \u03b1 that satisfy : .Terminals are always assigned a length of 1 .Words are generated as for usual context - free grammars .", "label": "", "metadata": {}, "score": "81.440186"}
{"text": "BACKGROUND .Field of the Invention .The present invention relates to the field of speech recognition and , more particularly , to speech - based user interfaces .Description of the Related Art .Conventional data processing systems frequently incorporate speech - based user interfaces to provide users with speech access to a corpus of data stored and managed by a data processing system .", "label": "", "metadata": {}, "score": "81.55766"}
{"text": "Keywords : . stochastic context - free grammar ; length - dependency ; RNA secondary structure prediction .Introduction .Single - stranded RNA molecules consist of a sequence of nucleotides connected by phosphordiester bonds .Nucleotides only differ by the bases involved , them being adenine , cytosine , guanine and uracil .", "label": "", "metadata": {}, "score": "82.177185"}
{"text": "Example of an RNA secondary structure .Letters represent bases , the colored band marks the phosphordiester bonds , short edges mark hydrogen bonds .( The different colors only serve to identify the corresponding parts in the formal language represantation below . )", "label": "", "metadata": {}, "score": "82.51103"}
{"text": "In step 430 , the system determines n - grams in the LM to be boosted or increased .In step 440 , the system interpolates LMs or boosts n - gram probabilities , such that when the user speaks , step 450 , the system has an increased likelihood of recognizing the speech in step 460 .", "label": "", "metadata": {}, "score": "82.92888"}
{"text": "claim 31 , wherein the probabilistically adjusted embedded grammars are reset to the static grammar after the user responds to the prompt .The method of .claim 31 , wherein the probabilistically adjusted embedded grammars are reset to the static grammar after the user cancels a prompt and resets the system to generate a new prompt .", "label": "", "metadata": {}, "score": "83.05495"}
{"text": "claim 3 , wherein the interpolated grammar is used for future responses .The method of .claim 3 , wherein the interpolated grammar is reset to the static grammar after the user responds to the prompt .The method of .", "label": "", "metadata": {}, "score": "84.42987"}
{"text": "In step 230 , the system induces a grammar or a LM from these responses .In step 240 , the system determines an interpolation grammar that is used by the system such that when the user speaks , step 250 , the system has an increased likelihood of recognizing the speech in step 260 .", "label": "", "metadata": {}, "score": "84.85324"}
{"text": "claim 32 , wherein the interpolated n - gram language model is reset to the static n - gram language model after the user responds to the prompt .The method of .claim 32 , wherein the interpolated n - gram language model is reset to the static n - gram language model after the user cancels a prompt and resets the system to generate a new prompt .", "label": "", "metadata": {}, "score": "85.06256"}
{"text": "claim 23 , wherein the interpolated embedded grammars are used for future responses .The method of .claim 23 , wherein the interpolated embedded grammars are reset to the static grammar after the user responds to the prompt .The method of .", "label": "", "metadata": {}, "score": "85.63836"}
{"text": "word1 how many ... .or as follows : .word1 how many ... .Tokens must be indexed with non - negative integers .Numbering should be contiguous to minimize storage needed for indexing , but this is not required and the data can be presented in any particular order .", "label": "", "metadata": {}, "score": "85.76573"}
{"text": "For example : . money .Then in a first step , when the phrases in R(prompt ) are processed , by the grammar , only the 2nd and 3rd phrases in R(prompt ) are parsable by the grammar .The grammar does not allow \" withdraw my money \" , since this requires an empty prefix .", "label": "", "metadata": {}, "score": "87.11739"}
{"text": "claim 18 , wherein the adapted n - gram language model is reset to the static n - gram language model after the user responds to the prompt .The method of .claim 18 , wherein the adapted n - gram language model is reset to the static n - gram language model after the user cancels a prompt and resets the system to generate a new prompt .", "label": "", "metadata": {}, "score": "88.48682"}
{"text": "This document is a W3C Working Draft for review by W3C members and other interested parties .It is a draft document and may be updated , replaced , or obsoleted by other documents at any time .It is inappropriate to use W3C Working Drafts as reference material or to cite them as other than \" work in progress \" .", "label": "", "metadata": {}, "score": "91.069275"}
{"text": "claim 1 , wherein the step of determining a response set comprises : . inverting question - asking syntax into a response syntax ; . including at least one choice mentioned to the user ; and .utilizing at least one additional constituent mentioned in the prompt .", "label": "", "metadata": {}, "score": "92.2372"}
{"text": "The procedure is fully implemented and has proved viable and useful in practice .all Addendum Article Book Review Case Report Comment Commentary Communication Concept Paper Conference Report Correction Creative Data Descriptor Discussion Editorial Erratum Essay Interesting Images Letter New Book Received Obituary Opinion Project Report Reply Retraction Review Short Note Technical Note .", "label": "", "metadata": {}, "score": "94.5737"}
{"text": "T . ) s .A .R .f .s .T . )A . r .A .R . q .Q .i . q .P .r .i . ) j . q .", "label": "", "metadata": {}, "score": "95.17688"}
{"text": "r .j .T . ) s .A .R .j . q .f .s .j .T . ) s .A .R .j . q .f .s .j .", "label": "", "metadata": {}, "score": "98.792244"}
{"text": "i .A .B .A . i .A .i .R .n .A . i .B .R .n .P .n .w . )b .i .P .n .", "label": "", "metadata": {}, "score": "100.46214"}
{"text": "P .n .A .P .n . a . a .i .A . i .A .i .R .n .A .i .R .n .P .n .A . i .", "label": "", "metadata": {}, "score": "100.48502"}
{"text": "[ Google Scholar ] .\u00a9 2011 by the authors ; licensee MDPI , Basel , Switzerland .A method , a system , and an apparatus biasing a speech recognizer based on prompt context .The present invention is capable of analyzing the words used in the prompt given to the user .", "label": "", "metadata": {}, "score": "103.15523"}
{"text": "A . k . ) w . k . ) w .j . )T .j . k .j .k . w .j . )i .j .P .n .S .S . i . )", "label": "", "metadata": {}, "score": "104.32442"}
{"text": "L .T .P .A .I . q .Q .L .T .P .A . q .A .R .i .p .i . )f .A . i .T . )", "label": "", "metadata": {}, "score": "105.00273"}
{"text": "I .n .S .A . i .A .I . i .A . i .A .R .i .R .n .S .S . i . i .A . i .", "label": "", "metadata": {}, "score": "108.647156"}
{"text": "A .R .i .A . i . w .A . i . w .w . k .A . i . k . k . ) w . k . )A .R .w .", "label": "", "metadata": {}, "score": "110.041725"}
{"text": "University of Kaiserslautern , Department of Computer Sciences , Gottlieb - Daimler - Strasse , D-67663 Kaiserslautern , Germany .Author to whom correspondence should be addressed ; Tel . : +49 - 631 - 205 - 3979 ; Fax : +49 - 631 - 205 - 2573 .", "label": "", "metadata": {}, "score": "126.56975"}
