{"text":"Incremental versions of the Baum - Welch algorithm that approximate the β - values used in the backward procedure are commonly used for this problem , since their memory complexity is independent of ... \" .We address the problem of learning discrete hidden Markov models from very long sequences of observations .","label":"Background","metadata":{},"score":"32.335323"}
{"text":"Surprisingly , the standardly used iterative scaling algorithms perform quite poorly in comparison to the others , and for all of the test problems , a limited - memory variable metric algorithm outperformed the other choices .Robert Malouf , A comparison of algorithms for maximum entropy parameter estimation .","label":"Background","metadata":{},"score":"33.282486"}
{"text":"Based upon results from noisy simulated data , the new algorithm appears to be more robust than Misell 's approach and to produce better results from low signal - to - noise ratio data .The convergence of the new algorithm is examined .","label":"Background","metadata":{},"score":"33.962772"}
{"text":"It is emphasized that the results do not detract from the use of Markov random fields as representers of local spatial properties , which is their main purpose in the implementation of Bayesian statistical approaches to image analysis .Brief allusion is made to the issue of parameter estimation .","label":"Background","metadata":{},"score":"35.75952"}
{"text":"Our approach is applicable to any string classification problem that may be solved using a similarity function against a database of labeled prototypes .Markov random fields are typically used as priors in Bayesian image restoration methods to represent spatial information in the image .","label":"Background","metadata":{},"score":"36.014267"}
{"text":"In cases where these procedures strictly follow the EM formulation , the convergence properties of the estimation procedures are well understood .In some instances there are practical reasons ... \" .The EM algorithm is widely used to develop iterative parameter estimation procedures for statistical models .","label":"Background","metadata":{},"score":"36.145893"}
{"text":"There are two main contributions in this paper .First , we pose the hard clustering problem in terms of minimizing the loss in Bregman information , a quantity motivated by rate - distortion theory , and present an algorithm to minimize this loss .","label":"Background","metadata":{},"score":"37.14487"}
{"text":"There are two main contributions in this paper .First , we pose the hard clustering problem in terms of minimizing the loss in Bregman information , a quantity motivated by rate - distortion theory , and present an algorithm to minimize this loss .","label":"Background","metadata":{},"score":"37.14487"}
{"text":"That is to say , when characterizing some unknown events with a statistical model , we should always choose the one that has Maximum Entropy .Maximum Entropy Modeling has been successfully applied to Computer Vision , Spatial Physics , Natural Language Processing and many other fields .","label":"Background","metadata":{},"score":"37.37593"}
{"text":"This paper proposes the use of maximum entropy techniques for text classification .Maximum entropy is a probability distribution estimation technique widely used for a variety of natural language tasks , such as language modeling , part - of - speech tagging , and text segmentation .","label":"Background","metadata":{},"score":"37.398613"}
{"text":"By establishing a clustering tree of HMM Gaussian mixture components , the finest affine transformation parameters for individual HMM Gaussian mixture components can be dynamically searched .The on - line Bayesian learning technique proposed in our recent work is used for recursive maximum a posteriori estimation of affine transformation parameters .","label":"Background","metadata":{},"score":"38.066193"}
{"text":"We introduce an improved incremental Baum - Welch algorithm with a new backward procedure that approximates the β - values based on a one - step lookahead in the training sequence .We justify the new approach analytically , and report empirical results that show it converges faster than previous incremental algorithms . .","label":"Background","metadata":{},"score":"38.55225"}
{"text":"This paper describes a number of log - linear parsing models for an automatically extracted lexicalized grammar .The models are \" full \" parsing models in the sense that probabilities are defined for complete parses , rather than for independent events derived by decomposing the parse tree .","label":"Background","metadata":{},"score":"38.793915"}
{"text":"This paper describes a number of log - linear parsing models for an automatically extracted lexicalized grammar .The models are \" full \" parsing models in the sense that probabilities are defined for complete parses , rather than for independent events derived by decomposing the parse tree .","label":"Background","metadata":{},"score":"38.793915"}
{"text":"We describe the maximum - likelihood parameter estimation problem and how the Expectation - form of the EM algorithm as it is often given in the literature .We derive the update equations in fairly explicit detail but we do not prove any convergence properties .","label":"Background","metadata":{},"score":"39.15886"}
{"text":"We describe the maximum - likelihood parameter estimation problem and how the Expectation - form of the EM algorithm as it is often given in the literature .We derive the update equations in fairly explicit detail but we do not prove any convergence properties .","label":"Background","metadata":{},"score":"39.15886"}
{"text":"Finally , the Kullback - Leibler information measure can also be maximized incrementally to obtain HMMs with improved convergence and reduced memory requirements , compared to models estimated using of ... . by Thomas Plötz , Gernot A. Fink - INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING , 2002 . \" ...","label":"Background","metadata":{},"score":"39.44437"}
{"text":"Given consistent statistical evidence , a unique ME solution is guaranteed to exist , and an iterative algorithm exists which is guaranteed to converge to it .The ME framework is extremely general : any phenomenon that can be described in terms of statistics of the text can be readily incorporated .","label":"Background","metadata":{},"score":"39.586205"}
{"text":"The focus in this tutorial is on the foundation common to the two algorithms : convex functions and their convenient properties .Where examples are called for , we draw from applications in human language technology .This note concerns the improved iterative scaling algorithm for computing maximum - likelihood estimates of the parameters of exponential models .","label":"Background","metadata":{},"score":"40.21692"}
{"text":"We present a tree - structured architecture for supervised learning .The statistical model underlying the architecture is a hi - erarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models ( GLIM 's ) .","label":"Background","metadata":{},"score":"40.274685"}
{"text":"We present a tree - structured architecture for supervised learning .The statistical model underlying the architecture is a hi - erarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models ( GLIM 's ) .","label":"Background","metadata":{},"score":"40.274685"}
{"text":"We present positive experimental results on the segmentation of FAQ 's . \" ...This paper proposes the use of maximum entropy techniques for text classification .Maximum entropy is a probability distribution estimation technique widely used for a variety of natural language tasks , such as language modeling , part - of - speech tagging , and text segmentation .","label":"Background","metadata":{},"score":"40.38826"}
{"text":"In this paper , we develop a new framework for statistical signal processing based on wavelet - domain hidden Markov models ( HMM 's ) that concisely models the statistical dependencies and non - Gaussian statistics encountered in real - world signals .","label":"Background","metadata":{},"score":"40.945473"}
{"text":"In this paper , we develop a new framework for statistical signal processing based on wavelet - domain hidden Markov models ( HMM 's ) that concisely models the statistical dependencies and non - Gaussian statistics encountered in real - world signals .","label":"Background","metadata":{},"score":"40.945473"}
{"text":"This paper presents a new Markovian sequence model , closely related to HMMs , that allows observations to be represented as arbitrary overlapping features ( such as word , capitalization , formatting , part - of - speech ) , and defines the conditional probability of state sequences given observation sequences .","label":"Background","metadata":{},"score":"40.990253"}
{"text":"We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .In addition , we introduce two novel smoothing techniques , one a variation of Jelinek - Mercer smoothing and one a very simple linear interpolation technique , both of which outperform existing methods . \" ...","label":"Background","metadata":{},"score":"41.399796"}
{"text":"The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non - Markovian and have a large number of parameters that must be estimated .","label":"Background","metadata":{},"score":"41.537872"}
{"text":"Four iterative parameter estimation algorithms are compared on several NLP tasks .L - BFGS is observed to be the most effective parameter estimation method for Maximum Entropy model , much better than IIS and GIS .( Wallach 02 ) reported similar results on parameter estimation of Conditional Random Fields .","label":"Background","metadata":{},"score":"41.923946"}
{"text":"Bayesian adaptive learning is an optimal way to combine . ... continuously adjusting to a new operational environment without the requirement of storing a large set of previously used training data .In this section , we will discuss one type of on - line adaptation approach , which is base ... . by Asela Gunawardana , William Byrne , I. Jordan - Journal of Machine Learning Research , 2005 . \" ...","label":"Background","metadata":{},"score":"41.93472"}
{"text":"This link is to the Maximum Entropy Modeling Toolkit , for parameter estimation and prediction for maximum entropy models in discrete domains .The software comes with documentation , and was used as the basis of the 1996 Johns Hopkins workshop on language modelling .","label":"Background","metadata":{},"score":"42.316185"}
{"text":"Many approaches to Bayesian image segmentation have used maximum a posteriori ( MAP ) estimation in conjunction with Markov random fields ( MRF ) .While this approach performs well , it has a number of disadvantages .In particular , exact MAP estimates can not be computed , approximate MAP estimates are com ... \" .","label":"Background","metadata":{},"score":"42.364777"}
{"text":"Many approaches to Bayesian image segmentation have used maximum a posteriori ( MAP ) estimation in conjunction with Markov random fields ( MRF ) .While this approach performs well , it has a number of disadvantages .In particular , exact MAP estimates can not be computed , approximate MAP estimates are com ... \" .","label":"Background","metadata":{},"score":"42.364777"}
{"text":"Comparison with other machine learning technique ( Naive Bayes , Transform Based Learning , Decision Tree etc . ) was given .Ratnaparkhi also had a short introduction paper on ME .Abney applies Improved Iterative Scaling algorithm to parameters estimation of Attribute - Value grammars , which can not be corrected calculated by ERF method ( though it works on PCFG ) .","label":"Background","metadata":{},"score":"42.676765"}
{"text":"image reconstruction ; Bayes methods ; Markov processes ; Markov random fields ; Bayesian image restoration methods ; moderate - to - large scale clustering ; iterative algorithms ; Markov mesh models ; strong directional effects ; parameter estimation .CITATION .","label":"Background","metadata":{},"score":"42.72009"}
{"text":"We present an unsupervised technique for visual learning which is based on density estimation in high - dimensional spaces using an eigenspace decomposition .Two types of density estimates are derived for modeling the training data : a multivariate Gaussian ( for a unimodal distribution ) and a multivari ... \" .","label":"Background","metadata":{},"score":"42.79316"}
{"text":"We present an unsupervised technique for visual learning which is based on density estimation in high - dimensional spaces using an eigenspace decomposition .Two types of density estimates are derived for modeling the training data : a multivariate Gaussian ( for a unimodal distribution ) and a multivari ... \" .","label":"Background","metadata":{},"score":"42.79316"}
{"text":"While this approach performs well , it has a number of disadvantages .In particular , exact MAP estimates can not be computed , approximate MAP estimates are computationally expensive to compute , and unsupervised parameter estimation of the MRF is difficult .","label":"Background","metadata":{},"score":"43.106293"}
{"text":"While this approach performs well , it has a number of disadvantages .In particular , exact MAP estimates can not be computed , approximate MAP estimates are computationally expensive to compute , and unsupervised parameter estimation of the MRF is difficult .","label":"Background","metadata":{},"score":"43.106293"}
{"text":"Efficient expectation maximization algorithms are developed for fitting the HMM 's to observational signal data .The new framework is suitable for a wide range of applications , including signal estimation , detection , classification , prediction , and even synthesis .","label":"Background","metadata":{},"score":"43.24777"}
{"text":"Efficient expectation maximization algorithms are developed for fitting the HMM 's to observational signal data .The new framework is suitable for a wide range of applications , including signal estimation , detection , classification , prediction , and even synthesis .","label":"Background","metadata":{},"score":"43.24777"}
{"text":"We present an unsupervised technique for visual learning which is based on density estimation in high - dimensional spaces using an eigenspace decomposition .Two types of density estimates are derived for modeling the training data : a multivariate Gaussian ( for unimodal distributions ) and a Mixture - of - Gaussians model ( for multimodal distributions ) .","label":"Background","metadata":{},"score":"43.31846"}
{"text":"We present an unsupervised technique for visual learning which is based on density estimation in high - dimensional spaces using an eigenspace decomposition .Two types of density estimates are derived for modeling the training data : a multivariate Gaussian ( for unimodal distributions ) and a Mixture - of - Gaussians model ( for multimodal distributions ) .","label":"Background","metadata":{},"score":"43.31846"}
{"text":"e . i . k .x .U .A . x .z .r . )Tools . \" ...We present a tree - structured architecture for supervised learning .The statistical model underlying the architecture is a hi - erarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models ( GLIM 's ) .","label":"Background","metadata":{},"score":"43.80369"}
{"text":"The point of this document is twofold : first , to motivate the improved iterative scaling algorithm for conditional models , and second , to do so in a way that is minimizes the mathematical burden on the reader .This tutorial explains how to build maximum entropy models for natural language applications such as information retrieval and speech recognition .","label":"Background","metadata":{},"score":"44.06914"}
{"text":"Our stochastic model allows us to learn a string edit distance function from a corpus of examples .We illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of words in conversational speech .","label":"Background","metadata":{},"score":"44.25419"}
{"text":"Our stochastic model allows us to learn a string edit distance function from a corpus of examples .We illustrate the utility of our approach by applying it to the difficult problem of learning the pronunciation of words in conversational speech .","label":"Background","metadata":{},"score":"44.25419"}
{"text":"As a consequence , a model - space adaptation would require the models to be repeatedly updated .Another example can be given ... . by Shaojun Wang Yunxin , Shaojun Wang , Yunxin Zhao - IEEE Trans .Speech and Audio Proc , 2001 . \" ...","label":"Background","metadata":{},"score":"44.36635"}
{"text":"We present an information geometric framework for describing such algorithms and analyzing their convergence properties .We apply this framework to analyze the convergence properties of incremental EM and variational EM .For incremental EM , we discuss conditions under these algorithms converge in likelihood .","label":"Background","metadata":{},"score":"44.479263"}
{"text":"The most common approach to this so - called phase retrieval problem is a variation of the well - known Gerchberg - Saxton algorithm devised by Misell ( J. Phys .D 6 , L6 , 1973 ) , which is efficient and extremely simple to implement .","label":"Background","metadata":{},"score":"44.771034"}
{"text":"We show here how to train a conditional random field to achieve performance as good as any reported base noun - phrase chunking method on the CoNLL task , and better than any reported single model .Improved training methods based on modern optimization algorithms were critical in achieving these results .","label":"Background","metadata":{},"score":"45.11864"}
{"text":"231 - 275 .R. Malouf , \" A comparison of algorithms for maximum entropy parameter estimation , \" in International Conference on Computational Linguistics , Proceedings of the 6th Conference on Natural Language in Learning , 20 , 49 - 55 ( Association for Computational Linguistics , 2002 ) .","label":"Background","metadata":{},"score":"45.341045"}
{"text":"These models are unrealistic for many real - world signals .In this paper , we develop a new framework for statistical signal pr ... \" .Abstract - Wavelet - based statistical signal processing techniques such as denoising and detection typically model the wavelet coefficients as independent or jointly Gaussian .","label":"Background","metadata":{},"score":"45.890255"}
{"text":"These models are unrealistic for many real - world signals .In this paper , we develop a new framework for statistical signal pr ... \" .Abstract - Wavelet - based statistical signal processing techniques such as denoising and detection typically model the wavelet coefficients as independent or jointly Gaussian .","label":"Background","metadata":{},"score":"45.890255"}
{"text":"Constraints on the distribution , derived from labeled training data , inform the technique where to be minimally non - uniform .The maximum entropy formulation has a unique solution which can be found by the improved iterative scaling algorithm .In this paper , maximum entropy is used for text classification by estimating the conditional distribution of the class variable given the document .","label":"Background","metadata":{},"score":"45.9618"}
{"text":"In this paper , we propose and analyze parametric hard and soft clustering algorithms based on a large class of distortion functions known as Bregman divergences .The proposed algorithms unify centroid - based parametric clustering approaches , such as classical kmeans and information - theoretic clustering , which arise by special choices of the Bregman divergence .","label":"Background","metadata":{},"score":"46.00605"}
{"text":"In this paper , we propose and analyze parametric hard and soft clustering algorithms based on a large class of distortion functions known as Bregman divergences .The proposed algorithms unify centroid - based parametric clustering approaches , such as classical kmeans and information - theoretic clustering , which arise by special choices of the Bregman divergence .","label":"Background","metadata":{},"score":"46.00605"}
{"text":"However , HMMs and KFMs are limited in their \" expressive power \" .Dynamic Bayesian Networks ( DBNs ) generalize HMMs by allowing the state space to be represented in factored form , instead of as a single discrete random variable .","label":"Background","metadata":{},"score":"46.020332"}
{"text":"However , HMMs and KFMs are limited in their \" expressive power \" .Dynamic Bayesian Networks ( DBNs ) generalize HMMs by allowing the state space to be represented in factored form , instead of as a single discrete random variable .","label":"Background","metadata":{},"score":"46.020332"}
{"text":"In this thesis , I will discuss how to represent many different kinds of models as DBNs , how to perform exact and approximate inference in DBNs , and how to learn DBN models from sequential data .However , perhaps the main value of the thesis is its catholic presentation of the field of sequential data modelling . .","label":"Background","metadata":{},"score":"46.27597"}
{"text":"In this thesis , I will discuss how to represent many different kinds of models as DBNs , how to perform exact and approximate inference in DBNs , and how to learn DBN models from sequential data .However , perhaps the main value of the thesis is its catholic presentation of the field of sequential data modelling . .","label":"Background","metadata":{},"score":"46.27597"}
{"text":"Instead , we apply the principle of Maximum Entropy ( ME ) .Each information source gives rise to a set of constraints , to be imposed on the combined estimate .The intersection of these constraints is the set of probability functions which are consistent with all the information sources .","label":"Background","metadata":{},"score":"46.282066"}
{"text":"The bijection enables the development of an alternative interpretation of an ecient EM scheme for learning models involving mixtures of exponential distributions .This leads to a simple soft clustering algorithm for all Bregman divergences . ... ta , we revisit EM for mixture model estimation for this class of problems .","label":"Background","metadata":{},"score":"46.486427"}
{"text":"The bijection enables the development of an alternative interpretation of an ecient EM scheme for learning models involving mixtures of exponential distributions .This leads to a simple soft clustering algorithm for all Bregman divergences . ... ta , we revisit EM for mixture model estimation for this class of problems .","label":"Background","metadata":{},"score":"46.486427"}
{"text":"The concept of Maximum Entropy can be traced back along multiple threads to Biblical times .However , not until the late of 21st century has computer become powerful enough to handle complex problems with statistical modeling technique like Maxent .Maximum Entropy was first introduced to NLP area by Berger , et al ( 1996 ) and Della Pietra , et al .","label":"Background","metadata":{},"score":"46.739563"}
{"text":"Markov mesh models , a causal subclass of Markov random fields , are , however , readily simulated .We describe an empirical study of simulated realizations from various models used in the literature , and we introduce some new mesh - type models .","label":"Background","metadata":{},"score":"46.952858"}
{"text":"Our approach is applicable to any string classification problem that may be solved using a similarity function against a database of labeled prototypes .Tools . \" ...We present a tree - structured architecture for supervised learning .The statistical model underlying the architecture is a hi - erarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models ( GLIM 's ) .","label":"Background","metadata":{},"score":"46.98974"}
{"text":"Two types of density estimates are derived for modeling the training data : a multivariate Gaussian ( for a unimodal distribution ) and a multivariate Mixture - of - Gaussians model ( for multimodal distributions ) .These probability densities are then used to formulate a maximum - likelihood estimation framework for visual search and target detection for automatic object recognition .","label":"Background","metadata":{},"score":"47.18676"}
{"text":"Two types of density estimates are derived for modeling the training data : a multivariate Gaussian ( for a unimodal distribution ) and a multivariate Mixture - of - Gaussians model ( for multimodal distributions ) .These probability densities are then used to formulate a maximum - likelihood estimation framework for visual search and target detection for automatic object recognition .","label":"Background","metadata":{},"score":"47.18676"}
{"text":"Later , Rosenfeld and his group proposed a Whole Sentence Exponential Model that overcome the computation bottleneck of conditional ME model .You can find more on my SLM page .This dissertation discusses the application of maxent model to various Natural Language Dis - ambiguity tasks in detail .","label":"Background","metadata":{},"score":"47.436455"}
{"text":"Index Terms - Hidden Markov model , probabilistic graph , wavelets . ... rbitrarily close for densities with a finite number of discontinuities [ 25].However , the two - state , zero - mean Gaussian mixture model is simple , robust , and easy - to - use - attractive features for many applications .","label":"Background","metadata":{},"score":"47.532883"}
{"text":"Index Terms - Hidden Markov model , probabilistic graph , wavelets . ... rbitrarily close for densities with a finite number of discontinuities [ 25].However , the two - state , zero - mean Gaussian mixture model is simple , robust , and easy - to - use - attractive features for many applications .","label":"Background","metadata":{},"score":"47.532883"}
{"text":"The data had relatively low SNR .These images can be compared with the true phase shown in Fig .4b as well as the reconstructions from higher SNR data shown in Figs .8a , 8b .The optimum number of iterations were used for each method , as determined from the curves in Fig .","label":"Background","metadata":{},"score":"47.591133"}
{"text":"49 - 55 .[ ps ] [ ps.gz ] [ pdf ] [ bibtex ] What is Maximum Entropy Modeling .In his famous 1957 paper , Ed .T. Jaynes wrote : Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge , and leads to a type of statistical inference which is called the maximum entropy estimate .","label":"Background","metadata":{},"score":"48.17733"}
{"text":"L. I. Perlovsky and R. W. Deming , \" A mathematical theory for learning and its application to time - varying computed tomography , \" New Math .Natural Comput . [CrossRef ] .C. L. Byrne , \" Iterative image reconstruction algorithms based on cross - entropy minimization , \" IEEE Trans .","label":"Background","metadata":{},"score":"48.20198"}
{"text":"As a demonstration of the method , we describe its application to the problem of automatic word classification in natural language processing .An adaptive statistical language model is described , which successfullyintegrates long distance linguistic information with other knowledge sources .","label":"Background","metadata":{},"score":"48.513996"}
{"text":"The learning paradigm builds increasingly complex fields by allowing potential functions , or features , that are supported by increasingly large subgraphs .Each feature has a weight that is trained by minimizing the Kullback - Leibler divergence between the model and the empirical distribution of the training data .","label":"Background","metadata":{},"score":"48.777184"}
{"text":"We also develop an on - line learning algorithm in which the pa - rameters are updated incrementally .Com - parative simulation results are presented in the robot dynamics domain . \" ...We present an unsupervised technique for visual learning which is based on density estimation in high - dimensional spaces using an eigenspace decomposition .","label":"Background","metadata":{},"score":"48.890068"}
{"text":"We also develop an on - line learning algorithm in which the pa - rameters are updated incrementally .Com - parative simulation results are presented in the robot dynamics domain . \" ...We present an unsupervised technique for visual learning which is based on density estimation in high - dimensional spaces using an eigenspace decomposition .","label":"Background","metadata":{},"score":"48.890068"}
{"text":"Because they do not exploit dependencies between labels , such techniques are only well - suited to problems in which categories are independen ... \" .Common approaches to multi - label classification learn independent classifiers for each category , and employ ranking or thresholding schemes for classification .","label":"Background","metadata":{},"score":"49.265167"}
{"text":"Maximum - likelihood point estimation is by far the most prevailing training method .However , due to the problems of unknown speech distributions , sparse training data , high spectral and temporal variabilities in speech , and possible mismatch between training and testing conditions , a dynamic training strategy is needed .","label":"Background","metadata":{},"score":"49.424652"}
{"text":"CrossRef ] [ PubMed ] .R. Malouf , \" A comparison of algorithms for maximum entropy parameter estimation , \" in International Conference on Computational Linguistics , Proceedings of the 6th Conference on Natural Language in Learning , 20 , 49 - 55 ( Association for Computational Linguistics , 2002 ) .","label":"Background","metadata":{},"score":"49.677574"}
{"text":"CrossRef ] .N. J. Dusaussoy and I. E. Abdou , \" The extended MENT algorithm : amaximum entropy type algorithm using prior knowlege for computerized tomography , \" IEEE Trans .Signal Process . [CrossRef ] .R. W. Deming , \" Reconstruction of time - varying objects in computerized tomography using a model - based neural network , \" in Proceedings of the IEEE International Symposium on Intelligent Control ( ISIC ' 98 ) ( IEEE , 1998 ) , pp .","label":"Background","metadata":{},"score":"50.100975"}
{"text":"These images can be compared with the true field shown in Fig . 14a .The optimum number of iterations were used for each method , i.e. , ten iterations for GS and four iterations for MRE .G .A . )","label":"Background","metadata":{},"score":"50.226433"}
{"text":"The most commonly used language models are very simple ( e.g. a Katz - smoothed trigram model ) .There are many improvements over this simple model however , including caching , clustering , higherorder n - grams , skipping models , and sentence - mixture models , all of which we will describe below .","label":"Background","metadata":{},"score":"50.292625"}
{"text":"Since then , Maximum Entropy technique ( and the more general framework Random Fields ) has enjoyed intensive research in NLP community .YASMET --Yet Another Simple Maximum Entropy Toolkit with Feature Selection .YASMET(2 ) --Yet Another Small MaxEnt Toolkit .","label":"Background","metadata":{},"score":"50.295227"}
{"text":"For HMM parameter estimation this algorithm is also called the Baum - Welch algorithm .The EM algorithm is an iterative procedure for approximating ML estimates in the general case of models involving ... . \" ...We describe the maximum - likelihood parameter estimation problem and how the Expectation - form of the EM algorithm as it is often given in the literature .","label":"Background","metadata":{},"score":"50.79163"}
{"text":"For HMM parameter estimation this algorithm is also called the Baum - Welch algorithm .The EM algorithm is an iterative procedure for approximating ML estimates in the general case of models involving ... . \" ...We describe the maximum - likelihood parameter estimation problem and how the Expectation - form of the EM algorithm as it is often given in the literature .","label":"Background","metadata":{},"score":"50.79163"}
{"text":"For example , HMMs have been used for speech recognition and bio - sequence analysis , and KFMs have bee ... \" .Modelling sequential data is important in many areas of science and engineering .Hidden Markov models ( HMMs ) and Kalman filter models ( KFMs ) are popular for this because they are simple and flexible .","label":"Background","metadata":{},"score":"50.802277"}
{"text":"For example , HMMs have been used for speech recognition and bio - sequence analysis , and KFMs have bee ... \" .Modelling sequential data is important in many areas of science and engineering .Hidden Markov models ( HMMs ) and Kalman filter models ( KFMs ) are popular for this because they are simple and flexible .","label":"Background","metadata":{},"score":"50.802277"}
{"text":"The new method replaces the MRF model with a novel multiscale random field ( MSRF ) , and replaces the MAP estimator with a sequential MAP ( SMAP ) estimator derived from a novel estimation criteria .Together , the proposed estimator and model result in a segmentation algorithm which is not iterative and can be computed in time proportional to MN where M is the number of classes and N is the number of pixels .","label":"Background","metadata":{},"score":"51.112507"}
{"text":"The new method replaces the MRF model with a novel multiscale random field ( MSRF ) , and replaces the MAP estimator with a sequential MAP ( SMAP ) estimator derived from a novel estimation criteria .Together , the proposed estimator and model result in a segmentation algorithm which is not iterative and can be computed in time proportional to MN where M is the number of classes and N is the number of pixels .","label":"Background","metadata":{},"score":"51.112507"}
{"text":"Among sequence labeling tasks in language processing , shallow parsing has received much attention , with the development of standard evaluati ... \" .Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position .","label":"Background","metadata":{},"score":"51.334564"}
{"text":"In this report , we start with a revisit to the statistical formulation of the automatic speech recognition ( ASR ) problem , and identify the factors which might in uence the performance of the conventional plug - in MAP decision rule for ASR .","label":"Background","metadata":{},"score":"51.50329"}
{"text":"These images can be compared with the true field shown in Fig . 4a .The optimum number of iterations were used for each method , as determined from the curves in Fig .11 , i.e. , ten iterations for GS and eight iterations for MRE .","label":"Background","metadata":{},"score":"51.890533"}
{"text":"Our goal is to incrementally transform ( or adapt ) the entire set of HMM parameters for a new speaker or new acoustic enviroment from a small amount of adaptation data .By est ... \" .This paper presents a new recursive Bayesian learning approach for transformation parameter estimation in speaker adaptation .","label":"Background","metadata":{},"score":"52.171455"}
{"text":"Tools . \" ...We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .","label":"Background","metadata":{},"score":"52.31644"}
{"text":"Adam Berger , Stephen Della Pietra , and Vincent Della Pietra Computational Linguistics , ( 22 - 1 ) , March 1996 ; .The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .","label":"Background","metadata":{},"score":"52.364567"}
{"text":"Another must read paper on maxent .It deals with a more general frame work : Random Fields and proposes an Improved Iterative Scaling algorithm for estimating parameters of Random Fields .This paper gives theoretical background to Random Fields ( and hence Maxent model ) .","label":"Background","metadata":{},"score":"52.520523"}
{"text":"In some instances there are practical reasons to develop procedures that do not strictly fall within the EM framework .We study EM variants in which the E - step is not performed exactly , either to obtain improved rates of convergence , or due to approximations needed to compute statistics under a model family over which E - steps can not be realized .","label":"Background","metadata":{},"score":"52.743805"}
{"text":"To extract information from further back in the document 's history , we propose and use trigger pairs as the basic information bearing elements .This allows the model to adapt its expectations to the topic of discourse .Next , statistical evidence from multiple sources must be combined .","label":"Background","metadata":{},"score":"52.75325"}
{"text":"It is entirely possible that two techniques that work well separately will not work well together , and , as we will show , even possible that some techniques will work better together than either one does by itself .In this ... . \" ...","label":"Background","metadata":{},"score":"52.98608"}
{"text":"However , in many domains labels are highly interdependent .This paper explores multilabel conditional random field ( CRF ) classification models that directly parameterize label co - occurrences in multi - label classification .Experiments show that the models outperform their singlelabel counterparts on standard text corpora .","label":"Background","metadata":{},"score":"53.029236"}
{"text":"Two important applications of linear transforms in acoustic modeling are the decorrelation of the feature vector and the constrained adaptation of the acoustic models to the speaker , the channel , and the task .Our focus in the first part of this talk is the development of training methods based on the Maximum Mutual Information ( MMI ) and the Maximum A Posteriori ( MAP ) criterion that estimate the parameters of the linear transforms .","label":"Background","metadata":{},"score":"53.142025"}
{"text":"We present a maximum - likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently , using as examples several problems in natural language processing .Stephen Della Pietra , Vincent Della Pietra , and John Lafferty IEEE Transactions on Pattern Analysis and Machine Intelligence 19:4 , pp.380 - -393 , April , 1997 .","label":"Background","metadata":{},"score":"53.935257"}
{"text":"A wide variety of distortion functions are used for clustering , e.g. , squared Euclidean distance , Mahalanobis distance and relative entropy .In this paper , we propose and analyze parametric hard and soft clustering algorithms based on a large class of distortion functions known as Bregman divergence ... \" .","label":"Background","metadata":{},"score":"54.11045"}
{"text":"A wide variety of distortion functions are used for clustering , e.g. , squared Euclidean distance , Mahalanobis distance and relative entropy .In this paper , we propose and analyze parametric hard and soft clustering algorithms based on a large class of distortion functions known as Bregman divergence ... \" .","label":"Background","metadata":{},"score":"54.11045"}
{"text":"Tables do this by employing layout patterns to efficiently indicate fields and records in two - dimensional form . ...When the training labels make the state sequence unambiguous ( as they often do in practice ) , the likelihood function in exponential models such as CRFs is convex , so there are no local maxima , and t .. \" ...","label":"Background","metadata":{},"score":"54.49981"}
{"text":"Both of these are heuristic .Recently , there have been many research e orts towards exploiting more constraints on the relationships between HMM parameters via a tree , so that di erent degrees of constraints can be applied to g .. \" ...","label":"Background","metadata":{},"score":"54.78045"}
{"text":"A key component of the parsing system , for both training and testing , is a Maximum Entropy supertagger which assigns CCG lexical categories to words in a sentence .The supertagger makes the discriminative training feasible , and also leads to a highly efficient parser .","label":"Background","metadata":{},"score":"54.8845"}
{"text":"Much future work remains , but the re ... . \" ...Conditional Random Fields ( CRFs ) are undirected graphical models , a special case of which correspond to conditionally - trained finite state machines .A key advantage of CRFs is their great flexibility to include a wide variety of arbitrary , non - independent features of the input .","label":"Background","metadata":{},"score":"55.231106"}
{"text":"We compare models which use all CCG derivations , including nonstandard derivations , with normal - form models .The performances of the two models are comparable and the results are competitive with existing wide - coverage CCG parsers . ...","label":"Background","metadata":{},"score":"55.432243"}
{"text":"Thus , given training sentences S1 , . . ., Sm , gold - standard dependency structures , π1 , . . ., πm , and the definition of the probability of a dependency structure ( 13 ) , the ob ... . \" ...","label":"Background","metadata":{},"score":"55.52459"}
{"text":"This is useful in a large variety of areas including speech recognition , optical character recognition , handwriting recognition , machine translation , and spelling correction ( Church , 1988 ; Brown et al . , 1990 ; Hull , 1 ... \" .","label":"Background","metadata":{},"score":"55.820404"}
{"text":"CrossRef ] .M. A. Tzannes , D. Politis , and N. S. Tzannes , \" A general method of minimum cross - entropy spectral estimation , \" IEEE Trans .Acoust . , Speech , Signal Process .ASSP-33 , 748 - 752 ( 1985 ) .","label":"Background","metadata":{},"score":"55.938553"}
{"text":"file to read the events from ( required ) .-params_out . file to write parameter values to .-method . -monitor .display progress towards convergence .-max_it .-frtol .-fatol . -checkpoint . -converge . use a simplified convergence test ( for benchmarking ) .","label":"Background","metadata":{},"score":"56.031258"}
{"text":"Hidden Markov models ( HMMs ) are a powerful probabilistic tool for modeling sequential data , and have been applied with success to many text - related tasks , such as part - of - speech tagging , text segmentation and information extraction .","label":"Background","metadata":{},"score":"56.096016"}
{"text":"Hidden Markov models ( HMMs ) are a powerful probabilistic tool for modeling sequential data , and have been applied with success to many text - related tasks , such as part - of - speech tagging , text segmentation and information extraction .","label":"Background","metadata":{},"score":"56.096016"}
{"text":"SPIE ( 2 ) .R. H. Boucher , \" Convergence of algorithms for phase retrieval from two intensity distributions , \" Proc .SPIE 231 , 130 - 141 ( 1980 ) .B. H. Dean , D. L. Aronstein , J. S. Smith , R. Shiri , and D. S. Acton , \" Phase retrieval algorithm for JWST flight and testbed telescope , \" Proc .","label":"Background","metadata":{},"score":"56.23941"}
{"text":"Notes .In ' 'A maximum entropy approach to natural language processing ' ' ( Computational Linguistics 22:1 , March 1996 ) , the appendix describes an approach to computing the gain of a single feature f .This note elaborates on the equations presented there ; in particular , we show how to derive equations ( 37 ) and ( 38 ) .","label":"Background","metadata":{},"score":"56.66622"}
{"text":"Other ( 11 ) .S. Kullback , Information Theory and Statistics ( Wiley , 1959 ) .L. I. Perlovsky , Neural Networks and Intellect ( Oxford U. Press , 2001 ) , Chapters 4 and 6 .R. W. Deming , \" Reconstruction of time - varying objects in computerized tomography using a model - based neural network , \" in Proceedings of the IEEE International Symposium on Intelligent Control ( ISIC ' 98 ) ( IEEE , 1998 ) , pp .","label":"Background","metadata":{},"score":"56.728714"}
{"text":"Linear transforms have been used extensively for both training and adaptation of Hidden Markov Model ( HMM ) based automatic speech recognition ( ASR ) systems .Two important applications of linear transforms in acoustic modeling are the decorrelation of the feature vector and the constrained adaptation ... \" .","label":"Background","metadata":{},"score":"56.79763"}
{"text":"136 - 141 .T. M. Cover and J. A. Thomas , Elements of Information Theory , 2nd ed .( Wiley , 2006 ) , Chapter 2 .J. E. Shore , \" Minimum cross - entropy spectral analysis , \" IEEE Trans .","label":"Background","metadata":{},"score":"57.14747"}
{"text":"T. M. Cover and J. A. Thomas , Elements of Information Theory , 2nd ed .( Wiley , 2006 ) , Chapter 2 . C. Giacovazzo , Direct Phasing in Crystallography ( Oxford U. Press , 1998 ) .J. C. Dainty and J. R. Fienup , \" Phase retrieval and image reconstruction for astronomy , \" in Image Recovery : Theory and Application , H.Stark , ed .","label":"Background","metadata":{},"score":"57.20791"}
{"text":"Keywords : output alphabet is finite , and continuous if the output alphabet is not necessarily finite , e.g. , each state is governed by a para - metric density function [ 32,34,80].Theoretical and empirical results have shown that , given an adequate number of states and a sufficiently rich set of data , HMMs are capable of representing probability distributions corresponding to complex real - world phenomena in terms of 0020 - 0255/ $ - see front matter 2012 Elsevier Inc.","label":"Background","metadata":{},"score":"57.32033"}
{"text":"The only available information is the test data along with a set of pre - trained speech models and the decision parameters .We focus on two types of Bayesian techniques , namely on - line Bayesian adaptation of hidden Markov model parameters and the Bayesian predictive classi cation approach .","label":"Background","metadata":{},"score":"57.700974"}
{"text":"ii 1 Maximum - likelihood Recall the definition of the maximum - likelihood estimation problem .That is , we assume that these data vectors are independent and . by Matthew S. Crouse , Student Member , Robert D. Nowak , Richard G. Baraniuk , Senior Member - IEEE Transactions on Signal Processing , 1998 . \" ...","label":"Background","metadata":{},"score":"58.167107"}
{"text":"ii 1 Maximum - likelihood Recall the definition of the maximum - likelihood estimation problem .That is , we assume that these data vectors are independent and . by Matthew S. Crouse , Student Member , Robert D. Nowak , Richard G. Baraniuk , Senior Member - IEEE Transactions on Signal Processing , 1998 . \" ...","label":"Background","metadata":{},"score":"58.167107"}
{"text":"MaxEnt and Exponential Models .This page contains pedagogically - oriented material on maximum entropy and exponential models .The emphasis is towards modelling of discrete - valued stochastic processes which arise in human language applications , such as language modelling .","label":"Background","metadata":{},"score":"58.567947"}
{"text":"This is a high - level tutorial on how to use MaxEnt for modelling discrete stochastic processes .The motivating example is the task of determining the most appropriate translation of a French word in context .The tutorial discusses the process of growing an exponential model by automatic feature selection ( \" inductive learning , \" if you will ) and also the task of estimating maximum - likelihood parameters for a model containing a fixed set of features .","label":"Background","metadata":{},"score":"58.766888"}
{"text":"ASSP-29 , 230 - 237 ( 1981 ) .[CrossRef ] .S. Kullback , Information Theory and Statistics ( Wiley , 1959 ) .L. I. Perlovsky , Neural Networks and Intellect ( Oxford U. Press , 2001 ) , Chapters 4 and 6 . L. I. Perlovsky , C. P. Plum , P. R. Franchi , E. J. Tichovolsky , D. S. Choi , and B. Weijers , \" Einsteinian neural network for spectrum estimation , \" Neural Networks 10 , 1541 - 1546 ( 1997 ) .","label":"Background","metadata":{},"score":"59.211548"}
{"text":"CrossRef ] .IEEE Trans .Signal Process . D. N. Politis , \" ARMA models , prewhitening and minimum cross entropy , \" IEEE Trans .Signal Process . [CrossRef ] .N. J. Dusaussoy and I. E. Abdou , \" The extended MENT algorithm : amaximum entropy type algorithm using prior knowlege for computerized tomography , \" IEEE Trans .","label":"Background","metadata":{},"score":"59.27738"}
{"text":"print performance summary .-trmalloc . use error - checking memory allocator ( without this , the memory statistics reported by -summary are meaningless ) .There are some recent options which we have not provided documentation for as yet .Feel free to tinker with the options ( the SNES options look particularly interesting and particularly daunting ) , and let me know if any of them improve anything .","label":"Background","metadata":{},"score":"59.892834"}
{"text":"[CrossRef ] .J. W. Goodman , Introduction to Fourier Optics ( McGraw - Hill , 1968 ) , Chapter 3 . S. B. Howell , Handbook of CCD Astronomy ( Cambridge U. Press , 2000 ) .G. Holst , \" Noise in imaging : the good , the bad and the right , in Photonics Spectra , 40 , 88 - 92 ( Laurin , 2006 ) , Vol .","label":"Background","metadata":{},"score":"60.509163"}
{"text":"The transforms obtained under the MMI criterion are termed Discriminative Likelihood Linear Transforms ( DLLT ) .Experimental results show that DLLT provides a discriminative estimation framework for feature normalization in HMM training for large vocabulary continuous speech recognition tasks that outperforms its Maximum Likelihood counterpart .","label":"Background","metadata":{},"score":"60.593864"}
{"text":"Thus environmental adaptation regarding arbitrary acoustic scenarios beyond speaker changes becomes possible .For deploying acoustic adaptation in interactive applications , such as human machine interaction , a time - synchronous adaptation approach is proposed .For different corpora the evaluation of our approaches shows significant improvements in recognition accuracy while satisfying the constraint of time - synchronous processing . by","label":"Background","metadata":{},"score":"60.9319"}
{"text":"Inspired by an existing broadcast news transcription system [ 1 ] we refined the identification of acoustic scenarios by using a combined GMM / HMM method .Thus environ ... \" .In this paper we describe system architectures for robust MLLR based environmental adaptation of continuous speech recognition systems .","label":"Background","metadata":{},"score":"61.084373"}
{"text":"G. Leone , R. Pierri , and F. Soldovieri , \" On the performances of two algorithms in phaseless antenna measurements , \" in Tenth International Conference on Antennas and Propagation ( IEEE , 1997 ) , Vol . 1 , pp .","label":"Background","metadata":{},"score":"61.255814"}
{"text":"CrossRef ] . D. Titterington , \" On the iterative image space reconstruction algorithm for ECT , \" IEEE Trans .Med .Imaging MI-6 , 52 - 56 ( 1987 ) .[CrossRef ] . D. N. Politis , \" ARMA models , prewhitening and minimum cross entropy , \" IEEE Trans .","label":"Background","metadata":{},"score":"61.441162"}
{"text":"Estimation of such large models is not only expensive , but also , due to sparsely distributed features , sensitive to round - off errors .Input format .The first part of the file is a header , bracketed by lines containing & header and / .","label":"Background","metadata":{},"score":"62.121834"}
{"text":"5 , pp .507 - 513 , May 1994 , doi:10.1109/34.291447 TADM .The Toolkit for Advanced Discriminative Modeling .Introduction .The Toolkit for Advanced Discriminative Modeling ( TADM ) is a C++ implementation for estimating the parameters of discriminative models , such as maximum entropy models .","label":"Background","metadata":{},"score":"62.361115"}
{"text":"In the second experiment , a pattern of square objects within the aperture attenuates the amplitude locally , but leaves the phase unaffected .( a ) shows the true magnitude distribution on the aperture plane .z .( b ) shows the noisy , diffracted , intensity data acquired on the first scan plane at axial position . using ( a ) GS and ( b ) MRE , from the noisy intensity data in shown Fig .","label":"Background","metadata":{},"score":"63.15919"}
{"text":"You need GCC 2.9x to compile the source .link2 .MEGA Model Optimization Package .A recently appeared ME implementation by Hal Daumé III .The software features CG and LM - BFGS Optimization and is written in OCaml .Although I no longer use OCaml , I 'd say that 's a great language , and is worth learning .","label":"Background","metadata":{},"score":"63.23311"}
{"text":"Our learning technique is applied to the probabilistic visual modeling , detection , recognition , and coding of human faces and non - rigid objects such as hands . \" ...Modelling sequential data is important in many areas of science and engineering .","label":"Background","metadata":{},"score":"63.31182"}
{"text":"Our learning technique is applied to the probabilistic visual modeling , detection , recognition , and coding of human faces and non - rigid objects such as hands . \" ...Modelling sequential data is important in many areas of science and engineering .","label":"Background","metadata":{},"score":"63.31182"}
{"text":"We identify the strength and weakness of individual algorithms and offer recommendations for practitioners to make intelligent use of these adaptation algorithms for different purposes in different applications . \" ...Keywords : output alphabet is finite , and continuous if the output alphabet is not necessarily finite , e.g. , each state is governed by a para - metric density function [ 32,34,80].","label":"Background","metadata":{},"score":"63.374386"}
{"text":"Well , it 's time to have a look at this one .Edwin Thompson Jaynes presented some insightful results of maximum entropy principle in this 1957 paper published in Physics Reviews .This is also his first paper in information theory .","label":"Background","metadata":{},"score":"63.3935"}
{"text":"It was written by Rob Malouf and is now being developed as an open source project on Sourceforge in collaboration with Jason Baldridge and Miles Osborne .It is licensed under the Lesser GNU Public License .Background .A feature of maximum entropy ( ME ) modeling that makes it very attractive is that it is a general purpose technique which can be applied to a wide variety of problems in natural language processing .","label":"Background","metadata":{},"score":"64.14165"}
{"text":"In many applications , it is necessary to determine the similarity of two strings .A widely - used notion of string similarity is the edit distance : the minimum number of insertions , deletions , and substitutions required to transform one string into the other .","label":"Background","metadata":{},"score":"64.20511"}
{"text":"In many applications , it is necessary to determine the similarity of two strings .A widely - used notion of string similarity is the edit distance : the minimum number of insertions , deletions , and substitutions required to transform one string into the other .","label":"Background","metadata":{},"score":"64.20511"}
{"text":"In many applications , it is necessary to determine the similarity of two strings .A widely - used notion of string similarity is the edit distance : the minimum number of insertions , deletions , and substitutions required to transform one string into the other .","label":"Background","metadata":{},"score":"64.20511"}
{"text":"In many applications , it is necessary to determine the similarity of two strings .A widely - used notion of string similarity is the edit distance : the minimum number of insertions , deletions , and substitutions required to transform one string into the other .","label":"Background","metadata":{},"score":"64.20511"}
{"text":"We report the experimental results of using these algorithms for on - line speaker adaptation in a continuous Mandarin Chinese speech recognition system .We identify the strengt ... \" .We conduct a comparative study of five representative incremental HMM adaptation algorithms developed in the past few years .","label":"Background","metadata":{},"score":"64.88139"}
{"text":"Conditional Random Fields ( CRFs ) are undirected graphical models , a special case of which correspond to conditionally - trained finite state machines .A key advantage of CRFs is their great flexibility to include a wide variety of arbitrary , non - independent features of the input .","label":"Background","metadata":{},"score":"64.91308"}
{"text":"However , the flexibility of ME models is not without cost .While parameter estimation for ME models is conceptually straightforward , in practice ME models for typical natural language tasks are very large , and may well contain many thousands of free parameters .","label":"Background","metadata":{},"score":"65.027626"}
{"text":"An word morphology application for English was developed . longer version .This paper applies ME technique to statistical language modeling task .More specifically , it builds a conditional Maximum Entropy model that incorporates traditional N - gram , distant N - gram and trigger pair features .","label":"Background","metadata":{},"score":"65.062744"}
{"text":"Suitable for text categorization and related NLP tasks .Here is another small maxent package in C++ with a BSD - like license , written by Dekang Lin .A must read paper on applying maxent technique to Natural Language Processing .","label":"Background","metadata":{},"score":"66.129486"}
{"text":"Documents often contain tables in order to communicate densely packed , multi - dimensional information .Tables do this by employing layout pa ... \" .The ability to find tables and extract information from them is a necessary component of data mining , question answering , and other information retrieval tasks .","label":"Background","metadata":{},"score":"66.32962"}
{"text":"CrossRef ] [ PubMed ] .IEEE Trans .Med .Imaging ( 1 ) .D. Titterington , \" On the iterative image space reconstruction algorithm for ECT , \" IEEE Trans .Med .Imaging MI-6 , 52 - 56 ( 1987 ) .","label":"Background","metadata":{},"score":"66.76816"}
{"text":"Each feature can appear only once in an event , and must have a value greater than zero .You can have events with a zero frequency -- these are used in computing Z(x ) for each context , but ignored for computing the entropy and KL divergence .","label":"Background","metadata":{},"score":"66.96164"}
{"text":"Changes .version 0.9.5 - First TADM release , basically Rob Malouf 's original code relicensed under the Lesser GNU Public License . \" ...Recent advances in automatic speech recognition are accomplished by designing a plug - in maximum a posteriori decision rule such that the forms of the acoustic and language model distributions are specified and the parameters of the assumed distributions are estimated from a collection of speech and ... \" .","label":"Background","metadata":{},"score":"66.97726"}
{"text":"Whole Sentence Language Model ) with sampling based training .Now seems to be part of scipy .Stanford Classifer is another open source implementation of Maximum Entropy Model in java , suitable for NLP tagging and parsing tasks .NLTK includes a maxent classifier written entirely in Python .","label":"Background","metadata":{},"score":"67.968124"}
{"text":"Antennas Propag .AP-53 , 3135 - 3141 ( 2005 ) .[CrossRef ] .IEEE Trans .Image Process .C. L. Byrne , \" Iterative image reconstruction algorithms based on cross - entropy minimization , \" IEEE Trans .","label":"Background","metadata":{},"score":"68.11894"}
{"text":"R. Pierri , G. D'Elia , and F. Soldovieri , \" A two probes scanning phaseless near - field far - field transformation technique , \" IEEE Trans .Antennas Propag .AP-47 , 792 - 802 ( 1999 ) .[","label":"Background","metadata":{},"score":"68.484695"}
{"text":"G. Holst , \" Noise in imaging : the good , the bad and the right , in Photonics Spectra , 40 , 88 - 92 ( Laurin , 2006 ) , Vol .Magnitude of the field reconstructed at the aperture plane using ( a ) GS and ( b ) MRE ( the approach introduced in this paper ) .","label":"Background","metadata":{},"score":"68.83443"}
{"text":"As a consequence , online speaker adaptation in general requires less computation and memory as compared with batch adaptation .One approach [ 48 ] , [ 50 ] applied expectation - maximization ( EM ) algorithm or segmental -means algorithm sequentially to online test speech to accomplish unsupervised lear ... . \" ...","label":"Background","metadata":{},"score":"69.16122"}
{"text":"CrossRef ] .T. Isernia , G. Leone , and R. Pierri , \" Radiation pattern evaluation from near - field intensities on planes , \" IEEE Trans .Antennas Propag .AP-44 , 701 - 710 ( 1996 ) .[","label":"Background","metadata":{},"score":"69.49243"}
{"text":".. consisting of a term and a label , as well as a feature for each triplet consisting of a term and two labels .Abstract .A recursive algorithm , which appears to be new , is presented for estimating the amplitude and phase of a wave field from intensity - only measurements on two or more scan planes at different axial positions .","label":"Background","metadata":{},"score":"69.76078"}
{"text":"INTRODUCTION Adaptation technique has been widely studied for practical speech recognition systems in the last decade .It can be classified into the following two major approaches : Bayesian appro ...","label":"Background","metadata":{},"score":"69.86289"}
{"text":"Event files can be compressed using gzip .As event files tend to get very large , this can save a lot of disk space and improve performance dramatically .Usage .The tadm executable takes all its commands as options on the command line .","label":"Background","metadata":{},"score":"70.27469"}
{"text":"Claude Elwood Shannon 's influential 1948 paper that laid the foundation of information theory and changed the whole world since then .I see no reason who has read the above papers does not want to read this one .Information Theory and Statistical Mechanics ( Jaynes , E. T. , 1957 )","label":"Background","metadata":{},"score":"71.574"}
{"text":"This paper describes and evaluates log - linear parsing models for Combinatory Categorial Grammar ( CCG ) .A parallel implementation of the L - BFGS optimisation algorithm is described , which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation .","label":"Background","metadata":{},"score":"72.40326"}
{"text":"This paper describes and evaluates log - linear parsing models for Combinatory Categorial Grammar ( CCG ) .A parallel implementation of the L - BFGS optimisation algorithm is described , which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation .","label":"Background","metadata":{},"score":"72.40326"}
{"text":"When interfaced to SPHINX - II , Carnegie Mellon 's speech recognizer , it reduced its error rate by 10%--14 % .This thus illustrates the feasibility of incorporating many diverse knowledge sources in a single , unified statistical framework .A comparison of algorithms for maximum entropy parameter estimation .","label":"Background","metadata":{},"score":"72.93973"}
{"text":"This file can also have alias statements , to allow abbreviations for some of the option names .For example , my . petscrc contains : . -monitor alias -in -events_in alias -out -params_out .Parallel processing .Since tadm uses MPI for interprocess communication , it can easily be ported to a wide range of parallel architectures , including SMP and Beowulf - type clusters .","label":"Background","metadata":{},"score":"73.83275"}
{"text":"Signal Process . [CrossRef ] .IEEE Trans .Antennas Propag .W. Chalodhorn and D. DeBoer , \" Use of microwave lenses in phase retrieval microwave holography of reflector antennas , \" IEEE Trans .Antennas Propag .AP-50 , 1274 - 1284 ( 2002 ) .","label":"Background","metadata":{},"score":"74.0562"}
{"text":"CrossRef ] .Inverse Probl .T. Isernia , G. Leone , and R. Pierri , \" Phase retrieval of radiated fields , \" Inverse Probl . [CrossRef ] .Optik ( Stuttgart ) ( 1 ) .R. W. Gerchberg and W. O. Saxton , \" A practical algorithm for the determination of phase from image diffraction plane pictures , \" Optik ( Stuttgart ) 35 , 237 - 246 ( 1972 ) .","label":"Background","metadata":{},"score":"75.52226"}
{"text":"1998 International Symposium on Chinese Spoken Language Processing ( Singapore , 1998 . \" ...In this report , we start with a revisit to the statistical formulation of the automatic speech recognition ( ASR ) problem , and identify the factors which might in uence the performance of the conventional plug - in MAP decision rule for ASR .","label":"Background","metadata":{},"score":"76.28565"}
{"text":"The first line of each block is the number of events for that context ( 2 and 3 for the two contexts here ) .Then come the events .Each event line has a frequency , the number of feature value pairs , then pairs of feature number and value .","label":"Background","metadata":{},"score":"77.10317"}
{"text":"The lexicalized grammar formalism used is Combinatory Categorial Grammar ( CCG ) , and the grammar is automatically extracted from CCGbank , a CCG version of the Penn Treebank .The combination of discriminative training and an automatically extracted grammar leads to a significant memory requirement ( over 20 GB ) , which is satisfied using a parallel implementation of the BFGS optimisation algorithm running on a Beowulf cluster .","label":"Background","metadata":{},"score":"78.21365"}
{"text":"e . i . k . )z .j .e . i . k . x . ] . . .i .L .b . k . ) j .J .x .x .d .","label":"Background","metadata":{},"score":"80.01994"}
{"text":"J .x .x .d .x .I . m .x .z .j . )I .A .M .x .z .j . )U .A .M .x .","label":"Background","metadata":{},"score":"80.73271"}
{"text":"d . k .A . k . )e . i . k . )z .j .e . i . k . x . ] . . .A .M . ) k . )J .","label":"Background","metadata":{},"score":"82.65543"}
{"text":"j . ) a . k . )L . a . k . ) j .J .x .x .d .x .I . m .x .z .j . )I .A . x .","label":"Background","metadata":{},"score":"83.435974"}
{"text":"I . m .x .z .j . )I .A . x .z .j . )[ .U .A . x .z .j . )e . i . k . )","label":"Background","metadata":{},"score":"83.758"}
{"text":"j . ) . ] j .J .x .x .d .x .I . m .x .z .j . )I .A . x .z .j . )I .A . x .","label":"Background","metadata":{},"score":"84.467064"}
{"text":"j . )[ .U .A . x .z .j . )e . i . k . )z .j .e . i . k .x .U .A . x .z .","label":"Background","metadata":{},"score":"86.14497"}
{"text":"j .e . i . k .x .U .A . x .z .j . )e . i . k . )z .j .e . i . k . x . ][ . a . k . )","label":"Background","metadata":{},"score":"86.18111"}
{"text":"L . a . k . ) j .J .x .x .d .x .I . m .x .z .j . ) a . k . )[ .ln .I .A . x .","label":"Background","metadata":{},"score":"86.20043"}
{"text":"j . )e . i . k . )z .j .e . i . k . x . . .U .A . x .z .s . )P .r .s .U .","label":"Background","metadata":{},"score":"86.85922"}
{"text":"U .A . x .z .j . )e . i . k . )z .j .e . i . k . x . . .α . k . k .A . k . ) d . k .","label":"Background","metadata":{},"score":"87.49973"}
{"text":"J .x .x .d .x .I . m .x .z .j . )I .A . x .z .j . )U .A . x .z .j . )","label":"Background","metadata":{},"score":"87.860245"}
{"text":"b . k . ) . ]A . k . ) j .J .x .x .d .x .I . m .x .z .j . )I .A . x .z .","label":"Background","metadata":{},"score":"88.40507"}
{"text":"z .r . ) k . k .d . k .e . i . k . x .e . i . k . )z . s .z .r . ) x .x .d .","label":"Background","metadata":{},"score":"89.44324"}
{"text":"A . )R .x . k . k .A . k .d . k .L .A . )[ .R .x . k . k . a . k .b . k .","label":"Background","metadata":{},"score":"95.27585"}
