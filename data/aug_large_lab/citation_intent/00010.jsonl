{"text":"Using tools from matrix perturbation theory , we analyze the algorithm , and give conditions under which it can be expected to do well .Andrew Y. Ng , Michael Jordan , and Yair Weiss in NIPS 14 2002 .We compare discriminative and generative learning as typified by logistic regression and naive Bayes .","label":"Uses","metadata":{},"score":"30.661121"}
{"text":"[ 6 ] proposed a Semi - Supervised Learning ( SSL ) algorithm for heterogeneous datasets having both labeled and unlabeled samples .Their example data were comprised of DNA microarray expressions and phylogenic reconstructions , with class labels corresponding to gene function .","label":"Uses","metadata":{},"score":"30.899914"}
{"text":"ACL , 2000 . \" ...This paper describes a set of comparative experiments , including cross - corpus evaluation , between five alternative algorithms for supervised Word Sense Disambiguation ( WSD ) , namely Naive Bayes , Exemplar - based learning , SNOW , Decision Lists , and Boosting .","label":"Uses","metadata":{},"score":"31.686213"}
{"text":"We develop a sparse variant of the deep belief networks of Hinton et al .( 2006 ) .We learn two layers of nodes in the network , and demonstrate that the first layer , similar to prior work on sparse coding and ICA , results in localized , oriented , edge filters , similar to the Gabor functions known to model V1 cell receptive fields .","label":"Uses","metadata":{},"score":"31.78011"}
{"text":"Specifically , we systematically evaluated Naive Bayes and Logistic Regression classifiers , as well as mixtures of Naive Bayes and Logistic Regression in a sequence - based 10-fold cross - validation setup .The results of our experiments show that global sequence similarity through the means of the mixture of experts approach can be exploited to improve the performance of classifiers trained to label biomolecular sequence data .","label":"Uses","metadata":{},"score":"32.046146"}
{"text":"Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features , called naive Bayes , is competitive with state - of - the - art classifiers such as C4.5 .This fact raises the question of whether a classifier with less restrictive assumptions can perform even better .","label":"Uses","metadata":{},"score":"32.73074"}
{"text":"Moreover , we examined the performance of our hybrid classifier when the labeled and unlabeled data distributions were different .INDEX TERMS .generative model , maximum entropy principle , bias correction , unlabeled samples , text classification .CITATION .[ 10 ] A.Y. Ng and M.I. Jordan , \" On Discriminative vs. Generative Classifiers : A Comparison of Logistic Regression and Naive Bayes , \" Advances in Neural Information Processing Systems 14 , pp.841 - 848 , MIT Press , 2002 .","label":"Uses","metadata":{},"score":"32.976753"}
{"text":"Other differences are that the naive Bayes model uses only positive traini ... . \" ...The most effective paradigm for word sense disambiguation , supervised learning , seems to be stuck because of the knowledge acquisition bottleneck .In this paper we take an in - depth study of the performance of decision lists on two publicly available corpora and an additional corpus automatically acq ... \" .","label":"Uses","metadata":{},"score":"33.093094"}
{"text":"We show that the accuracy of a naive Bayes text classifier can be significantly improved by taking advantage of a hierarchy of classes .We adopt an established statistical technique called shrinkage that smoothes parameter estimates of a data - sparse child with its parent in order to obtain more robust parameter estimates .","label":"Uses","metadata":{},"score":"33.60661"}
{"text":"This article 's results also imply that detecting attribute dependence is not necessarily the best way to extend the Bayesian classifier , and this is also verified empirically . \" ...Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features , called naive Bayes , is competitive with state - of - the - art classifiers such as C4.5 .","label":"Uses","metadata":{},"score":"34.024544"}
{"text":"In our approach , we first consider a generative model trained by using labeled samples and introduce a bias correction model , where these models belong to the same model family , but have different parameters .Then , we construct a hybrid classifier by combining these models based on the maximum entropy principle .","label":"Uses","metadata":{},"score":"34.07453"}
{"text":"More recently , supervised machine learning ( ML ) technologies have received considerable attention and have shown promising results [ 16 - 18 ] .Bruce [ 19 ] applied a Bayesian algorithm and chose features based on their \" informative \" nature .","label":"Uses","metadata":{},"score":"34.157276"}
{"text":"In a demonstration of their technique aimed at web page classification , the addition of unlabeled samples decreased classification error relative to classification using only labeled data .In a subsequent study , Nigam and Ghani [ 5 ] further examined the performance of the co - training algorithm and specifically its sensitivity to the independence of the feature sets .","label":"Uses","metadata":{},"score":"34.59877"}
{"text":"Despite their naive design and apparently oversimplified assumptions , naive Bayes classifiers have worked quite well in many complex real - world situations .In 2004 , an analysis of the Bayesian classification problem showed that there are sound theoretical reasons for the apparently implausible efficacy of naive Bayes classifiers .","label":"Uses","metadata":{},"score":"34.78543"}
{"text":"J. Zico Kolter and Andrew Y. Ng in Proceedings of Robotics : Science and Systems 2007 .Using an implicit differentiation trick , we derive an efficient gradient - based method for learning Gaussian regularization priors with multiple hyperparameters .In both simulations and the real - world task of computational RNA secondary structure prediction , we find that multiple hyperparameter learning can provide a significant boost in accuracy compared to using only a single regularization hyperparameter .","label":"Uses","metadata":{},"score":"34.823303"}
{"text":"We speed up object detection by using a segmentation algorithm to select a small number of image regions on which to run a classifier .A directed Steiner tree optimization problem is solved approximately to select the segmentation algorithm parameters .Compared to the sliding window approach , our method results in two orders of magnitude fewer regions considered , and significant running time speedups .","label":"Uses","metadata":{},"score":"35.527298"}
{"text":"In this paper we abandon the normality ... \" .When modeling a probability distribution with a Bayesian network , we are faced with the problem of how to handle continuous variables .Most previous work has either solved the problem by discretizing , or assumed that the data are generated by a single Gaussian .","label":"Uses","metadata":{},"score":"35.53068"}
{"text":"We present a generalization of sparse coding to learning with data drawn from any exponential family distribution , which is better suited to model other data types than Gaussian .We present an algorithm for solving the L1regularized optimization problem defined by this model , and show that it is especially efficient when the optimal solution is sparse and improves self - taught learning performance when applied to text classification and to a robotic perception task .","label":"Uses","metadata":{},"score":"35.553276"}
{"text":"We present the convolutional deep belief network , a hierarchical generative model which scales to realistic image sizes .With probabilistic max - pooling , a novel technique which shrinks the representations of higher layers in a probabilistically sound way , the algorithm learns useful high - level visual features and can perform hierarchical inference over full - sized images .","label":"Uses","metadata":{},"score":"35.71073"}
{"text":"5 ] Still , a comprehensive comparison with other classification algorithms in 2006 showed that Bayes classification is outperformed by other approaches , such as boosted trees or random forests .[ 6 ] .An advantage of naive Bayes is that it only requires a small amount of training data to estimate the parameters necessary for classification .","label":"Uses","metadata":{},"score":"36.02462"}
{"text":"Given a set of examples for which an at ... . by Claudia Perlich , Foster Provost , Jeffrey S. Simonoff - CEDER WORKING PAPER # IS-01 - 02 , STERN SCHOOL OF BUSINESS , 2001 . \" ...Tree induction and logistic regression are two standard , off - the - shelf methods for building models for classi cation .","label":"Uses","metadata":{},"score":"36.40285"}
{"text":"In this manner , the overall classifier can be robust enough to ignore serious deficiencies in its underlying naive probability model .[ 3 ] Other reasons for the observed success of the naive Bayes classifier are discussed in the literature cited below .","label":"Uses","metadata":{},"score":"36.604538"}
{"text":"We explain how this \" overfitting \" really occurs , and show the surprising result that it can be overcome by selecting a hypothesis with a higher cross - validation error , over others with lower cross - validation errors .We give reasons for not selecting the hypothesis with the lowest cross - validation error , and propose a new algorithm , LOOCVCV , that uses a computationally efficient form of leave - one - out crossvalidation to select such a hypothesis .","label":"Uses","metadata":{},"score":"36.70872"}
{"text":"This implies that the Bayesian classifier has a much greater range of applicability than previously thought .For example , in this article it is shown to be optimal for learning conjunctions and disjunctions , even though they violate the independence assumption .","label":"Uses","metadata":{},"score":"36.922417"}
{"text":"Related work .Blum and Mitchell [ 4 ] introduced the co - training algorithm for improving the sample classification performance when there are few labeled samples and many unlabeled samples .The co - training algorithm assumes that there are two independent sets of features available , such that each feature set is good enough to train a good classifier .","label":"Uses","metadata":{},"score":"37.07133"}
{"text":"For a naive Bayesian classifier , we present experimental results on a variety of natural and artificial domains , comparing two methods of density estimation : assuming normality and modeling each conditional distribution with a single Gaussian ; and using nonparametric kernel density estimation .","label":"Uses","metadata":{},"score":"37.258896"}
{"text":"We present an approach in which supervised learning is first used to estimate depths from single monocular images .We present results evaluating the predictive ability of the algorithm both on held out test data , and in actual autonomous driving experiments .","label":"Uses","metadata":{},"score":"37.365303"}
{"text":"The L ... \" .This paper describes a set of comparative experiments , including cross - corpus evaluation , between five alternative algorithms for supervised Word Sense Disambiguation ( WSD ) , namely Naive Bayes , Exemplar - based learning , SNOW , Decision Lists , and Boosting . by","label":"Uses","metadata":{},"score":"37.47484"}
{"text":"We present a general Bayesian framework for hyperparameter tuning in L2-regularized supervised learning models and find a local optimum of the resulting non - convex optimization problem .Empirical results on a variety of supervised learning models show that our algorithm is competitive with both grid - search and gradient - based algorithms , but is more efficient and far easier to implement .","label":"Uses","metadata":{},"score":"37.743095"}
{"text":"Our algorithm uses an optimization criterion different from maximum likelihood , and allows us to learn models that capture longer range effects , but without giving up the benefits of using first - order Markov models .Pieter Abbeel and Andrew Y. Ng in NIPS 17 2005 .","label":"Uses","metadata":{},"score":"37.89624"}
{"text":"- 274 .Paccanaro A , Casbon JA , Saqi MAS : Spectral clustering of protein sequences .Nucleic Acids Research 2006 , 34(5):1571 - 1580 .Mitchell TM : Machine Learning McGraw Hill ; 1997 .Ng AY , Jordan MI : On discriminative vs. generative classifiers : A comparison of logistic regression and naive Bayes .","label":"Uses","metadata":{},"score":"38.261436"}
{"text":"However , the use of unlabeled data via the basic EM algorithm often causes disastrous performance degradation instead of improving classification performance , resulting in poor classification performance on average .In this study , we introduce a class distribution constraint into the iteration process of the EM algorithm .","label":"Uses","metadata":{},"score":"38.29641"}
{"text":"During learning , we train either a collection of M Naïve Bayes classifiers or a collection of M Logistic Regres- be the leaf nodes and be sion classifiers , one classifier at each leaf node 1 , ? , M. Naïve Bayes and Logistic Regression are briefly described in the next section .","label":"Uses","metadata":{},"score":"38.421516"}
{"text":"Combining a naive Bayes classifier with the EM algorithm is one of the promising approaches for making use of unlabeled data for disambiguation tasks when using local context features including word sense disambiguation and spelling correction .However , the use of unlabeled data via the basic ... \" .","label":"Uses","metadata":{},"score":"38.55888"}
{"text":"Since naive Bayes is also a linear model for the two \" discrete \" event models , it can be reparametrised as a linear function .Obtaining the probabilities is then a matter of applying the logistic function to , or in the multiclass case , the softmax function .","label":"Uses","metadata":{},"score":"38.61682"}
{"text":"We present a new machine learning framework called \" self - taught learning \" for using unlabeled data in supervised classification tasks .We describe an approach to self - taught learning that uses sparse coding to construct higher - level features using the unlabeled data .","label":"Uses","metadata":{},"score":"38.74138"}
{"text":"We report results in document modeling , text classification , and collaborative filtering , comparing to a mixture of unigrams model and the probabilistic LSI model .David Blei , Andrew Y. Ng , and Michael Jordan in NIPS 14 2002 .","label":"Uses","metadata":{},"score":"38.848717"}
{"text":"J. Zico Kolter and Andrew Y. Ng in ICML 2009 .By proposing a regularization framework for the LSTD algorithm that overcomes these difficulties and focusing on the case of l1 regularization , we present an algorithm similar to the Least Angle Regression ( LARS ) algorithm that can efficiently compute the optimal solution .","label":"Uses","metadata":{},"score":"38.90052"}
{"text":"where is the probability of class generating the term .This event model is especially popular for classifying short texts .It has the benefit of explicitly modelling the absence of terms .Note that a naive Bayes classifier with a Bernoulli event model is not the same as a multinomial NB classifier with frequency counts truncated to one .","label":"Uses","metadata":{},"score":"38.947594"}
{"text":"Clustering techniques [ 2 ] are applied to the datasets for assigning samples to their corresponding group solely based on similar expression levels .Supervised algorithms on the other hand classify [ 3 ] samples according to their externally determined class .","label":"Uses","metadata":{},"score":"38.997017"}
{"text":"[28 ] D.J. Miller and H.S. Uyar , \" A Mixture of Experts Classifier with Learning Based on Both Labelled and Unlabelled Data , \" Advances in Neural Information Processing Systems 9 , pp .571 - 577 , MIT Press , 1997 .","label":"Uses","metadata":{},"score":"39.06163"}
{"text":"We evaluate the power of decision tables as a hypothesis space for supervised learning algorithms .Decision tables are one of the simplest hypothesis spaces possible , and usually they are easy to understand .Experimental results show that on artificial and real - world domains containing only discrete features , IDTM , an algorithm inducing decision tables , can sometimes outperform state - of - the - art algorithms such as C4.5 .","label":"Uses","metadata":{},"score":"39.244827"}
{"text":"Hence , instead of using a \" hard \" partitioning of the data , the authors use a \" soft \" partitioning , i.e. , the data is allowed to simultaneously lie in more than one region .The HME has a tree - structured architecture that is known a priori .","label":"Uses","metadata":{},"score":"39.369377"}
{"text":"A dynamic pooling layer computes a fixed - sized representation of variable - sized matrices , which is then used as input to a classifier .Richard Socher , Eric H. Huang , Jeffrey Pennington , Andrew Y. Ng , and Christopher D. Manning in NIPS 2011 .","label":"Uses","metadata":{},"score":"39.79463"}
{"text":"A supervised localization method is employed to select graspable segments .Deepak Rao , Quoc V. Le , Thanathorn Phoka , Morgan Quigley , Attawith Sudsand and Andrew Y. Ng in IROS 2010 .We develop a probabilistic method for combining closed - loop control in the well - modeled regions and open - loop control in the difficult - to - model regions and combine an inaccurate model of the system and a demonstration of the desired behavior .","label":"Uses","metadata":{},"score":"40.017998"}
{"text":"Andrew Y. Ng and Stuart Russell in ICML 2000 .We propose a new approach to the problem of searching a space of policies for a Markov decision process ( MDP ) or a partially observable Markov decision process ( POMDP ) , given a model .","label":"Uses","metadata":{},"score":"40.302826"}
{"text":"Andrew Y. Ng , Alice X. Zheng and Michael Jordan in IJCAI 2001 .LDA is a three - level hierarchical Bayesian model , in which each item of a collection is modeled as a finite mixture over an underlying set of topics .","label":"Uses","metadata":{},"score":"40.389942"}
{"text":"We propose a Markov chain model , whose stationary distribution is used to give word probability estimates .We show how to automatically learn a rich set of parameters for the Markov chain 's transition probabilities .We consider supervised learning in the presence of very many irrelevant features , and study two different regularization methods for preventing overfitting .","label":"Uses","metadata":{},"score":"40.41864"}
{"text":"We propose a method for space - indexed dynamic programming that overcomes the weaknesses of dynamic programming algorithms .We begin by showing how a dynamical system can be rewritten in terms of a spatial index variable rather than as a function of time and show that these algorithms perform well on a variety of control tasks , both in simulation and on real systems .","label":"Uses","metadata":{},"score":"40.72303"}
{"text":"J. Zico Kolter and Andrew Y. Ng in RSS 2009 .We present a simple algorithm , and prove that with high probability it is able to perform ǫ - close to the true optimal Bayesian policy after some small number of time steps .","label":"Uses","metadata":{},"score":"40.74409"}
{"text":"We present a fast approximation method , based on kd - trees , that significantly reduces both the prediction and the training times of Gaussian process regression .Yirong Shen , Andrew Y. Ng and Matthias Seeger in NIPS 18 2006 .","label":"Uses","metadata":{},"score":"40.8461"}
{"text":"The analysis of the results was performed using recently proposed statistical methodology including nonparametric tests followed by post - hoc procedures designed especially for multiple 1×n and n×n comparisons .GLM ( general linear model ) architectures of mixture of experts achieved better results for ME with an adaptive variance parameter for each expert , whereas MLP ( multilayer perceptron ) architectures - for standard mixtures of experts .","label":"Uses","metadata":{},"score":"40.900482"}
{"text":"Andrew Y. Ng in ICML 1998 .Our new algorithm that , given only a generative model for an arbitrary MDP , performs near - optimal planning with a running time that has no dependence on the number of states .Our results establish for the first time that there are no theoretical barriers to computing near - optimal policies in arbitrarily large , unstructured MDPs .","label":"Uses","metadata":{},"score":"41.14859"}
{"text":"Michael Kearns , Yishay Mansour and Andrew Y. Ng in NIPS 12 2000 .We propose a new approach to the problem of searching a space of stochastic controllers for a Markov decision process ( MDP ) or a partially observable Markov decision process ( POMDP ) .","label":"Uses","metadata":{},"score":"41.23698"}
{"text":"In general , to find an optimal graph partitioning is NP complete .Shi and Malik [ 15 ] proposed an approxi- mate spectral clustering algorithm that optimizes the nor- malized cut ( NCut ) objective function .It is a divisive , hierarchical clustering algorithm that recursively bi - parti- tions the graph until some criterion is reached , producing a tree structure .","label":"Uses","metadata":{},"score":"41.3244"}
{"text":"We present a new neural network architecture which 1 ) learns word embeddings that better capture the semantics of words by incorporating both local and global document context , and 2 ) accounts for homonymy and polysemy by learning multiple embeddings per word .","label":"Uses","metadata":{},"score":"41.369865"}
{"text":"We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models .Within this framework , we have developed two algorithms for large - scale distributed training .We have successfully used our system to train a deep network 30x larger than previously reported in the literature , and achieves state - of - the - art performance on ImageNet .","label":"Uses","metadata":{},"score":"41.391033"}
{"text":"Using all of the knowledge sources , the SVM method achieved the highest accuracy rate of 65.4 % .Another type of WSD approach uses established knowledge from curated terminology systems [ 23 , 24 ] .In the biomedical domain , Schijvenaars [ 13 ] developed a simple thesaurus - based algorithm to disambiguate human gene symbols using training data from PubMed abstracts and annotations from the Online Mendelian Inheritance in Man(OMIM )","label":"Uses","metadata":{},"score":"41.62148"}
{"text":"Our approach combines unsupervised and supervised learning techniques .Given a set of sequences and a similarity measure defined on pairs of sequences , we learn a mixture of experts model by using spectral clustering to learn the hierarchical structure of the model and by using bayesian techniques to combine the predictions of the experts .","label":"Uses","metadata":{},"score":"41.63936"}
{"text":"Given a collection of labeled samples L and unlabeled samples U , start by training a naive Bayes classifier on L .Until convergence , do : .Predict class probabilities for all examples x in .Re- train the model based on the probabilities ( not the labels ) predicted in the previous step .","label":"Uses","metadata":{},"score":"41.694885"}
{"text":"Eric Xing , Andrew Y. Ng , Michael Jordan , and Stuart Russell in NIPS 15 2003 .Latent Dirichlet Allocation ( LDA ) is a three - level hierarchical Bayesian model , in which each item of a collection is modeled as a finite mixture over an underlying set of topics .","label":"Uses","metadata":{},"score":"42.01915"}
{"text":"In this paper , we also demonstrated that ambiguity of biomedical entities is a significant problem , which has a substantial impact on text mining and retrieval tasks in the biomedical domain .ML methods are still needed for WSD , which is critical for increasing the accuracy of biomedical natural language , text mining , and information retrieval systems .","label":"Uses","metadata":{},"score":"42.08141"}
{"text":"The original ensemble method is Bayesian averaging , but more recent algorithms include error - correcting output coding , Bagging , and boostin ... \" .Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a ( weighted ) vote of their predictions .","label":"Uses","metadata":{},"score":"42.101555"}
{"text":"This paper describes a set of experiments carried out to explore the domain dependence of alternative supervised Word Sense Disambignation algorithms .The aim of the work is threefold : studying the performance of these algorithms when tested on a different corpus from that they were trained on ; expl ... \" .","label":"Uses","metadata":{},"score":"42.165905"}
{"text":"Empirical results showing that it performs surprisingly well in many domains containing clear attribute dependences suggest that the answer to this question may be positive .This article shows that , although the Bayesian classifier 's probability estimates are only optimal under quadratic loss if the independence assumption holds , the classifier itself can be optimal under zero - one loss ( misclassification rate ) even when this assumption is violated by a wide margin .","label":"Uses","metadata":{},"score":"42.19536"}
{"text":"In addition , we used an SVM classifier for all the experiments .Since the goals of our study did not include the comparison of different algorithms , we do not present related results here .Other studies showed that different ML algorithms had similar performance for WSD tasks [ 29 , 30 ] .","label":"Uses","metadata":{},"score":"42.196167"}
{"text":"Our best performance is based on K - means clustering and we achieve performance beyond all previously published results on the CIFAR-10 and NORB datasets .Adam Coates , Honglak Lee and Andrew Ng in AISTATS 14 , 2011 .We propose using unsupervised feature learning as a way to learn features directly from video data .","label":"Uses","metadata":{},"score":"42.22641"}
{"text":"Word Sense Disambiguation [ J&M 20.1 ] .Simple supervised WSD algorithm : naive Bayes [ J&M 20.2.2 ] .Semi - supervised WSD algorithm [ J&M 20.5 ] .Based on Gale / Yarowsky 's \" one sense per collocation \" and \" one sense per discourse \" observation ( generally true for coarse word senses ) Allows bootstrapping ( semi - supervised learning ) from a small set of sense - annotated seeds Basic idea of bootstrapping : . start with a small set of labeled seeds L and a large set of unlabeled examples U repeat . train classifier C on L apply C to U identify examples with most confident labels ; remove them from U and add them ( with labels ) to L .","label":"Uses","metadata":{},"score":"42.31515"}
{"text":"In this paper we propose the Genetic Learning Across Datasets concept ( GLAD ) , and demonstrate an implementation that enables feature selection across unlabeled and labeled datasets .GLAD algorithms are distinct from previous approaches of semi - supervised learning in that the datasets analyzed may have very different statistical distributions , such as would arise in datasets collected independently by labs using different measurement technology .","label":"Uses","metadata":{},"score":"42.326626"}
{"text":"We consider the scaling of the number of examples necessary to achieve good performance in distributed , cooperative , multi - agent reinforcement learning , as a function of the the number of agents n. We demonstrate , however , that there is a class of algorithms that , by taking advantage of local reward signals in large distributed Markov Decision Processes , are able to ensure good performance with a number of samples that scales as O(log n ) .","label":"Uses","metadata":{},"score":"42.388298"}
{"text":"Our experimental results for four text data sets confirmed that the generalization ability of our hybrid classifier was much improved by using a large number of unlabeled samples for training when there were too few labeled samples to obtain good performance .","label":"Uses","metadata":{},"score":"42.44767"}
{"text":"We introduce a model based on a combination of convolutional and recursive neural networks ( CNN and RNN ) for learning features and classifying RGB - D images .Our main result is that even RNNs with random weights compose powerful features .","label":"Uses","metadata":{},"score":"42.619347"}
{"text":"We introduce a model that maps variable - length word utterances to a word vector space using convolutional neural networks .Our approach models entire word acoustics rather than short windows as in previous work .We introduce the notion of mapping these word inputs to a word vector space , rather than trying to solve the mas- sively multi - class problem of word classification .","label":"Uses","metadata":{},"score":"42.642036"}
{"text":"MH boosting algorithm is applied to the Word Sense Disambiguation ( WSD ) problem .Initial experiments on a set of 15 selected polysemous words show that the boosting approach surpasses Naive Bayes and Exemplar - based approaches , which represent stat ... \" .","label":"Uses","metadata":{},"score":"42.69418"}
{"text":"View Article .Engelson SP , Dagan I : Minimizing manual annotation cost in supervised training from corpora .34th Annual Meeting of Association for Computational Linguistics 319 - 326 .Pustejovsky J , Castano J , Cochran B , Kotecki M , Morrell M : Automatic extraction of acronym - meaning pairs from MEDLINE databases .","label":"Uses","metadata":{},"score":"42.73931"}
{"text":"For testing the classification accuracy on the independent set , only unique classifiers were used .Figure 2 compares the performance of the unique classifiers on the testing set for two approaches : 1 - using only labeled samples 2 - using labeled plus unlabeled samples .","label":"Uses","metadata":{},"score":"42.79042"}
{"text":"We propose an algorithm flexibly incorporates evidence from multiple classifiers over heterogenous relationships to optimize the entire structure of the taxonomy , using knowledge of a word 's coordinate terms to help in determining its hypernyms , and vice versa .We apply our algorithm on the problem of sense - disambiguated noun hyponym acquisition , where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy ( WordNet 2.1 ) .","label":"Uses","metadata":{},"score":"42.86743"}
{"text":"In our method , independent models are developed using subsets of genes for the annotated and unannotated datasets .These models are evaluated according to a scoring function that incorporates terms for classification accuracy on annotated data , and relative cluster separation in unannotated data .","label":"Uses","metadata":{},"score":"42.904144"}
{"text":"Jiquan Ngiam , Zhenghao Chen , Pangwei Koh and Andrew Y. Ng in ICML 2011 .We show that more sophisticated off - the - shelf optimization methods such as Limited memory BFGS ( L - BFGS ) and Conjugate gradient ( CG ) with line search can significantly simplify and speed up the process of pretraining deep algorithms .","label":"Uses","metadata":{},"score":"43.058186"}
{"text":"[ 7 ] introduced a Bayesian Semi - Supervised approach termed BGEN ( Bayesian GENeralization ) .The BGEN method trains a kernel classifier using both labeled and unlabeled data .Their example data consisted of expression profiles of wild type and mutant C. elegant embryos and identified enriched genes , with a small subset of genes labeled according to involvement in development of cell lineage .","label":"Uses","metadata":{},"score":"43.060944"}
{"text":"Therefore it is important that we understand the different elements affecting their performance .Methods .After manually reviewing a set of WSD papers in the biomedical domain , different issues associated with performance were enumerated .For an initial study , we conducted experiments to evaluate the effect of three confounding issues : \" sample size \" , \" sense distribution \" and \" degree of difficulty \" , and we used an automatically generated data set .","label":"Uses","metadata":{},"score":"43.101406"}
{"text":"We use a green screen to rapidly collect example images ; we then use a probabilistic model to rapidly synthesize a much larger training set that attempts to capture desired invariants in the object 's foreground and background .We demonstrate this procedure on our own mobile robotics platform , where we achieve 135x savings in the time / effort needed to obtain a training set .","label":"Uses","metadata":{},"score":"43.54352"}
{"text":"We study several different methods of assignment , including the Õhard \" assignments used by -means and the Õsoft \" assignments used by EM .The cornerstone of our results is a simple decomposition of the expected distortion , showing that -means must implicitly manage a trade - offbetween how similar the data assigned to each cluster are , and how the data are balanced among the clusters .","label":"Uses","metadata":{},"score":"43.559002"}
{"text":"The function found by our algorithm then defines a new learning algorithm for text classification , which we can apply to novel classification tasks .We find that our learned classifier outperforms existing methods on a variety of multiclass text classification tasks .","label":"Uses","metadata":{},"score":"43.662193"}
{"text":"We introduce a novel machine learning frame- work based on recursive autoencoders for sentence - level prediction of sentiment label distributions , which learns vector space representations for multi - word phrases , that outperforms other state - of - the - art approaches .","label":"Uses","metadata":{},"score":"43.666126"}
{"text":"This agrees with work by Rifkin and Klatau [ 34 ] .A description of the different multi - class algorithms is provided in the Methods section .Table 5 .Results for BPD data set .Annotation of the table : Dist : Distribution of senses ; S. Size : sample size ; Err .","label":"Uses","metadata":{},"score":"43.73208"}
{"text":"1.3 Our Approach The approach we take to detecting people in static images borrows ideas from the elds of object detection in images and data classi cation .In particular , the system attempts to de ... . \" ...Tree induction is one of the most effective and widely used methods for building classification models .","label":"Uses","metadata":{},"score":"43.82695"}
{"text":"We develop general principles for massively parallelizing unsupervised learning tasks using graphics processors and show that these principles can be applied to successfully scaling up learning algorithms for both DBNs and sparse coding .Our implementation of DBN learning is up to 70 times faster than a dual - core CPU implementation for large models while our sparse coding algorithm leads to a 5 to 15-fold speedup over previous methods .","label":"Uses","metadata":{},"score":"43.839832"}
{"text":"To reliably choose a near - best strategy from a restricted class of strategies in a partially observable Markov decision process ( POMDP ) , we generate a \" small \" set of trajectories that provide an accurate estimate of the value of any strategy in the class .","label":"Uses","metadata":{},"score":"43.964157"}
{"text":"Our study is different because it quantified the effect of similarity of senses , and studied the relation between \" similarity of senses \" and other issues such as \" sample size \" and \" sense distribution \" .Podowski 's [ 28 ] work covered task types 2 and 3 , while Hatzivassiloglou 's [ 10 ] work addressed task type 4 .","label":"Uses","metadata":{},"score":"44.066277"}
{"text":"Supervised ML methods have also been applied to WSD in the biomedical domain .Hatzivassiloglou [ 10 ] developed a disambiguation system to determine the class of a known biomedical named entity by choosing one of three pre - defined senses : gene , RNA , protein .","label":"Uses","metadata":{},"score":"44.172264"}
{"text":"For BPD , which has three different senses , three different multi - class SVM methods [ 47 ] : \" mc - svm \" , \" one - vs - rest \" , \" one - vs - one \" , were used . \" Mc - svm \" implements the algorithm with a decision function which considers all classes at once , while \" one - vs - rest \" and \" one - ve - one \" are constructed by combining several binary SVM classifiers .","label":"Uses","metadata":{},"score":"44.24102"}
{"text":"These algorithms also correctly predict the radically altered receptive fields found in animals raised in experimentally manipulated environments .Andrew Saxe , Maneesh Bhand , Ritvik Mudur , Bipin Suresh and Andrew Y. Ng in NIPS 2011 .Sparse filtering is a simple new algorithm that optimizes a simple cost function , and only has one hyperparameter , the number of features to learn .","label":"Uses","metadata":{},"score":"44.318974"}
{"text":"First , no comparison was made between MetaCost 's final model and the internal cost - sensitive classifier on which MetaCost depends .It is credible that the internal cost - sensitive classifier may outperform the final model without the additional computation required to derive the final model .","label":"Uses","metadata":{},"score":"44.367493"}
{"text":"In addition , papers should also characterize the difficulty of the WSD task , the WSD situations addressed and not addressed , as well as the ML methods and features used .This should lead to an improved understanding of the generalizablility and the limitations of the methodology .","label":"Uses","metadata":{},"score":"44.367825"}
{"text":"To test the null hypothesis of no differences in the error rates among the different sample sizes ( and overall probability distribution ) for the BSA and PCA abbreviations , we used Friedman 's test .Then we performed sub - analysis using the sign - test ( see Methods section for details ) .","label":"Uses","metadata":{},"score":"44.400208"}
{"text":"Honglak Lee , Yirong Shen , Chih - Han Yu , Gurjeet Singh , and Andrew Y. Ng in ICRA 2006 .We present a novel architecture , which models greedy 1-best pipelines as Bayesian networks , with each low level task corresponding to a variable in the network , and then we perform approximate inference to find the best labeling .","label":"Uses","metadata":{},"score":"44.52711"}
{"text":"[ 11 ] .Despite the fact that the far - reaching independence assumptions are often inaccurate , the naive Bayes classifier has several properties that make it surprisingly useful in practice .In particular , the decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one - dimensional distribution .","label":"Uses","metadata":{},"score":"44.540478"}
{"text":"The link between the two can be seen by observing that the decision function for naive Bayes ( in the binary case ) can be rewritten as \" predict class if the odds of exceed those of \" .Expressing this in log - space gives : .","label":"Uses","metadata":{},"score":"44.679714"}
{"text":"Michael Kearns , Yishay Mansour and Andrew Y. Ng in IJCAI 1999 .We investigate conditions under which modifications to the reward function of a Markov decision process preserve the optimal policy .These results shed light on the practice of reward shaping , a method used in reinforcement learning whereby additional training rewards are used to guide the learning agent .","label":"Uses","metadata":{},"score":"44.77478"}
{"text":"Jiquan Ngiam , Pangwei Koh , Zhenghao Chen , Sonia Bhaskar and Andrew Y. Ng in NIPS 2011 .By using a robust soft reconstruction cost for ICA that allows us to learn highly overcomplete sparse feature even on unwhitened data , we reveal formal connections between ICA and sparse autoencoders , that were previously only empirically observed .","label":"Uses","metadata":{},"score":"44.823097"}
{"text":"\" Sample size ' , \" sense distribution \" and \" degree of difficulty \" were three of multiple confounding issues that affect the performance of a WSD classifier .Results from our experiments demonstrated that these three factors were intrinsically connected .","label":"Uses","metadata":{},"score":"44.904583"}
{"text":"MH boosting algorithm is applied to the Word Sense Disambiguation ( WSD ) problem .Initial experiments on a set of 15 selected polysemous words show that the boosting approach surpasses Naive Bayes and Exemplar - based approaches , which represent state - of - the - art accuracy on supervised WSD .","label":"Uses","metadata":{},"score":"44.93324"}
{"text":"The key to this finding lies in the distinction between classification and probability estimation : correct classification can be achieved even when the probability estimates used contain large errors .We show that the previously - assumed region of optimality of the SBC is a second - order infinitesimal fraction of the actual one .","label":"Uses","metadata":{},"score":"44.993774"}
{"text":"While naive Bayes often fails to produce a good estimate for the correct class probabilities , [ 12 ] this may not be a requirement for many applications .For example , the naive Bayes classifier will make the correct MAP decision rule classification so long as the correct class is more probable than any other class .","label":"Uses","metadata":{},"score":"45.014122"}
{"text":"We present a novel framework based on factor graphs for automatically inferring specifications directly from programs .The key strength of the approach is that it can incorporate many disparate sources of evidence , allowing us to squeeze significantly more information from our observations than previously published techniques .","label":"Uses","metadata":{},"score":"45.021835"}
{"text":"We illustrate these two approaches on two types of data , one collected from the web , mainly publication lists from homepages , the other collected from the DBLP citation databases . ...author 's research area and his or her individual patterns of coauthoring .","label":"Uses","metadata":{},"score":"45.064342"}
{"text":"In these domains , both costs and probabilities are unknown for test examples , so both cost estimators and probability estimators must be learned .This paper rst discusses how to make optimal decisions given cost and probability estimates , and then presents decision tree learning methods for obtaining well - calibrated probability estimates .","label":"Uses","metadata":{},"score":"45.110435"}
{"text":"We describe an efficient procedure for performing these projections , derive a worst case mistake bound on the similarity predictions , and discuss a dual version of the algorithm in which it is simple to incorporate kernel operators .The online algorithm also serves as a building block for deriving a large - margin batch algorithm .","label":"Uses","metadata":{},"score":"45.13224"}
{"text":"Artificial Intelligence Workshop Machine Learning for Information Filtering , pp.61 - 67 , 1999 .[26 ] R. Bekkerman , R. El - Yaniv , N. Tishby , and Y. Winter , \" On Feature Distributional Clustering for Text Classification , \" Proc . 24th ACM Int'l Conf .","label":"Uses","metadata":{},"score":"45.29287"}
{"text":"In this paper we take an in - depth study of the performance of decision lists on two publicly available corpora and an additional corpus automatically acquired from the Web , using the fine - grained highly polysemous senses in WordNet .","label":"Uses","metadata":{},"score":"45.417763"}
{"text":"Andrew L. Maas , Raymond E. Daly , Peter T. Pham , Dan Huang , Andrew Y. Ng , and Christopher Potts in ACL 2011 .We apply several off - the - shelf feature learning algorithms to NORB and CIFAR datasets using only single - layer networks .","label":"Uses","metadata":{},"score":"45.471203"}
{"text":"Our hypothesis is that for memory - based learning ( MBL ) , a reduced amount of data is more beneficial than the full range of features used in the past .Our experiments show that MBL combined with a restricted set of features and a feature selection method that minimizes the feature set leads to competitive results , outperforming all systems that participated in the SENSEVAL-3 competition on the Romanian data .","label":"Uses","metadata":{},"score":"45.516"}
{"text":"Finally , ( 4 ) the domains on which tree induction and logistic regression are ultimately preferable canbecharacterized surprisingly well by a simple measure of signal - to - noise ratio . ... despread .73.1.3PETs and pruning If we 're going to compare tree induction to logistic regression using their probability estimates ... . by Gerard Escudero , Lluis Marquez , German Rigau - IN PROCEEDINGS OF THE 12TH EUROPEAN CONFERENCE ON MACHINE LEARNING , 2000 . \" ...","label":"Uses","metadata":{},"score":"45.530064"}
{"text":"We describe an efficient algorithm which - when given access to a few trajectory demonstrations - can automatically infer good trade - offs between the different costs .Pieter Abbeel , Dmitri Dolgov , Andrew Y. Ng and Sebastian Thrun in IROS 2008 .","label":"Uses","metadata":{},"score":"45.55963"}
{"text":"We submitted two runs for the main QA track ( AskMSR and AskMSR2 ) .Data - driven methods have proven to be powerful techniques for natural language processing .It is still unclear to what extent this success can be attributed to specific techniques , versus simply the data itself .","label":"Uses","metadata":{},"score":"45.57086"}
{"text":"Mooney RJ : Comparative experiments on disambiguating word senses : An illustration of the role of bias in machine learning .Proc 1996 Conf on Empirical Methods in Natural Language Processing 82 - 91 .Ng HT , Lee HB : Integrating multiple knowledge sources to disambiguate word sense : An examplar - based approach .","label":"Uses","metadata":{},"score":"45.761337"}
{"text":"The aim of the work is threefold : firstly , studying the performance of these algorithms when tested on a different corpus from that they were trained o ... \" .This report describes a set of experiments carried out to explore the portability of alternative supervised Word Sense Disambiguation algorithms .","label":"Uses","metadata":{},"score":"45.845276"}
{"text":"In the situation that such a split is not available , a random assignment of features into two sets still performs better than using only one feature set .They also introduced the co - EM algorithm , a hybrid that iteratively updates the unlabeled data labels using EM .","label":"Uses","metadata":{},"score":"45.9737"}
{"text":"Experimental results from using 26 confusion sets and a large amount of unlabeled data show that our proposed method for using unlabeled data considerably improves classification performance when the amount of labeled data is small . . ..In this section , we briefly review the naive Bayes classifier and the EM algorithm that is used for making use of unlabeled data .","label":"Uses","metadata":{},"score":"46.04787"}
{"text":"Both of these can cause problems for NLP applications , including infomation extraction .In this section we take a closer look at word meanings .This will also give us an opportunity to see a wide range of approaches : hand - coded resources and methods involving supervised , semi - supervised , and unsupervised training .","label":"Uses","metadata":{},"score":"46.16006"}
{"text":"Earlier studies have investigated a number of the issues discussed here in the context of constructing better classifiers .A discussion of some of the issues involved can be found in [ 43 ] .Here , we examined these issues in the context of word sense disambiguation .","label":"Uses","metadata":{},"score":"46.21652"}
{"text":"Ninth International Conference on Information and Knowledge Management ( CIKM-2000 ) 2000 , 86 - 93 .Li T , Zhu S , Li Q , Ogihara M : Gene Functional Classification by Semi - supervised Learning from heterogeneous data .Proceedings of The 18th Annual ACM Symposium on Applied Computing ( SAC 2003)-Bioinformatics Track 2003 , 78 - 82 .","label":"Uses","metadata":{},"score":"46.341576"}
{"text":"Most of the above papers reporting on the use of ML for WSD follow a similar pattern .A set of ambiguous words is selected , a corpus for each word is collected , and the different senses within the corpus are annotated ( automatically or manually ) .","label":"Uses","metadata":{},"score":"46.45169"}
{"text":"This training algorithm is an instance of the more general expectation - maximization algorithm ( EM ) : the prediction step inside the loop is the E -step of EM , while the re - training of naive Bayes is the M -step .","label":"Uses","metadata":{},"score":"46.551785"}
{"text":"The results of the study show several remarkable things .( 1 ) Contrary to prior observations , logistic regression does not generally outperform tree induction .( 2 )More specifically , and not surprisingly , logistic regression is better for smaller training sets and tree induction for larger data sets .","label":"Uses","metadata":{},"score":"46.56154"}
{"text":"A unique two - term scoring function was derived to independently score the labeled and unlabeled data models .An overall score is computed as a weighted average of the two terms as shown below .We defined the labeled data model score as the standard leave - one - out - cross - validation accuracy for the labeled training samples .","label":"Uses","metadata":{},"score":"46.576797"}
{"text":"The resulting system is useful for multi- ( and single- ) antenna receiver development , as it allows for simple data collection .The data files can then be used as inputs to software receivers for algo- rithm development , testing , and quantitative comparisons .","label":"Uses","metadata":{},"score":"46.642773"}
{"text":"We study the computational and sample complexity of parameter and structure learning in graphical models .Our main result shows that the class of factor graphs with bounded degree can be learned in polynomial time and from a polynomial number of training examples , assuming that the data is generated by a network in this class .","label":"Uses","metadata":{},"score":"46.717007"}
{"text":"Our main result shows that the class of factor graphs with bounded factor size and bounded connectivity can be learned in polynomial time and polynomial number of samples , assuming that the data is generated by a network in this class .","label":"Uses","metadata":{},"score":"46.832954"}
{"text":"We overview supervised , unsupervised , and knowledge - based approaches .The assessment of WSD systems is discussed in the context of the Senseval / Semeval campaigns , aiming at the objective evaluation of systems participating in several different disambiguation tasks .","label":"Uses","metadata":{},"score":"46.852974"}
{"text":"Therefore , this type of method may not be applicable and ML approaches may be useful .Recently , Humphrey [ 26 ] proposed another type of statistical - based method to resolve the ambiguity problem within the UMLS Metathesaurus .They used a Journal Descriptor Indexing ( JDI ) method , which is ultimately based on statistical associations between words in a training set of MEDLNE citations and a small set of journal descriptors assumed to be inherited by the citations .","label":"Uses","metadata":{},"score":"46.97661"}
{"text":"The simple Bayesian classifier is known to be optimal when attributes are independent given the class , but the question of whether other sufficient conditions for its optimality exist has so far not been explored .Empirical results showing that it performs surprisingly well in many domains containin ... \" .","label":"Uses","metadata":{},"score":"47.06038"}
{"text":"We present a dynamic Bayesian network model capable of resolving some of these ambiguities and recovering 3d information for many images .When the image is produced under perspective geometry , we show that this model can be used for 3d reconstruction from a single image .","label":"Uses","metadata":{},"score":"47.32465"}
{"text":"Richard Socher , Jeffrey Pennington , Eric Huang , Andrew Y. Ng , and Christopher D. Manning in EMNLP 2011 .We use unsupervised recursive autoencoders ( RAE ) that learn feature vectors for phrases in syntactic trees , for paraphrase detection .","label":"Uses","metadata":{},"score":"47.478363"}
{"text":"17 ] F.G. Cozman and I. Cohen , \" Unlabeled Data Can Degrade Classification Performance of Generative Classifiers , \" Proc .15th Int'l Florida Artificial Intelligence Research Soc .Conf . , pp .327 - 331 , 2002 .[","label":"Uses","metadata":{},"score":"47.542397"}
{"text":"As is evident in figure 1 , adding unlabeled samples increased the mean accuracy of the models significantly .This figures shows the improvement of the classification by adding unlabeled samples into the experiments .Figure 2 displays the improvements of the classification accuracies for the population of unique classifiers in each cancer group .","label":"Uses","metadata":{},"score":"47.55298"}
{"text":"This paper investigates two important issues centered on the procedure which were ignored in the paper proposing MetaCost .First , no comparison was made between MetaCost 's ... \" .MetaCost is a recently proposed procedure that converts an error - based learning algorithm into a cost - sensitive algorithm .","label":"Uses","metadata":{},"score":"47.56652"}
{"text":"We address the problem of inverse reinforcement learning ( IRL ) in Markov decision processes , that is , the problem of extracting a reward function given observed , optimal behavior .To remove degeneracy , we suggest some natural heuristics that attempt to pick a reward function that maximally differentiates the observed policy from other , sub - optimal policies .","label":"Uses","metadata":{},"score":"47.632008"}
{"text":"David Blei , Andrew Y. Ng and Michael Jordan in Journal of Machine Learning Research , 3:993 - 1022 2003 .We describe a hybrid model in which a high - dimensional subset of the parameters are trained to maximize generative likelihood , and another , small , subset of parameters are discriminatively trained to maximize conditional likelihood .","label":"Uses","metadata":{},"score":"47.696953"}
{"text":"Several papers [ 29 , 30 ] realized this issue and reported results for the baseline .More specifically , they excluded samples with a majority sense larger than a threshold because they realized the contribution of the classifier would not be much for those cases .","label":"Uses","metadata":{},"score":"47.814465"}
{"text":"It is unclear whether violation of the assumption has an impact on MetaCost 's performance .We study these issues using two boosting procedures , and compare with the performance of the original form of MetaCost which employs bagging .Naive Bayes has been studied extensively since the 1950s .","label":"Uses","metadata":{},"score":"47.8721"}
{"text":"Such name ambiguity affects the performance of document retrieval , web search , database integration , and may cause improper attribution to authors .This paper investigates two supervised learning approaches to disambiguate authors in the citations 1 .One approach uses the naive Bayes probability model , a generative model ; the other uses Support Vector Machines(SVMs ) [ 39 ] and the vector space representation of citations , a discriminative model .","label":"Uses","metadata":{},"score":"47.892292"}
{"text":"We propose a new representation for orientations - and a class of learning and inference algorithms using this representation - that allows us to learn orientations for symmetric or asymmetric objects as a function of a single image .We extensively evaluate our algorithm for learning orientations of objects from six categories .","label":"Uses","metadata":{},"score":"47.934227"}
{"text":"By empirically eavaluating various audio classification tasks by applying convolutional deep belief networks to audio data , we show that the learned features correspond to phone / phonemes and show very good performance for multiple audio classification tasks .Honglak Lee , Yan Largman , Peter Pham and Andrew Y. Ng in NIPS 2009 .","label":"Uses","metadata":{},"score":"47.97917"}
{"text":"Thus , we compared the PR curves for NB and the mixture of NB models as well as LR and mixture of LR models on both RNA- and DNA - protein interface predic- tion tasks .While this is true for any identity cutoff for both RNA- and DNA - pro- tein sequence data sets , we show results only for 30 % identity cutoff .","label":"Uses","metadata":{},"score":"48.041496"}
{"text":"The results presented here agree with general results presented in the literature on the performance of classifiers [ 43 - 45 ] .Future work .To further analyze the effects of \" sample size \" , \" sense distribution \" and \" degree of difficulty \" on the error rate , an error decomposition model will be explored .","label":"Uses","metadata":{},"score":"48.19949"}
{"text":"As with the above approaches , independent models are developed for each dataset .They show that minimizing the disagreement in predictions between these models leads to improved accuracy , and introduced a co - updating technique for iteratively improving prediction concordance .","label":"Uses","metadata":{},"score":"48.23121"}
{"text":"This could also be due to the existence of other confounding factors in the datasets that were used .In our study , we controlled for this factor by using \" bag - of - word \" features in all experiments , but it would be interesting to see if the performance improves when different feature vectors are used .","label":"Uses","metadata":{},"score":"48.289024"}
{"text":"We investigate the benefits of online search in such cases .We examine \" local \" searches , where the agent performs a finite - depth lookahead search , and \" global \" searches , where the agent performs a search for a trajectory all the way from the current state to a goal state .","label":"Uses","metadata":{},"score":"48.30884"}
{"text":"Tools . \" ...Word sense disambiguation ( WSD ) is the ability to identify the meaning of words in context in a computational manner .WSD is considered an AI - complete problem , that is , a task whose solution is at least as hard as the most difficult problems in artificial intelligence .","label":"Uses","metadata":{},"score":"48.33607"}
{"text":"We study the Voting Gibbs classifier , which is the extension of this scheme to the full Monte Carlo setting , in which N samples are drawn from the posterior and new inputs are classified by voting the N resulting classifiers .","label":"Uses","metadata":{},"score":"48.39525"}
{"text":"Ginter [ 27 ] introduced a new family of classifiers , which were based on an ordering and weighing of the feature vectors obtained from word counts and word co - occurrence in the text .This method was used to determine whether a term was a gene versus a protein and achieved 86 % accuracy .","label":"Uses","metadata":{},"score":"48.46131"}
{"text":"For example , the SBC is optimal for learning arbitrary conjunctions and disjunctions , even though they violate the independence assumption .The paper also reports empirical evidence of the SBC 's competitive performance in domains containing substantial degrees of attribute dependence . by Scott E. Hudson , James Fogarty , Christopher G. Atkeson , Daniel Avrahami , Jodi Forlizzi , Sara Kiesler , Johnny C. Lee , Jie Yang - CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS , 2003 . \" ...","label":"Uses","metadata":{},"score":"48.597176"}
{"text":"Jenny Finkel , Chris Manning and Andrew Y. Ng in EMNLP 2006 .We propose an efficient algorithm for L1 regularized logistic regression .Our algorithm iteratively approximates the objective function by a quadratic approximation at the current point , while maintaining the L1 constraint .","label":"Uses","metadata":{},"score":"48.777977"}
{"text":"Thus , the feature selec ... . by George John , Pat Langley - In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence , 1995 . \" ...When modeling a probability distribution with a Bayesian network , we are faced with the problem of how to handle continuous variables .","label":"Uses","metadata":{},"score":"48.779297"}
{"text":"[ 4 ] and Kim et al .[5 ] used Support Vector Machines to identify residues in a protein sequence that undergo post - translational modifications .The classifier is trained to label the target element .This procedure can produce reli- able results in settings where there exists a local sequence pattern that is predictive of the label for the target site .","label":"Uses","metadata":{},"score":"48.843224"}
{"text":"Morgan Quigley , Alan Asbeck and Andrew Y. Ng in ICRA 2011 .We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term - document information as well as rich sentiment content .","label":"Uses","metadata":{},"score":"48.9233"}
{"text":"The i th binary SVM classifier is trained by considering all instances associated with the i th class as positive examples and the others as negative instances .It applies the N classifiers and chooses the one with the highest confidence .","label":"Uses","metadata":{},"score":"48.965694"}
{"text":"We present an automated system for deciding whether a given sentence is entailed from a body of text .We present results on the Recognizing Textual Entailment ( RTE ) dataset ( Dagan et al . , 2005 ) , compare to other approaches , discuss common classes of errors , and discuss directions for improvement .","label":"Uses","metadata":{},"score":"48.97705"}
{"text":"We show that using a simple , common smoothing method -- the Laplace correction -- uniformly improves probability - based rankings .In addition , bagging substantioJly improves the rankings , and is even more effective for this purpose than for improving accuracy .","label":"Uses","metadata":{},"score":"49.28569"}
{"text":"Jiquan Ngiam , Aditya Khosla , Mingyu Kim , Juhan Nam , Honglak Lee and Andrew Y. Ng in ICML 2011 .We propose deep energy models , which use deep feedforward neural networks to model the energy landscapes that define probabilistic models .","label":"Uses","metadata":{},"score":"49.38616"}
{"text":"We find that choosing a good encoder is more important that spending resources on training .Adam Coates and Andrew Y. Ng in ICML 2011 .We introduce a max - margin structure prediction architecture , based on recursive neural networks , that can successfully recover structure from bother complex scene images and sentences .","label":"Uses","metadata":{},"score":"49.41613"}
{"text":"Tree induction and logistic regression are two standard , off - the - shelf methods for building models for classi cation .We present a large - scale experimental comparison of logistic regression and tree induction , assessing classification accuracy and the quality of rankings based on class - membership probabilities .","label":"Uses","metadata":{},"score":"49.626472"}
{"text":"Our results show that the addition of unannotated data into training , significantly improves classifier robustness .Background .The introduction of DNA microarray technology in 1995 [ 1 ] has likely resulted in a huge volume of as yet undiscovered and potentially medically useful knowledge within gene expression profiles .","label":"Uses","metadata":{},"score":"49.6647"}
{"text":"If available , labels were removed from one of the component datasets , thus creating a combined dataset with both labeled and unlabeled subsets .All datasets were produced using Affymetrix GeneChips , and in two cases the labeled and unlabeled datasets were collected with different Affymetrix GeneChips .","label":"Uses","metadata":{},"score":"49.681343"}
{"text":"In most cases , classification accuracy using the reduced feature set equaled or bettered accuracy using the complete feature set . ... a feature X that is perfectly correlated with a second feature Y , then treating them as independent means that X ( or Y ) has twice as much affect on Equation 2.1 as it should have .","label":"Uses","metadata":{},"score":"49.697388"}
{"text":"We first segment the data by the class , and then compute the mean and variance of in each class .Let be the mean of the values in associated with class c , and let be the variance of the values in associated with class c .","label":"Uses","metadata":{},"score":"49.71393"}
{"text":"These assumptions lead to two distinct models , which are often confused .[ 9 ] [ 10 ] .When dealing with continuous data , a typical assumption is that the continuous values associated with each class are distributed according to a Gaussian distribution .","label":"Uses","metadata":{},"score":"49.72264"}
{"text":"Comparison of Correlation Coefficient for Naïve Bayes and mixture of Naïve Bayes models that capture global sequence similarity on the non - redundant RNA - protein data sets constructed using various identity cutoffs , starting from 30 % and ending at 90 % in steps of 10 .","label":"Uses","metadata":{},"score":"49.725227"}
{"text":"In our study , we proposed a simple \" full - term substitution \" method , which is described in more detail in the Methods section , to automatically generate training data , but this is only applicable for abbreviations .In this study , we used a \" full - form substitution \" method to automatically generate the data set for the experiments , which is an artificial training set .","label":"Uses","metadata":{},"score":"49.73049"}
{"text":"Using this approach , we have created a larger percentage of qualitatively and quantitatively more correct 3-d models for images downloaded from the internet .Ashutosh Saxena , Min Sun , and Andrew Y. Ng in ICCV 2007 .We present a learning algorithm that neither requires , nor tries to build , a 3-d model of the object .","label":"Uses","metadata":{},"score":"49.75252"}
{"text":"Using tools from matrix perturbation theory and Markov chain theory , we provide conditions under which methods for identifying \" authoritative \" or \" influential \" articles , given hyperlink or citation information , are stable and give specific examples of instability when these conditions are violated .","label":"Uses","metadata":{},"score":"49.79358"}
{"text":"^ Narasimha Murty , M. ; Susheela Devi , V. ( 2011 ) .Pattern Recognition : An Algorithmic Approach .ISBN 0857294946 .^ John , George H. ; Langley , Pat ( 1995 ) .Estimating Continuous Distributions in Bayesian Classifiers .","label":"Uses","metadata":{},"score":"49.866264"}
{"text":"View Article .Pedersen T , Bruce R : Distinguishing word senses in untagged text .Second Conference on Empirical Methods in Natural Language Processing .Hsu G , Lin C : A comparison of methods for multi - class support vector machines .","label":"Uses","metadata":{},"score":"49.96837"}
{"text":"Bioinformatics 2004 , 20 : 2597 - 2604 .View Article PubMed .Schijvenaars BJ , Mons B , Weeber M , Schuemie MJ , van Mulligen EM , Wain HM , et al .: Thesaurus - based disambiguation of gene symbols .","label":"Uses","metadata":{},"score":"49.975166"}
{"text":"Science 1999 , 286 : 531 - 537 .View Article PubMed .Blum A , Mitchell TM : Combining labeled and unlabeled data with co - training .Proceedings of the Eleventh Annual Conference on Computational Learning Theory 1998 , 92 - 100 .","label":"Uses","metadata":{},"score":"50.02506"}
{"text":"All experiments were performed using real - world datasets taken from a cadastral system .The analysis of the results was performed using statistical methodology including nonparametric tests followed by post - hoc procedures designed especially for multiple 1×N and N×N comparisons .","label":"Uses","metadata":{},"score":"50.028816"}
{"text":"In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence , Morgan Kaufmann Publishers , San Mateo , 1995 1 Introduction In rec ... . \" ...The simple Bayesian classifier ( SBC ) is commonly thought to assume that attributes are independent given the class , but this is apparently contradicted by the surprisingly good performance it exhibits in many domains that contain clear attribute dependences .","label":"Uses","metadata":{},"score":"50.167267"}
{"text":"The cluster separation term is given by a modified ratio of the inter - cluster distance to the mean cluster size .The consistent proportion term , is defined as the RMS difference between the sorted actual and expected class priors .","label":"Uses","metadata":{},"score":"50.20035"}
{"text":"View Article PubMed .Maglott D , Ostell J , Pruitt KD , Tatusova T : Entrez Gene : Gene - centered information at NCBI .Nucleic Acids Res 2005 , 3 : D54-D58 .Yngve VH : Syntax and the problem of multiple meaning .","label":"Uses","metadata":{},"score":"50.212353"}
{"text":"By training a model with 1 billion connections on 16,000 CPU cores , we show that unsupervised deep learning is able to learn high - level , class - specific feature detectors , such as learning a \" cat \" detector from watching YouTube .","label":"Uses","metadata":{},"score":"50.345844"}
{"text":"The same study , which was also performaned for the Fly organism , showed similar results , but with slightly higher ambiguity rates .For a more complete description of this study and the results , please [ see additional file 1 ] .","label":"Uses","metadata":{},"score":"50.405327"}
{"text":"A central problem in machine learning is identifying a representative set of features from which to construct a classification model for a particular task .This thesis addresses the problem of feature selection for machine learning through a correlation based approach .","label":"Uses","metadata":{},"score":"50.57831"}
{"text":"A central problem in machine learning is identifying a representative set of features from which to construct a classification model for a particular task .This thesis addresses the problem of feature selection for machine learning through a correlation based approach .","label":"Uses","metadata":{},"score":"50.57831"}
{"text":"The number in each node indicates the number of protein sequences belonging to it .The Needleman - Wunch global alignment score was used as a pairwise similarity measure during the clustering process .Page 12 .Thus , having the hierarchical clustering stored , we devise a procedure that allows each sequence in the training set to simultaneously lie in all clusters , with a different weigth in each cluster . , n in the training set compute its cluster membership as follows : , we 1 .","label":"Uses","metadata":{},"score":"50.58321"}
{"text":"We introduce a model which uses a deep recurrent auto encoder neural network to denoise input features for robust ASR .We demonstrate the model is competitive with existing feature denoising approaches on the Aurora2 task , and outperforms a tandem approach where deep networks are used to predict phoneme posteriors directly .","label":"Uses","metadata":{},"score":"50.586838"}
{"text":"We present a system for textual inference ( the task of inferring whether a sentence follows from another text ) that uses learning and a logical - formula semantic representation of the text .Our approach can be viewed as combining statistical machine learning and classical logical reasoning , in the hope of marrying the robustness and scalability of learning with the preciseness and elegance of logical theorem proving .","label":"Uses","metadata":{},"score":"50.621918"}
{"text":"Shipp MA , et al . :Diffuse large B - cell lymphoma outcome prediction by gene expression profiling and supervised machine learning .Nature Medicine 2002 , 8 : 68 - 74 .View Article PubMed .Savage KJ , et al .","label":"Uses","metadata":{},"score":"50.694427"}
{"text":"That is , .[ 4 ] .With a multinomial event model , samples ( feature vectors ) represent the frequencies with which certain events have been generated by a multinomial where is the probability that event i occurs ( or K such multinomials in the multiclass case ) .","label":"Uses","metadata":{},"score":"50.7325"}
{"text":"Recent approaches to Word Sense Disambiguation ( WSD ) generally fall into two classes : ( 1 ) information - intensive approaches and ( 2 ) information - poor approaches .Our hypothesis is that for memory - based learning ( MBL ) , a reduced amount of data is more beneficial than the full range of features used in ... \" .","label":"Uses","metadata":{},"score":"50.78769"}
{"text":"With the same sample size change , the error rate for PCA dropped from 43.00 % to only 28.53 % .Results for BPD are shown in Table 5 , which contains the results from three different multi - class SVM algorithms .","label":"Uses","metadata":{},"score":"50.859104"}
{"text":"But some reported that certain classification algorithms were better than others .It is still an unclear issue , probably due to the interaction of different combinations of issues .The comparison between different classifiers should be a carefully controlled experiment .","label":"Uses","metadata":{},"score":"50.867363"}
{"text":"We prompt a new algorithm that , again under the idealization of performing search exactly , has sample complexity ( and error ) that grows logarithmically in the number of \" irrelevant \" features and search heuristics are again seen to be directly trying to reach this bound .","label":"Uses","metadata":{},"score":"50.883766"}
{"text":"We also demonstrate on a commercial wheeled rover that our Kalman filter 's learned noise covariance parameters - obtained quickly and fully automatically - significantly outperform an earlier , carefully and laboriously hand - designed one .Pieter Abbeel , Adam Coates , Mike Montemerlo , Andrew Y. Ng and Sebastian Thrun in Proceedings of Robotics : Science and Systems , 2005 .","label":"Uses","metadata":{},"score":"50.94464"}
{"text":"Sham Kakade and Andrew Y. Ng in NIPS 17 2005 .We present a new algorithm for automatically learning hypernym ( is - a ) relations from text .Given a training set of text containing known hypernym pairs , our algorithm automatically extracts useful dependency paths and applies them to new corpora to identify novel pairs .","label":"Uses","metadata":{},"score":"50.973717"}
{"text":"Berman H , Westbrook J , Feng Z , Gilliland G , Bhat T , Weissig H , Shindyalov I , Bourne P : The Protein Data Bank .Nucleic Acid Res 2000 , 28:235 - 242 .Allers J , Shamoo Y : Structure - based analysis of protein - RNA interactions using the program ENTANGLE .","label":"Uses","metadata":{},"score":"51.00067"}
{"text":"J. Zico Kolter , Youngjun Kim and Andrew Y. Ng in ICRA 2009 .We discuss how high - resolution depth information can be combined with visual imagery to improve the performance of object detection beyond what is ( currently ) achievable with 2D images alone , and we present door - opening and inventory - taking experiments .","label":"Uses","metadata":{},"score":"51.111496"}
{"text":"Other confounding issues of WSD .Other issues in addition to sample size , distribution of senses , and difficulty of the task also affect the performance and subsequent assessment of WSD classifiers , as noted below : .As often discussed in various papers , different features were evaluated to see their contribution to classifier performance [ 10 , 20 , 29 ] .","label":"Uses","metadata":{},"score":"51.113876"}
{"text":"Note that the PR curves of the ensembles are closer to those of the baseline classifiers .Discussion Reliable methods for identifying putative functional sites in protein sequences is an important problem with broad applications in computational biology , e.g. , rational drug design .","label":"Uses","metadata":{},"score":"51.186104"}
{"text":"Initially , a framework for de ning the theoretically optimal , but computationally intractable , method for feature subset selection is presented .We show that our goal should be to eliminate a feature if it g ... \" .In this paper , we examine a method for feature subset selection based on Information Theory .","label":"Uses","metadata":{},"score":"51.20268"}
{"text":"Our heuristic significantly reduces the scheduling delay in the execution of experts when re - execution of the task - compression algorithm is not needed from O(N2 ) time , where N denotes the number of experts , to O(N ) time .","label":"Uses","metadata":{},"score":"51.23043"}
{"text":"Ashutosh Saxena , Min Sun , and Andrew Y. Ng in ICCV 2007 .We use a Markov Random Field ( MRF ) to infer a set of \" plane parameters \" that capture both the 3-d location and 3-d orientation of the patch .","label":"Uses","metadata":{},"score":"51.267517"}
{"text":"381 - 402 , North Holland Publishing , 1980 . \" ...In the feature subset selection problem , a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention , while ignoring the rest .","label":"Uses","metadata":{},"score":"51.282547"}
{"text":"Naive Bayes is a simple technique for constructing classifiers : models that assign class labels to problem instances , represented as vectors of feature values , where the class labels are drawn from some finite set .It is not a single algorithm for training such classifiers , but a family of algorithms based on a common principle : all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature , given the class variable .","label":"Uses","metadata":{},"score":"51.403923"}
{"text":"It applies these N(N-1)/2 SVM classifiers and the class assignment is determined by a voting strategy ( e.g. the class chose by the maximum number of SVM classifiers wins ) .The performance was measured using both a 5-fold and a 10-fold cross - validation method .","label":"Uses","metadata":{},"score":"51.41215"}
{"text":"Abstractly , naive Bayes is a conditional probability model : given a problem instance to be classified , represented by a vector representing some n features ( independent variables ) , it assigns to this instance probabilities .The problem with the above formulation is that if the number of features n is large or if a feature can take on a large number of values , then basing such a model on probability tables is infeasible .","label":"Uses","metadata":{},"score":"51.44736"}
{"text":"With any classifier , it is possible to tradeoff the Precision against Recall .Hence , it is more informative to compare the Pre- cision - Recall ( PR ) curves which show the tradeoff over their entire range of possible values than to compare the performance of the classifiers for a particular choice of the .","label":"Uses","metadata":{},"score":"51.526035"}
{"text":"The Blosum62 sub- stitution matrix was used for costs .The resulting entries in the matrix W are normalized and scaled so that each value is between 0 and 1 .The mixture of experts models consist of NB and LR at the leaves , respectively ( see Methods sec- tion for further details ) .","label":"Uses","metadata":{},"score":"51.579445"}
{"text":"Three machine learning algorithms were used : C4.5 ( a decision tree learner ) , IB1 ( an instance based learner ) , and naive Bayes .Experiments on artificial datasets showed that CFS quickly identifies and screens irrelevant , redundant , and noisy features , and identifies relevant features as long as their relevance does not strongly depend on other features .","label":"Uses","metadata":{},"score":"51.673973"}
{"text":"As illustrated in the tables , the mixture of experts models that capture global sequence similarity outperform the baseline models .Comparison of Precision - Recall curves for Naïve Bayes , mixture of Naïve Bayes and ensemble of Naïve Bayes mod- els on the non - redundant RNA - protein data set at 30 % identity cutoff .","label":"Uses","metadata":{},"score":"51.732204"}
{"text":"Consider the problem of classifying documents by their content , for example into spam and non - spam e - mails .Imagine that documents are drawn from a number of classes of documents which can be modelled as sets of words where the ( independent ) probability that the i - th word of a given document occurs in a document from class C can be written as .","label":"Uses","metadata":{},"score":"51.75137"}
{"text":"The simple Bayesian classifier ( SBC ) is commonly thought to assume that attributes are independent given the class , but this is apparently contradicted by the surprisingly good performance it exhibits in many domains that contain clear attribute dependences .No explanation for this has been proposed so far .","label":"Uses","metadata":{},"score":"51.81937"}
{"text":"Each class has an equal numberofunweighted rules .A new example is classified by applying all rules and assigning the example to the class with the most satisfied rules .The induction m ... \" .A lightweight rule induction method is described that generates compact Disjunctive Normal Form ( DNF ) rules .","label":"Uses","metadata":{},"score":"52.00061"}
{"text":"[ 2 ] It also finds application in automatic medical diagnosis .[ 3 ] .In the statistics and computer science literature , Naive Bayes models are known under a variety of names , including simple Bayes and independence Bayes .","label":"Uses","metadata":{},"score":"52.157448"}
{"text":"The latter problem is called sample selection bias in econometrics .Our solution to it is based on Nobel prize - winning work due to the economist James Heckman .We show that the methods we propose are s .. K + 98 ] , whosnd that performing no pruning and variants of pruning adapted to loss minimization both lead to similar performance .","label":"Uses","metadata":{},"score":"52.163223"}
{"text":"It was presented to show the degree of difficulty among different abbreviations .Results from 5-fold cross - validation showed no statistical difference with results from 10-fold cross - validation , which indicated 5-fold cross - validation might be used in evaluation in order to save computational power ( for a discussion of the relative merits of 5-fold cross - validation vs. 10-fold cross - validation , see [ 35 ] ) .","label":"Uses","metadata":{},"score":"52.2724"}
{"text":"We apply supervised learning to predict the value of the depthmap as a function of the image .Ashutosh Saxena , Sung H. Chung , and Andrew Y. Ng in IJCV 2007 .We present our vision - based system for grasping novel objects in clut- tered environments .","label":"Uses","metadata":{},"score":"52.30756"}
{"text":"Word sense disambiguation ( WSD ) is the ability to identify the meaning of words in context in a computational manner .WSD is considered an AI - complete problem , that is , a task whose solution is at least as hard as the most difficult problems in artificial intelligence .","label":"Uses","metadata":{},"score":"52.37925"}
{"text":"Hence , there is an urgent need for devel- opment of computational tools that can accurately anno- tate biomolecular data .Machine learning methods currently offer one of the most cost - effective approaches to construction of predictive models in applications where representative training data are available .","label":"Uses","metadata":{},"score":"52.57813"}
{"text":"Richard Socher , Brody Huval , Bharath Bhat , Christopher D. Manning , Andrew Y. Ng in NIPS 2012 .we develop an end - to - end system for detecting text from natural images .Using a K - means based unsupervised feature learning algorithm and multilayer neural network , we achieve state - of - the - art performance on the Streetview Text and ICDAR 2003 benchmarks .","label":"Uses","metadata":{},"score":"52.581844"}
{"text":", m , and m is the number of classifiers in the collection .Each individual classifier in the collection was trained on approximately instances , where l represents the total number of train- ing instances available to the ensemble .Performance evaluation To assess the performance of classifiers in this study , we report the following measures : Precision , Recall , Correla- tion Coefficient ( CC ) , and F - Measure ( FM ) .","label":"Uses","metadata":{},"score":"52.6325"}
{"text":"Liu H , Teller V , Friedman C : A multi - aspect comparison study of supervised word sense disambiguation .J Am Med Inform Assoc 2004 , 11 : 320 - 331 .View Article PubMed .Leroy G , Rindflesch TC : Effects of information and machine learning algorithms on word sense disambiguation with small datasets .","label":"Uses","metadata":{},"score":"52.739563"}
{"text":"During classification , given a test sequence xtest , we extract the local windows corresponding to its elements . , N in the hierarchical clus- tering root node that makes the final prediction .Page 13 .Logistic Regression Logistic Regression ( LR ) [ 19 ] is a supervised learning algo- rithm that belongs to the class of discriminative models .","label":"Uses","metadata":{},"score":"52.759933"}
{"text":"SIGIR'04 Workshop on Search and Discovery in BioInformatics .Hatzivassiloglou V , Duboue PA , Rzhetsky A : Disambiguating proteins , genes , and RNA in text : a machine learning approach .Bioinformatics 2001 , 17 ( Suppl 1 ) : S97 - 106 .","label":"Uses","metadata":{},"score":"52.77555"}
{"text":"Cheng - Tao Chu , Sang Kyun Kim , Yi - An Lin , YuanYuan Yu , Gary Bradski , Andrew Y. Ng and Kunle Olukotun in NIPS 19 2007 .We present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems : an L1-regularized least squares problem and an L2-constrained least squares problem , which result in a significant speedup .","label":"Uses","metadata":{},"score":"52.87859"}
{"text":"Then , a reinforcement learning algorithm was applied to automatically learn a controller for autonomous inverted hovering .Andrew Y. Ng , Adam Coates , Mark Diel , Varun Ganapathi , Jamie Schulte , Ben Tse , Eric Berger and Eric Liang in International Symposium on Experimental Robotics , 2004 .","label":"Uses","metadata":{},"score":"52.890755"}
{"text":"Given a set of sequences and a similarity measure defined on pairs of sequences , we learn a mixture of experts model by using spectral clustering to learn the hierarchical structure of the model and by using bayesian techniques to combine the predictions of the experts .","label":"Uses","metadata":{},"score":"52.94255"}
{"text":"Probability estimation trees ( PETs ) have the same attractive features as classification trees ( e.g. , c ... \" .Tree induction is one of the most effective and widely used methods for building classification models .However , many applications require cases to be ranked by the probability of class membership .","label":"Uses","metadata":{},"score":"52.954483"}
{"text":"As many available datasets will not have the desired annotation for any samples , this method extends the usability of the limited number of adequately annotated microarray datasets .Methods .Datasets .We conducted three experiments , each addressing a different cancer diagnostic problem : ALL / AML differential diagnosis , prediction of response to imatinib in CML , and prediction of outcome in DLBCL .","label":"Uses","metadata":{},"score":"52.97973"}
{"text":"[ 13 ] .This prior probability distribution might be based on our knowledge of frequencies in the larger population , or on frequency in the training set .However , given the sample the evidence is a constant and thus scales both posteriors equally .","label":"Uses","metadata":{},"score":"53.018173"}
{"text":"These papers are important in that they report on useful methods and provide insights and overall results .However , a deeper and more systematic analysis is needed in order to obtain a better understanding of the different factors affecting the performance of ML methods for WSD .","label":"Uses","metadata":{},"score":"53.033882"}
{"text":"Based on two scalable clustering algorithms , we find that our simple system can discover features sensitive to a commonly occurring object class ( human faces ) and can also combine these into detectors in - variant to significant global distortions .","label":"Uses","metadata":{},"score":"53.079094"}
{"text":"We consider the apprenticeship learning setting in which a teacher demonstration of the task is available .We show that , given the initial demonstration , no explicit exploration is necessary , and we can attain near - optimal performance ( compared to the teacher ) simply by repeatedly executing \" exploitation policies \" that try to maximize rewards .","label":"Uses","metadata":{},"score":"53.1802"}
{"text":"By changing the cost function , different ... \" .1 Introduction Recent papers [ 20 ] have shown that boosting , arcing , and related ensemble methods ( hereafter summarized asboosting ) can be viewed as margin maximization in function space .","label":"Uses","metadata":{},"score":"53.180206"}
{"text":"The gat- ing nodes combine the predictions of the expert classifiers based on an estimate of the cluster membership of a test protein sequence .The combination scheme Table 2 : Experimental results on the DNA - protein sequence data set .","label":"Uses","metadata":{},"score":"53.201736"}
{"text":"It is spam if ( i.e. , ) , otherwise it is not spam .^ Caruana , R. ; Niculescu - Mizil , A. ( 2006 ) .An empirical comparison of supervised learning algorithms .Proc . 23rdInternational Conference on Machine Learning .","label":"Uses","metadata":{},"score":"53.275215"}
{"text":"We propose an approach to grasping that estimates the stability of different grasps , given only noisy estimates of the shape of visible portions of an object , such as that obtained from a depth sensor .By combining this with a kinematic description of a robot arm and hand , our algorithm is able to compute a specific positioning of the robot 's fingers so as to grasp an object .","label":"Uses","metadata":{},"score":"53.28646"}
{"text":"These networks are factored representations of probability distributions that generalize the naive Bayesian classifier and explicitly represent statements about independence .Among these approaches we single out a method we call Tree Augmented Naive Bayes ( TAN ) , which outperforms naive Bayes , yet at the same time maintains the computational simplicity ( no search involved ) and robustness that characterize naive Bayes .","label":"Uses","metadata":{},"score":"53.295456"}
{"text":"We subsequently based our assessment of performance on error rates and associated standard errors .Our method also differs from related work because the sample size for each sense is always fixed , whereas in related work the sample size for the entire corpus is generally fixed but not the sample sizes of the senses .","label":"Uses","metadata":{},"score":"53.337517"}
{"text":"To understand the effects of increased sample size on the error rate , we stratified by the sense distribution and then tested the null hypothesis of no difference between the error rates obtained under the different sample sizes using the sign test .","label":"Uses","metadata":{},"score":"53.44481"}
{"text":"by Ron Kohavi - Proceedings of the European Conference on Machine Learning , 1995 .We evaluate the power of decision tables as a hypothesis space for supervised learning algorithms .Decision tables are one of the simplest hypothesis spaces possible , and usually they are easy to understand .","label":"Uses","metadata":{},"score":"53.47528"}
{"text":"This paper describes an algorithm that tracks and localizes a helicopter using a ground - based trinocular camera array .This system has successfully been integrated with an IMU to provide a positioning system for autonomous hovering .Masa Matsuoka , Surya Singh , Alan Chen , Adam Coates , Andrew Y. Ng and Sebastian Thrun in Proceedings of the Fifth International Conference on Field Service Robotics , 2005 .","label":"Uses","metadata":{},"score":"53.50524"}
{"text":"Jordan MI , Jacobs RA : Hierarchical mixtures of experts and the EM algorithm .Neural Computation 1994 , 6:181 - 214 .Dempster AP , Laird NM , Rubin DB : Maximum likelihood from incomplete data via the EM algorithm .","label":"Uses","metadata":{},"score":"53.589127"}
{"text":"The CVG improves the PCFG of the Stanford Parser by 3.8 % to obtain an F1 score of 90.4 % .John Bauer , Richard Socher , Christopher D. Manning , Andrew Y. Ng in ACL 2013 .We introduce a neural tensor network ( NTN ) model which predicts new relationship entries that can be added to the database .","label":"Uses","metadata":{},"score":"53.610374"}
{"text":"As shown in Figure 5 , PCA , which has two very close senses , had much higher error rates than BSA , which has two unrelated senses .Therefore , when comparing the performance of different WSD systems , data sets with the same degree of difficulty should be used .","label":"Uses","metadata":{},"score":"53.621395"}
{"text":".. nitial Model Construction This subsection presents models constructed using several standard machine learning techniques .We have obtained similar results with support vector machines [ 5 ] and AdaBoost with decision stumps [ 17 ] , but will not discuss them here for the sake of brevity .","label":"Uses","metadata":{},"score":"53.68917"}
{"text":"Andrew Y. Ng in ICML 2004 .Our algorithm is based on using \" inverse reinforcement learning \" to try to recover the unknown reward function .Pieter Abbeel and Andrew Y. Ng in ICML 2004 .We describe a successful application of reinforcement learning to designing a controller for sustained inverted flight on an autonomous helicopter .","label":"Uses","metadata":{},"score":"53.694614"}
{"text":"They reported an F - measure of over 0.7 for genes with sufficient number of known document references .Liu [ 29 ] investigated the effect of window size and claimed that biomedical ambiguous words needed a larger window size than general English ambiguous words .","label":"Uses","metadata":{},"score":"53.710686"}
{"text":"We introduce context - sensitive recursive neural network ( CRNN ) architecture for jointly parsing natural language and learning vector space representations for variable - sized inputs .By inducing distributed feature representations for unseen phrases and providing syntactic information to accurately predict phrase structure trees , we achieve an unlabeled bracketing F - measure of 92.1 % on the Wall Street Journal dataset for sentences up to length 15 .","label":"Uses","metadata":{},"score":"53.720478"}
{"text":"Conclusion .Several different independent aspects affect performance when using ML techniques for WSD .We found that combining them into one single result obscures understanding of the underlying methods .Although we studied only four abbreviations , we utilized a well - established statistical method that guarantees the results are likely to be generalizable for abbreviations with similar characteristics .","label":"Uses","metadata":{},"score":"53.723232"}
{"text":"Table 1 : Experimental results on the RNA - protein sequence data set .Experimental results with Naive Bayes ( NB ) and Logistic Regression ( LR ) models , and Mixture of Experts ( ME ) models on the non - redundant RNA - protein sequence data set , where the identity cutoffs are 30 % and 90 % .","label":"Uses","metadata":{},"score":"53.835617"}
{"text":"Rajat Raina , Yirong Shen , Andrew Y. Ng and Andrew McCallum in NIPS 16 2004 .We consider the policy search approach to reinforcement learning .We show that if a \" baseline distribution \" is given , then we can derive a policy search algorithm that terminates in a finite number of steps , and for which we can provide non - trivial performance guarantees .","label":"Uses","metadata":{},"score":"53.95066"}
{"text":"[ 1 ] trained Naïve Bayes classifiers to identify RNA - protein interface residues in a protein sequence .Yan et al .[ 2 ] developed a two - stage classifier to identify protein - pro- tein interaction sites .Qian and Sejnowski [ 3 ] trained Neu- ral Networks to predict protein secondary structure , i.e. , classifying each residue in a protein sequence into one of the three classes : helix ( H ) , strand ( E ) or coil ( C ) .","label":"Uses","metadata":{},"score":"54.02043"}
{"text":"The first is achieved by using approximate string searching , and the second by expanding the dictionary with a probabilistic variant generator , which we propose in this paper .Key words : protein name recognition , naive Bayes classifier , approximate string search , spelling variant generator 2 1 . by Gerard Escudero , Lluís Màrquez , German Rigau - IN PROC .","label":"Uses","metadata":{},"score":"54.024216"}
{"text":"The expert networks output class probabilities for each input x , while the gating networks learn how to combine the predictions of the experts up the tree with the final prediction output by the root .The parameters of the gat- ing networks are learned using Expectation Maximization algorithm [ 10].","label":"Uses","metadata":{},"score":"54.056168"}
{"text":"The original data set for PCA contained 6 different senses , but we only used the two that were very similar for our experiments .We used a simple \" full - form substitution \" method to automatically generate a data set for the experiments described in this paper , and this dataset was partitioned into training and testing sets .","label":"Uses","metadata":{},"score":"54.081917"}
{"text":"Pieter Abbeel , Adam Coates and Andrew Y. Ng in IJRR 2010 .We develop a method , based upon structured prediction , for discriminatively training sparse coding algorithms specifically to maximize disaggregation performance .We show that this significantly improves the performance of sparse coding algorithms on the energy task and illustrate how these disaggregation results can provide useful information about energy usage .","label":"Uses","metadata":{},"score":"54.271057"}
{"text":"Rajat Raina , Alexis Battle , Honglak Lee , Benjamin Packer and Andrew Y. Ng in ICML 2007 .e train a discriminative classifier over a wide variety of features derived from WordNet structure , corpus - based evidence , and evidence from other lexical resources .","label":"Uses","metadata":{},"score":"54.271217"}
{"text":"Quoc V. Le , Jiquan Ngiam , Adam Coates , Abhik Lahiri , Bobby Prochnow and Andrew Y. Ng in ICML 2011 .We investigate the reasons for the success of sparse coding over vector quantization ( VQ ) by decoupling the training and encoding phases , which separates out the contributions of each phase .","label":"Uses","metadata":{},"score":"54.358456"}
{"text":"Both authors read and approved the final manuscript .Authors ' Affiliations .Exagen Diagnostics , Inc. Houston .Department of Electrical and Computer Engineering , Texas A&M University .References .Schena M , Shalon D , Davis RW , Brown PO : Quantitative monitoring of gene expression patterns with a complementary DNA microarray .","label":"Uses","metadata":{},"score":"54.398445"}
{"text":"Merkel M , Andersson M : Combination of contextual features for word sense disambiguation .SENSEVAL-2 Workshop 123 - 127 .Bruce R , Wiebe J : Word sense disambiguation using decomposable models .Proceedings of the Thirty - second Annual Meeting of the Association of Computational Linguistics 139 - 146 .","label":"Uses","metadata":{},"score":"54.430233"}
{"text":"This figure shows the plots of \" error rate \" versus \" sample size \" for BSA , RSV and PCA data sets with fixed distribution of \" ( 0.5 , 0.5 ) \" using 5-fold cross validation .Discussion .","label":"Uses","metadata":{},"score":"54.543156"}
{"text":"For experiments aimed at answering a clinical question , such information might include patient disease stage , or response to a particular drug .The cost of producing adequately annotated datasets has been a barrier to the widespread application of microarray technology in medicine .","label":"Uses","metadata":{},"score":"54.551598"}
{"text":"Andrew Saxe , Pangwei Koh , Zhenghao Chen , Maneesh Bhand , Bipin Suresh and Andrew Y. Ng in ICML 2011 .We demonstrate cross modality feature learning , where better features for one modality ( e.g. , video ) can be learned if multiple modalities ( e.g. , audio and video ) are present at feature learning time .","label":"Uses","metadata":{},"score":"54.56288"}
{"text":"We also describe an incremental method for performing crossvalidation that is applicable to incremental learning algorithms including IDTM .Using incremental cross - validation , it is possible to cross - validate a given dataset and IDTM in time that is linear in the number of instances , the number of features , and the number of label values .","label":"Uses","metadata":{},"score":"54.720367"}
{"text":"Page 11 .of We first compute the pairwise similarity matrix Wn × n for the protein sequences in the training set based on a com- mon global sequence alignment method .Second , using this similarity matrix , we apply 2-way spectral clustering algorithm , described in the next subsection , to recursively bipartition the training set of protein sequences until a splitting criterion is met .","label":"Uses","metadata":{},"score":"54.780807"}
{"text":"With any classifier , it is possible to tradeoff the Precision against Recall .Hence , it is more informative to compare the Precision - Recall curves which show the tradeoff over their entire range of possible values than to compare the performance of the classifiers for a particular choice of the tradeoff .","label":"Uses","metadata":{},"score":"54.851185"}
{"text":"2-Way spectral clustering Spectral clustering has been successfully applied in many applications , including image segmentation [ 15 ] , docu- ment clustering [ 16 ] , grouping related proteins according to their structural SCOP classification [ 17].","label":"Uses","metadata":{},"score":"54.91855"}
{"text":"Instead it predicts , directly as a function of the images , a point at which to grasp the object .Our algorithm is trained via supervised learning , using synthetic images for the training set .Ashutosh Saxena , Justin Driemeyer , Justin Kearns and Andrew Y. Ng in NIPS 19 2007 .","label":"Uses","metadata":{},"score":"54.927628"}
{"text":"The results of our experiments show that global sequence similarity can be exploited to improve the performance of classifiers trained to label biomolecular sequence data .Conclusion : The mixture of experts model helps improve the performance of machine learning methods for identifying functionally important sites in biomolecular sequences .","label":"Uses","metadata":{},"score":"55.01203"}
{"text":"Page 4 .Hence , we conclude that global similarity is helpful in improving the perform- ance of classifiers trained to label biomolecular sequence data .An ensemble of classifiers [ 7,8 ] is simply a collection of classifiers , each trained on a bal- anced subsample of the training data .","label":"Uses","metadata":{},"score":"55.02791"}
{"text":"Aronson AR : Effective mapping of biomedical text to the UMLS Metathesaurus : the MetaMap program .Proc AMIA Symp 2001 , 17 - 21 : 17 - 21 .Weeber M , Klein H , Aronson AR , Mork JG , de Jong - van den Berg LT , Vos R : Text - based discovery in biomedicine : the architecture of the DAD - system .","label":"Uses","metadata":{},"score":"55.03273"}
{"text":"The discussion so far has derived the independent feature model , that is , the naive Bayes probability model .The naive Bayes classifier combines this model with a decision rule .One common rule is to pick the hypothesis that is most probable ; this is known as the maximum a posteriori or MAP decision rule .","label":"Uses","metadata":{},"score":"55.06626"}
{"text":"We show that our goal should be to eliminate a feature if it gives us little or no additional information beyond that subsumed by the remaining features .In particular , this will be the case for both irrelevant and redundant features .","label":"Uses","metadata":{},"score":"55.087234"}
{"text":"Proc EMNLP 2002 , 41 - 48 .Mohammad S , Pedersen T : Combining lexical and syntactic features for supervised word sense disambiguation .Proc of the CoNLL .Wilks Y , Fass D , Guo C , MacDonald J , Plate T , Slator B : Providing Machine Tractable Dictionary Tools Cambridge , MA : MIT Press 1990 .","label":"Uses","metadata":{},"score":"55.12357"}
{"text":"When comparing the performance of WSD classifiers , those metrics are critical because they indicate whether or not an improvement is statistically significant ; if there is a large deviation , there may not actually be an improvement even though one error rate is smaller than the other .","label":"Uses","metadata":{},"score":"55.195473"}
{"text":"Effects of \" sense distribution \" have been addressed in other papers [ 30 , 37 ] because it is believed that the performance of a WSD classifier may change if the distribution of the different senses is unbalanced .For example , when there is a majority sense for an ambiguous word , the improvement of a WSD classifier is believed to be very small .","label":"Uses","metadata":{},"score":"55.208504"}
{"text":"Then the probability that a given document D contains all of the words , given a class C , is .( This technique of \" log - likelihood ratios \" is a common technique in statistics .In the case of two mutually exclusive alternatives ( such as this example ) , the conversion of a log - likelihood ratio to a probability takes the form of a sigmoid curve : see logit for details . )","label":"Uses","metadata":{},"score":"55.233574"}
{"text":"[ 2 ] .In the multivariate Bernoulli event model , features are independent booleans ( binary variables ) describing inputs .Like the multinomial model , this model is popular for document classification tasks , [ 9 ] where binary term occurrence features are used rather than term frequencies .","label":"Uses","metadata":{},"score":"55.254234"}
{"text":"Einat Minkov , William Cohen and Andrew Y. Ng in ACM SIGIR 2006 .We present a lightweight interaction model for group scheduling ( GS ) that can extend its reach beyond users of current group calendaring solutions .By expressing availability in terms of preferences , we create a flexible framework for GS that preserves plausible deniability while exerting social pressure to encourage honesty among users .","label":"Uses","metadata":{},"score":"55.28161"}
{"text":"Honglak Lee , Ekanadham Chaitanya , and Andrew Y. Ng in NIPS 2007 .We apply the hierarchical apprenticeship learning algorithm to the task of quadruped locomotion over extreme terrain , and achieve , to the best of our knowledge , results superior to any previously published work .","label":"Uses","metadata":{},"score":"55.32044"}
{"text":"The data classi cation is handled by several support vector machine classi ers arranged in two layers .This architecture is known as Adaptive Combination of Classi ers ( ACC ) .The system performs very well and is capable of detecting people even when all components of a person are not found .","label":"Uses","metadata":{},"score":"55.380074"}
{"text":"Duda R , Hart E , Stork D : Pattern Classification Second edition .Wiley ; 2001 .Shi J , Malik J : Normalized cuts and image segmentation .Pattern Analysis and Machine Intelligence 2000 , 22(8):888 - 905 .Dhillon IS : Co - clustering documents and words using bipartite spectral graph partitioning .","label":"Uses","metadata":{},"score":"55.58085"}
{"text":"This is the event model typically used for document classification , with events representing the occurrence of a word in a single document ( see bag of words assumption ) .The likelihood of observing a histogram x is given by .","label":"Uses","metadata":{},"score":"55.599686"}
{"text":"Dietterich TG : Approximate statistical tests for comparing supervised classification learning algorithms .Neural Computation 1998 , 10 : 1895 - 1924 .View Article PubMed .Salzberg SL : On comparing classifiers : Pitfalls to avoid and a recommended approach .","label":"Uses","metadata":{},"score":"55.699017"}
{"text":"We present an efficient algorithm for approximately minimizing the pre- diction error over long time scales .We present empirical results on two different helicopters .Pieter Abbeel , Varun Ganapathi and Andrew Y. Ng in NIPS 18 2006 .Then , we apply supervised learning to predict the depthmap as a function of the image .","label":"Uses","metadata":{},"score":"55.70354"}
{"text":"In this work we sought to improve the performance of classifiers that make predictions on residues in protein sequences by taking into account the global similarity between the protein sequences in the data set in addition to the local features extracted around each residue .","label":"Uses","metadata":{},"score":"55.71219"}
{"text":"Quoc V. Le , Marc'Aurelio Ranzato , Rajat Monga , Matthieu Devin , Kai Chen , Greg S. Corrado , Jeffrey Dean and Andrew Y. Ng in ICML 2012 .This paper presents a method for designing semi - supervised classifiers trained on labeled and unlabeled samples .","label":"Uses","metadata":{},"score":"55.76673"}
{"text":"The results of our experiments show that global sequence similarity can be exploited to improve the performance of classifiers trained to label biomolecular sequence data .The mixture of experts model helps improve the performance of machine learning methods for identifying functionally important sites in biomolecular sequences .","label":"Uses","metadata":{},"score":"55.83571"}
{"text":"For example , progress on sequenc- ing technologies has resulted in the release of hundreds of complete genome sequences .With the exponentially from IEEE International Conference on Bioinformatics and Biomedicine ( BIBM ) 2008 Philadelphia , PA , USA .Page 2 .","label":"Uses","metadata":{},"score":"55.843395"}
{"text":"All the words in the title and abstract of the articles were used as features for machine learning and an SVM algorithm was used to generate a classifier .We used a package called \" Spider \" [ 46 ] to perform all the SVM training and testing .","label":"Uses","metadata":{},"score":"55.86995"}
{"text":"Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days , and we show that it can scale to networks with over 11 billion parameters using just 16 machines .Adam Coates , Brody Huval , Tao Wang , David J. Wu , Bryan Catanzaro and Andrew Y. Ng in ICML 2013 .","label":"Uses","metadata":{},"score":"56.068535"}
{"text":"We applied our features to four datasets and observe a consistent improvement of 4 % to 5 % in classification accuracy , and achieve state - of - the - art recognition accuracy .Will Y. Zou , Shenghuo Zhu , Andrew Y. Ng , Kai Yu in NIPS 2012 .","label":"Uses","metadata":{},"score":"56.13594"}
{"text":"This figure shows the plots of ' ' error rate ' ' versus ' ' sample size ' ' with different sense distributions of PCA data set ( case where the 2 ambiguous senses are very similar ) using 5-fold cross validation .","label":"Uses","metadata":{},"score":"56.31359"}
{"text":"View Article .Weston J , Watkins C : Multiclass support vector machines .Proceedings of ESANN99 .Copyright .© Xu et al .2006 .This article is published under license to BioMed Central Ltd.CSCI - GA.2590 - Natural Language Processing - Spring 2013 Prof. Grishman .","label":"Uses","metadata":{},"score":"56.44924"}
{"text":"In this study , we used 4 abbreviations from Liu 's abbreviation list .However , we used a different method to collect the datasets because we wanted to control the sample sizes of the senses for our experiments .Leroy [ 30 ] tried to reduce the training sample size by supplying external knowledge from the UMLS for supervised machine learning algorithms , but the results were not promising .","label":"Uses","metadata":{},"score":"56.513565"}
{"text":"We focus on two key issues : the problem of selecting relevant features , and the problem of selecting relevant examples .We describe the advances that have been mad ... \" .In this survey , we review work in machine learning on methods for handling data sets containing large amounts of irrelevant information .","label":"Uses","metadata":{},"score":"56.675323"}
{"text":"..he first preposition found before the ambiguous word Table 1 : The complete list of features used in the experiments of this approach to the fact that it did not use feature weighting .After they introduced feature weighting ... . by Gerard Escudero , Lluis Marquez , German Rigau , Jordi Girona Salgado , 2000 . \" ...","label":"Uses","metadata":{},"score":"56.755318"}
{"text":"Ted Kremenek , Paul Twohey , Godmar Back , Andrew Y. Ng and Dawson Engler in OSDI 2006 .Focusing on logistic regression , we present an algorithm for automatically constructing a multivariate Gaussian prior with a full covariance matrix for a given supervised learning task .","label":"Uses","metadata":{},"score":"56.78245"}
{"text":"Danqi Chen , Richard Socher , Christopher D. Manning , Andrew Y. Ng in ICLR 2013 .We compare three well - known model selection algorithms .We attempt to identify their relative and absolute strengths and weaknesses , and we provide some general methods for analyzing the behavior and performance of model selection algorithms .","label":"Uses","metadata":{},"score":"56.89096"}
{"text":"For each abbreviation , we measured error rates of the SVM classifier under different combinations of sample size , sense distribution , cross validation scheme ( 5-fold vs. 10-fold ) , and multi - class SVM algorithms ( for BPD only , which has 3 different senses ) .","label":"Uses","metadata":{},"score":"56.924023"}
{"text":"We evaluate our approach to learning a mixture of experts model on two biomolecular sequence labeling tasks : RNA- and DNA - protein interface predic- tion tasks .Results The main result of our study is that taking into account global sequence similarity through the means of a mixture of experts model can improve the performance of the clas- sifiers trained to label biomolecular sequence data .","label":"Uses","metadata":{},"score":"56.925697"}
{"text":"J. Zico Kolter , Christian Plagemann , David T. Jackson , Andrew Y. Ng and Sebastian Thrun in ICRA 2010 .Adam Coates and Andrew Y. Ng in ICRA 2010 .We extend the method of machine learning to learn one grasp point in an image and a point cloud to accommodate grasps with multiple contacts and also use a method that learns the ranking between candidates .","label":"Uses","metadata":{},"score":"56.94142"}
{"text":"For each sense , we recorded all the retrieved PMIDs , randomly selected 250 , and then obtained the corresponding abstracts to form a data pool , from which all the experiments were drawn .Feature vector and machine - learning algorithm .","label":"Uses","metadata":{},"score":"57.072857"}
{"text":"It is useful for testing whether one random variable in a pair tends to have larger ( smaller or simply different ) values than the other random variable in the pair .In our case , the random variables in the pair are the error rates obtained under the different sample sizes used .","label":"Uses","metadata":{},"score":"57.084366"}
{"text":"We also present empirical results for our approach on a small discrete problem , and on a complex continuous state / continuous action problem involving learning to ride a bicycle .Andrew Y. Ng and Michael Jordan in Uncertainty in Artificial Intelligence , Proceedings of the Sixteenth Conference 2000 .","label":"Uses","metadata":{},"score":"57.099693"}
{"text":"Identifying similar words .Distance metric for Wordnet [ J&M 20.6 ] .Similarity metric from corpora [ J&M 20.7 ] .See the Thesaurus demo by Patrick Pantel .By applying clustering methods we have an unsupervised way of creating semantic word classes .","label":"Uses","metadata":{},"score":"57.29025"}
{"text":"Quoc V. Le , Jiquan Ngiam , Zhenghao Chen , Daniel Chia , Pangwei Koh and Andrew Y. Ng in NIPS 2010 .We introduce a model which learns semantically focused word vectors using a probabilistic model of documents .We evaluate the model 's word vectors in two tasks of sentiment analysis .","label":"Uses","metadata":{},"score":"57.37732"}
{"text":", m , the task is to learn a classifier that can predict the labels for each element of a new input sequence , xtest .There is a large body of work on learning predictive mod- els to label biomolecular sequence data .","label":"Uses","metadata":{},"score":"57.381138"}
{"text":"Rion Snow , Dan Jurafsky and Andrew Y. Ng in NIPS 17 2005 .We propose a method for \" monitoring \" the controllers suggested by the learning algorithm online , and rejecting controllers leading to instability .We prove that even if an arbitrary online learning method is used with our algorithm to control a linear dynamical system , the resulting system is stable .","label":"Uses","metadata":{},"score":"57.385124"}
{"text":"A new example is classified by applying all rules and assigning the example to the class with the most satisfied rules .The induction method attempts to minimize the training error with no pruning .An overall design is specified by setting limits on the size and number of rules .","label":"Uses","metadata":{},"score":"57.401627"}
{"text":": Semi - supervised analysis of gene expression profiles for lineage - specific development in the Caenorhabditis elegans embryo .Bioinformatics 2006 , 22 ( 14 ) : e417 - 423 .View Article PubMed .Armstrong SA , et al .","label":"Uses","metadata":{},"score":"57.49101"}
{"text":"The induction method is nearly linear in time relative to an increase in the number of induced rules or the number of cases .Experimental results on large benchmark data sets demonstrate that predictive performance can rival the best reported results in the literature . by Kai Ming Ting - In : Proceedings of the Eleventh European Conference on Machine Learning . \" ...","label":"Uses","metadata":{},"score":"57.538494"}
{"text":"The hierarchical structure of the mixture of experts model is constructed based on global sequence similarity .Page 6 .The prediction of the ensemble of classifiers is computed from the predictions of the indi- vidual classifiers using majority voting .An example is misclassified by the ensemble if a majority of the classifi- ers misclassifies it .","label":"Uses","metadata":{},"score":"57.570297"}
{"text":"Liu H , Johnson SB , Friedman C : Automatic resolution of ambiguous terms based on machine learning and conceptual relations in the UMLS .J Am Med Inform Assoc 2002 , 9 : 621 - 636 .View Article PubMed .","label":"Uses","metadata":{},"score":"57.61299"}
{"text":"Ginter F , Boberg J , Salakoski T , Salakoski T : New Techniques for Disambiguation in Natural Language and Their Application to Biological Text .Journal of Machine Learning Research 2004 , 5 : 605 - 621 .Podowski RM , Cleary JG , Goncharoff NT , Amoutzias G , Hayes WS : AZuRE , a scalable system for automated term disambiguation of gene and protein names .","label":"Uses","metadata":{},"score":"57.623108"}
{"text":"Richard Socher , Cliff Lin , Andrew Y. Ng and Christopher Manning in ICML 2011 .We apply large - scale algorithms for learning the features automatically from unlabeled data and show that they allow us to construct highly effective classifiers for both detection and recognition to be used in a high accuracy end - to - end system .","label":"Uses","metadata":{},"score":"57.646736"}
{"text":"We focus on the problem of automatic 3d reconstruction of indoor scenes , specifically ones ( sometimes called \" Manhattan worlds \" ) that consist mainly of orthogonal planes .We use a Markov random field ( MRF ) model to identify the different planes and edges in the scene , as well as their orientations .","label":"Uses","metadata":{},"score":"57.673706"}
{"text":"..e may infer that TAN should perform rather well in comparison to C4.5 .The latter approach searches for the subset of attributes over which naive Bayes has the best performance .The results , displayed in Figures 5 and 6 and in Table 2 , show that T .. \" ...","label":"Uses","metadata":{},"score":"57.698578"}
{"text":"This paper reviews these methods and explains why ensembles can often perform better than any single classifier .Some previous studies comparing ensemble methods are reviewed , and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly . \" ...","label":"Uses","metadata":{},"score":"57.713394"}
{"text":"We present a novel grasp selection algorithm to enable a robot with a two - fingered end - effector to autonomously grasp unknown objects .The robot must identify how to grasp an object , locate the barcode on the object , and read the numeric code .","label":"Uses","metadata":{},"score":"57.7144"}
{"text":"Markatou M , Tian H , Biswas S , Hripcsak G : Analysis of variance of cross - validation estimators of the generalization error .Journal of Machine Learning Research 2005 , 6 : 1127 - 1168 .Resnik P , Yarowsky D : Distinguishing systems and distinguishing senses : New evaluation tools for words sense disambiguation .","label":"Uses","metadata":{},"score":"57.71891"}
{"text":"An additional issue is that this study limited the disambiguation of gene symbols to gene senses and one other category called \" non - gene sense \" , but the actual sense in this category was not resolved .This could be critical for NLP systems accessing phenotypic or disease - related information .","label":"Uses","metadata":{},"score":"57.889877"}
{"text":"View Article PubMed .Friedman M : The use of ranks to avoid the assumption of normality implicit in the analysis of variance .Journal of the American Statistical Association 1937 , 32 : 675 - 701 .View Article .Rifkin R , Klatau A : In defense of one - vs - all classification .","label":"Uses","metadata":{},"score":"57.94255"}
{"text":"Yan C , Dobbs D , Honavar V : A Two - Stage Classifier for Identi- fication of Protein - Protein Interface Residues .Bioinformatics 2004 , 20(Suppl 1):i371-i378 .Qian N , Sejnowski T : Predicting the secondary structure of globular proteins using neural networks models .","label":"Uses","metadata":{},"score":"57.981766"}
{"text":"If each ambiguous gene symbol in an article were accompanied by its corresponding long form , the disambiguation task would be much easier .Schijvenaars [ 13 ] showed that 33 % of the human genes in their thesaurus were affected by homonymy .","label":"Uses","metadata":{},"score":"58.082645"}
{"text":"We describe an object detection system that is designed to scale gracefully to large data sets and leverages upward trends in computational power and memory .We show that our GPU - based detector is up to 90 times faster than a well - optimized software version and can be easily trained on millions of examples .","label":"Uses","metadata":{},"score":"58.088036"}
{"text":"Andrew Y. Ng and Michael Jordan in NIPS 14 2002 .We present an algorithm that , given examples of similar ( and , if desired , dissimilar ) pairs of points in , learns a distance metric over that respects these relationships .","label":"Uses","metadata":{},"score":"58.136627"}
{"text":"Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain .We study the strengths and weaknesses of the wrapper approach andshow a series of improved designs .We compare the wrapper approach to induction without feature subset selection and to Relief , a filter approach to feature subset selection .","label":"Uses","metadata":{},"score":"58.227127"}
{"text":"This figure shows the plots of \" error rate \" versus \" sample size \" with different sense distributions of BPD data set ( where there are 3 ambiguous senses that are different ) using 5-fold cross validation and \" one - vs - rest \" algorithm .","label":"Uses","metadata":{},"score":"58.264984"}
{"text":"We demonstrate its portability on two applications : ( 1 ) manipulating a box and ( 2 ) grasping a door handle .Anya Petrovskaya , Oussama Khatib , Sebastian Thrun , and Andrew Y. Ng in ICRA 2006 .We describe a successful application of reinforcement learning to the problem of negotiating obstacles with a quadruped robot .","label":"Uses","metadata":{},"score":"58.36298"}
{"text":"In the feature subset selection problem , a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention , while ignoring the rest .To achieve the best possible performance with a particular learning algorithm on a particular training set , a feature subset selection method should consider how the algorithm and the training set interact .","label":"Uses","metadata":{},"score":"58.449234"}
{"text":"For the details of the ambiguity study , please refer to the sub - section \" Gene Ambiguity for mining MEDLINE \" in the Methods section .Research in automated WSD can be traced back to the 1950s [ 15 ] .","label":"Uses","metadata":{},"score":"58.52354"}
{"text":"Comparison of F - Measure for Naïve Bayes and mixture of Naïve Bayes models on the DNA - protein data set Figure 8 Comparison of F - Measure for Naïve Bayes and mixture of Naïve Bayes models on the DNA - protein data set .","label":"Uses","metadata":{},"score":"58.578766"}
{"text":"This article has been published as part of BMC Bioinformatics Volume 10 Sup- plement 4 , 2009 : Proceedings of the IEEE International Conference on Bio- informatics and Biomedicine ( BIBM ) 2008 .References 1 .Terribilini M , Lee JH , Yan C , Jernigan RL , Honavar V , Dobbs D : Pre- dicting RNA - binding Sites from Amino Acid Sequence .","label":"Uses","metadata":{},"score":"58.62367"}
{"text":"We consider extended similarity metrics for documents and other objects embedded in graphs , facilitated via a lazy graph walk .We provide a detailed instantiation of this framework for email data , where content , social networks and a timeline are integrated in a structural graph .","label":"Uses","metadata":{},"score":"58.655445"}
{"text":"By replacing hand - designed features with our learned features , we achieve classification results superior to all previous published results on the multiple datasets , and benefit from ease and efficiency of training and prediction .Quoc V. Le , Will Zou , Serena Yeung and Andrew Y. Ng in CVPR 2011 .","label":"Uses","metadata":{},"score":"58.66362"}
{"text":"The system achieved an accuracy rate of 92.7 % on an automatically generated testing set .Schijvenaars 's study described an effective method for gene disambiguation , but the evaluation results were limited to certain conditions .The automatically generated testing set contained human genes symbols that appeared as long - form and short - form pairs ( e.g. prostate specific antigen ( PSA ) ) in articles , where at least 6 articles were determined to be associated with each gene sense .","label":"Uses","metadata":{},"score":"58.69102"}
{"text":"CH and NG were employees of Exagen Diagnostics during the course of this research and the preparation of this manuscript .Additionally , CH owns stock in Exagen Diagnostics .Authors ' contributions .CH devised and implemented the GLAD algorithm , and contributed to the final preparation of the manuscript .","label":"Uses","metadata":{},"score":"58.732803"}
{"text":"To estimate the parameters for a feature 's distribution , one must assume a distribution or generate nonparametric models for the features from the training set .[ 8 ] .The assumptions on distributions of features are called the event model of the Naive Bayes classifier .","label":"Uses","metadata":{},"score":"58.82496"}
{"text":"Meanwhile , we also recorded the percentage of ambiguous words that were removed from the ambiguous word - list for different thresholds .We removed words with frequencies higher than 10 % , 1 % , 0.1 % and 0.05 % from the two lists of the mouse organism .","label":"Uses","metadata":{},"score":"58.9273"}
{"text":"In many machine learning domains , misclassication costs are dierent for dierent examples , in the same way that class membership probabilities are exampledependent .In these domains , both costs and probabilities are unknown for test examples , so both cost estimators and probability estimators must be ... \" .","label":"Uses","metadata":{},"score":"58.94397"}
{"text":"The inferred specifications are highly accurate and with them we have discovered numerous bugs .Ted Kremenek , Andrew Y. Ng and Dawson Engler in IJCAI 2007 .We propose a unified approach that dynamically models the objects to be manipulated and localizes the robot at the same time .","label":"Uses","metadata":{},"score":"58.980995"}
{"text":"We develop a recursive neural network ( RNN ) that learns compositionsl vector representations for phrases and sentences of arbitrary syntactic type and length .In each parse tree node , a vector captures the meaning and a matrix captures how it changes the meaning of neighboring words or phrases .","label":"Uses","metadata":{},"score":"59.011078"}
{"text":"Against this background , we hypothesize that classifiers trained to label biomolecular sequence data can be improved by taking into account the global sequence sim- ilarity between the protein sequences in addition to the local features extracted around each site .The intuition behind this hypothesis is that the more similar two sequences are , the greater the likelihood that their func- tional sites have similar patterns .","label":"Uses","metadata":{},"score":"59.18714"}
{"text":"Yong ASM , et al .: Molecular profiling of CD34 + cells identifies low expression of CD7 , along with high expression of proteinase 3 or elastase , as predictors of longer survival in patients with CML .Blood 2006 , 107 : 205 - 12 .","label":"Uses","metadata":{},"score":"59.37187"}
{"text":"The framework that is described here for people is easily applied to other objects as well .The motivatio ... \" .In this paper we present a component based person detection system that is capable of detecting frontal , rear and near side views of people , and partially occluded persons in cluttered scenes .","label":"Uses","metadata":{},"score":"59.38621"}
{"text":"The methodology is easy to apply , it provides a principled way of studying the effects of the different factors on the error rate , and since it is based on a strong theoretical foundation , it guarantees that the results to apply to all abbreviations with similar characteristics .","label":"Uses","metadata":{},"score":"59.420166"}
{"text":"When resources are insufficient to run all experts in every cycle , it becomes necessary to execute the most responsible experts within each cycle .The problem of adaptively scheduling experts with dynamic responsibilities can be formulated as a succession of optimization problems .","label":"Uses","metadata":{},"score":"59.598667"}
{"text":"Rion Snow , Sushant Prakash , Dan Jurafsky and Andrew Y. Ng in EMNLP 2007 .We present an efficient algorithm for learning SISC bases based on iteratively solving two large convex optimization problems .We show that SISC 's learned high - level representations of speech and music provide useful features for classification tasks within those domains .","label":"Uses","metadata":{},"score":"59.619987"}
{"text":"We combine a number of ideas in the literature to derive a unified framework that jointly calibrates many sensors a large system .We show that this gives a simple method that is easily applicable to calibrating large systems . , and also reduces calibration error and requires less human time .","label":"Uses","metadata":{},"score":"59.74437"}
{"text":"The hierarchical structure of the mixture of experts model is constructed based on global sequence similarity .Page 5 .In Figures 1 , 2 , 3 , and 4 we show the PR curves for the mix- ture and the ensemble models on both RNA- and DNA- protein sequence data sets for 30 % identity cutoff .","label":"Uses","metadata":{},"score":"59.755196"}
{"text":"Reliable identification of such interaction sites from protein sequences has broad applications ranging from rational drug design to the analysis of metabolic and signal transduction networks .The RNA- and DNA - protein interface data sets consist of RNA- and DNA - binding protein sequences , respectively , extracted from structures in the Protein Data Bank ( PDB ) [ 11].","label":"Uses","metadata":{},"score":"59.762566"}
{"text":"However the observed decrease in error rate was more dramatic in the cases where the different senses were well separated .For example , in BSA , the error rate dropped to approximately 5 % when the sample size was 80 and the sense distributions were almost balanced , and it was approximately 8 % for other distributions with the same size .","label":"Uses","metadata":{},"score":"59.88511"}
{"text":"Table 3 : Number of sequences , as well as positive and negative instances used in our experiments for the RNA- and DNA - protein data sets .Number of sequences as well as number of positive ( + ) and negative ( - ) instances in the non - redundant RNA- and DNA - protein sequence data sets for 30 % , 60 % , and 90 % identity cutoffs .","label":"Uses","metadata":{},"score":"59.945187"}
{"text":"Boosting seems to be especially susceptible to noise [ 10].For instance , Freund and Schapire [ 16 ] tested AdaBoost on a set of UCI benchmark datasets [ 27 ] using C4.5 [ 29 ] asa weak learning algorithm , as well as an algorithm whichs4:1/0.27,4/0.17 5:0/0.26,5/0 ... . by Thomas G. Dietterich - MULTIPLE CLASSIFIER SYSTEMS , LBCS-1857 , 2000 . \" ...","label":"Uses","metadata":{},"score":"60.152946"}
{"text":"Shatkay H , Feldman R : Mining the biomedical literature in the genomic era : an overview .J Comput Biol 2003 , 10 : 821 - 855 .View Article PubMed .Fukuda K , Tamura A , Tsunoda T , Takagi T : Toward information extraction : identifying protein names from biological papers .","label":"Uses","metadata":{},"score":"60.185684"}
{"text":"To be able to identify whether there are significant differences in the error rates due to different sample sizes and sense distributions while controlling for the abbreviation used , we used Friedman 's procedure .Notice that if we stratify by the abbreviation , the mean error rates form a two - way table where the columns correspond to different sample sizes and the rows correspond to different sense distributions .","label":"Uses","metadata":{},"score":"60.224346"}
{"text":"1471 - 2105 - 7 - 334-S1.doc Additional File 1 : Supplementary material for gene ambiguity for mining MEDLINE ( DOC 925 KB ) .Authors ' contributions .HX carried out data collection , programming , experiments using SVM and drafted the manuscript .","label":"Uses","metadata":{},"score":"60.4218"}
{"text":"Conclusion .In this paper , we aimed to further an understanding of the different factors affecting the performance of ML techniques for WSD by systematically simulating a variety of situations where different sample size , sense distribution , degree of difficulty , and cross validation methods were used .","label":"Uses","metadata":{},"score":"60.499146"}
{"text":"Figures 1 , 2 and 3 show the error rate versus the sample size for each distribution of the BSA , PCA and RSV data sets with 5-fold cross - validation .As the figures indicate , the reduction of the error rate as a function of the sample size is more dramatic for BSA than for PCA .","label":"Uses","metadata":{},"score":"60.638885"}
{"text":"Andrew Y. Ng and Michael Jordan in ICML 2001 .We extend the analysis of the Kleinberg HITS and the Google PageRank algorithms and show how it gives insight into ways of designing stable link analysis methods .This in turn motivates two new algorithms , whose performance we study empirically using citation data and web hyperlink data .","label":"Uses","metadata":{},"score":"60.70359"}
{"text":"Let K0 out of K sequences go to the left child node , and K1 out of K go to the right child node .We recursively place the sequence xi in all the nodes of with different weights , starting from the root , based on its estimated cluster membership computed above .","label":"Uses","metadata":{},"score":"60.709885"}
{"text":"Experimental determination of such sites lags far behind the number of known biomolecular sequences .Hence , there is a need to develop reliable computational methods for identifying functionally important sites from biomolecular sequences .Results : We present a mixture of experts approach to biomolecular sequence labeling that takes into account the global similarity between biomolecular sequences .","label":"Uses","metadata":{},"score":"60.88804"}
{"text":"We will also connect these results to other well - known algorithms to make clear when K - means can be most useful and convey intuitions about its behavior that are useful for debugging and engineering new systems .Adam Coates and Andrew Y. Ng in Neural Networks : Tricks of the Trade , Reloaded , Springer LNCS 2012 .","label":"Uses","metadata":{},"score":"61.07305"}
{"text":"For the test scenario considered in the paper , the average output error of a real - time MoE system due to the use of limited resources is less than 7 % .[ Show abstract ] [ Hide abstract ] ABSTRACT : Several experiments were conducted in order to investigate the usefulness of mixture of experts ( ME ) approach to an online internet system assisting in real estate appraisal .","label":"Uses","metadata":{},"score":"61.191216"}
{"text":"The experiments reveal , among other facts , that SemCor can be an acceptable ( 0.7 precision for polysemous words ) starting point for an all - words system .The results on the DSO corpus show that for some highly polysemous words 0.7 precision seems to be the current state - of - the - art limit .","label":"Uses","metadata":{},"score":"61.41578"}
{"text":"Chen L , Liu H , Friedman C : Gene name ambiguity of eukaryotic nomenclatures .Bioinformatics 2005 , 21 : 248 - 256 .View Article PubMed .Schuemie MJ , Weeber M , Schijvenaars BJA , van Mulligen EM , van der Eijk CC , Jelier R , et al .","label":"Uses","metadata":{},"score":"61.46296"}
{"text":"A novel heuristic is proposed to enable real - time execution rate adaptation in MoE systems with insufficient resources .In any given cycle , the heuristic uses sensitivity analysis to test whether one of two pre - computed schedules is the optimal solution of the optimization problem to avoid re - optimization when the test result is positive .","label":"Uses","metadata":{},"score":"61.608788"}
{"text":"ME - NB - local exploits the local sequence similarity to construct the hierarchical structure .ME - NB - random randomizes the global similarity matrix and constructs the hierarchical structure based on the randomized matrix .Page 7 .Hence , to learn the hierarchical structure of the model , we use hierarchical clustering of the sequences in the data set .","label":"Uses","metadata":{},"score":"61.65193"}
{"text":"Our algorithm successfully grasps a wide variety of objects , and is also used in unloading items from a dishwasher .Ashutosh Saxena , Justin Driemeyer , and Andrew Y. Ng in IJRR 2008 .We present a hierarchical control architecture that enables the \" LittleDog \" robot to walk over rough terrain .","label":"Uses","metadata":{},"score":"61.71109"}
{"text":"There are different types of WSD and some are more difficult than others .For example , if two senses are syntactically different , a reliable part of speech tagging method could be effective in resolving the ambiguity .For senses that correspond to the same syntactic category , the similarity of their semantic categories will affect the difficulty of the task ( i.e. the bovine serum albumin sense of BSA is substantially different from the body surface area sense ) .","label":"Uses","metadata":{},"score":"61.747936"}
{"text":"For example , when the total sample size is 20 and 5-fold cross validation is used the size of the training set is 16 , while if the sample size is 80 the size of the training set is 64 .","label":"Uses","metadata":{},"score":"61.856243"}
{"text":"For each combination of error rates we have a sample of 30 observations .To exemplify , assume the pair consisted of the error rates obtained under sample size 20 and 40 .Then the set of observations was comprised of those error rates obtained from the 30 simulation runs .","label":"Uses","metadata":{},"score":"61.916462"}
{"text":"IEEE 7th International Symposium on Bioinformatics 2007:320- 326 .Davis J , Goadrich M : The Relationship Between Precision-Recall and ROC Curves .Proceedings of the 23rdInternational Con- ference on Machine Learning 2006:233 - 240 .Baldi P , Brunak S , Chauvin Y , Andersen C , Nielsen H : Assessing the accuracy of prediction algorithms for classification : an over- view .","label":"Uses","metadata":{},"score":"61.923805"}
{"text":"Nature Genetics 2002 , 30 : 41 - 47 .View Article PubMed .Frank O , et al . :Gene expression signature of primary imatinib - resistant chronic myeloid leukemia patients .Leukemia 2006 , 20 : 1400 - 7 .","label":"Uses","metadata":{},"score":"61.96129"}
{"text":"A feature evaluation formula , based on ideas from test theory , provides an operational definition of this hypothesis .CFS ( Correlation based Feature Selection ) is an algorithm that couples this evaluation formula with an appropriate correlation measure and a heuristic search strategy .","label":"Uses","metadata":{},"score":"62.060112"}
{"text":"Error rates for each combination of sense distribution and sample size were averaged using 30 runs .Statistical methodology .To quantify the effects of sample size , sense distribution and difficulty of the task on the error rate , appropriate statistical methods were used .","label":"Uses","metadata":{},"score":"62.120415"}
{"text":"Tables 2 , 3 , 4 show the improvements in more detail .The addition of unlabeled samples increases the range from 70 % to 100 % .In CML experiments , adding unlabeled samples increases the minimum accuracy from 0 % to 11.11 % .","label":"Uses","metadata":{},"score":"62.228493"}
{"text":"Rajat Raina , Andrew Y. Ng and Daphne Koller in ICML 2006 .We present a hybrid algorithm that requires only an approximate model , and only a small number of real - life trials .The key idea is to successively \" ground \" the policy evaluations using real - life trials , but to rely on the approximate model to suggest local changes .","label":"Uses","metadata":{},"score":"62.313683"}
{"text":"We present a manipulator design combining accelerometer - based sensing with low - cost actuation , and conclude by demonstrating the utility of consumer - grade accelerometers even on high - precision manipulators .Morgan Quigley , Reuben Brewer , Sai P. Soundararaj , Vijay Pradeep , Quoc V. Le and Andrew Y. Ng in IROS 2010 .","label":"Uses","metadata":{},"score":"62.368652"}
{"text":"Hand DJ : Construction and assessment of classification rules Chichester , England : John Wiley & Sons 1997 .Hand DJ : Assessing Classification Rules .Journal of Applied Statistics 1994 , 21 : 3 - 16 .View Article .Fukunaga K , Hayes RR : Effect of sample size in classifier design .","label":"Uses","metadata":{},"score":"62.452133"}
{"text":"Declarations .Acknowledgements .We thank Exagen Diagnostics for support in conducting this research and presenting these results .This article has been published as part of BMC Genomics Volume 9 Supplement 2 , 2008 : IEEE 7 th International Conference on Bioinformatics and Bioengineering at Harvard Medical School .","label":"Uses","metadata":{},"score":"62.458145"}
{"text":"Conclusion .In this study we proposed a new technique for concurrently mining labeled and unlabeled datasets .This method supplements standard supervised learning with clustering of data lacking clinical annotation to estimate the predictive power of gene subsets .The performance of our algorithm was evaluated in comparison with supervised learning only on microarray data from three different cancer types .","label":"Uses","metadata":{},"score":"62.579266"}
{"text":"Recursively bipartition each subgraph obtained at Step 3 . if necessary .Hierarchical structure produced by spectral clustering on a data set of 147 protein sequences Figure 9 Hierarchical structure produced by spectral cluster- ing on a data set of 147 protein sequences .","label":"Uses","metadata":{},"score":"62.94169"}
{"text":"We test our robotic grasping system using STAIR platforms .Ashutosh Saxena , Lawson Wong , Morgan Quigley and Andrew Y. Ng in ISRR 2007 .We show how monocular image cues can be combined with triangulation cues to build a photo - realistic model of a scene given only a few images - even ones taken from very different viewpoints or with little overlap .","label":"Uses","metadata":{},"score":"63.196793"}
{"text":"Diettrich TG : Machine Learning for Sequential Data : A Review .Proceedings Joint IAPR International Workshop on Structural , Syntactic , and Statistical Pattern Recognition 2002:15 - 30 .Dietterich TG : Ensemble Methods in Machine Learning .Lecture Notes in Computer Science 2000 , 1857:1 - 15 .","label":"Uses","metadata":{},"score":"63.303146"}
{"text":"We show that by adding monocular cues to stereo ( triangulation ) ones , we obtain significantly more accurate depth estimates than is possible using either monocular or stereo cues alone .Ashutosh Saxena , Jamie Schulte and Andrew Y. Ng in IJCAI 2007 .","label":"Uses","metadata":{},"score":"63.59072"}
{"text":"However there is a concern that a very limited set of words may have accounted for the vast majority of ambiguity .Therefore , for each ambiguous word , we calculated its frequency , which is defined as the ratio between the number of abstracts containing the word and the total number of abstracts in the pool .","label":"Uses","metadata":{},"score":"63.63179"}
{"text":"ME - NB - global and ME - LR - global use NB and LR at the leaves and exploits the global sequence similarity to construct the hierarchical structure .ME - NB - local exploits the local sequence similarity to construct the hierarchical structure .","label":"Uses","metadata":{},"score":"63.758385"}
{"text":"We present the first successful autonomous completion on a real RC helicopter of the following four aerobatic maneuvers : forward flip and sideways roll at low speed , tail - in funnel , and nose - in funnel .Pieter Abbeel , Adam Coates , Morgan Quigley and Andrew Y. Ng in NIPS 19 2007 .","label":"Uses","metadata":{},"score":"63.941887"}
{"text":"The main variations are usually in the selection of features and choice of machine - learning algorithms .Experiments are usually performed on a fixed amount of documents ( i.e. 1,000 abstracts ) per an ambiguous word , where the entire set consists of all the senses , and the sense distribution is generally uneven .","label":"Uses","metadata":{},"score":"63.983067"}
{"text":"Dictionary - based protein name recognition is the first step for practical information extraction from biomedical documents because it provides ID information of recognized terms unlike machine learning based approaches .However , dictionary based approaches have two serious problems : ( 1 ) a lar ... \" .","label":"Uses","metadata":{},"score":"64.00906"}
{"text":"We present a hidden Markov model for deobfuscating spam emails .We empirically demonstrate that our model is robust to many types of obfuscation including misspellings , incorrect segmentations ( adding / removing spaces ) , and substitutions / insertions of non - alphabetic characters .","label":"Uses","metadata":{},"score":"64.16772"}
{"text":"Using Bayes ' theorem , the conditional probability can be decomposed as .In practice , there is interest only in the numerator of that fraction , because the denominator does not depend on and the values of the features are given , so that the denominator is effectively constant .","label":"Uses","metadata":{},"score":"64.183426"}
{"text":"However , dictionary based approaches have two serious problems : ( 1 ) a large number of false recognitions mainly caused by short names .( 2 ) low recall due to spelling variation . \" ...Dictionary - based protein name recognition is often a first step in extracting in - formation from biomedical documents because it can provide ID information on recognized terms .","label":"Uses","metadata":{},"score":"64.20412"}
{"text":"Inaccurate probabilities are partially the result of decision - tree induction algorithms that focus on maximizing classification accuracy and minimizing tree size ( for example via reduced - error pruning ) .Larger trees can be better for probability estimation , even if the extra size is superfluous for accuracy maximization .","label":"Uses","metadata":{},"score":"64.344"}
{"text":"Lexical Semantics [ J&M Chap 19 and 20 ] .Until now we have focussed primarily on syntactic issues : part of speech , names , noun and verb groups , and some larger structures .For our semantics -- our meaning - bearing elements -- we have relied on words .","label":"Uses","metadata":{},"score":"64.35614"}
{"text":"Publications .Each of these MOOCs offered learners the opportunity to watch online lectures , do machine-­‐graded homework , ... .Very few robot hands are available for purchase in the commercial market .In this paper , we present a hand designed for minimalistic dexterous manipulation , in which every stage of the design process also considered its manufacturing cost .","label":"Uses","metadata":{},"score":"64.37712"}
{"text":"Roger Grosse , Rajat Raina , Helen Kwong and Andrew Y. Ng in Proceedings of the Twenty - third Conference on Uncertainty in Artificial Intelligence 2007 .We propose a method that uses a ( possibly inaccurate ) simulator to identify a low - dimensional subspace of policies that spans the variations in model dynamics , and formulate an optimization problem that can be solved using the Reduced Rank Regression ( RRR ) algorithm .","label":"Uses","metadata":{},"score":"64.3877"}
{"text":"Quoc V. Le , Alex Karpenko , Jiquan Ngiam and Andrew Y. Ng in NIPS 2011 .This allows us to use even simple unsupervised training algorithms to train successful multi - layered networks .Adam Coates and Andrew Y. Ng in NIPS 2011 .","label":"Uses","metadata":{},"score":"64.53457"}
{"text":"We describe the advances that have been made on these topics in both empirical and theoretical work in machine learning , and we present a general framework that we use to compare different methods .We close with some challenges for future work in this area . by Daphne Koller , Mehran Sahami - In 13th International Conference on Machine Learning , 1995 . \" ...","label":"Uses","metadata":{},"score":"64.55144"}
{"text":"View Article PubMed .Humphrey SM , Rogers WJ , Kilicoglu H , Demner - Fushman D , Rindflesch TC : Word sense disambiguation by selecting the best semantic type based on Journal Descriptor Indexing : Preliminary experiment .Journal of the American Society for Information Science and Technology 2006 , 57 : 96 - 113 .","label":"Uses","metadata":{},"score":"64.593765"}
{"text":"The root node is the largest cluster containing all the protein sequences such that each node in the training set .Once a cluster is partitioned into its two subclusters , it becomes their parent in the resulting tree structure .We store all the intermediate clusters computed by the algorithm .","label":"Uses","metadata":{},"score":"64.68399"}
{"text":"Results for BSA data set .Annotation of the table : Dist : Distribution of senses ; S. Size : sample size ; Err .Rate : Error Rate ; SE : Standard Error of error rates ; CV : cross - validation ; .","label":"Uses","metadata":{},"score":"64.75608"}
{"text":"This is problematic because it will wipe out all information in the other probabilities when they are multiplied .Therefore , it is often desirable to incorporate a small - sample correction , called pseudocount , in all probability estimates such that no probability is ever set to be exactly zero .","label":"Uses","metadata":{},"score":"65.009155"}
{"text":"We present a novel approach to speaker identification in robot dialogue that allows a robot to recognize people during natural conversation and address them by name .Our STanford AI Robot ( STAIR ) dialogue system attempts to mirror the human speaker identification process .","label":"Uses","metadata":{},"score":"65.08429"}
{"text":"We then leverage these demonstrations to learn all of the necessary components for our control system .In particular , the demonstrations allow us to learn a model of the helicopter dynamics , as well as appropriate choices of target trajectories and reward parameters for input into a reinforcement learning or optimal control algorithm .","label":"Uses","metadata":{},"score":"65.2749"}
{"text":"No assumptions are made about the original distribution ( e.g. normal vs. other ) of the documents .Analysis of variance models are versatile statistical tools for studying the relation between error rates and sense distribution , sample size , and degree of difficulty of a task .","label":"Uses","metadata":{},"score":"65.27672"}
{"text":"For all other sense distributions , an increase in the sample size did not produce a significant reduction in the error rate - that is , there are no statistically significant differences between the error rates .We would like to stress here a limitation of the current study .","label":"Uses","metadata":{},"score":"65.33925"}
{"text":"When the senses are well separated , any increase in the sample size results in a statistically significant decrease of the error rate .This holds for all sense distributions and it is in agreement with the finding that for BSA there was no significant effect of the sense distributions on the error rates for the different sample sizes used .","label":"Uses","metadata":{},"score":"65.360016"}
{"text":"Unfortunately , decision trees have been found to provide poor probability estimates .Several techniques have been proposed to build more accurate PETs , but , to our knowledge , there has not been a systematic experimental analysis of which techniques actually improve the probability - based rankings , and by how much .","label":"Uses","metadata":{},"score":"65.43547"}
{"text":"Experience sampling is used to simultaneously collect randomly distributed self - reports of interruptibility .Based on these simulated sensors , we construct statistical models predicting human interruptibility and compare their predictions with the collected self - report data .The results of these models , although covering a demographically limited sample , are very promising , with the overall accuracy of several models reaching about 78 % .","label":"Uses","metadata":{},"score":"65.44243"}
{"text":"Multiple comparisons , adjusted for multiple testing , indicated that when the overall significance level is 0.1 , the sense distributions ( 0.5 , 0.5 ) and ( 0.6 , 0.4 ) impact the error rate .These results show that almost balanced sense distributions and rather large training sample sizes reduce the error rate to approximately half of our best guess , which is using the majority sense .","label":"Uses","metadata":{},"score":"65.533875"}
{"text":"Data set for experiments .Four abbreviations were used in the experiments .Table 1 lists the detailed information about the abbreviations and their senses .These abbreviations were originally specified in the ABBR data set [ 8 ] .We chose them by considering the different levels of semantic similarity among their senses .","label":"Uses","metadata":{},"score":"65.56456"}
{"text":"AAAI Fall Symp 93 98 - 107 .Hamosh A , Scott AF , Amberger J , Bocchini C , Valle D , McKusick VA : Online Mendelian Inheritance in Man ( OMIM ) , a knowledgebase of human genes and genetic disorders .","label":"Uses","metadata":{},"score":"65.60261"}
{"text":"This figure shows the plots of \" error rate \" versus \" sample size \" with different sense distributions of RSV data set ( case where the 2 ambiguous senses both refer to viruses but the viruses are different types of viruses ) using 5-fold cross validation .","label":"Uses","metadata":{},"score":"65.649536"}
{"text":"MoE systems find application in many domains , including classification , image processing , time - series prediction , data mining , fault - tolerance , modeling , etc [ 4 ] [ 6 ] [ 11 ] [ 15 ] [ 16].","label":"Uses","metadata":{},"score":"65.80658"}
{"text":"..e - art results In order to compare decision lists with other state of the art algorithms we tagged all 191 words in the DSO corpus .The same test set is also used in ( Escudero et al .2000b ) which presents a boosting appro ... . by","label":"Uses","metadata":{},"score":"65.90416"}
{"text":"Pieter Abbeel , Adam Coates , Timothy Hunter and Andrew Y. Ng in ISER 2008 .We propose a system for improving grasping using fingertip optical proximity sensors that allows us to perform online grasp adjustments to an initial grasp point without requiring premature object contact or regrasping strategies .","label":"Uses","metadata":{},"score":"65.96463"}
{"text":"Dictionary - based protein name recognition is often a first step in extracting in - formation from biomedical documents because it can provide ID information on recognized terms .However , dictionary - based approaches present two fundamental difficulties : ( 1 ) false recognition mainly caused by short names ; ( 2 ) low recall due to spelling variations .","label":"Uses","metadata":{},"score":"66.02914"}
{"text":"In their study , rare senses ( senses appearing in less than 40 documents ) were excluded from the testing set .This makes the disambiguation task easier because it reduces the problem of sparse senses .In addition , the training set was created based on long - form and short - form pairs , where ambiguous words not having long - forms were not tested .","label":"Uses","metadata":{},"score":"66.26439"}
{"text":"We now determine the probability distribution for the sex of the sample . where and are the parameters of normal distribution which have been previously determined from the training set .Note that a value greater than 1 is OK here - it is a probability density rather than a probability , because height is a continuous variable .","label":"Uses","metadata":{},"score":"66.270485"}
{"text":"The full - form in the title or abstract of the article was then replaced with the ambiguous abbreviation , and the appropriate sense was noted separately .Table 1 shows the number of articles that were obtained for the different abbreviations and senses .","label":"Uses","metadata":{},"score":"66.52255"}
{"text":"Page 10 .The leaf nodes consist of expert classifiers , while the gating nodes com- bine the output of each classifier to the root of the tree which makes the final prediction .The gating nodes com- bine the predictions of the expert classifiers based on an estimate of the cluster membership of a test protein sequence .","label":"Uses","metadata":{},"score":"66.61905"}
{"text":"Statistical tests [ 39 , 40 ] can be used to compare different classifiers .It is very important that the baseline of a classification task is reported because it shows how much of an improvement there is using a classifier as compared to the baseline .","label":"Uses","metadata":{},"score":"66.774925"}
{"text":"Comparison of Naïve Bayes , mixture of Naïve Bayes and ensemble of Naïve Bayes models on the DNA - protein data set Figure 3 Comparison of Naïve Bayes , mixture of Naïve Bayes and ensemble of Naïve Bayes models on the DNA - protein data set .","label":"Uses","metadata":{},"score":"67.09666"}
{"text":"For example , in identify- ing RNA - protein and DNA - protein interface residues from amino acid sequences , there is typically no consensus sequence around each site .Classifiers trained using machine learning to distinguish \" positive \" examples from the \" negative \" ones , must \" learn \" to do so by learning the characteristics associated with known \" positive \" and \" negative \" examples .","label":"Uses","metadata":{},"score":"67.11549"}
{"text":"A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple , regardless of any possible correlations between the color , roundness and diameter features .For some types of probability models , naive Bayes classifiers can be trained very efficiently in a supervised learning setting .","label":"Uses","metadata":{},"score":"67.12953"}
{"text":"We ran- domly selected a sequence from each cluster returned by BlastClust .Thus , the resulting non - redundant RNA - pro- tein sequence data set for 30 % identity cutoff has 180 pro- tein sequences .The total number of amino acid residues is 33,235 .","label":"Uses","metadata":{},"score":"67.431595"}
{"text":"Rion Snow , Brendan O'Connor , Daniel Jurafsky and Andrew Y. Ng in EMNLP 2008 .We have created qualitatively correct 3-d models for 64.9 % of 588 images downloaded from the internet .We have also extended our model to produce large scale 3d models from a few images .","label":"Uses","metadata":{},"score":"67.48534"}
{"text":"1 Introduction Word Sense Disambiguation ( WSD ) is the problem of assigning the appropriate meaning ( sense ) to a given word in a text or discourse where this meaning is distinguishable from other senses potentially attributable to that word .","label":"Uses","metadata":{},"score":"67.680305"}
{"text":"Affiliated with .Abstract .The growing body of DNA microarray data has the potential to advance our understanding of the molecular basis of disease .However annotating microarray datasets with clinically useful information is not always possible , as this often requires access to detailed patient records .","label":"Uses","metadata":{},"score":"67.83854"}
{"text":"The best variant , which we call LazyBoosting , is tested on the largest sense -- tagged corpus available containing 192,800 examples of the 191 most frequent and ambiguous English words .Again , boosting compares favourably to the other benchmark algorithms . \" ...","label":"Uses","metadata":{},"score":"67.937744"}
{"text":"We present a method that exploits the multiple sensor modalities available on a robotic platform .We demonstrate our method on a working robotic system and evaluate its performance on a number of common household / office objects .Stephen Gould , Paul Baumstarck , Morgan Quigley , Andrew Y. Ng and Daphne Koller in M2SFA2 2008 .","label":"Uses","metadata":{},"score":"67.94638"}
{"text":"Table 3 shows the number of sequences as well as the number of positive ( + ) and negative ( - ) instances in the non - redundant RNA- and DNA - protein sequence data sets for 30 % , 60 % , and 90 % identity cutoffs .","label":"Uses","metadata":{},"score":"67.9755"}
{"text":"Tables 2 , 3 and 4 display the results for BSA , PCA and RSV , each of which has two senses .The distribution shown with bold font in column 1 is the estimated distribution of the senses , which is calculated based on the number of retrieved articles for each sense and the number of retrieved articles for all the senses .","label":"Uses","metadata":{},"score":"67.98207"}
{"text":"We propose a machine learning approach to monaural localization , using only a single microphone and an \" artificial pinna . \"Ashutosh Saxena and Andrew Y. Ng in ICRA 2009 .We present a method for cubic spline optimization and apply this approach to the tasks of planning foot and body trajectory the \" LittleDog , \" and show that the proposed approach improves over previous work on this robot .","label":"Uses","metadata":{},"score":"67.98575"}
{"text":"The conditions under which the approximate algorithm is successful are examined .Empirical results are given on a number of data sets , showing that the algorithm e ectively handles datasets with a very large number of features . ... or domains with more than 25 - 30 features .","label":"Uses","metadata":{},"score":"68.03264"}
{"text":"MM and CF conceived of the study , and participated in its design and coordination and helped to draft the manuscript .MM also performed statistical analysis and interpreted the results .HL advised in the design of study .All authors read and approved the final manuscript .","label":"Uses","metadata":{},"score":"68.035614"}
{"text":"For example , a sample testing set with size 20 and sense distribution ( 0.5 , 0.5 ) means 10 samples in the set are with one sense and the other 10 samples are with the other sense .The estimated sense distribution is listed in the last column of Table 1 , which is calculated based on the number of retrieved articles for each sense and the number of retrieved articles for all the senses .","label":"Uses","metadata":{},"score":"68.290245"}
{"text":"Results .Support Vector Machine ( SVM ) classifiers were applied to an automatically generated data set containing four ambiguous biomedical abbreviations : BPD , BSA , PCA , and RSV , which were chosen because of varying degrees of differences in their respective senses .","label":"Uses","metadata":{},"score":"68.361176"}
{"text":"The online version of this article ( doi : 10 .1186/​1471 - 2105 - 7 - 334 ) contains supplementary material , which is available to authorized users .Background .During the last few years , there has been a surge of interest in information extraction and text mining of the biomedical literature [ 1 , 2 ] .","label":"Uses","metadata":{},"score":"68.366104"}
{"text":"We discuss how ROS relates to existing robot software frameworks , and briefly overview some of the available application software which uses ROS .Morgan Quigley , Brian Gerkey , Ken Conley , Josh Faust , Tully Foote , Jeremy Leibs , Eric Berger , Rob Wheeler , and Andrew Y. Ng in ICRA 2009 .","label":"Uses","metadata":{},"score":"68.51355"}
{"text":"View Article PubMed .Eisen MB , Spellman PT , Brown PO , Botstein D : Cluster analysis and display of genome - wide expression patterns .PNAS 1998 , 95 : 14863 - 14868 .View Article PubMed .Golub TR , et al .","label":"Uses","metadata":{},"score":"68.59235"}
{"text":"Caragea C , Sinapov J , Silvescu A , Dobbs D , Honavar V : Glycosyla- tion site prediction using ensembles of Support Vector Machine classifiers .BMC Bioinformatics 2007 , 8:438 .Kim JH , Lee J , Oh B , Kimm K , Koh I : Prediction of phosphoryla- tion sites using SVMs .","label":"Uses","metadata":{},"score":"68.84137"}
{"text":"Stephen Gould , Joakim Arfvidsson , Adrian Kaehler , Benjamin Sapp , Marius Meissner , Gary Bradski , Paul Baumstarck , Sukwon Chung , Andrew Y. Ng in IJCAI 2007 .To develop a broadly applicable parallel programming method , we adapt Google 's map - reduce [ 7 ] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms .","label":"Uses","metadata":{},"score":"68.86844"}
{"text":"Filip Krsmanovic , Curtis Spencer , Daniel Jurafsky and Andrew Y. Ng in ICSLP 2006 .We present a learning algorithm which predicts , as a function of the images , the position at which to grasp the object .Ashutosh Saxena , Justin Driemeyer , Justin Kearns , Chioma Osondu , and Andrew Y. Ng in ISER 2006 .","label":"Uses","metadata":{},"score":"68.91973"}
{"text":"Identification of functionally important sites in biomolecular sequences has broad applications ranging from rational drug design to the analysis of metabolic and signal transduction networks .Experimental determination of such sites lags far behind the number of known biomolecular sequences .Hence , there is a need to develop reliable computational methods for identifying functionally important sites from biomolecular sequences .","label":"Uses","metadata":{},"score":"68.950516"}
{"text":"View Article PubMed .Gaudan S , Krisch H , Rebholz - Schuhmann D : Resolving abbreviations to their senses in Medline .Bioinformatics 2005 , 21 : 3658 - 3664 .View Article PubMed .Schuemie MJ , Kors JA , Mons B : Word sense disambiguation in the biomedical domain : an overview .","label":"Uses","metadata":{},"score":"69.41191"}
{"text":"We present algorithms for estimating depth from a single still image .We then apply these ideas to create 3-d models that are visually - pleasing as well as quantitatively accurate from individual images .Ashutosh Saxena , Min Sun , and Andrew Y. Ng in AAAI 2008 .","label":"Uses","metadata":{},"score":"69.70817"}
{"text":"Ellen Klingbeil , Ashutosh Saxena , Andrew Y. Ng in RSS Workshop on Robotic Manipulation 2008 .We propose a technique for bias correction that significantly improves annotation quality on two of five tasks investigated on Amazon 's Mechanical Turk system .","label":"Uses","metadata":{},"score":"69.93326"}
{"text":"When such sequences are removed from the data sets , the number of sequences reduces from 435 to 246 in the case of RNA - protein interface data set , and from 1259 to 317 in the case of DNA - protein interface data set .","label":"Uses","metadata":{},"score":"70.049904"}
{"text":"In particular , we notice that when the sense distribution ( P1 , P2 ) was very unbalanced ( i.e. 0.9 , 0.1 ) , then the error rate was almost equal to the minority sense proportion .All these findings indicate that the effectiveness of an increase in the sample size is very dependent on the degree of difficulty .","label":"Uses","metadata":{},"score":"70.14311"}
{"text":"Figure 9 shows the tree structure produced by the 2-way spectral clustering algorithm when applied to a set of 147 RNA - protein sequences .The similarity matrix is com- puted based on the Needleman - Wunsch global alignment algorithm .","label":"Uses","metadata":{},"score":"70.223305"}
{"text":"Eleventh Conf . on Uncertainty in Artificial Intelligence .Morgan Kaufmann .pp .338 - 345 .Affiliated with .Abstract .Background .Automated techniques have been developed that address the WSD problem for a number of text processing situations , but the problem is still a challenging one .","label":"Uses","metadata":{},"score":"70.33729"}
{"text":"Page 9 .A residue was identified as interface residue using Entangle with the default parameters [ 12].While construct- ing our non - redundant sequence data sets , we applied var- ious identity cutoffs , starting from 30 % and ending at 90 % in steps of 10 .","label":"Uses","metadata":{},"score":"70.62349"}
{"text":"Figure 4 shows plots of the error rate versus sample size for each distribution of the BPD data set based on the 5-fold cross validation using the \" one - vs - rest \" algorithm .The plots for the four different sense distributions are very similar and actually agree with results obtained indicating that the effect of the different distributions on the error rate is insignificant .","label":"Uses","metadata":{},"score":"70.79595"}
{"text":"\" [ Show abstract ] [ Hide abstract ] ABSTRACT : Mixture - of - Experts ( MoE ) systems solve intricate problems by combining results generated independently by multiple computational models ( the \" experts \" ) .Given an instance of a problem , the responsibility of an expert measures the degree to which the expert 's output contributes to the final solution .","label":"Uses","metadata":{},"score":"70.90031"}
{"text":"The same study , which was also performed for the Fly organism , showed similar results , but with slightly higher ambiguity rates .This study shows that the ambiguity among gene symbols , English words and other biomedical terms is extensive and the distribution of ambiguity is very sparse .","label":"Uses","metadata":{},"score":"71.32522"}
{"text":"For each experiment , we did the following : .Iterate GLAD algorithm on labeled training data only .Iterate GLAD algorithm on labeled and unlabeled training data .Compare model accuracy on test data across generated populations of models .In these experiments , GLAD was run for 100 iterations with a population size of 5000 , and a subset size of 3 features .","label":"Uses","metadata":{},"score":"71.355896"}
{"text":"GenBank Gene Accession Numbers were used to generate the common features .Table 1 provides additional details on these datasets .Demonstrations .For this study we implemented a GLAD algorithm as a wrapper technique for feature selection .A Genetic Algorithm ( GA ) is used for generating a population of relevant feature subsets .","label":"Uses","metadata":{},"score":"71.42273"}
{"text":"As the Friedman 's test indicated , the effect of the sense distribution on the error rate is significant .When the sense distribution is ( 0.5 , 0.5 ) there are statistically significant differences between the pairs of error rates produced under sample size ( 20 and 120 ) , the sample sizes ( 40 and 120 ) and the sample sizes ( 80 and 120 ) .","label":"Uses","metadata":{},"score":"71.50475"}
{"text":"Authors ' contributions CC and JS carried out the computations .CC prepared an initial draft of the manuscript .All authors participated in experimental design , discussions , and manuscript prepa- ration .All authors read and approved the final manu- script .","label":"Uses","metadata":{},"score":"71.617096"}
{"text":"Quoc Le , David Kamm and Andrew Y. Ng in ICRA 2010 .To address the task of detecting , localizing , and labeling elevator buttons , we use state - of - the - art vision algorithms along with machine learning techniques to take advantage of contextual features .","label":"Uses","metadata":{},"score":"72.088524"}
{"text":"Comparison of Logistic Regression , mixture of Logistic Regression and ensemble of Logistic Regression models on the RNA - protein data set Figure 2 Comparison of Logistic Regression , mixture of Logistic Regression and ensemble of Logistic Regression mod- els on the RNA - protein data set .","label":"Uses","metadata":{},"score":"72.27451"}
{"text":"Learning mixture of experts models Here we present our approach to learning a mixture of experts model that takes into account the global similarity between biomolecular sequences .Unlike the Hierarchical Mixture of Experts model [ 9 ] , we assume that the structure of our model is not known a priori .","label":"Uses","metadata":{},"score":"72.50148"}
{"text":"Four ambiguous abbreviations : BPD , BSA , PCA , and RSV , were used in this study .They were chosen because they were associated with varying degrees of differences between their respective senses .For example , two of the senses of PCA studied are very similar whereas two senses of BSA are very different .","label":"Uses","metadata":{},"score":"72.65829"}
{"text":"The range of sample size per sense ranges from 10 - 40 , with increments of 10 per sense .Average error rates ( Err .Rate ) and average standard errors ( SE ) were reported for each combination of distribution and sample size ( see Methods section ) .","label":"Uses","metadata":{},"score":"72.98593"}
{"text":"For PCA , the estimated distribution was ( 0.67 , 0.33 ) .Four different sample sizes were used ( 20 , 40 , 80 and 120 ) , and for each , a proportional sample for each sense was obtained based on the particular distribution .","label":"Uses","metadata":{},"score":"73.2408"}
{"text":"It has also been shown that if a curve dominates in PR space , it also dominates in ROC space [ 22].Page 14 .Publish with BioMed Central and every scientist can read your work free of charge \" BioMed Central will be the most significant development for disseminating the results of biomedical research in our lifetime . \"","label":"Uses","metadata":{},"score":"74.182365"}
{"text":"Error Rate versus Sample Size with different sense distributions of BSA data set .This figure shows the plots of \" error rate \" versus \" sample size \" with different sense distributions of BSA data set ( case where the 2 ambiguous senses are very different ) using 5-fold cross - validation .","label":"Uses","metadata":{},"score":"74.18524"}
{"text":"On the other hand , today 's computer systems are almost entirely oblivious to the human world they operate in , and typically have no way to take into account the interruptibility of the user .This paper presents a Wizard of Oz study exploring whether , and how , robust sensor - based predictions of interruptibility might be constructed , which sensors might be most useful to such predictions , and how simple such sensors might be .","label":"Uses","metadata":{},"score":"74.19554"}
{"text":"A SLAM- generated map of an office environment can be annotated with text labels by including text detection and recognition modules to robotic mapping .We present a series of additions to the typical mapping pipeline that allows us to generate an annotated map that enables our robot to recognize named locations specified by a user in 84 % of cases .","label":"Uses","metadata":{},"score":"74.31445"}
{"text":"We computed the standard deviation of the error rate as follows .Recall that for each abbreviation , each sense distribution and each sample size we run the experiment 30 times .The error rate was computed using both a 5-fold and a 10-fold cross - validation scheme .","label":"Uses","metadata":{},"score":"74.689766"}
{"text":"We present a novel method for identifying and tracking objects in multi - resolution digital video of partially cluttered environments .Our method is motivated by biological vision systems and uses a learned \" attentive \" interest map on a low resolution data stream to direct a high resolution \" fovea .","label":"Uses","metadata":{},"score":"75.420364"}
{"text":"Two ambiguous gene symbol lists were formed as a result of the comparisons : a gene - English list ( containing gene symbols ambiguous with general English words ) and a gene - UMLS list ( containing gene symbols ambiguous with biomedical terms ) .","label":"Uses","metadata":{},"score":"75.662056"}
{"text":"To demonstrate the extent of the ambiguity problem in MEDLINE we searched MEDLINE abstracts to determine how many abstracts contained gene symbols that were ambiguous with general English words or biomedical terms .We repeated the same procedure for the fly and yeast organisms as well .","label":"Uses","metadata":{},"score":"76.019"}
{"text":"In 2000 , the UMLS Metathesaurus [ 6 ] , a comprehensive resource that specifies and categorizes biomedical concepts , contained 9,416 ambiguous terms , and in 2004 , the number increased to 21,295 , an increase of 126 % within 4 years [ 7 ] .","label":"Uses","metadata":{},"score":"76.38987"}
{"text":"Smaller increases in the sample size had an insignificant effect .We performed further sub - analysis using non - parametric multiple comparisons to identify the pairs of sample sizes that differ when the abbreviations BSA and RSV were analyzed .This analysis revealed that in the case of BSA the improvements in terms of error rate were statistically significant across distributions as the sample size increased from 20 to 40 .","label":"Uses","metadata":{},"score":"76.92796"}
{"text":"Performing graph - cut inference in the trained MRF can then be used to segment new scenes very efficiently .Drago Anguelov , Ben Taskar , Vasco Chatalbashev , Daphne Koller , Dinkar Gupta , Geremy Heitz and Andrew Y. Ng in CVPR 2005 .","label":"Uses","metadata":{},"score":"77.21907"}
{"text":"Department of Biomedical Informatics , Columbia University .Department of Biostatistics , Bioinformatics and Biomathematics , Georgetown University Medical Center .References .Krallinger M , Valencia A : Text - mining and information - retrieval services for molecular biology .Genome Biol 2005 , 6 : 224 .","label":"Uses","metadata":{},"score":"77.994354"}
{"text":"Finally , we present the results of experiments in which the robotic hand was affixed to a manipulator arm and teleoperated to grasp and manipulate a variety of objects .Morgan Quigley , Curt Salisbury , Andrew Y. Ng and J. Kenneth Salisbury .","label":"Uses","metadata":{},"score":"78.09161"}
{"text":"Acknowledgements .This work was supported by part by Grants R01 LM7659 , R01 LM8635 from the National Library of Medicine , and Grants NSF - DMS-0504957 , NSF- IIS-0430743 from the National Science Foundation .We would like to thank Lyudmila Shagina for providing technical support .","label":"Uses","metadata":{},"score":"78.663895"}
{"text":"Due to name abbreviations , identical names , name misspellings , and pseudonyms in publications or bibliographies ( citations ) , an author may have multiple names and multiple authors may share the same name .Such name ambiguity affects the performance of document retrieval , web search , database integra ... \" .","label":"Uses","metadata":{},"score":"78.76017"}
{"text":"This assessment allows for behavior we perceive as natural , socially appropriate , or simply polite .On the other hand , today 's computer systems are almost entirely oblivious to the huma ... \" .A person seeking someone else 's attention is normally able to quickly assess how interruptible they are .","label":"Uses","metadata":{},"score":"79.095"}
{"text":"Mixture of experts - our approach Our approach to learning a mixture of experts model takes into account the global similarity between biomolecular sequences in a data set .Comparison of Precision - Recall curves for Logistic Regression , mixture of Logistic Regression and ensemble of Logistic Regression models on the non - redundant DNA - protein data set at 30 % identity cutoff .","label":"Uses","metadata":{},"score":"79.341255"}
{"text":"In the first evaluation of GLAD performance , test data classification accuracy was compared between models identified using only labeled data and models using both labeled and unlabeled data .Figure 1 shows the results for three cancer groups .The top 5 % of the model populations were used to generate these histograms .","label":"Uses","metadata":{},"score":"79.75603"}
{"text":"For BSA , RSV and BPD , we found that the effect of the sense distribution on the error rate was insignificant .For PCA this effect was significant .The effect of different sample sizes on the error rate was significant for BSA , RSV , and BPD .","label":"Uses","metadata":{},"score":"80.47226"}
{"text":"The estimate of the standard error was then obtained by averaging the above values over the 30 runs .Gene ambiguity for mining MEDLINE .To determine the extent of the gene ambiguity problem in MEDLINE , we searched MEDLINE abstracts to determine how many abstracts contained gene symbols that were ambiguous with general English words or biomedical terms .","label":"Uses","metadata":{},"score":"81.13757"}
{"text":"Ellen Klingbeil , Deepak Drao , Blake Carpenter , Varun Ganapathi , Oussama Khatib , Andrew Y. Ng in ICRA 2011 .We present the design of a new low - cost series- elastic robotic arm and explore the design decisions and tradeoffs made in achieving this combination of price and performance .","label":"Uses","metadata":{},"score":"81.312836"}
{"text":"Ellen Klingbeil , Blake Carpenter , Olga Russakovsky and Andrew Y. Ng in ICRA 2010 .We present apprenticeship learning algorithms , which leverage expert demonstrations to efficiently learn good controllers for tasks being demonstrated by an expert .Our experimental results include the first autonomous execution of a wide range of maneuvers and complete airshows .","label":"Uses","metadata":{},"score":"83.598755"}
{"text":"Conclusion Identification of functionally important sites in biomo- lecular sequences has broad applications ranging from rational drug design to the analysis of metabolic and sig- nal transduction networks .With the rapid increase in the amount of data ( e.g. , protein sequences ) there is a grow- ing need for reliable procedures to accurately identify such sites .","label":"Uses","metadata":{},"score":"83.75482"}
{"text":"A number of natural language processing systems in the biomedical domain reported decreased precision due to the ambiguity problem [ 3 , 4 ] .Weeber [ 5 ] found that in order to replicate Swanson 's literature - based discovery of the involvement of magnesium deficiency in migraine , it was important to resolve the ambiguity of an abbreviation mg , which can denote either magnesium or milligram .","label":"Uses","metadata":{},"score":"84.525154"}
{"text":"Blood 2003 , 102 : 3871 - 9 .View Article PubMed .Copyright .© Harris and Ghaffari .This article is published under license to BioMed Central Ltd.","label":"Uses","metadata":{},"score":"90.015495"}
{"text":"Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .Publisher conditions are provided by RoMEO .","label":"Uses","metadata":{},"score":"101.71884"}
