{"text":"The application of active learning to tasks requiring such complex outputs has not been well studied , the exceptions being Hwa ( 2001 ) , Soderland ( 1999 ) , and Thompson et al .( 1999 ) .The latter two include work on active learning applied to information extraction , and Thompson et al .","label":"Background","metadata":{},"score":"25.099232"}
{"text":"The above papers contain a number of theoretiical models , especially the Blum & Mitchell 1998 paper , and the Abney paper .Following are more recent theoretical models for how and when unlabeled data can improve learning .These papers provide PAC - style bounds on co - training and related learning settings that go beyond those provided in the original co - training paper .","label":"Background","metadata":{},"score":"26.08534"}
{"text":"Collins and Singer ( 1999 ) proposed a version of Co - Training that is biased towards learning hypotheses that predict the same label on most of the unlabeled examples .They introduce an explicit objective function that measures the compatibility of the learned hypotheses and use a boosting algorithm to optimize this objective function .","label":"Background","metadata":{},"score":"29.609411"}
{"text":"In order to cope with this problem , Goldman and Zhou ( 2000 ) advocate the use of multiple biases instead of multiple views .The authors introduce an algorithm similar to Co - Training , which bootstraps from each other hypotheses learned by two different base learners ; this approach relies on the assumption that the base learners generate hypotheses that partition the instance space into equivalence classes .","label":"Background","metadata":{},"score":"30.343931"}
{"text":"Active learning with committees for text categorization .In The 14th National Conference on Artificial Intelligence ( AAAI-97 ) , pp .591 - 596 .Lindenbaum , M. , Markovitch , S. , & Rusakov , D. ( 2004 ) .","label":"Background","metadata":{},"score":"31.04131"}
{"text":"Improving generalization with active learning .Machine Learning , 15 , 201 - 221 .Cohn , D. , Ghahramani , Z. , & Jordan , M. ( 1996 ) .Active learning with statistical models .In Advances in Neural Information Processing Systems , Vol . 9 , pp .","label":"Background","metadata":{},"score":"31.153103"}
{"text":"278 - 292 ) .Cohn , D. A. , Ghahramani , A. , & Jordan , M. I. ( 1996 ) .Active learning with statistical models .Journal of Artificial Intelligence Research , 4 ( 1 ) , 129 - 145 .","label":"Background","metadata":{},"score":"31.393204"}
{"text":"Co- EM can be seen as the closest implementation of the theoretical framework proposed by Blum and Mitchell ( 1998 ) .209Muslea , Minton , & Knoblock - Ghani ( 2002 ) uses Error - Correcting Output Codes to allow Co - Training and Co - EM to scale up to problems with a large number of classes . -","label":"Background","metadata":{},"score":"33.89257"}
{"text":"Analyzing the effectiveness and applicability of co - training .In Proceedings of Information and Knowledge Management , pp .86 - 93 .Pierce , D. , & Cardie , C. ( 2001 ) .Limitations of co - training for natural language learning from large datasets .","label":"Background","metadata":{},"score":"34.25077"}
{"text":"Related work .Blum and Mitchell [ 4 ] introduced the co - training algorithm for improving the sample classification performance when there are few labeled samples and many unlabeled samples .The co - training algorithm assumes that there are two independent sets of features available , such that each feature set is good enough to train a good classifier .","label":"Background","metadata":{},"score":"34.452644"}
{"text":"They focus on a neural network approach to active learning in a version - space of concepts .For example , Liere and Tadepalli ( 1997 ) apply active learning with committees to the problem of text categorization .They show improvements with active learning similar to those that we obtain , but use a committee of Winnow - based learners on a traditional classification task .","label":"Background","metadata":{},"score":"34.58796"}
{"text":"First , we introduce the concepts and notation , followed by a comprehensive survey of the literature on active and multi - view learning .Then we formally introduce the Co - Testing family of algorithms and we present our empirical evaluation on a variety of real - world domains .","label":"Background","metadata":{},"score":"34.816483"}
{"text":"Machine Learning , 245 - 295 .Thompson , C. , Califf , M. E. , & Mooney , R. ( 1999 ) .Active learning for natural language parsing and information extraction .In Proceedings of the 16th International Conference on Machine Learning ( ICML-99 ) , pp .","label":"Background","metadata":{},"score":"34.9649"}
{"text":"Note that both approaches above use the strong and weak views for passive , rather than active learning .That is , given a fixed set of labeled and no unlabeled examples , these algorithms learn one weak and one strong hypothesis that are then used to craft a domainspecific predictor that outperforms each individual hypothesis .","label":"Background","metadata":{},"score":"35.12319"}
{"text":"In Proceedings of the 26th ACM Symposium on the Theory of Computing , pp .253 - 262 .Blum , A. , & Mitchell , T. ( 1998 ) .Combining labeled and unlabeled data with co - training .In Proceedings of the 1988 Conference on Computational Learning Theory , pp .","label":"Background","metadata":{},"score":"35.134495"}
{"text":"42 - 53 .Jones , R. , Ghani , R. , Mitchell , T. , & Riloff , E. ( 2003 ) .Active learning for information extraction with multiple view feature sets .In Proceedings of the ECML-2004 Workshop on Adaptive Text Extraction and Mining ( ATEM-2003 ) .","label":"Background","metadata":{},"score":"35.19893"}
{"text":"In a demonstration of their technique aimed at web page classification , the addition of unlabeled samples decreased classification error relative to classification using only labeled data .In a subsequent study , Nigam and Ghani [ 5 ] further examined the performance of the co - training algorithm and specifically its sensitivity to the independence of the feature sets .","label":"Background","metadata":{},"score":"35.270496"}
{"text":"In contrast to Queryby - Bagging , which has a poor performance on courses and wrapper induction , Co - Testing obtains the highest accuracy among the considered algorithms .Co- Testing 's success is due to its ability to discover the mistakes made in each view .","label":"Background","metadata":{},"score":"35.363388"}
{"text":"This is the most obvious solution for 2-view learning tasks in which the base learner can not ( reliably ) estimate the confidence of its predictions .However , in practice , one also encounters views in which one can accurately learn only a concept that is strictly more general or more specific than the concept of interest ( Muslea , Minton , & Knoblock , 2003 ) .","label":"Background","metadata":{},"score":"35.707237"}
{"text":"This approach has several advantages : - it converges quickly to the target concept because it is based on the idea of learning from mistakes ( remember that each contention point is guaranteed to represent a mistake in at least one of the views ) .","label":"Background","metadata":{},"score":"36.156395"}
{"text":"Journal of Machine Learning Research , 2 , 45 - 66 .Valiant , L. ( 1984 ) .A theory of the learnable .Communications of the ACM , 27 ( 11 ) , 1134- 1142 .Watkin , T. , & Rau , A. ( 1992 ) .","label":"Background","metadata":{},"score":"36.162125"}
{"text":"In section 5.2.2 we describe a Co - Testing algorithm that exploits strong and weak views for wrapper induction domains ( Muslea et al . , 2003 ; Muslea , Minton , & Knoblock , 2001 ) .4.3 Co - Testing vs. Related Approaches As we already mentioned in section 3.3 , existing multi - view approaches are typically semisupervised learners that bootstrap the views from each other .","label":"Background","metadata":{},"score":"36.19016"}
{"text":"Unfortunately , this paper introduces just a theoretical definition for the weak dependence of the views , without providing an intuitive explanation of its practical consequences .Researchers proposed two main types of extensions to the original Co - Training algorithm : modifications of the actual algorithm and changes aiming to extend its practical applicability .","label":"Background","metadata":{},"score":"36.233963"}
{"text":"This approach is motivated by the observation that the quality of the bootstrapped data is crucial for Co - Training 's convergence .-Co - Boost ( Collins & Singer , 1999 ) and Greedy Agreement ( Abney , 2002 ) are Co - Training algorithms that explicitly aim to minimize the number of contention points .","label":"Background","metadata":{},"score":"36.57241"}
{"text":": they are determinist learners that are noise sensitive , provide no confidence in their predictions , and make no mistakes on the training set .In contrast , Co - Testing applies naturally to both wrapper induction ( Muslea et al . , 2000 , 2003 ) and information extraction from free text ( Jones et al . , 2003 ) .","label":"Background","metadata":{},"score":"36.77903"}
{"text":"It is also demonstrated that both of these online classifiers perform much better than a standard Naïve Bayes method .The theoretical and implementation computational complexity of these two classifiers are very low , and they are very easily adaptively updated .","label":"Background","metadata":{},"score":"36.96991"}
{"text":"As mentioned in section 3.3 , such algorithms bootstrap the views from each other by training each view on the examples labeled with high - confidence by the other view .For ad and tf , we could not use multi - view , semi - supervised learning because the base learners ib and mc4 do not provide a ( reliable ) estimate of the confidence in their predictions .","label":"Background","metadata":{},"score":"36.982197"}
{"text":"Previously , this topic was largely ignored , though the idea clearly shows up in applications such as word sense disambiguation ( Yarowsky , 1995 ) and speech recognition ( de Sa & Ballard , 1998 ) .Blum and Mitchell proved that two independent , compatible views can be used to pac - learn ( Valiant , 1984 ) a concept based on few labeled and many unlabeled examples .","label":"Background","metadata":{},"score":"37.03264"}
{"text":"289 - 296 ) .Harpale , S. , & Yang , Y. ( 2008 ) .Personalized active learning for collaborative filtering .In Proceedings of the 31st annual international ACM SIGIR conference on research and development in information retrieval ( pp .","label":"Background","metadata":{},"score":"37.05639"}
{"text":"In The 19th International Conference on Machine Learning ( ICML-2002 ) , pp .443 - 450 .Muslea , I. , Minton , S. , & Knoblock , C. ( 2003 ) .Active learning with strong and weak views : a case study on wrapper induction .","label":"Background","metadata":{},"score":"37.119972"}
{"text":"Fedorov , V. V. ( 1972 ) .Theory of optimal experiment .Academic Press .Finn , A. , & Kushmerick , N. ( 2003 ) .Active learning selection strategies for information extraction .In Proceedings of the ECML-2004 Workshop on Adaptive Text Extraction and Mining ( ATEM-2003 ) .","label":"Background","metadata":{},"score":"37.362064"}
{"text":"Yu , K. , Bi , J. , & Tresp , V. ( 2006 ) .Active learning via transductive experimental design .In Proceedings of the 23rd international conference on machine learning ( pp .1081 - 1088 ) .About this Article .","label":"Background","metadata":{},"score":"37.43477"}
{"text":"Training Invariant Support .Machine Learning , 46 , pp.161 - 90 .[ Kearns & Vazirani , 1994 ] Kearns , M. & Vazirani , U. , 1994 .An Introduction to Computational Learning Theory .MIT Press .[ Mahoney & Mooney , 1992 ] Mahoney , J.J. & Mooney , R.J. , 1992 .","label":"Background","metadata":{},"score":"37.527924"}
{"text":"First , we introduce Co - Testing , which is the first approach to multi - view active learning .Second , we extend the multi - view learning framework by also exploiting weak views , which are adequate only for learning a concept that is more general / specific than the target concept .","label":"Background","metadata":{},"score":"37.752644"}
{"text":"Compared with previous work , Co - Testing is unique in several ways : 1 .In contrast , Co - Testing is the first algorithm that exploits multiple views for active learning purposes .Furthermore , Co - Testing allows the simultaneous use of strong and weak views without additional data engineering costs .","label":"Background","metadata":{},"score":"37.969162"}
{"text":"In Proceedings of the 1988 Conference on Computational Learning Theory ( COLT-72 ) , pp .287 - 294 .Shapiro , E. ( 1981 ) .A general incremental algorithm that infers theories from facts .In Proceedings of the 7th International Joint Conference on Artificial Intelligence , pp .","label":"Background","metadata":{},"score":"38.13422"}
{"text":"Scheffer , T. , Decomain , C. , & Wrobel , S. ( 2001 ) .Active hidden Markov models for information extraction .In Proceedings of the 4th international conference on advances in intelligent data analysis , IDA ' 01 ( pp .","label":"Background","metadata":{},"score":"38.260834"}
{"text":"Active learning with local models .Neural Processing Letters , 7 , 107 - 117 .Hsu , C.-N. , & Dung , M.-T. Generating finite - state transducers for semi - structured data extraction from the web .Journal of Information Systems , 23(8 ) , 521 - 538 .","label":"Background","metadata":{},"score":"38.47287"}
{"text":"Learning from queries and examples with tree - structured bias .In Proceedings of the 10th International Conference on Machine Learning ( ICML-93 ) , pp .322 - 329 .Tadepalli , P. , & Russell , S. ( 1998 ) .","label":"Background","metadata":{},"score":"38.626858"}
{"text":"Roy , N. , & McCallum , A. ( 2001 ) .Toward optimal active learning through sampling estimation of error reduction .In Proceedings of the 18thInternational Conference on Machine Learning ( ICML-2001 ) , pp .441 - 448 .","label":"Background","metadata":{},"score":"38.702934"}
{"text":"Learning with unreliable boundary queries .Journal of Computer and System Sciences , 56 ( 2 ) , 209 - 222 .228 Active Learning with Multiple Views Blum , A. , Furst , M. , Jackson , J. , Kearns , M. , Mansour , Y. , & Rudich , S. ( 1994 ) .","label":"Background","metadata":{},"score":"38.901768"}
{"text":"We have also introduced and evaluated a Co - Testing algorithm that simultaneously exploits both strong and weak views .Our empirical results show that Co - Testing is a powerful approach to active learning .In all these scenarios , Co - Testing clearly outperforms the single - view , state of the art active learning algorithms .","label":"Background","metadata":{},"score":"39.030426"}
{"text":"Gervasio , M. T. , Moffitt , M. D. , & Pollack , M. E. ( 2005 ) .Active preference learning for personalized calender scheduling assistance .In Proceedings of the 10th international conference on intelligent user interfaces ( pp .","label":"Background","metadata":{},"score":"39.154755"}
{"text":"Such a content - based view is a weak view because it often represents a concept more general than the target one .In this weak view , we use as base learner a version of DataPro ( Lerman , Minton , & Knoblock , 2003 ) that is described elsewhere ( Muslea et al . , 2003 ) . \" and \" AlphaNum .","label":"Background","metadata":{},"score":"39.23419"}
{"text":"Learning multiple tasks with kernel methods .Journal of Machine Learning Research , 6 , 615 - 637 .MathSciNet MATH .Fedorov , V. V. ( 1972 ) .Theory of optimal experiments .New York : Academic Press .Ford , I. , & Silvey , S. D. ( 1980 ) .","label":"Background","metadata":{},"score":"39.254494"}
{"text":"Angluin , D. , & Slonim , D. ( 1991 ) .Randomly fallible teachers : learning monotone DNF with an incomplete membership oracle .Machine Learning , 14 ( 1 ) , 7 - 26 .Argamon - Engelson , S. , & Dagan , I. ( 1999 ) .","label":"Background","metadata":{},"score":"39.29403"}
{"text":"Dror , H. A. , & Steinberg , D. M. ( 2008 ) .Sequential experimental designs for generalized linear models .Journal of the American Statistical Association , 103 ( 481 ) , 288 - 298 .MathSciNet MATH CrossRef .","label":"Background","metadata":{},"score":"39.31127"}
{"text":"Also note that Co - Testing can be combined with virtually any of the existing single - view active learners : among the contention points , Co - Testing can select the next query based on any of the heuristics discussed in section 3.2 . - single - view active learners are typically designed for a particular ( class of ) base learner(s ) .","label":"Background","metadata":{},"score":"39.32531"}
{"text":"Lexical acquisition via constraint solving In Papers from the 1995 AAAI Symposium on the Representation and Acquisition of Lexical Knowledge : Polysemy , Ambiguity , and Generativity , 118 - 122 Stanford , CA .Riloff , E. Jones , R. 1999 .","label":"Background","metadata":{},"score":"39.439186"}
{"text":"John , G. H. ( 1995 ) .Robust decision trees : removing outliers from databases .In Knowledge discovery and data mining ( pp .174 - 179 ) .Knorr , E. M. , & Ng , R. T. ( 1999 ) .","label":"Background","metadata":{},"score":"39.513588"}
{"text":"Machine Learning , 54 ( 2 ) , 125 - 152 .Marcu , D. , Carlson , L. , & Watanabe , M. ( 2000 ) .The automatic translation of discourse structures .In Proceedings of the 1stAnnual Meeting of the North American Chapter of the Association for Computational Linguistics ( NAACL-2000 ) , pp .","label":"Background","metadata":{},"score":"39.548767"}
{"text":"As with the above approaches , independent models are developed for each dataset .They show that minimizing the disagreement in predictions between these models leads to improved accuracy , and introduced a co - updating technique for iteratively improving prediction concordance .","label":"Background","metadata":{},"score":"39.623962"}
{"text":"Wolpert , D. H. ( 1996 ) .The lack of a priori distinctions between learning algorithms .Neural Computation , 8 ( 7 ) , 1341 - 1390 .CrossRef .Zadrozny , B. , & Elkan , C. ( 2001 ) .","label":"Background","metadata":{},"score":"39.749836"}
{"text":"Intuitively , Dasgupta et al .( 2001 ) show that the ratio of contention points to unlabeled examples is an upper - bound on the error rate of the classifiers learned in the two views .Abney ( 2002 ) extends the work of Dasgupta et al . by relaxing the view independence assumption .","label":"Background","metadata":{},"score":"39.7923"}
{"text":"Zhou , Y. , & Goldman , S. ( 2004 ) .Democratic co - learning .In Proceedings of the International Conference on Tools with Artificial Intelligence , pp .594 - 602 .With respect to additional active learning techniques , Cohn et al .","label":"Background","metadata":{},"score":"39.896828"}
{"text":"Improved models are iteratively generated using a genetic algorithm feature selection technique .Our results show that the addition of unannotated data into training , significantly improves classifier robustness .Background .The introduction of DNA microarray technology in 1995 [ 1 ] has likely resulted in a huge volume of as yet undiscovered and potentially medically useful knowledge within gene expression profiles .","label":"Background","metadata":{},"score":"40.274647"}
{"text":"Intuitively , stalker creates a forward or a 2 .A recent paper ( Brefeld & Scheffer , 2004 ) shows that - for text classification - svm is more appropriate than Naive Bayes as base learner for Co - EM , though not necessarily for Co - Training .","label":"Background","metadata":{},"score":"40.50687"}
{"text":"A unique two - term scoring function was derived to independently score the labeled and unlabeled data models .An overall score is computed as a weighted average of the two terms as shown below .We defined the labeled data model score as the standard leave - one - out - cross - validation accuracy for the labeled training samples .","label":"Background","metadata":{},"score":"40.584595"}
{"text":"Text on websites can judge the relevance of link classifiers , hence the term \" co - training \" .Mitchell claims that other search algorithms are 86 % accurate , whereas co - training is 96 % accurate .[ 6 ] .","label":"Background","metadata":{},"score":"40.70453"}
{"text":"Active learning for logistic regression : an evaluation .Machine Learning , 68 ( 3 ) , 235 - 265 .CrossRef .Seeger , M. W. ( 2008 ) .Bayesian inference and optimal design for the sparse linear model .","label":"Background","metadata":{},"score":"40.892715"}
{"text":"Shapiro , E. ( 1982 ) .Algorithmic program diagnosis .In Proceedings of the 9th ACM Symposium on Principles of Programming Languages , pp .299 - 308 .Sloan , R. , & Turan , G. ( 1994 ) .","label":"Background","metadata":{},"score":"40.973717"}
{"text":"Learning concepts by asking questions .In Carbonell , R. S. M. , Carbonell , J. , & 2 ) , T. M. M. V. ( Eds . ) , Machine Learning : An Artificial Intelligence Approach , pp .167 - 192 .","label":"Background","metadata":{},"score":"40.99083"}
{"text":"In the early 1980s , the machine learning community started recognizing the advantages of inductive systems that are capable of querying their instructors .For example , in order to detect errors in Prolog programs , the Algorithmic Debugging System ( Shapiro , 1981 , 1982 ) was allowed to ask the user several types of queries .","label":"Background","metadata":{},"score":"41.02926"}
{"text":"The Co- Testing approach to active learning has both advantages and disadvantages .On one hand , Co - Testing can not be applied to problems that do not have at least two views .On the other hand , for any multi - view problem , Co - Testing can be used with the best base learner for that particular task .","label":"Background","metadata":{},"score":"41.234818"}
{"text":"Query learning using boosting and bagging .In Proceedings of the 15th International Conference on Machine Learning ( ICML-98 ) , pp . 1 - 10 .Abney , S. ( 2002 ) .Bootstrapping .In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pp .","label":"Background","metadata":{},"score":"41.274025"}
{"text":"In general , committee - based sampling tends to be associated with the version space reduction approach .However , for base learners such as support vector machines , one can use a single hypothesis to make queries that remove ( approximately ) half of the version space ( Tong & Koller , 2001 ) .","label":"Background","metadata":{},"score":"41.30634"}
{"text":"As the two experimental setups are not identical ( i.e. , crossvalidation vs. random splits ) this is just an informal comparison ; however , it puts our results into perspective by contrasting Co - Testing with another state of the art approach to wrapper induction .","label":"Background","metadata":{},"score":"41.35182"}
{"text":"The second , more sophisticated approach to selective sampling , expected - error minimization , is based on the statistically optimal solution to the active learning problem .In this scenario , the intuition is to query the unlabeled example that minimizes the error rate of the ( future ) classifier on the test set .","label":"Background","metadata":{},"score":"41.388477"}
{"text":"Consequently , researchers proposed methods to estimate the error reduction for various types of base learners .The heuristic approach to expected - error minimization can be summarized as follows .First , one chooses a loss function ( Roy & McCallum , 2001 ) that is used to estimate the future error rate .","label":"Background","metadata":{},"score":"41.404446"}
{"text":"However , there is no general method to include domain knowledge into all inductive learning algorithms as all hybrid methods are highly specialized for a particular algorithm .We present an algorithm that will take domain knowledge in the form of propositional rules , generate artificial examples from the rules and also remove instances likely to be flawed .","label":"Background","metadata":{},"score":"41.435123"}
{"text":"By asking the user to label such a contention point , Co - Testing is guaranteed to provide useful information for the view that made the mistake .In this paper we make several contributions .First , we introduce Co - Testing , a family of active learners for multi - view learning tasks .","label":"Background","metadata":{},"score":"41.45984"}
{"text":"Kanninen , B. ( 2002 ) .Optimal design for multinomial choice experiments .Journal of Marketing Research , 39 , 307 - 317 .CrossRef .Lewi , J. , Butera , R. , & Paninski , L. ( 2009 ) .","label":"Background","metadata":{},"score":"41.55257"}
{"text":"The goal of this paper is to better understand the data used in machine learning problems by identifying and analyzing the instances that are frequently misclassified by learning algorithms that have shown utility to date and are commonly used in practice .","label":"Background","metadata":{},"score":"41.57444"}
{"text":"[5 ] .Co- training has been used to classify web pages using the text on the page as one view and the anchor text of hyperlinks on other pages that point to the page as the other view .","label":"Background","metadata":{},"score":"41.690067"}
{"text":"Brefeld , U. , & Scheffer , T. ( 2004 ) .Co- EM support vector learning .In Proceedings of the 21stInternational Conference on Machine Learning ( ICML-2004 ) , pp .121 - 128 .Breiman , L. ( 1996 ) .","label":"Background","metadata":{},"score":"41.849228"}
{"text":"Journal of Artificial Intelligence Research , 11 , 335 - 360 .Baum , E. ( 1991 ) .Neural net algorithms that learn in polynomial time from examples and queries .IEEE Transactions on Neural Networks , 2 , 5 - 19 .","label":"Background","metadata":{},"score":"41.88617"}
{"text":"An alternative is discussed by Finn and Kushmerick ( 2003 ) , who explore a variety of ie - specific heuristics that can be used for active learning purposes and analyze the trade - offs related to using these heuristics .- for wrapper induction , where the goal is to extract data from Web pages that share the same underlying structure , there are no reported results for applying ( singleview ) active learning .","label":"Background","metadata":{},"score":"41.901863"}
{"text":"Liere R , Tadepalli P ( 1998 )Active learning with committees in text categorization : preliminary results in comparing winnow and perceptron .In : Learning for text categorization , technical report WS-98 - 05 .AAAI Press , Menlo Park .","label":"Background","metadata":{},"score":"42.019516"}
{"text":"415 - 420 .Nahm , U.-Y. , & Mooney , R. ( 2000 ) .A mutually beneficial integration of data mining and information extraction .In The 17th National Conference on Artificial Intelligence ( AAAI-2000 ) , pp .627 - 632 .","label":"Background","metadata":{},"score":"42.16938"}
{"text":"In their work , a committee of hidden Markov models is used to select examples for annotation .Lewis and Catlett ( 1994 ) use heterogeneous certainty - based methods , in which a simple classifier is used to select examples that are then annotated and presented to a more powerful classifier .","label":"Background","metadata":{},"score":"42.185646"}
{"text":"A metric for unsupervised metalearning .Intelligent Data Analysis , 15 ( 6 ) , 827 - 841 .Lewis , D. D. , & Gale , W. A. ( 1994 ) .A sequential algorithm for training text classifiers .In Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval ( pp .","label":"Background","metadata":{},"score":"42.253582"}
{"text":"We have implemented all active learners as extensions of the MLC++ library ( Kohavi , Sommerfield , & Dougherty , 1997 ) .For each domain , we choose the base learner as follows : after applying all primitive learners in MLC++ on the dataset ( 10-fold cross - validation ) , we select the one that obtains the best performance .","label":"Background","metadata":{},"score":"42.259544"}
{"text":"It was introduced by Avrim Blum and Tom Mitchell in 1998 .Contents .Co- training is a semi - supervised learning technique that requires two views of the data .It assumes that each example is described using two different feature sets that provide different , complementary information about the instance .","label":"Background","metadata":{},"score":"42.336796"}
{"text":"In Proceedings of the 18th national conference on artificial intelligence ( pp .526 - 532 ) .Bordley , R. F. ( 1982 ) .A multiplicative formula for aggregating probability assessments .Management Science , 28 ( 10 ) , 1137 - 1148 .","label":"Background","metadata":{},"score":"42.79107"}
{"text":"In : Proceedings of the 20th ACM international conference on research and development in information retrieval , pp 67 - 73 .Zhang T ( 2001 ) Regularized Winnow methods .Adv Neural Inf Process Syst 13:703 - 709 .Kivinen J , Warmuth MK , Auer P ( 1997 )","label":"Background","metadata":{},"score":"43.070557"}
{"text":"[ 6 ] proposed a Semi - Supervised Learning ( SSL ) algorithm for heterogeneous datasets having both labeled and unlabeled samples .Their example data were comprised of DNA microarray expressions and phylogenic reconstructions , with class labels corresponding to gene function .","label":"Background","metadata":{},"score":"43.363823"}
{"text":"MATH .McCallum , A. , & Nigam , K. ( 1998 ) .Employing EM and pool - based active learning for text classification .In Proceedings of the 15th international conference on machine learning ( pp .350 - 358 ) .","label":"Background","metadata":{},"score":"43.390495"}
{"text":"In Proceedings of the 19th conference on uncertainty in artificial intelligence ( pp .616 - 623 ) .Yu , K. , Tresp , V. , & Schwaighofer , A. ( 2005 ) .Learning Gaussian processes from multiple tasks .","label":"Background","metadata":{},"score":"43.449192"}
{"text":"Within the multi - view framework , Nigam and Ghani ( 2000 ) show that , for \" bag - ofwords \" text classification , one can create two views by arbitrarily splitting the original set of features into two sub - sets .","label":"Background","metadata":{},"score":"43.498848"}
{"text":"105 - 112 ) .San Mateo : Morgan Kaufmann .Freund , Y. , & Schapire , R. E. ( 1996 ) .Experiments with a new boosting algorithm .In Thirteenth international conference on machine learning ( pp .148 - 156 ) .","label":"Background","metadata":{},"score":"43.579376"}
{"text":"Campbell , C. , Cristianini , N. , & Smola , A. ( 2000 ) .Query learning with large margin classifiers .In Proceedings of the 17th International Conference on Machine Learning ( ICML-2000 ) , pp . 111 - 118 .","label":"Background","metadata":{},"score":"43.65168"}
{"text":"However , in all fairness , it is unlikely that Co - Testing could lead to an even faster convergence .As shown by Muslea et al .( 2001 ) , the end of the phone number can be found in a similar manner .","label":"Background","metadata":{},"score":"43.6992"}
{"text":"Dagan , I. , & Engelson , S. P. ( 1995 ) .Committee - based sampling for training probabilistic classifiers .In Proceedings of the 12th international conference on machine learning ( pp .150 - 157 ) .Dasgupta , S. , & Hsu , D. ( 2008 ) .","label":"Background","metadata":{},"score":"43.87195"}
{"text":"In Proceedings of the fifth annual workshop on computational learning theory ( pp .287 - 294 ) .CrossRef .Smith , M. R. , & Martinez , T. ( 2011 ) .Improving classification accuracy by identifying and removing instances that should be misclassified .","label":"Background","metadata":{},"score":"43.979744"}
{"text":"McCallum , A. , & Nigam , K. ( 1998 ) .Employing EM in pool - based active learning for text classification .In Proceedings of the 15th International Conference on Machine Learning , pp .359 - 367 .Melville , P. , & Mooney , R. J. ( 2004 ) .","label":"Background","metadata":{},"score":"43.99818"}
{"text":"Sarkar , A. ( 2001 ) .Applying co - training methods to statistical parsing .In Proceedings of the 2nd Annual Meeting of the North American Chapter of the Association for Computational Linguistics ( NAACL-2001 ) , pp .175 - 182 .","label":"Background","metadata":{},"score":"44.112686"}
{"text":"Dagan I , Karov Y , Roth D ( 1997 ) Mistake - driven learning in text categorization .In : Proceedings of the 2nd conference on empirical methods in natural language processing , pp 55 - 63 .Ng HT , Goh WB , Low KL ( 1997 )","label":"Background","metadata":{},"score":"44.11336"}
{"text":"San Francisco , 1991 .[Pazzani et al . , 1997 ] Pazzani , M. , Mani , S. & Shankle , W.R. , 1997 .Comprehensible knowledge discovery in databases .In CogSci-97 . , 1997 .[ Quinlan , 1993 ] Quinlan , J.R. , 1993 .","label":"Background","metadata":{},"score":"44.126125"}
{"text":"In this paper , we introduce Co - Testing , an active learning technique for multi - view learning tasks ; i.e. , tasks that have several disjoint subsets of features ( views ) , each of which is sufficient to learn the concepts of interest .","label":"Background","metadata":{},"score":"44.135666"}
{"text":"Machine Learning , 24(2 ) , 123 - 140 .Califf , M. E. , & Mooney , R. ( 1999 ) .Relational learning of pattern - match rules for information extraction .In Proceedings of the Sixteenth National Conference on Artificial Intelligence ( AAAI-99 ) , pp .","label":"Background","metadata":{},"score":"44.167892"}
{"text":"Furthermore , since obtaining labels is expensive , we optimally choose which data to ask a subject for labelling to obtain the most of information about her / his preferences .This paradigm - called active learning - has hardly been studied in a multi - task formalism .","label":"Background","metadata":{},"score":"44.23007"}
{"text":"Knowledge - based artificial neural networks .Artif .Intel ., 70 , pp.50 - 62 .[ Vapnik , 1998 ] Vapnik , V.N. , 1998 .Statistical Learning Theory .New York : Wiley .[ Yu , 2007 ] Yu , T. , 2007 .","label":"Background","metadata":{},"score":"44.248608"}
{"text":"Raskutti , B. , Ferra , H. , & Kowalczyk , A. ( 2002 ) .Combining clustering and co - training to enhance text classification using unlabeled data .In Proceedings of the SIGKDD International Conference on Knowledge Discovery and Data Mining , pp .","label":"Background","metadata":{},"score":"44.250435"}
{"text":"Neural Computation , 21 ( 3 ) , 619 - 687 .MathSciNet MATH CrossRef .Lewis , D. , & Gale , W. ( 1994 ) .A sequential algorithm for training text classifiers .In Proceedings of the 17th annual international ACM SIGIR conference on research and development in information retrieval ( pp .","label":"Background","metadata":{},"score":"44.365395"}
{"text":"It presents a statistical model for estimating accuracy for bootstrap learning of named entity and relation extractors , under the assumption that correct entities and relations will be repeatedly extracted from a large corpus , and that correct extractions will be repeatedly more frequently than incorrect extractions .","label":"Background","metadata":{},"score":"44.463375"}
{"text":"Science 1999 , 286 : 531 - 537 .View Article PubMed .Blum A , Mitchell TM : Combining labeled and unlabeled data with co - training .Proceedings of the Eleventh Annual Conference on Computational Learning Theory 1998 , 92 - 100 .","label":"Background","metadata":{},"score":"44.64389"}
{"text":"In Proceedings of the 12th international conference on knowledge discovery and data mining ( pp .504 - 509 ) .New York : ACM .Barnett , V. , & Lewis , T. ( 1978 ) .Outliers in statistical data ( 2nd ed . )","label":"Background","metadata":{},"score":"44.790546"}
{"text":"Aha , D. ( 1992 ) .Tolerating noisy , irrelevant and novel attributes in instance - based learning algorithms .International Journal of Man - Machine Studies , 36 ( 1 ) , 267 - 287 .Amoth , T. , Cull , P. , & Tadepalli , P. ( 1998 ) .","label":"Background","metadata":{},"score":"44.80426"}
{"text":"MathSciNet MATH .Settles , B. , & Craven , M. ( 2008 ) .An analysis of active learning strategies for sequence labeling tasks .In Proceedings of the conference on empirical methods in natural language processing ( pp .1070 - 1079 ) .","label":"Background","metadata":{},"score":"44.81723"}
{"text":"The strength of weak learnability .Machine Learning , 5(2 ) , 197 - 227 .Scheffer , T. , & Wrobel , S. ( 2001 ) .Active learning of partially hidden Markov models .In Proceedings of the ECML / PKDD-2001 Workshop on Active Learning , Database Sampling , Experimental Design : Views on Instance Selection .","label":"Background","metadata":{},"score":"44.950584"}
{"text":"Artifical Intelligence , 18 ( 2 ) , 203 - 226 .CrossRef .Orriols - Puig , A. , Macià , N. , Bernadó - Mansilla , E. , & Ho , T. K. ( 2009 ) .Documentation for the data complexity library in C++ ( Tech .","label":"Background","metadata":{},"score":"45.121952"}
{"text":"230 Active Learning with Multiple Views Lewis , D. , & Gale , W. ( 1994 ) .A sequential algorithm for training text classifiers .In Proceedings of Research and Development in Information Retrieval , pp .3 - 12 .","label":"Background","metadata":{},"score":"45.3554"}
{"text":"Finally , the system queries the unlabeled example that leads to the largest estimated reduction in the error rate .Finally , a typical version space reduction active learner works as follows : it generates a committee of several hypotheses , and it queries the unlabeled examples on which the disagreement within the committee is the greatest .","label":"Background","metadata":{},"score":"45.363712"}
{"text":"References .Abe , N. , & Mamitsuka , H. ( 1998 ) .Query learning strategies using boosting and bagging .In Proceedings of the fifteenth international conference on machine learning ( pp . 1 - 9 ) .Abe , N. , Zadrozny , B. , & Langford , J. ( 2006 ) .","label":"Background","metadata":{},"score":"45.36624"}
{"text":"[ Mitchell , 1997 ] Mitchell , T.M. , 1997 .Concept learning and general to specific ordering .In Mitchell , T.M. Machine learning .McGraw - Hill . pp.20- 50 .[Niyogi et al . , 1998 ] Niyogi , P. , Girosi , F. & Poggio , T. , 1998 .","label":"Background","metadata":{},"score":"45.375122"}
{"text":"363 - 369 ) .Chaloner , K. , & Verdinelli , I. ( 1995 ) .Bayesian experimental design : a review .Statistical Science , 10 , 273 - 304 .MathSciNet MATH CrossRef .Christensen , R. ( 1997 ) .","label":"Background","metadata":{},"score":"45.38558"}
{"text":"Query - based learning applied to partially trained multilayer perceptrons .IEEE Transactions on Neural Networks , 2 , 131 -136 .Jackson , J. ( 1994 ) .An efficient membership - query algorithm for learning DNF with respect to the uniform distribution .","label":"Background","metadata":{},"score":"45.443855"}
{"text":"Collins , M. , & Singer , Y. ( 1999 ) .Unsupervised models for named entity classification .In Proceedings of the Empirical NLP and Very Large Corpora Conference , pp .100 - 110 .Dagan , I. , & Engelson , S. ( 1995 ) .","label":"Background","metadata":{},"score":"45.4474"}
{"text":"In Proceedings of the 22nd international conference on machine learning .Clyde , M. , Muller , P. , & Parmigiani , G. ( 1993 ) .Optimal design for heart defibrillators .In Case studies in Bayesian statistics ( Vol .","label":"Background","metadata":{},"score":"45.520164"}
{"text":"Kushmerick , N. ( 2000 ) .Wrapper induction : efficiency and expressiveness .Artificial Intelligence Journal , 118 ( 1 - 2 ) , 15 - 68 .Kushmerick , N. , Johnston , E. , & McGuinness , S. ( 2001 ) .","label":"Background","metadata":{},"score":"45.731586"}
{"text":"Proceedings of IEEE , 86 , pp.2196 - 209 .[Pazzani et al . , 1991 ] Pazzani , M. , Brunk , C. & & Silverstein , G. , 1991 .A knowledge - intensive approach to learning relational concepts .","label":"Background","metadata":{},"score":"45.895348"}
{"text":"Automatic acquisition of the lexical semantics of verbs from sentence frames In Proceedings of the 27thAnnual Meeting of the Association for Computational Linguistics ( ACL-89 ) , 177 - 184 .Yamazaki , T. , Pazzani , M. , Merz , C. 1995 .","label":"Background","metadata":{},"score":"45.89843"}
{"text":"Inducing a semantically annotated lexicon via EM - based clustering In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics , 104 - 111 .Schneider , R. 1998 .A lexically - intensive algorithm for domain - specific knowledge acquisition In Proceedings of the Joint Conference on New Methods in Language Processing and Computational Natural Language Learning , 19 - 28 .","label":"Background","metadata":{},"score":"45.98403"}
{"text":"London : Springer .CrossRef .Segata , N. , Blanzieri , E. , & Cunningham , P. ( 2009 ) .A scalable noise reduction technique for large case - based systems .In Proceedings of the 8th international conference on case - based reasoning : case - based reasoning research and development ( pp .","label":"Background","metadata":{},"score":"46.0353"}
{"text":"Depending on the method used to generate the committee , one can distinguish several types of active learners : - Query - by - Committee selects a committee by randomly sampling hypotheses from the version space .Query - by - Committee was applied to a variety of base learners such as perceptrons ( Freund et al . , 1997 ) , Naive Bayes ( McCallum & Nigam , 1998 ) , and Winnow ( Liere & Tadepalli , 1997 ) .","label":"Background","metadata":{},"score":"46.057648"}
{"text":"Second , we extend the multi - view learning framework by also exploiting weak views , which are adequate only for learning a concept that is moregeneral / specific than the target concept .Finally , we empirically show that Co - Testing outperforms existing active learners on a variety of real world domains such as wrapperinduction , Web page classification , advertisement removal , and discourse tree parsing .","label":"Background","metadata":{},"score":"46.05867"}
{"text":"Consequently , a query is \" wasted \" on a totally irrelevant image .Similar situations appear in many real world tasks such as text classification , information extraction , or speech recognition : whenever the active learner artificially builds a query for such a domain , it is highly unlikely that the newly created object has any meaning for the human user .","label":"Background","metadata":{},"score":"46.197227"}
{"text":"These two hypotheses are generated by modifying the base learner so that it learns a classifier that labels as many as possible of the unlabeled examples in the working set as positive or negative , respectively .This approach has an obvious drawback : it requires the user to modify the base learner so that it can generate \" most - general \" and \" most - specific \" classifiers .","label":"Background","metadata":{},"score":"46.306927"}
{"text":"In Proceedings of the 17th International Conference on Machine Learning ( ICML-2000 ) , pp .327 - 334 .Gross , K. ( 1991 ) .Concept acquisition through attribute evolution and experiment selection .Ph.D. thesis , School of Computer Science , Carnegie Mellon University .","label":"Background","metadata":{},"score":"46.548584"}
{"text":"1649 - 1652 ) .Kriegel , H. P. , Kröger , P. , Schubert , E. , & Zimek , A. ( 2011 ) .Interpreting and unifying outlier scores .In SDM ( pp .13 - 24 ) .","label":"Background","metadata":{},"score":"46.56102"}
{"text":"Advances in instance selection for instance - based learning algorithms .Data Mining and Knowledge Discovery , 6 ( 2 ) , 153 - 172 .MathSciNet CrossRef .Brodley , C. E. , & Friedl , M. A. ( 1999 ) .","label":"Background","metadata":{},"score":"46.562965"}
{"text":"Journal of Artificial Intelligence Research , 11 , 131 - 167 .Brodley , C. E. , & Utgoff , P. E. ( 1995 ) .Multivariate decision trees .Machine Learning , 19 ( 1 ) , 45 - 77 .","label":"Background","metadata":{},"score":"46.683327"}
{"text":"Blei , D. M. , Ng , A. Y. , & Jordan , M. I. ( 2003 ) .Latent Dirichlet allocation .Journal of Machine Learning Research , 3 , 993 - 1022 .MATH .Blythe , J. ( 2002 ) .","label":"Background","metadata":{},"score":"46.693016"}
{"text":"In Proceedings of the 25th international conference on very large data bases ( pp .211 - 222 ) .Kriegel , H. P. , Kröger , P. , Schubert , E. , & Zimek , A. ( 2009 ) .Loop : local outlier probabilities .","label":"Background","metadata":{},"score":"46.70952"}
{"text":"5.2.3 The Experimental Results In our empirical comparison , we use the 33 most difficult wrapper induction tasks from the testbed introduced by Kushmerick ( 1998 , 2000 ) .These tasks , which were previously used in the literature ( Muslea et al . , 2003 ; Muslea , 2002 ) , are briefly described in Table 4 .","label":"Background","metadata":{},"score":"46.753525"}
{"text":"Clustering techniques [ 2 ] are applied to the datasets for assigning samples to their corresponding group solely based on similar expression levels .Supervised algorithms on the other hand classify [ 3 ] samples according to their externally determined class .","label":"Background","metadata":{},"score":"46.79584"}
{"text":"i to denote an unlabeled examples ) .In this paper the terms active learning and selective sampling are used interchangeably .In the traditional , single - view machine learning scenario , a learner has access to the entire set of domain features .","label":"Background","metadata":{},"score":"46.86868"}
{"text":"Co- training first learns a separate classifier for each view using any labeled examples .The most confident predictions of each classifier on the unlabeled data are then used to iteratively construct additional labeled training data .[ 1 ] .","label":"Background","metadata":{},"score":"46.98618"}
{"text":"In the situation that such a split is not available , a random assignment of features into two sets still performs better than using only one feature set .They also introduced the co - EM algorithm , a hybrid that iteratively updates the unlabeled data labels using EM .","label":"Background","metadata":{},"score":"47.008415"}
{"text":"Using this notation , the next query is the contention point for which min(n1 , n2 ) has the largest value .In the empirical evaluation below , we compare these two Co - Testing algorithms with Random Sampling and Query - by - Bagging .","label":"Background","metadata":{},"score":"47.04908"}
{"text":"Siskind , J. M. 1992 .Naive Physics , Event Perception , Lexical Semantics and Language Acquisition .Ph.D. thesis , Department of Electrical Engineering and Computer Science , Massachusetts Institute of Technology , Cambridge , MA .Suppes , P. , Liang , L. , Böttner , M. 1991 .","label":"Background","metadata":{},"score":"47.134277"}
{"text":"In The IJCAI-2001 Workshop on Adaptive Text Extraction and Mining .Lang , K. , & Baum , E. ( 1992 ) .Query learning can work poorly when a human oracle is used .In Proceedings of the IEEE International Joint Conference on Neural Networks .","label":"Background","metadata":{},"score":"47.229473"}
{"text":"Platt , J. ( 2000 ) .Probabilistic outputs for support vector machines and comparison to regularized likelihood methods .In Advances in large margin classifiers .Quinlan , J. R. ( 1993 ) .C4.5 : programs for machine learning .","label":"Background","metadata":{},"score":"47.25361"}
{"text":"An alternative solution is proposed by Raskutti , Ferra , and Kowalczyk ( 2002 ) , where the authors create a second view that consists of a variety of features that measure the examples ' similarity with the N largest clusters in the domain .","label":"Background","metadata":{},"score":"47.331257"}
{"text":"Acquisition of a lexicon from semantic representations of sentences In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics ( ACL-95 ) , 335 - 337 Cambridge , MA .Tong , S. Koller , D. 2000 .Support vector machine active learning with applications to text classification In Proceedings of the Seventeenth International Conference on Machine Learning ( ICML-2000 ) , 999 - 1006 Stanford , CA .","label":"Background","metadata":{},"score":"47.382263"}
{"text":"In contrast , Random Sampling and Query - by - Bagging learn 100 % accurate rules for only seven and twelve of the tasks , respectively .In other words , both Co - Testing algorithms learn the correct target concept for more than twice as many tasks than Query - by - Bagging or Random Sampling .","label":"Background","metadata":{},"score":"47.510334"}
{"text":"[ 7 ] .Nigam , Kamal ; Rayid Ghani ( 2000 ) .\" Analyzing the Effectiveness and Applicability of Co - training \" .Proceedings of the ninth international Conference on Information and Knowledge Management ( NY , USA : ACM ) : 86 - 93 .","label":"Background","metadata":{},"score":"47.531876"}
{"text":"Hwa ( 2001 ) describes an interesting method for evaluating a statistical parser 's uncertainty , when applied for syntactic parsing .Next : Future Work Up : Related Work Previous : Lexicon Acquisition Beckwith , R. , Fellbaum , C. , Gross , D. , Miller , G. 1991 .","label":"Background","metadata":{},"score":"47.572357"}
{"text":"The learning task consists of classifying Web pages as course homepages and other pages .In courses the two views consist of words that appear in the page itself and words that appear in hyperlinks pointing to them , respectively .- tf ( Marcu , Carlson , & Watanabe , 2000 ) is a classification problem with seven classes , 99 features and 11,193 examples .","label":"Background","metadata":{},"score":"47.720753"}
{"text":"- in its simplest form ( i.e. , Naive Co - Testing , which is described in section 4 ) , it makes no assumptions about the properties of the base learner .More precisely , by simply querying an arbitrary contention point , Co - Testing is guaranteed to provide \" the mistaken view \" with a highly informative example . - by considering only the contention points as query candidates , it allows the use of query selection heuristics that - computationally - are too expensive to be applied to the entire set of unlabeled examples .","label":"Background","metadata":{},"score":"47.78312"}
{"text":"In Proceedings of the 2010 European conference on machine learning and knowledge discovery in databases : part III ( pp .499 - 514 ) .CrossRef .Yu , K. , Schwaighofer , A. , Tresp , V. , Ma , W. Y. , & Zhang , H. J. ( 2003 ) .","label":"Background","metadata":{},"score":"47.839394"}
{"text":"La Salle , Universitat Ramon Llull .Peterson , A. H. , & Martinez , T. R. ( 2005 ) .Estimating the potential for combining learning models .In Proceedings of the ICML workshop on meta - learning ( pp .","label":"Background","metadata":{},"score":"47.89946"}
{"text":"Here is a lightly - annotated bibliography of papers on learning from labeled and unlabeled data .It focuses on methods especially relevant to bootstrap learning for natural language analysis , and on theoretical models for how and when we should expect unlabeled data to be helpful .","label":"Background","metadata":{},"score":"47.914276"}
{"text":"Learning to parse database queries using inductive logic programming In Proceedings of the Thirteenth National Conference on Artificial Intelligence ( AAAI-96 ) , 1050 - 1055 Portland , OR .Abstract .Most data complexity studies have focused on characterizing the complexity of the entire data set and do not provide information about individual instances .","label":"Background","metadata":{},"score":"48.29764"}
{"text":"In Proceedings of the 19th European conference on artificial intelligence ( pp .249 - 254 ) .Guo , S. , & Sanner , S. ( 2010 ) .Real - time multiattribute Bayesian preference elicitation with pairwise comparison queries .","label":"Background","metadata":{},"score":"48.344837"}
{"text":"Manning , C. D. 1993 .Automatic acquisition of a large subcategorization dictionary from corpora In Proceedings of the 31stAnnual Meeting of the Association for Computational Linguistics ( ACL-93 ) , 235 - 242 Columbus , OH .McCallum , A. K. Nigam , K. 1998 .","label":"Background","metadata":{},"score":"48.40738"}
{"text":"The remainder of this section is organized as follows : first , we formally present the CoTesting family of algorithms and we discuss several of its members .Then we introduce the concepts of strong and weak views , and we analyze how Co - Testing can exploit both types of views ( previous multi - view learners could only use strong views ) .","label":"Background","metadata":{},"score":"48.45868"}
{"text":"We introduce now the notion of a weak view , in which one can accurately learn only a concept that is strictly more general or more specific than the target concept .In the context of learning with strong and weak views , we redefine contention points as the unlabeled examples on which the strong views predict a different label .","label":"Background","metadata":{},"score":"48.53341"}
{"text":"In Proceedings of the 24th international conference on machine learning ( pp .935 - 942 ) .New York : ACM .Webb , G. I. ( 2000 ) .Multiboosting : a technique for combining boosting and wagging .Machine Learning , 40 ( 2 ) , 159 - 196 .","label":"Background","metadata":{},"score":"48.561558"}
{"text":"Angluin , D. ( 1982 ) .A note on the number of queries needed to identify regular languages .Information and Control , 51 , 76 - 87 .Angluin , D. ( 1988 ) .Queries and concept learning .","label":"Background","metadata":{},"score":"48.57168"}
{"text":"For experiments aimed at answering a clinical question , such information might include patient disease stage , or response to a particular drug .The cost of producing adequately annotated datasets has been a barrier to the widespread application of microarray technology in medicine .","label":"Background","metadata":{},"score":"48.635597"}
{"text":"A comparison of classifiers and document representations for the routing problem .In : Proceedings of the 18th ACM international conference on research and development in information retrieval , pp 229 - 237 .Wiener ED , Pedersen JO , Weigend AS ( 1995 )","label":"Background","metadata":{},"score":"48.743553"}
{"text":"On courses we investigate two of the Co - Testing query selection strategies : naive and conservative .The third , aggressive query selection strategy is not appropriate for courses because the \" hyperlink view \" is significantly less accurate than the other one ( after all , one rarely encounters more than a handful of words in a hyperlink ) .","label":"Background","metadata":{},"score":"48.833584"}
{"text":"Berlin : Springer .Breunig , M. M. , Kriegel , H. P. , Ng , R. T. , & Sander , J. ( 2000 ) .Lof : identifying density - based local outliers .SIGMOD Record , 29 ( 2 ) , 93 - 104 .","label":"Background","metadata":{},"score":"48.936085"}
{"text":"The Co- Testing Family of Algorithms In this section , we discuss in detail the Co - Testing family of algorithms .As we already mentioned , Co - Testing can be seen as a two - step iterative process : first , it uses a few labeled examples to learn a hypothesis in each view ; then it queries an unlabeled example for which the views predict different labels .","label":"Background","metadata":{},"score":"48.98381"}
{"text":"In KDD ( pp .204 - 213 ) .Zadrozny , B. , & Elkan , C. ( 2002 ) .Transforming classifier scores into accurate multiclass probability estimates .In KDD ( pp .694 - 699 ) .New York : ACM .","label":"Background","metadata":{},"score":"48.999294"}
{"text":"Cuisine : ... R1 R2 Figure 3 : Both the forward and backward rules detect the beginning of the phone number .5.2 Co - Testing for Wrapper Induction We focus now on a different type of learning application , wrapper induction ( Muslea et al . , 2001 ; Kushmerick , 2000 ) , in which the goal is to learn rules that extract relevant sub - strings from a collection of documents .","label":"Background","metadata":{},"score":"49.03006"}
{"text":"Fürnkranz , J. , & Hüllermeier , E. ( 2010 ) .Preference learning .Berlin : Springer .MATH .Gelman , A. , Carlin , J. B. , Stern , H. S. , & Rubin , D. B. ( 2003 ) .","label":"Background","metadata":{},"score":"49.185318"}
{"text":"Boosting trees for anti - spam email filtering .In : Proceedings of European conference on recent advances in NLP , pp 58 - 64 .Lewis DD , Schapire RE , Callan JP , Papka R ( 1996 )Training algorithms for linear text classifiers .","label":"Background","metadata":{},"score":"49.31556"}
{"text":"Journal of Machine Learning Research , 4 , 83 - 99 .Berger , M. P. F. ( 1994 ) .D - optimal sequential sampling designs for item response theory models .Journal of Educational and Behavioral Statistics , 19 , 43 - 56 .","label":"Background","metadata":{},"score":"49.38086"}
{"text":"Experimental results of different scenarios are shown that demonstrate this method to be more effective than simple inductive learning .Keywords : .Rule based learning , hybrid learning , virtual sample , virtual example , artificial sample , artificial example , pruning dataset .","label":"Background","metadata":{},"score":"49.38491"}
{"text":"Information Retrieval , 13 ( 4 ) , 346 - 374 .CrossRef .Sanborn , A. N. , & Griffiths , T. L. ( 2008 ) .Markov chain Monte Carlo with people .In : Advances in neural information processing systems .","label":"Background","metadata":{},"score":"49.442192"}
{"text":"This theoretic work focused on learning classes of concepts such as regular sets , monotone dnf expressions , and u - expressions .Besides membership queries such as \" is this an example of the target concept ? , \" Angluin also used more sophisticated types of queries such as equivalence queries ( \" is this concept equivalent with the target concept ? \" ) or superset queries ( \" is this concept a superset of the target concept ? \" )","label":"Background","metadata":{},"score":"49.49419"}
{"text":"Springer - Verlag , Berlin .Johnston , M. , Boguraev , B. , Pustejovsky , J. 1995 .The acquisition and interpretation of complex nominals In Papers from the 1995 AAAI Symposium on the Representation and Acquisition of Lexical Knowledge : Polysemy , Ambiguity , and Generativity , 69 - 74 Stanford , CA .","label":"Background","metadata":{},"score":"49.509426"}
{"text":"Empirical Validation In this section we empirically compare Co - Testing with other state of the art learners .Our goal is to test the following hypothesis : given a multi - view learning problem , Co - Testing converges faster than its single - view counterparts .","label":"Background","metadata":{},"score":"49.694122"}
{"text":"Co- training can only work if one of the classifiers correctly labels a piece of data that the other classifier previously misclassified .If both classifiers agree on all the unlabeled data , i.e. they are not independent , labeling the data does not create new information .","label":"Background","metadata":{},"score":"49.95215"}
{"text":"Building an MT dictionary from parallel texts based on linguistic and statistical information In Proceedings of the Fifteenth International Conference on Computational Linguistics , 76 - 81 .Lewis , D. D. Catlett , J. 1994 .Heterogeneous uncertainty sampling for supervised learning In Proceedings of the Eleventh International Conference on Machine Learning ( ICML-94 ) , 148 - 156 San Francisco , CA .","label":"Background","metadata":{},"score":"49.95444"}
{"text":"In this paper we propose the Genetic Learning Across Datasets concept ( GLAD ) , and demonstrate an implementation that enables feature selection across unlabeled and labeled datasets .GLAD algorithms are distinct from previous approaches of semi - supervised learning in that the datasets analyzed may have very different statistical distributions , such as would arise in datasets collected independently by labs using different measurement technology .","label":"Background","metadata":{},"score":"49.97747"}
{"text":"Bridle , J. S. ( 1989 ) .Probabilistic interpretation of feedforward classification network outputs , with relationships to statistical pattern recognition .In Neuro - computing : algorithms , architectures and applications ( pp .227 - 236 ) .Berlin : Springer .","label":"Background","metadata":{},"score":"50.028717"}
{"text":"Settles , B. ( 2010 ) .Active learning literature survey ( Tech .Rep. Computer Sciences Technical Report 1648 ) .University of Wisconsin - Madison .Seung , H. S. , Opper , M. , & Sompolinsky , H. ( 1992 ) .","label":"Background","metadata":{},"score":"50.184574"}
{"text":"Active preference learning with discrete choice data .In J. C. Platt , Y. Koller , D. Singer & S. Roweis ( Eds . ) , Advances in neural information processing systems ( Vol .20 , pp .409 - 416 ) .","label":"Background","metadata":{},"score":"50.19483"}
{"text":"In addition to these two views , which rely mostly on the context of the item to be extracted ( i.e. , the text surrounding the item ) , one can use a third view that describes the content of the item to be extracted . \" , end with \" .","label":"Background","metadata":{},"score":"50.469505"}
{"text":"Yarowsky , David .Unsupervised word sense disambiguation rivaling supervised methods .In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics , pages 189 - 196 .This has been used to train web page classifiers , named entity recognizers , image classifiers , and more .","label":"Background","metadata":{},"score":"50.478676"}
{"text":"both views : the \" good view \" makes the correct prediction on them , while the \" bad view \" is inadequate to learn the target concept .In order to cope with this problem , we introduced a view validation algorithm ( Muslea et al . , 2002b ) that predicts whether the views are appropriate for a particular task .","label":"Background","metadata":{},"score":"50.57006"}
{"text":"Batista , G. E. A. P. A. , Prati , R. C. , & Monard , M. C. ( 2004 ) .A study of the behavior of several methods for balancing machine learning training data .SIGKDD Explorations Newsletter , 6 ( 1 ) , 20 - 29 .","label":"Background","metadata":{},"score":"50.5959"}
{"text":"5.1 Co - Testing for Classification We begin our empirical study by using three classification tasks to compare Co - Testing with existing active learners .We first introduce these three domains and their respective views ; then we discuss the learners used in the evaluation and analyze the experimental results .","label":"Background","metadata":{},"score":"50.63928"}
{"text":"Morgan Kaufmann .Zelle , J. M. 1995 .Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers .Ph.D. thesis , Department of Computer Sciences , University of Texas , Austin , TX .Also appears as Artificial Intelligence Laboratory Technical Report AI 96 - 249 .","label":"Background","metadata":{},"score":"50.66741"}
{"text":"The various members of the Co - Testing family differ from each other with two respects : the strategy used to select the next query , and the manner in which the output hypothesis is constructed .In other words , each Co - Testing algorithm is uniquely defined by the choice of the functions SelectQuery ( ) and CreateOutputHypothesis ( ) .","label":"Background","metadata":{},"score":"50.68459"}
{"text":"Active Learning with Multiple Views .Abstract : Active learners alleviate the burden of labeling large amounts of data by detecting and asking the user to label only the most informative examples in the domain .We focus here on active learning for multi - view domains , in which there are several disjoint subsets of features ( views ) , each of which is sufficient to learn the target concept .","label":"Background","metadata":{},"score":"50.71599"}
{"text":"Wrapper maintenance : A machine learning approach .Journal of Artificial Intelligence Research , 18 , 149 - 181 .Lewis , D. , & Catlett , J. ( 1994 ) .Heterogeneous uncertainty sampling for supervised learning .In Proceedings of the 11th International Conference on Machine Learning ( ICML-94 ) , pp .","label":"Background","metadata":{},"score":"50.757683"}
{"text":"[ 2 ] Co - training can work on \" unlabeled \" text that has not already been classified or tagged , which is typical for the text appearing on web pages and in emails .According to Tom Mitchell , \" The features that describe a page are the words on the page and the links that point to that page .","label":"Background","metadata":{},"score":"50.849083"}
{"text":"All rights reserved .Muslea , Minton , & Knoblock Co - Testing is a two - step iterative algorithm that requires as input a few labeled and many unlabeled examples .First , Co - Testing uses the few labeled examples to learn a hypothesis in each view .","label":"Background","metadata":{},"score":"50.87637"}
{"text":"Fukumoto , F. Tsujii , J. 1995 .Representation and acquisition of verbal polysemy In Papers from the 1995 AAAI Symposium on the Representation and Acquisition of Lexical Knowledge : Polysemy , Ambiguity , and Generativity , 39 - 44 Stanford , CA .","label":"Background","metadata":{},"score":"50.93034"}
{"text":"For testing the classification accuracy on the independent set , only unique classifiers were used .Figure 2 compares the performance of the unique classifiers on the testing set for two approaches : 1 - using only labeled samples 2 - using labeled plus unlabeled samples .","label":"Background","metadata":{},"score":"51.04599"}
{"text":"2690 - 2697 ) .Thomson , K. , & McQueen , R. J. ( 1996 ) .Machine learning applied to fourteen agricultural datasets ( Tech .Rep. 96/18 ) .The University of Waikato .Tomek , I. ( 1976 ) .","label":"Background","metadata":{},"score":"51.153786"}
{"text":"Committee - based sampling for training probabilistic classifiers .In Proceedings of the 12th international conference on machine learning ( pp .150 - 157 ) .Domingos , P. , & Pazzani , M. J. ( 1996 ) .Beyond independence : conditions for the optimality of the simple Bayesian classifier .","label":"Background","metadata":{},"score":"51.15663"}
{"text":"This is particularly true for base learners such as stalker , which do not improve the current hypothesis unless they are provided with examples of misclassified instances .As a limitation , Co - Testing can be applied only to multi - view tasks ; that is , unless the user can provide two views , Co - Testing can not be used at all .","label":"Background","metadata":{},"score":"51.173637"}
{"text":"363 - 383 ) .Argyriou , A. , Micchelli , C. , & Pontil , M. ( 2008 ) .When is there a representer theorem ?Vector versus matrix regularizers .Journal of Machine Learning Research .Bakker , B. , & Heskes , T. ( 2003 ) .","label":"Background","metadata":{},"score":"51.201828"}
{"text":"This straightforward strategy is appropriate for base learners that lack the capability of reliably estimating the confidence of their predictions .As this naive query selection strategy is independent of both the domain and the base learner properties , it follows that it can be used for solving any multi - view learning task .","label":"Background","metadata":{},"score":"51.20585"}
{"text":"Caruana , R. ( 1997 ) .Multitask learning .Machine Learning , 28 ( 1 ) , 41 - 75 .MathSciNet CrossRef .Chajewska , U. , Koller , D. , & Parr , R. ( 2000 ) .Making rational decisions using adaptive utility elicitation .","label":"Background","metadata":{},"score":"51.215714"}
{"text":"Diverse ensembles for active learning .In Proceedings of the 21st international conference on machine learning ( pp .584 - 591 ) .Minka , T. ( 2001 ) .A family of approximation methods for approximate Bayesian inference .PhD thesis , MIT .","label":"Background","metadata":{},"score":"51.226295"}
{"text":"Angluin , D. ( 1994 ) .Exact learning of u - DNF formulas with malicious membership queries .Tech . rep .YALEU / DCS / TR-1020 , Yale University .Angluin , D. , & Krikis , M. ( 1994 ) .","label":"Background","metadata":{},"score":"51.398365"}
{"text":"Anand , P. ( 1993 ) .The philosophy of intransitive preferences .The Economic Journal , 103 ( 417 ) , 337 - 346 .CrossRef .Arehart , K. H. , Kates , J. M. , Anderson , C. A. , & Harvey , L. O. Jr. ( 2007 ) .","label":"Background","metadata":{},"score":"51.49186"}
{"text":"Mansilla , E. B. , & Ho , T. K. ( 2004 ) .On classifier domains of competence .In ICPR ( Vol . 1 , pp .136 - 139 ) .Mitchell , T. M. ( 1982 ) .","label":"Background","metadata":{},"score":"51.51602"}
{"text":"Neural Computation , ( 7 ) .[Aha et al . , 1991 ] Aha , D. , Kibler , D. & & Albert , M. , 1991 .Instance - based learning algorithms .Machine learning , 6 , pp.37 - 66 .","label":"Background","metadata":{},"score":"51.536983"}
{"text":"Selective sampling with redundant views .In Proceedings of National Conference on Artificial Intelligence ( AAAI-2000 ) , pp .621 - 626 .Muslea , I. , Minton , S. , & Knoblock , C. ( 2001 ) .Hierarchical wrapper induction for semistructured sources .","label":"Background","metadata":{},"score":"51.56818"}
{"text":"Our literature review below is structured as follows .First , we discuss the early , mostly theoretical results on query construction .Then we focus on selective sampling algorithms , which select as the next query one of the unlabeled examples from the working set .","label":"Background","metadata":{},"score":"51.573414"}
{"text":"Nonparametric statistical methods .New York : Wiley .MATH .Jin , R. , & Si , L. ( 2004 ) .A Bayesian approach toward active learning for collaborative filtering .In Proceedings of the 20th conference on uncertainty in artificial intelligence ( pp .","label":"Background","metadata":{},"score":"51.68131"}
{"text":"SIGKDD Explorations Newsletter , 11 ( 1 ) , 10 - 18 .CrossRef .Ho , T. K. , & Basu , M. ( 2002 ) .Complexity measures of supervised classification problems .IEEE Transactions on Pattern Analysis and Machine Intelligence , 24 , 289 - 300 .","label":"Background","metadata":{},"score":"51.77107"}
{"text":"[ 7 ] introduced a Bayesian Semi - Supervised approach termed BGEN ( Bayesian GENeralization ) .The BGEN method trains a kernel classifier using both labeled and unlabeled data .Their example data consisted of expression profiles of wild type and mutant C. elegant embryos and identified enriched genes , with a small subset of genes labeled according to involvement in development of cell lineage .","label":"Background","metadata":{},"score":"52.238365"}
{"text":"CrossRef .Glickman , M. , & Jensen , S. ( 2005 ) .Adaptive paired comparison design .Journal of Statistical Planning and Inference , 127 , 279 - 293 .MathSciNet MATH CrossRef .Groot , P. C. , Birlutiu , A. , & Heskes , T. ( 2010 ) .","label":"Background","metadata":{},"score":"52.29504"}
{"text":"MacKay , D. J. C. ( 1992 ) .Information - based objective functions for active data selection .Neural Computation , 4 , 590 - 604 .CrossRef .MacKay , D. J. C. ( 2002 ) .Information theory , inference & learning algorithms .","label":"Background","metadata":{},"score":"52.345825"}
{"text":"Birlutiu , A. , & Heskes , T. ( 2007 ) .Expectation propagation for rating players in sports competitions .In Proceedings of the 11th European conference on principles and practice of knowledge discovery in databases ( pp .374 - 381 ) .","label":"Background","metadata":{},"score":"52.485863"}
{"text":"London : Chapman & Hall / CRC .Geman , S. , Bienenstock , E. , & Doursat , R. ( 1992 ) .Neural networks and the bias / variance dilemma .Neural Computation , 4 ( 1 ) , 1 - 58 .","label":"Background","metadata":{},"score":"52.517715"}
{"text":"Existing multiview learners are semi - supervised algorithms : they exploit unlabeled examples to boost the accuracy of the classifiers learned in each view by bootstrapping the views from each other .In multi - view learning , an example x is described by a different set of features in each view .","label":"Background","metadata":{},"score":"52.55579"}
{"text":"An evaluation of Naive Bayesian anti - spam filtering .In : Proceedings of the workshop on machine learning in the new information age , 11th European conference on machine learning , pp 9 - 17 .Androutsopoulos I , Paliouras G , Michelakis E ( 2004 )","label":"Background","metadata":{},"score":"52.6616"}
{"text":"In contrast , for wrapper induction we use a testbed of 33 distinct tasks .This imbalance in the number of available datasets requires different presentation styles for the results . - in contrast to typical classification , a major requirement for wrapper induction is to learn ( close to ) 100%-accurate extraction rules from just a handful of examples ( Muslea , 2002 , pages 3 - 6 ) .","label":"Background","metadata":{},"score":"52.780094"}
{"text":"Boutilier , C. ( 2002 ) .A POMDP formulation of preference elicitation problems .In Proceedings of the 18th national conference on artificial intelligence ( pp .239 - 246 ) .Boutilier , C. , Zemel , R. S. , & Marlin , B. ( 2003 ) .","label":"Background","metadata":{},"score":"52.897655"}
{"text":"Rocchio J ( 1971 )Relevance feedback in information retrieval .In : The SMART retrieval system : experiments in automatic document processing , pp 313 - 323 .Prentice Hall Inc. , Englewood Cliffs .Rosenblatt E ( 1988 )The perceptron : a probabilistic model for information storage and organization in the brain .","label":"Background","metadata":{},"score":"52.96561"}
{"text":"In this paper , the implementation of Random Sampling is identical with that of Naive Co - Testing with winner takes all , except that it randomly queries one of the unlabeled examples from the working set .For Query - by - Bagging , the committee of hypotheses is created by repeatedly re - sampling ( with substitution ) the examples in the original training set L. We use a relatively small committee ( i.e. , 10 extraction rules ) because when learning from a handful of examples , re - sampling with replacement leads to just a few distinct training sets .","label":"Background","metadata":{},"score":"53.027893"}
{"text":"CrossRef .Heskes , T. , & de Vries , B. ( 2005 ) .Incremental utility elicitation for adaptive personalization .In Proceedings of the 17th Belgium - Netherlands conference on artificial intelligence ( pp .127 - 134 ) .","label":"Background","metadata":{},"score":"53.046677"}
{"text":"In this domain , the strong view consists of the words that appear on each line , based on which a Naive Bayes text classifier is learned .This weak view defines a class of concepts that is more general than the target concept : all line orderings are possible , even though they are not equally probable .","label":"Background","metadata":{},"score":"53.192226"}
{"text":"Bennett , P. N. ( 2000 ) .Assessing the calibration of naive Bayes ' posterior estimates ( Tech .Rep. CMU - CS-00 - 155 ) .Carnegie Mellon University .Brazdil , P. , Giraud - Carrier , C. , Soares , C. , & Vilalta , R. ( 2009 ) .","label":"Background","metadata":{},"score":"53.250458"}
{"text":"Extracted Text .We focus here on active learning for multi - view domains , in which there are several disjoint subsets offeatures ( views ) , each of which is sufficient to learn the target concept .In this paper we make several contributions .","label":"Background","metadata":{},"score":"53.441734"}
{"text":"Tong , S. , & Koller , D. ( 2000 ) .Active learning for parameter estimation in Bayesian networks .In Advances in Neural Information Processing Systems , Vol . 13 , pp .647 - 653 .232 Active Learning with Multiple Views Tong , S. , & Koller , D. ( 2001 ) .","label":"Background","metadata":{},"score":"53.471798"}
{"text":"Committee - based sampling for training probabilistic classifiers In Proceedings of the Twelfth International Conference on Machine Learning ( ICML-95 ) , 150 - 157 San Francisco , CA .Morgan Kaufman .Fillmore , C. 1988 .The mechanisms of ' ' Construction Grammar ' '","label":"Background","metadata":{},"score":"53.638653"}
{"text":"It generates a large and diverse committee by successively augmenting the original training set with additional sets of artificially - generated examples .More precisely , it generates artificial examples in keeping with the distribution of the instance space ; then it applies the current committee to each such example , and it labels the artificial example with the label that contradicts most of the committee 's predictions .","label":"Background","metadata":{},"score":"53.691708"}
{"text":"In : Proceedings of the 4th annual symposium on document analysis and information retrieval , pp 317 - 332 .Yang Y , Pedersen JP ( 1997 )A comparative study on feature selection in text categorization .In : Proceedings of the 14th international conference on machine learning , pp 412 - 420 .","label":"Background","metadata":{},"score":"53.791046"}
{"text":"Depending on the source of unlabeled examples , there are two main types of sampling algorithms : stream- and pool- based .The uncertainty reduction approach to selective sampling works as follows : first , one uses the labeled examples to learn a classifier ; then the system queries the unlabeled example on which this classifier makes the least confident prediction .","label":"Background","metadata":{},"score":"53.826485"}
{"text":"Journal of Physics A : Mathematical and General , 25 ( 1 ) , 113 - 121 .Yarowsky , D. ( 1995 ) .Unsupervised word sense disambiguation rivaling supervised methods .In Proceedings of the 33rd annual meeting of the Association of Computational Linguistics , pp .","label":"Background","metadata":{},"score":"53.873352"}
{"text":"Aggressive CoTesting learns 100%-accurate rules on 30 of the 33 tasks ; for all these tasks , the extraction rules are learned from at most seven queries .Naive Co - Testing learns 100 % accurate rules on 28 of the 33 tasks .","label":"Background","metadata":{},"score":"53.90603"}
{"text":"[ 2 ] The paper has been cited over 1000 times , and received the 10 years Best Paper Award at the 25th International Conference on Machine Learning ( ICML 2008 ) , a renowned computer science conference .[ 3 ] [ 4 ] .","label":"Background","metadata":{},"score":"53.932224"}
{"text":"In practice , this may raise some serious problems ; for example , consider a hand - writing recognizer that must discriminate between the 10 digits ( Lang & Baum , 1992 ) .In this scenario , an informative query may consist of an image that represents a \" fusion \" of two similarly - looking digits , such as \" 3 \" and \" 5 .","label":"Background","metadata":{},"score":"53.982883"}
{"text":"The advantage of this alternative is the reduced computation costs and reduced time subjects are involved .We validate empirically our approach on three real - world data sets involving the preferences of people .Keywords .Learning preferences Active learning Experimental design Multi - task learning Hierarchical modeling .","label":"Background","metadata":{},"score":"54.056324"}
{"text":"Multi - task preference learning with an application to hearing aid personalization .Neurocomputing , 73 ( 7 - 9 ) , 1177 - 1185 .Bishop , C. M. ( 2006 ) .Pattern recognition and machine learning .Berlin : Springer .","label":"Background","metadata":{},"score":"54.121178"}
{"text":"In Proceedings of the International Conference on Machine Learning , pp .584 - 591 .Muslea , I. ( 2002 ) .Active Learning with Multiple Views .Ph.D. thesis , Department of Computer Science , University of Southern California .","label":"Background","metadata":{},"score":"54.22306"}
{"text":"Conservative Co - Testing is appropriate for noisy domains , where the aggressive strategy may end up querying mostly noisy examples .Creating the output hypothesis also allows the user to choose from a variety of alternatives , such as : - weighted vote : combines the vote of each hypothesis , weighted by the confidence of their respective predictions .","label":"Background","metadata":{},"score":"54.311455"}
{"text":"In Proceedings of the 1992 Machine Learning Workshop on Integrated Learning in Real Domains . , 1992 .[ Mitchell , 1997 ] Mitchell , T.M. , 1997 .Artificial neural networks .In Mitchell , T.M. Machine learning .McGraw - Hill Science / Engineering / Math .","label":"Background","metadata":{},"score":"54.384228"}
{"text":"Second , it applies the learned rules to a large , unlabeled corpus of job postings and creates a database that is populated with the extracted data .Third , by text mining this database , discotex learns to predict the value of each item based on the values of the other fields ; e.g. , it may discover that \" IF the job requires c++ and corba THEN the development platforms include Windows . \"","label":"Background","metadata":{},"score":"54.41176"}
{"text":"Conclusion In this paper we introduce Co - Testing , which is an active learning technique for multi - view learning tasks .This novel approach to active learning is based on the idea of learning from mistakes ; i.e. , Co - Testing queries unlabeled examples on which the views predict a different label ( such contention points are guaranteed to represent mistakes made in one of the views ) .","label":"Background","metadata":{},"score":"54.46116"}
{"text":"IEEE Transactions on Systems , Man and Cybernetics , 6 , 448 - 452 .MathSciNet CrossRef .Tong , S. , & Koller , D. ( 2001 ) .Support vector machine active learning with applications to text classification .Journal of Machine Learning Research , 2 , 45 - 66 . van Hulse , J. , Khoshgoftaar , T. M. , & Napolitano , A. ( 2007 ) .","label":"Background","metadata":{},"score":"54.61662"}
{"text":"This can be done by first detecting the contention points - if any - on which the weak view disagrees with both strong views ; among these , the next query is the one on which the weak view makes the most confident prediction .","label":"Background","metadata":{},"score":"54.709663"}
{"text":"The experimental results show that both the Perceptron and Winnow perform much better when using IG or DF than using odds ratio .It is further demonstrated that when using IG or DF , the classifiers are insensitive to the number of features and the number of training iterations , and not greatly sensitive to the size of training set .","label":"Background","metadata":{},"score":"54.750435"}
{"text":"Berlin : Springer .MATH .Chu , W. , & Ghahramani , Z. ( 2005a ) .Extensions of Gaussian processes for ranking : semi - supervised and active learning .In NIPS workshop on learning to rank .Chu , W. , & Ghahramani , Z. ( 2005b ) .","label":"Background","metadata":{},"score":"54.777477"}
{"text":"Abstract .The performance of two online linear classifiers - the Perceptron and Littlestone 's Winnow - is explored for two anti - spam filtering benchmark corpora - PU1 and Ling - Spam .We study the performance for varying numbers of features , along with three different feature selection methods : information gain ( IG ) , document frequency ( DF ) and odds ratio .","label":"Background","metadata":{},"score":"54.94699"}
{"text":"For example , in the case of correlated views , the hypotheses learned in each view may be so similar that there are no contention points among which to select the next query .In terms of view incompatibility , remember that , for three of the 33 wrapper induction tasks , one of the views was so inaccurate that the Co - Testing could not outperform Random Sampling .","label":"Background","metadata":{},"score":"54.97223"}
{"text":"References .Adomavicius , A. , Sankaranarayanan , R. , Sen , S. , & Tuzhilin , A. ( 2005 ) .Incorporating contextual information in recommender systems using a multidimensional approach .ACM Transactions on Information Systems , 23 ( 1 ) , 103 - 145 .","label":"Background","metadata":{},"score":"55.27349"}
{"text":"Tables 2 , 3 , 4 show the improvements in more detail .The addition of unlabeled samples increases the range from 70 % to 100 % .In CML experiments , adding unlabeled samples increases the minimum accuracy from 0 % to 11.11 % .","label":"Background","metadata":{},"score":"55.551163"}
{"text":"An additional application domain with strong and weak views , wrapper induction , is discussed at length in section 5.2 .The discotex ( Nahm & Mooney , 2000 ) system was designed to extract job titles , salaries , locations , etc from computer science job postings to the newsgroup austin.jobs .","label":"Background","metadata":{},"score":"55.66598"}
{"text":"In contrast , Naive Co - Testing converges in a single query on just four of the 33 tasks , while the other two learners never converge in a single query .For the three tasks on which Aggressive Co - Testing does not learn 100 % accurate rules , the failure is due to the fact that one of the views is significantly less accurate than the other one .","label":"Background","metadata":{},"score":"55.844856"}
{"text":"Schölkopf et al . , 1996 ] Schölkopf , B. , Burges , C. & Vapnik , a .V. , 1996 .Incorporating invariances in support vector .In Proceedings of ICANN , Springer Lecture notes in Computer Science . , 1996 .","label":"Background","metadata":{},"score":"56.091213"}
{"text":"Rank analysis of incomplete block designs , I : the method of paired comparisons .Biometrika , 39 , 324 - 345 .MathSciNet MATH .Brinker , K. ( 2004 ) .Active learning of label ranking functions .In Proceedings of the 27th international conference on machine learning .","label":"Background","metadata":{},"score":"56.096344"}
{"text":"From ranking to intransitive preference learning : rock - paper - scissors and beyond .In Proceedings of the ECML / PKDD workshop on preference learning ( pp .84 - 100 ) .Qin , T. , Liu , T. Y. , Xu , J. , & Li , H. ( 2010 ) .","label":"Background","metadata":{},"score":"56.423203"}
{"text":"The results in Table 3 can be summarized as follows .First of all , no single - view algorithm outperforms Co - Testing in a statistically significant manner on any of the comparison points .Furthermore , except for the comparison with Query - by - Bagging and -Boosting on ad , where the difference in accuracy is statistically insignificant on almost all comparison points , CoTesting clearly outperform all algorithms on all domains .","label":"Background","metadata":{},"score":"56.56122"}
{"text":"Each algorithm starts with two randomly chosen examples and then makes 18 successive queries .The results below can be summarized as follows : for 12 tasks , only the two Co - Testing algorithms learn 100 % accurate rules ; for another 18 tasks , Co - Testing and at least another algorithm reach 100 % accuracy , but Co - Testing requires the smallest number of queries .","label":"Background","metadata":{},"score":"56.596992"}
{"text":"In Proceedings of the Conference on Computational Learing Theory , pp .237 - 245 .Soderland , S. ( 1999 ) .Learning extraction rules for semi - structured and free text .Machine Learning , 34 , 233 - 272 .","label":"Background","metadata":{},"score":"56.691956"}
{"text":"Salojärvi , J. , Puolamäki , K. , Simola , J. , Kovanen , L. , Kojo , I. , & Kaski , S. ( 2005 ) .Inferring relevance from eye movements : Feature extraction ( Tech .Rep. A82 ) .","label":"Background","metadata":{},"score":"56.719223"}
{"text":"In this study we introduce GLAD , a new Semi - Supervised Learning ( SSL ) method for combining independent annotated datasets and unannotated datasets with the aim of identifying more robust sample classifiers .In our method , independent models are developed using subsets of genes for the annotated and unannotated datasets .","label":"Background","metadata":{},"score":"56.883232"}
{"text":"Muslea , I. , Minton , S. , & Knoblock , C. ( 2002a ) .In The 19th International Conference on Machine Learning ( ICML-2002 ) , pp .435 - 442 .Muslea , I. , Minton , S. , & Knoblock , C. ( 2002b ) .","label":"Background","metadata":{},"score":"57.12721"}
{"text":"Littlestone N ( 1988 )Learning quickly when irrelevant attributes abound : a new linear - threshold algorithm .Mach Learn 2(4):285 - 318 .Grove AJ , Littlestone N , Schuurmans D ( 1997 )General convergence results for linear discriminant updates .","label":"Background","metadata":{},"score":"57.316517"}
{"text":"229 Muslea , Minton , & Knoblock Goldberg , P. , Goldman , S. , & Mathias , D. ( 1994 ) .Learning unions of boxes with membership and equivalence queries .In Proceedings of the Conference on Computational Learing Theory , pp .","label":"Background","metadata":{},"score":"57.332764"}
{"text":"Abstract .This paper presents a framework for optimizing the preference learning process .In many real - world applications in which preference learning is involved the available training data is scarce and obtaining labeled training data is expensive .Fortunately in many of the preference learning situations data is available from multiple subjects .","label":"Background","metadata":{},"score":"57.365173"}
{"text":"Cohen W ( 1995 )Fast effective rule induction .In : Machine learning : Proceedings of the 12th international conference , pp 115 - 123 .Drucker H , Wu D , Vapnik VN ( 1999 ) Support vector machines for spam categorization .","label":"Background","metadata":{},"score":"57.50896"}
{"text":"Roark , B. Charniak , E. 1998 .Noun - phrase co - occurrence statistics for semi - automatic semantic lexicon construction In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and COLING-98 ( ACL / COLING-98 ) , 1110 - 1116 .","label":"Background","metadata":{},"score":"57.75218"}
{"text":"In contrast , Uncertainty Sampling ( US ) is applied only on ad and courses because mc4 , which is the base learner for tf , does not provide an estimate of the confidence of its prediction .As there is no known method for randomly sampling from the ib or mc4 version spaces , Query - by - Committee ( QBC ) is not applied to ad and tf .","label":"Background","metadata":{},"score":"57.77027"}
{"text":"As is evident in figure 1 , adding unlabeled samples increased the mean accuracy of the models significantly .This figures shows the improvement of the classification by adding unlabeled samples into the experiments .Figure 2 displays the improvements of the classification accuracies for the population of unique classifiers in each cancer group .","label":"Background","metadata":{},"score":"57.781174"}
{"text":"Artif Intell 97(1 - 2):325 - 343 MATH MathSciNet CrossRef .Bel N , Koster CHA , Villegas M ( 2003 ) Cross - lingual text categorization .In : Proceedings the 7th European conference on digital library , LNCS 2769 , pp 126 - 139 .","label":"Background","metadata":{},"score":"58.089935"}
{"text":"Finally , the version space V SH , L represents the subset of hypotheses in H that are consistent with the training set L. By definition , a passive learning algorithm takes as input a randomly chosen training set L. In contrast , active learning algorithms have the ability to choose the examples in L. That is , they detect the most informative examples in the instance space X and ask the user to label only them ; the examples that are chosen for labeling are called queries .","label":"Background","metadata":{},"score":"58.12355"}
{"text":"Goldman , S. , & Mathias , D. ( 1992 ) .Learning k -term DNF formulas with an incomplete membership oracle .In Proceedings of the Conference on Computational Learing Theory , pp .77 - 84 .Goldman , S. , & Zhou , Y. ( 2000 ) .","label":"Background","metadata":{},"score":"58.362106"}
{"text":"Active - Decorate was successfully used for domains with nominal and numeric features , but it is unclear how it could be applied to domains such as text classification or extraction , where generating the artificial examples may be problematic .208 Active Learning with Multiple Views - Query - by - Bagging and Query - by - Boosting ( Abe & Mamitsuka , 1998 ) create the committee by using the well - known bagging ( Breiman , 1996 ) and boosting ( Schapire , 1990 ) algorithms , respectively .","label":"Background","metadata":{},"score":"58.4427"}
{"text":"Category learning from multi - modality .Neural Computation , 10 ( 5 ) , 1097 - 1117 .Dempster , A. , Laird , N. , & Rubin , D. ( 1977 ) .Maximum likelihood from incomplete data vie the em algorithm .","label":"Background","metadata":{},"score":"58.46104"}
{"text":"Biometrika , 67 , 381 - 388 .MathSciNet MATH CrossRef .Freund , Y. , Shamir , E. , & Tishby , N. ( 1997 ) .Selective sampling using the query by committee algorithm .Machine Learning , 28 ( 2 - 3 ) , 133 - 168 .","label":"Background","metadata":{},"score":"58.589123"}
{"text":"95 % ) obtained in a pair - wise comparison of the various algorithms .These comparisons are performed on the right - most half of each learning curve ( i.e. , towards convergence ) .The best way to explain the results in Table 3 is via examples : the results of comparing Naive Co - Testing and Random Sampling on ad appear in the first three columns of the first row .","label":"Background","metadata":{},"score":"58.65455"}
{"text":"A case frame learning method for Japanese polysemous verbs In Papers from the 1995 AAAI Symposium on the Representation and Acquisition of Lexical Knowledge : Polysemy , Ambiguity , and Generativity , 45 - 50 Stanford , CA .Hastings , P. 1996 .","label":"Background","metadata":{},"score":"58.68634"}
{"text":"Both authors read and approved the final manuscript .Authors ' Affiliations .Exagen Diagnostics , Inc. Houston .Department of Electrical and Computer Engineering , Texas A&M University .References .Schena M , Shalon D , Davis RW , Brown PO : Quantitative monitoring of gene expression patterns with a complementary DNA microarray .","label":"Background","metadata":{},"score":"59.05458"}
{"text":"Journal of the Acoustical Society of America , 122 ( 2 ) , 1150 - 1164 .CrossRef .Arens , R. ( 2008 ) .Learning SVM ranking function from user feedback using document metadata and active learning in the biomedical domain .","label":"Background","metadata":{},"score":"59.151985"}
{"text":"Shipp MA , et al . :Diffuse large B - cell lymphoma outcome prediction by gene expression profiling and supervised machine learning .Nature Medicine 2002 , 8 : 68 - 74 .View Article PubMed .Savage KJ , et al .","label":"Background","metadata":{},"score":"59.24286"}
{"text":"Ninth International Conference on Information and Knowledge Management ( CIKM-2000 ) 2000 , 86 - 93 .Li T , Zhu S , Li Q , Ogihara M : Gene Functional Classification by Semi - supervised Learning from heterogeneous data .Proceedings of The 18th Annual ACM Symposium on Applied Computing ( SAC 2003)-Bioinformatics Track 2003 , 78 - 82 .","label":"Background","metadata":{},"score":"59.438046"}
{"text":"If available , labels were removed from one of the component datasets , thus creating a combined dataset with both labeled and unlabeled subsets .All datasets were produced using Affymetrix GeneChips , and in two cases the labeled and unlabeled datasets were collected with different Affymetrix GeneChips .","label":"Background","metadata":{},"score":"59.50436"}
{"text":"Morgan Kaufman .Oates , T. , Eyler - Walker , Z. , Cohen , P. 1999 .Using syntax to learn semantics : an experiment in language acquisition with a mobile robot 99 - 35 , University of Massachusetts , Computer Science Department .","label":"Background","metadata":{},"score":"59.559784"}
{"text":"Nahm and Mooney ( 2000 ) show that the mined rules improve the extraction accuracy by capturing information that complements the rapier extraction rules .Another domain with strong and weak views is presented by Kushmerick et al .( 2001 ) .","label":"Background","metadata":{},"score":"59.690617"}
{"text":"Springer - Verlag .Thompson , C. A. , Califf , M. E. , Mooney , R. J. 1999 .Active learning for natural language parsing and information extraction In Proceedings of the Sixteenth International Conference on Machine Learning ( ICML-99 ) , 406 - 414 Bled , Slovenia .","label":"Background","metadata":{},"score":"59.84784"}
{"text":"4.1 The Family of Algorithms Table 1 provides a formal description on the Co - Testing family of algorithms .Co- Testing algorithms work as follows : first , they learn the classifiers h1 , h2 , . . ., hk by applying the algorithm L to the projection of the examples in L onto each view .","label":"Background","metadata":{},"score":"59.87384"}
{"text":"Finally , on tf the algorithms start with 110 randomly chosen examples and make 20 queries after each of the 100 learning episodes .Figures 1 and 2 display the learning curves of the various algorithms on ad , tf , and course .","label":"Background","metadata":{},"score":"59.9015"}
{"text":"San Mateo , CA : Morgan Kaufmann .[ Sassano , 2003 ] Sassano , M. , 2003 .Virtual Examples for Text Classification with Support Vector Machines .In Proceedings of the 2003 conference on Empirical methods in natural language processing . , 2003 .","label":"Background","metadata":{},"score":"60.01367"}
{"text":"On the remaining 15 tasks , wien requires between 25 and 90 examples4 to learn the correct rule .For the same 15 tasks , both Aggressive and Naive Co - Testing learn 100 % accurate rules based on at most eight examples ( two random plus at most six queries ) .","label":"Background","metadata":{},"score":"60.05184"}
{"text":"5.1.1 The Views used by Co - Testing We applied Co - Testing to three real - world classification domains for which there is a natural , intuitive way to create two views : - ad ( Kushmerick , 1999 ) is a classification problem with two classes , 1500 attributes , and 3279 examples .","label":"Background","metadata":{},"score":"60.184692"}
{"text":"Data mining using MLC++ , a machine learning library in C++ .International Journal of AI Tools , 6(4 ) , 537 - 566 .Kushmerick , N. ( 1999 ) .Learning to remove internet advertisements .In Proceedings of the Third International Conference on Autonomous Agents ( Agents-99 ) , pp .","label":"Background","metadata":{},"score":"60.25946"}
{"text":"In the Bayesian framework , one can create the committee by sampling classifiers according to their posterior distributions ; that is , the better a hypothesis explains the training data , the more likely it is to be sampled .The main limitation of Query - by - Committee is that it can be applied only to base learners for which it is feasible to randomly sample hypotheses from the version space .","label":"Background","metadata":{},"score":"60.333336"}
{"text":"In Proceedings of the 12th International Conference on Machine Learning , pp .150 - 157 .Dasgupta , S. , Littman , M. , & McAllester , D. ( 2001 ) .PAC generalization bounds for cotraining .In Neural Information Processing Systems , pp .","label":"Background","metadata":{},"score":"60.367306"}
{"text":"It follows that rules such as R1 and R2 represent descriptions of the same concept ( i.e. , beginning of phone number ) that are learned in two different views : the sequences of tokens that precede and follow the beginning of the item , respectively .","label":"Background","metadata":{},"score":"60.679176"}
{"text":"Tech . rep .YALEU / DCS / TR-1019 , Yale University .Angluin , D. , Krikis , M. , Sloan , R. , & Turan , G. ( 1997 ) .Malicious omissions and errors in answers to membership queries .","label":"Background","metadata":{},"score":"60.73987"}
{"text":"Androutsopoulos I , Koutsias J , Chandrinos KV , Spyropoulos CD ( 2000 )An experimental comparison of Naive Bayesian and keyword - based anti - spam filtering with encrypted personal e - mail messages .In : Proceedings of the 23rd annual international ACM SIGIR conference on research and development in information retrieval , pp 160 - 167 .","label":"Background","metadata":{},"score":"60.884712"}
{"text":"Information , prediction , and query by committee .In Advances in neural information processing systems ( NIPS ) ( pp .483 - 490 ) .Hall , M. , Frank , E. , Holmes , G. , Pfahringer , B. , Reutemann , P. , & Witten , I. H. ( 2009 ) .","label":"Background","metadata":{},"score":"60.894257"}
{"text":"Technical Report Report UBE - SOL , IMCR-008 , Internet Mail Consortium .Androutsopoulos I , Paliouras G , Karkaletsis V , Sakkis G , Spyropoulos CD , Stamatopoulos P ( 2000 ) Learning to filter spam e - mail : a comparison of a Naive Bayesian and a memory - based approach .","label":"Background","metadata":{},"score":"61.677433"}
{"text":"5.1.3 The Experimental Results The learners ' performance is evaluated based on 10-fold , stratified cross validation .On ad , each algorithm starts with 150 randomly chosen examples and makes 10 queries after each of the 40 learning episodes , for a total of 550 labeled examples .","label":"Background","metadata":{},"score":"61.69934"}
{"text":"In Proceedings of the 25th international conference on machine learning ( pp .208 - 215 ) .CrossRef .Doyle , J. ( 2004 ) .Prospects for preferences .Computational Intelligence , 20 ( 2 ) , 111 - 136 .","label":"Background","metadata":{},"score":"61.9784"}
{"text":"In Advances in neural information processing systems ( pp .640 - 646 ) .Tversky , A. ( 1998 ) .Preference , belief , and similarity .Cambridge : MIT Press .Xu , Z. , Kersting , K. , & Joachims , T. ( 2010 ) .","label":"Background","metadata":{},"score":"62.10697"}
{"text":"Hidalgo JMG ( 2002 )Evaluating cost - sensitive unsolicited bulk email categorization .In : Proceedings of ACM symposium on applied computing , pp 615 - 620 .Yang L , Xiaoping D , Ping L , Zhihui H , Chen G , Huanlin L ( 2002 )","label":"Background","metadata":{},"score":"62.17209"}
{"text":"231 Muslea , Minton , & Knoblock Reddy , C. , & Tadepalli , P. ( 1997 ) .Learning horn definitions with equivalence and membership queries .In Proceedings of the 7th International Workshop on Inductive Logic Programming , pp .","label":"Background","metadata":{},"score":"62.260643"}
{"text":"As many available datasets will not have the desired annotation for any samples , this method extends the usability of the limited number of adequately annotated microarray datasets .Methods .Datasets .We conducted three experiments , each addressing a different cancer diagnostic problem : ALL / AML differential diagnosis , prediction of response to imatinib in CML , and prediction of outcome in DLBCL .","label":"Background","metadata":{},"score":"62.392815"}
{"text":"In Proceedings of the 19th annual conference on uncertainty in artificial intelligence ( pp .98 - 106 ) .Boyd , S. , & Vandenberghe , L. ( 2004 ) .Convex optimization .Cambridge : Cambridge University Press .MATH .","label":"Background","metadata":{},"score":"62.585663"}
{"text":"Lawrence Erlbaum , Hillsdale , NJ .Collins , M. Singer , Y. 1999 .Unsupervised models for named entity classification In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Very Large Corpora ( EMNLP / VLC-99 ) University of Maryland .","label":"Background","metadata":{},"score":"62.60694"}
{"text":"Even though the weak views are inadequate for learning the target concept , they can be exploited by Co - Testing both in the SelectQuery ( ) and CreateOutputHypothesis ( ) functions .In particular , weak views are extremely useful for domains that have only two strong views : - the weak view can be used in CreateOutputHypothesis ( ) as a tie - breaker when the two strong views predict a different label . -","label":"Background","metadata":{},"score":"62.6416"}
{"text":", hk to all unlabeled examples in U and create the set of contention points , which consists of all unlabeled examples for which at least two of these hypotheses predict a different label .Finally , they query one of the contention points and then repeat the whole process for a number of iterations .","label":"Background","metadata":{},"score":"62.652428"}
{"text":"Blood 2003 , 102 : 3871 - 9 .View Article PubMed .Copyright .© Harris and Ghaffari .This article is published under license to BioMed Central Ltd. Co- training .Co- training is a machine learning algorithm used when there are only small amounts of labeled data and large amounts of unlabeled data .","label":"Background","metadata":{},"score":"62.737007"}
{"text":"The cluster separation term is given by a modified ratio of the inter - cluster distance to the mean cluster size .The consistent proportion term , is defined as the RMS difference between the sorted actual and expected class priors .","label":"Background","metadata":{},"score":"63.11254"}
{"text":"5.2.1 The Views used by Co - Testing Consider the illustrative task of extracting phone numbers from documents similar to the fragment in Figure 3 . R2 ignores everything until it finds \" Cuisine \" and then , again , skips to the first number between parentheses .","label":"Background","metadata":{},"score":"63.182983"}
{"text":"Selective sampling using the query by committee algorithm .Machine Learning , 28 , 133 - 168 .Ghani , R. ( 2002 ) .Combining labeled and unlabeled data for multiclass text classification .In Proceedings of the 19th International Conference on Machine Learning ( ICML-2002 ) , pp .","label":"Background","metadata":{},"score":"63.325"}
{"text":"In : Proceedings of 12th Chinese computer society conference on network and data communication , pp 211 - 215 .Sahami M , Dumais S , Heckerman D , Horvitz E ( 1998 )A Bayesian approach to filtering junk e - mail .","label":"Background","metadata":{},"score":"63.463486"}
{"text":"We then use a set of hardness measures to understand why some instances are harder to classify correctly than others .We find that class overlap is a principal contributor to instance hardness .We seek to integrate this information into the training process to alleviate the effects of class overlap and present ways that instance hardness can be used to improve learning .","label":"Background","metadata":{},"score":"63.473648"}
{"text":": Semi - supervised analysis of gene expression profiles for lineage - specific development in the Caenorhabditis elegans embryo .Bioinformatics 2006 , 22 ( 14 ) : e417 - 423 .View Article PubMed .Armstrong SA , et al .","label":"Background","metadata":{},"score":"63.725647"}
{"text":"CH and NG were employees of Exagen Diagnostics during the course of this research and the preparation of this manuscript .Additionally , CH owns stock in Exagen Diagnostics .Authors ' contributions .CH devised and implemented the GLAD algorithm , and contributed to the final preparation of the manuscript .","label":"Background","metadata":{},"score":"63.780003"}
{"text":"Less is more : Active learning with support vector machines .In Proceedings of the 17th International Conference on Machine Learning ( ICML2000 ) , pp .839 - 846 .Seung , H. S. , Opper , M. , & Sompolinski , H. ( 1992 ) .","label":"Background","metadata":{},"score":"63.937244"}
{"text":"In this paper we are concerned mostly with examples that are represented as feature vectors that store the values of the various attributes or features that describe the example .The concept to be learned is called the target concept , and it can be seen as a function c : X !","label":"Background","metadata":{},"score":"63.981255"}
{"text":"Conclusion .In this study we proposed a new technique for concurrently mining labeled and unlabeled datasets .This method supplements standard supervised learning with clustering of data lacking clinical annotation to estimate the predictive power of gene subsets .The performance of our algorithm was evaluated in comparison with supervised learning only on microarray data from three different cancer types .","label":"Background","metadata":{},"score":"64.366615"}
{"text":"The notationh x , c(x)i denotes such a training example .The symbol L is used to denote the set of labeled training examples ( also known as the training set ) .Given a training set L for the target concept c , an inductive learning algorithm L searches for a function h : X !","label":"Background","metadata":{},"score":"64.74533"}
{"text":"- the output hypothesis is the rule learned in the view that makes the fewest mistakes over the allowed number of queries ( i.e. , winner - takes - all ) .Given the additional , content - based view , we can also implement an aggressive version of Co - Testing for wrapper induction : - the contention points are , again , the unlabeled examples on which the rules learned in the strong views do not extract the same string .","label":"Background","metadata":{},"score":"65.054276"}
{"text":"The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements , either expressed or implied , of any of the above organizations or any person connected with them .","label":"Background","metadata":{},"score":"65.082115"}
{"text":"For example , researchers considered learning with : - incomplete queries , for which the query 's answer may be \" I do n't know . \" 3.2 Selective Sampling Selective sampling represents an alternative active learning approach .It typically applies to classification tasks in which the learner has access to a large number of unlabeled examples .","label":"Background","metadata":{},"score":"65.61618"}
{"text":"In this scenario , the rapier rules represent the strong view because they are sufficient for extracting the data of interest .In contrast , the mined rules represent the weak view because they can not be learned or used by themselves .","label":"Background","metadata":{},"score":"65.90091"}
{"text":"Declarations .Acknowledgements .We thank Exagen Diagnostics for support in conducting this research and presenting these results .This article has been published as part of BMC Genomics Volume 9 Supplement 2 , 2008 : IEEE 7 th International Conference on Bioinformatics and Bioengineering at Harvard Medical School .","label":"Background","metadata":{},"score":"66.03729"}
{"text":"Seung , H. S. , Opper , M. , & Sompolinsky , H. ( 1992 ) .Query by Committee .In Proceedings of the 5th annual workshop on computational learning theory ( pp .287 - 294 ) .Thrun , S. ( 1995 ) .","label":"Background","metadata":{},"score":"66.533554"}
{"text":"On courses , we have applied both Co - Training and Co - EM in conjunction with the Naive Bayes base learner .Both these multi - view learners reach their maximum accuracy ( close to 95 % ) based on solely 12 labeled and 933 unlabeled examples , after which their 221 Muslea , Minton , & Knoblock Name : Gino 's .","label":"Background","metadata":{},"score":"66.5972"}
{"text":"In Proceedings of the Conference on Computational Learing Theory , pp .175 - 186 .Amoth , T. , Cull , P. , & Tadepalli , P. ( 1999 ) .Exact learning of unordered tree patterns from queries .In Proceedings of the Conference on Computational Learing Theory , pp .","label":"Background","metadata":{},"score":"67.188286"}
{"text":"TREC 11 experiments at CAS - ICT : filtering and web .In : Proceedings of the 11th text retrieval conference , pp 105 - 115 Affiliated with .Affiliated with .Abstract .The growing body of DNA microarray data has the potential to advance our understanding of the molecular basis of disease .","label":"Background","metadata":{},"score":"67.59044"}
{"text":"References .Vaughan Nichols SJ ( 2003 )Saving private e - mail .IEEE Spectr 40(8):40 - 44 CrossRef .Whitworth B , Whitworth E ( 2004 )Spam and the social - technical gap .IEEE Comput 37(10):37 - 45 .","label":"Background","metadata":{},"score":"68.35646"}
{"text":"In this case , V1 uses features specific to the shift - reduce parser : the elements in the input list and the partial trees in the stack .V2 consists of features specific to the Japanese tree given as input .","label":"Background","metadata":{},"score":"68.46311"}
{"text":"Inductive logic programming for corpus - based acquisition of semantic lexicons In Proceedings of 2nd Learning Language in Logic ( LLL )Workshop Lisbon , Portugal .Siklossy , L. 1972 .Natural language learning by computer In Simon , H. A. Siklossy , L. , Representation and meaning : Experiments with Information Processsing Systems .","label":"Background","metadata":{},"score":"68.62718"}
{"text":"Nature Genetics 2002 , 30 : 41 - 47 .View Article PubMed .Frank O , et al . :Gene expression signature of primary imatinib - resistant chronic myeloid leukemia patients .Leukemia 2006 , 20 : 1400 - 7 .","label":"Background","metadata":{},"score":"69.09914"}
{"text":"Yong ASM , et al .: Molecular profiling of CD34 + cells identifies low expression of CD7 , along with high expression of proteinase 3 or elastase , as predictors of longer survival in patients with CML .Blood 2006 , 107 : 205 - 12 .","label":"Background","metadata":{},"score":"69.7679"}
{"text":"Available under Open Access This content is freely available online to anyone , anywhere at any time .Abstract .Hybrid learning methods use theoretical knowledge of a domain and a set of classified examples to develop a method for classification .","label":"Background","metadata":{},"score":"70.115974"}
{"text":"Acknowledgments This research is based upon work supported in part by the National Science Foundation under Award No . IIS-0324955 and grant number 0090978 , in part by the Defense Advanced Research Projects Agency ( DARPA ) , through the Department of the Interior , NBC , Acquisition Services Division , under Contract No . NBCHD030010 , and in part by the Air Force 227 Muslea , Minton , & Knoblock Office of Scientific Research under grant number FA9550 - 04 - 1 - 0105 .","label":"Background","metadata":{},"score":"71.378105"}
{"text":"Select the SEEK icon to attempt to find the referenced article .If it does not appear to be in cogprints you will be forwarded to the paracite service .Poorly formated references will probably not work .[ Abu¬Mostafa , 1995 ] Abu - Mostafa , Y.S. , 1995 .","label":"Background","metadata":{},"score":"71.74446"}
{"text":", xk , ?i. For any example x , Vi(x ) denotes the descriptions xi of x in Vi .Similarly , Vi(L ) consists of the descriptions in Vi of all the examples in L. 205 Muslea , Minton , & Knoblock 3 .","label":"Background","metadata":{},"score":"71.79243"}
{"text":"For each experiment , we did the following : .Iterate GLAD algorithm on labeled training data only .Iterate GLAD algorithm on labeled and unlabeled training data .Compare model accuracy on test data across generated populations of models .In these experiments , GLAD was run for 100 iterations with a population size of 5000 , and a subset size of 3 features .","label":"Background","metadata":{},"score":"73.624374"}
{"text":"By convention , the rightmost point on the X axis , which is labeled \" 19 queries \" , represents the number of tasks that require more than the allowed 18 queries to learn a 100 % accurate rule .Query - by - Bagging need hundreds of queries to learn the correct rules , the histograms would become difficult to read if the entire X axis were shown .","label":"Background","metadata":{},"score":"73.72485"}
{"text":"Figure 4 shows the aggregate performance of the four algorithms over the 33 tasks .In each of the six graphs , the X axis shows the number of queries made by the algorithm , while the Y axis shows the number of tasks for which a 100 % accurate rule was learned based on exactly X queries .","label":"Background","metadata":{},"score":"73.8766"}
{"text":"Technical report 2004/2 , NCSR \" Demokritos \" .Schneider K ( 2003 )A comparison of event models for Naive Bayes anti - spam e - mail filtering .In : Proceedings of the 10th conference of the European chapter of the association for computational linguistics , pp 307 - 314 .","label":"Background","metadata":{},"score":"75.73178"}
{"text":"View Article PubMed .Eisen MB , Spellman PT , Brown PO , Botstein D : Cluster analysis and display of genome - wide expression patterns .PNAS 1998 , 95 : 14863 - 14868 .View Article PubMed .Golub TR , et al .","label":"Background","metadata":{},"score":"76.64992"}
{"text":"Table 3 summarizes the statistical significance results ( t - test confidence of at least 1 .In a preliminary experiment , we have also ran the algorithms of the individual views .The results on V1 and V2 were either worse then those on V1 [ V2 or the differences were statistically insignificant .","label":"Background","metadata":{},"score":"77.84213"}
{"text":"html ) to add more citations .Yarowsky wrote an early paper describing how to learn to disambiguate word senses .It makes the assumption that each occurance of a word ( e.g. , \" bank \" ) in a document has the same meaning ( e.g. , river bank or financial bank ) .","label":"Background","metadata":{},"score":"78.582886"}
{"text":"The analysis and promising experimental results indicate that the Perceptron and Winnow are two very competitive classifiers for anti - spam filtering .Keywords .Online linear classifier Perceptron Winnow Anti - spam filtering .This work was carried out while the first author was visiting Dublin City University supported by a China State Scholarship .","label":"Background","metadata":{},"score":"82.72928"}
{"text":"GenBank Gene Accession Numbers were used to generate the common features .Table 1 provides additional details on these datasets .Demonstrations .For this study we implemented a GLAD algorithm as a wrapper technique for feature selection .A Genetic Algorithm ( GA ) is used for generating a population of relevant feature subsets .","label":"Background","metadata":{},"score":"83.80272"}
{"text":"The view V1 consists of all textual features that describe the image ; e.g. , 1-grams and 2-grams from the caption , from the url of the page that contains the image , from the url of the page the image points to , etc .","label":"Background","metadata":{},"score":"87.19432"}
{"text":"In the first evaluation of GLAD performance , test data classification accuracy was compared between models identified using only labeled data and models using both labeled and unlabeled data .Figure 1 shows the results for three cancer groups .The top 5 % of the model populations were used to generate these histograms .","label":"Background","metadata":{},"score":"87.361374"}
{"text":"PhD Thesis .Sydney , Australia : University of Technology Sydney .","label":"Background","metadata":{},"score":"94.18596"}
{"text":"For example , a page that contains a list of 100 names , all labeled , represents a single labeled example .In contrast , for stalker the same labeled document represents 100 distinct labeled examples .In order to compare the wien and stalker results , we convert the wien data to stalker - like data by multiplying the number of labeled wien pages by the average number of item occurrences in each page .","label":"Background","metadata":{},"score":"100.779884"}
