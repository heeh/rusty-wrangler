{"text":"This has led to concer ... \" .Statistical parsers trained and tested on the Penn Wall Street Journal ( WSJ ) treebank have shown vast improvements over the last 10 years .Much of this improvement , however , is based upon an ever - increasing number of features to be trained on ( typically ) the WSJ treebank data .","label":"CompareOrContrast","metadata":{},"score":"40.686684"}{"text":"In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"CompareOrContrast","metadata":{},"score":"42.741043"}{"text":"In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"CompareOrContrast","metadata":{},"score":"42.741043"}{"text":"Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"CompareOrContrast","metadata":{},"score":"43.95389"}{"text":"Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"CompareOrContrast","metadata":{},"score":"43.95389"}{"text":"Thus , within a longitudinal patient record , one expects to observe heavy redundancy .In this paper , we ask three research questions : ( i ) How can redundancy be quantified in large - scale text corpora ?( ii )","label":"CompareOrContrast","metadata":{},"score":"45.16693"}{"text":"200 characters per pasting .B. 3 sites with both kinds of outputs .The Stanford NLP Group 's java - based Parser can compute and report a dependency equivalent of its constituent - structure - based parses .Their set of dependency relations is becoming widely known and is described here .","label":"CompareOrContrast","metadata":{},"score":"45.510296"}{"text":"This increase has been driven by the availability of treebanks in a wide variety of languages - due in large part to the CoNLL shared tasks - as well as the straightforward mechanisms by w ... \" .There has been a rapid increase in the volume of research on data - driven dependency parsers in the past five years .","label":"CompareOrContrast","metadata":{},"score":"46.146416"}{"text":"In this article , our aim is to take a step back and analyze the progress that has been made through an analysis of the two predominant paradigms for data - driven dependency parsing , which are often called graph - based and transition - based dependency parsing .","label":"CompareOrContrast","metadata":{},"score":"46.45405"}{"text":"Such worries have merit .The standard \" Charniak parser \" checks in at a labeled precisionrecall f - measure of 89.7 % on the Penn WSJ test set , but only 82.9 % on the test set from the Brown treebank corpus .","label":"CompareOrContrast","metadata":{},"score":"46.46475"}{"text":"The system consists of two components : an unlabeled dependency parser using Gibbs sampling which can incorporate sentence - level ( global ) features as well as token - leve ... \" .In this paper , we describe a two - stage multilingual dependency parser used for the multilingual track of the CoNLL 2007 shared task .","label":"CompareOrContrast","metadata":{},"score":"46.61268"}{"text":"The system consists of two components : an unlabeled dependency parser using Gibbs sampling which can incorporate sentence - level ( global ) features as well as token - leve ... \" .In this paper , we describe a two - stage multilingual dependency parser used for the multilingual track of the CoNLL 2007 shared task .","label":"CompareOrContrast","metadata":{},"score":"46.61268"}{"text":"We show that the automatically induced latent variable grammars of Petrov et al .2006 vary widely in their underlying representations , depending on their EM initialization point .We use this to our advantage , combining multiple automatically learned grammars into an unweighted product model , which gives significantly improved performance over state - of - the - art individual grammars .","label":"CompareOrContrast","metadata":{},"score":"46.842113"}{"text":"We observe redundancy levels of about 30 % and non - standard distribution of both words and concepts .We measure the impact of redundancy on two standard text - mining applications : collocation identification and topic modeling .We compare the results of these methods on synthetic data with controlled levels of redundancy and observe significant performance variation .","label":"CompareOrContrast","metadata":{},"score":"46.90838"}{"text":"For example , our clus- tering algorithm grouped first names in one group and measurements in another .We then added the cluster membership as a lexical feature to the parser .None of the resulting features helped adaptation .3.2Diversity Training diversity may be an effective source for adaptation .","label":"CompareOrContrast","metadata":{},"score":"47.028"}{"text":"We tried a number of criteria to weigh sentences without suc- cess , including sentence length and number of verbs .Next , we trained a discriminative model on the pro- vided unlabeled data to predict the domain of each sentence based on POS n - grams in the sentence .","label":"CompareOrContrast","metadata":{},"score":"47.628838"}{"text":"Previous sentence segmentation systems have typically been very local , using low - level prosodic and lexical features to independently decide whether or not to segment at each word boundary position .In this work , we leverage global syntactic information from a syn- tactic parser , which is better able to capture long distance depen- dencies .","label":"CompareOrContrast","metadata":{},"score":"47.703484"}{"text":"However , our results were obtained without adap- tation .Given our position in the ranking , this sug- gests that no team was able to significantly improve performance on either test domain beyond that of a state - of - the - art parser .","label":"CompareOrContrast","metadata":{},"score":"48.201763"}{"text":"It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure .While previous work has focused primarily on English , we extend these results to other languages along two dimensions .","label":"CompareOrContrast","metadata":{},"score":"48.457672"}{"text":"It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure .While previous work has focused primarily on English , we extend these results to other languages along two dimensions .","label":"CompareOrContrast","metadata":{},"score":"48.457672"}{"text":"Using these observations , we present an integrated system based on a stacking learning framework and show that such a system can learn to overcome the shortcomings of each non - integrated system . \" ...Martins et al .( 2008 ) presented what to the best of our knowledge still ranks as the best overall result on the CONLL - X Shared Task datasets .","label":"CompareOrContrast","metadata":{},"score":"48.480064"}{"text":"Our formulation uses a factorization analogous to the standard dynamic programs for parsing .In particular , it allows one to efficiently learn a model which discriminates among the entire space of parse trees , as opposed to reranking the top few candidates .","label":"CompareOrContrast","metadata":{},"score":"48.88976"}{"text":"Unfortunately the sentence in Figure 1(b ) is highly unusual in its amount of dependency conservation .To get a feel for the typical case , we used off - the - shelf parsers ( McDonald et al . , 2005 ) for E .. by Ivan Titov , James Henderson - IN PROCEEDINGS OF CONLL-2007 SHARED TASK .","label":"CompareOrContrast","metadata":{},"score":"49.360542"}{"text":"Unfortunately the sentence in Figure 1(b ) is highly unusual in its amount of dependency conservation .To get a feel for the typical case , we used off - the - shelf parsers ( McDonald et al . , 2005 ) for E .. by Ivan Titov , James Henderson - IN PROCEEDINGS OF CONLL-2007 SHARED TASK .","label":"CompareOrContrast","metadata":{},"score":"49.360542"}{"text":"We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes .We show how first - order logical constraints can be handled efficiently , even though the corresponding subproblems are no longer combinatorial , and report experiments in dependency parsing , with state - of - the - art results . \" ...","label":"CompareOrContrast","metadata":{},"score":"49.38115"}{"text":"In the Penn Treebank , transitions between upper- and lowercase tokens tend to align with the boundaries of base ( English ) noun phrases .Such signals can be used as partial bracketing constraints to train a grammar inducer : in our experiments , directed dependency accuracy increased by 2.2 % ( average over 14 languages having case information ) .","label":"CompareOrContrast","metadata":{},"score":"49.47323"}{"text":"Current statistical parsers tend to perform well only on their training domain and nearby genres .While strong performance on a few related domains is sufficient for many situations , it is advantageous for parsers to be able to generalize to a wide variety of domains .","label":"CompareOrContrast","metadata":{},"score":"49.527126"}{"text":"Current statistical parsers tend to perform well only on their training domain and nearby genres .While strong performance on a few related domains is sufficient for many situations , it is advantageous for parsers to be able to generalize to a wide variety of domains .","label":"CompareOrContrast","metadata":{},"score":"49.527126"}{"text":"Additionally , methods based only on key lexical dependencies have been shown to be very effective in choosing between valid syntactic forms [ 1]. \" ...This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm .","label":"CompareOrContrast","metadata":{},"score":"49.57259"}{"text":"Our approach can significantly advance the state - of - the - art pars - ing accuracy on two widely used target tree - banks ( Penn Chinese Treebank 5.1 and 6.0 ) using the Chinese Dependency Treebank as the source treebank .","label":"CompareOrContrast","metadata":{},"score":"49.71572"}{"text":"In this paper , we propose a machine learning algorithm for shallow semantic parsing , extending the work of Gildea and Jurafsky ( 2002 ) , Surdeanu et al .( 2003 ) and others .Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers .","label":"CompareOrContrast","metadata":{},"score":"49.979538"}{"text":"In this paper , we propose a machine learning algorithm for shallow semantic parsing , extending the work of Gildea and Jurafsky ( 2002 ) , Surdeanu et al .( 2003 ) and others .Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers .","label":"CompareOrContrast","metadata":{},"score":"49.979538"}{"text":"Unlike existing preordering models , we train feature - rich discriminative classifiers that directly predict the target - side word order .Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long - distance reorderings using the structure of the parse tree , while utilizing a discriminative model with a rich set of features , including lexical features .","label":"CompareOrContrast","metadata":{},"score":"50.746136"}{"text":"These can be searched by word , phrase , or subtree phrasal configurations .Clipslogo The latest incarnation of the Memory Based Tagger and Timbl learning software provides a shallow parser demo trained on either the Wall Street Journal corpus ( for general English ) or on a bio - medical corpus .","label":"CompareOrContrast","metadata":{},"score":"50.96032"}{"text":"We study this problem as a new task - multiple source parser adaptation .Our system trains on corpora from many different domains .It learns not only statistics of those domains but quantitative measures of domain differences and how those differences affect parsing accuracy .","label":"CompareOrContrast","metadata":{},"score":"51.12209"}{"text":"In particular we note the effects of two comparatively recent techniques for parser improvement .Then a reranking phase uses more detailed features , features which would ( mostly ) be ... . \" ...We formulate dependency parsing as a graphical model with the novel ingredient of global constraints .","label":"CompareOrContrast","metadata":{},"score":"51.30301"}{"text":".. our corpus .The second question is how one can estimate NLP systems ' performance when gold standard on the test data does not exist .Our experiments show that the predicted scores are close to the real scores when tested on the CTB data .","label":"CompareOrContrast","metadata":{},"score":"51.343605"}{"text":"Mareček and Zabokrtsk´y , 2011 , inter alia ) . \" ...We present a novel algorithm for multilingual dependency parsing that uses annotations from a diverse set of source languages to parse a new unannotated language .Our motivation is to broaden the advantages of multilingual learning to languages that exhibit significant differences from existing reso ... \" .","label":"CompareOrContrast","metadata":{},"score":"51.378143"}{"text":"In addition , our discriminative approach integrally admits features beyond local tree configurations .We present a multi - scale training method along with an efficient CKY - style dynamic program .On a variety of domains and languages , this method produces the best published parsing accuracies with the smallest reported grammars .","label":"CompareOrContrast","metadata":{},"score":"51.55024"}{"text":"The second question is how one can estimate NLP systems ' performance when gold standard on the test data does not exist .To answer the question , we extend the parsing prediction model in ( Ravi et al . , 2008 ) to provide prediction for word segmentation and POS tagging as well .","label":"CompareOrContrast","metadata":{},"score":"51.573067"}{"text":"This indicates that most of the loss comes from missing these edges .The primary problem for nouns is the difference between structures in each domain . tion guidelines for the Penn Treebank flattened noun phrases to simplify annotation ( Marcus et al . , 1993 ) , so there is no complex structure to NPs . K¨ ubler ( 2006 ) showed that it is difficult to compare the Penn Treebank to other treebanks with more com- plexnounstructures , suchasBIO.ConsidertheWSJ phrase \" the New York State Insurance Department \" .","label":"CompareOrContrast","metadata":{},"score":"51.863823"}{"text":"Finally , we present multilingual experiments which show that parsing with hierarchical state - splitting is fast and accurate in multiple languages and domains , even without any language - specific tuning .This work describes systems for detecting semantic categories present in news video .","label":"CompareOrContrast","metadata":{},"score":"51.959873"}{"text":"However , most ... \" .It is well known that parsing accuracy suffers when a model is applied to out - of - domain data .It is also known that the most beneficial data to parse a given domain is data that matches the domain ( Sekine , 1997 ; Gildea , 2001 ) .","label":"CompareOrContrast","metadata":{},"score":"51.994743"}{"text":"In contrast to annotation projection approaches ( Yarowsky et al . , 2001 ; Hwa et al . , 2005 ; Ganchev et al ., 2009 ; Spreyer and Kuhn , 2009 ) , delexicalized transfer methods do not rely on ... . \" ...","label":"CompareOrContrast","metadata":{},"score":"52.1521"}{"text":"Many studies have indeed shown that cross - domain learned corpora yield poor language models [ 35].The field of domain adaptation attempts to compensate for the poor quality of cross - domain data , by adding carefully picked text from other domains [ 36,37 ] or other statistical mitigation techniques .","label":"CompareOrContrast","metadata":{},"score":"52.22132"}{"text":"Our method does not assume any knowledge about the target language ( in particular no tagging dictionary is assumed ) , making it applicable to a wide array of resource - poor languages .We use graph - based label propagation for cross - lingual knowledge transfer and use the projected labels as features in an unsupervised model ( Berg - Kirkpatrick et al .","label":"CompareOrContrast","metadata":{},"score":"52.368576"}{"text":"Gives multiple parsings .In addition , it now can graph dependency relations and is a good way to learn to read the ENGCG function labels .Also has POS - tagged corpora .B. 2 phrase structure grammars .The Penn Treebank is a large corpus of articles from the Wall Street Journal that have been tagged with Penn Treebank tags and then parsed into properly bracketed trees according to a simple set of phrase structure rules conforming to Chomsky 's Government and Binding syntax .","label":"CompareOrContrast","metadata":{},"score":"52.493824"}{"text":"Text - mining methods in particular can help disease modeling by mapping named - entities mentions to terminologies and clustering semantically related terms .EHR corpora , however , exhibit specific statistical and linguistic characteristics when compared with corpora in the biomedical literature domain .","label":"CompareOrContrast","metadata":{},"score":"52.565205"}{"text":"( 2008 ) can label unlabeled data for each other in a way similar to co - tra ... \" .Martins et al .( 2008 ) presented what to the best of our knowledge still ranks as the best overall result on the CONLL - X Shared Task datasets .","label":"CompareOrContrast","metadata":{},"score":"52.61921"}{"text":"For example , noun phrases might be split into subcategories for subjects and objects , singular and plural , and so on .This splitting process admits an efficient incremental inference scheme which reduces parsing times by orders of magnitude .Furthermore , it produces the best parsing accuracies across an array of languages , in a fully language - general fashion .","label":"CompareOrContrast","metadata":{},"score":"52.627502"}{"text":"This factorization provides conceptual simplicity , straightforward opportunities for separately improving the component mod ... \" .We present a novel generative model for natural language tree structures in which semantic ( lexical dependency ) and syntactic ( PCFG ) structures are scored with separate models .","label":"CompareOrContrast","metadata":{},"score":"52.711884"}{"text":"The algorithm uses a similarity graph to encourage similar n - grams to have similar POS tags .We demonstrate the efficacy of our approach on a domain adaptation task , where we assume that we have access to large amounts of unlabeled data from the target domain , but no additional labeled data .","label":"CompareOrContrast","metadata":{},"score":"52.75097"}{"text":"We compare our approach to other semisupervised learning algorithms . ... strengths of diverse learning algorithms .Nivre and McDonald ( 2008 ) were first to introduce stacking in the context of dependency parsing .Semi - supervised learning is typically motivated ...","label":"CompareOrContrast","metadata":{},"score":"52.81826"}{"text":"Finally , we present multilingual experiments which show that parsing with hierarchical state - splitting is fast and accurate in multiple languages and domains , even without any language - specific tuning . ... g ( Matsuzaki et al . , 2005 ; Petrov et al . , 2006 ) .","label":"CompareOrContrast","metadata":{},"score":"52.839493"}{"text":"Web - scale experiments show that the DMV , perhaps because it is unlexicalized , does not benefit from orders of magnitude more annotated but noisier data .Our model , trained on a single blog , generalizes to 53.3 % accuracy out - of - domain , against the Brown corpus - nearly 10 % higher than the previous published best .","label":"CompareOrContrast","metadata":{},"score":"52.89457"}{"text":"We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative improvement over the baseline approach that uses a fixed context window of adjacent words .","label":"CompareOrContrast","metadata":{},"score":"52.895386"}{"text":"We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative improvement over the baseline approach that uses a fixed context window of adjacent words .","label":"CompareOrContrast","metadata":{},"score":"52.895386"}{"text":"We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .We propose using corpus - level statistics for lexicon learning decisions .","label":"CompareOrContrast","metadata":{},"score":"52.90764"}{"text":"The model is formally a latent variable CRF grammar over trees , learned by iteratively splitting grammar productions ( not categories ) .Different regions of the grammar are refined to different degrees , yielding grammars which are three orders of magnitude smaller than the single - scale baseline and 20 times smaller than the split - and - merge grammars of Petrov et al .","label":"CompareOrContrast","metadata":{},"score":"52.94874"}{"text":"This paper presents novel improvements to the induction of translation lexicons from monolingual corpora using multilingual dependency parses .We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative ... \" .","label":"CompareOrContrast","metadata":{},"score":"53.145805"}{"text":"This paper presents novel improvements to the induction of translation lexicons from monolingual corpora using multilingual dependency parses .We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative ... \" .","label":"CompareOrContrast","metadata":{},"score":"53.145805"}{"text":"This unified representation plays a crucial role in cross - lingual syntactic transfer of multilingual dependency parsers .Until now , however , such conversion schemes have been created manually .Our central hypothesis is that a valid mapping yields POS annotations with coherent linguistic properties which are consistent across source and target languages .","label":"CompareOrContrast","metadata":{},"score":"53.210865"}{"text":"Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext .","label":"CompareOrContrast","metadata":{},"score":"53.243492"}{"text":"In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level , we instead use features to transfer the behavior of words at a type level .In a discriminative dependency parsing framework , our approach produces gains across a range of target languages , using two different lowresource training methodologies ( one weakly supervised and one indirectly supervised ) and two different dictionary sources ( one manually constructed and one automatically constructed ) . ... me , correctly suggesting attachments to Verzicht and Gewerkschaften .","label":"CompareOrContrast","metadata":{},"score":"53.48289"}{"text":"Nonetheless , the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks .We demonstrate that log - linear grammars with latent variables can be practically trained using discriminative methods .Central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectations to be efficiently approximated in a gradient - based procedure .","label":"CompareOrContrast","metadata":{},"score":"53.545982"}{"text":"This paper outlines our participation in the 2007 CoNLL Shared Task on Domain Adaptation ( Nivre et al . , 2007 ) .The goal was to adapt a parser trained on a single source domain to a new target domain us- ing only unlabeled data .","label":"CompareOrContrast","metadata":{},"score":"53.580597"}{"text":"We take two popular dependency parsers - one graph - based and one transition - based - and compare results for both .Results show that using semisupervised learning in the form of self - training and co - training yields only very modest improvements in parsing accuracy .","label":"CompareOrContrast","metadata":{},"score":"53.682137"}{"text":"We describe some challenges of adaptation in the 2007 CoNLL Shared Task on Domain Adaptation .Our error analysis for this task suggests that a primary source of error is differences in annotation guidelines between treebanks .Our suspicions are supported by the observation that no team was able to improve target domain performance substantially over a state of the art baseline .","label":"CompareOrContrast","metadata":{},"score":"53.731895"}{"text":".. rget languages in which no or few such resources are available ( Hwa et al . , 2005 ) .Figure 1 : Cross - lingual word cluster features for parsing .Top - left : Cross - lingual ( EN - ES ) word clustering model .","label":"CompareOrContrast","metadata":{},"score":"53.84517"}{"text":"Second , and more interestingly , we provide an algorithm for inducing cross - lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross - lingual structure prediction .Specifically , we show that by augmenting direct - transfer systems with cross - lingual cluster features , the relative error of delexicalized dependency parsers , trained on English treebanks and transferred to foreign languages , can be reduced by up to 13 % .","label":"CompareOrContrast","metadata":{},"score":"53.966812"}{"text":"Both words were unknown only 5 % of the time in BIO , while one of the words being un- known was more common , reflecting 27 % of deci- sions .Upon further investigation , the majority of unknown words were nouns , which indicates that unknown word errors were caused by the problems discussed above .","label":"CompareOrContrast","metadata":{},"score":"54.147125"}{"text":"Our suspicions are supported by the observation that no team was able to im- prove target domain performance substan- tially over a state of the art baseline . 1 Introduction Dependency parsing , an important NLP task , can be done with high levels of accuracy .","label":"CompareOrContrast","metadata":{},"score":"54.152874"}{"text":"In what follows , we provide an er- ror analysis that attributes domain loss for this task to a difference in annotation guidelines between do- mains .We then overview our attempts to improve adaptation .While we were able to show limited adaptation on reduced training data or with first- order features , no modifications improved parsing with all the training data and second - order features .","label":"CompareOrContrast","metadata":{},"score":"54.17984"}{"text":"That is , we score a aligned ... \" .We connect two scenarios in structured learning : adapting a parser trained on one corpus to another annotation style , and projecting syntactic annotations from one language to another .We propose quasisynchronous grammar ( QG ) features for these structured learning tasks .","label":"CompareOrContrast","metadata":{},"score":"54.427105"}{"text":"We present a new family of models for unsupervised parsing , Dependency and Boundary models , that use cues at constituent boundaries to inform head - outward dependency tree generation .We build on three intuitions that are explicit in phrase - structure grammars but only implicit in standard dependency ... \" .","label":"CompareOrContrast","metadata":{},"score":"54.563015"}{"text":"We highlight the use of this resource via two experiments , including one that reports competitive accuracies for unsupervised grammar induction without gold standard part - of - speech tags .We present an online learning algorithm for training structured prediction models with extrinsic loss functions .","label":"CompareOrContrast","metadata":{},"score":"54.628574"}{"text":"We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences .Given this fixed network representation , we learn a final layer using the structured perceptron with beam - search decoding .On the Penn Treebank , our parser reaches 94.26 % unlabeled and 92.41 % labeled attachment accuracy , which to our knowledge is the best accuracy on Stanford Dependencies to date .","label":"CompareOrContrast","metadata":{},"score":"54.67646"}{"text":"It is well known that parsing accuracy suffers when a model is applied to out - of - domain data .It is also known that the most beneficial data to parse a given domain is data that matches the domain ( Sekine , 1997 ; Gildea , 2001 ) .","label":"CompareOrContrast","metadata":{},"score":"54.70688"}{"text":"This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm .We show how the algorithms can be efficiently applied to exponential sized representations of parse trees , such as the \" all subtrees \" ( DOP ) representation described by ( Bod 98 ) , or a representation tracking all sub - fragments of a tagged sentence .","label":"CompareOrContrast","metadata":{},"score":"54.82418"}{"text":"Comput .Linguist . , 2010 . \" ...Information - extraction ( IE ) systems seek to distill semantic relations from naturallanguage text , but most systems use supervised learning of relation - specific examples and are thus limited by the availability of training data .","label":"CompareOrContrast","metadata":{},"score":"54.82898"}{"text":"However , we plan to extend the system to improve parse coverage , depth and accuracy . ... realistic texts .Evaluation of such systems has been primarily in terms of the PARSEVAL scheme tree similarity measures of ( labelled ) precision and recall and crossing bracket rate .","label":"CompareOrContrast","metadata":{},"score":"54.883087"}{"text":"No additional knowledge about the target domain is included .A more realistic approach assumes that only raw text from the target domain is available .This assumption lends itself well to semi - supervised learning methods since these utilize both labeled and unlabeled examples .","label":"CompareOrContrast","metadata":{},"score":"54.93299"}{"text":"Further experiments showed that any decrease in training data hurt parser perfor- mance .It would seem that the parser has no dif- ficulty learning important training sentences in the presence of unimportant training examples .A related idea focused on words , weighing highly tokens that appeared frequently in the target domain .","label":"CompareOrContrast","metadata":{},"score":"55.02358"}{"text":"Unlike previous approaches , our framework does not require full projected parses , allowing partial , approximate transfer through linear expectation constraints on the space of distributions over trees .We consider several types of constraints that range from generic dependency conservation to language - specific annotation rules for auxiliary verb analysis .","label":"CompareOrContrast","metadata":{},"score":"55.07122"}{"text":"Unlike previous approaches , our framework does not require full projected parses , allowing partial , approximate transfer through linear expectation constraints on the space of distributions over trees .We consider several types of constraints that range from generic dependency conservation to language - specific annotation rules for auxiliary verb analysis .","label":"CompareOrContrast","metadata":{},"score":"55.07122"}{"text":"These fine grained tags provide more information than coarse tags ; experiments that removed fine grained tags 1052 .Page 3 . hurt WSJ performance but did not affect BIO .Finally , we examined the effect of unknown words .Not surprisingly , the most significant dif- ferences in error rates concerned dependencies be- tween words of which one or both were unknown to the parser .","label":"CompareOrContrast","metadata":{},"score":"55.15763"}{"text":"Moreover , an indirect comparison indicates that our approach also outperforms previous work based on treebank conversion .However , the SL framework trains two parsers on the same treebank and therefore does not need to consider the problem of annotation inconsistencies . \" ...","label":"CompareOrContrast","metadata":{},"score":"55.208885"}{"text":"[ Show abstract ] [ Hide abstract ] ABSTRACT : Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"CompareOrContrast","metadata":{},"score":"55.349968"}{"text":"Tools . by Kuzman Ganchev , Jennifer Gillenwater , Ben Taskar - In ACL - IJCNLP , 2009 . \" ...Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext .","label":"CompareOrContrast","metadata":{},"score":"55.50962"}{"text":"Our first- , second- , and third - order models achieve accuracies comparable to those of their unpruned counterparts , while exploring only a fraction of the search space .We observe speed - ups of up to two orders of magnitude compared to exhaustive search .","label":"CompareOrContrast","metadata":{},"score":"55.571953"}{"text":"To address conflicting annota-1053 .Page 4 . tions , we added slack variables to the MIRA learn- ing algorithm ( Crammer et al . , 2006 ) used to train the parsers , without success .We measured diversity by comparing the parses of each model .","label":"CompareOrContrast","metadata":{},"score":"55.664047"}{"text":"aFor text mining , preprocessing the EHR corpus with fingerprinting yields significantly better results .Conclusions Before applying text - mining techniques , one must pay careful attention to the structure of the analyzed corpora .While the importance of data cleaning has been known for low - level text characteristics ( e.g. , encoding and spelling ) , high - level and difficult - to - quantify corpus characteristics , such as naturally occurring redundancy , can also hurt text mining .","label":"CompareOrContrast","metadata":{},"score":"56.006897"}{"text":"Finally , we conduct a multi - lingual evaluation that demonstrates the robustness of the overall structured neural approach , as well as the benefits of the extensions proposed in this work .Our research further demonstrates the breadth of the applicability of neural network methods to dependency parsing , as well as the ease with which new features can be added to neural parsing models .","label":"CompareOrContrast","metadata":{},"score":"56.077435"}{"text":"We apply this idea to dependency and constituent parsing , generating results that surpass state - of - theart ... \" .We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers .","label":"CompareOrContrast","metadata":{},"score":"56.1578"}{"text":"Our formulation is able to handle non - local output features in an efficient manner ; not only is it compatible with prior knowledge encoded as hard constraints , it can also learn soft constraints from data .In particular , our model is able to learn correlations among neighboring arcs ( siblings and grandparents ) , word valency , and tendencies toward nearly - projective parses .","label":"CompareOrContrast","metadata":{},"score":"56.18116"}{"text":"A second group of papers does parsing by a sequence of independent , discriminative decisions , either greedily or with use of a small beam ( Ratnaparkhi , 1997 ; Henderson , 2004 ) .This paper extends th ... \" ...","label":"CompareOrContrast","metadata":{},"score":"56.21242"}{"text":"Our best results show a 26-fold speedup compared to a sequential C implementation .We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data .We first demonstrate that delexicalized parsers can be directly transferred between languages , producing significantly higher accuracies than unsupervised parsers .","label":"CompareOrContrast","metadata":{},"score":"56.29322"}{"text":"Such lexicon compilation requires highly reliable predicate - argument structures to practically contribute to Natural Language Processing ( NLP ) applications , such as paraphrasing , text entailment , and machine translation .We first apply chunking to raw corpora and then extract reliable chunks to ensure that high - quality predicate - argument structures are obtained from the chunks .","label":"CompareOrContrast","metadata":{},"score":"56.366825"}{"text":"We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .We apply the framework to word segmentation , joint segmentation and POStagging , dependency parsing , and phrase - structure parsing .","label":"CompareOrContrast","metadata":{},"score":"56.415092"}{"text":"We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .We apply the framework to word segmentation , joint segmentation and POStagging , dependency parsing , and phrase - structure parsing .","label":"CompareOrContrast","metadata":{},"score":"56.415092"}{"text":", 2009 ; Biber & Gray , 2010 ) , but the most interesting usages apply the divergence to a machine learning system .Despite the fact that authors have shown that a divergence ( Van Asch & Daelemans , 2010 ; Plank , 2011 ) or a linear combination of divergences ( McClosky , 2010 ) can be successfully used to link the sim ... . \" ...","label":"CompareOrContrast","metadata":{},"score":"56.41737"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"CompareOrContrast","metadata":{},"score":"56.42924"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"CompareOrContrast","metadata":{},"score":"56.42924"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"CompareOrContrast","metadata":{},"score":"56.42924"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"CompareOrContrast","metadata":{},"score":"56.42924"}{"text":"We show that dependency parsers have more difficulty parsing questions than constituency parsers .In particular , deterministic shift - reduce dependency parsers , which are of highest interest for practical applications because of their linear running time , drop to 60 % labeled accuracy on a question test set .","label":"CompareOrContrast","metadata":{},"score":"56.463417"}{"text":"We also describe the multiplicative combination of this dependency model with a model of linear constituency .The product model outperforms both components on their respective evaluation metrics , giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing .","label":"CompareOrContrast","metadata":{},"score":"56.55791"}{"text":"Second , how can we efficiently infer optimal structures within them ?Hierarchical coarse - to - fine methods address both questions .Coarse - to - fine approaches exploit a sequence of models which introduce complexity gradually .At the top of the sequence is a trivial model in which learning and inference are both cheap .","label":"CompareOrContrast","metadata":{},"score":"56.61872"}{"text":"Experimental results show that the global features are useful in all the languages . ... mines unlabeled dependency structures only , and we attach dependency relation labels using Support Vector Machines afterwards . \" ...We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .","label":"CompareOrContrast","metadata":{},"score":"56.685448"}{"text":"Experimental results show that the global features are useful in all the languages . ... mines unlabeled dependency structures only , and we attach dependency relation labels using Support Vector Machines afterwards . \" ...We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .","label":"CompareOrContrast","metadata":{},"score":"56.685448"}{"text":"Specifically , an ini- tial hypothesis lattice is constrcuted using local features .Candidate sentences are then assigned syntactic language model scores .These global syntactic scores are combined with local low - level scores in a log - linear model .","label":"CompareOrContrast","metadata":{},"score":"56.72857"}{"text":"WSJ data ( Petrov and Klein , 2007 ; Foster , 2010 ) .The parser uses the English signature list described in Attia et al ( 2010 ) to assign partof - speech tags to unknown words . \" ...Domain adaptation is an important task in order for NLP systems to work well in real applications .","label":"CompareOrContrast","metadata":{},"score":"56.74319"}{"text":"Yet , various grammar parameters are expected to be correlated because the elements in language they represent share linguistic properties .In this paper , we suggest an alternative to the Dirichlet prior , a family of logistic normal distributions .We derive an inference algorithm for this family of distributions and experiment with the task of dependency grammar induction , demonstrating performance improvements with our priors on a set of six treebanks in different natural languages .","label":"CompareOrContrast","metadata":{},"score":"56.743458"}{"text":"Yet , various grammar parameters are expected to be correlated because the elements in language they represent share linguistic properties .In this paper , we suggest an alternative to the Dirichlet prior , a family of logistic normal distributions .We derive an inference algorithm for this family of distributions and experiment with the task of dependency grammar induction , demonstrating performance improvements with our priors on a set of six treebanks in different natural languages .","label":"CompareOrContrast","metadata":{},"score":"56.743458"}{"text":"The VISL project tags with Constraint Grammar tags , along with tense , case , and number information and grammatical function in the sentence . \"Flat Structure \" actually returns a dependency parse .For VISL parsing , see below .","label":"CompareOrContrast","metadata":{},"score":"56.796688"}{"text":"We present a method for acquiring reliable predicate - argument structures from raw corpora for automatic compilation of case frames .Such lexicon compilation requires highly reliable predicate - argument structures to practically contribute to Natural Language Processing ( NLP ) applications , such as par ... \" .","label":"CompareOrContrast","metadata":{},"score":"56.852478"}{"text":"To confirm that nouns were problem- atic , we modified a first - order parser ( no second or- der features ) by adding a feature indicating correct noun - noun edges , forcing the parser to predict these edges correctly .","label":"CompareOrContrast","metadata":{},"score":"56.875893"}{"text":"proposed an ensemble method ( Reichart and Rappoport , 2007 ) .They regarded parses as being of high quality if 20 different parsers agreed .They used an SVM regression approach on the basis of text - based and parse - based features .","label":"CompareOrContrast","metadata":{},"score":"56.90763"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"CompareOrContrast","metadata":{},"score":"57.040276"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"CompareOrContrast","metadata":{},"score":"57.040276"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"CompareOrContrast","metadata":{},"score":"57.040276"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"CompareOrContrast","metadata":{},"score":"57.040276"}{"text":"In the second submission , also a constituency parsing system , the n - best lists of various parsing models are combined using an approximate sentence - level product model .The third system , the highest ranked system in the dependency parsing track , uses voting over dependency arcs to combine the output of three constituency parsing systems which have been converted to dependency trees .","label":"CompareOrContrast","metadata":{},"score":"57.101395"}{"text":"Similar head - finding rules were used for Chinese experiments .The ... . \" ...We discuss the relevance of k - best parsing to recent applications in natural language processing , and develop efficient algorithms for k - best trees in the framework of hypergraph parsing .","label":"CompareOrContrast","metadata":{},"score":"57.131275"}{"text":"Starting from a mono - phone model , we learn increasingly refined models that capture phone internal structures , as well as context - dependent variations in an automatic way .Our approaches reduces error rates compared to other baseline approaches , while streamlining the learning procedure .","label":"CompareOrContrast","metadata":{},"score":"57.14483"}{"text":"First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar without a treebank .In our experiments , hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy .","label":"CompareOrContrast","metadata":{},"score":"57.504505"}{"text":"First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar without a treebank .In our experiments , hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy .","label":"CompareOrContrast","metadata":{},"score":"57.504505"}{"text":"First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar without a treebank .In our experiments , hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy .","label":"CompareOrContrast","metadata":{},"score":"57.504505"}{"text":"Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions .Syntactic parsing is a fundamental problem in computational linguistics and natural language processing .Traditional approaches to parsing are highly complex and problem specific .","label":"CompareOrContrast","metadata":{},"score":"57.534313"}{"text":"Unlike previous work on projecting syntactic resources , we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers .The projected parsers from our system result in state - of - the - art performance when compared to previously studied unsupervised and projected parsing systems across eight different languages .","label":"CompareOrContrast","metadata":{},"score":"57.567333"}{"text":"We consider the problem of using a bilingual dictionary to transfer lexico - syntactic information from a resource - rich source language to a resource - poor target language .In contrast to past work that used bitexts to transfer analyses of specific sentences at the token level , we instead use features ... \" .","label":"CompareOrContrast","metadata":{},"score":"57.781918"}{"text":"Based on such TPs , ... \" .We present a simple and effective framework for exploiting multiple monolingual treebanks with different annotation guidelines for pars - ing .Several types of transformation patterns ( TP ) are designed to capture the systematic an - notation inconsistencies among different tree - banks .","label":"CompareOrContrast","metadata":{},"score":"58.024532"}{"text":"We present a novel discriminative approach to parsing inspired by the large - margin criterion underlying support vector machines .Our formulation uses a factorization analogous to the standard dynamic programs for parsing .In particular , it allows one to efficiently learn a model which discriminates ... \" .","label":"CompareOrContrast","metadata":{},"score":"58.275673"}{"text":"Combining multiple grammars that were self - trained on disjoint sets of unlabeled data results in a final test accuracy of 92.5\\% on the WSJ test set and 89.6\\% on our Broadcast News test set .This work shows how to improve state - of - the - art monolingual natural language processing models using unannotated bilingual text .","label":"CompareOrContrast","metadata":{},"score":"58.355343"}{"text":"References Shai Ben - David , John Blitzer , Koby Crammer , and Fer- nando Pereira .Analysis of representations for domain adaptation .In NIPS .Leo Breiman .Learning , 24(2):123 - 140 .Bagging predictors .Machine R. Brown .","label":"CompareOrContrast","metadata":{},"score":"58.450943"}{"text":"We present several models to this end ; in particular a partially observed conditional random field model , where coupled token and type constraints provide a partial signal for training .Averaged across eight previously studied Indo - European languages , our model achieves a 25 % relative error reduction over the prior state of the art .","label":"CompareOrContrast","metadata":{},"score":"58.45744"}{"text":"The first submission , the highest ranked constituency parsing system , uses a combination of PCFG - LA product grammar parsing and self - training .In the second submission , also a constituency parsing ... \" .The DCU - Paris13 team submitted three systems to the SANCL 2012 shared task on parsing English web text .","label":"CompareOrContrast","metadata":{},"score":"58.463406"}{"text":"When restricted to local features , cube summing reduces to a novel semiring ( k - best+residual ) that generalizes many of the semirings of Goodman ( 1999 ) .When non - local features are included , cube summing does not reduce to any semiring , but is compatible with generic techniques for solving dynamic programming equations . ... onal inference ( Jordan et al . , 1999 ; MacKay , 1997 ; Kurihara and Sato , 2006 ) .","label":"CompareOrContrast","metadata":{},"score":"58.51185"}{"text":"Our er- ror analysis suggests that the primary cause of loss from adaptation is from differences in the annotation guidelines themselves .Therefore , significant im- provements can not be made without specific knowl- edgeofthetargetdomain'sannotationstandards .No amount of source training data can help if no rele- vant structure exists in the data .","label":"CompareOrContrast","metadata":{},"score":"58.52767"}{"text":"Standard inference can be used at test time .Our approach is able to scale to very large problems and yields significantly improved target domain accuracy .It is well known that parsing accuracies drop significantly on out - of - domain data .","label":"CompareOrContrast","metadata":{},"score":"58.59529"}{"text":"We extend and improve upon recent work in structured training for neural network transition - based dependency parsing .We do this by experimenting with novel features , additional transition systems and by testing on a wider array of languages .In particular , we introduce set - valued features to encode the predicted morphological properties and part - of - speech confusion sets of the words being parsed .","label":"CompareOrContrast","metadata":{},"score":"58.72673"}{"text":"Our experiments show that unsupervised QG projection improves on parses trained using only highprecision projected annotations and far outperforms , by more than 35 % absolute dependency accuracy , learning an unsupervised parser from raw target - language text alone .When a few target - language parse trees are available , projection gives a boost equivalent to doubling the number of target - language trees . .","label":"CompareOrContrast","metadata":{},"score":"58.821434"}{"text":"But how does the observed EHR redundancy affect text mining ?Does such redundancy introduce a bias that distorts learned models ?Or does the redundancy introduce benefits by highlighting stable and important subsets of the corpus ?( iii )How can one mitigate the impact of redundancy on text mining ?","label":"CompareOrContrast","metadata":{},"score":"58.835617"}{"text":"Unlike previous work , our final model does not require any additional resources at run - time .Compared to a state - of - the - art approach , we achieve more than 20 % relative error reduction .Additionally , we annotate a corpus of search queries with part - of - speech tags , providing a resource for future work on syntactic query analysis .","label":"CompareOrContrast","metadata":{},"score":"58.84258"}{"text":"However , most previous work on domain adaptation relied on the implicit assumption that domains are somehow given .As more and more data becomes available , automatic ways to select data that is beneficial for a new ( unknown ) target domain are becoming attractive .","label":"CompareOrContrast","metadata":{},"score":"58.985413"}{"text":"( 2006 ) , which achieved top results in the 2006 We were given around CoNLL - X shared task .Preliminary experiments in- dicated that the edge labeler was fairly robust to do- main adaptation , lowering accuracy by 3 % in the de- velopment domain as opposed to 2 % in the source , so we focused on unlabeled dependency parsing .","label":"CompareOrContrast","metadata":{},"score":"59.146366"}{"text":"..METU - Sabancı treebank ( Atalay et al . , 2003 ; Oflazer et al . , 2003 ) from the CoNLL shared task in 2006 .Whenever using CoNLL shared task data , we used the first 80 % of the data d .. \" ...","label":"CompareOrContrast","metadata":{},"score":"59.235916"}{"text":"..METU - Sabancı treebank ( Atalay et al . , 2003 ; Oflazer et al . , 2003 ) from the CoNLL shared task in 2006 .Whenever using CoNLL shared task data , we used the first 80 % of the data d .. \" ...","label":"CompareOrContrast","metadata":{},"score":"59.235916"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"CompareOrContrast","metadata":{},"score":"59.30316"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"CompareOrContrast","metadata":{},"score":"59.30316"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"CompareOrContrast","metadata":{},"score":"59.30316"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"CompareOrContrast","metadata":{},"score":"59.30316"}{"text":"The Stanford NLP Group has put up java - based maximum entropy POS tagger that can tag large amounts of text .It tags each word of continuous text with a PennTree POS .Special feature : it has a much slower bidirectional mode as well as \" left three words \" mode of operation .","label":"CompareOrContrast","metadata":{},"score":"59.31421"}{"text":"Participants were to build a single parsing system that is robust to domain changes and can handle noisy text that is commonly encountered on the web .There was a constituency and a dependency parsing track and 11 sites submitted a total of 20 systems .","label":"CompareOrContrast","metadata":{},"score":"59.322258"}{"text":"Their symbolic component is amenable to inspection by humans , while their probabilistic component helps resolve ambiguity .They also permit the use of well - understood , generalpurpose learn ... \" .Probabilistic grammars offer great flexibility in modeling discrete sequential data like natural language text .","label":"CompareOrContrast","metadata":{},"score":"59.391655"}{"text":"Their symbolic component is amenable to inspection by humans , while their probabilistic component helps resolve ambiguity .They also permit the use of well - understood , generalpurpose learn ... \" .Probabilistic grammars offer great flexibility in modeling discrete sequential data like natural language text .","label":"CompareOrContrast","metadata":{},"score":"59.391655"}{"text":"In the Penn Treebank , transitions between upper- and lowercase tokens tend to align with the boundaries of base ( English ) noun phrases .Such signals can be used as partial bracketing constraints to train a grammar inducer : in ou ... \" .","label":"CompareOrContrast","metadata":{},"score":"59.393044"}{"text":"Manually annotated corpora are valuable but scarce resources , yet for many annotation tasks such as treebanking and sequence labeling there exist multiple corpora with different and incompatible annotation guidelines or standards .This seems to be a great waste of human efforts , and it would be nice to automatically adapt one annotation standard to another .","label":"CompareOrContrast","metadata":{},"score":"59.45411"}{"text":"Our quasi - synchronous model assigns positive probability to any alignment of any trees , in contrast to a synchronous grammar , which would insist on some form of structural parallelism .In monolingual dependency parser adaptation , we achieve high accuracy in translating among multiple annotation styles for the same sentence .","label":"CompareOrContrast","metadata":{},"score":"59.713158"}{"text":"( 2008 ) can label unlabeled data for each other in a way similar to co - training and produce end parsers that are significantly better than any of the stacked input parsers .We evaluate our system on five datasets from the CONLL - X Shared Task and obtain 10 - 20 % error reductions , incl .","label":"CompareOrContrast","metadata":{},"score":"59.812428"}{"text":"We evaluate the performance of our parser on data in several natural languages , achieving improvements over existing state - of - the - art methods . \" ...We connect two scenarios in structured learning : adapting a parser trained on one corpus to another annotation style , and projecting syntactic annotations from one language to another .","label":"CompareOrContrast","metadata":{},"score":"59.882133"}{"text":"The annotations are produced automatically with statistical models that are specifically adapted to historical text .The corpus will facilitate the study of linguistic trends , especially those related to the evolution of syntax .Syntactic analysis of search queries is important for a variety of information- retrieval tasks ; however , the lack of annotated data makes training query analysis models difficult .","label":"CompareOrContrast","metadata":{},"score":"59.93042"}{"text":"We also show that our techniques can be applied to full - scale parsing applications by demonstrating its effectiveness in learning state - split grammars .Treebank parsing can be seen as the search for an optimally refined grammar consistent with a coarse training treebank .","label":"CompareOrContrast","metadata":{},"score":"60.017258"}{"text":"The resulting grammars are extremely compact com- pared to other high - performance parsers , yet the parser gives the best published accuracies on several languages , as well as the best generative parsing numbers in English .In addi- tion , we give an associated coarse - to - fine inference scheme which vastly improves inference time with no loss in test set accuracy .","label":"CompareOrContrast","metadata":{},"score":"60.0761"}{"text":"Being largely languageuniversal , the selection component is learned in a supervised fashion from all the training languages .In contrast , the ordering decisions are only influenced by languages with similar properties .We systematically model this cross - lingual sharing using typological features .","label":"CompareOrContrast","metadata":{},"score":"60.236183"}{"text":"For more extensive description , see Annotating Predicate Argument Structure The full ( 318 page ) manual for PennTreebank II markup is available as a PDF .The first 10 % Penn TreeBank sentences are available with both standard PennTree and also Dependency parsing as part of the free dataset for the Python - based Natural Language Tool Kit ( NLTK ) .","label":"CompareOrContrast","metadata":{},"score":"60.290474"}{"text":"We present experiments with sequence models on part - of - speech tagging and named entity recognition tasks , and with syntactic parsers on dependency parsing and machine translation reordering tasks .Low - latency solutions for syntactic parsing are needed if parsing is to become an integral part of user - facing natural language applications .","label":"CompareOrContrast","metadata":{},"score":"60.293156"}{"text":"This corpus is not only marked up for part of speech ; each part is also assigned a syntactic function following the Quirk et al . scheme of SVOA etc .So it displays its texts in trees ( oriented side- , top- , or bottom - up as you please ) with dual labelling of each node ( see sample ) .","label":"CompareOrContrast","metadata":{},"score":"60.298286"}{"text":"B. 1 dependency grammars .Connexor was founded by some of the Helsinki group and offers for sale parsing tools for several languages .For English , it uses an improved version of ENGCG ( ) POS tagging with a nifty little java applet for a dependency tree display of grammatical relations ( some of which look more like semantic relations ) .","label":"CompareOrContrast","metadata":{},"score":"60.39141"}{"text":"We present several improvements to unlexicalized parsing with hierarchically state - split PCFGs .First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar wi ... \" .","label":"CompareOrContrast","metadata":{},"score":"60.49813"}{"text":"Latent variable grammars take an observed ( coarse ) treebank and induce more fine - grained grammar categories , that are better suited for modeling the syntax of natural languages .Estimation can be done in a generative or a discriminative framework , and results in the best published parsing accuracies over a wide range of syntactically divergent languages and domains .","label":"CompareOrContrast","metadata":{},"score":"60.706375"}{"text":"We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .","label":"CompareOrContrast","metadata":{},"score":"60.718517"}{"text":"We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .","label":"CompareOrContrast","metadata":{},"score":"60.718517"}{"text":"We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .","label":"CompareOrContrast","metadata":{},"score":"60.718517"}{"text":"Our motivation is to broaden the advantages of multilingual learning to languages that exhibit significant differences from existing resource - rich languages .The algorithm learns which aspects of the source languages are relevant for the target language and ties model parameters accordingly .","label":"CompareOrContrast","metadata":{},"score":"60.894005"}{"text":"Tools . by Slav Petrov , Dipanjan Das , Ryan McDonald - IN ARXIV:1104.2086 , 2011 . \" ...To facilitate future research in unsupervised induction of syntactic structure and to standardize best - practices , we propose a tagset that consists of twelve universal part - of - speech categories .","label":"CompareOrContrast","metadata":{},"score":"60.97091"}{"text":"While prior feature - based dynamic programming parsers have restricted training and evaluation to artificially short sentences , we present the first general , featurerich discriminative parser , based on a conditional random field model , which has been successfully scaled to the full WSJ parsing data .","label":"CompareOrContrast","metadata":{},"score":"61.020226"}{"text":"We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system .We use a corpus of weakly - labeled reference reorderings to guide parser training .Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress .","label":"CompareOrContrast","metadata":{},"score":"61.140114"}{"text":"We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings , word - to - word alignments from an MT system , and syntactic structure from parse - trees of source and target language sentences .","label":"CompareOrContrast","metadata":{},"score":"61.20437"}{"text":"In addition to a change in the annotation guide- lines for NPs , we observed an important difference in the distribution of POS tags .NN tags were almost twice as likely in the BIO domain ( 14 % in WSJ and 25 % in BIO ) .","label":"CompareOrContrast","metadata":{},"score":"61.228226"}{"text":"These approaches assume access to a common input representation in the form of universal tags , which enables the model to connect patterns observed in the source language to th ... . \" ...In this paper , we study direct transfer methods for multilingual named entity recognition .","label":"CompareOrContrast","metadata":{},"score":"61.256004"}{"text":"We show how web mark - up can be used to improve unsupervised dependency parsing .Starting from raw bracketings of four common HTML tags ( anchors , bold , italics and underlines ) , we refine approximate partial phrase boundaries to yield accurate parsing constraints .","label":"CompareOrContrast","metadata":{},"score":"61.646626"}{"text":"( 2012 ) , which is based on cross - lingual word cluster features .First , we show that by using multiple source languages , combined with self - training for target language adaptation , we can achieve significant improvements compared to using only single source direct transfer .","label":"CompareOrContrast","metadata":{},"score":"61.750923"}{"text":"State - of - the - art natural language processing models are anything but compact .Syntactic parsers have huge grammars , machine translation systems have huge transfer tables , and so on across a range of tasks .With such complexity come two challenges .","label":"CompareOrContrast","metadata":{},"score":"61.8128"}{"text":"We generalize the evaluation to other word - types , and show that the performance can be increased to 18 % relative by preserving part - of - speech equivalencies during translation .We further differentiate ourselves from previous work by conducting a second evaluation which examines the accuracy of translating all word types , rather than just nouns .","label":"CompareOrContrast","metadata":{},"score":"61.900818"}{"text":"We generalize the evaluation to other word - types , and show that the performance can be increased to 18 % relative by preserving part - of - speech equivalencies during translation .We further differentiate ourselves from previous work by conducting a second evaluation which examines the accuracy of translating all word types , rather than just nouns .","label":"CompareOrContrast","metadata":{},"score":"61.900818"}{"text":"First , there may be room for adaptation with our domains if a common annotation scheme is used .Second , we have stressed that typical adaptation , modifying a model trained on the source domain , will fail but there may be unsupervised parsing techniques that improve performance after adaptation , such as a rule based NP parser for BIO based on knowledge of the annotations .","label":"CompareOrContrast","metadata":{},"score":"61.906372"}{"text":"Finally , we present multilingual experiments which show that parsing with hierarchical state - splitting is fast and accurate in multiple languages and domains , even without any language - specific tuning . ... state splitting ( Matsuzaki et al . , 2005 ; Petrov et al . , 2006 ) .","label":"CompareOrContrast","metadata":{},"score":"62.042206"}{"text":"We found certain scaling techniques obtained tiny improvements on the target domain that , while significant compared to competition results , are not statistically significant .We also attempted a sim- ilar approach on the feature level .A very predic- tive source domain feature is not useful if it does not appear in the target domain .","label":"CompareOrContrast","metadata":{},"score":"62.195984"}{"text":"The largest improvement is achieved on the non Indo - European languages yielding a gain of 14.4 % . ... parser trained using parallel data .The underlying parsing model is the dependency model with valance ( DMV ) ( Klein and Manning , 2004 ) .","label":"CompareOrContrast","metadata":{},"score":"62.49195"}{"text":"These methods require labeled examples of syntactic structures to learn statistical patterns governing these structures .Labeled data typically requires expert annotators which makes it both time consuming and costly to produce .Furthermo ... \" .Current efforts in syntactic parsing are largely data - driven .","label":"CompareOrContrast","metadata":{},"score":"62.50862"}{"text":"We discuss how the general framework is applied to each of the problems studied in this article , making comparisons with alternative learning and decoding algorithms .We also show how the comparability of candidates considered by the beam is an important factor in the performance .","label":"CompareOrContrast","metadata":{},"score":"62.529068"}{"text":"We discuss how the general framework is applied to each of the problems studied in this article , making comparisons with alternative learning and decoding algorithms .We also show how the comparability of candidates considered by the beam is an important factor in the performance .","label":"CompareOrContrast","metadata":{},"score":"62.529068"}{"text":"Instead , we scaled each feature 's value by a factor proportional to its frequency in the target do- main and trained the parser on these scaled feature values .We obtained small improvements on small amounts of training data .Target Focused Learning 4 Future Directions Given our pessimistic analysis and the long list of failed methods , one may wonder if parser adapta- tion is possible at all .","label":"CompareOrContrast","metadata":{},"score":"62.566612"}{"text":"( iii ) Sentence - internal punctuation boundaries help with longer - distance dependencies , since punctuation correlates with constituent edges .Our models induce state - of - the - art dependency grammars for many languages without special knowledge of optimal input sentence lengths or biased , manually - tuned initializers . ...","label":"CompareOrContrast","metadata":{},"score":"62.63599"}{"text":"We consider generative and di ... \" .Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext .","label":"CompareOrContrast","metadata":{},"score":"62.639038"}{"text":"( 2012 ) , which is based on cross - lingual word cluster features .First , we show that by using multiple source languages , combined with sel ... \" .In this paper , we study direct transfer methods for multilingual named entity recognition .","label":"CompareOrContrast","metadata":{},"score":"62.6884"}{"text":"We build on three intuitions that are explicit in phrase - structure grammars but only implicit in standard dependency formulations : ( i ) Distributions of words that occur at sentence boundaries - such as English determiners - resemble constituent edges .","label":"CompareOrContrast","metadata":{},"score":"62.70819"}{"text":"We could also introduce new variables , e.g. , nonterminal refinements ( Matsuzaki et al . , 2005 ) , or secondary links Mij ( not constrai ... . by Jin - dong Kim , Tomoko Ohta , Sampo Pyysalo , Yoshinobu Kano - In Proceedings of Natural Language Processing in Biomedicine ( BioNLP )","label":"CompareOrContrast","metadata":{},"score":"62.774155"}{"text":"non - parallel , multilingual corpus . 1 Introduction Probabilistic grammars have become an important tool in natural language processing .An attractive property of probabilistic grammars is that the ... . by Fei Wu , Daniel S. Weld - in Proc . 48th Annu .","label":"CompareOrContrast","metadata":{},"score":"62.826286"}{"text":"We present an automatic method for mapping language - specific part - of - speech tags to a set of universal tags .This unified representation plays a crucial role in cross - lingual syntactic transfer of multilingual dependency parsers .Until now , however , such conversion schemes have been created manually ... \" .","label":"CompareOrContrast","metadata":{},"score":"62.83055"}{"text":"Because each refinement introduces only limited complexity , both learning and inference can be done in an incremental fashion .In this dissertation , we describe several coarse - to - fine systems .In the domain of syntactic parsing , complexity is in the grammar .","label":"CompareOrContrast","metadata":{},"score":"62.834255"}{"text":"In particular , we show that the reranking parser described in Charniak and Johnson ( 2005 ) improves performance of the parser on Brown to 85.2 % .Furthermore , use of the self - training techniques described in ( Mc - Closky et al . , 2006 ) raise this to 87.8 % ( an error reduction of 28 % ) again without any use of labeled Brown data .","label":"CompareOrContrast","metadata":{},"score":"62.95096"}{"text":"Discriminative feature - based methods are widely used in natural language processing , but sentence parsing is still dominated by generative methods .While prior feature - based dynamic programming parsers have restricted training and evaluation to artificially short sentences , we present the first gene ... \" .","label":"CompareOrContrast","metadata":{},"score":"62.957626"}{"text":"Despite its simplicity , a product of eight automatically learned grammars improves parsing accuracy from 90.2 % to 91.8 % on English , and from 80.3 % to 84.5 % on German .Pruning can massively accelerate the computation of feature expectations in large models .","label":"CompareOrContrast","metadata":{},"score":"63.0359"}{"text":"We added features indicating when an edge was predicted by another parser and if an edge crossed a predicted edge , as well as conjunctions with edge types .This failed to improve BIO accuracy since these features were less reliable at test time .","label":"CompareOrContrast","metadata":{},"score":"63.15686"}{"text":"The results show that all three systems achieve competitive performance , with a best labeled attachment score over 88 % .All three parsers benefit from the use of automatically derived lemmas , while morphological features seem to be less important .","label":"CompareOrContrast","metadata":{},"score":"63.39067"}{"text":"The results show that all three systems achieve competitive performance , with a best labeled attachment score over 88 % .All three parsers benefit from the use of automatically derived lemmas , while morphological features seem to be less important .","label":"CompareOrContrast","metadata":{},"score":"63.39067"}{"text":"Our methods result in state - of - the - art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .","label":"CompareOrContrast","metadata":{},"score":"63.613663"}{"text":"Information - extraction ( IE ) systems seek to distill semantic relations from naturallanguage text , but most systems use supervised learning of relation - specific examples and are thus limited by the availability of training data .Open IE systems such as TextRunner , on the other hand , aim to handle the unbounded number of relations found on the Web .","label":"CompareOrContrast","metadata":{},"score":"63.62116"}{"text":"In Conference on Natural Language Learning ( CoNLL ) .J. Nivre , J. Hall , S. K¨ ubler , R. McDonald , J. Nils- son , S. Riedel , and D. Yuret .2007 shared task on dependency parsing . of the CoNLL 2007 Shared Task .","label":"CompareOrContrast","metadata":{},"score":"63.97886"}{"text":"However , in cases where lightweight decompositions are not readily available ( e.g. , due to the presence of rich features or logical constraints ) , the original subgradient algor ... \" .Dual decomposition has been recently proposed as a way of combining complementary models , with a boost in predictive power .","label":"CompareOrContrast","metadata":{},"score":"64.012146"}{"text":"Given the exponential size of the mapping space , we propose a novel method for optimizing over soft mappings , and use entropy regularization to drive those towards hard mappings .Our results demonstrate that automatically induced mappings rival the quality of their manually designed counterparts when evaluated in the context of multilingual parsing . ...","label":"CompareOrContrast","metadata":{},"score":"64.07127"}{"text":"However , parsing accuracies for Arabic usually lag behind non - semitic languages .Moreover , whil ...Tools . by Kuzman Ganchev , Jennifer Gillenwater , Ben Taskar - In ACL - IJCNLP , 2009 . \" ...Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .","label":"CompareOrContrast","metadata":{},"score":"64.09889"}{"text":"We began with the first ap- proach and removed a large number of features that we believed transfered poorly , such as most features for noun - noun edges .We obtained a small improve- ment in BIO performance on limited data only .","label":"CompareOrContrast","metadata":{},"score":"64.1676"}{"text":"We discuss the relevance of k - best parsing to recent applications in natural language processing , and develop efficient algorithms for k - best trees in the framework of hypergraph parsing .To demonstrate the efficiency , scalability and accuracy of these algorithms , we present experiments on Bikel 's implementation of Collins ' lexicalized PCFG model , and on Chiang 's CFG - based decoder for hierarchical phrase - based translation .","label":"CompareOrContrast","metadata":{},"score":"64.25978"}{"text":"We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process ( HDP ) .Our HDP - PCFG model allows the complexity of the grammar to grow as more training data is available .In addition to presenting a fully Bayesian model for the PCFG , we also develop an efficient variational inference procedure .","label":"CompareOrContrast","metadata":{},"score":"64.26611"}{"text":"Self - training creates semi - supervised learners from existing supervised learners with minimal effort .We first show results on self - training for constituency parsing within a single domain .While self - training has failed here in the past , we present a simple modification which allows it to succeed , producing state - of - the - art results for English constituency parsing .","label":"CompareOrContrast","metadata":{},"score":"64.31318"}{"text":"With 100 K unlabeled and 2 K labeled questions , uptraining is able to improve parsing accuracy to 84 % , closing the gap between in - domain and out - of - domain performance .We study self - training with products of latent variable grammars in this paper .","label":"CompareOrContrast","metadata":{},"score":"64.37245"}{"text":"Lawrence Erlbaum .M. Marcus , B. Santorini , and M. Marcinkiewicz .Building a large annotated corpus of English : the Penn Treebank .Computational Linguistics , 19(2):313 - 330 .Ryan McDonald , Kevin Lerman , and Fernando Pereira .","label":"CompareOrContrast","metadata":{},"score":"64.38722"}{"text":"For the most common 1While only 8 teams participated in the closed track with us , our score beat all of the teams in the open track .Page 2 .The parser was trained on the provided WSJ data .Digits are less than 4 % of the tokens in BIO .","label":"CompareOrContrast","metadata":{},"score":"64.41904"}{"text":"Nouns are far more The annota- 2We measured these drops on several other dependency parsers and found similar results .ery token is headed by \" Department \" .In contrast , a similar BIO phrase has a very different structure , pursuant to the BIO guidelines .","label":"CompareOrContrast","metadata":{},"score":"64.47262"}{"text":"We describe a robust accurate domain - independent approach to statistical parsing incorporated into the new release of the ANLT toolkit , and publicly available as a research tool .The system has been used to parse many well known corpora in order to produce data for lexical acquisition efforts ; it has also been used as a component in an open - domain question answering project .","label":"CompareOrContrast","metadata":{},"score":"64.61035"}{"text":"Across eight European languages , our approach results in an average absolute improvement of 10.4 % over a state - of - the - art baseline , and 16.7 % over vanilla hidden Markov models induced with the Expectation Maximization algorithm .","label":"CompareOrContrast","metadata":{},"score":"64.71596"}{"text":"The best accuracies were in the 80 - 84\\% range for F1 and LAS ; even part - of - speech accuracies were just above 90\\% .Coarse - to - fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy .","label":"CompareOrContrast","metadata":{},"score":"64.9123"}{"text":"This ' universal ' treebank is made freely available in order to facilitate research on multilingual dependency parsing .We consider the construction of part - of - speech taggers for resource - poor languages .Recently , manually constructed tag dictionaries from Wiktionary and dictionaries projected via bitext have been used as type constraints to overcome the scarcity of annotated data in this setting .","label":"CompareOrContrast","metadata":{},"score":"65.02237"}{"text":"The paper presents the design and implementation of the BioNLP'09 Shared Task , and reports the final results with analysis .The shared task consists of three sub - tasks , each of which addresses bio - molecular event extraction at a different level of specificity .","label":"CompareOrContrast","metadata":{},"score":"65.07172"}{"text":"The paper presents the design and implementation of the BioNLP'09 Shared Task , and reports the final results with analysis .The shared task consists of three sub - tasks , each of which addresses bio - molecular event extraction at a different level of specificity .","label":"CompareOrContrast","metadata":{},"score":"65.07172"}{"text":"We test the efficacy of this method in the context of Chinese word segmentation and part - of - speech tagging , where no segmentation and POS tagging standards are widely accepted due to the lack of morphology in Chinese . ... hen it is input into the target classifier with this classification result as additional information to get the final result . by André F. T. Martins , Noah A. Smith , Pedro M. Q. Aguiar , Mário A. T. Figueiredo , 2011 . \" ...","label":"CompareOrContrast","metadata":{},"score":"65.305405"}{"text":"We divided the available WSJ data into a train and test set , trained a parser on the train set and compared errors on the test set and BIO .Accuracy dropped from 90 % on WSJ to 84 % on BIO .","label":"CompareOrContrast","metadata":{},"score":"65.33601"}{"text":"In this work , we present 1 . an effective method for pruning in split PCFGs 2 . a comparison of ... . \" ...We describe a robust accurate domain - independent approach to statistical parsing incorporated into the new release of the ANLT toolkit , and publicly available as a research tool .","label":"CompareOrContrast","metadata":{},"score":"65.52351"}{"text":"We formulate the problem of nonprojective dependency parsing as a polynomial - sized integer linear program .Our formulation is able to handle non - local output features in an efficient manner ; not only is it compatible with prior knowledge encoded as hard constraints , it can also learn soft constraint ... \" .","label":"CompareOrContrast","metadata":{},"score":"65.62578"}{"text":"Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .On the other hand , our grammars are much more compact and substantially more accurate than previous work on automatic annotation .Despite its simplicity , our best grammar achieves an F1 of 90.2 % on the Penn Treebank , higher than fully lexicalized systems . ... reebank , higher than fully lexicalized systems .","label":"CompareOrContrast","metadata":{},"score":"65.665085"}{"text":"This paper revisits an assump - tion that genre variation is continuous along multiple dimensions , and an early use of principal component analysis to find these dimensions .Results on a very heterogeneous corpus of post-1990s American English reveal four major dimensions , three of which echo those found in prior work and the fourth depending on features not used in the earlier study .","label":"CompareOrContrast","metadata":{},"score":"65.749695"}{"text":"While we believe this is not enough diversity , it was not feasible to repeat our experiment with a large number of parsers . 3.3Another approach to adaptation is to favor training examples that are similar to the target .We first mod- ified the weight given by the parser to each training sentence based on the similarity of the sentence to target domain sentences .","label":"CompareOrContrast","metadata":{},"score":"65.886955"}{"text":"It is written in C # for Windows and is free with a harmless registration .Ram hungry , but a nice piece of work .Provides multiple parses .No new versions since 2008 .The venerable Survey of English Usage at University College London weighs in with its contribution to the International Corpus of English , namely the International Corpus of English , or at least the British part of it .","label":"CompareOrContrast","metadata":{},"score":"65.971985"}{"text":"Two other demos are trained on bio - medical corpora .[ Online demos seem to be down 03/2015].SVMTool is a recent tagger using Support Vector Machines that claims very good accuracy .It is trained on WSJ corpus .","label":"CompareOrContrast","metadata":{},"score":"66.06836"}{"text":"However , parsing accuracies for Arabic usually lag behind non - semitic languages .Moreover , whil ... \" ...We formulate dependency parsing as a graphical model with the novel ingredient of global constraints .We show how to apply loopy belief propagation ( BP ) , a simple and effective tool for approximate learning and inference .","label":"CompareOrContrast","metadata":{},"score":"66.08627"}{"text":"These works aim to predict the parser performance on a given target sentence .Ravi et al .( 2008 ) frame this as a regression problem .Kawahara and Uchimoto ( 2008 ) treat ... . \" ...Genre classification has been found to improve performance in many applications of statistical NLP , including language modeling for spoken language , domain adaptation of statistical parsers , and machine translation .","label":"CompareOrContrast","metadata":{},"score":"66.12331"}{"text":"McClosky et al .( 2010 ) coined the term multiple source domain adaptation .Similar to us , McClosky et al .( 2010 ) regard a target domain as mixture of source domains , b .. by Joseph Le Roux , Jennifer Foster , Joachim Wagner , Rasul Samad , Zadeh Kaljahi , Anton Bryl . \" ...","label":"CompareOrContrast","metadata":{},"score":"66.29113"}{"text":"Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .On the other hand , our grammars are much more compact and substantially more accurate than previous work on automatic annotation .Despite its simplicity , our best grammar achieves an F1 of 89.9 % on the Penn Treebank , higher than most fully lexicalized systems .","label":"CompareOrContrast","metadata":{},"score":"66.29619"}{"text":"We then show how the parser can be relexicalized and adapted using unlabeled target language data and a learning method that can incorporate diverse knowledge sources through ambiguous labelings .In the latter scenario , we exploit two sources of knowledge : arc marginals derived from the base parser in a self - training algorithm , and arc predictions from multiple transfer parsers in an ensemble - training algorithm .","label":"CompareOrContrast","metadata":{},"score":"66.32194"}{"text":"This robustness led to the third best overall average labeled attachment score in the task , despite using no discriminative methods .We also demonstrate that the parser is quite fast , and can provide even faster parsing times without much loss of accuracy . \" ...","label":"CompareOrContrast","metadata":{},"score":"66.339554"}{"text":"This robustness led to the third best overall average labeled attachment score in the task , despite using no discriminative methods .We also demonstrate that the parser is quite fast , and can provide even faster parsing times without much loss of accuracy . \" ...","label":"CompareOrContrast","metadata":{},"score":"66.339554"}{"text":"In our method , a first - order parser derives such input features from its own previous ... . by Terry Koo , Alexander M. Rush , Michael Collins , Tommi Jaakkola , David Sontag - In Proc . of EMNLP , 2010 . \" ...","label":"CompareOrContrast","metadata":{},"score":"66.39932"}{"text":"( 2014 ) presented a task - agnostic method for learning to map input sequences to output sequences that achieved strong results on a large scale machine translation problem .In this work , we show that precisely the same sequence - to - sequence method achieves results that are close to state - of - the - art on syntactic constituency parsing , whilst making almost no assumptions about the structure of the problem .","label":"CompareOrContrast","metadata":{},"score":"66.399765"}{"text":"Shay , 2009 . \" ...We present a family of priors over probabilistic grammar weights , called the shared logistic normal distribution .This family extends the partitioned logistic normal distribution , enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar , prov ... \" .","label":"CompareOrContrast","metadata":{},"score":"66.436325"}{"text":"We present a generative model for the unsupervised learning of dependency structures .We also describe the multiplicative combination of this dependency model with a model of linear constituency .The product model outperforms both components on their respective evaluation metrics , giving the best pu ... \" .","label":"CompareOrContrast","metadata":{},"score":"66.50309"}{"text":"We also present a proof that owl - qn is guaranteed to converge to a globally optimal parameter vector . \" ...Statistical parsers trained and tested on the Penn Wall Street Journal ( WSJ ) treebank have shown vast improvements over the last 10 years .","label":"CompareOrContrast","metadata":{},"score":"66.52072"}{"text":"The beam - search decoder only requires the syntactic processing task to be broken into a sequence of decisions , such that , at each stage in the process , the decoder is able to consider the top - n candidates and generate all possibilities for the next stage .","label":"CompareOrContrast","metadata":{},"score":"66.5265"}{"text":"The beam - search decoder only requires the syntactic processing task to be broken into a sequence of decisions , such that , at each stage in the process , the decoder is able to consider the top - n candidates and generate all possibilities for the next stage .","label":"CompareOrContrast","metadata":{},"score":"66.5265"}{"text":"We first show how recent insights on selective parameter sharing , based on typological and language - family fe ... \" .We study multi - source transfer parsing for resource - poor target languages ; specifically methods for target language adaptation of delexicalized discriminative graph - based dependency parsers .","label":"CompareOrContrast","metadata":{},"score":"66.537186"}{"text":"This paper presents WOE , an open IE system which improves dramatically on TextRunner 's precision and recall .The key to WOE 's performance is a novel form of self - supervised learning for open extractors - using heuristic matches between Wikipedia infobox attribute values and corresponding sentences to construct training data .","label":"CompareOrContrast","metadata":{},"score":"66.76778"}{"text":"They also permit the use of well - understood , generalpurpose learning algorithms .There has been an increased interest in using probabilistic grammars in the Bayesian setting .To date , most of the literature has focused on using a Dirichlet prior .","label":"CompareOrContrast","metadata":{},"score":"66.831055"}{"text":"They also permit the use of well - understood , generalpurpose learning algorithms .There has been an increased interest in using probabilistic grammars in the Bayesian setting .To date , most of the literature has focused on using a Dirichlet prior .","label":"CompareOrContrast","metadata":{},"score":"66.831055"}{"text":"Its tags are spelled out as words , but the full strings of symbols can be found in the Machinese Syntax parser - grapher .Refer to table of Tag Descriptions . A. 3 .PennTree tags .The complete , detailed PennTree Guide to Part of Speech Tagging is here ( 31 pages ) .","label":"CompareOrContrast","metadata":{},"score":"66.90756"}{"text":"To globally model parsing actions of all steps that are taken on the inpu ... \" .Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .They only determine parsing actions stepwisely by a trained classifier .","label":"CompareOrContrast","metadata":{},"score":"66.94756"}{"text":"To globally model parsing actions of all steps that are taken on the inpu ... \" .Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .They only determine parsing actions stepwisely by a trained classifier .","label":"CompareOrContrast","metadata":{},"score":"66.94756"}{"text":"For ex- ample , trained on in - domain data , nouns that occur more often tend to be heads .However , none of these features transfered between domains .A final type of feature we added was based on the behavior of nouns , adjectives and verbs in each domain .","label":"CompareOrContrast","metadata":{},"score":"66.97885"}{"text":"Our generative self - trained grammars reach F scores of 91.6 on the WSJ test set and surpass even discriminative reranking systems without self - training .Additionally , we show that multiple self - trained grammars can be combined in a product model to achieve even higher accuracy .","label":"CompareOrContrast","metadata":{},"score":"67.137436"}{"text":"The CoNLL In Proc .Lawrence Saul and Fernando Pereira . gate and mixed - order markov models for statistical language modeling .In EMNLP . Aggre-1055 .Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .","label":"CompareOrContrast","metadata":{},"score":"67.15248"}{"text":"The tree with the maximal probability is outputted .The experiments are carried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser . ... arried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser .","label":"CompareOrContrast","metadata":{},"score":"67.32491"}{"text":"The tree with the maximal probability is outputted .The experiments are carried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser . ... arried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser .","label":"CompareOrContrast","metadata":{},"score":"67.32491"}{"text":"This naive grammar ... . \" ...We present several improvements to unlexicalized parsing with hierarchically state - split PCFGs .First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar wi ... \" .","label":"CompareOrContrast","metadata":{},"score":"67.41675"}{"text":"\" ...We show how web mark - up can be used to improve unsupervised dependency parsing .Starting from raw bracketings of four common HTML tags ( anchors , bold , italics and underlines ) , we refine approximate partial phrase boundaries to yield accurate parsing constraints .","label":"CompareOrContrast","metadata":{},"score":"67.66798"}{"text":"In our method the first , monolingual view consists of supervised predictors learned separately for each language .The second , bilingual view consists of log - linear predictors learned over both languages on bilingual text .Our training procedure estimates the parameters of the bilingual model using the output of the monolingual model , and we show how to combine the two models to account for dependence between views .","label":"CompareOrContrast","metadata":{},"score":"67.72335"}{"text":"Integrated annotation for biomedical information ex- traction .In Proc . of the Human Language Technol- ogy Conference and the Annual Meeting of the North American Chapter of the Association for Computa- tional Linguistics ( HLT / NAACL ) .B. MacWhinney .","label":"CompareOrContrast","metadata":{},"score":"67.77903"}{"text":"A mixture grammar fit with the EM algorithm shows improvement over a single PCFG , both in parsing accuracy and in test data likelihood .We argue that this improvement comes from the learning of specialized grammars that capture non - local correlations .","label":"CompareOrContrast","metadata":{},"score":"68.13991"}{"text":"On full - scale treebank parsing experiments , the discriminative latent models outperform both the comparable generative latent models as well as the discriminative non - latent baselines .We present a maximally streamlined approach to learning HMM - based acoustic models for automatic speech recognition .","label":"CompareOrContrast","metadata":{},"score":"68.16435"}{"text":"We provide an efficient algorithm for learning such models and show experimental evidence of the model 's improved performance over a natural baseline model and a lexicalized probabilistic context - free grammar . by Shankar Kumar , William Byrne , Speech Processing - In Proceedings of HLT - NAACL , 2004 . \" ...","label":"CompareOrContrast","metadata":{},"score":"68.175575"}{"text":"Since these annotations are new with respect to the WSJ guidelines , it is impossi- ble to parse these without injecting knowledge of the annotation guidelines.3 common , comprising 33 % of BIO and 30 % of WSJ tokens , the most popular POS tag by far .","label":"CompareOrContrast","metadata":{},"score":"68.191574"}{"text":"Adaptation techniques focus on the former since it is impossible to determine the lat- ter without knowledge of the labeling function .In parsing adaptation , the former corresponds to a dif- ference between the features seen in each domain , such as new words in the target domain .","label":"CompareOrContrast","metadata":{},"score":"68.384445"}{"text":"( Tagger is trainable HMM - type .Works for other languages too .Available for Linux and a demo version for Windows . )Version 3.1 is quite impressive and is an entry in the Great PennTree Tagger Contest ( below ) .","label":"CompareOrContrast","metadata":{},"score":"68.414154"}{"text":"most languages are projective .In Figure 8 An example Chinese dependency tree .Although non - projec ... . \" ...Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .","label":"CompareOrContrast","metadata":{},"score":"68.451805"}{"text":"most languages are projective .In Figure 8 An example Chinese dependency tree .Although non - projec ... . \" ...Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .","label":"CompareOrContrast","metadata":{},"score":"68.451805"}{"text":"Labeled data typically requires expert annotators which makes it both time consuming and costly to produce .Furthermore , once training data has been created for one textual domain , portability to similar domains is limited .This domain - dependence has inspired a large body of work since syntactic parsing aims to capture syntactic patterns across an entire language rather than just a specific domain .","label":"CompareOrContrast","metadata":{},"score":"68.4563"}{"text":"To manage this complexity , we translate into target language clusterings of increasing vocabulary size .This approach gives dramatic speed - ups while additionally increasing final translation quality .The intersection of tree transducer - based translation models with n - gram language models results in huge dynamic programs for machine translation decoding .","label":"CompareOrContrast","metadata":{},"score":"68.50641"}{"text":"The development data was 200 sentences of labeled biomedical oncology text ( BIO , the ONCO portion of the Penn Biomedical Treebank ) , as well as 200 K unlabeled sentences ( Kulick et al ., 2004 ) .The two test domains were a collection of medline chem- istry abstracts ( pchem , the CYP portion of the Penn Biomedical Treebank ) and the Child Language Data Exchange System corpus ( CHILDES ) ( MacWhin- ney , 2000 ; Brown , 1973 ) .","label":"CompareOrContrast","metadata":{},"score":"68.55697"}{"text":"In this paper , we address two issues that are related to domain adaptation .The first question is how much genre variation will affect NLP systems ' performance .We investigate the effect of genre variation on the performance of three NLP tools , namely , word segmenter , POS tagger , and parser .","label":"CompareOrContrast","metadata":{},"score":"68.59829"}{"text":"In fact , if you have the ICE - GB corpus installed , you can check the diagram for any sentence in the Oxford English Grammar .In addition , the Survey of English Usage offers an online tutorial in English syntax of the double - layered kind used in ICE .","label":"CompareOrContrast","metadata":{},"score":"68.64379"}{"text":"The cause for this is clear when the annotation guide- lines are considered .The proper nouns in WSJ are names of companies , people and places , while in BIO they are names of genes , proteins and chemi- cals .","label":"CompareOrContrast","metadata":{},"score":"69.00251"}{"text":"The new Viewer adds three features for more powerful search : wildcards , morphological inflections , and capitalization .These additions allow the discovery of patterns that were previously difficult to find and further facilitate the study of linguistic trends in printed text .","label":"CompareOrContrast","metadata":{},"score":"69.32829"}{"text":"E ... \" .We formulate dependency parsing as a graphical model with the novel ingredient of global constraints .We show how to apply loopy belief propagation ( BP ) , a simple and effective tool for approximate learning and inference .","label":"CompareOrContrast","metadata":{},"score":"69.35075"}{"text":"We present a new edition of the Google Books Ngram Corpus , which describes how often words and phrases were used over a period of five centuries , in eight languages ; it reflects 6 % of all books ever published .","label":"CompareOrContrast","metadata":{},"score":"69.37587"}{"text":"This simple framework performs surprisingly well , giving accuracy results competitive with the state - of - the - art on all the tasks we consider .The computational simplicity of the decoder and training algorithm leads to significantly higher test speeds and lower training times than their main alternatives , including log - linear and large - margin training algorithms and dynamic - programming for decoding .","label":"CompareOrContrast","metadata":{},"score":"69.44978"}{"text":"This simple framework performs surprisingly well , giving accuracy results competitive with the state - of - the - art on all the tasks we consider .The computational simplicity of the decoder and training algorithm leads to significantly higher test speeds and lower training times than their main alternatives , including log - linear and large - margin training algorithms and dynamic - programming for decoding .","label":"CompareOrContrast","metadata":{},"score":"69.44978"}{"text":"This decision effectively removes NNP from the BIO domain and renders all features that depend on the NNP tag ineffective .In our above BIO NP example , all nouns are labeled NN , whereas the WSJ example contains NNP tags .","label":"CompareOrContrast","metadata":{},"score":"69.503204"}{"text":"This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance .We describe a hierarchy of loss functions that incorporate different levels of l ... \" .We present Minimum Bayes - Risk ( MBR ) decoding for statistical machine translation .","label":"CompareOrContrast","metadata":{},"score":"69.51183"}{"text":"We present a novel approach which employs a randomized sequence of pruning masks .Formally , we apply auxiliary variable MCMC sampling to generate this sequence of masks , thereby gaining theoretical guarantees about convergence .Because each mask is generally able to skip large portions of an underlying dynamic program , our approach is particularly compelling for high - degree algorithms .","label":"CompareOrContrast","metadata":{},"score":"69.60835"}{"text":"The Noah 's Ark Research Group at Carnegie Mellon has a demo of TurboParser , which implements a syntactic parsing and graphing of sentences in Stanford Dependency relations and , along with it a FrameNet semantic parsing .The parse appears to be done directly into grammatical relations and not by conversion from a phrase stucture parse ( as with the Stanford Parser Core engine ) .","label":"CompareOrContrast","metadata":{},"score":"69.66637"}{"text":"Primary acoustic , speech , and vision systems were trained to discriminate instances of the categories .Higher - level systems exploited correlations among the categories , incorporated sequential context , and combined the joint evidence from the three information sources .","label":"CompareOrContrast","metadata":{},"score":"69.668915"}{"text":"We describe experiments on learning latent variable grammars for various German treebanks , using a language - agnostic statistical approach .In our method , a minimal initial grammar is hierarchically refined using an adaptive split - and - merge EM procedure , giving compact , accurate grammars .","label":"CompareOrContrast","metadata":{},"score":"70.05898"}{"text":"An example is the above BIO NP , in which the phrase \" glutathione transferase P1 - 1 \" is an appositive indicating which \" enzyme \" is meant .However , since there are no commas , the parser thinks \" P1 - 1 \" is the head .","label":"CompareOrContrast","metadata":{},"score":"70.07004"}{"text":"On WSJ15 , we attain a state - of - the - art F - score of 90.9 % , a 14 % relative reduction in error over previous models , while being two orders of magnitude faster .On sentences of length 40 , our system achieves an F - score of 89.0 % , a 36 % relative reduction in error over a generative baseline . ...","label":"CompareOrContrast","metadata":{},"score":"70.113754"}{"text":"At its ... \" .Genre classification has been found to improve performance in many applications of statistical NLP , including language modeling for spoken language , domain adaptation of statistical parsers , and machine translation .It has also been found to benefit retrieval of spoken or written docu - ments .","label":"CompareOrContrast","metadata":{},"score":"70.11421"}{"text":"It is inspired by cube pruning ( Chiang , 2007 ; ... \" .We introduce cube summing , a technique that permits dynamic programming algorithms for summing over structures ( like the forward and inside algorithms ) to be extended with non - local features that violate the classical structural independence assumptions .","label":"CompareOrContrast","metadata":{},"score":"70.14979"}{"text":"You can submit a paragraph of up to 300 words to the tagger and it will return a tagged version fairly quickly .You can choose coarser or finer tag set , using CLAWS 5 ( 60 parts of speech - used for bulk of BNC ) or CLAWS 7 ( over 160 parts - used for the BNC sampler , BNC World , and BNC XML ) .","label":"CompareOrContrast","metadata":{},"score":"70.22633"}{"text":"In this work , we present 1 . an effective method for pruning in split PCFGs 2 . a comparison of objective functions for infe ... . \" ...The l - bfgs limited - memory quasi - Newton method is the algorithm of choice for optimizing the parameters of large - scale log - linear models with L2 regularization , but it can not be used for an L1-regularized loss due to its non - differentiability whenever some parameter is zero .","label":"CompareOrContrast","metadata":{},"score":"70.762764"}{"text":"3 Adaptation Approaches We survey the main approaches we explored for this task .While some of these approaches provided a modest performance boost to a simple parser ( lim- ited data and first - order features ) , no method added any performance to our best parser ( all data and second - order features ) .","label":"CompareOrContrast","metadata":{},"score":"71.38469"}{"text":"Also , Bernard Bou 's Java - based GrammarScope uses the Stanford package and can display both Phrase Structure trees and grammatical relations as colors .VERY nice once you get the hang of it ( you can paste sentences in from the clipboard as well as feed it text files ) .","label":"CompareOrContrast","metadata":{},"score":"71.534805"}{"text":"Despite the much simplified training process , our acoustic model achieves state - of - the - art results on phone classification ( where it outperforms almost all other methods ) and competitive performance on phone recognition ( where it outperforms standard CD triphone / subphone / GMM approaches ) .","label":"CompareOrContrast","metadata":{},"score":"71.70923"}{"text":"Since the guidelines differ , we observe no corresponding structure in the WSJ .It is telling that the parser labels this BIO example by attaching ev- ery token to the final proper noun \" P1 - 1 \" , exactly as the WSJ guidelines indicate .","label":"CompareOrContrast","metadata":{},"score":"71.915306"}{"text":"LLCCM retains the simplicity of the original CCM but extends robustly to long sentences .On sentences of up to length 40 , LLCCM outperforms CCM by 13.9 % bracketing F1 and outperforms a right - branching baseline in regimes where CCM does not . ... ependency grammar induction .","label":"CompareOrContrast","metadata":{},"score":"72.0011"}{"text":"We present a number of semi - supervised parsing experiments on the Irish language carried out using a small seed set of manually parsed trees and a larger , yet still relatively small , set of unlabelled sentences .We take two popular dependency parsers - one graph - based and one transition - based - and ... \" .","label":"CompareOrContrast","metadata":{},"score":"72.18631"}{"text":"To facilitate future research in unsupervised induction of syntactic structure and to standardize best - practices , we propose a tagset that consists of twelve universal part - of - speech categories .In addition to the tagset , we develop a mapping from 25 different treebank tagsets to this universal set .","label":"CompareOrContrast","metadata":{},"score":"72.36929"}{"text":"To facilitate future research in unsupervised induction of syntactic structure and to standardize best - practices , we propose a tagset that consists of twelve universal part - of - speech categories .In addition to the tagset , we develop a mapping from 25 different treebank tagsets to this universal set .","label":"CompareOrContrast","metadata":{},"score":"72.36929"}{"text":"Many variant layouts available .Antelope logo Proxem Antelope is a package of taggers , chunkers , parsers , and graphers that can draw trees that are both PennTree constituent style and marked for grammatical relations ( using the Stanford parser ) .","label":"CompareOrContrast","metadata":{},"score":"72.6636"}{"text":"We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data .Results indicate that ( a ) MST - parser performs better on Hebrew data than Malt - Parser , and ( b ) both parsers do not make good use of morphological information when parsing Hebrew . ... s on Hebrew dependency parsing .","label":"CompareOrContrast","metadata":{},"score":"73.30885"}{"text":"We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data .Results indicate that ( a ) MST - parser performs better on Hebrew data than Malt - Parser , and ( b ) both parsers do not make good use of morphological information when parsing Hebrew . ... s on Hebrew dependency parsing .","label":"CompareOrContrast","metadata":{},"score":"73.30885"}{"text":"This family extends the partitioned logistic normal distribution , enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar , providing a new way to encode prior knowledge about an unknown grammar .We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors .","label":"CompareOrContrast","metadata":{},"score":"73.59364"}{"text":"As a parsing algorithm , BP is both asymptotically and empirically efficient .E ... \" .We formulate dependency parsing as a graphical model with the novel ingredient of global constraints .We show how to apply loopy belief propagation ( BP ) , a simple and effective tool for approximate learning and inference .","label":"CompareOrContrast","metadata":{},"score":"73.64955"}{"text":"We present LLCCM , a log - linear variant of the constituent context model ( CCM ) of grammar induction .LLCCM retains the simplicity of the original CCM but extends robustly to long sentences .On sentences of up to length 40 , LLCCM outperforms CCM by 13.9 % bracketing F1 and outperforms a right - branchin ... \" .","label":"CompareOrContrast","metadata":{},"score":"73.67842"}{"text":"The Stanford Parser is used to derive dependencies from CJ50 and gold parse trees .Figure 8 shows the detailed P / R curves .We can see that although today ... .by Jenny Rose Finkel , Alex Kleeman , Christopher D. Manning - In Proc .","label":"CompareOrContrast","metadata":{},"score":"74.0586"}{"text":"Even with second - order features or latent variables , which would make exact parsing considerably slower or NP - hard , BP needs only O(n3 ) time with a small constant factor .Furthermore , such features significantly improve parse accuracy over exact first - order methods .","label":"CompareOrContrast","metadata":{},"score":"74.0889"}{"text":"Even with second - order features or latent variables , which would make exact parsing considerably slower or NP - hard , BP needs only O(n3 ) time with a small constant factor .Furthermore , such features significantly improve parse accuracy over exact first - order methods .","label":"CompareOrContrast","metadata":{},"score":"74.0889"}{"text":"The penalty for a recognition failure is often small : if two con- figurations are confused , they are often similar to each other , and the illusion works well enough , for instance , to drive a graphics animation of the moving hand .","label":"CompareOrContrast","metadata":{},"score":"74.09779"}{"text":"The results show that an unsupervised technique based on topic models is effective - it outperforms random data selection on both languages examined , English and Dutch .Moreover , the technique works better than manually assigned labels gathered from meta - data that is available for English . ...","label":"CompareOrContrast","metadata":{},"score":"74.339966"}{"text":"Tested across six domains , our system outperforms all non - oracle baselines including the best domain - independent parsing model .Thus , we are able to demonstrate the value of customizing parsing models to specific domains . ... train models in many different domains but sidestep the problem of domain detection .","label":"CompareOrContrast","metadata":{},"score":"74.8963"}{"text":"Computers fail to track these in fast video , but sleight of hand fools humans as well : what happens too quickly we just can not see .We show a 3D tracker for these types of motions that relies on the recognition of familiar configurations in 2D images ( classification ) , and fills the gaps in - between ( interpolation ) .","label":"CompareOrContrast","metadata":{},"score":"74.90393"}{"text":"Another problem concerns appositives .For ex- ample , the phrase \" Howard Mosher , president and chief executive officer , \" has \" Mosher \" as the head of \" Howard \" and of the appositive NP delimited by commas .","label":"CompareOrContrast","metadata":{},"score":"75.020935"}{"text":"WOE can operate in two modes : when restricted to POS tag features , it runs as quickly as TextRunner , but when set to use dependency - parse features its precision and recall rise even higher . ... h recall .","label":"CompareOrContrast","metadata":{},"score":"75.316826"}{"text":"Across various hierarchical encoding schemes and for multiple language pairs , we show speed - ups of up to 50 times over single - pass decoding while improving BLEU score .Moreover , our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram - to - trigram decoder .","label":"CompareOrContrast","metadata":{},"score":"75.64561"}{"text":"In Proc . of the 16th Nordic Conference on Computational Linguistics ( NODALIDA ) .Extended Sandra K¨ ubler . schemes influence parsing results ? or how not to com- pare apples and oranges .In RANLP .How do treebank annotation 1054 .","label":"CompareOrContrast","metadata":{},"score":"76.28841"}{"text":"We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task .For languages from different families the improvements often exceed 2 BLEU .Many of these gains are also significant in human evaluations .We present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages : German , English , Swedish , Spanish , French and Korean .","label":"CompareOrContrast","metadata":{},"score":"76.4088"}{"text":"NN , NNP - NNP - NNP , NN - IN - NN , and IN - NN - NN .However , when we examine the coarse POS tags , which do not distinguish between nouns , these dif- ferences disappear .","label":"CompareOrContrast","metadata":{},"score":"76.74645"}{"text":"The l - bfgs limited - memory quasi - Newton method is the algorithm of choice for optimizing the parameters of large - scale log - linear models with L2 regularization , but it can not be used for an L1-regularized loss due to its non - differentiability whenever some parameter is zero .","label":"CompareOrContrast","metadata":{},"score":"76.808846"}{"text":"It too will produce analysis in terms of grammatical dependency relations ( in RASP set of relations ) .Try the demo with \" GR \" ticked .VISL under Eckhard Bick has extensive tools for tagging , parsing , and graphing , and not just for English .","label":"CompareOrContrast","metadata":{},"score":"78.2992"}{"text":"We focus on parsing algorithms for nonprojective head automata , a generalization of head - automata models to non - projective structures .The dual decomposition algorithms are simple and efficient , relying on standa ... \" .This paper introduces algorithms for nonprojective parsing based on dual decomposition .","label":"CompareOrContrast","metadata":{},"score":"78.35827"}{"text":"Meanwhile , Graphics Processor Units ( GPUs ) have become widely available , offering the opportunity to alleviate this bottleneck by exploiting the fine - grained data parallelism found in the CKY algorithm .In this paper , we explore the design space of parallelizing the dynamic programming computations carried out by the CKY algorithm .","label":"CompareOrContrast","metadata":{},"score":"78.49265"}{"text":"We selected with replacement 2000 training examples from the training data and trained three parsers .Each parser then tagged the remain- ing 13 K sentences , yielding 39 K parsed sentences .We then shuffled these sentences and trained a final parser .","label":"CompareOrContrast","metadata":{},"score":"79.759094"}{"text":"We present an algorithm Orthant - Wise Limited - memory Quasi - Newton ( owlqn ) , based on l - bfgs , that can efficiently optimize the L1-regularized log - likelihood of log - linear models with millions of parameters .","label":"CompareOrContrast","metadata":{},"score":"80.110855"}{"text":"These include beam search ... . by Zhenghua Li , Ting Liu , Wanxiang Che - In Proceedings of the 50th ACL , pages 675 - 684 , Jeju Island , Korea , 2012 . \" ...We present a simple and effective framework for exploiting multiple monolingual treebanks with different annotation guidelines for pars - ing .","label":"CompareOrContrast","metadata":{},"score":"80.82613"}{"text":"Harvard University Press .Koby Crammer , Ofer Dekel , Joseph Keshet , Shai Shalev-Shwartz , and Yoram Singer .Online passive- aggressive algorithms .Journal of Machine Learning Research , 7:551 - 585 , Mar. R. Johansson and P. Nugues .","label":"CompareOrContrast","metadata":{},"score":"81.52532"}{"text":"DGA - the dependency Grammar Annotator- is a little Java - based tool for drawing dependency trees with labels .The online demo offers one set of labels and relations ; to customize the list , you have to download the DGA and change the Configuration file .","label":"CompareOrContrast","metadata":{},"score":"81.53824"}{"text":"In this paper , we address two issues that are related to domain adaptation .The first question is how much genre variation will affect NLP systems ' per ... \" .Domain adaptation is an important task in order for NLP systems to work well in real applications .","label":"CompareOrContrast","metadata":{},"score":"82.149605"}{"text":"Version 2 ! draws dependency arcs and other CoreNLP tasks ( NER , coreference ) .Bou has also written a Java - based grapher tydevi ( typed dependencies viewer ) that uses the Stanford parser to draw static diagrams with curved labelled nodes .","label":"CompareOrContrast","metadata":{},"score":"82.33394"}{"text":"5 Acknowledgments We thank Joel Wallenberg and Nikhil Dinesh for their informative and helpful linguistic expertise , Kevin Lerman for his edge labeler code , and Koby Crammer for helpful conversations .Dredze is sup- ported by a NDSEG fellowship ; Ganchev and Taluk- dar by NSF ITR EIA-0205448 ; and Blitzer by DARPA under Contract No . NBCHD03001 .","label":"CompareOrContrast","metadata":{},"score":"82.39736"}{"text":"Ball Tracking :The reliable tracking of the ball is vital in robot soccer .Therefore a Kalman - filter based system for estimating the ball position and velocity in the presence of occlusions is developped . -Sensor Fusion : The robot perceives its environment through several independent sensors ( camera , odometer , etc . ) , which have different delays .","label":"CompareOrContrast","metadata":{},"score":"82.5409"}{"text":"Finally , we show that we can significantly improve target language performance , even after annotating up to 64,000 tokens in the target language , by simply concatenating source and target language annotations .Here we perform a set of experiments where we investigate the potential of multi - source transfer for NER , in German ( DE ) , English ( EN ) , Spanish ( ES ) and Dutch ( NL ) , using cro ... . \" ...","label":"CompareOrContrast","metadata":{},"score":"82.86682"}{"text":"For instance , 14.4 % of section 23 is tagged differently by ( 1 ) and ( 2 ) 8 .5 The Neutral Edge Direction ( NED ) Me ... . by Shay B. Cohen , Noah A. Smith , Alex Clark , Dorota Glowacka , Colin De La Higuera , Mark Johnson , John Shawe - taylor . \" ...","label":"CompareOrContrast","metadata":{},"score":"83.30032"}{"text":"For instance , 14.4 % of section 23 is tagged differently by ( 1 ) and ( 2 ) 8 .5 The Neutral Edge Direction ( NED ) Me ... . by Shay B. Cohen , Noah A. Smith , Alex Clark , Dorota Glowacka , Colin De La Higuera , Mark Johnson , John Shawe - taylor . \" ...","label":"CompareOrContrast","metadata":{},"score":"83.30032"}{"text":"Old Time Religion Gene Moutoux of Eastern High School in Louisville , KY , ret.,has put up extensive tutorial examples of sentences diagrammed according to Reed - Kellogg principles ( 1877 et seq . )For more than a century , this was sentence diagramming in America .","label":"CompareOrContrast","metadata":{},"score":"83.84407"}{"text":"The small training size and overregularization of the QG parser mildly hurts in - domain parsing performance .825 % Dependency Accuracy on Target CoNLL - PrepHead CoNLL - PrepChild Prague - PrepHead Prague ... . \" ...Manually annotated corpora are valuable but scarce resources , yet for many annotation tasks such as treebanking and sequence labeling there exist multiple corpora with different and incompatible annotation guidelines or standards .","label":"CompareOrContrast","metadata":{},"score":"84.56697"}{"text":"Behavior Control : Finally we show how all these elements can be incorporated into a goal keeping robot .We develop simple behaviors that can be used in a layered architecture and enable the robot to block most balls that are being shot at the goal . A. 1 CLAWS tag set(s ) .","label":"CompareOrContrast","metadata":{},"score":"84.58507"}{"text":"These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .We present a new measure , Neutral Edge Direction ( NED ) , and show that it greatly reduces this undesired phenomenon .","label":"CompareOrContrast","metadata":{},"score":"85.14365"}{"text":"These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .We present a new measure , Neutral Edge Direction ( NED ) , and show that it greatly reduces this undesired phenomenon .","label":"CompareOrContrast","metadata":{},"score":"85.14365"}{"text":"These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .We present a new measure , Neutral Edge Direction ( NED ) , and show that it greatly reduces this undesired phenomenon .","label":"CompareOrContrast","metadata":{},"score":"85.14365"}{"text":"Publisher conditions are provided by RoMEO .Differing provisions from the publisher 's actual policy or licence agreement may be applicable .\" One of the main challenges in natural language processing ( NLP ) is to correct for biases in the manually annotated data available to system engineers .","label":"CompareOrContrast","metadata":{},"score":"87.19561"}{"text":"Downloaded version also has chunker .Binaries available for Linux and Windows , also a GUI for Windows .FreeLing has been developed by the TALP Research Center at the Polytechnic University of Catelona .It includes a tagger with on - line ( limited ) demo and is downloable for Linux / Unix .","label":"CompareOrContrast","metadata":{},"score":"89.30788"}{"text":"The shared task was run over 12 weeks , drawing initial interest from 42 teams .Of these teams , 24 submitted final results .The evaluation results are encouraging , indicating that state - of - the - art performance is approaching a practically applicable level and revealing some remaining challenges . ... parsers . \" ...","label":"CompareOrContrast","metadata":{},"score":"90.0343"}{"text":"The dual decomposition algorithms are simple and efficient , relying on standard dynamic programming and minimum spanning tree algorithms .They provably solve an LP relaxation of the non - projective parsing problem .Empirically the LP relaxation is very often tight : for many languages , exact solutions are achieved on over 98 % of test sentences .","label":"CompareOrContrast","metadata":{},"score":"94.667206"}{"text":"No demo , but a cross - platform Java program .They test out very well .No demos .Great PennTree Tagger Contest : Results of the second heat : Here are slightly edited taggings of Lincoln 's Gettysburg Address by SVM Tool , OpenNLP , TreeTagger , Stanford Tagger , CCG , and FreeLing tagger , with CLAWS added for comparison , though with different tag set .","label":"CompareOrContrast","metadata":{},"score":"97.876976"}{"text":"Proceedings of the CoNLL Shared Task Session of EMNLP - CoNLL 2007 , pp .1051 - 1055 , Prague , June 2007 .c ?upenn.edu 2L2F - INESC - ID Lisboa / IST , Rua Alves Redol 9 , 1000 - 029 , Lisboa , Portugal javg@l2f.inesc-id.pt Abstract We describe some challenges of adaptation in the 2007 CoNLL Shared Task on Domain Adaptation .","label":"CompareOrContrast","metadata":{},"score":"101.114914"}{"text":"This Master 's thesis describes parts of the control software used by the soccer robots of the Free University of Berlin , the so called FU - Fighters .The FU - Fighters compete in the Middle Sized League of RoboCup and reached the semi - finals during the 2004 RoboCup World Cup in Lisbon , Portugal .","label":"CompareOrContrast","metadata":{},"score":"109.59735"}{"text":"The others are : .George L. Dillon University of Washington 4 November 1999 Revised 10 March 2000 Again , 16 April 2000 Again , 12 February 2001 Again , 18 November 2001 , January 2003 , 2004 , March 2005 , May 2007 , December 2009 , November 2010 , March 2015","label":"CompareOrContrast","metadata":{},"score":"111.48179"}