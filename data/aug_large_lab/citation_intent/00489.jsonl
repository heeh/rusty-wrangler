{"text":"It is therefore possible to study its computational generation , comprehension , and learning properties .Some results on generation are presented .Unlike less restricted theories using Generalized Alignment , OTP grammars can derive optimal surface forms with finite - state methods adapted from Ellison ( 1994 ) .","label":"Background","metadata":{},"score":"23.738678"}
{"text":"It is therefore possible to study its computational generation , comprehension , and learning properties .Some results on generation are presented .Unlike less restricted theories using Generalized Alignment , OTP grammars can derive optimal surface forms with finite - state methods adapted from Ellison ( 1994 ) .","label":"Background","metadata":{},"score":"23.738678"}
{"text":"Among the additional ideas provided in this p .. ...We develop an extensible description logic for stating the content of optimalitytheoretic constraints in phonology , and specify a class of structures for interpreting it .The aim is a transparent formalisation of OT .","label":"Background","metadata":{},"score":"25.018051"}
{"text":"However , OT grammars - and even the very simple OTP grammars - are more powerful than FSTs .This makes them linguistically suspect as well as inconvenient .Directional Constraint Evaluation in Optimality Theory ( COLING 2000 ) offers a solution .","label":"Background","metadata":{},"score":"29.045357"}
{"text":"We develop an extensible description logic for stating the content of optimalitytheoretic constraints in phonology , and specify a class of structures for interpreting it .The aim is a transparent formalisation of OT .We show how to state a wide range of constraints , including markedness , input - output faithfulness and base - reduplicant faithfulness .","label":"Background","metadata":{},"score":"29.849056"}
{"text":"The new algorithm has been implemented and used in a large parsing experiment ( Eisner 1996 ) .We also give a useful extension to the case where the parser must undo a stochastic transduction that has altered the input .This paper introduces computational linguists to primitive Optimality Theory ( OTP ) , a clean and linguistically motivated formalization of OT .","label":"Background","metadata":{},"score":"30.72381"}
{"text":"The new algorithm has been implemented and used in a large parsing experiment ( Eisner 1996 ) .We also give a useful extension to the case where the parser must undo a stochastic transduction that has altered the input .This paper introduces computational linguists to primitive Optimality Theory ( OTP ) , a clean and linguistically motivated formalization of OT .","label":"Background","metadata":{},"score":"30.72381"}
{"text":"We then unify these variants , showing that compilation is possible if all components of the grammar are regular relations , including the harmony ordering on scored candidates .A side benefit of our construction is a far simpler implementation of directional OT ( Eisner , 2000 ) .","label":"Background","metadata":{},"score":"31.001837"}
{"text":"We then unify these variants , showing that compilation is possible if all components of the grammar are regular relations , including the harmony ordering on scored candidates .A side benefit of our construction is a far simpler implementation of directional OT ( Eisner , 2000 ) .","label":"Background","metadata":{},"score":"31.001837"}
{"text":"I was in graduate school when Optimality Theory took over phonology .There was no computational treatment of OT yet .I provided key finite - state algorithms for generation and comprehension , and proposed plausible modifications to the formalism to keep it within finite - state power .","label":"Background","metadata":{},"score":"32.502827"}
{"text":"I was in graduate school when Optimality Theory took over phonology .There was no computational treatment of OT yet .I provided key finite - state algorithms for generation and comprehension , and proposed plausible modifications to the formalism to keep it within finite - state power .","label":"Background","metadata":{},"score":"32.502827"}
{"text":"The \" primitive OT \" analysis of metrical stress is arguably superior because it predicted previously unexplained typological gaps .More recently , my students and I have worked on recovering the phonological underlying forms of a language , jointly with a probabilistic phonology .","label":"Background","metadata":{},"score":"32.588413"}
{"text":"The \" primitive OT \" analysis of metrical stress is arguably superior because it predicted previously unexplained typological gaps .More recently , my students and I have worked on recovering the phonological underlying forms of a language , jointly with a probabilistic phonology .","label":"Background","metadata":{},"score":"32.588413"}
{"text":"To treat Optimality Theory ( OT ) as a real proposal , one must put some cards on the table : What kinds of constraints may an OT grammar state ?And how can anyone tell what data this grammar predicts , without constructing infinite tableaux ?","label":"Background","metadata":{},"score":"33.76946"}
{"text":"To treat Optimality Theory ( OT ) as a real proposal , one must put some cards on the table : What kinds of constraints may an OT grammar state ?And how can anyone tell what data this grammar predicts , without constructing infinite tableaux ?","label":"Background","metadata":{},"score":"33.76946"}
{"text":"This paper successfully reanalyzes \" iterative \" metrical stress within OTP , not using GA at all .The new analysis is arguably better : it explains typological gaps that were previously mysterious or unremarked .One would like to compile phonological grammars into finite - state transducers ( FSTs ) .","label":"Background","metadata":{},"score":"34.070652"}
{"text":"The chapter also touches upon unsupervised grammar induction and reviews recent progress on learning syntactic structure from unannotated language samples .It closes with a discussion on methods for finite - state approximations .The authors emphasize the point that processing with context - free models comes at a large computational cost relative to finite - state models , and it is therefore useful to start with a context - free model and modify it to make it finite - state equivalent .","label":"Background","metadata":{},"score":"35.056866"}
{"text":"The chapter also touches upon unsupervised grammar induction and reviews recent progress on learning syntactic structure from unannotated language samples .It closes with a discussion on methods for finite - state approximations .The authors emphasize the point that processing with context - free models comes at a large computational cost relative to finite - state models , and it is therefore useful to start with a context - free model and modify it to make it finite - state equivalent .","label":"Background","metadata":{},"score":"35.056866"}
{"text":"Theoretically , this may be justified as ( discriminatively ) minimizing an imputed empirical risk .Empirically , we demonstrate that augmenting supervised training with unsupervised data improves translation performance over the supervised case for both IWSLT and NIST tasks .The field of statistical natural language processing has been turning toward morphologically rich languages .","label":"Background","metadata":{},"score":"35.39048"}
{"text":"Theoretically , this may be justified as ( discriminatively ) minimizing an imputed empirical risk .Empirically , we demonstrate that augmenting supervised training with unsupervised data improves translation performance over the supervised case for both IWSLT and NIST tasks .The field of statistical natural language processing has been turning toward morphologically rich languages .","label":"Background","metadata":{},"score":"35.39048"}
{"text":"First , it discusses how to perform comprehension under Optimality Theory grammars consisting of finite - state constraints .Comprehension has not been much studied in OT ; we show that unlike production , it does not always yield a regular set , making finite - state methods inapplicable .","label":"Background","metadata":{},"score":"36.05652"}
{"text":"First , it discusses how to perform comprehension under Optimality Theory grammars consisting of finite - state constraints .Comprehension has not been much studied in OT ; we show that unlike production , it does not always yield a regular set , making finite - state methods inapplicable .","label":"Background","metadata":{},"score":"36.05652"}
{"text":"Though Chomsky argued convincingly that such approaches can not fully model grammaticality judgments on word sequences , there are reasons to use them : .Many other linguistic phenomena do appear to be genuinely finite - state : phonology , morphology , subcategorization ( the right - hand sides of phrase - structure rules ) , and syntactic chunking .","label":"Background","metadata":{},"score":"37.465813"}
{"text":"This algorithm will work for any grammar in Optimality Theory employing regular position structures and universal constraints which may be evaluated on the basis of local information .Computing Optimal Forms In Optimality Theory : Basic Syllabification . ... are unviolated in optimal forms , so I here treat them as part of GEN .","label":"Background","metadata":{},"score":"38.809532"}
{"text":"Grammar learning is always a great problem .Unfortunately , in OT , it is hard ( coNP - complete ) even to check whether a given grammar generates the observed pronunciation .The learning problem of finding such a grammar is even harder .","label":"Background","metadata":{},"score":"38.94428"}
{"text":"In order to do this computational work , I first had to conjecture a universal set of legal constraints ( i.e. , universal grammar ) .Nearly all the constraints I found in hundreds of OT papers fit into my simple taxonomy of \" primitive constraints .","label":"Background","metadata":{},"score":"39.34449"}
{"text":"In order to do this computational work , I first had to conjecture a universal set of legal constraints ( i.e. , universal grammar ) .Nearly all the constraints I found in hundreds of OT papers fit into my simple taxonomy of \" primitive constraints .","label":"Background","metadata":{},"score":"39.34449"}
{"text":"Further work beginning in 2015 extended the approach to use latent underlying morphs , allowing it to treat derivational morphology as well .Most syntax - based models of translation assume that in training data , a sentence and its translation have isomorphic syntactic structure .","label":"Background","metadata":{},"score":"39.417465"}
{"text":"Further work beginning in 2015 extended the approach to use latent underlying morphs , allowing it to treat derivational morphology as well .Most syntax - based models of translation assume that in training data , a sentence and its translation have isomorphic syntactic structure .","label":"Background","metadata":{},"score":"39.417465"}
{"text":"But our contribution is positive .Proponents of both output - output correspondence and sympathy have offered alternatives that fit into the general OT picture .We show how to state these in a reasonable extension of our formalism .The problematic constraint types were developed to deal with opaque phenomena .","label":"Background","metadata":{},"score":"39.98491"}
{"text":"We exhibit a linear - time chart parsing algorithm with a low grammar constant . \" Bootstrapping \" methods for learning require a small amount of supervision to seed the learning process .We show that it is sometimes possible to eliminate this last bit of supervision , by trying many candidate seeds and selecting the one with the most plausible outcome .","label":"Background","metadata":{},"score":"40.144493"}
{"text":"We exhibit a linear - time chart parsing algorithm with a low grammar constant . \" Bootstrapping \" methods for learning require a small amount of supervision to seed the learning process .We show that it is sometimes possible to eliminate this last bit of supervision , by trying many candidate seeds and selecting the one with the most plausible outcome .","label":"Background","metadata":{},"score":"40.144493"}
{"text":"Some of this reduction is shown to stem from the transformation model 's ability to match observed probabilities , and some from its ability to generalize .Model averaging yields a final 24 % perplexity reduction .We consider the problem of ranking a set of OT constraints in a manner consistent with data .","label":"Background","metadata":{},"score":"40.62146"}
{"text":"Some of this reduction is shown to stem from the transformation model 's ability to match observed probabilities , and some from its ability to generalize .Model averaging yields a final 24 % perplexity reduction .We consider the problem of ranking a set of OT constraints in a manner consistent with data .","label":"Background","metadata":{},"score":"40.62146"}
{"text":"Under categorial grammars that have powerful rules like composition , a simple n - word sentence can have exponentially many parses that are semantically equivalent .Generating all parses is inefficient and obscures whatever true semantic ambiguities are in the input .","label":"Background","metadata":{},"score":"41.20291"}
{"text":"Under categorial grammars that have powerful rules like composition , a simple n - word sentence can have exponentially many parses that are semantically equivalent .Generating all parses is inefficient and obscures whatever true semantic ambiguities are in the input .","label":"Background","metadata":{},"score":"41.20291"}
{"text":"Stoyanov et al .( 2011 ) recently argued for training parameters to minimize the task - specific loss of whatever approximate inference and decoding methods will be used at test time .We apply their method to three NLP problems , showing that ( i ) using more complex CRFs leads to improved performance , and that ( ii ) minimum - risk training learns more accurate models .","label":"Background","metadata":{},"score":"41.408432"}
{"text":"Stoyanov et al .( 2011 ) recently argued for training parameters to minimize the task - specific loss of whatever approximate inference and decoding methods will be used at test time .We apply their method to three NLP problems , showing that ( i ) using more complex CRFs leads to improved performance , and that ( ii ) minimum - risk training learns more accurate models .","label":"Background","metadata":{},"score":"41.408432"}
{"text":"Such use of bracketed strings and local trees gives rise to methods that decompose tree automata into simpler components many of which have regular yields .The relevant topics include : . representation of tree grammars and tree automata through decompositions into constraints .","label":"Background","metadata":{},"score":"41.57457"}
{"text":"We discuss implications for learning from overt data only , as well as other learning issues .We argue that Optimality Theory promotes confluence of the demands of more effective learnability and deeper linguistic explanation .We propose a theory of phonotactic grammars and a learning algorithm that constructs such grammars from positive evidence .","label":"Background","metadata":{},"score":"41.707977"}
{"text":"While most work has been focused on developing problem - specific techniques , little is known about how to systematically design the node searching strategy on a branch - and - bound tree .We address the key challenge of learning an adaptive node searching order for any class of problem solvable by branch - and - bound .","label":"Background","metadata":{},"score":"41.798843"}
{"text":"While most work has been focused on developing problem - specific techniques , little is known about how to systematically design the node searching strategy on a branch - and - bound tree .We address the key challenge of learning an adaptive node searching order for any class of problem solvable by branch - and - bound .","label":"Background","metadata":{},"score":"41.798843"}
{"text":"Connections between such families and NLP tasks have not yet been fully elaborated .If we are lucky , the study of the connections will lead to new fundamental results that give rise to freshly identified subfamilies of finite state methodologies in NLP .","label":"Background","metadata":{},"score":"41.877964"}
{"text":"The Linear Ordering Problem has the most attractive computational properties of the three , all of which are NP - hard optimization problems .The dissertation expends significant effort developing neighborhoods for local search on the LOP , and uses grammars and other tools from natural language parsing to introduce several new results , including a state - of - the - art local search procedure .","label":"Background","metadata":{},"score":"41.983376"}
{"text":"The Linear Ordering Problem has the most attractive computational properties of the three , all of which are NP - hard optimization problems .The dissertation expends significant effort developing neighborhoods for local search on the LOP , and uses grammars and other tools from natural language parsing to introduce several new results , including a state - of - the - art local search procedure .","label":"Background","metadata":{},"score":"41.983376"}
{"text":"feasible constituent , dependency and alignment bracketing schemes for grammars , tree banks and parallel corpora . intersection grammars and conjunctive grammars with recognizable sets of string or trees .chart parsing , restarting automata , nested word automata and visibly pushdown languages .","label":"Background","metadata":{},"score":"42.225372"}
{"text":"This was the dawn for the currently existing finite state approached to phonology and morphology with wide - spread applications to written languages .In addition , many other NLP areas admit that they employ finite state systems or their extensions .","label":"Background","metadata":{},"score":"42.446266"}
{"text":"Such approximations can also be reasonable theories of human performance , since in practice humans can not process more than a couple of levels of center - embedding .Finite - state machines can implement common engineering models for many tasks : speech recognition , machine translation , segmentation , normalization , part - of - speech tagging , and other markup .","label":"Background","metadata":{},"score":"42.8957"}
{"text":"The fundamental result in these fields states that one can construct finite state transducers aka lexical transducers from phonological ( Johnson 1972 , Kaplan and Kay 1994 ) and morphological grammars ( Koskenniemi 1983 , Karttunen 1994 , Beesley and Karttunen 2003 ) .","label":"Background","metadata":{},"score":"42.92312"}
{"text":"Our new methods are shown to achieve significant improvements over maximum likelihood estimation and maximum a posteriori estimation , using the EM algorithm , for a state - of - the - art probabilistic model used in dependency grammar induction ( Klein and Manning , 2004 ) .","label":"Background","metadata":{},"score":"42.95989"}
{"text":"Our new methods are shown to achieve significant improvements over maximum likelihood estimation and maximum a posteriori estimation , using the EM algorithm , for a state - of - the - art probabilistic model used in dependency grammar induction ( Klein and Manning , 2004 ) .","label":"Background","metadata":{},"score":"42.95989"}
{"text":"This model is based on a generative story for inflectional morphology that naturally incorporates common linguistic notions , such as lexemes , paradigms and inflections .Sampling algorithms are presented that perform inference over large text corpora and their implicit , hidden morphological paradigms .","label":"Background","metadata":{},"score":"42.962574"}
{"text":"This model is based on a generative story for inflectional morphology that naturally incorporates common linguistic notions , such as lexemes , paradigms and inflections .Sampling algorithms are presented that perform inference over large text corpora and their implicit , hidden morphological paradigms .","label":"Background","metadata":{},"score":"42.962574"}
{"text":"We also relate bootstrapping to entropy regularization and apply it in a feature - rich setting of grammar induction .Intelligent systems may be structured to do approximate probabilistic inference under some carefully crafted model .However , they should be trained discriminatively , so that their actual decisions or predictions maximize task - specific performance .","label":"Background","metadata":{},"score":"43.05287"}
{"text":"We also relate bootstrapping to entropy regularization and apply it in a feature - rich setting of grammar induction .Intelligent systems may be structured to do approximate probabilistic inference under some carefully crafted model .However , they should be trained discriminatively , so that their actual decisions or predictions maximize task - specific performance .","label":"Background","metadata":{},"score":"43.05287"}
{"text":"Beyond devising exact algorithms , we have developed several principled approximations for speeding up parsing , both for basic models and for enriched models where exact parsing would be impractical .A number of our papers ( not all shown below ) try to improve the actual models of linguistic syntax that are used in parsing .","label":"Background","metadata":{},"score":"43.065674"}
{"text":"Beyond devising exact algorithms , we have developed several principled approximations for speeding up parsing , both for basic models and for enriched models where exact parsing would be impractical .A number of our papers ( not all shown below ) try to improve the actual models of linguistic syntax that are used in parsing .","label":"Background","metadata":{},"score":"43.065674"}
{"text":"( 2015 ) extended the morphological modeling approach to handle latent underlying morphs and derivational morphology .We connect two scenarios in structured learning : adapting a parser trained on one corpus to another annotation style , and projecting syntactic annotations from one language to another .","label":"Background","metadata":{},"score":"43.28418"}
{"text":"( 2015 ) extended the morphological modeling approach to handle latent underlying morphs and derivational morphology .We connect two scenarios in structured learning : adapting a parser trained on one corpus to another annotation style , and projecting syntactic annotations from one language to another .","label":"Background","metadata":{},"score":"43.28418"}
{"text":"The resulting learning procedure specifically exploits the grammatical structure of Optimality Theory , independent of the content of substantive constraints defining any given grammatical module .We decompose the learning problem and present formal results for a central subproblem , deducing the constraint ranking particular to a target language , given structural descriptions of positive examples .","label":"Background","metadata":{},"score":"43.58199"}
{"text":"This paper proposes a novel class of PCFG parameterizations that support linguistically reasonable priors over PCFGs .To estimate the parameters is to discover a notion of relatedness among context - free rules such that related rules tend to have related probabilities .","label":"Background","metadata":{},"score":"43.95999"}
{"text":"This paper proposes a novel class of PCFG parameterizations that support linguistically reasonable priors over PCFGs .To estimate the parameters is to discover a notion of relatedness among context - free rules such that related rules tend to have related probabilities .","label":"Background","metadata":{},"score":"43.95999"}
{"text":"We also show improvements in labeled dependency parsing .We introduce a novel decoding procedure for statistical machine translation and other ordering tasks based on a family of Very Large - Scale Neighborhoods , some of which have previously been applied to other NP - hard permutation problems .","label":"Background","metadata":{},"score":"44.194336"}
{"text":"We also show improvements in labeled dependency parsing .We introduce a novel decoding procedure for statistical machine translation and other ordering tasks based on a family of Very Large - Scale Neighborhoods , some of which have previously been applied to other NP - hard permutation problems .","label":"Background","metadata":{},"score":"44.194336"}
{"text":"The algorithm is not provided with any constraints in advance , but uses its own resources to form constraints and weight them .A baseline model , in which Universal Grammar is reduced to a feature set and an SPE - style constraint format , suffices to learn many phonotactic phenomena .","label":"Background","metadata":{},"score":"44.28214"}
{"text":"The latter is far superior , but surprisingly few annotated examples are required .The experimentation focuses on a single dependency grammar induction task , in depth .The aim is to give strong support for the usefulness of the new techniques in one scenario .","label":"Background","metadata":{},"score":"44.29607"}
{"text":"The latter is far superior , but surprisingly few annotated examples are required .The experimentation focuses on a single dependency grammar induction task , in depth .The aim is to give strong support for the usefulness of the new techniques in one scenario .","label":"Background","metadata":{},"score":"44.29607"}
{"text":"Following Matsuzaki et al .( 2005 ) and Prescher ( 2005 ) , we may for example split NP without supervision into NP[0 ] and NP[1 ] , which behave differently .We first propose to learn a PCFG that adds such features to nonterminals in such a way that they respect patterns of linguistic feature passing : each node 's nonterminal features are either identical to , or independent of , those of its parent .","label":"Background","metadata":{},"score":"44.374775"}
{"text":"Following Matsuzaki et al .( 2005 ) and Prescher ( 2005 ) , we may for example split NP without supervision into NP[0 ] and NP[1 ] , which behave differently .We first propose to learn a PCFG that adds such features to nonterminals in such a way that they respect patterns of linguistic feature passing : each node 's nonterminal features are either identical to , or independent of , those of its parent .","label":"Background","metadata":{},"score":"44.374775"}
{"text":"Crucially , the same parameters should be used not only at different positions in the sentence ( as in convolutional networks for vision ) but also at different levels in the tree .We consider how several ideas from deep learning can help construct a PCFG from the bottom up while resisting bad local optima .","label":"Background","metadata":{},"score":"44.438225"}
{"text":"Crucially , the same parameters should be used not only at different positions in the sentence ( as in convolutional networks for vision ) but also at different levels in the tree .We consider how several ideas from deep learning can help construct a PCFG from the bottom up while resisting bad local optima .","label":"Background","metadata":{},"score":"44.438225"}
{"text":"designs of weighted phonological and morphological grammars .feasible finite state restrictions of optimality - theoretic and multi - tiered phonology .efficient finite state re - implementation of competitively efficient ad hoc methods ( see the article by S. Wintner in NLE 14(4 ) 2007 ) .","label":"Background","metadata":{},"score":"44.456238"}
{"text":"The new algorithm has been implemented and used in a large parsing experiment ( Eisner 1996 ) .We also give a useful extension to the case where the parser must undo a stochastic transduction that has altered the input .This paper points out some computational inefficiencies of standard TAG parsing algorithms when applied to LTAGs .","label":"Background","metadata":{},"score":"44.556007"}
{"text":"The new algorithm has been implemented and used in a large parsing experiment ( Eisner 1996 ) .We also give a useful extension to the case where the parser must undo a stochastic transduction that has altered the input .This paper points out some computational inefficiencies of standard TAG parsing algorithms when applied to LTAGs .","label":"Background","metadata":{},"score":"44.556007"}
{"text":"We give results on some NLP tasks .We show how to train the fast dependency parser of Smith and Eisner ( 2008 ) for improved accuracy .This parser can consider higher - order interactions among edges while retaining O ( n 3 ) runtime .","label":"Background","metadata":{},"score":"44.86228"}
{"text":"Stump , G. ( 2001 ) ._ Inflectional Morphology : A Theory of Paradigm Structure_.Cambridge , UK : Cambridge University Press .Yarowsky , D. , and Wicentowski , R. ( 2001 ) .Minimally supervised morphological analysis by multimodal alignment .","label":"Background","metadata":{},"score":"44.914223"}
{"text":"Stump , G. ( 2001 ) ._ Inflectional Morphology : A Theory of Paradigm Structure_.Cambridge , UK : Cambridge University Press .Yarowsky , D. , and Wicentowski , R. ( 2001 ) .Minimally supervised morphological analysis by multimodal alignment .","label":"Background","metadata":{},"score":"44.914223"}
{"text":"We also analytically show that interpolating these n -gram models for different n is similar to minimum - risk decoding for BLEU ( Tromble et al . , 2008 ) .Experiments show that our approach improves the state of the art .","label":"Background","metadata":{},"score":"44.921356"}
{"text":"We also analytically show that interpolating these n -gram models for different n is similar to minimum - risk decoding for BLEU ( Tromble et al . , 2008 ) .Experiments show that our approach improves the state of the art .","label":"Background","metadata":{},"score":"44.921356"}
{"text":"Our results thus offer novel , learning - theoretic support for such representations .We apply the model to English syllable onsets , Shona vowel harmony , quantity - insensitive stress typology , and the full phonotactics of Wargamay , showing that the learned grammars capture the distributional generalizations of these languages and accurately predict the findings of a phonotactic experiment . .","label":"Background","metadata":{},"score":"44.942947"}
{"text":"Finite - state methods figure fairly prominently throughout the book .In particular , the computational approaches to morphology discussed in the book are primarily finite - state approaches , while other approaches , such as work on inflectional morphology in the DATR framework ( e.g. , Corbett and Fraser , 1993 ) , are only mentioned in passing .","label":"Background","metadata":{},"score":"44.976746"}
{"text":"Finite - state methods figure fairly prominently throughout the book .In particular , the computational approaches to morphology discussed in the book are primarily finite - state approaches , while other approaches , such as work on inflectional morphology in the DATR framework ( e.g. , Corbett and Fraser , 1993 ) , are only mentioned in passing .","label":"Background","metadata":{},"score":"44.976746"}
{"text":"However , it did not yield improvements when training on the Penn Treebank .An orthogonal strategy was more successful : to improve the performance of the EM learner by treebank preprocessing and by annealing methods that split nonterminals selectively .Using these methods , we can maintain high parsing accuracy while dramatically reducing the model size .","label":"Background","metadata":{},"score":"45.09932"}
{"text":"However , it did not yield improvements when training on the Penn Treebank .An orthogonal strategy was more successful : to improve the performance of the EM learner by treebank preprocessing and by annealing methods that split nonterminals selectively .Using these methods , we can maintain high parsing accuracy while dramatically reducing the model size .","label":"Background","metadata":{},"score":"45.09932"}
{"text":"The model is based on finite - state operations and seamlessly handles concatenative and nonconcatenative morphology .Graphical models are often used \" inappropriately , \" with approximations in the topology , inference , and prediction .Yet it is still common to train their parameters to approximately maximize training likelihood .","label":"Background","metadata":{},"score":"45.11319"}
{"text":"The model is based on finite - state operations and seamlessly handles concatenative and nonconcatenative morphology .Graphical models are often used \" inappropriately , \" with approximations in the topology , inference , and prediction .Yet it is still common to train their parameters to approximately maximize training likelihood .","label":"Background","metadata":{},"score":"45.11319"}
{"text":"Finally , I will sketch a more radical extension , directional evaluation , which changes how a constraint ranks candidates .This change brings back some of the descriptive convenience of Generalized Alignment , but it also constrains OT grammars to describe only regular relations , which is linguistically and computationally desirable .","label":"Background","metadata":{},"score":"45.167694"}
{"text":"Finally , I will sketch a more radical extension , directional evaluation , which changes how a constraint ranks candidates .This change brings back some of the descriptive convenience of Generalized Alignment , but it also constrains OT grammars to describe only regular relations , which is linguistically and computationally desirable .","label":"Background","metadata":{},"score":"45.167694"}
{"text":"I 'm not above engineering tweaks , but I do try to do them in an elegant and general way .There are many recurring techniques that appear in NLP and more generally in AI .This is true both at the high level of algorithms and at the low level of implementation .","label":"Background","metadata":{},"score":"45.26859"}
{"text":"Please read carefully and make sure you really understand the models .Could such models be trained with less supervision ?Noah writes : There are several papers on van Zaanen 's site about \" Alignment - based Learning .\" If this one is n't at the right level , take a look at the COLING ' 00 , CLUK ' 00 , or CLN ' 99 papers ... or his dissertation ( 2001 ) .","label":"Background","metadata":{},"score":"45.362076"}
{"text":"We describe a general approach to the probabilistic parsing of context - free grammars .The method integrates context - sensitive statistical knowledge of various types ( e.g. , syntactic and semantic ) and can be trained incrementally from a bracketed corpus .","label":"Background","metadata":{},"score":"45.432785"}
{"text":"We describe a general approach to the probabilistic parsing of context - free grammars .The method integrates context - sensitive statistical knowledge of various types ( e.g. , syntactic and semantic ) and can be trained incrementally from a bracketed corpus .","label":"Background","metadata":{},"score":"45.432785"}
{"text":"Indeed the generation problem is shown NP - complete in this sense .However , techniques are discussed for making Ellison 's approach fast and practical in the typical case , including a simple trick that alone provides a 100-fold speedup on a grammar fragment of moderate size .","label":"Background","metadata":{},"score":"45.454445"}
{"text":"Indeed the generation problem is shown NP - complete in this sense .However , techniques are discussed for making Ellison 's approach fast and practical in the typical case , including a simple trick that alone provides a 100-fold speedup on a grammar fragment of moderate size .","label":"Background","metadata":{},"score":"45.454445"}
{"text":"Our model learns these component distributions and the structure of how to combine subsets of them into topics .The SCTM can represent topics in a much more compact representation than LDA and achieves better perplexity with fewer parameters .We propose an algorithm to find the best path through an intersection of arbitrarily many weighted automata , without actually performing the intersection .","label":"Background","metadata":{},"score":"45.458115"}
{"text":"Our model learns these component distributions and the structure of how to combine subsets of them into topics .The SCTM can represent topics in a much more compact representation than LDA and achieves better perplexity with fewer parameters .We propose an algorithm to find the best path through an intersection of arbitrarily many weighted automata , without actually performing the intersection .","label":"Background","metadata":{},"score":"45.458115"}
{"text":"Many models in NLP involve latent variables , such as unknown parses , tags , or alignments .Finding the optimal model parameters is then usually a difficult nonconvex optimization problem .The usual practice is to settle for local optimization methods such as EM or gradient ascent .","label":"Background","metadata":{},"score":"45.461895"}
{"text":"Many models in NLP involve latent variables , such as unknown parses , tags , or alignments .Finding the optimal model parameters is then usually a difficult nonconvex optimization problem .The usual practice is to settle for local optimization methods such as EM or gradient ascent .","label":"Background","metadata":{},"score":"45.461895"}
{"text":"It then demonstrates how a single computational operation , composition , can be used to describe both syntagmatic and paradigmatic aspects of morphology with finite - state devices .The authors draw upon examples from a variety of languages to illustrate a spectrum of syntagmatic variation , including simple concatenation , prosodically governed concatenation , phonological changes induced by affixation , subsegmental and subtractive morphology , extrametrical and positively circumscribed infixation , root - and - pattern morphology , and morphomic components .","label":"Background","metadata":{},"score":"45.79882"}
{"text":"It then demonstrates how a single computational operation , composition , can be used to describe both syntagmatic and paradigmatic aspects of morphology with finite - state devices .The authors draw upon examples from a variety of languages to illustrate a spectrum of syntagmatic variation , including simple concatenation , prosodically governed concatenation , phonological changes induced by affixation , subsegmental and subtractive morphology , extrametrical and positively circumscribed infixation , root - and - pattern morphology , and morphomic components .","label":"Background","metadata":{},"score":"45.79882"}
{"text":"The real problem the task seeks to simulate - the induction of syntactic structure in natural language text - is certainly of interest to the community , but this thesis does not directly approach the problem of exploiting induced syntax in applications .","label":"Background","metadata":{},"score":"45.842377"}
{"text":"The real problem the task seeks to simulate - the induction of syntactic structure in natural language text - is certainly of interest to the community , but this thesis does not directly approach the problem of exploiting induced syntax in applications .","label":"Background","metadata":{},"score":"45.842377"}
{"text":"Chapter 3 learns HMM topologies ( probabilistic finite - state grammars ) - joint work with Stephen Omohundro .Jose Oncina ( 1998 ) .The data driven approach applied to the OSTIA algorithm .In Proceedings of the Fourth International Colloquium on Grammatical Inference Lecture Notes on Artificial Intelligence Vol .","label":"Background","metadata":{},"score":"45.887848"}
{"text":"This result is empirically and computationally awkward .We propose replacing these unbounded constraints , as well as non - finite - state Generalized Alignment constraints , with a new class of finite - state directional constraints .We give linguistic applications , results on generative power , and algorithms to compile grammars into transducers .","label":"Background","metadata":{},"score":"45.972775"}
{"text":"This result is empirically and computationally awkward .We propose replacing these unbounded constraints , as well as non - finite - state Generalized Alignment constraints , with a new class of finite - state directional constraints .We give linguistic applications , results on generative power , and algorithms to compile grammars into transducers .","label":"Background","metadata":{},"score":"45.972775"}
{"text":"This leads to problems with data sparseness and calls for models that can deal with this abundance of related words - models that can learn , analyze , reduce and generate morphological inflections .But surprisingly , statistical approaches to morphology are still rare , which stands in contrast to the many recent advances of sophisticated models in parsing , grammar induction , translation and many other areas of natural language processing .","label":"Background","metadata":{},"score":"46.021378"}
{"text":"This leads to problems with data sparseness and calls for models that can deal with this abundance of related words - models that can learn , analyze , reduce and generate morphological inflections .But surprisingly , statistical approaches to morphology are still rare , which stands in contrast to the many recent advances of sophisticated models in parsing , grammar induction , translation and many other areas of natural language processing .","label":"Background","metadata":{},"score":"46.021378"}
{"text":"Our generative model explains similarities among the strings by supposing that some strings in the collection were not generated ab initio , but were instead derived by transduction from other , \" similar \" strings in the collection .Our variational EM learning algorithm alternately reestimates this phylogeny and the transducer parameters .","label":"Background","metadata":{},"score":"46.23742"}
{"text":"Our generative model explains similarities among the strings by supposing that some strings in the collection were not generated ab initio , but were instead derived by transduction from other , \" similar \" strings in the collection .Our variational EM learning algorithm alternately reestimates this phylogeny and the transducer parameters .","label":"Background","metadata":{},"score":"46.23742"}
{"text":"It considers simpler systems of transformations to be more probable a priori .Experiments show that the learned transformations are more effective than previous statistical models at predicting the probabilities of lexical entries , especially those for which the learner had no direct evidence .","label":"Background","metadata":{},"score":"46.25733"}
{"text":"It considers simpler systems of transformations to be more probable a priori .Experiments show that the learned transformations are more effective than previous statistical models at predicting the probabilities of lexical entries , especially those for which the learner had no direct evidence .","label":"Background","metadata":{},"score":"46.25733"}
{"text":"This stacking of multiple runs is what makes the method deep .We also mention extensions that involve supervised fine - tuning or richer , vector - valued representations of words and nonterminals .Users want natural language processing ( NLP ) systems to be both fast and accurate , but quality often comes at the cost of speed .","label":"Background","metadata":{},"score":"46.27433"}
{"text":"This stacking of multiple runs is what makes the method deep .We also mention extensions that involve supervised fine - tuning or richer , vector - valued representations of words and nonterminals .Users want natural language processing ( NLP ) systems to be both fast and accurate , but quality often comes at the cost of speed .","label":"Background","metadata":{},"score":"46.27433"}
{"text":"Furthermore , it makes unsupervised log - linear estimation tractable , which allows the beneficial incorporation of arbitrary features such as spelling .Finite - state automata and their stochastic cousins , Hidden Markov Models , are respectively standard topics in the CS and EE curricula .","label":"Background","metadata":{},"score":"46.446358"}
{"text":"One distinct feature of the book is the authors ' clear effort in relating computational models of morphological and syntactic processing to issues of interest to theoretical linguistics and natural language processing .The book includes extensive descriptions of the formal characterization of morphological operations as well as a wide range of grammar formalisms , using multilingual data to illustrate these operations and formalisms wherever necessary .","label":"Background","metadata":{},"score":"46.48623"}
{"text":"One distinct feature of the book is the authors ' clear effort in relating computational models of morphological and syntactic processing to issues of interest to theoretical linguistics and natural language processing .The book includes extensive descriptions of the formal characterization of morphological operations as well as a wide range of grammar formalisms , using multilingual data to illustrate these operations and formalisms wherever necessary .","label":"Background","metadata":{},"score":"46.48623"}
{"text":"Advanced Syntactic and Semantic Parsing .Lexicalized PCFGs , word classes , parsing with limited supervision , latent variable grammars .Induction of grammars , inside - outside algorithm .Temporal and Spatial Recognition .Temporal expression recognition , temporal normalization , TimeBank .","label":"Background","metadata":{},"score":"46.599968"}
{"text":"The papers below introduce novel dynamic programming algorithms ( primarily for parsing and machine translation ) .Other cool papers , not listed below , show how dynamic programming algorithms can be embedded as efficient subroutines within variational inference ( belief propagation ) , relaxation ( dual decomposition , row generation ) , and large - neighborhood local search .","label":"Background","metadata":{},"score":"46.661793"}
{"text":"The above algorithms find word - to - word dependencies in order to let the parser evaluate whether the dependencies are plausible .Usually this evaluation considers the two words being linked .But there are other ways to evalute a dependency : .","label":"Background","metadata":{},"score":"46.822495"}
{"text":"Does n't the machine learning community do structured prediction ?Yes : graphical model inference must predict discrete vectors .But linguistics must predict strings and trees .Our systems must guess the syntactic structure of a sentence , the translation of a sentence , the grammar of a language , or the set of real - world facts that is consistent with a set of documents .","label":"Background","metadata":{},"score":"46.902077"}
{"text":"An unsupervised discriminative training procedure is proposed for estimating a language model ( LM ) for machine translation ( MT ) .An English - to - English synchronous context - free grammar is derived from a baseline MT system to capture translation alternatives : pairs of words , phrases or other sentence fragments that potentially compete to be the translation of the same source - language fragment .","label":"Background","metadata":{},"score":"46.903587"}
{"text":"An unsupervised discriminative training procedure is proposed for estimating a language model ( LM ) for machine translation ( MT ) .An English - to - English synchronous context - free grammar is derived from a baseline MT system to capture translation alternatives : pairs of words , phrases or other sentence fragments that potentially compete to be the translation of the same source - language fragment .","label":"Background","metadata":{},"score":"46.903587"}
{"text":"It also shows how to extend the approach to other cases , including CFGs , link grammars ( whose original parsing algorithm is quite similar ) , and the composition of a bilexical dependency grammar with a finite - state transducer .","label":"Background","metadata":{},"score":"46.916595"}
{"text":"One may need to build a statistical parser for a new language , using only a very small labeled treebank together with raw text .We argue that bootstrapping a parser is most promising when the model uses a rich set of redundant features , as in recent models for scoring dependency parses ( McDonald et al . , 2005 ) .","label":"Background","metadata":{},"score":"46.98008"}
{"text":"One may need to build a statistical parser for a new language , using only a very small labeled treebank together with raw text .We argue that bootstrapping a parser is most promising when the model uses a rich set of redundant features , as in recent models for scoring dependency parses ( McDonald et al . , 2005 ) .","label":"Background","metadata":{},"score":"46.98008"}
{"text":"Each problem instance can be thought of as optimization of a function that applies to the set of permutations .The dissertation treats word reordering for machine translation as another instance of a combinatorial optimization problem .The approach introduced is to combine three different functions of permutations .","label":"Background","metadata":{},"score":"47.032654"}
{"text":"Each problem instance can be thought of as optimization of a function that applies to the set of permutations .The dissertation treats word reordering for machine translation as another instance of a combinatorial optimization problem .The approach introduced is to combine three different functions of permutations .","label":"Background","metadata":{},"score":"47.032654"}
{"text":"Finite - state approaches to morphology and syntax figure prominently in the book , but context - free and context - sensitive approaches to syntax are also covered .The authors discuss the implications of the computational approaches for morphological and syntactical theories , their relation to formal mechanisms , and their applications in practical natural language processing tasks .","label":"Background","metadata":{},"score":"47.051693"}
{"text":"These models can be learned with minimal supervision for any language that has inflectional morphology .As training data , some sample paradigms and a raw , unannotated text corpus can be used .The models over morphological paradigms are developed in three main chapters that start with smaller components and build up to larger ones .","label":"Background","metadata":{},"score":"47.176334"}
{"text":"These models can be learned with minimal supervision for any language that has inflectional morphology .As training data , some sample paradigms and a raw , unannotated text corpus can be used .The models over morphological paradigms are developed in three main chapters that start with smaller components and build up to larger ones .","label":"Background","metadata":{},"score":"47.176334"}
{"text":"Further , our iterative learning algorithms assume a fixed batch of data that can be repeatedly accessed , not a long stream of data observed over time in tandem with acquisition .( Of course , the cognitive criticisms apply to virtually all existing learning methods in natural language processing , not just the new ones presented here . )","label":"Background","metadata":{},"score":"47.273197"}
{"text":"Further , our iterative learning algorithms assume a fixed batch of data that can be repeatedly accessed , not a long stream of data observed over time in tandem with acquisition .( Of course , the cognitive criticisms apply to virtually all existing learning methods in natural language processing , not just the new ones presented here . )","label":"Background","metadata":{},"score":"47.273197"}
{"text":"The proper treatment of optimality in computational phonology .In _ Proceedings of the 2nd International Workshop on Finite - State Methods in Natural Language Processing _ , pp . 1 - 12 .Koskenniemi , K. ( 1983 ) ._","label":"Background","metadata":{},"score":"47.38298"}
{"text":"The proper treatment of optimality in computational phonology .In _ Proceedings of the 2nd International Workshop on Finite - State Methods in Natural Language Processing _ , pp . 1 - 12 .Koskenniemi , K. ( 1983 ) ._","label":"Background","metadata":{},"score":"47.38298"}
{"text":"Where existing learning algorithms for recurrent and non - recurrent networks only attempt to train a network 's position in activation space , the models presented here can also explicitly and successfully prescribe the nature of its movement through activation space .","label":"Background","metadata":{},"score":"47.506126"}
{"text":"Where existing learning algorithms for recurrent and non - recurrent networks only attempt to train a network 's position in activation space , the models presented here can also explicitly and successfully prescribe the nature of its movement through activation space .","label":"Background","metadata":{},"score":"47.506126"}
{"text":"This thesis is about estimating probabilistic models to uncover useful hidden structure in data ; specifically , we address the problem of discovering syntactic structure in natural language text .We present three new parameter estimation techniques that generalize the standard approach , maximum likelihood estimation , in different ways .","label":"Background","metadata":{},"score":"47.825153"}
{"text":"This thesis is about estimating probabilistic models to uncover useful hidden structure in data ; specifically , we address the problem of discovering syntactic structure in natural language text .We present three new parameter estimation techniques that generalize the standard approach , maximum likelihood estimation , in different ways .","label":"Background","metadata":{},"score":"47.825153"}
{"text":"approaches to the adaptation of lexical transducers to a cluster of languages . seamless construction of computational lexicons from dynamically changing linguistic descriptions . model - checking and automatic verification of phonological grammars .finite state approaches to language variation and diachronic description . portability and long - term archiving of resources in computational morphology and morpho - syntax . constructing refreshed lexical transducers quickly from updated extended regular expressions .","label":"Background","metadata":{},"score":"47.879143"}
{"text":"We apply machine learning to the Linear Ordering Problem in order to learn sentence - specific reordering models for machine translation .We demonstrate that even when these models are used as a mere preprocessing step for German - English translation , they significantly outperform Moses ' integrated lexicalized reordering model .","label":"Background","metadata":{},"score":"47.909996"}
{"text":"We apply machine learning to the Linear Ordering Problem in order to learn sentence - specific reordering models for machine translation .We demonstrate that even when these models are used as a mere preprocessing step for German - English translation , they significantly outperform Moses ' integrated lexicalized reordering model .","label":"Background","metadata":{},"score":"47.909996"}
{"text":"The paper tries a few interesting tricks , including training on short sentences only .For evaluating the performance of a ( learned ) grammar , you may also be interested in chapter 3 ( pp .120 - 165 ) of Goodman ( 1998 ) .","label":"Background","metadata":{},"score":"47.94297"}
{"text":"The field has been manually exploring various speed - accuracy tradeoffs for particular problems or datasets .We aim to explore this space automatically , focusing here on the case of agenda - based syntactic parsing ( Kay , 1986 ) .","label":"Background","metadata":{},"score":"48.06513"}
{"text":"The field has been manually exploring various speed - accuracy tradeoffs for particular problems or datasets .We aim to explore this space automatically , focusing here on the case of agenda - based syntactic parsing ( Kay , 1986 ) .","label":"Background","metadata":{},"score":"48.06513"}
{"text":"Similarities and differences are discussed .After presenting a novel O ( n 3 ) parsing algorithm for dependency grammar , we develop three contrasting ways to stochasticize it .We also give preliminary empirical results from evaluating the three models ' parsing performance on annotated Wall Street Journal training text ( derived from the Penn Treebank ) .","label":"Background","metadata":{},"score":"48.122063"}
{"text":"Similarities and differences are discussed .After presenting a novel O ( n 3 ) parsing algorithm for dependency grammar , we develop three contrasting ways to stochasticize it .We also give preliminary empirical results from evaluating the three models ' parsing performance on annotated Wall Street Journal training text ( derived from the Penn Treebank ) .","label":"Background","metadata":{},"score":"48.122063"}
{"text":"This paper presents a novel forrealization of optimality theory .Unlike pre- yions treatments of optimality in computational linguistics , starting with Ellison ( 1994 ) , the new approach does not require any explicit marking and counting of constraint violations .","label":"Background","metadata":{},"score":"48.15512"}
{"text":"It has been argued that rule - based phonological descriptions can uniformly be expressed as map - pings carried out by finite - state transducers , and therefore fall within the class of rational relations .If this property of generative capacity is an empirically correct characterization of phonological mappings , it should hold of any sufficiently restrictive theory of phonology , whether it utilizes con - straints or rewrite rules .","label":"Background","metadata":{},"score":"48.162308"}
{"text":"The selection of an implicit negative evidence class - a \" neighborhood\"-appropriate to a given task has strong implications , but a good neighborhood can target the objective of grammar induction to a specific application .Conditional random fields ( Lafferty et al . , 2001 ) are quite effective at sequence labeling tasks like shallow parsing ( Sha and Pereira , 2003 ) and namedentity extraction ( McCallum and Li , 2003 ) .","label":"Background","metadata":{},"score":"48.250725"}
{"text":"The selection of an implicit negative evidence class - a \" neighborhood\"-appropriate to a given task has strong implications , but a good neighborhood can target the objective of grammar induction to a specific application .Conditional random fields ( Lafferty et al . , 2001 ) are quite effective at sequence labeling tasks like shallow parsing ( Sha and Pereira , 2003 ) and namedentity extraction ( McCallum and Li , 2003 ) .","label":"Background","metadata":{},"score":"48.250725"}
{"text":"These are applicable to lemmatization or to relate a past tense form to its associated present tense form , or for similar morphological tasks .It turns out they are general enough to tackle the popular task of transliteration very well , as well as other string - to - string tasks .","label":"Background","metadata":{},"score":"48.269913"}
{"text":"These are applicable to lemmatization or to relate a past tense form to its associated present tense form , or for similar morphological tasks .It turns out they are general enough to tackle the popular task of transliteration very well , as well as other string - to - string tasks .","label":"Background","metadata":{},"score":"48.269913"}
{"text":"Language typology and divergences .Rule - based models : direct translation , transfer systems , and interlingua systems .Statistical models : word - based , phrase - based and syntax - based .Synchronous context free grammars , synchronous tree substitution .","label":"Background","metadata":{},"score":"48.46405"}
{"text":"Instead , we develop a variational approximation , which considers all the derivations but still allows tractable decoding .Our particular variational distributions are parameterized as n - gram models .We also analytically show that interpolating these n - gram models for different n is similar to lattice - based minimum - risk decoding for BLEU .","label":"Background","metadata":{},"score":"48.468292"}
{"text":"Instead , we develop a variational approximation , which considers all the derivations but still allows tractable decoding .Our particular variational distributions are parameterized as n - gram models .We also analytically show that interpolating these n - gram models for different n is similar to lattice - based minimum - risk decoding for BLEU .","label":"Background","metadata":{},"score":"48.468292"}
{"text":"Gen freely proposes gestures and prosodic constituents ; the constraints try to force these to coincide or not coincide temporally .An efficient algorithm exists to find the optimal candidate .I will argue that despite its simplicity , primitive OT is expressive enough to describe and unify most of the work in OT phonology .","label":"Background","metadata":{},"score":"48.49903"}
{"text":"Gen freely proposes gestures and prosodic constituents ; the constraints try to force these to coincide or not coincide temporally .An efficient algorithm exists to find the optimal candidate .I will argue that despite its simplicity , primitive OT is expressive enough to describe and unify most of the work in OT phonology .","label":"Background","metadata":{},"score":"48.49903"}
{"text":"Our algorithm is designed to admit future generalizations , including cyclic and infinite circuits and propagation of delta updates .Many linguistic and textual processes involve transduction of strings .We show how to learn a stochastic transducer from an unorganized collection of strings ( rather than string pairs ) .","label":"Background","metadata":{},"score":"48.606007"}
{"text":"Our algorithm is designed to admit future generalizations , including cyclic and infinite circuits and propagation of delta updates .Many linguistic and textual processes involve transduction of strings .We show how to learn a stochastic transducer from an unorganized collection of strings ( rather than string pairs ) .","label":"Background","metadata":{},"score":"48.606007"}
{"text":"Three families of grammars are presented .The authors begin with unification grammars , focusing in particular on Lexical - Functional Grammar .They then introduce several lexicalized grammar formalisms , including Tree - Adjoining Grammars ( TAG ) , Combinatory Categorical Grammars ( CCG ) , and two other mildly context - sensitive approaches , namely Multicomponent TAG and Minimalist Grammars .","label":"Background","metadata":{},"score":"48.623837"}
{"text":"Three families of grammars are presented .The authors begin with unification grammars , focusing in particular on Lexical - Functional Grammar .They then introduce several lexicalized grammar formalisms , including Tree - Adjoining Grammars ( TAG ) , Combinatory Categorical Grammars ( CCG ) , and two other mildly context - sensitive approaches , namely Multicomponent TAG and Minimalist Grammars .","label":"Background","metadata":{},"score":"48.623837"}
{"text":"A parser that is biased toward short dependencies can be faster and sometimes more accurate , as shown by experiments .In fact , placing a hard limit on dependency length allows O(n ) ( linear - time ) parsing , by modifying the algorithms above .","label":"Background","metadata":{},"score":"48.76966"}
{"text":"Unlike previous statistical formalisms ( limited to isomorphic trees ) , synchronous tree substitution grammar allows local distortion of the tree topology .We reformulate it to permit dependency trees , and sketch EM / Viterbi algorithms for alignment , training , and decoding .","label":"Background","metadata":{},"score":"48.9451"}
{"text":"Unlike previous statistical formalisms ( limited to isomorphic trees ) , synchronous tree substitution grammar allows local distortion of the tree topology .We reformulate it to permit dependency trees , and sketch EM / Viterbi algorithms for alignment , training , and decoding .","label":"Background","metadata":{},"score":"48.9451"}
{"text":"We show how to adjust the model parameters to compensate for the errors introduced by this approximation , by following the gradient of the actual loss on training data .We find the gradient by back - propagation , treating the entire parser ( approximations and all ) as a differentiable circuit , as Stoyanov et al .","label":"Background","metadata":{},"score":"48.949818"}
{"text":"We show how to adjust the model parameters to compensate for the errors introduced by this approximation , by following the gradient of the actual loss on training data .We find the gradient by back - propagation , treating the entire parser ( approximations and all ) as a differentiable circuit , as Stoyanov et al .","label":"Background","metadata":{},"score":"48.949818"}
{"text":"The theory of classical string automata has a natural extension to tree automata , which found applications in NLP .In 1982 , Joshi and Levy pointed out in Computational Linguistics 8(1 ) that phrase structure grammars actually generalize to tree automata that bring more descriptive power .","label":"Background","metadata":{},"score":"48.956432"}
{"text":"Even with second - order features or latent variables , which would make exact parsing asymptotically slower or NP - hard , approximate inference with belief propagation is as efficient as a simple edge - factored parser times a constant factor .","label":"Background","metadata":{},"score":"49.002136"}
{"text":"Even with second - order features or latent variables , which would make exact parsing asymptotically slower or NP - hard , approximate inference with belief propagation is as efficient as a simple edge - factored parser times a constant factor .","label":"Background","metadata":{},"score":"49.002136"}
{"text":"We give an overview of the project , including pedagogical motivation , modeling of the learner , data collection , user interface design , linguistic issues , and our use of machine translation and reinforcement learning inside the system .Natural language processing must sometimes consider the internal structure of words , e.g. , in order to understand or generate an unfamiliar word .","label":"Background","metadata":{},"score":"49.142914"}
{"text":"It then delves into Stump 's lexical - incremental and inferential - realization theories .Chapter four , ' ' A Brief History of Computational Morphology ' ' , surveys the history of computational morphology , focusing on finite - state approaches .","label":"Background","metadata":{},"score":"49.233093"}
{"text":"It then delves into Stump 's lexical - incremental and inferential - realization theories .Chapter four , ' ' A Brief History of Computational Morphology ' ' , surveys the history of computational morphology , focusing on finite - state approaches .","label":"Background","metadata":{},"score":"49.233093"}
{"text":"The course introduces students into the principles and methods of natural language processing , blending traditional and current approaches .The course additionally aims at an understanding of the underlying computational properties of natural language and of current research directions .We study core tasks in natural language processing , including language modeling , syntactic analysis , semantic interpretation , coreference resolution , discourse analysis and machine translation .","label":"Background","metadata":{},"score":"49.264763"}
{"text":"Unlike the very well - known methods such as smoothed mutual information , or the log - likelihood independence test of Dunning ( 1993 ) , it looks at context .Jeffrey Mark Siskind : .( 1995 ) ' Robust Lexical Acquisition Despite Extremely Noisy Input , ' Proceedings of the 19th Boston University Conference on Language Development ( edited by D. MacLaughlin and S. McEwen ) , Cascadilla Press , March .","label":"Background","metadata":{},"score":"49.608055"}
{"text":"Proceedings of the 20th Meeting of the Association for Computational Linguistics .scanned PDF version .A follow - up paper is Schabes , Roth & Osborne ( 1993 ) , which uses a fully -bracketed corpus ( on harder data - WSJ rather than ATIS ) .","label":"Background","metadata":{},"score":"49.65574"}
{"text":"Under this paradigm , we use weights from the expectation semiring ( Eisner , 2002 ) , to compute first - order statistics ( e.g. , the expected hypothesis length or feature counts ) over packed forests of translations ( lattices or hypergraphs ) .","label":"Background","metadata":{},"score":"49.69909"}
{"text":"Under this paradigm , we use weights from the expectation semiring ( Eisner , 2002 ) , to compute first - order statistics ( e.g. , the expected hypothesis length or feature counts ) over packed forests of translations ( lattices or hypergraphs ) .","label":"Background","metadata":{},"score":"49.69909"}
{"text":"All the above methods have been implemented in an open - source machine translation toolkit Joshua .In this dissertation , the methods have mainly been applied to a machine translation task , butwe expect that they will also find applications in other areas of natural language processing ( e.g. , parsing and speech recognition ) .","label":"Background","metadata":{},"score":"49.706554"}
{"text":"All the above methods have been implemented in an open - source machine translation toolkit Joshua .In this dissertation , the methods have mainly been applied to a machine translation task , butwe expect that they will also find applications in other areas of natural language processing ( e.g. , parsing and speech recognition ) .","label":"Background","metadata":{},"score":"49.706554"}
{"text":"The review first discusses the key aspects of the system : the representation of dictionaries as tries , the representation of morphological concatenation via continuation lexica , and the finite - state transducers that implement the surface - lexical phonological correspondences .","label":"Background","metadata":{},"score":"49.760597"}
{"text":"The review first discusses the key aspects of the system : the representation of dictionaries as tries , the representation of morphological concatenation via continuation lexica , and the finite - state transducers that implement the surface - lexical phonological correspondences .","label":"Background","metadata":{},"score":"49.760597"}
{"text":"We evaluate our approach on morphological tasks and demonstrate that latent variables can dramatically improve results , even when trained on small data sets .On the task of generating morphological forms , we outperform a baseline method reducing the error rate by up to 48 % .","label":"Background","metadata":{},"score":"49.813698"}
{"text":"We evaluate our approach on morphological tasks and demonstrate that latent variables can dramatically improve results , even when trained on small data sets .On the task of generating morphological forms , we outperform a baseline method reducing the error rate by up to 48 % .","label":"Background","metadata":{},"score":"49.813698"}
{"text":"Such choices are of course often necessary .However , it is useful for the potential reader to be aware of the authors ' chosen focus , especially if they are looking for a book that covers all different approaches to computational morphology .","label":"Background","metadata":{},"score":"49.931618"}
{"text":"Such choices are of course often necessary .However , it is useful for the potential reader to be aware of the authors ' chosen focus , especially if they are looking for a book that covers all different approaches to computational morphology .","label":"Background","metadata":{},"score":"49.931618"}
{"text":"The experiments presented in this thesis give one of the most thorough explorations to date of unsupervised parameter estimation for models of discrete structures .Two sides of the problem are considered in depth : the choice of objective function to be optimized during training , and the method of optimizing it .","label":"Background","metadata":{},"score":"50.031372"}
{"text":"The experiments presented in this thesis give one of the most thorough explorations to date of unsupervised parameter estimation for models of discrete structures .Two sides of the problem are considered in depth : the choice of objective function to be optimized during training , and the method of optimizing it .","label":"Background","metadata":{},"score":"50.031372"}
{"text":"Each paradigm is sampled all at once from a graphical model , whose potential functions are weighted finite - state transducers with language - specific parameters to be learned .These assumptions naturally lead to an elegant empirical Bayes inference procedure that exploits Monte Carlo EM , belief propagation , and dynamic programming .","label":"Background","metadata":{},"score":"50.151463"}
{"text":"Each paradigm is sampled all at once from a graphical model , whose potential functions are weighted finite - state transducers with language - specific parameters to be learned .These assumptions naturally lead to an elegant empirical Bayes inference procedure that exploits Monte Carlo EM , belief propagation , and dynamic programming .","label":"Background","metadata":{},"score":"50.151463"}
{"text":"It will acquire more over time : we intend for it to generalize and encapsulate best practices , and serve as a testbed for new practices .Dyna is now being used for parsing , machine translation , morphological analysis , grammar induction , and finite - state modeling .","label":"Background","metadata":{},"score":"50.170048"}
{"text":"It will acquire more over time : we intend for it to generalize and encapsulate best practices , and serve as a testbed for new practices .Dyna is now being used for parsing , machine translation , morphological analysis , grammar induction , and finite - state modeling .","label":"Background","metadata":{},"score":"50.170048"}
{"text":"Invited talk : Approximation and Exactness in Finite State OT Dale Gerdemann and Gertjan van Noord Frank & Satta ( 1998 ) showed that OT with gradient constraints generally is not finite - state .We present an improvement of the approximation of Karttunen ( 1998 ) .","label":"Background","metadata":{},"score":"50.186436"}
{"text":"Among others , I will show our own recent successes using message - passing approximate inference policies for graphical models .The form of these policies is determined by the structure of our intractable and surely mismatched domain model , but we tune the parameters to minimize loss ( Stoyanov & Eisner , 2012 ) .","label":"Background","metadata":{},"score":"50.18797"}
{"text":"Among others , I will show our own recent successes using message - passing approximate inference policies for graphical models .The form of these policies is determined by the structure of our intractable and surely mismatched domain model , but we tune the parameters to minimize loss ( Stoyanov & Eisner , 2012 ) .","label":"Background","metadata":{},"score":"50.18797"}
{"text":"Specifically , we will focus o .. \" ...This paper presents a novel forrealization of optimality theory .Unlike pre- yions treatments of optimality in computational linguistics , starting with Ellison ( 1994 ) , the new approach does not require any explicit marking and counting of constraint violations .","label":"Background","metadata":{},"score":"50.237022"}
{"text":"K. Lari and S. Young ( 1990 ) .The estimation of stochastic context - free grammars using the inside - outside algorithm .Computer Speech and Language 4:35 - 56 . scanned PDF version .Fernando Pereira and Yves Schabes ( 1992 ) .","label":"Background","metadata":{},"score":"50.324028"}
{"text":"Structural annealing applies a domain - specific search bias during early learning .Finally , we have designed objectives for semi - supervised learning .Bootstrapping is a general strategy for semi - supervised learning .Bootstrapping algorithms sometimes get confused and perform poorly .","label":"Background","metadata":{},"score":"50.335205"}
{"text":"Structural annealing applies a domain - specific search bias during early learning .Finally , we have designed objectives for semi - supervised learning .Bootstrapping is a general strategy for semi - supervised learning .Bootstrapping algorithms sometimes get confused and perform poorly .","label":"Background","metadata":{},"score":"50.335205"}
{"text":"( Note : Briscoe & Waegner ( 1992 ) were the first to try such a restriction . )De Marcken suggests that link grammars may be easier to learn , using arguments that also apply to dependency grammar .Glenn Carroll and Mats Rooth ( 1998 ) .","label":"Background","metadata":{},"score":"50.451008"}
{"text":"This tutorial reviews those methods , building up strategies step by step so as to expose the insights behind the algorithms .Implementation details are clarified , and some generalizations are given .It also considers Frederickson 's method for maintaining an MST in time O ( sqrt ( ( m ) ) per change to the graph .","label":"Background","metadata":{},"score":"50.648346"}
{"text":"This tutorial reviews those methods , building up strategies step by step so as to expose the insights behind the algorithms .Implementation details are clarified , and some generalizations are given .It also considers Frederickson 's method for maintaining an MST in time O ( sqrt ( ( m ) ) per change to the graph .","label":"Background","metadata":{},"score":"50.648346"}
{"text":"We close with the hope that partial or restricted algorithms may be found that are still powerful enough to have practical use .Note : Dreyer & Eisner ( 2009 ) and Paul & Eisner ( 2012 ) make some progress on the uncomputable problem of joining or auto - intersecting FSMs .","label":"Background","metadata":{},"score":"50.67026"}
{"text":"We close with the hope that partial or restricted algorithms may be found that are still powerful enough to have practical use .Note : Dreyer & Eisner ( 2009 ) and Paul & Eisner ( 2012 ) make some progress on the uncomputable problem of joining or auto - intersecting FSMs .","label":"Background","metadata":{},"score":"50.67026"}
{"text":"We show how to locally optimize this risk using back - propagation and stochastic meta - descent .Over a range of synthetic - data problems , compared to the usual practice of choosing approximate MAP parameters , our approach significantly reduces loss on test data , sometimes by an order of magnitude .","label":"Background","metadata":{},"score":"50.673367"}
{"text":"We show how to locally optimize this risk using back - propagation and stochastic meta - descent .Over a range of synthetic - data problems , compared to the usual practice of choosing approximate MAP parameters , our approach significantly reduces loss on test data , sometimes by an order of magnitude .","label":"Background","metadata":{},"score":"50.673367"}
{"text":"A basic version that bases relatedness on weighted edit distance yields superior smoothing of grammars learned from the Penn Treebank ( 20 % reduction of rule perplexity over the best previous method ) .In the Bayesian framework , a language learner should seek a grammar that explains observed data well and is also a priori probable .","label":"Background","metadata":{},"score":"50.726913"}
{"text":"A basic version that bases relatedness on weighted edit distance yields superior smoothing of grammars learned from the Penn Treebank ( 20 % reduction of rule perplexity over the best previous method ) .In the Bayesian framework , a language learner should seek a grammar that explains observed data well and is also a priori probable .","label":"Background","metadata":{},"score":"50.726913"}
{"text":"We have developed two methods , one discriminative [ 2 ] and one generative [ 3 ] , that use these rationales during training to obtain significant accuracy improvements over two strong baselines .Our generative model in particular could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks .","label":"Background","metadata":{},"score":"50.774647"}
{"text":"We have developed two methods , one discriminative [ 2 ] and one generative [ 3 ] , that use these rationales during training to obtain significant accuracy improvements over two strong baselines .Our generative model in particular could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks .","label":"Background","metadata":{},"score":"50.774647"}
{"text":"Much recent work in natural language processing treats linguistic analysis as an inference problem over graphs .This development opens up useful connections between machine learning , graph theory , and linguistics .The first part of this dissertation formulates syntactic dependency parsing as a dynamic Markov random field with the novel ingredient of global constraints .","label":"Background","metadata":{},"score":"50.816597"}
{"text":"Much recent work in natural language processing treats linguistic analysis as an inference problem over graphs .This development opens up useful connections between machine learning , graph theory , and linguistics .The first part of this dissertation formulates syntactic dependency parsing as a dynamic Markov random field with the novel ingredient of global constraints .","label":"Background","metadata":{},"score":"50.816597"}
{"text":"The new working implementation under development is available here on github .We show how to train the fast dependency parser of Smith and Eisner ( 2008 ) for improved accuracy .This parser can consider higher - order interactions among edges while retaining O ( n 3 ) runtime .","label":"Background","metadata":{},"score":"50.937805"}
{"text":"We are particularly interested in understanding and reconciling the formal linguistic and computational virtues of different phonological frameworks .PROGRAM - -----Taking Primitive Optimality Theory Beyond the Finite State Daniel M. Albro Extends the Primitive Optimality Theory formalism ( Eisner 1997 ) to handle reduplication .","label":"Background","metadata":{},"score":"50.93795"}
{"text":"A grammar is not explicitly specified ; the rules and contextual probabilities of occurrence are automatically generated from the corpus .The parser is extremely successful at producing and identifying the correct parse , and nearly deterministic in the number of parses that it produces .","label":"Background","metadata":{},"score":"50.96119"}
{"text":"A grammar is not explicitly specified ; the rules and contextual probabilities of occurrence are automatically generated from the corpus .The parser is extremely successful at producing and identifying the correct parse , and nearly deterministic in the number of parses that it produces .","label":"Background","metadata":{},"score":"50.96119"}
{"text":"While n -gram counts do not directly provide sentences , a distribution over sentences can be estimated from them in the same way that n -gram language models are estimated .We treat this distribution over sentences as an approximate corpus and show how unsupervised learning can be performed on such a corpus using variational inference .","label":"Background","metadata":{},"score":"50.964478"}
{"text":"While n -gram counts do not directly provide sentences , a distribution over sentences can be estimated from them in the same way that n -gram language models are estimated .We treat this distribution over sentences as an approximate corpus and show how unsupervised learning can be performed on such a corpus using variational inference .","label":"Background","metadata":{},"score":"50.964478"}
{"text":"A natural - language grammar will generally contain many related syntactic constructions for a given word ( e.g. , active and passive ) .Most grammar formalisms explain this redundancy by assuming some mechanism for generating new constructions systematically from old ones .","label":"Background","metadata":{},"score":"50.964867"}
{"text":"A natural - language grammar will generally contain many related syntactic constructions for a given word ( e.g. , active and passive ) .Most grammar formalisms explain this redundancy by assuming some mechanism for generating new constructions systematically from old ones .","label":"Background","metadata":{},"score":"50.964867"}
{"text":"It inferred the deep relationships from a sample of observed constructions , enabling it to generalize to unseen constructions ( \" transformational smoothing \" ) .This work introduced the more general technique of transformation modeling .The 2008 , 2009 , and 2011 papers below built up an elegant model of inflectional morphology , with each paper building on the previous one .","label":"Background","metadata":{},"score":"50.96728"}
{"text":"It inferred the deep relationships from a sample of observed constructions , enabling it to generalize to unseen constructions ( \" transformational smoothing \" ) .This work introduced the more general technique of transformation modeling .The 2008 , 2009 , and 2011 papers below built up an elegant model of inflectional morphology , with each paper building on the previous one .","label":"Background","metadata":{},"score":"50.96728"}
{"text":"In sum , finite state methods , both statistical and knowledge - based , are now being used in various areas of linguistic computation , ranging from speech and character recognition to message understanding and statistical machine translation .Finite state methods in NLP continue to be an area of further research .","label":"Background","metadata":{},"score":"51.045364"}
{"text":"It also naturally predicts the spelling of unobserved forms that are missing from these paradigms , and discovers inflectional principles ( grammar ) that generalize to wholly unobserved words .Our Bayesian generative model of the data explicitly represents tokens , types , inflections , paradigms , and locally conditioned string edits .","label":"Background","metadata":{},"score":"51.045944"}
{"text":"It also naturally predicts the spelling of unobserved forms that are missing from these paradigms , and discovers inflectional principles ( grammar ) that generalize to wholly unobserved words .Our Bayesian generative model of the data explicitly represents tokens , types , inflections , paradigms , and locally conditioned string edits .","label":"Background","metadata":{},"score":"51.045944"}
{"text":"However , we are also interested in learning to produce predictions quickly .For example , one can speed up loopy belief propagation by choosing sparser models and by stopping at some point before convergence .We manage the speed - accuracy tradeoff by explicitly optimizing for a linear combination of speed and accuracy .","label":"Background","metadata":{},"score":"51.13025"}
{"text":"However , we are also interested in learning to produce predictions quickly .For example , one can speed up loopy belief propagation by choosing sparser models and by stopping at some point before convergence .We manage the speed - accuracy tradeoff by explicitly optimizing for a linear combination of speed and accuracy .","label":"Background","metadata":{},"score":"51.13025"}
{"text":"Both the reordering model and the optimization techniques have broad applicability , and the availability of machine learning makes even new problems without obvious structure approachable .Cross - document coreference resolution : A key technology for learning by reading .In AAAI 2009 Spring Symposium on Learning by Reading and Learning to Read .","label":"Background","metadata":{},"score":"51.168922"}
{"text":"Both the reordering model and the optimization techniques have broad applicability , and the availability of machine learning makes even new problems without obvious structure approachable .Cross - document coreference resolution : A key technology for learning by reading .In AAAI 2009 Spring Symposium on Learning by Reading and Learning to Read .","label":"Background","metadata":{},"score":"51.168922"}
{"text":"Regular languages , regular grammars , and finite state automata , and their limitations .Context free grammars and their limitations .Weak Generative Capacity and Strong Generative Capacity .Tree substitution grammars .Tree adjoining grammars .Finite State Transducers and their use in morphology .","label":"Background","metadata":{},"score":"51.184853"}
{"text":"This raises the question of how grammatical forms might be computed .This paper presents an analysis of the Basic CV Syllable Theory ( Prince & Smolensky 1993 ) showing that , ... \" .In Optimality Theory , grammaticality is defined in terms of optimization over a large ( often infinite ) space of candidates .","label":"Background","metadata":{},"score":"51.22019"}
{"text":"Note : This paper is just a synthesis of Zaidan et al .( 2007 ) and Zaidan & Eisner ( 2008 ) .( I hate doing multiple papers on the same work , but I wanted the ML community outside of NLP to see these results , so I asked the workshop organizers for permission to republish them here . )","label":"Background","metadata":{},"score":"51.3596"}
{"text":"Note : This paper is just a synthesis of Zaidan et al .( 2007 ) and Zaidan & Eisner ( 2008 ) .( I hate doing multiple papers on the same work , but I wanted the ML community outside of NLP to see these results , so I asked the workshop organizers for permission to republish them here . )","label":"Background","metadata":{},"score":"51.3596"}
{"text":"For each verb , we construct the \" document \" of all nouns that have appeared as its object .Our method improves 14.6 % over Witten - Bell smoothing on the conditional perplexity of objects given the verb , and 27.5 % over random on detecting the most common senses of nouns in the SemCor corpus .","label":"Background","metadata":{},"score":"51.379173"}
{"text":"For each verb , we construct the \" document \" of all nouns that have appeared as its object .Our method improves 14.6 % over Witten - Bell smoothing on the conditional perplexity of objects given the verb , and 27.5 % over random on detecting the most common senses of nouns in the SemCor corpus .","label":"Background","metadata":{},"score":"51.379173"}
{"text":"The methods for producing a table of correspondences between roots and their inflected forms as well as for producing a morphological analyzer that can extend to new cases are discussed .The chapter concludes with a brief summary of other recent work in and relevant evidence for morphological induction .","label":"Background","metadata":{},"score":"51.575043"}
{"text":"The methods for producing a table of correspondences between roots and their inflected forms as well as for producing a morphological analyzer that can extend to new cases are discussed .The chapter concludes with a brief summary of other recent work in and relevant evidence for morphological induction .","label":"Background","metadata":{},"score":"51.575043"}
{"text":"The third family of grammars presented is transduction grammars that allow for the parsing of two or more related strings simultaneously .The authors also examine the task of parse selection , focusing on methods for defining a distribution for unification grammars and their usage for re - ranking the output of context - free parsers .","label":"Background","metadata":{},"score":"51.582104"}
{"text":"The third family of grammars presented is transduction grammars that allow for the parsing of two or more related strings simultaneously .The authors also examine the task of parse selection , focusing on methods for defining a distribution for unification grammars and their usage for re - ranking the output of context - free parsers .","label":"Background","metadata":{},"score":"51.582104"}
{"text":"We hypothesize that in some situations , providing rationales is a more fruitful use of an annotator 's time than annotating more examples .In unsupervised learning , where no training takes place , one simply hopes that the unsupervised learner will work well on any unlabeled test collection .","label":"Background","metadata":{},"score":"51.62278"}
{"text":"We hypothesize that in some situations , providing rationales is a more fruitful use of an annotator 's time than annotating more examples .In unsupervised learning , where no training takes place , one simply hopes that the unsupervised learner will work well on any unlabeled test collection .","label":"Background","metadata":{},"score":"51.62278"}
{"text":"We aim to explore this space automatically , focusing here on the case of agenda - based syntactic parsing ( Kay , 1986 ) .Unfortunately , off - the - shelf reinforcement learning techniques fail to learn good policies : the state space is simply too large to explore naively .","label":"Background","metadata":{},"score":"51.6296"}
{"text":"We aim to explore this space automatically , focusing here on the case of agenda - based syntactic parsing ( Kay , 1986 ) .Unfortunately , off - the - shelf reinforcement learning techniques fail to learn good policies : the state space is simply too large to explore naively .","label":"Background","metadata":{},"score":"51.6296"}
{"text":"We demonstrate the methods by jointly predicting morphological forms .Note : Additional details are given in the dissertation of Dreyer ( 2011 ) , and on the associated slides .Cotterell and Eisner ( 2015 ) give an improved inference method for graphical models over strings .","label":"Background","metadata":{},"score":"51.746292"}
{"text":"We demonstrate the methods by jointly predicting morphological forms .Note : Additional details are given in the dissertation of Dreyer ( 2011 ) , and on the associated slides .Cotterell and Eisner ( 2015 ) give an improved inference method for graphical models over strings .","label":"Background","metadata":{},"score":"51.746292"}
{"text":"Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing ( EMNLP ) .You may prefer to read the longer writeup of this work here .Excellent background on lexicalized PCFG models is Charniak ( 1997 ) .Week of Apr. 2 : Maximum Entropy Parsing Models .","label":"Background","metadata":{},"score":"51.82245"}
{"text":"This combines two finite - state traditions : algebraic ( hand - built expert systems ) and statistical ( empirically trainable systems ) .A leisurely journal paper on this work is coming soon .In their general form , some of the ideas were adapted from my thesis .","label":"Background","metadata":{},"score":"51.829224"}
{"text":"Chapter six , ' ' Finite - state Approaches to Syntax ' ' , introduces some of the most widely used syntactic models and several dynamic programming algorithms for efficient inference with weighted finite - state models .It then turns to class - based language models that alleviate the data sparseness problem by making generalizations about word behavior based on the behavior of similar words .","label":"Background","metadata":{},"score":"51.836273"}
{"text":"Chapter six , ' ' Finite - state Approaches to Syntax ' ' , introduces some of the most widely used syntactic models and several dynamic programming algorithms for efficient inference with weighted finite - state models .It then turns to class - based language models that alleviate the data sparseness problem by making generalizations about word behavior based on the behavior of similar words .","label":"Background","metadata":{},"score":"51.836273"}
{"text":"[Applications ] Prereq : 600.465 or perm req'd .The main goals of the seminar are ( a ) to cover some techniques people have tried for inducing hidden structure from text , ( b ) to get you thinking about how to do it better .","label":"Background","metadata":{},"score":"51.898865"}
{"text":"Could we explicitly train test - time inference heuristics to trade off accuracy and efficiency ?We focus our discussion on agenda - based natural language parsing under a weighted context - free grammar .We frame the problem as reinforcement learning , discuss its special properties , and propose new strategies .","label":"Background","metadata":{},"score":"51.928185"}
{"text":"Could we explicitly train test - time inference heuristics to trade off accuracy and efficiency ?We focus our discussion on agenda - based natural language parsing under a weighted context - free grammar .We frame the problem as reinforcement learning , discuss its special properties , and propose new strategies .","label":"Background","metadata":{},"score":"51.928185"}
{"text":"These are used to relate the many inflections in an inflectional paradigm to one another , and they use the probability models from Chapter 2 as components .A novel version of belief propagation is presented , which propagates distributions over strings through a network of connected finite - state transducers , to perform inference in morphological paradigms ( or other string fields ) .","label":"Background","metadata":{},"score":"52.073277"}
{"text":"These are used to relate the many inflections in an inflectional paradigm to one another , and they use the probability models from Chapter 2 as components .A novel version of belief propagation is presented , which propagates distributions over strings through a network of connected finite - state transducers , to perform inference in morphological paradigms ( or other string fields ) .","label":"Background","metadata":{},"score":"52.073277"}
{"text":"Our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features , while computing fewer than 30 % of the feature templates .We present an open - source virtual manipulative for conditional log - linear models .","label":"Background","metadata":{},"score":"52.12252"}
{"text":"Our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features , while computing fewer than 30 % of the feature templates .We present an open - source virtual manipulative for conditional log - linear models .","label":"Background","metadata":{},"score":"52.12252"}
{"text":"Indeed it develops a full statistical framework for lexicalized syntax .The learner 's job is to discover the system of probabilistic transformations ( often called lexical redundancy rules ) that underlies the patterns of regular and irregular syntactic constructions listed in the lexicon .","label":"Background","metadata":{},"score":"52.13898"}
{"text":"Indeed it develops a full statistical framework for lexicalized syntax .The learner 's job is to discover the system of probabilistic transformations ( often called lexical redundancy rules ) that underlies the patterns of regular and irregular syntactic constructions listed in the lexicon .","label":"Background","metadata":{},"score":"52.13898"}
{"text":"We describe a variant , skewed DA , which can incorporate a good initializer when it is available , and show significant improvements over EM on a grammar induction task .Language modeling , a technology found in many computerized speech recognition systems , can also be used in a text editor to implement an automated phrase completion feature that significantly reduces the number of keystrokes required to generate a radiology report , therefore increasing typing speed .","label":"Background","metadata":{},"score":"52.171852"}
{"text":"We describe a variant , skewed DA , which can incorporate a good initializer when it is available , and show significant improvements over EM on a grammar induction task .Language modeling , a technology found in many computerized speech recognition systems , can also be used in a text editor to implement an automated phrase completion feature that significantly reduces the number of keystrokes required to generate a radiology report , therefore increasing typing speed .","label":"Background","metadata":{},"score":"52.171852"}
{"text":"In the case of string - valued random variables , this allows us to use a non - parametric message family , related to variable - order n -gram models .The method automatically calibrates the complexity of each message to balance speed and accuracy .","label":"Background","metadata":{},"score":"52.21515"}
{"text":"In the case of string - valued random variables , this allows us to use a non - parametric message family , related to variable - order n -gram models .The method automatically calibrates the complexity of each message to balance speed and accuracy .","label":"Background","metadata":{},"score":"52.21515"}
{"text":"SUMMARY This book provides a critical overview of the state - of - the - art computational techniques and algorithms for handling morphological and syntactic phenomena .Finite - state approaches to morphology and syntax figure prominently in the book , but context - free and context - sensitive approaches to syntax are also covered .","label":"Background","metadata":{},"score":"52.427963"}
{"text":"We present O ( n 4 ) parsing algorithms for two bilexical formalisms ( see title ) , improving the previous upper bounds of O ( n 5 ) .Also , for a common special case that was known to allow O ( n 3 ) parsing ( Eisner , 1997 ) , we present an O ( n 3 ) algorithm with an improved grammar constant .","label":"Background","metadata":{},"score":"52.47254"}
{"text":"We present O ( n 4 ) parsing algorithms for two bilexical formalisms ( see title ) , improving the previous upper bounds of O ( n 5 ) .Also , for a common special case that was known to allow O ( n 3 ) parsing ( Eisner , 1997 ) , we present an O ( n 3 ) algorithm with an improved grammar constant .","label":"Background","metadata":{},"score":"52.47254"}
{"text":"Special Issue Description .The languages described by regular expressions are exactly those recognized by automata that have a finite number of states ( Kleene 1956 ) .This fundamental result has been extended to string transductions , sets of trees , formal power series , grammars , semigroups , and finite models .","label":"Background","metadata":{},"score":"52.495277"}
{"text":"We describe an approach to coreference resolution that relies on the intuition that easy decisions should be made early , while harder decisions should be left for later when more information is available .We are inspired by the recent success of the rule - based system of Raghunathan et al .","label":"Background","metadata":{},"score":"52.572296"}
{"text":"We describe an approach to coreference resolution that relies on the intuition that easy decisions should be made early , while harder decisions should be left for later when more information is available .We are inspired by the recent success of the rule - based system of Raghunathan et al .","label":"Background","metadata":{},"score":"52.572296"}
{"text":"Specifically , recursive queries look up and aggregate relevant or potentially relevant values .If the results of these queries are memoized for reuse , the memos may need to be updated through change propagation .We propose a declarative language , which generalizes Datalog , to support this work in a generic way .","label":"Background","metadata":{},"score":"52.638603"}
{"text":"Specifically , recursive queries look up and aggregate relevant or potentially relevant values .If the results of these queries are memoized for reuse , the memos may need to be updated through change propagation .We propose a declarative language , which generalizes Datalog , to support this work in a generic way .","label":"Background","metadata":{},"score":"52.638603"}
{"text":"The approach with hard bounds , \" vine grammar , \" accepts only a regular language , even though it happily retains a context - free parameterization and defines meaningful parse trees .We show how to parse this language in O ( n ) time , using a novel chart parsing algorithm with a low grammar constant ( rather than an impractically large finite - state recognizer with an exponential grammar constant ) .","label":"Background","metadata":{},"score":"52.644375"}
{"text":"The approach with hard bounds , \" vine grammar , \" accepts only a regular language , even though it happily retains a context - free parameterization and defines meaningful parse trees .We show how to parse this language in O ( n ) time , using a novel chart parsing algorithm with a low grammar constant ( rather than an impractically large finite - state recognizer with an exponential grammar constant ) .","label":"Background","metadata":{},"score":"52.644375"}
{"text":"In summer 2002 , a team at the Johns Hopkins CLSP Summer Workshop investigated \" tree - to - tree \" translation ( on dependency trees ) .That is , the training and decoding methods are given parse trees rather than sentences , and pay attention to syntax .","label":"Background","metadata":{},"score":"52.65868"}
{"text":"Network morphology : A DATR account of Russian nominal inflection ._ Journal of Linguistics _ 29 , 113 - 142 .Goldsmith , J. ( 2001 ) .Unsupervised acquisition of the morphology of a natural language ._ Computational Linguistics _ 27(2 ) , 153 - 198 .","label":"Background","metadata":{},"score":"52.688896"}
{"text":"Network morphology : A DATR account of Russian nominal inflection ._ Journal of Linguistics _ 29 , 113 - 142 .Goldsmith , J. ( 2001 ) .Unsupervised acquisition of the morphology of a natural language ._ Computational Linguistics _ 27(2 ) , 153 - 198 .","label":"Background","metadata":{},"score":"52.688896"}
{"text":"Probabilistic CKY parsing .Lexical Semantics .Word sense disambiguation , collocations and lexical acquisition from large text corpora .Word relations ( similarity , hyponymy , etc . ) , distributional semantics , distributional models of meaning .Computational Semantics .","label":"Background","metadata":{},"score":"52.69974"}
{"text":"This leads to interesting algorithmic problems , connected to query planning , deductive databases , and adaptive systems .The forthcoming version of the language is described in Eisner & Filardo ( 2011 ) , which illustrates its power on a wide range of problems in statistical AI and beyond .","label":"Background","metadata":{},"score":"52.724953"}
{"text":"This leads to interesting algorithmic problems , connected to query planning , deductive databases , and adaptive systems .The forthcoming version of the language is described in Eisner & Filardo ( 2011 ) , which illustrates its power on a wide range of problems in statistical AI and beyond .","label":"Background","metadata":{},"score":"52.724953"}
{"text":"We propose a new framework for supervised machine learning .Our goal is to learn from smaller amounts of supervised training data , by collecting a richer kind of training data : annotations with \" rationales .\" When annotating an example , the human teacher will also highlight evidence supporting this annotation - thereby teaching the machine learner why the example belongs to the category .","label":"Background","metadata":{},"score":"52.7412"}
{"text":"We propose a new framework for supervised machine learning .Our goal is to learn from smaller amounts of supervised training data , by collecting a richer kind of training data : annotations with \" rationales .\" When annotating an example , the human teacher will also highlight evidence supporting this annotation - thereby teaching the machine learner why the example belongs to the category .","label":"Background","metadata":{},"score":"52.7412"}
{"text":"The study includes but is not restricted to : . representations of languages by tree automata and tree grammars .non - projectivity and partial commutativity of yields of tree automata and grammars .language generation through tree logics . efficient and dynamic construction of automata from updated rules or logical formulas . representations of trees and automata - based query languages for tree banks .","label":"Background","metadata":{},"score":"52.843887"}
{"text":"The likelihood of the data decomposes into a sum of local gains , one for each primitive in the final structure .We focus on a specific subclass of networks which are binary forests .Structure optimization corresponds to an integer linear program and the maximizing composition can be computed for reasonably large numbers of variables .","label":"Background","metadata":{},"score":"52.893368"}
{"text":"The likelihood of the data decomposes into a sum of local gains , one for each primitive in the final structure .We focus on a specific subclass of networks which are binary forests .Structure optimization corresponds to an integer linear program and the maximizing composition can be computed for reasonably large numbers of variables .","label":"Background","metadata":{},"score":"52.893368"}
{"text":"The review details Goldsmith 's methods for candidate generation and evaluation , which are based on weighted mutual information and minimum description length respectively .The review describes the method for finding affixes and for capturing pairs of potential morphological variants using a combination of semantic , orthographic , and syntactic environment - based probabilities .","label":"Background","metadata":{},"score":"52.922356"}
{"text":"The review details Goldsmith 's methods for candidate generation and evaluation , which are based on weighted mutual information and minimum description length respectively .The review describes the method for finding affixes and for capturing pairs of potential morphological variants using a combination of semantic , orthographic , and syntactic environment - based probabilities .","label":"Background","metadata":{},"score":"52.922356"}
{"text":"An LM is then trained to discriminate between the original sentences and the impostors .The procedure is applied to the IWSLT Chinese - to - English translation task , and promising improvements on a state - of - the - art MT system are demonstrated .","label":"Background","metadata":{},"score":"52.984592"}
{"text":"An LM is then trained to discriminate between the original sentences and the impostors .The procedure is applied to the IWSLT Chinese - to - English translation task , and promising improvements on a state - of - the - art MT system are demonstrated .","label":"Background","metadata":{},"score":"52.984592"}
{"text":"Their form is simple but novel .They assess , based on features of the input sentence , how strongly each pair of input word tokens wi;wj would like to reverse their relative order .Combining all these pairwise preferences to find the best global reordering is NP - hard .","label":"Background","metadata":{},"score":"52.989388"}
{"text":"Their form is simple but novel .They assess , based on features of the input sentence , how strongly each pair of input word tokens wi;wj would like to reverse their relative order .Combining all these pairwise preferences to find the best global reordering is NP - hard .","label":"Background","metadata":{},"score":"52.989388"}
{"text":"This work demonstrates how representations can be compiled into a form that can be directly manipulated by finite state machines .Independently of this , we also need to provide a means for phonolog ... . \" ...A fundamental debate in the machine learning of language has been the role of prior knowledge in the learning process .","label":"Background","metadata":{},"score":"53.015553"}
{"text":"We construct our ... . \" ...It has been argued that rule - based phonological descriptions can uniformly be expressed as map - pings carried out by finite - state transducers , and therefore fall within the class of rational relations .","label":"Background","metadata":{},"score":"53.12912"}
{"text":"Next , the chapter discusses approaches to part - of - speech tagging , focusing on HMM models and log - linear models .The Viterbi and Forward - backward algorithms are presented , so are Maximum Entropy models , Conditional Random Fields , and the Perceptron algorithm .","label":"Background","metadata":{},"score":"53.1565"}
{"text":"Next , the chapter discusses approaches to part - of - speech tagging , focusing on HMM models and log - linear models .The Viterbi and Forward - backward algorithms are presented , so are Maximum Entropy models , Conditional Random Fields , and the Perceptron algorithm .","label":"Background","metadata":{},"score":"53.1565"}
{"text":"Our system therefore chooses an optimal phrase length for each prediction , using Bellman - style dynamic programming to minimize the expected cost of typing the rest of the document .This computation considers what the user is likely to type in the future , and how many keystrokes it will take , considering the future effect of phrase completion as well .","label":"Background","metadata":{},"score":"53.274323"}
{"text":"Our system therefore chooses an optimal phrase length for each prediction , using Bellman - style dynamic programming to minimize the expected cost of typing the rest of the document .This computation considers what the user is likely to type in the future , and how many keystrokes it will take , considering the future effect of phrase completion as well .","label":"Background","metadata":{},"score":"53.274323"}
{"text":"How do we set the weights in the hypergraph ?Which particular translation ( among the possible translations encoded in a hypergraph ) should we present to an end user ?These correspond to three fundamental problems : inference , training , and decoding , for which this dissertation will present novel techniques .","label":"Background","metadata":{},"score":"53.289696"}
{"text":"How do we set the weights in the hypergraph ?Which particular translation ( among the possible translations encoded in a hypergraph ) should we present to an end user ?These correspond to three fundamental problems : inference , training , and decoding , for which this dissertation will present novel techniques .","label":"Background","metadata":{},"score":"53.289696"}
{"text":"Besides the linear loss functions used in previous work , we also describe techniques for optimizing nonlinear functions such as precision or the BLEU metric .We present experiments training log - linear combinations of models for dependency parsing and for machine translation .","label":"Background","metadata":{},"score":"53.301003"}
{"text":"Besides the linear loss functions used in previous work , we also describe techniques for optimizing nonlinear functions such as precision or the BLEU metric .We present experiments training log - linear combinations of models for dependency parsing and for machine translation .","label":"Background","metadata":{},"score":"53.301003"}
{"text":"In many domains , our best models are computationally intractable .This problem will only get worse as we manage to build more richly detailed models of specific domains .Fortunately , the practical goal of artificial or natural intelligence is not to do perfect detailed inference , but rather to answer specific questions by reasoning from observed data .","label":"Background","metadata":{},"score":"53.55885"}
{"text":"In many domains , our best models are computationally intractable .This problem will only get worse as we manage to build more richly detailed models of specific domains .Fortunately , the practical goal of artificial or natural intelligence is not to do perfect detailed inference , but rather to answer specific questions by reasoning from observed data .","label":"Background","metadata":{},"score":"53.55885"}
{"text":"We describe finite - state constraint relaxation , a method for applying global constraints , expressed as automata , to sequence model decoding .We present algorithms for both hard constraints and binary soft constraints .On the CoNLL-2004 semantic role labeling task , we report a speedup of at least 16x over a previous method that used integer linear programming .","label":"Background","metadata":{},"score":"53.621635"}
{"text":"We describe finite - state constraint relaxation , a method for applying global constraints , expressed as automata , to sequence model decoding .We present algorithms for both hard constraints and binary soft constraints .On the CoNLL-2004 semantic role labeling task , we report a speedup of at least 16x over a previous method that used integer linear programming .","label":"Background","metadata":{},"score":"53.621635"}
{"text":"We show how to apply loopy belief propagation ( BP ) , a simple and effective tool for approximate learning and inference .As a parsing algorithm , BP is both asymptotically and empirically efficient .Even with second - order features or latent variables , which would make exact parsing considerably slower or NP - hard , BP needs only O ( n 3 ) time with a small constant factor .","label":"Background","metadata":{},"score":"53.65528"}
{"text":"We show how to apply loopy belief propagation ( BP ) , a simple and effective tool for approximate learning and inference .As a parsing algorithm , BP is both asymptotically and empirically efficient .Even with second - order features or latent variables , which would make exact parsing considerably slower or NP - hard , BP needs only O ( n 3 ) time with a small constant factor .","label":"Background","metadata":{},"score":"53.65528"}
{"text":"Furthermore , we present heuristics that perform the maximization in a greedy manner , and we demonstrate their effectiveness with empirical results from multi - spectral imaging .A finite - state machine with n tapes describes a rational ( or regular ) relation on n strings .","label":"Background","metadata":{},"score":"53.743397"}
{"text":"Furthermore , we present heuristics that perform the maximization in a greedy manner , and we demonstrate their effectiveness with empirical results from multi - spectral imaging .A finite - state machine with n tapes describes a rational ( or regular ) relation on n strings .","label":"Background","metadata":{},"score":"53.743397"}
{"text":"Finally , I will suggest that machine learning should be used to search for the right strategies for a program on a particular workload .This dissertation is about ordering .The problem of arranging a set of n items in a desired order is quite common , as well as fundamental to computer science .","label":"Background","metadata":{},"score":"53.780373"}
{"text":"Finally , I will suggest that machine learning should be used to search for the right strategies for a program on a particular workload .This dissertation is about ordering .The problem of arranging a set of n items in a desired order is quite common , as well as fundamental to computer science .","label":"Background","metadata":{},"score":"53.780373"}
{"text":"Skewed deterministic annealing locally maximizes likelihood using a cautious parameter search strategy that starts with an easier optimization problem than likelihood , and iteratively moves to harder problems , culminating in likelihood .Structural annealing is similar , but starts with a heavy bias toward simple syntactic structures and gradually relaxes the bias .","label":"Background","metadata":{},"score":"53.805183"}
{"text":"Skewed deterministic annealing locally maximizes likelihood using a cautious parameter search strategy that starts with an easier optimization problem than likelihood , and iteratively moves to harder problems , culminating in likelihood .Structural annealing is similar , but starts with a heavy bias toward simple syntactic structures and gradually relaxes the bias .","label":"Background","metadata":{},"score":"53.805183"}
{"text":"We use Integer Linear Programming to assign consistent directions to the labeled links in a corpus of several thousand parses produced by the Link Grammar Parser , which has broad - coverage hand - written grammars of English as well as Russian and other languages .","label":"Background","metadata":{},"score":"53.81878"}
{"text":"We use Integer Linear Programming to assign consistent directions to the labeled links in a corpus of several thousand parses produced by the Link Grammar Parser , which has broad - coverage hand - written grammars of English as well as Russian and other languages .","label":"Background","metadata":{},"score":"53.81878"}
{"text":"To students of computational linguistics who want to learn more about computational morphology and syntax , the book provides an excellent systematic overview of the current state - of - the - art of the field .They will walk away with a good understanding of the key issues involved in computational modeling of morphology and syntax as well as some of the most widely used approaches to handling these issues .","label":"Background","metadata":{},"score":"53.85273"}
{"text":"To students of computational linguistics who want to learn more about computational morphology and syntax , the book provides an excellent systematic overview of the current state - of - the - art of the field .They will walk away with a good understanding of the key issues involved in computational modeling of morphology and syntax as well as some of the most widely used approaches to handling these issues .","label":"Background","metadata":{},"score":"53.85273"}
{"text":"Gormley et al .( 2015 ) applied it to dependency parsing by belief propagation .Stoyanov and Eisner ( 2011 , 2012b ) gave preliminary extensions to optimize speed jointly with accuracy .Modern statistical AI systems are quite large and complex ; this interferes with research , development , and education .","label":"Background","metadata":{},"score":"53.89439"}
{"text":"Gormley et al .( 2015 ) applied it to dependency parsing by belief propagation .Stoyanov and Eisner ( 2011 , 2012b ) gave preliminary extensions to optimize speed jointly with accuracy .Modern statistical AI systems are quite large and complex ; this interferes with research , development , and education .","label":"Background","metadata":{},"score":"53.89439"}
{"text":"Discriminative training for machine translation has been well studied in the recent past .A limitation of the work to date is that it relies on the availability of high - quality in - domain bilingual text for supervised training .We present an unsupervised discriminative training framework to incorporate the usually plentiful target - language monolingual data by using a rough \" reverse \" translation system .","label":"Background","metadata":{},"score":"53.983368"}
{"text":"Discriminative training for machine translation has been well studied in the recent past .A limitation of the work to date is that it relies on the availability of high - quality in - domain bilingual text for supervised training .We present an unsupervised discriminative training framework to incorporate the usually plentiful target - language monolingual data by using a rough \" reverse \" translation system .","label":"Background","metadata":{},"score":"53.983368"}
{"text":"Since I 'm interested not only in competence grammars , but also in robust techniques for comprehension and acquisition , statistical models form an important part of my repertoire .I tend to think of the world as a big parametric probability distribution .","label":"Background","metadata":{},"score":"54.009758"}
{"text":"The trees T 2 allowed by this monolingual grammar are inspired by pieces of substructure in T 1 and aligned to T 1 at those points .We describe experiments learning quasi - synchronous context - free grammars from bitext .As with other monolingual language models , we evaluate the crossentropy of QGs on unseen text and show that a better fit to bilingual data is achieved by allowing greater syntactic divergence .","label":"Background","metadata":{},"score":"54.02666"}
{"text":"The trees T 2 allowed by this monolingual grammar are inspired by pieces of substructure in T 1 and aligned to T 1 at those points .We describe experiments learning quasi - synchronous context - free grammars from bitext .As with other monolingual language models , we evaluate the crossentropy of QGs on unseen text and show that a better fit to bilingual data is achieved by allowing greater syntactic divergence .","label":"Background","metadata":{},"score":"54.02666"}
{"text":"We investigate dual decomposition for joint MAP inference of many strings .Given an arbitrary graphical model , we decompose it into small acyclic sub - models , whose MAP configurations can be found by finite - state composition and dynamic programming .","label":"Background","metadata":{},"score":"54.064827"}
{"text":"We investigate dual decomposition for joint MAP inference of many strings .Given an arbitrary graphical model , we decompose it into small acyclic sub - models , whose MAP configurations can be found by finite - state composition and dynamic programming .","label":"Background","metadata":{},"score":"54.064827"}
{"text":"Normal - Form Parsing .In unrelated work , en route to grad school I devised a normal - form parsing algorithm for Combinatory Categorial Grammar ( CCG ) , whose correctness I proved the following year .Rather than falling prey to \" spurious ambiguity \" ( the bane of CCG ) , it was guaranteed to find exactly one parse from every semantic equivalence class : .","label":"Background","metadata":{},"score":"54.07708"}
{"text":"The idea has been adopted elsewhere in NLP and in computer vision .How do young children listen to their native language and figure out its linguistic structure ?I 'm dying to know , but I would settle for solving the related NLP problem of grammar induction .","label":"Background","metadata":{},"score":"54.090576"}
{"text":"The idea has been adopted elsewhere in NLP and in computer vision .How do young children listen to their native language and figure out its linguistic structure ?I 'm dying to know , but I would settle for solving the related NLP problem of grammar induction .","label":"Background","metadata":{},"score":"54.090576"}
{"text":"Weighted finite - state machines with n tapes describe n -ary rational string relations .The join n -ary relation is very important in applications .It is shown how to compute it via a more simple operation , the auto - intersection .","label":"Background","metadata":{},"score":"54.092327"}
{"text":"Weighted finite - state machines with n tapes describe n -ary rational string relations .The join n -ary relation is very important in applications .It is shown how to compute it via a more simple operation , the auto - intersection .","label":"Background","metadata":{},"score":"54.092327"}
{"text":"The rest of the book is organized into two parts .Part one , ' ' Computational Approaches to Morphology ' ' , consists of Chapters two through five .Chapter two , ' 'The Formal Characterization of Morphological Operations ' ' , describes the formal devices used in different languages to encode information morphologically .","label":"Background","metadata":{},"score":"54.1149"}
{"text":"To perform each operation , we may implement a dedicated dynamic programming algorithm .However , a more general framework to specify these algorithms is semiring - weighted logic programming .Within this framework , we first extend the expectation semiring , which is originally proposed for a finite state automaton , to a hypergraph .","label":"Background","metadata":{},"score":"54.189224"}
{"text":"To perform each operation , we may implement a dedicated dynamic programming algorithm .However , a more general framework to specify these algorithms is semiring - weighted logic programming .Within this framework , we first extend the expectation semiring , which is originally proposed for a finite state automaton , to a hypergraph .","label":"Background","metadata":{},"score":"54.189224"}
{"text":"The chapter first describes techniques for estimating probabilistic and lexicalized CFGs from treebanks and presents two well - known statistical parsers : Collin 's parser and Charniak parser .It then moves away from constituent structure to lexical dependencies , and introduces dependency parsing .","label":"Background","metadata":{},"score":"54.189674"}
{"text":"The chapter first describes techniques for estimating probabilistic and lexicalized CFGs from treebanks and presents two well - known statistical parsers : Collin 's parser and Charniak parser .It then moves away from constituent structure to lexical dependencies , and introduces dependency parsing .","label":"Background","metadata":{},"score":"54.189674"}
{"text":"The bulk of the chapter comprises an overview of ( weighted ) finite - state automata and transducers and a synopsis of fundamental algorithmic issues that relate to composition , minimization , determinization , and epsilon removal .The chapter concludes with a discussion of the broad applicability of finite - state methods , their roles in computational approaches to morphology and syntax , and issues involved in building efficient and robust syntactic models .","label":"Background","metadata":{},"score":"54.203556"}
{"text":"To illustrate the improvement from conditioning on context , we model typos found in social media text .Feature computation and exhaustive search have significantly restricted the speed of graph - based dependency parsing .We propose a faster framework of dynamic feature selection , where features are added sequentially as needed , edges are pruned early , and decisions are made online for each sentence .","label":"Background","metadata":{},"score":"54.258224"}
{"text":"To illustrate the improvement from conditioning on context , we model typos found in social media text .Feature computation and exhaustive search have significantly restricted the speed of graph - based dependency parsing .We propose a faster framework of dynamic feature selection , where features are added sequentially as needed , edges are pruned early , and decisions are made online for each sentence .","label":"Background","metadata":{},"score":"54.258224"}
{"text":"We show how to iterate this reordering process within a local search algorithm , which we use in training .Statistical models in machine translation exhibit spurious ambiguity .That is , the probability of an output string is split among many distinct derivations ( e.g. , trees or segmentations ) .","label":"Background","metadata":{},"score":"54.259895"}
{"text":"We show how to iterate this reordering process within a local search algorithm , which we use in training .Statistical models in machine translation exhibit spurious ambiguity .That is , the probability of an output string is split among many distinct derivations ( e.g. , trees or segmentations ) .","label":"Background","metadata":{},"score":"54.259895"}
{"text":"At the center of this approach stands the notion of inflectional paradigms .These paradigms cluster the large vocabulary of a language into structured chunks ; inflections of the same word , like break , broke , breaks , breaking , ... , all belong in the same paradigm .","label":"Background","metadata":{},"score":"54.328384"}
{"text":"At the center of this approach stands the notion of inflectional paradigms .These paradigms cluster the large vocabulary of a language into structured chunks ; inflections of the same word , like break , broke , breaks , breaking , ... , all belong in the same paradigm .","label":"Background","metadata":{},"score":"54.328384"}
{"text":"We search for the maximum - likelihood model parameters and corpus parse , subject to posterior constraints .We show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints .We use the Reformulation Linearization Technique to produce convex relaxations during branch - and - bound .","label":"Background","metadata":{},"score":"54.337467"}
{"text":"We search for the maximum - likelihood model parameters and corpus parse , subject to posterior constraints .We show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints .We use the Reformulation Linearization Technique to produce convex relaxations during branch - and - bound .","label":"Background","metadata":{},"score":"54.337467"}
{"text":"In this paper we suggest that an alternative to the purely nativist or purely empiricist learning paradigms is to represent the prior knowledge of language as a set of abstract learning biases , which guide an empirical inductive learning algorithm .We test our idea by examining the machine learning of simple Sound Pattern of English ( S P E)-style phonological rules .","label":"Background","metadata":{},"score":"54.346565"}
{"text":"Tools . \" ...In this article we show how Optimality Theory yields a highly general Constraint Demotion principle for grammar learning .The resulting learning procedure specifically exploits the grammatical structure of Optimality Theory , independent of the content of substantive constraints defining any given gr ... \" .","label":"Background","metadata":{},"score":"54.411804"}
{"text":"Experiments from the \" 20 Newsgroups \" corpus show that , although both techniques improve the performance of the baseline algorithms , \" strapping \" is clearly a better choice for cross - instance tuning .Iterative denoising trees were used by Karakos et al .","label":"Background","metadata":{},"score":"54.46763"}
{"text":"Experiments from the \" 20 Newsgroups \" corpus show that , although both techniques improve the performance of the baseline algorithms , \" strapping \" is clearly a better choice for cross - instance tuning .Iterative denoising trees were used by Karakos et al .","label":"Background","metadata":{},"score":"54.46763"}
{"text":"In the machine translation setting , the weights are not given , only words .The dissertation applies machine learning techniques to derive a LOP from each given sentence using a corpus of sentences and their translations for training .It proposes a set of features for such learning and argues their propriety for translation based on an analogy to dependency parsing .","label":"Background","metadata":{},"score":"54.517643"}
{"text":"In the machine translation setting , the weights are not given , only words .The dissertation applies machine learning techniques to derive a LOP from each given sentence using a corpus of sentences and their translations for training .It proposes a set of features for such learning and argues their propriety for translation based on an analogy to dependency parsing .","label":"Background","metadata":{},"score":"54.517643"}
{"text":"Since the error surface for many natural language problems is piecewise constant and riddled with local minima , many systems instead optimize log - likelihood , which is conveniently differentiable and convex .We propose training instead to minimize the expected loss , or risk .","label":"Background","metadata":{},"score":"54.545967"}
{"text":"Since the error surface for many natural language problems is piecewise constant and riddled with local minima , many systems instead optimize log - likelihood , which is conveniently differentiable and convex .We propose training instead to minimize the expected loss , or risk .","label":"Background","metadata":{},"score":"54.545967"}
{"text":"Conditional Random Fields ( CRFs ) are a popular formalism for structured prediction in NLP .It is well known how to train CRFs with certain topologies that admit exact inference , such as linear - chain CRFs .Some NLP phenomena , however , suggest CRFs with more complex topologies .","label":"Background","metadata":{},"score":"54.663628"}
{"text":"Conditional Random Fields ( CRFs ) are a popular formalism for structured prediction in NLP .It is well known how to train CRFs with certain topologies that admit exact inference , such as linear - chain CRFs .Some NLP phenomena , however , suggest CRFs with more complex topologies .","label":"Background","metadata":{},"score":"54.663628"}
{"text":"Many standard algorithms are special cases , and there are many further applications .Yet the algorithm remains surprisingly simple because all the difficult work is subcontracted to existing algorithms for semiring - weighted automata .The trick is to use a novel semiring .","label":"Background","metadata":{},"score":"54.68838"}
{"text":"Many standard algorithms are special cases , and there are many further applications .Yet the algorithm remains surprisingly simple because all the difficult work is subcontracted to existing algorithms for semiring - weighted automata .The trick is to use a novel semiring .","label":"Background","metadata":{},"score":"54.68838"}
{"text":"During decoding , we are interested in finding a translation that has a maximum posterior probability ( i.e. , MAP decoding ) .However , this is intractable due to spurious ambiguity , a situation where the probability of a translation string is split among many distinct derivations ( e.g. , trees or segmentations ) .","label":"Background","metadata":{},"score":"54.802"}
{"text":"During decoding , we are interested in finding a translation that has a maximum posterior probability ( i.e. , MAP decoding ) .However , this is intractable due to spurious ambiguity , a situation where the probability of a translation string is split among many distinct derivations ( e.g. , trees or segmentations ) .","label":"Background","metadata":{},"score":"54.802"}
{"text":"It is also more principled , and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks .String - to - string transduction is a central problem in computational linguistics and natural language processing .It occurs in tasks as diverse as name transliteration , spelling correction , pronunciation modeling and inflectional morphology .","label":"Background","metadata":{},"score":"54.805058"}
{"text":"It is also more principled , and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks .String - to - string transduction is a central problem in computational linguistics and natural language processing .It occurs in tasks as diverse as name transliteration , spelling correction , pronunciation modeling and inflectional morphology .","label":"Background","metadata":{},"score":"54.805058"}
{"text":"We have an accompanying library of techniques for learning model parameters .To design the language , the compiler , and the machine learning library , we have had to embed the various techniques we know into a common framework .This has been fruitful and enlightening work , and we all use the result to get our research done .","label":"Background","metadata":{},"score":"54.91998"}
{"text":"It involves looking for the \" best \" parameters for a grammar .But how do we define \" best \" ?And how do we find the globally \" best \" parameters when the objective function has many local maxima ?","label":"Background","metadata":{},"score":"54.9599"}
{"text":"Part one , ' ' Computational Approaches to Morphology ' ' , consists of Chapters two through five .Chapter two , ' 'The Formal Characterization of Morphological Operations ' ' , describes the formal devices used in different languages to encode information morphologically .","label":"Background","metadata":{},"score":"55.070717"}
{"text":"Incorporating additional features increases the runtime additively rather than multiplicatively .The second part extends these models to capture correspondences among non - isomorphic structures .When bootstrapping a parser in a low - resource target language by exploiting a parser in a high - resource source language , models that score the alignment and the correspondence of divergent syntactic configurations in translational sentence pairs achieve higher accuracy in parsing the target language .","label":"Background","metadata":{},"score":"55.09498"}
{"text":"Incorporating additional features increases the runtime additively rather than multiplicatively .The second part extends these models to capture correspondences among non - isomorphic structures .When bootstrapping a parser in a low - resource target language by exploiting a parser in a high - resource source language , models that score the alignment and the correspondence of divergent syntactic configurations in translational sentence pairs achieve higher accuracy in parsing the target language .","label":"Background","metadata":{},"score":"55.09498"}
{"text":"We show that OSTIA , a general - purpose transducer induction algorithm , was incapable of learning simple phonological rules like flapping .We then augmented OSTIA with three kinds of learning biases that are specific to natural language phonology , and that are assumed explicitly or implicitly by every theory of phonology : faithfulness ( underlying segments . by Daniel Matthew Albro , Daniel Matthew Albro , Bruce P. Hayes , Donca Steriade , Edward P. Stabler , Committee Chair , 1997 . \" ...","label":"Background","metadata":{},"score":"55.210663"}
{"text":"The study was pioneered by Schtzenberger , McNaughton , Papert and Kamp , who established the equivalence between star - free generalized regular expressions , counter - free automata , first - order logic and temporal logic .Kleene 's theorem , its extensions and its restrictions have tremendous methodological relevance to speech and natural language processing ( NLP ) .","label":"Background","metadata":{},"score":"55.219337"}
{"text":"Given enough direct evidence about a lexical entry 's probability , our Bayesian approach trusts the evidence ; but when less evidence or no evidence is available , it relies more on the transformations ' rates to guess how often the entry will be derived from related entries .","label":"Background","metadata":{},"score":"55.317215"}
{"text":"Given enough direct evidence about a lexical entry 's probability , our Bayesian approach trusts the evidence ; but when less evidence or no evidence is available , it relies more on the transformations ' rates to guess how often the entry will be derived from related entries .","label":"Background","metadata":{},"score":"55.317215"}
{"text":"An interesting prediction of ( b ) and ( c ) is that left - to - right trochees should be incompatible with extrametricality .This prediction is robustly confirmed in Hayes .This paper introduces weighted bilexical grammars , a formalism in which individual lexical items , such as verbs and their arguments , can have idiosyncratic selectional influences on each other .","label":"Background","metadata":{},"score":"55.335194"}
{"text":"An interesting prediction of ( b ) and ( c ) is that left - to - right trochees should be incompatible with extrametricality .This prediction is robustly confirmed in Hayes .This paper introduces weighted bilexical grammars , a formalism in which individual lexical items , such as verbs and their arguments , can have idiosyncratic selectional influences on each other .","label":"Background","metadata":{},"score":"55.335194"}
{"text":"The first paper gives an approximate algorithm for the real or tropical semiring ; the second gives an algorithm for the tropical semiring that is exact if it terminates .Integrated Sensing and Processing Decision Trees ( ISPDTs ) were introduced in [ 1 ] as a tool for supervised classification of high - dimensional data .","label":"Background","metadata":{},"score":"55.395363"}
{"text":"The first paper gives an approximate algorithm for the real or tropical semiring ; the second gives an algorithm for the tropical semiring that is exact if it terminates .Integrated Sensing and Processing Decision Trees ( ISPDTs ) were introduced in [ 1 ] as a tool for supervised classification of high - dimensional data .","label":"Background","metadata":{},"score":"55.395363"}
{"text":"We show that a crude way to use dependency length as a parsing feature can substantially improve parsing speed and accuracy in English and Chinese , with more mixed results on German .We then show similar improvements by imposing hard bounds on dependency length and ( additionally ) modeling the resulting sequence of parse fragments .","label":"Background","metadata":{},"score":"55.397396"}
{"text":"We show that a crude way to use dependency length as a parsing feature can substantially improve parsing speed and accuracy in English and Chinese , with more mixed results on German .We then show similar improvements by imposing hard bounds on dependency length and ( additionally ) modeling the resulting sequence of parse fragments .","label":"Background","metadata":{},"score":"55.397396"}
{"text":"This second - order semiring is essential for many interesting training paradigms such as minimum risk , deterministic annealing , active learning , and semi - supervised learning , where gradient descent optimization requires computing the gradient of entropy or risk .","label":"Background","metadata":{},"score":"55.407364"}
{"text":"This second - order semiring is essential for many interesting training paradigms such as minimum risk , deterministic annealing , active learning , and semi - supervised learning , where gradient descent optimization requires computing the gradient of entropy or risk .","label":"Background","metadata":{},"score":"55.407364"}
{"text":"We present two unsupervised discriminative training methods : minimum imputed - risk training , and contrastive language model estimation , both can exploit monolingual English data to perform discriminative training .In minimum imputed - risk training , we first use a reverse translation model to impute the missing inputs , and then train a discriminative forward model by minimizing the expected loss of the forward translations of the missing inputs .","label":"Background","metadata":{},"score":"55.486515"}
{"text":"We present two unsupervised discriminative training methods : minimum imputed - risk training , and contrastive language model estimation , both can exploit monolingual English data to perform discriminative training .In minimum imputed - risk training , we first use a reverse translation model to impute the missing inputs , and then train a discriminative forward model by minimizing the expected loss of the forward translations of the missing inputs .","label":"Background","metadata":{},"score":"55.486515"}
{"text":"Subsets of primitives are combined in a lego - like fashion to construct a probabilistic graphical model ; only a small fraction of the primitives will participate in any valid construction .Since primitives can be precomputed , parameter estimation and structure search are separated .","label":"Background","metadata":{},"score":"55.48654"}
{"text":"Subsets of primitives are combined in a lego - like fashion to construct a probabilistic graphical model ; only a small fraction of the primitives will participate in any valid construction .Since primitives can be precomputed , parameter estimation and structure search are separated .","label":"Background","metadata":{},"score":"55.48654"}
{"text":"Our systems must guess the syntactic structure of a sentence , the translation of a sentence , the grammar of a language , or the set of real - world facts that is consistent with a set of documents .Dynamic programming is extremely useful for analyzing sequence data .","label":"Background","metadata":{},"score":"55.488953"}
{"text":"This fills in some missing material from the previous ( short ) article .Just read section 6 , \" The Noise - Free Monosemous Case , \" which starts on p. 54 of the print version and p. 19 of the online version .","label":"Background","metadata":{},"score":"55.50261"}
{"text":"On a real - world corpus of sentences from software testing documents , with 23 possible parses for a sentence of average length , the system accurately finds the correct parse in 99 % of cases , while producing only 1.02 parses per sentence .","label":"Background","metadata":{},"score":"55.55938"}
{"text":"On a real - world corpus of sentences from software testing documents , with 23 possible parses for a sentence of average length , the system accurately finds the correct parse in 99 % of cases , while producing only 1.02 parses per sentence .","label":"Background","metadata":{},"score":"55.55938"}
{"text":"Also appears in Natural Language Processing Using Very Large Corpora , 1997 .Shows how to use a dictionary and a lot of text to train an unsupervised tagger , by bootstrapping off the unambiguous words .This tagger could be compiled into a transducer as in last week 's Roche & Schabes paper .","label":"Background","metadata":{},"score":"55.61795"}
{"text":"In initial experiments , this surpassed EM for training a simple feature - poor generative model , and also improved the performance of a feature - rich , conditionally estimated model where EM could not easily have been applied .For our models and training sets , more peaked measures of confidence , measured by Rnyi entropy , outperformed smoother ones .","label":"Background","metadata":{},"score":"55.695343"}
{"text":"In initial experiments , this surpassed EM for training a simple feature - poor generative model , and also improved the performance of a feature - rich , conditionally estimated model where EM could not easily have been applied .For our models and training sets , more peaked measures of confidence , measured by Rnyi entropy , outperformed smoother ones .","label":"Background","metadata":{},"score":"55.695343"}
{"text":"We show that this is true when GEN is itself a rational relation , and each of the constraints distinguishes among finitely many regular sets of candidates .ts of well - formedness constraints ( Paradis 1988 , Scobbie 1991 , Prince and Smolensky 1993 , Burzio 1994 ) .","label":"Background","metadata":{},"score":"55.71666"}
{"text":"A broad approach is developed for training dynamical behaviors in connectionist networks .General recurrent networks are powerful computational devices , necessary for difficult tasks like constraint satisfaction and temporal processing .These tasks are discussed here in some detail .From both theoretical and empirical considerations , it is concluded that such tasks are best addressed by recurrent networks that operate continuously in time - and further , that effective learning rules for these continuous - time networks must be able to prescribe their dynamical properties .","label":"Background","metadata":{},"score":"55.85981"}
{"text":"A broad approach is developed for training dynamical behaviors in connectionist networks .General recurrent networks are powerful computational devices , necessary for difficult tasks like constraint satisfaction and temporal processing .These tasks are discussed here in some detail .From both theoretical and empirical considerations , it is concluded that such tasks are best addressed by recurrent networks that operate continuously in time - and further , that effective learning rules for these continuous - time networks must be able to prescribe their dynamical properties .","label":"Background","metadata":{},"score":"55.85981"}
{"text":"The new formalism can be used to describe bilexical approaches to both dependency and phrase - structure grammars , and a slight modification yields link grammars .Its scoring approach is compatible with a wide variety of probability models .The obvious parsing algorithm for bilexical grammars ( used by most authors ) takes time O ( n 5 ) .","label":"Background","metadata":{},"score":"55.948463"}
{"text":"The new formalism can be used to describe bilexical approaches to both dependency and phrase - structure grammars , and a slight modification yields link grammars .Its scoring approach is compatible with a wide variety of probability models .The obvious parsing algorithm for bilexical grammars ( used by most authors ) takes time O ( n 5 ) .","label":"Background","metadata":{},"score":"55.948463"}
{"text":"The new formalism can be used to describe bilexical approaches to both dependency and phrase - structure grammars , and a slight modification yields link grammars .Its scoring approach is compatible with a wide variety of probability models .The obvious parsing algorithm for bilexical grammars ( used by most authors ) takes time O ( n 5 ) .","label":"Background","metadata":{},"score":"55.948463"}
{"text":"The new formalism can be used to describe bilexical approaches to both dependency and phrase - structure grammars , and a slight modification yields link grammars .Its scoring approach is compatible with a wide variety of probability models .The obvious parsing algorithm for bilexical grammars ( used by most authors ) takes time O ( n 5 ) .","label":"Background","metadata":{},"score":"55.948463"}
{"text":"Primitive Optimality Theory is a simple formal model of a subset of Optimality Theory ( Prince and Smolensky 1993 ) .The work presented here implements this model and extends it .The implementation is used to evaluate the Primitive Optimality Theory model , and is in itself a useful tool for linguistic analysis .","label":"Background","metadata":{},"score":"55.983055"}
{"text":"Searching for the most likely structure is a combinatorial optimization problem .Other combinatorial problems compute the probability of a particular structure or sub - structure .Does n't the machine learning community do structured prediction ?Yes : graphical model inference must predict discrete vectors .","label":"Background","metadata":{},"score":"56.01674"}
{"text":"The first is to summarize the state of the art in the finite state methods and models of the NLP community .The second is to increase awareness of the community about open issues and new perspectives : the study of the adequacy of finite - state methods and models extends from the traditional richness and use to new generalizations and ambitious tasks .","label":"Background","metadata":{},"score":"56.01812"}
{"text":"The authors argue that the only morphological operation that can not be handled elegantly by composition is reduplication , which seems to require a non - regular operation , namely copying .Chapter three , ' 'The Relevance of Computational Issues for Morphological Theory ' ' , establishes that from a computational perspective , the ' ' Item - and - Arrangement ' ' and ' ' Item - and - Process ' ' approaches that have often divided the field of theoretical morphology are of little difference .","label":"Background","metadata":{},"score":"56.093002"}
{"text":"The authors argue that the only morphological operation that can not be handled elegantly by composition is reduplication , which seems to require a non - regular operation , namely copying .Chapter three , ' 'The Relevance of Computational Issues for Morphological Theory ' ' , establishes that from a computational perspective , the ' ' Item - and - Arrangement ' ' and ' ' Item - and - Process ' ' approaches that have often divided the field of theoretical morphology are of little difference .","label":"Background","metadata":{},"score":"56.093002"}
{"text":"Please be as concrete as possible - and write clearly , since your classmates will be reading your words of wisdom !Optional reading explaining the advanced clustering method that Lee uses : K. Rose ( 1998 ) . \"Deterministic Annealing for Clustering , Compression , Classification , Regression , and Related Optimization Problems , \" Proceedings of the IEEE , vol .","label":"Background","metadata":{},"score":"56.105114"}
{"text":"Study of new questions raised by fundamental results in finite state phonology and morphology .In linguistics , morphological grammars describe the structure of word forms in a language , and phonological grammars describe the systematic use of sounds as units that encode the word forms .","label":"Background","metadata":{},"score":"56.400455"}
{"text":"Our best results on most of the six languages involve both improved objectives and improved search .The methods presented in this thesis were originally presented in Smith and Eisner ( 2004 , 2005a , b , 2006 ) .The thesis gives a more thorough exposition , relating the methods to other work , presents more experimental results and error analysis , and directly compares the methods to each other .","label":"Background","metadata":{},"score":"56.496296"}
{"text":"Our best results on most of the six languages involve both improved objectives and improved search .The methods presented in this thesis were originally presented in Smith and Eisner ( 2004 , 2005a , b , 2006 ) .The thesis gives a more thorough exposition , relating the methods to other work , presents more experimental results and error analysis , and directly compares the methods to each other .","label":"Background","metadata":{},"score":"56.496296"}
{"text":"To read about the excellent performance of Brill 's method , see his 1995 Computational Linguistics article or his 1992 thesis .For discussion of why it works well and a comparison to decision trees , see Ramshaw & Marcus ( 1996 ) .","label":"Background","metadata":{},"score":"56.54996"}
{"text":"With a few exceptions , extensions to latent Dirichlet allocation ( LDA ) have focused on the distribution over topics for each document .Much less attention has been given to the underlying structure of the topics themselves .As a result , most topic models generate topics independently from a single underlying distribution and require millions of parameters , in the form of multinomial distributions over the vocabulary .","label":"Background","metadata":{},"score":"56.621273"}
{"text":"With a few exceptions , extensions to latent Dirichlet allocation ( LDA ) have focused on the distribution over topics for each document .Much less attention has been given to the underlying structure of the topics themselves .As a result , most topic models generate topics independently from a single underlying distribution and require millions of parameters , in the form of multinomial distributions over the vocabulary .","label":"Background","metadata":{},"score":"56.621273"}
{"text":"In a given language , some transformations apply frequently and others rarely .We describe how to estimate the rates of the transformations from a sample of lexical entries .More deeply , we learn which properties of a transformation increase or decrease its rate in the language .","label":"Background","metadata":{},"score":"56.69772"}
{"text":"In a given language , some transformations apply frequently and others rarely .We describe how to estimate the rates of the transformations from a sample of lexical entries .More deeply , we learn which properties of a transformation increase or decrease its rate in the language .","label":"Background","metadata":{},"score":"56.69772"}
{"text":"Constraint - based reasoning , integer linear programming , column generation algorithms .Alignment Algorithms for Machine Translation .Sentence alignment , word alignment , phrase alignment , tree alignment .Alignment algorithms : Expectation Maximization algorithm , IBM models , HMM alignment , discriminative models .","label":"Background","metadata":{},"score":"56.700768"}
{"text":"Chapter one , ' ' Introduction and Preliminaries ' ' , delimits the scope of the book and reviews the basics of finite - state automata and transducers .The bulk of the chapter comprises an overview of ( weighted ) finite - state automata and transducers and a synopsis of fundamental algorithmic issues that relate to composition , minimization , determinization , and epsilon removal .","label":"Background","metadata":{},"score":"56.73725"}
{"text":"Other cool papers , not listed below , show how dynamic programming algorithms can be embedded as efficient subroutines within variational inference ( belief propagation ) , relaxation ( dual decomposition , row generation ) , and large - neighborhood local search .","label":"Background","metadata":{},"score":"56.755455"}
{"text":"The observed pronunciations or spellings of words are often explained as arising from the \" underlying forms \" of their morphemes .These forms are latent strings that linguists try to reconstruct by hand .We propose to reconstruct them automatically at scale , enabling generalization to new words .","label":"Background","metadata":{},"score":"56.787666"}
{"text":"The observed pronunciations or spellings of words are often explained as arising from the \" underlying forms \" of their morphemes .These forms are latent strings that linguists try to reconstruct by hand .We propose to reconstruct them automatically at scale , enabling generalization to new words .","label":"Background","metadata":{},"score":"56.787666"}
{"text":"Multiple pre - filled dictionaries may be utilized by the compression software at the beginning of the compression process , where the most appropriate dictionary for maximum compression is identified and used to compress the current data .These modifications are made to any of the known Lempel - Ziv compression techniques based on the variants detailed in 1977 and 1978 articles by Ziv and Lempel .","label":"Background","metadata":{},"score":"56.865654"}
{"text":"Multiple pre - filled dictionaries may be utilized by the compression software at the beginning of the compression process , where the most appropriate dictionary for maximum compression is identified and used to compress the current data .These modifications are made to any of the known Lempel - Ziv compression techniques based on the variants detailed in 1977 and 1978 articles by Ziv and Lempel .","label":"Background","metadata":{},"score":"56.865654"}
{"text":"Our method would eventually find the global maximum ( up to a user - specified  ) if run for long enough , but at any point can return a suboptimal solution together with an upper bound on the global maximum .","label":"Background","metadata":{},"score":"56.89093"}
{"text":"Our method would eventually find the global maximum ( up to a user - specified  ) if run for long enough , but at any point can return a suboptimal solution together with an upper bound on the global maximum .","label":"Background","metadata":{},"score":"56.89093"}
{"text":"The root nonterminal of each phrase is predicted from the context of that phrase .Formally this is a kind of autoencoder that predicts the sentence from itself , but structured as a semi - Markov CRF whose emission distribution is a PCFG , and which predicts each phrase only from context .","label":"Background","metadata":{},"score":"57.12391"}
{"text":"The root nonterminal of each phrase is predicted from the context of that phrase .Formally this is a kind of autoencoder that predicts the sentence from itself , but structured as a semi - Markov CRF whose emission distribution is a PCFG , and which predicts each phrase only from context .","label":"Background","metadata":{},"score":"57.12391"}
{"text":"We demonstrate the algorithm on the Steiner consensus string problem , both on synthetic data and on consensus decoding for speech recognition .This involves implicitly intersecting up to 100 automata .Unsupervised learning techniques can take advantage of large amounts of unannotated text , but the largest text corpus ( the Web ) is not easy to use in its full form .","label":"Background","metadata":{},"score":"57.14653"}
{"text":"We demonstrate the algorithm on the Steiner consensus string problem , both on synthetic data and on consensus decoding for speech recognition .This involves implicitly intersecting up to 100 automata .Unsupervised learning techniques can take advantage of large amounts of unannotated text , but the largest text corpus ( the Web ) is not easy to use in its full form .","label":"Background","metadata":{},"score":"57.14653"}
{"text":"2210 - 2239 , November .Lexicon learning .Many additional details are given in de Marcken 's thesis ( 1996 ) , if you 're curious or just need clarification .( Compare agglomerative vs. divisive clustering . )Venkataraman ( 1999 ) is a variation on Brent ( 1999 ) that models the corpus as generated by a traditional smoothed bigram or trigram model rather than Brent 's unigram - ish model .","label":"Background","metadata":{},"score":"57.268368"}
{"text":"Makoto Kanazawa ( 1996 ) .Identification in the Limit of Categorial Grammars .Journal of Logic , Language and Information 5(2 ) , 115 - 155 . scanned PDF version .This turned into Kanazawa 's thesis book ( 1998 ) , Learnable Classes of Categorial Grammars .","label":"Background","metadata":{},"score":"57.331993"}
{"text":"Our system , however , automatically learns from training data what constitutes an easy decision .Thus , we can utilize more features , learn more precise weights , and adapt to any dataset for which training data is available .Experiments show that our system outperforms recent state - of - the - art coreference systems including Raghunathan et al . 's system as well as a competitive baseline that uses a pairwise classifier .","label":"Background","metadata":{},"score":"57.354763"}
{"text":"Our system , however , automatically learns from training data what constitutes an easy decision .Thus , we can utilize more features , learn more precise weights , and adapt to any dataset for which training data is available .Experiments show that our system outperforms recent state - of - the - art coreference systems including Raghunathan et al . 's system as well as a competitive baseline that uses a pairwise classifier .","label":"Background","metadata":{},"score":"57.354763"}
{"text":"Specifically , we define weighted versions of the folding and unfolding transformations , whose unweighted versions are used in the logic programming and deductive database communities .We then present a novel transformation called speculation - a powerful generalization of folding that is motivated by gap - passing in categorial grammar .","label":"Background","metadata":{},"score":"57.36232"}
{"text":"Specifically , we define weighted versions of the folding and unfolding transformations , whose unweighted versions are used in the logic programming and deductive database communities .We then present a novel transformation called speculation - a powerful generalization of folding that is motivated by gap - passing in categorial grammar .","label":"Background","metadata":{},"score":"57.36232"}
{"text":"A domain expert constructs the parameterized graph , and a vertex is likely according to whether random walks tend to halt at it .Transformation models are suited to any domain where \" related \" events ( as defined by the graph ) may have positively covarying probabilities .","label":"Background","metadata":{},"score":"57.513947"}
{"text":"A domain expert constructs the parameterized graph , and a vertex is likely according to whether random walks tend to halt at it .Transformation models are suited to any domain where \" related \" events ( as defined by the graph ) may have positively covarying probabilities .","label":"Background","metadata":{},"score":"57.513947"}
{"text":"See also the \" continuous stack \" idea of Mozer & Das ( 1993 ) .The following two 6-page papers overlap considerably , so read one and flip through the other to find the differences .John M. Zelle and Raymond J. Mooney ( 1996 ) .","label":"Background","metadata":{},"score":"57.5552"}
{"text":"Nearly half the sentences are parsed with no misattachments ; two - thirds are parsed with at most one misattachment .Differences are statistically significant .If tags are roughly known in advance , search error is all but eliminated and the new model attains an attachment accuracy of 93 % .","label":"Background","metadata":{},"score":"57.583183"}
{"text":"Nearly half the sentences are parsed with no misattachments ; two - thirds are parsed with at most one misattachment .Differences are statistically significant .If tags are roughly known in advance , search error is all but eliminated and the new model attains an attachment accuracy of 93 % .","label":"Background","metadata":{},"score":"57.583183"}
{"text":"Previous techniques have relied on being able to accurately guess words and phrases that appear in one of the plaintext messages , making it far easier to claim that \" an attacker would never be able to do that . \" In this paper , we show how an adversary can automatically recover messages encrypted under the same keystream if only the type of each message is known ( e.g. an HTML page in English ) .","label":"Background","metadata":{},"score":"57.58513"}
{"text":"Previous techniques have relied on being able to accurately guess words and phrases that appear in one of the plaintext messages , making it far easier to claim that \" an attacker would never be able to do that . \" In this paper , we show how an adversary can automatically recover messages encrypted under the same keystream if only the type of each message is known ( e.g. an HTML page in English ) .","label":"Background","metadata":{},"score":"57.58513"}
{"text":"That is , we score a aligned pair of source and target trees based on local features of the trees and the alignment .Our quasi - synchronous model assigns positive probability to any alignment of any trees , in contrast to a synchronous grammar , which would insist on some form of structural parallelism .","label":"Background","metadata":{},"score":"57.7529"}
{"text":"That is , we score a aligned pair of source and target trees based on local features of the trees and the alignment .Our quasi - synchronous model assigns positive probability to any alignment of any trees , in contrast to a synchronous grammar , which would insist on some form of structural parallelism .","label":"Background","metadata":{},"score":"57.7529"}
{"text":"The model parameters can be locally optimized by gradient - based methods or by Expectation - Maximization .Exact algorithms ( matrix inversion ) and approximate ones ( relaxation ) are provided , with optimizations .Variations on the idea are also discussed .","label":"Background","metadata":{},"score":"58.061012"}
{"text":"The model parameters can be locally optimized by gradient - based methods or by Expectation - Maximization .Exact algorithms ( matrix inversion ) and approximate ones ( relaxation ) are provided , with optimizations .Variations on the idea are also discussed .","label":"Background","metadata":{},"score":"58.061012"}
{"text":"semiring parsing and other generalizations of classical parsing algorithms . applications and case studies of methods based on the theory of tree automata .In 1963 , Chomsky and Schtzenberger gave a morphic representation for the context - free languages i.e. the yields of local tree automata .","label":"Background","metadata":{},"score":"58.101906"}
{"text":"Multi - Syllable Phonotactic Modelling Anja Belz An approach to describing word - level phonotactics in terms of syllable classes .Such \" multi - syllable \" phonotactic models can be expressed in a formalism that facilitates automatic model construction and generalisation .","label":"Background","metadata":{},"score":"58.140694"}
{"text":"We propose a theory of phonotactic grammars and a learning algorithm that constructs such grammars from positive evidence .Our grammars consist of constraints that are assigned numerical weights according to the principle of maximum entropy .Possible words are assessed by these grammars based on the weighted sum of their constraint violations .","label":"Background","metadata":{},"score":"58.18588"}
{"text":"We present efficient new minimization algorithms that apply much more generally , while being simpler and about as fast .We also point out theoretical limits on minimization algorithms .We characterize the kind of \" well - behaved \" weight semirings where our methods work .","label":"Background","metadata":{},"score":"58.265427"}
{"text":"We present efficient new minimization algorithms that apply much more generally , while being simpler and about as fast .We also point out theoretical limits on minimization algorithms .We characterize the kind of \" well - behaved \" weight semirings where our methods work .","label":"Background","metadata":{},"score":"58.265427"}
{"text":"Next , it explicates Koskenniemi 's formalism for two - level rewrite rules that specify the transducers he built manually , and compares them with traditional generative phonological rules .The authors consider Koskenniemi 's system as a historical accident , as it was motivated by the difficulty in compiling complex rule system with the insufficient speed and memory of the computers in the late 1970s .","label":"Background","metadata":{},"score":"58.312767"}
{"text":"Next , it explicates Koskenniemi 's formalism for two - level rewrite rules that specify the transducers he built manually , and compares them with traditional generative phonological rules .The authors consider Koskenniemi 's system as a historical accident , as it was motivated by the difficulty in compiling complex rule system with the insufficient speed and memory of the computers in the late 1970s .","label":"Background","metadata":{},"score":"58.312767"}
{"text":"We review two novel methods for text categorization , based on a new framework that utilizes richer annotations that we call annotator rationales .A human annotator provides hints to a machine learner by highlighting contextual \" rationales \" in support of each of his or her annotations .","label":"Background","metadata":{},"score":"58.33429"}
{"text":"We review two novel methods for text categorization , based on a new framework that utilizes richer annotations that we call annotator rationales .A human annotator provides hints to a machine learner by highlighting contextual \" rationales \" in support of each of his or her annotations .","label":"Background","metadata":{},"score":"58.33429"}
{"text":"Finite - state machines are a pleasure to work with : they are efficient , flexible , scalable , and easy to design .The finite - state machines used for language are generalizations of the usual FSAs and HMMs .They can produce output as well as reading input , and they can do so nondeterministically .","label":"Background","metadata":{},"score":"58.338684"}
{"text":"The resulting parses in English appear reasonably linguistically plausible , though differing in style from CoNLL - style parses of the same sentences ; we discuss the differences .Entity clustering must determine when two named - entity mentions refer to the same entity .","label":"Background","metadata":{},"score":"58.34275"}
{"text":"The resulting parses in English appear reasonably linguistically plausible , though differing in style from CoNLL - style parses of the same sentences ; we discuss the differences .Entity clustering must determine when two named - entity mentions refer to the same entity .","label":"Background","metadata":{},"score":"58.34275"}
{"text":"We link to current applications in real world settings .Content .What is Natural Language Processing ( NLP ) ?Ambiguity and uncertainty in language .Introduction to the different analysis levels of NLP .Applications of NLP .Evaluation in NLP : Precision , recall , F - score , x - fold cross - validation , gold standards , good practices in NLP experiments , BLEU .","label":"Background","metadata":{},"score":"58.39612"}
{"text":"Free Language .Constraints , however , remain finite - state .Efficient candidate filtering is possible via an extended Earley 's algorithm .Invited talk : Finite - State Non - Concatenative Morphotactics Kenneth R. Beesley and Lauri Karttunen A new finite - state technique , \" compile - replace \" , lets a regexp compiler reapply and modify its own output , freeing morphotactic description to use any finite - state operation .","label":"Background","metadata":{},"score":"58.421066"}
{"text":"\" Most finite - state constructions can be easily adapted to weighted machines , but minimization is a bit harder .Since Feb. 2001 I have served as President of the ACL 's Special Interest Group for Computational Phonology ( SIGPHON ) .","label":"Background","metadata":{},"score":"58.44626"}
{"text":"My first real parsing research , summers 1989 - 1992 , was a collaboration with Mark A. Jones at AT&T Bell Labs .This was early in the days of probabilistic NLP .The two of us eventually built a probabilistic left - to - right LFG parser , with a hand - built domain grammar and history - based probabilities trained from data .","label":"Background","metadata":{},"score":"58.46981"}
{"text":"We are interested in speeding up approximate inference in Markov Random Fields ( MRFs ) .We present a new method that uses gates - binary random variables that determine which factors of the MRF to use .Which gates are open depends on the observed evidence ; when many gates are closed , the MRF takes on a sparser and faster structure that omits \" unnecessary \" factors .","label":"Background","metadata":{},"score":"58.611042"}
{"text":"We are interested in speeding up approximate inference in Markov Random Fields ( MRFs ) .We present a new method that uses gates - binary random variables that determine which factors of the MRF to use .Which gates are open depends on the observed evidence ; when many gates are closed , the MRF takes on a sparser and faster structure that omits \" unnecessary \" factors .","label":"Background","metadata":{},"score":"58.611042"}
{"text":"Remark : The probabilities under lexicalized models can capture some crude semantic preferences along with syntax ( i.e. , selectional preferences ) .In fact , in our very early work , we actually conditioned probabilities on words according to their role in a semantic representation .","label":"Background","metadata":{},"score":"58.61721"}
{"text":"Remark : The probabilities under lexicalized models can capture some crude semantic preferences along with syntax ( i.e. , selectional preferences ) .In fact , in our very early work , we actually conditioned probabilities on words according to their role in a semantic representation .","label":"Background","metadata":{},"score":"58.61721"}
{"text":"For deterministic parsing algorithms , the authors cover shift - reduce parsing , pushdown automata , and top - down and left - corner parsing ; for non - deterministic parsing algorithms , they cover re - analysis and beam - search , CYK parsing , Earley parsing , inside - outside algorithms , and labeled recall parsing .","label":"Background","metadata":{},"score":"58.658363"}
{"text":"For deterministic parsing algorithms , the authors cover shift - reduce parsing , pushdown automata , and top - down and left - corner parsing ; for non - deterministic parsing algorithms , they cover re - analysis and beam - search , CYK parsing , Earley parsing , inside - outside algorithms , and labeled recall parsing .","label":"Background","metadata":{},"score":"58.658363"}
{"text":"We discuss how these costs might apply to MT , and some possibilities for training them .We show how to search and sample from exponentially large neighborhoods using efficient dynamic programming algorithms that resemble statistical parsing .We also incorporate techniques from statistical parsing to improve the runtime of our search .","label":"Background","metadata":{},"score":"58.73403"}
{"text":"We discuss how these costs might apply to MT , and some possibilities for training them .We show how to search and sample from exponentially large neighborhoods using efficient dynamic programming algorithms that resemble statistical parsing .We also incorporate techniques from statistical parsing to improve the runtime of our search .","label":"Background","metadata":{},"score":"58.73403"}
{"text":"Further suppose that all these PFSAs are similar because they are drawn from a single ( but unknown ) prior distribution over PFSAs .We wish to infer the prior , obtain smoothed estimates of the individual PFSAs , and reconstruct the hidden paths by which the unknown PFSAs generated their documents .","label":"Background","metadata":{},"score":"59.088654"}
{"text":"Further suppose that all these PFSAs are similar because they are drawn from a single ( but unknown ) prior distribution over PFSAs .We wish to infer the prior , obtain smoothed estimates of the individual PFSAs , and reconstruct the hidden paths by which the unknown PFSAs generated their documents .","label":"Background","metadata":{},"score":"59.088654"}
{"text":"By default , these classes run a generalization of agenda - based parsing , prioritizing the partial parses by some figure of merit .The classes can also perform an exact backward ( outside ) pass in the service of parameter training .","label":"Background","metadata":{},"score":"59.182518"}
{"text":"By default , these classes run a generalization of agenda - based parsing , prioritizing the partial parses by some figure of merit .The classes can also perform an exact backward ( outside ) pass in the service of parameter training .","label":"Background","metadata":{},"score":"59.182518"}
{"text":"We describe some basic operations on n -ary rational relations and propose notation for them .( For generality we give the semiring - weighted case in which each tuple has a weight . )Unfortunately , the join operation is problematic : if two rational relations are joined on more than one tape , it can lead to non - rational relations with undecidable properties .","label":"Background","metadata":{},"score":"59.264935"}
{"text":"We describe some basic operations on n -ary rational relations and propose notation for them .( For generality we give the semiring - weighted case in which each tuple has a weight . )Unfortunately , the join operation is problematic : if two rational relations are joined on more than one tape , it can lead to non - rational relations with undecidable properties .","label":"Background","metadata":{},"score":"59.264935"}
{"text":"Note : Expectation semirings are included in the excellent OpenFST library .I believe that Zhifei Li , Ariya Rastrow , Markus Dreyer , and Roy Tromble have all written implementations that work with OpenFST or other packages .Some of these support the higher - order expectation semirings of Li & Eisner ( 2009 ) .","label":"Background","metadata":{},"score":"59.324287"}
{"text":"Note : Expectation semirings are included in the excellent OpenFST library .I believe that Zhifei Li , Ariya Rastrow , Markus Dreyer , and Roy Tromble have all written implementations that work with OpenFST or other packages .Some of these support the higher - order expectation semirings of Li & Eisner ( 2009 ) .","label":"Background","metadata":{},"score":"59.324287"}
{"text":"Chapter seven , ' ' Basic Context - free Approaches to Syntax ' ' , focuses on approaches to syntax using context - free grammars ( CFGs ) .The chapter opens with an introduction to the definitions and properties of CFGs , derivations , and trees .","label":"Background","metadata":{},"score":"59.33644"}
{"text":"Chapter seven , ' ' Basic Context - free Approaches to Syntax ' ' , focuses on approaches to syntax using context - free grammars ( CFGs ) .The chapter opens with an introduction to the definitions and properties of CFGs , derivations , and trees .","label":"Background","metadata":{},"score":"59.33644"}
{"text":"Our technique involves loopy belief propagation in a natural directed graphical model whose variables are unknown strings and whose conditional distributions are encoded as finite - state machines with trainable weights .We define training and evaluation paradigms for the task of surface word prediction , and report results on subsets of 7 languages .","label":"Background","metadata":{},"score":"59.34046"}
{"text":"Our technique involves loopy belief propagation in a natural directed graphical model whose variables are unknown strings and whose conditional distributions are encoded as finite - state machines with trainable weights .We define training and evaluation paradigms for the task of surface word prediction , and report results on subsets of 7 languages .","label":"Background","metadata":{},"score":"59.34046"}
{"text":"Locally maximizing likelihood ( the inside - outside algorithm ) does quite poorly on this task for a variety of reasons .We have tried to get some insight into the problems and address them with a variety of search methods and modified objective functions .","label":"Background","metadata":{},"score":"59.48564"}
{"text":"Locally maximizing likelihood ( the inside - outside algorithm ) does quite poorly on this task for a variety of reasons .We have tried to get some insight into the problems and address them with a variety of search methods and modified objective functions .","label":"Background","metadata":{},"score":"59.48564"}
{"text":"Under multi - headed dependency grammar , a parse is a connected DAG rather than a tree .Such formalisms can be more syntactically and semantically expressive .However , it is hard to train , test , or improve multi - headed parsers because few multi - headed corpora exist , particularly for the projective case .","label":"Background","metadata":{},"score":"59.49886"}
{"text":"Under multi - headed dependency grammar , a parse is a connected DAG rather than a tree .Such formalisms can be more syntactically and semantically expressive .However , it is hard to train , test , or improve multi - headed parsers because few multi - headed corpora exist , particularly for the projective case .","label":"Background","metadata":{},"score":"59.49886"}
{"text":"The signature result of the thesis is the application of a machine learned set of linear ordering problems to machine translation .Using reorderings found by search as a preprocessing step significantly improves translation of German to English , and significantly more than the lexicalized reordering model that is the default of the translation system .","label":"Background","metadata":{},"score":"59.62037"}
{"text":"The signature result of the thesis is the application of a machine learned set of linear ordering problems to machine translation .Using reorderings found by search as a preprocessing step significantly improves translation of German to English , and significantly more than the lexicalized reordering model that is the default of the translation system .","label":"Background","metadata":{},"score":"59.62037"}
{"text":"A weighted logic program defines a generalized arithmetic circuit - the weighted version of a proof forest , with nodes having arbitrary rather than boolean values .In this paper , we focus on finite circuits .We present a flexible algorithm for efficiently querying node values as they change under updates to the circuit 's inputs .","label":"Background","metadata":{},"score":"59.796135"}
{"text":"A weighted logic program defines a generalized arithmetic circuit - the weighted version of a proof forest , with nodes having arbitrary rather than boolean values .In this paper , we focus on finite circuits .We present a flexible algorithm for efficiently querying node values as they change under updates to the circuit 's inputs .","label":"Background","metadata":{},"score":"59.796135"}
{"text":"Ranking in OT : Algorithms and Complexity Jason Eisner A simple version of the automatic constraint ranking problem is easier than previously known ( linear on the number of constraints ) .But slightly more realistic versions are as bad as Sigma_2-complete .","label":"Background","metadata":{},"score":"59.876797"}
{"text":"These semirings can be used to compute a large number of expectations ( e.g. , entropy and its gradient ) over the exponentially many trees presented in a hypergraph .The weights used in a hypergraph are usually learnt by a discriminative training method .","label":"Background","metadata":{},"score":"59.98575"}
{"text":"These semirings can be used to compute a large number of expectations ( e.g. , entropy and its gradient ) over the exponentially many trees presented in a hypergraph .The weights used in a hypergraph are usually learnt by a discriminative training method .","label":"Background","metadata":{},"score":"59.98575"}
{"text":"An adaptive compression technique which is an improvement to Lempel - Ziv ( LZ ) compression techniques , both as applied for purposes of reducing required storage space and for reducing the transmission time associated with transferring data from point to point .","label":"Background","metadata":{},"score":"60.15346"}
{"text":"An adaptive compression technique which is an improvement to Lempel - Ziv ( LZ ) compression techniques , both as applied for purposes of reducing required storage space and for reducing the transmission time associated with transferring data from point to point .","label":"Background","metadata":{},"score":"60.15346"}
{"text":"This technical report is an appendix to Eisner ( 1996 ) : it gives superior experimental results that were reported only in the talk version of that paper .The present paper describes some details of the experiments and repeats them with a larger training set of 25,000 sentences .","label":"Background","metadata":{},"score":"60.16533"}
{"text":"This technical report is an appendix to Eisner ( 1996 ) : it gives superior experimental results that were reported only in the talk version of that paper .The present paper describes some details of the experiments and repeats them with a larger training set of 25,000 sentences .","label":"Background","metadata":{},"score":"60.16533"}
{"text":"Ph.D. thesis , University of Helsinki , Helsinki .Schone , P. , and Jurafsky , D. , ( 2000 ) .Knowledge - free induction of morphology using latent semantic analysis .In _ Proceedings of the 4th Conference on Computational Natural Language Learning _ , pp .","label":"Background","metadata":{},"score":"60.26985"}
{"text":"Ph.D. thesis , University of Helsinki , Helsinki .Schone , P. , and Jurafsky , D. , ( 2000 ) .Knowledge - free induction of morphology using latent semantic analysis .In _ Proceedings of the 4th Conference on Computational Natural Language Learning _ , pp .","label":"Background","metadata":{},"score":"60.26985"}
{"text":"Course Description .Catalog description : This course focuses on past and present research that has attempted , with mixed success , to induce the structure of language from raw data such as text .Lectures will be intermixed with reading and discussion of the primary literature .","label":"Background","metadata":{},"score":"60.37767"}
{"text":"This is the first inference method for arbitrary graphical models over strings that does not require approximations such as random sampling , message simplification , or a bound on string length .Provided that the inference method terminates , it gives a certificate of global optimality ( though MAP inference in our setting is undecidable in general ) .","label":"Background","metadata":{},"score":"60.482666"}
{"text":"This is the first inference method for arbitrary graphical models over strings that does not require approximations such as random sampling , message simplification , or a bound on string length .Provided that the inference method terminates , it gives a certificate of global optimality ( though MAP inference in our setting is undecidable in general ) .","label":"Background","metadata":{},"score":"60.482666"}
{"text":"The approach is motivated by compositional models and Bayesian networks , and designed to adapt to small sample sizes .We start with a large , overlapping set of elementary statistical building blocks , or \" primitives , \" which are low - dimensional marginal distributions learned from data .","label":"Background","metadata":{},"score":"60.488373"}
{"text":"The approach is motivated by compositional models and Bayesian networks , and designed to adapt to small sample sizes .We start with a large , overlapping set of elementary statistical building blocks , or \" primitives , \" which are low - dimensional marginal distributions learned from data .","label":"Background","metadata":{},"score":"60.488373"}
{"text":"Context free grammars : constituency , Chomsky normal form , chart parsing ( Earley algorithm ) , top - down and bottom - up parsing , Cocke - Younger - Kasami ( CYK ) algorithm , efficiency .Limitations of context free grammars , implementing feature constraints .","label":"Background","metadata":{},"score":"60.500275"}
{"text":"OT has eclipsed previous approaches to phonology .It says that when you 're trying to pronounce a word ( \" resign \" or \" resignation \" ) , you consider all possible pronunciations and pick the best one .A grammar in OT simply specifies the criteria on which pronunciations are judged , and especially the relative importance of those criteria , which are known as violable constraints .","label":"Background","metadata":{},"score":"60.507843"}
{"text":"On the more difficult problem of cross - lingual parser projection , we learn a dependency parser for a target language by using bilingual text , an English parser , and automatic word alignments .Our experiments show that unsupervised QG projection improves on parses trained using only high - precision projected annotations and far outperforms , by more than 35 % absolute dependency accuracy , learning an unsupervised parser from raw target - language text alone .","label":"Background","metadata":{},"score":"60.550888"}
{"text":"On the more difficult problem of cross - lingual parser projection , we learn a dependency parser for a target language by using bilingual text , an English parser , and automatic word alignments .Our experiments show that unsupervised QG projection improves on parses trained using only high - precision projected annotations and far outperforms , by more than 35 % absolute dependency accuracy , learning an unsupervised parser from raw target - language text alone .","label":"Background","metadata":{},"score":"60.550888"}
{"text":"( 4 ) As for ranking , determining whether any consistent ranking exists is coNP - hard ( but in  2 p ) if the forms are fully observed , and  2 p -complete if not .Finally , we show ( 5 ) generation and ranking are easier in derivational theories : in P , and NP - complete .","label":"Background","metadata":{},"score":"60.553085"}
{"text":"( 4 ) As for ranking , determining whether any consistent ranking exists is coNP - hard ( but in  2 p ) if the forms are fully observed , and  2 p -complete if not .Finally , we show ( 5 ) generation and ranking are easier in derivational theories : in P , and NP - complete .","label":"Background","metadata":{},"score":"60.553085"}
{"text":"These objects are usually equipped with real - valued weights that define a structured prediction problem ( see here ) .One can treat a wider variety of problems by allowing the weights to be elements of an arbitrary semiring .Our work on weighted logic programming ( see papers on the Dyna language ) has led us to develop flexible algorithms for maintaining truth values in arithmetic circuits .","label":"Background","metadata":{},"score":"60.650642"}
{"text":"These objects are usually equipped with real - valued weights that define a structured prediction problem ( see here ) .One can treat a wider variety of problems by allowing the weights to be elements of an arbitrary semiring .Our work on weighted logic programming ( see papers on the Dyna language ) has led us to develop flexible algorithms for maintaining truth values in arithmetic circuits .","label":"Background","metadata":{},"score":"60.650642"}
{"text":"We have plenty of ideas , but it is increasingly laborious to try them out , as our models become more ambitious and our datasets become larger , noisier , and more heterogeneous .The software engineering burden makes it hard to start new work ; hard to reuse and combine existing ideas ; and hard to educate our students .","label":"Background","metadata":{},"score":"61.087616"}
{"text":"We have plenty of ideas , but it is increasingly laborious to try them out , as our models become more ambitious and our datasets become larger , noisier , and more heterogeneous .The software engineering burden makes it hard to start new work ; hard to reuse and combine existing ideas ; and hard to educate our students .","label":"Background","metadata":{},"score":"61.087616"}
{"text":"Named Entity Recognition and Semantic Role Labeling .Discriminative supervised learning models : loglinear models , maximum entropy Markov models , conditional random fields , BIO - tagging and chunking .PropBank , FrameNet .Discourse Analysis .Noun phrase coreference resolution .","label":"Background","metadata":{},"score":"61.12633"}
{"text":"The tree construction involves projecting the data onto low - dimensional spaces , as a means of smoothing their empirical distributions , as well as splitting each node based on an information - theoretic maximization objective .The single parameter alpha of the Jensen - Renyi divergence is chosen based on the \" strapping \" methodology [ 2 ] , which learns a meta - classifer on a related task .","label":"Background","metadata":{},"score":"61.168213"}
{"text":"The tree construction involves projecting the data onto low - dimensional spaces , as a means of smoothing their empirical distributions , as well as splitting each node based on an information - theoretic maximization objective .The single parameter alpha of the Jensen - Renyi divergence is chosen based on the \" strapping \" methodology [ 2 ] , which learns a meta - classifer on a related task .","label":"Background","metadata":{},"score":"61.168213"}
{"text":"Most of this work tries to use statistical prediction to help users cope with information overload .The lawyers uglified the writing , though .What are all these discrete algorithms doing ? \"Structured prediction \" is the problem of modeling unknown variables that are themselves complex structures , such as vectors , strings , or trees .","label":"Background","metadata":{},"score":"61.187"}
{"text":"The experimental work was reported in two papers : .In subsequent papers I developed the algorithmic point further .Others had been using O(n 5 ) algorithms for bilexical parsing models , whereas the above papers had found an O(n 3 ) algorithm : .","label":"Background","metadata":{},"score":"61.206055"}
{"text":"If an underlying form has outputs that can meet a given constraint , lenient composition enforces the constraint ; ff none of the output candidates meets the constraint , lenient composition allows all of them .For the sake of greater efficiency , we may \" eniently compose \" the a. relation and all the constraints into a single finite - state transducer that maps each underlying form directly into its op- timal surface realizations , and vice versa .","label":"Background","metadata":{},"score":"61.32552"}
{"text":"Spatial relation recognition .Course material .Handbooks Daniel Jurafsky and James H. Martin , Speech and Language Processing , Prentice - Hall , 2006 ( 2nd edition ) .Christopher D. Manning and Hinrich Schtze , Foundations of Statistical Natural Language Processing , MIT Press , 1999 .","label":"Background","metadata":{},"score":"61.33619"}
{"text":"We study graphical modeling in the case of string - valued random variables .Whereas a weighted finite - state transducer can model the probabilistic relationship between two strings , we are interested in building up joint models of three or more strings .","label":"Background","metadata":{},"score":"61.3444"}
{"text":"We study graphical modeling in the case of string - valued random variables .Whereas a weighted finite - state transducer can model the probabilistic relationship between two strings , we are interested in building up joint models of three or more strings .","label":"Background","metadata":{},"score":"61.3444"}
{"text":"It also asks how statistical modeling fits into the broader program of cognitive science .This paper gives the first EM algorithm for general probabilistic finite - state transducers ( with epsilon ) .Furthermore , the approach is powerful enough to fit machines ' parameters even after the machines are combined by operations of the finite - state calculus , such as composition and minimization .","label":"Background","metadata":{},"score":"61.396603"}
{"text":"It also asks how statistical modeling fits into the broader program of cognitive science .This paper gives the first EM algorithm for general probabilistic finite - state transducers ( with epsilon ) .Furthermore , the approach is powerful enough to fit machines ' parameters even after the machines are combined by operations of the finite - state calculus , such as composition and minimization .","label":"Background","metadata":{},"score":"61.396603"}
{"text":"This fact can be exploited by parsers .We first show that a crude way to use dependency length as a parsing feature can substantially improve parsing speed and accuracy in English and Chinese , with more mixed results on German .","label":"Background","metadata":{},"score":"61.56522"}
{"text":"This fact can be exploited by parsers .We first show that a crude way to use dependency length as a parsing feature can substantially improve parsing speed and accuracy in English and Chinese , with more mixed results on German .","label":"Background","metadata":{},"score":"61.56522"}
{"text":"One should design a sensible space of decision rules ( policies ) and explicitly search for one having high expected performance over D .In practice this can greatly improve accuracy .Do you have a realistic model of your domain ?","label":"Background","metadata":{},"score":"61.574688"}
{"text":"One should design a sensible space of decision rules ( policies ) and explicitly search for one having high expected performance over D .In practice this can greatly improve accuracy .Do you have a realistic model of your domain ?","label":"Background","metadata":{},"score":"61.574688"}
{"text":"But do you really need exact inference ?Often , you could confidently make a prediction without reasoning about all of the potentially relevant variables .Many variables have redundant or negligible influence on the final decision .\" Cost - aware learning \" refers to learning policies for cheap but accurate inference .","label":"Background","metadata":{},"score":"61.84498"}
{"text":"But do you really need exact inference ?Often , you could confidently make a prediction without reasoning about all of the potentially relevant variables .Many variables have redundant or negligible influence on the final decision .\" Cost - aware learning \" refers to learning policies for cheap but accurate inference .","label":"Background","metadata":{},"score":"61.84498"}
{"text":"We present the first version of a new declarative programming language .Dyna has many uses but was designed especially for rapid development of new statistical NLP systems .A Dyna program is a small set of equations , resembling Prolog inference rules , that specify the abstract structure of a dynamic programming algorithm .","label":"Background","metadata":{},"score":"61.894367"}
{"text":"We present the first version of a new declarative programming language .Dyna has many uses but was designed especially for rapid development of new statistical NLP systems .A Dyna program is a small set of equations , resembling Prolog inference rules , that specify the abstract structure of a dynamic programming algorithm .","label":"Background","metadata":{},"score":"61.894367"}
{"text":"Probabilistic language models , n - grams , generative models , unsupervised and semi - supervised models , smoothed estimation .Expectation Maximization ( EM ) techniques .Hidden Markov Models ( HMMs ) , Viterbi algorithm , forward - backward algorithm .","label":"Background","metadata":{},"score":"62.132217"}
{"text":"However , finding the best string ( e.g. , during decoding ) is then computationally intractable .Therefore , most systems use a simple Viterbi approximation that measures the goodness of a string using only its most probable derivation .Instead , we develop a variational approximation , which considers all the derivations but still allows tractable decoding .","label":"Background","metadata":{},"score":"62.280003"}
{"text":"However , finding the best string ( e.g. , during decoding ) is then computationally intractable .Therefore , most systems use a simple Viterbi approximation that measures the goodness of a string using only its most probable derivation .Instead , we develop a variational approximation , which considers all the derivations but still allows tractable decoding .","label":"Background","metadata":{},"score":"62.280003"}
{"text":"Many syntactic models in machine translation are channels that transform one tree into another , or synchronous grammars that generate trees in parallel .We present a new model of the translation process : quasi - synchronous grammar ( QG ) .","label":"Background","metadata":{},"score":"62.29419"}
{"text":"Many syntactic models in machine translation are channels that transform one tree into another , or synchronous grammars that generate trees in parallel .We present a new model of the translation process : quasi - synchronous grammar ( QG ) .","label":"Background","metadata":{},"score":"62.29419"}
{"text":"Primitive Optimality Theory is a simple formal model of a subset of Optimality Theory ( Prince and Smolensky 1993 ) .The work presented here implements this model and extends it .The implementation is used to evaluate the Primitive Optimality Theory model , and is in itself a usef ... \" .","label":"Background","metadata":{},"score":"62.38006"}
{"text":"In particular , combinatorial subroutines enforce the constraint that the parser 's output must form a tree .This is the first application that uses efficient computation of marginals for combinatorial problems to improve the speed and accuracy of belief propagation .","label":"Background","metadata":{},"score":"62.481266"}
{"text":"In particular , combinatorial subroutines enforce the constraint that the parser 's output must form a tree .This is the first application that uses efficient computation of marginals for combinatorial problems to improve the speed and accuracy of belief propagation .","label":"Background","metadata":{},"score":"62.481266"}
{"text":"We propose a hybrid reinforcement / apprenticeship learning algorithm that , even with few inexpensive features , can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state - of - the - art baselines .We present an instance - specific test - time dynamic feature selection algorithm .","label":"Background","metadata":{},"score":"62.801506"}
{"text":"We propose a hybrid reinforcement / apprenticeship learning algorithm that , even with few inexpensive features , can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state - of - the - art baselines .We present an instance - specific test - time dynamic feature selection algorithm .","label":"Background","metadata":{},"score":"62.801506"}
{"text":"Chapter five , ' ' Machine Learning of Morphology ' ' , reviews three influential studies on morphological induction .The authors begin with Goldsmith ( 2001 ) , which they consider the gold standard for later work on unsupervised acquisition of morphology .","label":"Background","metadata":{},"score":"62.829586"}
{"text":"Chapter five , ' ' Machine Learning of Morphology ' ' , reviews three influential studies on morphological induction .The authors begin with Goldsmith ( 2001 ) , which they consider the gold standard for later work on unsupervised acquisition of morphology .","label":"Background","metadata":{},"score":"62.829586"}
{"text":"In recent work , my students and I have been trying to build a unified perspective on the techniques that we use in this field .Our Dyna programming language lets you write down models and algorithms at a high level .","label":"Background","metadata":{},"score":"62.881363"}
{"text":"We present penalized expectation propagation , a novel algorithm for approximate inference in graphical models .Expectation propagation is a well - known message - passing algorithm that prevents messages from growing in complexity by forcing them back into a given family of distributions .","label":"Background","metadata":{},"score":"62.93388"}
{"text":"We present penalized expectation propagation , a novel algorithm for approximate inference in graphical models .Expectation propagation is a well - known message - passing algorithm that prevents messages from growing in complexity by forcing them back into a given family of distributions .","label":"Background","metadata":{},"score":"62.93388"}
{"text":"Topics of Interest .New or updated work on the traditional topics of FSMNLP workshops .The traditional topics in the series of FSMNLP workshops are appropriate .The submitted paper must clearly deal with finite states .An extended version of a meeting paper can be submitted , provided that the contribution has been updated as appropriate given the progress in the field .","label":"Background","metadata":{},"score":"62.972668"}
{"text":"Message scheduling is shown to be very effective in belief propagation ( BP ) algorithms .In this paper , we propose a reinforcement learning based message scheduling framework ( RLBP ) to learn the heuristics automatically which generalizes to any graph structures and distributions .","label":"Background","metadata":{},"score":"62.98548"}
{"text":"Message scheduling is shown to be very effective in belief propagation ( BP ) algorithms .In this paper , we propose a reinforcement learning based message scheduling framework ( RLBP ) to learn the heuristics automatically which generalizes to any graph structures and distributions .","label":"Background","metadata":{},"score":"62.98548"}
{"text":"Our experiments on the Canadian Hansards show that our unsupervised technique is significantly more effective than picking seeds by hand ( Yarowsky , 1995 ) , which in turn is known to rival supervised methods .Weighted deduction with aggregation is a powerful theoretical formalism that encompasses many NLP algorithms .","label":"Background","metadata":{},"score":"63.043705"}
{"text":"Our experiments on the Canadian Hansards show that our unsupervised technique is significantly more effective than picking seeds by hand ( Yarowsky , 1995 ) , which in turn is known to rival supervised methods .Weighted deduction with aggregation is a powerful theoretical formalism that encompasses many NLP algorithms .","label":"Background","metadata":{},"score":"63.043705"}
{"text":"In this paper , we propose a model for cross - document coreference resolution that achieves robustness by learning similarity from unlabeled data .The generative process assumes that each entity mention arises from copying and optionally mutating an earlier name from a similar context .","label":"Background","metadata":{},"score":"63.37281"}
{"text":"In this paper , we propose a model for cross - document coreference resolution that achieves robustness by learning similarity from unlabeled data .The generative process assumes that each entity mention arises from copying and optionally mutating an earlier name from a similar context .","label":"Background","metadata":{},"score":"63.37281"}
{"text":"Such an account implies that NPI any moves over negation at LF - which is confirmed by scope tests .It also explains some well - known problems concerning NPI any in non - downward - entailing environments and under sorry vs. glad .","label":"Background","metadata":{},"score":"63.374104"}
{"text":"Such an account implies that NPI any moves over negation at LF - which is confirmed by scope tests .It also explains some well - known problems concerning NPI any in non - downward - entailing environments and under sorry vs. glad .","label":"Background","metadata":{},"score":"63.374104"}
{"text":"We present a generative model of how a given annotator , knowing the true  , stochastically chooses rationales .Thus , observing the rationales helps us infer the true  .We collect substring rationales for a sentiment classification task ( Pang and Lee , 2004 ) and use them to obtain significant accuracy improvements for each annotator .","label":"Background","metadata":{},"score":"63.487656"}
{"text":"We present a generative model of how a given annotator , knowing the true  , stochastically chooses rationales .Thus , observing the rationales helps us infer the true  .We collect substring rationales for a sentiment classification task ( Pang and Lee , 2004 ) and use them to obtain significant accuracy improvements for each annotator .","label":"Background","metadata":{},"score":"63.487656"}
{"text":"Tagging English text with a probabilistic model .Computational Linguistics 20(2):155 - 172 .scanned PDF version .This gives a good account of Eric Brill 's supervised tagging method , and shows how to compile it into a finite - state transducer once it 's trained .","label":"Background","metadata":{},"score":"63.55638"}
{"text":"These predictions can be used to make profitable trading strategies .More specifically , company names can be recognized and simple templates describing company actions can be automatically filled using parsing or pattern matching on words in or near the sentence containing the company name .","label":"Background","metadata":{},"score":"63.575867"}
{"text":"These predictions can be used to make profitable trading strategies .More specifically , company names can be recognized and simple templates describing company actions can be automatically filled using parsing or pattern matching on words in or near the sentence containing the company name .","label":"Background","metadata":{},"score":"63.575867"}
{"text":"Next week we 'll probably look at Brill 's extension to the unsupervised case .Week of Feb. 25 : Unsupervised Finite - State Topology .Eric Brill ( 1995 ) .Unsupervised Learning of Disambiguation Rules for Part of Speech Tagging .","label":"Background","metadata":{},"score":"63.7026"}
{"text":"However , Eisner ( 2002 ) shows that the mathematical result can actually be obtained as a special case of a more general theorem that is based on a simpler algebraic construction .This paper introduces weighted bilexical grammars , a formalism in which individual lexical items , such as verbs and their arguments , can have idiosyncratic selectional influences on each other .","label":"Background","metadata":{},"score":"63.737495"}
{"text":"However , Eisner ( 2002 ) shows that the mathematical result can actually be obtained as a special case of a more general theorem that is based on a simpler algebraic construction .This paper introduces weighted bilexical grammars , a formalism in which individual lexical items , such as verbs and their arguments , can have idiosyncratic selectional influences on each other .","label":"Background","metadata":{},"score":"63.737495"}
{"text":"We describe deterministic annealing ( Rose et al . , 1990 ) as an appealing alternative to the Expectation - Maximization algorithm ( Dempster et al . , 1977 ) .Seeking to avoid search error , DA begins by globally maximizing an easy concave function and maintains a local maximum as it gradually morphs the function into the desired non - concave likelihood function .","label":"Background","metadata":{},"score":"63.768288"}
{"text":"We describe deterministic annealing ( Rose et al . , 1990 ) as an appealing alternative to the Expectation - Maximization algorithm ( Dempster et al . , 1977 ) .Seeking to avoid search error , DA begins by globally maximizing an easy concave function and maintains a local maximum as it gradually morphs the function into the desired non - concave likelihood function .","label":"Background","metadata":{},"score":"63.768288"}
{"text":"In particular , the ranking of optimality constraints corresponds to the ordering of rewrite rules . ... impler than the Yolmts vowel alternation we just discussed .3.1 GEN for syllabification We assume that the input to OEN consists of strings of vowels V and consonants C. GEN allows each segment to play a role in the syllable or to r .. therefore free ( in fact , obliged bysOccam 's Razor ) to remove gradient HNuc from CON.sReplacing gradient HNuc with categorical constraints is possible be - scause the sonority scale is finite .","label":"Background","metadata":{},"score":"63.845078"}
{"text":"Most work on metrical stress within Optimality Theory ( OT ) has adopted this typology without explaining the gaps .Moreover , the OT versions use uncomfortably non - local constraints ( Align , FootForm , FtBin ) .This paper presents a rather different and in some ways more explanatory typology of stress , couched in the framework of primitive Optimality Theory ( OTP ) , which allows only primitive , radically local constraints .","label":"Background","metadata":{},"score":"63.91723"}
{"text":"Most work on metrical stress within Optimality Theory ( OT ) has adopted this typology without explaining the gaps .Moreover , the OT versions use uncomfortably non - local constraints ( Align , FootForm , FtBin ) .This paper presents a rather different and in some ways more explanatory typology of stress , couched in the framework of primitive Optimality Theory ( OTP ) , which allows only primitive , radically local constraints .","label":"Background","metadata":{},"score":"63.91723"}
{"text":"We will show how to build joint probability models over many strings .These models are capable of predicting unobserved strings , or predicting the relationships among observed strings .However , computing the predictions of these models can be computationally hard .","label":"Background","metadata":{},"score":"63.95616"}
{"text":"+ a 10-minute talk -- that 's all it deserves until the team gets real empirical results ) .Grammar Learning .In my thesis work ( \" transformational smoothing \" ) , I took up the question of learning deep structure , showing how to learn the probabilities of transformations or lexical redundancy rules that could turn one lexicalized context - free rule ( or other lexical entry ) into another : .","label":"Background","metadata":{},"score":"64.02879"}
{"text":"This paper presents an analysis of the Basic CV Syllable Theory ( Prince & Smolensky 1993 ) showing that , despite the nature of the formal definition , computing the optimal form does not require explicitly generating and evaluating all possible candidates .","label":"Background","metadata":{},"score":"64.04863"}
{"text":"Dyna is a declarative programming language that combines logic programming with functional programming .It also supports modularity .It may be regarded as a kind of deductive database , theorem prover , truth maintenance system , or equation solver .I will illustrate how Dyna makes it easy to specify the combinatorial structure of typical computations needed in natural language processing , machine learning , and elsewhere in AI .","label":"Background","metadata":{},"score":"64.27554"}
{"text":"Dyna is a declarative programming language that combines logic programming with functional programming .It also supports modularity .It may be regarded as a kind of deductive database , theorem prover , truth maintenance system , or equation solver .I will illustrate how Dyna makes it easy to specify the combinatorial structure of typical computations needed in natural language processing , machine learning , and elsewhere in AI .","label":"Background","metadata":{},"score":"64.27554"}
{"text":"Relations are more powerful than languages or functions .Learning of finite - state machines seems particularly important .Currently I 'm focusing on the case of hand - built probabilistic machines with learnable parameters .Besides giving the first EM algorithm for the basic case of learning independent arc probabilities , I showed how my solution generalized to more complex parameterizations .","label":"Background","metadata":{},"score":"64.469444"}
{"text":"This expectation can be taken over training data ( empirical risk minimization ) , or over samples from a posterior belief about the target task ( which I will call imputed risk minimization ) .The latter case requires some sort of prior model , but this is necessary when the empirical risk estimate suffers from sparse , non - independent , or out - of - domain training data .","label":"Background","metadata":{},"score":"64.567696"}
{"text":"This expectation can be taken over training data ( empirical risk minimization ) , or over samples from a posterior belief about the target task ( which I will call imputed risk minimization ) .The latter case requires some sort of prior model , but this is necessary when the empirical risk estimate suffers from sparse , non - independent , or out - of - domain training data .","label":"Background","metadata":{},"score":"64.567696"}
{"text":"Programming in Dyna is meant to be easy .A program is a short , high - level schematic description of the structure of a computation .It simply defines various values in terms of other values .The user can query and update values at runtime .","label":"Background","metadata":{},"score":"64.62046"}
{"text":"Programming in Dyna is meant to be easy .A program is a short , high - level schematic description of the structure of a computation .It simply defines various values in terms of other values .The user can query and update values at runtime .","label":"Background","metadata":{},"score":"64.62046"}
{"text":"And how can a speaker manage to sift through an infinite set to find the best pronunciation ?The two questions are related .Only after pinning down the formalism can one say much about computation .So I tried to do both : .","label":"Background","metadata":{},"score":"64.63829"}
{"text":"It extends logic programming with weights in a way that resembles functional programming .The weights are often probabilities .Yet Dyna does not enforce a probabilistic semantics , since many AI and ML methods work with inexact probabilities ( e.g. , bounds ) and other numeric and non - numeric quantities .","label":"Background","metadata":{},"score":"64.81016"}
{"text":"It extends logic programming with weights in a way that resembles functional programming .The weights are often probabilities .Yet Dyna does not enforce a probabilistic semantics , since many AI and ML methods work with inexact probabilities ( e.g. , bounds ) and other numeric and non - numeric quantities .","label":"Background","metadata":{},"score":"64.81016"}
{"text":"Some recent approaches give strong performance guarantees by training the policy iteratively .However , it is important to note that these guarantees depend on how well the policy we found can imitate the oracle on the training data .When there is a substantial difference between the oracle 's ability and the learner 's policy space , we may fail to find a policy that has low error on the training set .","label":"Background","metadata":{},"score":"64.82106"}
{"text":"Some recent approaches give strong performance guarantees by training the policy iteratively .However , it is important to note that these guarantees depend on how well the policy we found can imitate the oracle on the training data .When there is a substantial difference between the oracle 's ability and the learner 's policy space , we may fail to find a policy that has low error on the training set .","label":"Background","metadata":{},"score":"64.82106"}
{"text":"The course should also help to increase your comfort with the building blocks of statistical NLP - weighted transducers , probabilistic grammars , graphical models , etc . , and the supervised training procedures for these building blocks .Grading : 30 % written responses ( graded as check / check - plus , etc . ) , 30 % class participation , 40 % project .","label":"Background","metadata":{},"score":"64.90596"}
{"text":"To train on unlabeled data , we require unsupervised estimation methods for log - linear models ; few exist .We describe a novel approach , contrastive estimation .We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efficient .","label":"Background","metadata":{},"score":"64.979004"}
{"text":"To train on unlabeled data , we require unsupervised estimation methods for log - linear models ; few exist .We describe a novel approach , contrastive estimation .We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efficient .","label":"Background","metadata":{},"score":"64.979004"}
{"text":"It stops the selection process to make a prediction according to a user - specified accuracy - cost trade - off .We cast the sequential decision - making problem as a Markov Decision Process and apply imitation learning techniques .We address the problem of learning and inference jointly in a simple multiclass classification setting .","label":"Background","metadata":{},"score":"65.01167"}
{"text":"It stops the selection process to make a prediction according to a user - specified accuracy - cost trade - off .We cast the sequential decision - making problem as a Markov Decision Process and apply imitation learning techniques .We address the problem of learning and inference jointly in a simple multiclass classification setting .","label":"Background","metadata":{},"score":"65.01167"}
{"text":"Feasible finite state methods in natural language processing are an application of a vast body of basic research in mathematics and computer science .There remain , however , situations where straightforward and general methods fail to be applicable or efficient .","label":"Background","metadata":{},"score":"65.0276"}
{"text":"Incorporating additional features would increase the runtime additively rather than multiplicatively .A human annotator can provide hints to a machine learner by highlighting contextual \" rationales \" for each of his or her annotations ( Zaidan et al . , 2007 ) .","label":"Background","metadata":{},"score":"65.07854"}
{"text":"Incorporating additional features would increase the runtime additively rather than multiplicatively .A human annotator can provide hints to a machine learner by highlighting contextual \" rationales \" for each of his or her annotations ( Zaidan et al . , 2007 ) .","label":"Background","metadata":{},"score":"65.07854"}
{"text":"Directional evaluation is also attractive on empirical grounds ( it naturally describes iterative phenomena without using GA ) and on aesthetic grounds ( it resembles constraint ranking ) .Comprehension and Compilation in Optimality Theory ( ACL 2002 ) unifies directional constraints with other proposals for changing the constraint evaluation strategy .","label":"Background","metadata":{},"score":"65.29493"}
{"text":"In accordance with the invention , the compression dictionary is pre - filled , prior to the beginning of the data compression , with letter sequences , words and/or phrases frequent in the domain from which the data being compressed is drawn .","label":"Background","metadata":{},"score":"65.31793"}
{"text":"In accordance with the invention , the compression dictionary is pre - filled , prior to the beginning of the data compression , with letter sequences , words and/or phrases frequent in the domain from which the data being compressed is drawn .","label":"Background","metadata":{},"score":"65.31793"}
{"text":"( Warning : The layout is a bit confusing because each rule is explained before it is stated . )See Thompson & Mooney ( 1999 ) for a different approach and an empirical comparison to Siskind 's system .Week of Feb. 18 : HMMs and Part - of - Speech Tagging .","label":"Background","metadata":{},"score":"65.36982"}
{"text":"50 - 56 Springer - Verlag , Berlin 1998 .ftp://altea.dlsi.ua.es/people/oncina/articulos/icgi98.ps.gz ( draft ) .Kevin Knight and Jonathan Graehl ( 1998 ) .Machine Transliteration .Computational Linguistics 24(4):599 - 612 , December .[ Hardcopy available and preferred ; in a pinch , read the slightly less detailed ACL-97 version . ] One stage of this work effectively learns weights for a weighted edit distance , a problem discussed more directly by Ristad & Yianilos .","label":"Background","metadata":{},"score":"65.59326"}
{"text":"Moreover , it is not specifically tuned for the known reward function .We propose a hybrid reinforcement / apprenticeship learning algorithm that , even with only a few inexpensive features , can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state - of - the - art baselines .","label":"Background","metadata":{},"score":"65.59607"}
{"text":"Moreover , it is not specifically tuned for the known reward function .We propose a hybrid reinforcement / apprenticeship learning algorithm that , even with only a few inexpensive features , can automatically learn weights that achieve competitive accuracies at significant improvements in speed over state - of - the - art baselines .","label":"Background","metadata":{},"score":"65.59607"}
{"text":"We welcome discussion of this book review on the list , and particularly invite the author(s ) or editor(s ) of this book to join in .To start a discussion of this book , you can use the Discussion form on the LINGUIST List website .","label":"Background","metadata":{},"score":"65.66977"}
{"text":"In that case , their probability should covary - observing one should raise the estimated probability of the other .A transformation model captures this by positing that some instances of y were derived by transformation from x .Indeed , p ( y ) is defined by summing over all transformation sequences that would terminate at y .","label":"Background","metadata":{},"score":"65.862335"}
{"text":"In that case , their probability should covary - observing one should raise the estimated probability of the other .A transformation model captures this by positing that some instances of y were derived by transformation from x .Indeed , p ( y ) is defined by summing over all transformation sequences that would terminate at y .","label":"Background","metadata":{},"score":"65.862335"}
{"text":"Just as programming is the traditional introduction to computer science , writing grammars by hand is an excellent introduction to many topics in computational linguistics .We present and justify a well - tested introductory activity in which teams of mixed background compete to write probabilistic context - free grammars of English .","label":"Background","metadata":{},"score":"65.90019"}
{"text":"Just as programming is the traditional introduction to computer science , writing grammars by hand is an excellent introduction to many topics in computational linguistics .We present and justify a well - tested introductory activity in which teams of mixed background compete to write probabilistic context - free grammars of English .","label":"Background","metadata":{},"score":"65.90019"}
{"text":"We also extend our algorithm to parse weighted - FSA inputs such as lattices .A hypergraph or \" packed forest \" is a compact data structure that uses structure - sharing to represent exponentially many trees in polynomial space .Given a weighted / probabilistic hypergraph , we might ask three questions .","label":"Background","metadata":{},"score":"66.1187"}
{"text":"We also extend our algorithm to parse weighted - FSA inputs such as lattices .A hypergraph or \" packed forest \" is a compact data structure that uses structure - sharing to represent exponentially many trees in polynomial space .Given a weighted / probabilistic hypergraph , we might ask three questions .","label":"Background","metadata":{},"score":"66.1187"}
{"text":"This finds a ranking so each attested form x i beats or ties a particular competitor y i .( 2 ) We also generalize RCD so each x i beats or ties all possible competitors .That is , one can not improve qualitatively upon brute force : ( 3 ) Merely checking that a single ( given ) ranking is consistent with given forms is coNP - complete if the surface forms are fully observed and  2 p -complete if not .","label":"Background","metadata":{},"score":"66.130264"}
{"text":"This finds a ranking so each attested form x i beats or ties a particular competitor y i .( 2 ) We also generalize RCD so each x i beats or ties all possible competitors .That is , one can not improve qualitatively upon brute force : ( 3 ) Merely checking that a single ( given ) ranking is consistent with given forms is coNP - complete if the surface forms are fully observed and  2 p -complete if not .","label":"Background","metadata":{},"score":"66.130264"}
{"text":"Nevertheless , depending on background and training , the reader may find the formal mechanisms , pseudo code algorithms , or complex notation intimidating .As the authors suggest , the reader should prepare to get his or her hands dirty .","label":"Background","metadata":{},"score":"66.13443"}
{"text":"Nevertheless , depending on background and training , the reader may find the formal mechanisms , pseudo code algorithms , or complex notation intimidating .As the authors suggest , the reader should prepare to get his or her hands dirty .","label":"Background","metadata":{},"score":"66.13443"}
{"text":"What does it mean to learn ?What objective should a learning algorithm maximize ?Decision - making systems should be trained discriminatively even if they are structured generatively .Contrastive estimation guides an unsupervised learner to learn the \" right \" latent variables , by asking it to discriminate between positive data and certain implicit negative data .","label":"Background","metadata":{},"score":"66.327805"}
{"text":"What does it mean to learn ?What objective should a learning algorithm maximize ?Decision - making systems should be trained discriminatively even if they are structured generatively .Contrastive estimation guides an unsupervised learner to learn the \" right \" latent variables , by asking it to discriminate between positive data and certain implicit negative data .","label":"Background","metadata":{},"score":"66.327805"}
{"text":"As part of this evaluation , a comprehensive , implemented analysis is given for the harmony and disharmony phenomena of Turkish .In addition to an evaluation of the Primitive Optimality Theory model , concrete proposals are suggested for possible extensions to the model , and for improved models that , unlike Primitive Optimality Theory , can model non - concatenative morphology , Paradigm Uniformity , and reduplication . ...","label":"Background","metadata":{},"score":"66.40045"}
{"text":"Weighted finite - state transducers suffer from the lack of a training algorithm .Training is even harder for transducers that have been assembled via finite - state operations such as composition , minimization , union , concatenation , and closure , as this yields tricky parameter tying .","label":"Background","metadata":{},"score":"66.85483"}
{"text":"Weighted finite - state transducers suffer from the lack of a training algorithm .Training is even harder for transducers that have been assembled via finite - state operations such as composition , minimization , union , concatenation , and closure , as this yields tricky parameter tying .","label":"Background","metadata":{},"score":"66.85483"}
{"text":"Contrastive Estimation : Training Log - Linear Models on Unlabeled Data ( ACL 2005 , with Noah A. Smith ) shows dramatic gains on unsupervised part - of - speech tagging .Instead of maximizing the likelihood of the example strings , it tries to make them more likely than other , superficially similar strings in the same \" neighborhood .","label":"Background","metadata":{},"score":"67.37314"}
{"text":"Use \" xv -rotate 90 \" to view the TIFF files .( Sorry about the format - my copy was missing and I had to get a colleague elsewhere to send me a scanned version . )Note : If anyone wants it , I have a hardcopy of the original , more theoretical paper by Angluin on k - reversible learning .","label":"Background","metadata":{},"score":"67.469185"}
{"text":"Dynamic programming algorithms in statistical natural language processing can be easily described as weighted logic programs .We give a notation and semantics for such programs .We then describe several source - to - source transformations that affect a program 's efficiency , primarily by rearranging computations for better reuse or by changing the search strategy .","label":"Background","metadata":{},"score":"67.72659"}
{"text":"Dynamic programming algorithms in statistical natural language processing can be easily described as weighted logic programs .We give a notation and semantics for such programs .We then describe several source - to - source transformations that affect a program 's efficiency , primarily by rearranging computations for better reuse or by changing the search strategy .","label":"Background","metadata":{},"score":"67.72659"}
{"text":"The lesson is taught from a live , commented spreadsheet that implements the algorithm and graphs its behavior on a whimsical toy example .By experimenting with different inputs , one can help students develop intuitions about HMMs in particular and Expectation Maximization in general .","label":"Background","metadata":{},"score":"68.10706"}
{"text":"The lesson is taught from a live , commented spreadsheet that implements the algorithm and graphs its behavior on a whimsical toy example .By experimenting with different inputs , one can help students develop intuitions about HMMs in particular and Expectation Maximization in general .","label":"Background","metadata":{},"score":"68.10706"}
{"text":"I give a semantic account of FC any as an \" irrealis \" quantifier .This explains some curious ( new and old ) facts about FC any 's semantics and licensing environments .Furthermore , it predicts that negation and other NPI - licensing environments should license FC any , which would then have just the same meaning as NPI any ( pace Ladusaw ( 1979 ) , Carlson ( 1980 ) ) .","label":"Background","metadata":{},"score":"68.196106"}
{"text":"I give a semantic account of FC any as an \" irrealis \" quantifier .This explains some curious ( new and old ) facts about FC any 's semantics and licensing environments .Furthermore , it predicts that negation and other NPI - licensing environments should license FC any , which would then have just the same meaning as NPI any ( pace Ladusaw ( 1979 ) , Carlson ( 1980 ) ) .","label":"Background","metadata":{},"score":"68.196106"}
{"text":"Only a small subgraph is visible at any given time .We sketch how we lay out the visible subgraph , and how we update the layout smoothly and dynamically in an asynchronous environment .We also sketch our user interface for browsing and annotating such graphs - in particular , how we try to make keyboard navigation usable .","label":"Background","metadata":{},"score":"68.297806"}
{"text":"Only a small subgraph is visible at any given time .We sketch how we lay out the visible subgraph , and how we update the layout smoothly and dynamically in an asynchronous environment .We also sketch our user interface for browsing and annotating such graphs - in particular , how we try to make keyboard navigation usable .","label":"Background","metadata":{},"score":"68.297806"}
{"text":"The new working implementation under development is available here on github .We propose that one should learn a foreign language by reading interesting prose .But how can one get started ?We are building an intelligent user interface that partially translates text , leaning at first on the learner 's native vocabulary but gradually introducing new foreign words and constructions in context .","label":"Background","metadata":{},"score":"68.31873"}
{"text":"In S. Wermter , E. Riloff and G. Scheler ( Eds . ) , Symbolic , Connectionist , and Statistical Approaches to Learning for Natural Language Processing .Springer Verlag .Robert C. Berwick and Sam Pilato ( 1987 ) .Learning Syntax by Automata Induction .","label":"Background","metadata":{},"score":"68.53433"}
{"text":"Declarative phonology is a program of research that was motivated in part by the need for theories of phonology that can be implemented on a computer .While it is clear that such a development would be beneficial for both theoretical and field phonology , it is not immediately obvious how one should go about implementing phonological models .","label":"Background","metadata":{},"score":"68.59749"}
{"text":"Submission .Articles submitted to this special issue must adhere to the NLE instructions for contributors .Style Guide and LaTeX style files can be found here .We encourage authors to keep their submissions below 30 pages .Directory .This August 6 workshop is held in conjunction with the COLING 2000 conference ( which begins July 31 ) , but may be attended separately .","label":"Background","metadata":{},"score":"68.86941"}
{"text":"This is because the \" transformation models \" that I developed in the thesis can be regarded as probabilistic finite - state automata .Most arcs are epsilon - arcs ( hence there are many epsilon - cycles ) , and the arc probabilities have a log - linear parameterization .","label":"Background","metadata":{},"score":"68.95729"}
{"text":"Chengxiang Zhai ( 1997 ) .Exploiting context to identify lexical atoms : A statistical view of linguistic context .Proceedings of the International and Interdisciplinary Conference on Modelling and Using Context ( CONTEXT-97 ) , Rio de Janeiro , Brzil , Feb. 4 - 6 , 1997 .","label":"Background","metadata":{},"score":"69.09854"}
{"text":"We apply our algorithm to linear programming based branch - and - bound for solving mixed integer programs ( MIP ) .We compare our method with one of the fastest open - source solvers , SCIP ; and a very efficient commercial solver , Gurobi .","label":"Background","metadata":{},"score":"69.110535"}
{"text":"We apply our algorithm to linear programming based branch - and - bound for solving mixed integer programs ( MIP ) .We compare our method with one of the fastest open - source solvers , SCIP ; and a very efficient commercial solver , Gurobi .","label":"Background","metadata":{},"score":"69.110535"}
{"text":"We find that our method can effectively find name variants in a corpus of web strings used to refer to persons in Wikipedia , improving over standard untrained distances such as Jaro - Winkler and Levenshtein distance .Note : The experiments in this paper are incomplete - we regrettably had to omit some other experimental results because of a bug in the code .","label":"Background","metadata":{},"score":"69.44968"}
{"text":"We find that our method can effectively find name variants in a corpus of web strings used to refer to persons in Wikipedia , improving over standard untrained distances such as Jaro - Winkler and Levenshtein distance .Note : The experiments in this paper are incomplete - we regrettably had to omit some other experimental results because of a bug in the code .","label":"Background","metadata":{},"score":"69.44968"}
{"text":"The modified system is no more difficult for voters than current proposals - and it provides virtually all the benefits of STV , together with some new ones .This talk for a general audience gives a sketch of what the field of cognitive science is about .","label":"Background","metadata":{},"score":"69.978874"}
{"text":"The modified system is no more difficult for voters than current proposals - and it provides virtually all the benefits of STV , together with some new ones .This talk for a general audience gives a sketch of what the field of cognitive science is about .","label":"Background","metadata":{},"score":"69.978874"}
{"text":"The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses ; that is , spurious ambiguity ( as carefully defined ) is shown to be both safely and completely eliminated .English any is often treated as two unrelated or semi - related lexemes : a negative - polarity item , NPI any , and a universal quantifier , free - choice ( FC ) any .","label":"Background","metadata":{},"score":"70.48982"}
{"text":"The parser is proved to find exactly one parse in each semantic equivalence class of allowable parses ; that is , spurious ambiguity ( as carefully defined ) is shown to be both safely and completely eliminated .English any is often treated as two unrelated or semi - related lexemes : a negative - polarity item , NPI any , and a universal quantifier , free - choice ( FC ) any .","label":"Background","metadata":{},"score":"70.48982"}
{"text":"Latent Dirichlet Allocation ( LDA ) has been used to learn selectional preferences as soft disjunctions over flat semantic classes .Our model , the SCTM , also learns the structure of each class as a soft conjunction of high - level semantic features .","label":"Background","metadata":{},"score":"71.23041"}
{"text":"Latent Dirichlet Allocation ( LDA ) has been used to learn selectional preferences as soft disjunctions over flat semantic classes .Our model , the SCTM , also learns the structure of each class as a soft conjunction of high - level semantic features .","label":"Background","metadata":{},"score":"71.23041"}
{"text":"Structured prediction \" is the problem of modeling unknown variables that are themselves complex structures , such as vectors , strings , or trees .The number of values for such a variable may be astronomically large .Searching for the most likely structure is a combinatorial optimization problem .","label":"Background","metadata":{},"score":"71.301285"}
{"text":"We present a block Gibbs sampler for posterior inference and an empirical evaluation on several datasets .String similarity is most often measured by weighted or unweighted edit distance d ( x , y ) .We generalize this so that the probability of choosing each edit operation can depend on contextual features .","label":"Background","metadata":{},"score":"71.42856"}
{"text":"We present a block Gibbs sampler for posterior inference and an empirical evaluation on several datasets .String similarity is most often measured by weighted or unweighted edit distance d ( x , y ) .We generalize this so that the probability of choosing each edit operation can depend on contextual features .","label":"Background","metadata":{},"score":"71.42856"}
{"text":"Likely shoppers can be identified , then enticed with the most effective visual and textual advertisements ; deals can be offered to them , either on - line or off - line ; detailed product information screens can be subtly rearranged from one type of shopper to the next .","label":"Background","metadata":{},"score":"71.56154"}
{"text":"Likely shoppers can be identified , then enticed with the most effective visual and textual advertisements ; deals can be offered to them , either on - line or off - line ; detailed product information screens can be subtly rearranged from one type of shopper to the next .","label":"Background","metadata":{},"score":"71.56154"}
{"text":"Part - of - Speech tagsets , often used linguistic categories .Part - of - Speech tagging : rule based and probabilistic models .Phrase chunking : rule based and probabilistic models .Rule - based and Statistical Parsing and Chunking .","label":"Background","metadata":{},"score":"71.81115"}
{"text":"Contact - mailto : sigphon2000 cs.rochester.edu .AUTHORS : Roark , Brian and Sproat , Richard TITLE : Computational Approaches to Morphology and Syntax SERIES : Oxford Surveys in Syntax and Morphology PUBLISHER : Oxford University Press YEAR :2007 .Xiaofei Lu , Department of Applied Linguistics , The Pennsylvania State University .","label":"Background","metadata":{},"score":"71.9579"}
{"text":"By a reduction of learning by demonstration to online learning , we prove that coaching can yield a lower regret bound than using the oracle .We apply our algorithm to a novel cost - sensitive dynamic feature selection , a hard decision problem that considers a user - specified accuracy - cost trade - off .","label":"Background","metadata":{},"score":"72.001434"}
{"text":"By a reduction of learning by demonstration to online learning , we prove that coaching can yield a lower regret bound than using the oracle .We apply our algorithm to a novel cost - sensitive dynamic feature selection , a hard decision problem that considers a user - specified accuracy - cost trade - off .","label":"Background","metadata":{},"score":"72.001434"}
{"text":"Many strategies could be used to actually solve those systems .Our examples motivate certain extensions to Datalog , which are connected to functional and object - oriented programming paradigms .Confusion network decoding for MT system combination .Antti - Veikko Rosti , Eugene Matusov , Jason Smith , Necip Ayan , Jason Eisner , Damianos Karakos , Sanjeev Khudanpur , Gregor Leusch , Zhifei Li , Spyros Matsoukas , Hermann Ney , Richard Schwartz , B. Zhang , and J. Zheng ( 2011 ) .","label":"Background","metadata":{},"score":"72.16874"}
{"text":"Many strategies could be used to actually solve those systems .Our examples motivate certain extensions to Datalog , which are connected to functional and object - oriented programming paradigms .Confusion network decoding for MT system combination .Antti - Veikko Rosti , Eugene Matusov , Jason Smith , Necip Ayan , Jason Eisner , Damianos Karakos , Sanjeev Khudanpur , Gregor Leusch , Zhifei Li , Spyros Matsoukas , Hermann Ney , Richard Schwartz , B. Zhang , and J. Zheng ( 2011 ) .","label":"Background","metadata":{},"score":"72.16874"}
{"text":"The visualization displays a regularized training objective ; it supports gradient ascent by optionally displaying gradients on the sliders and providing \" Step \" and \" Solve \" buttons .The user can sample parameters and datasets of different sizes and compare their own parameters to the truth .","label":"Background","metadata":{},"score":"72.24222"}
{"text":"The visualization displays a regularized training objective ; it supports gradient ascent by optionally displaying gradients on the sliders and providing \" Step \" and \" Solve \" buttons .The user can sample parameters and datasets of different sizes and compare their own parameters to the truth .","label":"Background","metadata":{},"score":"72.24222"}
{"text":"The task of science is generative modeling .Given a data sample and some knowledge of the underlying processes , what is the true data distribution D ?The task of engineering is to find a decision rule that is expected to be accurate and perhaps also fast ( on distribution D ) .","label":"Background","metadata":{},"score":"72.45177"}
{"text":"The task of science is generative modeling .Given a data sample and some knowledge of the underlying processes , what is the true data distribution D ?The task of engineering is to find a decision rule that is expected to be accurate and perhaps also fast ( on distribution D ) .","label":"Background","metadata":{},"score":"72.45177"}
{"text":"Our papers are below .A human annotator is an AI system hidden inside a skull .Fortunately , it is not a black - box system .We should n't just ask annotators to give us the right answers on training data - while they 're at it , they can also mark why they chose those answers .","label":"Background","metadata":{},"score":"72.4828"}
{"text":"Our papers are below .A human annotator is an AI system hidden inside a skull .Fortunately , it is not a black - box system .We should n't just ask annotators to give us the right answers on training data - while they 're at it , they can also mark why they chose those answers .","label":"Background","metadata":{},"score":"72.4828"}
{"text":"The first paper is high - level and easy to read .The second is a hastily written draft but has a different perspective and many more experimental results .Monday , May 13 : Due date for final project .Wednesday , May 15 , 9am-12pm : Project presentation party ( in lieu of final exam ) with 20-minute talks Journal of Natural Language Engineering Special Issue on : .","label":"Background","metadata":{},"score":"72.80342"}
{"text":"If you are interested in reviewing a book for LINGUIST , look for the most recent posting with the subject \" Reviews : AVAILABLE FOR REVIEW \" , and follow the instructions at the top of the message .You can also contact the book review staff directly .","label":"Background","metadata":{},"score":"72.84711"}
{"text":"The vertically gradient v .. by Steven Bird , John Coleman , Janet Pierrehumbert , James Scobbie - PROCEEDINGS OF THE XVTH INTERNATIONAL CONGRESS OF LINGUISTS .UNIVERSIT LAVAL , QUBEC , 1992 . \" ...Declarative phonology is a program of research that was motivated in part by the need for theories of phonology that can be implemented on a computer .","label":"Background","metadata":{},"score":"73.05681"}
{"text":"The target task is a distribution not only over test - time data but also over which variables will be observed and queried .The loss function may explicitly penalize for runtime ( or data acquisition ) .This story leaves open an engineering question : What space of policies should we search ?","label":"Background","metadata":{},"score":"73.37462"}
{"text":"The target task is a distribution not only over test - time data but also over which variables will be observed and queried .The loss function may explicitly penalize for runtime ( or data acquisition ) .This story leaves open an engineering question : What space of policies should we search ?","label":"Background","metadata":{},"score":"73.37462"}
{"text":"160 - 167 .ABOUT THE REVIEWER Xiaofei Lu is currently Assistant Professor of Applied Linguistics in the Department of Applied Linguistics at The Pennsylvania State University .His research interests are primarily in computational linguistics , corpus linguistics , and intelligent computer - assisted language learning .","label":"Background","metadata":{},"score":"73.548744"}
{"text":"160 - 167 .ABOUT THE REVIEWER Xiaofei Lu is currently Assistant Professor of Applied Linguistics in the Department of Applied Linguistics at The Pennsylvania State University .His research interests are primarily in computational linguistics , corpus linguistics , and intelligent computer - assisted language learning .","label":"Background","metadata":{},"score":"73.548744"}
{"text":"A communications method utilizes memory areas to buffer portions of the media streams .These buffer areas are shared by user applications , with the desirable consequence of reducing workload for the server system distributing media to the user ( client ) applications .","label":"Background","metadata":{},"score":"73.55377"}
{"text":"A communications method utilizes memory areas to buffer portions of the media streams .These buffer areas are shared by user applications , with the desirable consequence of reducing workload for the server system distributing media to the user ( client ) applications .","label":"Background","metadata":{},"score":"73.55377"}
{"text":"In Bayesian modeling , one often uses a Dirichlet distribution or Dirichlet process as a prior for a discrete distribution .These priors have the neutrality property : if event x is observed , we raise our posterior estimate of p ( x ) and correspondingly scale down the estimate of p ( y ) for all other y .","label":"Background","metadata":{},"score":"73.81736"}
{"text":"In Bayesian modeling , one often uses a Dirichlet distribution or Dirichlet process as a prior for a discrete distribution .These priors have the neutrality property : if event x is observed , we raise our posterior estimate of p ( x ) and correspondingly scale down the estimate of p ( y ) for all other y .","label":"Background","metadata":{},"score":"73.81736"}
{"text":"We define a class of triples ( A , i , j ) such that the auto - intersection of the machine A on tapes i and j can be computed by a delay - based algorithm .We point out how to extend this class and hope that it is sufficient for many practical applications .","label":"Background","metadata":{},"score":"74.39004"}
{"text":"We define a class of triples ( A , i , j ) such that the auto - intersection of the machine A on tapes i and j can be computed by a delay - based algorithm .We point out how to extend this class and hope that it is sufficient for many practical applications .","label":"Background","metadata":{},"score":"74.39004"}
{"text":"Purely empirical approache ... \" .A fundamental debate in the machine learning of language has been the role of prior knowledge in the learning process .Purely nativist approaches , such as the Principles and Parameters model , build parameterized linguistic generalizations directly into the learning system .","label":"Background","metadata":{},"score":"74.537384"}
{"text":"+ recent articles : e.g. , of the proceedings of the Meetings of the Association for Computational Linguistics .","label":"Background","metadata":{},"score":"75.17611"}
{"text":"This criterion is a generalization of the function maximized by the Expectation - Maximization algorithm [ Dempster et al . , 1977].CE is a natural fit for log - linear models , which can include arbitrary features but for which EM is computationally difficult .","label":"Background","metadata":{},"score":"75.938194"}
{"text":"This criterion is a generalization of the function maximized by the Expectation - Maximization algorithm [ Dempster et al . , 1977].CE is a natural fit for log - linear models , which can include arbitrary features but for which EM is computationally difficult .","label":"Background","metadata":{},"score":"75.938194"}
{"text":"The classic \" easy \" optimization problem is to find the MST of a connected , undirected graph .Good polynomial - time algorithms have been known since 1930 .Over the last 10 years , however , the standard O ( m log n ) results of Kruskal and Prim have been improved to linear or near - linear time .","label":"Background","metadata":{},"score":"76.20419"}
{"text":"The classic \" easy \" optimization problem is to find the MST of a connected , undirected graph .Good polynomial - time algorithms have been known since 1930 .Over the last 10 years , however , the standard O ( m log n ) results of Kruskal and Prim have been improved to linear or near - linear time .","label":"Background","metadata":{},"score":"76.20419"}
{"text":"We propose a Markov Random Field in which each factor ( potential function ) is a weighted finite - state machine , typically a transducer that evaluates the relationship between just two of the strings .The full joint distribution is then a product of these factors .","label":"Background","metadata":{},"score":"76.44806"}
{"text":"We propose a Markov Random Field in which each factor ( potential function ) is a weighted finite - state machine , typically a transducer that evaluates the relationship between just two of the strings .The full joint distribution is then a product of these factors .","label":"Background","metadata":{},"score":"76.44806"}
{"text":"Secure data interchange .Frederick S. M. Herz , Walter Paul Labys , David C. Parkes , Sampath Kannan , and Jason M. Eisner ( filed 2000 ) .Patent pending .A secure data interchange system enables information about bilateral and multilateral interactions between multiple persistent parties to be exchanged and leveraged within an environment that uses a combination of techniques to control access to information , release of information , and matching of information back to parties .","label":"Background","metadata":{},"score":"76.71474"}
{"text":"Secure data interchange .Frederick S. M. Herz , Walter Paul Labys , David C. Parkes , Sampath Kannan , and Jason M. Eisner ( filed 2000 ) .Patent pending .A secure data interchange system enables information about bilateral and multilateral interactions between multiple persistent parties to be exchanged and leveraged within an environment that uses a combination of techniques to control access to information , release of information , and matching of information back to parties .","label":"Background","metadata":{},"score":"76.71474"}
{"text":"Submission : Email me written responses to the whole week 's readings by 11 am each Monday .Academic honesty : dept . policy ( but you can work in pairs on reading responses ) .Readings and Responses .Generally we will discuss about 3 related papers each week .","label":"Background","metadata":{},"score":"77.293335"}
{"text":"Temperamentally I 'm more of a mathematician than an engineer .That is not a value judgment about engineering , nor a scientific judgment that human cognition is neat rather than scruffy .It 's just a research style .It means that I prefer formalisms and algorithms that are clean enough to be easily communicated , analyzed , and modified .","label":"Background","metadata":{},"score":"77.39595"}
{"text":"A data owner can specify a price for different types and amounts of information access .The system for the automatic determination of customized prices and promotions automatically constructs product offers tailored to individual shoppers , or types of shopper , in a way that attempts to maximize the vendor 's profits .","label":"Background","metadata":{},"score":"77.908905"}
{"text":"A data owner can specify a price for different types and amounts of information access .The system for the automatic determination of customized prices and promotions automatically constructs product offers tailored to individual shoppers , or types of shopper , in a way that attempts to maximize the vendor 's profits .","label":"Background","metadata":{},"score":"77.908905"}
{"text":"Because of the neutrality property , a Dirichlet ( process ) prior on a discrete distribution can not capture correlations among the probabilities of \" similar \" events .We propose obtaining the discrete distribution instead from a random walk model or transformation model , in which each observed event has evolved via a latent sequence of transformations .","label":"Background","metadata":{},"score":"78.59956"}
{"text":"Because of the neutrality property , a Dirichlet ( process ) prior on a discrete distribution can not capture correlations among the probabilities of \" similar \" events .We propose obtaining the discrete distribution instead from a random walk model or transformation model , in which each observed event has evolved via a latent sequence of transformations .","label":"Background","metadata":{},"score":"78.59956"}
{"text":"I will print the responses out for everyone , and they will anchor our class discussion .They will also be a useful source of ideas for your final projects .A typical response is 1 - 3 paragraphs ; in a given week you might respond at greater length to some papers than others .","label":"Background","metadata":{},"score":"78.629196"}
{"text":"What should you write about ?Some possibilities : .Idea for a new experiment , model or other research opportunity inspired by the reading .A clearer explanation of some point that everyone probably had to struggle with .Other ways the research could be improved ( e.g. , flaws you spotted ) .","label":"Background","metadata":{},"score":"78.850235"}
{"text":"The North American Computational Linguistics Olympiad ( NACLO ) includes computational puzzles in addition to purely linguistic ones .This paper explores the computational subject matter we want to convey via NACLO , as well as some of the challenges that arise when adapting problems in computational linguistics to an audience that may have no background in computer science , linguistics , or advanced mathematics .","label":"Background","metadata":{},"score":"79.974"}
{"text":"The North American Computational Linguistics Olympiad ( NACLO ) includes computational puzzles in addition to purely linguistic ones .This paper explores the computational subject matter we want to convey via NACLO , as well as some of the challenges that arise when adapting problems in computational linguistics to an audience that may have no background in computer science , linguistics , or advanced mathematics .","label":"Background","metadata":{},"score":"79.974"}
{"text":"Temiar Reduplication in One - Level Prosodic Morphology Markus Walther This paper presents the first computational analysis of a difficult piece of prosodic morphology , aspectual reduplication in the Malaysian language Temiar , using the novel finite - state approach of One - Level Prosodic Morphology ( Walther 1999b , 2000 ) .","label":"Background","metadata":{},"score":"81.66199"}
{"text":"In countries with high illiteracy , however , this system may be unworkable .This paper proposes a practical modification of STV .In the modified system , each citizen votes for only one candidate .Voters need not specify their second , third , and fourth choices .","label":"Background","metadata":{},"score":"81.81406"}
{"text":"In countries with high illiteracy , however , this system may be unworkable .This paper proposes a practical modification of STV .In the modified system , each citizen votes for only one candidate .Voters need not specify their second , third , and fourth choices .","label":"Background","metadata":{},"score":"81.81406"}
{"text":"It produces up to 99 % accuracy on realistic data and can process ciphertexts at 200ms per byte on a $ 2,000 PC .To further demonstrate the practical effectiveness of the method , we show that our tool can recover documents encrypted by Microsoft Word 2002 .","label":"Background","metadata":{},"score":"82.92592"}
{"text":"It produces up to 99 % accuracy on realistic data and can process ciphertexts at 200ms per byte on a $ 2,000 PC .To further demonstrate the practical effectiveness of the method , we show that our tool can recover documents encrypted by Microsoft Word 2002 .","label":"Background","metadata":{},"score":"82.92592"}
{"text":"They are communicated either to the vendor , who may act on them as desired , or to an on - line computer shopping system that directly makes such offers to shoppers .Largely by tracking the behavior of shoppers , the system accumulates extensive profiles of the shoppers and the offers that they consider .","label":"Background","metadata":{},"score":"83.334625"}
{"text":"They are communicated either to the vendor , who may act on them as desired , or to an on - line computer shopping system that directly makes such offers to shoppers .Largely by tracking the behavior of shoppers , the system accumulates extensive profiles of the shoppers and the offers that they consider .","label":"Background","metadata":{},"score":"83.334625"}
{"text":"Two papers reported the state of the work as of 1991 : .The bilexical idea ultimately proved to be my most important contribution to that project , and it so dramatically improved performance that it continued to influence my direction .","label":"Background","metadata":{},"score":"85.21399"}
{"text":"This led to a probabilistic dependency parser that I built for fun in 1994 , which was interesting for both its cubic - time algorithm and its bilexical parameterization .Unfortunately I did n't bother to evaluate it on a sufficient volume of training and test data until 1996 , spurred by the strong results of my friend and colleague Mike Collins , who was working along similar lines .","label":"Background","metadata":{},"score":"86.81504"}
{"text":"A centerpiece of the course is the requirement to respond thoughtfully to each paper in writing .You should email me your responses to the upcoming week 's papers , in separate plaintext or postscript messages , by noon each Monday .","label":"Background","metadata":{},"score":"88.60139"}
{"text":"Additionally , a cryptographically - based pseudonym proxy server is provided to ensure the privacy of a user 's target profile interest summary , by giving the user control over the ability of third parties to access this summary and to identify or contact the user .","label":"Background","metadata":{},"score":"89.23511"}
{"text":"Additionally , a cryptographically - based pseudonym proxy server is provided to ensure the privacy of a user 's target profile interest summary , by giving the user control over the ability of third parties to access this summary and to identify or contact the user .","label":"Background","metadata":{},"score":"89.23511"}
{"text":"Look at the first page of this shorter version for some thoughts about this .Week of Mar. 11 : Inside - Outside Algorithm .If you need to review the inside - outside algorithm , check my course slides before reading the following papers .","label":"Background","metadata":{},"score":"90.45497"}
{"text":"\" Winner take all \" electoral systems are not fully representative .Unfortunately , the ANC 's proposed system of proportional representation is not much better .Because it ensconces party politics , it is only slightly more representative , and poses a serious threat to accountability .","label":"Background","metadata":{},"score":"92.50504"}
{"text":"\" Winner take all \" electoral systems are not fully representative .Unfortunately , the ANC 's proposed system of proportional representation is not much better .Because it ensconces party politics , it is only slightly more representative , and poses a serious threat to accountability .","label":"Background","metadata":{},"score":"92.50504"}
