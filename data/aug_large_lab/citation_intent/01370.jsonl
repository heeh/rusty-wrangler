{"text":"In addition , our discriminative approach integrally admits features beyond local tree configurations .We present a multi - scale training method along with an efficient CKY - style dynamic program .On a variety of domains and languages , this method produces the best published parsing accuracies with the smallest reported grammars .","label":"Background","metadata":{},"score":"35.866272"}{"text":"This iterative implementation is repeated until two feature components remain .At each loop in the algorithm , the classification performance of the system is tested by using test data and stored for determining the best features .So , the optimal input size is also determined as the number of the most significant feature components .","label":"Background","metadata":{},"score":"36.411274"}{"text":"We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .We propose using corpus - level statistics for lexicon learning decisions .","label":"Background","metadata":{},"score":"36.53372"}{"text":"Note that feature numbers are integral , starting at 1 .Feature values are real numbers .Features for each token ( e.g. \" see \" ) must appear in increasing order of their feature number FEATNUM ; you should skip features whose values are zero , although leaving them in wo n't do harm other than increasing runtime .","label":"Background","metadata":{},"score":"37.113323"}{"text":"Because of the relatively few images in the dataset , we use cross validation to estimate the performance of the predictive function generated in step 2 .We randomly split the overall set of images into two different subsets : the training set and the validation set .","label":"Background","metadata":{},"score":"37.944885"}{"text":"Due to the nature of the algorithm , the SVM is also trained and tested while the feature selection is implemented .All these implementations are done for each two feature sets separately in the same way .The feature extraction of the first feature set involves the DCT coefficients of data .","label":"Background","metadata":{},"score":"38.24253"}{"text":"However , parsing and training times are still relatively long .To determine why , we analyzed the time usage of a dependency parser .We illustrate that the mapping of the features onto thei ... \" .In addition to a high accuracy , short parsing and training times are the most important properties of a parser .","label":"Background","metadata":{},"score":"38.252327"}{"text":"We use such an approach [ Henderson et al . , 2008 ] as our baseline .In this paper we adopt a simplified version of this approach , where we introduce a single new action .Although the resulting parser is not powerful enough to parse all non - planar structures , this s .. \" ...","label":"Background","metadata":{},"score":"38.55146"}{"text":"We provide experimental evaluations on the Penn Treebank . ... , or build a single tree by means of shift - reduce parsing actions ( Yamada & Matsumoto , 2003 ) .These parsers process the sentence sequentially , hence their efficiency makes them suitable for processing large amounts of text , as required , for example , in information retrieval applications .","label":"Background","metadata":{},"score":"38.615273"}{"text":"Unlike existing preordering models , we train feature - rich discriminative classifiers that directly predict the target - side word order .Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long - distance reorderings using the structure of the parse tree , while utilizing a discriminative model with a rich set of features , including lexical features .","label":"Background","metadata":{},"score":"38.922497"}{"text":"Our best results show a 26-fold speedup compared to a sequential C implementation .We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data .We first demonstrate that delexicalized parsers can be directly transferred between languages , producing significantly higher accuracies than unsupervised parsers .","label":"Background","metadata":{},"score":"39.408974"}{"text":"Second , how can we efficiently infer optimal structures within them ?Hierarchical coarse - to - fine methods address both questions .Coarse - to - fine approaches exploit a sequence of models which introduce complexity gradually .At the top of the sequence is a trivial model in which learning and inference are both cheap .","label":"Background","metadata":{},"score":"39.56973"}{"text":"Unlike previous work , our final model does not require any additional resources at run - time .Compared to a state - of - the - art approach , we achieve more than 20 % relative error reduction .Additionally , we annotate a corpus of search queries with part - of - speech tags , providing a resource for future work on syntactic query analysis .","label":"Background","metadata":{},"score":"40.420277"}{"text":"We present a novel approach which employs a randomized sequence of pruning masks .Formally , we apply auxiliary variable MCMC sampling to generate this sequence of masks , thereby gaining theoretical guarantees about convergence .Because each mask is generally able to skip large portions of an underlying dynamic program , our approach is particularly compelling for high - degree algorithms .","label":"Background","metadata":{},"score":"40.45692"}{"text":"First , the sensitivity of the output to the inputs in SVM is derived .Then , a recursive algorithm is presented not only to remove irrelevant features but also to find the optimal solution .A real EEG data set for classification is evaluated in the experiments .","label":"Background","metadata":{},"score":"40.883194"}{"text":"We focus on one of the simplest and most efficient architectures , based on a deterministic shift - reduce algorithm , trained with the perceptron .By adopting second - order feature maps , the primal form of the perceptron produces models with comparable accuracy to more complex architectures , with no need for approximations .","label":"Background","metadata":{},"score":"41.37113"}{"text":"These approaches are typically used to address the class imbalance problem .I ncrease the training data size .Over sample the minority class data or under sample the majority class data in case of imbalance .Reduce features by removing features irrelevant to the class attribute .","label":"Background","metadata":{},"score":"41.489243"}{"text":"In our experiments , the training set is composed of 90\\% of the dataset images and the validation set is composed of the remaining 10\\% .The same experiment was conducted on the other binarization methods .-Among the 18 features , most models embed about 7 features .","label":"Background","metadata":{},"score":"41.532555"}{"text":"Our method does not assume any knowledge about the target language ( in particular no tagging dictionary is assumed ) , making it applicable to a wide array of resource - poor languages .We use graph - based label propagation for cross - lingual knowledge transfer and use the projected labels as features in an unsupervised model ( Berg - Kirkpatrick et al .","label":"Background","metadata":{},"score":"41.65103"}{"text":"Since data is processed as soon as it becomes available , processing delay is minimized improving data throughput .The processing modules can be written in C++ or in Python and can be combined using few lines of Python scripts to produce full NLP applications .","label":"Background","metadata":{},"score":"41.75968"}{"text":"Starting from a mono - phone model , we learn increasingly refined models that capture phone internal structures , as well as context - dependent variations in an automatic way .Our approaches reduces error rates compared to other baseline approaches , while streamlining the learning procedure .","label":"Background","metadata":{},"score":"41.899757"}{"text":"We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .","label":"Background","metadata":{},"score":"41.986496"}{"text":"In the notation of the previous section , we would have : .Such a model is trained by taking each tag occurrence and its conditioning context in the training data as a separate training instance [ 17 ] .This training process is simpler than that for CRFs as it does not require forward - backward computations .","label":"Background","metadata":{},"score":"42.054142"}{"text":"We focus on one of the simplest and most efficient architectures , based on a deterministic shift - reduce algorithm , trained with the perceptron .By adopting second - order feature maps , the primal form of the perce ... \" .","label":"Background","metadata":{},"score":"42.146187"}{"text":"In this paper , we consider the parallel implementation of a multi - objective feature selection that makes it possible to apply it to complex classification problems such as those having many features to select , and specifically high - dimensional data sets with much more features than data items .","label":"Background","metadata":{},"score":"42.15732"}{"text":"For example , noun phrases might be split into subcategories for subjects and objects , singular and plural , and so on .This splitting process admits an efficient incremental inference scheme which reduces parsing times by orders of magnitude .Furthermore , it produces the best parsing accuracies across an array of languages , in a fully language - general fashion .","label":"Background","metadata":{},"score":"42.503227"}{"text":"At this step , there is no automatic rule to decide whether a model is valid or not .Because of the relatively few images in the dataset , we use cross validation to estimate the performance of the predictive function generated in step 2 .","label":"Background","metadata":{},"score":"42.525074"}{"text":"We present several models to this end ; in particular a partially observed conditional random field model , where coupled token and type constraints provide a partial signal for training .Averaged across eight previously studied Indo - European languages , our model achieves a 25 % relative error reduction over the prior state of the art .","label":"Background","metadata":{},"score":"42.7649"}{"text":"KD tree is a binary search tree , extended for multi dimensional space .If we build a KD tree with training samples , the order of complexity becomes O(log(m ) x n ) .However , Hadoop is not at all suitable for implementing the iterative logic to construct and navigate such trees .","label":"Background","metadata":{},"score":"42.979614"}{"text":"The second set of results are for the system that also contains features extracted from external lexicons .These results are presented in the row Lexicons .Discussion .Adding the ABGene lexicons made a significant improvement to both precision and recall .","label":"Background","metadata":{},"score":"43.217236"}{"text":"By taking tokens as well as columns , final expression of static feature becomes \" F:-2 .In this case , you can use \" F:-2 . which means same as \" F:-2 .Dynamic Features T : Dynamic features are decided dynamically during the tagging of chunk labels .","label":"Background","metadata":{},"score":"43.416397"}{"text":"By taking tokens as well as columns , final expression of static feature becomes \" F:-2 .In this case , you can use \" F:-2 . which means same as \" F:-2 .Dynamic Features T : Dynamic features are decided dynamically during the tagging of chunk labels .","label":"Background","metadata":{},"score":"43.416397"}{"text":"First , we define a function that maps a single state at input position j to a set of allowed next states at position j + 1 , T j ( s ) .These values are calculated by the following recurrences : .","label":"Background","metadata":{},"score":"43.608315"}{"text":"Our methods result in state - of - the - art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .","label":"Background","metadata":{},"score":"43.778168"}{"text":"However , any changes to a maximum entropy tagging model can be trivially applied to the corresponding CRF model , including changes in model order and feature choice .For all the tagging tasks that we have attempted , CRF models can be trained in under 20 hours ( and in most cases under 10 hours ) , which is quite practical since training is done only a few times .","label":"Background","metadata":{},"score":"43.78557"}{"text":"Unlike previous work on projecting syntactic resources , we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers .The projected parsers from our system result in state - of - the - art performance when compared to previously studied unsupervised and projected parsing systems across eight different languages .","label":"Background","metadata":{},"score":"43.79042"}{"text":"In our example , the class is a binary variable indicating whether a student completed or dropped out .We simply take the majority class among the neighbors and declare the test point to belong to that class .Unlike other classification techniques , KNN does not build any predictive model and It operates directly on the data .","label":"Background","metadata":{},"score":"43.792393"}{"text":"These include iterative scaling techniques [ 10 ] and gradient - based techniques [ 11 , 12 ] .All these methods require the calculation of the empirical and model expectations at each iteration .The empirical expectations are trivially calculated by counting in the training data the number of times each feature occurs .","label":"Background","metadata":{},"score":"44.08574"}{"text":"The model is formally a latent variable CRF grammar over trees , learned by iteratively splitting grammar productions ( not categories ) .Different regions of the grammar are refined to different degrees , yielding grammars which are three orders of magnitude smaller than the single - scale baseline and 20 times smaller than the split - and - merge grammars of Petrov et al .","label":"Background","metadata":{},"score":"44.40737"}{"text":"The algorithm uses a similarity graph to encourage similar n - grams to have similar POS tags .We demonstrate the efficacy of our approach on a domain adaptation task , where we assume that we have access to large amounts of unlabeled data from the target domain , but no additional labeled data .","label":"Background","metadata":{},"score":"44.450172"}{"text":"Our first- , second- , and third - order models achieve accuracies comparable to those of their unpruned counterparts , while exploring only a fraction of the search space .We observe speed - ups of up to two orders of magnitude compared to exhaustive search .","label":"Background","metadata":{},"score":"44.544006"}{"text":"We also give an overview of the parsing approaches that participants took and the results that they achieved .Finally , we try to draw general conclusions about multi - lingual parsing : What makes a particular language , treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser ?","label":"Background","metadata":{},"score":"44.581635"}{"text":"After the feature extraction , the real data feature sets are fed to SVM for classification of EPs .EPs and non - EPs are represented by 1 for both training and testing procedures .First feature set involving DCT coefficients explained above is fed to the SVM .","label":"Background","metadata":{},"score":"44.58577"}{"text":"Implementation .Presented here is an outline of conditional random fields and the implementation specifics of the model we use .Conditional random fields .Gene identification as a tagging problem .A sample tagging of a sentence using the beginning , inside and outside tag labels .","label":"Background","metadata":{},"score":"44.763138"}{"text":"To determine which lexicons gave the best performance , we conducted experiments examining the effect of adding each type of lexicon individually to the model and tested the model on the development data .These results are outlined in Table 2 .","label":"Background","metadata":{},"score":"44.775246"}{"text":"Nonetheless , the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks .We demonstrate that log - linear grammars with latent variables can be practically trained using discriminative methods .Central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectations to be efficiently approximated in a gradient - based procedure .","label":"Background","metadata":{},"score":"45.05526"}{"text":"We highlight the use of this resource via two experiments , including one that reports competitive accuracies for unsupervised grammar induction without gold standard part - of - speech tags .We present an online learning algorithm for training structured prediction models with extrinsic loss functions .","label":"Background","metadata":{},"score":"45.06864"}{"text":"Specifically , an ini- tial hypothesis lattice is constrcuted using local features .Candidate sentences are then assigned syntactic language model scores .These global syntactic scores are combined with local low - level scores in a log - linear model .","label":"Background","metadata":{},"score":"45.26458"}{"text":"Results .Our system was initially trained on 7500 annotated MEDLINE sentences with a development set of 2500 sentences .Training with feature induction took approximately 15 hours , which is substantially longer than training without feature induction .Once trained , the system can annotate sentences in less than a second .","label":"Background","metadata":{},"score":"45.36922"}{"text":"The distance calculation is done using sifarish , which is another OSS project of mine for personalization and recommendation implemented on Hadoop and Storm .K Nearest Neighbor .Conceptually KNN is very simple .The training data consists of students engagement data ( input or feature variables ) from past classes and whether the student eventually completed the course or not ( class or output variable ) .","label":"Background","metadata":{},"score":"45.473633"}{"text":"We show that the automatically induced latent variable grammars of Petrov et al .2006 vary widely in their underlying representations , depending on their EM initialization point .We use this to our advantage , combining multiple automatically learned grammars into an unweighted product model , which gives significantly improved performance over state - of - the - art individual grammars .","label":"Background","metadata":{},"score":"45.520638"}{"text":"p. rp . orp .The first step in using the YamCha is to create training and test files .Here , I take the Base NP Chunking task as a case study .Assume a data set like this .First column represents a word .","label":"Background","metadata":{},"score":"45.641273"}{"text":"p. rp . orp .The first step in using the YamCha is to create training and test files .Here , I take the Base NP Chunking task as a case study .Assume a data set like this .First column represents a word .","label":"Background","metadata":{},"score":"45.641273"}{"text":"This is a single probabilistic tagging model with no application - specific pre- or post - processing steps or voting over multiple classifiers .This makes the model quite general in that it may be extended to various other biological entities , provided appropriate lexicons are available .","label":"Background","metadata":{},"score":"45.75052"}{"text":"Because each refinement introduces only limited complexity , both learning and inference can be done in an incremental fashion .In this dissertation , we describe several coarse - to - fine systems .In the domain of syntactic parsing , complexity is in the grammar .","label":"Background","metadata":{},"score":"45.870914"}{"text":"We also show that our techniques can be applied to full - scale parsing applications by demonstrating its effectiveness in learning state - split grammars .Treebank parsing can be seen as the search for an optimally refined grammar consistent with a coarse training treebank .","label":"Background","metadata":{},"score":"45.89338"}{"text":"We present experiments with sequence models on part - of - speech tagging and named entity recognition tasks , and with syntactic parsers on dependency parsing and machine translation reordering tasks .Low - latency solutions for syntactic parsing are needed if parsing is to become an integral part of user - facing natural language applications .","label":"Background","metadata":{},"score":"45.954918"}{"text":"Figure 1 summarizes the system architecture .We detail the parsing All authors contributed equally to this work . ...The parser processes input tokens advancing on the input from left to right with Shift actions and accumulates processed tokens on a stack with ... . \" ...","label":"Background","metadata":{},"score":"46.12298"}{"text":"In this study , then , the performance of two feature sets from the same records is comparatively examined to justify the invariance of the proposed perturbation method with respect to the used feature extraction technique .The best features are selected by applying the adaptive feature selection method , which is proposed with the algorithm in Section 4 in detail , for each two feature sets separately .","label":"Background","metadata":{},"score":"46.223114"}{"text":"Previous sentence segmentation systems have typically been very local , using low - level prosodic and lexical features to independently decide whether or not to segment at each word boundary position .In this work , we leverage global syntactic information from a syn- tactic parser , which is better able to capture long distance depen- dencies .","label":"Background","metadata":{},"score":"46.674385"}{"text":"We extend and improve upon recent work in structured training for neural network transition - based dependency parsing .We do this by experimenting with novel features , additional transition systems and by testing on a wider array of languages .In particular , we introduce set - valued features to encode the predicted morphological properties and part - of - speech confusion sets of the words being parsed .","label":"Background","metadata":{},"score":"46.75732"}{"text":"Recently , Sutskever et al .( 2014 ) presented a task - agnostic method for learning to map input sequences to output sequences that achieved strong results on a large scale machine translation problem .In this work , we show that precisely the same sequence - to - sequence method achieves results that are close to state - of - the - art on syntactic constituency parsing , whilst making almost no assumptions about the structure of the problem .","label":"Background","metadata":{},"score":"46.786617"}{"text":"In our method the first , monolingual view consists of supervised predictors learned separately for each language .The second , bilingual view consists of log - linear predictors learned over both languages on bilingual text .Our training procedure estimates the parameters of the bilingual model using the output of the monolingual model , and we show how to combine the two models to account for dependence between views .","label":"Background","metadata":{},"score":"46.854836"}{"text":"While these models are easy to train , they are independently normalized at each position j ( equation 4 ) .In contrast , conditional random fields have a single combined normalizing denominator of Z ( o ) for the entire tag sequence t ( equations 1 and 2 ) .","label":"Background","metadata":{},"score":"46.87693"}{"text":"Standard inference can be used at test time .Our approach is able to scale to very large problems and yields significantly improved target domain accuracy .It is well known that parsing accuracies drop significantly on out - of - domain data .","label":"Background","metadata":{},"score":"47.289837"}{"text":"After testing and calculating the performance of the system , sensitivity values are calculated by using Eq .12 and Eq .Two feature components corresponding to the smallest sensitivity values are deleted , after sensitivity values are ranked in a descending order .","label":"Background","metadata":{},"score":"47.334427"}{"text":"Despite its simplicity , a product of eight automatically learned grammars improves parsing accuracy from 90.2 % to 91.8 % on English , and from 80.3 % to 84.5 % on German .Pruning can massively accelerate the computation of feature expectations in large models .","label":"Background","metadata":{},"score":"47.35148"}{"text":"With 100 K unlabeled and 2 K labeled questions , uptraining is able to improve parsing accuracy to 84 % , closing the gap between in - domain and out - of - domain performance .We study self - training with products of latent variable grammars in this paper .","label":"Background","metadata":{},"score":"47.426575"}{"text":"2005 ] and the new algorithm of SVM struct V3.10 [ Joachims et al .2009].Unlike versions of SVM hmm prior to V3.xx , this version . can easily handle tagging problems with millions of words and millions of features ; . can train higher order models with arbitrary length dependencies for both the transitions and the emissions ; . includes an optional beam search for fast approximate training and prediction ; .","label":"Background","metadata":{},"score":"47.556217"}{"text":"Indeed , some features may not be significant for predicting a specific binarization method .Moreover , even if a feature is highly correlated to the accuracy of an algorithm , it may have a weak contribution to the final prediction model .","label":"Background","metadata":{},"score":"47.566723"}{"text":"To determine why , we analyzed the time usage of a dependency parser .We illustrate that the mapping of the features onto their weights in the support vector machine is the major factor in time complexity .To resolve this problem , we implemented the passive - aggressive perceptron algorithm as a Hash Kernel .","label":"Background","metadata":{},"score":"47.940437"}{"text":"First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar without a treebank .In our experiments , hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy .","label":"Background","metadata":{},"score":"47.94317"}{"text":"Applying the trained model uses exactly the same Viterbi algorithm for CRFs as for maximum - entropy classification .It would be useful to compare directly the two approaches for gene and protein identification , although there are already results for similar tagging tasks [ 12 ] .","label":"Background","metadata":{},"score":"48.003494"}{"text":"Algorithm for input dimension reduction is first formulated and then applied to real electroencephalography ( EEG ) data for recognition of epileptiform patterns .1 Introduction SVM is a widely used tool for data classification [ 1 ] , function approximation [ 2 ] etc . , due to its generalization ability .","label":"Background","metadata":{},"score":"48.161667"}{"text":"% yamcha -C -m case_study .Note that the interpretation of test data varies according to the -C option .With -C option : the last ( or more ) columns are interpreted as candidates .Classification costs of SVMs are much larger than those of other algorithms , such as maximum entropy or decision lists .","label":"Background","metadata":{},"score":"48.208313"}{"text":"% yamcha -C -m case_study .Note that the interpretation of test data varies according to the -C option .With -C option : the last ( or more ) columns are interpreted as candidates .Classification costs of SVMs are much larger than those of other algorithms , such as maximum entropy or decision lists .","label":"Background","metadata":{},"score":"48.208313"}{"text":"We decompose the problem into three subtasks : parsing , predicate identification and classification ( PIC ) , and argument identification and classification ( AIC ) .We address each of these subtasks with separate components without backward feedback between sub - tasks .","label":"Background","metadata":{},"score":"48.23323"}{"text":"Non - linear Kernels are not supported .In the best case , training with non - linear kernels is slow , in the worst case it may be buggy .V1.01 - V2.13 .New training algorithm based on equivalent 1-slack reformulation of the training problem .","label":"Background","metadata":{},"score":"48.279686"}{"text":"In total , there were more than 400000 of such features , and the overall dimensionality of the weight vector was greater than 18 million .In my experience binary features work well , and I often discretize real - valued features into multiple binary features .","label":"Background","metadata":{},"score":"48.311756"}{"text":"To process non - planarity online , the semantic transition - based parser uses a new technique to dynamically reorder nodes during the derivation .While the synchronised derivations allow different structures to be built for the semantic non - planar graphs and syntactic dependency trees , useful statistical dependencies between these structures are modeled using latent variables .","label":"Background","metadata":{},"score":"48.352367"}{"text":"Perhaps with feature analysis I could get rid of attributes that redundant with respect to other attributes and / or not relevant to the output .Time spent with content .Time spent in discussion with others .Time spent with organizer .","label":"Background","metadata":{},"score":"48.359283"}{"text":"Only those candidates causing the highest gain are included into the current set of model features .Intuitively , features causing high gain provide strong evidence for many decisions .Thus , feature induction tends to discard infrequent features or non - discriminating features since , independently , their overall effect on the likelihood of the entire training set is usually marginal .","label":"Background","metadata":{},"score":"48.465206"}{"text":"Sensitivity analysis in this feature set has shown the fourth level wavelet coefficients to be more significant than others .Consequently , the most significant features for two different feature sets are determined by using sensitivity analysis based on SVM .Thus , not only the system is optimized for having the best performance , but also the input dimension is reduced .","label":"Background","metadata":{},"score":"48.561493"}{"text":"A Tanl pipeline can be processed in parallel on a cluster of computers by means of a modified version of Hadoop streaming .We present the architecture , its modules and some sample applications . ... trees .The module takes as input a stream of vectors of tokens , and produces a stream of sentences .","label":"Background","metadata":{},"score":"48.573784"}{"text":"Limited - memory quasi - Newton methods [ 14 ] approximate the Hessian by storing a history of update directions previously taken by the algorithm and combines them linearly with the current gradient to create the new search direction .These methods have been shown to be quite effective [ 11 , 12 ] for training log - linear models like CRFs .","label":"Background","metadata":{},"score":"48.833534"}{"text":"If provided with one data set , it computes distance between each pair of data points .If provided with two sets , it finds pair wise distance between the two data sets .We are using it in the two data sets mode .","label":"Background","metadata":{},"score":"48.90099"}{"text":"Number of chat messages .Score is tests .Score in assignments .Number of pages book marked .These are all numerical variables .The class attribute is a binary variable , where P implies that the student completed the course and it 's F otherwise .","label":"Background","metadata":{},"score":"48.908554"}{"text":"To manage this complexity , we translate into target language clusterings of increasing vocabulary size .This approach gives dramatic speed - ups while additionally increasing final translation quality .The intersection of tree transducer - based translation models with n - gram language models results in huge dynamic programs for machine translation decoding .","label":"Background","metadata":{},"score":"48.91566"}{"text":"The algorithmic complexity of our brute force way to find nearest neighbor is O(m x n ) , where m is the number of training samples and n is the number of test samples .In other words , for each test sample , we scan through all training samples .","label":"Background","metadata":{},"score":"49.023342"}{"text":"Many bioinformatics applications belong to this type .Recently , some approaches for supervised and unsupervised feature selection as a multi - objective optimization problem have been proposed .As the performance of unsupervised classification is evaluated through the quality of the obtained groups or clusters in the data set to be classified , it is difficult to define a suitable objective function that drives the selection of the features .","label":"Background","metadata":{},"score":"49.08006"}{"text":"This label bias problem [ 6 ] motivated the development of CRFs , and has been shown to adversely affect accuracy in realistic tagging tasks [ 12 ] .It might be argued that the lower accuracy of maximum entropy taggers is compensated by their faster training , which allows more complex models to be considered , for instance models with higher Markov order or more feature types .","label":"Background","metadata":{},"score":"49.09375"}{"text":"This was primarily due to the availability of MALLET [ 15 ] , which includes effcient implementations of both conditional random fields and feature induction .Once the basic tagger was implemented , the remaining effort focused on testing various spelling , contextual and lexicon features on the development data to improve performance .","label":"Background","metadata":{},"score":"49.197258"}{"text":"Indeed , without an heterogenous dataset , we would train our prediction model on a subset of images .The trained prediction model would not be usable in real life .These datasets are primarily used as data for binarization contests and contain a heterogeneous set of images from difficult to easy to binarize .","label":"Background","metadata":{},"score":"49.318066"}{"text":"\" ...Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .","label":"Background","metadata":{},"score":"49.334515"}{"text":"Computing this sum directly is impractical because the number of possible tag sequences is exponential on training instance length .This is not a problem for maximum entropy models [ 13 ] because they model only single - label decisions so their expectations are just over the next label , not whole label sequences .","label":"Background","metadata":{},"score":"49.49611"}{"text":"Weights associated with feature are computed in the neighborhood of the test point by means of a local feature relevance factor .Input .Our input for student engagement with eLearning consists of eight attributes as below .We are assuming that the eLearning system can generate all these signals .","label":"Background","metadata":{},"score":"49.52532"}{"text":"Unlike in V1.01 , the value of C is divided by the number of training examples .So , to get results equivalent to V1.01 , multiply C by the number of training examples .The smaller EPSILON , the longer and the more memory training takes , but the solution is more precise .","label":"Background","metadata":{},"score":"49.535786"}{"text":"Input File Format .Input files of training and test examples follow the usual SVM light input file format .Each line in an example file holds one tag y j i , the feature vector x j i of the associated token , and an optional comment .","label":"Background","metadata":{},"score":"49.570343"}{"text":"- definition of features ( changing window - size ) .FEATURE is used to change the feature sets ( window - size ) for chunking .The default setting is \" F:-2 .T:-2 . \"F:-2 .T:-2 . -1 \" implies that contexts in the blue box are used as feature sets to identify the tag in the red box .","label":"Background","metadata":{},"score":"49.744766"}{"text":"- definition of features ( changing window - size ) .FEATURE is used to change the feature sets ( window - size ) for chunking .The default setting is \" F:-2 .T:-2 . \"F:-2 .T:-2 . -1 \" implies that contexts in the blue box are used as feature sets to identify the tag in the red box .","label":"Background","metadata":{},"score":"49.744766"}{"text":"Consequently , the only remaining component in Eq . 3 x 2 x 1x P R Fig . 1 .Then it is asked whether all P dimensions of input vectors contribute to the output changes or not .In other words , the output is constant in similar geometric representation for perceptron networks .","label":"Background","metadata":{},"score":"49.82921"}{"text":"Results .We employ a diverse feature set containing standard orthographic features combined with expert features in the form of gene and biological term lexicons to achieve a precision of 86.4 % and recall of 78.7 % .An analysis of the contribution of the various features of the model is provided .","label":"Background","metadata":{},"score":"50.065903"}{"text":"Finally , we conduct a multi - lingual evaluation that demonstrates the robustness of the overall structured neural approach , as well as the benefits of the extensions proposed in this work .Our research further demonstrates the breadth of the applicability of neural network methods to dependency parsing , as well as the ease with which new features can be added to neural parsing models .","label":"Background","metadata":{},"score":"50.09961"}{"text":"Use some of the other variations of KNN listed earlier .As I mentioned before , I deliberately introduced large class overlap to produce poor quality result .I am interested in finding out which of these techniques is effective in improving the prediction quality .","label":"Background","metadata":{},"score":"50.10871"}{"text":"This has lead to a higher accuracy .We could further increase the parsing and training speed with a parallel feature extraction and a parallel parsing algorithm .We are convinced that the Hash Kernel and the parallelization can be applied successful to other NLP applications as well such as transition based dependency parsers , phrase structrue parsers , and machine translation . by Massimiliano Ciaramita - Proc . of the 12th International Workshop on Parsing Technologies ( IWPT , 2007 . \" ...","label":"Background","metadata":{},"score":"50.165062"}{"text":"Static Features F : In this figure , the tokens at -2 , -1 , 0 , 1 , and 2 position are used as features .( green box ) .It means that [ beginning positing of token ] is -2 and [ end position of token ] is +2 .","label":"Background","metadata":{},"score":"50.337162"}{"text":"Static Features F : In this figure , the tokens at -2 , -1 , 0 , 1 , and 2 position are used as features .( green box ) .It means that [ beginning positing of token ] is -2 and [ end position of token ] is +2 .","label":"Background","metadata":{},"score":"50.337162"}{"text":"If this configuration occurs numerous times , the binarization can lead to a document image highly degraded by many small black spots between characters .Let $ \\cma$ be the set of degradation components that are not connected to any ink component : .","label":"Background","metadata":{},"score":"50.49636"}{"text":"Latent variable grammars take an observed ( coarse ) treebank and induce more fine - grained grammar categories , that are better suited for modeling the syntax of natural languages .Estimation can be done in a generative or a discriminative framework , and results in the best published parsing accuracies over a wide range of syntactically divergent languages and domains .","label":"Background","metadata":{},"score":"50.62874"}{"text":"2x does not contribute to the output j 2x .On the other hand , in .Page 5 .466 N. Acır and C. Güzeli ?4 Measurement of Sensitivity The purpose of measuring sensitivity is to rank the significance of feature inputs over the entire training set .","label":"Background","metadata":{},"score":"50.64618"}{"text":"A ) System containing no lexicon features and does not use feature induction .B ) Same as A , except feature induction is used .C ) Same as B , except features using the infrequent trigram lexicon are used .","label":"Background","metadata":{},"score":"50.6929"}{"text":"Finally , we present multilingual experiments which show that parsing with hierarchical state - splitting is fast and accurate in multiple languages and domains , even without any language - specific tuning .This work describes systems for detecting semantic categories present in news video .","label":"Background","metadata":{},"score":"50.86474"}{"text":"Shallow Semantic Parsing using Support Vector Machines HLT - NAACL 2004 [ PDF ] YamCha : Yet Another Multipurpose CHunk Annotator .$ I d : index.html,v 1.37 2005/12/24 14:18:58 taku Exp $ ; .Introduction .YamCha is a generic , customizable , and open source text chunker oriented toward a lot of NLP tasks , such as POS tagging , Named Entity Recognition , base NP chunking , and Text Chunking .","label":"Background","metadata":{},"score":"50.864822"}{"text":"The observation list for each token will include a predicate for every regular expression that token matches .Even with this very simple set of predicate - based features , performance on the development data was reasonable ( see Table 2 row A ) .","label":"Background","metadata":{},"score":"50.86817"}{"text":"It is then in this higher dimensional space that a separating hyperplane is constructed to maximize the margin .In the lower dimensional data space , this hyperplane becomes a non - linear separating function . ) )In a further discussion , we assume that some certain inputs carry none or little relationship to the output .","label":"Background","metadata":{},"score":"50.90657"}{"text":"classify.tags will receive lists of tags predicted for the test examples , one tag for each line in test_input . dat .NOTE :The default value for this parameter is unlikely to work well for your particular problem .A good value for C must be selected via cross - validation , ideally exploring values over several orders of magnitude .","label":"Background","metadata":{},"score":"50.933212"}{"text":"FeatureCondProbJoiner joins training and test data distances with the feature class conditional probability of the training data in preparation for the round of processing for KNN prediction .NearestNeighbor makes the class prediction .Additionally , it provides some additional output which reflect the quality of the prediction .","label":"Background","metadata":{},"score":"50.938484"}{"text":"Improving Classification Accuracy .There are several options worth trying to improve the classification accuracy , as below .I will come back with another post when I have tried some of these options .Some of them are general practices followed and independent of the particular classification algorithm .","label":"Background","metadata":{},"score":"51.133224"}{"text":"The method scores candidate features f with their log - likelihood gain : . where F is the current set of model features , the log - likelihood of the data using feature set F and the log - likelihood of the data with the model extended with feature f .","label":"Background","metadata":{},"score":"51.150784"}{"text":"This ' universal ' treebank is made freely available in order to facilitate research on multilingual dependency parsing .We consider the construction of part - of - speech taggers for resource - poor languages .Recently , manually constructed tag dictionaries from Wiktionary and dictionaries projected via bitext have been used as type constraints to overcome the scarcity of annotated data in this setting .","label":"Background","metadata":{},"score":"51.263878"}{"text":"The remaining fields are the feature attributes .Here is some sample output from the final map reduce .The first field is the student ID .The field before the last is the actual class attribute value and the last one is the predicted class attribute value .","label":"Background","metadata":{},"score":"51.33006"}{"text":"Combining multiple grammars that were self - trained on disjoint sets of unlabeled data results in a final test accuracy of 92.5\\% on the WSJ test set and 89.6\\% on our Broadcast News test set .This work shows how to improve state - of - the - art monolingual natural language processing models using unannotated bilingual text .","label":"Background","metadata":{},"score":"51.41442"}{"text":"2 Support Vector Machines Support Vector Machine ( SVM ) is a powerful widely used technique for solving supervised classification problems due to its generalization ability .In essence , SVM classifiers maximize the margin between the training data and the decision boundary , which can be formulated as a quadratic optimization problem in the feature space .","label":"Background","metadata":{},"score":"51.429695"}{"text":"Table 2 also shows the performance of the system without lexicons and feature induction .An examination of system errors on the development data shows that a primary source of error came from properly labeled mentions that are off by one or more tokens .","label":"Background","metadata":{},"score":"51.484123"}{"text":"TAG is a natural number ( i.e. 1 . k ) that identifies the tag y j i that is assigned to the example .As the maximum tag number impacts the memory use , it is recommended to use the numbers 1 to k for a problem with k tags .","label":"Background","metadata":{},"score":"51.48929"}{"text":"The resulting grammars are extremely compact com- pared to other high - performance parsers , yet the parser gives the best published accuracies on several languages , as well as the best generative parsing numbers in English .In addi- tion , we give an associated coarse - to - fine inference scheme which vastly improves inference time with no loss in test set accuracy .","label":"Background","metadata":{},"score":"51.503304"}{"text":"The annotations are produced automatically with statistical models that are specifically adapted to historical text .The corpus will facilitate the study of linguistic trends , especially those related to the evolution of syntax .Syntactic analysis of search queries is important for a variety of information- retrieval tasks ; however , the lack of annotated data makes training query analysis models difficult .","label":"Background","metadata":{},"score":"51.559654"}{"text":"For any test point ( i.e. a student about whom we want to make prediction ) , we find the K nearest neighbors .Each student is represented as a data point in a multi dimensional feature hyper space .Similarity is defined as the distance between data points using Euclidian and some other distance metric .","label":"Background","metadata":{},"score":"51.67491"}{"text":"If the redundancy in training data vectors exist , the proposed technique based on perturbation method for input size reduction allows for building more efficient SVM network models .This can be achieved at a relatively low computational cost and a simple SVM modeling as outlined in the paper .","label":"Background","metadata":{},"score":"51.71679"}{"text":"txtmodel.gz files learned with ONE - VS - REST mode , please try the following command to re - create a correct model file .YamCha is distributed in the hope that it will be useful , but WITHOUT ANY WARRANTY ; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE .","label":"Background","metadata":{},"score":"51.947384"}{"text":"txtmodel.gz files learned with ONE - VS - REST mode , please try the following command to re - create a correct model file .YamCha is distributed in the hope that it will be useful , but WITHOUT ANY WARRANTY ; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE .","label":"Background","metadata":{},"score":"51.947384"}{"text":"Keeping all features in each prediction model would lead to overfitt models .Indeed , some features may not be significant for predicting a specific binarization method .Moreover , even if a feature is highly correlated to the accuracy of an algorithm , it may have a weak contribution to the final prediction model .","label":"Background","metadata":{},"score":"51.95645"}{"text":"Classifier ... \" .This paper describes the DeSRL system , a joined effort of Yahoo !Research Barcelona and Università di Pisa for the CoNLL-2008 Shared Task ( Surdeanu et al . , 2008 ) .The system is characterized by an efficient pipeline of linear complexity components , each carrying out a different sub - task .","label":"Background","metadata":{},"score":"52.060402"}{"text":"training data .BayesianDistribution . finds class conditional probability for training data attributes .training data .BayesianPredictor . finds class conditional probability for training data vector .output of the first and previous job .FeatureCondProbJoiner . joins training test data distance with conditional probability for training data vector .","label":"Background","metadata":{},"score":"52.07888"}{"text":"+ Among the 18 features , most models embed about 7 features .Globally the selected features are consistent with the binarization algorithm : the step wise selection process tends to keep global ( resp .local ) features for global ( resp .","label":"Background","metadata":{},"score":"52.190666"}{"text":"In this definition , we assume that the j th input token is represented by a set o j of predicates that hold of the token or its neighborhood in the input sequence .Each feature function f i specifies an association between the predicates that hold at a position and the state for that position , and the feature weight λ i specifies whether that association should be favored or disfavored .","label":"Background","metadata":{},"score":"52.235565"}{"text":"The results are shown in Table 3 .Entities were correctly identified by the system if and only if all and only the tokens of the entity were correctly detected .Table 3 .Precision and recall numbers for the system on the unseen evaluation data .","label":"Background","metadata":{},"score":"52.28932"}{"text":"A mixture grammar fit with the EM algorithm shows improvement over a single PCFG , both in parsing accuracy and in test data likelihood .We argue that this improvement comes from the learning of specialized grammars that capture non - local correlations .","label":"Background","metadata":{},"score":"52.547836"}{"text":"We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences .Given this fixed network representation , we learn a final layer using the structured perceptron with beam - search decoding .On the Penn Treebank , our parser reaches 94.26 % unlabeled and 92.41 % labeled attachment accuracy , which to our knowledge is the best accuracy on Stanford Dependencies to date .","label":"Background","metadata":{},"score":"52.677483"}{"text":"The following optimization problem corresponds to a model with first - order transitions and zeroth - order emissions , but it should be obvious how it generalizes to higher order models given the discriminant function from above .As the loss function Δ ( y i , y ) , the number of misclassified tags in the sentence is used .","label":"Background","metadata":{},"score":"52.813934"}{"text":"Recently , SVM has been applied to a wide variety of domains such as bioinformatics and pattern recognition [ 3 ] with great success .Different approaches for data reduction were presented for models trained in a supervised way [ 4].","label":"Background","metadata":{},"score":"52.861633"}{"text":"The main aim here is to reduce the original dimension of input vector x .Thus , a smaller SVM network can be modeled with a little or no loss of accuracy .Page 4 .An Application of Support Vector Machine in Bioinformatics 465 In Eq .","label":"Background","metadata":{},"score":"52.866146"}{"text":"After each deletion the criteria is computed .The last strategy consists in testing all the possible combinations .As we have only 18 features , we decided to use the exhaustive strategy .In the following section , these f - scores are called ground truth f - scores .","label":"Background","metadata":{},"score":"52.93936"}{"text":"While this problem has exponentially many constraints , we use the cutting - plane algorithms implemented in SVM struct to solve this problem up to a precision of ε in polynomial time [ Tsochantaridis et al .2004 , Tsochantaridis et al .","label":"Background","metadata":{},"score":"52.942245"}{"text":"So a recursive algorithm is needed to be .Page 6 .( 12 ) and ( 13 ) , where P is the dimension of ) .( 12 ) iS in a descending order , k S knk n .","label":"Background","metadata":{},"score":"52.95457"}{"text":"Features can describe any property of the current token and its relationship to any other token in the sequence .For example , in the part - of - speech tagging experiments reported in [ Joachims et al . , 2009 ] , each token was described by binary features indicating each possible prefix and suffix of the current token , the previous token , and the next token .","label":"Background","metadata":{},"score":"52.982426"}{"text":"Precision is measured by the fraction of predicted gene mentions that are correct and recall by the fraction of actual gene mentions that were identified .Two system results are provided .The first is for the system that contains only features extracted from the training data .","label":"Background","metadata":{},"score":"53.003857"}{"text":"F:-2 . \"F:0 .Note that the expression of \" -2,0,2 \" is different from \" -2 .represents a range between beginning and end position .Call - back function to rewrite features in detail ( require C++ knowledge ) .","label":"Background","metadata":{},"score":"53.056973"}{"text":"F:-2 . \"F:0 .Note that the expression of \" -2,0,2 \" is different from \" -2 .represents a range between beginning and end position .Call - back function to rewrite features in detail ( require C++ knowledge ) .","label":"Background","metadata":{},"score":"53.056973"}{"text":"The same idea , except that instead of linear , we apply a kernel function centered on the neighbor .The nature of the kernel function decides how the influence of a neighbor decays as the distance from the test point grows .","label":"Background","metadata":{},"score":"53.30102"}{"text":"We describe experiments on learning latent variable grammars for various German treebanks , using a language - agnostic statistical approach .In our method , a minimal initial grammar is hierarchically refined using an adaptive split - and - merge EM procedure , giving compact , accurate grammars .","label":"Background","metadata":{},"score":"53.390846"}{"text":"So , the input size is reduced to 13 by selecting the most significant feature components .Then , the SVM classifier is again performed for different C values with determined 13 features .The number of support vectors is 16 which also corresponds to the number of hidden neurons .","label":"Background","metadata":{},"score":"53.45626"}{"text":"We used the MALLET [ 15 ] implementation of CRFs and limited - memory quasi - Newton training .In addition , we relied on MALLET 's feature induction capability [ 16 ] , which is discussed in the next section .","label":"Background","metadata":{},"score":"53.474503"}{"text":"Shallow Semantic Parsing using Support Vector Machines HLT - NAACL 2004 [ PDF ] Comments ( 0 ) .Files changed ( 3 ) .The first part of our work is to identify the degradation within document images .The related works are ancient document image enhancement methods with a first step that often consists of identifying and localizing specific degradations pixels .","label":"Background","metadata":{},"score":"53.546227"}{"text":"A good binarization should preserve the shape of the objects and avoid the creation of unwanted black or white components .Obviously , the location of the degradation pixels is a significant characteristic that can influence the binarization result .b and c ) .","label":"Background","metadata":{},"score":"53.624645"}{"text":"State - of - the - art natural language processing models are anything but compact .Syntactic parsers have huge grammars , machine translation systems have huge transfer tables , and so on across a range of tasks .With such complexity come two challenges .","label":"Background","metadata":{},"score":"53.633194"}{"text":"Information extraction from biomedical text has attracted increasing research interest over the past few years .Several large scale annotated corpora have been developed [ 1 ] or are being developed [ 2 ] to facilitate this process .The first step in most information extraction systems is to identify the named entities that are relevant to the concepts , relations and events described in the text .","label":"Background","metadata":{},"score":"53.813004"}{"text":"Feature set .Feature - based models like CRFs are attractive because they reduce each problem to that of finding a feature set that adequately represents the task at hand .These predicates help the system recognize informative substrings ( e.g. ' homeo ' or ' ase ' ) in words that were not seen in training .","label":"Background","metadata":{},"score":"53.81636"}{"text":"This paper investigates a generative history - based parsing model that synchronises the derivation of non - planar graphs representing semantic dependencies with the derivation of dependency trees representing syntactic structures .To process non - planarity online , the semantic transition - based parser u ... \" .","label":"Background","metadata":{},"score":"53.87434"}{"text":"The weight could also be learnt iteratively from the training data set .Apply class conditional weighting on the vote of a neighbor .This approach being used in this post .Use traditional techniques to find a neighborhood and then apply other distance metric techniques based on class distribution within the neighborhood to recalculate distances .","label":"Background","metadata":{},"score":"53.912636"}{"text":"Meanwhile , Graphics Processor Units ( GPUs ) have become widely available , offering the opportunity to alleviate this bottleneck by exploiting the fine - grained data parallelism found in the CKY algorithm .In this paper , we explore the design space of parallelizing the dynamic programming computations carried out by the CKY algorithm .","label":"Background","metadata":{},"score":"54.10109"}{"text":"Koichi Takeuchi and Nigel Collier ( 2002 ) Use of support vector machines in extended named entity , CoNLL-2002 .Kadri Hacioglu and Wayne Ward ( 2003 )Target Word Detection and Semantic Role Chunking using Support Vector Machines , HLT - NAACL 2003 Short Parpers .","label":"Background","metadata":{},"score":"54.1875"}{"text":"Koichi Takeuchi and Nigel Collier ( 2002 ) Use of support vector machines in extended named entity , CoNLL-2002 .Kadri Hacioglu and Wayne Ward ( 2003 )Target Word Detection and Semantic Role Chunking using Support Vector Machines , HLT - NAACL 2003 Short Parpers .","label":"Background","metadata":{},"score":"54.1875"}{"text":"Consequently , proposed method is successfully performed in a classification example .In result , the best performance is obtained for DCT coefficients with a reduced input size .So , the proposed method can be generally used in pattern classification tasks .","label":"Background","metadata":{},"score":"54.21652"}{"text":"Across various hierarchical encoding schemes and for multiple language pairs , we show speed - ups of up to 50 times over single - pass decoding while improving BLEU score .Moreover , our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram - to - trigram decoder .","label":"Background","metadata":{},"score":"54.266315"}{"text":"The prediction can be improved by using only a pertinent subset of features among the $ 18 $ independent computed features .There are three main ways to carry out a selection .First , the forward strategy consists in computing a criteria ( linked to the $ R^2 $ value ) by adding one feature at a time .","label":"Background","metadata":{},"score":"54.343563"}{"text":"Having calculated the model expectations it is then possible to calculate the gradient of the objective function .This allows for the use of many gradient based optimization algorithms , the most simple of which is gradient ascent : .The gradient provides a search direction and a step size η , which can be chosen statically or be maximized dynamically by a line search .","label":"Background","metadata":{},"score":"54.34687"}{"text":"Even if the global features based on histogram analysis are meaningful , they are not sufficient in that case to choose the best binarization method .The skewness of the second global histogram $ s$ is much higher than that of the the first image , indicating that the background of the second image is easy to separate using a global thresholding method .","label":"Background","metadata":{},"score":"54.44157"}{"text":"Even if the global features based on histogram analysis are meaningful , they are not sufficient in that case to choose the best binarization method .The skewness of the second global histogram $ s$ is much higher than that of the the first image , indicating that the background of the second image is easy to separate using a global thresholding method .","label":"Background","metadata":{},"score":"54.44157"}{"text":"Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .The tenth CoNLL ( CoNLL - X ) saw a shared task on Multilingual Dependency Parsing .","label":"Background","metadata":{},"score":"54.44615"}{"text":"We introduce an approach based on perturbation method for input dimension reduction in Support Vector Machine ( SVM ) classifiers .If there exists redundant data components in training data set , they can be discarded by analyzing the total disturbance of the SVM output corresponding to the perturbed inputs .","label":"Background","metadata":{},"score":"54.508617"}{"text":"Modify the decision threshold .If the classification is based on probability or some score , the decision boundary does not have to be at the halfway point .With cost based classification , we assign misclassification cost and then choose a classification that minimizes the cost .","label":"Background","metadata":{},"score":"54.61975"}{"text":"It appears that it is relatively easy to find pieces of text mentioning genes , but much harder to determine the exact boundaries of that mention .This hurts the system performance significantly since the scoring metric requires exact matches .However , entity tagging primarily exists to give some structure to text for higher level information extraction systems such as relation detection , fact generation and question answering .","label":"Background","metadata":{},"score":"54.654793"}{"text":"We explored a single stage approach to opinion mining , retrieving opinionated documents ranked with a special ranking function which exploits an index enriched with opinion tags .A set of subjective words are used as tags for identifying opinionated sentences .","label":"Background","metadata":{},"score":"54.74154"}{"text":"We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process ( HDP ) .Our HDP - PCFG model allows the complexity of the grammar to grow as more training data is available .In addition to presenting a fully Bayesian model for the PCFG , we also develop an efficient variational inference procedure .","label":"Background","metadata":{},"score":"54.8179"}{"text":"We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system .We use a corpus of weakly - labeled reference reorderings to guide parser training .Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress .","label":"Background","metadata":{},"score":"54.842506"}{"text":"In this paper we propose parallel multi - objective optimization approaches to cope with high - dimensional feature selection problems .Several parallel multi - objective evolutionary alternatives are proposed , and experimentally evaluated by using some synthetic and BCI ( Brain - Computer Interface ) benchmarks .","label":"Background","metadata":{},"score":"54.859467"}{"text":"See changes introduced by SVM struct V3.00 for a general properties of the new structural SVM training algorithm .New IO routines that are faster for reading large data and model files .V1.00 - 1.01 .References .T. Joachims , T. Finley , Chun - Nam Yu , Cutting - Plane Training of Structural SVMs , Machine Learning Journal , 77(1):27 - 59 , 2009 .","label":"Background","metadata":{},"score":"54.954155"}{"text":"To avoid degeneracy when some features are perfectly correlated and to reduce overfitting for rarely occurring features , we penalize the likelihood with a spherical Gaussian prior over feature weights [ 9 ] : .The variance hyper - parameter σ 2 determines the modeling trade - off between fitting exactly the observed feature frequencies and the squared norm of the weight vector .","label":"Background","metadata":{},"score":"55.015675"}{"text":"For irrelevant features which provide little information to the classifier , the output produces a small value of sensitivity measure which indicates that the output is insensitive to those features .On the other hand , the output produces a large value of sensitivity measure for important features .","label":"Background","metadata":{},"score":"55.04953"}{"text":"I. Tsochantaridis , T. Joachims , T. Hofmann , and Y. Altun , Large Margin Methods for Structured and Interdependent Output Variables , Journal of Machine Learning Research ( JMLR ) , 6(Sep):1453 - 1484 , 2005 .[ PDF ] .","label":"Background","metadata":{},"score":"55.43697"}{"text":"To identify the boundary between sentences , just put an empty line ( or just put ' EOS ' ) .You can give as many columns as you like , however the number of columns must be fixed through all tokens .","label":"Background","metadata":{},"score":"55.52236"}{"text":"To identify the boundary between sentences , just put an empty line ( or just put ' EOS ' ) .You can give as many columns as you like , however the number of columns must be fixed through all tokens .","label":"Background","metadata":{},"score":"55.52236"}{"text":"YamCha : Yet Another Multipurpose CHunk Annotator .$ I d : index.html,v 1.37 2005/12/24 14:18:58 taku Exp $ ; .Introduction .YamCha is a generic , customizable , and open source text chunker oriented toward a lot of NLP tasks , such as POS tagging , Named Entity Recognition , base NP chunking , and Text Chunking .","label":"Background","metadata":{},"score":"55.58281"}{"text":"The theoretical analysis demonstrates its advantages in efficiency and precision over other smooth functions .PWSSVM is solved using the fast Newton - Armijo algorithm .Experimental results are given to show the training speed and classification performance of our approach .","label":"Background","metadata":{},"score":"55.624588"}{"text":"Transition - based dependency parsers are often forced to make attachment decisions at a point when only partial information about the relevant graph configuration is available .In this paper , we describe a model that takes into account complete structures as they become available to rescore the elements of a beam , combining the advantages of transition - based and graph - based approaches .","label":"Background","metadata":{},"score":"55.78413"}{"text":"The log - likelihood function of conditional random fields , which generalizes the well - known case of logistic regression , is easily seen to be concave [ 6 ] , as is the penalized likelihood ( 3 ) .To maximize the penalized log - likelihood , we compute its partial derivatives with respect to the weights : . is the empirical feature count of feature f i and E [ f i ] is the model expectation of feature f i .","label":"Background","metadata":{},"score":"55.862938"}{"text":"% ls case_study .log : log of training case_study .model : model file ( binary , architecture dependent ) case_study.txtmodel.gz : model file ( text , architecture independent ) case_study .se : support examples case_study .svmdata : training data for SVMs .","label":"Background","metadata":{},"score":"56.035416"}{"text":"% ls case_study .log : log of training case_study .model : model file ( binary , architecture dependent ) case_study.txtmodel.gz : model file ( text , architecture independent ) case_study .se : support examples case_study .svmdata : training data for SVMs .","label":"Background","metadata":{},"score":"56.035416"}{"text":"KNN is a popular and reliable classification technique .The input features consist of various signals based on the engagement level of the student with the eLearning system and the student 's performance so far .With the identification of students who are likely to drop out , the instructors can be more vigilant and intervene to prevent attrition .","label":"Background","metadata":{},"score":"56.048065"}{"text":"Computers Math .Applic .Belue , L.M. and Bauer , K.W.- JR . : Determining input features for multilayer perceptrons .Neurocomputing .Zurada , J.M. , Malinowski , A. , Usui , S. : Perturbation Method for deleting redundant inputs of perceptron networks .","label":"Background","metadata":{},"score":"56.061287"}{"text":"Wrapping Up .This is still work in progress , As I mentioned I will try some of the techniques listed above to improves the prediction accuracy .I have prepared the data set in a way that makes the classification extremely challenging .","label":"Background","metadata":{},"score":"56.115036"}{"text":"If the 1st token must be B tag and the 2nd token must be selected only from B and I , you give yamcha the following test data : .Rockwell NNP B International NNP B I .Generally speaking , in the partial chunking mode , candidates are listed instead of last column .","label":"Background","metadata":{},"score":"56.15866"}{"text":"If the 1st token must be B tag and the 2nd token must be selected only from B and I , you give yamcha the following test data : .Rockwell NNP B International NNP B I .Generally speaking , in the partial chunking mode , candidates are listed instead of last column .","label":"Background","metadata":{},"score":"56.15866"}{"text":"The system already has tens of thousands of singleton features , making it infeasible to create all such conjunctions .Even if it were computationally feasible to train a model with all conjunctions , it would be difficult to gather sufficient statistics on them since most conjunctions occur rarely if ever .","label":"Background","metadata":{},"score":"56.170677"}{"text":"Based on this i ... \" .Abstract .This paper explores the idea that non - projective dependency parsing can be conceived as the outcome of two interleaved processes , one that sorts the words of a sentence into a canonical order , and one that performs strictly projective dependency parsing on the sorted input .","label":"Background","metadata":{},"score":"56.19049"}{"text":"Often in practice , a separating hyperplane does not exist .Eq .( 2 ) forces a rescaling on ) , ( b w so w [ 8].Maximizing the iξ N , .Page 3 .464 N. Acır and C. Güzeli ?","label":"Background","metadata":{},"score":"56.27912"}{"text":"Here is a summary of pros and cons of KNN .Does not work well for high dimensional data .Works well even when different class populations are not linearly separable and decision hyper plane is complex .There are various improvisations of the basic KNN .","label":"Background","metadata":{},"score":"56.32808"}{"text":"470 N. Acır and C. Güzeli ?For second feature set , involving DWT approximation coefficients , the same training and feature selection implementations are repeated without any change .After , the C value initially set to 100 , the algorithm is applied to the set .","label":"Background","metadata":{},"score":"56.389446"}{"text":"Moreover , the value of $ \\mA$ is high , meaning that many components do not touch text pixels .This type of degradation is likely to produce binarization errors with windows based methods such as Sauvola 's method .+ The proposed features characterize three different aspects of degradation : intensity , quantity and location .","label":"Background","metadata":{},"score":"56.612415"}{"text":"The compatibility of binary models is broken .You need to recompile model files as follows : . % yamcha - mkmodel foo.txtmodel.gz foo.model .Text model files will be found in your working directories .Support PKE ( Polynomial Kernel Extended ) which makes chunking ( classification ) speed significantly faster than the original yamcha .","label":"Background","metadata":{},"score":"56.675674"}{"text":"The compatibility of binary models is broken .You need to recompile model files as follows : . % yamcha - mkmodel foo.txtmodel.gz foo.model .Text model files will be found in your working directories .Support PKE ( Polynomial Kernel Extended ) which makes chunking ( classification ) speed significantly faster than the original yamcha .","label":"Background","metadata":{},"score":"56.675674"}{"text":"T:-3 . -1 \" to the FEATURE parameter .T:-3 . -1 \" train .The expression \" -2 . .2\" can be also expressed as \" -2,-1,0,-1,2 \" .In addition , if the beginning position and end position are same , you can omit the end position .","label":"Background","metadata":{},"score":"56.73065"}{"text":"T:-3 . -1 \" to the FEATURE parameter .T:-3 . -1 \" train .The expression \" -2 . .2\" can be also expressed as \" -2,-1,0,-1,2 \" .In addition , if the beginning position and end position are same , you can omit the end position .","label":"Background","metadata":{},"score":"56.73065"}{"text":"-% Depending of the algorithm that we aim to predict , all these measures may not be use on the same prediction model .A sub - selection of measures is necessary .This process is done in an automated way witch is presented in following section .","label":"Background","metadata":{},"score":"56.905434"}{"text":"Moreover , the value of $ \\mA$ is high , meaning that many components do not touch text pixels .This type of degradation is likely to produce binarization errors with windows based methods such as Sauvola 's method .-The proposed features characterize three different aspects of degradation : intensity , quantity and location .","label":"Background","metadata":{},"score":"56.932056"}{"text":"As I mentioned , feature analysis should be have been done on the input feature variables .It would have benefited us in two ways .This entropy and mutual information based analysis is available in avenir .Here is an earlier post on the topic .","label":"Background","metadata":{},"score":"56.948025"}{"text":"+ % Depending of the algorithm that we aim to predict , all these measures may not be use on the same prediction model .A sub - selection of measures is necessary .This process is done in an automated way witch is presented in following section .","label":"Background","metadata":{},"score":"56.960075"}{"text":"E ) Same as B , except features using the gene lexicon are used .F ) Same as B , except features using all lexicons are used .A straightforward method of integrating these post - processing steps into our model is to create predicates indicating whether a token occurs in one of the ABGene lexicons .","label":"Background","metadata":{},"score":"57.10773"}{"text":"We use only binary features , for instance : .The weight λ i for each feature should ideally be highly positive if the feature tends to be on for the correct labeling , highly negative if the feature tends to be off for the correct labeling , and around zero if the feature is uninformative .","label":"Background","metadata":{},"score":"57.174927"}{"text":"Testing the SVM in the system shows a sensitivity of 96.0 % , specificity of 92.1 % and accuracy of 93.9 % .The effect of the input size , determined at each iterations of adaptive feature selection algorithm , on the accuracy rate for Feature Set II .","label":"Background","metadata":{},"score":"57.247757"}{"text":"However , the problem can be somewhat alleviated by making certain independence assumptions on the parameters as well as only including statistics on positions in the sequence that are mislabeled by the current parameter settings .McCallum [ 16 ] describes the procedure in more detail .","label":"Background","metadata":{},"score":"57.25379"}{"text":"The penalty for a recognition failure is often small : if two con- figurations are confused , they are often similar to each other , and the illusion works well enough , for instance , to drive a graphics animation of the moving hand .","label":"Background","metadata":{},"score":"57.496384"}{"text":"In particular , we solve the one - slack reformulation of the training problems using SVM struct V3.10 [ Joachims et al .2009 ] , which makes this version of SVM hmm substantially faster than previous versions .See [ Joachims et al .","label":"Background","metadata":{},"score":"57.860146"}{"text":"The predicate set o j used to represent the j th input token picks out useful properties of the token and its context .The tag sequence uses the possible tags B - GENE , I - GENE and O , representing the beginning , inside and outside of a gene mention respectively .","label":"Background","metadata":{},"score":"57.984985"}{"text":"Table 2 rows C through F summarizes the effect of adding these lexicons to the system .Rows C through F assume the use of feature induction , which is explored in the next section .Feature induction .So far we have only described features over a single predicate .","label":"Background","metadata":{},"score":"58.120846"}{"text":"Input to SameTypeSimilarity consists of two data sets , one for training and the other for validation .They are generated by a python script .Each input attribute is assumed to have normal distribution with specified mean and standard deviation .","label":"Background","metadata":{},"score":"58.187416"}{"text":"The following features are meant to capture , the possible creation of unwanted black components , and the possible deformation of the characters through the binarization process . %Some local binarization algorithms are sensitive to the proximity between degraded pixels and ink pixels .","label":"Background","metadata":{},"score":"58.261288"}{"text":"McCallum A , Freitag D , Pereira F : Maximum entropy Markov models for information extraction and segmentation .Proceedings of ICML 2000 .Kudo T , Matsumoto Y : Chunking with Support Vector Machines .Proc NAACL 2001 ACL 2001 .","label":"Background","metadata":{},"score":"58.26951"}{"text":"Participants were to build a single parsing system that is robust to domain changes and can handle noisy text that is commonly encountered on the web .There was a constituency and a dependency parsing track and 11 sites submitted a total of 20 systems .","label":"Background","metadata":{},"score":"58.582718"}{"text":"Moreover , a p - value is computed for each selected feature indicating its significance : a low p - value leads to reject the hypothesis that the selected feature is not significant ( null hypothesis ) .At this step , there is no automatic rule to decide whether a model is valid or not .","label":"Background","metadata":{},"score":"58.610477"}{"text":"SVM_PARAM is used to change the training condition of SVMs .Default setting is \" -t1 -d2 -c1 \" , which means the 2nd degree of polynomial kernel and 1 slack variable are used .Note that YamCha only supports polynomial kernels .","label":"Background","metadata":{},"score":"58.69462"}{"text":"SVM_PARAM is used to change the training condition of SVMs .Default setting is \" -t1 -d2 -c1 \" , which means the 2nd degree of polynomial kernel and 1 slack variable are used .Note that YamCha only supports polynomial kernels .","label":"Background","metadata":{},"score":"58.69462"}{"text":"( a ) the original grayscale document image .We aim to compute the following global statistic features of the grayscale histogram : mean , variance and skewness .The skewness quantifies the asymmetry of the histogram .For example , a negative skewness indicates that the distribution of pixels gray - levels has relatively few low values .","label":"Background","metadata":{},"score":"58.958897"}{"text":"-Let $ S$ be a set of pixels .We denote the set of the 4-connected components of $ S$ by $ CC(S)$. + Let $ S$ be a set of pixels .We denote the set of the 4-connected components of $ S$ by $ CC(S)$. a ) , the original character will not be altered by the binarization process .","label":"Background","metadata":{},"score":"58.959938"}{"text":"Tanl pipelines are data driven , i.e. each stage pulls data from the preceding stage and transforms them for use by the next stage .Since data is processed as s ... \" .Tanl ( Natural Language Text Analytics ) is a suite of tools for text analytics based on the software architecture paradigm of data pipelines .","label":"Background","metadata":{},"score":"59.111973"}{"text":"Despite the much simplified training process , our acoustic model achieves state - of - the - art results on phone classification ( where it outperforms almost all other methods ) and competitive performance on phone recognition ( where it outperforms standard CD triphone / subphone / GMM approaches ) .","label":"Background","metadata":{},"score":"59.24282"}{"text":"Class Conditional Probability Weighting .I have used a technique called class conditional weighting ( CCW ) to combat class imbalance and overlap .Details are available here .The vote by a neighbor is weighted by class conditional probability of the data point .","label":"Background","metadata":{},"score":"59.27909"}{"text":"Kluwer Academic Publishers , Boston , pp .55 - 85 , ( 1998 ) .Acır , N. , Öztura , ? , Kuntalp , M. , Baklan , B. , Güzeli ? , C. : Automatic spike detection in EEG by a two stage procedure based on ANNs .","label":"Background","metadata":{},"score":"59.291916"}{"text":"Please copy the Makefile to the local working directory .% yamcha - config --libexecdir /usr / local / libexec / yamcha % cp /usr / local / libexec / yamcha / Makefile .There are two mandatory parameters for training .","label":"Background","metadata":{},"score":"59.450836"}{"text":"Please copy the Makefile to the local working directory .% yamcha - config --libexecdir /usr / local / libexec / yamcha % cp /usr / local / libexec / yamcha / Makefile .There are two mandatory parameters for training .","label":"Background","metadata":{},"score":"59.450836"}{"text":"Introduction .SVM hmm is an implementation of structural SVMs for sequence tagging [ Altun et .al , 2003 ] ( e.g. part - of - speech tagging , named - entity recognition , motif finding ) using the training algorithm described in [ Tsochantaridis et al .","label":"Background","metadata":{},"score":"59.69751"}{"text":"Conclusion .Overall , our experiments show that CRF models with carefully designed features can identify gene and protein mentions with fairly high accuracy even without features containing domain specific knowledge .However , such features , which in our case take the form of lexicon membership , can lead to improved system performance .","label":"Background","metadata":{},"score":"59.707336"}{"text":"The goal is improve the correctness of prediction .Try different values for neighborhood count ( K ) .Lower values will cause large error due to variance .Higher values will cause large error due to bias .Use inverse distance to neighbor weighting in neighbor 's vote .","label":"Background","metadata":{},"score":"59.793827"}{"text":"The best accuracies were in the 80 - 84\\% range for F1 and LAS ; even part - of - speech accuracies were just above 90\\% .Coarse - to - fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy .","label":"Background","metadata":{},"score":"59.96228"}{"text":"Our generative self - trained grammars reach F scores of 91.6 on the WSJ test set and surpass even discriminative reranking systems without self - training .Additionally , we show that multiple self - trained grammars can be combined in a product model to achieve even higher accuracy .","label":"Background","metadata":{},"score":"60.02289"}{"text":"local ) features for global ( resp .local ) binarization algorithms .We also note that $ \\mS$ is never selected by any prediction model .Indeed , the binarization accuracy is measured at the pixel level ( f - score ) .","label":"Background","metadata":{},"score":"60.032715"}{"text":"To facilitate future research in unsupervised induction of syntactic structure and to standardize best - practices , we propose a tagset that consists of twelve universal part - of - speech categories .In addition to the tagset , we develop a mapping from 25 different treebank tagsets to this universal set .","label":"Background","metadata":{},"score":"60.108147"}{"text":"19 EEG records which contain 127 EPs and 184 non - EPs are used to train the SVM while a different set of 10 records with 76 EPs and 89 non - EPs are reserved for testing purposes .The patterns are aimed to be separated from each other by a nonlinear SVM that would function as a classifier whose input is a vector of 70 consecutive sample values obtained from each peak .","label":"Background","metadata":{},"score":"60.144993"}{"text":"Primary acoustic , speech , and vision systems were trained to discriminate instances of the categories .Higher - level systems exploited correlations among the categories , incorporated sequential context , and combined the joint evidence from the three information sources .","label":"Background","metadata":{},"score":"60.334084"}{"text":"( 7 ) can be considered as sensitivity coefficients . ik x , the support vectors sx s α .All these techniques can be used to obtain the relative significance of each input feature to the output .The calculated sensitivity values are used to rank the relative significance of input features .","label":"Background","metadata":{},"score":"60.423237"}{"text":"We attempt to solve the primal programming problems of SVM by converting them into smooth unconstrained minimization problems .In this paper , a new twice continuously differentiable piecewise - smooth function is proposed to approximate the plus function , and it issues a piecewise - smooth support vector machine ( PWSSVM ) .","label":"Background","metadata":{},"score":"60.804695"}{"text":"This work was supported in part by NSF grant ITR 0205448 .Authors ' Affiliations .Department of Computer and Information Science , University of Pennsylvania , Levine Hall .References .Ohta T , Tateisi Y , Kim J , Lee S , Tsujii J : GENIA corpus : A semantically annotated corpus in molecular biology domain .","label":"Background","metadata":{},"score":"60.845055"}{"text":"This problem occurs when scanning documents with large bookbindings .The authors propose a line - by - line thresholding to localize the boundary of the dark area near the bookbinding .The recto pixels corresponding to the verso ones can then be labelled as bleed - through pixels .","label":"Background","metadata":{},"score":"61.068077"}{"text":"The most common strategy uses the swap transition ( Nivre , 2009 ; Nivre et al . , 2009 ) , an alternative solution uses two planes and a switch transition to switch between the two planes ( G .. \" ... Abstract .","label":"Background","metadata":{},"score":"61.118515"}{"text":"You simply use the command : . % yamcha -m case_study .NNP I I 's POS B B Tulsa NNP I I unit NN I I said VBD O O ... .The last column is given ( estimated ) tag .","label":"Background","metadata":{},"score":"61.208885"}{"text":"You simply use the command : . % yamcha -m case_study .NNP I I 's POS B B Tulsa NNP I I unit NN I I said VBD O O ... .The last column is given ( estimated ) tag .","label":"Background","metadata":{},"score":"61.208885"}{"text":"Proceedings of Pacific Symposium on Biocomputing 2003 .Kazama J , Makino T , Ohta Y , Tsujii J : Tuning Support Vector Machines for Biomedical Named Entity Recognition .Proceedings of Natural Language Processing in the Biomedical Domain , ACL 2002 .","label":"Background","metadata":{},"score":"61.35878"}{"text":"In contrast , the data carrying conflicting information decrease the performance of the system .Our aim in this paper is to exploit the redundancy in input data vector for improving the efficiency of the SVM classification algorithm .To determine which input data components are necessary for a satisfactory neural network performance , a measure known as sensitivity was introduced in [ 5].","label":"Background","metadata":{},"score":"61.399834"}{"text":"Across eight European languages , our approach results in an average absolute improvement of 10.4 % over a state - of - the - art baseline , and 16.7 % over vanilla hidden Markov models induced with the Expectation Maximization algorithm .","label":"Background","metadata":{},"score":"61.44221"}{"text":"Behavior Control : Finally we show how all these elements can be incorporated into a goal keeping robot .We develop simple behaviors that can be used in a layered architecture and enable the robot to block most balls that are being shot at the goal .","label":"Background","metadata":{},"score":"61.5075"}{"text":"However , the smaller η is , the slower the algorithm will converge .Furthermore , gradient ascent does not take into account the curvature of the function , which also fundamentally slows its convergence speed .In order to consider the function 's curvature , we require second order derivative information in the form of a Hessian .","label":"Background","metadata":{},"score":"61.594826"}{"text":"The new Viewer adds three features for more powerful search : wildcards , morphological inflections , and capitalization .These additions allow the discovery of patterns that were previously difficult to find and further facilitate the study of linguistic trends in printed text .","label":"Background","metadata":{},"score":"61.657837"}{"text":"The results of application show that it performs very good for classification problems in bioinformatics .References 1 .Vapnik , V. : Statistical Learning Theory .John Wiley , NY , ( 1998 ) .Vapnik V. : The support vector method of function estimation .","label":"Background","metadata":{},"score":"61.685146"}{"text":"In addition , a token consists of multiple ( but fixed - numbers ) columns .The definition of tokens depends on tasks , however , in most of typical cases , they simply correspond to words .Each token must be represented in one line , with the columns separated by white space ( spaces or tabular characters ) .","label":"Background","metadata":{},"score":"61.903374"}{"text":"In addition , a token consists of multiple ( but fixed - numbers ) columns .The definition of tokens depends on tasks , however , in most of typical cases , they simply correspond to words .Each token must be represented in one line , with the columns separated by white space ( spaces or tabular characters ) .","label":"Background","metadata":{},"score":"61.903374"}{"text":"An Application of Support Vector Machine in Bioinformatics 463 This paper focuses on a perturbation method , for reduction of input dimension for SVM providing continuous mapping .Perturbation method for discarding redundant inputs of perceptron networks was introduced in [ 7 ] which highly motivated us to apply the perturbation method onto the SVM classification .","label":"Background","metadata":{},"score":"61.911694"}{"text":"The list should not be considered as exhaustive .Some of the of the techniques towards the end of the list have their origins in the academic world and they may not be well established in the business world .Make the vote by a neighbor inversely proportionally to it 's distance or square of distance from the test point .","label":"Background","metadata":{},"score":"62.044792"}{"text":"Please use -m SIZE option to increase the memory for training if possible .This option drastically reduce the computational cost and time .Here is an example of assigning 512 Mb memory to the SVMs : .Output format .The -V option sets verbose mode , where yamcha outputs tag and scores of all candidates .","label":"Background","metadata":{},"score":"62.236702"}{"text":"Please use -m SIZE option to increase the memory for training if possible .This option drastically reduce the computational cost and time .Here is an example of assigning 512 Mb memory to the SVMs : .Output format .The -V option sets verbose mode , where yamcha outputs tag and scores of all candidates .","label":"Background","metadata":{},"score":"62.236702"}{"text":"φ trans ( y i - j , ... , y i ) is an indicator vector that has exactly one entry set to 1 corresponding to the sequence y i - j , ... , y i .Note that in contrast to a conventional HMM , the observations x 1 ... x l can naturally be expressed as feature vectors , not just as atomic tokens .","label":"Background","metadata":{},"score":"62.310066"}{"text":"F : and T : should be written in the following format : .F:[beginning pos . of token].[ end pos . of token]:[beginning pos . of column].[ end pos . of column ] T:[beginning pos . of tag].","label":"Background","metadata":{},"score":"62.344696"}{"text":"F : and T : should be written in the following format : .F:[beginning pos . of token].[ end pos . of token]:[beginning pos . of column].[ end pos . of column ] T:[beginning pos . of tag].","label":"Background","metadata":{},"score":"62.344696"}{"text":"That is why features such as $ \\mIInk$ , $ \\mu$ and $ v$ are significant and have such low p - values .Affiliated with .Affiliated with .Abstract .Background .We present a model for tagging gene and protein mentions from text using the probabilistic sequence tagging framework of conditional random fields ( CRFs ) .","label":"Background","metadata":{},"score":"62.50331"}{"text":"the vector representing the DCT coefficients of each pattern .Page 8 .An Application of Support Vector Machine in Bioinformatics 469 The feature extraction of second feature set involves DWT coefficients of data .Feature vectors are formed using Daubechies-2 wavelet [ 13].","label":"Background","metadata":{},"score":"62.53961"}{"text":"The SVM is trained until finding the best result in accordance with the algorithm presented in Section 4 .The best result is obtained for 18 coefficients .Thus , the input dimension is reduced to 18 by selecting the best feature components .","label":"Background","metadata":{},"score":"62.624832"}{"text":"Third column is true answer tag associated with the word ( I , O or B ) .The chunks are represented using IOB2 model .The sentences are presumed to be separated by one blank line .First of all , run yamcha - config with --libexecdir option .","label":"Background","metadata":{},"score":"62.74607"}{"text":"Third column is true answer tag associated with the word ( I , O or B ) .The chunks are represented using IOB2 model .The sentences are presumed to be separated by one blank line .First of all , run yamcha - config with --libexecdir option .","label":"Background","metadata":{},"score":"62.74607"}{"text":"Comments ( 0 ) .Files changed ( 1 ) .To estimate the f - score of a binarization algorithm , we automatically build a prediction model based on the most significant features among the $ 18$. The linear regression models , as an hyperplane , the relationship between the features and the groundtruthed f - scores .","label":"Background","metadata":{},"score":"63.08055"}{"text":"MODEL :Prefix name of model file(s ) .Here is an example in which CORPUS is set as ' train.data ' and MODEL is set as ' case_study ' .T:-2 . data perl -w /usr / local / libexec / yamcha / mksvmdata case_study . omit .","label":"Background","metadata":{},"score":"63.092026"}{"text":"MODEL :Prefix name of model file(s ) .Here is an example in which CORPUS is set as ' train.data ' and MODEL is set as ' case_study ' .T:-2 . data perl -w /usr / local / libexec / yamcha / mksvmdata case_study . omit .","label":"Background","metadata":{},"score":"63.092026"}{"text":"Hiroyasu Yamada , Taku Kudo , Yuji Matsumoto ( 2002 )Japanese Named Entity Extraction using Support Vector Machine ' ' , Transactions of IPSJ , Vol .43 , No . 1 , pages 44 - 53 , 2002 .( in Japanese ) .","label":"Background","metadata":{},"score":"63.09279"}{"text":"Hiroyasu Yamada , Taku Kudo , Yuji Matsumoto ( 2002 )Japanese Named Entity Extraction using Support Vector Machine ' ' , Transactions of IPSJ , Vol .43 , No . 1 , pages 44 - 53 , 2002 .( in Japanese ) .","label":"Background","metadata":{},"score":"63.09279"}{"text":"For Otsu 's prediction model , we can explain the feature selection by the fact that Otsu 's binarization method is based on global thresholding .That is why features such as $ \\mIInk$ , $ \\mu$ and $ v$ are significant and have such low p - values .","label":"Background","metadata":{},"score":"63.107327"}{"text":"Based on the score , the top n attributes could be selected for inclusion in the prediction model .Class Imbalance and Overlap .These are some thorny issues faced by any machine learning classification technique .We have imbalance in data when the distribution of data conditioned on the class attribute value is highly skewed .","label":"Background","metadata":{},"score":"63.586723"}{"text":"+ We denote the mean of the global histogram by $ \\mu$ , its variance by $ v$ , and its skewness by $ s$. A good value for the skewness is a high negative value : the left tail of the histogram is longer , the intensities are concentrated on the right and the histogram has relatively few gray values .","label":"Background","metadata":{},"score":"63.73423"}{"text":"The program is free for scientific use .Please contact me , if you are planning to use the software for commercial purposes .The software must not be further distributed without prior permission of the author .The implementation was developed on Linux with gcc , but compiles also on Cygwin , Windows ( using the MinGW option of Cygwin ) , and Mac ( after small modifications , see FAQ ) .","label":"Background","metadata":{},"score":"64.00569"}{"text":"On full - scale treebank parsing experiments , the discriminative latent models outperform both the comparable generative latent models as well as the discriminative non - latent baselines .We present a maximally streamlined approach to learning HMM - based acoustic models for automatic speech recognition .","label":"Background","metadata":{},"score":"64.10091"}{"text":"For more detail , see example / example . cpp .Multi - class methods .MULTI_CLASS is used to change the strategy for the multi - class problem .The default setting is pair wise method .If \" 2 \" is specified , ' one vs rest ' is used .","label":"Background","metadata":{},"score":"64.1396"}{"text":"For more detail , see example / example . cpp .Multi - class methods .MULTI_CLASS is used to change the strategy for the multi - class problem .The default setting is pair wise method .If \" 2 \" is specified , ' one vs rest ' is used .","label":"Background","metadata":{},"score":"64.1396"}{"text":"The authors propose a line - by - line thresholding to localize the boundary of the dark area near the bookbinding .The first step of the proposed method is to locate areas suffering from illumination defects : a line - by - line thresholding is used to localize the boundary of the dark area near the bookbinding .","label":"Background","metadata":{},"score":"64.16261"}{"text":"For example ase occurring at the end of a word is much more informative then just knowing ase is contained in the word ( i.e laser vs. kinase ) .We include predicates that indicate whether the current token occurs within brackets or inside quotations .","label":"Background","metadata":{},"score":"64.30174"}{"text":"-By applying linear regression to the training set , we compute a new prediction function with its associated $ R^2$. We apply this new predictive function to the validation set .The obtained predicted f - scores are compared with the ground truth f - scores using a linear regression that provides the slope coefficient $ \\alpha$. + We apply this new predictive function to the validation set .","label":"Background","metadata":{},"score":"64.882744"}{"text":"% yamcha - mkmodel -e foo.txtmodel.gz foo.model .Note that PKE is an approximation of the original SVMs .Please see here for details .Fix FATAL error on compiling ONE - VS - REST model file .With this bug , YamCha would output an internal reserved tag ( _ _","label":"Background","metadata":{},"score":"64.949814"}{"text":"% yamcha - mkmodel -e foo.txtmodel.gz foo.model .Note that PKE is an approximation of the original SVMs .Please see here for details .Fix FATAL error on compiling ONE - VS - REST model file .With this bug , YamCha would output an internal reserved tag ( _ _","label":"Background","metadata":{},"score":"64.949814"}{"text":"The results show a significant improvement in precision for both topic relevance and opinion relevance . ...Results We performed a few experiments using the TREC 2006 Blog topics n .. \" ...Transition - based dependency parsers are often forced to make attachment decisions at a point when only partial information about the relevant graph configuration is available .","label":"Background","metadata":{},"score":"65.23726"}{"text":"Computers fail to track these in fast video , but sleight of hand fools humans as well : what happens too quickly we just can not see .We show a 3D tracker for these types of motions that relies on the recognition of familiar configurations in 2D images ( classification ) , and fills the gaps in - between ( interpolation ) .","label":"Background","metadata":{},"score":"65.390305"}{"text":"Bioinformatics 2002 . , 18(8 ) : .Lafferty J , McCallum A , Pereira F : Conditional random fields : Probabilistic models for segmenting and labeling sequence data .Proceedings of ICML 2001 .McCallum A : Effciently inducing features of conditional random fields .","label":"Background","metadata":{},"score":"65.730446"}{"text":"ABGene is a hybrid model that uses a statistical part - of - speech tagger to identify candidate genes by labeling them with a special part - of - speech ' GENE ' .Once the candidate genes are found a series of post processing rules are initiated to improve the results .","label":"Background","metadata":{},"score":"65.78813"}{"text":"..But first of all , we need to define the notion of a dependency graph a little more precisely . \" ...This paper describes the DeSRL system , a joined effort of Yahoo !Research Barcelona and Università di Pisa for the CoNLL-2008 Shared Task ( Surdeanu et al . , 2008 ) .","label":"Background","metadata":{},"score":"66.00453"}{"text":"We present here a method for identifying gene and protein mentions in text with conditional random fields ( CRFs ) [ 6 ] , which are discussed in the next section .Narayanaswamy et al .[ 3 ] suggest that rule - based systems make decisions on sets of textual indicator features and that these features may easily be exploited by supervised statistical approaches .","label":"Background","metadata":{},"score":"66.052185"}{"text":"We also note that $ \\mS$ is never selected by any prediction model .Indeed , the binarization accuracy is measured at the pixel level ( f - score ) .With this accuracy measure , the feature $ \\mSG$ becomes more significant than $ \\mS$ , which may not have been the case with another evaluation measure .","label":"Background","metadata":{},"score":"66.18033"}{"text":"It can be interpreted as a correlation between the ground truth and the prediction .Moreover , a p - value is computed for each selected feature indicating its significance : a low p - value leads to reject the hypothesis that the selected feature is not significant ( null hypothesis ) .","label":"Background","metadata":{},"score":"66.19408"}{"text":"/svm_hmm_classify example7/gettysburg_address.dat declaration.model test.outtags .The fraction of misclassified tags ( i.e. \" average loss per word \" written to stdout by the classification routine ) should be about 0.28 .Tagging accuracy is 1 - this number , or 72 % .","label":"Background","metadata":{},"score":"66.28968"}{"text":"Proceedings of Human Language Technology and North American Chapter of the Association for Computatitonal Linguists 2003 .Copyright .© McDonald and Pereira 2005 .This article is published under license to BioMed Central Ltd.Syntactic parsing is a fundamental problem in computational linguistics and natural language processing .","label":"Background","metadata":{},"score":"66.333145"}{"text":"Semantic Role Parsing : Adding Semantic Structure to Unstructured Text , ICDM 2003 [ PDF ] .Pradhan , S. , Sun , H. , Ward , W. , Martin , J. , Jurafsky , D. , ( 2003 )Parsing Arguments of Nominalizations in English and Chinese , HLT - NAACL 2004 [ PDF ] .","label":"Background","metadata":{},"score":"66.79155"}{"text":"Semantic Role Parsing : Adding Semantic Structure to Unstructured Text , ICDM 2003 [ PDF ] .Pradhan , S. , Sun , H. , Ward , W. , Martin , J. , Jurafsky , D. , ( 2003 )Parsing Arguments of Nominalizations in English and Chinese , HLT - NAACL 2004 [ PDF ] .","label":"Background","metadata":{},"score":"66.79155"}{"text":"Kulick S , Bies A , Liberman M , Mandel M , McDonald R , Palmer M , Pancoast E , Schein A , Ungar L , White P , Winters S : Integrated annotation for biomedical information extraction .Proceedings of Biolink 2004 2004 .","label":"Background","metadata":{},"score":"66.858315"}{"text":"The system participated in the closed challenge ranking third in the complete problem evaluation with the following scores : 82.06 labeled macro F1 for the overall task , 86.6 labeled attachment for syntactic dependencies , and 77.5 labeled F1 for semantic dependencies .","label":"Background","metadata":{},"score":"66.95775"}{"text":"Conditional random fields are closely related to other conditional formalisms in the literature .The strongest connection is between CRFs and maximum entropy classification models [ 13 ] .Maximum entropy models give the conditional probability of a class given an observation by : .","label":"Background","metadata":{},"score":"66.986015"}{"text":"DIRECTION is used to change the parsing direction .The default setting is forward parsing mode ( LEFT to RIGHT ) .If \" -B \" is specified , backward parsing mode ( RIGHT to LEFT ) is used .Please see my paper for more detail about the parsing direction .","label":"Background","metadata":{},"score":"67.21356"}{"text":"DIRECTION is used to change the parsing direction .The default setting is forward parsing mode ( LEFT to RIGHT ) .If \" -B \" is specified , backward parsing mode ( RIGHT to LEFT ) is used .Please see my paper for more detail about the parsing direction .","label":"Background","metadata":{},"score":"67.21356"}{"text":"445 - 448 , Istanbul , ( 2003 ) .Devijver , P.A. , Kittler , J. : Pattern Recognition : A Statistical Approach , Prentice - Hall , Englewood Cliffs , NJ , ( 1982 ) .Steppe , J.M. , Bauer , K.W. , JR .","label":"Background","metadata":{},"score":"67.33341"}{"text":"Support Vector Machine Learning for Interdependent and Structured Output Spaces .International Conference on Machine Learning ( ICML ) , 2004 .[Postscript ] [ PDF ] . Y. Altun , I. Tsochantaridis , T. Hofmann , Hidden Markov Support Vector Machines .","label":"Background","metadata":{},"score":"67.63692"}{"text":"[ Show abstract ] [ Hide abstract ] ABSTRACT : Many machine learning and pattern recognition applications require reducing dimensionality to improve learning accuracy while irrelevant inputs are removed .This way , feature selection has become an important issue on these researching areas .","label":"Background","metadata":{},"score":"67.99851"}{"text":"PKI and PKE are about 3 - 12 and 10 - 300 holds faster than the original SVMs respectively .By default , PKI is used .To enable PKE , please recompile model files with -e option : .Taku Kudo , Yuji Matsumoto ( 2003 )","label":"Background","metadata":{},"score":"68.44374"}{"text":"PKI and PKE are about 3 - 12 and 10 - 300 holds faster than the original SVMs respectively .By default , PKI is used .To enable PKE , please recompile model files with -e option : .Taku Kudo , Yuji Matsumoto ( 2003 )","label":"Background","metadata":{},"score":"68.44374"}{"text":"How to Use .SVM hmm is built on top of SVM struct , a general implementation of SVMs for predicting complex structures containing interactions between elements .The site includes examples of its use for other applications as well as for sequence tagging .","label":"Background","metadata":{},"score":"68.91517"}{"text":"Map Reduce Workflow .Multiple map reduce jobs are orchestrated for KNN prediction as listed below .The first MR SameTypeSimilarity is from my Recommendation and Personalization engine project sifarish and it finds distance between objects with multiple attributes .It 's used in sifarish for for content based recommendation .","label":"Background","metadata":{},"score":"69.07231"}{"text":"the for more details .Please let me know if you use YamCha for research purpose or find any research publication where YamCha is applied .Both the training file and the test file need to be in a particular format for YamCha to work properly .","label":"Background","metadata":{},"score":"69.31772"}{"text":"the for more details .Please let me know if you use YamCha for research purpose or find any research publication where YamCha is applied .Both the training file and the test file need to be in a particular format for YamCha to work properly .","label":"Background","metadata":{},"score":"69.31772"}{"text":"Ball Tracking :The reliable tracking of the ball is vital in robot soccer .Therefore a Kalman - filter based system for estimating the ball position and velocity in the presence of occlusions is developped . -Sensor Fusion : The robot perceives its environment through several independent sensors ( camera , odometer , etc . ) , which have different delays .","label":"Background","metadata":{},"score":"69.44661"}{"text":"Full - text .T. Yakhno ( Ed . ) : ADVIS 2004 , LNCS 3261 , pp .462 - 471 , 2004 .© Springer - Verlag Berlin Heidelberg 2004 An Application of Support Vector Machine in Bioinformatics : Automated Recognition of Epileptiform Patterns in EEG Using SVM Classifier Designed by a Perturbation Method Nurettin Acır and Cüneyt Güzeli ?","label":"Background","metadata":{},"score":"69.59606"}{"text":"SVM hmm discriminatively trains models that are isomorphic to an kth - order hidden Markov model using the Structural Support Vector Machine ( SVM ) formulation .The SVM hmm learns one emission weight vector w y i - k ...y i for each different kth - order tag sequence y i - k ...","label":"Background","metadata":{},"score":"69.663925"}{"text":"These include general biological terms , amino acids , restriction enzymes , cell lines , organism names and non - biological terms meant to identify tokens that have been mislabeled as ' GENE ' .To recover false negatives , ABGene utilizes large gene lexicons coupled with context lists to identify possible mentions .","label":"Background","metadata":{},"score":"69.733475"}{"text":"Many real worlds data sets tend to have high class imbalance , including our eLearning data set .Students dropping out will be small percentage of the total population .Class imbalance negatively impacts many machine learning techniques resulting in poor performance in predicting the the minority class .","label":"Background","metadata":{},"score":"69.869484"}{"text":"To learn a model and then classify a test set , run .svm_hmm_learn -c -e training_input . dat modelfile . dat .svm_hmm_classify test_input . dat modelfile . dat classify.tags . where .training_input . dat and test_input . dat are files containing the training and test examples .","label":"Background","metadata":{},"score":"69.9209"}{"text":"Taku Kudo , Yuji Matsumoto ( 2001 )Chunking with Support Vector Machines , NAACL 2001 [ PDF ] .Taku Kudo , Yuji Matsumoto ( 2000 ) Use of Support Vector Learning for Chunk Identification , CoNLL-2000 [ PS ] .","label":"Background","metadata":{},"score":"69.97977"}{"text":"Taku Kudo , Yuji Matsumoto ( 2001 )Chunking with Support Vector Machines , NAACL 2001 [ PDF ] .Taku Kudo , Yuji Matsumoto ( 2000 ) Use of Support Vector Learning for Chunk Identification , CoNLL-2000 [ PS ] .","label":"Background","metadata":{},"score":"69.97977"}{"text":"For instance , the following feature would be useful : .This is because the token p-53 can either be in a gene mention ( when it is followed by the word ' mutant ' ) or be in a mutation mention ( when it is followed by the word ' mutations ' ) .","label":"Background","metadata":{},"score":"70.06808"}{"text":"Default setting is empty ( \" \") .Here is an example of changing the sentence boundary marker to \" EOS \" .If you know in advance the candidates of answer tags by using some ' prior ' knowledge , you may want to select answer only from these candidates .","label":"Background","metadata":{},"score":"70.11486"}{"text":"Default setting is empty ( \" \") .Here is an example of changing the sentence boundary marker to \" EOS \" .If you know in advance the candidates of answer tags by using some ' prior ' knowledge , you may want to select answer only from these candidates .","label":"Background","metadata":{},"score":"70.11486"}{"text":"For example , 1st column is ' word ' , second column is ' POS tag ' third column is ' sub - category of POS ' and so on .The last column represents a true answer tag which is going to be trained by SVMs .","label":"Background","metadata":{},"score":"70.522514"}{"text":"For example , 1st column is ' word ' , second column is ' POS tag ' third column is ' sub - category of POS ' and so on .The last column represents a true answer tag which is going to be trained by SVMs .","label":"Background","metadata":{},"score":"70.522514"}{"text":"Because the Otsu method is based on a global threshold , the spot pixels tend to be misclassified as ink .On the contrary , the local method is more likely to achieve a correct separation of ink and background on the defective area , which explains why the respective f - scores of the Otsu and Sauvola methods are $ 0.4 $ and $ 0.7 $ on this image .","label":"Background","metadata":{},"score":"70.62187"}{"text":"We show that dependency parsers have more difficulty parsing questions than constituency parsers .In particular , deterministic shift - reduce dependency parsers , which are of highest interest for practical applications because of their linear running time , drop to 60 % labeled accuracy on a question test set .","label":"Background","metadata":{},"score":"70.99425"}{"text":"I am passionate about technology and green and sustainable living .My technical interest areas are Big Data , Distributed Processing , NOSQL databases , Data Mining and Programming languages .I am fascinated by problems that do n't have neat closed form solution .","label":"Background","metadata":{},"score":"71.0338"}{"text":"Haykin S. : Neural Networks : A comprehensive foundation .Prentice Hall , New Jersey , ( 1999 ) .Bertsekas , D.P. : Nonlinear Programming .Athenas Scientific , Belmont , ( 1995 ) .Page 10 .An Application of Support Vector Machine in Bioinformatics 471 10 .","label":"Background","metadata":{},"score":"71.5599"}{"text":"We apply the new transition - based parser on typologically different languages such as English , Chinese , Czech , and German and report competitive labeled and unlabeled attachment scores . ... restricted to projective dependency trees and used pseudo - projective parsing ( Kahane et al .","label":"Background","metadata":{},"score":"71.792694"}{"text":"Although it may occur .Page 7 .468 N. Acır and C. Güzeli ? alone , an EP is usually followed by a slow wave , which lasts 150 - 350 ms , together forming what is known as a \" spike and slow wave complex \" [ 11].","label":"Background","metadata":{},"score":"71.80653"}{"text":"This expands the archive into the current directory , which now contains all relevant files .If you downloaded the source code , you can compile SVM hmm using the command : . make .This will produce the executables svm_hmm_learn ( the learning module ) and svm_hmm_classify ( the classification module ) .","label":"Background","metadata":{},"score":"72.03745"}{"text":"gave me a patch to fix the bug .2004 - 10 - 03 : yamcha 0.31 Released .Fix bugs in mkmodel . mkmodel did not work well with Perl 5.8.1 or higer version .2004 - 03 - 28 : yamcha 0.30 Released .","label":"Background","metadata":{},"score":"72.85322"}{"text":"gave me a patch to fix the bug .2004 - 10 - 03 : yamcha 0.31 Released .Fix bugs in mkmodel . mkmodel did not work well with Perl 5.8.1 or higer version .2004 - 03 - 28 : yamcha 0.30 Released .","label":"Background","metadata":{},"score":"72.85322"}{"text":"Note that our methodology involves the creation of different predictive models , one for each parameter set . %For example , Sauvola 's method with a $ 5 \\times 5 $ window size is different from Sauvola 's method with an $ 8 \\times 8 $ window size and will require the creation of a different prediction model .","label":"Background","metadata":{},"score":"72.86242"}{"text":"Note that our methodology involves the creation of different predictive models , one for each parameter set . %For example , Sauvola 's method with a $ 5 \\times 5 $ window size is different from Sauvola 's method with an $ 8 \\times 8 $ window size and will require the creation of a different prediction model .","label":"Background","metadata":{},"score":"72.86242"}{"text":"It means that [ beginning position of column ] is 0 and [ the end position of column ] is 1 .You can omit the [ end position of column].If omitted , the last column is set as [ end position of column].","label":"Background","metadata":{},"score":"73.09886"}{"text":"It means that [ beginning position of column ] is 0 and [ the end position of column ] is 1 .You can omit the [ end position of column].If omitted , the last column is set as [ end position of column].","label":"Background","metadata":{},"score":"73.09886"}{"text":"The hyper plane separating classes is not very crisp in many data sets resulting in what is known as the class overlap problem .When the data points belonging to one class encroach deeply into the region occupied by data points belonging to the other class , we have class overlap problem .","label":"Background","metadata":{},"score":"73.31257"}{"text":"NearestNeighbor . predicts class of test data .BayesianDistribution is from the bayesian package of avenir .It calculates feature attribute class conditional probability , feature prior probability and class prior probability .BayesianPredictor is also from the same package .It makes class prediction for Naive Bayes classification .","label":"Background","metadata":{},"score":"73.76982"}{"text":"We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task .For languages from different families the improvements often exceed 2 BLEU .Many of these gains are also significant in human evaluations .We present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages : German , English , Swedish , Spanish , French and Korean .","label":"Background","metadata":{},"score":"74.0081"}{"text":"In some cases , depending on the content and the eLearning system being used , high drop out rates have been reported as a serious problem .Here is an article form the Journal of Online Learning and Teaching on this topic .","label":"Background","metadata":{},"score":"74.18844"}{"text":"All duplicate entries are deleted .Here are more complicated examples .F:-3 .T:-3 .F:-2 . .1 F:0 . .1T:-1 .F:-3 .F:0 .F:2 .T:-3 .F:-3 . .1F:-1 . .0F:2 . .1T:-3 .","label":"Background","metadata":{},"score":"74.20128"}{"text":"All duplicate entries are deleted .Here are more complicated examples .F:-3 .T:-3 .F:-2 . .1 F:0 . .1T:-1 .F:-3 .F:0 .F:2 .T:-3 .F:-3 . .1F:-1 . .0F:2 . .1T:-3 .","label":"Background","metadata":{},"score":"74.20128"}{"text":"The following data is invalid , since the number of columns of second and third are 2 .( They have no POS column . )The number of columns should be fixed .He PRP B - NP reckons B - VP the B - NP current JJ I - NP account NN I - NP .","label":"Background","metadata":{},"score":"75.0952"}{"text":"The following data is invalid , since the number of columns of second and third are 2 .( They have no POS column . )The number of columns should be fixed .He PRP B - NP reckons B - VP the B - NP current JJ I - NP account NN I - NP .","label":"Background","metadata":{},"score":"75.0952"}{"text":"There are total 12 columns ; 1 : word , 2 : contains number(Y / N ) , 3 : capitalized(Y / N ) , 4:contains symbol ( Y / N ) 5 . .8( prefixes from 1 to 4 ) 9 . .12","label":"Background","metadata":{},"score":"75.22026"}{"text":"There are total 12 columns ; 1 : word , 2 : contains number(Y / N ) , 3 : capitalized(Y / N ) , 4:contains symbol ( Y / N ) 5 . .8( prefixes from 1 to 4 ) 9 . .12","label":"Background","metadata":{},"score":"75.22026"}{"text":"+ In the first image , the values of $ \\mIInk$ and $ \\mIBack$ are low , indicating that a global thresholding method is likely to fail to correctly classify the pixels .The value of $ \\mSG$ is also high , indicating that there are large spots around the characters .","label":"Background","metadata":{},"score":"75.47077"}{"text":"In indexing the collection , we recovered the relevant content from the blog permalink pages , exploiting HTML metadata about the generator and heuristics to remove irrelevant parts from the body .The index also contains information about the occurrence of opinionated words , extracted from an analysis of WordNet glosses .","label":"Background","metadata":{},"score":"75.48823"}{"text":"The classification performance of the system is determined by measuring the sensitivity , specificity and accuracy .Sensitivity is the ratio of true positives to the total number of EPs determined by expert as \" EP \" .Specificity is the ratio of \" false positive \" to the summing of false positive with correct rejections .","label":"Background","metadata":{},"score":"75.79315"}{"text":"We present a new edition of the Google Books Ngram Corpus , which describes how often words and phrases were used over a period of five centuries , in eight languages ; it reflects 6 % of all books ever published .","label":"Background","metadata":{},"score":"75.80972"}{"text":"Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .On the other hand , our grammars are much more compact and substantially more accurate than previous work on automatic annotation .Despite its simplicity , our best grammar achieves an F1 of 89.9 % on the Penn Treebank , higher than most fully lexicalized systems .","label":"Background","metadata":{},"score":"76.272354"}{"text":"( light - blue box )It means that [ beginning positing of tag ] is -2 and [ end position of tag ] is -1 .Note that [ end potion of tag ] must smaller than -1 , since the right - side tags ( 0,+1,+2,+3 ... ) have not been identified yet and can not be used as features .","label":"Background","metadata":{},"score":"76.29611"}{"text":"( light - blue box )It means that [ beginning positing of tag ] is -2 and [ end position of tag ] is -1 .Note that [ end potion of tag ] must smaller than -1 , since the right - side tags ( 0,+1,+2,+3 ... ) have not been identified yet and can not be used as features .","label":"Background","metadata":{},"score":"76.29611"}{"text":"Fix FATAL bugs in feature_index.cpp Multiple feature templates ( e.g , F:-2 . .15 F:1 . .15 F:0 .did n't work in the previous releases .Mr. Sameer Pradhan gave me a detailed report on this bug .Support NULL ' \\0 ' string in the parameters .","label":"Background","metadata":{},"score":"76.43628"}{"text":"Fix FATAL bugs in feature_index.cpp Multiple feature templates ( e.g , F:-2 . .15 F:1 . .15 F:0 .did n't work in the previous releases .Mr. Sameer Pradhan gave me a detailed report on this bug .Support NULL ' \\0 ' string in the parameters .","label":"Background","metadata":{},"score":"76.43628"}{"text":"-In the first image , the values of $ \\mIInk$ and $ \\mIBack$ are low , indicating that a global thresholding method is likely to fail to correctly classify the pixels .The value of $ \\mSG$ is also high , indicating that there are large spots around the characters .","label":"Background","metadata":{},"score":"76.584366"}{"text":"Electroencephalogr .Clin .Neurophysiol . , 37 ( 1974 ) 538 - 548 .Kalaycı , T. , and Özdamar , Ö. : Wavelet preprocessing for automated neural network detection of EEG spikes .IEEE Eng .Med .Biol . , 14 - 2 ( 1995 ) 160 - 166 .","label":"Background","metadata":{},"score":"77.79257"}{"text":"On this image , the respective f - scores of Otsu and Sauvola are $ 0.8 $ and $ 0.4$. The Sauvola method is not robust to the background speckles , which are classified as ink .The faded ink defect is a drawback for a global method and lowers the performance of Otsu 's method .","label":"Background","metadata":{},"score":"78.12137"}{"text":"Can be any number larger than 1 .( default 1 ) .Can be any number larger than 0 .( default 0 ) .The value is the width of the beam used ( e.g. 100 ) .( default 0 ) .","label":"Background","metadata":{},"score":"78.40448"}{"text":"I strongly advise against using non - linear Kernels .Since the code was not written with Kernels in mind , it is going to be painfully slow and I have never tested it .Example Problem .You will find an example part - of - speech ( POS ) tagging dataset ( thanks to Evan Herbst ) at .","label":"Background","metadata":{},"score":"78.90843"}{"text":"Masayuki Asahara and Yuji Matsumoto ( 2003 )Japanese Named Entity Extraction with Redundant Morphological Analysis , HLT - NAACL 2003 [ PDF ] .Goh Chooi Ling and Masayuki Asahara and Yuji Matsumoto ( 2003 ) Chinese Unknown Word Identification Using Position Tagging and Chunking ACL 2003 Interractive Posters / Demo [ PDF ] .","label":"Background","metadata":{},"score":"79.64572"}{"text":"Masayuki Asahara and Yuji Matsumoto ( 2003 )Japanese Named Entity Extraction with Redundant Morphological Analysis , HLT - NAACL 2003 [ PDF ] .Goh Chooi Ling and Masayuki Asahara and Yuji Matsumoto ( 2003 ) Chinese Unknown Word Identification Using Position Tagging and Chunking ACL 2003 Interractive Posters / Demo [ PDF ] .","label":"Background","metadata":{},"score":"79.64572"}{"text":"If there is no entry in a column , dummy field ( \" _ _ nil _ _ \" ) is assigned .Rockwell N Y N R Ro Roc Rock l ll ell well NNP International N Y N I In Int Inte l al nal onal NNP Corp. N Y N C Co","label":"Background","metadata":{},"score":"80.139084"}{"text":"If there is no entry in a column , dummy field ( \" _ _ nil _ _ \" ) is assigned .Rockwell N Y N R Ro Roc Rock l ll ell well NNP International N Y N I In Int Inte l al nal onal NNP Corp. N Y N C Co","label":"Background","metadata":{},"score":"80.139084"}{"text":"Herein , accuracy , sensitivity and specificity which will be defined below can be considered as performance values .5 Application 5.1 Recognition of Epileptiform Patterns in EEG This application presents the classification of epileptiform patterns ( EPs ) in a one dimensional and single - channel EEG signal .","label":"Background","metadata":{},"score":"84.50859"}{"text":"Different distance metric choices are offered in sifarish .But for this example , I have use Euclidian .Use different weights for different attributes when calculating distance .This feature is available in sifarish , but I have not used it .","label":"Background","metadata":{},"score":"84.91313"}{"text":"For an extreme example , consider the string interleukin-1 [ IL-1 ] , tumor necrosis factor - alpha [ TNF - alpha ] , which the tagger incorrectly returns as being one entity .The gold standard identifies 4 different entities within this string , interleukin-1 , IL-1 , tumor necrosis factor - alpha and TNF - alpha .","label":"Background","metadata":{},"score":"86.964615"}{"text":"# without -V % yamcha -m case_study .NNP I I 's POS B B Tulsa NNP I I unit NN I I said VBD O O . # with -V % yamcha -V -m case_study . model Rockwell NNP B B B/0.630616 I/-0.974367 O/-0.721942 International NNP I I B/-0.789851 I/0.561522 O/-0.833703 Corp. Sentence boundary marker .","label":"Background","metadata":{},"score":"87.46983"}{"text":"# without -V % yamcha -m case_study .NNP I I 's POS B B Tulsa NNP I I unit NN I I said VBD O O . # with -V % yamcha -V -m case_study . model Rockwell NNP B B B/0.630616 I/-0.974367 O/-0.721942 International NNP I I B/-0.789851 I/0.561522 O/-0.833703 Corp. Sentence boundary marker .","label":"Background","metadata":{},"score":"87.46983"}{"text":"Intent mining is a special kind of document analysis whose goal is to assess the attitude of the document author with respect to a given subject .Opinion mining is a kind of intent mining where the attitude is a positive or negative opinion .","label":"Background","metadata":{},"score":"87.832275"}{"text":"Intent mining is a special kind of document analysis whose goal is to assess the attitude of the document author with respect to a given subject .Opinion mining is a kind of intent mining where the attitude is a positive or negative opinion .","label":"Background","metadata":{},"score":"87.832275"}{"text":"Like this : .About Pranab .I am Pranab Ghosh , a software professional in the San Francisco Bay area .I manipulate bits and bytes for the good of living beings and the planet .I have worked with myriad of technologies and platforms in various business domains for early stage startups , large corporations and anything in between .","label":"Background","metadata":{},"score":"91.19622"}{"text":"All lines with the same EXNUM have to be in consecutive order .3 qid:1 1:1 2:1 3:1 # see 2 qid:1 4:1 5:1 # jane 3 qid:1 2:-1 6:1 7:2.5 # play 2 qid:1 3:-1 8:1 # ball 5 qid:1 9:1 10:1 # . 1 qid:2 3:1 11:1 12:1 # she 3 qid:2 1:-1 13:1 14:2 # is 4 qid:2 15:1 16:0.1 17:-1 # good 5 qid:2 9:0.5 18:1 # .","label":"Background","metadata":{},"score":"91.844955"}{"text":"Journal of medicine ( The New England ) , 293 - 5 ( 1975 ) 211 - 215 .Daubechies , I. : Ten lectures on wavelets .Capital city press , Montpelier , VT , ( 1992 ) .Data provided are for informational purposes only .","label":"Background","metadata":{},"score":"92.40318"}{"text":"This Master 's thesis describes parts of the control software used by the soccer robots of the Free University of Berlin , the so called FU - Fighters .The FU - Fighters compete in the Middle Sized League of RoboCup and reached the semi - finals during the 2004 RoboCup World Cup in Lisbon , Portugal .","label":"Background","metadata":{},"score":"93.65416"}{"text":"He PRP B - NP reckons VBZ B - VP the DT B - NP current JJ I - NP account NN I - NP deficit NN I - NP will MD B - VP narrow VB I - VP to TO B - PP only RB B - NP # # I - NP 1.8 CD I - NP billion CD I - NP in IN B - PP September NNP B - NP .","label":"Background","metadata":{},"score":"93.82836"}{"text":"He PRP B - NP reckons VBZ B - VP the DT B - NP current JJ I - NP account NN I - NP deficit NN I - NP will MD B - VP narrow VB I - VP to TO B - PP only RB B - NP # # I - NP 1.8 CD I - NP billion CD I - NP in IN B - PP September NNP B - NP .","label":"Background","metadata":{},"score":"93.82836"}{"text":"Acknowledgements .The authors would like to thank our collaborators Mark Liberman , Andy Schein , Pete White and Scott Winters for useful discussions and suggestions .We would also like to thank Lorraine Tanabe for making the ABGene lexicons available to us .","label":"Background","metadata":{},"score":"96.57622"}{"text":"Software Consultant ...San Francisco Bay Area ... .MIT ... UC ...IIT ...Web ... Java .JEE ...Spring ...Hibernate ...Ruby ...Rails ...Oracle ... MySQL ... NOSQL ...Cassandra ... HBase .... Hadoop ...Hive .... BI ...","label":"Background","metadata":{},"score":"96.583824"}{"text":"The data generating script with logic is such that there is class imbalance which is natural for this data set .It also deliberately introduces large class overlap to make classification more challenging .Here is some sample input .1242987,375,28,59,8,69,55,197,52,1,P 1375258,307,39,52,13,69,33,97,32,0,F 1545012,155,44,55,6,38,52,33,33,10,F 1416276,203,78,26,14,10,10,79,47,14,P 1212312,536,83,23,0,30,61,79,72,20,P 1259055,191,70,2,1,75,21,136,84,25,P 1241411,166,113,36,14,95,100,172,75,9,P .","label":"Background","metadata":{},"score":"104.70701"}{"text":"P1973128,P,1.563226260137538E-17,P , P P1974227,F,5.094622163781374E-18,P,1.3913280652371146E-17,P , P P1977663,P,4.499679277419704E-18,F , P P1979022,F,6.229582705344719E-18,P,1.2507761942439003E-18,F , F P1981268,F,7.90091893283086E-19,P,3.9097666365799436E-18,F , P P1984678,F,9.797444241127851E-19,P,1.385907383740041E-17,P , P P1987694,F,3.8811744913728846E-18,P,2.7169442275428592E-17,P , P .We have number of miss classifications and they are false negative i.e. student who has actually dropped out is predicted as having completed the course .","label":"Background","metadata":{},"score":"112.079254"}{"text":"The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .Publisher conditions are provided by RoMEO .Differing provisions from the publisher 's actual policy or licence agreement may be applicable .","label":"Background","metadata":{},"score":"131.78247"}