{"text":"But in a Boosting method , later classifiers focus on increasing the margins for examples with poor current margins .As Schapire et al .[1997 ] note , this is a very effective strategy if the overall accuracy of the resulting classifier does not drop significantly .","label":"Future","metadata":{},"score":"22.365658"}
{"text":"But in a Boosting method , later classifiers focus on increasing the margins for examples with poor current margins .As Schapire et al .[1997 ] note , this is a very effective strategy if the overall accuracy of the resulting classifier does not drop significantly .","label":"Future","metadata":{},"score":"22.365673"}
{"text":"Freund and Shapire [ 1996 ] suggested that the sometimes poor performance of Boosting results from overfitting the training set since later training sets may be over - emphasizing examples that are noise ( thus creating extremely poor classifiers ) .This argument seems especially pertinent to Boosting for two reasons .","label":"Future","metadata":{},"score":"24.450436"}
{"text":"[ 1998 ] hypothesize that Boosting methods , as additive models , may see increases in error in those situations where the bias of the base classifier is appropriate for the problem being learned .We test this hypothesis in our second set of results presented in this section .","label":"Future","metadata":{},"score":"25.564363"}
{"text":"Some initial theory is presented which indicates that a lack of correlation between the errors of individual classifiers is a key factor in this variance reduction .Freund and Shapire [ 1996 ] suggested that the sometimes poor performance of Boosting results from overfitting the training set since later training sets may be over - emphasizing examples that are noise ( thus creating extremely poor classifiers ) .","label":"Future","metadata":{},"score":"27.798092"}
{"text":"For a few data sets Boosting produced dramatic reductions in error ( even compared to Bagging ) , but for other data sets it actually increases in error over a single classifier ( particularly with neural networks ) .In further tests we examined the effects of noise and support Freund and Schapire 's [ 1996 ] conjecture that Boosting 's sensitivity to noise may be partly responsible for its occasional increase in error .","label":"Future","metadata":{},"score":"28.783215"}
{"text":"In this paper we evaluate these methods on 23 data sets using both neural networks and decision trees as our classification algorithm .Our results clearly indicate a number of conclusions .First , while Bagging is almost always more accurate than a single classifier , it is sometimes much less accurate than Boosting .","label":"Future","metadata":{},"score":"29.264122"}
{"text":"Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm .An alternative approac ... \" .Abstract .Bagging and boosting are methods that generate a diverse ensemble of classifiers by manipulating the training data given to a \" base \" learning algorithm .","label":"Future","metadata":{},"score":"31.00991"}
{"text":"First , while Bagging is almost always more accurate than a single classifier , it is sometimes much less accurate than Boosting .On the other hand , Boosting can create ensembles that are less accurate than a single classifier - especially when using neural networks .","label":"Future","metadata":{},"score":"31.072338"}
{"text":"First , while Bagging is almost always more accurate than a single classifier , it is sometimes much less accurate than Boosting .On the other hand , Boosting can create ensembles that are less accurate than a single classifier - especially when using neural networks .","label":"Future","metadata":{},"score":"31.072338"}
{"text":"This general approach has been studied previously by Ali and Pazzani and by Dietterich and Kong .This paper compares the effectiveness of randomization , bagging , and boosting for improving the performance of the decision - tree algorithm C4.5 .The experiments show that in situations with little or no classification noise , randomization is competitive with ( and perhaps slightly superior to ) bagging but not as accurate as boosting .","label":"Future","metadata":{},"score":"31.718712"}
{"text":"One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large , and often is observed to decrease even after the training error reaches zero .","label":"Future","metadata":{},"score":"32.02628"}
{"text":"Some authors go even further , and argue that the importance of interpretability has been marginalized in the design of these algorithms , and put behind the need to devise classifiers with strong classification power [ Ridgeway et al.1998 ] .But interpretability also governs the quality of a model by providing answers to how it is working , and , most importantly , why .","label":"Future","metadata":{},"score":"32.080723"}
{"text":"It is widely accepted , and formally proven in certain cases [ Schapire et al.1998 , Schapire Singer1998 ] , that their power actually relies on the ability to build potentially very large classifiers .It has even been observed experimentally that such an ensemble can sometimes be as large as ( or larger than ) the data used to build the ensemble [ Margineantu Dietterich1997 ] !","label":"Future","metadata":{},"score":"32.324413"}
{"text":"Ensemble classification techniques such as bagging , ( Breiman , 1996a ) , boosting ( Freund & Schapire , 1997 ) and arcing algorithms ( Breiman , 1997 ) have received much attention in recent literature .Such techniques have been shown to lead to reduced classification error on unseen cases .","label":"Future","metadata":{},"score":"32.3749"}
{"text":"This paper is organized as follows .In the next section we present an overview of classifier ensembles and discuss Bagging and Boosting in detail .Next we present an extensive empirical analysis of Bagging and Boosting .Following that we present future research and additional related work before concluding .","label":"Future","metadata":{},"score":"32.585663"}
{"text":"Further , studies in artificial domains show that it will often outperform more powerful classifiers for common training set sizes and numbers of attributes , even if its bias is a priori much less appropriate to the domain .This article 's results also imply that detecting attribute dependence is not necessarily the best way to extend the Bayesian classifier , and this is also verified empirically . .","label":"Future","metadata":{},"score":"32.78501"}
{"text":"Analysis indicates that the performance of the Boosting methods is dependent on the characteristics of the data set being examined .In fact , further results show that Boosting ensembles may overfit noisy data sets , thus decreasing its performance .Finally , consistent with previous studies , our work suggests that most of the gain in an ensemble 's performance comes in the first few classifiers combined ; however , relatively large gains can be seen up to 25 classifiers when Boosting decision trees . set .","label":"Future","metadata":{},"score":"32.86554"}
{"text":"Our results indicate that this ensemble technique is surprisingly effective , often producing results as good as Bagging .Research by Ali and Pazzani [ 1996 ] demonstrated similar results using randomized decision tree algorithms .Our results also show that the ensemble methods are generally consistent ( in terms of their effect on accuracy ) when applied either to neural networks or to decision trees ; however , there is little inter - correlation between neural networks and decision trees except for the Boosting methods .","label":"Future","metadata":{},"score":"33.09709"}
{"text":"The second reason is that the classifiers are combined using weighted voting .Previous work [ Sollich Krogh1996 ] has shown that optimizing the combining weights can lead to overfitting while an unweighted voting scheme is generally resilient to overfitting .Friedman et al .","label":"Future","metadata":{},"score":"33.31787"}
{"text":"A third parameter influencing comprehensibility is the nature of the algorithm 's output .Inside the broad scope of symbolic classifiers , some classes of concept representations appear to offer a greater comfort for interpretation .Decision trees belong to this set [ Breiman et al.1984 ] , though they also raise some interpretability problems : Kohavi Sommerfield ( 1998 ) quote that . ''","label":"Future","metadata":{},"score":"33.994377"}
{"text":"In situations with substantial classification noise , bagging is much better than boosting , and sometimes better than randomization . \" ...In real - world environments it is usually difficult to specify target operating conditions precisely .This uncertainty makes building robust classification systems problematic .","label":"Future","metadata":{},"score":"34.24359"}
{"text":"An alternative approach to generating an ensemble is to randomize the internal decisions made by the base algorithm .This general approach has been studied previously by Ali and Pazzani and by Dietterich and Kong .This paper compares the effectiveness of randomization , bagging , and boosting for improving the performance of the decision - tree algorithm C4.5 .","label":"Future","metadata":{},"score":"34.26696"}
{"text":"In fact , further results show that Boosting ensembles may overfit noisy data sets , thus decreasing its performance .Finally , consistent with previous studies , our work suggests that most of the gain in an ensemble 's performance comes in the first few classifiers combined ; however , relatively large gains can be seen up to 25 classifiers when Boosting decision trees . \" ...","label":"Future","metadata":{},"score":"34.39181"}
{"text":"Krogh and Vedelsby [ 13 ] prove that the overall error rate can be reduced by classifier ... \" ...In the feature subset selection problem , a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention , while ignoring the rest .","label":"Future","metadata":{},"score":"34.707108"}
{"text":"Two popular methods for creating accurate ensembles are Bagging [ Breiman1996a ] and Boosting [ Freund Schapire1996 , Schapire1990 ] .These methods rely on ' ' resampling ' ' techniques to obtain different training sets for each of the classifiers .","label":"Future","metadata":{},"score":"34.82483"}
{"text":"We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples .Finally , we compare our explanation to those based on the bias - variance decomposition . \" ...The simple Bayesian classifier is known to be optimal when attributes are independent given the class , but the question of whether other sufficient conditions for its optimality exist has so far not been explored .","label":"Future","metadata":{},"score":"35.24314"}
{"text":"Bagging and boosting are methods that generate a diverse ensemble of classifiers by manipulating the training data given to a \" base \" learning algorithm .Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm .","label":"Future","metadata":{},"score":"35.647785"}
{"text":"We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples .Finally , we compare our explanation to those based on the bias - variance decomposition . ... a , overly complex weak hypotheses or weak hypotheses which are too weak .","label":"Future","metadata":{},"score":"35.782124"}
{"text":"Focusing primarily on the AdaBoost algorithm , this chapter overviews some of the recent work on boosting including analyses of AdaBoost 's training error and generalization error ; boosting 's connecti ... \" .Boosting is a general method for improving the accuracy of any given learning algorithm . by Charles Elkan - In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence , 2001 . \" ...","label":"Future","metadata":{},"score":"36.02475"}
{"text":"In fact , further results show that Boosting ensembles may overfit noisy data sets , thus decreasing its performance .Finally , consistent with previous studies , our work suggests that most of the gain in an ensemble 's performance comes in the first few classifiers combined ; however , relatively large gains can be seen up to 25 classifiers when Boosting decision trees . tworks .","label":"Future","metadata":{},"score":"36.58188"}
{"text":"The simple Bayesian classifier is known to be optimal when attributes are independent given the class , but the question of whether other sufficient conditions for its optimality exist has so far not been explored .Empirical results showing that it performs surprisingly well in many domains containing clear attribute dependences suggest that the answer to this question may be positive .","label":"Future","metadata":{},"score":"36.886246"}
{"text":"Abstract .Bagging and boosting are methods that generate a diverse ensemble of classifiers by manipulating the training data given to a \" base \" learning algorithm .Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm .","label":"Future","metadata":{},"score":"37.083244"}
{"text":"Tools . by Thomas G. Dietterich - MULTIPLE CLASSIFIER SYSTEMS , LBCS-1857 , 2000 . \" ...Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a ( weighted ) vote of their predictions .","label":"Future","metadata":{},"score":"37.234863"}
{"text":"This paper presents some interesting results from an empirical study performed on a set of representative datasets using the decision tree learner C4.5 ( Quinlan , 1993 ) .An exponential - like decay in the variance of the edge is observed as the number of boosting trials is increased .","label":"Future","metadata":{},"score":"37.297455"}
{"text":"Our empirical study shows that the proposed methods have substantial advantage over single - classifier approaches in prediction accuracy , and the ensemble framework is effective for a variety of classification models . \" ...Diversity among the members of a team of classifiers is deemed to be a key issue in classifier combination .","label":"Future","metadata":{},"score":"37.62693"}
{"text":"Abstract .One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large , and often is observed to decrease even after the training error reaches zero .","label":"Future","metadata":{},"score":"37.785244"}
{"text":"Abstract .One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large , and often is observed to decrease even after the training error reaches zero .","label":"Future","metadata":{},"score":"37.785244"}
{"text":"Abstract .One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large , and often is observed to decrease even after the training error reaches zero .","label":"Future","metadata":{},"score":"37.785244"}
{"text":"For the two - class case , we prove a theorem that shows how to change the proportion of negative examples in a training set in order to make optimal cost - sensitive classification decisions using a classifier learned by a standard non - costsensitive learning method .","label":"Future","metadata":{},"score":"38.291775"}
{"text":"Second , neural networks have been extensively applied across numerous domains [Arbib1995 ] .Finally , by studying neural networks in addition to decision trees we can examine how Bagging and Boosting are influenced by the learning algorithm , giving further insight into the general characteristics of these approaches .","label":"Future","metadata":{},"score":"38.597885"}
{"text":"The first and most obvious reason is that their method for updating the probabilities may be over - emphasizing noisy examples .The second reason is that the classifiers are combined using weighted voting .Previous work [ Sollich Krogh1996 ] has shown that optimizing the combining weights can lead to overfitting while an unweighted voting scheme is generally resilient to overfitting .","label":"Future","metadata":{},"score":"38.79286"}
{"text":"The method is efficient and incremental , minimizes the management of classifier performance data , and allows for clear visual comparisons and sensitivity analyses .We then show that it is possible to build a hybrid classifier that will perform at least as well as the best available classifier for any target conditions .","label":"Future","metadata":{},"score":"38.8456"}
{"text":".. voting the decisions of the individual classifiers in the ensemble .Two of the most popular techniques for constructing ensembles are boostrap aggregation ( \" bagging \" ; Breiman , 1996a ) and the Adaboost family of algorithms ( \" boosting \" ; Freund & ... . \" ...","label":"Future","metadata":{},"score":"39.132793"}
{"text":"In further tests we demonstrate that Bagging is more resilient to noise than Boosting .Finally , we investigated the question of how many component classifiers should be used in an ensemble .Consistent with previous research [ Freund Schapire1996 , Quinlan1996 ] , our results show that most of the reduction in error for ensemble methods occurs with the first few additional classifiers .","label":"Future","metadata":{},"score":"39.230846"}
{"text":"We train an ensemble of classification models , such as C4.5 , RIPPER , naive Bayesian , etc . , from sequential chunks of the data stream .The classifiers in the ensemble are judiciously weighted based on their expected classification accuracy on the test data under the time - evolving environment .","label":"Future","metadata":{},"score":"39.720177"}
{"text":"The original ensemble method is Bayesian averaging , but more recent algorithms include error - correcting output coding , Bagging , and boostin ... \" .Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a ( weighted ) vote of their predictions .","label":"Future","metadata":{},"score":"40.004383"}
{"text":"This helps explain why such simple methods are often competitive with and sometimes superior to more sophisticated ones for classification , and why \" bagging / aggregating \" classifiers can often improve accuracy .These results also suggest simple modifications to these procedures that can ( sometimes dramatically ) further improve their classification performance . .","label":"Future","metadata":{},"score":"40.356483"}
{"text":"Our neural network and decision tree results led us to a number of interesting conclusions .The first is that a Bagging ensemble generally produces a classifier that is more accurate than a standard classifier .Thus one should feel comfortable always Bagging their decision trees or neural networks .","label":"Future","metadata":{},"score":"40.68409"}
{"text":"We highlight similarities among scaling techniques by categorizing them into three main approaches .For each approach , we then describe , compare , and contrast the different constituent techniques , drawing on specific examples from published papers .Finally , we use the preceding analysis to suggest how to proceed when dealing with a large problem , and where to focus future research .","label":"Future","metadata":{},"score":"40.897427"}
{"text":"Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a ( weighted ) vote of their predictions .The original ensemble method is Bayesian averaging , but more recent algorithms include error - correcting output coding , Bagging , and boosting .","label":"Future","metadata":{},"score":"41.049828"}
{"text":"We have found and studied ten statistics which can measure diversity among binary ... \" .Diversity among the members of a team of classifiers is deemed to be a key issue in classifier combination .However , measuring diversity is not straightforward because there is no generally accepted formal definition .","label":"Future","metadata":{},"score":"41.095642"}
{"text":"The purpose of the study is to improve our understanding of why and when these algorithms , which use perturbation , reweighting , and combination techniques , affect classification error .We provide a bias and variance decomposition of the error to show how different methods and variants influence these two terms .","label":"Future","metadata":{},"score":"41.18187"}
{"text":"We desire a meta - learning architecture that exhibits two key behaviors .First , the meta - learning strategy must produce an accurate final classification system .This means that a meta - learning architecture must produce a final outcome that is at least as accurate as a conventional learning algorithm applied to all available data .","label":"Future","metadata":{},"score":"41.83956"}
{"text":"These efforts have conc ... . by David Opitz , Richard Maclin - Journal of Artificial Intelligence Research , 1999 . \" ...An ensemble consists of a set of individually trained classifiers ( such as neural networks or decision trees ) whose predictions are combined when classifying novel instances .","label":"Future","metadata":{},"score":"41.889153"}
{"text":"One of the more effective is bagging ( Breiman [ 1996a ] ) Here , modified training sets are formed by resampling from the original training set , classifiers con ... \" .Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error .","label":"Future","metadata":{},"score":"41.93476"}
{"text":"It is a software environment for implementing algorithms and running experiments for online learning from evolving data streams .MOA includes a collection of offline and online methods as well as tools for evaluation .In particular , it implements boosting , bagging , and Hoeffding Trees , all with and without Naive Bayes classifiers at the leaves .","label":"Future","metadata":{},"score":"42.087723"}
{"text":"That work combines models in order to boost performance for a fixed cost and class distribution .The rocch - hybrid combines models for robustness across different cost and class distributions .In pr ... . by David Opitz , Richard Maclin - Journal of Artificial Intelligence Research , 1999 . \" ...","label":"Future","metadata":{},"score":"42.719482"}
{"text":"We first give a formal framework for the problem .We then describe and analyze a new boosting ... \" .The problem of combining preferences arises in several applications , such as combining the results of different search engines .This work describes an efficient algorithm for combining multiple preferences .","label":"Future","metadata":{},"score":"43.495358"}
{"text":"In particular the bias and variance components of the estimation error combine to influence classification in a very different way than with squared error on the probabilities themselves .Certain types of ( very high ) bias can be canceled by low variance to produce accurate classification .","label":"Future","metadata":{},"score":"43.73287"}
{"text":"We test this hypothesis in our second set of results presented in this section .To evaluate the hypothesis that Boosting may be prone to overfitting we performed a set of experiments using the four ensemble neural network methods .We introduced 5 % , 10 % , 20 % , and 30 % noise 2 into four different data sets .","label":"Future","metadata":{},"score":"44.57883"}
{"text":"Bagging ( Breiman , 1996c ) and Boosting ( Freund & Schapire , 1996 ; Schapire , 1990 ) are two relatively new but popular methods for producing ensembles .In this paper we evaluate these methods on 23 data sets using both neural networks and decision trees as our classification algorithm .","label":"Future","metadata":{},"score":"44.84377"}
{"text":"Bagging ( Breiman , 1996c ) and Boosting ( Freund & Schapire , 1996 ; Schapire , 1990 ) are two relatively new but popular methods for producing ensembles .In this paper we evaluate these methods on 23 data sets using both neural networks and decision trees as our classification algorithm .","label":"Future","metadata":{},"score":"44.84377"}
{"text":"They also remark that ' ' voting techniques usually result in incomprehensible classifiers that can not easily be shown to users ' ' .Comprehensibility is , on the other hand , a hard mining issue [ Buja Lee2001 ] : it depends on parameters such as the type of classifiers used , the algorithm inducing the classifiers , the user mining the outputs , etc . .","label":"Future","metadata":{},"score":"44.847816"}
{"text":"Previous research has shown that an ensemble is often more accurate than any of the single classifiers in the ensemble .Baggi ... \" .An ensemble consists of a set of individually trained classifiers ( such as neural networks or decision trees ) whose predictions are combined when classifying novel instances .","label":"Future","metadata":{},"score":"45.260544"}
{"text":"Previous research has shown that an ensemble is often more accurate than any of the single classifiers in the ensemble .Baggi ... \" .An ensemble consists of a set of individually trained classifiers ( such as neural networks or decision trees ) whose predictions are combined when classifying novel instances .","label":"Future","metadata":{},"score":"45.260544"}
{"text":"In real - world environments it is usually difficult to specify target operating conditions precisely .This uncertainty makes building robust classification systems problematic .We present a method for the comparison of classifier performance that is robust to imprecise class distributions and misclassification costs .","label":"Future","metadata":{},"score":"45.274128"}
{"text":"Ensemble learning with SVM models is particularly useful for semi - supervised tasks .Changes : .The library has been updated and features a variety of new functionality as well as more efficient implementations of original features .The following key improvements have been made : .","label":"Future","metadata":{},"score":"45.32509"}
{"text":"Discussions with previous researchers reveal that many authors concentrated on decision trees due to their fast training speed and well - established default parameter settings .Neural networks present difficulties for testing both in terms of the significant processing time required and in selecting training parameters ; however , we feel there are distinct advantages to including neural networks in our study .","label":"Future","metadata":{},"score":"45.508774"}
{"text":"The region of quadratic - loss optimality of the Bayesian classifier is in fact a second - order infinitesimal fraction of the region of zero - one optimality .This implies that the Bayesian classifier has a much greater range of applicability than previously thought .","label":"Future","metadata":{},"score":"45.77173"}
{"text":"We explore two arcing algorithms , compare them to each other and to bagging , and try to understand how arcing works .We introduce the definitions of bias and variance for a classifier as components of the test set error .","label":"Future","metadata":{},"score":"46.299625"}
{"text":"One of the defining challenges for the KDD research community is to enable inductive learning algorithms to mine very large databases .This paper summarizes , categorizes , and compares existing work on scaling up inductive algorithms .We concentrate on algorithms that build decision trees and rule sets , in order to provide focus and specific details ; the issues and techniques generalize to other types of data mining .","label":"Future","metadata":{},"score":"46.94276"}
{"text":"The technology for building knowledge - based systems by inductive inference from examples has been demonstrated successfully in several practical applications .This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems , and it describes one such syste ... \" .","label":"Future","metadata":{},"score":"47.284874"}
{"text":"Meta - Learning refers to a general strategy that seeks to learn how to combine a number of separate learning processes in an intelligent fashion .We desire a meta - learning arch ... \" .In this paper , we describe a general approach to scaling data mining applications that we have come to call meta - learning .","label":"Future","metadata":{},"score":"47.28565"}
{"text":"s : those with greater representational power , and thus greater ability to respond to the sample , tend to have lower bias , but also higher variance .In particular , Friedman ( 1996 ) has shown , using normal approximations ... . \" ...","label":"Future","metadata":{},"score":"47.385643"}
{"text":"In this paper , we propose a new algorithm that combines the merits of some existing techniques , namely bagging , arcing and stacking .The basic structure of the algorithm resembles bagging .However , the misclassification cost of each training point is repeatedly adjusted according to its observed out - of - bag vote margin .","label":"Future","metadata":{},"score":"47.421"}
{"text":"This conclusion dovetails nicely with Schapire et al . 's [ 1997 ] recent discussion where they note that the effectiveness of a voting method can be measured by examining the margins of the examples .( The margin is the difference between the number of correct and incorrect votes for an example . )","label":"Future","metadata":{},"score":"47.62811"}
{"text":"This conclusion dovetails nicely with Schapire et al . 's [ 1997 ] recent discussion where they note that the effectiveness of a voting method can be measured by examining the margins of the examples .( The margin is the difference between the number of correct and incorrect votes for an example . )","label":"Future","metadata":{},"score":"47.62811"}
{"text":"Freund and Schapire [ 1995,1996 ] propose an algorithm the basis of which is to adaptively resample and combine ( hence the acronym -- arcing ) so that the weights in the resampling are increased for those cases most often misclassified and the combining is done by weighted voting .","label":"Future","metadata":{},"score":"47.65944"}
{"text":"This class is also the dual of the one implicitly used by Buja Lee ( 2001 ) to cast their size measure for decision trees ( to state whether the concept represented is simple or not ) .It is our aim in this paper to propose theoretical results and approximation algorithms related to the induction of very particular voting classifiers , drawing their roots on simple rule sets ( like DNF ) , with the objective to keep a tradeoff between simplicity and accuracy .","label":"Future","metadata":{},"score":"47.924847"}
{"text":"Despite many studies and conjectures , the reasons behind this improved performance and understanding of the underlying probabilistic structures remain open and challenging problems .More recently , diagnostics such as edge and margin ( Breiman , 1997 ; Freund & Schapire , 1997 ; Schapire et al .","label":"Future","metadata":{},"score":"47.978317"}
{"text":"36 , no . 1 - 2 , pp .105 - 139 , 1999 , citeseer .ist.psu .edubauer99empirical.html . [5 ] T.G. Dietterich , \" An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees : Bagging , Boosting , and Randomization , \" Machine Learning , vol .","label":"Future","metadata":{},"score":"48.130287"}
{"text":"This extreme . \" ...Machine Learning research has been making great progress in many directions .This article summarizes four of these directions and discusses some current open problems .The four directions are ( a ) improving classification accuracy by learning ensembles of classifiers , ( b ) methods for scaling up super ... \" .","label":"Future","metadata":{},"score":"48.32175"}
{"text":"Some previous studies comparing ensemble methods are reviewed , and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly . by Thomas G. Dietterich , Doug Fisher - Bagging , boosting , and randomization .Machine Learning , 2000 . \" ... Abstract .","label":"Future","metadata":{},"score":"48.46309"}
{"text":"For each data set we created a simple hyperplane concept based on a set of the features ( and also included some irrelevant features ) .A set of random points were then generated and labeled based on which side of the hyperplane they fell .","label":"Future","metadata":{},"score":"49.04367"}
{"text":"This paper focussed primarily on issues related to the accuracy and efficacy of metalearning as a general strategy .A number of empirical results are presented demonstrating that meta - learning is technically feasible in wide - ar ... . ... ach that may exhibit favorable scaling characteristics as we discuss later .","label":"Future","metadata":{},"score":"49.188072"}
{"text":"We use scatterplots that graphically show how AdaBoost reweights instances , emphasizing not only \" hard \" areas but also outliers and noise . \" ...The problem of combining preferences arises in several applications , such as combining the results of different search engines .","label":"Future","metadata":{},"score":"49.576477"}
{"text":"This article summarizes four of these directions and discusses some current open problems .The four directions are ( a ) improving classification accuracy by learning ensembles of classifiers , ( b ) methods for scaling up supervised learning algorithms , ( c ) reinforcement learning , and ( d ) learning complex stochastic models . by Wei Fan , Salvatore J. Stolfo - In Proc . 16th","label":"Future","metadata":{},"score":"49.638256"}
{"text":"Yi Zhang , W. Nick Street , \" Bagging with Adaptive Costs \" , IEEE Transactions on Knowledge & Data Engineering , vol.20 , no .5 , pp .577 - 588 , May 2008 , doi:10.1109/TKDE.2007.190724 .[ 1 ] A. Krogh and J. Vedelsby , \" Neural Network Ensembles , Cross Validation , and Active Learning , \" Advances in Neural Information Processing Systems , G. Tesauro , D. Touretzky , and T. Leen , eds . , vol .","label":"Future","metadata":{},"score":"50.04386"}
{"text":"AdaCost , a variant of AdaBoost , is a misclassification cost - sensitive boosting method .It uses the cost of misclassifications to update the training distribution on successive boosting rounds .The purpose is to reduce the cumulative misclassification cost more than AdaBoost .","label":"Future","metadata":{},"score":"50.17036"}
{"text":"AdaCost , a variant of AdaBoost , is a misclassification cost - sensitive boosting method .It uses the cost of misclassifications to update the training distribution on successive boosting rounds .The purpose is to reduce the cumulative misclassification cost more than AdaBoost .","label":"Future","metadata":{},"score":"50.17036"}
{"text":"This paper reviews these methods and explains why ensembles can often perform better than any single classifier .Some previous studies comparing ensemble methods are reviewed , and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly . by Thomas G. Dietterich , Doug Fisher - Bagging , boosting , and randomization .","label":"Future","metadata":{},"score":"50.506607"}
{"text":"Notice that this problem might hold for any class of concept representation integrating an ordering prior to classification : decision lists [ Rivest1987 ] , alternating decision trees [ Freund Mason1999 ] , branching programs [ Mansour McAllester2000 ] , etc . .","label":"Future","metadata":{},"score":"50.810356"}
{"text":"In opposition to the classical decision tree induction method , the trees of the ensemble are built by selecting the tests during their induction fully at random .This extreme ... \" .This paper presents a new learning algorithm based on decision tree ensembles .","label":"Future","metadata":{},"score":"50.867844"}
{"text":"Accordingly , the recommended way of applying one of these methods in a domain with differing misclassification costs is to learn a classifier from the training set as given , and then to compute optimal decisions ... . \" ...In this paper we present a component based person detection system that is capable of detecting frontal , rear and near side views of people , and partially occluded persons in cluttered scenes .","label":"Future","metadata":{},"score":"51.04354"}
{"text":"We trained five ensembles of neural networks ( perceptrons ) for each data set and averaged the ensembles ' predictions .Thus these experiments involve learning in situations where the original bias of the learner ( a single hyperplane produced by a perceptron ) is appropriate for the problem , and as Friedman et al .","label":"Future","metadata":{},"score":"51.32012"}
{"text":"In this decompo ... . \" ...Recently , mining data streams with concept drifts for actionable insights has become an important and challenging task for a wide range of applications including credit card fraud protection , target marketing , network intrusion detection , etc .","label":"Future","metadata":{},"score":"51.683754"}
{"text":"Baggi ... \" .An ensemble consists of a set of individually trained classifiers ( such as neural networks or decision trees ) whose predictions are combined when classifying novel instances .Previous research has shown that an ensemble is often more accurate than any of the single classifiers in the ensemble .","label":"Future","metadata":{},"score":"52.017273"}
{"text":"We characterize precisely but intuitively when a cost matrix is reasonable , and we show how to avoid the mistake of defining a cost matrix that is economically i ... \" .This paper revisits the problem of optimal learning and decision - making when different misclassification errors incur different penalties .","label":"Future","metadata":{},"score":"52.035553"}
{"text":"Recently , mining data streams with concept drifts for actionable insights has become an important and challenging task for a wide range of applications including credit card fraud protection , target marketing , network intrusion detection , etc .Conventional knowledge discovery tools are facing two challenges , the overwhelming volume of the streaming data , and the concept drifts .","label":"Future","metadata":{},"score":"52.350357"}
{"text":"Empirical evaluations have shown significant reduction in the cumulative misclassification cost over AdaBoost without consuming additional computing power .We do not have any \" business - oriented \" misclassification cost models for the remaining data sets .Instead , we varied the cost ratio R of positive vs. negative from 2 to 9 .","label":"Future","metadata":{},"score":"52.4023"}
{"text":"This is obviously not an absolute rule , rather an approximate proxy for interpretability : pathologic cases exist in which , for example , a large and unbalanced tree can be very simple to understand [ Buja Lee2001 ] .Note that in this example , the authors explain that the tree is simple because all its nodes can be described using few clauses .","label":"Future","metadata":{},"score":"52.696667"}
{"text":"In Figure 10 we show the reduction in error rate for each of the ensemble methods compared to using a single neural network classifier .These results demonstrate that as the noise level grows , the efficacy of the Simple and Bagging ensembles generally increases while the Arcing and Ada - Boosting ensembles gains in performance are much smaller ( or may actually decrease ) .","label":"Future","metadata":{},"score":"52.952263"}
{"text":"Stacked generalization is a general method of using a high - level model to combine lower - level models to achieve greater predictive accuracy .We find that best results are obtained when the higher - level model combines the confidence ( and not just the predictions ) of the lower - level ones . by Philip Chan , Salvatore J. Stolfo - Journal of Intelligent Information Systems , 1996 .","label":"Future","metadata":{},"score":"52.990307"}
{"text":"The provided framework also allows you to efficiently program your own , novel aggregation schemes .Full code transition to C++11 , the latest C++ standard , which enabled various performance improvements .The new release requires moderately recent compilers , such as gcc 4.7.2 + or clang 3.2 + .","label":"Future","metadata":{},"score":"53.002293"}
{"text":"Since both of these are embarassingly parallel , this has induced a significant speedup ( 3-fold on quad - core ) .Extensive programming framework for aggregation of base model predictions which allows highly efficient prototyping of new aggregation approaches .","label":"Future","metadata":{},"score":"53.068596"}
{"text":"They were looking for those two or three attributes and values ( e.g. a combination of geographic and industries ) where something ' ' interesting ' ' was happening .In addition , they felt it was too limiting that the nodes in a decision tree represent rules that all start with the same attributes . ' '","label":"Future","metadata":{},"score":"53.113354"}
{"text":"Although there are proven connections between diversity and accuracy in some special cases , our results raise some doubts about the usefulness of diversity measures in building classifier ensembles in real - life pattern recognition problems .Recent advances in the study of voting classification algorithms have brought empirical and theoretical results clearly showing the discrimination power of ensemble classifiers [ Bauer Kohavi1999 , Breiman1996 , Dietterich2000 , Opitz Maclin1999 , Schapire Singer1998 ] .","label":"Future","metadata":{},"score":"53.237915"}
{"text":"We observed that Arc - x4 behaves differently than AdaBoost if reweighting is used instead of resampling , indicating a fundamental difference .Voting variants , some of which are introduced in this paper , include : pruning versus no pruning , use of probabilistic estimates , weight perturbations ( Wagging ) , and backfitting of data .","label":"Future","metadata":{},"score":"53.383354"}
{"text":"In the feature subset selection problem , a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention , while ignoring the rest .To achieve the best possible performance with a particular learning algorithm on a particular training set , a feature subset selection method should consider how the algorithm and the training set interact .","label":"Future","metadata":{},"score":"53.492405"}
{"text":"Figure 10 : Simple , Bagging , and Boosting ( Arcing and Ada ) neural network ensemble reduction in error as compared to using a single neural network .To further demonstrate the effect of noise on Boosting we created several sets of artificial data specifically designed to mislead Boosting methods .","label":"Future","metadata":{},"score":"53.625443"}
{"text":"This suggests that Boosting 's poor performance for certain data sets may be partially explained by overfitting noise .Figure 10 : Simple , Bagging , and Boosting ( Arcing and Ada ) neural network ensemble reduction in error as compared to using a single neural network .","label":"Future","metadata":{},"score":"53.710297"}
{"text":"For instance , Freund and Schapire [ 16 ] tested AdaBoost on a set of UCI benchmark datasets [ 27 ] using C4.5 [ 29 ] as a weak learning algorithm , as well as an algorithm whichsFigure 5 : A sample of the e .. by Thomas G. Dietterich - MULTIPLE CLASSIFIER SYSTEMS , LBCS-1857 , 2000 . \" ...","label":"Future","metadata":{},"score":"53.83799"}
{"text":"The motivatio ... \" .In this paper we present a component based person detection system that is capable of detecting frontal , rear and near side views of people , and partially occluded persons in cluttered scenes .The framework that is described here for people is easily applied to other objects as well .","label":"Future","metadata":{},"score":"53.866325"}
{"text":"For the experiments shown below we generated five data sets where the concept was based on two linear features , had four irrelevant features , and 20 % of the data was mislabeled .We trained five ensembles of neural networks ( perceptrons ) for each data set and averaged the ensembles ' predictions .","label":"Future","metadata":{},"score":"53.88689"}
{"text":"Figure 11 shows the resulting error rates for Ada - Boosting , Arcing , and Bagging by the number of networks being combined in the ensemble .These results indicate clearly that in cases where there is noise Bagging 's error rate will not increase as the ensemble size increases whereas the error rate of the Boosting methods may indeed increase as ensemble size increases .","label":"Future","metadata":{},"score":"54.89911"}
{"text":"Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain .We study the strengths and weaknesses of the wrapper approach andshow a series of improved designs .We compare the wrapper approach to induction without feature subset selection and to Relief , a filter approach to feature subset selection .","label":"Future","metadata":{},"score":"55.134758"}
{"text":"The paper concludes with illustrations of current research directions . \" ...Abstract .Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest .","label":"Future","metadata":{},"score":"55.14332"}
{"text":"Computational experiments show that this algorithm performs consistently better than bagging and arcing with linear and nonlinear base classifiers .In view of the characteristics of bacing , a hybrid ensemble learning strategy , which combines bagging and different versions of bacing , is proposed and studied empirically .","label":"Future","metadata":{},"score":"55.820526"}
{"text":"A set of random points were then generated and labeled based on which side of the hyperplane they fell .Then a certain percentage of the points on one side of the hyperplane were mislabeled as being part of the other class .","label":"Future","metadata":{},"score":"56.157524"}
{"text":"We introduced 5 % , 10 % , 20 % , and 30 % noise 2 into four different data sets .At each level we created five different noisy data sets , performed a 10-fold cross validation on each , then averaged over the five results .","label":"Future","metadata":{},"score":"56.46845"}
{"text":"Introduction The knowledge discovery and data ... . by Kai Ming Ting , Ian H. Witten - Journal of Artificial Intelligence Research , 1999 . \" ...Stacked generalization is a general method of using a high - level model to combine lower - level models to achieve greater predictive accuracy .","label":"Future","metadata":{},"score":"56.68943"}
{"text":"We then describe and analyze a new boosting algorithm for combining preferences called RankBoost .We also describe an efficient implementation of the algorithm for certain natural cases .We discuss two experiments we carried out to assess the performance of RankBoost .","label":"Future","metadata":{},"score":"57.205013"}
{"text":"We measure tree sizes and show an interesting positive correlation between the increase in the average tree size in AdaBoost trials and its success in reducing the error .We compare the mean - squared error of voting methods to non - voting methods and show that the voting methods lead to large and significant reductions in the mean - squared errors .","label":"Future","metadata":{},"score":"57.20566"}
{"text":"The \" early stopping \" feature can now based on any metric output with the --outputinfo command line argument .About : The EnsembleSVM library offers functionality to perform ensemble learning using Support Vector Machine ( SVM ) base models .In particular , we offer routines for binary ensemble models using SVM base classifiers .","label":"Future","metadata":{},"score":"57.555458"}
{"text":"About : MultiBoost is a multi - purpose boosting package implemented in C++ .It is based on the multi - class / multi - task AdaBoost .MH algorithm [ Schapire - Singer , 1999].Basic base learners ( stumps , trees , products , Haar filters for image processing ) can be easily complemented by new data representations and the corresponding base learners , without interfering with the main boosting engine .","label":"Future","metadata":{},"score":"58.01133"}
{"text":"This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems , and it describes one such system , ID3 , in detail .Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete .","label":"Future","metadata":{},"score":"58.69736"}
{"text":"At issue is how error in the estimates of these probabilities affects classificat ... \" .Abstract .At issue is how error in the estimates of these probabilities affects classification error when the estimates are used in a classification rule .","label":"Future","metadata":{},"score":"58.789455"}
{"text":"[ 1998 ] suggest , using an additive model may harm performance .Figure 11 shows the resulting error rates for Ada - Boosting , Arcing , and Bagging by the number of networks being combined in the ensemble .These results indicate clearly that in cases where there is noise Bagging 's error rate will not increase as the ensemble size increases whereas the error rate of the Boosting methods may indeed increase as ensemble size increases .","label":"Future","metadata":{},"score":"59.078583"}
{"text":"Abstract .Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest .The generalization error for forests converges a.s . to a limit as the number of trees in the forest becomes large .","label":"Future","metadata":{},"score":"59.186836"}
{"text":"This architecture is known as Adaptive Combination of Classi ers ( ACC ) .The system performs very well and is capable of detecting people even when all components of a person are not found .The performance of the system is signi cantly better than a full body . by David Opitz , Richard Maclin - Journal of Artificial Intelligence Research , 1999 . \" ...","label":"Future","metadata":{},"score":"59.349163"}
{"text":"A first requirement for the algorithm is obviously its generalization abilities : without classification strength , it is pointless to search for interesting models of the data .A second requirement , more related to mining , is the size of the classifiers [ Nock Gascuel1995 , Nock Jappy1998 ] .","label":"Future","metadata":{},"score":"59.7503"}
{"text":"These results demonstrate that as the noise level grows , the efficacy of the Simple and Bagging ensembles generally increases while the Arcing and Ada - Boosting ensembles gains in performance are much smaller ( or may actually decrease ) .Note that this effect is more extreme for Ada - Boosting which supports our hypothesis that Ada - Boosting is more affected by noise .","label":"Future","metadata":{},"score":"59.80088"}
{"text":"Internal estimates monitor error , strength , and correlation and these are used to show the response to increasing the number of features used in the splitting .Internal estimates are also used to measure variable importance .These ideas are also applicable to regression . by Robert E. Schapire , Peter Bartlett , Yoav Freund , Wee Sun Lee - In Proceedings International Conference on Machine Learning , 1997 . \" ...","label":"Future","metadata":{},"score":"61.205402"}
{"text":"Changes : .Moved repository to GitHub , and added thread support to use the main table lookups in multi - threaded code .It is yet another implementation of ( convolutional ) neural network .It is in C++ , with about 1000 lines of network layer implementations , easily configuration via config file , and can get the state of art performance .","label":"Future","metadata":{},"score":"61.812473"}
{"text":"Next : Our Contribution Up :Inducing Interpretable Voting Classifiers Previous : Inducing Interpretable Voting Classifiers . 2002 AI Access Foundation and Morgan Kaufmann Publishers .All rights reserved .Many researchers have investigated the technique of combining the predictions of multiple classifiers to produce a single classifier [ Breiman1996a , Clemen1989 , Perrone1993 , Wolpert1992 ] .","label":"Future","metadata":{},"score":"62.37484"}
{"text":"For this task , we compare the performance of RankBoost to the individual search strategies .The second experiment is a collaborative - filtering task for making movie recommendations .Here , we present results comparing RankBoost to nearest - neighbor and regression algorithms . \" ...","label":"Future","metadata":{},"score":"63.257637"}
{"text":"139 - 157 , 2000 .[17 ] S. Abney , R. Schapire , and Y. Singer , \" Boosting Applied to Tagging and PP Attachment , \" Proc .Joint SIGDAT Conf .About : Generalised Stirling Numbers for Pitman - Yor Processes : this library provides ways of computing generalised 2nd - order Stirling numbers for Pitman - Yor and Dirichlet processes .","label":"Future","metadata":{},"score":"64.21639"}
{"text":"Foster Provost , Venkateswarlu Kolluri - Data Mining and Knowledge Discovery , 1999 .One of the defining challenges for the KDD research community is to enable inductive learning algorithms to mine very large databases .This paper summarizes , categorizes , and compares existing work on scaling up inductive algorithms .","label":"Future","metadata":{},"score":"65.3613"}
{"text":"We review these algorithms and describe a large empirical study comparing several variants in co ... \" .Methods for voting classification algorithms , such as Bagging and AdaBoost , have been shown to be very successful in improving the accuracy of certain classifiers for artificial and real - world datasets .","label":"Future","metadata":{},"score":"70.4857"}
{"text":"The API and ABI have undergone significant changes , many of which are due to the transition to C++11 .Variance reduction trends on ' boosted ' classifiers .Copyright  2004 Virginia Wheway .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .","label":"Future","metadata":{},"score":"117.74567"}
