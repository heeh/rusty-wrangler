{"text":"In practice , data are often not linearly separable ; and even if they are , a greater margin can be achieved by allowing the classifier to misclassify some points - see Figure 3 .Theory and experimental results show that the resulting larger margin will generally provide better performance than the hard margin SVM .","label":"Motivation","metadata":{},"score":"25.748596"}{"text":"Thus , we are characterizing a sequence by its effect on the model .Other kernels based on probabilistic models include the Covariance kernel [ 40 ] and Marginalized kernels [ 41 ] .Summary and Further Reading .This tutorial introduced the concepts of large margin classification as implemented by SVMs , an idea that is both intuitive and also supported by theoretical results in statistical learning theory .","label":"Motivation","metadata":{},"score":"28.087934"}{"text":"We refer the reader to the publications listed in Section 14.7 for a treatment of the bias - variance tradeoff that takes into account these complexities .In this section , linear and nonlinear classifiers will simply serve as proxies for weaker and stronger learning methods in text classification .","label":"Motivation","metadata":{},"score":"28.549927"}{"text":"The bias - variance tradeoff provides insight into their success .Typical classes in text classification are complex and seem unlikely to be modeled well linearly .However , this intuition is misleading for the high - dimensional spaces that we typically encounter in text applications .","label":"Motivation","metadata":{},"score":"28.628036"}{"text":"However , with a neural network classifier and much higher dimensionality features , all feature sets perform similarly in terms of classification accuracy .As just illustrated , dimensionality reduction is not necessarily advantageous in terms of accuracy for classifiers trained with enough data and the \" right \" classifier .","label":"Motivation","metadata":{},"score":"30.66258"}{"text":"We show that , in spite of similar performance overall , the two models produce different types of errors , in a w ... \" .We present a comparative error analysis of the two dominant approaches in datadriven dependency parsing : global , exhaustive , graph - based models , and local , greedy , transition - based models .","label":"Motivation","metadata":{},"score":"30.920506"}{"text":"If I can reasonably imagine that the new system B might possibly be scaled up and , if so , I think it would continue to do well , then I 'm not unhappy with a gimped comparison .For example , I can probably buy the syntactic language modeling example above ( and to a lesser degree the MT example ) .","label":"Motivation","metadata":{},"score":"30.931406"}{"text":"What about the histogram method that we introduced on day3 ?It can also be used to classify text documents .In fact , it has a lot in common with logistic regression .Both are linear classifiers , in the sense that they take a linear function of the predictors and compare it to zero .","label":"Motivation","metadata":{},"score":"31.21437"}{"text":"The variance is higher than for the one - feature classifiers , but still small : The dashed line with long dashes deviates only slightly from the true boundary between the two classes , and so will almost all linear decision boundaries learned from training sets .","label":"Motivation","metadata":{},"score":"31.438179"}{"text":"In this article , we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing , formalized as transition systems .We then describe and analyze two families of such algorithms : stack - based and list - based algorithms .","label":"Motivation","metadata":{},"score":"31.51855"}{"text":"Otherwise , one will overestimate the accuracy of the classifier on unseen data points .Techniques such as N -fold cross - validation can help if the parts become too small to reliably measure prediction performance ( see , for example , [ 47 ] , [ 48 ] ) .","label":"Motivation","metadata":{},"score":"31.668812"}{"text":"Our goal in text classification then is to find a classifier such that , averaged over documents , is as close as possible to the true probability .We measure this using mean squared error : . where is the expectation with respect to .","label":"Motivation","metadata":{},"score":"32.957977"}{"text":"For large margin separation , it turns out that not the exact location but only the relative position or similarity of the points to each other is important .In the simplest case of linear classification , the similarity of two objects is computed by the dot - product ( a.k.a . scalar or inner product ) between the corresponding feature vectors .","label":"Motivation","metadata":{},"score":"32.964577"}{"text":"If the underlying assumption of zero mean multivariate Gaussian random variables is satisfied , then this method of feature reduction generally performs very well .The principal components are also statistically independent for the Gaussian case .The principal components \" account for \" or explain the maximum amount of variance of the original data .","label":"Motivation","metadata":{},"score":"33.046055"}{"text":"We demonstrate the effectiveness of the approach in a series of dep ... \" .We present a simple and effective semisupervised method for training dependency parsers .We focus on the problem of lexical representation , introducing features that incorporate word clusters derived from a large unannotated corpus .","label":"Motivation","metadata":{},"score":"33.2604"}{"text":"Logistic regression could easily have found the same linear classifier as the histogram method .But it chose another classifier which seemed better on the training set .The additional constraint imposed by the histogram method has helped it find a better classifier when the amount of data is small .","label":"Motivation","metadata":{},"score":"33.54013"}{"text":"For some problems , there exists a nonlinear classifier with zero classification error , but no such linear classifier .Does that mean that we should always use nonlinear classifiers for optimal effectiveness in statistical text classification ?To answer this question , we introduce the bias - variance tradeoff in this section , one of the most important concepts in machine learning .","label":"Motivation","metadata":{},"score":"33.615692"}{"text":"In this paper , we show how these results can be exploited to improve parsing accuracy by integrating a graph ... \" .Previous studies of data - driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference .","label":"Motivation","metadata":{},"score":"33.74505"}{"text":"Here , the margin of a linear classifier is defined as the distance of the closest example to the decision boundary , as shown in Figure 2 .Let us adjust b such that the hyperplane is half way in between the closest positive and negative examples .","label":"Motivation","metadata":{},"score":"33.858418"}{"text":"We can think of bias as resulting from our domain knowledge ( or lack thereof ) that we build into the classifier .If we know that the true boundary between the two classes is linear , then a learning method that produces linear classifiers is more likely to succeed than a nonlinear method .","label":"Motivation","metadata":{},"score":"34.697266"}{"text":"Normalization .Large margin classifiers are known to be sensitive to the way features are scaled ( see , for example [ 42 ] , in the context of SVMs ) .It can therefore be essential to normalize the data .","label":"Motivation","metadata":{},"score":"34.710785"}{"text":"A smaller value of C allows us to ignore points close to the boundary , and increases the margin .The decision boundary between negative examples and positive examples is shown as a thick line .Instead of the abstract idea of points in space , one can think of our data points as representing objects using a set of features derived from measurements performed on each object .","label":"Motivation","metadata":{},"score":"34.813423"}{"text":"Conclusions In this paper we presented a sparse version of the kernel lo- gistic regression for the task of speaker identification .The two KLR classification methods outperform the SVM and the GMM baseline system on the speaker identification task .Furthermore , theSKLRprovidesaverysparsesolutionbyanincrementalfea- ture selection procedure .","label":"Motivation","metadata":{},"score":"34.900726"}{"text":"If the data are primarily clustered on curved subspaces embedded in high dimensionality feature spaces , linear transformations for feature dimensionality reduction are not well suited .For example , the data depicted in Figure 2 would be better approximated by its position with respect to a curved U - shape line rather the straight line obtained with linear PCA .","label":"Motivation","metadata":{},"score":"35.01078"}{"text":"The performance of classification algorithms is application specific and depends on the underlying theoretical foudations of each classifier .We comparatively evaluated the performances of a range of text classification algorithms , including naïve- bayes , maximum entropy and decision trees .","label":"Motivation","metadata":{},"score":"35.230648"}{"text":"A consequence is that the logistic regression coefficients are determined completely by the boundary points ( three points , in this case ) .Some people say this is a good thing , because it makes the estimates robust to noise in most of the dataset .","label":"Motivation","metadata":{},"score":"35.53588"}{"text":"Nevertheless , it has been shown that such algorithms , combi ... \" .Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars .","label":"Motivation","metadata":{},"score":"35.793663"}{"text":"Chen et al.5apply SVMs to document classification of biological literature .Brank et al.6study the interaction of feature ranking and selection with the learning algorithm , in particular feature selection through linear SVMs , which then are used to train the SVM classifier .","label":"Motivation","metadata":{},"score":"36.14108"}{"text":"Eventually it got to a point where every new classification algorithm was called a neural network .Consequently the term carries little information today .So what 's the difference between logistic regression and the histogram method ?For one thing , the histogram method has a very specific idea of what the predictors are .","label":"Motivation","metadata":{},"score":"36.153393"}{"text":"As a theory of learning , this seems too crude .Bayesian statistics provides a better explanation .According to Bayesian statistics , most classifiers , including logistic regression and trees , are using the wrong criteria to learn from a training set .","label":"Motivation","metadata":{},"score":"36.22298"}{"text":"Relative performance difference in the cross - validated training set of including a single new feature relative to the baseline system that used only unigrams , evaluated using the \" Oa \" optimization , sorted by F 1 .See Table 2 for an explanation ... .","label":"Motivation","metadata":{},"score":"36.279743"}{"text":"While this difference is worth noting , it does not preclude us from investigating off - the - shelf implementations of classifiers used for other NLP tasks .It simply requires recasting into the more familiar problem where the classifier assigns exactly one label to each sentence .","label":"Motivation","metadata":{},"score":"36.392395"}{"text":"There are newer versions of these methods such as Heteroscedastic Discriminant Analysis ( HDA ) ( Kumar & Andreou , 1998 ; Saon et al . , 2000 ) .However , in all cases certain assumptions are made about the statistical properties of the original data ( such as multivariate Gaussian ) ; even more fundamentally , the transformations are restricted to be linear .","label":"Motivation","metadata":{},"score":"36.504997"}{"text":"Selecting an appropriate learning method is therefore an unavoidable part of solving a text classification problem .Throughout this section , we use linear and nonlinear classifiers as prototypical examples of ' ' less powerful ' ' and ' ' more powerful ' ' learning , respectively .","label":"Motivation","metadata":{},"score":"36.58638"}{"text":"We look at two strategies and provide convergence bounds for a particular mode of distributed structured perceptron training based on iterative parameter mixing ( or averaging ) .We present experiments on two structured prediction problems - namedentity recognition and dependency parsing - to highlight the efficiency of this method . ... converged models .","label":"Motivation","metadata":{},"score":"36.781185"}{"text":"As another powerful approach , the softmax function takes all the nodes in a layer into account and calculates the output of a node as a posterior probability .When the outputs of the network are to be used as transformed features for the HMM recognition , a linear function or a softmax function is appropriate to generate the data with a more diverse distribution , such as one that would be well - modeled with a GMM .","label":"Motivation","metadata":{},"score":"36.81137"}{"text":"Normalization can be performed at the level of the input features or at the level of the kernel ( normalization in feature space ) , or both .When features are measured in different scales and have different ranges of possible values , it is often beneficial to scale them to a common range , e.g. , by standardizing the data ( for each feature , subtracting its mean and dividing by its standard deviation ) .","label":"Motivation","metadata":{},"score":"36.86319"}{"text":"However , the PICO framework only provides a task - based model of the clinical information - seeking process ; no operational methods are provided in the framework to automatically extract structured information .In an RCT abstract , the sentence type information ( e.g. , background , objective , method , result and conclusion ) as well as sentence content , is useful to automatically extract information to populate either the PICO model or the RCT Schema .","label":"Motivation","metadata":{},"score":"36.98748"}{"text":"Because noise documents can move the decision boundary arbitrarily , test documents close to noise documents in the training set will be misclassified - something that a linear learning method is unlikely to do .It is perhaps surprising that so many of the best - known text classification algorithms are linear .","label":"Motivation","metadata":{},"score":"37.09545"}{"text":"Page 2 .Score Normalization There are several methods of score normalization in speaker recognition systems , e.g the T - norm .This normalization has no effect on the decision of equation ( 4 ) , but we will need this normalization for the comparison of the different scores produced by the GMM , SVM and ( S)KLR models .","label":"Motivation","metadata":{},"score":"37.195923"}{"text":"However , it is also possible that one of the nonlinear methods for feature reduction is more effective , that is enable higher ASR accuracy , than any of the linear methods .The comparisons of these various methods can only be done experimentally .","label":"Motivation","metadata":{},"score":"37.398834"}{"text":"The explicit assumption for LDA is that the within class covariance of each category is the same , which is rarely true in practice .Nevertheless , for many practical classification problems , features reduced by LDA often are as effective or even advantageous to original higher dimensional features .","label":"Motivation","metadata":{},"score":"37.54142"}{"text":"However , the recognition accuracy is often degraded .The more hidden nodes a network has , the more complex a decision surface can be formed , and thus better classification accuracy can be expected ( Meng , 2006 ) .Generally , the number of hidden nodes is empirically determined by a combination of accuracy and computational considerations , as applied to a particular application .","label":"Motivation","metadata":{},"score":"37.725525"}{"text":"We call this the emotionless classifier , since it differentiates between sentences \" with emotion \" and \" emotionless \" sentences .We combined the output of the emotionless classifier with each of the individual emotion classifiers described above .Given a sentence that was identified as having some emotion by the emotionless classifier , we slightly increased the confidence of each of the individual emotion classifiers for that sentence .","label":"Motivation","metadata":{},"score":"37.78145"}{"text":"Given the across - the - board improvement due to split point optimization , Table 4 presents the results of using each of the features individually ( relative to using only using word unigrams ) using only this optimization .Table 5 shows the performance of the single best classifier using cross - validation .","label":"Motivation","metadata":{},"score":"37.86878"}{"text":"This begs the question whether the machinery of linear classifiers can be extended to generate nonlinear decision boundaries .Furthermore , can we handle domains such as biological sequences where a vector space representation is not necessarily available ?There is a straightforward way of turning a linear classifier nonlinear , or making it applicable to nonvectorial data .","label":"Motivation","metadata":{},"score":"38.01506"}{"text":"This results in high variation from training set to training set .High - variance learning methods are prone to overfitting the training data .The goal in classification is to fit the training data to the extent that we capture true properties of the underlying distribution .","label":"Motivation","metadata":{},"score":"38.201073"}{"text":"To simplify the calculations in this section , we do not count the number of errors on the test set when evaluating a classifier , but instead look at how well the classifier estimates the conditional probability of a document being in a class .","label":"Motivation","metadata":{},"score":"38.347904"}{"text":"Indeed , the performance of the classifiers for these emotions was so poor that we had better results simply ignoring these emotions rather than include them in our final labeling .Improving our performance on these emotions should be the focus of the continuing development of this work ; we suspect that additional training data would have aided .","label":"Motivation","metadata":{},"score":"38.355354"}{"text":"We define a classifier to be optimal for a distribution if it minimizes .Minimizing MSE is a desideratum for classifiers .We also need a criterion for learning methods .Recall that we defined a learning method as a function that takes a labeled training set as input and returns a classifier .","label":"Motivation","metadata":{},"score":"38.437767"}{"text":"Another problem with maximum - likelihood logistic regression is that it will always choose a classifier which separates the data , if this is possible .Along with maximum - margin , this is what causes logistic regression to overfit .A proper criterion would allow logistic regression to make errors on the training set , even if it did n't have to .","label":"Motivation","metadata":{},"score":"38.59639"}{"text":"For good classifications we should want to use as much information about the message as possible .Logistic regression is quite different from trees in that it uses all of the variables .We can make a logistic regression classifier as follows : .","label":"Motivation","metadata":{},"score":"38.864525"}{"text":"We first describe the standard methods in speaker recognition in section 2 . section 3 and 4 we give a short overview about kernel - based methods and the SVM respectively .We give a brief introduc- tion to logistic regression and its non - linear extension to kernel functions in section 5 .","label":"Motivation","metadata":{},"score":"38.920174"}{"text":"Log - linear and maximum - margin models are two commonly - used methods in supervised machine learning , and are frequently used in structured prediction problems .Efficient learning of parameters in these models is therefore an important problem , and becomes a key factor when learning from very large data sets .","label":"Motivation","metadata":{},"score":"39.011482"}{"text":"We have demonstrated that introducing the emotionless classifier to boost the confidence of our labelings provided a large increase in recall ; however , this yielded a large decrease in precision , with the F 1 score remaining largely unchanged .We also explored using logistic regression to select a panel of orthogonal classifiers with combinations of features that might better balance precision and recall .","label":"Motivation","metadata":{},"score":"39.128483"}{"text":"The predictors must refer to disjoint possibilities .For example , word.freq.make and capital.run.length.average are not disjoint , because \" make \" could be capitalized .To use the histogram method on the spam dataset , we have to restrict ourselves to the word variables .","label":"Motivation","metadata":{},"score":"39.235413"}{"text":"As we can see the SVM clearly outperforms all other systems , and considering the distribution of the scores in the histograms it is not surprising that the KLR method decreases the baseline error rate more than the SKLR .Table 3 : Equal Error Rates of the speaker identification experi- ments using a decision threshold .","label":"Motivation","metadata":{},"score":"39.330482"}{"text":"Large Margin Separation .Linear separation with hyperplanes .In this section , we introduce the idea of linear classifiers .Support vector machines are an example of a linear two - class classifier .The notation x i will denote the i th vector in a dataset , where y i is the label associated with x i , and n is the number of examples .","label":"Motivation","metadata":{},"score":"39.451157"}{"text":"In this paper , we ... . by Michael Collins , Amir Globerson , Terry Koo , Xavier Carreras , Peter L. Bartlett , 2008 . \" ...Log - linear and maximum - margin models are two commonly - used methods in supervised machine learning , and are frequently used in structured prediction problems .","label":"Motivation","metadata":{},"score":"39.463074"}{"text":"The decision for one learning method vs. another is then not simply a matter of selecting the one that reliably produces good classifiers across training sets ( small variance ) or the one that can learn classification problems with very difficult decision boundaries ( small bias ) .","label":"Motivation","metadata":{},"score":"39.47735"}{"text":"Logistic regression chooses coefficients which separate the training data as well as possible .The histogram method tries to model the two classes , based on an independence assumption .This constraint means that the histogram method can only achieve a subset of the possible linear classifiers .","label":"Motivation","metadata":{},"score":"39.49282"}{"text":"By exploiting the sequential nature of sentences in the abstract , classifiers augmented with HMM achieved significantly better performance than corresponding un - augumented conclusion , we have shown that text classification methods were able to effectively categorize sentences in RCT abstracts when combined with the HMM technique .","label":"Motivation","metadata":{},"score":"39.52858"}{"text":"The problem is having enough data to figure out what that boundary is .If the tree does n't have enough data , it will use a boundary with lots of corners .Therefore logistic regression wins in the above example because the true boundary is close to linear and the training set is n't big enough for the tree to figure this out .","label":"Motivation","metadata":{},"score":"39.61697"}{"text":"Our future work includes experimenting with feature reduction and classifier ensembling techniques to see whether the performance can be further improved .We have , yet , to test our classification method on unstructured abstracts .In McKnight 's study , SVM had similar performance results on structured and unstructured abstracts .","label":"Motivation","metadata":{},"score":"39.68947"}{"text":"Results ( precision : 0.98 , recall : 0.99 ) of our approach significantly outperform previously reported work on automated categorization of sentences in RCT abstracts .Full - text .Combining Text Classification and Hidden Markov Modeling Techniques for Structuring Randomized Clinical Trial Abstracts .","label":"Motivation","metadata":{},"score":"39.719368"}{"text":"\" On the other hand , drug - disease associations in objective sections are weaker .We previously developed an algorithm by combining text classification and hidden Markov modeling techniques to automatically structure MEDLINE abstracts [ 31].In the future , we plan to assign a confidence score to each extracted association by taking sentence type into account .","label":"Motivation","metadata":{},"score":"39.76007"}{"text":"Note that this dataset is much smaller than the original dataset , and is also less unbalanced .In the graphical examples in this tutorial , we show only a small and selected subset of the data suitable for illustration purposes .","label":"Motivation","metadata":{},"score":"39.82228"}{"text":"Thus , linear models in high - dimensional spaces are quite powerful despite their linearity .Even more powerful nonlinear learning methods can model decision boundaries that are more complex than a hyperplane , but they are also more sensitive to noise in the training data .","label":"Motivation","metadata":{},"score":"39.98717"}{"text":"Here , specificity would be the proportion of sentences that should not be annotated with a given emotional label correctly not having that label applied .The use of F 1 score as a summary in this setting completely ignores the true negatives ( ie , those that were correctly not labeled with a given emotion ) .","label":"Motivation","metadata":{},"score":"40.2051"}{"text":"Linear methods like Rocchio and Naive Bayes have a high bias for nonlinear problems because they can only model one type of class boundary , a linear hyperplane .If the generative model has a complex nonlinear class boundary , the bias term in Equation 162 will be high because a large number of points will be consistently misclassified .","label":"Motivation","metadata":{},"score":"40.20538"}{"text":"The extension to continuous speech recognition is done via rescoring of N - best lists or lattices .No preview · Article · Sep 2010 · IEEE Transactions on Audio Speech and Language Processing Day 32 - Applying logistic regression .Today we look at practical aspects of using logistic regression for classification , and how it compares to the other classification methods in this course .","label":"Motivation","metadata":{},"score":"40.28304"}{"text":"This could review posts and activate access to support networks .However , only systems with very high precision would be of any practical value : high precision is more important than high recall because we would not wish to propose interventions unless we were extremely confident in our predictions .","label":"Motivation","metadata":{},"score":"40.357666"}{"text":"In addition , a difficulty in neural network training is that the input data has a wide range of means and variances for each feature component .In order to avoid this , the input data of neural networks is often scaled so that all feature components have the same mean ( zero ) and variance ( so that range of values is approximately ± 1 ) .","label":"Motivation","metadata":{},"score":"40.370033"}{"text":"So what about constrained classifiers ?By the above arguments , constrained classifiers succeed because their constraints bring them close to the average classifier , when the training set is small .Hence some constraints really are better than others .This issue seems to come up implicitly or explicitly in a large variety of circumstances .","label":"Motivation","metadata":{},"score":"40.40983"}{"text":"The model set of background speaker for the ( S)KLR and the SVM systems were trained in the same manner ( one - versus - one ) as the speaker in the closed set .In figure 2 the histograms of the true speakers ( right ) and the aver- age of the 10-best alternatives in the N - best list ( left ) are given .","label":"Motivation","metadata":{},"score":"40.421738"}{"text":"The linear kernel provides a useful baseline , and in many bioinformatics applications it is hard to beat , in particular if the dimensionality of the inputs is large and the number of examples small .The flexibility of the Gaussian and polynomial kernels can lead to overfitting in high - dimensional datasets with a small number of examples , such as in micro - array datasets .","label":"Motivation","metadata":{},"score":"40.709606"}{"text":"This model serves as basis of a structured query interface to retrieve relevant information from RCT papers that are encoded as instances of the RCT Schema .Currently , this information is manually extracted from RCT papers , and is archived in the RCT Bank .","label":"Motivation","metadata":{},"score":"40.93895"}{"text":"The results ( precision of 0.94 , recall of 0.93 ) of our approach are a significant improvement over previously reported work on automated sentences categorization in RCT abstracts .INTRODUCTION .Randomized clinical trial ( RCT ) papers are a critical resource for the biomedical community 's ability to get reliable information on the efficacy of medical interventions .","label":"Motivation","metadata":{},"score":"41.045586"}{"text":"First , many nonlinear models subsume linear models as a special case .For instance , a nonlinear learning method like kNN will in some cases produce a linear classifier .Second , there are nonlinear models that are less complex than linear models .","label":"Motivation","metadata":{},"score":"41.128166"}{"text":"In the running example , we use the region up to 40 nt upstream and downstream of the consensus site AG .For large values of σ ( A ) , the decision boundary is nearly linear .As σ decreases , the flexibility of the decision boundary increases ( B ) .","label":"Motivation","metadata":{},"score":"41.194477"}{"text":"So , is the fair comparison to a similarly limited version of A or to the full - blown A ?Let 's instantiate this with two examples .( Yes , these are examples of papers that have been published ; you can see if you can figure out which ones they are .","label":"Motivation","metadata":{},"score":"41.213554"}{"text":"Linear learning methods have low variance because most randomly drawn training sets produce similar decision hyperplanes .The circular enclave in Figure 14.11 will be consistently misclassified .Nonlinear methods like kNN have high variance .It is apparent from Figure 14.6 that kNN can model very complex boundaries between two classes .","label":"Motivation","metadata":{},"score":"41.338528"}{"text":"The goal of text classification is to label a document with a predefined set of categories .Usually the problem is ap- proached as supervised learning where classifiers are learned from examples in an automated way .A fairly recent development in the machine learning world has been the Management Architecture advent of support vector machines ( SVMs).2,3Joachims4 explores the use of SVMs for learning text classifiers .","label":"Motivation","metadata":{},"score":"41.362915"}{"text":"The standard approach to addressing this issue is to assign a different misclassification cost to each class .For SVMs , this is achieved by associating a different soft - margin constant to each class according to the number of examples in the class ( see , e.g. , [ 43 ] for a general overview of the issue ) .","label":"Motivation","metadata":{},"score":"41.364166"}{"text":"In the next section , we will show that the same holds true for the discriminant function given by Equation 1 .This will allow us to \" kernelize \" the algorithm .Kernels : From Linear to Nonlinear Classifiers .In many applications , a nonlinear classifier provides better accuracy .","label":"Motivation","metadata":{},"score":"41.385944"}{"text":"They determine the margin by which the two classes are separated .We modified the toy dataset by moving the point shaded in gray to a new position indicated by an arrow , which significantly reduces the margin with which a hard - margin SVM can separate the data .","label":"Motivation","metadata":{},"score":"41.592545"}{"text":"For the data in Figure 6 , a degree 2 polynomial is already flexible enough to discriminate between the two classes with a good margin .The degree 5 polynomial yields a similar decision boundary , with greater curvature .Normalization ( cf .","label":"Motivation","metadata":{},"score":"41.645515"}{"text":"Whenever a dataset such as is shown in Figure 1 is linearly separable , i.e. , there exists a hyperplane that correctly classifies all data points , there exist many such separating hyperplanes .We are thus faced with the question of which hyperplane to choose , ensuring that not only the training data , but also future examples , unseen by the classifier at training time , are classified correctly .","label":"Motivation","metadata":{},"score":"41.65989"}{"text":"Approximate solvers that can be trained in linear time without a significant loss of accuracy were also developed [ 61 ] .Another class of software includes machine learning libraries that provide a variety of classification methods and other facilities such as methods for feature selection , preprocessing , etc .","label":"Motivation","metadata":{},"score":"41.781113"}{"text":"Numerous studies in machine learning have shown that this sort of behavior is typical : more constrained classifiers do better on small datasets .But does this mean that constraints are good ?If so , which ones ?Many machine learning researchers have pondered this question , and the answer remains controversial .","label":"Motivation","metadata":{},"score":"41.792652"}{"text":"To exploit the sequential ordering of sentences in an abstract , we used HMM to label sentence types .HMMs are commonly used for speech recognition and biological sequence alignment .In our case , we have transformed the sentence categorization problem into a HMM sequence alignment probem .","label":"Motivation","metadata":{},"score":"41.8351"}{"text":"It is also shown that speech recognition accuracy can be as high as or even higher using reduced dimensionality features versus original features , with \" properly \" trained systems .Background .Both approaches are used .Thus , for good training of model parameters , increasing dimensionality from 40 to 50 , considered a modest increase , could easily increase the need for more data by a factor of 1000 or more .","label":"Motivation","metadata":{},"score":"41.889805"}{"text":"The decision boundary defined by a hyperplane ( cf .Equation 1 ) is said to be linear because it is linear in the input .A classifier with a linear decision boundary is called a linear classifier .In the next section , we introduce one particular linear classifier , the ( linear ) Support Vector Machine , which turns out to be particularly well suited to high - dimensional data .","label":"Motivation","metadata":{},"score":"41.97646"}{"text":"The resulting classifier has a quadratic discriminant function ( see example in Figure 6B ) .This approach of explicitly computing nonlinear features does not scale well with the number of input features .The dimensionality of the feature space associated with the above example is quadratic in the number of dimensions of the input space .","label":"Motivation","metadata":{},"score":"42.005093"}{"text":"All these two parts work together , resulting in very competitive performance with reasonable computational cost .Full - text · Article · Aug 2014 · IEEE / ACM Transactions on Audio , Speech , and Language Processing .[ Show abstract ] [ Hide abstract ] ABSTRACT : The sparsity driven classification technologies have attracted much attention in recent years , due to their capability of providing more compressive representations and clear interpretation .","label":"Motivation","metadata":{},"score":"42.301773"}{"text":"Parser actions are determined by a classifier , based on features that represent the current state of the parser .We apply this pars ... \" .We present a data - driven variant of the LR algorithm for dependency parsing , and extend it with a best - first search for probabilistic generalized LR dependency parsing .","label":"Motivation","metadata":{},"score":"42.32248"}{"text":"Normalization was not applied to the features in order to retain verb tense information , which is important for temporal resolution .These initial features were used to build a linear SVM model for Layer 3 classifi- cation .The features were then ranked by their weights .","label":"Motivation","metadata":{},"score":"42.353386"}{"text":"Guha , R. , McCool , R. , and Miller , E. Semantic Search .Proceedings of WWW 2003 .Sim I , Owens DK , Lavori PW , Rennels GD .Electronic Trial Banks : A Complementary Randomized Trials .Med Decis Making 2000;20(4):440 - 450 . D. Demner - Fushman and J. Lin .","label":"Motivation","metadata":{},"score":"42.376907"}{"text":"Since the overall F 1 score is the mean across all possible other sets of features , one can view the values in Table 6 as an approximation of the additive amount of increase in F 1 that can be obtained by adding each feature to the standard classifier .","label":"Motivation","metadata":{},"score":"42.39462"}{"text":"Intuitively , all other training examples do not contribute to the geometric location of the large margin hyperplane - the solution would have been the same even if they had not been in the training set to begin with .It is thus not surprising that they drop out of the expansion in Equation 5 .","label":"Motivation","metadata":{},"score":"42.499725"}{"text":"Given the HMM model , state probabilities , and the state transition probabilities , the Viterbi algorithm [ 7 ] was used to compute most- likely sequence of states that emit all the sentences in the abstract .Subsequently , the state associated with the sentence was extracted from the most - likely sequence of states .","label":"Motivation","metadata":{},"score":"42.528305"}{"text":"Since the original data is not multivariate Gaussian , the PCA basis vector is no longer a good way to approximate the data .In fact , since the data primarily follows a curved path in the 2-D space , no linear transform method , resulting in a straight line subspace , will be a good way to approximate the data with one dimension .","label":"Motivation","metadata":{},"score":"42.61625"}{"text":"The first stage is based on the unlabeled dependency parsing models described by McDonald and Pereira ( 2006 ) augmented with morphological features for a subset of the languages .The second stage takes the ... \" .We present a two - stage multilingual dependency parser and evaluate it on 13 diverse languages .","label":"Motivation","metadata":{},"score":"42.667915"}{"text":"The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph .We report results on the CoNLL - X shared task ( Buchholz et al . , 2006 ) data sets and present an error analysis . .","label":"Motivation","metadata":{},"score":"42.690865"}{"text":"SVM Classifier We built a linear SVM sentence classifier from the entire corpus with the features described above with an unordered bag - of - words representation using the Weka SVM imple- mentation .Layer 2 : Classifying Nonsmoker Sentences We customized NegEx , a negation detection algorithm de- veloped by Chapman et al.7for the Layer 2 classification of the nonsmoker category .","label":"Motivation","metadata":{},"score":"42.747425"}{"text":"For all discriminatively based transformations , either linear or nonlinear , an implicit assumption is that training data exists which has been labeled according to category .For the case of classification , such as the experiments just described , this labeled data is needed anyway , for both training and test data , to conduct classification experiments ; thus the need for category labeled data is not any extra burden .","label":"Motivation","metadata":{},"score":"42.789017"}{"text":"Discussion .All of the systems we submitted achieved results in line with the performance we obtained using cross - validation on the training set .One possibility is that the individualized optimization of split points on a per - emotion basis used in S 2 led to overtraining .","label":"Motivation","metadata":{},"score":"42.83129"}{"text":"We submitted three sets of results with our top performing models , which we describe in the next section .We used precision , recall , and F - score as our evaluation metrics : Because our system makes assignments to every document that is processed , totalNumberOfDocumentsClassi- fiedByTheSystem and numberOfDocumentsInTestSet are the same .","label":"Motivation","metadata":{},"score":"42.8639"}{"text":"Unlike SVMs the kernel logistic regression also provides an es- timate of the conditional probability of class membership .In speaker identification experiments the SKLR methods out- perform the SVM and the GMM baseline system on the POLY- COST database .Introduction InthisworkwewanttointroducetheSparseKernelLogisticRe- gression as a classification method for text - independent speaker identification .","label":"Motivation","metadata":{},"score":"42.8842"}{"text":"For example , a Naïve Bayes classifier may report a probability of 0.48 for the given sentence to belong to background section , 0.42 for objective section , 0.01 for result section , 0.04 for the method section , and 0.05 that it belongs to the conclusion section .","label":"Motivation","metadata":{},"score":"42.93866"}{"text":"This is our informal evaluation set up for which we report results .For the final competitive evaluation , we trained our models on the data from Set 1 and Set 2 with the best configurations as determined from our informal evaluation experimenta- tion .","label":"Motivation","metadata":{},"score":"43.0099"}{"text":"SVMs use two key concepts to solve this problem : large margin separation and kernel functions .The idea of large margin separation can be motivated by classification of points in two dimensions ( see Figure 1 ) .A simple way to classify the points is to draw a straight line and call points lying on one side positive and on the other side negative .","label":"Motivation","metadata":{},"score":"43.071285"}{"text":"6 Alternatives were generated by applying single character insertions , deletions , substitutions and transpositions .We included \" space \" as a valid insertion character given the large number of tokens in the dataset that were the result of two words joined together without a space ( eg , \" thepapers \" , \" knowit \" ) .","label":"Motivation","metadata":{},"score":"43.086487"}{"text":"This leads to a powerful discriminative classifier that naturally handles sequential data .In this approach , speech classification is done using affine combinations of HMM log - likelihoods .We believe that such combinations of HMMs lead to a more accurate classifier than the conventional HMM - based classifier .","label":"Motivation","metadata":{},"score":"43.137543"}{"text":"Often when data is unbalanced , the cost of misclassification is also unbalanced ; for example , having a false negative is more costly than a false positive .In some cases , considering the SVM score directly rather than just the sign of the score is more useful .","label":"Motivation","metadata":{},"score":"43.17222"}{"text":"Alternatively , the neural network architecture and/or training constraints could be modified so that the nonlinearly transformed features are more suitable as input features for a Hidden Markov Model .References . 2 - S. B. Davis , P. Mermelstein , 1980 Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences , IEEE Trans . on Acoustics , Speech and Signal Processing , 28 357 366 .","label":"Motivation","metadata":{},"score":"43.347412"}{"text":"In contrast to informative classifiers like the linear discriminant analysis ( LDA ) where the underlying class densities are estimated , discriminative classifiers like LR focus on modeling the a - posteriori probability of the membership to each of C classes .","label":"Motivation","metadata":{},"score":"43.471863"}{"text":"However , there were many instances in both the training and test dataset where we found that emotions were inappropriately applied or omitted by the annotators , as detected by our classifiers .For example , the guidelines state that a sentence should be annotated with \" forgiveness \" if the author is forgiving someone , not if the author is asking for forgiveness .","label":"Motivation","metadata":{},"score":"43.70314"}{"text":"The dependency parsing approach presented here extends the existing body of work mainly in four ways : 1 .Although stepwise 1 dependency parsing has commonly been performed using parsing algo1 Stepw ... . \" ...Perceptron training is widely applied in the natural language processing community for learning complex structured models .","label":"Motivation","metadata":{},"score":"43.724148"}{"text":"This is the maximum - likelihood formula given on day31 .Look at what happens when the classes are perfectly separable : In such cases , it can be shown that maximum - likelihood will try to place the boundary as far away from the data as possible .","label":"Motivation","metadata":{},"score":"43.86862"}{"text":"This study has two core objectives .First , we investigate more robust back - ends to address noisy multi - session enrollment data for speaker recognition .This task is achieved by proposing novel back - end algorithms .Second , we construct a highly discriminative speaker verification framework .","label":"Motivation","metadata":{},"score":"43.99553"}{"text":"The area under the curve quantifies the quality of the classifier , and a larger value indicates better performance .Research has shown that it is a better measure of classifier performance than the success or error rate of the classifier [ 15 ] , in particular when the fraction of examples in one class is much smaller than the other .","label":"Motivation","metadata":{},"score":"44.191875"}{"text":"The number of hidden nodes in the second hidden layer was varied from 1 to 39 , according to the dimensionality being evaluated .For the case of NLDA2 , the network used for dimensionality reduction was also a classifier .For the sake of consistency , the outputs of the hidden nodes from the bottleneck neural network were used as features for a classifier , using either another neural network or the MXL classifier .","label":"Motivation","metadata":{},"score":"44.269638"}{"text":"Clearly , for this example , the two basis vectors are quite different , and clearly the projection of data onto the first LDA basis vector would be more effective for separating the two categories than data projected onto the first PCA basis vector .","label":"Motivation","metadata":{},"score":"44.2818"}{"text":"Comparative plot of F1 measure on text- classification algorithms before HMM and after using HMM augmentation .The results show F1 improvements across five type of abstract sections with HMM augmentation .Table 2 shows percentage improvements of using HMM on three classification algorithms NB , ME and DT respectively , averaging across three modification options ( basic , with boosting , with bagging ) and five section types .","label":"Motivation","metadata":{},"score":"44.303734"}{"text":"We apply this parsing framework to both tracks of the CoNLL 2007 shared task , in each case taking advantage of multiple models trained with different learners .In the multilingual track , we train three LR models for each of the ten languages , and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme .","label":"Motivation","metadata":{},"score":"44.324486"}{"text":"Theoretically , the nonlinear methods have the potential to be more \" efficient \" than linear methods , that is , give better representations with fewer dimensions .In addition , some examples are shown from experiments with Automatic Speech Recognition ( ASR ) where the nonlinear methods in fact perform better , resulting in higher ASR accuracy than obtained with either the original speech features , or linearly reduced feature sets .","label":"Motivation","metadata":{},"score":"44.39553"}{"text":"In this chapter , some techniques are presented for reducing feature dimensionality while preserving category ( i.e. , phonetic for the case of speech ) discriminability .Since the techniques presented for reducing dimensionality are statistically based , these methods also are subject to \" curse of dimensionality \" issues .","label":"Motivation","metadata":{},"score":"44.476273"}{"text":"Automated approaches [ 6 ] for labeling sentences in an abstract with subheadings have used machine learning techniques such as Support Vector Machines and linear classifiers .The performance metrics of these approaches range from 60 - 80 % ( precision ) and 65 - 85 % ( recall ) .","label":"Motivation","metadata":{},"score":"44.476738"}{"text":"To test this theory , a logistic regression classifier ( fit ) and a histogram classifier ( fit2 ) were trained on the spam dataset using word variables only .Here are the results : .This is the same train / test split used above , so we see that restricting to word variables has worsened the logistic regression to 216/2300 errors .","label":"Motivation","metadata":{},"score":"44.596413"}{"text":"If our data are high - dimensional to begin with , such as in the case of gene expression data , this is not acceptable .Kernel methods avoid this complexity by avoiding the step of explicitly mapping the data to a high - dimensional feature space .","label":"Motivation","metadata":{},"score":"44.694145"}{"text":"( 30 )But for comparison with binary classifiers like the SVM we de-cided to use a common one - versus - one approach where a clas- sifier learns to discriminate one class from one other class .Experiments We compare the presented approach of ( S)KLR to a baseline GMM and a SVM system on the POLYCOST dataset [ 19].","label":"Motivation","metadata":{},"score":"44.781334"}{"text":"For this case , 2-D pseudo random data was created to lie along a U shaped curve , similar to the data depicted in Figure 2 .A neural network ( 2 - 5 - 1 - 5 - 2 ) was then trained as an identify map .","label":"Motivation","metadata":{},"score":"44.81617"}{"text":"The process iterates until a manageable number of trials is returned .We evaluated eTACTS in terms of filtering efficiency , diversity of the search results , and user eligibility to the filtered trials using both qualitative and quantitative methods .Results eTACTS ( 1 ) rapidly reduced search results from over a thousand trials to ten ; ( 2 ) highlighted trials that are generally not top - ranked by conventional search engines ; and ( 3 ) retrieved a greater number of suitable trials than existing search engines .","label":"Motivation","metadata":{},"score":"44.84952"}{"text":"Ideally , the targets are uncorrelated , which enables quicker convergence of weight updates .The targets can also be viewed as multidimensional vectors , with a value of \" 1 \" for the target category and \" 0s \" for the non - target categories .","label":"Motivation","metadata":{},"score":"44.89892"}{"text":"We have focused on kernels for real - valued and sequence data ; and while this covers many bioinformatics applications , often data is better modeled by more complex data types .Many types of bioinformatics data can be modeled as graphs , and the inputs can be either nodes in the graph , e.g. , proteins in an interaction network , or the inputs can be represented by graphs , e.g. , proteins modeled by phylogenetic trees .","label":"Motivation","metadata":{},"score":"45.03052"}{"text":"Potentially , we could have looked to further improve precision with detrimental effects on recall and F 1 score , though it is encouraging that we achieved high precision while maintaining an F 1 score similar to the mean of all systems submitted to this shared task .","label":"Motivation","metadata":{},"score":"45.21276"}{"text":"The main idea of boosting and bagging is to generate many , relatively weak classification rules and to combine these into a single highly accurate classification rule .To compare the performance of each classification method , the same training and testing samples were used for all classifiers .","label":"Motivation","metadata":{},"score":"45.2314"}{"text":"Overfitting increases MSE and frequently is a problem for high - variance learning methods .We can also think of variance as the model complexity or , equivalently , memory capacity of the learning method - how detailed a characterization of the training set it can remember and then apply to new data .","label":"Motivation","metadata":{},"score":"45.431503"}{"text":"Therefore , an arbitrary number of reduced dimensions can be obtained , independent of the input feature dimensions and the nature of the training targets .A lower dimensional representation of the input features is easily obtained by simply deploying fewer nodes in the middle layer than the input layer .","label":"Motivation","metadata":{},"score":"45.524376"}{"text":"The combination was done by linearly combining the confidence values of each classifier on a sentence - by - sentence basis .Each classifier was equally weighted in this linear combination , so the final confidence value was always equal to the mean of the confidence scores of the individual classifiers .","label":"Motivation","metadata":{},"score":"45.53591"}{"text":"Each classifier performs a binary labeling for that emotion ( estimating it as present or absent ) .Doing so allows us to form the final classification for each sentence by taking the union of the sets assigned by each individual classifier .","label":"Motivation","metadata":{},"score":"45.56629"}{"text":"As seen from the examples in Figures 6 and 7 , the width parameter of the Gaussian kernel and the degree of polynomial kernel determine the flexibility of the resulting SVM in fitting the data .Large degree or small width values can lead to overfitting and suboptimal performance ( Figure 7C ) .","label":"Motivation","metadata":{},"score":"45.59456"}{"text":"Our experiments confirm that the online algorithms are much faster than the batch algorithms in practice .We describe how the EG updates factor in a convenient way for structured prediction problems , allowing the algorithms to be . ... in McDonald et al .","label":"Motivation","metadata":{},"score":"45.737083"}{"text":"By letting one model generate features for the other , we consistently improve accuracy for both models , resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL - X shared task . ...","label":"Motivation","metadata":{},"score":"45.828568"}{"text":"For the discussed splice site data , the results differed considerably when using different normalizations for the linear , polynomial , and Gaussian kernels .Generally , our experience shows that normalization often leads to improved performance for both linear and nonlinear kernels , and can also lead to faster convergence .","label":"Motivation","metadata":{},"score":"45.830223"}{"text":"Also , SVMs were integrated into HMMs to model the acoustic feature vectors [ 3 ] in contin- uous speech recognition .There have been several approaches of integrating SVMs into speaker verification environments . to discriminate between frames but between entire utterances .","label":"Motivation","metadata":{},"score":"46.17474"}{"text":"Such optimization problems can be solved using standard tools from convex optimization ( see , e.g. , [ 21 ] ) .For specific optimization problems like the one above , there exist specialized techniques to efficiently solve such optimization problems for millions of examples or dimensions . )","label":"Motivation","metadata":{},"score":"46.22201"}{"text":"Note that unlike phonetic recognition experiments , for the case of classification , the timing labels in the database are explicitly used for both training and testing .Thus classification is \" easier \" than recognition , and accuracies typically higher , since phone boundaries are known in advance and used .","label":"Motivation","metadata":{},"score":"46.373383"}{"text":"Conclusions : Our study demonstrates the feasibility of establishing a biorepository of plasma , serum and DNA , with relatively rapid annotation of clinical variables using EMR - based algorithms .Abstract .An ensemble of supervised maximum entropy classifiers can accurately detect and identify sentiments expressed in suicide notes .","label":"Motivation","metadata":{},"score":"46.468483"}{"text":"True negatives are not shown in any of the models as they do not impact the F 1 score .Comparing the first and third rows , we can see how introducing bigrams , spelling correction , stemming and dependencies greatly reduced the false positives but at the cost of fewer true positives and more false negatives .","label":"Motivation","metadata":{},"score":"46.475075"}{"text":"Hence , its value is not necessarily connected with the success of identifying rare events .The area under the so - called precision recall curve is better suited to evaluate how well one can find rare events [ 16 ] . )","label":"Motivation","metadata":{},"score":"46.598885"}{"text":"In this section , instead of using the number of correctly classified test documents ( or , equivalently , the error rate on test documents ) as evaluation measure , we adopt an evaluation measure that addresses the inherent uncertainty of labeling .","label":"Motivation","metadata":{},"score":"46.71868"}{"text":"Some of the limitations as applied to the use - case are negation detection and temporal resolution .J Am Med Inform Assoc .2008;15:25 - 28 .DOI 10.1197/jamia .M2437 .Hence , it is difficult to produce comparable results , evaluate techniques , and share platforms .","label":"Motivation","metadata":{},"score":"46.82164"}{"text":"For all cases , including original features , and all versions of the transformed features , a neural network classifier with 100 hidden nodes and 10 output nodes , trained with backpropagation , was used as the classifier .In addition , a Bayesian maximum likelihood Mahalanobis distance based Gaussian assumption classifier ( MXL ) was used for evaluation .","label":"Motivation","metadata":{},"score":"46.85106"}{"text":"HDA suffers from two drawbacks .There is no known closed form solution for minimizing the objective function required to solve for the transformation - rather a complex numerically based gradient search is required .More fundamentally , with actual speech data , HDA alone was found to perform far worse than LDA .","label":"Motivation","metadata":{},"score":"46.91251"}{"text":"This issue of category labels is described in more detail in the following two subsections .Figure 12 .Classification accuracies of neural network ( top panel ) and MXL ( bottom panel ) classifiers using 10 % of group 2 training data for training classifier .","label":"Motivation","metadata":{},"score":"47.03138"}{"text":"The figure style follows that of Figure 3 .The domain knowledge inherent in any classification task is captured by defining a suitable kernel ( i.e. , similarity ) between objects .Running example : Splice site recognition .Throughout this tutorial we are going to use an example problem for illustration .","label":"Motivation","metadata":{},"score":"47.03205"}{"text":"Let 's say I 'm doing syntactic language modeling .In this case , the baseline would be ( say ) a trigram language model .My model requires parse trees to train .So I train my language model on the Penn Treebank .","label":"Motivation","metadata":{},"score":"47.068596"}{"text":"[ 2 ] - Most , but not all researchers , have used the recommended training and test sets .For all ASR experiments reported in this chapter , the SA sentences were removed , the recommended training and test sets were used , and the phone set was collapsed to the same 39 phones used in most ASR experiments with TIMIT .","label":"Motivation","metadata":{},"score":"47.122112"}{"text":"The log of the test / train ratio illustrates whether the training set represented ... .An external team first collated the notes and subsequently anonymized them so that no personal or identifiable data remained ; names and addresses were substituted with a small selection of alternatives .","label":"Motivation","metadata":{},"score":"47.205204"}{"text":"Accessed 11 August 2008 .Logistic regression is a well known classification method in the field of statistical learning .Recently , a kernelized version of logistic regression has become very popular , because it allows non - linear probabilistic classification and shows promising results on several benchmark problems .","label":"Motivation","metadata":{},"score":"47.226692"}{"text":"That is .Y .A .T .X , where X , Y are again column vectors as for PCA .The big difference is in how A is computed .For LDA , it has been shown that the columns of A correspond to the m largest eigenvalues of .","label":"Motivation","metadata":{},"score":"47.285923"}{"text":"clinical question Personalized semantic search , intelligent clinical question answering and summarization are approaches that aim to improve the precision of the search results [ 1 , 2].In the healthcare setting , these approaches entail extracting structured and machine understandable information from papers and patient records .","label":"Motivation","metadata":{},"score":"47.426964"}{"text":"This almost never happens , but I think it would be useful both during reviewing as well as just for posterity . )16 comments : .I agree that we should ideally have both evaluation .If we only have time to do one , though , I would prefer comparing the gimped A , since it is more informative from the research standpoint to reduce the number of free variables and understand how methods really differ .","label":"Motivation","metadata":{},"score":"47.481674"}{"text":"Classifiers performed best when trained on data sets with imbalance ratio below 10 .Conclusions We were able to achieve high sensitivity with moderate specificity for automatic case identification on two data sets of electronic medical records .Such a high - sensitive case identification system can be used as a pre - filter to significantly reduce the burden of manual record validation .","label":"Motivation","metadata":{},"score":"47.546364"}{"text":"The neural network features also should be linearly transformed with a principal components transform in order to be effective for use by a Hidden Markov Model .For use with a multi - hidden - state Hidden Markov Model , the nonlinear transform should be trained with state - specific targets , but using \" do n't cares , \" to account for imprecise information about state boundaries .","label":"Motivation","metadata":{},"score":"47.66008"}{"text":"Here 's what happens if we do the same thing , but with the training set reduced to 231 examples : .Logistic regression does much better on the training set , as expected , but worse on the test set .","label":"Motivation","metadata":{},"score":"47.70498"}{"text":"The features which were dimensionality reduced by PCA and LDA alone were also evaluated for the purpose of comparison .Figure 16 shows recognition accuracies of dimensionality reduced features using 1-state and 3-state HMMs with 3 mixtures per state .","label":"Motivation","metadata":{},"score":"47.767548"}{"text":"[ Show abstract ] [ Hide abstract ] ABSTRACT : Hidden Markov models ( HMMs ) are powerful generative models for sequential data that have been used in automatic speech recognition for more than two decades .Despite their popularity , HMMs make inaccurate assumptions about speech signals , thereby limiting the achievable performance of the conventional speech recognizer .","label":"Motivation","metadata":{},"score":"47.782005"}{"text":"Then a neural network ( 3 - 10 - 2 - 10 - 3 ) was trained as an identity map .After training , the outputs of the neural network are plotted in the right panel of Figure 6 .Clearly the neural network \" learned \" a 2-D internal representation , at the bottleneck layer , from which it could reconstruct the original data .","label":"Motivation","metadata":{},"score":"47.80317"}{"text":"To parse all the sentences in the PDT , one must use a non - projectiv ... .by Ryan McDonald , Kevin Lerman , Fernando Pereira - IN PROCEEDINGS OF THE CONFERENCE ON COMPUTATIONAL NATURAL LANGUAGE LEARNING ( CONLL , 2006 . \" ...","label":"Motivation","metadata":{},"score":"47.877182"}{"text":"Nonlinear Dimensionality Reduction Methods for Use with Automatic Speech Recognition .Stephen A. Zahorian 1 and Hongbing Hu 1 .Introduction .For nearly a century , researchers have investigated and used mathematical techniques for reducing the dimensionality of vector valued data used to characterize categorical data with the goal of preserving \" information \" or discriminability of the different categories in the reduced dimensionality data .","label":"Motivation","metadata":{},"score":"47.88607"}{"text":"Hidden Markov Models ( HMMs ) .Left - to - right Markov models with no skip were used and a total of 48 monophone HMMs were created from the training data using the HTK toolbox ( Verion 3.4 ) ( Young et al . , 2006 ) .","label":"Motivation","metadata":{},"score":"47.886803"}{"text":"In Section 13.1 ( page ) , we said that we want to minimize classification error on the test set .The implicit assumption was that training documents and test documents are generated according to the same underlying distribution .We will denote this distribution where is the document and its label or class . graphclassmodelbernoulligraph were examples of generative models that decompose into the product of and .","label":"Motivation","metadata":{},"score":"47.98185"}{"text":"This analysis leads to new directions for parser development . ... otated corpus .The advantage of such models is that they are easily ported to any domain or language in which annotated resources exist .The first is what Buchholz and Marsi ( 2006 ) call the \" all - pairs \" approach , where every possible arc is considered in the ... . by Kenji Sagae - In Proceedings of the Eleventh Conference on Computational Natural Language Learning , 2007 . \" ...","label":"Motivation","metadata":{},"score":"47.99506"}{"text":"The total effort invested in this project was approximately 160 hours , which includes manual sentence - level annotation , code develop- Table 2 y Best Results .Informal evaluation set up : training on Set 1 ; testing on Set 2 .","label":"Motivation","metadata":{},"score":"48.000862"}{"text":"We show the margin and decision boundary for an SVM with a very high value of C , which mimics the behavior of the hard - margin SVM since it implies that the slack variables ξ i ( and hence training mistakes ) have very high cost .","label":"Motivation","metadata":{},"score":"48.02426"}{"text":"Sev- eral methods like the Fisher - kernel [ 4 ] map the sequences into a highdimensionalscore - spacewhereaSVMisthenusedtoclas- sify the data , e.g. [ 5].Alternatively , one can use the sequence kernel introduced by [ 6 ] which maps the support vectors into a single vector for each speaker by a kernel which is derived from generalized linear discriminants .","label":"Motivation","metadata":{},"score":"48.04946"}{"text":"It is interesting to note that the specificity was extremely high ( and , therefore , very encouraging ) for all of the classifiers that we developed .The annotated datasets were the gold standard in these exercises and reflected the time and effort of many people .","label":"Motivation","metadata":{},"score":"48.202873"}{"text":"But this system used split points tuned specifically for each emotion and ensured at least one annotation per note .We then applied the Memorize labels heuristic .System 3 : Our final system , S 3 , was identical to S 2 except that the split point was the untuned value of 0.5 .","label":"Motivation","metadata":{},"score":"48.33656"}{"text":"The Rocchio classifier ( in form of the centroids defining it ) can not ' ' remember ' ' fine - grained details of the distribution of the documents in the training set .According to Equation 149 , our goal in selecting a learning method is to minimize learning error .","label":"Motivation","metadata":{},"score":"48.359245"}{"text":"A .T .X .Let .X .^ .B .Y be an approximation to .X .Note that .X .Y and .X .^ can all be viewed as ( column ) vector - valued random variables .","label":"Motivation","metadata":{},"score":"48.56848"}{"text":"We did not perform any entity detection on the arguments in the dependency relations , so relations such as \" dobj(love , life ) \" were also conflated into \" dobj(love , x ) \" .The Results section presents the performance of these features using five - fold cross - validation on the training data .","label":"Motivation","metadata":{},"score":"48.583523"}{"text":"Its classification performance is often compared with that of the popular support vector machine ( SVM ) .However , for speech classification , only limited success with PLR has been reported , partially due to the difficulty with sequential data .","label":"Motivation","metadata":{},"score":"48.599815"}{"text":"Variance is the variation of the prediction of learned classifiers : the average squared difference between and its average .Variance is large if different training sets give rise to very different classifiers .It is small if the training set has a minor effect on the classification decisions makes , be they correct or incorrect .","label":"Motivation","metadata":{},"score":"48.71349"}{"text":"Searching for trials relevant to some query is laborious due to the immense number of existing protocols .Apart from search , writing new trials includes composing detailed eligibility criteria , which might be time - consuming , especially for new researchers .","label":"Motivation","metadata":{},"score":"48.78296"}{"text":"However , the F1 measure of the classifiers combined with HMM in our study ranges from 0.86 to 0.98 , precision from 0.88 to 0.98 and recall from 0.81 to 0.99 .This is a significant improvement over McKnight study .We expect to get similar or better performance if we use more samples for training .","label":"Motivation","metadata":{},"score":"48.895073"}{"text":"The same holds true for a large class of linear algorithms , as shown by the representer theorem ( see [ 2 ] ) .Our discriminant function then becomes ( 7 ) .The representation in terms of the variables α i is known as the dual representation ( cf .","label":"Motivation","metadata":{},"score":"48.913498"}{"text":"In contrast with NLDA1 where dimensionality reduction is assigned to PCA , for NLDA2 , since the dimensionality reduction can be accomplished with the neural network only , the linear PCA is used specifically for reducing the feature correlation .Figure 9 .","label":"Motivation","metadata":{},"score":"48.942467"}{"text":"These results show the NLDA methods based on the state level training targets are able to form highly discriminative features in a dimensionality reduced space .MFCC experiments .For comparison , 39-dimensional MFCC features ( 12 coefficients plus energy with the delta and acceleration terms ) were reduced to 36 dimensions with the same configurations and evaluated .","label":"Motivation","metadata":{},"score":"49.01371"}{"text":"Structural and grammatical features should be investigated further .Conclusion .We have shown that it is possible to construct classifiers that can perform well on this task and we have shown that dependency relations can be used effectively as features in these classifiers .","label":"Motivation","metadata":{},"score":"49.09668"}{"text":"\" But this is just deferring the problem .Instead of you deciding which is the appropriate comparison , you are leaving the decision up to someone else .Ultimately , someone has to decide .Plus , if you compare to a non - gimped A , you 'll almost always lose by a ridiculously large margin .","label":"Motivation","metadata":{},"score":"49.112785"}{"text":"Most of these approaches will significantly benefit if the information available in the abstracts is structured into meaningful categories ( e.g. , background , objective , method , result and conclusion ) .While many journals use structured abstract format , the majority of RCT abstracts still remain unstructured .","label":"Motivation","metadata":{},"score":"49.256813"}{"text":"We can formalize this as minimizing learning error : . where the equivalence between and 162 is shown in Equation 157 in Figure 14.6 .Note that and are independent of each other .In general , for a random document and a random training set , does not contain a labeled instance of .","label":"Motivation","metadata":{},"score":"49.358574"}{"text":"The use of PCA improves accuracy on the order of 2 % to 10 % , depending on the conditions .In contrast , for NLDA2 , best performance was obtained with the nonlinearities used for both training and final transformations .","label":"Motivation","metadata":{},"score":"49.364063"}{"text":"By moving those two points , we can swing the estimated boundary all the way from vertical to nearly horizontal : .A better option in these situations is to consider all of the possible classifiers , not just the one which has some special geometrical property .","label":"Motivation","metadata":{},"score":"49.385056"}{"text":"There were 4,633 sentences in the training set , each of which could have received up to 15 different labels .Of the potential 69,495 labels that could have been assigned , 2,522 ( 3.63 % ) were assigned .Maximum entropy classifier and features .","label":"Motivation","metadata":{},"score":"49.40086"}{"text":"161 - 164 .[ 7 ] Kunio Matsui , Tomoko und Tanabe , \" Speaker identifica- tion with dual penalized logistic regression machine , \" in Proceedings of ODYSSEY - The Speaker and Language Recognition Workshop , 2004 , pp .","label":"Motivation","metadata":{},"score":"49.40724"}{"text":"That said , if A is too gimped , I guess that would raise questions on whether B is practical at all .I think this so true when The catch is that for whatever reasons , I have to limit B in some way ( perhaps in terms of the amount of data I train in on ) .","label":"Motivation","metadata":{},"score":"49.41898"}{"text":"Details of the \" chris4 \" algorithm can be found in the source code for the classifier .Preprocessing Options .Contraction normalization : A large number of common contractions were present in the dataset with different spellings .We developed rules to cover a range of possible misspellings for common contractions following the misspelled examples we found in the training data in order to accommodate misspellings which might be expected in the test data .","label":"Motivation","metadata":{},"score":"49.469692"}{"text":"In this section , we present two sets of results .We detail the performance of each feature used by our classifiers using 5-fold cross - validation methods on the training data .Then , we present the performance of our three submitted systems on the test data and compare that to system performance on cross - validated training data .","label":"Motivation","metadata":{},"score":"49.50429"}{"text":"But its bias is high since it will consistently misclassify squares in the lower left corner and ' ' solid circle ' ' documents with more than 50 Roman characters .Linear classifier .Shown as a dashed line with long dashes .","label":"Motivation","metadata":{},"score":"49.528908"}{"text":"Chances are that quite some time has gone in to optimizing ( say , for speed ) algorithms for solving A. Should I , as a developer of new models , also have to be a hard - core optimizer in order to make things work ?","label":"Motivation","metadata":{},"score":"49.638275"}{"text":"Most of these approaches will significantly benefit if the information available in the abstracts is structured into meaningful categories ( e.g. , background , objective , method , result and conclusion ) .While many journals use structured abstract format , majority of RCT abstracts still remain unstructured .","label":"Motivation","metadata":{},"score":"49.653347"}{"text":"As illustrated in one of the experiments presented later , the state dependent targets were shown to perform better than phone level targets .The simpler approach of using fixed ratios for state boundaries was as good as using the Viterbi alignment approach .","label":"Motivation","metadata":{},"score":"49.667778"}{"text":"In problems such as prediction of protein function or protein interactions , there are several sources of genomic data that are relevant , each of which may require a different kernel to model .Rather than choosing a single kernel , several papers have established that using a combination of multiple kernels can significantly boost classifier performance [ 44 ] - [ 46 ] .","label":"Motivation","metadata":{},"score":"49.749702"}{"text":"The complexity of training of nonlinear SVMs with solvers such as LIBSVM has been estimated to be quadratic in the number of training examples [ 57 ] , which can be prohibitive for datasets with hundreds of thousands of examples .Researchers have therefore explored ways to achieve faster training times .","label":"Motivation","metadata":{},"score":"49.873886"}{"text":"A very recent study on L 1/2 regularization theory in compressive sensing shows that L 1/2 sparse modeling can yield solutions more sparse than those of 1 norm and 2 norm , and , furthermore , the model can be efficiently solved by a simple iterative thresholding procedure .","label":"Motivation","metadata":{},"score":"49.8741"}{"text":"Differing provisions from the publisher 's actual policy or licence agreement may be applicable .\" Prior studies proposed automatic techniques to transform clinical trial specifications into a computable form that can be efficiently reused for classification , clustering , and retrieval [ 17 ] [ 18 ] [ 19 ] [ 20 ] [ 21 ] [ 22].","label":"Motivation","metadata":{},"score":"49.948456"}{"text":"We show an exclusive advantage of 1/2-KLR that the regularization parameter in the algorithm can be adaptively set whenever the sparsity ( correspondingly , the number of support vectors ) is given , which suggests a methodology of comparing sparsity promotion capability of different sparsity driven classifiers .","label":"Motivation","metadata":{},"score":"49.948906"}{"text":"We observe that the dual representation of the discriminant function depends on the data only through dot products in feature space .The same observation holds for the dual optimization problem ( Equation 4 ) when we replace x i with φ ( x i ) ( analogously for x j ) .","label":"Motivation","metadata":{},"score":"49.9995"}{"text":"We suggest a fast iterative thresholding algorithm for 1/2-KLR and prove its convergence .We provide a series of simulations to demonstrate that 1/2-KLR can often obtain more sparse solutions than the existing sparsity driven versions of KLR , at the same or better accuracy level .","label":"Motivation","metadata":{},"score":"50.074963"}{"text":"Conclusions .Nonlinear dimensionality reduction methods , based on the general nonlinear mapping abilities of neural networks , can be useful for capturing most of the information from high dimensional spectral / temporal features , using a much smaller number of features .","label":"Motivation","metadata":{},"score":"50.08464"}{"text":"Each sentence was categorized into one of the following four classes : Introduction , Method , Result and Conclusion .In our study , we used 23,185 sentences for classifier training , which is only 8 % of their training set .","label":"Motivation","metadata":{},"score":"50.158916"}{"text":"The dataset is divided into 4 baseline experiments ( BE1-BE4 ) from which we used the text- independent set BE4 for speaker identification experiments .During feature extraction the speech data is divided into frames of 25ms at a frame rate of 10ms and a voiced / unvoiced decision is obtained using a pitch detection algorithm [ 20].","label":"Motivation","metadata":{},"score":"50.249413"}{"text":"E .X .X .^ . ) is minimized .That is , .X .^ should approximate X as well as possible , in a mean square error sense .As has been shown in several references ( for example , Duda et al . , 2001 ) , this seemingly intractable problem has a very straightforward solution , provided X is zero mean and multivariate Gaussian .","label":"Motivation","metadata":{},"score":"50.27852"}{"text":"So the main difficulty in this setup is that the system has to deal with known and unknown speakers , so - called clients and impostors , respec- tively .In speaker recognition Gaussian mixtures are usually a good choice in modeling the distribution of the speaker - specific speech samples .","label":"Motivation","metadata":{},"score":"50.371735"}{"text":"These variables only describe the content of the message .In a real system , we would probably also want to compare the sender name to an address book .We split into train and test : .For a tree classifier , these are the commands we would use : .","label":"Motivation","metadata":{},"score":"50.38434"}{"text":"[ 8 ] Mark Przybocki and Alvin F. Martin , recognition evaluation chronicles , \" \" NIST speaker in Proceedings of .Page 6 .ODYSSEY - The Speaker and Language Recognition Workshop , 2004 .[ 9 ] Nachman Aronszajn , \" Theory of reproducing kernels , \" Transactions of the American Mathematical Society , vol .","label":"Motivation","metadata":{},"score":"50.48156"}{"text":"Consider this example : The maximum - margin solution is a vertical line , determined only by the two points at ( 0,0 ) and ( 1,0 ) .( The line is not perfectly vertical in the figure because the optimization was stopped early .","label":"Motivation","metadata":{},"score":"50.50006"}{"text":"We observe that the use of a nonlinear kernel , either Gaussian or polynomial , leads to a small improvement in classifier performance when compared to the linear kernel .For the large degree polynomial and small width Gaussian kernel , we obtained reduced accuracy , which is the result of a kernel that is too flexible , as described above .","label":"Motivation","metadata":{},"score":"50.53674"}{"text":"Our algorithm achieved a precision of 0.904 and a recall of 0.131 in extracting all pairs , and a precision of 0.904 and a recall of 0.842 in extracting frequent pairs .In addition , we have shown that the extracted pairs strongly correlate with both drug target genes and therapeutic classes , therefore may have high potential in drug discovery .","label":"Motivation","metadata":{},"score":"50.58229"}{"text":"Fit - training - set - perfectly ' ' classifier .Shown as a solid line .Here , the learning method constructs a decision boundary that perfectly separates the classes in the training set .This method has the lowest bias because there is no document that is consistently misclassified - the classifiers sometimes even get noise documents in the test set right .","label":"Motivation","metadata":{},"score":"50.651768"}{"text":"For example , the transition probability between the Method , Result and Start End Objective Methods Results Concl Background AMIA 2006 Symposium Proceedings Page - 825 .Page 3 . \" background \" state and the \" objective \" state is 0.2731 , since in our training set , of the 4152 sentences in the background section , 1134 have a succeeding sentence in the objective section .","label":"Motivation","metadata":{},"score":"50.6854"}{"text":"Categorization of sentence types in medical abstracts .In Proceedings of the 2003 AMIA conference .Rabiner , L. ( 1989 )A tutorial on hidden Markov models and selected applications in speech recognition Proc IEEE 77 ( 2 ) 257 - 286 8 .","label":"Motivation","metadata":{},"score":"50.765938"}{"text":"Performance improvements augmentation on three classification algorithms .With three modification options , HMM improves the results for each of them .Table 3 shows performance improvements of HMM augmentation for the three modification options : basic , with boosting and with bagging .","label":"Motivation","metadata":{},"score":"50.85525"}{"text":"Researchers have developed kernels to compare phylogenetic profiles modeled as trees [ 49 ] , protein structures modeled as graphs of secondary - structural elements [ 50 ] , [ 51 ] , and graphs representing small molecules [ 52 ] .","label":"Motivation","metadata":{},"score":"50.861248"}{"text":"We refer to this set of features as temporal resolution features .SVM Classifier The corpus comprising of sentences indicating past smoker , current smoker , and smoker was represented as vectors using the temporal resolution features .We built a linear SVM classifier from that corpus using Weka .","label":"Motivation","metadata":{},"score":"50.90862"}{"text":"In Table 1 , we show the number and percentage of sentences that were annotated with each label ( \" Labeled sentences \" ) .Since each sentence could have been labeled with up to 15 labels , we may wish to consider the number of total labels assigned ( \" Labels assigned \" ) .","label":"Motivation","metadata":{},"score":"50.909668"}{"text":"In the remaining part of this chapter , two versions of NLDA based on this strategy are described , followed by a series of experimental evaluations for the phonetic classification and recognition tasks .Nonlinear dimensionality reduction architecture .In a previous work ( Zahorian et al . , 2007 ) , NLPCA was applied to an isolated vowel classification task , and the nonlinear method based on neural networks was experimentally compared with linear methods for reducing the dimensionality of speech features .","label":"Motivation","metadata":{},"score":"51.007835"}{"text":"We used the Stanford Parser 9 to generate these dependencies , extracting \" collapsed dependencies \" .Collapsed dependencies combine dependency relations so that the resulting relations only contain content words , omitting prepositions and conjunctions , for example .Variable dependency relations : We also experimented with substituting individual words with variables in these dependency relations , expecting to conflate uncommon patterns in order to decrease data sparseness .","label":"Motivation","metadata":{},"score":"51.045387"}{"text":"Sequence similarity has been studied extensively in the bioinformatics community , and local alignment algorithms like BLAST and Smith - Waterman are good at revealing regions of similarity between proteins and DNA sequences .The statistics produced by these algorithms do not satisfy the mathematical condition required of a kernel function .","label":"Motivation","metadata":{},"score":"51.096436"}{"text":"Furthermore , evaluating the classification on the 5-best alternatives , the SVM reached the lowest error rate followed by the non - sparse KLR .Additionally , we want to compare the sparseness of the dif- ferent discriminative classifiers .","label":"Motivation","metadata":{},"score":"51.10415"}{"text":"We first review some traditional linear methods for dimensionality reduction before proceeding to the nonlinear transformation , the main subject of this chapter .Principal Components Analysis ( PCA ) .Principal Components Analysis ( PCA ) , also known as the Karhunen - Loeve Transform ( KLT ) , has been known of and in use for nearly a century ( Fodor , 2002 ; Duda et al . , 2001 ) , as a linear method for dimensionality reduction .","label":"Motivation","metadata":{},"score":"51.12451"}{"text":"Figure 13 .Training target vectors of the neural network .State - level targets .Due to the nonstationarity of speech signals , a speech signal varies even in a very short time interval ( e.g. a phoneme ) .For speech recognition tasks , instead of phone level training targets , state ( as in hidden states of an HMM ) dependent targets could be advantageous in training a versatile network for more highly discriminative speech features .","label":"Motivation","metadata":{},"score":"51.14099"}{"text":"Classification accuracies of neural network ( top panel ) and MXL ( bottom panel ) classifiers with various types of features .The results obtained with the neural network and MXL classifiers using 10 % of the group 2 training data ( that is , 5 % of the overall training data ) are shown in Figure 12 .","label":"Motivation","metadata":{},"score":"51.19719"}{"text":"A key concept required for defining a linear classifier is the dot product between two vectors , also referred to as the inner product or scalar product .A linear classifier is based on a linear discriminant function of the form ( 1 ) .","label":"Motivation","metadata":{},"score":"51.237053"}{"text":"With this exercise we were effectively running a series of yes / no annotation exercises on the same dataset .The clear majority of sentences were not labelled with each given emotion : annotated sentences for each given emotion were quite rare in both the training and test datasets .","label":"Motivation","metadata":{},"score":"51.29754"}{"text":"For both the neural network and MXL classifiers , NLDA2 clearly performs much better than the other transformations or the original features .However , the advantage of NLDA2 decreases with an increasing number of features , and as the percentage of group 2 data increases ( not shown in figure ) .","label":"Motivation","metadata":{},"score":"51.341087"}{"text":"Confidence tuning has a pronounced effect on the results regardless of the feature set used ; therefore , we begin by presenting results using our default 0.5 split point and the tuned split point found using cross - validation on two representative feature sets .","label":"Motivation","metadata":{},"score":"51.34697"}{"text":"Recent work done in manual and automatic construction of paraphrase corpora is also examined .We also discuss the strategies used for evaluating paraphrase generation techniques and briefly explore some future trends in paraphrase generation .this disparity could be that paraphrasing is not an application in and of itself .","label":"Motivation","metadata":{},"score":"51.40094"}{"text":"Moreover , the task of automatically generating or extracting semantic equivalences for the various units of language- words , phrases , and sentences - is an important part of natural language processing ( NLP ) and is being increasingly employed to improve the performance of several NLP applications .","label":"Motivation","metadata":{},"score":"51.43259"}{"text":"The columns of B are also the same eigenvectors .Thus the \" forward \" and \" reverse \" transformations are transposes of each other .The components of Y are uncorrelated .Furthermore the expected value of this normalized mean square error between original and re - estimated X vectors can be shown to equal the ratio of the sum of \" unused \" eignevalues to the sum of all eigenvalues .","label":"Motivation","metadata":{},"score":"51.54757"}{"text":"The first and the second order differences are added , resulting in feature vec- tors of 39 dimensions .The parameters of the baseline GMM models were estimated using the HTK toolkit [ 21].For the SVM and ( S)KLR approaches we used a modified version of HTK utilizing a runtime plug - in library for external computa- tion of probabilities .","label":"Motivation","metadata":{},"score":"51.57834"}{"text":"Therefore , to be able to separate true splice sites from decoys , one needs additional features derived from the same sequences .If we use the spectrum kernel for the splice site recognition task , we obtain considerable improvement over the simple GC content features ( see Table 2 ) .","label":"Motivation","metadata":{},"score":"51.607815"}{"text":"Further information on the age , gender and physical and psychiatric health of these people may be of value .Previous work in suicide note authorship detection used structural and grammatical features such as the number of paragraphs in the note , the number of misspellings , and the depth of parse tree .","label":"Motivation","metadata":{},"score":"51.706444"}{"text":"This formed part of the 2011 i2b2 NLP Shared Task , Track 2 .The precision and recall of these classifiers related strongly with the number of occurrences of each emotion in the training data .Evaluating on previously unseen test data , our best system achieved an F 1 score of 0.534 .","label":"Motivation","metadata":{},"score":"51.712967"}{"text":"The configuration of HMMs can be largely simplified by incorporating NLDA .Experiments with large network training .The results of the previous experiment showed large performance advantages for NLDA2 over NLDA1 and the original features , when using either a small number of features , or a \" small \" HMM .","label":"Motivation","metadata":{},"score":"51.714035"}{"text":"During the three - month period before the formal evaluation , we used Set 1 as a development and training set and Set 2 as our test set .After we determined the best configuration parameters on the training / development set , we built our models from Set 1 .","label":"Motivation","metadata":{},"score":"52.126266"}{"text":"Bias is large if the learning method produces classifiers that are consistently wrong .Bias is small if ( i ) the classifiers are consistently right or ( ii ) different training sets cause errors on different documents or ( iii ) different training sets cause positive and negative errors on the same documents , but that average out to close to 0 .","label":"Motivation","metadata":{},"score":"52.196377"}{"text":"1998 ) discusses several theoretical methods for determining these curved subspaces ( manifolds ) within higher dimensionality spaces .Another general method , and the one illustrated and explored in more detail in this chapter , is based on a \" bottleneck \" neural network ( Kramer , 1991 ) .","label":"Motivation","metadata":{},"score":"52.19989"}{"text":"Classifier Options .Sigma : The classifier uses sigma as a parameter to specify the strength of the prior .By default , this value is 1.0 .Values less than 1.0 indicate a stronger prior .We experimented with values between 0.25 and 2.0 .","label":"Motivation","metadata":{},"score":"52.232018"}{"text":"Scatter plot of 2-D multivariate Gaussian data and first principal components basis vector .No straight line can be a good fit to this data .Linear Discriminant Analysis ( LDA ) .Linear transforms for the purpose of reducing dimensionality while preserving discriminability between pre - defined categories have also long been known about and used ( Wang & Paliwal , 2003 ) , and are usually referred to as Linear Discriminant Analysis ( LDA ) .","label":"Motivation","metadata":{},"score":"52.235825"}{"text":"Even when similar patient populations are pooled together from multiple locations , differences in medical treatment and record systems can limit which outcome measures can be commonly analyzed .In total , these differences in medical research settings can lead to differing conclusions or can even prevent some studies from starting .","label":"Motivation","metadata":{},"score":"52.25988"}{"text":"Define variables which can be used to discriminate the two classes .This is sometimes called feature extraction .We 'll use the spam example to compare their properties .George Forman , a researcher at Hewlett - Packard , has made a spam dataset .","label":"Motivation","metadata":{},"score":"52.29357"}{"text":"Logistic regression makes less errors of both type , so it is a better classifier for this data no matter what the cost structure .Can a tree ever be better than logistic regression ?Absolutely .Remember that logistic regression always uses a linear boundary between the classes .","label":"Motivation","metadata":{},"score":"52.331787"}{"text":"Minimal annotation : Inspection of the training data revealed that there were very few notes that had no annotations at all , although most contained sentences that had no annotations .Therefore , we ensured that in each of our submitted systems , every note had at least one annotated sentence .","label":"Motivation","metadata":{},"score":"52.339886"}{"text":"Note that in this usage , \" outputs \" may be from the final outputs or from one of the internal hidden layers .Figure 7 .Overview of the NLDA transformation for speech recognition .The multilayer bottleneck neural network employed in NLDA contains an input layer , hidden layers including the bottleneck layer , and an output layer .","label":"Motivation","metadata":{},"score":"52.421345"}{"text":"To improve performance , we developed a novel approach of combining text classification methods with Hidden Markov Modeling ( HMM ) technique [ 7 ] to categorize sentences in RCT abstracts .DATA & METHODS .3,896 structured RCT abstracts published from 2004 to 2005 were randomly selected , delabelled , and parsed into 46,370 sentences using a sentence parser that we developed .","label":"Motivation","metadata":{},"score":"52.4459"}{"text":"Many datasets encountered in bioinformatics and other areas of application are unbalanced , i.e. , one class contains a lot more examples than the other .For instance , in the case of splice site detection , there are 100 times fewer positive examples than negative ones .","label":"Motivation","metadata":{},"score":"52.463406"}{"text":"In addition , the label we refer to as \" happiness \" included both \" happiness \" and \" peacefulness \" .Many well - studied natural language processing ( NLP ) classification tasks ( part - of - speech tagging , word - sense disambiguation , spelling correction , named entity detection , sentiment analysis and spam filtering , to name a few ) also draw their labels from a pre - defined label inventory .","label":"Motivation","metadata":{},"score":"52.508858"}{"text":"Generating typed dependency parses from phrase structure parses .LREC .Brill E , Moore RC .An improved error model for noisy channel spelling correction .Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics ; 2000 .","label":"Motivation","metadata":{},"score":"52.529503"}{"text":"Thus , in one sense , the recognizer is a hybrid neural network / Hidden Markov Model ( NN / HMM ) recognizer .However , the neural network step is used for the task of nonlinear dimensionality reduction and is independent of the HMM .","label":"Motivation","metadata":{},"score":"52.662354"}{"text":"Figure 10 .Illustrations of a linear activation function ( left ) , a unipolar sigmoid function ( middle ) and a bipolar sigmoid function ( right ) .The weights of the neural network are estimated using the backpropagation algorithm to minimize the distance between the scaled input features and target data .","label":"Motivation","metadata":{},"score":"52.704872"}{"text":"In [ 7 ] a kind of multi - class KLR was used on a speaker identi- fication task .This version differs from our approach in the way of parameter estimation and it is not sparse so that the size of the training data is very limited .","label":"Motivation","metadata":{},"score":"52.738647"}{"text":"( 2007 ) , resulting in 2,500,554 features .The training data consists of 2,306 sentences ( 58,771 tokens ) .To evaluate validation error , we use 1,000 sentences ( 30,563 tokens ) and report accuracy ( rate of correct edges in a predicted parse t .. by Ryan Mcdonald - Proceedings of the Conference on Empirical Methods in Natural Language Processing and Natural Language Learning , 2007 . \" ...","label":"Motivation","metadata":{},"score":"52.761204"}{"text":"Our best F - score is 85.57 for this final formal evaluation ( most frequent category baseline is 60.58 ) .Our error analysis uncovered several areas for improve- ment .Currently , our negation detection does not account for nonnegated lexical items indicating nonsmoker status , e.g. , nonsmoker , nonsmoker .","label":"Motivation","metadata":{},"score":"52.764694"}{"text":"With the availabi ... . \" ...The task of paraphrasing is inherently familiar to speakers of all languages .Moreover , the task of automatically generating or extracting semantic equivalences for the various units of language- words , phrases , and sentences - is an important part of natural language processing ( NLP ) and is being inc ... \" .","label":"Motivation","metadata":{},"score":"52.77828"}{"text":"Therefore , an additional experiment was performed , using the state level targets with \" do n't cares , \" as mentioned previously , and a very large neural network for transforming features .The state targets were formed using either a constant length ratio ( ratio for 3 states : 1:4:1 ) or a Viterbi forced alignment approach , as described in Section 4.5 .","label":"Motivation","metadata":{},"score":"52.84693"}{"text":"Unsolicited bulk email , aka spam , is an annoyance for many people .Suppose you wanted to make a classifier to detect spam and filter it out of your mail .How would you do it ?There are four basic steps : .","label":"Motivation","metadata":{},"score":"52.925423"}{"text":"The use of confidence or fiducial limits illustrated in the case of the binomial .Biometrica 1934;26:404 - 13 .28 SAVOVA et al . , Mayo Clinic NLP System .Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .","label":"Motivation","metadata":{},"score":"52.950058"}{"text":"The main drawback of the kernel classifiers is the computational cost for the parameter estimation .Future work will concentrate on an objective measure of the score quality of the moderated SVM output and the SKLR probability scores .Furthermore , we are planning to use the SVM and the SKLR on speaker verification tasks .","label":"Motivation","metadata":{},"score":"52.99477"}{"text":"For the MXL classifier , NLDA2 features result in approximately 10 % higher classification accuracies as compared to all other features .For both the neural network and MXL classifiers , accuracy with NLPCA features was very similar to that obtained with linear PCA .","label":"Motivation","metadata":{},"score":"53.01609"}{"text":"To evaluate the classifier performance , we will use so - called receiver operating characteristic ( ROC ) curves [ 14 ] , which show the true positive rates ( y -axis ) over the full range of false positive rates ( x -axis ) .","label":"Motivation","metadata":{},"score":"53.14202"}{"text":"Using cross - validation , we were able to fine - tune this split point to optimize F 1 ( always by increasing recall at the expense of precision ) .In our first submitted system , we fine - tuned a single split point value for use with all emotion classifiers .","label":"Motivation","metadata":{},"score":"53.208946"}{"text":"The goal of this study is to extract a large number of accurate drug - disease treatment pairs from published literature .Results In this study , we developed a simple but highly accurate pattern - learning approach to extract treatment - specific drug - disease pairs from 20 million biomedical abstracts available on MEDLINE .","label":"Motivation","metadata":{},"score":"53.25845"}{"text":"( Numbers in brackets are 95 % exact confidence intervals )Training Set Test Set No .Documents Correctly Classified by System Total No .Documents Classified by System No .Page 4 . ment of the project 's UIMA components , model building , experimentation , and discussions .","label":"Motivation","metadata":{},"score":"53.28323"}{"text":"Similar experiments , with all identical conditions except using either phone level targets , or state level targets without \" do n't cares \" resulted in about 2 % lower accuracies .These results imply that the use of \" do n't cares \" is able to reduce errors introduced by inaccurate determination of state boundaries .","label":"Motivation","metadata":{},"score":"53.352432"}{"text":"[19 ] exploits a similar idea in a finer - grained level of syntactic units .The authors classify sentences of RCT abstracts in meaningful categories , i.e. introduction , objective , method , result and conclusion , combining text classification and Hidden Markov Modelling techniques .","label":"Motivation","metadata":{},"score":"53.44297"}{"text":"It typically has as many variables as the primal problem has constraints .Its objective value at optimality is equal to the optimal objective value of the primal problem , under certain conditions ; see , e.g. , [ 21 ] for more details . )","label":"Motivation","metadata":{},"score":"53.50257"}{"text":"Therefore , the whole set of SVs affects the value of the discriminant function at x , leading to a smooth decision boundary .As we decrease σ , the kernel becomes more local , leading to greater curvature of the decision surface .","label":"Motivation","metadata":{},"score":"53.56424"}{"text":"Input and output plot of the 3-D Gaussian before ( left ) and after ( right ) using neural network for NLPCA .Nonlinear Discriminant Analysis ( NLDA ) .Fortunately , only a minor modification to NLPCA is needed to form NLDA .","label":"Motivation","metadata":{},"score":"53.612755"}{"text":"This problem is alleviated if we use the mixed spectrum kernel : ( 12 ) where β d is a weighting for the different substring lengths ( details below ) .Kernels using positional information .The kernels mentioned above ignore the position of substrings within the input sequence .","label":"Motivation","metadata":{},"score":"53.616463"}{"text":"Let .X .[ .x .x .x .n . ]T be an n -dimensional ( column ) feature vector , and .Y .[ .y .y .y . m . ]T be an m -dimensional ( column ) feature vector , obtained as the linear transform of X , using the n by m transformation matrix A , i.e. .","label":"Motivation","metadata":{},"score":"53.649883"}{"text":"Neural networks .In optimizing the design of a neural network , an important consideration is the number of hidden layers and an appropriate number of hidden nodes in each layer .A neural network with no hidden layers can form only simple decision regions , which is not suitable for highly nonlinear and complex speech features .","label":"Motivation","metadata":{},"score":"53.65552"}{"text":"In the max - margin case , O ( 1 ε ) EG updates are required to reach a given accuracy ε in the dual ; in contrast , for log - linear models only O(log ( 1/ε ) ) updates are required .","label":"Motivation","metadata":{},"score":"53.65695"}{"text":"Table 2 : Sparseness ( % ) of SKLR compared to SVM on different training sizes of the BE4 dataset .The sparseness is defined as the portion of data not used in the final solution .The lower the sparseness the more feature vectors are part of the classifier .","label":"Motivation","metadata":{},"score":"53.665283"}{"text":"Best F1 measure were achieved with Maximum Entropy Model ( basic ) with HMM augmentation .AMIA 2006 Symposium Proceedings Page - 827 .Page 5 .DISCUSSION .McKnight and Srinivisan [ 6 ] have shown that Support Vector Machine classifier trained on structured RCT abstract sentences has F1 score ranging from 0.616 to 0.8345 , precision from 0.67 to 0.88 and recall from 0.49 to 0.86 using 10 fold classification [ 7].","label":"Motivation","metadata":{},"score":"53.714928"}{"text":"Perceptron training is widely applied in the natural language processing community for learning complex structured models .Like all structured prediction learning frameworks , the structured perceptron can be costly to train as training complexity is proportional to inference , which is frequently non - linear in example sequence length .","label":"Motivation","metadata":{},"score":"53.8005"}{"text":"Basic Precision 20.74 % Recall 25.23 % F1 22.77 % Table 3 .Performance improvements augmentation on modification options .HMM improves the classification in all five types of sentences , more significantly on those in the Background and Conclusion sections .","label":"Motivation","metadata":{},"score":"53.8658"}{"text":"Figures 6 and 7 ) , one can extend the idea of dot products between points with the help of kernel functions ( cf .the section Kernels : From Linear to Nonlinear Classifiers ) .Kernels compute the similarity of two points and are the second important concept of SVMs and kernel methods [ 2 ] , [ 7 ] .","label":"Motivation","metadata":{},"score":"53.887848"}{"text":"The classes would be well separated by a projection onto the first LDA basis vector , but poorly separated by a projection onto the first PCA basis vector .Heteroscedastic Discriminant Analysis ( HDA ) .Another linear transformation technique , related to linear discriminant analysis , but which accounts for the ( very common ) case where within class covariance matrices are not the same for all classes , is called Heterocscedastic Discriminant Analysis ( HDA ) ( Saon et al . , 2000 ) .","label":"Motivation","metadata":{},"score":"53.90745"}{"text":"131 - 154 .Jaakkola T , Diekhans M , Haussler D ( 2000 )A discriminative framework for detecting remote protein homologies .J Comp Biol 7 : 95 - 114 .Gärtner T ( 2008 )Kernels for structured data .","label":"Motivation","metadata":{},"score":"54.027683"}{"text":"The features which are the direct outputs of the network without PCA processing were also evaluated .Figure 17 shows accuracies using 1-state and 3-state HMMs with a varying number of mixtures per state .NLDA2 performed better than NLDA1 for all conditions -- approximately 2 % higher accuracy .","label":"Motivation","metadata":{},"score":"54.07243"}{"text":"So far we have shown how SVMs perform on our splice site example if we use kernels based only on the two GC content features derived from the exonic and intronic parts of the sequence .The small subset of the dataset shown in Figures 1 - 7 seems to suggest that these features are sufficient to distinguish between the true splice sites and the decoys .","label":"Motivation","metadata":{},"score":"54.21377"}{"text":"ASCOT uses text mining and data mining methods to enrich clinical trials with metadata , that in turn serve as effective tools to narrow down search .In addition , ASCOT integrates a component for recommending eligibility criteria based on a set of selected protocols .","label":"Motivation","metadata":{},"score":"54.26127"}{"text":"33 , pp .82 - 95 , 1971 .[16 ] Ji Zhu and Trevor Hastie , \" Kernel logistic regression and the import vector machine , \" Journal of Computational and Graphical Statistics , vol .14 , pp .","label":"Motivation","metadata":{},"score":"54.27088"}{"text":"The first experiment was conducted to evaluate the two NLDA versions with various dimensions in the reduced feature space with and without the use of PCA .As input features , 13 DCTCs , computed with 8 ms frames and 2 ms spacing , were represented with 6 DCSCs over a 500 ms block , for a total of 78 features ( 13 DCTCs x 6 DCSCs ) .","label":"Motivation","metadata":{},"score":"54.33374"}{"text":"We were motivated to include dependency relations based on our understanding of the guidelines provided to annotators ( Table 1 ) .For example , the guidelines for \" forgiveness \" state that the author of the note should be forgiving someone , not asking for forgiveness .","label":"Motivation","metadata":{},"score":"54.348625"}{"text":"23,185 sentences ( 50 % of total 46,370 ) were used for model training and 23,185(50 % of total 46,370 ) for testing .We used MALLET , a text classification toolkit , in our study [ 8].We found that sentences represented as a bag of words without preprocessing gave the best performance .","label":"Motivation","metadata":{},"score":"54.38108"}{"text":"The aim of the second experiment is a more thorough evaluation of NLDA1 and NLDA2 using a varying number of states and mixtures in HMMs .The 78 DCTC / DCSCs ( computed as mentioned in previous section ) were reduced to 36 dimensions based on the results of the previous experiment .","label":"Motivation","metadata":{},"score":"54.46593"}{"text":"Full - text .Recently , a kernelized version of logistic regression has become very popular , because it allows non - linear probabilistic classification and shows promising re- sults on several benchmark problems .In this paper we show that kernel logistic regression ( KLR ) and especially its sparse extensions ( SKLR ) are useful alterna- tives to standard Gaussian mixture models ( GMMs ) and Sup- port Vector Machines ( SVMs ) in Speaker Recognition .","label":"Motivation","metadata":{},"score":"54.512413"}{"text":"Experiment 1 .In the first experiment , all training data were used to train the transformations including LDA , PCA , NLPCA , and NLDA2 , and the classifiers .Figure 11 shows the results based on the neural network and MXL classifiers for each transformation method in terms of classification accuracy , as the number of features varies from 1 to 39 .","label":"Motivation","metadata":{},"score":"54.53589"}{"text":"Many of the problems in computational biology are in the form of prediction : starting from prediction of a gene 's structure , prediction of its function , interactions , and role in disease .Support vector machines ( SVMs ) and related kernel methods are extremely good at solving such problems [ 1 ] - [ 3 ] .","label":"Motivation","metadata":{},"score":"54.788204"}{"text":"Although most of the algorithms have been partially described in the literature before , this is the first comprehensive analysis and evaluation of the algorithms within a unified framework .( 2004 ) ( for English ) , using a different parsing algorithm first presented in Nivre ( 2003 ) . by Joakim Nivre , Ryan Mcdonald - In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies ( ACL-08 : HLT , 2008 . \" ...","label":"Motivation","metadata":{},"score":"54.817307"}{"text":"The idea of combining word clusters with discriminative learning has been previously explored by Miller et al .( 2004 ) , in the context of namedentity recognition , and their work ... .s132 J. Nivre et al .Matthias Trautner Kromann , Alberto Lavelli , Haitao Liu , Yuji Matsumoto , Ryan McDonald , Kemal Oflazer , Petya Osenova , Kiril Simov , Yannick Versley , ... . \" ...","label":"Motivation","metadata":{},"score":"54.90265"}{"text":"An exhaustive evaluation of all pairs and triples of classifier combinations found that no combination of two or three classifiers outperformed the best standalone classifier .In developing our classifiers , we tried to consider the practical applications of the findings from this exercise .","label":"Motivation","metadata":{},"score":"54.910835"}{"text":"It has to be noted that good performance of GMMs depends on a sufficient amount of data for the parameter estimation .In speaker recognition the amount of speech data of each client is limited .Normally , only a few seconds of speech are avail- able and so the parameter estimation might not be very robust .","label":"Motivation","metadata":{},"score":"54.91429"}{"text":"The effect of the choice of C is illustrated in Figure 3 .For a large value of C , a large penalty is assigned to errors .This is seen in Figure 3A , where the two points closest to the hyperplane strongly affect its orientation , leading to a hyperplane that comes close to several other data points .","label":"Motivation","metadata":{},"score":"54.998253"}{"text":"However , the F - scores turned out similar .For the formal evaluation and the final i2b2 submission , our models were built from Set 1 and Set 2 and were run on the official I2B2 test set ( Set 3 ) .","label":"Motivation","metadata":{},"score":"55.067703"}{"text":"Compared to the PCA and LDA reduced features , the NLDA1 and NLDA2 features performed considerably better for both the 1-state and 3-state HMMs .For the case of 3-state HMMs , the transformed features reduced to 24 dimensions resulted in the highest accuracy of 69.3 % for NLDA1 .","label":"Motivation","metadata":{},"score":"55.06785"}{"text":"References .Nigam K , Lafferty J , McCallum A. Using maximum entropy for text classification .IJCAI-99 Workshop on Machine Learning for Information Filtering .Ratnaparkhi A. A maximum entropy model for part - of - speech tagging .Proceedings of the First Conference on Empirical Methods in Natural Language Processing ; 1996 .","label":"Motivation","metadata":{},"score":"55.192204"}{"text":"In this paper , through extending the linear gradient function of L 1/2 regularization framework to the logistic function , we propose a novel sparse version of KLR , the 1/2 quasi - norm kernel logistic regression ( 1/2-KLR ) .","label":"Motivation","metadata":{},"score":"55.201794"}{"text":"The dependence of the log - likelihood of a sequence on the parameters of the model can be used to represent a variable - length sequence in a fixed dimensional vector space .The so - called Fisher - kernel uses the sensitivity of the log - likelihood of a sequence with respect to the model parameters as the feature space [ 38 ] ( see also [ 39 ] ) .","label":"Motivation","metadata":{},"score":"55.514885"}{"text":"For illustration purposes , we use only two features : the GC content in the exon and intron flanking potential acceptor sites .These features are motivated by the fact that the GC - content of exons is typically higher than that of introns ( see , e.g. , Figure 1 ) .","label":"Motivation","metadata":{},"score":"55.54311"}{"text":"Systems used for submission .As part of the evaluation exercise , participants were allowed to submit the results from up to three classifiers , with the understanding that the single best classifier , as determined by F 1 score , would be the system used to provide a ranking of all participants .","label":"Motivation","metadata":{},"score":"55.573524"}{"text":"Named entity recognition with a maximum entropy approach .Proceedings of CoNLL-2003 ; 2003 .pp .160 - 3 .Manning CD , Klein D. Optimization , maxent models , and conditional estimation without magic .HLT - NAACL Tutorial .","label":"Motivation","metadata":{},"score":"55.64798"}{"text":"For example , the NLDA2 features modeled by 3-state HMMs with 3 mixtures resulted in an accuracy of 69.4 % versus 63.2 % for the original features .Figure 16 .Figure 17 .Accuracies of the NLDA1 and NLDA2 features using 1-state ( top panel ) and 3-state HMMs ( bottom panel ) with various numbers of mixtures .","label":"Motivation","metadata":{},"score":"55.657375"}{"text":"The discriminant function then is ( 6 ) .Note that f ( x ) is linear in the feature space defined by the mapping φ ; but when viewed in the original input space , it is a nonlinear function of x if φ ( x ) is a nonlinear function .","label":"Motivation","metadata":{},"score":"55.677227"}{"text":"Automatic document classification of biological literature .BMC Bioinformatics 2006;7 : 370 .Brank J , Grobelnik M , Milic - Frayling N , Mladenic D. Feature selection using linear support vector machines .Microsoft Re- search Technical report MSR - TR-2002 - 63 .","label":"Motivation","metadata":{},"score":"55.736282"}{"text":"Labeling sentences in an abstract is equivalent to aligning the sentences to the HMM states ( Figure 1 ) .Figure 1 .HMM Model .States represent the five sentence categories .Directed edges represent the transition probability .There are five states in our HMM model : Background , Objective , Conclusion .","label":"Motivation","metadata":{},"score":"55.816177"}{"text":"A simple baseline is to assign the most frequent category to each report ( Unknown ) .The best F - score from our informal evaluation is 92.64 .The F - score baseline for this set is 63.31 .We limited our runs on Set 2 to three to avoid overtraining / overfitting on the test data .","label":"Motivation","metadata":{},"score":"55.81968"}{"text":"The parameter ν controls the fraction of support vectors , and of margin errors ( ν -SVM , see [ 2 ] , [ 7 ] ) .Dual formulation .Using the method of Lagrange multipliers ( see , e.g. , [ 21 ] ) , we can obtain the dual formulation .","label":"Motivation","metadata":{},"score":"55.85862"}{"text":"ClassifierIER ( % ) GMM10.84 SVM8.89 KLR8.58 SKLR8.58 5-best ( % ) 6.48 4.67 4.97 5.87 the best speaker in the 5-best alternatives in the right column of the table .As can be seen in table 1 both the SVM and the KLR classifiers clearly outperform the GMM baseline system .","label":"Motivation","metadata":{},"score":"55.86489"}{"text":"However , the projective algorithms often produce comparable results when combined with the technique known as pseudo - projective parsing .The linear time complexity of the stack - based algorithms gives them an advantage with respect to efficiency both in learning and in parsing , but the projective list - based algorithm turns out to be equally efficient in practice .","label":"Motivation","metadata":{},"score":"56.056896"}{"text":"Nonlinear Principal Components Analysis ( NLPCA ) .Since the final NN outputs are created from the internal NN representations at the bottleneck layer , the bottleneck outputs can be viewed as the reduced dimensionality version of the data .This idea was tested using pseudo - random data generated so as to cluster on curved subspaces .","label":"Motivation","metadata":{},"score":"56.067337"}{"text":"In theory , you just need a few potentiometers to represent the coefficients .At the time , they were called neural networks , by analogy to how neurons in the brain combine messages from their neighbors ( perhaps linearly ) and then decide to \" fire \" or \" not fire \" ( perhaps by thresholding at zero ) .","label":"Motivation","metadata":{},"score":"56.22702"}{"text":"In one of these methods , referred to as nonlinear PCA ( NLPCA ) , the goal of the nonlinear transformation is to minimize the mean square error between features estimated from reduced dimensionality features and original features .Thus this method is patterned after PCA .","label":"Motivation","metadata":{},"score":"56.243206"}{"text":"For each of the four algorithms , we give proofs of correctness and complexity .In addition , we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action , using data from thirteen languages .","label":"Motivation","metadata":{},"score":"56.355503"}{"text":"As expected this basis vector , represented by a straight line , is oriented along the axis with maximum data variation .Figure 1 .Scatter plot of 2-D multivariate Gaussian data and first principal components basis vector .Line is a good fit to data .","label":"Motivation","metadata":{},"score":"56.479557"}{"text":"The database used for all experiments reported in the remainder of this chapter is TIMIT .The TIMIT database was developed in the early 1980 's for expediting acoustic - phonetic ASR research ( Garofolo et al . , 1993 ; Zue et al , 1990 ) .","label":"Motivation","metadata":{},"score":"56.562645"}{"text":"results of ( S)KLR , SVMs and GMMs are compared in section 7 and we conclude with a short discussion in section 8 .In The speaker identification 2 . GMM Baseline System In the past several years there has been a lot of progress in the field of speaker recognition [ 8][2].","label":"Motivation","metadata":{},"score":"56.57445"}{"text":"ABSTRACT .Randomized clinical trials ( RCT ) papers provide reliable information about efficacy of medical interventions .Current keyword based search methods to retrieve medical evidence , overload users with irrelevant information as these methods often do not take in to consideration semantics encoded within abstracts and the search query .","label":"Motivation","metadata":{},"score":"56.576878"}{"text":"Words that appeared only once in the subcorpus were removed from the set of features .Features were chosen only from the smoking - related subcorpus to reduce those unre- lated to smoking .This assumes that sentences labeled un- known did not share useful features for classification , but simply lack the features that describe sentences labeled smoking - related .","label":"Motivation","metadata":{},"score":"56.67157"}{"text":"Nonlinear methods like kNN have low bias .We can see in Figure 14.6 that the decision boundaries of kNN are variable - depending on the distribution of documents in the training set , learned decision boundaries can vary greatly .As a result , each document has a chance of being classified correctly for some training sets .","label":"Motivation","metadata":{},"score":"56.72976"}{"text":"Other sequence kernels .Because of the importance of sequence data and the many ways of modeling it , there are many alternatives to the spectrum and weighted degree kernels .Most closely related to the spectrum kernel are extensions allowing for gaps or mismatches [ 28 ] .","label":"Motivation","metadata":{},"score":"56.79084"}{"text":"The simplest way is to represent a sequence in terms of its BLAST / Smith - Waterman scores against a database of sequences [ 36 ] .This is a general method for using a similarity measure as a kernel .An alternative approach taken was to modify the Smith - Waterman algorithm to consider the space of all local alignments , leading to the local alignment kernel [ 37 ] .","label":"Motivation","metadata":{},"score":"56.876656"}{"text":"Participants in the shared task were provided with 600 annotated notes as training data .The task organizers asked participants to optimize their systems according to F 1 score : the harmonic mean of precision and recall .Two of our systems attempted to achieve maximal F 1 score , as instructed , by balancing precision and recall .","label":"Motivation","metadata":{},"score":"56.926697"}{"text":"However , ASR accuracies obtained with a combination of LDA and MLLT were nearly as good as those obtained with HDA and MLLT .A detailed summary of HDA and MLLT are beyond the scope of this chapter ; however , these methods , either by themselves , or in conjunction with the nonlinear methods described in this chapter warrant further investigation .","label":"Motivation","metadata":{},"score":"56.962246"}{"text":"A Tutorial , Institute for Signal and Information Processing , Department of Electrical and Computer Engineering , Mississippi State University .Randomized clinical trials ( RCT ) papers provide reliable information about efficacy of medical interventions .Current keyword based search methods to retrieve medical evidence , overload users with irrelevant information as these methods often do not take in to consideration semantics encoded within abstracts and the search query .","label":"Motivation","metadata":{},"score":"57.019264"}{"text":"The resulting fit of the KLRridge approximates the full KLR model , while the number of kernel functions used in the final solution is only a small fraction of the number of training vectors .Multi - class problems Naturally , kernel logistic regression could be extended to multi- class problems .","label":"Motivation","metadata":{},"score":"57.07173"}{"text":"As shown in Figure 18 , both NLDA1 and NLDA2 using the expanded targets lead to a significant increase in accuracy .The NLDA2 accuracies are typically about 2 % higher than NLDA1 accuracies .The use of forced alignment for state boundaries resulted in the highest accuracy of 75.0 % with 64 mixtures .","label":"Motivation","metadata":{},"score":"57.158974"}{"text":"High - level architecture for the sentence classi- 26 SAVOVA et al . , Mayo Clinic NLP System .Page 3 .Data Sets and Evaluation The I2B2 challenge organizers released three data sets ( Table 1).1Set 1 and Set 2 were made available three months before the formal competitive evaluation .","label":"Motivation","metadata":{},"score":"57.17063"}{"text":"[17 ] Trevor Hastie and Robert Tibshirani , \" Classification by pairwise coupling , \" in Advances in Neural Information Processing Systems 10 , Michael I. Jordan , Michael J. Kearns , and Sara A. Solla , Eds .MIT Press , Cambridge , MA , USA , jun 1998 .","label":"Motivation","metadata":{},"score":"57.203285"}{"text":"The large - scale , accurate , machine - understandable drug - disease treatment knowledge base that is resultant of our study , in combination with pairs from structured databases , will have high potential in computational drug repurposing tasks . \"","label":"Motivation","metadata":{},"score":"57.290222"}{"text":"Skipping hard - to - predict emotions : On cross - validation ( see Results for details ) , our F 1 performance was hurt by low precision and low recall on the emotions which occurred infrequently in the training data ( see Discussion for comment ) .","label":"Motivation","metadata":{},"score":"57.33587"}{"text":"The second and fourth rows show how fine - tuning the split point could further improve the functioning of these classifiers .For every feature set , fine tuning a single split point for all 15 emotion classifiers yielded a decrease in precision , an increase in recall and a marked increase in F 1 in the cross - validated training set .","label":"Motivation","metadata":{},"score":"57.34983"}{"text":"The extracted PICO information can be used to automatically instantiate relevant parts of the RCT schema .An accurate structured representation of information extracted from RCT abstracts will facilitate personalized semantic search , intelligent answering and medical evidence summarization . classifiers .","label":"Motivation","metadata":{},"score":"57.353065"}{"text":"Maximum entropy classifiers have been widely used in NLP classification tasks , for example in part - of - speech tagging 3 and in named - entity recognition .4 In this work , we made use of the freely available Stanford Classifier .","label":"Motivation","metadata":{},"score":"57.357323"}{"text":"W .S .B , where S W is the within class covariance matrix and S B is the between class covariance matrix .Often S B is computed as the covariance of the category means ; alternatively , it is sometimes computed as the \" grand \" covariance matrix over all data , ignoring category labels , identical to the covariance matrix used to compute PCA basis vectors .","label":"Motivation","metadata":{},"score":"57.444"}{"text":"Typical choices include a linear activation function , a unipolar sigmoid function and a bipolar sigmoid function as illustrated in Figure 10 .The activation function should match the characteristic of the input or output data .For example , with training targets assigned the values of \" 0 \" and \" 1 \" , a sigmoid function with the outputs in the range of [ 0 , 1 ] is a good candidate for the output layer .","label":"Motivation","metadata":{},"score":"57.447243"}{"text":"The F 1 score is the harmonic mean of precision and recall .In other settings , precision is known as sensitivity ; it captures the proportion of annotated emotions that the classifier detects .Recall also uses the correct positive annotations , focusing on the proportion of positive estimates which were correct ; this is known in many other settings , notably health research , as positive predictive value .","label":"Motivation","metadata":{},"score":"57.469322"}{"text":"This reuse of code minimized the development effort related specifically to our smoking status classifier .We report precision , recall , F - score , and 95 % exact confidence intervals for each metric .Recasting the classification task for the sentence level and reusing code from other text analysis projects allowed us to quickly build a classification system that performs with a system F - score of 92.64 based on held - out data tests and of 85.57 on the formal evaluation data .","label":"Motivation","metadata":{},"score":"57.521286"}{"text":"This reuse of code minimized the development effort related specifically to our smoking status classifier .We report precision , recall , F - score , and 95 % exact confidence intervals for each metric .Recasting the classification task for the sentence level and reusing code from other text analysis projects allowed us to quickly build a classification system that performs with a system F - score of 92.64 based on held - out data tests and of 85.57 on the formal evaluation data .","label":"Motivation","metadata":{},"score":"57.521286"}{"text":"BG Obj Precision 26.37 % Recall 36.41 % F1 31.53 % Table 4 .Performance improvements augmentation on five section types .Page 4 .Classification performance ( Precision , Recall , F1 ) results for five categories ( Background , Objective , Methods , Result , Conclusion ) using classifiers : NB , ME , and DT with boosting , and with bagging .","label":"Motivation","metadata":{},"score":"58.15464"}{"text":"Figures .Citation : Ben - Hur A , Ong CS , Sonnenburg S , Schölkopf B , Rätsch G ( 2008 )Support Vector Machines and Kernels for Computational Biology .PLoS Comput Biol 4(10 ) : e1000173 .doi:10.1371/journal.pcbi.1000173 .","label":"Motivation","metadata":{},"score":"58.164463"}{"text":"doi:10.1093/nar / gkg600 .Accessed 11 August 2008 .Vert JP , Saigo H , Akutsu T ( 2004 )Local alignment kernels for biological sequences .In : B Schölkopf KT , Vert JP , editors .Kernel methods in computational biology .","label":"Motivation","metadata":{},"score":"58.26339"}{"text":"Given the high dimensionality of speech feature spaces used for automatic speech recognition , typically 39 or more , it is not feasible to visualize the distribution of data in feature space .It is possible that a reduced dimensionality subspace obtained by linear methods , such as PCA or LDA , forms an effective , or at least adequate subspace for implementing automatic speech recognition systems with a reduced dimensionality feature space .","label":"Motivation","metadata":{},"score":"58.35875"}{"text":"Figure 5 .Plot of input and output data for pseudo - random 2-D data .The output data ( red line ) is reconstructed data obtained after passing the input data through the trained neural network .In Figure 6 , NLPCA is illustrated by data which falls on a 2-D surface embedded in a 3-D space .","label":"Motivation","metadata":{},"score":"58.52581"}{"text":"LDC2006T13 , Linguistic Data Consortium .Philadelphia : 2006 .Bird S , Loper E , Klein E. O'Reilly Media Inc.Natural Language Processing with Python .Nastase V. Unsupervised all - words word sense disambiguation with grammatical dependenices .IJCNLP .","label":"Motivation","metadata":{},"score":"58.608906"}{"text":"The kernel trick can be used if the algorithm to be generalized uses the training vectors only in the form of Euclidean dot - products ( xT · y ) .Support Vector Machines SVMs were first introduced by Vapnik and developed from the theory of Structural Risk Minimization ( SRM ) [ 10].","label":"Motivation","metadata":{},"score":"58.613384"}{"text":"Morphological features : We experimented with using both character n - grams and word - stemming algorithms to complement our feature set .Character n - grams included only prefix and suffix n - grams ranging from 2-grams to 6-grams .","label":"Motivation","metadata":{},"score":"58.68338"}{"text":"The features include the frequency of various words ( e.g. \" money \" ) , special characters ( e.g. dollar signs ) , and the use of capital letters in the message .For each message , the following variables are recorded : .","label":"Motivation","metadata":{},"score":"58.694046"}{"text":"For one particular train / test split , the tree has 224/2300 errors while logistic regression has 168/2300 .This difference is statistically significant : .Cross - validation on tree size does not change the error rate .Misclassification rate is not an entirely appropriate measure for this problem , because the costs are not equal .","label":"Motivation","metadata":{},"score":"58.727783"}{"text":"Both PCA and LDA are based on linear , i.e. matrix multiplication , transformations .For the case of PCA , the transformation is based on minimizing mean square error between original data vectors and data vectors that can be estimated from the reduced dimensionality data vectors .","label":"Motivation","metadata":{},"score":"58.742332"}{"text":"In the sections below , we describe our activities with the annotated training data before focusing on our performance with the test data .Methods .Overview .In the task presented here , each sentence of a suicide note was classified as exhibiting zero or more emotions , annotated from a pre - defined set of 15 \" emotions \" .","label":"Motivation","metadata":{},"score":"58.76264"}{"text":"Our temporal resolution component does not include an explicit one - year rule for distinguishing between past smoker and current smoker , but relies on the features and labeled data to learn the differences .The most challenging category for our system to classify is past smoker .","label":"Motivation","metadata":{},"score":"58.95949"}{"text":"Experiment 2 .To simulate lack of training data , another experiment was conducted .In this experiment , the training data was separated into two groups , with about 50 % in each group .One group of data ( group 1 ) was used for \" training \" transformations while the other data ( group 2 ) was used for training classifiers .","label":"Motivation","metadata":{},"score":"59.020973"}{"text":"For these systems , we estimated only the higher frequency emotions which had performed well in cross - validation experiments : \" instructions \" , \" hopelessness \" , \" love \" , \" information \" , \" guilt \" and \" thankfulness \" .","label":"Motivation","metadata":{},"score":"59.032017"}{"text":"System 1 : Our first system , S 1 , used W1 - 2/C0 - 0/L+/M+/Dd , the feature set that performed best on cross - validation , with sigma set to 0.5 , confidence tuned to 0.198 , skipping hard - to - predict emotions .","label":"Motivation","metadata":{},"score":"59.05793"}{"text":"These vectors have 48 dimensions and each vector consists of only one peak value to indicate the category .Note that , in the TIMIT case , other reasonable choices for targets would be 61 ( the number of phone label categories ) , or 39 ( the number of collapses phone categories ) .","label":"Motivation","metadata":{},"score":"59.092022"}{"text":"This intuitive choice captures the idea of large margin separation , which is mathematically formulated in the section Classification with Large Margin .The grayscale level represents the value of the discriminant function f ( x ): dark for low values and a light shade for high values .","label":"Motivation","metadata":{},"score":"59.09712"}{"text":"Positive values indicate that , on average , the classifier improved ... .Performance of the three classifiers used for official submissions .Sorted by F 1 on the test data , results are presented for both using 5-fold cross - validation on the training data and on the test data .","label":"Motivation","metadata":{},"score":"59.11006"}{"text":"A Machine Learning for Language Toolkit . \"Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .","label":"Motivation","metadata":{},"score":"59.18312"}{"text":"For the identification experi- ments we used 2 sentences of each speaker for the training and 2 sentences as development test set .The evaluation set contains up to 5 sentences per speaker , all in all 664 true identity tests .","label":"Motivation","metadata":{},"score":"59.18564"}{"text":"337 - 404 , 1950 .[ 10 ] Vladimir N. Vapnik , The Nature of Statistical Learn- ing Theory , Information Science and Statistics .Springer , Berlin , 2nd edition , 2000 .[ 11 ] Christopher J. C. Burges , tor machines for pattern recognition , \" Data Mining and Knowledge Discovery , vol .","label":"Motivation","metadata":{},"score":"59.249832"}{"text":"121 - 167 , June 1998 .MIT Press , Cam- bridge , MA , USA , oct 2000 .[ 13 ] Arthur Hoerl and Robert Kennard , \" Ridge regression : Bi- ased estimation for nonorthogonal problems , \" Technomet- rics , vol .","label":"Motivation","metadata":{},"score":"59.265472"}{"text":"The recognition accuracies were further improved by about 3 % with PCA reduced dimensionality features versus the NLDA features for most cases , showing the effectiveness of PCA in de - correlating the network outputs .The accuracies obtained with the original 78 features , and 3 mixture HMMs , are approximately 58 % ( 1 state models ) and 63 % ( 3 state models ) .","label":"Motivation","metadata":{},"score":"59.29131"}{"text":"The detailed experiment results are shown in Table 1 .Precision , recall , and F1 are compared across five types of abstract sections with and without HMM augmentation .The results of F1 measure are summarized in Figure 2 .Each comparison is made on F1 before and after using HMM augmentation .","label":"Motivation","metadata":{},"score":"59.37629"}{"text":"Thus the method is patterned after LDA .In all cases , the dimensionality reduction is accomplished with a Neural Network ( NN ) , which internally encodes data with a reduced number of dimensions .The differences in the methods depend on error criteria used to train the network , the architecture of the network , and the extent to which the reduced dimensions are \" hidden \" in the neural network .","label":"Motivation","metadata":{},"score":"59.498695"}{"text":"These search engines employ keyword - based techniques to find relevant papers .More often than not these techniques have poor precision , mostly because they fail to consider semantics encoded in the paper as well as the query .The use of MeSH subheadings to index abstracts in PubMed aims to capture the semantics of the abstract .","label":"Motivation","metadata":{},"score":"59.57131"}{"text":"Layer 1 : Classifying Unknown and Smoking - related Sentences Feature Selection A subcorpus containing all sentences that were labeled a category other than unknown was created from the sen- tence - level training data described above .All features for the Layer 1 classifier were drawn from this subcorpus .","label":"Motivation","metadata":{},"score":"59.58707"}{"text":"This tradeoff is called the bias - variance tradeoff .Figure 14.10 provides an illustration , which is somewhat contrived , but will be useful as an example for the tradeoff .Some Chinese text contains English words written in the Roman alphabet like CPU , ONLINE , and GPS .","label":"Motivation","metadata":{},"score":"59.654877"}{"text":"After an initial search , eTACTS presents to the user a tag cloud representing the current results .When the user selects a tag , eTACTS retains only those trials containing that tag in their eligibility criteria and generates a new cloud based on tag frequency and co - occurrences in the remaining trials .","label":"Motivation","metadata":{},"score":"59.85148"}{"text":"While the classification results of KLR and SKLR are similar to the results of SVMs , we show that SKLR produces highly sparse models .Unlike SVMs the kernel logistic regression also provides an estimate of the conditional probability of class membership .","label":"Motivation","metadata":{},"score":"60.00105"}{"text":"One - feature classifier .Shown as a dotted horizontal line .This classifier uses only one feature , the number of Roman alphabet characters .Assuming a learning method that minimizes the number of misclassifications in the training set , the position of the horizontal decision boundary is not greatly affected by differences in the training set ( e.g. , noise documents ) .","label":"Motivation","metadata":{},"score":"60.01664"}{"text":"A suggested setting for β d is the weighting [ 29 ] , [ 30 ] .Note that using the WD kernel is equivalent to using a mixed spectrum kernel for each position of the sequence separately ( ignoring boundary effects ) .","label":"Motivation","metadata":{},"score":"60.07775"}{"text":"The feature space for the inhomogeneous kernel consists of all monomials with degree up to d [ 2 ] .And yet , its computation time is linear in the dimensionality of the input space .The degree of the polynomial kernel controls the flexibility of the resulting classifier ( Figure 6 ) .","label":"Motivation","metadata":{},"score":"60.129776"}{"text":"For the SVM and the KLR classi- fiers we used the RBF kernel function ( 8) and validated the ker- nel and the regularization parameters of the different classifiers on the development set .The IER is presented in the left column of table 1 .","label":"Motivation","metadata":{},"score":"60.200348"}{"text":"The nature of statistical learning theory .2nd edition .Springer .Müller KR , Mika S , Rätsch G , Tsuda K , Schölkopf B ( 2001 )An introduction to kernel - based learning algorithms .IEEE Trans Neural Netw 12 : 181 - 201 .","label":"Motivation","metadata":{},"score":"60.253555"}{"text":"Results For the hepatobiliary data set , we obtained a high sensitivity of 0.95 ( on a par with manual annotators , as compared to 0.91 for a baseline classifier ) with specificity 0.56 .For the acute renal failure data set , sensitivity increased from 0.69 to 0.89 , with specificity 0.59 .","label":"Motivation","metadata":{},"score":"60.316612"}{"text":"User evaluation was limited to one case study and a small group of evaluators due to the long duration of the experiment .Although a larger - scale evaluation could be conducted , this feasibility study demonstrated significant advantages of eTACTS over existing clinical trial search engines .","label":"Motivation","metadata":{},"score":"60.371193"}{"text":"Back then , SVM training would take thousands of times longer than naive Bayes .Today , ( linear )SVM training really is n't that much slower ( maybe a small constant times longer ) .Yet from the perspective of a consumer , there 's something fundamentally unpleasant about having to gimp an existing system .... you have to ask yourself : why should I care ?","label":"Motivation","metadata":{},"score":"60.459595"}{"text":"Memorize labels : The training data contained sentences that were identical to sentences in the test data .When using this heuristic , we copied the label from the identical training data sentence onto the matching test sentence .Classifier combinations : We trained individual classifiers based on each of the 48 combinations of the features listed above .","label":"Motivation","metadata":{},"score":"60.66607"}{"text":"Evaluation of the proposed framework is performed on the NIST SRE2012 corpus .Results not only confirm individual sub - system advancements over an established baseline , the final grand fusion solution also represents a comprehensive overall advancement for the NIST SRE2012 core tasks .","label":"Motivation","metadata":{},"score":"60.6756"}{"text":"Methods : The Vascular Disease Biorepository at Mayo Clinic was established to archive DNA , plasma , and serum from patients with suspected AVD .AVD phenotypes , relevant risk factors and comorbid conditions were ascertained by electronic medical record ( EMR)-based electronic algorithms that included diagnosis and procedure codes , laboratory data and text searches to ascertain medication use .","label":"Motivation","metadata":{},"score":"60.719635"}{"text":"Really trustworthy blog .Please keep updating with great posts like this one .I have booked marked your site and am about to email it to a few friends of mine that I know would enjoy reading . sesli sohbet sesli chat sesli sohbet siteleri Figure 3 . 2-D 2-class data , along with first LDA basis vector and first PCA basis vector .","label":"Motivation","metadata":{},"score":"60.766502"}{"text":"Word bigrams and trigrams : In addition to unigram features , we also used word bigrams and word trigrams as features .More specifically , token bigrams and trigrams , as we did not distinguish between words and punctuation in creating these n - grams .","label":"Motivation","metadata":{},"score":"60.928627"}{"text":"203 - 210 , 2005 .\" Speaker verification [ 6 ] William Campbell , \" Generalized linear discriminant se- quence kernels for speaker recognition , \" in International Conference on Acoustics , Speech , and Signal Processing , Orlando , Florida , 2002 , vol .","label":"Motivation","metadata":{},"score":"61.038925"}{"text":"The general network configuration is shown in Figure 4 .Figure 4 .Architecture of Bottleneck Neural Network .If data lies along a single curved line in a higher dimensionality space , 1 node in the bottleneck layer should be sufficient .","label":"Motivation","metadata":{},"score":"61.061394"}{"text":"a. Accessed October 25 , 2007 .Chapman WW , Bridewell W , Hanbury P , Cooper GF , Buchanan BG .A simple algorithm for identifying negated findings and diseases in discharge summaries .J Biomed Inform 2001;34 : 301 - 10 .","label":"Motivation","metadata":{},"score":"61.09352"}{"text":"All the training sentences ( 4620 sentences ) were used to extract a total of 31,300 vowel tokens for training .All the test sentences ( 1680 sentences ) were used to extract a total of 11,625 vowel tokens for testing .","label":"Motivation","metadata":{},"score":"61.13771"}{"text":"NLDA1 .In the first approach , which is referred to as NLDA1 , the transformed features are produced from the final output layer of the network .This approach is similar to the use of tandem neural networks used in some automatic speech recognition studies ( Hermansky & Sharma , 2000 ; Ellis et al . , 2001 ) .","label":"Motivation","metadata":{},"score":"61.199608"}{"text":"Several of the kernels described above are based on the framework of convolution kernels [ 54 ] , which is a method for developing kernels for an object based on kernels defined on its sub - parts , such as a protein structure composed of secondary structural elements [ 50 ] .","label":"Motivation","metadata":{},"score":"61.30781"}{"text":"\" There is a substantial amount of literature on identifying and extracting information from EMRs [ 12].Schuemie et al .[ 24 ] compared several machine - learning methods for identifying patients with liver disorder from free - text medical records .","label":"Motivation","metadata":{},"score":"61.384186"}{"text":"Advances in kernel methods : Support vector machines .Cambridge ( Massachusetts ) : MIT Press .Chapter 11 .Joachims T ( 2006 )Training linear SVMs in linear time .pp .217 - 226 .Demsar J , Zupan B , Leban G ( 2004 )","label":"Motivation","metadata":{},"score":"61.436962"}{"text":"The training of the neural network ( NLDA1 and NLDA2 ) requires category information for creating training targets .For the case of databases such as TIMIT , the data is labeled using 61 phone categories , and the starting point for training discriminative transformations would seem to be these phonetic labels .","label":"Motivation","metadata":{},"score":"61.567802"}{"text":"55 - 67 , 1970 .[14 ] Ian T. Nabney , \" Efficient training of rbf networks for clas- sification , \" in Artificial Neural Networks - ICANN 1999 , 1999 , vol .1 , pp .210 - 215 .","label":"Motivation","metadata":{},"score":"61.78914"}{"text":"All hidden nodes and output nodes had a bipolar sigmoidal activation function .After training with backpropagation , all data were transformed by the neural network .In Figure 5 , the original data is shown as blue symbols , and the transformed data is shown by red .","label":"Motivation","metadata":{},"score":"62.310604"}{"text":"Figure 16 .Accuracies of NLDA1 and NLDA2 with various dimensionality reduced features based on 1-state ( top panel ) and 3-state HMMs ( bottom panel ) .The NLDA1 features without PCA are always 48 dimensions .","label":"Motivation","metadata":{},"score":"62.313454"}{"text":"More details on kernels can be found in books such as [ 2 ] , [ 5 ] , [ 7 ] , [ 56 ] .SVM training algorithms and software .The popularity of SVMs has led to the development of a large number of special - purpose solvers for the SVM optimization problem [ 57 ] .","label":"Motivation","metadata":{},"score":"62.383484"}{"text":"If the word was determined to be negated , then we labeled the sentence with nonsmoker .All sentences that were not labeled nonsmoker were passed to the Layer 3 classifier .Layer 3 : Classifying Current Smoker , Past Smoker , and Smoker Sentences Feature Selection A subcorpus containing all sentences that were labeled current smoker or past smoker was created from the sen- tence - level training data described above .","label":"Motivation","metadata":{},"score":"62.386177"}{"text":"2that 2 ? 2 2 ( 19 ) where λ is the regularization parameter .( 22 )We apply this expansion of β to equation ( 20 ) so that all sam- ples appear in the form of dot products ( Φ(xi)Φ(xj ) ) in the feature space .","label":"Motivation","metadata":{},"score":"62.424454"}{"text":"SVM SKLR Figure 1 : Sparseness ( % ) of the discriminative classifiers using different amounts of training data .Page 5 .Using the same parameters as for the full dataset the results in table 2 show that on all subsets the models of the SKLR approach are sparser than the SVM models .","label":"Motivation","metadata":{},"score":"62.426018"}{"text":"In contrast , the nonlinear technique NLDA based on minimizing classification error was quite effective for improving accuracy .The general form of the NLDA transformer and its relationship to the HMM recognizer are depicted in Figure 7 .NLDA is based on a multilayer bottleneck neural network and performs a nonlinear feature transformation of the input data .","label":"Motivation","metadata":{},"score":"62.58786"}{"text":"Accuracies of the NLDA1 and NLDA2 features using 1-state ( top panel ) and 3-state HMMs ( bottom panel ) with various numbers of mixtures .Figure 18 .Recognition accuracies of the NLDA dimensionality reduced features using the state level targets . \"","label":"Motivation","metadata":{},"score":"62.69071"}{"text":"The \" kernel trick \" is also applicable to other types of data , e.g. , sequence data , which we illustrated in the problem of predicting splice sites in C. elegans .In the rest of this section , we outline issues that we have not covered in this tutorial and provide pointers for further reading .","label":"Motivation","metadata":{},"score":"62.748"}{"text":"\" [ Show abstract ] [ Hide abstract ] ABSTRACT : Objective Information overload is a significant problem facing online clinical trial searchers .We present eTACTS , a novel interactive retrieval framework using common eligibility tags to dynamically filter clinical trial search results .","label":"Motivation","metadata":{},"score":"62.79973"}{"text":"The WD kernel with shifts [ 31 ] is an extension of the WD kernel , allowing some positional flexibility of matching substrings .The locality improved kernel [ 32 ] and the oligo kernel [ 33 ] achieve a similar goal in a slightly different way .","label":"Motivation","metadata":{},"score":"63.05197"}{"text":"Now let 's say I 'm doing MT .My baseline is Moses and I build some fancy MT system that can only train on sentences of length 10 or less .Should I compare to Moses trained on sentences of length 10 or less , or all sentences ?","label":"Motivation","metadata":{},"score":"63.1024"}{"text":"Some variables which appear consistently are the frequency of dollar signs , exclamation points , and the word \" hp \" , which is a good indicator of non - spam in this dataset .The tree is rather complicated , but only uses a few variables .","label":"Motivation","metadata":{},"score":"63.29104"}{"text":"All computational results in this tutorial were generated using the Shogun -based Easysvm tool [ 17 ] written in python [ 18 ] , [ 19 ] .That site also provides a Web service that allows one to train and evaluate SVMs .","label":"Motivation","metadata":{},"score":"63.30838"}{"text":"However , there were at least 15 sentences in the test data that appeared in the training data with different annotations .Although the particular context of the sentence could affect the labeling , there were two pairs of notes appearing in both the test and training data that were nearly identical .","label":"Motivation","metadata":{},"score":"63.338516"}{"text":"993 - 996 .11 , pp .487 - 493 , MIT Press .[5 ] Vincent Wan and Steve Renals , using sequence discriminant support vector machines , \" IEEE Transactions on Speech and Audio Processing , vol .","label":"Motivation","metadata":{},"score":"63.476326"}{"text":"24 - S. A. Zahorian , T. Singh , H. Hu , 2007 Dimensionality Reduction of Speech Features using Nonlinear Principal Components Analysis .Proc .INTERSPEECH ' 07 , 1134 1137 , Antwerb , Belgium , Aug. 27 - 31 , 2007 .","label":"Motivation","metadata":{},"score":"63.573784"}{"text":"The field of speaker recognition can be divided into two tasks , namely speaker identification and speaker verification .The speaker identification task consists of a set of known speak- ers or clients ( closed - set ) and the problem is to decide which person from the set is talking .","label":"Motivation","metadata":{},"score":"63.62298"}{"text":"For both NLDA1 and NLDA2 , the networks were configured with 78 - 500 - 36 - 500 - 144 nodes , going from input to output .Figure 18 .Recognition accuracies of the NLDA dimensionality reduced features using the state level targets . \"","label":"Motivation","metadata":{},"score":"63.624546"}{"text":"4 , pp .4072 - 4075 .in Proc .IEEE International [ 3 ] Sven E. Krüger , Martin Schafföner , Marcel Katz , Edin Andelic , and Andreas Wendemuth , \" Speech recognition with support vector machines in a hybridsystem , \" inProc .","label":"Motivation","metadata":{},"score":"63.846024"}{"text":"A search engine might offer Chinese users without knowledge of English ( but who understand loanwords like CPU ) the option of filtering out mixed pages .We use two features for this classification task : number of Roman alphabet characters and number of Chinese characters on the web page .","label":"Motivation","metadata":{},"score":"63.874596"}{"text":"Thus NTIMIT is more bandlimited ( approximately 300Hz to 3400 Hz ) , more noisy , but has the identical \" raw \" speech .DCTC / DCSC speech features .The modified DCTC is used for representing speech spectra , and the modified DCSC is used to represent spectral trajectories .","label":"Motivation","metadata":{},"score":"63.90165"}{"text":"286 - 93 .Foster T. Suicide note themes and suicide prevention .International journal of psychiatry in medicine .[PubMed ] .Pestian J , Nasrallah H , Matykiewicz P , Bennett A , Leenaars A. Suicide note classification using natural language processing : A content analysis .","label":"Motivation","metadata":{},"score":"64.08546"}{"text":"The number of DCTCs used was 13 , and number of DCS terms was varied from 4 to 7 , for a total number of features ranging from 52 to 91 .These numbers are given for each experiment .Additionally , as a control , one experiment was conducted with Mel - frequency Cepstral Coefficients ( MFCCs ) ( Davis & Mermelstein , 1980 ) , since these MFCC features are most typically used in ASR experiments .","label":"Motivation","metadata":{},"score":"64.1525"}{"text":"The database is further divided into a suggested training set ( 4620 sentences , 462 speakers ) and suggested test set ( 1680 sentences , 168 speakers ) .The training and test sets are balanced in terms of representing dialect regions and male / female speakers .","label":"Motivation","metadata":{},"score":"64.21966"}{"text":"A question frequently posed by practitioners is \" which kernel with which parameters should I use for my data ?\" There are several answers to this question .The first is that it is , like most practical questions in machine learning , data - dependent , so several kernels should be tried .","label":"Motivation","metadata":{},"score":"64.26147"}{"text":"For example , in the case of English unlabeled second - order parsing , we improve from a baseline accuracy of 92.02 % to 93.16 % , and in the case of Czech unlabeled second - order parsing , we improve from a baseline accuracy of 86.13 % to 87.13 % .","label":"Motivation","metadata":{},"score":"64.27178"}{"text":"Each kNN neighborhood makes an independent classification decision .The parameter in this case is the estimate from Figure 14.7 .Thus , kNN 's capacity is only limited by the size of the training set .It can memorize arbitrarily large training sets .","label":"Motivation","metadata":{},"score":"64.30872"}{"text":"The Shogun toolbox contains eight different SVM implementations together with a large collection of different kernels for real - valued and sequence data .Acknowledgments .References .Boser BE , Guyon IM , Vapnik VN ( 1992 )A training algorithm for optimal margin classifiers .","label":"Motivation","metadata":{},"score":"64.31143"}{"text":"Kernels for real - valued data .Real - valued data , i.e. , data where the examples are vectors of a given dimensionality , are common in bioinformatics and other areas .The two most commonly used kernel functions for real - valued data are the polynomial and the Gaussian kernel .","label":"Motivation","metadata":{},"score":"64.359695"}{"text":"The large body of published biomedical research articles and clinical case reports available on MEDLINE is a rich source of FDA - approved drug - disease indication as well as drug - repurposing knowledge that is crucial for applying FDA - approved drugs for new diseases .","label":"Motivation","metadata":{},"score":"64.538925"}{"text":"We compared different approaches to develop an automatic case identification system with high sensitivity to assist manual annotators .Methods We used four different machine - learning algorithms to build case identification systems for two data sets , one comprising hepatobiliary disease patients , the other acute renal failure patients .","label":"Motivation","metadata":{},"score":"64.616394"}{"text":"One note was in the training data , one in the test data .Even here we find inconsistencies .In one pair , 17 annotations were made to the note found in the training data , yet only 7 annotations were made to the note in the test data .","label":"Motivation","metadata":{},"score":"65.27467"}{"text":"Because of theoretical and empirical evidence showing that SVMs are well - suited for text categorization,4,5our efforts focus on building such a classifier for the task .To build a sentence - level classifier , we manually identified every sentence that we judged related to the patient 's smoking status in each document and assigned that sentence the smoking status category assigned to the document .","label":"Motivation","metadata":{},"score":"65.38695"}{"text":"These variable dependencies introduced much noise , possibly because we were not differentiating between the arguments of the dependencies we were conflating .For example , the annotation guidelines for the blame label state that the author of the note should have been blaming someone .","label":"Motivation","metadata":{},"score":"65.54776"}{"text":"This article describes our system entry for the 2006 I2B2 contest \" Challenges in Natural Language Processing for Clinical Data \" for the task of identifying the smoking status of patients .Our system makes the simplifying assumption that patient - level smoking status determination can be achieved by accurately classifying individual sentences from a patient 's record .","label":"Motivation","metadata":{},"score":"65.57074"}{"text":"N ( 25 ) with q training samples .The IVM aims to iteratively minimize the NLL by adding training samples to a subset S of selected training vectors until the minimum of the NLL is reached .( 28 ) While in the original KLR algorithm we iteratively estimate the parameter α by the IRLS algorithm , we can use a one - step approximation here [ 16].","label":"Motivation","metadata":{},"score":"65.629364"}{"text":"The anchor words that we used were from a small dictionary we created that contained the top 10 features as ranked by the weights in the SVM model built for Layer 1 and included words like smoke , smoker , tobacco , and cigarette .","label":"Motivation","metadata":{},"score":"65.64287"}{"text":"It plays a similar role as the degree of the polynomial kernel in controlling the flexibility of the resulting classifier ( see Figures 6 and 7 ) .The discriminant function ( Equation 7 ) is thus a sum of Gaussian \" bumps \" centered around each support vector ( SV ) .","label":"Motivation","metadata":{},"score":"66.43022"}{"text":"Some of the limitations as applied to the use - case are negation detection and temporal resolution .Full - text .Technical Brief ?Our system makes the simplifying assumption that patient - level smoking status determination can be achieved by accurately classifying individual sentences from a patient 's record .","label":"Motivation","metadata":{},"score":"66.5656"}{"text":"Of the text material in the database , two dialect sentences ( SA sentences ) were designed to expose the specific variants of the speakers and were read by all 630 speakers .There are 450 phonetically - compact sentences ( SX sentences ) which provide a good coverage of pairs of phones .","label":"Motivation","metadata":{},"score":"66.593254"}{"text":"Our system has three layers of sentence classification ( Figure 1 ) .Layer 1 classifies sentences as unknown or smoking- related ( an umbrella category for nonsmoker , past smoker , current smoker , and smoker ) .All sentences labeled smok- ing - related are passed on to Layer 2 .","label":"Motivation","metadata":{},"score":"66.62985"}{"text":"Sentences not marked as nonsmoker by Layer 2 are passed to Layer 3 .Layer 3 assigns current smoker , past smoker , and the generic smoker categories by performing temporal resolution .After each sentence in a document is classified , we apply precedence rules to assign the document level smoking status .","label":"Motivation","metadata":{},"score":"66.67721"}{"text":"Structured abstracts , introduced in 1987 , explicitly provide type information for all the sentences in an abstract .While many journals use structured abstract medical evidence extracting structured The structured AMIA 2006 Symposium Proceedings Page - 824 .Page 2 . format , the majority of RCT abstracts still remain unstructured [ 5].","label":"Motivation","metadata":{},"score":"66.95691"}{"text":"Figure 8 .Use of network outputs in NLDA1 . NLDA2 .Figure 9 illustrates the use of network outputs in NLDA2 .These two versions of NLDA were experimentally tested , with and without PCA following the neural network transformer , with some variations of the nonlinearities in the networks .","label":"Motivation","metadata":{},"score":"66.97916"}{"text":"Word shape : One of the features that can be automatically generated by the Stanford Classifier is word shape .This feature is used to conflate words that \" look \" the same .For example , all 4-letter words beginning with a capital letter could both be represented as \" Xxxx \" and all two - digit monetary amounts could be represented as \" $ dd \" .","label":"Motivation","metadata":{},"score":"67.07707"}{"text":"The largest distance of the score means and the highest confidence can be observed for the SVM models .Introducing a threshold for accepting or rejecting a speaker of the identification set , the Equal Error Rate ( EER ) gives the lowest error rate by an equal rate of false - accepts and false- reject errors .","label":"Motivation","metadata":{},"score":"67.13573"}{"text":"pp .42 - 63 .Shawe - Taylor J , Cristianini N ( 2004 ) Kernel methods for pattern analysis .Cambridge ( United Kingdom ) : Cambridge University Press .Burge C , Tuschl T , Sharp P ( 1999 ) Splicing of precursors to mRNAs by the spliceosomes .","label":"Motivation","metadata":{},"score":"67.54776"}{"text":"Introduction .The goal of this study was to identify emotional content expressed in suicide notes .This was undertaken as part of the 2011 i2b2 NLP Shared Task , 1 Track 2 .The organizers of the shared task created a fixed inventory of fifteen emotions ( Table 1 ) .","label":"Motivation","metadata":{},"score":"67.74076"}{"text":"References [ 1 ] Volker Roth , \" Probabilistic discriminative kernel classi- fiers for multi - class problems , \" in Lecture Notes in Com- puter Science .2001 , vol .2191 , pp .246 - 253 , Springer .","label":"Motivation","metadata":{},"score":"67.82741"}{"text":"\" There is also ongoing research exploring the application of LR in speaker verification .Kernel LR is proposed in [ 48 ] , [ 49 ] for an MFCC frame - level , feature - based speaker verification solution .i - Vector - based LR was recently investigated for the telephone - telephone condition of SRE2010 [ 12].","label":"Motivation","metadata":{},"score":"68.03501"}{"text":"Figure 14 .Figure 15 .Illustration of the state level training targets with \" do n't cares . \"The -1 values are used to denote \" do n't cares . \"Two approaches are used to determine state boundaries .","label":"Motivation","metadata":{},"score":"68.10149"}{"text":"The training vectors x for which the weights α are greater than zero are called support vectors .Interpreting the .Page 3 .( 18 )To avoid over - fitting to the training data it is necessary to im- pose a penalty on large fluctuations of the estimated parameters β .","label":"Motivation","metadata":{},"score":"68.22348"}{"text":"There is no doubt in our minds that the sentiments expressed in the suicide notes must have been present prior to the actual time of suicide .We consider that there may have been previous efforts to express these emotions to other people .","label":"Motivation","metadata":{},"score":"68.356735"}{"text":"In Proceedings of the AAAI 2005 Workshop on Question Answering in Restricted Domains , 2005 .Ad Hoc Working Group for Critical Appraisal of the Medical Literature .A proposal for more informative abstracts of clinical articles .Annals of Internal Medicine 106:595 - 604 .","label":"Motivation","metadata":{},"score":"68.58403"}{"text":"In addition , there were instances where exactly same sentence did not attract the same emotion from the annotators .In the training data , some sentences appeared multiple times across notes .For example , the sentence \" I love you . \" appeared in 7 training sentences : 5 times annotated with \" love \" and 2 left unannotated .","label":"Motivation","metadata":{},"score":"68.69786"}{"text":"The SKLR gives a good performance with 21.9 % .Naturally , the non - sparse KLR needs all training vectors in the experiments and its sparsity level is 0 % .As can be seen in figure 1 the number of support vectors increased nearly linear with the amount of training data which results in a decreasing sparseness , while the number of relevant feature vectors for the SKLR increased only slightly for larger datasets . is computational advantage over the SVM because a much smaller portion of kernel products have to be calculated in the test .","label":"Motivation","metadata":{},"score":"69.09653"}{"text":"Thus the estimation of state boundary information is required .This boundary information may be in error due to the nature of unclear state boundaries and the lack of a reliable estimation approach .Therefore , in the discriminative training process , \" do n't cares \" were used to account for this lack of precision in determining state boundaries .","label":"Motivation","metadata":{},"score":"69.370346"}{"text":"San Francisco , CA : Morgan Kaufmann Publishers , 2000 .Joachims T. Text categorization with support vector machines : learning with many relevant features .In : Machine Learning : ECML-98 . 10th European Conference on Machine Learning .Heidelberg , Germany : Springer - Verlag , 1998:137 - 42 .","label":"Motivation","metadata":{},"score":"69.562485"}{"text":"Uzuner Ö , Goldstein I , Luo Y , Kohane I. Identifying patient smoking status from medical discharge records .J Am Med Inform Assoc .2008;15:xxx .Cortes C , Vapnik V. Support - vector networks .Machine Learning 1995;10:273 - 97 .","label":"Motivation","metadata":{},"score":"70.30107"}{"text":"The vector w is known as the weight vector , and the scalar b is called the bias .The bias translates the hyperplane with respect to the origin ( see Figure 1 ) .( Unlike many schematic representations that the reader may have seen , the figures in this paper are generated by actually applying the SVM on the data points as shown .","label":"Motivation","metadata":{},"score":"70.34134"}{"text":"In this tutorial , we consider the problem of recognizing acceptor splice sites as a running example , which allows us to illustrate different properties of SVMs using different kernels ( similar results can be obtained for donor splice sites as well [ 13 ] ) .","label":"Motivation","metadata":{},"score":"70.39546"}{"text":"While the spectrum kernel is in principle able to recognize such motifs , it can not distinguish where exactly the motif appears in the sequence .However , this is crucial in deciding where exactly the splice site is located .And indeed , Position Weight Matrices ( PWMs ) are able to predict splice sites with high accuracy .","label":"Motivation","metadata":{},"score":"70.727844"}{"text":"The sparsification of SVM has been well studied , and many sparse versions of 2-norm SVM , such as 1-norm SVM ( 1-SVM ) , have been developed .But , the sparsification of KLR has been less studied .","label":"Motivation","metadata":{},"score":"70.95495"}{"text":"Conclusion In this article , we described our system for identifying patient smoking status as part of the First Shared Task on Natural Language Challenges for Clinical Data .We reduce the problem of document classification to a sentence classi- fication task to discover the relevant smoking information from which the final document level assignment is derived .","label":"Motivation","metadata":{},"score":"71.19435"}{"text":"ICML .New York : ACM Press . pp .233 - 240 .Cai C , Han L , Ji Z , Chen X , Chen Y ( 2003 )SVM - Prot : Web - based support vector machine software for functional classification of a protein from its primary sequence .","label":"Motivation","metadata":{},"score":"71.48642"}{"text":"AB is partially supported by National Science Foundation grant DBI-0754247 .Competing interests : The authors have declared that no competing interests exist .Introduction .The increasing wealth of biological data coming from a large variety of platforms and the continued development of new high - throughput methods for probing biological systems require increasingly more sophisticated computational approaches .","label":"Motivation","metadata":{},"score":"71.94853"}{"text":"ACKNOWLEDGEMENTS Author is supported by the training grant 5T15LM007033 - 22 from U.S. National Library Medicine ( NLM ) .References .McKeown , K. Elhadad , N. ; and Hatzivassiloglou , V. Leveraging a common representation for personalized search and summarization in a medical digital library .","label":"Motivation","metadata":{},"score":"72.29985"}{"text":"\" [ Show abstract ] [ Hide abstract ] ABSTRACT : Results of medical research studies are often contradictory or can not be reproduced .One reason is that there may not be enough patient subjects available for observation for a long enough time period .","label":"Motivation","metadata":{},"score":"72.38721"}{"text":"( Incidentally , although this issue did n't come up in my recent reviewing , I suppose that if I were reviewing a paper that had this issue , it probably would n't hurt if the authors were to ease me through this imagination process .","label":"Motivation","metadata":{},"score":"72.56414"}{"text":"The state training targets with \" do n't cares \" uses \" do n't care \" states for each phoneme model , so that one neural network trained with the targets can generate state dependent outputs .As illustrated in Figure 15 , the phone - specific training targets in Figure 14 are expanded to 144 dimensions by duplicating the phoneme specific target by the required number of the states .","label":"Motivation","metadata":{},"score":"73.02226"}{"text":"Journal of the American Medical Informatics AssociationVolume 15 Number 1 Jan / Feb 2008 25 .Page 2 .We used the WEKA SVM implementation for our classification task .One of the MAWUI components provides a way to generate Weka - compliant data files suitable for training classifiers from features created by UIMA annotators .","label":"Motivation","metadata":{},"score":"73.15532"}{"text":"To hedge our bets among them , we should pick the average : 45 degrees .According to Bayesian statistics , you should always average when making predictions .So I am recommending a Bayesian approach to linear classification .For my PhD thesis , I developed a technique for efficiently computing the average linear classifier .","label":"Motivation","metadata":{},"score":"73.37568"}{"text":"The purpose of this report is to provide open source software for multi - site clinical studies and to report on early uses of this application .At this time SHRINE implementations have been used for multi - site studies of autism co - morbidity , juvenile idiopathic arthritis , peripartum cardiomyopathy , colorectal cancer , diabetes , and others .","label":"Motivation","metadata":{},"score":"73.448166"}{"text":"Potential en- hancements are the inclusion metadata information as fea- tures , e.g. , section headings , and experimenting with higher order SVMs , especially for temporal resolution .We also noticed interesting cases such as the following report , which contained the sentence \" He does drink alcohol three drinks per day , denies any current tobacco use . \" The final classification as provided by the challenge organizers is unknown despite the fact that based on the above sentence one would be tempted to assign the nonsmoker label .","label":"Motivation","metadata":{},"score":"73.81538"}{"text":"Various numbers of states and mixtures were evaluated as described in the following experiments .In all cases diagonal covariance matrices were used .For final evaluations of accuracy , some of these 48 monophones were combined to create the \" standard \" set of 39 phone categories .","label":"Motivation","metadata":{},"score":"73.90402"}{"text":"Figure 5 .Sequence logo for acceptor splice sites : splice sites have quite strong consensus sequences , i.e. , almost every position in a small window around the splice site is representative of the most frequently occurring nucleotide when many existing sequences are compared in an alignment .","label":"Motivation","metadata":{},"score":"73.95556"}{"text":"The RNA world .2nd edition .Cold Spring Harbor Laboratory Press . pp .525 - 560 .Nilsen T ( 2003 )The spliceosome : The most complex macromolecular machine in the cell ?Bioessays 25 : .Provost FJ , Fawcett T , Kohavi R ( 1998 )","label":"Motivation","metadata":{},"score":"74.40201"}{"text":"Final Resolution : Discovering Smoking Status at the Document Level After each sentence in a document was classified into one of the unknown , past smoker , current smoker , smoker , or non- smoker categories , we applied additional logic to assign the final document - level smoking status .","label":"Motivation","metadata":{},"score":"74.58458"}{"text":"A total of 1890 phonetically - diverse sentences ( SI sentences ) were selected from existing text sources to add diversity in sentence types and phonetic contexts .Each speaker read 3 of these sentences , with each text being read only by a single speaker .","label":"Motivation","metadata":{},"score":"74.89712"}{"text":"The targets were chosen as the 48 ( collapsed ) phones in the training data .The number of hidden layers was experimentally determined as well as the number of nodes included in those layers .However , most typically three hidden layers were used .","label":"Motivation","metadata":{},"score":"74.9657"}{"text":"Proc .ICASSP ' 97 , 1011 1014 , Munich , Germany , April 21 - 24 , 1997 .23 - S. A. Zahorian , A. M. Zimmer , F. Meng , 2002 Vowel Classification for Computer - based Visual Feedback for Speech Training for the Hearing Impaired , Proc .","label":"Motivation","metadata":{},"score":"77.90407"}{"text":"21 - S. A. Zahorian , D. Qian , A. J. Jagharghi , 1991 Acoustic - phonetic transformations for improved speaker - independent isolated word recognition .Proc .ICASSP'91 , 561 564 .Toronto , Ontario , Canada , May 14 - 17 , 1991 .","label":"Motivation","metadata":{},"score":"79.00137"}{"text":"Had we used entity detection to determine that both Mary and John were people , we could have constructed \" dobj(blame , PERSON ) \" , separating those examples from \" dobj(blame , THING ) \" and potentially improving on our performance .","label":"Motivation","metadata":{},"score":"79.03141"}{"text":"[19 ] Håkan Melin and Johan Lindberg , \" Guidelines for ex- periments on the polycost database , \" in Proceedings of a COST 250 workshop on Application of Speaker Recogni- tion Techniques in Telephony , Vigo , Spain , 1996 , pp .","label":"Motivation","metadata":{},"score":"79.1026"}{"text":"[ PMC free article ] [ PubMed ] Tools . by Terry Koo , Xavier Carreras , Michael Collins - In Proc .ACL / HLT , 2008 . \" ...We present a simple and effective semisupervised method for training dependency parsers .","label":"Motivation","metadata":{},"score":"79.419136"}{"text":"There were also instances where the algorithm corrected words that were n't incorrect .Some of these errors may have been addressed more accurately by using an n - gram language model to estimate the best possible correction .10 For example , the phrase \" get bettery charged \" should have been corrected as \" get battery charged \" but was actually changed to \" get better charged \" .","label":"Motivation","metadata":{},"score":"79.85298"}{"text":"Cambridge ( Massachusetts ) : MIT Press .Vert JP ( 2007 ) Kernel methods in genomics and computational biology .In : Camps - Valls G , Rojo - Alvarez JL , Martinez - Ramon M , editors .Kernel methods in bioengineering , signal and image processing .","label":"Motivation","metadata":{},"score":"80.73679"}{"text":"In : Shavlik J , editor .pp .445 - 453 .ICML ' 98 : Proceedings of the Fifteenth International Conference on Machine Learning .San Francisco : Morgan Kaufmann Publishers Inc. .Davis J , Goadrich M ( 2006 )","label":"Motivation","metadata":{},"score":"81.707664"}{"text":"The variable dependencies feature ( Dv ) conflated dependencies such as \" dobj(blame , John ) \" and \" dobj(blame , Mary ) \" into \" dobj(blame , x ) \" .We expected that this could help with data sparsity issues and we demonstrated large gains in recall when using this feature .","label":"Motivation","metadata":{},"score":"81.92644"}{"text":"The authors thank Lesa Rohde for manual annotations and the JAMIA reviewers for their critiques .Correspondence : Guergana K. Savova , PhD , Biomedical Informatics Research , Mayo Clinic , 200 First Street SW , Rochester , MN 55902 ; e - mail : ? mailto:savova.guergana@mayo.edu ? .","label":"Motivation","metadata":{},"score":"82.061066"}{"text":"The mean age of the initial 2182 patients recruited was 70.4 ± 11.2 years , 62.6 % were men and 97.6 % were whites .The prevalences of AVD phenotypes were : carotid artery stenosis 48 % , abdominal aortic aneurysm 21 % and peripheral arterial disease 38 % .","label":"Motivation","metadata":{},"score":"82.46601"}{"text":"This is because documents from different classes can be mapped to the same document representation .For example , the one - sentence documents China sues France and France sues China are mapped to the same document representation in a bag of words model .","label":"Motivation","metadata":{},"score":"83.19249"}{"text":"We are confident that further improvements could be achieved using off - the - shelf components as done here .Acknowledgments .We acknowledge the efforts of the i2b2 organizers and all of the brave efforts of the volunteers who kindly annotated the extensive and sensitive dataset .","label":"Motivation","metadata":{},"score":"84.55589"}{"text":"Bottou L , Chapelle O , DeCoste D , Weston J , editors .( 2007 ) Large scale kernel machines .Cambridge ( Massachusetts ) : MIT Press .Accessed 11 August 2008 .Joachims T ( 1998 )Making large - scale support vector machine learning practical .","label":"Motivation","metadata":{},"score":"85.11372"}{"text":"Our system was built on IBM 's UIMA , which is a framework that facilitates the construction of reusable text analysis components ( see Figure 2 , available as a JAMIA on - line data Affiliations of the authors : Biomedical Informatics , Mayo Clinic , Rochester , MN .","label":"Motivation","metadata":{},"score":"85.662384"}{"text":"Introns are excised from premature mRNAs in a processing step after transcription ( see Figure 4 and , for instance , [ 8 ] - [ 12 ] for more details ) .The vast majority of all splice sites are characterized by the presence of specific dimers on the intronic side of the splice site : GT for donor and AG for acceptor sites ( see Figure 5 ) .","label":"Motivation","metadata":{},"score":"87.52329"}{"text":"Faculty of Computer and Information Science , University of Ljubljana .Gawande K , Webers C , Smola A , Vishwanathan S , Gunter S , et al .( 2007 ) ELEFANT user manual ( revision 0.1 ) .Technical report .","label":"Motivation","metadata":{},"score":"88.80853"}{"text":"Publisher conditions are provided by RoMEO .Differing provisions from the publisher 's actual policy or licence agreement may be applicable . \"Important study variables , such as smoking status[57 ] , [ 58 ] , co - morbidities[59 ] , and family disease history[60 ] are often missing from the coded record and more likely to appear in physician notes .","label":"Motivation","metadata":{},"score":"88.86032"}{"text":"The suicide notes provided in the data set were transcriptions of hand - written notes .These notes contained many spelling errors and tokenization inconsistencies .For example , \" capsuls \" was amended by our system as \" capsule \" instead of \" capsules \" .","label":"Motivation","metadata":{},"score":"89.95631"}{"text":"[20 ] Paul Taylor , Richard Caley , Alan W. Black , and Simon King , \" Edinburgh speech tools library , \" Tech .Rep. , Uni- versity of Edinburgh , 1999 .[21 ] Steve Young , Gunnar Evermann , Dan Kershaw , Gareth Moore , Julian Odell , Dave Ollason , Dan Povey , Valtcho Valtchev , and Phil Woodland , The HTK Book , Cambridge University Engineering Department , 2002 .","label":"Motivation","metadata":{},"score":"92.11673"}{"text":"One necessary step in the process of obtaining mature mRNA is called splicing .The mRNA sequence of a eukaryotic gene is \" interrupted \" by noncoding regions called introns .A gene starts with an exon and may then be interrupted by an intron , followed by another exon , intron , and so on until it ends in an exon .","label":"Motivation","metadata":{},"score":"93.2233"}{"text":"Smoking status was ascertained by NLP as described previously 6 and smokers were defined as either current or past smokers .\" [ Show abstract ] [ Hide abstract ] ABSTRACT : Background : Atherosclerotic vascular disease ( AVD ) , a leading cause of morbidity and mortality , is increasing in prevalence in the developing world .","label":"Motivation","metadata":{},"score":"93.89383"}{"text":"Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .Publisher conditions are provided by RoMEO .","label":"Motivation","metadata":{},"score":"94.25251"}{"text":"If this article contains identifiable human subject(s ) author(s ) were required to supply signed patient consent prior to publication .Author(s ) have confirmed that the published article is unique and not under consideration nor published by any other publication and that they have consent to reproduce any copyrighted material .","label":"Motivation","metadata":{},"score":"96.576355"}{"text":"5th Annual ACM Workshop on COLT .Pittsburgh ( Pennsylvania ) : ACM Press . pp .144 - 152 .Accessed 11 August 2008 .Schölkopf B , Smola A ( 2002 ) Learning with kernels .Cambridge ( Massachusetts ) : MIT Press .","label":"Motivation","metadata":{},"score":"96.89279"}{"text":"Splice sites have quite strong consensus sequences , i.e. , almost every position in a small window around the splice site is representative of the most frequently occurring nucleotide when many existing sequences are compared in an alignment ( cf .Figure 5 ) .","label":"Motivation","metadata":{},"score":"97.99001"}{"text":"Published : October 31 , 2008 .Copyright : © 2008 Ben - Hur et al .This is an open - access article distributed under the terms of the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited .","label":"Motivation","metadata":{},"score":"115.85485"}