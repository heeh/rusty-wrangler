{"text":"Other Repositories / Bibliography .BibTeX .Share .OpenURL .Abstract .This paper presents a corpus - based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers , each of which is based on lexical features that represent co - occurring words in varying sized windows of context .","label":"Background","metadata":{},"score":"31.684769"}{"text":"The senses of the repeated words are tied together , thus making any change of sense more noticeable ( in the resulting energy value ) .This leads to a more accurate disambiguation of topic words which in turn lead to an increased disambiguation accuracy of other nouns .","label":"Background","metadata":{},"score":"31.901676"}{"text":"Print .Tools . \" ...This paper presents context - group discrimination , a disambiguation algorithm based on clustering .Senses are interpreted as groups ( or clusters ) of similar contexts of the ambiguous word .Words , contexts , and senses are represented in Word Space , a high - dimensional , real - valued space in which closen ... \" .","label":"Background","metadata":{},"score":"32.90679"}{"text":"Likewise , if only one possible sense tag was observed for any POS - level lemma analysis , then this unambiguous sense tag was also returned .keywords : word sense disambiguation , morphological analysis , part - of - speech tagging , highly inflected languages .","label":"Background","metadata":{},"score":"33.382698"}{"text":"As our system extracts all information from the parallel corpus at hand , it is a very flexible and language - independent approach that allows to bypass the knowledge acquisition bottleneck for Word Sense Disambiguation .We recast the task of disambiguating polysemous nouns as a multilingual classification task .","label":"Background","metadata":{},"score":"33.433754"}{"text":"This paper describes a semi - automatic method of inducing underspecified semantic classes from WordNet verbs and nouns .An underspecified semantic class is an abstract semantic class which represents systematic polysemy : a set of word senses that are related in systematic and predictable ways .","label":"Background","metadata":{},"score":"33.643196"}{"text":"This paper describes a semi - automatic method of inducing underspecified semantic classes from WordNet verbs and nouns .An underspecified semantic class is an abstract semantic class which represents systematic polysemy : a set of word senses that are related in systematic and predictable ways .","label":"Background","metadata":{},"score":"33.643196"}{"text":"This paper proposes an algorithm for word sense disambiguation based on a vector representation of word similarity derived from lexical co - occurrence .It differs from standard approaches by allowing for as fine grained distinctions as is warranted by the information at hand , rather than supposing a ... \" .","label":"Background","metadata":{},"score":"34.71827"}{"text":"Thanks to previous experiments , we noticed that for some words or even some synsets , the information useful for disambiguation purpose are present in variable window sizes around the term to be disambiguated .In order to select automatically the appropriate window size , we have designed a mixed approach combining the SCT and a long - range similarity measure ( like in document retrieval ) , in some particular cases .","label":"Background","metadata":{},"score":"36.187805"}{"text":"Our approach uses a linguistic filter to identify word classes which are expected to convey salient information about the term , and a semantics - based matching technique to find terms occurring in similar environments .Emphasis is placed on the relationship between a term and its context words , rather than simply relying on statistical information about frequency of occurrence .","label":"Background","metadata":{},"score":"36.380417"}{"text":"Based on a hierarchical structure containing nouns with 12 levels of depth and verbs with 4 levels of depth , conceptual distance and conceptual density for application to WSD have been studied .Twenty - five semantic categories in widely distributed as those in Chinese or Japanese , the WSD of homographs needs to be studied .","label":"Background","metadata":{},"score":"36.695282"}{"text":"We present a novel beam - search decoder for grammatical error correction .The decoder iteratively generates new hypothesis corrections from current hypotheses and scores them based on features of grammatical correctness and fluency .These features include scores from discriminative classifiers for specific error categories , such as articles and prepositions .","label":"Background","metadata":{},"score":"36.720818"}{"text":"In this paper , we present a novel approach which incorporates the web - derived selectional preferences to improve statistical dependency parsing .Conventional selectional preference learning methods have usually focused on word - to - class relations , e.g. , a verb selects as its subject a given nominal c ... \" .","label":"Background","metadata":{},"score":"36.806293"}{"text":"Description : This system takes a supervised learning approach to word sense disambiguation , where a Naive Bayesian classifier is learned from sense - tagged training examples .No information from WordNet is utilized by this system .The Naive Bayesian classifier is based on a set of features that consists of unigrams ( one word sequences ) that are identified in a filtering set prior to learning .","label":"Background","metadata":{},"score":"37.015003"}{"text":"Most recent research in learning approaches to natural language have studied fairly \" low - level \" tasks such as morphology , part - ofspeech tagging , and syntactic parsing .However , I believe that logical approaches may have the most relevance and impact at the level of semantic interpretation , where a logical representation of sentence meaning is important and useful .","label":"Background","metadata":{},"score":"37.449966"}{"text":"This thesis presents a machine learning approach to Word Sense Disambiguation ( WSD ) , the task that consists in selecting the correct sense of an ambiguous word in a given context .We recast the task of disambiguating polysemous nouns as a multilingual classification task .","label":"Background","metadata":{},"score":"37.7536"}{"text":"In this paper , a different approach to formulating a probabili ... \" .Most probabilistic classifiers used for word - sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features .","label":"Background","metadata":{},"score":"38.327515"}{"text":"From the pronunciation and sense - tagged corpora , features are extracted and used to correctly classify instances of ambiguous words in new data .With learning algorithms such as the Bayesian classification model proposed in [ 7 ] , or a decision tree adopted in [ 10 , 25 , 26 ] , many studies have been reported to achieve high accuracies in WSD .","label":"Background","metadata":{},"score":"38.547546"}{"text":"The system is based on Yarowsky 's decision list .It sorts the features according to the log - likelihood value and chooses the sense of the feature with the highest value .Features occurring only once were pruned .In the case of the English all - words task , Semcor 1.6 was used for training , via a automatically produced WordNet 1.6 - 1.7 map .","label":"Background","metadata":{},"score":"38.679245"}{"text":"This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings .We present a new neural network architecture which 1 ) learns word embeddings that better capture the semantics of words by incorporating both local and global document context , and 2 ) accounts for homonymy and polysemy by learning multiple embeddings per word .","label":"Background","metadata":{},"score":"38.707073"}{"text":"Other differences are that the naive Bayes model uses only positive traini ... . \" ...The most effective paradigm for word sense disambiguation , supervised learning , seems to be stuck because of the knowledge acquisition bottleneck .In this paper we take an in - depth study of the performance of decision lists on two publicly available corpora and an additional corpus automatically acq ... \" .","label":"Background","metadata":{},"score":"38.72078"}{"text":"As a consequence , the task is turned into a cross - lingual WSD task , that consists in selecting the contextually correct translation of an ambiguous target word .In order to evaluate the viability of cross - lingual Word Sense Disambiguation , we constructed a lexical sample data set of twenty ambiguous nouns .","label":"Background","metadata":{},"score":"39.041935"}{"text":"As a consequence , the task is turned into a cross - lingual WSD task , that consists in selecting the contextually correct translation of an ambiguous target word .In order to evaluate the viability of cross - lingual Word Sense Disambiguation , we constructed a lexical sample data set of twenty ambiguous nouns .","label":"Background","metadata":{},"score":"39.041935"}{"text":"For every subsystem the features included not only bag - of - words in a fixed context window and contextual n - grams , but also a rich variety of syntactic features including subjects , objects and objects of prepositions of verbs and several modification relationships for nouns and adjectives .","label":"Background","metadata":{},"score":"39.210625"}{"text":"In order to train the models , we took advantage of the Semcor ( release 1.6 ) .Moreover , we chose to apply a special treatment on words fulfilling two constraints : .To be i ) one of the most words to be disambiguated in the all - words task , ii ) one of the words to be disambiguated in the lexical sample task .","label":"Background","metadata":{},"score":"39.353497"}{"text":"The data is preprocessed : SGML tags are eliminated , the text is tokenized , part of speech tagged and Named Entities are identified .2 ) Compound concepts are identified : we determine the maximum sequence of words which form a compound word defined in WordNet .","label":"Background","metadata":{},"score":"39.7347"}{"text":"it is important to capture both the syntax and semantics of words .Several studies in psychology have also shown that global context can help language comprehension ( Hess et al . , 1995 ) and acquisition ( Li et al . , 2000 ) .","label":"Background","metadata":{},"score":"39.95588"}{"text":"With the aid of the matrix we have enriched sense descriptions and also we have used it to filter the context of the words to be disambiguated taking into account the part of speech involved .As back - off strategies we have used the same one discarding the frequency filter and for the few words left the first sense .","label":"Background","metadata":{},"score":"40.14779"}{"text":"With the aid of the matrix we have enriched sense descriptions and also we have used it to filter the context of the words to be disambiguated taking into account the part of speech involved .As back - off strategies we have used the same one discarding the frequency filter and for the few words left the first sense .","label":"Background","metadata":{},"score":"40.14779"}{"text":"The JHU - English system differed slightly from our other lexical sample systems in its treatment of phrasal senses .Because phrasal compounds such as verb - particle pairs were explicitly marked , both in training and test data , this additional provided information was used as follows : If a phrasal compound was marked in the data , then only compound senses ( e.g. verb - particle ) were considered .","label":"Background","metadata":{},"score":"40.20867"}{"text":"When presented with a test example , each classifier assigns a probability to each possible sense .These probabilities are summed and sense with the largest value is assigned .This is loosely based on the NAACL-00 paper \" A Simple Approach to Building Ensembles of Naive Bayesian Classifiers for Word Sense Disambiguation \" by Ted Pedersen . keywords : supervised learning , Naive Bayesian classifier , ensemble .","label":"Background","metadata":{},"score":"40.212646"}{"text":"With the aid of the matrix we have enriched sense descriptions .We have added the information of the five first hyponyms where possible and also we have used the matrix to filter the context of the words to be disambiguated .","label":"Background","metadata":{},"score":"40.48132"}{"text":"These assignments are then used as seeds for a bootstrapping algorithm ( a la Yarowsky , 1995 ) which disambiguates the whole corpus .The result for the lexicographer is a number of \" Sense - Sketches \" , showing significant patterns for the individual senses of the word , while for automatic WSD we have a decision list of clues for sense disambiguation , consisting of grammatical relation patterns , words - in - context , and n - grams .","label":"Background","metadata":{},"score":"40.51274"}{"text":"The examples from WordNet 1.7 , in which the words from the synsets they correspond to are disambiguated .2 ) SemCor - which was translated such that it points to senses from WordNet 1.7 .3 )A large additional set of sense tagged word - word pairs , generated from the pairs created at step 1 and 2 , based on a set of heuristics .","label":"Background","metadata":{},"score":"40.76545"}{"text":"Then , the underspecified classes are automatically induced by partitioning the ambiguous senses according to the nodes covered in the graph .We discuss the advantages and difficulties of our method by comparing our results and those in CORELEX ( Buitelaar , 1997 , 1998 ) .","label":"Background","metadata":{},"score":"40.821014"}{"text":"Description : This system takes a supervised learning approach to word sense disambiguation , where three Naive Bayesian classifiers are induced from sense - tagged training examples .A weighted vote is taken among these to assign senses to test examples .","label":"Background","metadata":{},"score":"40.874924"}{"text":"In most recent work , the set has been taken from a general - purpose lexical resource , with the assumption that the lexical resource describes the word senses of English / French/ ... , between which NLP applications will need to disambiguate .","label":"Background","metadata":{},"score":"40.918747"}{"text":"The discourse cues for identifying major subtopic shifts are patterns of lexical co - occurrence and distribution .The algorithm is fully implemented and is shown to produce segmentation t ... \" .TextTiling is a technique for subdividing texts into multi - paragraph units that represent passages , or subtopics .","label":"Background","metadata":{},"score":"41.25774"}{"text":"The features used in decision lists are content words and part - of - speech tags in a window .In Iwanami Kokugo Jiten ( a Japanese dictionary ) , sense inventory of this task , word sense descriptions contain some example sentences .","label":"Background","metadata":{},"score":"41.535423"}{"text":"This paper describes a set of experiments carried out to explore the domain dependence of alternative supervised Word Sense Disambignation algorithms .The aim of the work is threefold : studying the performance of these algorithms when tested on a different corpus from that they were trained on ; expl ... \" .","label":"Background","metadata":{},"score":"41.63543"}{"text":"word 's surrounding context .The attributes of the classifier are the log counts of different fillers occurring in the context patterns .We apply add - one smoothing to all counts .Every classifier also has bias features ( for every class ) .","label":"Background","metadata":{},"score":"41.65172"}{"text":"Another , more practical , experiment attempts sense disambiguation of all open class words in a text assigning LDOCE homographs as sense tags using only part - of - speech information .We report that 92 % of open class words can be successfully tagged in this way .","label":"Background","metadata":{},"score":"41.67614"}{"text":"These relationships were approximated using heuristic patterns over base noun phrase bracketed sentences ( Florian and Ngai , 2001 ) .Additional features included parts - of - speech and lemmas in all syntactic positions , extracted using a Brill - style POS tagger and morphological analysis based on Yarowsky and Wicentowski , 2000 ) .","label":"Background","metadata":{},"score":"41.77059"}{"text":"Of the 2,369 sentences containing the sense - tagged usages of interest , 600 were randomly selected and set aside to serve as the test set .The distribution of sense tags in the dat ... . \" ...The automatic disambiguation of word senses has been an interest and concern since the earliest days of computer treatment of language in the 1950 's .","label":"Background","metadata":{},"score":"41.797676"}{"text":"From an unannotated Italian web - mined corpus ( approx 6 M words ) , we selected all the sentences containing these words ( the words v ) .We considered the contexts containing the word as being representative for the corresponding sense with the appropriate weight , and we used a Bayes similarity - based model to run an adaptive clustering model , in a k - means fashion .","label":"Background","metadata":{},"score":"41.840794"}{"text":"There were a collection of first level word sense classifiers , mainly using Naive Bayes methods , but also including vector space , n - gram , and KNN classifiers , and implementing a range of windowing , distance weighting , and smoothing techniques .","label":"Background","metadata":{},"score":"42.20336"}{"text":"In order to cope with the lack of training corpus , we have introduced rough semantic features as Wordnet coarse Semantic Classes ( SC ) in the question set .This multi - level view of the context improves dramatically the coverage of SCT on various productions .","label":"Background","metadata":{},"score":"42.22116"}{"text":"Senses are interpreted as groups ( or clusters ) of similar contexts of the ambiguous word .Words , contexts , and senses are represented in Word Space , a high - dimensional , real - valued space in which closeness corresponds to semantic similarity .","label":"Background","metadata":{},"score":"42.404842"}{"text":"With system design facilitating analysis of the contribution of different types of information , further implementation ( using Senseval-1 data ) will allow some useful assessments of the importance of various lexical information .keywords : dictionary definitions , sense inventory mapping , context assessment , multiword units .","label":"Background","metadata":{},"score":"42.56344"}{"text":"The relevance score between a feature and a sense is proportion to the conditional probability of the feature given the sense .The equations that we used are in the following web site .keywords : Classification Information model , local context , topical context , bigram context .","label":"Background","metadata":{},"score":"42.58407"}{"text":"In this paper we present an approach to person name disambiguation that clusters documents on the basis of textual features using cosine similarity and a machinely learned meta similarity measure .The approach achieves an F - measure of B - Cubed Precision and Recall of 0.74 1 on the Clustering Subtask for WePS-2 .","label":"Background","metadata":{},"score":"42.961422"}{"text":"The information used to represent examples is : . -Local information : using part - of - speech and lemmas of words placed in a 7-word window around the target word .- Topic information : treating open - class words as a bag of words .","label":"Background","metadata":{},"score":"43.059227"}{"text":"This is loosely based on the NAACL-00 paper \" A Simple Approach to Building Ensembles of Naive Bayesian Classifiers for Word Sense Disambiguation \" by Ted Pedersen .This is the same approach as taken in duluth1 for English .The only difference is in the stop list .","label":"Background","metadata":{},"score":"43.12284"}{"text":"Similar examples are defined as sentences which share the head word and several words around it .Given a sentence , our system outputs the head word in the closest cluster to the given sentence .The closest cluster is selected based on machine learning systems such as SVM .","label":"Background","metadata":{},"score":"43.162277"}{"text":"We describe experiments that show that the concepts of rhetorical analysis and nuclearity can be used effectively for determining the most important units in a text .We show how these concepts can be implemented and we discuss results that we obtained with a discourse - based summarization program . 1 Motivation The evaluation of automatic summarizers has always been a thorny problem : most papers on summarization describe the approach that they use and give some \" convincing \" samples of the output .","label":"Background","metadata":{},"score":"43.303062"}{"text":"This energy is minimized using simulated annealing .( See Preiss 2001 for a more detailed explanation . )This method is combined with an anaphora resolution algorithm ( Kennedy and Boguraev 1996 ) .Resolving pronouns and replacing them in the text with their antecedents leads to the repetition of some words that are likely to be a ' ' topic stamp ' ' ( Boguraev et al .","label":"Background","metadata":{},"score":"43.349316"}{"text":"The features for a verb w are : .The system computes the probablity of each sense for a test instance based on the maximum entropy model , filters out senses using the satellites , and outputs the senses that have probability within a factor of .80 of the highest probablity sense .","label":"Background","metadata":{},"score":"43.363785"}{"text":"He finds that a large proportion of the words he investigates is used with several senses in one context .... . \" ...Word sense disambiguation ( WSD ) is a computational linguistics task likely to benefit from the tradition of combining different knowledge sources in artificial in telligence research .","label":"Background","metadata":{},"score":"43.62232"}{"text":"Most probabilistic classifiers used for word - sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features .In this paper , a different approach to formulating a probabili ... \" .","label":"Background","metadata":{},"score":"43.708157"}{"text":"In this paper we take an in - depth study of the performance of decision lists on two publicly available corpora and an additional corpus automatically acquired from the Web , using the fine - grained highly polysemous senses in WordNet .","label":"Background","metadata":{},"score":"43.759163"}{"text":"Because no sense - tagged training data was used , it was not possible to know the lexical priors or even majority sense for each word .Thus baseline performance is significantly less than for other lexical - sample tasks .keywords : unsupervised word sense disambiguation , bayes similarity , adaptive clustering , unsupervised learning .","label":"Background","metadata":{},"score":"43.76499"}{"text":"The lexicon provided has been created specifically for the task and it consists of a definition for each sense linked to the Spanish version of EuroWordNet and , thus , to the English WordNet 1.5 , the syntactic category and , sometimes , examples and synonyms are also provided .","label":"Background","metadata":{},"score":"43.779198"}{"text":"For the few words which are still ambiguous at this point , we assign the most frequent sense from WordNet .( An initial , simpler version of this algorithm is described in \" An Iterative Approach to Word Sense Disambiguation \" , Mihalcea & Moldovan , in Proceedings of Flairs-2000 ) .","label":"Background","metadata":{},"score":"43.781853"}{"text":"Description : This system takes a supervised learning approach to word sense disambiguation , where three different classifiers are induced from sense - tagged training examples .Each classifier is based on the same feature set .A weighted vote is taken among these classifiers to assign senses to test examples .","label":"Background","metadata":{},"score":"43.838554"}{"text":"The selectional preferences are acquired for subject and direct object slots .For each slot , the verb and argument head training data is obtained from grammatical relations automatically extracted from parses of the BNC .These are then used to populate the verb and noun WordNet hypernym hierarchy with frequencies .","label":"Background","metadata":{},"score":"43.991356"}{"text":"A second memory - based learner is fed with a co - occurence vector consisting of possible desambiguating keywords above a predefined threshold of one sentence to the right and one to the left of the focus .This vector further contains the sense words available in the WordNet definitions .","label":"Background","metadata":{},"score":"44.032055"}{"text":"In this case , the sense tag selected would be the sense tag ( of those tags which could apply to \" art \" ) most frequently assigned to the other words with the same part of speech in the same section of text .","label":"Background","metadata":{},"score":"44.187386"}{"text":"..Of the 2,369 sentences containing the sense - tagged usages of interest , 600 were randomly selected and set aside to serve as the test set .The distribution of sense tags in the data set ... . \" ...The automatic disambiguation of word senses has been an interest and concern since the earliest days of computer treatment of language in the 1950 's .","label":"Background","metadata":{},"score":"44.309814"}{"text":"A weighted vote is taken among these to assign senses to test examples .No information from WordNet is utilized by this system .Each Naive Bayesian classifier is based on a different set of features that are identified in a filtering step prior to learning .","label":"Background","metadata":{},"score":"44.35167"}{"text":"The senses are hierarchically organized , and the definition , synonyms and examples are provided for each sense , among other lexicographical data .Due to the complex structure of the dictionary , a flat list of word senses and multiword terms is also provided .","label":"Background","metadata":{},"score":"44.421795"}{"text":"With the aid of the matrix we have enriched sense descriptions .We have added the information of the five first hyponyms where possible .We have added the training information to the definitions .Also we have used the matrix to filter the context of the words to be disambiguated .","label":"Background","metadata":{},"score":"44.568478"}{"text":"The third feature set is based on bigrams that may include one intervening word that is ignored and that meet the following criteria : .A Naive Bayesian classifier is learned based on each feature set .When presented with a test example , each classifier assigns a probability to each possible sense .","label":"Background","metadata":{},"score":"44.61515"}{"text":"We have detected multiword terms as present in WordNet .According to the file cntlist we have established a filter , discarding in the first heuristic the senses that have not appeared more than 10 % in the wordnet files .We have made a relevance matrix between words with the data from 3200 books in English from the Gutenberg Project ( Click here ) .","label":"Background","metadata":{},"score":"44.943764"}{"text":"However , few existing studies have studied the practical application of this approach .In this paper we propose two improvements to the use of synonym substitution for encoding hidden bits of information .First , we use the Web 1 T Google n - gram corpus for checking the applicability of a synonym in context , and we evaluate this method using data from the SemEval lexical substitution task .","label":"Background","metadata":{},"score":"44.960045"}{"text":"Page 5 .RFA .In Section 4.1 , extraction and training of contextual features for the WSD of homographic classifiers are shown with sample sentences in the RFA - tagged corpus , and in Section 4.2 , genera- lized re - categorization of contextual features is performed based on the lexical hierarchy in KorLex Noun 1.0 .","label":"Background","metadata":{},"score":"45.174084"}{"text":"In disambiguation , these features are combined according to a voting scheme .In the learning phase we consider features that have a total frequency above a set threshold ( 3 ) , and calculate for each of these : . 1 ) the relative frequency of each sense for the feature , 2 ) a probability measure , using a Student - t distribution to test the hypothesis , that the observed distribution for a feature is significantly different from the overall distribution of senses in the training data .","label":"Background","metadata":{},"score":"45.338306"}{"text":"This paper describes a corpus - based method for word sense disambiguation which usesaversatile maximum entropy technique on simple local lexical features and a rich description of the syntactic context of a word to distinguish between its various senses .Given training sentences which useaword in a particular sense , a maximum entropy toolis used to build a model for e cient word sense disambiguation .","label":"Background","metadata":{},"score":"45.363754"}{"text":"These are conditioned on verb classes , so only the noun data which has occurred in the relationship specified by the slot with a member of the verb class is used .For selectional preference acquisition , the members of a verb class include direct members and hyponyms which have 10 senses or less and have occurred in the BNC with a frequency of 20 or more .","label":"Background","metadata":{},"score":"45.4144"}{"text":"These are conditioned on verb classes , so only the noun data which has occurred in the relationship specified by the slot with a member of the verb class is used .For selectional preference acquisition , the members of a verb class include direct members and hyponyms which have 10 senses or less and have occurred in the BNC with a frequency of 20 or more .","label":"Background","metadata":{},"score":"45.4144"}{"text":"These are conditioned on verb classes , so only the noun data which has occurred in the relationship specified by the slot with a member of the verb class is used .For selectional preference acquisition , the members of a verb class include direct members and hyponyms which have 10 senses or less and have occurred in the BNC with a frequency of 20 or more .","label":"Background","metadata":{},"score":"45.4144"}{"text":"The aim of the work is threefold : firstly , studying the performance of these algorithms when tested on a different corpus from that they were trained o ... \" .This report describes a set of experiments carried out to explore the portability of alternative supervised Word Sense Disambiguation algorithms .","label":"Background","metadata":{},"score":"45.464096"}{"text":"Conventional selectional preference learning methods have usually focused on word - to - class relations , e.g. , a verb selects as its subject a given nominal class .This paper extends previous work to wordto - word selectional preferences by using webscale data .","label":"Background","metadata":{},"score":"45.55649"}{"text":"This is the main step of the algorithm and it disambiguates all ambiguous instances which have not been previously disambiguated .We use an instance based learning algorithm and a large pool of features that are actively selected .The learner is trained on the training data provided and then applied on the test instances .","label":"Background","metadata":{},"score":"45.630775"}{"text":"The model is adapted to an online left to right chart - parser for word lattices , integrating acoustic , n - gram , and parser probabilities .The parser uses structural and lexical dependencies not considered by ngram models , conditioning recognition on more linguistically - grounded relationships .","label":"Background","metadata":{},"score":"45.693455"}{"text":"The selectional preferences are acquired for subject and direct object slots .For each slot , the verb and argument head training data is obtained from grammatical relations automatically extracted from parses of the BNC produced by a shallow parser .These are then used to populate the verb and noun WordNet hypernym hierarchy with frequencies .","label":"Background","metadata":{},"score":"45.799927"}{"text":"The selectional preferences are acquired for subject and direct object slots .For each slot , the verb and argument head training data is obtained from grammatical relations automatically extracted from parses of the BNC produced by a shallow parser .These are then used to populate the verb and noun WordNet hypernym hierarchy with frequencies .","label":"Background","metadata":{},"score":"45.799927"}{"text":"R01 - 2007 - 000 - 20517 - 0 ) 8 .REFERENCES [ 1 ] Agirre , E. and Rigau , G. 1996 .Word sense disambiguation using conceptual density ( Paper presented at the COL- ING1996 ) [ 2 ] Allan , K. 1997 .","label":"Background","metadata":{},"score":"45.85832"}{"text":"In an exact sense , RFA is not a sense tag of homographic clas- sifiers .However , as we can see in Table 2 , not only does RFA differ from the context but it also becomes a good indicator of word senses .","label":"Background","metadata":{},"score":"45.889225"}{"text":"Such name ambiguity affects the performance of document retrieval , web search , database integration , and may cause improper attribution to authors .This paper investigates two supervised learning approaches to disambiguate authors in the citations 1 .One approach uses the naive Bayes probability model , a generative model ; the other uses Support Vector Machines(SVMs ) [ 39 ] and the vector space representation of citations , a discriminative model .","label":"Background","metadata":{},"score":"45.995003"}{"text":"We develop a novel method in which words are the vertices in a graph , synonyms are linked by edges , and the bits assigned to a word are determined by a vertex colouring algorithm .This method ensures that each word encodes a unique sequence of bits , without cutting out large number of synonyms , and thus maintaining a reasonable embedding capacity .","label":"Background","metadata":{},"score":"46.1521"}{"text":"We describe a method for formulating probabilistic models that use multiple contextual features for word - sense disambiguafion , without requiring untested assumptions regarding the form of the model .Using this approach , the joint distribution of all variables is described by only the most systematic variable interactions , thereby limiting the number of parameters to be estimated , supporting computational efficiency , and providing an understanding of the data . .","label":"Background","metadata":{},"score":"46.152184"}{"text":"In ( E 1-a , b , d , e ) , classifiers following Arabic numerals play an important role in determining the reading of those numerals .How- ever , when a homographic classifier follows an Arabic numeral , multiple meanings and readings of an Arabic numeral are accepta- ble , as shown in ( E 1-f ) .","label":"Background","metadata":{},"score":"46.199013"}{"text":"Yarowsky [ 25 ] suggested WSD by adopting approximate conceptual classes from Roget 's thesaurus for resolving the ambi- guities of twelve polysemous words .The method achieved 92 % accuracy ; however , it had a limitation in minor sense distinction within a category .","label":"Background","metadata":{},"score":"46.2172"}{"text":"Based on the SEMCOR corpus , semantic word experts are trained for the multi - sense words .The word experts combine different types of learning algorithms , viz .memory - based learning ( TiMBL ) and rule induction ( Ripper ) , which take as input different knowledge sources : . - The input for the memory - based learner is a feature vector consisting of the target word and lemma and three words to its right and left , along with a more fine - grained part - of - speech .","label":"Background","metadata":{},"score":"46.519894"}{"text":"a ) for each feature , keep t - values that are above the 95 % signifcance level , and single out the sense , that has the highest t - value , and give the vote to this sense .b ) order the senses according to vote , .","label":"Background","metadata":{},"score":"46.643364"}{"text":"Based on the synonym and near - syn- onym relations in the dictionary , the contextual features were ex- panded .Disappointingly , however , the accuracy dropped to 81.8 % , because there is no distinctive information for classifying each sense of homographs or polysemic words .","label":"Background","metadata":{},"score":"46.705956"}{"text":"Specifically , we exploit short - distance cues to hypernymy , semantic compatibility , and semantic context , as well as general lexical co - occurrence ... \" .To address semantic ambiguities in coreference resolution , we use Web n - gram features that capture a range of world knowledge in a diffuse but robust way .","label":"Background","metadata":{},"score":"46.708363"}{"text":"The resulting multilingual sense inventory then served as the basis for the annotation of the test data .The ParaSense WSD system we propose in this thesis presents a truly multilingual classification - based approach to WSD that directly incorporates evidence from four other languages .","label":"Background","metadata":{},"score":"46.714706"}{"text":"The resulting multilingual sense inventory then served as the basis for the annotation of the test data .The ParaSense WSD system we propose in this thesis presents a truly multilingual classification - based approach to WSD that directly incorporates evidence from four other languages .","label":"Background","metadata":{},"score":"46.714706"}{"text":"1.3 Our Approach The approach we take to detecting people in static images borrows ideas from the elds of object detection in images and data classi cation .In particular , the system attempts to de ... . \" ...Tree induction is one of the most effective and widely used methods for building classification models .","label":"Background","metadata":{},"score":"46.76535"}{"text":"( H-1 ) is based on the idea that lexical relations between the word sense of homographic words and their context can reduce or remove the ambiguities .In inheritance systems , a hyponym inherits all of the features of the more generic concept and adds at least one feature that distin- guishes it from its superordinate and from any other hyponyms of that superordinate [ 24].","label":"Background","metadata":{},"score":"46.888"}{"text":"The Naive Bayesian classifier is based on a set of features that consists of unigrams ( one word sequences ) that are identified in a filtering set prior to learning .These features must meet the following criteria : . 1 ) occur 5 or more times and 2 ) are not found on the stop - list .","label":"Background","metadata":{},"score":"47.00447"}{"text":"Semantic domain information : using a semantic hierarchy linked to WordNet 1.6 .The domain weigths depends on the words of the context , the number of senses of these words and their distribution on the context .Multiwords have been preprocessed separately in a previous task .","label":"Background","metadata":{},"score":"47.02952"}{"text":"keywords : context vector , cosine similarity , semantic feature , syntactic relation .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"47.264675"}{"text":"They characterize the types of Arabic numeral clusters in texts [ 10].For example , if \" 97 - 06 - 04 \" , and \" 02 - 513 - 4463 \" are input , they are converted into a pattern , \" N - N - N \" .","label":"Background","metadata":{},"score":"47.32497"}{"text":"It can be broken up into two distinctive phases .The first phase consists in identifying the coarse Semantic Classes ( SC ) related to each word in the test .For this purpose , we have applied the Viterbi algorithm , using a trisem model , in order to select the most likely path in the SC graph built for each sentence .","label":"Background","metadata":{},"score":"47.335014"}{"text":"Combining a naive Bayes classifier with the EM algorithm is one of the promising approaches for making use of unlabeled data for disambiguation tasks when using local context features including word sense disambiguation and spelling correction .However , the use of unlabeled data via the basic ... \" .","label":"Background","metadata":{},"score":"47.337597"}{"text":"We illustrate these two approaches on two types of data , one collected from the web , mainly publication lists from homepages , the other collected from the DBLP citation databases . ...author 's research area and his or her individual patterns of coauthoring .","label":"Background","metadata":{},"score":"47.366028"}{"text":"We introduce the reader to the motivations for solving the ambiguity of words and provide a description of the task .We overview supervised , unsupervised , and knowledge - based approaches .The assessment of WSD systems is discussed in the context of the Senseval / Semeval campaigns , aiming at the objective evaluation of systems participating in several different disambiguation tasks .","label":"Background","metadata":{},"score":"47.39502"}{"text":"They can also provide insight into important issues in human language acquisition .However , within AI , computational linguistics , and machine learning , there has been relatively little research on developing systems that learn such semantic parsers . ...","label":"Background","metadata":{},"score":"47.507412"}{"text":"A distance function based on the WordNet noun hierarchy forms the core of the WSD algorithm .We assign a weight to every node in WordNet ; this is proportional to the number of descendants of the node .By combining appropriate weights , we can derive the ' distance ' between any two noun senses .","label":"Background","metadata":{},"score":"47.55699"}{"text":"Description ( 250 words max ) : Our Word Sense Disambiguation ( WSD ) System was entered for the English all words task in Senseval 2001 .The system makes use of the WordNet 1.7 hierarchy and an anaphora resolution algorithm to assign precisely one sense to each noun from the text .","label":"Background","metadata":{},"score":"47.574642"}{"text":"Research on learning for information extraction a .. by Raymond J. Mooney - IN PROCEEDINGS OF LEARNING LANGUAGE IN LOGIC , LLL99 , 1999 . \" ...Most recent research in learning approaches to natural language have studied fairly \" low - level \" tasks such as morphology , part - ofspeech tagging , and syntactic parsing .","label":"Background","metadata":{},"score":"47.580124"}{"text":"The basic design is similar to CL Research 's system for Senseval-1 ; however , many of the disambiguation routines were not able to be reimplemented by the submission date .For Senseval-2 , the implemented routines included special routines for examining multiword units and examining contextual clues ( both specific word , Lesk - style use of definition content words , and subject matter analyses ) ; syntactic constraints have not yet been employed .","label":"Background","metadata":{},"score":"47.7638"}{"text":"In this paper , a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguafion of the noun interest .We describe a method for formulating probabilistic models that use multiple contextual features for word - sense disambiguafion , without requiring untested assumptions regarding the form of the model .","label":"Background","metadata":{},"score":"47.77385"}{"text":"The three systems are different in the number of primitives used in the MTD as well as in the semantic distance matrix .They could either be a little over 4,000 or less than 500 .keywords : descriptive - semantic - primitives ; machine - tractable - dictionary ; sense - tagging ; semantic - disambiguation .","label":"Background","metadata":{},"score":"47.793095"}{"text":".. ographic \" ) clusters of usages .All this has led some to despair and call it all sense - distinction , some going even so far as to say that words have more or less one sense each ( e.g. [ 12 ] ) .","label":"Background","metadata":{},"score":"47.868248"}{"text":"We train separate classifiers with monolingual and bilingual features and iteratively improve them via co - training .The co - trained classifier achieves close to 96 % accuracy on Treebank data and makes 20 % fewer errors than a supervised system trained with Treebank annotations .","label":"Background","metadata":{},"score":"47.995148"}{"text":"We can also provide the complete mapping between WordNet 1.5 and 1.6 versions ( see this site ) .Description : The lexical sample task in SENSEVAL-2 for Swedish consisted of 40 lemmas ; ( 145 senses , 304 sub - senses ) .","label":"Background","metadata":{},"score":"48.197205"}{"text":"The choice of combination method and the parameters of weighted voting and the features and weights of the loglinear model were chosen by crossvalidation on the training data .The first level and combined classifiers simply reported a single most likely sense choice for each test word .","label":"Background","metadata":{},"score":"48.22355"}{"text":"keywords : Trisem Model , Semantic Classes , Semantic Classification Tree . organisation : University of Maryland , College Park , Linguistics Department & UMIACS , USA .Task / s : English all words .Did you use any training data provided in an automatic training procedure ?","label":"Background","metadata":{},"score":"48.39248"}{"text":"We conclude that word sense ambiguity is only problematic to an IR system when it is retrieving from very short queries .In addition we argue that if a word sense disambiguator is to be of any use to an IR system , the disambiguator must be abl ... . ...","label":"Background","metadata":{},"score":"48.40021"}{"text":"When presented with a test example , each classifier outputs a probability for each possible sense .These are summed and the sense with the maximum probability is assigned to a test example .This is the same approach as taken in duluthA for English .","label":"Background","metadata":{},"score":"48.59436"}{"text":"The belief is that if ambiguous words can be correctly disambiguated , IR performance will increase .However , recent research into the application of a word sense disambiguator to an IR system failed to show any performance increase .From these results it has become clear that more basic research is needed to investigate the relationship between sense ambiguity , disambiguation , and IR .","label":"Background","metadata":{},"score":"48.684967"}{"text":"It requires a parallel corpus that is sentence aligned and a sense inventory for the language that needs to be sense tagged .It produces both corpora tagged with sense IDs from the sense inventory .Basically , the system assumes that the parallel corpus is token aligned .","label":"Background","metadata":{},"score":"48.699722"}{"text":"According to the file cntlist we have established a filter , discarding in the first heuristic the senses that have not appeared more than 10 % in the wordnet files .We have made a relevance matrix between words with the data from 3200 books in English from the Gutenberg Project ( Click here ) .","label":"Background","metadata":{},"score":"48.716972"}{"text":"According to the file cntlist we have established a filter , discarding in the first heuristic the senses that have not appeared more than 10 % in the wordnet files .We have made a relevance matrix between words with the data from 3200 books in English from the Gutenberg Project ( Click here ) .","label":"Background","metadata":{},"score":"48.716972"}{"text":"According to the file cntlist we have established a filter , discarding in the first heuristic the senses that have not appeared more than 10 % in the wordnet files .We have made a relevance matrix between words with the data from 3200 books in English from the Gutenberg Project ( Click here ) .","label":"Background","metadata":{},"score":"48.716972"}{"text":"The textual approach is inefficient , because , for each word , the ... . \" ...This squib claims that Large - scale Automatic Sense Tagging of text ( LAST ) can be done at a high - level of accuracy and with far less complexity and computational effort than has been believed until now .","label":"Background","metadata":{},"score":"48.74104"}{"text":"This squib claims that Large - scale Automatic Sense Tagging of text ( LAST ) can be done at a high - level of accuracy and with far less complexity and computational effort than has been believed until now .Moreover , it can be done for all open class words , and not just carefully selected opposed pairs as in some recent work .","label":"Background","metadata":{},"score":"48.759987"}{"text":"..he first preposition found before the ambiguous word Table 1 : The complete list of features used in the experiments of this approach to the fact that it did not use feature weighting .After they introduced feature weighting ... . by Gerard Escudero , Lluis Marquez , German Rigau , Jordi Girona Salgado , 2000 . \" ...","label":"Background","metadata":{},"score":"48.768394"}{"text":"The sense with the highest probability is assigned to the test example .This system implements a standard benchmark , the Naive Bayesian classifier based on a bag of words feature set .This is the same approach as taken in duluth4 for English .","label":"Background","metadata":{},"score":"48.825626"}{"text":"French , Dutch , Italian , Spanish and German ) as classification output .The feature vectors incorporate both local context features as well as translation features that are extracted from the aligned translations .The hypothesis underlying the construction of a multilingual WSD system is that adding translational evidence from multiple languages will be more informative than using only monolingual or bilingual information .","label":"Background","metadata":{},"score":"49.19458"}{"text":"French , Dutch , Italian , Spanish and German ) as classification output .The feature vectors incorporate both local context features as well as translation features that are extracted from the aligned translations .The hypothesis underlying the construction of a multilingual WSD system is that adding translational evidence from multiple languages will be more informative than using only monolingual or bilingual information .","label":"Background","metadata":{},"score":"49.19458"}{"text":"Results show that this sense disambiguation algorithm improves performance by between 7o and 1o on aver - age . ...l , two assumptions have been made : ffl Only one sense of a word is used in each occurrence .","label":"Background","metadata":{},"score":"49.21759"}{"text":"For evaluation , we distribute test documents with marked target words .Participants is required to assign one or more sense IDs to each target word , optionally with associated probabilities .Test documents will take the form of newspaper articles annotated with UDC codes .","label":"Background","metadata":{},"score":"49.266945"}{"text":"Each Naive Bayesian classifier is based on a different set of features that are identified in a filtering step prior to learning .The first feature set is based on bigrams ( two word sequences ) that meet the following criteria : .","label":"Background","metadata":{},"score":"49.413185"}{"text":"( H-1 ) semantic ambiguities caused by homographs or polysemic words can be reduced or removed by mapping the words to the hierarchy properly in KorLex ; ( H-2 ) hyponyms inherit semantic characteristics from their hypernyms in KorLex .According to Lyons [ 15 ] , hyponymy is transitive and asymme- trical , and , since there is normally a single superordinate , it gene- rates a hierarchical semantic structure , in which a hyponym is said to be below its superordinate .","label":"Background","metadata":{},"score":"49.583805"}{"text":"The algorithm uses domain - independent lexical frequency and distribution information to recognize the interactions of multi ... \" .This paper describes TextTiling , an algorithm for partitioning expository texts into coherent multi - paragraph discourse units which reflect the subtopic structure of the texts .","label":"Background","metadata":{},"score":"49.622055"}{"text":"We add one to all counts for ... . \" ...We present a novel beam - search decoder for grammatical error correction .The decoder iteratively generates new hypothesis corrections from current hypotheses and scores them based on features of grammatical correctness and fluency .","label":"Background","metadata":{},"score":"49.63482"}{"text":"keywords : word experts , word sense disambiguation as deduction , supervised learning , transformation - based learning .Did you use any training data provided in an automatic training procedure ?N .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"49.67187"}{"text":"The key problem is how to encode the multiple word senses which fluctuate depending on the particular context in which the word is used .For example , ... . ...d senses .2 Note that systematic polysemy should be contrasted with homonymy which refers to words which have more 1 in systematic and predictable ways .","label":"Background","metadata":{},"score":"49.69053"}{"text":"Classification information model(CIM ) CIM disambiguates word sense considering the discrimination score(DS ) of features .The DS of a feature is the sum of relevance scores between the feature and each sense .The relevance score between a feature and a sense is proportion to the conditional probability of the feature given the sense .","label":"Background","metadata":{},"score":"49.71823"}{"text":"No information from WordNet is utilized by this system .This system uses a filter to perform feature identification prior to learning .All bigrams ( two word sequences ) that meet the following criteria form a set of candidate features : .","label":"Background","metadata":{},"score":"49.743195"}{"text":"Description : For a target word , accumulate all WordNet 1.7 examples ( stuff in quotes ) that are related to the word 's plausible synsets and any of their related synsets .( Related synsets include all ancestors for parent relations , immediate children for child relations , and transitive closure within a relation for all other relations . )","label":"Background","metadata":{},"score":"49.865536"}{"text":"In this paper I describe a pilot e ... \" .There are now many computer programs for automatically determining which sense a word is being used in .One would like to be able to say which were better , which worse , and also which words , or varieties of language , presented particular problems to which programs .","label":"Background","metadata":{},"score":"49.878357"}{"text":"ABSTRACT The high frequency of the use of Arabic numerals in informative texts and their multiple senses and readings deteriorate the accu- racy of TTS systems .This paper presents a hybrid word sense disambiguation method exploiting a tagged corpus and a Korean wordnet , KorLex 1.0 , for the correct and efficient conversion of Arabic numerals into Korean phonemes according to their senses .","label":"Background","metadata":{},"score":"50.240494"}{"text":"The proposed WSD model based on local contextual features showed 85.2 % accuracy , whereas the baseline showed 73.2 % accuracy .Therefore , in Section 4.2 , generalized and dynamic categoriza- tion of contextual features based on KorLex Noun 1.0 , in which lexical information has been structured hierarchically , will be described . 4.2 Generalization of semantic categories of contextual features In this section , resolutions of the two problems underlying WSD using a tagged corpus are suggested , based on the lexical hierarchy and semantic relations contained in KorLex Noun 1.0 .","label":"Background","metadata":{},"score":"50.305237"}{"text":"However , most of these models are built with only local context and one representation per word .This is problematic because words are often polysemous and ... \" .Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems .","label":"Background","metadata":{},"score":"50.31893"}{"text":"Each classifier is based on the same feature set .A weighted vote is taken among these classifiers to assign senses to test examples .No information from WordNet is utilized by this system .The three classifiers are a bagged J48 decision tree , a Naive Bayesian classifier , and a nearest neighbor classifier ( IBk ) .","label":"Background","metadata":{},"score":"50.38878"}{"text":"The training examples are converted into feature vectors , where each feature represents whether or not a unigram occurs in the context of a specific training example .These features vectors are used to make the estimates of the parameters of the Naive Bayesian classifier .","label":"Background","metadata":{},"score":"50.573776"}{"text":"Step 2 : Mapping words by cluster to the KorLex hierarchy .Step 3 : Listing all common hypernyms of synset nodes mapped from contextual features .Step 4 : Finding the least upper bound of synset nodes in a cluster mapped from contextual features .","label":"Background","metadata":{},"score":"50.63354"}{"text":"The decision tree learner performs its own feature selection based on the gain ratio , which measures how well a feature partitions the training examples into senses .This is the same approach as taken in duluth3 for English .The only difference is in the stop list .","label":"Background","metadata":{},"score":"50.663635"}{"text":"For each word in the test data , a \" domain vector \" is built considering a text window around the target word .Then , the resulting domain vector is compared with the domain vectors previously acquired for each word sense from the training data , and the most similar one is selected .","label":"Background","metadata":{},"score":"50.705105"}{"text":"The algorithm is automatic and unsupervised in both training and application : senses are induced from a corpus without labeled training insta , nces or other external knowledge sources .The paper demonstrates good performance of context - group discrimination for a sample of natural and artificial ambiguous words . by Rebecca Bruce , Janyce Wiebe - In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics , 1994 . \" ...","label":"Background","metadata":{},"score":"50.803757"}{"text":"( a ) WSD based on Sense - tagged corpora : Since sense tags vary depending on applications , available corpora can be different .For speech synthesis , the correct sense and the pronunciation of a target ambiguous word in its context needs to be tagged .","label":"Background","metadata":{},"score":"50.92623"}{"text":"Previous experiments [ Loupy , 2000 ] using the Semcor have shown some good results for word SC affectation ( about 90 % ) .For this reason , we could expect to achieve results at least as good as those obtained with an approach based on unisem model .","label":"Background","metadata":{},"score":"50.929237"}{"text":"We study the linguistic phenomenon of informal words in the domain of Chi - nese microtext and present a novel method for normalizing Chinese informal words to their formal equivalents .We formal - ize the task as a classification problem and propose rule - based and statistical fea - tures to model three plausible channels that explain the connection between for - mal and informal pairs .","label":"Background","metadata":{},"score":"50.98982"}{"text":"The parameter smoothing is performed by recasting the WordNet hierarchy as Bayesian networks , in a processed called Semantic Backoff .The sentential structure is then used to construct another Bayesian network , quantitated via the parameters established earlier .WSD is performed by the Maximum A Posteriori estimation , using the Join Tree inferencing algorithm .","label":"Background","metadata":{},"score":"50.992516"}{"text":"We present the first application of the head - driven statistical parsing model of Collins ( 1999 ) as a simultaneous language model and parser for largevocabulary speech recognition .The model is adapted to an online left to right chart - parser for word lattices , integrating acoustic , n - gram , and parser ... \" .","label":"Background","metadata":{},"score":"51.167027"}{"text":"Table 2 .Homographic classifiers and RFA ( excerpted )Twenty - five homographic classifiers were analyzed .6 Homographic classifiers Pronunciation 1 In this paper , letters in italics stand for grapheme - to - phoneme conversions of Korean , and phrases in quotation marks or pa- rentheses are the interpretation of such sample phrases .","label":"Background","metadata":{},"score":"51.17293"}{"text":"The three classifiers are a bagged J48 decision tree , a Naive Bayesian classifier , and a nearest neighbor classifier ( IBk ) .This system uses a filter to perform feature identification prior to learning .All non - consecutive bigrams ( that may include zero , one , or two intervening words that are ingored ) and that meet the following criteria form a set of candidate features : .","label":"Background","metadata":{},"score":"51.201397"}{"text":"Such unigrams form a set of features .The training examples are converted into feature vectors , where each feature represents whether or not a unigram occurs in the context of a specific training example .These features vectors are used to make the estimates of the parameters of the Naive Bayesian classifier .","label":"Background","metadata":{},"score":"51.20524"}{"text":"In the third step , the target Arabic numerals are recognized , and their pattern features and arithmetic features are extracted .Pattern features are the \" number of numerals in one word \" , the \" number of text symbols combined with Arabic numerals in one word \" and others .","label":"Background","metadata":{},"score":"51.285706"}{"text":"Their corres- ponding Korean words are obtained for the translation - candidate English words in WordNet automatically .Correct Korean words are selected considering their definitions and examples as listed in a Korean dictionary by linguists [ 9].At this stage , the main prob- lem results from the several senses suggested by the first automatic mapping using the machine readable dictionary , which can not be disambiguated completely .","label":"Background","metadata":{},"score":"51.322098"}{"text":"keywords : semantic domains , domain driven disambiguation , similarity .Did you use any training data provided in an automatic training procedure ?Yes .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"51.356064"}{"text":"( 1 )S : Sample set of Arabic numerals , X : Elements , T : Training sets Cj : Class to which S belongs ( e.g. C_b[+D ] , Kca_b ) .Page 7 .Table 6 presents and compares the accuracies of the proposed hybrid WSD method and others .","label":"Background","metadata":{},"score":"51.46042"}{"text":"Tags P and U have not been used .There is no special treatment for multiword detection .keywords : Supervised learning , decision lists , agglutinative languages .Did you use any training data provided in an automatic training procedure ?","label":"Background","metadata":{},"score":"51.52607"}{"text":"No information from WordNet is utilized by this system .This system uses a filter to perform feature identification prior to learning .Two different kinds of bigrams are identified as candidate features .The first is a consecutive two word sequence that meets the following criteria : .","label":"Background","metadata":{},"score":"51.587654"}{"text":"Each of those systems outputs probabilities for each sense when presented with a test example , so all of these are summed together and the sense with the maximum probability is assigned to a test example .keywords : supervised learning , ensemble .","label":"Background","metadata":{},"score":"51.803543"}{"text":"WSD is considered an AI - complete problem , that is , a task whose solution is at least as hard as the most difficult problems in artificial intelligence .We introduce the reader to the ... \" .Word sense disambiguation ( WSD ) is the ability to identify the meaning of words in context in a computational manner .","label":"Background","metadata":{},"score":"51.84929"}{"text":"This system uses a filter to perform feature identification prior to learning .All bigrams ( two word sequences ) that meet the following criteria form a set of candidate features : .The training examples are converted into feature vectors , where each feature represents whether a candidate feature occurs in the context of a specific training example .","label":"Background","metadata":{},"score":"51.863174"}{"text":"Tools . \" ...We investigate one technique to produce a summary of an original text without requiring its full semantic interpretation , but instead relying on a model of the topic progression in the text derived from lexical chains .We present a new algorithm to compute lexical chains in a text , merging several r ... \" .","label":"Background","metadata":{},"score":"51.921642"}{"text":"Page 6 . because contextual features in natural language texts affect each other , and the decision rules can be easily derived from the result of decision tree construction , a decision tree was adopted as our classification model .In order to construct the decision tree , the information gain for each feature is calculated , and then the best feature is selected step by step .","label":"Background","metadata":{},"score":"52.09675"}{"text":"Keywords Word sense disambiguation , Arabic numeral , TTS .INTRODUCTION Mapping from texts to phones for text - to - speech transliteration relies heavily on pronunciation dictionaries or letter - to - sound rules .However , this mapping is very difficult because not all pho- netic realizations of graphemes are found in dictionaries , and the same graphemes , according to the context , might not correspond to the same phonetic symbols [ 4].","label":"Background","metadata":{},"score":"52.16744"}{"text":"keywords : Classification Information model , local context , topical context , bigram context .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"52.16833"}{"text":"Relevancies of Neighboring Words Measured by MI and  2  are 2 ( exerpted )Classifier Sense Neighboring words MI 2  .Since too many parameters lower the learning efficiency , words proceeding or following the combination of Arabic numer- als and homographic classifiers should be clustered into semantic categories .","label":"Background","metadata":{},"score":"52.19481"}{"text":"It extracts a basic feature set : . i ) local features for english : bigrams and trigrams around the target word , consisting on lemmas or word forms or parts of speech .Also a bag of lemmas constructed using the content words in a + /- 4 word window around the target . ii ) local features for Basque : being Basque an agglutinative language , part of the syntactic information is in the inflectional suffixes .","label":"Background","metadata":{},"score":"52.230217"}{"text":"( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : This system takes a supervised learning approach to word sense disambiguation , where a decision tree is induced from sense - tagged training examples .","label":"Background","metadata":{},"score":"52.370476"}{"text":"Word sense disambiguation ( WSD ) is a computational linguistics task likely to benefit from the tradition of combining different knowledge sources in artificial in telligence research .An important step in the exploration of this hypothesis is to determine which linguistic knowledge sources are most useful and whether their combination leads to improved results .","label":"Background","metadata":{},"score":"52.423115"}{"text":"The features used in the model is outputs of morphological and syntactic analysis .We used a hybrid model of support vector machine and simple Bayes for learning .Did you use any training data provided in an automatic training procedure ?","label":"Background","metadata":{},"score":"52.51358"}{"text":"Early attempts to build disambiguators [ 2 , 3 , 4 ] relied on a combination of hand built lexicons and rules .Although working well for the examples they were programmed f ..Tools . by Eric H. Huang , Richard Socher , Christopher D. Manning , Andrew Y. Ng - In Proc . of the Annual Meeting of the Association for Computational Linguistics ( ACL , 2012 . \" ...","label":"Background","metadata":{},"score":"52.558517"}{"text":"( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Y .Description : UST is an unsupervised system for word sense tagging .","label":"Background","metadata":{},"score":"52.583206"}{"text":"Senses of ' dae ' ( E 3-a ) taegsi - neun eobs - go beoseu 30dae -wa sugbag - eobso - ui seunghabcha - man unhaengdoe - n - da Sample sentences RFA 1 . \"Content words are separated from function morphemes and are lemmatized through morphological analysis .","label":"Background","metadata":{},"score":"52.67266"}{"text":"When presented with a test example , each decision tree outputs probabilities for each possible sense .These probabilities are summed and the sense with the maximum value is assigned to the test example .No information from WordNet is utilized by this system .","label":"Background","metadata":{},"score":"52.675903"}{"text":"When presented with a test example , each decision tree outputs probabilities for each possible sense .These probabilities are summed and the sense with the maximum value is assigned to the test example .No information from WordNet is utilized by this system .","label":"Background","metadata":{},"score":"52.675903"}{"text":"keywords : Bayesian networks , semantic distance and density , parameter smoothing , MAP .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"52.973015"}{"text":"Once the pattern features are obtained , the arithmetic features are extracted from their input strings .The first features of \" 97 - 06 - 04 \" and \" 02 - 513 - 4463 \" are \" 9 \" and \" 0 \" , re- spectively .","label":"Background","metadata":{},"score":"53.22505"}{"text":"( Boston : Houghton Mifflin ) [ 7 ] Gale , W. A. , Church , K. W. , and Yarowsky , D. 1992 .A method for disambiguating word senses in a large corpus .Computers and the Humanities , 26 , 415 - 439 [ 8 ] Hausser , R. 1999 .","label":"Background","metadata":{},"score":"53.377106"}{"text":"Least upper bound synsets among common hypernyms of contextual features were obtained from the KorLex hierarchy , and they were used as semantic categories of the contextual features of Arabic numerals .The semantic classes were trained to classify the meaning and the reading of Arabic numerals using decision tree and to compose grapheme - to - phoneme rules for an automatic transliteration system for Arabic numerals .","label":"Background","metadata":{},"score":"53.470627"}{"text":"Least upper bound synsets among common hypernyms of contextual features were obtained from the KorLex hierarchy , and they were used as semantic categories of the contextual features of Arabic numerals .The semantic classes were trained to classify the meaning and the reading of Arabic numerals using decision tree and to compose grapheme - to - phoneme rules for an automatic transliteration system for Arabic numerals .","label":"Background","metadata":{},"score":"53.470627"}{"text":"The algorithm follows two steps : first a domain is chosen for a word ( among those allowed by the word senses in wordnet ) ; then a sense , among those belonging to the preferred domain , is selected .The first step ( i.e. word domain disambiguation ) is based on a similarity function among semantic domains , which was trained on an English corpus .","label":"Background","metadata":{},"score":"53.505966"}{"text":"Page 3 .Table 1 .Reading Formulae of Arabic Numerals WordNet have been applied in [ 1].In that work , the accuracy of WSD using twenty four coarse - grained semantic categories was higher than that of WSD using individual synsets .","label":"Background","metadata":{},"score":"53.53601"}{"text":"keywords : selectional preferences , grammatical relations , one sense per discourse , anaphor resolution .organisation : Dept . of Computer Science , University of California Los Angeles .Task / s : English all words .Did you use any training data provided in an automatic training procedure ?","label":"Background","metadata":{},"score":"53.550377"}{"text":"The experimental results confirm the validity of our approach : the classifiers that employ translational evidence constantly outperform the classifiers that only exploit local context information for four out of five target languages , viz .French , Spanish , German and Dutch .","label":"Background","metadata":{},"score":"53.570637"}{"text":"The experimental results confirm the validity of our approach : the classifiers that employ translational evidence constantly outperform the classifiers that only exploit local context information for four out of five target languages , viz .French , Spanish , German and Dutch .","label":"Background","metadata":{},"score":"53.570637"}{"text":"In the first step , the input sentences are segmented into words by white space .In Korean , one word is composed of content words and function morphemes such as case makers , postpositions , or Workshop on Knowledge endings .","label":"Background","metadata":{},"score":"53.650436"}{"text":"Description : This system takes a supervised learning approach to word sense disambiguation , where the results of the systems duluth1 , duluth2 , duluth3 , duluth4 , duluth5 , duluthA , and duluthB are combined into an ensemble .Each of those systems outputs probabilities for each sense when presented with a test example , so all of these are summed together and the sense with the maximum probability is assigned to a test example .","label":"Background","metadata":{},"score":"53.71853"}{"text":".. in all cases dealing with texts from particular domains .6Following ... . by Daniel Marcu - In Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization , 1997 . \" ...We describe experiments that show that the concepts of rhetorical analysis and nuclearity can be used effectively for determining the most important units in a text .","label":"Background","metadata":{},"score":"54.038315"}{"text":"All non - consecutive bigrams ( that may include zero , one , or two intervening words that are ingored ) and that meet the following criteria form a set of candidate features : .The training examples are converted into feature vectors , where each feature represents whether a candidate feature occurs in the context of a specific training example .","label":"Background","metadata":{},"score":"54.07099"}{"text":"Classification produces a confidence score for every sense ; our answers include just the highest scoring sense for each instance .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"54.120712"}{"text":"YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : This system takes a supervised learning approach to word sense disambiguation , where a decision tree is induced from sense - tagged training examples and then used to assign senses to the test examples .","label":"Background","metadata":{},"score":"54.1537"}{"text":"As a result , they gen- erate many types of incorrect readings for numerals , showing an accuracy of only 55%-87.7%4 [ 10].Specifically , they seldom produced the correct pronunciation when Arabic numerals were combined with homographs .Since homographs in Korean are as 2.2 Word Sense Disambiguation Based on Language Resources A number of studies have been carried out to disambiguate word senses based on various language resources .","label":"Background","metadata":{},"score":"54.16141"}{"text":"The neighboring words of Arabic numerals are also seg- mented , and their POS determined .The lemmatized neighboring .Figure 1 .System Architecture of Auto - TAN .words are treated as candidates of contextual features , as described in Section 4.1 .","label":"Background","metadata":{},"score":"54.198708"}{"text":"ACL , 2000 . \" ...This paper describes a set of comparative experiments , including cross - corpus evaluation , between five alternative algorithms for supervised Word Sense Disambiguation ( WSD ) , namely Naive Bayes , Exemplar - based learning , SNOW , Decision Lists , and Boosting .","label":"Background","metadata":{},"score":"54.208107"}{"text":"( Paper presented at COLING1992 ) [ 26 ] Yarowsky , D. 1996 .Homograph disambiguation in text - to- speech synthesis .Disambiguating the senses of non - text symbols for Mandarin TTS systems with a three - layer clas- sifier .","label":"Background","metadata":{},"score":"54.26137"}{"text":"It has often been thought that word sense ambiguity is a cause of poor performance in Information Retrieval ( IR ) systems .The belief is that if ambiguous words can be correctly disambiguated , IR performance will increase .However , recent research into the application of a word sense disambiguator to ... \" .","label":"Background","metadata":{},"score":"54.431156"}{"text":"In the machine - learning field , a classifier is a learn- ing model that classifies target answers .In order to avoid con- fusion , in this paper , the term ' classification model ' is used to indicate the latter .","label":"Background","metadata":{},"score":"54.54643"}{"text":"In these cases , the fine - grained output of the Czech lemmatizer was ignored ( in both training and test ) and a generic lexical sample sense classifier was trained on the training data ( see JHU_Swedish for further details of this ) .","label":"Background","metadata":{},"score":"54.58636"}{"text":"( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ? :No .Description : We have used tokenized , lemmatized , stripped the stop words out of the contexts and detected person names and numbers .","label":"Background","metadata":{},"score":"54.788094"}{"text":"The features used in the model is outputs of morphological and syntactic analysis .We used simple Bayes for learning .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"54.82605"}{"text":"Recent approaches to Word Sense Disambiguation ( WSD ) generally fall into two classes : ( 1 ) information - intensive approaches and ( 2 ) information - poor approaches .Our hypothesis is that for memory - based learning ( MBL ) , a reduced amount of data is more beneficial than the full range of features used in ... \" .","label":"Background","metadata":{},"score":"54.862717"}{"text":"Second method disambiguates all ambiguous verbs and adjectives instances in the test data .This algorithm implements a supervised learning method ( Maximum Entropy Probability Models ) consisting of the estimation of functions for classifying word senses by learning on a training data provided and then applied on the test instances .","label":"Background","metadata":{},"score":"54.981895"}{"text":"Procedure of sense disambiguation .( 1 ) Filter out senses using the satellite features .( 2 ) Disambiguate word sense using the classification information model .Classification information model(CIM ) CIM disambiguates word sense considering the discrimination score(DS ) of features .","label":"Background","metadata":{},"score":"55.031456"}{"text":"Each word in these texts is manually annotated with its appropriate sense , by six persons who all processed a different part of the data .The sense inventory is roughly based on a Dutch children 's dictionary .Sense tags are non - hierarchical .","label":"Background","metadata":{},"score":"55.1465"}{"text":"However , dictionary based approaches have two serious problems : ( 1 ) a large number of false recognitions mainly caused by short names .( 2 ) low recall due to spelling variation . \" ...Dictionary - based protein name recognition is often a first step in extracting in - formation from biomedical documents because it can provide ID information on recognized terms .","label":"Background","metadata":{},"score":"55.164433"}{"text":"There were 29 nouns and 15 adjectives , with between 70 and 455 instances per word ( total instances : 7567 , divided 2:1 between training data and test data ) .Inter - tagger - agreement was 85.5 % .( verb data is not covered here as that was prepared by Martha Palmer and colleagues at UPenn ) .","label":"Background","metadata":{},"score":"55.18326"}{"text":"words from the target ambiguous word in Korean texts [ 11].Based on the result from the Korean corpus , 3 words from the target Arabic numerals combined with homo- graphic classifiers were extracted from our corpus .All instances of Arabic numerals appearing with homographic classifiers were counted to 13,196 .","label":"Background","metadata":{},"score":"55.214417"}{"text":"..or each sents file considering the set of unigrams occurring in sentences . \" ...We present a simple technique for learning better SVMs using fewer training examples .Rather than using the standard SVM regularization , we regularize toward low weight - variance .","label":"Background","metadata":{},"score":"55.29854"}{"text":"The experiments reveal , among other facts , that SemCor can be an acceptable ( 0.7 precision for polysemous words ) starting point for an all - words system .The results on the DSO corpus show that for some highly polysemous words 0.7 precision seems to be the current state - of - the - art limit .","label":"Background","metadata":{},"score":"55.482162"}{"text":"Features were collected in 7-word window around the target word , and decision list method was used for learning .Did you use any training data provided in an automatic training procedure ?N .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"55.605797"}{"text":"The best variant , which we call LazyBoosting , is tested on the largest sense -- tagged corpus available containing 192,800 examples of the 191 most frequent and ambiguous English words .Again , boosting compares favourably to the other benchmark algorithms . \" ...","label":"Background","metadata":{},"score":"55.610756"}{"text":"In these domains , both costs and probabilities are unknown for test examples , so both cost estimators and probability estimators must be learned .This paper rst discusses how to make optimal decisions given cost and probability estimates , and then presents decision tree learning methods for obtaining well - calibrated probability estimates .","label":"Background","metadata":{},"score":"55.68194"}{"text":"Such bigrams must meet the following criteria : .This is the same approach as taken in duluth5 for English .The only difference is in the stop list .This is loosely based on the NAACL-01 paper \" A Decision Tree of Bigrams is an Accurate Predictor of Word Sense \" by Ted Pedersen . keywords : supervised learning , decision tree of bigrams , bagging .","label":"Background","metadata":{},"score":"55.704185"}{"text":"This work goes against the growing trend in computational linguistics of focusing on shallow but broad - coverage natural language tasks ( \" scaling up by dumbing down \" ) and instead concerns using logic - based learning to develop narrower , domain - specific systems that perform relatively deep processing .","label":"Background","metadata":{},"score":"55.75751"}{"text":"The test documents are annotated with morphological information ( word segmentation , POS tag , reading and base form , all automatically annotated ) for all words .For each target word , the ID of the sense in the TM best approximating that usage must be submitted .","label":"Background","metadata":{},"score":"55.7978"}{"text":"Our results are not always in line with some practices in the field .For example , we show that omitting no ... \" .This article describes the results of a systematic indepth study of the criteria used for word sense disambiguation .","label":"Background","metadata":{},"score":"55.846306"}{"text":"Moreover , WSD based on a tagged corpus is expensive because established sense - tagged corpora are rare and the cost of construct- ing the corpora is a time- and labor - consuming job .( b ) WSD based on dictionaries and wordnets : Semantic catego- ries can be obtained from definitions in machine - readable dictio- naries or from thesauri or wordnets .","label":"Background","metadata":{},"score":"55.894478"}{"text":"The corpus citations , not the word senses , are the basic objects in the ontology .The corpus citations will be clustered into senses according to the purposes of whoever or whatever does the clustering .In the absence of such purposes , word senses do not exist .","label":"Background","metadata":{},"score":"55.920452"}{"text":"Experimental results from using 26 confusion sets and a large amount of unlabeled data show that our proposed method for using unlabeled data considerably improves classification performance when the amount of labeled data is small . . ..In this section , we briefly review the naive Bayes classifier and the EM algorithm that is used for making use of unlabeled data .","label":"Background","metadata":{},"score":"55.99537"}{"text":"A topical context consists of the following features for all open - class morphemes within a window .Morpheme : an open - class morpheme The window size of all sentences was empirically chosen .A bigram context consists of the following features for all word pairs within a window .","label":"Background","metadata":{},"score":"56.004646"}{"text":"Description : Because no training data was provided for Italian , the JHU_Italian system was unsupervised , exploiting hierarchical cluster models induced from the Italian WordNet .Because several relationship types ( e.g. hypernymy ) are represented in the Italian WordNet , each relationship type was arbitrarily assigned a generic weight indicating a rough semantic similarity implied by that relationship ( e.g. synonymy received a 0.5 , while meronomy received a 10 ) .","label":"Background","metadata":{},"score":"56.018322"}{"text":"The L ... \" .This paper describes a set of comparative experiments , including cross - corpus evaluation , between five alternative algorithms for supervised Word Sense Disambiguation ( WSD ) , namely Naive Bayes , Exemplar - based learning , SNOW , Decision Lists , and Boosting . by","label":"Background","metadata":{},"score":"56.08222"}{"text":"No .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ? :Yes .Description : We have used tokenized , lemmatized , stripped the stop words out of the contexts and detected person names and numbers .","label":"Background","metadata":{},"score":"56.18894"}{"text":"The first step ( i.e. word domain disambiguation ) considers , for each word , a text window of about 100 words .A score is computed which takes into account the domains of the words within the window as well as their distance from the target word .","label":"Background","metadata":{},"score":"56.253563"}{"text":"No .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ? :No .Description : We have used tokenized , lemmatized , stripped the stop words out of the contexts and detected person names and numbers .","label":"Background","metadata":{},"score":"56.312294"}{"text":"Term extraction is important for many information systems applications .Although terms should be monoreferential , in reality they exhibit a high degree of ambiguity .Whilst conventional solutions mainly involve statistical approaches , this paper proposes a more hybrid technique .","label":"Background","metadata":{},"score":"56.397316"}{"text":"Term extraction is important for many information systems applications .Although terms should be monoreferential , in reality they exhibit a high degree of ambiguity .Whilst conventional solutions mainly involve statistical approaches , this paper proposes a more hybrid technique .","label":"Background","metadata":{},"score":"56.397316"}{"text":"Language , 53(2 ) , 285 - 311 [ 3 ] Chae , W. 1983 .A study on numerals and numeral classifier constructions in Korean .Linguistics Study , 19(1 ) , 19 - 34 [ 4 ] Daelmans , W. and Bosch , A. 1994 .","label":"Background","metadata":{},"score":"56.61869"}{"text":"( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : .Our approach for the all - words disambiguation task is based on statistical models .","label":"Background","metadata":{},"score":"56.654507"}{"text":"Dictionary - based protein name recognition is often a first step in extracting in - formation from biomedical documents because it can provide ID information on recognized terms .However , dictionary - based approaches present two fundamental difficulties : ( 1 ) false recognition mainly caused by short names ; ( 2 ) low recall due to spelling variations .","label":"Background","metadata":{},"score":"56.66044"}{"text":"The automatic disambiguation of word senses has been an interest and concern since the earliest days of computer treatment of language in the 1950 's .Sense disambiguation is an \" intermediate task \" ( Wilks and Stevenson , 1996 ) which is not an end in itself , but rather is necessary at one level or another to accomplish most natural language processing tasks .","label":"Background","metadata":{},"score":"56.66475"}{"text":"The automatic disambiguation of word senses has been an interest and concern since the earliest days of computer treatment of language in the 1950 's .Sense disambiguation is an \" intermediate task \" ( Wilks and Stevenson , 1996 ) which is not an end in itself , but rather is necessary at one level or another to accomplish most natural language processing tasks .","label":"Background","metadata":{},"score":"56.66475"}{"text":"To com- pensate for this strong bias , a modification of the measure called the gain ratio is widely used .After the decision tree is constructed , the performance of the model is tested using a 10-fold cross - validation checking method on the learning data containing 13,196 occurrences of Arabic numerals appearing with homographic classifiers .","label":"Background","metadata":{},"score":"56.67304"}{"text":"The meanings of these multi - word expressions can not be broken down into the set of meanings of the individual words in the expressions .Multi - word expressions cover idiomatic expressions ( in de steek laten , aan de hand zijn ) , sayings and proverbs ( Boontje komt om zijn loontje ) and strong collocations ( derde wereld , klavertje vier ) .","label":"Background","metadata":{},"score":"56.692547"}{"text":"Constraints are neces- sary to prevent one - way ascending .Second , KorLex 1.0 construction , referenced to Princeton WordNet , has potential problems in application to Korean lan- guage processing .The overall mapping rate of contextual features is not high not only because numerous Korean words or concepts that do not exist in WordNet are missing in KorLex1.0 , but also because too - fine - grained synsets in Princeton WordNet were translated into phrases .","label":"Background","metadata":{},"score":"56.70559"}{"text":"Tested accuracy exceeds 94 % on our evaluation corpus .Our system attempts to disambiguate all content words in running text rather than limiting itself to treating a restricted vocabulary of words .It is argued that this approach is more likely to assist the creation of practical systems . \" ...","label":"Background","metadata":{},"score":"56.711773"}{"text":"Dictionary - based protein name recognition is the first step for practical information extraction from biomedical documents because it provides ID information of recognized terms unlike machine learning based approaches .However , dictionary based approaches have two serious problems : ( 1 ) a lar ... \" .","label":"Background","metadata":{},"score":"56.72747"}{"text":"Rele- vant words were selected among the neighboring words of the target Arabic numeral and homographic classifier as contextual features .The individual contextual features were re - categorized into forty six semantic classes based on the lexical hierarchy in KorLex .","label":"Background","metadata":{},"score":"56.759205"}{"text":"The lexicon was generated the 2001 - 04 - 24 from GLDB / SDB ( Click here ) 8,718 annotated instances were provided as training material and 1,527 unannotated instances were provided for testing .URL containing additional information ( optional ) : Kokkinakis D. , Jarborg J. and Cederholm Y. ( May , 2001 ) , Swedish SENSEVAL ; a Developers ' Perspective .","label":"Background","metadata":{},"score":"56.783836"}{"text":"We describe the English Lexical Simplification task at SemEval-2012 .This is the first time such a shared task has been organized and its goal is to provide a framework for the evaluation of systems for lexical simplification and foster research on context - aware lexical simplification approaches .","label":"Background","metadata":{},"score":"56.850796"}{"text":"We describe the English Lexical Simplification task at SemEval-2012 .This is the first time such a shared task has been organized and its goal is to provide a framework for the evaluation of systems for lexical simplification and foster research on context - aware lexical simplification approaches .","label":"Background","metadata":{},"score":"56.850796"}{"text":"The similarity is calculated by considering an agreement between Japanese examples in TM and an input sentence .Otherwise , our system returns a translation as follows : .TM entries are classified according to the English head word after reinforced by a bilingual corpus .","label":"Background","metadata":{},"score":"56.85955"}{"text":"keywords : memory - based learning , rule induction , classifier combination , word experts .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"57.012897"}{"text":"A topical context consists of the following features for all open - class words within a window .Word : an open - class word The window size of + -1 sentences was empirically chosen .A bigram context consists of the following features for all word pairs within a window .","label":"Background","metadata":{},"score":"57.08956"}{"text":"Given a set of examples for which an at ... . by Claudia Perlich , Foster Provost , Jeffrey S. Simonoff - CEDER WORKING PAPER # IS-01 - 02 , STERN SCHOOL OF BUSINESS , 2001 . \" ...Tree induction and logistic regression are two standard , off - the - shelf methods for building models for classi cation .","label":"Background","metadata":{},"score":"57.291203"}{"text":"Description : The CL Research disambiguation system is part of the DIMAP dictionary software , which has been designed to use any full dictionary as the basis for disambiguation .Senseval-2 results were generated using WordNet , but also using the New Oxford Dictionary of English ( NODE ) .","label":"Background","metadata":{},"score":"57.35947"}{"text":"YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : This system takes a supervised learning approach to word sense disambiguation , where a Naive Bayesian classifier is learned from sense - tagged training examples .","label":"Background","metadata":{},"score":"57.36176"}{"text":"Subsequently , NODE was used on the Senseval-2 training data ( which had not otherwise been used ) .NODE definitions were then automatically mapped into WordNet , so that results could be compared with the use of WordNet on the training data .","label":"Background","metadata":{},"score":"57.36182"}{"text":"Secondly , any particular dictionary is written with a particular target audience in mind , and with a particular editorial philosophy in ... . \" ...There are now many computer programs for automatically determining which sense a word is being used in .","label":"Background","metadata":{},"score":"57.41003"}{"text":"First , no comparison was made between MetaCost 's final model and the internal cost - sensitive classifier on which MetaCost depends .It is credible that the internal cost - sensitive classifier may outperform the final model without the additional computation required to derive the final model .","label":"Background","metadata":{},"score":"57.483185"}{"text":"Our hypothesis is that for memory - based learning ( MBL ) , a reduced amount of data is more beneficial than the full range of features used in the past .Our experiments show that MBL combined with a restricted set of features and a feature selection method that minimizes the feature set leads to competitive results , outperforming all systems that participated in the SENSEVAL-3 competition on the Romanian data .","label":"Background","metadata":{},"score":"57.492218"}{"text":"Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : The system selects the most similar TM entry based on the cosine similarity between context vectors , which were constructed from semantic features and syntactic relations of neighboring words of the target word .","label":"Background","metadata":{},"score":"57.52138"}{"text":"In the final step , the default RFA is applied to the Arabic num- erals for which all disambiguation through contextual features , and pattern and arithmetic features , has failed .As a result , after the procedure , transliterated strings of the in- put Arabic numerals are output .","label":"Background","metadata":{},"score":"57.68758"}{"text":"keywords : machine translation system , translation dictionary , semantic analysis .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"57.75192"}{"text":"When added to a state - of - the - art coreference baseline , our Web features give significant gains on multiple datasets ( ACE 2004 and ACE 2005 ) and metrics ( MUC and B 3 ) , resulting in the best results reported to date for the end - to - end task of coreference resolution . ... shallower sources of semantics .","label":"Background","metadata":{},"score":"57.784492"}{"text":"The decision tree learner performs its own feature selection based on the gain ratio , which measures how well a feature partitions the training examples into senses .The bagging process for each decision tree is as described in duluth2 , and the features used as the basis for each decision tree are the same as in duluth1 .","label":"Background","metadata":{},"score":"57.80703"}{"text":"Description : The sussex system2ospd was applied to the all - words task , but processed only the plain text , ignoring the supplied bracketings .The system parses the text to identify subject / verb and verb / direct object relationships , and the nouns and verbs involved are disambiguated using class - based selectional preferences acquired from unsupervised training .","label":"Background","metadata":{},"score":"57.82299"}{"text":"1 Introduction Word Sense Disambiguation ( WSD ) is the problem of assigning the appropriate meaning ( sense ) to a given word in a text or discourse where this meaning is distinguishable from other senses potentially attributable to that word .","label":"Background","metadata":{},"score":"57.914795"}{"text":"The criteria for subcategorizing reading formulae for the Korean nu-meric system are the origin of numerals , the part of speech ( POS ) .5 Korean DSMs ( Decimal Scale Markers ) are sib ( \" 10 \" ) , baeg ( \" 100 \" ) , cheon ( \" 1,000 \" ) , man ( \" 10,000 \" ) , eog ( \" 100,000,000 \" ) , and others [ 9]. of numerals , the meaning of numerals , the distribution of allo- morphemes , and the existence of decimal scale markers ( DSM ) 5 .","label":"Background","metadata":{},"score":"58.084892"}{"text":"For all lemmas exhibiting only one sense in the training data , this sense was returned .Likewise , if there was insufficient data for word - specific training ( the sum of the minority sense examples for the word in training data was below a threshold ) the majority sense in training was returned for all instances of that lemma .","label":"Background","metadata":{},"score":"58.175106"}{"text":"Linguists validated and revised the automatic labeling of RFA .The training corpus is composed up of 407,489 occurrences of Arabic numerals .( b ) KorLex 1.0 ( Korean Lexico - Semantic Network ) : KorLex1.0 is constructed based on WordNet2.0 .","label":"Background","metadata":{},"score":"58.20832"}{"text":"The only difference is that morphological information in the evaluation data was corrected automatically .We learned the transformation rules like Brill 's POS tagger to correct morphological information .keywords : decision list , correcting morphological information , transformation rules .","label":"Background","metadata":{},"score":"58.28588"}{"text":"For each pair of inputs , calculate the string similarity by way of Dice 's coefficient over character bigrams 3 .For each input and translation record combination , calculate the maximum \" linked similarity \" via each other input , as the product of the input - input and input - translation record similarities 4 .","label":"Background","metadata":{},"score":"58.358524"}{"text":"Within the lexicography and linguistics literature , they are known to be very slippery entities .The paper looks at problems with existing accounts of ' word sense ' and describes the various kinds of ways in which a word 's meaning can deviate from its core meaning .","label":"Background","metadata":{},"score":"58.431065"}{"text":"So , no test example is tagged with one of these labels .keywords : hierarchical LazyBoosting , semantic domain attributes , multiword preprocessing , AdaBoost .MH .Did you use any training data provided in an automatic training procedure ?","label":"Background","metadata":{},"score":"58.466377"}{"text":"The sense with the highest probability is assigned to the test example .This system implements a standard benchmark , the Naive Bayesian classifier based on a bag of words feature set .keywords : supervised learning , Naive Bayesian classifier , bag of words .","label":"Background","metadata":{},"score":"58.483627"}{"text":"Additional features included parts - of - speech and lemmas in all syntactic positions , extracted using JHU - developed algorithms based on minimally supervised learning ( including Yarowsky and Wicentowski , 2000 ; Cucerzan and Yarowsky , 2000 ) .The output of each subsystem was merged by a classifier combination algorithm using weighted and thresholded voting and score combination .","label":"Background","metadata":{},"score":"58.55582"}{"text":"Description : Word senses are defined according to the Iwanami Kokugo Jiten , a Japanese dictionary published by Iwanami Shoten .The Iwanami Kokugo Jiten is distributed to all participants .For each sense in the dictionary , a corresponding sense ID and morphological information ( word segmentation , POS tag , base form and reading , all manually post - edited ) will be supplied .","label":"Background","metadata":{},"score":"58.59145"}{"text":"Each class has an equal numberofunweighted rules .A new example is classified by applying all rules and assigning the example to the class with the most satisfied rules .The induction m ... \" .A lightweight rule induction method is described that generates compact Disjunctive Normal Form ( DNF ) rules .","label":"Background","metadata":{},"score":"58.62924"}{"text":"2000 ) amount of word experts makes it also easy to parallelise the training process .For the classification of a given test item , it is first checked whether a word expert is available .If so , the best performing algorithm on the train set is applied with its optimal parameter settings to classify the item .","label":"Background","metadata":{},"score":"58.70201"}{"text":"Score the match quality of each example .IIT2 reduces the effect of mismatches distant from the target word over IIT1 .IIT3 ( All Word only ) restricts senses of context words to the \" best \" sense for words to the left of the target word before beginning the example match .","label":"Background","metadata":{},"score":"58.726807"}{"text":"Comparison of WSD accuracies WSD method Baseline WSD based on local context Hybrid WSD based on local context and semantic relations in KorLex WSD with expansion on synonym dictionary .Though the result is encouraging , several problems remain .First , disambiguation of homographic or polysemic contextual features based on the least upper bound algorithm does not per- form sufficiently well .","label":"Background","metadata":{},"score":"58.853554"}{"text":"A local context consists of the following features for all words within a window .Morpheme_Position : a morpheme and its position .Morpheme_POS : a morpheme and its part - of - speech .POS_Position : the part - of - speech and position of a morpheme Eojeol_Position : an \" eojeol \" and its position .","label":"Background","metadata":{},"score":"58.894894"}{"text":"The framework that is described here for people is easily applied to other objects as well .The motivatio ... \" .In this paper we present a component based person detection system that is capable of detecting frontal , rear and near side views of people , and partially occluded persons in cluttered scenes .","label":"Background","metadata":{},"score":"58.955826"}{"text":"N .Description : Our systems are descriptive - semantic - primitive - based , general - domain systems that do not require training or supervision .There are three components to the systems : a machine tool level machine - tractable dictionary ( MTD ) , a semantic distance matrix of the primitives , and a semantic tagger that uses a simple summation algorithm .","label":"Background","metadata":{},"score":"58.97389"}{"text":"There is no data like more data , performance improves log - linearly with the number of parameters ( unique N - grams ) .More importantly , when operating on new domains , we show that using web - derived selectional preferences is essential for achieving robust performance . ... le corpus : one is the web , which is the largest data set that is available for NLP ( Keller and Lapata , 2003 ) .","label":"Background","metadata":{},"score":"59.149185"}{"text":"( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : The JHU SENSEVAL-2 system for the lexical sample tasks consists of 6 diverse supervised learning subsystems integrated via classifier combination .","label":"Background","metadata":{},"score":"59.18963"}{"text":"The second is a non - consecutive two word sequence , where there may be zero or one intervening word that is ignored .Such bigrams must meet the following criteria : .The process of converting the training examples into feature vectors , bagging the decision tree , and making sense assignments is identical to duluth2 .","label":"Background","metadata":{},"score":"59.27346"}{"text":"One of the major transformations used in Linguistic Steganography is synonym substitution .However , few existing studies have studied the practical application of this approach .In this paper we propose two impro ... \" .Linguistic Steganography is concerned with hiding information in natural language text .","label":"Background","metadata":{},"score":"59.293804"}{"text":"This method has been trained and evaluated on two sense - tagged corpora : the interest corpus from NMSU and the Singapore DSO corpus .2400 sentence ) interest corpus and 72.1 % on the much larger ( approx .Tools . \" ...","label":"Background","metadata":{},"score":"59.412777"}{"text":"( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : This probabilistic WSD system uses synonym permutations to form n - grams , then queries AltaVista for word counts as the basis for establishing the probabilities .","label":"Background","metadata":{},"score":"59.430202"}{"text":"Y ( SemCor ) .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : The semantic disambiguation of a word is performed based on its relations with the preceding and succeeding words .","label":"Background","metadata":{},"score":"59.52778"}{"text":"The distribution of Arabic numerals having multiple RFA is wide : its ratio is 45.5 % , and the distribution ratio of multiple RFA derived from homographic classifiers is 14.2 % [ 10].Since many Chinese homographic classifiers are combined with Arabic numerals and select different RFA depending on the senses of the classifiers [ 3 ] , precedent word sense disambiguation of the homographic classifiers is required for selecting the correct RFA .","label":"Background","metadata":{},"score":"59.532974"}{"text":"Also , the most frequent semantic tags of the whole available context .We chose the ib1 algorithm with weigheted overlap metric + gain ration weighting , parameters that gave best results in a similar exercise for Swedish conducted in the past .","label":"Background","metadata":{},"score":"59.56819"}{"text":"The notion of simplicity is biased towards non - native speakers of English .Out of nine participating systems , the best scoring ones combine contextdependent and context - independent information , with the strongest individual contribution given by the frequency of the substitute regardless of its context . ... process of randomization is such that is allows the occurrence of ties .","label":"Background","metadata":{},"score":"59.627953"}{"text":"They generate not only incorrect reading of Arabic numerals but incorrect text symbols because they do not recognize the sense of Arabic numerals combined with text symbols .Due to lack of refined WSD module , the two systems show serious defect in reading numerals occurred with homographic classifiers .","label":"Background","metadata":{},"score":"59.711586"}{"text":"Page 4 .The corpus cov- ered politics , economics , society , international news , sports , enter- tainment , and opinion .Sentences containing Arabic numerals and their neighboring words are randomly sampled .Then the correct RFA tags are labeled7 .","label":"Background","metadata":{},"score":"59.76222"}{"text":"( E 3 ' )a. taegsi , eobs- , beoseu , sugbag - eobso , seunghabchadoe b. mihonnam , boheomlyo , gyeolhonha- , namja , yeoja , nop-c. yadang , yeongsuhoedam , jeonje jogeon , jujangha .Step 1 : Measuring Relevancies of Neighboring Words Among the lemmatized content words , three nouns from the left and three from the right of the target Arabic numeral and homo- graphic classifier considered to be related to each sense of the homographic classifier are extracted .","label":"Background","metadata":{},"score":"59.935852"}{"text":"The assumption is that if words clustered together based on their translations then the relevant senses will get higher weights given the appropriate similarity measure .keywords : parallel corpora , token alignments , WordNet , information - theoretic similarity measure . organisation : TALP Research Center - Technical University of Catalonia .","label":"Background","metadata":{},"score":"59.98578"}{"text":"Several words on the left and on the right of the head word in a given sentence , and their POS assigned by the morphological analyzer JUMAN .N - grams including the head word in a given sentence .English head word in the cluster where the longest string in a given sentence is found , and the length .","label":"Background","metadata":{},"score":"60.14842"}{"text":"Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : The system selects the appropriate translation record based on the character - bigram - based similarity , as follows .","label":"Background","metadata":{},"score":"60.159203"}{"text":"Resolving coordination ambiguity is a classic hard problem .This paper looks at coordination disambiguation in complex noun phrases ( NPs ) .Parsers trained on the Penn Treebank are reporting impressive numbers these days , but they do n't do very well on this problem ( 79 % ) .","label":"Background","metadata":{},"score":"60.181973"}{"text":"Resolving coordination ambiguity is a classic hard problem .This paper looks at coordination disambiguation in complex noun phrases ( NPs ) .Parsers trained on the Penn Treebank are reporting impressive numbers these days , but they do n't do very well on this problem ( 79 % ) .","label":"Background","metadata":{},"score":"60.181973"}{"text":"However , the use of unlabeled data via the basic EM algorithm often causes disastrous performance degradation instead of improving classification performance , resulting in poor classification performance on average .In this study , we introduce a class distribution constraint into the iteration process of the EM algorithm .","label":"Background","metadata":{},"score":"60.358692"}{"text":"Stochastic Korean word - spacing with smoothing using Korean spelling checker .Computer Processing of Oriental Languages , 17 , 239 - 252 [ 13 ] Leacock , C. and Chodorow , M. 1998 .Combining local con- text and WordNet similarity for word sense identification .","label":"Background","metadata":{},"score":"60.375843"}{"text":"The rule induction method takes as input both context information and all possible keywords within the context of three sentences .Both memory - based learners are cross - validated to determine the optimal parameter settings for each word expert .On these combined classifier outputs and the WordNet most frequent sense , majority voting and weighted voting are performed .","label":"Background","metadata":{},"score":"60.41172"}{"text":"WSD based on dictionaries or wordnets have limitations in handling real data produced currently .Thus a hybrid method ex- ploiting a medium - sized tagged corpus and a wordnet together for WSD is suggested in this paper .AUTOMATIC TRANSLITERATION OF ARABIC NUMERALS Computer - readable texts contain not merely alphabetic letters but also non - alphabetic symbols .","label":"Background","metadata":{},"score":"60.46225"}{"text":"Feature .A local context consists of the following features for all words within a window .Word_Position : a word and its position Word_POS : a word and its part - of - speech .POS_position : the part - of - speech and position of a word .","label":"Background","metadata":{},"score":"60.587955"}{"text":"If none of these sequences were found , we would consider the word \" art \" in isolation .If there was just one sense in the sense index for \" art \" , that sense would be returned , if there were no sense , then \" sense unknown \" would be returned .","label":"Background","metadata":{},"score":"60.634583"}{"text":"Step 5 : Selecting the least upper bound as a semantic category for the cluster of contextual features .It can differ from case to case .Fig . 2 .Automatic and dynamic selection of Least Upper Bound .By application of the semantic relations in the KorLex hierarchy to the training corpus , forty - six semantic categories were obtained .","label":"Background","metadata":{},"score":"60.66382"}{"text":"The data classi cation is handled by several support vector machine classi ers arranged in two layers .This architecture is known as Adaptive Combination of Classi ers ( ACC ) .The system performs very well and is capable of detecting people even when all components of a person are not found .","label":"Background","metadata":{},"score":"60.702393"}{"text":"Word sense disambiguation ( WSD ) is a key problem in computational linguistics , with applications in areas such asmachine translation and information retrieval .This paper describes a corpus - based method for word sense disambiguation which usesaversatile maximum entropy technique on simple local lexi ... \" .","label":"Background","metadata":{},"score":"60.765778"}{"text":"..e - art results In order to compare decision lists with other state of the art algorithms we tagged all 191 words in the DSO corpus .The same test set is also used in ( Escudero et al .2000b ) which presents a boosting appro ... . by","label":"Background","metadata":{},"score":"60.868835"}{"text":"The subsystems included decision lists ( Yarowsky , 2000 ) , transformation - based error - driven learning ( Brill , 1995 ; Florian and Ngai , 2001 ) , cosine - based vector models , decision stumps and two feature - enhanced naive Bayes systems ( one trained on words , one trained on lemmas ) .","label":"Background","metadata":{},"score":"60.94863"}{"text":"An On - line Lexical Prob- lems .[ 18 ] Mitchell , T. M. 1997 .Machine learning .( New York : McGraw - Hill ) [ 19 ] Olinsky , C. and Black . A. W. 2000 .Non - standard word and homograph resolution for Asian language text analysis .","label":"Background","metadata":{},"score":"61.216156"}{"text":"No .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ? :Yes .Description : The system uses \" semantic domains \" ( e.g. Medicine , Sport , Architecture ) associated to wordnet synsets .","label":"Background","metadata":{},"score":"61.262127"}{"text":"keywords : classifier combination , word sense disambiguation , bayes classifiers , cosine similarity , decision lists , transformation based learning .name : David Yarowsky , Silviu Cucerzan , Radu Florian , Charles Schafer and Richard Wicentowski . jhu.edu .organisation : Computer Science Department and Center for Language and Speech Processing , Johns Hopkins University .","label":"Background","metadata":{},"score":"61.272587"}{"text":"MH boosting algorithm is applied to the Word Sense Disambiguation ( WSD ) problem .Initial experiments on a set of 15 selected polysemous words show that the boosting approach surpasses Naive Bayes and Exemplar - based approaches , which represent stat ... \" .","label":"Background","metadata":{},"score":"61.714634"}{"text":"In particular , the reason ... . byAobo Wang , Min - yen Kan , Daniel Andrade , Takashi Onishi , Kai Ishikawa . \" ...We study the linguistic phenomenon of informal words in the domain of Chi - nese microtext and present a novel method for normalizing Chinese informal words to their formal equivalents .","label":"Background","metadata":{},"score":"61.72538"}{"text":"Within the lexicography and linguistics literature , they are known to be very slippery entities .The paper looks at problems with existing accounts of ' word sense ' and describes the various kinds of ways in which a word 's meaning can deviate from its c ... \" .","label":"Background","metadata":{},"score":"61.810204"}{"text":"This is based on the NAACL-01 paper \" A Decision Tree of Bigrams is an Accurate Predictor of Word Sense \" by Ted Pedersen .The use of bagging and a stop list is new for Senseval .keywords : supervised learning , decision tree of bigrams , bagging .","label":"Background","metadata":{},"score":"61.88909"}{"text":"So , consider how well each example matches the target context .Align the context target word to the synset word in the example .For each example word ( working out from the synset word ) find the closet word in the target context that is related under WordNet relations .","label":"Background","metadata":{},"score":"61.972458"}{"text":"The only difference is in the stop list .This system is motivated by the relative success of decision stumps as reported in the NAACL-01 paper \" A Decision Tree of Bigrams is an Accurate Predictor of Word Sense \" by Ted Pedersen . keywords : supervised learning , decision stumps .","label":"Background","metadata":{},"score":"62.02495"}{"text":"Description : The WASPS - Workbench is a browser - based tool which integrates lexicography and automatic WSD for the benefit of both parties .The user enters a word to be analyzed and the Workbench calculates a \" Word - Sketch \" : a page of statistically significant collocation patterns for that word ( currently based on the BNC ) .","label":"Background","metadata":{},"score":"62.218987"}{"text":"The first is achieved by using approximate string searching , and the second by expanding the dictionary with a probabilistic variant generator , which we propose in this paper .Key words : protein name recognition , naive Bayes classifier , approximate string search , spelling variant generator 2 1 . by Gerard Escudero , Llus Mrquez , German Rigau - IN PROC .","label":"Background","metadata":{},"score":"62.24964"}{"text":"Table 1 gives an overview of the data sets . byLorenza Romano , Krisztian Buza , Lars Schmidt - thieme , Claudio Giuliano . \" ...In this paper we present an approach to person name disambiguation that clusters documents on the basis of textual features using cosine similarity and a machinely learned meta similarity measure .","label":"Background","metadata":{},"score":"62.267937"}{"text":"We also use Mainichi Simbun newspaper articles for 1994 as additional training data .The Basque task consists of lexical samples for 40 words .There will be approx .The samples would comprise 5 sentences centered in the target word , taken from a newspaper corpus , and , if there is interest , the whole documents will be available via internet .","label":"Background","metadata":{},"score":"62.271152"}{"text":"keywords : supervised learning , Naive Bayesian classifier , bag of words .URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .Did you use any training data provided in an automatic training procedure ?","label":"Background","metadata":{},"score":"62.338753"}{"text":"The feature vectors are the input to the J48 learning algorithm , the Weka implementation of the C4.5 decision tree learner .The decision tree learner is \" bagged \" .The training examples are sampled ten times ( with replacement ) and a decision tree is learned for each sample .","label":"Background","metadata":{},"score":"62.86893"}{"text":"The disambiguated version of the Wordnet dictionary was then used in handling the test data .In the sense - tagging of the test sentences , all nonheads ( words that ddi not require tagging ) were removed from each sentence , leaving only the heads ( words to be sense - tagged ) behind .","label":"Background","metadata":{},"score":"63.046574"}{"text":"The phonetic distinctions such as /sam/ and /s ? possibly generated by the TTS systems ; however , they are ignored since they do not change the sense of ' 3 ' .Given that correct trans- literation of an Arabic numeral is readily synthesized to speech by searching the phoneme database , the accuracies between Auto- TAN and the two TTS systems can be compared at the identical level .","label":"Background","metadata":{},"score":"63.05739"}{"text":"The results of the study show several remarkable things .( 1 ) Contrary to prior observations , logistic regression does not generally outperform tree induction .( 2 )More specifically , and not surprisingly , logistic regression is better for smaller training sets and tree induction for larger data sets .","label":"Background","metadata":{},"score":"63.157394"}{"text":"For example , the context examples containing the verb \" dress down \" are separated from the examples containing only \" dress \" .Words which are monosemous are eliminated at this step , as well as words which can be tagged as proper nouns ( if they are tagged as such by the part of speech tagger and if they have a role identified by the Named Entity recognizer ) .","label":"Background","metadata":{},"score":"63.220737"}{"text":"2.1 Normalization of Non - standard Words Researchers performing practical studies on the implementation of TTS systems have focused on the importance of the text norma- lization .This basic non standard word model , after undergoing slight modification , was applied to deal with NSW in Japanese and Chinese texts [ 19].","label":"Background","metadata":{},"score":"63.253838"}{"text":"Arabic numerals in new data .In Section 2 , related work on text normalization and WSD is presented .In Section 3 , the linguistic issue of the ambiguities of reading Arabic numerals in Korean is revealed , language resources used for word sense disambiguation of homographs are described , and our system is illustrated in overview .","label":"Background","metadata":{},"score":"63.278313"}{"text":"For the assistant , the length of the interaction with the system ranged from 3 to 25 mins , with an average under 15 mins .Due to time constraints we only managed to submit results for nouns within the deadline .","label":"Background","metadata":{},"score":"63.386726"}{"text":"( Berlin , Heidelberg : Springer - Verlag Germany ) [ 9 ] Hwang , S. , and Yoon , A. 2005 .Semantic feature inheritance revisited for building Korean lexical semantic network ( 1 ) : A case study using sex feature .","label":"Background","metadata":{},"score":"63.406563"}{"text":"( Paper presented at ESCA - IEEE Speech Synthesis Conference ) [ 5 ] Fellbaum , C. ( Ed . ) WordNet - An electronic lexical database .( Cambridge , MA : MIT Press ) [ 6 ] Francis , W. and Kuera , H. 1982 .","label":"Background","metadata":{},"score":"63.460148"}{"text":"We show that using a simple , common smoothing method -- the Laplace correction -- uniformly improves probability - based rankings .In addition , bagging substantioJly improves the rankings , and is even more effective for this purpose than for improving accuracy .","label":"Background","metadata":{},"score":"63.513294"}{"text":"( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : SST is a supervised word sense tagger .This tagger uses support vector machine ( SVM ) learning to build classifiers from the training data .","label":"Background","metadata":{},"score":"63.51616"}{"text":"Computational systems that learn to transform naturallanguage sentences into semantic representations have important practical applications in building naturallanguage interfaces .They can also provide insight into important issues in human language acquisition .However , within AI , computationa ... \" .","label":"Background","metadata":{},"score":"63.62979"}{"text":"The decision tree learner is \" bagged \" .The training examples are sampled ten times ( with replacement ) and a decision tree is learned for each sample .Each test example is assigned a sense based on a vote taken from among the learned trees .","label":"Background","metadata":{},"score":"63.680008"}{"text":"Only 149,556 words in this text is manually annotated for sense .The text is also annotated with morphological information ( word segmentation , POS tag and base form and reading , all manually post - edited ) for all words .","label":"Background","metadata":{},"score":"63.68708"}{"text":"Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : Given an input sentence , our system returns an entry number in TM or a translation of head word ( or phrase ) .","label":"Background","metadata":{},"score":"63.828552"}{"text":"Disambiguation based on Wordnet for transliteration of Arabic numerals for Korean TTS .LNCS , 3878 , 366 - 377 .[11 ] Kim , J. , Choi , H. , and Oak , C. 2003 .Disambiguation model of homographs based on statistic using weight .","label":"Background","metadata":{},"score":"63.834473"}{"text":"Finally , ( 4 ) the domains on which tree induction and logistic regression are ultimately preferable canbecharacterized surprisingly well by a simple measure of signal - to - noise ratio . ... despread .73.1.3PETs and pruning If we 're going to compare tree induction to logistic regression using their probability estimates ... . by Gerard Escudero , Lluis Marquez , German Rigau - IN PROCEEDINGS OF THE 12TH EUROPEAN CONFERENCE ON MACHINE LEARNING , 2000 . \" ...","label":"Background","metadata":{},"score":"63.946705"}{"text":"The second feature set is based on unigrams ( one word sequences ) that meet the following criteria : . 1 ) occur 5 or more times and 2 ) are not found on the stop - list .The third feature set is based on bigrams that may include one intervening word that is ignored and that meet the following criteria : .","label":"Background","metadata":{},"score":"63.949966"}{"text":"Description : The task for Spanish is a ' lexical sample ' for 40 words ( 18 nouns , 13 verbs and 9 adjectives ) .The items chosen can only belong to one of the syntactic categories and the sentences have been chosen to illustrated it .","label":"Background","metadata":{},"score":"64.031746"}{"text":"The original ensemble method is Bayesian averaging , but more recent algorithms include error - correcting output coding , Bagging , and boostin ... \" .Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a ( weighted ) vote of their predictions .","label":"Background","metadata":{},"score":"64.22095"}{"text":"The disadvantage of this is that we are often forced to make difficult , if not impossible , decisions in distingishing between senses .Also as we do not use the training data , we have no knowledge of the relative frequencies of the different senses .","label":"Background","metadata":{},"score":"64.232574"}{"text":"The use of bagging and a stop list is new for Senseval .This is the same approach as taken in duluth2 for English .The only difference is in the stop list .keywords : supervised learning , decision tree of bigrams , bagging .","label":"Background","metadata":{},"score":"64.25658"}{"text":"For SENSEVAL-2 , the dataset was divided in two parts .The training set consisted of 76 books ( about 115.000 words ) .The test set consisted of the remaining 26 books ( about 38.000 words ) .Lexicon used was WordNet 1.7 ; instances were mostly from the British Natinal Corpus with some from the Wall Street Journal .","label":"Background","metadata":{},"score":"64.287025"}{"text":"These are summed and the sense with the maximum probability is assigned to a test example .keywords : supervised learning , ensemble .URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .","label":"Background","metadata":{},"score":"64.31282"}{"text":"Notice that morphological information in the training data is post - edited , but not in the evaluation data , so participants may ignore morphological information in the evaluation data .The number of target words is 100 , 50 nouns and 50 verbs .","label":"Background","metadata":{},"score":"64.3492"}{"text":"In both cases , we filter out section headings , references , tables , etc .Table 1 gives an overview of the data sets .4 Experiments and Results This section reports experimental results of our system on the HOO - HELDOUT and the HOO - TEST data ... . \" ...","label":"Background","metadata":{},"score":"64.35176"}{"text":"Evaluation of our Automatic Transliteration system of Arabic numerals adopting the proposed model is performed compared with two customized Korean TTS systems in Section 5 .Conclu- sions and future work follow in Section 6 . RELATED STUDIES For the purpose of the practical application of natural language processing ( NLP ) techniques to TTS , numerous studies have been conducted .","label":"Background","metadata":{},"score":"64.366745"}{"text":"( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : This system takes a supervised learning approach to word sense disambiguation , where a decision stump is induced from sense - tagged training examples .","label":"Background","metadata":{},"score":"64.55665"}{"text":"The patterns are validated on the training data , and we keep only those which are 100 % accurate .The patterns are then applied on the test data .Only a few instances can be disambiguated this way , but with high confidence : previous experiments have shown that high accuracy is obtained with this procedure .","label":"Background","metadata":{},"score":"64.635086"}{"text":"No language resources were used .keywords : character bigram , Dice 's coefficient , input - input similarity .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"64.64908"}{"text":"It differs from standard approaches by allowing for as fine grained distinctions as is warranted by the information at hand , rather than supposing a fixed number of senses per word , and by allowing for more than one sense to be assigned to a given word occur - rance .","label":"Background","metadata":{},"score":"64.712105"}{"text":"This paper reviews these methods and explains why ensembles can often perform better than any single classifier .Some previous studies comparing ensemble methods are reviewed , and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly . \" ...","label":"Background","metadata":{},"score":"64.76074"}{"text":"Tree induction and logistic regression are two standard , off - the - shelf methods for building models for classi cation .We present a large - scale experimental comparison of logistic regression and tree induction , assessing classification accuracy and the quality of rankings based on class - membership probabilities .","label":"Background","metadata":{},"score":"64.81091"}{"text":"The learning algorithm is support vector machine .keywords : support vector machine , morphological analysis , syntactic analysis .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"64.83754"}{"text":"We present a simple technique for learning better SVMs using fewer training examples .Rather than using the standard SVM regularization , we regularize toward low weight - variance .Our new SVM objective remains a convex quadratic function of the weights , and is therefore computationally no harder to optimize than a standard SVM .","label":"Background","metadata":{},"score":"64.84158"}{"text":"Alternatively , submissions can take the form of actual target word translations , or translations of phrases or sentences including each target word .In this case , translation experts are asked to judge whether the supplied translation is appropriate or not .","label":"Background","metadata":{},"score":"65.02617"}{"text":"Such sequences are learned from sense - tagged data using transformation - based learning .The system as such consists of a PWE compiler which translates PWE specifications into Horn clause formulas ( similar to a DCG compiler ) .The rest is just a matter of performing Prolog - style constructive proofs .","label":"Background","metadata":{},"score":"65.13866"}{"text":"The TM contains , for each Japanese head word , a list of typical Japanese expressions ( phrases / sentences ) involving the head word and an English translation for each .Each pair is treated as a distinct sense and has a unique \" sense ID \" .","label":"Background","metadata":{},"score":"65.25194"}{"text":"MH boosting algorithm is applied to the Word Sense Disambiguation ( WSD ) problem .Initial experiments on a set of 15 selected polysemous words show that the boosting approach surpasses Naive Bayes and Exemplar - based approaches , which represent state - of - the - art accuracy on supervised WSD .","label":"Background","metadata":{},"score":"65.40083"}{"text":"Unfortunately , evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions .The position that we take in this paper is that , in order to build high - quality summarization programs , one ... .","label":"Background","metadata":{},"score":"65.56642"}{"text":"Probability estimation trees ( PETs ) have the same attractive features as classification trees ( e.g. , c ... \" .Tree induction is one of the most effective and widely used methods for building classification models .However , many applications require cases to be ranked by the probability of class membership .","label":"Background","metadata":{},"score":"65.64075"}{"text":"It is unclear whether violation of the assumption has an impact on MetaCost 's performance .We study these issues using two boosting procedures , and compare with the performance of the original form of MetaCost which employs bagging .Tools . \" ...","label":"Background","metadata":{},"score":"65.64284"}{"text":"YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : This system takes a supervised learning approach to word sense disambiguation , where a decision stump is induced from sense - tagged training examples .","label":"Background","metadata":{},"score":"65.64331"}{"text":"The main idea of boosting algorithms is to combine many simple and moderately accurate hypotheses ( weak classifiers ) into a single , highly accurate classifier .More specifically , LazyBoosting is a simple modification of AdaBoost .MH algorithm , wich consists in reducing the feature space that is explored whenever a weak classifier is learnt .","label":"Background","metadata":{},"score":"65.64899"}{"text":"Description : We searched for collocations within a window of one word only .For instance , if the word to be disambiguated was \" art \" and the sentence was the \" the pop art collection is owned by ... \" , we would have three sequences : 1 . pop art , 2 . pop art collection , 3 . art collection .","label":"Background","metadata":{},"score":"65.657166"}{"text":"Wide co - occurrences include all of the words in each instance whereas local collocation uses a window consisting of the 3 tokens immediately before and after the word to be tagged ( i.e. features include left_wd3 , left_wd2 , left_wd1 , right_wd1 , right_wd2 , right_wd3 ) .","label":"Background","metadata":{},"score":"65.65833"}{"text":"In this work , the balanced corpus was com- posed of instances showing exhaustive RFA according to the dis- tribution ratio of RFA in the training corpus .The accuracy of our Auto - TAN and the accuracies of two com- mercial TTS systems , \" VoiceWare \" and \" CoreVoice \" , were meas- ured and compared in the five test sets . \" VoiceWare \" and \" Core- Voice \" have been evaluated as the best Korean commercial TTS systems .","label":"Background","metadata":{},"score":"65.71602"}{"text":"System name : usm_english_tagger , usm_english_tagger2 , usm_english_tagger3 .Your contact details .Did you use any training data provided in an automatic training procedure ?No .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"65.74694"}{"text":"Their linguistic constraints in reading Arabic numerals and the transliteration rules of Arabic numeral expressions established by combining those constraints were well discussed in [ 10].To copy otherwise , or republish , to post on servers or to redistribute to lists , requires prior specific permission and/or a fee .","label":"Background","metadata":{},"score":"65.7977"}{"text":"265 - 283 ) , Cambridge , MA : MIT Press . )[ 14 ] Lee , E. , Lim , S. , and Kwon , H. 2004 .Output of Korean translation of WordNet 2.0 .( Paper presented at the 2nd Workshop on Knowledge Information Processing and On- tology , Daejeon , S.Korea ) [ 15 ] Lyons , J. 1977 .","label":"Background","metadata":{},"score":"66.020966"}{"text":"Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : Because of the importance of morphological analysis in a highly inflected language such as Czech , a part - of - speech tagger and lemmatizer kindly provided by Jan Hajic of Charles University was first applied to the data .","label":"Background","metadata":{},"score":"66.17772"}{"text":"Due to name abbreviations , identical names , name misspellings , and pseudonyms in publications or bibliographies ( citations ) , an author may have multiple names and multiple authors may share the same name .Such name ambiguity affects the performance of document retrieval , web search , database integra ... \" .","label":"Background","metadata":{},"score":"66.180145"}{"text":"This paper investigates two important issues centered on the procedure which were ignored in the paper proposing MetaCost .First , no comparison was made between MetaCost 's ... \" .MetaCost is a recently proposed procedure that converts an error - based learning algorithm into a cost - sensitive algorithm .","label":"Background","metadata":{},"score":"66.42375"}{"text":"Normalization of non - standard words .Computer Speech and Language , 15(3 ) , 287 - 333 [ 23 ] Tetschner , W. 2004 .The Mathematics of Inheritance Sys- tems .Los Altos , Calif.:Morgan Kaufmann .[ 25 ] Yarowsky , D. 1992 .","label":"Background","metadata":{},"score":"66.54556"}{"text":"Did you use any training data provided in an automatic training procedure ?Y .Description : .The TALP system can be defined as Hierarchical LazyBoosting .It works as Yarowsky 's hierarchical decision lists , but using LazyBoosting instead of Decision Lists .","label":"Background","metadata":{},"score":"66.5887"}{"text":"In the modern Korean language , numerals have three different origins - Korean , Chinese , and English - and they show a variety of variants .Their distribution is largely dependent on context1 .For example , a single numeral ' 3 ' can be read in 5 different ways de-","label":"Background","metadata":{},"score":"66.89812"}{"text":"Table 5 .Step 3 : Training learning features and testing performance The distinctive power of each feature is explicitly represented through a constructed tree , and consequently , decision tree has been adopted in numerous studies ( Mitchell , 1997 ) .","label":"Background","metadata":{},"score":"67.04639"}{"text":"[ 20 ] Quinlan , J. R. 1993 .C4.5 : programs for machine learning .( San Mateo : Morgan Kaufmann Publishers ) [ 21 ] Resnik , P. 1995 .Using information content to evaluate se- mantic similarity .( Paper presented at the 14th Conference of Artificial Intelligence ) .","label":"Background","metadata":{},"score":"67.11047"}{"text":"By leveraging some assistant data , the dependency parsing model can direct ... a , overly complex weak hypotheses or weak hypotheses which are too weak .Boosting seems to be especially susceptible to noise [ 10].For instance , Freund and Schapire [ 16 ] tested AdaBoost on a set of UCI benchmark datasets [ 27 ] using C4.5 [ 29 ] asa weak learning algorithm , as well as an algorithm whichs4:1/0.27,4/0.17 5:0/0.26,5/0 ... . by Thomas G. Dietterich - MULTIPLE CLASSIFIER SYSTEMS , LBCS-1857 , 2000 . \" ...","label":"Background","metadata":{},"score":"67.17344"}{"text":"organisation : Computer Science Department and Center for Language and Speech Processing , Johns Hopkins University .Task / s : English Lexical choice .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"67.24002"}{"text":"Keywords : please provide . ... using any one of the knowledge sources alone .That local collocation is the most predictive seems to agree with past observation that humans need a narrow window of only a few tokens to perform WSD .","label":"Background","metadata":{},"score":"67.28581"}{"text":"The hybrid WSD model showed 87.3 % accuracy for the sense disambiguation of homographic classifiers and thus prediction of correct RFA .Auto - TAN adopt- ing the proposed WSD model outperforms the current TTS sys- tems by 3.9 % to 20.3 % .","label":"Background","metadata":{},"score":"67.35479"}{"text":"It is part of Natural Language Processing ( NLP ) .Optical Character is used for separate out number text from image .Perform translation on number text by using rule based approach , so that it will convert number text from one regional language to another regional language .","label":"Background","metadata":{},"score":"67.40639"}{"text":"keywords : supervised learning , ensemble .URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .Did you use any training data provided in an automatic training procedure ?","label":"Background","metadata":{},"score":"67.45551"}{"text":"In many machine learning domains , misclassication costs are dierent for dierent examples , in the same way that class membership probabilities are exampledependent .In these domains , both costs and probabilities are unknown for test examples , so both cost estimators and probability estimators must be ... \" .","label":"Background","metadata":{},"score":"67.47282"}{"text":"( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : This supervised system is a variation of ehu - dlist - all .","label":"Background","metadata":{},"score":"67.63567"}{"text":"Thus , Auto - TAN fails to transliterate \" 3 \" in \" 3 dae - leul modu bonaessda \" , either .Page 8 .CONCLUSTIONS AND FUTURE WORD In this paper , the ambiguities in reading Arabic numeral were analyzed , and the hybrid word sense disambiguation model ex-","label":"Background","metadata":{},"score":"67.65634"}{"text":"The algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts .Multi - paragraph subtopic segmentation should be useful for many text analysis tasks , including information retrieval and summarization . \" ...","label":"Background","metadata":{},"score":"67.66027"}{"text":"As mentioned in Section 1 , classifiers or neighboring words of an Arabic numeral , provide key information to determine RFA .If \" beon - ho ( ID number ) \" comes with \" 97 - 06 - 04 \" , the numerals are transliterated as \" guchil gongyug gongsa ( C_b[-D ] ) \" .","label":"Background","metadata":{},"score":"67.77687"}{"text":"The occurrence frequency of Arabic numerals and text symbols in Korean newspaper articles is 9.7 % .In Section 3 , linguistic issues on recognizing and reading Arab- ic numerals are discussed and language resources are described for the purpose of implementing an Automatic transliteration system of Arabic numerals .","label":"Background","metadata":{},"score":"67.78604"}{"text":"A new example is classified by applying all rules and assigning the example to the class with the most satisfied rules .The induction method attempts to minimize the training error with no pruning .An overall design is specified by setting limits on the size and number of rules .","label":"Background","metadata":{},"score":"67.84722"}{"text":"Inaccurate probabilities are partially the result of decision - tree induction algorithms that focus on maximizing classification accuracy and minimizing tree size ( for example via reduced - error pruning ) .Larger trees can be better for probability estimation , even if the extra size is superfluous for accuracy maximization .","label":"Background","metadata":{},"score":"68.2261"}{"text":"Full - text .International Conference on Convergence and Hybrid Information Technology 2009 ICHIT'09 , August 27 - 29 , 2009 , Daejeon , Korea .Hybrid Word Sense Disambiguation Using LanguageHybrid Word Sense Disambiguation Using Language Resources for Transliteration of ArabicResources for Transliteration of Arabic Numerals in KoreanNumerals in Korean .","label":"Background","metadata":{},"score":"68.4067"}{"text":"By changing the cost function , different ... \" .1 Introduction Recent papers [ 20 ] have shown that boosting , arcing , and related ensemble methods ( hereafter summarized asboosting ) can be viewed as margin maximization in function space .","label":"Background","metadata":{},"score":"68.42966"}{"text":"The results show that the accuracy of Auto - TAN outper- formed \" VoiceWare \" and \" CoreVoice \" by 3.9%-20.3 % .In addition , Auto - TAN performed consistently regardless of the source or the manner in which a corpus was sampled .","label":"Background","metadata":{},"score":"68.44821"}{"text":"For future work , WSD for ambiguous contextual features , by adopting a scoring algorithm , should be continued .Since Kor- Lex1.0 has not yet been completely refined , continuous studies on WSD for other applications with the refined KorLex are promising .","label":"Background","metadata":{},"score":"68.50398"}{"text":"We used a hybrid model of two kinds of support vector machines and two kinds of simple Bayes for learning .Did you use any training data provided in an automatic training procedure ?YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"68.56679"}{"text":"Size matters : ( 1 ) is a million words , ( 2 ) is potentially billions of words and ( 3 ) is potentially trillions of words .The unannotated monolingual data is helpful when the ambiguity can be resolved through associations among the lexical items .","label":"Background","metadata":{},"score":"68.67767"}{"text":"This system is identical to duluth2 , except that it relies on a different feature set .No information from WordNet is utilized by this system .This system uses a filter to perform feature identification prior to learning .Two different kinds of bigrams are identified as candidate features .","label":"Background","metadata":{},"score":"68.8077"}{"text":"The KorLex 1.0 statistics are shown in Table 3 .Table 3 .KorLex 1.0 Statistics KorLex1.0 is the first version of the Korean Lexico - Semantic Network and was released at the 2nd Information Processing and Ontology [ 14]. 3.3 System Architecture of Auto - TAN Since Arabic numerals are widely distributed in Korean texts and are found in various contexts , an Arabic numeral translitera- tion system requires sub - modules for morphological analysis as well as word sense disambiguation and comprehensive dictiona- ries .","label":"Background","metadata":{},"score":"69.061554"}{"text":"Unfortunately , decision trees have been found to provide poor probability estimates .Several techniques have been proposed to build more accurate PETs , but , to our knowledge , there has not been a systematic experimental analysis of which techniques actually improve the probability - based rankings , and by how much .","label":"Background","metadata":{},"score":"69.42836"}{"text":"Tasks : English lexical sample ( official ) , Spanish lexical sample ( official ) , Swedish lexical sample ( official ) , Basque lexical sample ( unofficial ) .Did you use any training data provided in an automatic training procedure ?","label":"Background","metadata":{},"score":"69.54334"}{"text":"keywords : word sense disambiguation , morphological analysis , classifier combination .name : David Yarowsky , Silviu Cucerzan , Radu Florian , Charles Schafer and Richard Wicentowski . jhu.edu .organisation : Computer Science Department and Center for Language and Speech Processing , Johns Hopkins University .","label":"Background","metadata":{},"score":"69.88742"}{"text":"I will then attempt to encourage others to study such problems and explain why I believe logical approaches have the most to offer at the level of producing semantic interpretations of complete sentences . by Diana Maynard , Spohia Ananiadou - In Proc . of 1st","label":"Background","metadata":{},"score":"69.996376"}{"text":"The memory - based learning ( TiMBL ) was used .The input for the learner was a feature vector consisting of 100 features .The training data was taken i)from the syntactic examples in the dictionary and ii)the training corpus .","label":"Background","metadata":{},"score":"70.14992"}{"text":"organisation : Computer Science Department and Center for Language and Speech Processing , Johns Hopkins University .Task / s : Spanish lexical choice , Swedish lexical choice , Basque lexical choice .Did you use any training data provided in an automatic training procedure ?","label":"Background","metadata":{},"score":"70.17805"}{"text":"The tagging was only done with those features , getting a precision close to 0.85 at the cost of losing coverage .keywords : Supervised learning , decision lists , agglutinative languages , feature selection .organisation : Dept of Computer and Information Science , Linkping University .","label":"Background","metadata":{},"score":"70.35849"}{"text":"2 vols .New York : Cambridge University Press [ 16 ] Manning , C. D. and Schutze , H. 2001 .Foundations of statis- tical natural language processing .( Cambridge , Messachu- setts : MIT Press ) [17 ] Miller , G. A. , Beckwith , R. , Fellbaum , C. , and Gross , D. , 1993 .","label":"Background","metadata":{},"score":"70.380875"}{"text":"This result shows that translite- ration of Arabic numerals is still very challenging .The identification of numerals becomes more complicated due to the high error rate in using white space in Korean texts [ 12].However , few studies have focused on Korean text normalization that incorporates sufficient analysis on the characteristics of Arab- ic numerals and text symbols .","label":"Background","metadata":{},"score":"71.00821"}{"text":"Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : We used SVM , PCA and ICA for learning .","label":"Background","metadata":{},"score":"71.316635"}{"text":"The following 2 words that show more than 5 by MI and more than 100 by selected as contextual features for the disambiguation of ' dae ' .( Ex 3 ' ' )a. taegsi , beoseu , seunghabcha b. mihonnam , namja , yeoja c. jeonje , jogeon .","label":"Background","metadata":{},"score":"71.516716"}{"text":"( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description : We used a machine learning technique for constructing the WSD system .","label":"Background","metadata":{},"score":"72.17294"}{"text":"The latter problem is called sample selection bias in econometrics .Our solution to it is based on Nobel prize - winning work due to the economist James Heckman .We show that the methods we propose are s .. K + 98 ] , whosnd that performing no pruning and variants of pruning adapted to loss minimization both lead to similar performance .","label":"Background","metadata":{},"score":"72.30539"}{"text":"keywords : Semantic Classification Tree , Semantic Classes , Cosine Similarity Measure .name : Marc El - Beze email : marc.elbeze@lia.univ-avignon.fr organisation : Laboratoire Informatique d'Avignon .Task / s ( e.g. English all words ) : English all words .","label":"Background","metadata":{},"score":"72.40928"}{"text":"No .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ? :Yes .Description : The system tries to exploit the idea of \" Domain Driven Disambiguation \" .","label":"Background","metadata":{},"score":"72.82106"}{"text":"Article texts of Newspaper B have fewer RFA variations and fewer grammatical errors than those of Newspaper A. Since Newspaper B offers its daily news reading ( TTS ) service on its website , the articles are text normalized in order to be used as source texts for the TTS system .","label":"Background","metadata":{},"score":"72.87825"}{"text":"Parse each input and translation record , and generate a \" case frame \" for each 2 .Return the translation record for which the most case slot matches are produced , breaking ties according to the overall quality of match .","label":"Background","metadata":{},"score":"73.36584"}{"text":"Our results are not always in line with some practices in the field .For example , we show that omitting noncontent words decreases performance and that bigrams yield better results than unigrams . by John P. Pestian , Lukasz Itert , Preferred Affiliation , Charlotte Andersen , Preferred Affiliation , Wlodzislaw Duch , Preferred Affiliation . \" ...","label":"Background","metadata":{},"score":"73.752396"}{"text":"Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Description ( 250 words max ) : We used a machine learning technique for constructing the WSD system .","label":"Background","metadata":{},"score":"73.9709"}{"text":"Table 7 .Structure of Test Data Sets Data set Size Source data Sampling method Random Balanced Random Balanced Random Set 1 Set 2 Set 3 Set 4 Set 5 10,000 words 1,000 words 1,000 words 1,000 words 1,000 words Ten newspapers Newspaper A Newspaper A Newspaper B Newspaper B .","label":"Background","metadata":{},"score":"74.42372"}{"text":"( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?N .Description : A commercial MT system was utilized as it was .","label":"Background","metadata":{},"score":"74.51842"}{"text":"It is usually composed of the word 's lemma and a sense circumscription of one or two words , often using a related term ( drogen_nat , \" dry_wet \" ) or a reference of the grammatical category ( fiets_N , fietsen_V ) .","label":"Background","metadata":{},"score":"74.53238"}{"text":"( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Yes .Description : Sussex - sel - ospd - ana parses the plain text to identify subject / verb and verb / direct object relationships , and the nouns and verbs involved are disambiguated using class - based selectional preferences acquired from unsupervised training .","label":"Background","metadata":{},"score":"74.74823"}{"text":"Our decoder achieves an F1 correction score significantly higher than all previous published scores on the Helping Our Own ( HOO ) shared task data set . ... odels .We crawl all non - OCR documents from the anthology , except those documents that overlap with the HOO data .","label":"Background","metadata":{},"score":"75.91672"}{"text":"Rather than learned a bagged decision tree ( as duluth10 does ) this system simply learns a decision stump , a one node decision tree .The features used are the same as duluth10 .This system provides a baseline that can be used to compare the benefits of learning an entire decision tree ( duluth10 ) versus identifying a single node tree ( duluthB ) .","label":"Background","metadata":{},"score":"75.94211"}{"text":"Uppsala , Sweden ; Click here The high frequency of the use of Arabic numerals in informative texts and their multiple senses and readings deteriorate the accuracy of TTS systems .This paper presents a hybrid word sense disambiguation method exploiting a tagged corpus and a Korean wordnet , KorLex 1.0 , for the correct and efficient conversion of Arabic numerals into Korean phonemes according to their senses .","label":"Background","metadata":{},"score":"76.44806"}{"text":"N .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?Yes .Description : Sussex - sel was applied to the all - words task , but processed only the plain text , ignoring the supplied bracketings .","label":"Background","metadata":{},"score":"77.01901"}{"text":"keywords : selectional preferences , grammatical relations , one sense per discourse .organisation : The University of Sussex and the University of Sheffield .Task / s : English All Words .Did you use any training data provided in an automatic training procedure ?","label":"Background","metadata":{},"score":"77.282"}{"text":"Collins ( 1999 ) presents three lexicalized models which consider long - distance dependencies within a sentence .Grammar productions are conditioned on Bob Carpenter Alias I , Inc.Brooklyn , NY , USA ca ... . \" ...This article describes the results of a systematic indepth study of the criteria used for word sense disambiguation .","label":"Background","metadata":{},"score":"78.45256"}{"text":"Rather than learned a bagged decision tree ( as duluth5 does ) this system simply learns a decision stump , a one node decision tree .The features used are the same as duluth5 .This system provides a baseline that can be used to compare the benefits of learning an entire decision tree ( duluth5 ) versus identifying a single node tree ( duluthB ) .","label":"Background","metadata":{},"score":"80.67702"}{"text":"WSD of homographic classifiers with the application of Kor- Lex 1.0 obtained further improvement , showing 87.3 % accuracy in resolving the ambiguities of homographic classifiers and pre- dicting the correct RFA .In order to make a close analysis of the performance improve- ment produced by the WSD method with the application of Kor- Lex , the accuracy of WSD with the application of the Korean Synonym Dictionary was measured .","label":"Background","metadata":{},"score":"80.855545"}{"text":"The induction method is nearly linear in time relative to an increase in the number of induced rules or the number of cases .Experimental results on large benchmark data sets demonstrate that predictive performance can rival the best reported results in the literature . by Kai Ming Ting - In : Proceedings of the Eleventh European Conference on Machine Learning . \" ...","label":"Background","metadata":{},"score":"81.4459"}{"text":"System is created and developed by Kaarel Kaljurand .keywords : wordnet - based , conceptual density .URL containing additional information : Click here - in Estonian , sorry !Did you use any training data provided in an automatic training procedure ?","label":"Background","metadata":{},"score":"81.64461"}{"text":"URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .email : tpederse@d.umn.edu organisation : University of Minnesota Duluth .Task : English Lexical Sample .","label":"Background","metadata":{},"score":"81.73236"}{"text":"URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .email : tpederse@d.umn.edu organisation : University of Minnesota Duluth .Task : Spanish Lexical Sample .","label":"Background","metadata":{},"score":"82.10729"}{"text":"URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .Did you use any training data provided in an automatic training procedure ?YES .","label":"Background","metadata":{},"score":"82.3509"}{"text":"URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .Did you use any training data provided in an automatic training procedure ?YES .","label":"Background","metadata":{},"score":"82.3509"}{"text":"URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .Did you use any training data provided in an automatic training procedure ?YES .","label":"Background","metadata":{},"score":"82.3509"}{"text":"URL containing additional information : Complete source code and documentation for this system will be available by the end of August 2001 at : this site .Did you use any training data provided in an automatic training procedure ?YES .","label":"Background","metadata":{},"score":"82.3509"}{"text":"Did you use any training data provided in an automatic training procedure ?YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"82.75589"}{"text":"Did you use any training data provided in an automatic training procedure ?YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"82.75589"}{"text":"Did you use any training data provided in an automatic training procedure ?YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"82.75589"}{"text":"Did you use any training data provided in an automatic training procedure ?YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"82.75589"}{"text":"Did you use any training data provided in an automatic training procedure ?Yes .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"82.75589"}{"text":"Did you use any training data provided in an automatic training procedure ?Yes .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"82.75589"}{"text":"Did you use any training data provided in an automatic training procedure ?YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"82.75589"}{"text":"Did you use any training data provided in an automatic training procedure ?YES .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"82.75589"}{"text":"Did you use any training data provided in an automatic training procedure ?Yes .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"82.75589"}{"text":"Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"83.8584"}{"text":"Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"83.8584"}{"text":"Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"83.8584"}{"text":"Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"83.8584"}{"text":"Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"83.8584"}{"text":"Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"83.8584"}{"text":"Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"83.8584"}{"text":"Did you use any training data provided in an automatic training procedure ?N .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"84.253044"}{"text":"Did you use any training data provided in an automatic training procedure ?N .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"84.253044"}{"text":"Did you use any training data provided in an automatic training procedure ?N .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"84.253044"}{"text":"Did you use any training data provided in an automatic training procedure ?N .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"84.253044"}{"text":"Though Auto - TAN shows superior performance to those two customized systems , it needs to be improved .Auto - TAN applied a refined - however , in- correct-rule , \" If an Arabic numeral combined with Roman alpha- bet , then RFA is Brn \" in reading \" 21 \" of \" BK21 \" .","label":"Background","metadata":{},"score":"84.66491"}{"text":"The annotations include radiology reports , discharge summaries , and surgical and nursing notes .Hospitals typically produce millions of text - based medical records over the course of a year .These records a ... \" .Approximately 57 different types of clinical annotations construct a patient 's medical record .","label":"Background","metadata":{},"score":"84.69803"}{"text":"name : Marc El - Beze email : marc.elbeze@lia.univ-avignon.fr organisation : Laboratoire Informatique d'Avignon .Did you use any training data provided in an automatic training procedure ?Yes .Description ( 250 words max ) : The approach used in the Senseval-2 campaign is based on a multi - level view of the context .","label":"Background","metadata":{},"score":"85.86577"}{"text":"We present in this paper empirical results on the identification of strong chain and of significant sentences . ... olumbia University , Cornell University , and Beer - Sheva University in Israel .Percent agreement is the ratio of observed agreements with the majority opinion to possi ... . \" ...","label":"Background","metadata":{},"score":"86.42103"}{"text":"name : David Yarowsky , Silviu Cucerzan , Radu Florian , Charles Schafer and Richard Wicentowski . jhu.edu .organisation : Computer Science Department and Center for Language and Speech Processing , Johns Hopkins University .Task / s : Czech all words .","label":"Background","metadata":{},"score":"86.68892"}{"text":"This paper describes the submission of the National University of Singapore ( NUS ) to the Helping Our Own ( HOO ) Pilot Shared Task .Our system targets spelling , article , and preposition errors in a sequential processing pipeline . ... the HOO data4 .","label":"Background","metadata":{},"score":"87.625206"}{"text":"Hospitals typically produce millions of text - based medical records over the course of a year .These records are essential for the delivery of care , but many are underutilized or not utilized at all for clinical research .The textual data found in these annotations is a rich source of insights into aspects of clinical care and the clinical delivery system .","label":"Background","metadata":{},"score":"89.24252"}{"text":"korea.ac.kr .Did you use any training data provided in an automatic training procedure ?Y .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"89.83356"}{"text":"Accuracy ( % ) 73.2 85.2 87.3 81.8 5 .EXPERIMENTS AND RESULTS In this Section , the comparison of the accuracy of the Auto- TAN adopting the proposed WSD method and that of two com- mercial TTS systems is presented experimentally .","label":"Background","metadata":{},"score":"91.5867"}{"text":"Prospective informal - formal pairs are further classified by a supervised binary classifier to identify correct pairs .In the classification model , we incorporate both rule - based and statistical fea ... . \" ...This paper describes the submission of the National University of Singapore ( NUS ) to the Helping Our Own ( HOO ) Pilot Shared Task .","label":"Background","metadata":{},"score":"93.63934"}{"text":"Chicago .Lefever , Els . \" ParaSense : Parallel Corpora for Word Sense Disambiguation \" .Ghent , Belgium : Ghent University .Faculty of Sciences .APA .Lefever , E. ( 2012 ) .ParaSense : parallel corpora for word sense disambiguation .","label":"Background","metadata":{},"score":"94.43332"}{"text":"This article describes a practical approach with which Cincinnati Children 's Hospital Medical Center ( CCHMC ) , a large pediatric academic medical center with more than 761,000 annual patient encounters , developed open source software for making pediatric clinical text harmless without losing its rich meaning .","label":"Background","metadata":{},"score":"94.64102"}{"text":"email : Yvonne.Canning@sunderland.ac.uk , Michael.Oakes@sunderland.ac.uk , John.Tait@sunderland.ac.uk .organisation : The University of Sunderland .Did you use any training data provided in an automatic training procedure ?N .( if the answer to ( 4 ) is no ) did you use any training data provided in any way ( eg as a test set for debugging ) ?","label":"Background","metadata":{},"score":"95.49606"}{"text":"am/ of ' 3 ' are Table 8 .Comparison of Accuracy of Auto - TAN , VoiceWare , and CoreVoice Systems Systems Set 1 Set 2 Auto - TAN 99.1 97.7 VoiceWare 87.8 86.1 CoreVoice 87.1 88.7 Set 3 95.9 79.4 78.8 Set 4 97.3 91.7 87.8 Set 5 95.6 83.8 82.9 .","label":"Background","metadata":{},"score":"97.22102"}{"text":"[ Show abstract ] [ Hide abstract ] ABSTRACT : India is multilingual country , in India people speak different language , and they also used different ways to write number text .Due to lack of language knowledge , people are not able to read number text from one language to another language .","label":"Background","metadata":{},"score":"107.45203"}{"text":"Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .Publisher conditions are provided by RoMEO .","label":"Background","metadata":{},"score":"107.984634"}{"text":"Faculty of Sciences , Ghent , Belgium .Lefever E. ParaSense : parallel corpora for word sense disambiguation .[ Ghent , Belgium ] : Ghent University .Faculty of Sciences ; 2012 .MLA .Lefever , Els . \" ParaSense : Parallel Corpora for Word Sense Disambiguation .","label":"Background","metadata":{},"score":"111.24384"}