{"text":"In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning .In particular , we study the task of morphological segmentation of multiple languages .We present a nonparametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross - lingual morpheme patterns , or abstract morphemes .","label":"CompareOrContrast","metadata":{},"score":"28.888428"}{"text":"In this work , we propose a novel stacked subword model for this task , concerning both efficiency and effectiveness .Our solution is a two step process .First , one word - based segmenter , one character - based segmenter and one local character classifier are trained to produce coarse segmentation and POS information .","label":"CompareOrContrast","metadata":{},"score":"30.83599"}{"text":"We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .We apply the framework to word segmentation , joint segmentation and POStagging , dependency parsing , and phrase - structure parsing .","label":"CompareOrContrast","metadata":{},"score":"32.086437"}{"text":"We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .We apply the framework to word segmentation , joint segmentation and POStagging , dependency parsing , and phrase - structure parsing .","label":"CompareOrContrast","metadata":{},"score":"32.086437"}{"text":"We present a scalable joint language model designed to utilize fine - grain syntactic tags .We discuss challenges such a design faces and describe our solutions that scale well to large tagsets and corpora .We advocate the use of relatively simple tags that do not require deep linguistic knowledge of ... \" .","label":"CompareOrContrast","metadata":{},"score":"35.787148"}{"text":"The system consists of two components : an unlabeled dependency parser using Gibbs sampling which can incorporate sentence - level ( global ) features as well as token - leve ... \" .In this paper , we describe a two - stage multilingual dependency parser used for the multilingual track of the CoNLL 2007 shared task .","label":"CompareOrContrast","metadata":{},"score":"36.025173"}{"text":"The system consists of two components : an unlabeled dependency parser using Gibbs sampling which can incorporate sentence - level ( global ) features as well as token - leve ... \" .In this paper , we describe a two - stage multilingual dependency parser used for the multilingual track of the CoNLL 2007 shared task .","label":"CompareOrContrast","metadata":{},"score":"36.025173"}{"text":"Finally , we present multilingual experiments which show that parsing with hierarchical state - splitting is fast and accurate in multiple languages and domains , even without any language - specific tuning .This work describes systems for detecting semantic categories present in news video .","label":"CompareOrContrast","metadata":{},"score":"36.264732"}{"text":"Unlike previous work , our final model does not require any additional resources at run - time .Compared to a state - of - the - art approach , we achieve more than 20 % relative error reduction .Additionally , we annotate a corpus of search queries with part - of - speech tags , providing a resource for future work on syntactic query analysis .","label":"CompareOrContrast","metadata":{},"score":"37.235214"}{"text":"In addition , our discriminative approach integrally admits features beyond local tree configurations .We present a multi - scale training method along with an efficient CKY - style dynamic program .On a variety of domains and languages , this method produces the best published parsing accuracies with the smallest reported grammars .","label":"CompareOrContrast","metadata":{},"score":"37.294308"}{"text":"We show that the automatically induced latent variable grammars of Petrov et al .( 2006 ) vary widely in their underlying representations , depending on their EM initialization point .We use this to our advantage , combining multiple automatically learned grammars into an unweighted product model , which ... \" .","label":"CompareOrContrast","metadata":{},"score":"37.470837"}{"text":"In particular , we introduce set - valued features to encode the predicted morphological properties and part - of - speech confusion sets of the words being parsed .We also investigate the use of joint parsing and part - of - speech tagging in the neural paradigm .","label":"CompareOrContrast","metadata":{},"score":"37.622253"}{"text":"We demonstrate that this joint labeling approach , by enabling information sharing between tagging / chunking subtasks , outperforms the traditional method of tagging and chunking in succession .Further , we extend this into a novel model , Switching FHMM , to allow for explicit modeling of cross - sequence dependencies based on linguistic knowledge .","label":"CompareOrContrast","metadata":{},"score":"37.832138"}{"text":"We discuss challenges such a design faces and describe our solutions that scale well to large tagsets and corpora .We propose two fine - grain tagsets and evaluate our model using these tags , as well as POS tags and SuperARV tags in a speech recognition task and discuss future directions . ... and time constraints .","label":"CompareOrContrast","metadata":{},"score":"37.83803"}{"text":"Participants were to build a single parsing system that is robust to domain changes and can handle noisy text that is commonly encountered on the web .There was a constituency and a dependency parsing track and 11 sites submitted a total of 20 systems .","label":"CompareOrContrast","metadata":{},"score":"38.069664"}{"text":"To train on unlabele ... \" .Conditional random fields ( Lafferty et al . , 2001 ) are quite effective at sequence labeling tasks like shallow parsing ( Sha and Pereira , 2003 ) and namedentity extraction ( McCallum and Li , 2003 ) .","label":"CompareOrContrast","metadata":{},"score":"38.516846"}{"text":"This paper presents novel improvements to the induction of translation lexicons from monolingual corpora using multilingual dependency parses .We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative ... \" .","label":"CompareOrContrast","metadata":{},"score":"38.74954"}{"text":"This paper presents novel improvements to the induction of translation lexicons from monolingual corpora using multilingual dependency parses .We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative ... \" .","label":"CompareOrContrast","metadata":{},"score":"38.74954"}{"text":"In this work , we focus on prosodic breaks .Based on decision trees , ... \" ...Conditional random fields ( Lafferty et al . , 2001 ) are quite effective at sequence labeling tasks like shallow parsing ( Sha and Pereira , 2003 ) and namedentity extraction ( McCallum and Li , 2003 ) .","label":"CompareOrContrast","metadata":{},"score":"38.915897"}{"text":"Previous sentence segmentation systems have typically been very local , using low - level prosodic and lexical features to independently decide whether or not to segment at each word boundary position .In this work , we leverage global syntactic information from a syn- tactic parser , which is better able to capture long distance depen- dencies .","label":"CompareOrContrast","metadata":{},"score":"38.92806"}{"text":"Instead , the phylogenetic prior couples languages at a parameter level .Joint induction in the multilingual model substantially outperforms independent learning , with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages .Across eight languages , the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1 % and reaching as high as 39 % . ... inally given as simple multinomial distributions with one parameter per outcome . \" ...","label":"CompareOrContrast","metadata":{},"score":"39.108658"}{"text":"Unlike previous work on projecting syntactic resources , we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers .The projected parsers from our system result in state - of - the - art performance when compared to previously studied unsupervised and projected parsing systems across eight different languages .","label":"CompareOrContrast","metadata":{},"score":"39.169056"}{"text":"We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part - of - speech tagging , chunking , named entity recognition , and semantic role labeling .This versatility is achieved by trying to avoid task - specific eng ... \" .","label":"CompareOrContrast","metadata":{},"score":"39.231594"}{"text":"Our method does not require any translated texts or token - level alignments .Instead , the phylogenetic prior couples languages at a parameter level .Joint induction in the multiling ... \" .We present an approach to multilingual grammar induction that exploits a phylogeny - structured model of parameter drift .","label":"CompareOrContrast","metadata":{},"score":"39.336544"}{"text":"Our results demonstrate that learning morphological models in tandem reduces error by up to 24 % relative to monolingual models .Furthermore , we provide evidence that our joint model achieves better performance when applied to languages from the same family . ...","label":"CompareOrContrast","metadata":{},"score":"39.46266"}{"text":"So the M - estimator is much faster to train .Sutton and McCallum ( 2005 ) present approximate methods that keep a discriminative objective while avoiding full inference .We see M - estimation as a particularly promising method in settings where p ..","label":"CompareOrContrast","metadata":{},"score":"39.61184"}{"text":"We run a base tagger with different random initializations , and select the best tagging using the quality test .As a base tagger , we modify a leading unsupervised POS tagger ( Clark , 2003 ) to constrain the distributions of word types across clusters to be Zipfian , allowing us to utilize a perplexity - based quality test .","label":"CompareOrContrast","metadata":{},"score":"39.709274"}{"text":"Second , how can we efficiently infer optimal structures within them ?Hierarchical coarse - to - fine methods address both questions .Coarse - to - fine approaches exploit a sequence of models which introduce complexity gradually .At the top of the sequence is a trivial model in which learning and inference are both cheap .","label":"CompareOrContrast","metadata":{},"score":"39.837906"}{"text":"The experiments also highlight the benefits of our data selection method .Attempts have also been made to extend beyond n - gram dependencies by exploiting ( hidden ) syntax structure ( Chelba and Jelinek , 2000 ) and semantic or topical dependencies ( Khudanpur and Wu , 2000 ) .","label":"CompareOrContrast","metadata":{},"score":"39.8757"}{"text":"To achieve these results we need to mitigate the lack of domain knowledge in the model by providing it with a large amount of automatically parsed data .We extend and improve upon recent work in structured training for neural network transition - based dependency parsing .","label":"CompareOrContrast","metadata":{},"score":"39.964615"}{"text":"We present a method that performs joint decoding of separately trained Conditional Random Field ( CRF ) models , while guarding against violations of hard - constraints .Evaluated on Chinese word segmentation and part - of - speech ( POS ) tagging tasks , our proposed method achieved state - of - the - art performance on both the Penn Chinese Treebank and First SIGHAN Bakeoff datasets .","label":"CompareOrContrast","metadata":{},"score":"40.143093"}{"text":"Combining multiple grammars that were self - trained on disjoint sets of unlabeled data results in a final test accuracy of 92.5\\% on the WSJ test set and 89.6\\% on our Broadcast News test set .This work shows how to improve state - of - the - art monolingual natural language processing models using unannotated bilingual text .","label":"CompareOrContrast","metadata":{},"score":"40.247448"}{"text":"We show that the automatically induced latent variable grammars of Petrov et al .2006 vary widely in their underlying representations , depending on their EM initialization point .We use this to our advantage , combining multiple automatically learned grammars into an unweighted product model , which gives significantly improved performance over state - of - the - art individual grammars .","label":"CompareOrContrast","metadata":{},"score":"40.59852"}{"text":"We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative improvement over the baseline approach that uses a fixed context window of adjacent words .","label":"CompareOrContrast","metadata":{},"score":"40.777443"}{"text":"We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative improvement over the baseline approach that uses a fixed context window of adjacent words .","label":"CompareOrContrast","metadata":{},"score":"40.777443"}{"text":"( 2006 ) vary widely in their underlying representations , depending on their EM initialization point .We use this to our advantage , combining multiple automatically learned grammars into an unweighted product model , which gives significantly improved performance over state - ofthe - art individual grammars .","label":"CompareOrContrast","metadata":{},"score":"40.91246"}{"text":"We discuss how the general framework is applied to each of the problems studied in this article , making comparisons with alternative learning and decoding algorithms .We also show how the comparability of candidates considered by the beam is an important factor in the performance .","label":"CompareOrContrast","metadata":{},"score":"41.240696"}{"text":"We discuss how the general framework is applied to each of the problems studied in this article , making comparisons with alternative learning and decoding algorithms .We also show how the comparability of candidates considered by the beam is an important factor in the performance .","label":"CompareOrContrast","metadata":{},"score":"41.240696"}{"text":"These experiments help tease apart the contributions of rich features and discriminative training , which are shown to be more than additive . ...e model .So the M - estimator is much faster to train .Sutton and McCallum ( 2005 ) present approximate inference methods that aim to keep a discriminative objective while avoiding the full inference required by maximum conditional likelihood .","label":"CompareOrContrast","metadata":{},"score":"41.404495"}{"text":"We compare different network construction techniques and clustering algorithms based on the cohesiveness of the word clusters .Cohesiveness is measured against two gold - standard tagsets by means of the novel metric of tag - entropy .The approach presented here is a generic one that can be easily extended to any language . han morphological form .","label":"CompareOrContrast","metadata":{},"score":"41.44895"}{"text":"The proposed method does not rely on any prior knowledge , and the performance can be improved iteratively .The parallel sentence extraction method uses a binary classifier for parallel sentence identification .The extracted bilingual lexicons are used for the classifier to improve the performance of parallel sentence extraction .","label":"CompareOrContrast","metadata":{},"score":"41.472565"}{"text":"In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"CompareOrContrast","metadata":{},"score":"41.475166"}{"text":"In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"CompareOrContrast","metadata":{},"score":"41.475166"}{"text":"Experimental results show that the global features are useful in all the languages . ... mines unlabeled dependency structures only , and we attach dependency relation labels using Support Vector Machines afterwards . \" ...We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .","label":"CompareOrContrast","metadata":{},"score":"41.576206"}{"text":"Experimental results show that the global features are useful in all the languages . ... mines unlabeled dependency structures only , and we attach dependency relation labels using Support Vector Machines afterwards . \" ...We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .","label":"CompareOrContrast","metadata":{},"score":"41.576206"}{"text":"It surveys current research in this area , giving an overview of the state of the art and outlining the open problems .The survey covers transfer in both inductive learning and reinforcement learning , and discusses the issues of negative transfer and task mapping in depth . .","label":"CompareOrContrast","metadata":{},"score":"41.8992"}{"text":"Since most of these systems were developed and tested using data from the WSJ corpus , we compare their generalization abilities by testing on both WSJ and the multilingual Multext - East corpus .Finally , we introduce the idea of evaluating systems based on their ability to produce cluster prototypes that are useful as input to a prototype - driven learner .","label":"CompareOrContrast","metadata":{},"score":"41.978733"}{"text":"We describe a new loss function , due to Jeon and Lin ( 2006 ) , for estimating structured log - linear models on arbitrary features .The loss function can be seen as a ( generative ) alternative to maximum likelihood estimation with an interesting information - theoretic interpretation , and it is statistical ... \" .","label":"CompareOrContrast","metadata":{},"score":"42.01055"}{"text":"The algorithm uses a similarity graph to encourage similar n - grams to have similar POS tags .We demonstrate the efficacy of our approach on a domain adaptation task , where we assume that we have access to large amounts of unlabeled data from the target domain , but no additional labeled data .","label":"CompareOrContrast","metadata":{},"score":"42.309097"}{"text":"The coarse - to - fine search scheme is efficient , while in the sub - word tagging step rich contextual features can be approximately derived .Evaluation on the Penn Chinese Treebank shows that our model yields improvements over the best system reported in the literature .","label":"CompareOrContrast","metadata":{},"score":"42.4236"}{"text":"Therefore , reducing dependency parsing errors and selecting high quality dependencies is of primary importance .In this study , we present a language - independent approach for automatically selecting high quality dependencies from automatic parses .By considering several aspects that affect the accuracy of dependency parsing , we created a set of features for supervised classification of reliable dependencies .","label":"CompareOrContrast","metadata":{},"score":"42.494827"}{"text":"We highlight the use of this resource via two experiments , including one that reports competitive accuracies for unsupervised grammar induction without gold standard part - of - speech tags .We present an online learning algorithm for training structured prediction models with extrinsic loss functions .","label":"CompareOrContrast","metadata":{},"score":"42.52743"}{"text":"In this paper , we present a multitask learning ( MTL ) method for intent classification in goal oriented human - machine spoken dialog systems .MTL aims at training tasks in parallel while using a shared representation .What is learned for each task can help other tasks be learned better .","label":"CompareOrContrast","metadata":{},"score":"42.620193"}{"text":"In this paper , we present a multitask learning ( MTL ) method for intent classification in goal oriented human - machine spoken dialog systems .MTL aims at training tasks in parallel while using a shared representation .What is learned for each task can help other tasks be learned better .","label":"CompareOrContrast","metadata":{},"score":"42.620193"}{"text":"Syntactic parsing is a fundamental problem in computational linguistics and natural language processing .Traditional approaches to parsing are highly complex and problem specific .Recently , Sutskever et al .( 2014 ) presented a task - agnostic method for learning to map input sequences to output sequences that achieved strong results on a large scale machine translation problem .","label":"CompareOrContrast","metadata":{},"score":"42.77996"}{"text":"We then adapt these models to the problem of transfer learning for protein name extraction .In the process , we introduce a novel maximum entropy based technique , Iterative Feature Transformation ( IFT ) , and show that it achieves comparable performance with state - of - the - art transductive SVMs .","label":"CompareOrContrast","metadata":{},"score":"43.00505"}{"text":"We also show that our techniques can be applied to full - scale parsing applications by demonstrating its effectiveness in learning state - split grammars .Treebank parsing can be seen as the search for an optimally refined grammar consistent with a coarse training treebank .","label":"CompareOrContrast","metadata":{},"score":"43.486923"}{"text":"For example , noun phrases might be split into subcategories for subjects and objects , singular and plural , and so on .This splitting process admits an efficient incremental inference scheme which reduces parsing times by orders of magnitude .Furthermore , it produces the best parsing accuracies across an array of languages , in a fully language - general fashion .","label":"CompareOrContrast","metadata":{},"score":"43.488766"}{"text":"Our method does not assume any knowledge about the target language ( in particular no tagging dictionary is assumed ) , making it applicable to a wide array of resource - poor languages .We use graph - based label propagation for cross - lingual knowledge transfer and use the projected labels as features in an unsupervised model ( Berg - Kirkpatrick et al .","label":"CompareOrContrast","metadata":{},"score":"43.48944"}{"text":"Our first- , second- , and third - order models achieve accuracies comparable to those of their unpruned counterparts , while exploring only a fraction of the search space .We observe speed - ups of up to two orders of magnitude compared to exhaustive search .","label":"CompareOrContrast","metadata":{},"score":"43.533356"}{"text":"For centuries , the deep connection between languages has brought about major discoveries about human communication .In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning .In particular , we study the task of morphological segmentation ... \" .","label":"CompareOrContrast","metadata":{},"score":"43.621246"}{"text":"We propose a joint language mod ... . by John Blitzer , Amir Globerson , Fernando Pereira - IN PROCEEDINGS OF THE INTERNATIONAL WORKSHOP ON ARTIFICIAL INTELLIGENCE AND STATISTICS , 2005 . \" ...Low - dimensional representations for lexical co - occurrence data have become increasingly important in alleviating the sparse data problem inherent in natural language processing tasks .","label":"CompareOrContrast","metadata":{},"score":"43.690426"}{"text":"A mixture grammar fit with the EM algorithm shows improvement over a single PCFG , both in parsing accuracy and in test data likelihood .We argue that this improvement comes from the learning of specialized grammars that capture non - local correlations .","label":"CompareOrContrast","metadata":{},"score":"43.71157"}{"text":"We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al ., 2010a ... \" .Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .","label":"CompareOrContrast","metadata":{},"score":"44.150944"}{"text":"The target task might use entire source - task solutions as parts of its own , or it might use them in a more subtle way to improve learning .They then proceed to turn the problem into a multi - task learning problem by combi ...","label":"CompareOrContrast","metadata":{},"score":"44.271103"}{"text":"In this work we address the problem of unsupervised part - of - speech induction by bringing together several strands of research into a single model .We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman - Yor processes prior , providing an elegant and principled means of incorporating lexical characteristics .","label":"CompareOrContrast","metadata":{},"score":"44.36142"}{"text":"We present experiments with sequence models on part - of - speech tagging and named entity recognition tasks , and with syntactic parsers on dependency parsing and machine translation reordering tasks .Low - latency solutions for syntactic parsing are needed if parsing is to become an integral part of user - facing natural language applications .","label":"CompareOrContrast","metadata":{},"score":"44.416996"}{"text":"While they perform well in many categorization tasks , these methods are inherently limited when faced with more complicated tasks where external knowledge is essential .Recently , there have been efforts to augment these basic features with external knowledge , including semi - supervised learning and transfer learning .","label":"CompareOrContrast","metadata":{},"score":"44.418312"}{"text":"Starting from a mono - phone model , we learn increasingly refined models that capture phone internal structures , as well as context - dependent variations in an automatic way .Our approaches reduces error rates compared to other baseline approaches , while streamlining the learning procedure .","label":"CompareOrContrast","metadata":{},"score":"44.419548"}{"text":"( 1993 ) .In our model , arbitrary , nonindependent features may be freely incorporated , thereby overcoming the inherent limitation of generative models , which requ ... \" .We introduce a discriminatively trained , globally normalized , log - linear variant of the lexical translation models proposed by Brown et al .","label":"CompareOrContrast","metadata":{},"score":"44.44579"}{"text":"First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar without a treebank .In our experiments , hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy .","label":"CompareOrContrast","metadata":{},"score":"44.743156"}{"text":"The beam - search decoder only requires the syntactic processing task to be broken into a sequence of decisions , such that , at each stage in the process , the decoder is able to consider the top - n candidates and generate all possibilities for the next stage .","label":"CompareOrContrast","metadata":{},"score":"45.208794"}{"text":"The beam - search decoder only requires the syntactic processing task to be broken into a sequence of decisions , such that , at each stage in the process , the decoder is able to consider the top - n candidates and generate all possibilities for the next stage .","label":"CompareOrContrast","metadata":{},"score":"45.208794"}{"text":"Unlike existing preordering models , we train feature - rich discriminative classifiers that directly predict the target - side word order .Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long - distance reorderings using the structure of the parse tree , while utilizing a discriminative model with a rich set of features , including lexical features .","label":"CompareOrContrast","metadata":{},"score":"45.32656"}{"text":"Our approach enhances machine learning algorithms with features generated from domain - specific and common - sense knowledge .This knowledge is represented by ontologies that contain hundreds of thousands of concepts , further enriched through controlled Web crawling .Prior to text categorization , a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning .","label":"CompareOrContrast","metadata":{},"score":"45.39457"}{"text":"In this paper we address the subproblem of domain adaptation , in which a model trained over a source domain is generalized to perform well on a related target domain , where these two domains ' data are distributed similarly , but not identically .","label":"CompareOrContrast","metadata":{},"score":"45.528244"}{"text":"We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .We propose using corpus - level statistics for lexicon learning decisions .","label":"CompareOrContrast","metadata":{},"score":"45.80579"}{"text":"The ... \" .Low - dimensional representations for lexical co - occurrence data have become increasingly important in alleviating the sparse data problem inherent in natural language processing tasks .This work presents a distributed latent variable model for inducing these low - dimensional representations .","label":"CompareOrContrast","metadata":{},"score":"45.960464"}{"text":"We describe a new loss function , due to Jeon and Lin ( 2006 ) , for estimating structured log - linear models on arbitrary features .The loss function can be seen as a ( generative ) al - ternative to maximum likelihood estimation with an interesting information - theoretic in - terpretation , and it is statistically consis - tent .","label":"CompareOrContrast","metadata":{},"score":"46.070004"}{"text":"Across various hierarchical encoding schemes and for multiple language pairs , we show speed - ups of up to 50 times over single - pass decoding while improving BLEU score .Moreover , our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram - to - trigram decoder .","label":"CompareOrContrast","metadata":{},"score":"46.11435"}{"text":"Our methodology also relaxes the requirement for each ad to explicitly specify the exhaustive list of queries ( \" bid phrases \" ) that can trigger it . ... define new features for representing queries and ads .This approach is related to transfer learning , where knowledge learned in one domain is transferred to another domain .","label":"CompareOrContrast","metadata":{},"score":"46.117004"}{"text":"When Brown et al .( 1993 ) wan ... . \" ...We consider a new subproblem of unsupervised parsing from raw text , unsupervised partial parsing - the unsupervised version of text chunking .We show that addressing this task directly , using probabilistic finite - state methods , produces better results than relying on the local predictions of a current ... \" .","label":"CompareOrContrast","metadata":{},"score":"46.50632"}{"text":"We create a semi - supervised discriminative parser that combines edge and span features .Experiments show that span features improve accuracy and that further gain is obtained when they are combined with edge features .[ Show abstract ] [ Hide abstract ] ABSTRACT : This article proposes a predicate - argument structure based Textual Entailment Recognition system exploiting wide - coverage lexical knowledge .","label":"CompareOrContrast","metadata":{},"score":"46.563263"}{"text":"With transfer learning , one set of tasks is used to bias learning and improve performance on another task .However , transfer learning may actually hinder performance if the tasks are too dissimilar .As described in this paper , one challenge for transfer learning research is to develop approaches tha ... \" .","label":"CompareOrContrast","metadata":{},"score":"46.651558"}{"text":"On full - scale treebank parsing experiments , the discriminative latent models outperform both the comparable generative latent models as well as the discriminative non - latent baselines . ... , object NPs , and so on .In this work , we therefore investigate discriminative learning of latent PCFGs , hoping to gain the best from both lines of work .","label":"CompareOrContrast","metadata":{},"score":"46.799416"}{"text":"Coupled with the ability to generalize concepts using the ontology , this approach addresses two significant problems in natural language processing - synonymy and polysemy .Categorizing documents with the aid of knowledge - based features leverages information that can not be deduced from the training documents alone .","label":"CompareOrContrast","metadata":{},"score":"46.814754"}{"text":"As comparable corpora are far more available , many studies have been conducted to extract parallel sentences from them for SMT .Parallel sentence extraction relies highly on bilingual lexicons that are also very scarce .We propose an unsupervised bilingual lexicon extraction based parallel sentence extraction system that first extracts bilingual lexicons from comparable corpora and then extracts parallel sentences using the lexicons .","label":"CompareOrContrast","metadata":{},"score":"46.9794"}{"text":"Most existing algorithms derive morpheme lexicons by identifying recurring patterns in string distribution .The goal is to optimize the compactness of the data representation by finding a small lex ... . \" ...This paper examines unsupervised approaches to part - of - speech ( POS ) tagging for morphologically - rich , resource - scarce languages , with an emphasis on Goldwater and Griffiths 's ( 2007 ) fully - Bayesian approach originally developed for English POS tagging .","label":"CompareOrContrast","metadata":{},"score":"47.00444"}{"text":"In our method the first , monolingual view consists of supervised predictors learned separately for each language .The second , bilingual view consists of log - linear predictors learned over both languages on bilingual text .Our training procedure estimates the parameters of the bilingual model using the output of the monolingual model , and we show how to combine the two models to account for dependence between views .","label":"CompareOrContrast","metadata":{},"score":"47.25302"}{"text":"..METU - Sabancı treebank ( Atalay et al . , 2003 ; Oflazer et al . , 2003 ) from the CoNLL shared task in 2006 .Whenever using CoNLL shared task data , we used the first 80 % of the data d .. \" ...","label":"CompareOrContrast","metadata":{},"score":"47.307083"}{"text":"..METU - Sabancı treebank ( Atalay et al . , 2003 ; Oflazer et al . , 2003 ) from the CoNLL shared task in 2006 .Whenever using CoNLL shared task data , we used the first 80 % of the data d .. \" ...","label":"CompareOrContrast","metadata":{},"score":"47.307083"}{"text":"Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"CompareOrContrast","metadata":{},"score":"47.516487"}{"text":"Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"CompareOrContrast","metadata":{},"score":"47.516487"}{"text":", 2010a ) , a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures .These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .","label":"CompareOrContrast","metadata":{},"score":"47.544556"}{"text":"Meanwhile , we show empirically that a finer grained prosodic break is needed for language modeling .Experimental results showed that given prosodic breaks , we were able to reduce the LM perplexity by a significant margin , suggesting a prosodic N - best rescoring approach for ASR .","label":"CompareOrContrast","metadata":{},"score":"47.743557"}{"text":"Here we propose to go beyond the bag of words , and augment both queries and ads with additional knowledgerich features .We use Web search results initially returned for the query to create a pool of relevant documents .Classifying these documents with respect to an external taxonomy and identifying salient named entities give rise to two new feature types .","label":"CompareOrContrast","metadata":{},"score":"47.787193"}{"text":"We demonstrate that log - linear grammars with latent variables can be practically trained using discriminative methods .Central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectations to be efficiently approximated in a gradient - based procedure .","label":"CompareOrContrast","metadata":{},"score":"47.983074"}{"text":"We demonstrate that log - linear grammars with latent variables can be practically trained using discriminative methods .Central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectations to be efficiently approximated in a gradient - based procedure .","label":"CompareOrContrast","metadata":{},"score":"47.983074"}{"text":"We present several models to this end ; in particular a partially observed conditional random field model , where coupled token and type constraints provide a partial signal for training .Averaged across eight previously studied Indo - European languages , our model achieves a 25 % relative error reduction over the prior state of the art .","label":"CompareOrContrast","metadata":{},"score":"48.41829"}{"text":"We propose new type level evaluation measures that , contrary to existing measures , are applicable when items are polysemous , the common case in NLP .We demonstrate the benefits of our measures using a detailed case study , POS induction .","label":"CompareOrContrast","metadata":{},"score":"48.502426"}{"text":"We present three new parameter estimation techniques that generalize the standard approach , maximum likelihood estimation , in different ways .Contrastive estimation maximizes the conditional probability of the observed data given a \" neighborhood \" of implicit negative examples .Skewed deterministic annealing locally maximizes likelihood using a cautious parameter search strategy that starts with an easier optimization problem than likelihood , and iteratively moves to harder problems , culminating in likelihood .","label":"CompareOrContrast","metadata":{},"score":"48.617012"}{"text":"Nonetheless , the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks .We demonstrate that log - linear grammars with latent variables can be practically trained using discriminative methods .Central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectations to be efficiently approximated in a gradient - based procedure .","label":"CompareOrContrast","metadata":{},"score":"48.894447"}{"text":"We present a novel approach which employs a randomized sequence of pruning masks .Formally , we apply auxiliary variable MCMC sampling to generate this sequence of masks , thereby gaining theoretical guarantees about convergence .Because each mask is generally able to skip large portions of an underlying dynamic program , our approach is particularly compelling for high - degree algorithms .","label":"CompareOrContrast","metadata":{},"score":"48.921173"}{"text":"Furthermore , we point out two main problems in Chinese word segmentation for Chinese - Japanese MT , namely , unknown words and word segmentation granularity , and propose an approach exploiting common Chinese characters to solve these problems .We also propose a statistical method for detecting other semantically equivalent Chinese characters other than the common ones and a method for exploiting shared Chinese characters in phrase alignment .","label":"CompareOrContrast","metadata":{},"score":"49.12243"}{"text":"Moreover , we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the CP to NP candidates .It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data .","label":"CompareOrContrast","metadata":{},"score":"49.132954"}{"text":"Our model considers rich context by discriminating label sequences that specify spans from the CP to each SP candidate .It enables our model to learn the effect of relative word order among SP candidates as well as to learn the effect of distances from the training data .","label":"CompareOrContrast","metadata":{},"score":"49.139526"}{"text":"We show that for restricted model sizes this model gives better cross - entropy and speech recognition results than the conventional n - gram models , and also better recognition results than non - clustered varigram models built with another recently introduced method .","label":"CompareOrContrast","metadata":{},"score":"49.318523"}{"text":"We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process ( HDP ) .Our HDP - PCFG model allows the complexity of the grammar to grow as more training data is available .In addition to presenting a fully Bayesian model for the PCFG , we also develop an efficient variational inference procedure .","label":"CompareOrContrast","metadata":{},"score":"49.34214"}{"text":"We find that no standard tagging metrics correlate well with unsupervised parsing performance , an ... \" .This paper explores the relationship between various measures of unsupervised part - of - speech tag induction and the performance of both supervised and unsupervised parsing models trained on induced tags .","label":"CompareOrContrast","metadata":{},"score":"49.41391"}{"text":"Standard inference can be used at test time .Our approach is able to scale to very large problems and yields significantly improved target domain accuracy .It is well known that parsing accuracies drop significantly on out - of - domain data .","label":"CompareOrContrast","metadata":{},"score":"49.423347"}{"text":"On full - scale treebank parsing experiments , the discriminative latent models outperform both the comparable generative latent models as well as the discriminative non - latent baselines .We present a maximally streamlined approach to learning HMM - based acoustic models for automatic speech recognition .","label":"CompareOrContrast","metadata":{},"score":"49.443985"}{"text":"This is true both for 1 - 1 and M-1 mappings .Hence , the new measures show reasonable behavior in this example for all r values .Their mapping was not based on a well - defined scheme but on a heuristic .","label":"CompareOrContrast","metadata":{},"score":"49.4545"}{"text":"Our research further demonstrates the breadth of the applicability of neural network methods to dependency parsing , as well as the ease with which new features can be added to neural parsing models .We present structured perceptron training for neural network transition - based dependency parsing .","label":"CompareOrContrast","metadata":{},"score":"49.4778"}{"text":"Because each refinement introduces only limited complexity , both learning and inference can be done in an incremental fashion .In this dissertation , we describe several coarse - to - fine systems .In the domain of syntactic parsing , complexity is in the grammar .","label":"CompareOrContrast","metadata":{},"score":"49.54521"}{"text":"Our methods result in state - of - the - art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .","label":"CompareOrContrast","metadata":{},"score":"49.84307"}{"text":"Meanwhile , Graphics Processor Units ( GPUs ) have become widely available , offering the opportunity to alleviate this bottleneck by exploiting the fine - grained data parallelism found in the CKY algorithm .In this paper , we explore the design space of parallelizing the dynamic programming computations carried out by the CKY algorithm .","label":"CompareOrContrast","metadata":{},"score":"49.89828"}{"text":"We show that addressing this task directly , using probabilistic finite - state methods , produces better results than relying on the local predictions of a current best unsupervised parser , Seginer 's ( 2007 ) CCL .These finite - state models are combined in a cascade to produce more general ( full - sentence ) constituent structures ; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English , German and Chinese .","label":"CompareOrContrast","metadata":{},"score":"50.06127"}{"text":"Our analysis indicates that accuracy continues to improve substantially as the number of training languages increases . ...NLP .Many existing algorithms derive morpheme lexicons by identifying recurring patterns in words .The goal is to optimize the compactness of the data represen ... . \" ...","label":"CompareOrContrast","metadata":{},"score":"50.073277"}{"text":"This simple framework performs surprisingly well , giving accuracy results competitive with the state - of - the - art on all the tasks we consider .The computational simplicity of the decoder and training algorithm leads to significantly higher test speeds and lower training times than their main alternatives , including log - linear and large - margin training algorithms and dynamic - programming for decoding .","label":"CompareOrContrast","metadata":{},"score":"50.22809"}{"text":"This simple framework performs surprisingly well , giving accuracy results competitive with the state - of - the - art on all the tasks we consider .The computational simplicity of the decoder and training algorithm leads to significantly higher test speeds and lower training times than their main alternatives , including log - linear and large - margin training algorithms and dynamic - programming for decoding .","label":"CompareOrContrast","metadata":{},"score":"50.22809"}{"text":"However , most discrim ... . \" ...This thesis is about estimating probabilistic models to uncover useful hidden structure in data ; specifically , we address the problem of discovering syntactic structure in natural language text .We present three new parameter estimation techniques that generalize the standard approach , maximum likel ... \" .","label":"CompareOrContrast","metadata":{},"score":"50.24499"}{"text":"The problem of transfer learning , where information gained in one learning task is used to improve performance in another related task , is an important new area of research .In this paper we address the subproblem of domain adaptation , in which a model trained over a source domain is generalized to ... \" .","label":"CompareOrContrast","metadata":{},"score":"50.26675"}{"text":"Our best results show a 26-fold speedup compared to a sequential C implementation .We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data .We first demonstrate that delexicalized parsers can be directly transferred between languages , producing significantly higher accuracies than unsupervised parsers .","label":"CompareOrContrast","metadata":{},"score":"50.407578"}{"text":"However , the inefficiency of this approach increases with the order of the subtrees .This work explores a new reranking approach for dependency parsing that can utilize complex subtree representations by applying efficient subtree selection methods .We demonstrate the effectiveness of the approach in experiments conducted on the Penn Treebank and the Chinese Treebank .","label":"CompareOrContrast","metadata":{},"score":"50.51149"}{"text":"Since such relaxation comes at the expense of a drop in tagging accuracy , we propose two extensions to the Bayesian framework and demonstrate that they are effective in improving a fully - Bayesian POS tagger for Bengali , our representative morphologicallyrich , resource - scarce language . \" ...","label":"CompareOrContrast","metadata":{},"score":"50.51673"}{"text":"We describe a novel training criterion for probabilistic grammar induction models , contrastive estimation [ Smith and Eisner , 2005 ] , which can be interpreted as exploiting implicit negative evidence and includes a wide class of likelihood - based objective functions .","label":"CompareOrContrast","metadata":{},"score":"50.525375"}{"text":"All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi - supervised learning for the shared tasks .We show how both multitask learning and semi - supervised learning improve the generalization of the shared tasks , resulting in stateof - the - art performance . \" ...","label":"CompareOrContrast","metadata":{},"score":"50.617046"}{"text":"Consequently , clustering evaluation is of great importance .Many clustering algorithms are evaluated by their success in tagging corpus tokens .In this paper we discuss type level evaluation , which reflects class membership only and is independent of the tok ... \" .","label":"CompareOrContrast","metadata":{},"score":"50.638092"}{"text":"Despite the much simplified training process , our acoustic model achieves state - of - the - art results on phone classification ( where it outperforms almost all other methods ) and competitive performance on phone recognition ( where it outperforms standard CD triphone / subphone / GMM approaches ) .","label":"CompareOrContrast","metadata":{},"score":"50.72393"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"CompareOrContrast","metadata":{},"score":"50.74522"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"CompareOrContrast","metadata":{},"score":"50.74522"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"CompareOrContrast","metadata":{},"score":"50.74522"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"CompareOrContrast","metadata":{},"score":"50.74522"}{"text":"In this paper , we investigate the usefulness of character - level part - of - speech in the task of Chinese morphological analysis .We propose the first tagset designed for the task of character - level POS tagging .We propose a method that performs character - level POS tagging jointly with word segmentation and word - level POS tagging .","label":"CompareOrContrast","metadata":{},"score":"50.80134"}{"text":"The tree with the maximal probability is outputted .The experiments are carried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser . ... arried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser .","label":"CompareOrContrast","metadata":{},"score":"50.87877"}{"text":"The tree with the maximal probability is outputted .The experiments are carried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser . ... arried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser .","label":"CompareOrContrast","metadata":{},"score":"50.87877"}{"text":"The quality of the tagging induced by such algorithms is thus highly variable , and researchers report average results over several random initializations .Consequently , applications are not guaranteed to use an induced tagging of the quality reported for the algorithm .","label":"CompareOrContrast","metadata":{},"score":"51.001884"}{"text":"Index Terms : random forest language model , large - scale training , data scaling , speech recognition . ... popularity because of its simplicity and surprisingly good performance .Several new models have been proposed over the years with varying degrees of success .","label":"CompareOrContrast","metadata":{},"score":"51.027863"}{"text":"This robustness led to the third best overall average labeled attachment score in the task , despite using no discriminative methods .We also demonstrate that the parser is quite fast , and can provide even faster parsing times without much loss of accuracy . \" ...","label":"CompareOrContrast","metadata":{},"score":"51.4255"}{"text":"This robustness led to the third best overall average labeled attachment score in the task , despite using no discriminative methods .We also demonstrate that the parser is quite fast , and can provide even faster parsing times without much loss of accuracy . \" ...","label":"CompareOrContrast","metadata":{},"score":"51.4255"}{"text":"Experimental results over a range of datasets confirm improved performance compared to the bag of words document representation . ... ann , 2003 ) .More recently , there have been a number of efforts to add outside knowledge to supervised machine learning techniques .","label":"CompareOrContrast","metadata":{},"score":"51.480564"}{"text":"Our approach is based on a model that locally mixes between supervised models from the helper languages .Parallel dat ... \" .We describe a method for prediction of linguistic structure in a language for which only unlabeled data is available , using annotated data from a set of one or more helper languages .","label":"CompareOrContrast","metadata":{},"score":"51.61029"}{"text":"Additionally , it proves that the proposed method can perform well even when only a small amount of user feedback is available .For example , an improvement of 5.3 % in P@10 was achieved when user feedback constituted only 57 words .","label":"CompareOrContrast","metadata":{},"score":"51.640564"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"CompareOrContrast","metadata":{},"score":"51.664852"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"CompareOrContrast","metadata":{},"score":"51.664852"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"CompareOrContrast","metadata":{},"score":"51.664852"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"CompareOrContrast","metadata":{},"score":"51.664852"}{"text":"Despite its simplicity , a product of eight automatically learned grammars improves parsing accuracy from 90.2 % to 91.8 % on English , and from 80.3 % to 84.5 % on German . \" ...We extend discriminative n - gram language modeling techniques originally proposed for automatic speech recognition to a statistical machine translation task .","label":"CompareOrContrast","metadata":{},"score":"51.928932"}{"text":"To manage this complexity , we translate into target language clusterings of increasing vocabulary size .This approach gives dramatic speed - ups while additionally increasing final translation quality .The intersection of tree transducer - based translation models with n - gram language models results in huge dynamic programs for machine translation decoding .","label":"CompareOrContrast","metadata":{},"score":"51.9447"}{"text":"Specifically , an ini- tial hypothesis lattice is constrcuted using local features .Candidate sentences are then assigned syntactic language model scores .These global syntactic scores are combined with local low - level scores in a log - linear model .","label":"CompareOrContrast","metadata":{},"score":"51.99193"}{"text":"To train on unlabeled data , we require unsupervised estimation methods for log - linear models ; few exist .We describe a novel approach , contrastive estimation .We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efficient .","label":"CompareOrContrast","metadata":{},"score":"52.07948"}{"text":"The model is based on the Factorial Hidden Markov Model ( FHMM ) with distributed hidden states representing partof - speech and noun phrase se ... \" .We present new statistical models for jointly labeling multiple sequences and apply them to the combined task of partof - speech tagging and noun phrase chunking .","label":"CompareOrContrast","metadata":{},"score":"52.288452"}{"text":"Across eight European languages , our approach results in an average absolute improvement of 10.4 % over a state - of - the - art baseline , and 16.7 % over vanilla hidden Markov models induced with the Expectation Maximization algorithm . \" ...","label":"CompareOrContrast","metadata":{},"score":"52.509254"}{"text":"We generalize the evaluation to other word - types , and show that the performance can be increased to 18 % relative by preserving part - of - speech equivalencies during translation .We further differentiate ourselves from previous work by conducting a second evaluation which examines the accuracy of translating all word types , rather than just nouns .","label":"CompareOrContrast","metadata":{},"score":"52.541077"}{"text":"We generalize the evaluation to other word - types , and show that the performance can be increased to 18 % relative by preserving part - of - speech equivalencies during translation .We further differentiate ourselves from previous work by conducting a second evaluation which examines the accuracy of translating all word types , rather than just nouns .","label":"CompareOrContrast","metadata":{},"score":"52.541077"}{"text":"Our results demonstrate that incorporating POS categorization yields substantial performance gains on morphological segmentation of Arabic .Toutanova and Cherry ( 2009 ) were the first to systematically study how to incorporate part - of - speech information into lemmatization and empirically demonstrate the benefits ... . by Joydeep Nath , Monojit Choudhury , Animesh Mukherjee , Chris Biemann , Niloy Ganguly . \" ...","label":"CompareOrContrast","metadata":{},"score":"52.927395"}{"text":"In this paper we focus on tra ... . byAndrei Z. Broder , Peter Ciccolo , Marcus Fontoura , Evgeniy Gabrilovich , Vanja Josifovski , Lance Riedel - In Proc 17th .Intl .Conf . on Information and Knowledge Management , 2008 . \" ...","label":"CompareOrContrast","metadata":{},"score":"53.04943"}{"text":"In this study , we propose reusing the already labeled 142440469X/06/$20.00 © 2006 IEEE I 585 ICASSP 2006data across applications while training .This is similar to model adaptation , however in ... . \" ...Many problems in NLP require solving a cascade of subtasks .","label":"CompareOrContrast","metadata":{},"score":"53.392334"}{"text":"Across eight European languages , our approach results in an average absolute improvement of 10.4 % over a state - of - the - art baseline , and 16.7 % over vanilla hidden Markov models induced with the Expectation Maximization algorithm .","label":"CompareOrContrast","metadata":{},"score":"53.501076"}{"text":"We propose instead to treat morphological analysis ... \" .In this paper , we consider the problem of unsupervised morphological analysis from a new angle .Past work has endeavored to design unsupervised learning methods which explicitly or implicitly encode inductive biases appropriate to the task at hand .","label":"CompareOrContrast","metadata":{},"score":"53.645294"}{"text":"We define a universal morphological feature space in which every language and its morphological analysis reside .We develop a novel structured nearest neighbor prediction method which seeks to find the morphological analysis for each unlabeled language which lies as close as possible in the feature space to a training language .","label":"CompareOrContrast","metadata":{},"score":"53.784912"}{"text":"The model is formally a latent variable CRF grammar over trees , learned by iteratively splitting grammar productions ( not categories ) .Different regions of the grammar are refined to different degrees , yielding grammars which are three orders of magnitude smaller than the single - scale baseline and 20 times smaller than the split - and - merge grammars of Petrov et al .","label":"CompareOrContrast","metadata":{},"score":"53.829533"}{"text":"Unlike previous approaches , our framework does not require full projected parses , allowing partial , approximate transfer through linear expectation constraints on the space of distributions over trees .We consider several types of constraints that range from generic dependency conservation to language - specific annotation rules for auxiliary verb analysis .","label":"CompareOrContrast","metadata":{},"score":"53.888474"}{"text":"Unlike previous approaches , our framework does not require full projected parses , allowing partial , approximate transfer through linear expectation constraints on the space of distributions over trees .We consider several types of constraints that range from generic dependency conservation to language - specific annotation rules for auxiliary verb analysis .","label":"CompareOrContrast","metadata":{},"score":"53.888474"}{"text":"Tools . \" ... Interpolated Kneser - Ney is one of the best smoothing methods for n - gram language models .Previous explanations for its superiority have been based on intuitive and empirical justifications of specific properties of the method .","label":"CompareOrContrast","metadata":{},"score":"53.999855"}{"text":"Some of this improvement is from training , but more than half is from parsing with induced constraints , in inference .Punctuation - aware decoding works with existing ( even already - trained ) parsing models and always increased accuracy in our experiments . ... mplementation , extension , understanding and debugging . \" ...","label":"CompareOrContrast","metadata":{},"score":"54.040382"}{"text":"Despite its simplicity , a product of eight automatically learned grammars improves parsing accuracy from 90.2 % to 91.8 % on English , and from 80.3 % to 84.5 % on German .Pruning can massively accelerate the computation of feature expectations in large models .","label":"CompareOrContrast","metadata":{},"score":"54.109165"}{"text":"Abstract .Transfer learning is the improvement of learning in a new task through the transfer of knowledge from a related task that has already been learned .While most machine learning algorithms are designed to address single tasks , the development of algorithms that facilitate transfer learning i ... \" .","label":"CompareOrContrast","metadata":{},"score":"54.36946"}{"text":"Our generative self - trained grammars reach F scores of 91.6 on the WSJ test set and surpass even discriminative reranking systems without self - training .Additionally , we show that multiple self - trained grammars can be combined in a product model to achieve even higher accuracy .","label":"CompareOrContrast","metadata":{},"score":"54.465252"}{"text":"We describe a novel approach for inducing unsupervised part - of - speech taggers for languages that have no labeled training data , but have translated text in a resource - rich language .Our method does not assume any knowledge about the target language ( in particular no tagging dictionary is assumed ) , making it applicable to a wide array of resource - poor languages .","label":"CompareOrContrast","metadata":{},"score":"54.474457"}{"text":"As with syntactic models , however , scalability is again an issue .In this case , computational complexity is demanding , and if one has to choose between RFs with less data and KN smoothing with more ... . \" ...","label":"CompareOrContrast","metadata":{},"score":"54.550064"}{"text":"Our approach leverages derivation rules and onomatopoeia patterns , and correctly recognizes certain types of unknown words .Experiments revealed that our approach recognized about 4,500 unknown words in 100,000 Web sentences with only roughly 80 harmful side effects and a 6 % loss in speed .","label":"CompareOrContrast","metadata":{},"score":"54.61512"}{"text":"This paper examines unsupervised approaches to part - of - speech ( POS ) tagging for morphologically - rich , resource - scarce languages , with an emphasis on Goldwater and Griffiths 's ( 2007 ) fully - Bayesian approach originally developed for English POS tagging .","label":"CompareOrContrast","metadata":{},"score":"54.686974"}{"text":"Consequently , clustering evaluation is of great importance .Many clustering algorithms are evaluated by their success in tagging corpus tokens .In this paper we discuss type level evaluation , which reflects class membership only and is independent of the token statistics of a particular reference corpus .","label":"CompareOrContrast","metadata":{},"score":"54.69111"}{"text":"We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system .We use a corpus of weakly - labeled reference reorderings to guide parser training .Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress .","label":"CompareOrContrast","metadata":{},"score":"54.860245"}{"text":"Interpolated Kneser - Ney is one of the best smoothing methods for n - gram language models .Previous explanations for its superiority have been based on intuitive and empirical justifications of specific properties of the method .We propose a novel interpretation of interpolated Kneser - Ney as approximate inference in a hierarchical Bayesian model consisting of Pitman - Yor processes .","label":"CompareOrContrast","metadata":{},"score":"54.950584"}{"text":"Experimental results show that our method effectively compresses a sentence while preserving its important information and grammaticality .","label":"CompareOrContrast","metadata":{},"score":"55.229797"}{"text":"Unfortunately the sentence in Figure 1(b ) is highly unusual in its amount of dependency conservation .To get a feel for the typical case , we used off - the - shelf parsers ( McDonald et al . , 2005 ) for E .. by Ivan Titov , James Henderson - IN PROCEEDINGS OF CONLL-2007 SHARED TASK .","label":"CompareOrContrast","metadata":{},"score":"55.393837"}{"text":"Unfortunately the sentence in Figure 1(b ) is highly unusual in its amount of dependency conservation .To get a feel for the typical case , we used off - the - shelf parsers ( McDonald et al . , 2005 ) for E .. by Ivan Titov , James Henderson - IN PROCEEDINGS OF CONLL-2007 SHARED TASK .","label":"CompareOrContrast","metadata":{},"score":"55.393837"}{"text":"We then use the acquired knowledge to a case alternation task and show its usefulness .[ Show abstract ] [ Hide abstract ] ABSTRACT : Most relevance feedback methods re - rank search results using only the information of surface words in texts .","label":"CompareOrContrast","metadata":{},"score":"55.413982"}{"text":"Primary acoustic , speech , and vision systems were trained to discriminate instances of the categories .Higher - level systems exploited correlations among the categories , incorporated sequential context , and combined the joint evidence from the three information sources .","label":"CompareOrContrast","metadata":{},"score":"55.52312"}{"text":"The algorithm first identifies landmark clusters of words , serving as the cores of the induced POS categories .The rest of the words are subsequently mapped to these cl ... \" .We present a novel fully unsupervised algorithm for POS induction from plain text , motivated by the cognitive notion of prototypes .","label":"CompareOrContrast","metadata":{},"score":"55.63015"}{"text":"We describe a system that learns how to extract keywords from web pages for advertisement targeting .The system uses a number of features , such as term frequency of each . ... tudied areas like information extraction , named - entity recognition and phrase labeling : they all attempt to find important phrases in documents .","label":"CompareOrContrast","metadata":{},"score":"55.71936"}{"text":"Given this fixed network representation , we learn a final layer using the structured perceptron with beam - search decoding .On the Penn Treebank , our parser reaches 94.26 % unlabeled and 92.41 % labeled attachment accuracy , which to our knowledge is the best accuracy on Stanford Dependencies to date .","label":"CompareOrContrast","metadata":{},"score":"55.785408"}{"text":"Parallel data is not used , allowing the technique to be applied even in domains where human - translated texts are unavailable .We obtain state - of - theart performance for two tasks of structure prediction : unsupervised part - of - speech tagging and unsupervised dependency parsing . ... parsing .","label":"CompareOrContrast","metadata":{},"score":"55.80186"}{"text":"We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .","label":"CompareOrContrast","metadata":{},"score":"55.840572"}{"text":"Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext .","label":"CompareOrContrast","metadata":{},"score":"56.04222"}{"text":"Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext .","label":"CompareOrContrast","metadata":{},"score":"56.04222"}{"text":"However , transfer learning may actually hinder performance if the tasks are too dissimilar .As described in this paper , one challenge for transfer learning research is to develop approaches that detect and avoid negative transfer using very little data from the target task . ... esentations for the target task .","label":"CompareOrContrast","metadata":{},"score":"56.17063"}{"text":"In an empirical evaluation we show that our model consistently out - performs the current state - of - the - art across 10 languages . ...MM ) and also with the character LM ( 1HMM - LM ) .Starred entries denote results reported in CGS10 .","label":"CompareOrContrast","metadata":{},"score":"56.345745"}{"text":"The results show that all three systems achieve competitive performance , with a best labeled attachment score over 88 % .All three parsers benefit from the use of automatically derived lemmas , while morphological features seem to be less important .","label":"CompareOrContrast","metadata":{},"score":"56.892754"}{"text":"The results show that all three systems achieve competitive performance , with a best labeled attachment score over 88 % .All three parsers benefit from the use of automatically derived lemmas , while morphological features seem to be less important .","label":"CompareOrContrast","metadata":{},"score":"56.892754"}{"text":"Transfer learning is the improvement of learning in a new task through the transfer of knowledge from a related task that has already been learned .While most machine learning algorithms are designed to address single tasks , the development of algorithms that facilitate transfer learning is a topic of ongoing interest in the machine - learning community .","label":"CompareOrContrast","metadata":{},"score":"56.92164"}{"text":"To facilitate future research in unsupervised induction of syntactic structure and to standardize best - practices , we propose a tagset that consists of twelve universal part - of - speech categories .In addition to the tagset , we develop a mapping from 25 different treebank tagsets to this universal set .","label":"CompareOrContrast","metadata":{},"score":"57.08556"}{"text":"Pages 1385 - 1394 , Portland , Oregon , USA , ACL , Association for Computational Linguistics , 6/2011 The large combined search space of joint word segmentation and Part - of - Speech ( POS ) tagging makes efficient decoding very hard .","label":"CompareOrContrast","metadata":{},"score":"57.148697"}{"text":"We infer latent word distribution in each document in the search results using latent Dirichlet allocation ( LDA ) .When feedback is given , we also infer the latent word distribution in the feedback using LDA .We calculate the similarities between the user feedback and each document in the search results using both the surface and latent word distributions and re - rank the search results on the basis of the similarities .","label":"CompareOrContrast","metadata":{},"score":"57.347477"}{"text":"This ' universal ' treebank is made freely available in order to facilitate research on multilingual dependency parsing .We consider the construction of part - of - speech taggers for resource - poor languages .Recently , manually constructed tag dictionaries from Wiktionary and dictionaries projected via bitext have been used as type constraints to overcome the scarcity of annotated data in this setting .","label":"CompareOrContrast","metadata":{},"score":"57.454372"}{"text":"We carry out systema ... \" .We extend discriminative n - gram language modeling techniques originally proposed for automatic speech recognition to a statistical machine translation task .In this context , we propose a novel data selection method that leads to good models using a fraction of the training data .","label":"CompareOrContrast","metadata":{},"score":"57.574726"}{"text":"The loss function can be seen as a ( generative ) alternative to maximum likelihood estimation with an interesting information - theoretic interpretation , and it is statistically consistent .It is substantially faster than maximum ( conditional ) likelihood estimation of conditional random fields ( Lafferty et al . , 2001 ; an order of magnitude or more ) .","label":"CompareOrContrast","metadata":{},"score":"57.677094"}{"text":"However , parsing accuracies for Arabic usually lag behind non - semitic languages .Moreover , whil ...Tools . by Kuzman Ganchev , Jennifer Gillenwater , Ben Taskar - In ACL - IJCNLP , 2009 . \" ...Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .","label":"CompareOrContrast","metadata":{},"score":"57.716957"}{"text":"The best accuracies were in the 80 - 84\\% range for F1 and LAS ; even part - of - speech accuracies were just above 90\\% .Coarse - to - fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy .","label":"CompareOrContrast","metadata":{},"score":"57.751564"}{"text":"Existing solutions to this problem do not guarantee non - violation of hard - constraints imposed by subtasks and thus give rise t ... \" .Many problems in NLP require solving a cascade of subtasks .Traditional pipeline approaches yield to error propagation and prohibit joint training / decoding between subtasks .","label":"CompareOrContrast","metadata":{},"score":"57.81138"}{"text":"Many different methods have been proposed , yet comparisons are difficult to make since there is little consensus on evaluation framework , and many papers evaluate against only one or two competitor systems .Here we evaluate seven different POS induction systems spanning nearly 20 years of work , using a variety of measures .","label":"CompareOrContrast","metadata":{},"score":"57.91"}{"text":"With 100 K unlabeled and 2 K labeled questions , uptraining is able to improve parsing accuracy to 84 % , closing the gap between in - domain and out - of - domain performance .We study self - training with products of latent variable grammars in this paper .","label":"CompareOrContrast","metadata":{},"score":"57.98248"}{"text":"Our results are better in most evaluation measures than all results reported in the literature for this task , and are always better than the Clark average results . \" ...The connection between part - of - speech ( POS ) categories and morphological properties is well - documented in linguistics but underutilized in text processing systems .","label":"CompareOrContrast","metadata":{},"score":"58.140102"}{"text":"Children learn various levels of linguistic structure concurrently , yet most existing mod - els of language acquisition deal with only a single level of structure , implicitly assum - ing a sequential learning process .Developing models that learn multiple levels simultane - ously can provide important insights into how these levels might interact synergistically dur - ing learning .","label":"CompareOrContrast","metadata":{},"score":"58.18746"}{"text":"Their symbolic component is amenable to inspection by humans , while their probabilistic component helps resolve ambiguity .They also permit the use of well - understood , generalpurpose learn ... \" .Probabilistic grammars offer great flexibility in modeling discrete sequential data like natural language text .","label":"CompareOrContrast","metadata":{},"score":"58.18965"}{"text":"Their symbolic component is amenable to inspection by humans , while their probabilistic component helps resolve ambiguity .They also permit the use of well - understood , generalpurpose learn ... \" .Probabilistic grammars offer great flexibility in modeling discrete sequential data like natural language text .","label":"CompareOrContrast","metadata":{},"score":"58.18965"}{"text":"Their symbolic component is amenable to inspection by humans , while their probabilistic component helps resolve ambiguity .They also permit the use of well - understood , generalpurpose learn ... \" .Probabilistic grammars offer great flexibility in modeling discrete sequential data like natural language text .","label":"CompareOrContrast","metadata":{},"score":"58.18965"}{"text":"Parser quality is usually evaluated by comparing its output to a gold standard whose annotations are linguistically motivated .However , there are cases in which ... . by Shay B. Cohen , Noah A. Smith , Alex Clark , Dorota Glowacka , Colin De La Higuera , Mark Johnson , John Shawe - taylor . \" ...","label":"CompareOrContrast","metadata":{},"score":"58.321606"}{"text":"The rest of the words are subsequently mapped to these clusters .We utilize morphological and distributional representations computed in a fully unsupervised manner .We evaluate our algorithm on English and German , achieving the best reported results for this task . \" ...","label":"CompareOrContrast","metadata":{},"score":"58.623596"}{"text":"We describe experiments on learning latent variable grammars for various German treebanks , using a language - agnostic statistical approach .In our method , a minimal initial grammar is hierarchically refined using an adaptive split - and - merge EM procedure , giving compact , accurate grammars .","label":"CompareOrContrast","metadata":{},"score":"58.712395"}{"text":"No preview · Article · Jul 2014 · IEEE / ACM Transactions on Audio , Speech , and Language Processing .[ Show abstract ] [ Hide abstract ] ABSTRACT : The focus of recent studies on Chinese word segmentation , part - of - speech ( POS ) tagging and parsing has been shifting from words to characters .","label":"CompareOrContrast","metadata":{},"score":"58.722702"}{"text":"In this work , however , we study the more challenging problem of unsupervised transductive transfer learning , where no labeled data from the target domain are available at training time , but instead , unlabeled target test data are available during training .","label":"CompareOrContrast","metadata":{},"score":"58.738235"}{"text":"In matching between predicate - arguments , wide - coverage relations between words / phrases such as synonym and is - a are utilized , which are automatically acquired from a dictionary , Web corpus , and Wikipedia .No preview · Article · Dec 2012 · ACM Transactions on Asian Language Information Processing .","label":"CompareOrContrast","metadata":{},"score":"59.397385"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"CompareOrContrast","metadata":{},"score":"59.420593"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"CompareOrContrast","metadata":{},"score":"59.420593"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"CompareOrContrast","metadata":{},"score":"59.420593"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"CompareOrContrast","metadata":{},"score":"59.420593"}{"text":"Estimation Using an Online Algorit ... . by Noah A. Smith , Douglas Vail , John D. Lafferty , Noah A. Smith , Douglas L. Vail , John D. Lafferty . \" ...We describe a new loss function , due to Jeon and Lin ( 2006 ) , for estimating structured log - linear models on arbitrary features .","label":"CompareOrContrast","metadata":{},"score":"59.761963"}{"text":"Our estimation methods do not make use of annotated examples . by Noah A. Smith , Jason Eisner - In Proc . of IJCAI Workshop on Grammatical Inference Applications , 2005 . \" ...We describe a novel training criterion for probabilistic grammar induction models , contrastive estimation [ Smith and Eisner , 2005 ] , which can be interpreted as exploiting implicit negative evidence and includes a wide class of likelihood - based objective functions .","label":"CompareOrContrast","metadata":{},"score":"59.786667"}{"text":"We com - pare its performance and training time to an HMM , a CRF , an MEMM , and pseudolike - lihood on a shallow parsing task .These ex - periments help tease apart the contributions of rich features and discriminative training , which are shown to be more than additive . ...","label":"CompareOrContrast","metadata":{},"score":"59.898964"}{"text":"For this purpose , we propose an automated intent mapping algorithm across applications .We also propose employing active learning to selectively sample the data to be re - used .Our results indicate that we can achieve significant improvements in intent classification performance especially when the labeled data size is limited .","label":"CompareOrContrast","metadata":{},"score":"59.97328"}{"text":"Thus , although these newer methods have introduced potentially useful machine learning techniques , they should not be assumed to provide the best performance for unsupervised POS induction .In addi ... . \" ...Dependency parsing is a central NLP task .","label":"CompareOrContrast","metadata":{},"score":"60.128723"}{"text":"Yet , various grammar parameters are expected to be correlated because the elements in language they represent share linguistic properties .In this paper , we suggest an alternative to the Dirichlet prior , a family of logistic normal distributions .We derive an inference algorithm for this family of distributions and experiment with the task of dependency grammar induction , demonstrating performance improvements with our priors on a set of six treebanks in different natural languages .","label":"CompareOrContrast","metadata":{},"score":"60.188183"}{"text":"Yet , various grammar parameters are expected to be correlated because the elements in language they represent share linguistic properties .In this paper , we suggest an alternative to the Dirichlet prior , a family of logistic normal distributions .We derive an inference algorithm for this family of distributions and experiment with the task of dependency grammar induction , demonstrating performance improvements with our priors on a set of six treebanks in different natural languages .","label":"CompareOrContrast","metadata":{},"score":"60.188183"}{"text":"Yet , various grammar parameters are expected to be correlated because the elements in language they represent share linguistic properties .In this paper , we suggest an alternative to the Dirichlet prior , a family of logistic normal distributions .We derive an inference algorithm for this family of distributions and experiment with the task of dependency grammar induction , demonstrating performance improvements with our priors on a set of six treebanks in different natural languages .","label":"CompareOrContrast","metadata":{},"score":"60.188183"}{"text":"In our model , arbitrary , nonindependent features may be freely incorporated , thereby overcoming the inherent limitation of generative models , which require that features be sensitive to the conditional independencies of the generative process .However , unlike previous work on discriminative modeling of word alignment ( which also permits the use of arbitrary features ) , the parameters in our models are learned from unannotated parallel sentences , rather than from supervised word alignments .","label":"CompareOrContrast","metadata":{},"score":"60.310093"}{"text":"The resulting grammars are extremely compact com- pared to other high - performance parsers , yet the parser gives the best published accuracies on several languages , as well as the best generative parsing numbers in English .In addi- tion , we give an associated coarse - to - fine inference scheme which vastly improves inference time with no loss in test set accuracy .","label":"CompareOrContrast","metadata":{},"score":"60.588585"}{"text":"The new Viewer adds three features for more powerful search : wildcards , morphological inflections , and capitalization .These additions allow the discovery of patterns that were previously difficult to find and further facilitate the study of linguistic trends in printed text .","label":"CompareOrContrast","metadata":{},"score":"60.6044"}{"text":"To globally model parsing actions of all steps that are taken on the inpu ... \" .Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .They only determine parsing actions stepwisely by a trained classifier .","label":"CompareOrContrast","metadata":{},"score":"61.126476"}{"text":"To globally model parsing actions of all steps that are taken on the inpu ... \" .Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .They only determine parsing actions stepwisely by a trained classifier .","label":"CompareOrContrast","metadata":{},"score":"61.126476"}{"text":"Modern unsupervised POS taggers usually apply an optimization procedure to a nonconvex function , and tend to converge to local maxima that are sensitive to starting conditions .The quality of the tagging induced by such algorithms is thus highly variable , and researchers report average results over ... \" .","label":"CompareOrContrast","metadata":{},"score":"61.20849"}{"text":"Models of morphology induction generally operate over a le ...DFKI - LT - A Stacked Sub - Word Model for Joint Chinese Word Segmentation and Part - of - Speech Tagging .Weiwei Sun A Stacked Sub - Word Model for Joint Chinese Word Segmentation and Part - of - Speech Tagging 1 Proceedings of the 49th","label":"CompareOrContrast","metadata":{},"score":"61.34292"}{"text":"Yi Su , Frederick Jelinek , Sanjeev Khudanpur - in Proc .Interspeech , 2007 . \" ...The random forest language model ( RFLM ) has shown encouraging results in several automatic speech recognition ( ASR ) tasks but has been hindered by practical limitations , notably the space - complexity of RFLM estimation from large amounts of data .","label":"CompareOrContrast","metadata":{},"score":"61.38562"}{"text":"In addition , we show how simple relaxations , such as providing additional information like the proportion of positive examples in the test data , can significantly improve the performance of some of the transductive transfer learners . ...e labels ) is the same for both D source and D target , while D source and D target themselves are allowed to vary between domains .","label":"CompareOrContrast","metadata":{},"score":"61.669395"}{"text":"However , most of the benefit was due to using a varigram model instead of a full n - gram model .Their language model outperformed traditional n - gram model in both perplexity evaluation and speech recognition .An n - gram ... .","label":"CompareOrContrast","metadata":{},"score":"61.72466"}{"text":"Our grammar inducer is trained on the Wall Street Journal ( WSJ ) and achieves 59.5 % accuracy out - of - domain ( Brown sentences with 100 or fewer words ) , more than 6 % higher than the previous best results .","label":"CompareOrContrast","metadata":{},"score":"61.774887"}{"text":"This leaning strategy enables consideration of all of the rich context simultaneously .In our experiments , our model had higher BLUE and RIBES scores for Japanese - English , Chinese - English , and German - English translation compared to the lexical reordering models .","label":"CompareOrContrast","metadata":{},"score":"61.896973"}{"text":"CE is a natural fit for log - linear models , which can include arbitrary features but for which EM is computationally difficult .We show that , using the same features , log - linear dependency grammar models trained using CE can drastically outperform EMtrained generative models on the task of matching human linguistic annotations ( the MATCHLIN - GUIST task ) .","label":"CompareOrContrast","metadata":{},"score":"62.002953"}{"text":"State - of - the - art natural language processing models are anything but compact .Syntactic parsers have huge grammars , machine translation systems have huge transfer tables , and so on across a range of tasks .With such complexity come two challenges .","label":"CompareOrContrast","metadata":{},"score":"62.132263"}{"text":"We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data .Results indicate that ( a ) MST - parser performs better on Hebrew data than Malt - Parser , and ( b ) both parsers do not make good use of morphological information when parsing Hebrew . ... s on Hebrew dependency parsing .","label":"CompareOrContrast","metadata":{},"score":"62.15084"}{"text":"We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data .Results indicate that ( a ) MST - parser performs better on Hebrew data than Malt - Parser , and ( b ) both parsers do not make good use of morphological information when parsing Hebrew . ... s on Hebrew dependency parsing .","label":"CompareOrContrast","metadata":{},"score":"62.15084"}{"text":"Our model learns that words with com ... \" .The connection between part - of - speech ( POS ) categories and morphological properties is well - documented in linguistics but underutilized in text processing systems .This paper proposes a novel model for morphological segmentation that is driven by this connection .","label":"CompareOrContrast","metadata":{},"score":"62.242886"}{"text":"The topological properties of these networks reveal interesting insights into the morpho - syntax of the language , whereas clustering helps in the induction of the natural word classes leading to a pri ... \" .We present a study of the word interaction networks of Bengali in the framework of complex networks .","label":"CompareOrContrast","metadata":{},"score":"62.37178"}{"text":"Behavior Control : Finally we show how all these elements can be incorporated into a goal keeping robot .We develop simple behaviors that can be used in a layered architecture and enable the robot to block most balls that are being shot at the goal .","label":"CompareOrContrast","metadata":{},"score":"62.762917"}{"text":"The annotations are produced automatically with statistical models that are specifically adapted to historical text .The corpus will facilitate the study of linguistic trends , especially those related to the evolution of syntax .Syntactic analysis of search queries is important for a variety of information- retrieval tasks ; however , the lack of annotated data makes training query analysis models difficult .","label":"CompareOrContrast","metadata":{},"score":"62.885674"}{"text":"Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features .While they perform well in many categorization tasks , these methods are inherently limited when faced with more complicated tasks where external knowledge is ... \" .","label":"CompareOrContrast","metadata":{},"score":"62.907894"}{"text":"This versatility is achieved by trying to avoid task - specific engineering and therefore disregarding a lot of prior knowledge .Instead of exploiting man - made input features carefully optimized for each task , our system learns internal representations on the basis of vast amounts of mostly unlabeled training data .","label":"CompareOrContrast","metadata":{},"score":"63.36051"}{"text":"They also permit the use of well - understood , generalpurpose learning algorithms .There has been an increased interest in using probabilistic grammars in the Bayesian setting .To date , most of the literature has focused on using a Dirichlet prior .","label":"CompareOrContrast","metadata":{},"score":"63.36618"}{"text":"They also permit the use of well - understood , generalpurpose learning algorithms .There has been an increased interest in using probabilistic grammars in the Bayesian setting .To date , most of the literature has focused on using a Dirichlet prior .","label":"CompareOrContrast","metadata":{},"score":"63.36618"}{"text":"They also permit the use of well - understood , generalpurpose learning algorithms .There has been an increased interest in using probabilistic grammars in the Bayesian setting .To date , most of the literature has focused on using a Dirichlet prior .","label":"CompareOrContrast","metadata":{},"score":"63.36618"}{"text":"[ Show abstract ] [ Hide abstract ] ABSTRACT :In this paper , we propose the use of spans in addition to edges in noun compound analysis .A span is a sequence of words that can represent a noun compound .","label":"CompareOrContrast","metadata":{},"score":"63.436344"}{"text":"They can be reliably extracted from a huge amount of unannotated text .In addition , while the combinations of edges such as sibling and grandparent interactions are , in general , difficult to handle in parsing , it is quite easy to utilize spans with arbitrary width .","label":"CompareOrContrast","metadata":{},"score":"63.87348"}{"text":"However , parsing accuracies for Arabic usually lag behind non - semitic languages .Moreover , whil ...Tools . \" ...We describe a novel approach for inducing unsupervised part - of - speech taggers for languages that have no labeled training data , but have translated text in a resource - rich language .","label":"CompareOrContrast","metadata":{},"score":"64.09207"}{"text":"However , bunsetsu - based methods have the disadvantage that they can not drop unimportant words from each bunsetsu because they have to follow constraints under which each bunsetsu is treated as a unit .In this paper , we propose a novel compression method to overcome this disadvantage .","label":"CompareOrContrast","metadata":{},"score":"64.26062"}{"text":"We show that dependency parsers have more difficulty parsing questions than constituency parsers .In particular , deterministic shift - reduce dependency parsers , which are of highest interest for practical applications because of their linear running time , drop to 60 % labeled accuracy on a question test set .","label":"CompareOrContrast","metadata":{},"score":"64.35179"}{"text":"The penalty for a recognition failure is often small : if two con- figurations are confused , they are often similar to each other , and the illusion works well enough , for instance , to drive a graphics animation of the moving hand .","label":"CompareOrContrast","metadata":{},"score":"64.67699"}{"text":"We consider generative and di ... \" .Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext .","label":"CompareOrContrast","metadata":{},"score":"65.12531"}{"text":"While some grammar induction systems opera ... . \" ...Children learn various levels of linguistic structure concurrently , yet most existing mod - els of language acquisition deal with only a single level of structure , implicitly assum - ing a sequential learning process .","label":"CompareOrContrast","metadata":{},"score":"66.08557"}{"text":"Despite the importance of this area , little formal , published research exists .We describe ... \" .A large and growing number of web pages display contextual advertising based on keywords automatically extracted from the text of the page , and this is a substantial source of revenue supporting the web today .","label":"CompareOrContrast","metadata":{},"score":"66.19815"}{"text":"Latent variable grammars take an observed ( coarse ) treebank and induce more fine - grained grammar categories , that are better suited for modeling the syntax of natural languages .Estimation can be done in a generative or a discriminative framework , and results in the best published parsing accuracies over a wide range of syntactically divergent languages and domains .","label":"CompareOrContrast","metadata":{},"score":"66.6206"}{"text":"Ball Tracking :The reliable tracking of the ball is vital in robot soccer .Therefore a Kalman - filter based system for estimating the ball position and velocity in the presence of occlusions is developped . -Sensor Fusion : The robot perceives its environment through several independent sensors ( camera , odometer , etc . ) , which have different delays .","label":"CompareOrContrast","metadata":{},"score":"66.84354"}{"text":"[ Show abstract ] [ Hide abstract ] ABSTRACT : Many knowledge acquisition tasks are tightly dependent on fundamental analysis technologies , such as part of speech ( POS ) tagging and parsing .Dependency parsing , in particular , has been widely employed for the acquisition of knowledge related to predicate - argument structures .","label":"CompareOrContrast","metadata":{},"score":"66.963455"}{"text":"The random forest language model ( RFLM ) has shown encouraging results in several automatic speech recognition ( ASR ) tasks but has been hindered by practical limitations , notably the space - complexity of RFLM estimation from large amounts of data .","label":"CompareOrContrast","metadata":{},"score":"67.00392"}{"text":"Identifying relevant ads is challenging because queries are usually very short , and because users , consciously or not , choose terms intended to lead to optimal Web search results and not to optimal ads .Furthermore , the ads themselves are short and usually formulated to capture the reader 's attention rather than to facilitate query matching .","label":"CompareOrContrast","metadata":{},"score":"67.33196"}{"text":"Previous approaches of Japanese sentence compression can be divided into two groups .Word - based methods extract a subset of words from a sentence to shorten it , while bunsetsubased methods extract a subset of bunsetsu ( where a bunsetsu is a text unit that consists of content words and following function words ) .","label":"CompareOrContrast","metadata":{},"score":"67.53905"}{"text":"We test on child - directed utterances in English and Spanish and compare to single - task base - lines .In the morphologically poorer language ( English ) , the model improves morphological segmentation , while in the morphologically richer language ( Spanish ) , it leads to better syntactic categorization .","label":"CompareOrContrast","metadata":{},"score":"67.79328"}{"text":"No preview · Article · Oct 2013 · ACM Transactions on Asian Language Information Processing .[ Show abstract ] [ Hide abstract ] ABSTRACT : This paper proposes new distortion models for phrase - based SMT .In decoding , a distortion model estimates the source word position to be translated next ( NP ) given the last translated source word position ( CP ) .","label":"CompareOrContrast","metadata":{},"score":"69.168"}{"text":"UPPARSE , the software used for the experiments in this paper , is available under an open - sourc ... . \" ...In this work we address the problem of unsupervised part - of - speech induction by bringing together several strands of research into a single model .","label":"CompareOrContrast","metadata":{},"score":"69.29753"}{"text":"We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task .For languages from different families the improvements often exceed 2 BLEU .Many of these gains are also significant in human evaluations .We present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages : German , English , Swedish , Spanish , French and Korean .","label":"CompareOrContrast","metadata":{},"score":"69.533775"}{"text":"A key technical challenge in sponsored search is to select ads that are relevant for the user 's query .Iden ... \" .The business of Web search , a $ 10 billion industry , relies heavily on sponsored search , whereas a few carefully - selected paid advertisements are displayed alongside algorithmic search results .","label":"CompareOrContrast","metadata":{},"score":"70.745605"}{"text":"For instance , 14.4 % of section 23 is tagged differently by ( 1 ) and ( 2 ) 8 .5 The Neutral Edge Direction ( NED ) Me ... . by Shay B. Cohen , Noah A. Smith , Alex Clark , Dorota Glowacka , Colin De La Higuera , Mark Johnson , John Shawe - taylor . \" ...","label":"CompareOrContrast","metadata":{},"score":"71.15512"}{"text":"For instance , 14.4 % of section 23 is tagged differently by ( 1 ) and ( 2 ) 8 .5 The Neutral Edge Direction ( NED ) Me ... . by Shay B. Cohen , Noah A. Smith , Alex Clark , Dorota Glowacka , Colin De La Higuera , Mark Johnson , John Shawe - taylor . \" ...","label":"CompareOrContrast","metadata":{},"score":"71.15512"}{"text":"Computers fail to track these in fast video , but sleight of hand fools humans as well : what happens too quickly we just can not see .We show a 3D tracker for these types of motions that relies on the recognition of familiar configurations in 2D images ( classification ) , and fills the gaps in - between ( interpolation ) .","label":"CompareOrContrast","metadata":{},"score":"71.41508"}{"text":"most languages are projective .In Figure 8 An example Chinese dependency tree .Although non - projec ... . \" ...Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .","label":"CompareOrContrast","metadata":{},"score":"71.562744"}{"text":"most languages are projective .In Figure 8 An example Chinese dependency tree .Although non - projec ... . \" ...Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .","label":"CompareOrContrast","metadata":{},"score":"71.562744"}{"text":"We present a new edition of the Google Books Ngram Corpus , which describes how often words and phrases were used over a period of five centuries , in eight languages ; it reflects 6 % of all books ever published .","label":"CompareOrContrast","metadata":{},"score":"71.67476"}{"text":"We describe a natural alternative for training se - quence labeling models , based on MIRA ( Margin In - fused Relaxed Algorithm ) .In addition , we describe a novel method for performing Viterbi - like decoding .We test MIRA and contrast it with other training algo - rithms and contrast our decoding algorithm ... \" .","label":"CompareOrContrast","metadata":{},"score":"72.98978"}{"text":"Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .On the other hand , our grammars are much more compact and substantially more accurate than previous work on automatic annotation .Despite its simplicity , our best grammar achieves an F1 of 89.9 % on the Penn Treebank , higher than most fully lexicalized systems .","label":"CompareOrContrast","metadata":{},"score":"74.45412"}{"text":"These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .We present a new measure , Neutral Edge Direction ( NED ) , and show that it greatly reduces this undesired phenomenon .","label":"CompareOrContrast","metadata":{},"score":"75.15519"}{"text":"These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .We present a new measure , Neutral Edge Direction ( NED ) , and show that it greatly reduces this undesired phenomenon .","label":"CompareOrContrast","metadata":{},"score":"75.15519"}{"text":"Nouns are divided into 12257 classes based on proper vs. common , singular vs. plural and different case - marker ( gen .. \" ...In this paper , we consider the problem of unsupervised morphological analysis from a new angle .","label":"CompareOrContrast","metadata":{},"score":"76.40657"}{"text":"Part - of - speech ( POS ) induction is one of the most popular tasks in research on unsupervised NLP .Many different methods have been proposed , yet comparisons are difficult to make since there is little consensus on evaluation framework , and many papers evaluate against only one or two competitor syste ... \" .","label":"CompareOrContrast","metadata":{},"score":"76.72789"}{"text":"To deal with zero exophora , our model adds pseudo entities corresponding to zero exophora to candidate referents of zero pronouns .In addition , we automatically detect mentions that refer to the author and reader of a document by using lexico - syntactic patterns .","label":"CompareOrContrast","metadata":{},"score":"76.75881"}{"text":"The experimental results demonstrate the effectiveness of our model for not only zero exophora but also zero endophora .[ Show abstract ] [ Hide abstract ] ABSTRACT : We propose a method for automatically acquiring knowledge about case alternations between the passive / causative and active voices .","label":"CompareOrContrast","metadata":{},"score":"76.789604"}{"text":".. \" ...We show how punctuation can be used to improve unsupervised dependency parsing .Our linguistic analysis confirms the strong connection between English punctuation and phrase boundaries in the Penn Treebank .However , approaches that naively include punctuation marks in the grammar ( as if they were wo ... \" .","label":"CompareOrContrast","metadata":{},"score":"77.717705"}{"text":"[ Show abstract ] [ Hide abstract ] ABSTRACT : In pursuing machine understanding of human language , highly accurate syntactic analysis is a crucial step .In this work , we focus on dependency grammar , which models syntax by encoding transparent predicate - argument structures .","label":"CompareOrContrast","metadata":{},"score":"78.84263"}{"text":"[ Show abstract ] [ Hide abstract ] ABSTRACT : This article proposes a new distortion model for phrase - based statistical machine translation .In decoding , a distortion model estimates the source word position to be translated next ( subsequent position ; SP ) given the last translated source word position ( current position ; CP ) .","label":"CompareOrContrast","metadata":{},"score":"79.22802"}{"text":"Our linguistic analysis confirms the strong connection between English punctuation and phrase boundaries in the Penn Treebank .However , approaches that naively include punctuation marks in the grammar ( as if they were words ) do not perform well with Klein and Manning 's Dependency Model with Valence ( DMV ) .","label":"CompareOrContrast","metadata":{},"score":"79.887566"}{"text":"Since the Chinese characters in Japanese originated from ancient China , many common Chinese characters exist between these two languages .Since Chinese characters contain significant semantic information and common Chinese characters share the same meaning in the two languages , they can be quite useful in Chinese - Japanese machine translation ( MT ) .","label":"CompareOrContrast","metadata":{},"score":"81.94699"}{"text":"Our target language is Finnish , and in order to evade the problems of its rich morphology , we use sub - word units , morphs , as model units instead of the words .In the proposed model we apply incremental growing and clustering of the morph n - gram histories .","label":"CompareOrContrast","metadata":{},"score":"83.30156"}{"text":"In addition , we describe a novel method for performing Viterbi - like decoding .We test MIRA and contrast it with other training algo - rithms and contrast our decoding algorithm with the vanilla Viterbi algorithm . ces of the transition and emission features .","label":"CompareOrContrast","metadata":{},"score":"84.05592"}{"text":"This work concerns building n - gram language models that are suitable for large vocabulary speech recognition in devices that have a restricted amount of memory and space available .Our target language is Finnish , and in order to evade the problems of its rich morphology , we use sub - word units , morph ... \" .","label":"CompareOrContrast","metadata":{},"score":"89.12122"}{"text":"This Master 's thesis describes parts of the control software used by the soccer robots of the Free University of Berlin , the so called FU - Fighters .The FU - Fighters compete in the Middle Sized League of RoboCup and reached the semi - finals during the 2004 RoboCup World Cup in Lisbon , Portugal .","label":"CompareOrContrast","metadata":{},"score":"104.490776"}{"text":"[ Show abstract ] [ Hide abstract ] ABSTRACT : In Japanese , zero references often occur and many of them are categorized into zero ex - ophora , in which a referent is not mentioned in the document .However , previous studies have focused on only zero endophora , in which a referent explicitly appears .","label":"CompareOrContrast","metadata":{},"score":"105.19972"}