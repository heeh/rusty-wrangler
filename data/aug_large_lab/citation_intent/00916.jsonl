{"text":"Discriminative approaches have shown better performance given enough data , as they are better tailored to the prediction task and appear more robust to model misspecification .Generative modeling is a principled way of encoding this additional information , e.g. , through probabilistic graphical models or stochastic grammar rules .","label":"Background","metadata":{},"score":"30.23998"}{"text":"Discriminative classification : in this step , the objects in the generative embedding space are classified .In particular , we consider information theo- retic kernels , to be used in SVM and nearest neighbor techniques .The following subsections describe each of these step in detail .","label":"Background","metadata":{},"score":"30.381939"}{"text":"( 2005 ) conclude that hierarchical classification noticeably if still modestly outperforms flat classification .Classifier effectiveness remains limited by the very small number of training documents for many classes .For a more general approach that can be applied to modeling relations between classes , which may be arbitrary rather than simply the case of a hierarchy , see Tsochantaridis et al .","label":"Background","metadata":{},"score":"33.664795"}{"text":"Theoretical analysis of generative vs. discriminative learning .Techniques for combining generative and discriminative approaches .Successful applications of hybrids .Empirical comparison of generative vs. discriminative learning .Inclusion of prior knowledge in discriminative methods ( semi - supervised approaches , generalized expectation criteria , posterior regularization , etc . ) .","label":"Background","metadata":{},"score":"36.55092"}{"text":"This was one of several pieces of work from this time that established the strong reputation of SVMs for text classification .Another pioneering work on scaling and evaluating SVMs for text classification was ( Joachims , 1998 ) .We present some of his results from ( Joachims , 2002a ) in Table 15.2 .","label":"Background","metadata":{},"score":"37.745407"}{"text":"Tong and Koller ( 2001 ) explore active learning with SVMs for text classification ; Baldridge and Osborne ( 2004 ) point out that examples selected for annotation with one classifier in an active learning context may be no better than random examples when used with another classifier .","label":"Background","metadata":{},"score":"38.18691"}{"text":"Theoretical analysis of generative versus discriminative learning has a long history in statistics , where the focus was on asymptotic analyses ( e.g. [ Efron 75 ] ) .Ng and Jordan provided an initial comparison of generative versus discriminative learning in the non - asymptotic regime in the most cited paper on the topic in machine learning [ Ng 02].","label":"Background","metadata":{},"score":"38.563236"}{"text":"Doctoral dissertation , Université de Paris XI , 1991 .[ Bouchard 04 ] G. Bouchard and B. Triggs , The tradeoff between generative and discriminative classifiers .In J. Antoch , editor , Proc . of COMPSTAT'04 , 16th Symposium of IASC , volume 16 .","label":"Background","metadata":{},"score":"39.661682"}{"text":"It is again noticeable the extent to which different papers ' results for the same machine learning methods differ .In particular , based on replications by other researchers , the Naive Bayes results of ( Joachims , 1998 ) appear too weak , and the results in Table 13.9 should be taken as representative .","label":"Background","metadata":{},"score":"40.8218"}{"text":"It seems that working with simple term features can get one a long way .It is again noticeable the extent to which different papers ' results for the same machine learning methods differ .In particular , based on replications by other researchers , the Naive Bayes results of ( Joachims , 1998 ) appear too weak , and the results in Table 13.9 should be taken as representative .","label":"Background","metadata":{},"score":"41.492058"}{"text":"We consider the problem of training discriminative structured output predictors , such as conditional random fields ( CRFs ) and structured support vector machines ( SSVMs ) .A generalized loss function is introduced , which jointly maximizes the entropy and the margin of the solution .","label":"Background","metadata":{},"score":"41.896885"}{"text":"A detailed description of the algorithm can be found in [ Joachims , 1999c ] .A similar transductive learner , which can be thought of as a transductive version of k - Nearest Neighbor is the Spectral Graph Transducer .The code has been used on a large range of problems , including text classification [ Joachims , 1999c ] [ Joachims , 1998a ] , image recognition tasks , bioinformatics and medical applications .","label":"Background","metadata":{},"score":"42.68709"}{"text":"The CRF and SSVM emerge as special cases of our framework .The probabilistic interpretation of large margin methods reveals insights about margin and slack rescaling .Furthermore , we derive the corresponding extensions for latent variable models , in which training operates on partially observed outputs .","label":"Background","metadata":{},"score":"43.082966"}{"text":"Instead of relying on standard kernels , we investigate the use of the recently introduced information theoretic ( IT ) kernels [ 15 ] as a similarity measure between objects in the generative embedding space .The main idea is that , with such kernels , we can exploit the probabilistic nature of the generative embeddings , improving even more the classification results of the hybrid approaches - this has been already shown in other classification contexts [ 3,16].","label":"Background","metadata":{},"score":"43.085796"}{"text":"Over the last decade , much of the research on discriminative learning has focused on problems like classification and regression , where the prediction is a single univariate variable .But what if we need to predict complex objects like trees , orderings , or alignments ?","label":"Background","metadata":{},"score":"43.1048"}{"text":"The probabilistic interpretation of large margin methods reveals insights about margin and slack rescaling .Furthermore , we derive the corresponding extensions for latent variable models , in which training operates on partially observed outputs .Experimental results for multiclass , linear - chain models and multiple instance learning demonstrate that the generalized loss can improve accuracy of the resulting classifiers .","label":"Background","metadata":{},"score":"43.39048"}{"text":"International Conference on Machine Learning ( ICML ) , 1999 .[Postscript ( gz ) ] [ PDF ] [ BibTeX ] .[ Morik et al . , 1999a ] .K. Morik , P. Brockhausen , and T. Joachims , Combining statistical learning with a knowledge - based approach - A case study in intensive care monitoring .","label":"Background","metadata":{},"score":"43.467354"}{"text":"Quattoni , A. , Wang , S. , Morency , L. , Collins , M. , Darrell , T. : Hidden - state conditional random fields .PAMI 29(10 ) , 1848 - 1852 ( 2007 ) .Yu , C. , Joachims , T. : Learning structural SVMs with latent variables .","label":"Background","metadata":{},"score":"43.66616"}{"text":"Quattoni , A. , Wang , S. , Morency , L. , Collins , M. , Darrell , T. : Hidden - state conditional random fields .PAMI 29(10 ) , 1848 - 1852 ( 2007 ) .Yu , C. , Joachims , T. : Learning structural SVMs with latent variables .","label":"Background","metadata":{},"score":"43.66616"}{"text":"In : Advances in Neural Information Processing Systems ( 2009 ) 19 .Perina , A. , Cristani , M. , Castellani , U. , Murino , V. , Jojic , N. : A hybrid genera- tive / discriminative classification framework based on free - energy terms .","label":"Background","metadata":{},"score":"43.809853"}{"text":"( 1998 ) , who used MI feature selection ( Section 13.5.1 , page 13.5.1 ) to build classifiers with a much more limited number of features .The success of the linear SVM mirrors the results discussed in Section 14.6 ( page ) on other linear approaches like Naive Bayes .","label":"Background","metadata":{},"score":"43.8252"}{"text":"( 1998 ) given in Table 13.9 show SVMs clearly performing the best .This was one of several pieces of work from this time that established the strong reputation of SVMs for text classification .Another pioneering work on scaling and evaluating SVMs for text classification was ( Joachims , 1998 ) .","label":"Background","metadata":{},"score":"43.894127"}{"text":"In : Proceedings of the IEEE International Conference on Image Processing . pp .2661 - 2664 ( 2010 ) 4 .Blei , D. , Ng , A. , Jordan , M. : Latent Dirichlet allocation .Journal of Machine Learn- ing Research 3 , 993 - 1022 ( 2003 ) 5 .","label":"Background","metadata":{},"score":"44.34996"}{"text":"In particular we employ the pLSA model , for the reasons explained below .Generative embedding : in this step , all the objects involved in the prob- lem ( namely training and testing patterns ) are embedded , using the learned model , in a vector space .","label":"Background","metadata":{},"score":"44.43582"}{"text":"Computational issues in discriminatively trained generative models / hybrid models .Map of possible generative / discriminative approaches and combinations .Bayesian approaches optimized for predictive performance .Comparison of model - free and model - based approaches in statistics or reinforcement learning .","label":"Background","metadata":{},"score":"44.44439"}{"text":"The only important fact that needs to be pointed out here is that ( as the posterior distribution embedding ) , the components of the FESS embedding of any object are all non - negative .3.4Discriminative Classification In a typical hybrid generative - discriminative classification scenario , the feature vectors resulting from the generative embedding are used to feed some kernel- based classifier , namely , a support vector machine ( SVM ) with simple linear or radial basis function ( RBF ) kernels .","label":"Background","metadata":{},"score":"44.5005"}{"text":"Different approaches have been proposed in the past , each one with different characteristics , in terms of interpretability , efficacy , efficiency , and others .This representation with the topic posteriors has been already successfully used in computer vision tasks [ 9,5 ] as well as in medical informatics [ 8,2].","label":"Background","metadata":{},"score":"44.510025"}{"text":"[ Klinkenberg , Joachims , 2000a ] .R. Klinkenberg and T. Joachims , Detecting Concept Drift with Support Vector Machines .Proceedings of the Seventeenth International Conference on Machine Learning ( ICML ) , Morgan Kaufmann , 2000 .[Postscript ( gz ) ] [ PDF ( gz ) ] [ BibTeX ] .","label":"Background","metadata":{},"score":"44.787132"}{"text":"References .Tsochantaridis , I. , Hofmann , T. , Joachims , T. , Altun , Y. : Support vector machine learning for interdependent and structured output spaces .In : ICML , p. 104( 2004 ) .Taskar , B. , Guestrin , C. , Koller , D. : Max - margin Markov networks .","label":"Background","metadata":{},"score":"44.85516"}{"text":"Hofmann , T. : Unsupervised learning by probabilistic latent semantic analysis .Ma- chine Learning 42(1 - 2 ) , 177 - 196 ( 2001 ) 12 .Jaakkola , T. , Haussler , D. : Exploiting generative models in discriminative classi- fiers .","label":"Background","metadata":{},"score":"45.01168"}{"text":"Cao et al .( 2006 ) study how to make this approach effective in IR , and Qin et al .( 2007 ) suggest an extension involving using multiple hyperplanes .Yue et al .( 2007 ) study how to do ranking with a structural SVM approach , and in particular show how this construction can be effectively used to directly optimize for MAP ranked - evaluation , rather than using surrogate measures like accuracy or area under the ROC curve .","label":"Background","metadata":{},"score":"45.225883"}{"text":"This tutorial discusses recent advances in discriminative training methods for such structured prediction problems like Conditional Random Fields ( CRF ) , Maximum Margin Markov Networks , and Structural Support Vector Machines .In particular , the tutorial focuses on large - margin approaches to predicting structured outputs , and how the idea of margins can be generalized to complex prediction problems and a large range of loss functions .","label":"Background","metadata":{},"score":"45.5823"}{"text":"1169 - 1176 ( 2009 ) .Canu , S. , Smola , A.J. : Kernel methods and the exponential family .Neurocomputing 69(7 - 9 ) , 714 - 720 ( 2006 ) CrossRef .Chapelle , O. , Zien , A. : Semi - supervised classification by low density separation .","label":"Background","metadata":{},"score":"46.002167"}{"text":"1169 - 1176 ( 2009 ) .Canu , S. , Smola , A.J. : Kernel methods and the exponential family .Neurocomputing 69(7 - 9 ) , 714 - 720 ( 2006 ) CrossRef .Chapelle , O. , Zien , A. : Semi - supervised classification by low density separation .","label":"Background","metadata":{},"score":"46.002167"}{"text":"This .Page 7 . 7 embedding expresses how well each data point fits different parts of the gen- erative model , using the variational free energy as a lower bound on the nega- tive log - likelihood .It has been shown that the FESS embedding yields highly informative for discriminative representations that lead to state - of - the - art re- sults in several computational biology and computer vision problems ( namely , scene / object recognition ) [ 19,18].","label":"Background","metadata":{},"score":"46.21796"}{"text":"pp .487 - 493 ( 1999 ) 13 .Nature Medicine 4 , 844 - 847 ( 1998 ) 14 .Lasserre , J. , Bishop , C. , Minka , T. : Principled hybrids of generative and discrim- inative models .","label":"Background","metadata":{},"score":"46.279186"}{"text":"( 2007 ) study feature selection for the ranking problem .Workshop on the Generative and Discriminative Learning Interface In conjunction with the NIPS conference .Overview .Generative and discriminative learning are two of the major paradigms for solving prediction problems in machine learning , each offering important distinct advantages .","label":"Background","metadata":{},"score":"46.321266"}{"text":"[ McCallum 06 ] A. McCallum , C. Pal , G. Druck and X. Wang , Multi - Conditional Learning : Generative / Discriminative Training for Clustering and Classification .AAAI , 2006 .[ Ng 02 ] A. Y. Ng and M. I. Jordan , On Discriminative vs. Generative Classifiers : A comparison of logistic regression and Naive Bayes .","label":"Background","metadata":{},"score":"46.95085"}{"text":"The goal is to learn a function from preference examples , so that it orders a new set of objects as accurately as possible .Such ranking problems naturally occur in applications like search engines and recommender systems .Futhermore , this version includes an algorithm for training large - scale transductive SVMs .","label":"Background","metadata":{},"score":"47.202072"}{"text":"Tsochantaridis , I. , Hofmann , T. , Joachims , T. , Altun , Y. : Support vector machine learning for interdependent and structured output spaces .In : ICML , p. 104( 2004 ) .Taskar , B. , Guestrin , C. , Koller , D. : Max - margin Markov networks .","label":"Background","metadata":{},"score":"47.23368"}{"text":"[ Wettig 03 ] H. Wettig , P. Grünwald , T. Roos , P. Myllymäki and H.Tirri , When discriminative learning of Bayesian network parameters is easy .In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence ( IJCAI 2003 ) , 491 - 496 , August 2003 .","label":"Background","metadata":{},"score":"47.321747"}{"text":"Rubinstein , Y.D. , Hastie , T. : Discriminative vs informative learning .In : Proceed- ings of the Third International Conference on Knowledge Discovery and Data Min- ing .pp .49 - 53 .AAAI Press ( 1997 ) 21 .","label":"Background","metadata":{},"score":"47.541924"}{"text":"Given the trained generative model , two generative embedding spaces have been considered : the posterior distribution over topics ( as in [ 5,8 ] ) ; the very recently proposed free energy score space ( FESS ) [ 19,18].","label":"Background","metadata":{},"score":"47.56419"}{"text":"The representation of documents and words with topic models has one clear advan- tage : each topic is individually interpretable , providing a probability distribution over words that picks out a coherent cluster of correlated terms .This may be advantageous in the cancer detection context , since the final goal is to provide knowledge about complex systems , and provide possible hidden correlations .","label":"Background","metadata":{},"score":"47.662014"}{"text":"Joachims used a large number of term features in contrast to Dumais et al .( 1998 ) , who used MI feature selection ( Section 13.5.1 , page 13.5.1 ) to build classifiers with a much more limited number of features .","label":"Background","metadata":{},"score":"47.808228"}{"text":"Andrews , S. , Tsochantaridis , I. , Hofmann , T. : Support vector machines for multiple - instance learning .In : NIPS , pp .561 - 568 ( 2003 ) Abstract .We consider the problem of training discriminative structured output predictors , such as conditional random fields ( CRFs ) and structured support vector machines ( SSVMs ) .","label":"Background","metadata":{},"score":"48.06835"}{"text":"463 - 472 .Springer ( 2010 ) 17 .Ng , A. , Jordan , M. : On discriminative vs generative classifiers : A comparison of logistic regression and naive Bayes .In : Advances in Neural Information Processing Systems ( 2002 ) 18 .","label":"Background","metadata":{},"score":"48.09252"}{"text":"Results are shown for the 10 largest categories and for microaveraged performance over all 90 categories on the Reuters-21578 data set .We presented results in Section 13.6 showing that an SVM is a very effective text classifier .The results of Dumais et al .","label":"Background","metadata":{},"score":"48.108574"}{"text":"Shawe - Taylor , J. , Cristianini , N. : Kernel Methods for Pattern Analysis .Cambridge University Press ( 2004 ) 24 .Tsuda , K. , Kawanabe , M. , R¨ atsch , G. , Sonnenburg , S. , M¨ uller , K.R. : A new dis- criminative kernel from probabilistic models .","label":"Background","metadata":{},"score":"48.324284"}{"text":"57 - 64 ( 2005 ) .Zhang , T. : Class - size independent generalization analysis of some discriminative multi - category classification .In : NIPS , Cambridge , MA ( 2005 ) .Shi , Q. , Reid , M. , Caetano , T. : Hybrid model of conditional random field and support vector machine .","label":"Background","metadata":{},"score":"48.411106"}{"text":"57 - 64 ( 2005 ) .Zhang , T. : Class - size independent generalization analysis of some discriminative multi - category classification .In : NIPS , Cambridge , MA ( 2005 ) .Shi , Q. , Reid , M. , Caetano , T. : Hybrid model of conditional random field and support vector machine .","label":"Background","metadata":{},"score":"48.411106"}{"text":"In : International Conference on Med- ical Image Computing and Computer Assisted Intervention ( 2010 ) 9 .Cristani , M. , Perina , A. , Castellani , U. , Murino , V. : Geo - located image analysis using latent representations .","label":"Background","metadata":{},"score":"49.055832"}{"text":"NOTE :SVM rank is a new algorithm for training Ranking SVMs that is much faster than SVM light in ' -z p ' mode ( available here ) .Questions and Bug Reports .If you find bugs or you have problems with the code you can not solve by yourself , please contact me via email .","label":"Background","metadata":{},"score":"49.263283"}{"text":"ICML , 2006 .( paper ) [ Kent , Mark ] ( 20 min ) . 03/02: Brooke Cowan , Ivona Kucerova , and Michael Collins , A Discriminative Model for Tree - to - Tree Translation , EMNLP 2006 .","label":"Background","metadata":{},"score":"49.52089"}{"text":"Exploiting generative models in discriminative classifiers .In Advances in Neural Information Processing Systems 11 , 1998 .[ Jaakkola 99 ] T. Jaakkola , M. Meila , and T. Jebara .Maximum entropy discrimination .In Advances in Neural Information Processing Systems 12 .","label":"Background","metadata":{},"score":"49.55051"}{"text":"In : Hancock , E. , Wilson , R. , Windeatt , T. , Ulusoy , I. , Escolano , F. ( eds . )Proceedings of the International Workshop on Structural , Syntactic , and Statistical Pattern Recognition , Lecture Notes in Computer Science , vol .","label":"Background","metadata":{},"score":"50.01628"}{"text":"Machine Learning Course : If you would like to learn more about Machine Learning , you can find videos , slides , and readings of the course I teach at Cornell here .SVM struct : SVM learning for multivariate and structured outputs like trees , sequences , and sets ( available here ) .","label":"Background","metadata":{},"score":"50.196136"}{"text":"Almost unbiased estimates provides leave - one - out testing .SVM light exploits that the results of most leave - one - outs ( often more than 99 % ) are predetermined and need not be computed [ Joachims , 2002a ] .","label":"Background","metadata":{},"score":"51.020096"}{"text":"Zhang , T. , Oles , F.J. : Text categorization based on regularized linear classification methods .Information Retrieval 4 , 5 - 31 ( 2000 ) CrossRef .Collins , M. , Globerson , A. , Koo , T. , Carreras , X. , Bartlett , P.L. : Exponentiated gradient algorithms for conditional random fields and max - margin Markov networks .","label":"Background","metadata":{},"score":"51.222214"}{"text":"Zhang , T. , Oles , F.J. : Text categorization based on regularized linear classification methods .Information Retrieval 4 , 5 - 31 ( 2000 ) CrossRef .Collins , M. , Globerson , A. , Koo , T. , Carreras , X. , Bartlett , P.L. : Exponentiated gradient algorithms for conditional random fields and max - margin Markov networks .","label":"Background","metadata":{},"score":"51.222214"}{"text":"With NN classification , comparing Table 4 with Table 2 , we see that the accuracies increase except for FESS with JT - W2 .Table 3 .Average accuracies ( in percentage ) using pLSA and FESS embeddings with SVMs in the supervised pLSA learning setup .","label":"Background","metadata":{},"score":"51.376965"}{"text":"It also lets you directly optimize multivariate performance measures like F1-Score , ROC - Area , and the Precision / Recall Break - Even Point .( available here ) .SVM rank : New algorithm for training Ranking SVMs that is much faster than SVM light in ' -z p ' mode .","label":"Background","metadata":{},"score":"51.43627"}{"text":"T. Joachims , Estimating the Generalization Performance of a SVM Efficiently .Proceedings of the International Conference on Machine Learning , Morgan Kaufman , 2000 .[Postscript ( gz ) ] [ PDF ] [ BibTeX ] .[ Joachims , 1999a ] .","label":"Background","metadata":{},"score":"51.866394"}{"text":"[ Salojarvi 05 ] J. Salojärvi , K. Puolamäki and S. Kaski , Expectation maximization algorithms for conditional likelihoods .In Proceedings of the 22nd International Conference on Machine Learning ( ICML ) , 2005 .[ Schmah 09 ] T. Schmah , G. E Hinton , R. Zemel , S. L. Small and S. Strother , Generative versus discriminative training of RBMs for classification of fMRI images .","label":"Background","metadata":{},"score":"52.314915"}{"text":"[ Jebara 04 ] T. Jebara , Machine Learning - Discriminative and Generative .International Series in Engineering and Computer Science , Springer , Vol .[ Liang 08 ] P. Liang and M. I. Jordan , An asymptotic analysis of generative , discriminative , and pseudo - likelihood estimators .","label":"Background","metadata":{},"score":"52.31758"}{"text":"Typically , the feature vectors resulting from the generative embedding are used to feed some kernel - based classifier , namely , a support vector machine ( SVM ) with linear or radial basis function ( RBF ) kernels .In this paper , we follow an alternative route .","label":"Background","metadata":{},"score":"52.340935"}{"text":"Given these recent trends , a workshop on the interplay of generative and discriminative learning seem especially relevant .Hybrid generative - discriminative techniques face computational challenges .Alternatively , the use of generative models in predictive settings has been be explored , e.g. , through the use of Fisher kernels [ Jaakkola 98 ] or other probabilistic kernels .","label":"Background","metadata":{},"score":"52.448235"}{"text":"Description .SVM light is an implementation of Vapnik 's Support Vector Machine [ Vapnik , 1995 ] for the problem of pattern recognition , for the problem of regression , and for the problem of learning a ranking function .The optimization algorithms used in SVM light are described in [ Joachims , 2002a ] .","label":"Background","metadata":{},"score":"52.52974"}{"text":"The complexity of the data , as well as the intensive labor needed to obtain them , makes the development of such automatic tools very problem- atic .In this paper , we consider the problem of classifying cancer tissues from tissue microarray ( TMA ) data , a technology which enables studies associating molecular changes with clinical endpoints [ 13].","label":"Background","metadata":{},"score":"52.53553"}{"text":"SVM classifier break - even F from ( Joachims , 2002a , p. 114 ) .Results are shown for the 10 largest categories and for microaveraged performance over all 90 categories on the Reuters-21578 data set .We presented results in Section 13.6 showing that an SVM is a very effective text classifier .","label":"Background","metadata":{},"score":"52.633347"}{"text":"II .Computational aspects of mechanism design , including whether and how mechanisms ' outcomes can be efficiently computed ; algorithms for automatically designing the entire mechanism ; and limitations of classical results in the face of computationally bounded agents .Example applications will be provided throughout .","label":"Background","metadata":{},"score":"52.96347"}{"text":"A Support Vector Method for Optimizing Average Precision .SIGIR , 2007 .( paper ) . 03/04 : Yisong Yue , T. Joachims .Predicting Diverse Subsets Using Structural SVMs .ICML , 2008 .( paper ) .03/09 : Matthew Blaschko , Christoph Lampert .","label":"Background","metadata":{},"score":"52.973957"}{"text":"The goal of this workshop is to map out our current understanding of the empirical and theoretical advantages of each approach as well as their combination , and to identify open research directions .Background and Objectives .In generative approaches for prediction tasks , one models a joint distribution on inputs and outputs and parameters are typically estimated using a likelihood - based criterion .","label":"Background","metadata":{},"score":"53.016644"}{"text":"pp . 1 - 8 ( 2008 ) 10 .Fuchs , T. , Wild , P. , Moch , H. , Buhmann , J. : Computational pathology analy- sis of tissue microarrays predicts survival of renal clear cell carcinoma patients .","label":"Background","metadata":{},"score":"53.03714"}{"text":"The results of the NN classifier are shown in Table 2 .Although NN is not a good choice for this experiment ( baseline NN accuracy using Mahalanobis distance on the original data is 64.57 % ) , we still see the advantage of the IT kernels on the generative approach .","label":"Background","metadata":{},"score":"53.152645"}{"text":"The aim of this workshop is to provide a platform for both theoretical and applied researchers from different communities to discuss the status of our understanding on the interplay between generative and discriminative learning , as well as to identify forward - looking open problems of interest to the NIPS community .","label":"Background","metadata":{},"score":"53.29194"}{"text":"( see [ 5 ] ) .-o ] 0 . .2 ] - value of rho for XiAlpha - estimator and for pruning .leave - one - out computation ( default 1.0 ) .( see [ Joachims , 2002a ] ) .","label":"Background","metadata":{},"score":"53.38909"}{"text":"How can machine learning help understand and summarize content , trends , dependencies , and idea flows in such archives ?The content of the course will reflect a balance of learning methods , algorithms , and their theoretical understanding , putting an emphasis on approaches with practical relevance .","label":"Background","metadata":{},"score":"53.48455"}{"text":"If you use SVM light in your scientific work , please cite as .T. Joachims , Making large - Scale SVM Learning Practical .Advances in Kernel Methods - Support Vector Learning , B. Schölkopf and C. Burges and A. Smola ( ed . ) , MIT - Press , 1999 .","label":"Background","metadata":{},"score":"53.58811"}{"text":"He is best known for his work in inductive transfer , semi - supervised learning , and optimizing learning for different performance criteria .Rich likes to mix algorithm development with applications work to insure that the methods he developes really work in practice .","label":"Background","metadata":{},"score":"53.651226"}{"text":"In : Proceedings of the 32nd DAGM conference on Pattern recognition .pp .202 - 211 .Springer ( 2010 ) 22 .Sch¨ uffler , P. , Ula¸ s , A. , Castellani , U. , Murino , V. : A multiple kernel learning algo- rithm for cell nucleus classification of renal cell carcinoma .","label":"Background","metadata":{},"score":"53.66878"}{"text":"For the foundations by their originator , see ( Vapnik , 1998 ) .Some recent , more general books on statistical learning , such as ( Hastie et al . , 2001 ) also give thorough coverage of SVMs .The kernel trick was first presented in ( Aizerman et al . , 1964 ) .","label":"Background","metadata":{},"score":"53.680622"}{"text":"New York ( 2006 ) .Page 12 .Martins , A. , Smith , N. , Xing , E. , Aguiar , P. , Figueiredo , M. : Nonextensive infor- mation theoretic kernels on measures .Journal of Machine Learning Research 10 , 935 - 975 ( 2009 ) 16 .","label":"Background","metadata":{},"score":"53.8998"}{"text":"It can also be interesting to look at the \" training error \" of the ranking SVM .The equivalent of training error for a ranking SVM is the number of training pairs that are misordered by the learned model .To find those pairs , one can apply the model to the training file : .","label":"Background","metadata":{},"score":"53.93155"}{"text":"Gimpel , K. , Smith , N. : Softmax - margin crfs : Training log - linear models with cost functions .In : HLT , pp .733 - 736 ( 2010 ) .Crammer , K. , Singer , Y. : On the algorithmic implementation of multiclass kernel - based vector machines .","label":"Background","metadata":{},"score":"54.05648"}{"text":"Gimpel , K. , Smith , N. : Softmax - margin crfs : Training log - linear models with cost functions .In : HLT , pp .733 - 736 ( 2010 ) .Crammer , K. , Singer , Y. : On the algorithmic implementation of multiclass kernel - based vector machines .","label":"Background","metadata":{},"score":"54.05648"}{"text":"401 - 408 ( 2007 ) 7 .Boykov , Y. , Veksler , O. , Zabih , R. : Efficient approximate energy minimization via graph cuts .IEEE transactions on Pattern Analysis and Machine Intelligence 20(12 ) , 1222 - 1239 ( 2001 ) 8 .","label":"Background","metadata":{},"score":"54.12737"}{"text":"We 'll also briefly review the history of inductive transfer , discuss what kinds of applications will benefit from inductive transfer , and give a few heuristics for getting the most benefit when using transfer on real problems .Speaker Biography .","label":"Background","metadata":{},"score":"54.41564"}{"text":"svm_learn ( learning module ) svm_classify ( classification module ) .If you do not want to use the built - in optimizer but PR_LOQO instead , create a subdirectory in the svm_light directory with .How to use .This section explains how to use the SVM light software .","label":"Background","metadata":{},"score":"54.620667"}{"text":"Andrews , S. , Tsochantaridis , I. , Hofmann , T. : Support vector machines for multiple - instance learning .In : NIPS , pp .561 - 568 ( 2003 ) Learning to Predict Trees , Sequences , and other Structured Outputs , Thorsten Joachims ( Cornell University ) .","label":"Background","metadata":{},"score":"54.62258"}{"text":"Maximum entropy Markov models for information extraction and segmentation .ICML , 2000 .( paper ) [ Ruogu ] ( 20 min ) .02/23 : John Lafferty , Andrew McCallum , Fernando Pereira , Conditional Random Fields : Probabilistic Models for Segmenting and Labeling Sequence Data .","label":"Background","metadata":{},"score":"55.223904"}{"text":"Updated makefile to add the ability for compiling SVM - light into a shared - object library that gives external code easy access to learning and classification functions .References .T. Joachims , Optimizing Search Engines Using Clickthrough Data , Proceedings of the ACM Conference on Knowledge Discovery and Data Mining ( KDD ) , ACM , 2002 .","label":"Background","metadata":{},"score":"55.228004"}{"text":"The Advances in Neural Information Processing ( NIPS ) conferences have become the premier venue for theoretical machine learning work , such as on SVMs .Other venues such as SIGIR are much stronger on experimental methodology and using text - specific features to improve classifier effectiveness .","label":"Background","metadata":{},"score":"55.334072"}{"text":"04/20 : S. Knoll , A. Hoff , D. Fischer , S. Dumais and E. Cutrell ( 2009 ) .Viewing personal data over time .In Proceedings of CHI 2009 Workshop on Interacting with Temporal Data .AND also using the references therein .","label":"Background","metadata":{},"score":"55.563217"}{"text":"But limited training data and poor machine learning techniques meant that these pieces of work achieved only middling results , and hence they only had limited impact at the time .Taylor et al .( 2006 ) study using machine learning to tune the parameters of the BM25 family of ranking functions okapi - bm25 so as to maximize NDCG ( Section 8.4 , page 8.4 ) .","label":"Background","metadata":{},"score":"55.71802"}{"text":"His research interests center on a synthesis of theory and system building in the field of machine learning , with a focus on Support Vector Machines and machine learning with text .He authored the SVM - Light algorithm and software for support vector learning .","label":"Background","metadata":{},"score":"56.056126"}{"text":"Advances in Kernel Methods - Support Vector Learning , B. Schölkopf and C. Burges and A. Smola ( ed . ) , MIT Press , 1999 .[Postscript ( gz ) ] [ PDF ] [ BibTeX ] .[ Joachims , 1999c ] .","label":"Background","metadata":{},"score":"56.08362"}{"text":"In this paper , we propose a hybrid generative / discriminative classification scheme and apply it to the detection of renal cell carcinoma ( RCC ) on tissue microarray ( TMA ) images .In particular we use proba- bilistic latent semantic analysis ( pLSA ) as a generative model to perform generative embedding onto the free energy score space ( FESS ) .","label":"Background","metadata":{},"score":"56.08484"}{"text":"Bakir , G. , Hofmann , T. , Schölkopf , B. , Smola , A. , Taskar , B. , Vishwanathan , S.V.N. : Predicting Structured Data .MIT Press , Cambridge ( 2007 ) .Wainwright , M. , Jordan , M. : Graphical models , exponential families , and variational inference .","label":"Background","metadata":{},"score":"56.39479"}{"text":"Bakir , G. , Hofmann , T. , Schölkopf , B. , Smola , A. , Taskar , B. , Vishwanathan , S.V.N. : Predicting Structured Data .MIT Press , Cambridge ( 2007 ) .Wainwright , M. , Jordan , M. : Graphical models , exponential families , and variational inference .","label":"Background","metadata":{},"score":"56.39479"}{"text":"CVPR , 2005 .( paper ) [ Sarah ] ( 20 min ) .02/18 : J. Weston , O. Chapelle , A. Elisseeff , B. Schoelkopf and V. Vapnik , Kernel Dependency Estimation , NIPS , 2002 .( paper ) [ Alex ] ( 20 min ) .","label":"Background","metadata":{},"score":"56.430504"}{"text":"Postscript ( gz ) ] [ PDF ] [ BibTeX ] .[ Joachims , 1998c ] .Thorsten Joachims , Making Large - Scale SVM Learning Practical .LS8-Report , 24 , Universität Dortmund , LS VIII - Report , 1998 .","label":"Background","metadata":{},"score":"56.87316"}{"text":"In this paper , the classification problem described in the previous para- graph is addressed by using hybrid generative - discriminative schemes [ 12,14].Page 2 . 2 Fig.1 .The nuclei classification pipeline : detection , segmentation and classification into benign or cancerous .","label":"Background","metadata":{},"score":"56.881657"}{"text":"pp .2728 - 2731 ( 2010 ) 2 .Bicego , M. , Lovato , P. , Oliboni , B. , Perina , A. : Expression microarray classification using topic models .In : ACM Symposium on Applied Computing ( Bioinformatics and Computational Biology track ) ( 2010 ) 3 .","label":"Background","metadata":{},"score":"57.309246"}{"text":"[Postscript ( gz ) ] [ PDF ] [ BibTeX ] .[ Joachims , 1998a ] .T. Joachims , Text Categorization with Support Vector Machines : Learning with Many Relevant Features .Proceedings of the European Conference on Machine Learning , Springer , 1998 .","label":"Background","metadata":{},"score":"57.43758"}{"text":"The algorithm has scalable memory requirements and can handle problems with many thousands of support vectors efficiently .The software also provides methods for assessing the generalization performance efficiently .It includes two efficient estimation methods for both error rate and precision / recall .","label":"Background","metadata":{},"score":"57.49375"}{"text":"Neural Processing Letters , 28(3 ) , 169 - 187 , 2008 .[ Xue 09 ] J.-H Xue and D.M. Titterington , Interpretation of hybrid generative / discriminative algorithms .Neurocomputing , 72(7 - 9 ) , 1648 - 1655 , 2009 .","label":"Background","metadata":{},"score":"57.658146"}{"text":"NOTE :SVM rank is a new algorithm for training Ranking SVMs that is much faster than SVM light in ' -z p ' mode ( available here ) .In all modes , the result of svm_learn is the model which is learned from the training data in example_file .","label":"Background","metadata":{},"score":"57.83015"}{"text":"( paper ) .02/11 : Chun - Nam John Yu , T. Joachims , R. Elber , J. Pillardy .Support Vector Training of Protein Alignment Models .Journal of Computational Biology , 15(7 ) : 867 - 880 , September 2008 .","label":"Background","metadata":{},"score":"57.927174"}{"text":"This implementation makes use of this property which leads to a very compact and efficient representation .Source Code and Binaries .The program is free for scientific use .Please contact me , if you are planning to use the software for commercial purposes .","label":"Background","metadata":{},"score":"57.92933"}{"text":"In : Proceedings of the European Conference on Computer Vision ( 2006 ) 6 .Bosch , A. , Zisserman , A. , Munoz , X. : Representing shape with a spatial pyramid kernel .In : Proceedings of the 6th ACM International Conference on Image and Video Retrieval .","label":"Background","metadata":{},"score":"57.940727"}{"text":"The methods will be illustrated with examples from application problems .Speaker Biography .Thorsten Joachims is an Assistant Professor in the Department of Computer Science at Cornell University .In 2001 , he finished his dissertation with the title \" The Maximum - Margin Approach to Learning Text Classifiers : Methods , Theory , and Algorithms \" , advised by Prof. Katharina Morik at the University of Dortmund .","label":"Background","metadata":{},"score":"57.986446"}{"text":"Table 1 .Average accuracies ( in percentage ) using pLSA and FESS embeddings with SVMs .The baseline SVM accuracy with the linear kernel on the original feature space is 75.45 % .LINJSJT JT - W1 JT - W2 PLSA 76.78 79.31 80.17 74.22 80.17 FESS 77.41 73.21 78.87 72.31 79.96","label":"Background","metadata":{},"score":"58.069466"}{"text":"Average accuracies ( in percentage ) using pLSA and FESS embeddings with NN classification in the supervised pLSA learning setup .We applied the proposed approach to the diagnosis of Renal Cell Carcinoma on tissue micro array ( TMA ) images .","label":"Background","metadata":{},"score":"58.126663"}{"text":"Both procedures drastically improved the following segmentation of cell nuclei . 2.3 Segmentation Segmentation of cell nuclei was performed using the graph cuts approach [ 7 ] , with the gray levels used in the unary potentials .The binary potentials were linearly weighted based on their distance to the center , to encourage roundish objects lying in the center of the patch ( see Figure 3 ) .","label":"Background","metadata":{},"score":"58.222225"}{"text":"[ Bouchard 07 ] G. Bouchard , Bias - variance tradeoff in hybrid generative - discriminative models .In proc . of the Sixth International conference on Machine Learning and Applications ( ICMLA 07 ) , Cincinnati , Ohio , USA , 13 - 15 December 2007 .","label":"Background","metadata":{},"score":"58.509094"}{"text":"Learning with Humans in the Loop . 03/30 : T. Joachims , L. Granka , Bing Pan , H. Hembrooke , F. Radlinski , G. Gay .Evaluating the Accuracy of Implicit Feedback from Clicks and Query Reformulations in Web Search , ACM Transactions on Information Systems ( TOIS ) , Vol .","label":"Background","metadata":{},"score":"58.748802"}{"text":"Learn .Res .Mooij , J. : libDAI : A free / open source C++ library for Discrete Approximate Inference ( 2009 ) .Ray , S. , Craven , M. : Supervised versus multiple instance learning : An empirical comparison .","label":"Background","metadata":{},"score":"58.889893"}{"text":"Learn .Res .Mooij , J. : libDAI : A free / open source C++ library for Discrete Approximate Inference ( 2009 ) .Ray , S. , Craven , M. : Supervised versus multiple instance learning : An empirical comparison .","label":"Background","metadata":{},"score":"58.889893"}{"text":"In order to have a ground truth , these TMA images were independently labeled by two pathologists [ 10 ] , retaining only those nuclei on which the two pathologists agree on the label .2.2Image Normalization and Patching The images were adjusted to minimize illumination variations among the scans .","label":"Background","metadata":{},"score":"59.538925"}{"text":"positive examples outweight errors on negative . examples ( default 1 ) ( see [ Morik et al . , 1999 ] ) .-i [ 0,1 ] - remove inconsistent training examples . and retrain ( default 0 ) .Performance estimation options : .","label":"Background","metadata":{},"score":"59.572205"}{"text":"Ranking SVM .For the ranking SVM [ Joachims , 2002c ] , I created a toy example .It consists of only 12 training examples in 3 groups and 4 test examples .You find it at .This will create a subdirectory example3 .","label":"Background","metadata":{},"score":"59.579582"}{"text":"3 The proposed nuclei classification scheme In this section , the proposed hybrid generative - discriminative approach to clas- sify the nuclei is presented .After a brief overview , each step is thoroughly de- scribed .3.1Overview Given the characterization of each nucleus by the features described in the pre- vious section , the general scheme may be summarized as follows : 1 .","label":"Background","metadata":{},"score":"59.587894"}{"text":"Learning user interaction models for predicting web search preferences .SIGIR , 2006 .( paper ) [ Christie , Jacob ] ( 20 min ) .04/08 : D. Beeferman , A. Berger .Agglomerative clustering of search engine query logs .","label":"Background","metadata":{},"score":"59.59915"}{"text":"Introduction to Inductive Transfer , Learning - to - Learn , Multi - Task Learning and all that stuff , Rich Caruana ( Cornell University ) .Location : Upson 5130 .Inductive Transfer ( a.k.a . learning - to - learn , life - long learning , multitask learning , representation learning , ... ) is the process of transfering what has been learned for one problem to other problems so that they can be learned better ( more accurately , or with less data ) .","label":"Background","metadata":{},"score":"59.729504"}{"text":"A more detailed description of the parameters and how they link to the respective algorithms is given in the appendix of [ Joachims , 2002a ] .The input file example_file contains the training examples .The first lines may contain comments and are ignored if they start with # .","label":"Background","metadata":{},"score":"59.924812"}{"text":"All reported accuracies are percentual accuracies and are the averages over 10 folds .In all experiments the standard errors around the mean were inferior to 0.02 .4.1 One model for both classes In this setup , pLSA is trained in an unsupervised way , i.e. , we learn the pLSA model ignoring the class labels .","label":"Background","metadata":{},"score":"59.93505"}{"text":"CIKM , 2008 .( paper ) .04/06 : O. Chapelle and Y. Zhang .A dynamic Bayesian network click model for web search ranking .WWW Conference , 2009 .( paper ) [ Michaela , Vikram ] ( 20 min ) .","label":"Background","metadata":{},"score":"60.039917"}{"text":"Fig.3 .Two examples of nucleus segmentation using the graph cuts algorithm with the potentials described in the text ( the size of the patches in 80 × 80 pixels ) .Page 5 .5 2.4Feature extraction Given the patch image , several features are extracted , inspired by several intu- itive guidelines used by pathologists to visually classify the nuclei [ 21].","label":"Background","metadata":{},"score":"60.162178"}{"text":"Although the accuracies obtained in this case with the linear kernel are better than those obtained with a single pLSA model , the IT kernels yield a smaller improvement of this linear kernel .We believe that this may be due to curse of dimensionality ; when we use pLSA in a supervised way , we concatenate the outputs of each pLSA doubling the number of features .","label":"Background","metadata":{},"score":"60.25713"}{"text":"Structured Output Prediction : In conventional classification and regression , the prediction is a single number .Many application problems , however , require the prediction of complex multi - part objects like trees ( e.g. natural language parsing ) , alignments ( e.g. protein threading ) , rankings ( e.g. search engines ) , and paths ( e.g. navigation assistant ) .","label":"Background","metadata":{},"score":"60.478355"}{"text":"We compare our results with support vector machines based on standard linear kernels and with the nearest neighbor ( NN ) classifier based on the Mahalanobis distance .We conclude that the proposed hybrid approach achieves higher accuracy , revealing itself as a promising approach for this class of problems .","label":"Background","metadata":{},"score":"60.588596"}{"text":"Here , the idea is to consider the points of the generative embedding as multinomial distributions , thus valid arguments for the information theoretic kernels .The proposed approach has been tested on a dataset composed by 474 cell nuclei images , employing different features as well as different IT kernels , in comparison with standard kernels and nearest neighbor classifiers .","label":"Background","metadata":{},"score":"60.86411"}{"text":"04/22 : B. Shaparenko , T. Joachims , Information Genealogy : Uncovering the Flow of Ideas in Non - Hyperlinked Document Databases , KDD ) , 2007 .( paper ) .04/27 : D. Blei , A. Ng , M. Jordan .","label":"Background","metadata":{},"score":"60.953167"}{"text":"[ Bishop 07 ] C. M. Bishop and J. Lasserre , Generative or Discriminative ? getting the best of both worlds .In Bayesian Statistics 8 , Bernardo , J. M. et al .( Eds ) , Oxford University Press .","label":"Background","metadata":{},"score":"60.98271"}{"text":"The rationale behind this choice is that these kernels can exploit the probabilis- tic nature of the generative embeddings , possibly improving the classification results of hybrid approaches .In particular , we investigate a particular class of .Page 3 . 3 IT kernels , based on a non - extensive generalization of the classical Shannon information theory , and defined on normalized ( probability ) or unnormalized measures .","label":"Background","metadata":{},"score":"61.023132"}{"text":"Journal of the American Statistical Association , 70(352 ) , 892 - 898 , 1975 .[ Greiner 02 ] R. Greiner and W. Zhou .Structural extension to logistic regression : Discriminant parameter learning of belief net classifiers .In Proceedings of the Eighteenth Annual National Conference on Artificial Intelligence ( AAAI-02 ) , 167 - 173 , 2002 .","label":"Background","metadata":{},"score":"61.18288"}{"text":"SVM light consists of a learning module ( svm_learn ) and a classification module ( svm_classify ) .The classification module can be used to apply the learned model to new examples .See also the examples below for how to use svm_learn and svm_classify .","label":"Background","metadata":{},"score":"61.21096"}{"text":"Recently , there has been new advances on our theoretical understanding [ Liang 08 , Xue 08 ] and their combination [ Bouchard 07 , Xue 09].On the empirical side , combinations of discriminative and generative methodologies have been explored by several authors [ Raina 04 , Bouchard 04 , McCallum 06 , Bishop 07 , Schmah 09 ] in many fields such as natural language processing , speech recognition , and computer vision .","label":"Background","metadata":{},"score":"61.704323"}{"text":"02/16 : Ben Taskar , Carlos Guestrin and Daphne Koller .Max - Margin Markov Networks .NIPS , 2004 .( paper ) [ Lu ] ( 30 min ) .02/18 : D. Anguelov , B. Taskar , V. Chatalbashev , D. Koller , D. Gupta , G. Heitz , A. Ng .","label":"Background","metadata":{},"score":"62.034267"}{"text":"( default 0 ) .Transduction options ( see [ Joachims , 1999c ] , [ Joachims , 2002a ] ): .-p [ 0 . .1 ] - fraction of unlabeled examples to be classified . into the positive class ( default is the ratio of . positive and negative examples in the training data ) .","label":"Background","metadata":{},"score":"62.09471"}{"text":"In ranking mode [ Joachims , 2002c ] , the target value is used to generated pairwise preference constraints ( see STRIVER ) .A preference constraint is included for all pairs of examples in the example_file , for which the target value differs .","label":"Background","metadata":{},"score":"62.318893"}{"text":"-r float - parameter c in sigmoid / poly kernel .-u string - parameter of user defined kernel .Optimization options ( see [ Joachims , 1999a ] , [ Joachims , 2002a ] ): .-q [ 2 . .","label":"Background","metadata":{},"score":"62.5571"}{"text":"If you are unsure whether you fulfill the prerequisites , contact the instructor .This is a project - focused class that is part lecture and part seminar .A key component of the class is a semester - long research project .","label":"Background","metadata":{},"score":"63.287086"}{"text":"10 4.2One model per class In our second experimental setup , we apply pLSA in a supervised manner , i.e. the training set is split into the two classes and one pLSA model is trained for each class .The final feature space embedding is formed by the concatenation of the embeddings based on each of the two models .","label":"Background","metadata":{},"score":"63.34006"}{"text":"( paper ) [ Michael , Sudip ] ( 45 min ) .03/16 : Matthew Richardson , Pedro Domingos , Markov Logic Networks , Machine Learning , Vol .62 , Number 1 - 2 , pp .107 - 136 , 2006 .","label":"Background","metadata":{},"score":"63.48577"}{"text":"Mechanism design has traditionally been studied by game theorists , economists , and political scientists ; but new applications such as combinatorial auctions , job scheduling , and webpage ranking have drawn many computer scientists to the field .The tutorial is designed for computer scientists with no background in game theory or mechanism design .","label":"Background","metadata":{},"score":"63.56581"}{"text":"( paper ) [ Zhaoyin , Ainur ] ( 45 min ) .04/29 : J. Kleinberg .Bursty and Hierarchical Structure in Streams .KDD , 2002 .( paper ) [ Amir ] ( 20 min ) .CS4780 or CS6780 or an introductory machine learning class .","label":"Background","metadata":{},"score":"63.69426"}{"text":"Postscript ( gz ) ] [ PDF ] [ BibTeX ] .[ Vapnik , 1995a ] .Vladimir N. Vapnik , The Nature of Statistical Learning Theory .Springer , 1995 .","label":"Background","metadata":{},"score":"64.07495"}{"text":"ECCV , 2008 .( paper ) [ Adarsh , Yimeng ] ( 20 min ) .03/09 : Pieter Abbeel and Andrew Y. Ng . , Apprenticeship Learning via Inverse Reinforcement Learning , ICML , 2004 .( paper ) [ Vasu , Dane ] ( 20 min ) .","label":"Background","metadata":{},"score":"64.25222"}{"text":"2The Tissue Microarray ( TMA ) Pipeline In this section , the TMA pipeline is briefly summarized ; for more details , please refer to [ 21].In particular , we first describe how TMA are obtained , followed by the image normalization and patching ( how to segment the nuclei ) .","label":"Background","metadata":{},"score":"64.57138"}{"text":"The remainder of the paper is organized as follows : in Section 2 , we explain the tissue micro array pipeline and how the features are extracted ; Section 3 introduces our methods , while the experimental results are reported in Section 4 .","label":"Background","metadata":{},"score":"64.945404"}{"text":"The output in the predictions file can be used to rank the test examples .If you do so , you will see that it predicts the correct ranking .The values in the predictions file do not have a meaning in an absolute sense .","label":"Background","metadata":{},"score":"65.04819"}{"text":"Average accuracies ( in percentage ) using pLSA and FESS embeddings with NN classifiers .The baseline NN accuracy on the original feature space , with Maha- lanobis distances , is 64.57 % .MahalanobisJSJT JT - W1 JT - W2 PLSA 66.41 68.97 72.53 72.74 68.75 67.11 67.08 72.53 71.27 71.08 FESS .","label":"Background","metadata":{},"score":"65.583145"}{"text":"Page 6 .The parameters of this generative model may be obtained from a dataset using an expectation- maximization ( EM ) algorithm ; for more details , the reader is referred to [ 11].In our approach , we simply assume that the visual features previously de- scribed are the words in the pLSA model , while the nuclei are the documents .","label":"Background","metadata":{},"score":"65.78099"}{"text":"A class label of 0 indicates that this example should be classified using transduction .The predictions for the examples classified by transduction are written to the file specified through the -l option .The order of the predictions is the same as in the training data .","label":"Background","metadata":{},"score":"65.783676"}{"text":"One keypoint in the automatic TMA analysis for renal cell carcinoma is the nucleus classification .In this context , the main goal is to automatically classify cell nuclei into cancerous or benign , which is typically done by trained pathologists by visual inspection .","label":"Background","metadata":{},"score":"65.92125"}{"text":"Page 9 . 9 Figure 3 ) .For each fold , we learn a pLSA model from the training set and apply it to the test set .The number of topics has been chosen using leave - another - fold - out ( of the nine training folds , we used 9-fold cross validation to estimate the best number of topics ) cross validation procedure on the training set .","label":"Background","metadata":{},"score":"66.13963"}{"text":"Vincent Conitzer is a Ph.D. candidate at Carnegie Mellon University , advised by Tuomas Sandholm .He holds an M.S. in Computer Science from CMU ( 2003 ) and a B.A. in Applied Mathematics from Harvard ( 2001 ) .He has published over 30 distinct technical papers on computational issues in game theory , mechanism design , auctions , elections , and other negotiation settings .","label":"Background","metadata":{},"score":"66.16965"}{"text":"Please send me email and let me know that you got svm - light .I will put you on my mailing list to inform you about new versions and bug - fixes .SVM light comes with a quadratic programming tool for solving small intermediate quadratic programming problems .","label":"Background","metadata":{},"score":"66.36113"}{"text":"The somewhat quirky name support vector machine originates in the neural networks literature , where learning algorithms were thought of as architectures , and often referred to as ' ' machines ' ' .The distinctive element of this model is that the decision boundary to use is completely decided ( ' ' supported ' ' ) by a few training data points , the support vectors .","label":"Background","metadata":{},"score":"66.40625"}{"text":"Learn .Res .9 , 1775 - 1822 ( 2008 )MathSciNet .Bartlett , P.L. , Tewari , A. : Sparseness vs estimating conditional probabilities : Some asymptotic results .J. Mach .Learn .Res . 8 , 775 - 790 ( 2007 )","label":"Background","metadata":{},"score":"66.62897"}{"text":"Learn .Res .9 , 1775 - 1822 ( 2008 )MathSciNet .Bartlett , P.L. , Tewari , A. : Sparseness vs estimating conditional probabilities : Some asymptotic results .J. Mach .Learn .Res . 8 , 775 - 790 ( 2007 )","label":"Background","metadata":{},"score":"66.62897"}{"text":"( default classification ) . -c float - C : trade - off between training error . and margin ( default [ avg .-w [ 0 . .]- epsilon width of tube for regression .( default 0.1 ) .","label":"Background","metadata":{},"score":"66.96359"}{"text":"CS6784 is an advanced machine learning course for students that have already taken CS 4780 or CS 6780 or an equivalent machine learning class , giving in - depth coverage of currently active research areas in machine learning .The course will connect to open research questions in machine learning , giving starting points for future work .","label":"Background","metadata":{},"score":"67.0463"}{"text":"Once the kernel is defined , SVM learning can been applied .Recall that positive definiteness is a key condition for the applicability of a kernel in SVM learning .Standard results from kernel theory [ 23 , Proposition 3.22 ] guarantee that the kernel k defined in ( 8) inherits the positive definiteness of ki learning algorithms .","label":"Background","metadata":{},"score":"67.28104"}{"text":"Transductive SVM .To try out the transductive learner , you can use the following dataset ( see also Spectral Graph Transducer ) .I compiled it from the same Reuters articles as used in the example for the inductive SVM .","label":"Background","metadata":{},"score":"67.31912"}{"text":"Humans in the Loop : Much of the data used for machine learning is gathered by observing human behavior ( e.g. search engine logs , purchase data , fraud detection ) .However , it is known that this data is biased ( e.g. users can click only on results that were presented ) .","label":"Background","metadata":{},"score":"67.41573"}{"text":"The classification experiments have been carried using a subset of the data presented in [ 21].We selected a subset of three patients preserving the cancer- ous / benign cell ratio .From the labeled TMA images , we extracted 600 nuclei- patches of size 80×80 pixels .","label":"Background","metadata":{},"score":"67.42178"}{"text":"PDF ] [ Postscript ( gz ) ] [ BibTeX ] .I would also appreciate , if you sent me ( a link to ) your papers so that I can learn about your research .The implementation was developed on Solaris 2.5 with gcc , but compiles also on SunOS 3.1.4 , Solaris 2.7 , Linux , IRIX , Windows NT , and Powermac ( after small modifications , see FAQ ) .","label":"Background","metadata":{},"score":"67.9008"}{"text":"Audit is not allowed , unless you have very good arguments .Grades will be determined as follows : .Letter grade : project ( 50 % ) , paper presentation ( 25 % ) , quizzes and feedback ( 15 % ) , discussion ( 10 % ) .","label":"Background","metadata":{},"score":"68.21132"}{"text":"There is one line per test example in output_file containing the value of the decision function on that example .For classification , the sign of this value determines the predicted class .For regression , it is the predicted value itself , and for ranking the value can be used to order the test examples .","label":"Background","metadata":{},"score":"68.30832"}{"text":"Again , the predictions file shows the ordering implied by the model .The model ranks all training examples correctly .Note that ranks are comparable only between examples with the same qid .Note also that the target value ( first value in each line of the data files ) is only used to define the order of the examples .","label":"Background","metadata":{},"score":"68.74679"}{"text":"Nevertheless , if for some reason you want to use another solver , the new version still comes with an interface to PR_LOQO .The PR_LOQO optimizer was written by A. Smola .Installation .To install SVM light you need to download svm_light.tar.gz .","label":"Background","metadata":{},"score":"68.79484"}{"text":"Page 11 .11 Acknowledgements We acknowledge financial support from the FET programme within the EU FP7 , under the SIMBAD project ( contract 213250 ) .References 1 .Bicego , M. , Lovato , P. , Ferrarini , A. , Delledonne , M. : Biclustering of expression microarray data with topic models .","label":"Background","metadata":{},"score":"68.87575"}{"text":"He recieved his Ph.D. from CMU in 1997 where he worked with Tom Mitchell and Herb Simon .Before joining the faculty at Cornell in 2001 he was affiliated with the Medical School at UCLA , CMU 's Center for Learning and Discovery ( CALD ) , and Just Research .","label":"Background","metadata":{},"score":"69.33664"}{"text":"( default 1 ) .and uses them as starting point .( default ' disabled ' ) .number of iterations .( default 100000 ) .Output options : . -l char - file to write predicted labels of unlabeled examples . into after transductive learning .","label":"Background","metadata":{},"score":"69.43317"}{"text":"To make predictions on test examples , svm_classify reads this file .svm_classify is called with the following parameters : .svm_classify [ options ] example_file model_file output_file .Available options are : . -hHelp .-v [ 0 . .3","label":"Background","metadata":{},"score":"71.17402"}{"text":"The locations of the nuclei were known .Page 4 . 4 Fig.2 .Left : One 1500 × 1500 pixel quadrant of a TMA spot from an RCC patient .Right : A pathologist exhaustively labeled all cell nuclei and classified them into ma- lignant ( black ) and benign ( red ) .","label":"Background","metadata":{},"score":"71.765465"}{"text":"Or how can the learning algorithm gather unbiased data by not being a passive observer , but by actively interacting with the human ?Understanding Archives : We are capturing and archiving more and more data ( e.g. email , blogs , photos ) .","label":"Background","metadata":{},"score":"72.93984"}{"text":"( paper ) [ Guozhang , Rohit ] ( 20 min ) .02/25 : Ulf Brefeld , Tobias Scheffer , Semi - Supervised Learning for Structured Output Variables , ICML , 2006 .( paper ) [ Jean - Baptiste ] ( 20 min ) . 03/02 : Linli Xu , Dana Wilkinson , Finnegan Southey , Dale Schuurmans .","label":"Background","metadata":{},"score":"73.00176"}{"text":"Two examples are considered for a pairwise preference constraint only , if the value of \" qid \" is the same .For example , given the example_file .3 qid:1 1:0.53 2:0.12 2 qid:1 1:0.13 2:0.1 7 qid:2 1:0.87 2:0.12 . a preference constraint is included only for the first and the second example ( ie .","label":"Background","metadata":{},"score":"73.57965"}{"text":"- maximum size of QP - subproblems ( default 10 ) .-n [ 2 . q ] - number of new variables entering the working set .zig - zagging .-m [ 5 . .] - size of cache for kernel evaluations in MB ( default 40 ) .","label":"Background","metadata":{},"score":"73.71118"}{"text":"In settings such as auctions and elections , an outcome must be chosen based on the preferences of multiple parties ( agents ) .Typically , each agent ( initially ) only knows its own preferences , and will not disclose its true preferences unless it feels that doing so is in its own interest .","label":"Background","metadata":{},"score":"74.199356"}{"text":"This software is free only for non - commercial use .It must not be distributed without prior permission of the author .The author is not responsible for implications from the use of this software .History .V6.01 - V6.02 .","label":"Background","metadata":{},"score":"76.733536"}{"text":"The target value and each of the feature / value pairs are separated by a space character .Feature / value pairs MUST be ordered by increasing feature number .Features with value zero can be skipped .Check the FAQ for more details on how to implement your own kernel .","label":"Background","metadata":{},"score":"76.76924"}{"text":"2.1Tissue Micro Arrays A TMA is a microscope slide containing a set of small round tissue spots of ( possibly cancerous ) tissue , adequate for microscopic histological analysis .The diameter of the spots is of the order of 1 mm and the thickness corresponds to one cell layer .","label":"Background","metadata":{},"score":"78.729965"}{"text":"Immunohistochemical staining for the proliferation protein MIB-1 ( Ki-67 antigen ) makes the nuclei of cells in division status appear brown .For subsequent computer analysis , the TMA slides are scanned into three- channel color images at a resolution of 0.23 µm / pixel .","label":"Background","metadata":{},"score":"79.23001"}{"text":"( paper ) [ Cangmin , Ronan ] ( 20 min ) .04/08 : John Langford , Alexander Strehl , and Jennifer Wortman .Exploration Scavenging , ICML , 2008 .( paper ) [ Nikos , Devin ] ( 20 min ) . 04/13 : Yisong Yue , J. Broder , R. Kleinberg , T. Joachims .","label":"Background","metadata":{},"score":"79.43068"}{"text":"This course follows the Cornell University Code of Academic Integrity .Each student in this course is expected to abide by the Cornell University Code of Academic Integrity .Any work submitted by a student in this course for academic credit will be the student 's own work .","label":"Background","metadata":{},"score":"80.51395"}{"text":"-e float - eps : Allow that error for termination criterion .-h[ 5 . .]- number of iterations a variable needs to be . optimal before considered for shrinking ( default 100 ) .-f [ 0,1 ] - do final optimality check for variables removed by . shrinking .","label":"Background","metadata":{},"score":"83.85571"}{"text":"-t int - type of kernel function : . 0 : linear ( default ) .4 : user defined kernel from kernel.h .-d int - parameter d in polynomial kernel .-g float - parameter gamma in rbf kernel .","label":"Background","metadata":{},"score":"84.98714"}{"text":"[ COLT , 2009 ] , preprint of journal version .( paper ) .Understanding Archives . 04/20 : S. Pohl , F. Radlinski , T. Joachims .Recommending Related Papers Based on Digital Library Access Records .JCDL , 2007 .","label":"Background","metadata":{},"score":"85.51985"}{"text":"The transductive learner is invoked automatically , since train_transduction . dat contains unlabeled examples ( i. e. the 600 test examples ) .You can compare the results to those of the inductive SVM by running : .svm_learn example2/train_induction.dat example2/model svm_classify example2/test . dat example2/model example2/predictions .","label":"Background","metadata":{},"score":"85.6286"}{"text":"-f [ 0,1 ] 0 : old output format of V1.0 1 : output the value of decision function ( default ) .The test examples in example_file are given in the same format as the training examples ( possibly with 0 as class label ) .","label":"Background","metadata":{},"score":"88.44457"}{"text":"You find it at .This will create a subdirectory example2 .To run the example , execute the commands : .svm_learn example2/train_transduction.dat example2/model svm_classify example2/test . dat example2/model example2/predictions .","label":"Background","metadata":{},"score":"88.50714"}{"text":"This will create a subdirectory example1 .Documents are represented as feature vectors .Each feature corresponds to a word stem ( 9947 features ) .The task is to learn which Reuters articles are about \" corporate acquisitions \" .There are 1000 positive and 1000 negative examples in the file train.dat .","label":"Background","metadata":{},"score":"91.53507"}{"text":"( paper ) .04/01 : Ben Carterette , Rosie Jones .Evaluating Search Engines by Modeling the Relationship Between Relevance and Clicks .NIPS , 2007 .( paper ) [ CongCong ]( 20 min ) .04/01 : F. Radlinski , M. Kurup , T. Joachims .","label":"Background","metadata":{},"score":"92.12004"}{"text":"The feature numbers correspond to the line numbers in the file words .To run the example , execute the commands : .svm_learn example1/train . dat example1/model svm_classify example1/test . dat example1/model example1/predictions .","label":"Background","metadata":{},"score":"92.3072"}{"text":"Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .Publisher conditions are provided by RoMEO .","label":"Background","metadata":{},"score":"97.18031"}{"text":"2ETH Z¨ urich , Department of Computer Science , Z¨ urich , Switzerland .3Istituto Italiano di Tecnologia , Genova , Italy .4Instituto de Telecomunica¸ c˜ oes , Lisboa , Portugal .5Instituto de Sistemas e Rob ´ otica , Lisboa , Portugal 6Instituto Superior T ´ ecnico , Technical University of Lisbon , Portugal .","label":"Background","metadata":{},"score":"104.99751"}{"text":"+1 as the target value marks a positive example , -1 a negative example respectively .So , for example , the line . -11:0.43 3:0.12 9284:0.2 # abcdef . specifies a negative example for which feature number 1 has the value 0.43 , feature number 3 has the value 0.12 , feature number 9284 has the value 0.2 , and all the other features have value 0 .","label":"Background","metadata":{},"score":"105.75654"}