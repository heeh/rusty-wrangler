{"text":"The Penn Treebank has recently implemented a new syntactic annotation scheme , designed to highlight aspects of predicate - argument structure .This paper discusses the implementation of crucial aspects of this new annotation scheme .It incorporates a more consistent treatment of a wide range of gramma ... \" .","label":"CompareOrContrast","metadata":{},"score":"41.99698"}{"text":"This paper describes and evaluates log - linear parsing models for Combinatory Categorial Grammar ( CCG ) .A parallel implementation of the L - BFGS optimisation algorithm is described , which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation .","label":"CompareOrContrast","metadata":{},"score":"48.175583"}{"text":"This paper describes and evaluates log - linear parsing models for Combinatory Categorial Grammar ( CCG ) .A parallel implementation of the L - BFGS optimisation algorithm is described , which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation .","label":"CompareOrContrast","metadata":{},"score":"48.175583"}{"text":"This paper proposes the use of maximum entropy techniques for text classification .Maximum entropy is a probability distribution estimation technique widely used for a variety of natural language tasks , such as language modeling , part - of - speech tagging , and text segmentation .","label":"CompareOrContrast","metadata":{},"score":"49.633053"}{"text":"Proceedings of the Third Workshop on Very Large Corpora , Somerset , New Jersey , Association for Computational Linguistics , pp . 1 - 13 ( 1995 ) .Marcus , M.P. , Santorini , B. , Marcinkiewicz , M.A. : Building a large annotated corpus of english : The penn treebank .","label":"CompareOrContrast","metadata":{},"score":"53.43254"}{"text":"We present positive experimental results on the segmentation of FAQ 's . \" ...This paper proposes the use of maximum entropy techniques for text classification .Maximum entropy is a probability distribution estimation technique widely used for a variety of natural language tasks , such as language modeling , part - of - speech tagging , and text segmentation .","label":"CompareOrContrast","metadata":{},"score":"54.561066"}{"text":"In Proceedings of the Third ACL Workshop on Very Large Corpora , pages 82 - 94 .Cambridge MA , USA .Ratnaparkhi , A. ( 1996 ) .A maximum entropy model for part - of - speech tagging .In EMNLP 1996 : Proceedings of the 1st","label":"CompareOrContrast","metadata":{},"score":"54.745544"}{"text":"The Maximum Entropy principle ( ME ) is an ap- propriate framework for combining information of a diverse nature from several sources into the same language model .In order to incorporate long - distance information into the ME framework in a language model , a Whole Sentence Maximum Entropy Language Model ( WSME ) could be used .","label":"CompareOrContrast","metadata":{},"score":"55.703644"}{"text":"The Maximum Entropy principle ( ME ) is an ap- propriate framework for combining information of a diverse nature from several sources into the same language model .In order to incorporate long - distance information into the ME framework in a language model , a Whole Sentence Maximum Entropy Language Model ( WSME ) could be used .","label":"CompareOrContrast","metadata":{},"score":"55.703644"}{"text":"A simple pattern - matching algorithm for recovering empty nodes and their antecedents .In ACL 2002 : Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 136 - 143 .Johnson , M. , Geman , S. , Canon , S. , Chi , Z. , and Riezler , S. ( 1999 ) .","label":"CompareOrContrast","metadata":{},"score":"56.15463"}{"text":"They are important for a few reasons .First , at present the best performing parsers on the WSJ treebank ( Ratnaparkhi 1997 ; Charniak 1997 , 1999 ; Collins 1997 , 1999 ) are all cases of history - based mo .. \" ...","label":"CompareOrContrast","metadata":{},"score":"57.04519"}{"text":"Unlike the linear interpo ... \" . application .Furthermore , other structure knowledge such as context free grammar and link grammar [ 10 , 11 ] is introduced to language models for improving their performances .To combine statistical information from multiple sources , maximum entropy ( ME ) LM is presented [ 12].","label":"CompareOrContrast","metadata":{},"score":"57.86198"}{"text":"Unlike the linear interpo ... \" . application .Furthermore , other structure knowledge such as context free grammar and link grammar [ 10 , 11 ] is introduced to language models for improving their performances .To combine statistical information from multiple sources , maximum entropy ( ME ) LM is presented [ 12].","label":"CompareOrContrast","metadata":{},"score":"57.86198"}{"text":"Computational Linguistics 21:543 - 565 .Tools . by Adam L. Berger , Stephen A. Della Pietra , Vincent J. Della Pietra - COMPUTATIONAL LINGUISTICS , 1996 . \" ...The concept of maximum entropy can be traced back along multiple threads to Biblical times .","label":"CompareOrContrast","metadata":{},"score":"58.392254"}{"text":"This paper describes a number of log - linear parsing models for an automatically extracted lexicalized grammar .The models are \" full \" parsing models in the sense that probabilities are defined for complete parses , rather than for independent events derived by decomposing the parse tree .","label":"CompareOrContrast","metadata":{},"score":"59.26406"}{"text":"This paper describes a number of log - linear parsing models for an automatically extracted lexicalized grammar .The models are \" full \" parsing models in the sense that probabilities are defined for complete parses , rather than for independent events derived by decomposing the parse tree .","label":"CompareOrContrast","metadata":{},"score":"59.26406"}{"text":"Mächler , M. , Bühlmann , P. : Variable length markov chains : Methodology , computing and software .Research Report 104 , Eidgenossische Technische Hochschule ( ETH ) , CH-8091 Zürich , Switzerland ( 2002 ) Seminar fur Statistik .Rissanen , J. : A universal data compression system .","label":"CompareOrContrast","metadata":{},"score":"59.30137"}{"text":"Sha , F. and Pereira , F. ( 2003 ) .Shallow parsing with Conditional Random Fields .In NAACL 2003 : Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology , pages 134 - 141 .","label":"CompareOrContrast","metadata":{},"score":"59.480133"}{"text":"Maximum Entropy Estimation for Feature Forests .In HLT 2002 : Proceedings of Human Language Technology Conference , pages 292 - 297 .Miyao , Y. and Tsujii , J. ( 2004 ) .Deep linguistic analysis for the accurate identifica- tion of predicate - argument relations .","label":"CompareOrContrast","metadata":{},"score":"59.48284"}{"text":"MIT Press .Tsochantaridis , I. , Joachims , T. , Hofmann , T. , and Altun , Y. ( 2005 ) .Large margin methods for structured and interdependent output variables .Journal of Machine Learning Research , 6:1453 - 1484 .","label":"CompareOrContrast","metadata":{},"score":"59.82164"}{"text":"The estimation processes of the models are described in detail .Finally , experiments on the Wall Street Journal corpus are reported . \" ...The Maximum Entropy principle ( ME ) is an ap- propriate framework for combining information of a diverse nature from several sources into the same language model .","label":"CompareOrContrast","metadata":{},"score":"60.050446"}{"text":"The estimation processes of the models are described in detail .Finally , experiments on the Wall Street Journal corpus are reported . \" ...The Maximum Entropy principle ( ME ) is an ap- propriate framework for combining information of a diverse nature from several sources into the same language model .","label":"CompareOrContrast","metadata":{},"score":"60.050446"}{"text":"To learn the tree structures we use greedy hill - climbing with Bayesian scoring to evaluate next candidates ( Chickering et al . , 1997 ) .The remaining words are either unambiguous or there is not enough data to learn contextualized cpds .","label":"CompareOrContrast","metadata":{},"score":"60.30506"}{"text":"We introduce a new method for the reranking task , based on the boosting approach to ranking problems described in Freund et al .( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .","label":"CompareOrContrast","metadata":{},"score":"60.588463"}{"text":"( bracketing guidelines for Treebank II Style Penn Treebank Project , section 8.1 , p.l35 ) .Modifier sharing , however , is sometimes hard for people to judge and is not always consistently annotated in the Treebank .This limits the maximum performance of any Treebank - based NP tagger .","label":"CompareOrContrast","metadata":{},"score":"60.603172"}{"text":"Levy , R. and Manning , C. ( 2004 ) .Deep dependencies from context - free statistical parsers : correcting the surface dependency approximation .In ACL 2004 : Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics , pages 328 - 335 .","label":"CompareOrContrast","metadata":{},"score":"60.904682"}{"text":"The experiments in this project were conducted on one of the most commonly used such corpus , the Wall Street Journal articles from the Penn Treebank project ( Marcus et al . , 1994 ) , which contains over a million tagged words .","label":"CompareOrContrast","metadata":{},"score":"61.27402"}{"text":"The bracketing guidelines for the Penn Chinese treebank .Technical report , University of Pennsylvania .Yuret , D. and T¨ ure , F. ( 2006 ) .Learning morphological disambiguation rules for Turk- ish .In HLT - NAACL 2006 : Proceedings of the Human Language Technology Confer- ence of the North American Chapter of the Association of Computational Linguistics , pages 328 - 334 .","label":"CompareOrContrast","metadata":{},"score":"61.407604"}{"text":"Ngai and Yarowsky addressed this question : Abstract .There is a wide variety of statistical methods applied to Part - of - Speech ( PoS ) tagging , that associate words in a text to their corresponding PoS. The majority of those methods analyse a fixed , small neighborhood of words imposing some form of Markov restriction .","label":"CompareOrContrast","metadata":{},"score":"61.431107"}{"text":"Microsoft Technical Report MSR - TR-00 - 16 .[ Marcus et al . , 1994 ] Mitchel P. Marcus , Beatrice Santorini , and Mary Ann Marcinkiewicz .Building a large annotaded corpus of English : the Penn Treebank .Computational Linguistics 19(2):313 - 330 .","label":"CompareOrContrast","metadata":{},"score":"62.43018"}{"text":"In this paper , we propose the application of another sampling technique : the Perfect Sampling ( PS ) .The experiment has shown a reduction of 30 % in the perplexity of the WSME model over the trigram model and a reduc- tion of 2 % over the WSME model trained with MCMC . by Diego Linares , José - miguel Benedí , Joan - andreu Sánchez , Javeriana Cali . \" ... Abstract .","label":"CompareOrContrast","metadata":{},"score":"62.434616"}{"text":"In this paper , we propose the application of another sampling technique : the Perfect Sampling ( PS ) .The experiment has shown a reduction of 30 % in the perplexity of the WSME model over the trigram model and a reduc- tion of 2 % over the WSME model trained with MCMC . by Diego Linares , José - miguel Benedí , Joan - andreu Sánchez , Javeriana Cali . \" ... Abstract .","label":"CompareOrContrast","metadata":{},"score":"62.434616"}{"text":"This paper presents a new method for producing a dictionary of subcategorization frames from un- labelled text corpora .It is shown that statistical filtering of the results of a finite state parser running on the output of a stochastic tagger produces high quality results , despite the error rates of the tagger and the parser .","label":"CompareOrContrast","metadata":{},"score":"62.613724"}{"text":"Constraints on the distribution , derived from labeled training data , inform the technique where to be minimally non - uniform .The maximum entropy formulation has a unique solution which can be found by the improved iterative scaling algorithm .In this paper , maximum entropy is used for text classification by estimating the conditional distribution of the class variable given the document .","label":"CompareOrContrast","metadata":{},"score":"62.723072"}{"text":"Toutanova , K. , Klein , D. , Manning , C.D. , Singer , Y. : Feature - rich part - of - speech tagging with a cyclic dependency network .In : Proceedings of HLT - NAACL 2003 , pp .","label":"CompareOrContrast","metadata":{},"score":"62.735085"}{"text":"In CoNNL 2005 : Proceedings of the 9th Conference on Computational Natural Language Learning , pages 120 - 127 .Taskar , B. , Guestrin , C. , and Koller , D. ( 2004 ) .Max - margin Markov networks .","label":"CompareOrContrast","metadata":{},"score":"63.211746"}{"text":"Palmer , M. , Gildea , D. , and Kingsbury , P. ( 2005 ) .The proposition bank : An annotated corpus of semantic roles .Computational Linguistics , 31(1):71 - 106 .Ramshaw , L. and Marcus , M. ( 1995 ) .","label":"CompareOrContrast","metadata":{},"score":"63.318626"}{"text":"These techniques are ... . ... shown in Figure 7 .Hidden Markov models , while relatively new to information extraction , have enjoyed success in related natural language tasks . \" ...This paper presents a new method for producing a dictionary of subcategorization frames from un- labelled text corpora .","label":"CompareOrContrast","metadata":{},"score":"63.35026"}{"text":"We present a maximum - likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently , using as examples several problems in natural language processing . \" ... this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .","label":"CompareOrContrast","metadata":{},"score":"63.877163"}{"text":"Vapnik , V. N. ( 1998 ) .Statistical Learning Theory .Wiley - Interscience , New York , NY , USA .Vossen , P. , editor ( 1998 ) .EuroWordNet : A Multilingual Database with Lexical Semantic Networks .","label":"CompareOrContrast","metadata":{},"score":"63.941986"}{"text":"This paper presents a new Markovian sequence model , closely related to HMMs , that allows observations to be represented as arbitrary overlapping features ( such as word , capitalization , formatting , part - of - speech ) , and defines the conditional probability of state sequences given observation sequences .","label":"CompareOrContrast","metadata":{},"score":"63.966576"}{"text":"This paper presents a new Markovian sequence model , closely related to HMMs , that allows observations to be represented as arbitrary overlapping features ( such as word , capitalization , formatting , part - of - speech ) , and defines the conditional probability of state sequences given observation sequences .","label":"CompareOrContrast","metadata":{},"score":"63.966576"}{"text":"In Abeill ´ e , A. , editor , Treebanks : Building and Using Parsed Corpora , pages 249 - 260 .Kluwer Academic Publishers , Dordrecht .Lafferty , J. D. , McCallum , A. , and Pereira , F. C. N. ( 2001 ) .","label":"CompareOrContrast","metadata":{},"score":"64.07045"}{"text":"The intersection of these constraints is the set of probability functions which are consistent with all the information sources .The function with the highest entropy within that set is the ME solution .ME takes all the previous words as possible features , so training ME model is computational challenging and sometimes almost infeasible .","label":"CompareOrContrast","metadata":{},"score":"64.3107"}{"text":"The intersection of these constraints is the set of probability functions which are consistent with all the information sources .The function with the highest entropy within that set is the ME solution .ME takes all the previous words as possible features , so training ME model is computational challenging and sometimes almost infeasible .","label":"CompareOrContrast","metadata":{},"score":"64.310715"}{"text":"Learning multilingual morphology with CLOG .In Proceedings of the 8th International Conference on Inductive Logic Programming , pages 135 - 144 .Marcus , M. P. , Santorini , B. , and Marcinkiewicz , M. A. ( 1994 ) .Building a Large Anno- tated Corpus of English : The Penn Treebank .","label":"CompareOrContrast","metadata":{},"score":"64.3656"}{"text":"Summary thoughts on POS tagging .the Penn tag set is now the standard for assessing English tagging , but it forces annotators to make some hard decisions , and has an interannotator error of around 3 % .there are many supervised learning methods which can get 96 - 97 % accuracy on held - out Wall Street Journal data ; we looked at HMMs , TBL , and MaxEnt ; the error in the Penn Treebank probably masks differences between the methods .","label":"CompareOrContrast","metadata":{},"score":"64.81561"}{"text":"Hidden Markov models decompose the distribution P(W , T ) over words W 1 , ... , W n and tags T 1 , ... , T n as .In contrast , transformation - based method starts with an initial assignment of tags to words using the most common tag regardless of context .","label":"CompareOrContrast","metadata":{},"score":"65.0717"}{"text":"An O(ND ) difference algorithm and its variations .Algorithmica , 1(1):251 - 266 .Noreen , E. W. ( 1989 ) .Computer intensive methods for testing hypotheses .A Wiley- Interscience Publication , New York .O'Donovan , R. , Burke , M. , Cahill , A. , van Genabith , J. , and Way , A. ( 2004 ) .","label":"CompareOrContrast","metadata":{},"score":"65.082886"}{"text":"Experiments on the UPenn Treebank corpus are reported .These experiments have been carried out in terms of the test set perplexity and the word error rate in a speech recognition experiment . \" ... application .Furthermore , other structure knowledge such as context free grammar and link grammar [ 10 , 11 ] is introduced to language models for improving their performances .","label":"CompareOrContrast","metadata":{},"score":"65.516464"}{"text":"Experiments on the UPenn Treebank corpus are reported .These experiments have been carried out in terms of the test set perplexity and the word error rate in a speech recognition experiment . \" ... application .Furthermore , other structure knowledge such as context free grammar and link grammar [ 10 , 11 ] is introduced to language models for improving their performances .","label":"CompareOrContrast","metadata":{},"score":"65.516464"}{"text":"Miyao , Y. and Tsujii , J. ( 2005 ) .Probabilistic disambiguation models for wide - coverage HPSG parsing .In ACL 2005 : Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics , pages 83 - 90 .","label":"CompareOrContrast","metadata":{},"score":"65.5756"}{"text":"These state - of - the - art methods achieve roughly similar accuracy on the Wall Street Journal corpus of about 96.36 % to 96.82 % ( Brill et al ., 1998 ) .All of them use words and tags surrounding a word in a small window ( 1 - 3 on either side ) to assign a tags to all words in a sentence .","label":"CompareOrContrast","metadata":{},"score":"65.772934"}{"text":"Much future work remains , but the re ... . \" ...Conditional Random Fields ( CRFs ) are undirected graphical models , a special case of which correspond to conditionally - trained finite state machines .A key advantage of CRFs is their great flexibility to include a wide variety of arbitrary , non - independent features of the input .","label":"CompareOrContrast","metadata":{},"score":"66.36465"}{"text":"The lexicalized grammar formalism used is Combinatory Categorial Grammar ( CCG ) , and the grammar is automatically extracted from CCGbank , a CCG version of the Penn Treebank .The combination of discriminative training and an automatically extracted grammar leads to a significant memory requirement ( over 20 GB ) , which is satisfied using a parallel implementation of the BFGS optimisation algorithm running on a Beowulf cluster .","label":"CompareOrContrast","metadata":{},"score":"66.790634"}{"text":"We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .We investigate for the first time how factors such as training data size , corpus ( e.g. , Br ... \" .","label":"CompareOrContrast","metadata":{},"score":"67.345276"}{"text":"This paper discusses the implementation of crucial aspects of this new annotation scheme .INTRODUCTION During the first phase of the The Penn Treebank project [ 10 ] , ending in December 1992 , 4.5 million words of text were tagged for part - of - speech , with about two - thirds of this material also annotated with a skeletal syntactic bracketing .","label":"CompareOrContrast","metadata":{},"score":"67.55064"}{"text":"Conditional Random Fields ( CRFs ) are undirected graphical models , a special case of which correspond to conditionally - trained finite state machines .A key advantage of CRFs is their great flexibility to include a wide variety of arbitrary , non - independent features of the input .","label":"CompareOrContrast","metadata":{},"score":"67.619125"}{"text":"Thus , given training sentences S1 , . . ., Sm , gold - standard dependency structures , π1 , . . ., πm , and the definition of the probability of a dependency structure ( 13 ) , the ob ... . \" ...","label":"CompareOrContrast","metadata":{},"score":"68.425415"}{"text":"( 1993 ) proposed a probabilistic model for combining these features : .This model makes the approximation that those features are independent given the tag to keep the number of parameters small , but ignores certain correlations , for example , between capitalized and unknown .","label":"CompareOrContrast","metadata":{},"score":"68.50815"}{"text":"A maximum Entropy Model for Part - Of - Speech Tagging .In EMNLP 1 , pp .133 - 142 .[ Weischedel et al , 1993 ] Ralph Weischedel , Marie Meteer , Richar Schwartz , Lance Ramshaw and Jeff Palmucci .","label":"CompareOrContrast","metadata":{},"score":"68.67539"}{"text":"Memory - based morphological analy- sis .In ACL 1999 : Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics , pages 285 - 292 .van der Beek , L. , Bouma , G. , Malouf , R. , and van Noord , G. ( 2002 ) .","label":"CompareOrContrast","metadata":{},"score":"68.73347"}{"text":"Adwait Ratnaparkhi .Maximum Entropy Models for Natural Language Ambiguity Resolution .Ph.D. thesis , University of Pennsylvania .Data - driven grammar induction aims at producing wide - coverage grammars of human languages .Initial efforts in this field produced relatively shallow linguistic representations such as phrase - structure trees , which only encode constituent structure .","label":"CompareOrContrast","metadata":{},"score":"68.828"}{"text":"Our models are applied to the task of extracting important fields from the headers of computer science research papers , and achieve an extraction accuracy of 92.9 % . ... fication accuracy .Hidden Markov models , while relatively new to information extraction , have enjoyed success in related natural language tasks .","label":"CompareOrContrast","metadata":{},"score":"68.85374"}{"text":"In ICML 2001 : Proceedings of the Eighteenth International Conference on Machine Learning , pages 282 - 289 .Lavraˇ c , N. and Dˇ zeroski , S. ( 1994 ) .Inductive logic programming . E. Horwood New York .Le , Z. ( 2004 ) .","label":"CompareOrContrast","metadata":{},"score":"68.88816"}{"text":"Here 's an English example of a tagged sentence taken from the Wall Street Journal of the Penn Treebank : . than .IN . the .DT . overall .JJ . measures .NNS . . . . .ACOPOST is a set of freely available POS taggers modeled after well - known techniques .","label":"CompareOrContrast","metadata":{},"score":"69.17886"}{"text":"In addition , we introduce two novel smoothing techniques , one a variation of Jelinek - Mercer smoothing and one a very simple linear interpolation technique , both of which outperform existing methods . \" ...Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position .","label":"CompareOrContrast","metadata":{},"score":"69.45808"}{"text":"Lawrence R. Rabiner .A tutorial on hidden markov models and selected applications in speech recognition .In Alex Waibel & Kai - Fu Lee , ed . , Readings in Speech Recognition .Morgan Kaufmann , San Mateo , CA , USA , pages 267 - 290 .","label":"CompareOrContrast","metadata":{},"score":"69.53787"}{"text":"In ACL 156 .Page 171 .2004 : Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics , pages 367 - 374 .O'Donovan , R. , Cahill , A. , van Genabith , J. , and Way , A. ( 2005 ) .","label":"CompareOrContrast","metadata":{},"score":"69.57168"}{"text":"In this paper we des ... \" .The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .","label":"CompareOrContrast","metadata":{},"score":"69.5934"}{"text":"Maxwell , J. T. and Kaplan , R. M. ( 1996 ) .Unification - based parsers that automati- cally take advantage of context freeness .In LFG 1996 : Proceedings of the Lexical Functional Grammar Conference .McCallum , A. , Freitag , D. , and Pereira , F. ( 2000 ) .","label":"CompareOrContrast","metadata":{},"score":"70.049576"}{"text":"This project investigates a probabilistic method of exploiting this high accuracy of tagging most words to bootstrap tagging of difficult ones .The success of the above state - of - the - art models has shown that the tags of surrounding words provide a lot of information about the tag of a word .","label":"CompareOrContrast","metadata":{},"score":"70.16879"}{"text":"In these results , the generative model performs significantly better than the others , and does about equally well at assigning part - of - speech tags . \" ...Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .","label":"CompareOrContrast","metadata":{},"score":"70.18003"}{"text":"A key component of the parsing system , for both training and testing , is a Maximum Entropy supertagger which assigns CCG lexical categories to words in a sentence .The supertagger makes the discriminative training feasible , and also leads to a highly efficient parser .","label":"CompareOrContrast","metadata":{},"score":"70.27026"}{"text":"not aware of any separate study of human performance ( the best systems do almost 96 % against test data , so human consistency probably at least that high ) .Looking ahead .The best performance on the baseNP and chunking tasks was obtained using a Support Vector Machine method .","label":"CompareOrContrast","metadata":{},"score":"70.41979"}{"text":"Improved training methods based on modern optimization algorithms were critical in achieving these results .We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum - entropy models . \" ...","label":"CompareOrContrast","metadata":{},"score":"70.473755"}{"text":"Improved training methods based on modern optimization algorithms were critical in achieving these results .We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum - entropy models . \" ...","label":"CompareOrContrast","metadata":{},"score":"70.473755"}{"text":"In ICML 2000 : Proceedings of the International Conference on Machine Learning , pages 591 - 598 .Merlo , P. and Musillo , G. ( 2005 ) .Accurate function parsing .In HLT - EMNLP 2005 : Proceedings of the Conference on Human Language Technology and Empirical Meth- ods in Natural Language Processing , pages 620 - 627 .","label":"CompareOrContrast","metadata":{},"score":"70.564545"}{"text":"Experiments .The experiments were conducted by randomly splitting the Wall St. Journal corpus into a training and testing in roughly 90/10 proportion .There are several model parameters that need to be set .This maybe due to overfitting in the cpds for the larger contexts , but I did not investigate an alternative estimate smoothing or tree induction method .","label":"CompareOrContrast","metadata":{},"score":"70.87907"}{"text":"This paper advocates the use of machine learning techniques to greatly automate the creation and maintenance of domain - specific Internet portals .We describe new research in reinforcement learning , information extraction and text classification that enables efficient spidering , the identification of informative text segments , and the population of topic hierarchies .","label":"CompareOrContrast","metadata":{},"score":"71.03264"}{"text":"Inform .Theory IT-29 , 656 - 664 ( 1983 ) .IEL - UNICAMP and IME - USP : Corpus Anotado do Português Histórico Tycho Brahe , Acessado em 2005 ( 2005 ) .Church , K.W. : A stochastic parts program and noun phrase parser for unrestricted text .","label":"CompareOrContrast","metadata":{},"score":"71.244064"}{"text":"However , in many domains labels are highly interdependent .This paper explores multilabel conditional random field ( CRF ) classification models that directly parameterize label co - occurrences in multi - label classification .Experiments show that the models outperform their singlelabel counterparts on standard text corpora .","label":"CompareOrContrast","metadata":{},"score":"71.256775"}{"text":"A hybrid language model is defined as a combination of a word - based n - gram , which is used to capture the local relation ... \" .Abstract .This paper explores the use of initial Stochastic Context - Free Grammars ( SCFG ) obtained from a treebank corpus for the learning of SCFG by means of estimation algorithms .","label":"CompareOrContrast","metadata":{},"score":"71.616234"}{"text":"A hybrid language model is defined as a combination of a word - based n - gram , which is used to capture the local relation ... \" .Abstract .This paper explores the use of initial Stochastic Context - Free Grammars ( SCFG ) obtained from a treebank corpus for the learning of SCFG by means of estimation algorithms .","label":"CompareOrContrast","metadata":{},"score":"71.616234"}{"text":"The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .","label":"CompareOrContrast","metadata":{},"score":"71.62404"}{"text":"In CLIN 2001 : Computational Linguistics in the Netherlands , pages 8 - 22 .Vapnik , V. ( 2006 ) .Estimation of Dependences Based on Empirical Data .Springer .Vapnik , V. N. ( 1995 ) .The Nature of Statistical Learning Theory .","label":"CompareOrContrast","metadata":{},"score":"71.95551"}{"text":"Computer Speech & Language , 10(3):187 - 228 .Roth , D. ( 2001 ) .Reasoning with classifiers .In ECML 2001 : Proceedings of the Euro- pean Conference on Machine Learning , pages 506 - 510 .Page 172 .","label":"CompareOrContrast","metadata":{},"score":"72.19134"}{"text":"We explore the use of hidden Markov models for information extraction tasks , specifically focusing on how to learn model structure from data and how to make the best use of labeled and unlabeled data .We show that a manually - constructed model that contains multiple states per extraction field outperforms a model with one state per field , and discuss strategies for learning the model structure automatically from data .","label":"CompareOrContrast","metadata":{},"score":"72.28642"}{"text":"Ratnaparkhi , 1996 finds that distribution of tags for the word \" about \" ( as well as several others ) is fairly different for different annotators of the dataset , suggesting that there is a real limit to the level of achievable accuracy .","label":"CompareOrContrast","metadata":{},"score":"72.53283"}{"text":"Muggleton , S. ( 1991 ) .Inductive logic programming .New Generation Computing , 8(4):295 - 318 .Musillo , G. and Merlo , P. ( 2005 ) .Lexical and structural biases for function parsing .In Proceedings of the Ninth International Workshop on Parsing Technology , pages 83 - 92 .","label":"CompareOrContrast","metadata":{},"score":"72.921585"}{"text":"Top words according to such a criterion are the ones that are commonly reported as difficult : that , about , up , 's , etc . .Learning .In addition , there are usually many context - specific independencies in the conditional probability distribution ( cpd ) , e.g. given that the next tag is comma , it does not matter what the tag after the next tag is .","label":"CompareOrContrast","metadata":{},"score":"73.05861"}{"text":"Ratnaparkhi , A. : A maximum entropy model for part - of - speech tagging .In : Proceedings of the Empirical Methods in Natural Language Processing Conference , University of Pennsylvania ( 1996 ) .Brants , T. : Tnt - a statistical part - of - speech tagger .","label":"CompareOrContrast","metadata":{},"score":"73.08734"}{"text":"Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position .Among sequence labeling tasks in language processing , shallow parsing has received much attention , with the development of standard evaluation datasets and extensive comparison among methods .","label":"CompareOrContrast","metadata":{},"score":"73.10742"}{"text":"Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position .Among sequence labeling tasks in language processing , shallow parsing has received much attention , with the development of standard evaluation datasets and extensive comparison among methods .","label":"CompareOrContrast","metadata":{},"score":"73.10742"}{"text":"Speed and accuracy in shallow and deep stochastic parsing .In HLT - NAACL 2004 : Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics .Kurohashi , S. and Nagao , M. ( 2003 ) .","label":"CompareOrContrast","metadata":{},"score":"73.11528"}{"text":"Ramshaw and Marcus adapted the TBL method which had been introduced by Brill for POS tagging .They pointed out that one - level bracketing can be restated as a word tagging task .For NP chunking , they used 3 tags : I ( inside a baseNP ) , O ( outside a baseNP , ) , and B ( the start of a baseNP which immediately follows another baseNP ) .","label":"CompareOrContrast","metadata":{},"score":"73.34039"}{"text":"We also give an overview of the parsing approaches that participants took and the results that they achieved .Finally , we try to draw general conclusions about multi - lingual parsing : What makes a particular language , treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser ?","label":"CompareOrContrast","metadata":{},"score":"73.49734"}{"text":"By iteratively reassigning tags based on the current assignment of other tags , and keeping track of the most common assignments , we can infer the most likely tags for each word .Probability Model .HMM methods learn a joint distribution over both words and tags of a sentence by making conditional independence assumptions ( limited horizon dependence for states and independence of words given their tags ) that are only rough approximations .","label":"CompareOrContrast","metadata":{},"score":"73.86853"}{"text":"This project investigated a novel combination of statistical methods to define a flexible , though implicit , probability distribution for prediction of Part - of - Speech tags .The model can be improved upon in several ways .It is possible that using surrounding words , not just tags may be advantageous .","label":"CompareOrContrast","metadata":{},"score":"73.88303"}{"text":"This system outperforms previou ... \" .We describe a parsing system based upon a language model for English that is , in turn , based upon assigning probabilities to possible parses for a sentence .This model is used in a parsing system by finding the parse for the sentence with the highest probability .","label":"CompareOrContrast","metadata":{},"score":"74.22658"}{"text":"The most commonly used language models are very simple ( e.g. a Katz - smoothed trigram model ) .There are many improvements over this simple model however , including caching , clustering , higherorder n - grams , skipping models , and sentence - mixture models , all of which we will describe below .","label":"CompareOrContrast","metadata":{},"score":"74.24357"}{"text":"Lots of people have tried text chunking , using many different learning methods .The CoNLL-2000 shared task ( organized by the Special Interest Group of the ACL on Computational Natural Language Learning ) included both full chunking and noun phrase ( group ) chunking .","label":"CompareOrContrast","metadata":{},"score":"74.26621"}{"text":"Using LTAG based features in parse rerank- ing .In EMNLP 2003 : Proceedings of the ACL-03 Conference on Empirical Methods in Natural Language Processing , pages 89 - 96 .Stroppa , N. and Yvon , F. ( 2005 ) .","label":"CompareOrContrast","metadata":{},"score":"74.385895"}{"text":"© 2010 The editors and contributors .[ Show abstract ] [ Hide abstract ] ABSTRACT : Proceedings of the Ninth International Workshop on Treebanks and Linguistic Theories .Editors : Markus Dickinson , Kaili Müürisep and Marco Passarotti .NEALT Proceedings Series , Vol .","label":"CompareOrContrast","metadata":{},"score":"74.473114"}{"text":"Statistical machine learning techniques , while well proven in fields such as speech recognition , are just beginning to be applied to the information extraction domain .We explore the use of hidden Markov models for information extraction tasks , specifically focusing on how to learn model structure f ... \" .","label":"CompareOrContrast","metadata":{},"score":"74.592575"}{"text":"Finite - state machines solving analogies on words .Technical report , ENST .Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .","label":"CompareOrContrast","metadata":{},"score":"74.665924"}{"text":"In addition , many words may have not been previously encountered , so a tag must be decided upon based on various features of the word and its context .However , POS tagging is a simpler task than full syntactic parsing , since no attempt is made to create a tree - structured model of the sentence .","label":"CompareOrContrast","metadata":{},"score":"74.71039"}{"text":"In these cases , the observations are usually modeled as multinomial ... \" .Hidden Markov models ( HMMs ) are a powerful probabilistic tool for modeling sequential data , and have been applied with success to many text - related tasks , such as part - of - speech tagging , text segmentation and information extraction .","label":"CompareOrContrast","metadata":{},"score":"74.76396"}{"text":"In these cases , the observations are usually modeled as multinomial ... \" .Hidden Markov models ( HMMs ) are a powerful probabilistic tool for modeling sequential data , and have been applied with success to many text - related tasks , such as part - of - speech tagging , text segmentation and information extraction .","label":"CompareOrContrast","metadata":{},"score":"74.76396"}{"text":"Riezler , S. , King , T. H. , Kaplan , R. M. , Crouch , R. , John T. Maxwell , I. , and Johnson , M. ( 2001 ) .Parsing the Wall Street Journal using a Lexical - Functional Grammar and discriminative estimation techniques .","label":"CompareOrContrast","metadata":{},"score":"75.04294"}{"text":"This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this i ... \" .","label":"CompareOrContrast","metadata":{},"score":"75.147095"}{"text":"We present positive experimental results on the segmentation of FAQ 's . by Doug Cutting , Julian Kupiec , Jan Pedersen , Penelope Sibun - IN PROCEEDINGS OF THE THIRD CONFERENCE ON APPLIED NATURAL LANGUAGE PROCESSING , 1992 . \" ...We present an implementation of a part - of - speech tagger based on a hidden Markov model .","label":"CompareOrContrast","metadata":{},"score":"75.29414"}{"text":"References .[ Brill , 1995 ] Eric Brill .Transformation - based error - driven learning and natural language processing : A case study in part of speech tagging .Computational Linguistics 21:543 - 565 .[ Brill , 1998 ] Eric Brill and Jun Wu .","label":"CompareOrContrast","metadata":{},"score":"75.45497"}{"text":"Tiered tagging and combined language models classifiers .In TSD 1999 : Proceedings of the Second International Workshop on Text , Speech and Dialogue , pages 28 - 33 .Tufi¸ s , D. and Dragomirescu , L. ( 2004 ) .","label":"CompareOrContrast","metadata":{},"score":"75.59463"}{"text":"The gold - standard Cast3LB function labels are shown in the first row , the predicted tags in the first column .So e.g. suj was mistagged as cd in 26 cases .Page 168 . based learning .In ACL 2004 : Proceedings of the 42nd Annual Meeting of the Asso- ciation for Computational Linguistics , pages 311 - 318 .","label":"CompareOrContrast","metadata":{},"score":"75.91681"}{"text":"Weston , J. and Watkins , C. ( 1999 ) .Support vector machines for multiclass pattern recognition .In Proceedings of the Seventh European Symposium On Artificial Neural Networks .White , A. and Liu , W. ( 1994 ) .","label":"CompareOrContrast","metadata":{},"score":"75.95814"}{"text":"However , it can be implicitly defined through Gibbs sampling process .A sequential Gibbs sampler instantiates the variables to arbitrary initial values and loops over them , sampling from the conditionals .It can be shown ( Heckerman et al . , 2000 ) that if conditionals are positive , the process converges to a unique stationary distribution .","label":"CompareOrContrast","metadata":{},"score":"75.97603"}{"text":"Share .References .Rabiner , L.R. : A tutorial on hidden markov models and selected applications in speech recognition .Proceedings of the IEEE 77(2 ) , 257 - 285 ( 1989 ) CrossRef .Bühlmann , P. , Wyner , A.J. : Variable length markov chains .","label":"CompareOrContrast","metadata":{},"score":"76.03473"}{"text":"Ratnaparkhi points out that MaxEnt has the advantage of allowing specialized features ( like TBL ) and providing probabilities ( like an HMM ) .As an example of specialized features , he tried using conjunctions of features for difficult words , but found very little gain .","label":"CompareOrContrast","metadata":{},"score":"76.15287"}{"text":"In these results , the generative model performs significantly better than the others , and does about equally well at assigning part - of - speech tags . ... three lexicalist , linguistically perspicuous , qualitatively different models that we have developed and tested .","label":"CompareOrContrast","metadata":{},"score":"76.28421"}{"text":"Because they do not exploit dependencies between labels , such techniques are only well - suited to problems in which categories are independen ... \" .Common approaches to multi - label classification learn independent classifiers for each category , and employ ranking or thresholding schemes for classification .","label":"CompareOrContrast","metadata":{},"score":"76.59604"}{"text":"136 - 143 ( 1988 ) .DeRose , S.J. : Grammatical category disambiguation by statistical optimization .Computational Linguistics 14 , 31 - 39 ( 1988 ) .Brill , E. : Unsupervised learning of disambiguation rules for part of speech tagging .","label":"CompareOrContrast","metadata":{},"score":"77.175766"}{"text":"The version patched for 64-bit systems is ready in Git .The bugs in t3 and met related to large and/or noisy lexicons seem to have been fixed .The maintenance team has been expanded to three members .We have made it compile and work on Mac OS X , and have created autoconf / automake scripts , as well as an RPM spec file .","label":"CompareOrContrast","metadata":{},"score":"77.30556"}{"text":"Eric Brill .Automatic grammar induction and parsing free text : A transformation - based appraoch .In Proceedings of the 31stAnnual Meeting of the ACL .Walter Daelemans , Jakub Zavrel , Peter Berck & Steven Gillis .MBT : A memory - based part of speech tagger - generator .","label":"CompareOrContrast","metadata":{},"score":"77.31384"}{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"CompareOrContrast","metadata":{},"score":"77.40069"}{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"CompareOrContrast","metadata":{},"score":"77.40069"}{"text":"We show that the VLMC model performs better in terms of accuracy and almost equally in terms of tagging time , also doing very well in training time .However , the VLMC method actually fails to capture really long distance dependencies , and we analyse the reasons for such behaviour .","label":"CompareOrContrast","metadata":{},"score":"77.41661"}{"text":"Machine Learning , 15(3):321 - 329 .Xia , F. ( 1999 ) .Extracting Tree Adjoining Grammars from Bracketed Corpora .In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium , pages 398 - 403 .Page 174 .","label":"CompareOrContrast","metadata":{},"score":"77.75218"}{"text":"A linear programming formulation for global inference in natural language tasks .In CONLL 2004 : Eighth Conference on Computational Natural Language Learning , pages 1 - 8 .Schluter , N. and van Genabith , J. ( 2007 ) .Preparing , restructuring and augmenting a French treebank : Lexicalised parsing or coherent treebanks ?","label":"CompareOrContrast","metadata":{},"score":"77.97148"}{"text":"In LREC 2004 : Pro- 158 .Page 173 . ceedings of the Fourth International Language Resources and Evaluation Conference , pages 39 - 42 .van den Bosch , A. ( 2004 ) .Wrapped progressive sampling search for optimizing learn- ing algorithm parameters .","label":"CompareOrContrast","metadata":{},"score":"78.016174"}{"text":"Differing provisions from the publisher 's actual policy or licence agreement may be applicable .[ Show abstract ] [ Hide abstract ] ABSTRACT : Proceedings of the Ninth International Workshop on Treebanks and Linguistic Theories .Editors : Markus Dickinson , Kaili Müürisep and Marco Passarotti .","label":"CompareOrContrast","metadata":{},"score":"78.037506"}{"text":"Only a lexicon and some unlabeled training text are required .Accuracy exceeds 96 % .We describe implementation strategies and op ... \" .We present an implementation of a part - of - speech tagger based on a hidden Markov model .","label":"CompareOrContrast","metadata":{},"score":"78.055374"}{"text":"Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .The tenth CoNLL ( CoNLL - X ) saw a shared task on Multilingual Dependency Parsing .","label":"CompareOrContrast","metadata":{},"score":"78.2714"}{"text":"The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence .","label":"CompareOrContrast","metadata":{},"score":"78.46617"}{"text":"Feature forest models for probabilistic HPSG parsing .Computational Linguistics , 34(1):35 - 80 .Mooney , R. J. and Califf , M. E. ( 1995 ) .Induction of first - order decision lists : Results on learning the past tense of English verbs .","label":"CompareOrContrast","metadata":{},"score":"78.592606"}{"text":"Many theorists have dismissed a priori the idea that distributional information could play a significant role in syntactic category acquisition .We demonstrate empirically that such information provides a powerful cue to syntactic category membership , which can be exploited by a variety of simple , p ... \" .","label":"CompareOrContrast","metadata":{},"score":"78.670235"}{"text":"This project explores a novel approach to Part - of - Speech tagging that uses statistical techniques to train a model from a large POS - tagged corpus and assign tags to previously unseen text .The model uses decision trees based on tags of surrounding words and other features of a word to predict its tag .","label":"CompareOrContrast","metadata":{},"score":"78.82153"}{"text":"Part of speech tagging using maximum entropy .Ratnaparkhi used MaxEnt for POS tagging .In general , all features which occurred at least 10 times were kept .The search algorithm used was a beam search , keeping the N best tag sequences at each word .","label":"CompareOrContrast","metadata":{},"score":"79.03202"}{"text":"We present a detailed case study of this learni ... \" .this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance .","label":"CompareOrContrast","metadata":{},"score":"79.22258"}{"text":"We present a detailed case study of this learni ... \" .this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance .","label":"CompareOrContrast","metadata":{},"score":"79.22258"}{"text":"My aim is to investigate the issues arising when adapting an existing Lexical Functional Grammar ( LFG ) induction method to a new language and treebank , and find solutions which will generalize robustly across multiple languages .The research hypothesis is that by exploiting machine - learning algorithms to learn morphological features , lemmatization classes and grammatical functions from treebanks we can reduce the amount of manual specification and improve robustness , accuracy and domain- and language -independence for LFG parsing systems .","label":"CompareOrContrast","metadata":{},"score":"79.40901"}{"text":"They use the Baum - Welch algorithm [ 4 ] to train the model , and then use this trained model for tagging fresh text .It is not clear whether this approach of training on untagged text ... . by Andrew Kachites Mccallum , Kamal Nigam , Jason Rennie , Kristie Seymore - Information Retrieval , 2000 . \" ...","label":"CompareOrContrast","metadata":{},"score":"79.939705"}{"text":"ACL 1998 .[Chickering et al . , 1997 ] David Chickering , David Heckerman , Christopher Meek .A Bayesian Approach to Learning Bayesian Networks with Local Structure .Microsoft Technical Report MSR - TR-97 - 07 .[Heckerman et al . , 2000 ] David Chickering , Christopher Meek , Robert Rounthwaite , Carl Kadie .","label":"CompareOrContrast","metadata":{},"score":"80.05314"}{"text":"The contexts for TBL rules included words , part - of - speech assignments , and prior IOB tags .Results can be scored based on the correct assignment of tags , or on recall and precision of complete baseNPs .The latter is normally used as the metric , since it corresponds to the actual objective -- different tag sets can be used as an intermediate representation .","label":"CompareOrContrast","metadata":{},"score":"80.481705"}{"text":"Towards a Machine - Learning Architecture for Lexical Functional Grammar Parsing Grzegorz Chrupa ? la A dissertation submitted in fulfilment of the requirements for the award of Doctor of Philosophy ( Ph.D. ) to the Dublin City University School of Computing Supervisor : Prof. Josef van Genabith April 2008 .","label":"CompareOrContrast","metadata":{},"score":"80.57457"}{"text":"ACOPOST currently consists of four taggers which are based on different frameworks : .Maximum Entropy Tagger MET : This tagger uses an iterative procedure to successively improve parameters for a set of features that help to distinguish between relevant contexts .","label":"CompareOrContrast","metadata":{},"score":"80.81149"}{"text":"We demonstrate empirically that such information provides a powerful cue to syntactic category membership , which can be exploited by a variety of simple , psychologically plausible mechanisms .We present a range of results using a large corpus of child - directed speech and explore their psychological implications .","label":"CompareOrContrast","metadata":{},"score":"81.01092"}{"text":"Mart ´ ı , M. A. , Taul ´ e , M. , Bertran , M. , and M ' arquez , L. ( 2007 ) .AnCora : Multilingual and multilevel annotated corpora .edu / ancora / ancora - corpus .","label":"CompareOrContrast","metadata":{},"score":"81.15667"}{"text":"In ACL 1999 : Proceedings of the 37th Annual Conference of the Association for Computational Linguistics , pages 535 - 541 .Jurafsky , D. and Martin , J. H. ( 2008 ) .Speech and Language Processing .Prentice Hall , 2 edition .","label":"CompareOrContrast","metadata":{},"score":"81.20125"}{"text":"Eraall : brill@cs.jhu.edu .Word sense disambiguation , a problem which once seemed out of reach for systems without a great deal of hand cr ... . \" ...We describe a parsing system based upon a language model for English that is , in turn , based upon assigning probabilities to possible parses for a sentence .","label":"CompareOrContrast","metadata":{},"score":"81.730385"}{"text":"The general approach as well as the application to POS tagging has been proposed by Brill [ 1993].Example - based tagger ET : Example - based models ( also called memory - based , instance - based or distance - based ) rest on the assumption that cognitive behavior can be achieved by looking at past experiences that resemble the current problem rather than learning and applying abstract rules .","label":"CompareOrContrast","metadata":{},"score":"81.73655"}{"text":"We propose ( a ) a lexical affinity model where words struggle to modify each other , ( b ) a sense tagging model where words fluctuate randomly in their selectional prefe ... \" .After presenting a novel O(n³ ) parsing algorithm for dependency grammar , we develop three contrasting ways to stochasticize it .","label":"CompareOrContrast","metadata":{},"score":"81.763794"}{"text":"We propose ( a ) a lexical affinity model where words struggle to modify each other , ( b ) a sense tagging model where words fluctuate randomly in their selectional prefe ... \" .After presenting a novel O(n³ ) parsing algorithm for dependency grammar , we develop three contrasting ways to stochasticize it .","label":"CompareOrContrast","metadata":{},"score":"81.763794"}{"text":"baseNP chunking is a task for which people ( with some linguistics training ) can write quite good rules fairly quickly .This raises the practical question of whether we should be using machine learning at all .Clearly if there is already a large relevant resource , it makes sense to learn from it .","label":"CompareOrContrast","metadata":{},"score":"82.230774"}{"text":"It is , therefore , important to be able to perform morphological analysis in an accurate and robust way for morphologically rich languages .I propose a fully data - driven supervised method to simultaneously lemmatize and morphologically analyze text and obtain competitive or improved results on a range of typologically diverse languages .","label":"CompareOrContrast","metadata":{},"score":"82.31743"}{"text":"Rosenblatt , F. ( 1958 ) .The perceptron : A probabilistic model for information storage and organization in the brain .Psychological Review , ( 65):386 - 408 .Reprinted in Neurocomputing ( MIT Press , 1998 ) .Rosenfeld , R. ( 1996 ) .","label":"CompareOrContrast","metadata":{},"score":"82.51943"}{"text":"Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . .","label":"CompareOrContrast","metadata":{},"score":"82.6276"}{"text":"Levy , R. and Manning , C. ( 2003 ) .Is it harder to parse Chinese , or the Chinese tree- bank ?In ACL 2003 : Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics , pages 439 - 446 .","label":"CompareOrContrast","metadata":{},"score":"82.71047"}{"text":"[ Show abstract ] [ Hide abstract ] ABSTRACT : Proceedings of the Ninth International Workshop on Treebanks and Linguistic Theories .Editors : Markus Dickinson , Kaili Müürisep and Marco Passarotti .NEALT Proceedings Series , Vol .© 2010 The editors and contributors .","label":"CompareOrContrast","metadata":{},"score":"82.8288"}{"text":"About thirty different suffixes we distinguished , of which around twenty actually ended up being used by the induced decision tree .Tagging .Test sentences are tagged one at a time .N .n . select most commonly sampled tag for each T i .","label":"CompareOrContrast","metadata":{},"score":"83.07771"}{"text":"With a larger rule set , using Constraint Grammar rules , Voutilainen reports recall of 98%+ with precison of 95 - 98 % for noun chunks .Why do text chunking ?Partial parsing can be much faster , more robust , yet may be sufficient for many applications ( IE , QA ) .","label":"CompareOrContrast","metadata":{},"score":"83.1944"}{"text":"The proposed model combines trigram and structure knowledge of base phrase in which trigram is used to capture the local relation between words , while structure knowledge","label":"CompareOrContrast","metadata":{},"score":"83.28783"}{"text":"In Chinese , shallow parsing has gotten some promising results [ 13 , 14].However , due to the lacks of the fine - annotated corpus ( such as Treebanks ) and competitive syntactic parser , it is infeasible to build a language model that depends on the complete parsing technique such as Chelba [ 9].","label":"CompareOrContrast","metadata":{},"score":"83.435844"}{"text":"In Chinese , shallow parsing has gotten some promising results [ 13 , 14].However , due to the lacks of the fine - annotated corpus ( such as Treebanks ) and competitive syntactic parser , it is infeasible to build a language model that depends on the complete parsing technique such as Chelba [ 9].","label":"CompareOrContrast","metadata":{},"score":"83.43585"}{"text":"The proposed model combines trigram and structure knowledge of base phrase in which trigram is used to capture the local relation between words , while structure knowledge \" ...Lexical decoding is the obtaining of the most probable sequence of categories associated to a sequence of words .","label":"CompareOrContrast","metadata":{},"score":"83.921036"}{"text":"g been done in this area over the past few years ( eg .Part of speech tagging is also a very practical application , with uses in many areas , including speech recognition and gen .. \" ...Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position .","label":"CompareOrContrast","metadata":{},"score":"83.98222"}{"text":"Miyao , Y. , Ninomiya , T. , and Tsujii , J. ( 2003 ) .Probabilistic modeling of argument structures including non - local dependencies .In RANLP 2003 : Proceedings of the Conference on Recent Advances in Natural Language Processing , pages 285 - 291 .","label":"CompareOrContrast","metadata":{},"score":"84.0156"}{"text":"I thank all of my thesis committee members : John La erty from Carnegie Mellon University , Aravind Joshi , Lyle Ungar , and Mark Liberman , for their extremely valuable suggestions and comments about my thesis research .I thank Mike Collins , Jason Eisner , and Dan Melamed , with whom I 've had many stimulating and impromptu discussions in the LINC lab .","label":"CompareOrContrast","metadata":{},"score":"84.058655"}{"text":"Learning methods for text chunking .In his paper on POS tagging , Church also described a method for finding base noun phrases .He conducted an informal test ( 243 NPs ) and reported very good results ( 238 correct ) .","label":"CompareOrContrast","metadata":{},"score":"84.26233"}{"text":"Furhter morphological features can be used for tagging of unknown words .Recent work by Brill et al .( 1998 ) showed that combining several different state - of - the - art taggers ( HMM , MaxEnt , Transformation ) in a classifier ensemble can achieve performance of up to 97.2 % percent .","label":"CompareOrContrast","metadata":{},"score":"84.32785"}{"text":"Trigram Tagger T3 : This kind of tagger is based on Hidden Markov Models ( HMM ) where the states are tag pairs that emit words , i. e. , it 's based on transitional and lexical probabilities .The technique has been suggested by Rabiner [ 1990 ] and the implementation is influenced by Brants [ 2000].","label":"CompareOrContrast","metadata":{},"score":"84.79286"}{"text":"First , the initial samples must be discarded and sequential samples are not independent , so samples are actually counted after a short burn - in phase and with counts incremented every several iterations .Second , tags do not have to be sampled sequentially , and indeed , performance is improved when a random order is used .","label":"CompareOrContrast","metadata":{},"score":"85.20451"}{"text":"Tables do this by employing layout patterns to efficiently indicate fields and records in two - dimensional form . ...When the training labels make the state sequence unambiguous ( as they often do in practice ) , the likelihood function in exponential models such as CRFs is convex , so there are no local maxima , and t .. \" ...","label":"CompareOrContrast","metadata":{},"score":"85.40028"}{"text":"..The main aim of these approaches is practical utility , and deriving linguistically coherent categories is not a primary goal .Hence , this work , while suggestive , does n .. \" ...Lexical decoding is the obtaining of the most probable sequence of categories associated to a sequence of words .","label":"CompareOrContrast","metadata":{},"score":"85.4792"}{"text":"We compare models which use all CCG derivations , including nonstandard derivations , with normal - form models .The performances of the two models are comparable and the results are competitive with existing wide - coverage CCG parsers . ...","label":"CompareOrContrast","metadata":{},"score":"85.82297"}{"text":"The power of transformation - based approach comes partly from that fact that the initial assignment is already very accurate ( around 93 % ) .However , although the learning phase uses corpus statistics to induce rules , tagging itself is deterministic .","label":"CompareOrContrast","metadata":{},"score":"87.12633"}{"text":"Introduction We present a statistical parser that induces its grammar and probabilities from a hand - parsed corpus ( a tree - bank ) .Parsers induced from corpora are of interest both as simply exercises in machine learning and also because they are often the best parsers obtainable by any method .","label":"CompareOrContrast","metadata":{},"score":"87.63769"}{"text":"( Several models were tried , including max , min , product and mixture , but this one seemed to work best . )Since test data contains words not seen in the training data , we must predict tags for unknown words .","label":"CompareOrContrast","metadata":{},"score":"89.16294"}{"text":"Lexical decoding is the obtaining of the most probable sequence of categories associated to a sequence of words .This paper describes two lexical decoding combined models which are based on a stochastic category - based model and a probabilistic model of word distribution into linguistic categories .","label":"CompareOrContrast","metadata":{},"score":"89.62425"}{"text":"Lexical decoding is the obtaining of the most probable sequence of categories associated to a sequence of words .This paper describes two lexical decoding combined models which are based on a stochastic category - based model and a probabilistic model of word distribution into linguistic categories .","label":"CompareOrContrast","metadata":{},"score":"89.62425"}{"text":"The rules for conjoined NPs are complicated by the bracketing rules of the Penn Tree Bank .Conjoined prenominal nouns are generally treated as part of a single baseNP : \" brick and mortar university \" ( with \" brick and mortar \" modifying \" university \" ) .","label":"CompareOrContrast","metadata":{},"score":"90.090836"}{"text":"Page 7 .Page 8 .List of Tables 2.1 LFG Grammatical functions . . . . . . . . . . . . . . . . . . . . . . . ..10 5.1 Features included in POS tags .","label":"CompareOrContrast","metadata":{},"score":"90.257645"}{"text":"Once performed by hand , POS tagging is now done in the context of computational linguistics , using algorithms which associate discrete terms , as well as hidden parts of speech , in accordance with a set of descriptive tags .News .","label":"CompareOrContrast","metadata":{},"score":"90.267914"}{"text":"This is useful in a large variety of areas including speech recognition , optical character recognition , handwriting recognition , machine translation , and spelling correction ( Church , 1988 ; Brown et al . , 1990 ; Hull , 1 ... \" .","label":"CompareOrContrast","metadata":{},"score":"90.91644"}{"text":"Learning the Past Tense of English Verbs : The Symbolic Pattern Associator vs. Connectionist Models .Journal of Artificial Intelligence Research , 1:209 - 229 .Magerman , D. ( 1994 ) .Natural Language Parsing as Statistical Pattern Recognition .PhD thesis , Department of Computer Science , Stanford University , CA .","label":"CompareOrContrast","metadata":{},"score":"91.06718"}{"text":"Documents often contain tables in order to communicate densely packed , multi - dimensional information .Tables do this by employing layout pa ... \" .The ability to find tables and extract information from them is a necessary component of data mining , question answering , and other information retrieval tasks .","label":"CompareOrContrast","metadata":{},"score":"91.18387"}{"text":".. consisting of a term and a label , as well as a feature for each triplet consisting of a term and two labels . \" ... this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .","label":"CompareOrContrast","metadata":{},"score":"91.598785"}{"text":"Only a lexicon and some unlabeled training text are required .Accuracy exceeds 96 % .We describe implementation strategies and optimizations which result in high - speed operation .Three applications for tagging are described : phrase recognition ; word sense disambiguation ; and grammatical function assignment . \" ...","label":"CompareOrContrast","metadata":{},"score":"91.72116"}{"text":"In addition , many words are rare , so parameter estimation is unreliable because of sparsity of the data .Since many words only appear rarely and most words appear overwhelmingly with one tag , we should devote more attention to predicting tags for the common and difficult to tag words .","label":"CompareOrContrast","metadata":{},"score":"91.83731"}{"text":"Learning them reliably permits grammar induction to depend less on language - specific LFG annotation rules .I therefore propose ways to improve acquisition of function labels from treebanks and translate those improvements into better - quality f - structure parsing .","label":"CompareOrContrast","metadata":{},"score":"92.43344"}{"text":"In LFG 2005 : Proceedings of the Tenth International Conference on Lexical Functional Grammar , pages 334 - 352 .Oya , M. and van Genabith , J. ( 2007 ) .Automatic acquisition of Lexical - Functional Grammar resources from a Japanese dependency corpus .","label":"CompareOrContrast","metadata":{},"score":"92.73477"}{"text":"( Without lexical information , they got about 90.5 % recall and precision . )R&M mention two major sources of error ( and these are also error sources for simple finite - state patterns for baseNP ) : participles and conjunction .","label":"CompareOrContrast","metadata":{},"score":"92.909164"}{"text":".. rning deserves further study .There are many different ways one could try to construct a language learner .In [ 65 ] , a selforganizing language learner is proposed to be used for language modelling .In this work we take a different approach , namely starting with a s .. G22.2591 - Advanced Natural Language Processing - Spring 2004 .","label":"CompareOrContrast","metadata":{},"score":"92.954346"}{"text":"It is entirely possible that two techniques that work well separately will not work well together , and , as we will show , even possible that some techniques will work better together than either one does by itself .In this ... . \" ...","label":"CompareOrContrast","metadata":{},"score":"94.596085"}{"text":"Alves , C.D.C. , Finger , M. : Etiquetagem do português clássico baseada em córpora .In : Proceedings of IV Encontro para o Processamento Computacional da Língua Portuguesa Escrita e Falada ( PROPOR 1999 ) , Évora , Portugal , pp .","label":"CompareOrContrast","metadata":{},"score":"94.67943"}{"text":"Text Chunking ( J&M sec .What is text chunking ?Text chunking subsumes a range of tasks .The simplest is finding ' noun groups ' or ' base NPs ' ... non - recursive noun phrases up to the head ( for English ) .","label":"CompareOrContrast","metadata":{},"score":"94.909164"}{"text":"This functionality is not possible ... \" .Domain - specific internet portals are growing in popularity because they gather content from the Web and organize it for easy access , retrieval and search .This functionality is not possible with general , Web - wide search engines .","label":"CompareOrContrast","metadata":{},"score":"95.073364"}{"text":"Iwould like toacknowledge the following people for their contribution to my education : I thank my advisor Mitch Marcus , who gave me the intellectual freedom to pursue what I believed to be the best way to approach natural language processing , and also gave me direction when necessary .","label":"CompareOrContrast","metadata":{},"score":"95.75189"}{"text":".. 7 This inextensibility is also discussed by Hearst ( 1992 ) .8 A sample of 100 uses of in from the New York Times suggests ... . by Kristie Seymore , Andrew $ i Mccallum , Ronald Rosenfeld T - in Proc .","label":"CompareOrContrast","metadata":{},"score":"97.65328"}{"text":"Signed ( Grzegorz Chrupa ? la ) Student ID 55130089 Date April 2008 i .Page 3 .Page 4 .Page 5 .Page 6 .Below the corresponding ( simplified ) LFG f - structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .","label":"CompareOrContrast","metadata":{},"score":"97.670334"}{"text":"This level of performance , although not quite state - of - the - art , is quite reasonable .Some words are very difficult to classify correctly , perhaps due to the limited context window and linguistic depth of this model and other current state - of - the - art models .","label":"CompareOrContrast","metadata":{},"score":"97.68941"}{"text":"Finger , M. : Técnicas de otimização da precisão empregadas no etiquetador Tycho Brahe .In : Proceedings of V Encontro para o Processamento Computacional da Língua Portuguesa Escrita e Falada ( PROPOR 2000 ) , Atibaia , Brazil , pp .","label":"CompareOrContrast","metadata":{},"score":"98.67549"}{"text":"Renamed ICOPOST to ACOPOST and moved the package to the Sourceforge repository of open source projects .Released version 1.8.4 , which contained a preliminary user 's guide .The project was put on halt since Ingo Schröder ( the original maintainer ) would not have the time to maintain the package .","label":"CompareOrContrast","metadata":{},"score":"98.855774"}{"text":"[ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [ NP only $ 1.8 billion ] [ PP in ] [ NP September ] .In any case , the chunks are non - recursive structures which can potentially be handled by finite - state methods .","label":"CompareOrContrast","metadata":{},"score":"99.08741"}{"text":"In particular , the suffix of a word is often a good predictor ( e.g. -tion , -ed , -ly , -ing ) .Capitalization and whether the word comes after a period or quotation marks are also indicative .In addition , numbers are rarely seen in the training data , but often can be easily classified as such ( note that numbers can also act as list markers ) .","label":"CompareOrContrast","metadata":{},"score":"99.843185"}{"text":"Although the model does not achieve state - of - the - art accuracy ( 96.4 - 96.8 % ) , it comes respectably close ( 96.2 % ) .Introduction .Part - of - speech tagging consists of labeling each word in a sentence by its appropriate part of speech , e.g. verb , noun , adjective , adverb .","label":"CompareOrContrast","metadata":{},"score":"100.99158"}{"text":"Until we make the release , users interested in the new version should clone the Git repository .For more information on the project , please write me ( Tiago ) .Project changes : Tiago Tresoldi is the new maintainer ; besides a new home page , the programs are being adapted to 64-bit systems and code is being cleaned .","label":"CompareOrContrast","metadata":{},"score":"101.85453"}{"text":"Aires , R.V.X. : Implementação , adaptação , combinação e avaliação de etiquetadores para o português do brasil .Dissertação de mestrado , Instituto de Ciências Matemáticas e Computação , Universidade de São Paulo - Campus São Carlos ( 2000 )Welcome to the home page of ACOPOST , a free and open source collection of part - of - speech taggers .","label":"CompareOrContrast","metadata":{},"score":"103.684784"}{"text":"[ 1996].A detailed description , an extensive evaluation and new suggestions can be found in an accompanying technical report [ Schröder 2002 ] .References .Thorsten Brants .TnT - as statistical part - of - speech tagger .","label":"CompareOrContrast","metadata":{},"score":"116.51619"}{"text":"Released version 0.9.0 ( first public release ) .First public talk about ICOPOST .Web page started .What is ACOPOST about ?Part - of - speech ( POS ) tagging is the task of assigning grammatical classes to words in a natural language sentence .","label":"CompareOrContrast","metadata":{},"score":"117.41282"}