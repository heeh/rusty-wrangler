{"text":"During the evaluation phase , participants will be requested to rank system outputs of other participants through a web - based interface ( Appraise , Federmann 2010 ) .Automatic metrics include BLEU ( Papineni et .Al , 2002 ) , TER ( Snover et al . , 2006 ) and METEOR ( Lavie , 2005 ) .","label":"Uses","metadata":{},"score":"31.035648"}
{"text":"Comparison is based on finding sequences of words in the reference translations that match word sequences in the system output translation .More information on the evaluation algorithm may be obtained from the paper detailing the algorithm : BLEU : a Method for Automatic Evaluation of Machine Translation ( Papineni et al , 2002 ) .","label":"Uses","metadata":{},"score":"34.907234"}
{"text":"More information on the evaluation algorithm may be obtained from the paper detailing the algorithm : BLEU : a Method for Automatic Evaluation of Machine Translation ( Papineni et al , 2002 ) .The included scoring script was released with the original evaluation , intended for use with SGML - formatted data files , and is provided to ensure compatibility of user scoring results with results from the original evaluation .","label":"Uses","metadata":{},"score":"36.418564"}
{"text":"More information on the evaluation algorithm may be obtained from the paper detailing the algorithm : BLEU : a Method for Automatic Evaluation of Machine Translation ( Papineni et al , 2002 ) .The included scoring script was released with the original evaluation , intended for use with SGML - formatted data files , and is provided to ensure compatibility of user scoring results with results from the original evaluation .","label":"Uses","metadata":{},"score":"36.418564"}
{"text":"By measuring the output of the systems against the original corpus data for the target language the adequacy of the translation can be assessed .Koehn uses the BLEU metric by Papineni et al .( 2002 ) for this , which counts the coincidences of the two compared versions - SMT output and corpus data - and calculates a score on this basis .","label":"Uses","metadata":{},"score":"38.21251"}
{"text":"Furthermore , Koehn uses the SMT systems and the Europarl corpus data to investigate whether back translation is an adequate method for the evaluation of machine translation systems .[ 1 ] The results indicate that the scores for back translation are far higher than those for monodirectional translation and what is more important they do not correlate at all with the monodirectional scores .","label":"Uses","metadata":{},"score":"39.29446"}
{"text":"An updated scoring software package ( mteval-v13a-20091001.tar .gz ) , with XML support , additional options and bug fixes , documentation , and example translations , may be downloaded from the NIST Multimodal Information Group Tools website .Data .This release contains 494 documents with corresponding sets of four separate human expert reference translations .","label":"Uses","metadata":{},"score":"45.56759"}
{"text":"Scoring Tools .This evaluation kit includes a single perl script ( mteval - v09c . pl ) that may be used to produce a translation quality score for one ( or more ) MT systems .The script works by comparing the system output translation with a set of ( expert ) reference translations of the same source text .","label":"Uses","metadata":{},"score":"46.36546"}
{"text":"Scoring Tools .This evaluation kit includes a single Perl script ( mteval - v11a . pl ) that may be used to produce a translation quality score for one ( or more ) MT systems .The script works by comparing the system output translation with a set of ( expert ) reference translations of the same source text .","label":"Uses","metadata":{},"score":"46.57953"}
{"text":"^ Papineni , Kishore et al ( 2002 ) : BLEU .A method for automatic evaluation of machine translation , in : Proceedings of the 40th Annual Meeting of the Association of Computational Linguistics ( ACL ) , pp . 311 - 318 .","label":"Uses","metadata":{},"score":"47.117638"}
{"text":"\" nist \" -- a variant of BLEU which weights n - gram matches by how informative they are ( Doddington , 2002 ) .\" wer \" -- word error rate ( Nießen et al . , 2000 ) .For a comparison of these various metrics , see : .","label":"Uses","metadata":{},"score":"47.819298"}
{"text":"\" nist \" -- a variant of BLEU which weights n - gram matches by how informative they are ( Doddington , 2002 ) .\" wer \" -- word error rate ( Nießen et al . , 2000 ) .For a comparison of these various metrics , see : .","label":"Uses","metadata":{},"score":"47.819298"}
{"text":"P. Koehn ( 2004 ) .Statistical Significance Tests for Machine Translation Evaluation .EMNLP 2004 .P. Nakov , F. Guzmán , and S. Vogel ( 2012 ) .Optimizing for Sentence - Level BLEU+1 Yields Short Translations .COLING 2012 .","label":"Uses","metadata":{},"score":"48.612167"}
{"text":"There is also an implementation of Rampion in cdec .Version 0.2 also includes implementations of PRO and risk minimization as well as several additional forms of ramp loss from Gimpel ( 2012 ) .Also included are the improved sentence - level BLEU approximations from Nakov et al .","label":"Uses","metadata":{},"score":"49.199417"}
{"text":"The output has been annotated with system - internal meta - data information derived from the translation process of each of the systems . al . , 2009;and Huajian RBMT ) will be provided .( Participants are required to fill out a shared task evaluation agreement form and obtain the ZH - EN data from LDC ) .","label":"Uses","metadata":{},"score":"50.050236"}
{"text":"Participants can also make use of additional ( linguistic analysis , confidence estimation etc . ) tools , if their systems require so , but they have to explicitly declare this upon submission , so that they are judged as \" unconstrained \" systems .","label":"Uses","metadata":{},"score":"50.765663"}
{"text":"BLEU : a Method for Automatic Evaluation of Machine Translation .ACL 2002 .Acknowledgments .The workshop and associated shared task are an effort to trigger a systematic investigation on improving state - of - the - art hybrid machine translation , making use of advanced machine - learning ( ML ) methodologies .","label":"Uses","metadata":{},"score":"50.805553"}
{"text":"OBJECTIVE .\" bleu \" -- the percentage of common n - grams found in machine and reference translations ( Papineni et al . , 2002 ) .\" ter \" -- translation edit rate , i.e. shortest edit sequence to turn a machine translation into a reference ( Snover et al . , 2006 ) .","label":"Uses","metadata":{},"score":"52.5115"}
{"text":"Below is a link to code that implements the models described by Gimpel and Smith ( 2011a ) .These models can discover gappy patterns in either monolingual or bilingual ( word - aligned ) text .Sample data files and execution scripts are provided .","label":"Uses","metadata":{},"score":"55.00847"}
{"text":"[ 1 ] Koehn explains this with the fact that errors committed in the translation process might simply be reversed by back translation resulting in high coincidences of in- and output .[ 1 ] This , however , does not allow any conclusions about the quality of the text in the actual target language .","label":"Uses","metadata":{},"score":"58.89237"}
{"text":"Each reference file contains four independent translations of the data set .The evaluation year , source language , test set ( which , by default , is \" evalset \" ) , version of the data , and source vs. reference file ( with the latter being indicated by \" -ref \" ) are reflected in the file name .","label":"Uses","metadata":{},"score":"59.605827"}
{"text":"Each reference file contains four independent translations of the data set .The evaluation year , source language , test set ( which , by default , is \" evalset \" ) , version of the data , and source vs. reference file ( with the latter being indicated by \" -ref \" ) are reflected in the file name .","label":"Uses","metadata":{},"score":"59.605827"}
{"text":"While participants are encouraged to use machine learning techniques to explore the additional meta - data information sources , other general improvements in hybrid and combination based MT are welcome to participate in the challenge .For systems that exploit additional meta - data information the challenge is that additional meta - data is highly heterogeneous and ( individual ) system specific .","label":"Uses","metadata":{},"score":"59.910774"}
{"text":"For a comparison of these various metrics , see : . \" bleu \" -- the percentage of common n - grams found in machine and reference translations ( Papineni et al . , 2002 ) .\" ter \" -- translation edit rate , i.e. shortest edit sequence to turn a machine translation into a reference ( Snover et al . , 2006 ) .","label":"Uses","metadata":{},"score":"60.051353"}
{"text":"[ 1 ] After sentence splitting and tokenization the sentences were aligned across languages with the help of an algorithm developed by Gale & Church ( 1993 ) .[ 1 ] .Contents .In his paper \" Europarl : A Parallel Corpus for Statistical Machine Translation \" ( 2005 )","label":"Uses","metadata":{},"score":"60.368523"}
{"text":"Selected human reference translations and system translations for the NIST MT08 test sets are contained in NIST Open Machine Translation 2008 Evaluation ( MT08 ) Selected Reference and System Translations LDC2010T01 .Scoring Tools .This evaluation kit includes a single Perl script ( mteval - v11b . pl ) that may be used to produce a translation quality score for one ( or more ) MT systems .","label":"Uses","metadata":{},"score":"63.169968"}
{"text":"The package was compiled and scoring software was developed by researchers at NIST , making use of broadcast , newswire and web data and reference translations collected and developed by LDC .The objective of the NIST Open Machine Translation ( OpenMT ) evaluation series is to support research in , and help advance the state of the art of , machine translation ( MT ) technologies -- technologies that translate text between human languages .","label":"Uses","metadata":{},"score":"63.18684"}
{"text":"The first workshop also road - tested a shared task ( and associated data set ) and laid the basis for a broader reach in 2012 .The main focus of the Shared Task is to address the question : \" Can Hybrid MT and System Combination techniques benefit from extra information ( linguistically motivated , decoding , runtime , confidence scores , or other meta - data ) from the systems involved ?","label":"Uses","metadata":{},"score":"63.33068"}
{"text":"For each language , the test set consists of two files : a source and a reference file .Each file contains four independent translations of the data set .The evaluation year , source language , test set ( which , by default , is \" evalset \" ) , version of the data , and source vs. reference file ( with the latter being indicated by \" -ref \" ) are reflected in the file name .","label":"Uses","metadata":{},"score":"63.363056"}
{"text":"The OpenMT evaluations are intended to be of interest to all researchers working on the general problem of automatic translation between human languages .To this end , they are designed to be simple , to focus on core technology issues , and to be fully supported .","label":"Uses","metadata":{},"score":"63.378574"}
{"text":"The OpenMT evaluations are intended to be of interest to all researchers working on the general problem of automatic translation between human languages .To this end , they are designed to be simple , to focus on core technology issues , and to be fully supported .","label":"Uses","metadata":{},"score":"63.378574"}
{"text":"The OpenMT evaluations are intended to be of interest to all researchers working on the general problem of automatic translation between human languages .To this end , they are designed to be simple , to focus on core technology issues and to be fully supported .","label":"Uses","metadata":{},"score":"63.408775"}
{"text":"[ 1 ] Results reflect that some SMT systems perform better than others , e.g. , Spanish - French ( 40.2 ) in comparison to Dutch - Finnish ( 10.3 ) .[ 1 ] Koehn states that the reason for this is that related languages are easier to translate into each other than those that are not .","label":"Uses","metadata":{},"score":"64.09468"}
{"text":"They can provide solutions based on opensource systems , or develop their own mechanisms .The development set can be used for tuning the systems during the development phase .Final submissions have to include translation output on a test set , which will be made available one week after training data release .","label":"Uses","metadata":{},"score":"65.73381"}
{"text":"\" terp \" -- a variant of TER with synonym and paraphrase matching turned on ( super slow ) ( Snover et al . , 2009 ) .\" nist \" -- a variant of BLEU which weights n - gram matches by how informative they are ( Doddington , 2002 ) .","label":"Uses","metadata":{},"score":"65.99527"}
{"text":"The package was compiled and scoring software was developed by researchers at NIST , making use of newswire source data and reference translations collected and developed by LDC .The objective of the NIST OpenMT evaluation series is to support research in , and help advance the state of the art of , machine translation ( MT ) technologies -- technologies that translate text between human languages .","label":"Uses","metadata":{},"score":"66.14505"}
{"text":"The package was compiled and scoring software was developed by researchers at NIST , making use of newswire source data and reference translations collected and developed by LDC .The objective of the NIST OpenMT evaluation series is to support research in , and help advance the state of the art of , machine translation ( MT ) technologies -- technologies that translate text between human languages .","label":"Uses","metadata":{},"score":"66.14505"}
{"text":"For a comparison of these various metrics , see : . ini file under [ The following options are available : . \" bleu \" -- the percentage of common n - grams found in machine and reference translations ( Papineni et al . , 2002 ) .","label":"Uses","metadata":{},"score":"66.53311"}
{"text":"Our goal is to make machine translation systems better and faster , and also to develop techniques that can be useful for other areas of natural language processing and machine learning .Software .Rampion .Rampion ( Gimpel and Smith , 2012 ) is an algorithm for training statistical machine translation models based on minimizing structured ramp loss .","label":"Uses","metadata":{},"score":"67.55199"}
{"text":"As the manual evaluation will take longer , its results will be presented and published at the workshop .NIST Multimodal Information Group .NIST 2008 Open Machine Translation ( OpenMT ) Evaluation LDC2010T21 .Web Download .Philadelphia : Linguistic Data Consortium , 2010 .","label":"Uses","metadata":{},"score":"68.10167"}
{"text":"\" terp \" -- a variant of TER with synonym and paraphrase matching turned on ( super slow ) ( Snover et al . , 2009 ) .\" nist \" -- a variant of BLEU called the NIST which weights n - gram matches by how informative they are ( Doddington , 2002 ) .","label":"Uses","metadata":{},"score":"70.12817"}
{"text":"Shared task results should be submitted via email attachment .Please compress your results as .zip or .gz archive and send them to cfedermann@dfki.de .Use \" ML4HMT-12 Shared Task Submission \" as mail subject .Shared task results are due by October 28th .","label":"Uses","metadata":{},"score":"70.23622"}
{"text":"gz ) , with XML support , additional options and bug fixes , documentation , and example translations , may be downloaded from the NIST Multimodal Information Group Tools website .Data .The Chinese - language and Arabic - language source text included in this corpus is a reorganization of data that was initially released to the public respectively as Multiple - Translation Chinese ( MTC ) Part 4 ( LDC2006T04 ) and Multiple - Translation Arabic ( MTA ) Part 2 ( LDC2005T05 ) .","label":"Uses","metadata":{},"score":"70.391205"}
{"text":"The goal is for the output to be an adequate and fluent translation of the original .The MT evaluation series started in 2001 as part of the DARPA TIDES ( Translingual Information Detection , Extraction ) program .Beginning with the 2006 evaluation , the evaluations have been driven and coordinated by NIST as NIST OpenMT .","label":"Uses","metadata":{},"score":"70.60866"}
{"text":"The goal is for the output to be an adequate and fluent translation of the original .The MT evaluation series started in 2001 as part of the DARPA TIDES ( Translingual Information Detection , Extraction ) program .Beginning with the 2006 evaluation , the evaluations have been driven and coordinated by NIST as NIST OpenMT .","label":"Uses","metadata":{},"score":"70.60866"}
{"text":"The goal is for the output to be an adequate and fluent translation of the original .The MT evaluation series started in 2001 as part of the DARPA TIDES ( Translingual Information Dectection , Extraction ) program .Beginning with the 2006 evaluation , the evaluations have been driven and coordinated by NIST as NIST OpenMT .","label":"Uses","metadata":{},"score":"71.482544"}
{"text":"cdec . cdec ( Dyer et al . , 2010 ) is a flexible and efficient software framework for machine translation and other structured prediction tasks .It implements training and decoding algorithms for several commonly - used models in machine translation .","label":"Uses","metadata":{},"score":"72.78961"}
{"text":"[ 1 ] The latest release ( 2012 ) [ 2 ] comprised up to 50 million words per language with the newly added languages being slightly underrepresented as data for them is only available from 2007 onwards .[ 1 ] .","label":"Uses","metadata":{},"score":"73.7135"}
{"text":"He uses the corpus to develop SMT systems translating each language into each of the other ten languages of the corpus making it 110 systems .This enables Koehn to establish SMT systems for uncommon language pairs that have not been considered by SMT developers beforehand , such as Finnish - Italian for example .","label":"Uses","metadata":{},"score":"76.8959"}
{"text":"DARPA TIDES MT and NIST OpenMT evaluations used SGML - formatted test data until 2008 and XML - formatted test data thereafter .This files in this package are povided in both formats .Samples .For an example of the data in this corpus , please review the sample file .","label":"Uses","metadata":{},"score":"78.67879"}
{"text":"No updates have been issued as of this time .Copyright .NIST Multimodal Information Group .NIST 2003 Open Machine Translation ( OpenMT ) Evaluation LDC2010T11 .Web Download .Philadelphia : Linguistic Data Consortium , 2010 .Introduction .NIST 2003 Open Machine Translation ( OpenMT ) Evaluation is a package containing source data , reference translations , and scoring software used in the NIST 2003 OpenMT evaluation .","label":"Uses","metadata":{},"score":"79.77162"}
{"text":"Copyright .Portions © 2003 Agence France - Presse , © 2003 Xinhua News Agency , © 2004 - 2006 , 2010 Trustees of the University of Pennsylvania . \" bleu \" -- the percentage of common n - grams found in machine and reference translations ( Papineni et al . , 2002 ) .","label":"Uses","metadata":{},"score":"80.65147"}
{"text":"NIST 2008 Open Machine Translation ( OpenMT ) Evaluation , Linguistic Data Consortium ( LDC ) catalog number LDC2010T21 and isbn 1 - 58563 - 567 - 7 , is a package containing source data , reference translations and scoring software used in the NIST 2008 OpenMT evaluation .","label":"Uses","metadata":{},"score":"81.448074"}
{"text":"gz ) , with XML support , additional options and bug fixes , documentation , and example translations , may be downloaded from the NIST Multimodal Information Group Tools website .Data .This corpus consists of 150 Arabic newswire documents , 150 Chinese newswire documents , and 29 Chinese \" prepared speech \" documents , and a corresponding set of four separate human expert reference translations .","label":"Uses","metadata":{},"score":"87.17613"}
{"text":"NIST 2004 Open Machine Translation ( OpenMT ) Evaluation LDC2010T12 .Web Download .Philadelphia : Linguistic Data Consortium , 2010 .Introduction .NIST 2004 Open Machine Translation ( OpenMT ) Evaluation , is a package containing source data , reference translations , and scoring software used in the NIST 2004 OpenMT evaluation .","label":"Uses","metadata":{},"score":"88.024086"}
{"text":"All source data for this corpus is newswire text collected in January and February of 2003 from Agence France - Presse , and Xinhua News Agency .For details on the methodology of the source data collection and production of reference translations , see the documentation for the above - mentioned corpora .","label":"Uses","metadata":{},"score":"89.51652"}
{"text":"The files in this package are provided in both formats .Sample .Sample text file containing excerpts from different xml files included in this corpus , including reference translations and source text for a single newswire document .The file is encoded in UTF-8 .","label":"Uses","metadata":{},"score":"97.66872"}
{"text":"The files in this package are provided in both formats .Sample .Sample text file containing excerpts from different xml files included in this corpus , including reference translations and source text for a single newswire document .The file is encoded in UTF-8 .","label":"Uses","metadata":{},"score":"97.66872"}
{"text":"Europarl corpus .The Europarl Corpus is a corpus ( set of documents ) that consists of the proceedings of the European Parliament from 1996 to the present .In its first release in 2001 , it covered eleven official languages of the European Union ( Danish , Dutch , English , Finnish , French , German , Greek , Italian , Portuguese , Spanish , and Swedish ) .","label":"Uses","metadata":{},"score":"102.397804"}
{"text":"There are no updates available at this time .Copyright .ARK Machine Translation Research .This page is the home for machine translation research conducted by members of Noah 's ARK in the Language Technologies Institute at Carnegie Mellon University .","label":"Uses","metadata":{},"score":"110.15834"}
