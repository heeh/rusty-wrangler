{"text":"This paper describes a simple yet novel method for constructing sets of 50-best parses based on a co ... \" .Discriminative reranking is one method for constructing high - performance statistical parsers ( Collins , 2000 ) .A discriminative reranker requires a source of candidate parses for each sentence .","label":"Background","metadata":{},"score":"21.856525"}
{"text":"Although lexical amb ... \" ...Discriminative reranking is one method for constructing high - performance statistical parsers ( Collins , 2000 ) .A discriminative reranker requires a source of candidate parses for each sentence .This paper describes a simple yet novel method for constructing sets of 50-best parses based on a co ... \" .","label":"Background","metadata":{},"score":"24.315985"}
{"text":"A discriminative reranker requires a source of candidate parses for each sentence .This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse - to - fine generative parser ( Charniak , 2000 ) .","label":"Background","metadata":{},"score":"25.579998"}
{"text":"Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand - annotated training data . \" ...Discriminative reranking is one method for constructing high - performance statistical parsers ( Collins , 2000 ) .","label":"Background","metadata":{},"score":"25.892685"}
{"text":", 1998 ) is the boosting algorithm that we use in order to learn to rerank outputs .It also has been used with good effect in reranking outputs of a statistical parser ( Collins , 2000 ) and ranking sentence plans ( Walker , Rambow and Rogati , 2001 ) .","label":"Background","metadata":{},"score":"34.622074"}
{"text":", 1998 ) is the boosting algorithm that we use in order to learn to rerank outputs .It also has been used with good effect in reranking outputs of a statistical parser ( Collins , 2000 ) and ranking sentence plans ( Walker , Rambow and Rogati , 2001 ) .","label":"Background","metadata":{},"score":"34.622074"}
{"text":"This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this i ... \" .","label":"Background","metadata":{},"score":"36.248024"}
{"text":"This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this i ... \" .","label":"Background","metadata":{},"score":"36.248024"}
{"text":"This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence .","label":"Background","metadata":{},"score":"38.825394"}
{"text":"This could be accomplished by using learning methods to build a module that identifies partial hypotheses during search which are very unlikely to be part of the final parse .You could use the parser described in my thesis as the baseline system for this problem ; the goal would be to improve efficiency without compromising accuracy .","label":"Background","metadata":{},"score":"38.919342"}
{"text":"The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .","label":"Background","metadata":{},"score":"39.19284"}
{"text":"The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .","label":"Background","metadata":{},"score":"39.19284"}
{"text":"The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .","label":"Background","metadata":{},"score":"39.19284"}
{"text":"The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .","label":"Background","metadata":{},"score":"39.19284"}
{"text":"Given the data and code currently available , this project would involve : 1 ) writing code which identified features in parse trees ; 2 ) experimentation investigating how different feature choices affect performance .Algorithms for Parse Reranking .We 've described various algorithms applied to parameter estimation for parse reranking : the perceptron , boosting , and log - linear models , for example .","label":"Background","metadata":{},"score":"39.573708"}
{"text":"These algorithms have n't been applied to the reranking problem in the past , but it would be interesting to see how they perform .Datasets are available , in terms of files representing the parse data that is input to the parameter estimation code .","label":"Background","metadata":{},"score":"40.17537"}
{"text":"A second model then attempts to improve upon this i ... \" .This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .","label":"Background","metadata":{},"score":"40.723316"}
{"text":"In particular , it allows one to efficiently learn a model which discriminates among the entire space of parse trees , as opposed to reranking the top few candidates .Our models can condition on arbitrary features of input sentences , thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness .","label":"Background","metadata":{},"score":"42.045082"}
{"text":"Recently , in the global linear models section of class , we 've described reranking methods which represent parse trees using feature - vector representations .One learning algorithm we went over was boosting .Both datasets ( n - best parse trees ) , and boosting code are available .","label":"Background","metadata":{},"score":"42.503017"}
{"text":"My guess ( which I 've suggested and they seem interested in exploring ) is the following .The unreranked parser produces systematic errors , and so self - training just reinforces these errors leading to worse performance .In contrast , the reranked parser has a high variance of errors ( this is intuitively reasonable if you think about how reranking works ) and therefore these come across as \" noise \" when you retrain on the large corpus .","label":"Background","metadata":{},"score":"42.607117"}
{"text":"We also give an overview of the parsing approaches that participants took and the results that they achieved .Finally , we try to draw general conclusions about multi - lingual parsing : What makes a particular language , treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser ?","label":"Background","metadata":{},"score":"42.73297"}
{"text":"We introduce a new method for the reranking task , based on the boosting approach to ranking problems described in Freund et al .( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .","label":"Background","metadata":{},"score":"42.837456"}
{"text":"We introduce a new method for the reranking task , based on the boosting approach to ranking problems described in Freund et al .( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .","label":"Background","metadata":{},"score":"42.837456"}
{"text":"We introduce a new method for the reranking task , based on the boosting approach to ranking problems described in Freund et al .( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .","label":"Background","metadata":{},"score":"42.837456"}
{"text":"They are important for a few reasons .First , at present the best performing parsers on the WSJ treebank ( Ratnaparkhi 1997 ; Charniak 1997 , 1999 ; Collins 1997 , 1999 ) are all cases of history - based mo .. \" ...","label":"Background","metadata":{},"score":"43.240784"}
{"text":"This is a somewhat surprising story , and one that has plenty of counterexamples in the literature .These guys got it to work .The key was to use the Charniak + Johnson reranking parser as parser A , rather than just the Charniak parser .","label":"Background","metadata":{},"score":"43.325386"}
{"text":"This paper presents a new approach to phrase - level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions .With this approach , the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions , achieving results that are significantly better than baseline . ...","label":"Background","metadata":{},"score":"43.989"}
{"text":"A fundamental problem in statistical parsing is the choice of criteria and algorithms used to estimate the parameters in a model .The predominant approach in computational linguistics has been to use a parametric model with some variant of maximum - likelihood estimation .","label":"Background","metadata":{},"score":"44.44825"}
{"text":"A fundamental problem in statistical parsing is the choice of criteria and algorithms used to estimate the parameters in a model .The predominant approach in computational linguistics has been to use a parametric model with some variant of maximum - likelihood estimation .","label":"Background","metadata":{},"score":"44.44825"}
{"text":"I 'm short - listing some of them here ( including workshop papers ) , though I encourage others to list those that they liked , since one person can have at most a 33 % recall .Effective Self - Training for Parsing ( McClosky , Charniak + Johnson ) .","label":"Background","metadata":{},"score":"45.87046"}
{"text":"Charniak , Eugene , and Mark Johnson .\" Coarse - to - fine n - best parsing and MaxEnt discriminative reranking .\" Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics .Association for Computational Linguistics , 2005 .","label":"Background","metadata":{},"score":"46.04948"}
{"text":"We present a detailed case study of this learni ... \" .this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance .","label":"Background","metadata":{},"score":"46.4833"}
{"text":"We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm , and we give experimental results on the ATIS corpus of parse trees . ... lems .The method is derived by the transformation from ranking problems to a margin - based classification problem in [ 8].","label":"Background","metadata":{},"score":"46.7078"}
{"text":"The conversion and grammar extraction process imports linguistic generalisations that are missing the in original treebank .This supports the extraction of a linguistically sound grammar with maximal generalisation , as well as grammar induction techniques to capture unseen data in stochastic parsing .","label":"Background","metadata":{},"score":"46.90565"}
{"text":"The conversion and grammar extraction process imports linguistic generalisations that are missing the in original treebank .This supports the extraction of a linguistically sound grammar with maximal generalisation , as well as grammar induction techniques to capture unseen data in stochastic parsing .","label":"Background","metadata":{},"score":"46.90565"}
{"text":"\" ...We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .","label":"Background","metadata":{},"score":"47.185654"}
{"text":"We present several improvements to unlexicalized parsing with hierarchically state - split PCFGs .First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar without a treebank .","label":"Background","metadata":{},"score":"47.22793"}
{"text":"This represents a 13 % decrease in error rate over the best single - parser results on this corpus [ 9].The major technical innova- tion is the use of a \" maximum - entropy - inspired \" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events .","label":"Background","metadata":{},"score":"47.94638"}
{"text":"Computational Challenges in Parsing by Classification ( Turian + Melamed , CHPJISLP Workshop ) .This is a history - based parsing - as - classification model , where one essentially parses in a bottom - up , sequential fashion .The obvious explanation is that English is largely right - branching , and by searching right - to - left , most decisions remain strongly local .","label":"Background","metadata":{},"score":"47.952682"}
{"text":"In these results , the generative model performs significantly better than the others , and does about equally well at assigning part - of - speech tags . \" ...Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .","label":"Background","metadata":{},"score":"48.87332"}
{"text":"This method generates 50-best lists that are of substantially higher quality than previously obtainable . ...m search , keeping some large number of possibilities to extend by adding the next word , and then re - pruning .","label":"Background","metadata":{},"score":"49.655235"}
{"text":"( van Halteren , 1996 ) ) .RankBoost can also use a variety of local and long distance features more easily than n - gram - based approaches ( cf .( Chen , Bangalore and Vijay - Shanker , 1999 ) ) because it makes sparse data less of an issue . \" ...","label":"Background","metadata":{},"score":"49.759872"}
{"text":"( van Halteren , 1996 ) ) .RankBoost can also use a variety of local and long distance features more easily than n - gram - based approaches ( cf .( Chen , Bangalore and Vijay - Shanker , 1999 ) ) because it makes sparse data less of an issue . \" ...","label":"Background","metadata":{},"score":"49.759872"}
{"text":"We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .In addition , we introduce two novel smoothing techniques , one a variation of Jelinek - Mercer smoothing and one a very simple linear interpolation technique , both of which outperform existing methods . ... her applications as well , e.g. prepositional phrase attachment ( Collins & Brooks , 1995 ) , part - of - speech tagging ( Church , 1988 ) , and stochastic pars - S. Whenever data sparsity is an issue , smoothing can help performance , and data sparsity is almost always an issue in statistical modeling .","label":"Background","metadata":{},"score":"49.902714"}
{"text":"They analyze the sorts of transformations required to explain real parallel data ( does a parent / child end up as a parent / child after translation , or is sisterhood maintained ) .They propose a translation model that can account for much more varied transformations that standard SCFGs .","label":"Background","metadata":{},"score":"49.99475"}
{"text":", the label - bias problem ) .Of course , such observations are easy in retrospect : I do n't know how to identify them without seeing the goal .( Incremental improvements also serve a second role : they are advertisements for the original technique , for those who missed it the first time around . )","label":"Background","metadata":{},"score":"50.059536"}
{"text":"Our formulation uses a factorization analogous to the standard dynamic programs for parsing .In particular , it allows one to efficiently learn a model which discriminates ... \" .We present a novel discriminative approach to parsing inspired by the large - margin criterion underlying support vector machines .","label":"Background","metadata":{},"score":"50.226128"}
{"text":"In parsing we would have training examples fs i ; t i g where e .. \" ...This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm .We show how the algorithms can be efficiently applied to exponential sized representations of parse trees , such as the \" all subtrees \" ( DOP ) representation described by ( Bod 9 ... \" .","label":"Background","metadata":{},"score":"50.56159"}
{"text":"This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension .The paper proposes a simple informationtheoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel , incremental , probabi ... \" .","label":"Background","metadata":{},"score":"51.125282"}
{"text":"The strength of our approach is that it allows a tree to be represented as an arbitrary set of features , without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account .","label":"Background","metadata":{},"score":"51.472755"}
{"text":"Second , we compare various inference procedures for state - split PCFGs from the standpoint of risk minimization , paying particular attention to their practical tradeoffs .Finally , we present multilingual experiments which show that parsing with hierarchical state - splitting is fast and accurate in multiple languages and domains , even without any language - specific tuning . .","label":"Background","metadata":{},"score":"51.606293"}
{"text":"In this paper we study various reasons and mechanisms for combining Supertagging with Lexicalized Tree - Adjoining Grammar ( LTAG ) parsing .Because of the highly lexicalized nature of the LTAG formalism , we experimentally show that notions other than sentence length play a factor in observed parse times .","label":"Background","metadata":{},"score":"51.729073"}
{"text":"In this paper we study various reasons and mechanisms for combining Supertagging with Lexicalized Tree - Adjoining Grammar ( LTAG ) parsing .Because of the highly lexicalized nature of the LTAG formalism , we experimentally show that notions other than sentence length play a factor in observed parse times .","label":"Background","metadata":{},"score":"51.729073"}
{"text":"The lexicalized statistical parsers we described earlier in class ( for example Eugene Charniak 's parser , or the parser I developed ) can be computationally quite intensive when searching for the most likely parse for the sentence .It 's quite frequent for the search for the most likely parse to involve a few 10s of thousands of partial hypotheses ( entries in the dynamic programming structures ) .","label":"Background","metadata":{},"score":"51.843304"}
{"text":"In particular , we demonstrated in Petrov et al .( 2006 ) that a hierarchically split PCFG could exceed the accuracy of lexic ... . by Ben Taskar , Dan Klein , Michael Collins , Daphne Koller , Christopher Manning - In Proceedings of EMNLP , 2004 . \" ...","label":"Background","metadata":{},"score":"51.937584"}
{"text":"This paper shows how finite approximations of long distance dependency ( LDD ) resolution can be obtained automatically for wide - coverage , robust , probabilistic Lexical - Functional Grammar ( LFG ) resources acquired from treebanks .We extract LFG subcategorisation frames and paths linking LDD reentrancies from f - structures generated automatically for the Penn - II treebank trees and use them in an LDD resolution algorithm to parse new text .","label":"Background","metadata":{},"score":"52.337944"}
{"text":"We present a maximum - likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently , using as examples several problems in natural language processing . \" ... this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .","label":"Background","metadata":{},"score":"52.81916"}
{"text":"( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .","label":"Background","metadata":{},"score":"52.944206"}
{"text":"This paper discusses the statistical theory underlying various parameter - estimation methods , and gives algorithms which depend on alternatives to ( smoothed ) maximumlikelihood estimation .We first give an overview of results from statistical learning theory .We then show how important concepts from the classification literature -- specifically , generalization results based on margins on training data -- can be derived for parsing models .","label":"Background","metadata":{},"score":"52.958965"}
{"text":"I usually think of incremental improvements as the result of taking an existing result X and twiddling X a little to yield Y. This can happen in system development ( adding syntax to my MT system helped ) , theory ( Y is a straightforward corollary of X ) , etc .","label":"Background","metadata":{},"score":"53.324837"}
{"text":"Each type of learning method brings its own ' inductive bias ' to the task and will produce a classifier with slightly different characteristics , so that different methods will tend to produce different errors . ... eras , 1999 ) for combining ensembles of neural networks .","label":"Background","metadata":{},"score":"53.42811"}
{"text":"Each type of learning method brings its own ' inductive bias ' to the task and will produce a classifier with slightly different characteristics , so that different methods will tend to produce different errors . ... eras , 1999 ) for combining ensembles of neural networks .","label":"Background","metadata":{},"score":"53.42811"}
{"text":"The story seems to be that the most impactful papers , at least as measured by this criteria , are either MT papers , parsing papers or machine learning papers .This makes some historical sense : MT is sort of the quintessential NLP application , while parsing is essentially the language - for - language 's sake task .","label":"Background","metadata":{},"score":"53.597515"}
{"text":"We show how the algorithms can be efficiently applied to exponential sized representations of parse trees , such as the \" all subtrees \" ( DOP ) representation described by ( Bod 98 ) , or a representation tracking all sub - fragments of a tagged sentence .","label":"Background","metadata":{},"score":"53.731888"}
{"text":"The issues of consistency of argument structure across both polysemous and synonymous verbs are also discussed and we present our actual guidelines for these types of phenomena , along with numerous examples of tagged sentences and verb frames .We conclude with a summary of the current status of annotation process .","label":"Background","metadata":{},"score":"53.94959"}
{"text":"\" A maximum - entropy - inspired parser .\" Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference .Association for Computational Linguistics , 2000 .Overview .BLLIP Parser is a statistical natural language parser including a generative constituent parser ( first - stage ) and discriminative maximum entropy reranker ( second - stage ) .","label":"Background","metadata":{},"score":"54.338955"}
{"text":"In this paper , we present a system that automatically extracts lexicalized grammars from annotated corpora .The data produced by this system have been used in several tasks , such as training NLP tools ( such as Supertaggers ) and estimating the coverage of harid - crafted grammars .","label":"Background","metadata":{},"score":"54.9013"}
{"text":"In this paper , we present a system that automatically extracts lexicalized grammars from annotated corpora .The data produced by this system have been used in several tasks , such as training NLP tools ( such as Supertaggers ) and estimating the coverage of harid - crafted grammars .","label":"Background","metadata":{},"score":"54.9013"}
{"text":"Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation .","label":"Background","metadata":{},"score":"54.972015"}
{"text":"We propose ( a ) a lexical affinity model where words struggle to modify each other , ( b ) a sense tagging model where words fluctuate randomly in their selectional prefe ... \" .After presenting a novel O(nÂ³ ) parsing algorithm for dependency grammar , we develop three contrasting ways to stochasticize it .","label":"Background","metadata":{},"score":"55.038597"}
{"text":"In this project the goal would be to implement such a tagger .If implemented in sufficiently general form , the method could be applied to a number of tasks .The project could involve replicating results for the tagger on previously studied problems such as POS tagging of English ; or applying the tagger to new problems such as tagging in a language other than English .","label":"Background","metadata":{},"score":"55.55279"}
{"text":"Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . .","label":"Background","metadata":{},"score":"55.99269"}
{"text":"Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . .","label":"Background","metadata":{},"score":"55.99269"}
{"text":"Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . .","label":"Background","metadata":{},"score":"55.99269"}
{"text":"Tools . \" ...The accuracy of statistical parsing models can be improved with the use of lexical information .Statistical parsing using Lexicalized tree adjoining grammar ( LTAG ) , a kind of lexicalized grammar , has remained relatively unexplored .","label":"Background","metadata":{},"score":"56.084667"}
{"text":"( And , for completeness , since I asked this question at the talk , I would have liked to have seen at least one baseline model that made use of the prototypes , even in a naive way . )Quas i - Synchronous Grammars : Alignment by Soft Projection of Syntactic Dependencies ( D Smith + Eisner , SMT Workshop ) .","label":"Background","metadata":{},"score":"57.003414"}
{"text":"See second - stage / README for an overview . second - stage / README - retrain .rst details how to retrain the reranker .","label":"Background","metadata":{},"score":"57.132797"}
{"text":"Yet , mathematically , CRFs are only incrementally different from MEMMs .Experimentally , they only perform slightly better .Algorithmically , the optimization is only slightly more clever than inference in HMMs .Theoretically , they can solve only a slightly broader family of problems .","label":"Background","metadata":{},"score":"57.658463"}
{"text":"This proposal subsumes and clarifies findings that high - constraint contexts can facilitate lexical processing , and connects these findings to well - known models of parallel constraint - based comprehension .In addition , the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension , including the reversal of locality - based difficulty patterns in syntactically constrained contexts , and conditions under which increased ambiguity facilitates processing .","label":"Background","metadata":{},"score":"57.737297"}
{"text":"In this paper , we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured .We also give an overview of the parsing approaches that participants took and the results that they achieved .","label":"Background","metadata":{},"score":"57.759117"}
{"text":"The accuracy of statistical parsing models can be improved with the use of lexical information .Statistical parsing using Lexicalized tree adjoining grammar ( LTAG ) , a kind of lexicalized grammar , has remained relatively unexplored .We believe that is largely in part due to the absence of large corpora accurately bracketed in terms of a perspicuous yet broad coverage LTAG .","label":"Background","metadata":{},"score":"58.015953"}
{"text":"The accuracy of statistical parsing models can be improved with the use of lexical information .Statistical parsing using Lexicalized tree adjoining grammar ( LTAG ) , a kind of lexicalized grammar , has remained relatively unexplored .We believe that is largely in part due to the absence of large corpora accurately bracketed in terms of a perspicuous yet broad coverage LTAG .","label":"Background","metadata":{},"score":"58.015953"}
{"text":"This system outperforms previou ... \" .We describe a parsing system based upon a language model for English that is , in turn , based upon assigning probabilities to possible parses for a sentence .This model is used in a parsing system by finding the parse for the sentence with the highest probability .","label":"Background","metadata":{},"score":"58.059135"}
{"text":"That is , whenever a constituent with the same history is generated a second time , it is discarded if its probability is lower than the original version .I .. \" ...This article considers approaches which rerank the output of an existing probabilistic parser .","label":"Background","metadata":{},"score":"58.220406"}
{"text":"This document describes basic usage of the command - line interface and describes how to build and run the reranking parser .There are now Python and Java interfaces as well .The Python interface described in README - python .rst .","label":"Background","metadata":{},"score":"58.540947"}
{"text":"If it is indeed true that , for the most part , we can computationally move to more complex models , forgoing tractable search , then it is not implausible to imagine that perhaps humans do the same thing .My knowledge in this area is sparse , but my general understanding is that various models of human language processing are disfavored because they would be too computationally difficult .","label":"Background","metadata":{},"score":"58.549103"}
{"text":"This seems to be more true for NLP than for many other areas I know about , though perhaps this is because there exist NLP problems for which it is even possible to get a \" sufficient \" amount of data .","label":"Background","metadata":{},"score":"58.9869"}
{"text":"This idea has a nice analogue in NLP : should we ( A ) choose a simple model for which we can do exact inference or ( B ) choose a complex model that is closer to the truth for which exact inference is not tractable .","label":"Background","metadata":{},"score":"59.16056"}
{"text":"If so , I may be right .One can then do a more exhaustive error analysis .Other people have suggested other reasons which I think the Brown folk will also look into .I 'm very curious .Prototype - driven Learning for Sequence Models ( Haghighi + Klein ) .","label":"Background","metadata":{},"score":"59.23647"}
{"text":"I 'm sure other languages fall fairly close by and I recall seeing a study comparing Shannon - style tests in multiple languages ( anyone know a pointer ? ) , but , if pressed , this is my guess as to why n - grams work so well .","label":"Background","metadata":{},"score":"59.247917"}
{"text":"My guess is that n - gram models work significantly worse in languages with more free word order and ( though this usually comes as a package deal ) stronger morpology .As a counter - argument , though , some of my friends who have contact with people who do commercial speech recognition in languages other than English do actually use vanilla n - gram models in those other languages .","label":"Background","metadata":{},"score":"59.4795"}
{"text":"On a broader perspective our approach contributes to a better understanding on where corpuslinguistics and theoretical linguistics can meet and enrich each other .The need of large - scale corpora for higherlevel syntactic frameworks is addressed in Sadler et al ( 2000 ) , Frank ( 2000 ) , Frank et al ( 2001 ) , who develop methods to enrich treebanks with higher - level ... . by","label":"Background","metadata":{},"score":"59.551315"}
{"text":"On a broader perspective our approach contributes to a better understanding on where corpuslinguistics and theoretical linguistics can meet and enrich each other .The need of large - scale corpora for higherlevel syntactic frameworks is addressed in Sadler et al ( 2000 ) , Frank ( 2000 ) , Frank et al ( 2001 ) , who develop methods to enrich treebanks with higher - level ... . by","label":"Background","metadata":{},"score":"59.551315"}
{"text":"The Penn Treebank has recently implemented a new syntactic annotation scheme , designed to highlight aspects of predicate - argument structure .This paper discusses the implementation of crucial aspects of this new annotation scheme .It incorporates a more consistent treatment of a wide range of gramma ... \" .","label":"Background","metadata":{},"score":"59.589554"}
{"text":"RankBoost ( Freund et al ., 1998 ) is the boosting algorithm that we use in order to learn to rerank outputs .It also has been used with good effect in reranking outputs ... \" .this paper , we investigate an approach to such a choice based on reranking a set of candidate supertags and their confidence scores .","label":"Background","metadata":{},"score":"59.68098"}
{"text":"RankBoost ( Freund et al ., 1998 ) is the boosting algorithm that we use in order to learn to rerank outputs .It also has been used with good effect in reranking outputs ... \" .this paper , we investigate an approach to such a choice based on reranking a set of candidate supertags and their confidence scores .","label":"Background","metadata":{},"score":"59.68098"}
{"text":"The story that it 's hard to beat an n - gram language model is fairly ubiquitous .In fact , my most useful features for classification are based on the n - best outputs of disambig using the large LM .","label":"Background","metadata":{},"score":"59.734364"}
{"text":"We ... \" .this paper , we combine different systems employing known representations .The observation that suggests this approach is that systems that are designed differently , either because they use a different formalism or because they contain different knowledge , will typically produce different errors .","label":"Background","metadata":{},"score":"59.742165"}
{"text":"We ... \" .this paper , we combine different systems employing known representations .The observation that suggests this approach is that systems that are designed differently , either because they use a different formalism or because they contain different knowledge , will typically produce different errors .","label":"Background","metadata":{},"score":"59.742165"}
{"text":"We show how a Supertagger can be used to drastically reduce the syntactic lexical ambiguity for a given input and can be used in combination with an LTAG parser to radically improve parsing efficiency .We then turn our attention to from parsing efficiency to parsing accuracy and provide a method by which we can effectively combine the output of a Supertagger and a statistical LTAG parser using a co - training algorithm for bootstrapping new labeled data .","label":"Background","metadata":{},"score":"60.185345"}
{"text":"We show how a Supertagger can be used to drastically reduce the syntactic lexical ambiguity for a given input and can be used in combination with an LTAG parser to radically improve parsing efficiency .We then turn our attention to from parsing efficiency to parsing accuracy and provide a method by which we can effectively combine the output of a Supertagger and a statistical LTAG parser using a co - training algorithm for bootstrapping new labeled data .","label":"Background","metadata":{},"score":"60.185345"}
{"text":".. sister adjunction can be used to create parse trees for all input strings , with only a slight penalty in accuracy .The results are graphed in Figure 14 .They use a different set of Supertags and so we used their result simply to get an approxima ... . \" ...","label":"Background","metadata":{},"score":"60.30946"}
{"text":".. sister adjunction can be used to create parse trees for all input strings , with only a slight penalty in accuracy .The results are graphed in Figure 14 .They use a different set of Supertags and so we used their result simply to get an approxima ... . \" ...","label":"Background","metadata":{},"score":"60.30946"}
{"text":".. rning deserves further study .There are many different ways one could try to construct a language learner .In [ 65 ] , a selforganizing language learner is proposed to be used for language modelling .In this work we take a different approach , namely starting with a s ..","label":"Background","metadata":{},"score":"60.36328"}
{"text":"The idea in this paper is that we do n't want to have to label entire sequences to do normal supervised learning .Instead , what we do is , for each possible label , we list a small number of prototypical words from that label .","label":"Background","metadata":{},"score":"60.39611"}
{"text":"Although lexical amb ...Tools . \" ...The accuracy of statistical parsing models can be improved with the use of lexical information .Statistical parsing using Lexicalized tree adjoining grammar ( LTAG ) , a kind of lexicalized grammar , has remained relatively unexplored .","label":"Background","metadata":{},"score":"60.619026"}
{"text":"Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .In co ... \" .We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .","label":"Background","metadata":{},"score":"60.69211"}
{"text":"This would certainly cut down on the number of papers published , but I also feel that many purely incremental papers do end up being useful , if only as steps in a path .For instance , MEMMs themselves are much more incremental upon HMMs , maximum entropy models and some older work by both Brill and Roth .","label":"Background","metadata":{},"score":"60.709328"}
{"text":"Eraall : brill@cs.jhu.edu .Word sense disambiguation , a problem which once seemed out of reach for systems without a great deal of hand cr ... . \" ...We describe a parsing system based upon a language model for English that is , in turn , based upon assigning probabilities to possible parses for a sentence .","label":"Background","metadata":{},"score":"60.762733"}
{"text":"1992 ) .They are important for a few reasons .Many systems applied to part - ofspeech tagging , speech recognition and other language or speech tasks also fall into this class of model .Second , a partic ... . \" ...","label":"Background","metadata":{},"score":"60.77466"}
{"text":"The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence .","label":"Background","metadata":{},"score":"60.860176"}
{"text":"The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence .","label":"Background","metadata":{},"score":"60.860176"}
{"text":"Unfortunately , words are assigned on average a much higher number of Supertags than traditional POS .In this paper , we develop the notion of Hypertag , first introduced in Kinyon ( 00a ) ... \" .Srinivas ( 97 ) enriches traditional morpho - syntactic POS tagging with syntactic information by introducing Supertags .","label":"Background","metadata":{},"score":"60.99846"}
{"text":"Unfortunately , words are assigned on average a much higher number of Supertags than traditional POS .In this paper , we develop the notion of Hypertag , first introduced in Kinyon ( 00a ) ... \" .Srinivas ( 97 ) enriches traditional morpho - syntactic POS tagging with syntactic information by introducing Supertags .","label":"Background","metadata":{},"score":"60.99846"}
{"text":"Alignments in Machine Translation .In Machine translation Part IV , we went over phrasal models for machine translation .These methods used heuristic methods which started from the IBM model alignments , and then searched for alignments between phrases in two languages ( e.g. , see the Koehn , Och and Marcu paper ) .","label":"Background","metadata":{},"score":"61.495197"}
{"text":"Tree - based modeling still lacks many of the standard tools taken for granted in ( finite - state ) string - based modeling .The theory of tree transducer automata provides a possible framework to draw on , as it has been worked out in an extensive literature .","label":"Background","metadata":{},"score":"61.540897"}
{"text":"In order to capture inherent relations occurring in corpus texts that can be critical in real - world applications , many NP relations are included in the set of grammatical relations used .We provide a comparison of our system with Minipar and the Link parser .","label":"Background","metadata":{},"score":"61.858215"}
{"text":"Introduction As a first step prior to parsing , traditional Part of Speech ( POS ) tagging assigns limited morpho - syntactic information to lexical items .These labels can be more or less fine - grained depending on the tagset , but syntactic information is often absent or limited .","label":"Background","metadata":{},"score":"62.27879"}
{"text":"Introduction As a first step prior to parsing , traditional Part of Speech ( POS ) tagging assigns limited morpho - syntactic information to lexical items .These labels can be more or less fine - grained depending on the tagset , but syntactic information is often absent or limited .","label":"Background","metadata":{},"score":"62.27879"}
{"text":"This model is an extension of PCFG in which non - terminal symbols are augmented with latent variables .Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG - LA model using an EM - algorithm .","label":"Background","metadata":{},"score":"62.381977"}
{"text":"The CRF is a success largely because a single , relatively simple formalism made ( incremental ) improvements in at least four areas .I think you can make similar arguments about other \" must reads \" in NLP , such as Collins ' original parsing paper .","label":"Background","metadata":{},"score":"62.40995"}
{"text":"Unfortunately , the paper does n't answer this question .Though talking to Eugene , they are interested in ideas why .One obvious idea is that the epsilon difference in performance for reranking and not reranking happens to occur at a phase transition .","label":"Background","metadata":{},"score":"62.530785"}
{"text":"These examples are intended to both give you concrete suggestions for projects , and also to give illustrative examples of the kind of project that would be suitable .You could pick a project from the list below ( in which case , arrange an appointment with me so we can go over the details ) , or you could choose your own project .","label":"Background","metadata":{},"score":"62.650955"}
{"text":".. by Aoife Cahill , Michael Burke , Josef Van Genabith , Andy Way - In Proceedings of the 42nd Meeting of the ACL , 2004 . \" ...This paper shows how finite approximations of long distance dependency ( LDD ) resolution can be obtained automatically for wide - coverage , robust , probabilistic Lexical - Functional Grammar ( LFG ) resources acquired from treebanks .","label":"Background","metadata":{},"score":"62.87262"}
{"text":"In contrast with previous work , we are able to split various terminals to different degrees , as appropriate to the actual complexity in the data .Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .","label":"Background","metadata":{},"score":"63.388737"}
{"text":"Building a Machine Translation System .Resources for building statistical machine translation systems exist in publicly available sources on the web : for example , parallel aligned corpora in various language pairs ; the Giza++ system for training various IBM Models ; and the ISI rewrite decoder .","label":"Background","metadata":{},"score":"63.720238"}
{"text":"For the final project you should hand in a ( group ) written report .You should aim to partition the work in such a way that different people in the project are working on clearly defined \" components \" of the system .","label":"Background","metadata":{},"score":"63.75576"}
{"text":"Acknowledgement Many thanks to Amit Dubey and Yuval Krymolowski , the other two organizers of the shared task , for discussions , converting treebanks , writing software and helping with the papers . \" ...This paper presents a new approach to phrase - level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions .","label":"Background","metadata":{},"score":"63.79554"}
{"text":"The Makefile will attempt to automatically download and build Sparseval for you if you run make sparseval .We no longer distribute evalb with the parser since it sometimes skips sentences unnecessarily .Sparseval does not have these issues .More questions ?","label":"Background","metadata":{},"score":"64.034195"}
{"text":"We report on experiments over a 9,000,000 Web page corpus that compare TEXTRUNNER with KNOWITALL , a state - of - the - art Web IE system .TEXTRUNNER achieves an error reduction of 33 % on a comparable set of extractions .","label":"Background","metadata":{},"score":"64.059235"}
{"text":"There are 44 bins that ACL allowed you to select for your topic this year and while many of them largely overlap , there are many that do not .Why , for instance , does research in discourse , inference , phonology , question answering , semantics , and word sense disambiguation have comparatively little weight in the field as a whole ?","label":"Background","metadata":{},"score":"64.24341"}
{"text":"My guess is that this result will continue to hold for training ( though perhaps not predicting ) with more an more complex models .This is unfortunate , and there may be ways of coming up with learning algorithms that automatically initialize themselves by some mechanism for simplifying their own structure ( seems like a fun open question , somewhat related to recent work by Smith ) .","label":"Background","metadata":{},"score":"64.58095"}
{"text":"Language Modeling In Lecture 2 we went over language modeling techniques , the goal being to come up with a statistical model with low \" perplexity \" on naturally occurring text .Much of this work has been done on English datasets ; other languages might present very different challenges .","label":"Background","metadata":{},"score":"64.70777"}
{"text":"I think this idea is very interesting , and I am in general very interested in ways of using \" not quite right \" data to learn to solve problems .My only concern with this approach is that it is quite strict to require that all occurances of prototypical words get their associated tag .","label":"Background","metadata":{},"score":"64.879425"}
{"text":"The script parse - eval . sh takes a list of treebank files as arguments and extracts the terminal strings from them , runs the two - stage parser on those terminal strings and then evaluates the parsing accuracy with Sparseval .","label":"Background","metadata":{},"score":"64.89119"}
{"text":"..We just list two of them which seem to be most relevant : C4 uses a reduced tagset while C3 uses the PTB tagset . 9 Instead , we re - ran 8 All use Section 2 - 21 of the PTB for training , and Section 22 or 23 for testing .","label":"Background","metadata":{},"score":"64.967255"}
{"text":"..We just list two of them which seem to be most relevant : C4 uses a reduced tagset while C3 uses the PTB tagset . 9 Instead , we re - ran 8 All use Section 2 - 21 of the PTB for training , and Section 22 or 23 for testing .","label":"Background","metadata":{},"score":"64.967255"}
{"text":"In this paper , we develop the notion of Hypertag , first introduced in Kinyon ( 00a ) and in Kinyon ( 00b ) , which allows to factor the information contained in several Supertags into a single structure and to encode functional information in a systematic manner .","label":"Background","metadata":{},"score":"65.46129"}
{"text":"In this paper , we develop the notion of Hypertag , first introduced in Kinyon ( 00a ) and in Kinyon ( 00b ) , which allows to factor the information contained in several Supertags into a single structure and to encode functional information in a systematic manner .","label":"Background","metadata":{},"score":"65.46129"}
{"text":"This paper defines a generative probabilistic model of parse trees , which we call PCFG - LA .This model is an extension of PCFG in which non - terminal symbols are augmented with latent variables .Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG - LA model using an E ... \" .","label":"Background","metadata":{},"score":"65.7978"}
{"text":"In this paper we propose to use supertags to expose syntactic dependencies which are unavailable with POS tags .We first propose a novel method of app ... \" .Supertagging is the tagging process of assigning the correct elementary tree of LTAG , or the correct supertag , to each word of an input sentence .","label":"Background","metadata":{},"score":"66.00597"}
{"text":"In this paper we propose to use supertags to expose syntactic dependencies which are unavailable with POS tags .We first propose a novel method of app ... \" .Supertagging is the tagging process of assigning the correct elementary tree of LTAG , or the correct supertag , to each word of an input sentence .","label":"Background","metadata":{},"score":"66.00597"}
{"text":"Many probabilistic models for natural language are now written in terms of hierarchical tree structure .Tree - based modeling still lacks many of the standard tools taken for granted in ( finite - state ) string - based modeling .The theory of tree transducer automata provides a possible framework to ... \" .","label":"Background","metadata":{},"score":"66.522675"}
{"text":"Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .The tenth CoNLL ( CoNLL - X ) saw a shared task on Multilingual Dependency Parsing .","label":"Background","metadata":{},"score":"67.38123"}
{"text":"But in a broader sense , everything is an incremental improvement .It is vanishingly unlikely that a paper will come along that is not incremental in some sense .I think what often ends up making a paper more or less successful is how many incremental improvements it contains .","label":"Background","metadata":{},"score":"67.664566"}
{"text":"This assignment will make up 70 % of the final grade ( the remaining 30 % depends on the first assignment ) .The idea behind the project is to take some idea / technique / problem from the class , and develop it further .","label":"Background","metadata":{},"score":"68.363914"}
{"text":"It seems that all signs point to ( B ) .In almost every interesting case I know of , it helps ( or at the very least does n't hurt ) to move to more complex models that are more expressive , even if this renders learning or search intractable .","label":"Background","metadata":{},"score":"68.609535"}
{"text":"This manual labor scales linearly with the number of target relations .This paper introduces Open IE ( OIE ) , a new extraction paradigm where the system makes a single data - driven pass over its corpus and extracts a large set of relational tuples without requiring any human input .","label":"Background","metadata":{},"score":"68.78276"}
{"text":"Shifting to a new domain requires the user to name the target relations and to ma ... \" .Traditionally , Information Extraction ( IE ) has focused on satisfying precise , narrow , pre - specified requests from small homogeneous corpora ( e.g. , extract the location and time of seminars from a set of announcements ) .","label":"Background","metadata":{},"score":"69.10228"}
{"text":"The aim of this project would be to investigate language modeling for one of these languages which has different properties from English .We should be able to find relatively large datasets for this problem in a number of languages .Implementing a perceptron tagger .","label":"Background","metadata":{},"score":"69.64169"}
{"text":"Grammars are core elements of many NLP applications .In this paper , we present a system that automatically extracts lexicalized grammars from annotated corpora .The data produced by this system have been used in several tasks , such as training NLP tools ( such as Supertaggers ) and estimating the cove ... \" .","label":"Background","metadata":{},"score":"69.8343"}
{"text":"Grammars are core elements of many NLP applications .In this paper , we present a system that automatically extracts lexicalized grammars from annotated corpora .The data produced by this system have been used in several tasks , such as training NLP tools ( such as Supertaggers ) and estimating the cove ... \" .","label":"Background","metadata":{},"score":"69.8343"}
{"text":"We report statistics on TEXTRUNNER 's 11,000,000 highest probability tuples , and show that they contain over 1,000,000 concrete facts and over 6,500,000 more abstract assertions . \" ...We present several improvements to unlexicalized parsing with hierarchically state - split PCFGs .","label":"Background","metadata":{},"score":"69.9935"}
{"text":"The tenth CoNLL ( CoNLL - X ) saw a shared task on Multilingual Dependency Parsing .Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .","label":"Background","metadata":{},"score":"70.27985"}
{"text":"We present a system for identifying the semantic relationships , or semantic roles , filled by constituents of a sentence within a semantic frame .Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand - annotated training data .","label":"Background","metadata":{},"score":"70.63874"}
{"text":"And beyond that , there seem to be lots of interesting problems that no one is working on ( or at least that no one publishes at ACL ) .I do not doubt that these are interesting problems , but it seems that there is a severe skew that may not be healthy to the community .","label":"Background","metadata":{},"score":"70.65918"}
{"text":"If you 're planning a group project , please let me know the people involved , and how you plan to partition the work .The final report : The final report for the project should be around 10 pages in length ( 12pt , single spaced ) , excluding figures .","label":"Background","metadata":{},"score":"71.199844"}
{"text":"Our entry into the NIST MT eval this year has a recapitalization component , currently being done by a large language model ( 500 mb , gzipped ) together with SRILM 's \" disambig \" tool .I was recently conscripted to try to improve this .","label":"Background","metadata":{},"score":"71.53389"}
{"text":"We extract different LTAGs from the Penn Treebank .We show that certain strategies yield an improved extracted LTAG in terms of compactness , broad coverage , and supertagging accuracy .Furthermore , we perform a preliminary investigation in smoothing these grammars by means of an external linguistic resource , namely , the tree families of an XTAG grammar , a hand built grammar of English . by Hans Van Halteren , Jakub Zavrel , Walter Daelemans - Computational Linguistics , 2000 . \" ... this paper , we combine different systems employing known representations .","label":"Background","metadata":{},"score":"71.85264"}
{"text":"We extract different LTAGs from the Penn Treebank .We show that certain strategies yield an improved extracted LTAG in terms of compactness , broad coverage , and supertagging accuracy .Furthermore , we perform a preliminary investigation in smoothing these grammars by means of an external linguistic resource , namely , the tree families of an XTAG grammar , a hand built grammar of English . by Hans Van Halteren , Jakub Zavrel , Walter Daelemans - Computational Linguistics , 2000 . \" ... this paper , we combine different systems employing known representations .","label":"Background","metadata":{},"score":"71.85264"}
{"text":"If you encounter a bug , please use the issue tracker .BLLIP Parser is maintained by David McClosky .Parser details .For details on the running and training the parser , see first - stage / README .first - stage / TRAIN / README includes notes about how to retrain the parser and some information about the parser model file formats .","label":"Background","metadata":{},"score":"72.309906"}
{"text":"Despite its simplicity , our best grammar achieves an F1 of 90.2 % on the Penn Treebank , higher than fully lexicalized systems . ...e into smaller steps ) .In this paper , we investigate the learning of a grammar consistent with a treebank at the level of evaluation symbols ... . by Michele Banko , Michael J Cafarella , Stephen Soderland , Matt Broadhead , Oren Etzioni - IN IJCAI , 2007 . \" ...","label":"Background","metadata":{},"score":"72.585846"}
{"text":"Introduction We present a statistical parser that induces its grammar and probabilities from a hand - parsed corpus ( a tree - bank ) .Parsers induced from corpora are of interest both as simply exercises in machine learning and also because they are often the best parsers obtainable by any method .","label":"Background","metadata":{},"score":"73.53885"}
{"text":"INT'L CONF .ON LANGUAGE RESOURCES AND EVALUATION ( LREC , 2006 . \" ...This paper describes a system for extracting typed dependency parses of English sentences from phrase structure parses .In order to capture inherent relations occurring in corpus texts that can be critical in real - world applications , many NP relations are included in the set of grammatical relations ... \" .","label":"Background","metadata":{},"score":"75.6236"}
{"text":"Al- though the approach is applicable to any type of language model , we focus on the case of statistical disambiguators that are trained on annotated corpora .The examples of the task that are present in the corpus and its annotation are fed into a learning algorithm , which induces a model of the desired input - output mapping in the form of a classifier .","label":"Background","metadata":{},"score":"76.44473"}
{"text":"Al- though the approach is applicable to any type of language model , we focus on the case of statistical disambiguators that are trained on annotated corpora .The examples of the task that are present in the corpus and its annotation are fed into a learning algorithm , which induces a model of the desired input - output mapping in the form of a classifier .","label":"Background","metadata":{},"score":"76.44473"}
{"text":"I 've already been doing machine learning stuff , which I hope will turn out to be useful for other people .But the field is so crowded right now , it seems difficult to find a sufficiently large and interesting niche to carve out .","label":"Background","metadata":{},"score":"78.91119"}
{"text":"BLLIP Reranking Parser .Copyright Mark Johnson , Eugene Charniak , 24th November 2005 --- August 2006 .We request acknowledgement in any publications that make use of this software and any code derived from this software .Please report the release date of the software that you are using , as this will enable others to compare their results to yours .","label":"Background","metadata":{},"score":"79.12416"}
{"text":"Iwould like toacknowledge the following people for their contribution to my education : I thank my advisor Mitch Marcus , who gave me the intellectual freedom to pursue what I believed to be the best way to approach natural language processing , and also gave me direction when necessary .","label":"Background","metadata":{},"score":"80.27362"}
{"text":"Iwould like toacknowledge the following people for their contribution to my education : I thank my advisor Mitch Marcus , who gave me the intellectual freedom to pursue what I believed to be the best way to approach natural language processing , and also gave me direction when necessary .","label":"Background","metadata":{},"score":"80.27362"}
{"text":"We first propose a novel method of applying Sparse Network of Winnow ( SNoW ) to sequential models .Then we use . \" ...In this paper we study various reasons and mechanisms for combining Supertagging with Lexicalized Tree - Adjoining Grammar ( LTAG ) parsing .","label":"Background","metadata":{},"score":"80.58859"}
{"text":"We first propose a novel method of applying Sparse Network of Winnow ( SNoW ) to sequential models .Then we use . \" ...In this paper we study various reasons and mechanisms for combining Supertagging with Lexicalized Tree - Adjoining Grammar ( LTAG ) parsing .","label":"Background","metadata":{},"score":"80.58859"}
{"text":"The first method we discuss is based on a feature selection me ... . by Michael Collins , Nigel Duffy - Advances in Neural Information Processing Systems 14 , 2001 . \" ...We describe the application of kernel methods to Natural Language Processing ( NLP ) problems .","label":"Background","metadata":{},"score":"80.75004"}
{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"Background","metadata":{},"score":"81.363815"}
{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"Background","metadata":{},"score":"81.363815"}
{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"Background","metadata":{},"score":"81.363815"}
{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"Background","metadata":{},"score":"81.363815"}
{"text":"We present a method for rule - based structure conversion of existing treebanks , which aims at the extraction of linguistically sound , corpus - based grammars in a specific grammatical framework .We apply this method to the NEGRA treebank to derive an LTAG grammar of German .","label":"Background","metadata":{},"score":"81.883514"}
{"text":"We present a method for rule - based structure conversion of existing treebanks , which aims at the extraction of linguistically sound , corpus - based grammars in a specific grammatical framework .We apply this method to the NEGRA treebank to derive an LTAG grammar of German .","label":"Background","metadata":{},"score":"81.883514"}
{"text":"We present a method for rule - based structure conversion of existing treebanks , which aims at the extraction of linguistically sound , corpus - based grammars in a specific grammatical framework .We apply this method to the NEGRA treebank to derive an LTAG grammar of German .","label":"Background","metadata":{},"score":"81.883514"}
{"text":"We present a method for rule - based structure conversion of existing treebanks , which aims at the extraction of linguistically sound , corpus - based grammars in a specific grammatical framework .We apply this method to the NEGRA treebank to derive an LTAG grammar of German .","label":"Background","metadata":{},"score":"81.883514"}
{"text":".. ations are used to define grammatical roles .The original treebanks , in particular the Penn Treebank , were for English , and provided only phrase structure trees , and hence this is the native ou ... . by Slav Petrov , Leon Barrett , Romain Thibaux , Dan Klein - In ACL ' 06 , 2006 . \" ...","label":"Background","metadata":{},"score":"82.03566"}
{"text":"This paper describes our approach to the development of a Proposition Bank , which involves the addition of semantic information to the Penn English Treebank .Our primary goal is the labeling of syntactic nodes with specific argument labels that preserve the similarity of roles such as the window in John broke the window and the window broke .","label":"Background","metadata":{},"score":"83.39543"}
{"text":"Tools . by Adam L. Berger , Stephen A. Della Pietra , Vincent J. Della Pietra - COMPUTATIONAL LINGUISTICS , 1996 . \" ...The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .","label":"Background","metadata":{},"score":"83.92056"}
{"text":"This paper discusses the implementation of crucial aspects of this new annotation scheme .INTRODUCTION During the first phase of the The Penn Treebank project [ 10 ] , ending in December 1992 , 4.5 million words of text were tagged for part - of - speech , with about two - thirds of this material also annotated with a skeletal syntactic bracketing .","label":"Background","metadata":{},"score":"84.19799"}
{"text":"Having just completed my thesis , I 've been thinking a lot about what directions I should persue , post - graduation .While I 'll probably stick with some of the stuff I 've been working on in the past , this is also an opportunity to diversify .","label":"Background","metadata":{},"score":"85.50318"}
{"text":"I do n't know the secret sauce to beating an n - gram .They appear to work so well because language ( where , by \" language \" I mean \" English \" ) is really not that ambiguous , in a Shannon - experiment sense .","label":"Background","metadata":{},"score":"85.847"}
{"text":"We describe kernels for various natural ... \" .We describe the application of kernel methods to Natural Language Processing ( NLP ) problems .In many NLP tasks the objects being modeled are strings , trees , graphs or other discrete structures which require some mechanism to convert them into feature vectors .","label":"Background","metadata":{},"score":"86.4451"}
{"text":"( optional )For optimal speed , you may want to define $ GCCFLAGS specifically for your machine .This step can be safely skipped as the defaults should be okay .With csh or tcsh , try something like : .","label":"Background","metadata":{},"score":"86.9164"}
{"text":"In a dependency representation , every node in the tree structure is a surface word ( i.e. , there are no abstrac ... . by Paul Kingsbury , Martha Palmer - In Language Resources and Evaluation , 2002 . \" ...This paper describes our approach to the development of a Proposition Bank , which involves the addition of semantic information to the Penn English Treebank .","label":"Background","metadata":{},"score":"88.97081"}
{"text":"I thank all of my thesis committee members : John La erty from Carnegie Mellon University , Aravind Joshi , Lyle Ungar , and Mark Liberman , for their extremely valuable suggestions and comments about my thesis research .I thank Mike Collins , Jason Eisner , and Dan Melamed , with whom I 've had many stimulating and impromptu discussions in the LINC lab .","label":"Background","metadata":{},"score":"89.29605"}
{"text":"I thank all of my thesis committee members : John La erty from Carnegie Mellon University , Aravind Joshi , Lyle Ungar , and Mark Liberman , for their extremely valuable suggestions and comments about my thesis research .I thank Mike Collins , Jason Eisner , and Dan Melamed , with whom I 've had many stimulating and impromptu discussions in the LINC lab .","label":"Background","metadata":{},"score":"89.29605"}
{"text":"Relation to current / previous research .The project can be related to your current research .However , please do not submit any work which you have completed prior to taking the course .Due date : The project is due by Wednesday December 10th .","label":"Background","metadata":{},"score":"90.23453"}
{"text":"OS X uses the clang compiler by default which can not currently compile the parser .Try setting this environment variable before building to change the default C++ compiler : .Running the parser .Sentence 1 Sentence 2 ... .The parser distribution currently includes a basic Penn Treebank Wall Street Journal parsing models which parse.sh will use by default .","label":"Background","metadata":{},"score":"91.04833"}
{"text":"The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .","label":"Background","metadata":{},"score":"94.394684"}
{"text":"So that 's my short - short list .I 'd love to hear what papers other people liked ( so I know what to read ! )That said , I heard many rumors of lots of really cool sounding ACL and EMNLP papers , so I 'm really looking forward to Sydney !","label":"Background","metadata":{},"score":"111.82475"}
