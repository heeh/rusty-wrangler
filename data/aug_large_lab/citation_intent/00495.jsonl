{"text":"Traditional approaches to parsing are highly complex and problem specific .Recently , Sutskever et al .( 2014 ) presented a task - agnostic method for learning to map input sequences to output sequences that achieved strong results on a large scale machine translation problem .","label":"CompareOrContrast","metadata":{},"score":"28.772684"}
{"text":"Current studies on semi - supervised approaches show very mixed results .For example , Structural Correspondence Learning ( Blitzer et al . , 2006 ) was applied successfully to classification tasks , while only modest gains could be obtained for structured output tasks like parsing .","label":"CompareOrContrast","metadata":{},"score":"29.18978"}
{"text":"Our method does not assume any knowledge about the target language ( in particular no tagging dictionary is assumed ) , making it applicable to a wide array of resource - poor languages .We use graph - based label propagation for cross - lingual knowledge transfer and use the projected labels as features in an unsupervised model ( Berg - Kirkpatrick et al .","label":"CompareOrContrast","metadata":{},"score":"29.528969"}
{"text":"Unlike existing preordering models , we train feature - rich discriminative classifiers that directly predict the target - side word order .Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long - distance reorderings using the structure of the parse tree , while utilizing a discriminative model with a rich set of features , including lexical features .","label":"CompareOrContrast","metadata":{},"score":"30.030212"}
{"text":"We take two popular dependency parsers - one graph - based and one transition - based - and compare results for both .Results show that using semisupervised learning in the form of self - training and co - training yields only very modest improvements in parsing accuracy .","label":"CompareOrContrast","metadata":{},"score":"33.35072"}
{"text":"First , we shallow parse the source language sentences .Then , reordering rules are automatically learned from source - side chunks and word alignments .During translatio ... \" .In this paper , we describe a sourceside reordering method based on syntactic chunks for phrase - based statistical machine translation .","label":"CompareOrContrast","metadata":{},"score":"33.41181"}
{"text":"Previous sentence segmentation systems have typically been very local , using low - level prosodic and lexical features to independently decide whether or not to segment at each word boundary position .In this work , we leverage global syntactic information from a syn- tactic parser , which is better able to capture long distance depen- dencies .","label":"CompareOrContrast","metadata":{},"score":"34.383537"}
{"text":"( Alshawi et al . , 2000 ) represents each production in parallel dependency trees as a finite - state tr ... . \" ...This paper describes the application of discriminative reranking techniques to the problem of machine translation .For each sentence in the source language , we obtain from a baseline statistical machine translation system , a ranked nbest list of candidate translations in the target language .","label":"CompareOrContrast","metadata":{},"score":"35.23703"}
{"text":"( Alshawi et al . , 2000 ) represents each production in parallel dependency trees as a finite - state tr ... . \" ...This paper describes the application of discriminative reranking techniques to the problem of machine translation .For each sentence in the source language , we obtain from a baseline statistical machine translation system , a ranked nbest list of candidate translations in the target language .","label":"CompareOrContrast","metadata":{},"score":"35.23703"}
{"text":"However , current tree - based systems suffer from a major drawback : they only use the 1-best pa ... \" .Among syntax - based translation models , the tree - based approach , which takes as input a parse tree of the source sentence , is a promising direction being faster and simpler than its string - based counterpart .","label":"CompareOrContrast","metadata":{},"score":"35.274887"}
{"text":"However , current tree - based systems suffer from a major drawback : they only use the 1-best pa ... \" .Among syntax - based translation models , the tree - based approach , which takes as input a parse tree of the source sentence , is a promising direction being faster and simpler than its string - based counterpart .","label":"CompareOrContrast","metadata":{},"score":"35.274887"}
{"text":"The goal of this workshop is to provide a meeting - point for research that approaches the problem of adaptation from the varied perspectives of machine - learning and a variety of NLP tasks such as parsing , machine - translation , word sense disambiguation , etc .","label":"CompareOrContrast","metadata":{},"score":"36.76068"}
{"text":"Self - training creates semi - supervised learners from existing supervised learners with minimal effort .We first show results on self - training for constituency parsing within a single domain .While self - training has failed here in the past , we present a simple modification which allows it to succeed , producing state - of - the - art results for English constituency parsing .","label":"CompareOrContrast","metadata":{},"score":"36.894424"}
{"text":"The training of most syntactic SMT approaches involves two essential components , word alignment and monolingual parser .In the current state of the art these two components are mutually independent , thus causing problems like lack of rule generalization , and violation of syntactic correspondence in translation rules .","label":"CompareOrContrast","metadata":{},"score":"36.918045"}
{"text":"Then , reordering rules are automatically learned from source - side chunks and word alignments .During translation , the rules are used to generate a reordering lattice for each sentence .Experimental results are reported for a Chinese - to - English task , showing an improvement of 0.5%-1.8 % BLEU score absolute on various test sets and better computational efficiency than reordering during decoding .","label":"CompareOrContrast","metadata":{},"score":"36.939953"}
{"text":"Our research further demonstrates the breadth of the applicability of neural network methods to dependency parsing , as well as the ease with which new features can be added to neural parsing models .We present structured perceptron training for neural network transition - based dependency parsing .","label":"CompareOrContrast","metadata":{},"score":"36.951912"}
{"text":"Our best results show a 26-fold speedup compared to a sequential C implementation .We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data .We first demonstrate that delexicalized parsers can be directly transferred between languages , producing significantly higher accuracies than unsupervised parsers .","label":"CompareOrContrast","metadata":{},"score":"37.04605"}
{"text":"Using statistical machine translation techniques , a semantic parser based on a synchronous context - free grammar augmented with λ - operators is learned given a set of training sentences and their correct logical forms .The resulting parser is shown to be the best - performing system so far in a database query domain . \" ...","label":"CompareOrContrast","metadata":{},"score":"37.661438"}
{"text":"Using statistical machine translation techniques , a semantic parser based on a synchronous context - free grammar augmented with λ - operators is learned given a set of training sentences and their correct logical forms .The resulting parser is shown to be the best - performing system so far in a database query domain . \" ...","label":"CompareOrContrast","metadata":{},"score":"37.661438"}
{"text":"To achieve these results we need to mitigate the lack of domain knowledge in the model by providing it with a large amount of automatically parsed data .We extend and improve upon recent work in structured training for neural network transition - based dependency parsing .","label":"CompareOrContrast","metadata":{},"score":"37.828735"}
{"text":"This paper describes the application of discriminative reranking techniques to the problem of machine translation .For each sentence in the source language , we obtain from a baseline statistical machine translation system , a ranked nbest list of candidate translations in the target language .","label":"CompareOrContrast","metadata":{},"score":"37.921906"}
{"text":"This paper describes the application of discriminative reranking techniques to the problem of machine translation .For each sentence in the source language , we obtain from a baseline statistical machine translation system , a ranked nbest list of candidate translations in the target language .","label":"CompareOrContrast","metadata":{},"score":"37.921906"}
{"text":"This tutorial aims to cover the basic motivation , ideas , models and learning algorithms in deep learning for natural language processing .Recently , these methods have been shown to perform very well on various NLP tasks such as language modeling , POS tagging , named entity recognition , sentiment analysis and paraphrase detection , among others .","label":"CompareOrContrast","metadata":{},"score":"38.187523"}
{"text":"Unlike previous work , our final model does not require any additional resources at run - time .Compared to a state - of - the - art approach , we achieve more than 20 % relative error reduction .Additionally , we annotate a corpus of search queries with part - of - speech tags , providing a resource for future work on syntactic query analysis .","label":"CompareOrContrast","metadata":{},"score":"38.22628"}
{"text":"One is targeted self - training with a simple evaluation function ; the other is based on training data selection from forced alignment of bilingual data .We also propose an auxiliary method for boosting alignment quality , by symmetrizing alignment matrices with respect to parse trees .","label":"CompareOrContrast","metadata":{},"score":"38.557617"}
{"text":"One is targeted self - training with a simple evaluation function ; the other is based on training data selection from forced alignment of bilingual data .We also propose an auxiliary method for boosting alignment quality , by symmetrizing alignment matrices with respect to parse trees .","label":"CompareOrContrast","metadata":{},"score":"38.557617"}
{"text":"However , most ... \" .It is well known that parsing accuracy suffers when a model is applied to out - of - domain data .It is also known that the most beneficial data to parse a given domain is data that matches the domain ( Sekine , 1997 ; Gildea , 2001 ) .","label":"CompareOrContrast","metadata":{},"score":"38.670525"}
{"text":"These modifications allow the algorithm to work on tree structures .Applications include sentiment analysis and paraphrase detection .We also draw connections to recent work in semantic compositionality in vector spaces .The principle goal , again , is to make these methods appear intuitive and interpretable rather than mathematically confusing .","label":"CompareOrContrast","metadata":{},"score":"38.75756"}
{"text":"In particular , we introduce set - valued features to encode the predicted morphological properties and part - of - speech confusion sets of the words being parsed .We also investigate the use of joint parsing and part - of - speech tagging in the neural paradigm .","label":"CompareOrContrast","metadata":{},"score":"38.766163"}
{"text":"proposed an ensemble method ( Reichart and Rappoport , 2007 ) .They regarded parses as being of high quality if 20 different parsers agreed .They used an SVM regression approach on the basis of text - based and parse - based features .","label":"CompareOrContrast","metadata":{},"score":"39.0058"}
{"text":"In addition , our discriminative approach integrally admits features beyond local tree configurations .We present a multi - scale training method along with an efficient CKY - style dynamic program .On a variety of domains and languages , this method produces the best published parsing accuracies with the smallest reported grammars .","label":"CompareOrContrast","metadata":{},"score":"39.255096"}
{"text":"We propose a novel reordering model for phrase - based statistical machine translation ( SMT ) that uses a maximum entropy ( MaxEnt ) model to predicate reorderings of neighbor blocks ( phrase pairs ) .The model provides content - dependent , hierarchical phrasal reordering with generalization based on features automatically learned from a real - world bitext .","label":"CompareOrContrast","metadata":{},"score":"39.272602"}
{"text":"We propose a novel reordering model for phrase - based statistical machine translation ( SMT ) that uses a maximum entropy ( MaxEnt ) model to predicate reorderings of neighbor blocks ( phrase pairs ) .The model provides content - dependent , hierarchical phrasal reordering with generalization based on features automatically learned from a real - world bitext .","label":"CompareOrContrast","metadata":{},"score":"39.272602"}
{"text":"It is well known that parsing accuracy suffers when a model is applied to out - of - domain data .It is also known that the most beneficial data to parse a given domain is data that matches the domain ( Sekine , 1997 ; Gildea , 2001 ) .","label":"CompareOrContrast","metadata":{},"score":"39.733692"}
{"text":"The second question is how one can estimate NLP systems ' performance when gold standard on the test data does not exist .To answer the question , we extend the parsing prediction model in ( Ravi et al . , 2008 ) to provide prediction for word segmentation and POS tagging as well .","label":"CompareOrContrast","metadata":{},"score":"39.902885"}
{"text":"We highlight the use of this resource via two experiments , including one that reports competitive accuracies for unsupervised grammar induction without gold standard part - of - speech tags .We present an online learning algorithm for training structured prediction models with extrinsic loss functions .","label":"CompareOrContrast","metadata":{},"score":"39.915718"}
{"text":"In our method the first , monolingual view consists of supervised predictors learned separately for each language .The second , bilingual view consists of log - linear predictors learned over both languages on bilingual text .Our training procedure estimates the parameters of the bilingual model using the output of the monolingual model , and we show how to combine the two models to account for dependence between views .","label":"CompareOrContrast","metadata":{},"score":"40.175842"}
{"text":"Specifically , an ini- tial hypothesis lattice is constrcuted using local features .Candidate sentences are then assigned syntactic language model scores .These global syntactic scores are combined with local low - level scores in a log - linear model .","label":"CompareOrContrast","metadata":{},"score":"40.413754"}
{"text":"We also show that our techniques can be applied to full - scale parsing applications by demonstrating its effectiveness in learning state - split grammars .Treebank parsing can be seen as the search for an optimally refined grammar consistent with a coarse training treebank .","label":"CompareOrContrast","metadata":{},"score":"40.416615"}
{"text":"Thus it can be seen as combining fundamental ideas from b ... \" .We present a statistical machine translation model that uses hierarchical phrases - phrases that contain subphrases .The model is formally a synchronous context - free grammar but is learned from a parallel text without any syntactic annotations .","label":"CompareOrContrast","metadata":{},"score":"40.55609"}
{"text":"Therefore reordering can be modelled as a problem ofsclassification with ... . \" ...This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms .Using statistical machine translation techniques , a semantic parser based on a synchronous context - free grammar augmented with λ - operators is learned given a set of training ... \" .","label":"CompareOrContrast","metadata":{},"score":"40.676872"}
{"text":"Therefore reordering can be modelled as a problem ofsclassification with ... . \" ...This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms .Using statistical machine translation techniques , a semantic parser based on a synchronous context - free grammar augmented with λ - operators is learned given a set of training ... \" .","label":"CompareOrContrast","metadata":{},"score":"40.676872"}
{"text":"It turns out that these generalized parsers can do most of the work required to train and apply a syntax - aware statistical machine translation system . \" ...We propose a novel reordering model for phrase - based statistical machine translation ( SMT ) that uses a maximum entropy ( MaxEnt ) model to predicate reorderings of neighbor blocks ( phrase pairs ) .","label":"CompareOrContrast","metadata":{},"score":"40.92021"}
{"text":"It turns out that these generalized parsers can do most of the work required to train and apply a syntax - aware statistical machine translation system . \" ...We propose a novel reordering model for phrase - based statistical machine translation ( SMT ) that uses a maximum entropy ( MaxEnt ) model to predicate reorderings of neighbor blocks ( phrase pairs ) .","label":"CompareOrContrast","metadata":{},"score":"40.92021"}
{"text":"It is relatively inexpensive and has the potential to reduce ... \" .This article considers the task of automatically inducing role - semantic annotations in the FrameNet paradigm for new languages .We propose a general framework that is based on annotation projection , phrased as a graph optimization problem .","label":"CompareOrContrast","metadata":{},"score":"41.02256"}
{"text":"No additional knowledge about the target domain is included .A more realistic approach assumes that only raw text from the target domain is available .This assumption lends itself well to semi - supervised learning methods since these utilize both labeled and unlabeled examples .","label":"CompareOrContrast","metadata":{},"score":"41.077877"}
{"text":"In this paper , we present a novel approach to incorporate source - side syntactic reordering patterns into phrase - based SMT .The main contribution of this work is to use the lattice scoring approach to exploit and utilize reordering information that is favoured by the baseline PBSMT system .","label":"CompareOrContrast","metadata":{},"score":"41.357323"}
{"text":"We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .We propose using corpus - level statistics for lexicon learning decisions .","label":"CompareOrContrast","metadata":{},"score":"41.546936"}
{"text":"These methods require labeled examples of syntactic structures to learn statistical patterns governing these structures .Labeled data typically requires expert annotators which makes it both time consuming and costly to produce .Furthermo ... \" .Current efforts in syntactic parsing are largely data - driven .","label":"CompareOrContrast","metadata":{},"score":"41.566696"}
{"text":"We present experiments with sequence models on part - of - speech tagging and named entity recognition tasks , and with syntactic parsers on dependency parsing and machine translation reordering tasks .Low - latency solutions for syntactic parsing are needed if parsing is to become an integral part of user - facing natural language applications .","label":"CompareOrContrast","metadata":{},"score":"41.95636"}
{"text":"The joint probability model proposed by Marcu and Wong ( 2002 ) provides a strong probabilistic framework for phrase - based statistical machine translation ( SMT ) .The model 's usefulness is , however , limited by the computational complexity of estimating parameters at the phrase level .","label":"CompareOrContrast","metadata":{},"score":"42.28387"}
{"text":"The joint probability model proposed by Marcu and Wong ( 2002 ) provides a strong probabilistic framework for phrase - based statistical machine translation ( SMT ) .The model 's usefulness is , however , limited by the computational complexity of estimating parameters at the phrase level .","label":"CompareOrContrast","metadata":{},"score":"42.28387"}
{"text":"We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system .We use a corpus of weakly - labeled reference reorderings to guide parser training .Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress .","label":"CompareOrContrast","metadata":{},"score":"42.576767"}
{"text":"The model is formally a latent variable CRF grammar over trees , learned by iteratively splitting grammar productions ( not categories ) .Different regions of the grammar are refined to different degrees , yielding grammars which are three orders of magnitude smaller than the single - scale baseline and 20 times smaller than the split - and - merge grammars of Petrov et al .","label":"CompareOrContrast","metadata":{},"score":"42.639404"}
{"text":"The need for domain adaptation arises in almost all NLP tasks : part - of - speech tagging , semantic role labeling , statistical parsing and statistical machine translation , to name but a few .Thus , one conclusion from that line of work is that as long as there is a reasonable ( often even small ) amount of labeled target data , it is often more fruitful to just use that .","label":"CompareOrContrast","metadata":{},"score":"42.762325"}
{"text":"We show that the automatically induced latent variable grammars of Petrov et al .2006 vary widely in their underlying representations , depending on their EM initialization point .We use this to our advantage , combining multiple automatically learned grammars into an unweighted product model , which gives significantly improved performance over state - of - the - art individual grammars .","label":"CompareOrContrast","metadata":{},"score":"42.85743"}
{"text":"Constraining the joint model improves performance , showing results that are very close to state - of - the - art phrase - based models .It also allows it to scale up to larger corpora and therefore be more widely applicable . \" ...","label":"CompareOrContrast","metadata":{},"score":"43.150536"}
{"text":"Some reordering methods are carried out on syntactic source trees .( Xia and McCord , 2004 ) propose an ... . by Sebastian Padó , Mirella Lapata - Journal of Artificial Intelligence Research , 2009 . \" ...This article considers the task of automatically inducing role - semantic annotations in the FrameNet paradigm for new languages .","label":"CompareOrContrast","metadata":{},"score":"43.259476"}
{"text":"Unsupervised word vector learning 5 .BackpropagaGon Training 6 .Sharing staGsGcal strength 2 .Recursive Neural Networks 3 . ApplicaGons , Discussion , and Resources 18 .Outline of the Tutorial 1 .The Basics 2 .Recursive Neural Networks 1 .","label":"CompareOrContrast","metadata":{},"score":"43.39831"}
{"text":"Abstract .The training of most syntactic SMT approaches involves two essential components , word alignment and monolingual parser .In the current state of the art these two components are mutually independent , thus causing problems like lack of rule generalization , and violation of syntactic correspondence in translation rules .","label":"CompareOrContrast","metadata":{},"score":"43.425117"}
{"text":"In the second submission , also a constituency parsing system , the n - best lists of various parsing models are combined using an approximate sentence - level product model .The third system , the highest ranked system in the dependency parsing track , uses voting over dependency arcs to combine the output of three constituency parsing systems which have been converted to dependency trees .","label":"CompareOrContrast","metadata":{},"score":"44.367958"}
{"text":"We confirmed the effectiveness of our proposed method on a medium - sized corpus for Chinese - English machine translation task .Our method outperformed the baseline system by 1.67 % relative on a randomly selected testset and 8.56 % relative on the NIST 2008 testset in terms of BLEU score .","label":"CompareOrContrast","metadata":{},"score":"44.388386"}
{"text":"We show how to apply loopy belief propagation ( BP ) , a simple and effective tool for approximate learning and inference .As a parsing algorithm , BP is both asymptotically and empirically efficient .E ... \" .We formulate dependency parsing as a graphical model with the novel ingredient of global constraints .","label":"CompareOrContrast","metadata":{},"score":"44.52806"}
{"text":"We show how to apply loopy belief propagation ( BP ) , a simple and effective tool for approximate learning and inference .As a parsing algorithm , BP is both asymptotically and empirically efficient .E ... \" .We formulate dependency parsing as a graphical model with the novel ingredient of global constraints .","label":"CompareOrContrast","metadata":{},"score":"44.52806"}
{"text":"Second , how can we efficiently infer optimal structures within them ?Hierarchical coarse - to - fine methods address both questions .Coarse - to - fine approaches exploit a sequence of models which introduce complexity gradually .At the top of the sequence is a trivial model in which learning and inference are both cheap .","label":"CompareOrContrast","metadata":{},"score":"44.66611"}
{"text":"For instance , self - training the Charniak parser alone was not effective for adaptation ( it has been common wisdom that self - training is generally not effective ) , but self - training with a reranker was surprisingly highly effective ( McClosky et al . , 2006 ) .","label":"CompareOrContrast","metadata":{},"score":"44.738243"}
{"text":", 2009 ; Biber & Gray , 2010 ) , but the most interesting usages apply the divergence to a machine learning system .Despite the fact that authors have shown that a divergence ( Van Asch & Daelemans , 2010 ; Plank , 2011 ) or a linear combination of divergences ( McClosky , 2010 ) can be successfully used to link the sim ... . \" ...","label":"CompareOrContrast","metadata":{},"score":"44.980568"}
{"text":"Standard inference can be used at test time .Our approach is able to scale to very large problems and yields significantly improved target domain accuracy .It is well known that parsing accuracies drop significantly on out - of - domain data .","label":"CompareOrContrast","metadata":{},"score":"45.080875"}
{"text":"Our methods result in state - of - the - art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .","label":"CompareOrContrast","metadata":{},"score":"45.319595"}
{"text":"We address the problem of translating from morphologically poor to morphologically rich languages by adding per - word linguistic information to the source language .We use the syntax of the source sentence to extract information for noun cases and verb persons and annotate the corresponding words accordingly .","label":"CompareOrContrast","metadata":{},"score":"45.40909"}
{"text":"SMT has made tremendous strides in less than two decades , and many popular tec ... \" .Statistical machine translation ( SMT ) treats the translation of natural language as a machine learning problem .By examining many samples of human - produced translation , SMT algorithms automatically learn how to translate .","label":"CompareOrContrast","metadata":{},"score":"45.491203"}
{"text":"Nonetheless , the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks .We demonstrate that log - linear grammars with latent variables can be practically trained using discriminative methods .Central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectations to be efficiently approximated in a gradient - based procedure .","label":"CompareOrContrast","metadata":{},"score":"45.531822"}
{"text":"Because each refinement introduces only limited complexity , both learning and inference can be done in an incremental fashion .In this dissertation , we describe several coarse - to - fine systems .In the domain of syntactic parsing , complexity is in the grammar .","label":"CompareOrContrast","metadata":{},"score":"45.555267"}
{"text":"Wu ( 1997 ) introduced constraints on alignments using a probabilistic synchronous context - free grammar restricted to Chomskynormal form .Yamada and Knight ( 2001 ) used a statistical parser trained using a Treebank in the source language to produce parse trees ... . \" ...","label":"CompareOrContrast","metadata":{},"score":"45.56067"}
{"text":"Wu ( 1997 ) introduced constraints on alignments using a probabilistic synchronous context - free grammar restricted to Chomskynormal form .Yamada and Knight ( 2001 ) used a statistical parser trained using a Treebank in the source language to produce parse trees ... . \" ...","label":"CompareOrContrast","metadata":{},"score":"45.56067"}
{"text":"Our first- , second- , and third - order models achieve accuracies comparable to those of their unpruned counterparts , while exploring only a fraction of the search space .We observe speed - ups of up to two orders of magnitude compared to exhaustive search .","label":"CompareOrContrast","metadata":{},"score":"45.709267"}
{"text":"Latent variable grammars take an observed ( coarse ) treebank and induce more fine - grained grammar categories , that are better suited for modeling the syntax of natural languages .Estimation can be done in a generative or a discriminative framework , and results in the best published parsing accuracies over a wide range of syntactically divergent languages and domains .","label":"CompareOrContrast","metadata":{},"score":"46.177956"}
{"text":"Starting from a mono - phone model , we learn increasingly refined models that capture phone internal structures , as well as context - dependent variations in an automatic way .Our approaches reduces error rates compared to other baseline approaches , while streamlining the learning procedure .","label":"CompareOrContrast","metadata":{},"score":"46.289185"}
{"text":"The extracted patterns are then used to convert the parsed inputs into word lattices , which contain both the original source sentences and their potential reorderings .Weights of the word lattices are estimated from the observations of the syntactic reordering patterns in the training corpus .","label":"CompareOrContrast","metadata":{},"score":"46.373306"}
{"text":"We study this problem as a new task - multiple source parser adaptation .Our system trains on corpora from many different domains .It learns not only statistics of those domains but quantitative measures of domain differences and how those differences affect parsing accuracy .","label":"CompareOrContrast","metadata":{},"score":"46.446274"}
{"text":"Unlike previous work on projecting syntactic resources , we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers .The projected parsers from our system result in state - of - the - art performance when compared to previously studied unsupervised and projected parsing systems across eight different languages .","label":"CompareOrContrast","metadata":{},"score":"46.460304"}
{"text":"First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar without a treebank .In our experiments , hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy .","label":"CompareOrContrast","metadata":{},"score":"46.55715"}
{"text":"In this section applications include language modeling and POS tagging .In the second section we present recursive neural networks which can learn structured tree outputs as well as vector representations for phrases and sentences .We cover both equations as well as applications .","label":"CompareOrContrast","metadata":{},"score":"46.601746"}
{"text":"We present several models to this end ; in particular a partially observed conditional random field model , where coupled token and type constraints provide a partial signal for training .Averaged across eight previously studied Indo - European languages , our model achieves a 25 % relative error reduction over the prior state of the art .","label":"CompareOrContrast","metadata":{},"score":"46.65914"}
{"text":"Backpropagation Training Part 1.5 : The Basics 60 .Fprop : visit nodes in topo-­‐sort order -­‐ Compute value of node given predecessors 2 .Learning word - level classifiers : POS and NER Part 1.6 : The Basics 69 .The Model ( Collobert & Weston 2008 ; Collobert et al .","label":"CompareOrContrast","metadata":{},"score":"46.702324"}
{"text":"A mixture grammar fit with the EM algorithm shows improvement over a single PCFG , both in parsing accuracy and in test data likelihood .We argue that this improvement comes from the learning of specialized grammars that capture non - local correlations .","label":"CompareOrContrast","metadata":{},"score":"46.814026"}
{"text":"1 Introduction This article presents CYK+ , a bottom - up parsing algorithm for stochastic context - free grammars that is able : 1 . to deal multiple interpretations of sentences containing compound words ; 2 . to extract N - most probable parses in O(n 3 ) and compute at the same time all possible parses of any portion of the input sequence with their p ..","label":"CompareOrContrast","metadata":{},"score":"46.973904"}
{"text":"1 Introduction This article presents CYK+ , a bottom - up parsing algorithm for stochastic context - free grammars that is able : 1 . to deal multiple interpretations of sentences containing compound words ; 2 . to extract N - most probable parses in O(n 3 ) and compute at the same time all possible parses of any portion of the input sequence with their p ..","label":"CompareOrContrast","metadata":{},"score":"46.973904"}
{"text":"The resulting grammars are extremely compact com- pared to other high - performance parsers , yet the parser gives the best published accuracies on several languages , as well as the best generative parsing numbers in English .In addi- tion , we give an associated coarse - to - fine inference scheme which vastly improves inference time with no loss in test set accuracy .","label":"CompareOrContrast","metadata":{},"score":"47.00914"}
{"text":"The annotations are produced automatically with statistical models that are specifically adapted to historical text .The corpus will facilitate the study of linguistic trends , especially those related to the evolution of syntax .Syntactic analysis of search queries is important for a variety of information- retrieval tasks ; however , the lack of annotated data makes training query analysis models difficult .","label":"CompareOrContrast","metadata":{},"score":"47.504547"}
{"text":"These transducers are strictly more expressive than the special case of standard leftto - right finite - state transducers .Dependency transduction models are then defined as collections of weighted head transducers that are applied hierarchically .A dynamic programming search algorithm is described for finding the optimal transduction of an input string with respect to a dependency transduction model .","label":"CompareOrContrast","metadata":{},"score":"47.684418"}
{"text":"These transducers are strictly more expressive than the special case of standard leftto - right finite - state transducers .Dependency transduction models are then defined as collections of weighted head transducers that are applied hierarchically .A dynamic programming search algorithm is described for finding the optimal transduction of an input string with respect to a dependency transduction model .","label":"CompareOrContrast","metadata":{},"score":"47.684418"}
{"text":"Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees .We first introduce our approach to inducing such a grammar from parallel corpora .Second , we describe the graphical model for the machine translation task , which can also be viewed as a stochastic tree - to - tree transducer .","label":"CompareOrContrast","metadata":{},"score":"47.7581"}
{"text":"Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees .We first introduce our approach to inducing such a grammar from parallel corpora .Second , we describe the graphical model for the machine translation task , which can also be viewed as a stochastic tree - to - tree transducer .","label":"CompareOrContrast","metadata":{},"score":"47.7581"}
{"text":"The algorithm uses a similarity graph to encourage similar n - grams to have similar POS tags .We demonstrate the efficacy of our approach on a domain adaptation task , where we assume that we have access to large amounts of unlabeled data from the target domain , but no additional labeled data .","label":"CompareOrContrast","metadata":{},"score":"48.493942"}
{"text":"We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process ( HDP ) .Our HDP - PCFG model allows the complexity of the grammar to grow as more training data is available .In addition to presenting a fully Bayesian model for the PCFG , we also develop an efficient variational inference procedure .","label":"CompareOrContrast","metadata":{},"score":"48.54328"}
{"text":"With 100 K unlabeled and 2 K labeled questions , uptraining is able to improve parsing accuracy to 84 % , closing the gap between in - domain and out - of - domain performance .We study self - training with products of latent variable grammars in this paper .","label":"CompareOrContrast","metadata":{},"score":"48.604286"}
{"text":"The first submission , the highest ranked constituency parsing system , uses a combination of PCFG - LA product grammar parsing and self - training .In the second submission , also a constituency parsing ... \" .The DCU - Paris13 team submitted three systems to the SANCL 2012 shared task on parsing English web text .","label":"CompareOrContrast","metadata":{},"score":"48.709618"}
{"text":"Labeled data typically requires expert annotators which makes it both time consuming and costly to produce .Furthermore , once training data has been created for one textual domain , portability to similar domains is limited .This domain - dependence has inspired a large body of work since syntactic parsing aims to capture syntactic patterns across an entire language rather than just a specific domain .","label":"CompareOrContrast","metadata":{},"score":"48.76583"}
{"text":"The method first searches for hierarchical alignments of the training examples guided by correlation statistics , and then constructs the transitions of head transducers that are consistent with these alignments .Experimental results are given for applying the training method to translation from English to Spanish and Japanese . by Haitao Mi , Liang Huang , Qun Liu - In Proceedings of ACL-08 : HLT , 2008 . \" ...","label":"CompareOrContrast","metadata":{},"score":"49.01951"}
{"text":"The method first searches for hierarchical alignments of the training examples guided by correlation statistics , and then constructs the transitions of head transducers that are consistent with these alignments .Experimental results are given for applying the training method to translation from English to Spanish and Japanese . by Haitao Mi , Liang Huang , Qun Liu - In Proceedings of ACL-08 : HLT , 2008 . \" ...","label":"CompareOrContrast","metadata":{},"score":"49.01951"}
{"text":".. our corpus .The second question is how one can estimate NLP systems ' performance when gold standard on the test data does not exist .Our experiments show that the predicted scores are close to the real scores when tested on the CTB data .","label":"CompareOrContrast","metadata":{},"score":"49.457523"}
{"text":"We believe that the key to future success will be to exploit large collections of unlabeled data in addition to labeled data .Not only because unlabeled data is easier to obtain , but existing labeled resources are often not even close to the envisioned target application domain .","label":"CompareOrContrast","metadata":{},"score":"49.515278"}
{"text":"Check for implementaGon bugs with gradient check 3 .Parameter iniGalizaGon 4 .OpGmizaGon tricks 5 .If not , change model structure or make model \" larger \" 2 .Use 2nd order batch method such as LBFGS .On large datasets , SGD usually wins over all batch methods .","label":"CompareOrContrast","metadata":{},"score":"49.754547"}
{"text":"Finally , we present multilingual experiments which show that parsing with hierarchical state - splitting is fast and accurate in multiple languages and domains , even without any language - specific tuning .This work describes systems for detecting semantic categories present in news video .","label":"CompareOrContrast","metadata":{},"score":"49.876076"}
{"text":"We propose a forest - based approach that translates a packed forest of exponentially many parses , which encodes many more alternatives than standard n - best lists .Large - scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline .","label":"CompareOrContrast","metadata":{},"score":"49.935127"}
{"text":"We propose a forest - based approach that translates a packed forest of exponentially many parses , which encodes many more alternatives than standard n - best lists .Large - scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline .","label":"CompareOrContrast","metadata":{},"score":"49.935127"}
{"text":"Such lexicon compilation requires highly reliable predicate - argument structures to practically contribute to Natural Language Processing ( NLP ) applications , such as paraphrasing , text entailment , and machine translation .We first apply chunking to raw corpora and then extract reliable chunks to ensure that high - quality predicate - argument structures are obtained from the chunks .","label":"CompareOrContrast","metadata":{},"score":"49.93626"}
{"text":"Participants were to build a single parsing system that is robust to domain changes and can handle noisy text that is commonly encountered on the web .There was a constituency and a dependency parsing track and 11 sites submitted a total of 20 systems .","label":"CompareOrContrast","metadata":{},"score":"50.4694"}
{"text":"Combining multiple grammars that were self - trained on disjoint sets of unlabeled data results in a final test accuracy of 92.5\\% on the WSJ test set and 89.6\\% on our Broadcast News test set .This work shows how to improve state - of - the - art monolingual natural language processing models using unannotated bilingual text .","label":"CompareOrContrast","metadata":{},"score":"50.492016"}
{"text":"For example , noun phrases might be split into subcategories for subjects and objects , singular and plural , and so on .This splitting process admits an efficient incremental inference scheme which reduces parsing times by orders of magnitude .Furthermore , it produces the best parsing accuracies across an array of languages , in a fully language - general fashion .","label":"CompareOrContrast","metadata":{},"score":"50.521454"}
{"text":"In this paper , we present a syntax - based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar .Synchronous dependency insertion grammars are a ... \" .Syntax - based statistical machine translation ( MT ) aims at applying statistical models to structured data .","label":"CompareOrContrast","metadata":{},"score":"50.61023"}
{"text":"In this paper , we present a syntax - based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar .Synchronous dependency insertion grammars are a ... \" .Syntax - based statistical machine translation ( MT ) aims at applying statistical models to structured data .","label":"CompareOrContrast","metadata":{},"score":"50.61023"}
{"text":"We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .","label":"CompareOrContrast","metadata":{},"score":"50.98172"}
{"text":"Improvements can be seen both on the reordered sentences as well as on the rest of the test corpus .Local reorderings are especially important for the translation systems trained on the small corpus whereas long - range reorderings are more effective for the larger corpus .","label":"CompareOrContrast","metadata":{},"score":"51.05333"}
{"text":"Jiang , Jie and Du , Jinhua and Way , Andy ( 2010 )Improved phrase - based SMT with syntactic reordering patterns learned from lattice scoring .In : AMTA 2010 - 9th Conference of the Association for Machine Translation in the Americas , 31 October - 4 November 2010 , Denver , CO , USA .","label":"CompareOrContrast","metadata":{},"score":"51.10934"}
{"text":"Despite its simplicity , a product of eight automatically learned grammars improves parsing accuracy from 90.2 % to 91.8 % on English , and from 80.3 % to 84.5 % on German .Pruning can massively accelerate the computation of feature expectations in large models .","label":"CompareOrContrast","metadata":{},"score":"51.11728"}
{"text":"Current statistical parsers tend to perform well only on their training domain and nearby genres .While strong performance on a few related domains is sufficient for many situations , it is advantageous for parsers to be able to generalize to a wide variety of domains .","label":"CompareOrContrast","metadata":{},"score":"51.22209"}
{"text":"Current statistical parsers tend to perform well only on their training domain and nearby genres .While strong performance on a few related domains is sufficient for many situations , it is advantageous for parsers to be able to generalize to a wide variety of domains .","label":"CompareOrContrast","metadata":{},"score":"51.22209"}
{"text":"We conclude with an overview of evaluation and notes on future directions . by Maja Popović , Hermann Ney - in Proc . of the Fifth Int .Conf . on Language Resources and Evaluation ( LREC , 2006 . \" ...","label":"CompareOrContrast","metadata":{},"score":"51.280582"}
{"text":"However , most previous work on domain adaptation relied on the implicit assumption that domains are somehow given .As more and more data becomes available , automatic ways to select data that is beneficial for a new ( unknown ) target domain are becoming attractive .","label":"CompareOrContrast","metadata":{},"score":"51.296432"}
{"text":"We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software .The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality . ... guages or loose translations in real corpora , pose a major challenge to syntax - based statistical MT .","label":"CompareOrContrast","metadata":{},"score":"51.388844"}
{"text":"We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software .The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality . ... guages or loose translations in real corpora , pose a major challenge to syntax - based statistical MT .","label":"CompareOrContrast","metadata":{},"score":"51.388844"}
{"text":"Primary acoustic , speech , and vision systems were trained to discriminate instances of the categories .Higher - level systems exploited correlations among the categories , incorporated sequential context , and combined the joint evidence from the three information sources .","label":"CompareOrContrast","metadata":{},"score":"51.81122"}
{"text":"We describe experiments on learning latent variable grammars for various German treebanks , using a language - agnostic statistical approach .In our method , a minimal initial grammar is hierarchically refined using an adaptive split - and - merge EM procedure , giving compact , accurate grammars .","label":"CompareOrContrast","metadata":{},"score":"52.001667"}
{"text":"Web - scale experiments show that the DMV , perhaps because it is unlexicalized , does not benefit from orders of magnitude more annotated but noisier data .Our model , trained on a single blog , generalizes to 53.3 % accuracy out - of - domain , against the Brown corpus - nearly 10 % higher than the previous published best .","label":"CompareOrContrast","metadata":{},"score":"52.091476"}
{"text":"Despite these advantages , many researchers in NLP are not familiar with these methods .Our focus is on insight and understanding , using graphical illustrations and simple , intuitive derivations .The goal of the tutorial is to make the inner workings of these techniques transparent , intuitive and their results interpretable , rather than black boxes labeled \" magic here \" .","label":"CompareOrContrast","metadata":{},"score":"52.38054"}
{"text":"On full - scale treebank parsing experiments , the discriminative latent models outperform both the comparable generative latent models as well as the discriminative non - latent baselines .We present a maximally streamlined approach to learning HMM - based acoustic models for automatic speech recognition .","label":"CompareOrContrast","metadata":{},"score":"52.703674"}
{"text":"In our experiments on Chineseto - English translation , this MaxEnt - based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks . ... the reorderings of phrases , but also integrates some phrasal generalizations into the global model .","label":"CompareOrContrast","metadata":{},"score":"52.715324"}
{"text":"In our experiments on Chineseto - English translation , this MaxEnt - based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks . ... the reorderings of phrases , but also integrates some phrasal generalizations into the global model .","label":"CompareOrContrast","metadata":{},"score":"52.715324"}
{"text":"To manage this complexity , we translate into target language clusterings of increasing vocabulary size .This approach gives dramatic speed - ups while additionally increasing final translation quality .The intersection of tree transducer - based translation models with n - gram language models results in huge dynamic programs for machine translation decoding .","label":"CompareOrContrast","metadata":{},"score":"52.800976"}
{"text":"Given this fixed network representation , we learn a final layer using the structured perceptron with beam - search decoding .On the Penn Treebank , our parser reaches 94.26 % unlabeled and 92.41 % labeled attachment accuracy , which to our knowledge is the best accuracy on Stanford Dependencies to date .","label":"CompareOrContrast","metadata":{},"score":"53.50611"}
{"text":"We present a method for acquiring reliable predicate - argument structures from raw corpora for automatic compilation of case frames .Such lexicon compilation requires highly reliable predicate - argument structures to practically contribute to Natural Language Processing ( NLP ) applications , such as par ... \" .","label":"CompareOrContrast","metadata":{},"score":"53.542385"}
{"text":"These works aim to predict the parser performance on a given target sentence .Ravi et al .( 2008 ) frame this as a regression problem .Kawahara and Uchimoto ( 2008 ) treat ... . \" ...Genre classification has been found to improve performance in many applications of statistical NLP , including language modeling for spoken language , domain adaptation of statistical parsers , and machine translation .","label":"CompareOrContrast","metadata":{},"score":"53.58624"}
{"text":"We present a novel approach which employs a randomized sequence of pruning masks .Formally , we apply auxiliary variable MCMC sampling to generate this sequence of masks , thereby gaining theoretical guarantees about convergence .Because each mask is generally able to skip large portions of an underlying dynamic program , our approach is particularly compelling for high - degree algorithms .","label":"CompareOrContrast","metadata":{},"score":"54.04517"}
{"text":"WSJ data ( Petrov and Klein , 2007 ; Foster , 2010 ) .The parser uses the English signature list described in Attia et al ( 2010 ) to assign partof - speech tags to unknown words . \" ...Domain adaptation is an important task in order for NLP systems to work well in real applications .","label":"CompareOrContrast","metadata":{},"score":"54.111877"}
{"text":"Please take a moment to check if your company . operates such a program .Thank you very much for your support of LINGUIST !Tools . \" ...We present a statistical machine translation model that uses hierarchical phrases - phrases that contain subphrases .","label":"CompareOrContrast","metadata":{},"score":"54.264496"}
{"text":"contacting your human resources department and sending us a form that the .EMU Foundation fills in and returns to your employer .This is generally a simple .administrative procedure that doubles the value of your gift to LINGUIST , without .","label":"CompareOrContrast","metadata":{},"score":"54.565155"}
{"text":"Despite the much simplified training process , our acoustic model achieves state - of - the - art results on phone classification ( where it outperforms almost all other methods ) and competitive performance on phone recognition ( where it outperforms standard CD triphone / subphone / GMM approaches ) .","label":"CompareOrContrast","metadata":{},"score":"54.919556"}
{"text":"We use the Margin Infused Relaxed Algorithm of Crammer et al . to add a large number of new features to two machine translation systems : the Hiero hierarchical phrasebased translation system and our syntax - based translation system .On a large - scale Chinese - English translation task , we obtain statisti ... \" .","label":"CompareOrContrast","metadata":{},"score":"55.166134"}
{"text":"We use the Margin Infused Relaxed Algorithm of Crammer et al . to add a large number of new features to two machine translation systems : the Hiero hierarchical phrasebased translation system and our syntax - based translation system .On a large - scale Chinese - English translation task , we obtain statisti ... \" .","label":"CompareOrContrast","metadata":{},"score":"55.166134"}
{"text":"As a parsing algorithm , BP is both asymptotically and empirically efficient .Even with second - order features or latent variables , which would make exact parsing considerably slower or NP - hard , BP needs only O(n3 ) time with a small constant factor .","label":"CompareOrContrast","metadata":{},"score":"55.344887"}
{"text":"As a parsing algorithm , BP is both asymptotically and empirically efficient .Even with second - order features or latent variables , which would make exact parsing considerably slower or NP - hard , BP needs only O(n3 ) time with a small constant factor .","label":"CompareOrContrast","metadata":{},"score":"55.344887"}
{"text":"This ' universal ' treebank is made freely available in order to facilitate research on multilingual dependency parsing .We consider the construction of part - of - speech taggers for resource - poor languages .Recently , manually constructed tag dictionaries from Wiktionary and dictionaries projected via bitext have been used as type constraints to overcome the scarcity of annotated data in this setting .","label":"CompareOrContrast","metadata":{},"score":"55.375412"}
{"text":"Recursive Neural Networks for Structure Prediction on the mat .9 1 4 3 3 3 8 3 8 5 3 3 Neural \" Network \" 8 3 1.3 Inputs : two candidate children 's representaGons Outputs : 1 .The semanGc representaGon if the two nodes are merged .","label":"CompareOrContrast","metadata":{},"score":"55.41129"}
{"text":"This survey presents a tutorial overview of state - of - the - art SMT at the beginning of 2007 .We begin with the context of the current research , and then move to a formal problem description and an overview of the four main subproblems : translational equivalence modeling , mathematical modeling , parameter estimation , and decoding .","label":"CompareOrContrast","metadata":{},"score":"55.961395"}
{"text":"Interspeech 2011 ] MSR MAVIS Speech System [ Dahl et al .2012 ; Seide et al .2011 ; following Mohamed et al .Outline of the Tutorial 1 .The Basics 1 .MoGvaGons 2 .From logisGc regression to neural networks 3 .","label":"CompareOrContrast","metadata":{},"score":"56.0066"}
{"text":"McClosky et al .( 2010 ) coined the term multiple source domain adaptation .Similar to us , McClosky et al .( 2010 ) regard a target domain as mixture of source domains , b .. by Joseph Le Roux , Jennifer Foster , Joachim Wagner , Rasul Samad , Zadeh Kaljahi , Anton Bryl . \" ...","label":"CompareOrContrast","metadata":{},"score":"56.11058"}
{"text":"On a large - scale Chinese - English translation task , we obtain statistically significant improvements of +1.5 Bleu and +1.1 Bleu , respectively .We analyze the impact of the new features and the performance of the learning algorithm .The baseline model includes 12 features whose weights are optimized using MERT .","label":"CompareOrContrast","metadata":{},"score":"56.24257"}
{"text":"On a large - scale Chinese - English translation task , we obtain statistically significant improvements of +1.5 Bleu and +1.1 Bleu , respectively .We analyze the impact of the new features and the performance of the learning algorithm .The baseline model includes 12 features whose weights are optimized using MERT .","label":"CompareOrContrast","metadata":{},"score":"56.24257"}
{"text":"Socher et al .( NIPS 2011 ) 137 .2004 ) Method Acc .( ACL 2012 ) : String Re-­‐wriGng Kernel 76.3 -­‐-­‐ Unfolding Recursive Autoencoder ( NIPS 2011 ) 76.8 83.6 138 .Recursive Autoencoders for Full Sentence Paraphrase Detection 139 .","label":"CompareOrContrast","metadata":{},"score":"56.53609"}
{"text":"These representaGons are way be0er at encoding dimensions of similarity than we realized !Stunning new result at this conference !Unsupervised word vector learning Part 1.4 : The Basics 43 .A neural network for learning word vectors ( Collobert et al .","label":"CompareOrContrast","metadata":{},"score":"56.5661"}
{"text":"Incorporating additional features would increase the runtime additively rather than multiplicatively . ... , 2007)-but our footnote 11 suggests that BP might incorporate a language model rapidly .Finally , we can take advantage of improvements to BP proposed in the context of other applications .","label":"CompareOrContrast","metadata":{},"score":"56.67752"}
{"text":"Incorporating additional features would increase the runtime additively rather than multiplicatively . ... , 2007)-but our footnote 11 suggests that BP might incorporate a language model rapidly .Finally , we can take advantage of improvements to BP proposed in the context of other applications .","label":"CompareOrContrast","metadata":{},"score":"56.67752"}
{"text":"( ACL 2012 ) 172 .Deep Learning General Strategy and Tricks Part 3.2 177 .General Strategy 1 .Select network structure appropriate for problem 1 .Nonlinearity 2 .Check for implementaGon bugs with gradient checks 3 .Parameter iniGalizaGon 4 .","label":"CompareOrContrast","metadata":{},"score":"57.39356"}
{"text":"The new Viewer adds three features for more powerful search : wildcards , morphological inflections , and capitalization .These additions allow the discovery of patterns that were previously difficult to find and further facilitate the study of linguistic trends in printed text .","label":"CompareOrContrast","metadata":{},"score":"57.415398"}
{"text":"The paper defines weighted head transducers , finite - state machines that perform middle - out string transduction .These transducers are strictly more expressive than the special case of standard leftto - right finite - state transducers .Dependency transduction models are then defined as collections of wei ... \" .","label":"CompareOrContrast","metadata":{},"score":"57.754326"}
{"text":"The paper defines weighted head transducers , finite - state machines that perform middle - out string transduction .These transducers are strictly more expressive than the special case of standard leftto - right finite - state transducers .Dependency transduction models are then defined as collections of wei ... \" .","label":"CompareOrContrast","metadata":{},"score":"57.754326"}
{"text":"We provide experimental results on the NIST 2003 Chinese - English large data track evaluation .We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state - of - theart performance in machine translation . ... ectively .","label":"CompareOrContrast","metadata":{},"score":"57.862404"}
{"text":"We provide experimental results on the NIST 2003 Chinese - English large data track evaluation .We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state - of - theart performance in machine translation . ... ectively .","label":"CompareOrContrast","metadata":{},"score":"57.862404"}
{"text":"Workshop Topics We especially encourage submissions on semi - supervised approaches of domain adaptation with a deep analysis of models , data and results , although we do not exclude papers on supervised adaptation .In particular , we welcome submissions that address any of the following topics or other relevant issues : .","label":"CompareOrContrast","metadata":{},"score":"57.916313"}
{"text":"We show that dependency parsers have more difficulty parsing questions than constituency parsers .In particular , deterministic shift - reduce dependency parsers , which are of highest interest for practical applications because of their linear running time , drop to 60 % labeled accuracy on a question test set .","label":"CompareOrContrast","metadata":{},"score":"57.946762"}
{"text":"We show for the first time that integrating a WSD system improves the performance of a state - ofthe - art statistical MT system on an actual translation task .Furthermore , the improvement is statistically significant . \" ...Statistical machine translation ( SMT ) treats the translation of natural language as a machine learning problem .","label":"CompareOrContrast","metadata":{},"score":"58.04138"}
{"text":"We took this opportunity to experiment with various ways of adapting a statistical machine translation systems to a special domain ( here : news commentary ) , when most of the training data is from a different domain ( here : European Parliament speeches ) .","label":"CompareOrContrast","metadata":{},"score":"58.45019"}
{"text":"c , d ) !c , d ) !How do we train the weights W ?2005 , Gunawardana et al .Summary Knowing the meaning of words !Effective deep learning became possible through unsupervised pre - training [ Erhan et al . , JMLR 2010 ] Purely supervised neural net With unsupervised pre-­‐training ( with RBMs and Denoising Auto-­‐Encoders ) 0 - 9 handwri0en digit recogniGon error rate ( MNIST data ) 33 .","label":"CompareOrContrast","metadata":{},"score":"58.528244"}
{"text":"In this paper , we successfully integrate a state - of - the - art WSD system into a state - of - the - art hierarchical phrase - bas ... \" .Recent research presents conflicting evidence on whether word sense disambiguation ( WSD ) systems can help to improve the performance of statistical machine translation ( MT ) systems .","label":"CompareOrContrast","metadata":{},"score":"58.559532"}
{"text":"The Basics 2 .Recursive Neural Networks 3 . ApplicaGons , Discussion , and Resources 1 .Assorted Speech and NLP ApplicaGons 2 .Deep Learning : General Strategy and Tricks 3 .Resources ( readings , code , ... ) 4 .","label":"CompareOrContrast","metadata":{},"score":"58.60968"}
{"text":"The special challenge of the WMT 2007 shared task was domain adaptation .We took this opportunity to experiment with various ways of adapting a statistical machine translation systems to a special domain ( here : news commentary ) , when most of the training data is from a different domain ( here : Europe ... \" .","label":"CompareOrContrast","metadata":{},"score":"59.219383"}
{"text":"The best accuracies were in the 80 - 84\\% range for F1 and LAS ; even part - of - speech accuracies were just above 90\\% .Coarse - to - fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy .","label":"CompareOrContrast","metadata":{},"score":"59.248802"}
{"text":"In an ordinary syntactic parser , the input is a string , and the grammar ranges over strings .This paper explores generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples .","label":"CompareOrContrast","metadata":{},"score":"59.81002"}
{"text":"In an ordinary syntactic parser , the input is a string , and the grammar ranges over strings .This paper explores generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples .","label":"CompareOrContrast","metadata":{},"score":"59.81002"}
{"text":"In an ordinary syntactic parser , the input is a string , and the grammar ranges over strings .This paper explores generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples .","label":"CompareOrContrast","metadata":{},"score":"59.81002"}
{"text":"In an ordinary syntactic parser , the input is a string , and the grammar ranges over strings .This paper explores generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples .","label":"CompareOrContrast","metadata":{},"score":"59.81002"}
{"text":"State - of - the - art natural language processing models are anything but compact .Syntactic parsers have huge grammars , machine translation systems have huge transfer tables , and so on across a range of tasks .With such complexity come two challenges .","label":"CompareOrContrast","metadata":{},"score":"59.86691"}
{"text":"The results show that an unsupervised technique based on topic models is effective - it outperforms random data selection on both languages examined , English and Dutch .Moreover , the technique works better than manually assigned labels gathered from meta - data that is available for English . ...","label":"CompareOrContrast","metadata":{},"score":"60.09367"}
{"text":"We adapted our statistical machine translation system that performed successfully in previous DARPA competitions on open domain text translations .We participated in the supplied corpora transcription track .We achieved the highest BLEU score in 2 out of 5 language pairs and had competitive results for the other language pairs . ...","label":"CompareOrContrast","metadata":{},"score":"60.139763"}
{"text":"Training with Backpropagation 57 That 's almost backpropagaGon It 's simply taking derivaGves and using the chain rule !Remaining trick : we can re-­‐use derivaGves computed for higher layers in compuGng derivaGves for lower layers Example : last derivaGves of model , the word vectors in x .","label":"CompareOrContrast","metadata":{},"score":"60.19478"}
{"text":"Within this framework , we present projection models that exploit lexical and syntactic information .We provide an experimental evaluation on an English - German parallel corpus which demonstrates the feasibility of inducing high - precision German semantic role annotation both for manually and automatically annotated English data .","label":"CompareOrContrast","metadata":{},"score":"60.200104"}
{"text":"Results are presented on the European Parliament corpus containing about 700k sentences and 15 M ... \" .In this work we investigate new possibilities for improving the quality of statistical machine translation ( SMT ) by applying word reorderings of the source language sentences based on Part - of - Speech tags .","label":"CompareOrContrast","metadata":{},"score":"60.373108"}
{"text":"The penalty for a recognition failure is often small : if two con- figurations are confused , they are often similar to each other , and the illusion works well enough , for instance , to drive a graphics animation of the moving hand .","label":"CompareOrContrast","metadata":{},"score":"60.52633"}
{"text":"Neural word embeddings as a distributed representation Similar idea Combine vector space semanGcs with the predicGon of probabilisGc models ( Bengio et al .2003 , Collobert & Weston 2008 , Turian et al .Stunning new result at this conference !","label":"CompareOrContrast","metadata":{},"score":"60.678955"}
{"text":"At its ... \" .Genre classification has been found to improve performance in many applications of statistical NLP , including language modeling for spoken language , domain adaptation of statistical parsers , and machine translation .It has also been found to benefit retrieval of spoken or written docu - ments .","label":"CompareOrContrast","metadata":{},"score":"61.405113"}
{"text":"Algorithm for Parsing Images Same Recursive Neural Network as for natural language parsing !( Socher et al .ICML 2011 )Features Grass Tree Segments Semantic Representations People Building Parsing Natural Scene ImagesParsing Natural Scene Images 129 .Recursive Deep Learning 1 .","label":"CompareOrContrast","metadata":{},"score":"61.429363"}
{"text":"We present a number of semi - supervised parsing experiments on the Irish language carried out using a small seed set of manually parsed trees and a larger , yet still relatively small , set of unlabelled sentences .We take two popular dependency parsers - one graph - based and one transition - based - and ... \" .","label":"CompareOrContrast","metadata":{},"score":"61.62139"}
{"text":"POS WSJ ( acc . )Stacking for deep autoencoders 3 .Why do autoencoders improve deep neural nets so much ?84 Auto - Encoders Learn Salient Variations , like a non - linear PCA Minimizing reconstrucGon error forces latent representaGon of \" similar inputs \" to stay on manifold .","label":"CompareOrContrast","metadata":{},"score":"61.69852"}
{"text":"Parsing a sentence 5 2 3 3 8 3 5 4 7 3 9 1 5 3 The cat sat on the mat .Every candidate score in beam search needs a matrix-­‐vector product .2006 ) 91.0 Charniak -­‐ Self Trained-­‐ReRanked ( McClosky et al .","label":"CompareOrContrast","metadata":{},"score":"61.839935"}
{"text":"Large-­‐batch LBFGS extends the reach of LBFGS [ Le et al ICML'2011].Stochastic Gradient Descent ( SGD ) 185 .This can become very small or very large quickly [ Bengio et al 1994 ] , and the locality assumpGon of gradient descent breaks down .","label":"CompareOrContrast","metadata":{},"score":"62.122524"}
{"text":"We show how web mark - up can be used to improve unsupervised dependency parsing .Starting from raw bracketings of four common HTML tags ( anchors , bold , italics and underlines ) , we refine approximate partial phrase boundaries to yield accurate parsing constraints .","label":"CompareOrContrast","metadata":{},"score":"62.517586"}
{"text":"Behavior Control : Finally we show how all these elements can be incorporated into a goal keeping robot .We develop simple behaviors that can be used in a layered architecture and enable the robot to block most balls that are being shot at the goal .","label":"CompareOrContrast","metadata":{},"score":"63.036938"}
{"text":"Across various hierarchical encoding schemes and for multiple language pairs , we show speed - ups of up to 50 times over single - pass decoding while improving BLEU score .Moreover , our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram - to - trigram decoder .","label":"CompareOrContrast","metadata":{},"score":"63.355034"}
{"text":"A tutorial given at NAACL HLT 2013 .Richard Socher and Christopher Manning .Machine learning is everywhere in today 's NLP , but by and large machine learning amounts to numerical optimization of weights for human designed representations and features .","label":"CompareOrContrast","metadata":{},"score":"63.62493"}
{"text":"Computers fail to track these in fast video , but sleight of hand fools humans as well : what happens too quickly we just can not see .We show a 3D tracker for these types of motions that relies on the recognition of familiar configurations in 2D images ( classification ) , and fills the gaps in - between ( interpolation ) .","label":"CompareOrContrast","metadata":{},"score":"63.648155"}
{"text":"Our generative self - trained grammars reach F scores of 91.6 on the WSJ test set and surpass even discriminative reranking systems without self - training .Additionally , we show that multiple self - trained grammars can be combined in a product model to achieve even higher accuracy .","label":"CompareOrContrast","metadata":{},"score":"63.903805"}
{"text":"The last part of the tutorial gives a general overview of the different applications of deep learning in NLP , including bag of words models .We will provide a discussion of NLP - oriented issues in modeling , interpretation , representational power , and optimization .","label":"CompareOrContrast","metadata":{},"score":"63.98309"}
{"text":"Meanwhile , Graphics Processor Units ( GPUs ) have become widely available , offering the opportunity to alleviate this bottleneck by exploiting the fine - grained data parallelism found in the CKY algorithm .In this paper , we explore the design space of parallelizing the dynamic programming computations carried out by the CKY algorithm .","label":"CompareOrContrast","metadata":{},"score":"64.103676"}
{"text":"Across eight European languages , our approach results in an average absolute improvement of 10.4 % over a state - of - the - art baseline , and 16.7 % over vanilla hidden Markov models induced with the Expectation Maximization algorithm .","label":"CompareOrContrast","metadata":{},"score":"64.32602"}
{"text":"They use a German parse tree for moving German verbs towards the beginning of the clause .In this work , we investigate three languages - Spanish , English and German .The only additional language re ... . \" ...We address the problem of translating from morphologically poor to morphologically rich languages by adding per - word linguistic information to the source language .","label":"CompareOrContrast","metadata":{},"score":"64.41659"}
{"text":"Recursive Neural Networks for Parsing 3 .Theory : BackpropagaGon Through Structure 4 .ComposiGonal Vector Grammars : Parsing 5 .Recursive Autoencoders : Paraphrase DetecGon 6 .Recursive Neural Tensor Networks : SenGment Analysis 131 .( EMNLP 2011 )Reconstruction error Cross-­‐entropy error W(1 ) W(2 )","label":"CompareOrContrast","metadata":{},"score":"64.81803"}
{"text":"1 Our framework : the Moses MT system The open source Moses ( Koehn et al . , 2007 ) MT system was originally developed at the University . by Yee Seng Chan , Hwee Tou Ng - In 45th Annual Meeting of the Association for Computational Linguistics ( ACL-07 , 2007 . \" ...","label":"CompareOrContrast","metadata":{},"score":"65.55801"}
{"text":"From logistic regression to neural nets Part 1.2 : The Basics 21 .Demystifying neural networks Neural networks come with their own terminological baggage ... just like SVMs But if you understand how logisGc regression or maxent models work Then you already understand the operaGon of a basic neural network neuron !","label":"CompareOrContrast","metadata":{},"score":"65.72246"}
{"text":"In this paper , we address two issues that are related to domain adaptation .The first question is how much genre variation will affect NLP systems ' performance .We investigate the effect of genre variation on the performance of three NLP tools , namely , word segmenter , POS tagger , and parser .","label":"CompareOrContrast","metadata":{},"score":"65.75003"}
{"text":"This paper revisits an assump - tion that genre variation is continuous along multiple dimensions , and an early use of principal component analysis to find these dimensions .Results on a very heterogeneous corpus of post-1990s American English reveal four major dimensions , three of which echo those found in prior work and the fourth depending on features not used in the earlier study .","label":"CompareOrContrast","metadata":{},"score":"65.925415"}
{"text":"Our participation in the IWSLT 2005 speech translation task is our first effort to work on limited domain speech data .We adapted our statistical machine translation system that performed successfully in previous DARPA competitions on open domain text translations .We participated in the supplied co ... \" .","label":"CompareOrContrast","metadata":{},"score":"65.98602"}
{"text":"Layer - wise Unsupervised Pre - training ... ... input features 91 .Layer - wise Unsupervised Pre - training ... ... input features ...More abstract features 92 .Layer - Wise Unsupervised Pre - training Layer - wise Unsupervised Learning 93 . ... ... input features ...","label":"CompareOrContrast","metadata":{},"score":"66.1962"}
{"text":"We describe our system 's training and decoding methods in detail , and evaluate it for translation speed and translation accuracy .Using BLEU as a metric of translation accuracy , we find that our system performs significantly better than the Alignment Template System , a state - of - the - art phrasebased system . by Amittai Axelrod , Ra Birch Mayne , Chris Callison - burch , Miles Osborne , David Talbot - In Proc .","label":"CompareOrContrast","metadata":{},"score":"66.67326"}
{"text":"More abstract features ...Even more abstract features Layer - wise Unsupervised Learning 95 . ... ... input features ...More abstract features ...Even more abstract features Output f(X ) six Target Y two !Supervised Fine - Tuning 96 .","label":"CompareOrContrast","metadata":{},"score":"66.82209"}
{"text":"Tested across six domains , our system outperforms all non - oracle baselines including the best domain - independent parsing model .Thus , we are able to demonstrate the value of customizing parsing models to specific domains . ... train models in many different domains but sidestep the problem of domain detection .","label":"CompareOrContrast","metadata":{},"score":"67.596954"}
{"text":"If not , change model structure or make model \" larger \" 2 .Gradient Checks are Awesome !Implement your gradient 2 .Compare the two and make sure they are the same 182 , .General Strategy 1 .Select appropriate Network Structure 1 .","label":"CompareOrContrast","metadata":{},"score":"68.05455"}
{"text":"Recursive Neural Networks for Parsing 3 .OpGmizaGon and BackpropagaGon Through Structure 4 .ComposiGonal Vector Grammars : Parsing 5 .Recursive Autoencoders : Paraphrase DetecGon 6 .Recursive Neural Tensor Networks : SenGment Analysis 3 . ApplicaGons , Discussion , and Resources 19 .","label":"CompareOrContrast","metadata":{},"score":"68.08147"}
{"text":"To facilitate future research in unsupervised induction of syntactic structure and to standardize best - practices , we propose a tagset that consists of twelve universal part - of - speech categories .In addition to the tagset , we develop a mapping from 25 different treebank tagsets to this universal set .","label":"CompareOrContrast","metadata":{},"score":"68.20308"}
{"text":"\" ...We show how web mark - up can be used to improve unsupervised dependency parsing .Starting from raw bracketings of four common HTML tags ( anchors , bold , italics and underlines ) , we refine approximate partial phrase boundaries to yield accurate parsing constraints .","label":"CompareOrContrast","metadata":{},"score":"69.00655"}
{"text":"# 4 Learning multiple levels of representation Successive model layers learn deeper intermediate representaGons Layer 1 Layer 2 Layer 3 High-­‐level linguisGc representaGons [ Lee et al .ICML 2009 ; Lee et al .NIPS 2009 ] 12 .# 5 Why now ?","label":"CompareOrContrast","metadata":{},"score":"69.54848"}
{"text":"Ball Tracking :The reliable tracking of the ball is vital in robot soccer .Therefore a Kalman - filter based system for estimating the ball position and velocity in the presence of occlusions is developped . -Sensor Fusion : The robot perceives its environment through several independent sensors ( camera , odometer , etc . ) , which have different delays .","label":"CompareOrContrast","metadata":{},"score":"69.94397"}
{"text":"Word vectors as input to a neural network 47 .Training with Backpropagation DerivaGve of weight Wij : 53 x1 x2 x3+1 a1 a2 s U2 W23 .Training with Backpropagation DerivaGve of single weight Wij : Local error signal Local input signal 54 x1 x2 x3","label":"CompareOrContrast","metadata":{},"score":"70.613266"}
{"text":"LINGUIST is pleased to announce the launch of an exciting new feature : Easy Abstracts !Easy Abs is a free abstract submission and review facility designed to help conference organizers and reviewers accept and process abstracts online .With Easy Abstracts , submission and review will be as easy as 1 - 2 - 3 !","label":"CompareOrContrast","metadata":{},"score":"70.924515"}
{"text":"For English - Greek , we reduce the error on the verb conjugation from 19 % to 5.4 % and noun case agreement from 9 % to 6 % .We use syntax , not in 764Figure 2 : Classification of the errors on our EnglishGreek baseline system ( ch .","label":"CompareOrContrast","metadata":{},"score":"71.71618"}
{"text":"In order to investigate sparse training data scenarios , we also report results obtained on about 1 % of the original corpus .The source languages are Spanish and English and target languages are Spanish , English and German .We propose two types of reorderings depending on the language pair and the translation direction : local reorderings of nouns and adjectives for translation from and into Spanish and long - range reorderings of verbs for translation into German .","label":"CompareOrContrast","metadata":{},"score":"72.02151"}
{"text":"Select appropriate Network Structure 1 .Nonlinearity 2 .Check for implementaGon bugs with gradient check 3 .Parameter iniGalizaGon 4 .OpGmizaGon tricks 5 .If not , change model structure or make model \" larger \" 2 .Now , it 's Gme to regularize 188 .","label":"CompareOrContrast","metadata":{},"score":"72.13379"}
{"text":"MoGvaGon 2 .Recursive Neural Networks for Parsing 3 .Theory : BackpropagaGon Through Structure 4 .ComposiGonal Vector Grammars : Parsing 5 .Recursive Autoencoders : Paraphrase DetecGon 6 .Recursive Neural Tensor Networks : SenGment Analysis 140 .EnGty-­‐ Origin(e1,e2 )","label":"CompareOrContrast","metadata":{},"score":"72.23358"}
{"text":"We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task .For languages from different families the improvements often exceed 2 BLEU .Many of these gains are also significant in human evaluations .We present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages : German , English , Swedish , Spanish , French and Korean .","label":"CompareOrContrast","metadata":{},"score":"72.28066"}
{"text":"We 'd like have symbolic features like NP , VP , etc . and see why their combinaGon makes sense .","label":"CompareOrContrast","metadata":{},"score":"73.22563"}
{"text":"Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .On the other hand , our grammars are much more compact and substantially more accurate than previous work on automatic annotation .Despite its simplicity , our best grammar achieves an F1 of 89.9 % on the Penn Treebank , higher than most fully lexicalized systems .","label":"CompareOrContrast","metadata":{},"score":"73.44318"}
{"text":"This Year the LINGUIST List hopes to raise $ 65,000 .This money will go to help keep the List running by supporting all of our Student Editors for the coming year .See below for donation instructions , and do n't forget to check out our Space Fund Drive 2010 and join us for a great journey !","label":"CompareOrContrast","metadata":{},"score":"75.05765"}
{"text":"ComposiGonal Training Data 2 . Be0er ComposiGonal model .Part 3 1 .Assorted Speech and NLP ApplicaGons 2 .Deep Learning : General Strategy and Tricks 3 .Resources ( readings , code , ... ) 4 .Discussion 164 .","label":"CompareOrContrast","metadata":{},"score":"75.1255"}
{"text":"such can receive donations through the EMU Foundation , which is a registered .501(c ) Non Profit organization .Our Federal Tax number is 38 - 6005986 .These .donations can be offset against your federal and sometimes your state tax return .","label":"CompareOrContrast","metadata":{},"score":"75.125984"}
{"text":"Building on Word Vector Space Models 99 x2 x1 0 1 2 3 4 5 6 7 8 9 10 5 4 3 2 1 Monday 9 2 Tuesday 9.5 1.5 By mapping them into the same vector space ! 1 5 1.1 4 the country of my birth the place where I was born But how can we represent the meaning of longer phrases ?","label":"CompareOrContrast","metadata":{},"score":"75.35"}
{"text":"You can donate right now using our secure credit card form at https://linguistlist.org/donation/donate/donate1.cfm .Alternatively you can also pledge right now and pay later .To do so , go to : https://linguistlist.org/donation/pledge/pledge1.cfm .For all information on donating and pledging , including information on how to .","label":"CompareOrContrast","metadata":{},"score":"76.93436"}
{"text":"Five Reasons to Explore Deep Learning Part 1.1 : The Basics 4 . # 2 The need for distributed representations Current NLP systems are incredibly fragile because of their atomic symbol representaGons Crazy senten@al complement , such as for \" likes [ ( being ) crazy ] \" 6 .","label":"CompareOrContrast","metadata":{},"score":"78.31619"}
{"text":"Parsing a sentence 9 1 5 3 5 2 Neural \" Network \" 1.1 2 1 The cat sat on the mat .Neural \" Network \" 0.1 2 0 Neural \" Network \" 0.4 1 0 Neural \" Network \" 2.3 3 3 5 3 8 5 9 1 4 3 7 1 109 .","label":"CompareOrContrast","metadata":{},"score":"79.82134"}
{"text":"For more information visit the IRS Web - Site , or contact .your financial advisor .Many companies also offer a gift matching program , such that they will match .any gift you make to a non - profit organization .","label":"CompareOrContrast","metadata":{},"score":"80.39285"}
{"text":"We present a new edition of the Google Books Ngram Corpus , which describes how often words and phrases were used over a period of five centuries , in eight languages ; it reflects 6 % of all books ever published .","label":"CompareOrContrast","metadata":{},"score":"80.417694"}
{"text":"In this paper , we address two issues that are related to domain adaptation .The first question is how much genre variation will affect NLP systems ' per ... \" .Domain adaptation is an important task in order for NLP systems to work well in real applications .","label":"CompareOrContrast","metadata":{},"score":"80.75659"}
{"text":"Submissions should be in English and should not have been published previously .If essentially identical papers are submitted to other conferences or workshops as well , this fact must be indicated at submission time .The extended submission deadline is 23:59 CET on April 11 , 2010 ( Sunday ) .","label":"CompareOrContrast","metadata":{},"score":"81.92665"}
{"text":"Message-­‐ Topic(e2,e1 ) Roadside [ a0racGons]e1 are frequently adverGsed with [ billboards]e2 to a0ract tourists .Data : Movie Reviews Stealing Harvard does n't care about cleverness , wit or any other kind of intelligent humor .There are slow and repeGGve parts but it has just enough spice to keep it interesGng .","label":"CompareOrContrast","metadata":{},"score":"85.923996"}
{"text":"MoGvaGon 2 .Recursive Neural Networks for Parsing 3 .OpGmizaGon and BackpropagaGon Through Structure 4 .ComposiGonal Vector Grammars : Parsing 5 .Recursive Autoencoders : Paraphrase DetecGon 6 .Recursive Neural Tensor Networks : SenGment Analysis 102 .Sentence Parsing : What we want 9 1 5 3 8 5 9 1 4 3 NP NP PP S 7 1 VP The cat sat on the mat .","label":"CompareOrContrast","metadata":{},"score":"86.7305"}
{"text":"We created hand - written rules that move the Japanese verb from the end of the sentence to the beginning .However , we could not consistently achieve improvements using these rules .Since we did not ... . by Josh Schroeder - Prague , Czech Republic .","label":"CompareOrContrast","metadata":{},"score":"89.81061"}
{"text":"How should we map phrases into a vector space ?Models in this secGon can jointly learn parse trees and composiGonal vector representaGons x2 x1 0 1 2 3 4 5 6 7 8 9 10 5 4 3 2 1 the country of my birth the place where I was born Monday Tuesday France Germany 100 .","label":"CompareOrContrast","metadata":{},"score":"96.40281"}
{"text":"Coastal would n't disclose the terms Sales grew almost 7 % to $ UNK m. from $ UNK m. 1 .Sales rose more than 7 % to $ 94.9 m. from $ 88.3 m. 2 .Sales surged 40 % to UNK b. yen from UNK b. \" .","label":"CompareOrContrast","metadata":{},"score":"100.39428"}
{"text":"Invited speaker John Blitzer , University of California , United States .Organization Hal Daumé III , University of Utah , USA Tejaswini Deoskar , University of Amsterdam , The Netherlands David McClosky , Stanford University , USA Barbara Plank , University of Groningen , The Netherlands Jörg Tiedemann , Uppsala University , Sweden .","label":"CompareOrContrast","metadata":{},"score":"100.923355"}
{"text":"This Master 's thesis describes parts of the control software used by the soccer robots of the Free University of Berlin , the so called FU - Fighters .The FU - Fighters compete in the Middle Sized League of RoboCup and reached the semi - finals during the 2004 RoboCup World Cup in Lisbon , Portugal .","label":"CompareOrContrast","metadata":{},"score":"101.69728"}
