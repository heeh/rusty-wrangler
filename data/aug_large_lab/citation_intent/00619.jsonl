{"text":"This paper describes a simple yet novel method for constructing sets of 50-best parses based on a co ... \" .Discriminative reranking is one method for constructing high - performance statistical parsers ( Collins , 2000 ) .A discriminative reranker requires a source of candidate parses for each sentence .","label":"CompareOrContrast","metadata":{},"score":"20.4463"}{"text":"This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence .","label":"CompareOrContrast","metadata":{},"score":"21.270496"}{"text":"Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand - annotated training data . \" ...Discriminative reranking is one method for constructing high - performance statistical parsers ( Collins , 2000 ) .","label":"CompareOrContrast","metadata":{},"score":"22.875578"}{"text":"In addition , we utilize the RankBoost - based reranking algorithm to rerank the N - best outputs of the HMM - based tagger using various $ n$-gram , morphological , and dependency features .Two methods are proposed to improve the generalization performance of the reranking algorithm .","label":"CompareOrContrast","metadata":{},"score":"24.179356"}{"text":"A second model then attempts to improve upon this i ... \" .This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .","label":"CompareOrContrast","metadata":{},"score":"25.119516"}{"text":"This paper explores a parsimonious approach to Data - Oriented Parsing .While allowing , in principle , all possible subtrees of trees in the treebank to be productive elements , our approach aims at finding a manageable subset of these trees that can accurately describe empirical distributions over phrase - structure trees .","label":"CompareOrContrast","metadata":{},"score":"27.08681"}{"text":"In our approach ... \" .In this paper we present a general parsing strategy that arose from the development of an Earicy - type parsing algorithm for TAGs ( Schabes and Joshi 1988 ) and from recent linguistic work in TAGs ( Abeille 1988 ) .","label":"CompareOrContrast","metadata":{},"score":"27.235525"}{"text":"This paper presents a new approach to phrase - level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions .With this approach , the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions , achieving results that are significantly better than baseline . ...","label":"CompareOrContrast","metadata":{},"score":"27.473743"}{"text":"We evaluate the proposed algorithms on the 2007 CONLL Shared Task , and report errors analysis .Experimental results show that the system score is better than the average score among the participating systems .In the paper we describe a dependency parser that uses exact search and global learning ( Crammer et al . , 2006 ) to produce labelled dependency trees .","label":"CompareOrContrast","metadata":{},"score":"27.71724"}{"text":"In this paper we present new experiments to test this claim .We use the PARSEVAL metric , the Leaf - Ancestor metric as well as a dependency - based evaluation , and present novel approaches measuring the effect of controlled error insertion on treebank trees and parser output .","label":"CompareOrContrast","metadata":{},"score":"28.289898"}{"text":"We introduce an approximate inference method using Tree - based Reparameterization ( TRP ) to reduce computational cost .In experiments , our proposed model obtained significant improvements compare to baseline models that use Support Vector Machines .We introduce a technique for identifying the most salient participants in a discussion .","label":"CompareOrContrast","metadata":{},"score":"28.517487"}{"text":"The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this i ... \" .This article considers approaches which rerank the output of an existing probabilistic parser .","label":"CompareOrContrast","metadata":{},"score":"28.617321"}{"text":"In the domain adaptation track , we use two models to parse unlabeled data in the target domain to supplement the labeled out - of - domain training set , in a scheme similar to one iteration of co - training .","label":"CompareOrContrast","metadata":{},"score":"28.844486"}{"text":"use a more global discriminative model to rerank those candidates .However , these methods fail when the correct output is pruned away in the first pass .Closest to our proposal are gradient - descent methods that adjust the parameters of all of the local classifiers ... . ... ns , or quotation marks ) and have the same label 15 as a constituent in the treebank parse .","label":"CompareOrContrast","metadata":{},"score":"29.078354"}{"text":"Deterministic parsing has emerged as an effective alternative for complex parsing algorithms which search the entire search space to get the best probable parse tree .In this paper , we present an online large margin based training framework for deterministic parsing using Nivre 's Shift - Reduce parsing algorithm .","label":"CompareOrContrast","metadata":{},"score":"29.107956"}{"text":"A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence .The strength of our approach is that it allows a tree to be represented as an arbitrary set of features , without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account .","label":"CompareOrContrast","metadata":{},"score":"29.151001"}{"text":"We introduce a new method for the reranking task , based on the boosting approach to ranking problems described in Freund et al .( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .","label":"CompareOrContrast","metadata":{},"score":"29.269424"}{"text":"Our results suggest that our bootstrapping methods have considerable potential , and could be used to semi - automate an approach based on incremental manual annotation .In this paper , we consider the computational modelling of human plausibility judgements for verb - relation - argument triples , a task equivalent to the computation of selectional preferences .","label":"CompareOrContrast","metadata":{},"score":"29.323177"}{"text":"We describe new lookup algorithms for hierarchical phrase - based translation that reduce the empirical computation time by nearly two orders of magnitude , making on - the - fly lookup feasible for source phrases with gaps .This paper presents an empirical study on how different selections of input translation systems affect translation quality in system combination .","label":"CompareOrContrast","metadata":{},"score":"29.413929"}{"text":"We describe an incremental parser that was trained to minimize cost over sentences rather than over individual parsing actions .This is an attempt to use the advantages of the two top - scoring systems in the CoNLL - X shared task .","label":"CompareOrContrast","metadata":{},"score":"29.754486"}{"text":"In this paper , we present a three - step multi - lingual dependency parser based on a deterministic shift - reduce parsing algorithm .Different from last year , we separate the root - parsing strategy as sequential labeling task and try to link the neighbor word dependences via a near neighbor parsing .","label":"CompareOrContrast","metadata":{},"score":"29.821302"}{"text":"We present an information extraction system that decouples the tasks of finding relevant regions of text and applying extraction patterns .We create a self - trained relevant sentence classifier to identify relevant regions , and use a semantic affinity measure to automatically learn domain - relevant extraction patterns .","label":"CompareOrContrast","metadata":{},"score":"30.0476"}{"text":"We integrate these probabilities into the framework of fully - lexicalized parsing based on large - scale case frames .This approach simultaneously addresses two tasks of coordination disambiguation : the detection of coordinate conjunctions and the scope disambiguation of coordinate structures .","label":"CompareOrContrast","metadata":{},"score":"30.537117"}{"text":"The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .","label":"CompareOrContrast","metadata":{},"score":"30.716873"}{"text":"The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .","label":"CompareOrContrast","metadata":{},"score":"30.716873"}{"text":"The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .","label":"CompareOrContrast","metadata":{},"score":"30.716888"}{"text":"( 2005 ) , obtain very high accuracy on standard dependency parsing tasks and can be trained and applied without marginalization , ' ' summing trees ' ' permits some alternative techniques of interest .Using the summing algorithm , we present experimental results on four nonprojective languages , for maximum conditional likelihood estimation , minimum Bayes - risk parsing , and hidden variable training .","label":"CompareOrContrast","metadata":{},"score":"30.72705"}{"text":"Based on an extension to Harris 's distributional hypothesis , we use selectional preferences to gather evidence of inference directionality and plausibility .Experiments show empirical evidence that our approach can classify inference rules significantly better than several baselines .This paper assesses the role of multi - label classification in modelling polysemy for language acquisition tasks .","label":"CompareOrContrast","metadata":{},"score":"30.729092"}{"text":"In this paper , we describe a two - stage multilingual dependency parser used for the multilingual track of the CoNLL 2007 shared task .The system consists of two components : an unlabeled dependency parser using Gibbs sampling which can incorporate sentence - level ( global ) features as well as token - level ( local ) features , and a dependency relation labeling module based on Support Vector Machines .","label":"CompareOrContrast","metadata":{},"score":"30.841156"}{"text":"We describe our submission to the domain adaptation track of the CoNLL07 shared task in the open class for systems using external resources .Our main finding was that it was very difficult to map from the annotation scheme used to prepare training and development data to one that could be used to effectively train and adapt the RASP system unlexicalised parse ranking model .","label":"CompareOrContrast","metadata":{},"score":"30.85461"}{"text":"We introduce a general framework for answer extraction which exploits semantic role annotations in the FrameNet paradigm .We view semantic role assignment as an optimization problem in a bipartite graph and answer extraction as an instance of graph matching .Experimental results on the TREC datasets demonstrate improvements over state - of - the - art models .","label":"CompareOrContrast","metadata":{},"score":"30.894096"}{"text":"The appropriate output transformation for a given task can be selected by applying a hill - climbing approach to held - out data .On the NP Chunking task , our hill - climbing system finds a model structure that outperforms both first - order and second - order models with the same input feature set .","label":"CompareOrContrast","metadata":{},"score":"31.344292"}{"text":"Our results provide the first known empirical evidence that lexical semantics are indeed useful for SMT , despite claims to the contrary .This paper presents a tree - to - tree transduction method for text rewriting .Our model is based on synchronous tree substitution grammar , a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches .","label":"CompareOrContrast","metadata":{},"score":"31.35294"}{"text":"In this paper we address the problem of multiple citation concept alignment by combining and modifying the CRF based pairwise word alignment system of Blunsom & Cohn ( 2006 ) and a posterior decoding based multiple sequence alignment algorithm of Schwartz & Pachter ( 2007 ) .","label":"CompareOrContrast","metadata":{},"score":"31.411594"}{"text":"However , increasing a model 's order can lead to an increase in the number of model parameters , making the model more susceptible to sparse data problems .This paper shows how the notion of output transformation can be used to explore a variety of alternative model structures .","label":"CompareOrContrast","metadata":{},"score":"31.519917"}{"text":"We apply the proposed approach to enhance opinion sum - marization in a two - stage framework .Experimental results show that the proposed approach effectively ( 1 ) discriminates low - quality reviews from high - quality ones and ( 2 ) enhances the task of opinion summarization by detecting and filtering low - quality reviews .","label":"CompareOrContrast","metadata":{},"score":"31.53443"}{"text":"This paper describes a corpus - based study of plural descriptions , and proposes a psycholinguistically - motivated algorithm for plural reference generation .The descriptive strategy is based on partitioning , and incorporates corpus - derived heuristics .An exhaustive evaluation shows that the output closely matches human data .","label":"CompareOrContrast","metadata":{},"score":"31.54115"}{"text":"We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm , and we give experimental results on the ATIS corpus of parse trees . ... lems .The method is derived by the transformation from ranking problems to a margin - based classification problem in [ 8].","label":"CompareOrContrast","metadata":{},"score":"31.692593"}{"text":"We present a new generative alignment model which avoids these structural limitations , and show that it is effective when trained using both unsupervised and semi - supervised training methods .Experiments show strong improvements in word alignment accuracy and usage of the generated alignments in hierarchical and phrasal SMT systems increases the BLEU score .","label":"CompareOrContrast","metadata":{},"score":"31.730175"}{"text":"We show how the algorithms can be efficiently applied to exponential sized representations of parse trees , such as the \" all subtrees \" ( DOP ) representation described by ( Bod 98 ) , or a representation tracking all sub - fragments of a tagged sentence .","label":"CompareOrContrast","metadata":{},"score":"31.785694"}{"text":"We show how the algorithms can be efficiently applied to exponential sized representations of parse trees , such as the \" all subtrees \" ( DOP ) representation described by ( Bod 98 ) , or a representation tracking all sub - fragments of a tagged sentence .","label":"CompareOrContrast","metadata":{},"score":"31.785702"}{"text":"This view leads to a single algorithmic framework for the three problems .We prove worst case loss bounds for various algorithms for both the realizable case and the non - realizable case .The end result is new algorithms and accompanying loss bounds for hinge - loss regression and uniclass .","label":"CompareOrContrast","metadata":{},"score":"31.81855"}{"text":"Recent efforts have tried to overcome this issue by using statistics from speech lattices instead of only the 1-best transcripts ; however , these efforts have invariably used the classical vector space retrieval model .This paper presents a novel approach to lattice - based spoken document retrieval using statistical language models : a statistical model is estimated for each document , and probabilities derived from the document models are directly used to measure relevance .","label":"CompareOrContrast","metadata":{},"score":"32.103672"}{"text":"We argue that bootstrapping a parser is most promising when the model uses a rich set of redundant features , as in recent models for scoring dependency parses ( McDonald et al . , 2005 ) .Drawing on Abney 's ( 2004 ) analysis of the Yarowsky algorithm , we perform bootstrapping by entropy regularization : we maximize a linear combination of conditional likelihood on labeled data and confidence ( negative Renyi entropy ) on unlabeled data .","label":"CompareOrContrast","metadata":{},"score":"32.130524"}{"text":"This method generates 50-best lists that are of substantially higher quality than previously obtainable . ...m search , keeping some large number of possibilities to extend by adding the next word , and then re - pruning .","label":"CompareOrContrast","metadata":{},"score":"32.166298"}{"text":"This paper presents a novel unsupervised support vector machine ( U - SVM ) classifier for answer selection , which is independent of language and does not require hand - tagged training pairs .The key ideas are the following : 1 . unsupervised learning of training data for the classifier by clustering web search results ; and 2 . selecting the answer from the candidates by classifying the question .","label":"CompareOrContrast","metadata":{},"score":"32.37527"}{"text":"In order to analyze each node of a parse tree to rank parse trees , a parser must have a method of \" visiting \" each node .In other words , the nodes are examined in a particular order .A \" depth first tree walk \" is a typical method of visiting all the nodes in a parse tree .","label":"CompareOrContrast","metadata":{},"score":"32.42739"}{"text":"In order to analyze each node of a parse tree to rank parse trees , a parser must have a method of \" visiting \" each node .In other words , the nodes are examined in a particular order .A \" depth first tree walk \" is a typical method of visiting all the nodes in a parse tree .","label":"CompareOrContrast","metadata":{},"score":"32.42739"}{"text":"This factorization provides conceptual simplicity , straightforward opportunities for separately improving the component models , and a level of performance comparable to similar , non - factored models .This pruning is done for efficiency ; the question is whether it is hurting accuracy .","label":"CompareOrContrast","metadata":{},"score":"32.543686"}{"text":"Our model is inspired by theories of local coherence and formulated within the framework of Integer Linear Programming .Experimental results show significant improvements over a state - of - the - art discourse agnostic approach .Shallow semantic parsing , the automatic identification and labeling of sentential constituents , has recently received much attention .","label":"CompareOrContrast","metadata":{},"score":"32.62211"}{"text":"We can not use non - local features due to concerns about complexity with current major methods of sequence labeling such as CRFs .We propose a new perceptron algorithm that can use non - local features .Our algorithm allows the use of all types of non - local features whose values are determined from the sequence and the labels .","label":"CompareOrContrast","metadata":{},"score":"32.63702"}{"text":"More generally , we consider problems involving multiple dependent output variables , structured output spaces , and classification problems with class attributes .In order to accomplish this , we propose to appropriately generalize the well - known notion of a separation margin and derive a corresponding maximum - margin formulation .","label":"CompareOrContrast","metadata":{},"score":"32.7424"}{"text":"We present a novel generative model for natural language tree structures in which semantic ( lexical dependency ) and syntactic ( PCFG ) structures are scored with separate models .This factorization provides conceptual simplicity , straightforward opportunities for separately improving the component mod ... \" .","label":"CompareOrContrast","metadata":{},"score":"32.832195"}{"text":"Our best result , 91.44 % accuracy , reflects a 25 % reduction in error rate compared with the previous state of the art .We present a new approach to automatic summarization based on neural nets , called NetSum .We extract a set of features from each sentence that helps identify its importance in the document .","label":"CompareOrContrast","metadata":{},"score":"32.885826"}{"text":"A key idea is to introduce non - standard CCG combinators that relax certain parts of the grammar --- for example allowing flexible word order , or insertion of lexical items --- with learned costs .We also present a new , online algorithm for inducing a weighted CCG .","label":"CompareOrContrast","metadata":{},"score":"33.063404"}{"text":"Our approach recovers non - local dependencies at the level of Lexical - Functional Grammar f - structures , using automatically acquired subcategorisation frames and f - structure paths linking antecedents and traces in NLDs .Currently our algorithm achieves 92.2 % f - score for trace insertion and 84.3 % for antecedent recovery evaluating on gold - standard CTB trees , and 64.7 % and 54.7 % , respectively , on CTBtrained state - of - the - art parser output trees .","label":"CompareOrContrast","metadata":{},"score":"33.25981"}{"text":"Tools . \" ...This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .","label":"CompareOrContrast","metadata":{},"score":"33.25991"}{"text":"We present and evaluate here several methods that integrate LSA - based information with a standard language model : a semantic cache , partial reranking , and different forms of interpolation .We found that all methods show significant improvements , compared to the 4-gram baseline , and most of them to a simple cache model as well .","label":"CompareOrContrast","metadata":{},"score":"33.338875"}{"text":"In this study we show that analogical learning offers as well an elegant and effective solution to the problem of identifying potential translations of unknown words .We present a probabilistic model of diachronic phonology in which individual word forms undergo stochastic edits along the branches of a phylogenetic tree .","label":"CompareOrContrast","metadata":{},"score":"33.505955"}{"text":"We analyze the effect of resampling techniques , including under - sampling and over - sampling used in active learning .Experimental results show that under - sampling causes negative effects on active learning , but over - sampling is a relatively good choice .","label":"CompareOrContrast","metadata":{},"score":"33.515892"}{"text":"A notable gap in research on statistical dependency parsing is a proper conditional probability distribution over nonprojective dependency trees for a given sentence .We exploit the Matrix Tree Theorem ( Tutte , 1984 ) to derive an algorithm that efficiently sums the scores of all nonprojective trees in a sentence , permitting the definition of a conditional log - linear model over trees .","label":"CompareOrContrast","metadata":{},"score":"33.547054"}{"text":"Instead of passively accepting random training examples , the active learning algorithm iteratively selects unlabeled examples for the user to label , so that human effort is focused on labeling the most \" useful \" examples .Our method relies on the idea of uncertainty sampling , in which the algorithm selects unlabeled examples that it finds hardest to classify .","label":"CompareOrContrast","metadata":{},"score":"33.861633"}{"text":"This allows Support Vector Machines to discern between correct and incorrect predicate structures and to re - rank them based on the joint probability of their arguments .Experiments on the PropBank data show that both classification and re - ranking based on tree kernels can improve SRL systems . .","label":"CompareOrContrast","metadata":{},"score":"34.430557"}{"text":"In parsing we would have training examples fs i ; t i g where e .. \" ...This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm .We show how the algorithms can be efficiently applied to exponential sized representations of parse trees , such as the \" all subtrees \" ( DOP ) representation described by ( Bod 9 ... \" .","label":"CompareOrContrast","metadata":{},"score":"34.49775"}{"text":"Therefore , we use ancestor - descendant relation in addition to parent - child relation , so that the added redundancy helps errors be corrected .Experimental results show that the proposed method achieves higher accuracy .We propose a sequence - alignment based method for detecting and disambiguating coordinate conjunctions .","label":"CompareOrContrast","metadata":{},"score":"34.627686"}{"text":"We treat the graph as a Markov chain and compute a word - specific stationary distribution via a generalized PageRank algorithm .Semantic relatedness of a word pair is scored by a novel divergence measure , ZKL , that outperforms existing measures on certain classes of distributions .","label":"CompareOrContrast","metadata":{},"score":"34.640923"}{"text":"This paper focuses on the domain estimation problem in statistical machine translations .In the proposed method , a training corpus , which is a bilingual corpus , is automatically clustered to sub - corpuses .Each sub - corpus is regarded as a domain .","label":"CompareOrContrast","metadata":{},"score":"34.653675"}{"text":"This framework integrates multiple MT systems ' output at the word- , phrase- and sentence- levels .By boosting common word and phrase translation pairs , pruning unused phrases , and exploring decoding paths adopted by other MT systems , this framework achieves better translation quality with much less re - decoding time .","label":"CompareOrContrast","metadata":{},"score":"34.67565"}{"text":"Three different classifiers are trained to predict weighted soft - constraints on parts of the complex output .From these constraints , a standard weighted constraint satisfaction problem can be formed , the solution to which is a valid dependency tree .","label":"CompareOrContrast","metadata":{},"score":"34.708557"}{"text":"Rather than using syntactic features to augment existing statistical classifiers ( as in previous work ) , we build on the idea that questions and their ( correct ) answers relate to each other via loose but predictable syntactic transformations .We propose a probabilistic quasi - synchronous grammar , inspired by one proposed for machine translation ( D. Smith and Eisner , 2006 ) , and parameterized by mixtures of a robust non - lexical syntax / alignment model with a(n optional ) lexical - semantics - driven log - linear model .","label":"CompareOrContrast","metadata":{},"score":"34.75177"}{"text":"Furthermore , we explore the performance of information drawn from different levels of linguistic description , using feature sets based on morphology , syntax , semantics , and n -gram distribution .Finally , we demonstrate that ensemble classifiers are a powerful and adequate way to combine different types of linguistic evidence : a simple , majority voting ensemble classifier improves the accuracy from 62.5 % ( best single classifier ) to 84 % .","label":"CompareOrContrast","metadata":{},"score":"34.757423"}{"text":"We present experiments with a dependency parsing model defined on rich factors .Our model represents dependency trees with factors that include three types of relations between the tokens of a dependency and their children .We extend the projective parsing algorithm of Eisner ( 1996 ) for our case , and train models using the averaged perceptron .","label":"CompareOrContrast","metadata":{},"score":"34.94494"}{"text":"In addition , the system can access the schema associated with the application program .During run time , the system can discard parsed hypotheses ( as they are being generated ) if they are found to violate the structure imposed by the schema .","label":"CompareOrContrast","metadata":{},"score":"35.010563"}{"text":"The strength of our approach is that it allows a tree to be represented as an arbitrary set of features , without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account .","label":"CompareOrContrast","metadata":{},"score":"35.037193"}{"text":"A method of parsing a phrase to facilitate processing of such phrase by a computer , the method comprising : . generating at least one parse tree representing a syntactically valid parse of the phrase , wherein the parse tree has hierarchical nodes ; . calculating a syntactic history for each node , the syntactic history of a node is indicative of a relevant grammatical environment of that node .","label":"CompareOrContrast","metadata":{},"score":"35.122887"}{"text":"The constraints that are enforced can relate to the constituent boundaries and the semantic tags for the constituents .In another embodiment , the SLM operates in a left - to - right , bottom - up fashion and generates binary trees .","label":"CompareOrContrast","metadata":{},"score":"35.18817"}{"text":"Regarding the former choice , structural properties of full syntactic parses are largely employed as they represent ways to encode different principles suggested by the linking theory between syntax and semantics .The latter choice relates to several learning schemes over global views of the parses .","label":"CompareOrContrast","metadata":{},"score":"35.221375"}{"text":"Empirical results show that a near - optimal set of hyperparameters can be identified by our approach with very few training rounds and gradient computations . \" ...Lexical databases are invaluable sources of knowledge about words and their meanings , with numerous applications in areas like NLP , IR , and AI .","label":"CompareOrContrast","metadata":{},"score":"35.242622"}{"text":"Furthermore , the combination of parse trees can compensate for the reordering errors caused by single parse tree .Finally , experimental results show that the performance of our system is superior to that of the state - of - the - art phrase - based SMT system .","label":"CompareOrContrast","metadata":{},"score":"35.300106"}{"text":"The first stage consists in tuning a single - parser system for each language by optimizing parameters of the parsing algorithm , the feature model , and the learning algorithm .The second stage consists in building an ensemble system that combines six different parsing strategies , extrapolating from the optimal parameters settings for each language .","label":"CompareOrContrast","metadata":{},"score":"35.71959"}{"text":"An adaptation technique is used to avoid this problem .The second problem is domain estimation .For adaptation , the domain must be given in advance .However , in many cases , the domain is not given or changes dynamically .","label":"CompareOrContrast","metadata":{},"score":"35.88069"}{"text":"We find that the use of a decision tree improves on the basic approach only for the deep parser - based approach .We also show that combining both the shallow and deep decision tree features is effective .Our evaluation is carried out using a large test set of grammatical and ungrammatical sentences .","label":"CompareOrContrast","metadata":{},"score":"35.88456"}{"text":"The system has been used to parse many well known corpora in order to produce data for lexical acquisition efforts ; it has also been used as a component in an open - domain question answering project .The performance of the system is competitive with that of statistical parsers using highly lexicalised parse selection models .","label":"CompareOrContrast","metadata":{},"score":"35.94265"}{"text":"We present two techniques for training the MST parser : tree - normalized and graph - normalized conditional training .We describe the reranker features which include non - projective edge attributes .We provide an analysis of the errors made by our system and suggest changes to the models and features that might rectify the current system .","label":"CompareOrContrast","metadata":{},"score":"35.96924"}{"text":"A method for determining a statistical goodness measure ( SGM ) of a parse tree representing a parse of a phrase , the parse tree comprising one or more nodes , the method comprising : . combining probabilities of each node in the parse tree , wherein the probabilities of each node are determined by the steps comprising : . receiving language - usage probabilities based upon appearances of instances of combinations of linguistic features within a training corpus ; . calculating the probabilities of each node based upon linguistic features of each node and the language - usage probabilities ; . wherein during the combining , the probabilities of each node in the parse tree are combined in a top - down , generative approach .","label":"CompareOrContrast","metadata":{},"score":"36.06063"}{"text":"A conventional approach is to use a \" goodness \" function to calculate a \" goodness measure \" of each valid parse .Existing parsers differ in the extent to which they rely on a goodness function , but most parsers utilize one .","label":"CompareOrContrast","metadata":{},"score":"36.096928"}{"text":"A conventional approach is to use a \" goodness \" function to calculate a \" goodness measure \" of each valid parse .Existing parsers differ in the extent to which they rely on a goodness function , but most parsers utilize one .","label":"CompareOrContrast","metadata":{},"score":"36.096928"}{"text":"Significant improvement over the previous results in the literature is reported as well as a new benchmark dataset is introduced .Semi - supervised algorithms perform better than their supervised version by a wide margin especially when the amount of labeled data is limited .","label":"CompareOrContrast","metadata":{},"score":"36.15769"}{"text":"Under the assumption that the labeled examples are selected randomly from the positive examples , we show that a classifier trained on positive and unlabeled examples predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive .","label":"CompareOrContrast","metadata":{},"score":"36.19586"}{"text":"We propose a simple algorithm with convergence proofs to solve the model and obtain individual skill .Experiments on synthetic and real data demonstrate that the algorithm is useful for obtaining multi - class probability estimates .Moreover , we discuss four extensions of the proposed model : 1 ) weighted individual skill , 2 ) home - field advantage , 3 ) ties , and 4 ) comparisons with more than two teams .","label":"CompareOrContrast","metadata":{},"score":"36.232704"}{"text":"This allows for an efficient and elegant method for computing the goodness function .In the exemplar , the headwards can also be replaced by their lemmas .A training corpus of approximately 30,000 sentences is used to initially calculate the conditioned probabilities of factors such as transition name , headword , syntactic bigrams , phrase level , and syntactic history .","label":"CompareOrContrast","metadata":{},"score":"36.322"}{"text":"This allows for an efficient and elegant method for computing the goodness function .In the exemplar , the headwards can also be replaced by their lemmas .A training corpus of approximately 30,000 sentences is used to initially calculate the conditioned probabilities of factors such as transition name , headword , syntactic bigrams , phrase level , and syntactic history .","label":"CompareOrContrast","metadata":{},"score":"36.322"}{"text":"We present results that show that incorporating lexical and structural semantic information is effective for word sense disambiguation .We evaluated the method by using precise information from a large treebank and an ontology automatically created from dictionary sentences .Exploiting these information improves precision 2 - 3 % , especially 5.7 % for verb , over a model using only bag of words and n - gram features .","label":"CompareOrContrast","metadata":{},"score":"36.44673"}{"text":"This represents a 13 % decrease in error rate over the best single - parser results on this corpus [ 9].The major technical innova- tion is the use of a \" maximum - entropy - inspired \" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events .","label":"CompareOrContrast","metadata":{},"score":"36.459133"}{"text":"Current phrase - based SMT technologies are good at capturing local reordering but not global reordering .This paper introduces syntactic knowledge to improve global reordering capability of SMT system .Syntactic knowledge such as boundary words , POS information and dependencies is used to guide phrase reordering .","label":"CompareOrContrast","metadata":{},"score":"36.47957"}{"text":"We define the objective function of our hybrid model , which is written in log - linear form , by discriminatively combining discriminative structured predictor(s ) with generative model(s ) that incorporate unlabeled data .Then , unlabeled data is used in a generative manner to increase the sum of the discriminant functions for all outputs during the parameter estimation .","label":"CompareOrContrast","metadata":{},"score":"36.591244"}{"text":"In particular , it is important to see if the classification could accommodate new words from heterogeneous data sources , and whether simple similarity measures and clustering methods could cope with such variation .We use the cosine function for similarity and test it on automatically classifying 120 target words from four regions , using different datasets for the extraction of feature vectors .","label":"CompareOrContrast","metadata":{},"score":"36.60115"}{"text":"The increasing use of large open - domain document sources is exacerbating the problem of ambiguity in named entities .This paper explores the use of a range of syntactic and semantic features in unsupervised clustering of documents that result from ad hoc queries containing names .","label":"CompareOrContrast","metadata":{},"score":"36.626038"}{"text":"We also address the issue whether a corpus annotated by means of AL -- using a particular classifier and a particular feature set -- can be re - used to train classifiers different from the ones employed by AL , supplying alternative feature sets as well .","label":"CompareOrContrast","metadata":{},"score":"36.66879"}{"text":"This view leads to a single algorithmic framework for the three problems .We prove worst case loss bounds for various algorithms for both the realizable case and the non - realizable case .The end result is new alg ... \" .","label":"CompareOrContrast","metadata":{},"score":"36.688084"}{"text":"In our approach , proper nouns are expanded into new queries aimed at maximizing the probability of retrieving transliterations from existing search engines .The method involves learning the sublexical relationships between names and their transliterations .At run - time , a given name is automatically extended into queries with relevant morphemes , and transliterations in the returned search snippets are extracted and ranked .","label":"CompareOrContrast","metadata":{},"score":"36.7041"}{"text":"We present a comparative error analysis of the two dominant approaches in data - driven dependency parsing : global , exhaustive , graph - based models , and local , greedy , transition - based models .We show that , in spite of similar performance overall , the two models produce different types of errors , in a way that can be explained by theoretical properties of the two models .","label":"CompareOrContrast","metadata":{},"score":"36.756554"}{"text":"The issues of consistency of argument structure across both polysemous and synonymous verbs are also discussed and we present our actual guidelines for these types of phenomena , along with numerous examples of tagged sentences and verb frames .We conclude with a summary of the current status of annotation process .","label":"CompareOrContrast","metadata":{},"score":"36.863007"}{"text":"W . k .T . k . ) which ensures a proper probability over strings W , where S k is the set of all parses present in our stacks at the current stage k. .Each model component - word - predictor , tagger , parser - is initialized from a set of parsed sentences after undergoing headword percolation and binarization .","label":"CompareOrContrast","metadata":{},"score":"36.887726"}{"text":"We briefly describe each model , highlighting points where they differ .We include a quantitative comparison of the phrase pairs that each model has to work with , as well as the reasons why some phrase pairs are not learned by the syntax - based model .","label":"CompareOrContrast","metadata":{},"score":"36.950832"}{"text":"combining probabilities of each node in the parse tree , wherein the probabilities of each node are determined by the steps comprising : . receiving language - usage probabilities based upon appearances of instances of combinations of linguistic features within a training corpus ; . calculating the probabilities of each node based upon linguistic features of each node and the language - usage probabilities .","label":"CompareOrContrast","metadata":{},"score":"37.172356"}{"text":"To determine the parse with the highest probability of being correct ( i.e. , the highest goodness measure ) , each branch of each parse tree is given a probability .These probabilities are generated based upon a large database of correctly parsed sentences ( i.e. , \" training corpus \" ) .","label":"CompareOrContrast","metadata":{},"score":"37.223335"}{"text":"To determine the parse with the highest probability of being correct ( i.e. , the highest goodness measure ) , each branch of each parse tree is given a probability .These probabilities are generated based upon a large database of correctly parsed sentences ( i.e. , \" training corpus \" ) .","label":"CompareOrContrast","metadata":{},"score":"37.223335"}{"text":"This paper presents two approaches for obtaining class probabilities .Both methods can be reduced to linear systems and are easy to implement .Pairwise coupling is a popular multi - class classification method that combines together all pairwise comparisons for each pair of classes .","label":"CompareOrContrast","metadata":{},"score":"37.250774"}{"text":"There is no explicit grammar given that would produce the bracketing for the parses in the Penn Tree Bank .The great majority of those working on the Penn Tree Bank computationally induce a grammar from the annotation .The number of transitions so induced generally run to the thousands .","label":"CompareOrContrast","metadata":{},"score":"37.269833"}{"text":"There is no explicit grammar given that would produce the bracketing for the parses in the Penn Tree Bank .The great majority of those working on the Penn Tree Bank computationally induce a grammar from the annotation .The number of transitions so induced generally run to the thousands .","label":"CompareOrContrast","metadata":{},"score":"37.269833"}{"text":"We present a system for identifying the semantic relationships , or semantic roles , filled by constituents of a sentence within a semantic frame .Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand - annotated training data .","label":"CompareOrContrast","metadata":{},"score":"37.284775"}{"text":"( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .","label":"CompareOrContrast","metadata":{},"score":"37.31883"}{"text":"( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .","label":"CompareOrContrast","metadata":{},"score":"37.31883"}{"text":"Finally , we investigate when to stop active learning , and adopt two strategies , max - confidence and min - error , as stopping conditions for active learning .According to experimental results , we suggest a prediction solution by considering max - confidence as the upper bound and min - error as the lower bound of stopping conditions .","label":"CompareOrContrast","metadata":{},"score":"37.339264"}{"text":"Parsing algorithms can be represented directly as deduction systems , and a single deduction engine can interpret such deduction systems so as to implement the corresponding parser .The method generaliz ... \" .We present a system for generating parsers based directly on the metaphor of parsing as deduction .","label":"CompareOrContrast","metadata":{},"score":"37.348152"}{"text":"Three binary linear classifiers were trained to predict the existence of a preposition , etc , on unlabeled data and we used singular value decomposition to induce new features .During the training , the parser was trained with these additional features in addition to these described in ( McDonald et al . , 2005 ) .","label":"CompareOrContrast","metadata":{},"score":"37.52383"}{"text":"When tested on a corpus of Wikipedia articles , our hierarchically informed model predicts the correct insertion paragraph more accurately than baseline methods .In this paper we consider the problem of automatically identifying the arguments of discourse connectives ( e.g. and , because , nevertheless ) in the Penn Discourse TreeBank(PDTB ) .","label":"CompareOrContrast","metadata":{},"score":"37.53118"}{"text":"In particular , we wish to determine the best location in a text for a given piece of new information .For this process to succeed , the insertion algorithm should be informed by the existing document structure .Lengthy real - world texts are often hierarchically organized into chapters , sections , and paragraphs .","label":"CompareOrContrast","metadata":{},"score":"37.574745"}{"text":"In phrase - based models , this problem can be addressed by storing the training data in memory and using a suffix array as an efficient index to quickly lookup and extract rules on the fly .Hierarchical phrase - based translation introduces the added wrinkle of source phrases with gaps .","label":"CompareOrContrast","metadata":{},"score":"37.663933"}{"text":"Finally , we show a qualitative evaluation of the results of automatically adding extracted MWEs to existing linguistic resources .We argue that such a process improves qualitatively , if a more compositional approach to grammar / lexicon automated extension is adopted .","label":"CompareOrContrast","metadata":{},"score":"37.784054"}{"text":"Combinations of any of the above are also included within the scope of computer readable media .Conclusion .Although the ranking parser for NLP has been described in language specific to structural features and/or methodological steps , it is to be understood that the ranking parser defined in the appended claims is not necessarily limited to the specific features or steps described .","label":"CompareOrContrast","metadata":{},"score":"37.788822"}{"text":"Combinations of any of the above are also included within the scope of computer readable media .CONCLUSION .Although the ranking parser for NLP has been described in language specific to structural features and/or methodological steps , it is to be understood that the ranking parser defined in the appended claims is not necessarily limited to the specific features or steps described .","label":"CompareOrContrast","metadata":{},"score":"37.788822"}{"text":"Parser actions are determined by a classifier , based on features that represent the current state of the parser .We apply this parsing framework to both tracks of the CoNLL 2007 shared task , in each case taking advantage of multiple models trained with different learners .","label":"CompareOrContrast","metadata":{},"score":"37.878067"}{"text":"We show how partition functions and marginals for directed spanning trees can be computed by an adaptation of Kirchhoff 's Matrix - Tree Theorem .To demonstrate an application of the method , we perform experiments which use the algorithm in training both log - linear and max - margin dependency parsers .","label":"CompareOrContrast","metadata":{},"score":"37.90555"}{"text":"In this study , we present a system that generates lexical analogies automatically from text data .Our system discovers semantically related pairs of words by using dependency relations , and applies novel machine learning algorithms to match these word - pairs to form lexical analogies .","label":"CompareOrContrast","metadata":{},"score":"37.926147"}{"text":"First , it automatically determines a dy - namic context - sensitive tree span for relation extraction by extending the widely - used Shortest Path - enclosed Tree ( SPT ) to include necessary context information outside SPT .Second , it proposes a context - sensitive convolution tree kernel , which enumerates both context - free and context - sensitive sub - trees by considering their ancestor node paths as their contexts .","label":"CompareOrContrast","metadata":{},"score":"38.038834"}{"text":"We present conditional random fields , a framework for building probabilistic models to segment and label sequence data .Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks , including the ability to relax strong independence assumptions ... \" .","label":"CompareOrContrast","metadata":{},"score":"38.162727"}{"text":"A fundamental problem in statistical parsing is the choice of criteria and algorithms used to estimate the parameters in a model .The predominant approach in computational linguistics has been to use a parametric model with some variant of maximum - likelihood estimation .","label":"CompareOrContrast","metadata":{},"score":"38.17227"}{"text":"A fundamental problem in statistical parsing is the choice of criteria and algorithms used to estimate the parameters in a model .The predominant approach in computational linguistics has been to use a parametric model with some variant of maximum - likelihood estimation .","label":"CompareOrContrast","metadata":{},"score":"38.17227"}{"text":"It is better to say the most \" correct \" one .How does a NLP parser judge amongst the multiple grammatically valid parses and select the most \" correct \" parse ?Previous Approaches .Generally .A parser needs a way to accurately and efficiently rank these parse trees .","label":"CompareOrContrast","metadata":{},"score":"38.176994"}{"text":"It is better to say the most \" correct \" one .How does a NLP parser judge amongst the multiple grammatically valid parses and select the most \" correct \" parse ?Previous Approaches .Generally .A parser needs a way to accurately and efficiently rank these parse trees .","label":"CompareOrContrast","metadata":{},"score":"38.176994"}{"text":"However , previous approaches ignore this dependency .We propose a novel approach for this task , namely training Semi Markov models discriminatively using a Max - Margin method .This method allows us to model the sequence dependency of the problem and to incorporate properties of a whole paragraph , such as coherence , which can not be used in previous methods .","label":"CompareOrContrast","metadata":{},"score":"38.182076"}{"text":"We present the idea of estimating semantic distance in one , possibly resource - poor , language using a knowledge source in another , possibly resource - rich , language .We do so by creating cross - lingual distributional profiles of concepts , using a bilingual lexicon and a bootstrapping algorithm , but without the use of any sense - annotated data or word - aligned corpora .","label":"CompareOrContrast","metadata":{},"score":"38.268444"}{"text":"Using the RankNet learning algorithm , we train a pair - based sentence ranker to score every sentence in the document and identify the most important sentences .We apply our system to documents gathered from CNN.com , where each document includes highlights and an article .","label":"CompareOrContrast","metadata":{},"score":"38.345623"}{"text":"We consider the task of tuning hyperparameters in SVM models based on minimizing a smooth performance validation function , e.g. , smoothed k - fold crossvalidation error , using non - linear optimization techniques .The key computation in this approach is that of the gradient of the validation function with respect to hyperparameters .","label":"CompareOrContrast","metadata":{},"score":"38.540302"}{"text":"A method for determining a statistical goodness measure ( SGM ) of a parse tree representing a parse of a phrase , the parse tree comprising one or more nodes , the method comprising : . combining probabilities of each node in the parse tree , wherein the probabilities of each node are determined by the steps comprising : . receiving language - usage probabilities based upon appearances of instances of combinations of linguistic features within a training corpus ; . calculating the probabilities of each node based upon linguistic features of each node and the language - usage probabilities ; . wherein the combinations of linguistic features comprises : .","label":"CompareOrContrast","metadata":{},"score":"38.609894"}{"text":"In this paper , we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured .We also give an overview of the parsing approaches that participants took and the results that they achieved .","label":"CompareOrContrast","metadata":{},"score":"38.65969"}{"text":"Our error analysis for this task suggests that the primary source of error are differences in annotation guidelines among treebanks .Our suspicions are supported by the observation that no team was able to improve target domain performance substantially over a state of the art baseline .","label":"CompareOrContrast","metadata":{},"score":"38.692326"}{"text":"The parser first identifies dependencies using a discriminative classifier and then labels those dependencies as a sequence labeling problem .The features for two stages are proposed .For four languages have different values of ROOT , we design some special features for ROOT labeler .","label":"CompareOrContrast","metadata":{},"score":"38.695404"}{"text":"We demonstrate significant gains using features derived from a dependency parse representation over those derived from a constituency - based tree parse .By also capturing inter - argument dependencies using a log - linear re - ranking model we achieve very promising results on this difficult task identifying both arguments correctly for over 74 % of the connectives on held - out test data using gold - standard parses .","label":"CompareOrContrast","metadata":{},"score":"38.70018"}{"text":"By improving the goodness function , the parser improves its accuracy .In particular , the goodness function enables the parser to choose the best parse for an utterance .Each parse may be viewed as a tree with branches that eventually branch to each word in a sentence .","label":"CompareOrContrast","metadata":{},"score":"38.76574"}{"text":"By improving the goodness function , the parser improves its accuracy .In particular , the goodness function enables the parser to choose the best parse for an utterance .Each parse may be viewed as a tree with branches that eventually branch to each word in a sentence .","label":"CompareOrContrast","metadata":{},"score":"38.76574"}{"text":"In this paper , a novel method is proposed for use of web search results to improve the existing query spelling correction models solely based on query logs by leveraging the rich information on the web related to the query and its top - ranked candidate .","label":"CompareOrContrast","metadata":{},"score":"38.864304"}{"text":"There has been little effort reported on this in the research community .We argue that semantics is important for record extraction or finer - grained language processing tasks .We derive a data record template including semantic language models from unstructured text and represent them with a discourse level Conditional Random Fields ( CRF ) model .","label":"CompareOrContrast","metadata":{},"score":"38.994156"}{"text":"The SGM ( statistical goodness measure ) of the exemplary parser uses a generative grammar approach - each sentence has a top - down derivation consisting of a sequence of rule applications ( transitions ) .The probability of a parse tree is a the product of the probabilities of all the nodes within that tree .","label":"CompareOrContrast","metadata":{},"score":"39.040077"}{"text":"The main research in this area relates to the design choices for feature represen ... \" .The availability of large scale data sets of manually annotated predicate - argument structures has recently favored the use of machine learning approaches to the design of automated semantic role labeling ( SRL ) systems .","label":"CompareOrContrast","metadata":{},"score":"39.103493"}{"text":"Creating such a huge training corpus is infeasible .However , working from a smaller corpus creates sparse data problems .Statistical Hodgepodge Approach .Using this approach , the goodness of a parse may be determined by a collection of mostly unrelated statistical calculations based upon parts of speech , syntactic features , word probabilities , and selected heuristic rules .","label":"CompareOrContrast","metadata":{},"score":"39.13583"}{"text":"Creating such a huge training corpus is infeasible .However , working from a smaller corpus creates sparse data problems .Statistical Hodgepodge Approach .Using this approach , the goodness of a parse may be determined by a collection of mostly unrelated statistical calculations based upon parts of speech , syntactic features , word probabilities , and selected heuristic rules .","label":"CompareOrContrast","metadata":{},"score":"39.13583"}{"text":"The SGM ( statistical goodness measure ) of the exemplary parser uses a generative grammar approach - each sentence has a top - down derivation consisting of a sequence of rule applications ( transitions ) .The probability of a parse tree is the product of the probabilities of all the nodes within that tree .","label":"CompareOrContrast","metadata":{},"score":"39.15763"}{"text":"The online method adapts the translation model by redistributing the weight of each predefined submodels .Information retrieval model is used for the weighting scheme in both methods .Experimental results show that without using any additional resource , both methods can improve SMT performance significantly .","label":"CompareOrContrast","metadata":{},"score":"39.292767"}{"text":"Both methods can be reduced to linear systems and are easy to implement . ... ining set while one fold as the validation set . \" ...The input to an algorithm that learns a binary classifier normally consists of two sets of examples , where one set consists of positive examples of the concept to be learned , and the other set consists of negative examples .","label":"CompareOrContrast","metadata":{},"score":"39.394814"}{"text":"In this paper , we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages .In addition , we characterize the different approaches of the participating systems , report the test results , and provide a first analysis of these results .","label":"CompareOrContrast","metadata":{},"score":"39.509644"}{"text":"In common with other approaches to sequence modeling using perceptrons , and in contrast with comparable generative models , this model permits and transparently exploits arbitrary features of input strings .The simplicity of perceptron training lends more versatility than comparable approaches , allowing the model to be applied to a variety of problem types for which a learned edit model might be useful .","label":"CompareOrContrast","metadata":{},"score":"39.626835"}{"text":"Further , each lexical item is associated with as many supertags as the number of different syntactic contexts in which the lexical item can appear .This makes the number of different descriptions for each lexical item much larger , than when the descriptions are less complex ; thus increasing the local ambiguity for a parser .","label":"CompareOrContrast","metadata":{},"score":"39.68135"}{"text":"The disambiguation component employs a vector space model and a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document , as well as the agreement among the category tags associated with the candidate entities .","label":"CompareOrContrast","metadata":{},"score":"39.723984"}{"text":"Evaluation on the ACE RDC corpora shows that our dynamic context - sensitive tree span is much more suitable for relation extraction than SPT and our tree kernel outperforms the state - of - the - art Collins and Duffy 's convolution tree kernel .","label":"CompareOrContrast","metadata":{},"score":"39.754982"}{"text":"The second approach combines unsupervised hidden markov modelling with language models .Empirical evaluation of both systems pointed out that the hidden markov model managed best to learn the task of segmenting and labelling biological field book entries from a derived database only .","label":"CompareOrContrast","metadata":{},"score":"39.75566"}{"text":"n .X . )Count . trn .n .X . ) hw .n .X . )Formula .Two Phases of SGM Calculation .Typically , a parser of an NLP system ( such as the exemplary parser ) is designed to quickly calculate the goodness measure for many parse trees of parses of a phrase .","label":"CompareOrContrast","metadata":{},"score":"39.84884"}{"text":"This lexicon provides the initial lexical probabilities for EM training of a HMM model .We evaluate the method by applying it in the Biology domain and show that we achieve results that are comparable with some taggers developed for this domain .","label":"CompareOrContrast","metadata":{},"score":"39.954098"}{"text":"We then apply these two new methods to solve a real - world problem : identifying protein records that should be included in an incomplete specialized molecular biology database .Our experiments in this domain show that models trained using the new methods perform better than the current state - of - the - art biased SVM method for learning from positive and unlabeled examples . .","label":"CompareOrContrast","metadata":{},"score":"40.026543"}{"text":"By extending a recent model , we obtain a completely corpus - driven model for this task which achieves significant correlations with human judgements .It rivals or exceeds deeper , resource - driven models while exhibiting higher coverage .Moreover , we show that our model can be combined with deeper models to obtain better predictions than from either model alone .","label":"CompareOrContrast","metadata":{},"score":"40.052288"}{"text":"In this paper , we used syntactic subtrees that span potential argument structures of the target predicate in tree kernel functions .This ... \" .Recent work on Semantic Role Labeling ( SRL ) has shown that to achieve high accuracy a joint inference on the whole predicate argument structure should be applied .","label":"CompareOrContrast","metadata":{},"score":"40.059105"}{"text":"To characterise the arguments in a given grammatical relationship we experiment with three models of selectional preference .Two use WordNet and one uses the entries from a distributional thesaurus as classes for representation .In previous work on selectional preference acquisition , the classes used for representation are selected according to the coverage of argument tokens rather than being selected according to the coverage of argument types .","label":"CompareOrContrast","metadata":{},"score":"40.190002"}{"text":"We focus on two important subtasks of opinion extraction : ( a ) extracting aspect - evaluation relations , and ( b ) extracting aspect - of relations , and we approach each task using methods which combine contextual and statistical clues .","label":"CompareOrContrast","metadata":{},"score":"40.191242"}{"text":"In addition , there are well - known techniques for converting a ternary ( or higher ) rule into a set of binary rules .The exemplary parser defines phrase levels and labels them .Previous conventional approaches clustered transitions by segtype .","label":"CompareOrContrast","metadata":{},"score":"40.207756"}{"text":"In addition , there are well - known techniques for converting a ternary ( or higher ) rule into a set of binary rules .The exemplary parser defines phrase levels and labels them .Previous conventional approaches clustered transitions by segtype .","label":"CompareOrContrast","metadata":{},"score":"40.207756"}{"text":"The method of .claim 1 wherein identifying comprises : . generating a probability that generated overall parses occur given a word sequence ; . selecting an overall parse generated during parsing that has a highest probability of occurring ; and .","label":"CompareOrContrast","metadata":{},"score":"40.287872"}{"text":"A parser typically uses a goodness function to generate a \" goodness measure \" that ranks the parse trees .Conventional implementations use heuristic ( \" rule of thumb \" ) rules and/or statistics based on the part of speech of the words in the sentence and immediate syntactic context .","label":"CompareOrContrast","metadata":{},"score":"40.47685"}{"text":"A parser typically uses a goodness function to generate a \" goodness measure \" that ranks the parse trees .Conventional implementations use heuristic ( \" rule of thumb \" ) rules and/or statistics based on the part of speech of the words in the sentence and immediate syntactic context .","label":"CompareOrContrast","metadata":{},"score":"40.47685"}{"text":"The deep processing approach uses the XLE LFG parser and English grammar : two versions are presented , one which uses the XLE directly to perform the classification , and another one which uses a decision tree trained on features consisting of the XLE 's output statistics .","label":"CompareOrContrast","metadata":{},"score":"40.51843"}{"text":"Our approach is based on the analysis of the paths between two protein names in the dependency parse trees of the sentences .Given two dependency trees , we define two separate similarity functions ( kernels ) based on cosine similarity and edit distance among the paths between the protein names .","label":"CompareOrContrast","metadata":{},"score":"40.518803"}{"text":"Typically , not all the words in text are relevant to a particular frame .Assuming that the segments of text relevant to filling in the slots are non - overlapping , contiguous strings of words , one can represent the semantic frame as a simple semantic parse tree for the sentence to be processed .","label":"CompareOrContrast","metadata":{},"score":"40.554756"}{"text":"These category labels are used as features in a CRF - based NE tagger .We demonstrate using the CoNLL 2003 dataset that the Wikipedia category labels extracted by such a simple method actually improve the accuracy of NER .This paper presents a large - scale system for the recognition and semantic disambiguation of named entities based on information extracted automatically from a large encyclopedic collection and Web search results , over a space of more than 1.4 million entities .","label":"CompareOrContrast","metadata":{},"score":"40.56266"}{"text":"The experiments are carried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser .We use a generative history - based model to predict the most likely derivation of a dependency parse .","label":"CompareOrContrast","metadata":{},"score":"40.636406"}{"text":"This paper addresses the problem of detecting low - quality product reviews .Three types of biases in the existing evaluation standard of product reviews are discovered .To assess the quality of product reviews , a set of specifications for judging the quality of reviews is first defined .","label":"CompareOrContrast","metadata":{},"score":"40.64657"}{"text":"We describe an approach to improve Statistical Machine Translation ( SMT ) performance using multi - lingual , parallel , sentence - aligned corpora in several bridge languages .Our approach consists of a simple method for utilizing a bridge language to create a word alignment system and a procedure for combining word alignment systems from multiple bridge languages .","label":"CompareOrContrast","metadata":{},"score":"40.781128"}{"text":"Tools . \" ...Tree - adjoining grammars ( TAG ) have been proposed as a formalism for generation based on the intuition that the extended domain of syntactic locality that TAGs provide should aid in localizing semantic dependencies as well , in turn serving as an aid to generation from semantic representations .","label":"CompareOrContrast","metadata":{},"score":"40.785152"}{"text":"We describe a robust accurate domain - independent approach to statistical parsing incorporated into the new release of the ANLT toolkit , and publicly available as a research tool .The system has been used to parse many well known corpora in order to produce data for lexical acquisition efforts ; it ha ... \" .","label":"CompareOrContrast","metadata":{},"score":"40.835392"}{"text":"This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution .We investigate Gibbs Sampling ( GS ) and Variational Bayes ( VB ) estimators and show that VB converges faster than GS for this task and that VB significantly improves 1-to-1 tagging accuracy over EM .","label":"CompareOrContrast","metadata":{},"score":"40.84555"}{"text":"We also point out the high variance in all of these estimators , and that they require many more iterations to approach convergence than usually thought .This paper describes a probabilistic model for coordination disambiguation integrated into syntactic and case structure analysis .","label":"CompareOrContrast","metadata":{},"score":"40.965744"}{"text":"Finally , it shows that feature - based and tree kernel - based methods much complement each other and the composite kernel can well integrate both flat and structured features .Syntactic reordering approaches are an effective method for handling word - order differences between source and target languages in statistical machine translation ( SMT ) systems .","label":"CompareOrContrast","metadata":{},"score":"40.99499"}{"text":"We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process ( HDP ) .Our HDP - PCFG model allows the complexity of the grammar to grow as more training data is available .In addition to presenting a fully Bayesian model for the PCFG , we also develop an efficient variational inference procedure .","label":"CompareOrContrast","metadata":{},"score":"41.043007"}{"text":"After motivating the need for explicit predicate argument structure labels , we briefly discuss the theoretical considerations of predicate argument structure and the need to maintain consistency across syntactic alternations .The issues of consistency of argument structure across both polysemous and synonymous verbs are also discussed and we present our actual guidelines for these types of phenomena , along with numerous examples of tagged sentences and verb frames .","label":"CompareOrContrast","metadata":{},"score":"41.095238"}{"text":"\" ...We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .","label":"CompareOrContrast","metadata":{},"score":"41.102158"}{"text":"Key enablers of this high performance are features derived from previous natural language processing work in noun compound bracketing .For example , token association features beyond simple N - gram counts provide powerful indicators of segmentation .We present two machine learning approaches to information extraction from semi - structured documents that can be used if no annotated training data are available , but there does exist a database filled with information derived from the type of documents to be processed .","label":"CompareOrContrast","metadata":{},"score":"41.11319"}{"text":"However , this specific task raises a number of difficult issues that are quite distinct from the generic task of language generation : conciseness , complex sentences , floating concepts , historical background , paraphrasing power and implicit content .This model requires a new type of linguistic knowledge : revision operations , which specifyies the various ways a draft can ... . \" ...","label":"CompareOrContrast","metadata":{},"score":"41.284607"}{"text":"We perform both identification and resolution automatically , with two sets of easily computable features .Experimental results show that our proposed learning approach achieves anaphoric zero pronoun resolution accuracy comparable to a previous state - of - the - art , heuristic rule - based approach .","label":"CompareOrContrast","metadata":{},"score":"41.31138"}{"text":"Lexical databases are invaluable sources of knowledge about words and their meanings , with numerous applications in areas like NLP , IR , and AI .We propose a methodology for the automatic construction of a large - scale multilingual lexical database where words of many languages are hierarchically organized in terms of their meanings and their semantic relations to other words .","label":"CompareOrContrast","metadata":{},"score":"41.334732"}{"text":"The two constrained parsing steps illustrated by blocks 230 and 240 in .FIG .8 ensure that the constituents proposed by the SLM do not cross semantic constituent boundaries and that the labels proposed are the desired ones .where l is the left boundary of the constraint , r is the right boundary of the constraint and Q is the set of allowable non - terminal ( semantic ) tags for the constraint .","label":"CompareOrContrast","metadata":{},"score":"41.346825"}{"text":"FIG . 8 .During this process , the SLM is allowed to explore ( or generate parses for ) only the semantic parses found in the training data .Thus , the semantic constituent labels are taken into account .This means that a parse P -containing both syntactic and semantic information -is said to L - match S if and only if the set of labeled semantic constituents that defines S is identical to the set of semantic constituents that defines P. In the present embodiment , the semantic tree S has a two - level structure .","label":"CompareOrContrast","metadata":{},"score":"41.350273"}{"text":"The model is then trained by generating parses on the semantically annotated training data enforcing the semantic tags ( or labels ) as well as the annotated constituent boundaries found in the training data .In one embodiment , the structured language model operates with binary trees in a left - to - right , bottom - up fashion .","label":"CompareOrContrast","metadata":{},"score":"41.451042"}{"text":"Finally , the SLM is trained again using constrained parsing .This time , however , the constraint is not only to match the constituent spans or boundaries , but it is also constrained to match the annotated semantic labels and is thus referred to as enforcing the L - match ( for label - match ) constraint .","label":"CompareOrContrast","metadata":{},"score":"41.46205"}{"text":"We describe an extension of semisupervised structured conditional models ( SS - SCMs ) to the dependency ... .Collins , Michael ; Globerson , Amir ; Koo , Terry ; Carreras Prez , Xavier ; Bartlett , Peter ( 2008 - 08 )","label":"CompareOrContrast","metadata":{},"score":"41.531868"}{"text":"We also compare the translation accuracy for all variations .We achieved a state of the art performance in statistical machine translation by using a large number of features with an online large - margin training algorithm .The millions of parameters were tuned only on a small development set consisting of less than 1 K sentences .","label":"CompareOrContrast","metadata":{},"score":"41.543213"}{"text":"We extract effective expressions from the important segments to define various viewpoints .In text mining a viewpoint defines the important associations between key entities and it is crucial that the correct viewpoints are identified .We show the effectiveness of the method by using real datasets from a car rental service center .","label":"CompareOrContrast","metadata":{},"score":"41.64437"}{"text":"In this paper we introduce a joint arc - factored model for syntactic and semantic dependency parsing .The semantic role labeler predicts the full syntactic paths that connect predicates with their arguments .This process ... .Carreras Prez , Xavier ; Collins , Michael ( 2009 ) Conference report Open Access .","label":"CompareOrContrast","metadata":{},"score":"41.728127"}{"text":"Our proposal takes advantage of the one - sided error guarantees of the BF and simple inequalities that hold between related $ n$-gram statistics in order to further reduce the BF storage requirements and the error rate of the derived probabilities .","label":"CompareOrContrast","metadata":{},"score":"41.75059"}{"text":"x . )Count . trn .n .x . ) hw .n .x . )Formula .Two Phases of SGM Calculation .Typically , a parser of an NLP system ( such as the exemplary parser ) is designed to quickly calculate the goodness measure for many parse trees of parses of a phrase .","label":"CompareOrContrast","metadata":{},"score":"41.787594"}{"text":"Standard thesaurus - based measures of word pair similarity are based on only a single path between those words in the thesaurus graph .By contrast , we propose a new model of lexical semantic relatedness that incorporates information from every explicit or implicit path connecting the two words in the entire graph .","label":"CompareOrContrast","metadata":{},"score":"41.82541"}{"text":"More surprising , we have all found useful the guidelines which were published last year \" .FIG .6 a shows a parse tree 100 representing a parse of the above sentence , where the parse is done in accordance with the example grammar provided above .","label":"CompareOrContrast","metadata":{},"score":"41.930206"}{"text":"More surprising , we have all found useful the guidelines which were published last year \" .FIG .6 a shows a parse tree 100 representing a parse of the above sentence , where the parse is done in accordance with the example grammar provided above .","label":"CompareOrContrast","metadata":{},"score":"41.930206"}{"text":"We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation .We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result .","label":"CompareOrContrast","metadata":{},"score":"42.05932"}{"text":"In text categorization , term selection is an important step for the sake of both categorization accuracy and computational efficiency .Different dimensionalities are expected under different practical resource restrictions of time or space .Traditionally in text categorization , the same scoring or ranking criterion is adopted for all target dimensionalities , which considers both the discriminability and the coverage of a term , such as $ \\chi^2 $ or IG .","label":"CompareOrContrast","metadata":{},"score":"42.15042"}{"text":"First , we use SVMs to implement the boundary T BC and role T RC local classifiers .Second , we combine T BC and T RC probabilities to obtain the m most likely sequences s of tree nodes annotated with semantic roles .","label":"CompareOrContrast","metadata":{},"score":"42.224922"}{"text":"We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm , and we give experimental results on the ATIS corpus of parse trees . ... ion from ranking problems to a margin - based classification problem in [ 8].","label":"CompareOrContrast","metadata":{},"score":"42.25927"}{"text":"Collins ( 2000 ) uses a technique based on boosting algorithms for machine learning that reranks n - best output from model 2 in t .. \" ...The problem of combining preferences arises in several applications , such as combining the results of different search engines .","label":"CompareOrContrast","metadata":{},"score":"42.348896"}{"text":"Information extraction from text can be characterized as a template filling process .In other words , a given template or frame contains a certain number of slots that need to be filled in with segments of text .The label of the frame corresponds to a high level of understanding , such as the particular action being requested by the user .","label":"CompareOrContrast","metadata":{},"score":"42.396133"}{"text":"The most straightforward approach is the \" straw man approach .\" The goodness function of this approach computes the probability of a given parse tree based upon how often identical trees appeared in a training corpus .This approach is theoretical and is rarely ( if ever ) used in practice .","label":"CompareOrContrast","metadata":{},"score":"42.435772"}{"text":"The most straightforward approach is the \" straw man approach .\" The goodness function of this approach computes the probability of a given parse tree based upon how often identical trees appeared in a training corpus .This approach is theoretical and is rarely ( if ever ) used in practice .","label":"CompareOrContrast","metadata":{},"score":"42.435772"}{"text":"This paper discusses the statistical theory underlying various parameter - estimation methods , and gives algorithms which depend on alternatives to ( smoothed ) maximumlikelihood estimation .We first give an overview of results from statistical learning theory .We then show how important concepts from the classification literature -- specifically , generalization results based on margins on training data -- can be derived for parsing models .","label":"CompareOrContrast","metadata":{},"score":"42.483097"}{"text":"The results were achieved by using only information about heads and daughters as features to guide the parser which obeys strict incrementality .A memory - based learner was used to predict the next action of the parser .This paper presents an online algorithm for dependency parsing problems .","label":"CompareOrContrast","metadata":{},"score":"42.50695"}{"text":"In this paper , we try to address this gap and explore the problem of book summarization .We introduce a new data set specifically designed for the evaluation of systems for book summarization , and describe summarization techniques that explicitly account for the length of the documents .","label":"CompareOrContrast","metadata":{},"score":"42.525818"}{"text":"We demonstrate results for letter and digit recognition on datasets from the UCI repository , object recognition results on the Caltech-101 dataset , and scene categorization results on a dataset of 13 natural scene categories .The proposed method gives large reductions in the number of training examples required over random selection to achieve similar classification accuracy , with little computational overhead .","label":"CompareOrContrast","metadata":{},"score":"42.569004"}{"text":"It is an iterative and time consuming process that requires grammars to be written using a combination of knowledge and data , and then tested and refined using test data .Thus , the current approaches can tend to be not only time consuming , but quite costly .","label":"CompareOrContrast","metadata":{},"score":"42.603073"}{"text":"During label prediction , the system automatically selects for each feature an appropriate level of smoothing .We report on several experiments that we conducted with our system .In the shared task evaluation , it scored better than average .We present Pro3Gres , a deep - syntactic , fast dependency parser that combines a hand - written competence grammar with probabilistic performance disambiguation and that has been used in the biomedical domain .","label":"CompareOrContrast","metadata":{},"score":"42.65142"}{"text":"To simplify Formula 2 , it is noted that not all the parameters are independent .In particular , trn(n X ) and pl(n X ) imply pl(n Y ) and pl(n Z ) .Similarly , Formula 3 may be simplified because the ranking is not directly determined by what the syntactic history is , versus its effects on the structure .","label":"CompareOrContrast","metadata":{},"score":"42.658195"}{"text":"Each parse tree is given a SGM based upon the pre - calculated counts .Alternatively , the training and run - time phase may be performed nearly concurrently .The training phase may be performed on a training corpus ( or some subset of such corpus ) just before the run - time phase is performed .","label":"CompareOrContrast","metadata":{},"score":"42.81768"}{"text":"The model is then trained by generating parses on the semantically annotated training data enforcing the semantic tags or labels found in the training data .The trained model can then be used to extract information from test data using the parses generated by the model .","label":"CompareOrContrast","metadata":{},"score":"42.833267"}{"text":"A natural language parse ranker of a natural language processing ( NLP ) system employs a goodness function to rank the possible grammatically valid parses of an utterance .The goodness function generates a statistical goodness measure ( SGM ) for each valid parse .","label":"CompareOrContrast","metadata":{},"score":"42.847496"}{"text":"A natural language parse ranker of a natural language processing ( NLP ) system employs a goodness function to rank the possible grammatically valid parses of an utterance .The goodness function generates a statistical goodness measure ( SGM ) for each valid parse .","label":"CompareOrContrast","metadata":{},"score":"42.847496"}{"text":"Each parse tree is given a SGM based upon the pre - calculated counts .Alternatively , the training and ran - time phase may be performed nearly concurrently .The training phase may be performed on a training corpus ( or some subset of such corpus ) just before the run - time phase is performed .","label":"CompareOrContrast","metadata":{},"score":"42.965977"}{"text":"For example , suppose that probabilities are assigned to each transition shown in Table 1 above and those probabilities are based upon some training corpus .The training corpus would contain parsed sentences such that the system can count the number of times each transition occurred .","label":"CompareOrContrast","metadata":{},"score":"43.038147"}{"text":"For example , suppose that probabilities are assigned to each transition shown in Table 1 above and those probabilities are based upon some training corpus .The training corpus would contain parsed sentences such that the system can count the number of times each transition occurred .","label":"CompareOrContrast","metadata":{},"score":"43.038147"}{"text":"For this example , the transition probabilities are conditioned on segtype : .Prob . parse . ) i .Prob .n .i . ) i .Prob . trans .n .i . ) segtype .n .","label":"CompareOrContrast","metadata":{},"score":"43.08252"}{"text":"The tags annotating the nodes of the tree are purely syntactic during the training step 230 and are syntactic and semantic during the training step 240 .For a given word - parse k - prefix W k T k accept an adjoin transition if an only if : . Q. .","label":"CompareOrContrast","metadata":{},"score":"43.172207"}{"text":"In addition , we present work on experiments with named entities and other multi - word units , showing a statistically significant improvement of generation accuracy .Given multiple translations of the same source sentence , how to combine them to produce a translation that is better than any single system output ?","label":"CompareOrContrast","metadata":{},"score":"43.18578"}{"text":"p .i . k . ) . ] where : .W k is the word predicted by a word - predictor component ; .t k is the tag assigned to wk by a POS tagger component ; .N k -1 is the number of operations the parser executes at sentence position k before passing control to the word - predictor ( the N k -th operation at position k is the null transition ) N k is a function of T ; .","label":"CompareOrContrast","metadata":{},"score":"43.348595"}{"text":"Most current word prediction systems make use of n - gram language models ( LM ) to estimate the probability of the following word in a phrase .In the past years there have been many attempts to enrich such language models with further syntactic or semantic information .","label":"CompareOrContrast","metadata":{},"score":"43.3813"}{"text":"However , the description itself is not intended to limit the scope of this patent .Rather , the inventor has contemplated that the claimed ranking parser might also be embodied in other ways , in conjunction with other present or future technologies .","label":"CompareOrContrast","metadata":{},"score":"43.403637"}{"text":"However , the description itself is not intended to limit the scope of this patent .Rather , the inventor has contemplated that the claimed ranking parser might also be embodied in other ways , in conjunction with other present or future technologies .","label":"CompareOrContrast","metadata":{},"score":"43.403637"}{"text":"We address the problem of training the free parameters of a statistical machine translation system .We show significant improvements over a state - of - the - art minimum error rate training baseline on a large Chinese - English translation task .","label":"CompareOrContrast","metadata":{},"score":"43.537106"}{"text":"The exemplary ranking parser may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network .In a distributed computing environment , program modules may be located in both local and remote computer storage media including memory storage devices .","label":"CompareOrContrast","metadata":{},"score":"43.577457"}{"text":"The exemplary ranking parser may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network .In a distributed computing environment , program modules may be located in both local and remote computer storage media including memory storage devices .","label":"CompareOrContrast","metadata":{},"score":"43.577457"}{"text":"We achieve average results , which is partly due to difficulties in mapping to the dependency representation used for the shared task .Following ( Blitzer et al . , 2006 ) , we present an application of structural correspondence learning to non - projective dependency parsing ( McDonald et al . , 2005 ) .","label":"CompareOrContrast","metadata":{},"score":"43.610397"}{"text":"Next , the syntactic labels in the syntactic parse are replaced with joint syntactic and semantic labels .This is indicated by block 232 in .FIG .8 .An example of this step is illustrated in .FIG .7B .","label":"CompareOrContrast","metadata":{},"score":"43.822998"}{"text":"As mentioned above , the problem of information extraction can be viewed as the recovery of a two - level semantic parse for a given word sequence .In accordance with one embodiment of the present invention , a data driven approach to information extraction uses a SLM .","label":"CompareOrContrast","metadata":{},"score":"43.865852"}{"text":"The null transition is assigned probabilities just like other transitions .The exemplary parser defines each node 's syntactic history .Previous conventional approaches conditioned on linguistic phenomena associated with a node , its parent , and/or its children .However , such approaches are overly s limiting .","label":"CompareOrContrast","metadata":{},"score":"43.93071"}{"text":"The order in which the trees are examined does not affect the results .Therefore , any tree in the set of valid parse trees may be the first .Blocks 306 - 312 show the details of examinations and SGM calculations for all of the trees of a phrase .","label":"CompareOrContrast","metadata":{},"score":"43.95986"}{"text":"The order in which the trees are examined does not affect the results .Therefore , any tree in the set of valid parse trees may be the first .Blocks 306 - 312 show the details of examinations and SGM calculations for all of the trees of a phrase .","label":"CompareOrContrast","metadata":{},"score":"43.95986"}{"text":"We present experimental results from the CoNLL 2003 named entity recognition ( NER ) task to demonstrate the performance of the proposed algorithm .In this paper , we address a unique problem in Chinese language processing and report on our study on extending a Chinese thesaurus with region - specific words , mostly from the financial domain , from various Chinese speech communities .","label":"CompareOrContrast","metadata":{},"score":"43.9901"}{"text":"We train a discriminative classifier over a wide variety of features derived from WordNet structure , corpus - based evidence , and evidence from other lexical resources .Our learned similarity measure outperforms previously proposed automatic methods for sense clustering on the task of predicting human sense merging judgments , yielding an absolute F - score improvement of 4.1 % on nouns , 13.6 % on verbs , and 4.0 % on adjectives .","label":"CompareOrContrast","metadata":{},"score":"44.095665"}{"text":"The method of .claim 1 wherein identifying comprises : . generating a probability that generated overall parses occur given a word sequence ; . summing the probability over all parses having a common semantic parse ; and . selecting the semantic parse based on the summed probability .","label":"CompareOrContrast","metadata":{},"score":"44.136696"}{"text":"For our models and training sets , more peaked measures of confidence , measured by Renyi entropy , outperformed smoother ones .We discuss how our feature set could be extended with cross - lingual or cross - domain features , to incorporate knowledge from parallel or comparable corpora during bootstrapping .","label":"CompareOrContrast","metadata":{},"score":"44.22667"}{"text":"We first give a formal framework for the problem .We then describe and analyze a new boosting ... \" .The problem of combining preferences arises in several applications , such as combining the results of different search engines .This work describes an efficient algorithm for combining multiple preferences .","label":"CompareOrContrast","metadata":{},"score":"44.23214"}{"text":"We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .In addition , we introduce two novel smoothing techniques , one a variation of Jelinek - Mercer smoothing and one a very simple linear interpolation technique , both of which outperform existing methods . ... her applications as well , e.g. prepositional phrase attachment ( Collins & Brooks , 1995 ) , part - of - speech tagging ( Church , 1988 ) , and stochastic pars - S. Whenever data sparsity is an issue , smoothing can help performance , and data sparsity is almost always an issue in statistical modeling .","label":"CompareOrContrast","metadata":{},"score":"44.26068"}{"text":"Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . by Stuart M. Shieber , Yves Schabes , Fernando C. N. Pereira - JOURNAL OF LOGIC PROGRAMMING , 1995 . \" ...","label":"CompareOrContrast","metadata":{},"score":"44.26963"}{"text":"Although the present invention has been described with reference to particular embodiments , workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention .A natural language parse ranker of a natural language processing ( NLP ) system employs a goodness function to rank the possible grammatically valid parses of an utterance .","label":"CompareOrContrast","metadata":{},"score":"44.316113"}{"text":"This approach assumes a top - down , generative grammar approach .It defines a formulism for computing the probability of a transition given an arbitrary set of linguistic features .Features might include headword , segtype , and grammatical number , though the formulism is independent of the actual features used .","label":"CompareOrContrast","metadata":{},"score":"44.404522"}{"text":"This approach assumes a top - down , generative grammar approach .It defines a formulism for computing the probability of a transition given an arbitrary set of linguistic features .Features might include headword , segtype , and grammatical number , though the formulism is independent of the actual features used .","label":"CompareOrContrast","metadata":{},"score":"44.404522"}{"text":"The null transition is assigned probabilities just like other transitions .The exemplary parser defines each node 's syntactic history .Previous conventional approaches conditioned on linguistic phenomena associated with a node , its parent , and/or its children .However , such approaches are overly limiting .","label":"CompareOrContrast","metadata":{},"score":"44.416733"}{"text":"This paper presents a novel approach for exploiting the global context for the task of word sense disambiguation ( WSD ) .This is done by using topic features constructed using the latent dirichlet allocation ( LDA ) algorithm on unlabeled data .","label":"CompareOrContrast","metadata":{},"score":"44.434303"}{"text":"This conventional statistical goodness approach is typically done with little or no consideration for contextual words and phrases .SUMMARY .A natural language parse ranker of a natural language processing ( NLP ) system employs a goodness function to rank the possible grammatically valid parses of an utterance .","label":"CompareOrContrast","metadata":{},"score":"44.440033"}{"text":"This conventional statistical goodness approach is typically done with little or no consideration for contextual words and phrases .SUMMARY .A natural language parse ranker of a natural language processing ( NLP ) system employs a goodness function to rank the possible grammatically valid parses of an utterance .","label":"CompareOrContrast","metadata":{},"score":"44.440033"}{"text":"The probability for a given node is the probability that from the node one would take a specific transition , given the linguistic features .The SGM of the exemplary parser may be calculated using either of the following equivalent formulas : .","label":"CompareOrContrast","metadata":{},"score":"44.57334"}{"text":"x .y . )Prob .x .y . )Prob .y . )Count .x .y . )Count .y . )Therefore , the PredParamRule Probability and the SynBigram Probability can be calculated by counting the appearances of relevant events in the training corpus .","label":"CompareOrContrast","metadata":{},"score":"44.588688"}{"text":"x .y . )Prob .x .y . )Prob .y . )Count .x .y . )Count .y . )Therefore , the PredParamRule Probability and the SynBigram Probability can be calculated by counting the appearances of relevant events in the training corpus .","label":"CompareOrContrast","metadata":{},"score":"44.588688"}{"text":"FIGS .5 - 6 .FIG .5 shows the result of an adjoin - left operation and .FIG .6 shows the result of an adjoin - right operation .These operations ensure that all possible binary branching parses with all possible head - word and non - terminal label assignments for the w 1 . . .","label":"CompareOrContrast","metadata":{},"score":"44.60903"}{"text":"X . ) trn .n .X . ) hw .n .X . )Formula .B . where .n X : is the X th node in a parse tree .n y & n z : are the Y th and Z th nodes and children of the X th node .","label":"CompareOrContrast","metadata":{},"score":"44.661392"}{"text":"If the training corpus approximates the natural language usage of a given purpose ( general , specific , or customized ) , then the counts also approximate the natural language usage for the same purpose .At ran - time , these pre - calculated counts are used to quickly determine the probability of the parse tree .","label":"CompareOrContrast","metadata":{},"score":"44.66386"}{"text":"The parse ranker orders the parses based upon their SGM values .It presents the parse with the greatest SGM value as the one that most likely represents the intended meaning of the speaker .The goodness function of this parse ranker is highly accurate in representing the intended meaning of a speaker .","label":"CompareOrContrast","metadata":{},"score":"44.682777"}{"text":"The parse ranker orders the parses based upon their SGM values .It presents the parse with the greatest SGM value as the one that most likely represents the intended meaning of the speaker .The goodness function of this parse ranker is highly accurate in representing the intended meaning of a speaker .","label":"CompareOrContrast","metadata":{},"score":"44.682777"}{"text":"We present a method for improving word alignment for statistical syntax - based machine translation that employs a syntactically informed alignment model closer to the translation model than commonly - used word alignment models .This leads to extraction of more useful linguistic patterns and improved BLEU scores on translation experiments in Chinese and Arabic .","label":"CompareOrContrast","metadata":{},"score":"44.777534"}{"text":"3 ) and for which a frame label 222 and slot labels 224 and 226 have been added by annotation .As discussed with respect to frame 200 above , frame label 222 indicates the overall action being referred to by the input sentence .","label":"CompareOrContrast","metadata":{},"score":"44.845253"}{"text":"Of course , more or less sentences could be used .The more sentences used , the higher the accuracy of the probabilities .In addition , there are many known techniques for dealing with insufficient training data .The probabilities computation method has two phases : training and run - time .","label":"CompareOrContrast","metadata":{},"score":"44.899757"}{"text":"Of course , more or less sentences could be used .The more sentences used , the higher the accuracy of the probabilities .In addition , there are many known techniques for dealing with insufficient training data .The probabilities computation method has two phases : training and run - time .","label":"CompareOrContrast","metadata":{},"score":"44.899757"}{"text":"Specifically , the parser proposes a set of n syntactic binary parses for a given word string , all matching the constituent boundaries specified by the semantic parse .A parse T is said to match the semantic parse S denoted T .","label":"CompareOrContrast","metadata":{},"score":"44.90146"}{"text":"Alternatively , a parser generates reasonable trees based on linguistic knowledge and then uses the goodness function to choose between the reasonable trees .In either case , the problem is to implement an efficient goodness function that accurately reflects and measures the most likely meaning of an utterance .","label":"CompareOrContrast","metadata":{},"score":"44.904247"}{"text":"Alternatively , a parser generates reasonable trees based on linguistic knowledge and then uses the goodness function to choose between the reasonable trees .In either case , the problem is to implement an efficient goodness function that accurately reflects and measures the most likely meaning of an utterance .","label":"CompareOrContrast","metadata":{},"score":"44.904247"}{"text":"Koo , Terry ; Carreras Prez , Xavier ; Collins , Michael ( 2008 )Conference report Open Access .We present a simple and effective semisupervised method for training dependency parsers .We focus on the problem of lexical representation , introducing features that incorporate word clusters derived from a large unannotated ... .","label":"CompareOrContrast","metadata":{},"score":"44.95377"}{"text":"We show that the proposed model performs at least as well as an approach based on statistical machine translation on two problems of name transliteration , and provide evidence that the combination of the two approaches promises further improvement .In this paper we propose an instance based method for lexical entailment and apply it to automatic ontology population from text .","label":"CompareOrContrast","metadata":{},"score":"44.96472"}{"text":"Semantic inference is a core component of many natural language applications .In response , several researchers have developed algorithms for automatically learning inference rules from textual corpora .However , these rules are often either imprecise or underspecified in directionality .","label":"CompareOrContrast","metadata":{},"score":"44.98913"}{"text":"The basic idea is to approximate the class probability using a sigmoid function . \" ...We consider the task of tuning hyperparameters in SVM models based on minimizing a smooth performance validation function , e.g. , smoothed k - fold crossvalidation error , using non - linear optimization techniques .","label":"CompareOrContrast","metadata":{},"score":"45.03303"}{"text":"The statistical goodness measure ( SGM ) of the exemplary parser uses a generative grammar approach .In a generative grammar approach , each sentence has a top - down derivation consisting of a sequence of rule applications ( i.e. , transitions ) .","label":"CompareOrContrast","metadata":{},"score":"45.039806"}{"text":"The statistical goodness measure ( SGM ) of the exemplary parser uses a generative grammar approach .In a generative grammar approach , each sentence has a top - down derivation consisting of a sequence of rule applications ( i.e. , transitions ) .","label":"CompareOrContrast","metadata":{},"score":"45.039806"}{"text":"In both the English all - words task and the English lexical sample task , the method achieved significant improvement over the simple naive Bayes classifier and higher accuracy than the best offical scores on Senseval-3 for both task .We develop latent Dirichlet allocation with WordNet ( LDAWN ) , an unsupervised probabilistic topic model that includes word sense as a hidden variable .","label":"CompareOrContrast","metadata":{},"score":"45.057594"}{"text":"Each sentence - tree pair in a language has an associated top - down derivation consisting of a sequence of rule applications ( transitions ) of a grammar .Augmented Phrase Structured Grammar ( APSG ) .An APSG is a CFG that gives multiple names to each rule , thereby limiting the application of each \" named \" rule .","label":"CompareOrContrast","metadata":{},"score":"45.097305"}{"text":"Each sentence - tree pair in a language has an associated top - down derivation consisting of a sequence of rule applications ( transitions ) of a grammar .Augmented Phrase Structured Grammar ( APSG ) .An APSG is a CFG that gives multiple names to each rule , thereby limiting the application of each \" named \" rule .","label":"CompareOrContrast","metadata":{},"score":"45.097305"}{"text":"If the training corpus approximates the natural language usage of a given purpose ( general , specific , or customized ) , then the counts also approximate the natural language usage for the same purpose .At run - time , these pre - calculated counts are used to quickly determine the probability of the parse tree .","label":"CompareOrContrast","metadata":{},"score":"45.112606"}{"text":"We consistently observed significant improvements on several test sets in multiple languages covering different genres .This paper proposes a method using the existing Rule - based Machine Translation ( RBMT ) system as a black box to produce synthetic bilingual corpus , which will be used as training data for the Statistical Machine Translation ( SMT ) system .","label":"CompareOrContrast","metadata":{},"score":"45.159348"}{"text":"In deterministic approaches to this task , dependency trees are constructed by series of actions of attaching a bunsetsu chunk to one of the nodes in the tree being constructed .Conventional techniques select the node based on whether the new bunsetsu chunk and each node in the trees are in a parent - child relation or not .","label":"CompareOrContrast","metadata":{},"score":"45.1783"}{"text":"5 shows the result of an adjoin - left operation during parsing .FIG .6 shows the result of an adjoin - right operation during parsing .FIG .7A illustrates one embodiment of a sentence found in the annotated training data .","label":"CompareOrContrast","metadata":{},"score":"45.19818"}{"text":"2 illustrates one simplified embodiment of a template or frame that may be found in an application schema .FIG .3 illustrates a parse generated by a structured language model .FIG .4 illustrates a word - parse k - prefix .","label":"CompareOrContrast","metadata":{},"score":"45.223206"}{"text":"2 .Thus , the problem of information extraction can be viewed as the recovery of a two - level semantic parse for a given word sequence .In accordance with one embodiment of the present invention , a data driven approach to information extraction uses a structured language model ( SLM ) .","label":"CompareOrContrast","metadata":{},"score":"45.274544"}{"text":"All we need for training a model are retrieval examples in the form of triplet constraints , i.e. examples ... .Suzuki , Jun ; Isozaki , Hideki ; Carreras Prez , Xavier ; Collins , Michael ( 2009 ) Conference report Open Access .","label":"CompareOrContrast","metadata":{},"score":"45.282867"}{"text":"Instead of collecting more and more parallel training corpora , this paper aims to improve SMT performance by exploiting full potential of the existing parallel corpora .Two kinds of methods are proposed : offline data optimization and online model optimization .","label":"CompareOrContrast","metadata":{},"score":"45.30902"}{"text":"The semantic schema for an application will thus contain a set of templates , or frames , that define an action to be taken by the computer .The frames or templates have one or more slots that are to be filled in from the input text in order to prompt the action to be performed .","label":"CompareOrContrast","metadata":{},"score":"45.312904"}{"text":"Using the chain rule , this approach then conditions each feature on the parent feature and all features earlier in the sequence : .Prob .b .b .b .g .c .c .c . g . a . a . a .","label":"CompareOrContrast","metadata":{},"score":"45.35125"}{"text":"Using the chain rule , this approach then conditions each feature on the parent feature and all features earlier in the sequence : .Prob .b .b .b .g .c .c .c . g . a . a . a .","label":"CompareOrContrast","metadata":{},"score":"45.35125"}{"text":"A PCFG is a context free grammar where every transition is assigned a probability from zero to one .PCFGs have commonly been used to define a parser 's \" goodness \" function .\" Goodness \" is a calculated measurement of the likelihood that a parse represents the intended meaning of the human speaker .","label":"CompareOrContrast","metadata":{},"score":"45.389618"}{"text":"A PCFG is a context free grammar where every transition is assigned a probability from zero to one .PCFGs have commonly been used to define a parser 's \" goodness \" function .\" Goodness \" is a calculated measurement of the likelihood that a parse represents the intended meaning of the human speaker .","label":"CompareOrContrast","metadata":{},"score":"45.389618"}{"text":"We also show that our techniques can be applied to full - scale parsing applications by demonstrating its effectiveness in learning state - split grammars .We explore the use of Wikipedia as external knowledge to improve named entity recognition ( NER ) .","label":"CompareOrContrast","metadata":{},"score":"45.394917"}{"text":"g .b .b .g .c .c . g .BACKGROUND SUMMARY .It is desirable for a NLP parser to be able to computationally choose the most probable parse from the potentially large number of possible parses .","label":"CompareOrContrast","metadata":{},"score":"45.44341"}{"text":"g .b .b .g .c .c . g .BACKGROUND SUMMARY .It is desirable for a NLP parser to be able to computationally choose the most probable parse from the potentially large number of possible parses .","label":"CompareOrContrast","metadata":{},"score":"45.44341"}{"text":"The proposed method has important applications in areas such as computational biology , natural language processing , information retrieval / extraction , and optical character recognition .Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach . by Koby Crammer , Ofer Dekel , Shai Shalev - Shwartz , Yoram Singer - JMLR , 2006 . \" ...","label":"CompareOrContrast","metadata":{},"score":"45.535446"}{"text":"A method for ranking multiple parse trees , each tree representing a syntactically valid parse of a phase , the method comprising : . determining statistical goodness measures ( SGMs ) of each parse tree by the method as recited in . claim 10 to get an SGM values associated with each tree ; . organizing the trees in order of each tree 's associated SGM value .","label":"CompareOrContrast","metadata":{},"score":"45.561623"}{"text":"Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . .","label":"CompareOrContrast","metadata":{},"score":"45.572197"}{"text":"Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . .","label":"CompareOrContrast","metadata":{},"score":"45.572205"}{"text":"In scientific literature , sentences that cite related work can be a valuable resource for applications such as summarization , synonym identification , and entity extraction .In order to determine which equivalent entities are discussed in the various citation sentences , we propose aligning the words within these sentences according to semantic similarity .","label":"CompareOrContrast","metadata":{},"score":"45.64084"}{"text":"Experimental results using the TREC dataset are shown to significantly outperform strong state - of - the - art baselines .Previous machine learning techniques for answer selection in question answering ( QA ) have required question - answer training pairs .","label":"CompareOrContrast","metadata":{},"score":"45.703804"}{"text":"Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .They only determine parsing actions stepwisely by a trained classifier .To globally model parsing actions of all steps that are taken on the input sentence , we propose two kinds of probabilistic parsing action models that can compute the probability of the whole dependency tree .","label":"CompareOrContrast","metadata":{},"score":"45.779507"}{"text":"The following is a list of syntactic phenomena that are incorporated in a syntactic history .This list is intended to provide examples of syntactic phenomena tracked as syntactic history by the exemplary parser .This is list is not exclusive of other possible phenomena and is not intended to be limiting .","label":"CompareOrContrast","metadata":{},"score":"45.83741"}{"text":"The following is a list of syntactic phenomena that are incorporated in a syntactic history .This list is intended to provide examples of syntactic phenomena tracked as syntactic history by the exemplary parser .This is list is not exclusive of other possible phenomena and is not intended to be limiting .","label":"CompareOrContrast","metadata":{},"score":"45.83741"}{"text":"Carreras Prez , Xavier ; Collins , Michael ; Koo , Terry ( Coling 2008 Organizing Committee , 2008 )Conference report Open Access .We describe a parsing approach that makes use of the perceptron algorithm , in conjunction with dynamic programming methods , to recover full constituent - based parse trees .","label":"CompareOrContrast","metadata":{},"score":"45.961136"}{"text":"During the training phase , the exemplary parser pre - calculates the counts that are needed to compute the PredParamRule Probability and the SynBigram Probability at run - time .Although this process tends to be time - consuming , processor - intensive , and resource - intensive , it only need be once for a given training corpus .","label":"CompareOrContrast","metadata":{},"score":"46.094246"}{"text":"During the training phase , the exemplary parser pre - calculates the counts that are needed to compute the PredParamRule Probability and the SynBigram Probability at run - time .Although this process tends to be time - consuming , processor - intensive , and resource - intensive , it only need be once for a given training corpus .","label":"CompareOrContrast","metadata":{},"score":"46.094246"}{"text":"this paper , we have proposed novel methods for robust parsing that integrate the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques .Our thesis is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions ( Supertags ) that impose complex constraints in a local context .","label":"CompareOrContrast","metadata":{},"score":"46.11325"}{"text":"State - of - the - art performance on Hebrew Treebank parsing is demonstrated using the new method .The benefits of joint inference are modest with the current component models , but appear to increase as components themselves improve .This paper proposes a new bootstrapping approach to unsupervised part - of - speech induction for resource - scarce languages .","label":"CompareOrContrast","metadata":{},"score":"46.11477"}{"text":"The paper reports a hybridization experiment , where an existing ML dependency parser ( LingPars ) , was allowed access to Constraint Grammar analyses provided by a rule - based parser ( EngGram ) for the same data .Descriptive compatibility issues and their influence on performance are discussed , such as tokenization problems , category bundling and dependency head conventions .","label":"CompareOrContrast","metadata":{},"score":"46.116653"}{"text":"Using the WordNet hierarchy , we embed the construction of Abney and Light in the topic model and show that automatically learned domains improve WSD accuracy compared to alternative contexts .This paper focuses on the evaluation of methods for the automatic acquisition of Multiword Expressions ( MWEs ) for robust grammar engineering .","label":"CompareOrContrast","metadata":{},"score":"46.17646"}{"text":"It presents the parse with the greatest SGM value as the one that most likely represents the intended meaning of the speaker .The goodness function of this parse ranker is highly accurate in representing the intended meaning of a speaker .","label":"CompareOrContrast","metadata":{},"score":"46.181984"}{"text":"It presents the parse with the greatest SGM value as the one that most likely represents the intended meaning of the speaker .The goodness function of this parse ranker is highly accurate in representing the intended meaning of a speaker .","label":"CompareOrContrast","metadata":{},"score":"46.181984"}{"text":"These transitions are used to parse a phrase .This indicates that symbol ( symbolA ) on the left side of the rule may be rewritten as one or more symbols ( symbolB , symbolC , etc . ) on the right side of the rule .","label":"CompareOrContrast","metadata":{},"score":"46.184467"}{"text":"These transitions are used to parse a phrase .This indicates that symbol ( symbolA ) on the left side of the rule may be rewritten as one or more symbols ( symbolB , symbolC , etc . ) on the right side of the rule .","label":"CompareOrContrast","metadata":{},"score":"46.184467"}{"text":"In task ( 1 ) , cross - lingual measures are superior to conventional monolingual measures based on a wordnet .In task ( 2 ) , cross - lingual measures are able to solve more problems correctly , and despite scores being affected by many tied answers , their overall performance is again better than the best monolingual measures .","label":"CompareOrContrast","metadata":{},"score":"46.203358"}{"text":"Lexical chains have been successfully employed to evaluate lexical cohesion of text segments and to predict topic boundaries .Our approach is based in the notion of semantic cohesion .It uses spectral embedding to estimate semantic association between content nouns over a span of multiple text segments .","label":"CompareOrContrast","metadata":{},"score":"46.23813"}{"text":"With this parse ranker , the SGM of a particular parse is the combination of all of the probabilities of each node within the parse tree of such parse .The probability at a given node is the probability of taking a transition ( \" grammar rule \" ) at that point .","label":"CompareOrContrast","metadata":{},"score":"46.265358"}{"text":"With this parse ranker , the SGM of a particular parse is the combination of all of the probabilities of each node within the parse tree of such parse .The probability at a given node is the probability of taking a transition ( \" grammar rule \" ) at that point .","label":"CompareOrContrast","metadata":{},"score":"46.265358"}{"text":"With this parse ranker , the SGM of a particular parse is the combination of all of the probabilities of each node within the parse tree of such parse .The probability at a given node is the probability of taking a transition ( \" grammar rule \" ) at that point .","label":"CompareOrContrast","metadata":{},"score":"46.265358"}{"text":"With this parse ranker , the SGM of a particular parse is the combination of all of the probabilities of each node within the parse tree of such parse .The probability at a given node is the probability of taking a transition ( \" grammar rule \" ) at that point .","label":"CompareOrContrast","metadata":{},"score":"46.265358"}{"text":"Our approach combines a set of hand - written patterns together with a probabilistic model .Because the patterns heavily utilize regular expressions , the pertinent tree structures are covered using a limited number of patterns .The probabilistic model is essentially a probabilistic context - free grammar ( PCFG ) approach with the patterns acting as the terminals in production rules .","label":"CompareOrContrast","metadata":{},"score":"46.279266"}{"text":"\" No conventional approach has this property .Another example set is ' I admonish the children . ' and ' I admonish . 'In the exemplary parser , the transition probabilities are conditioned on s headwords .Using a training corpus , the exemplary parser counts up the number of times a specific headword is modified by a rule and the number of times it is n't .","label":"CompareOrContrast","metadata":{},"score":"46.343163"}{"text":"\" No conventional approach has this property .Another example set is ' I admonish the children . ' and ' I admonish . 'In the exemplary parser , the transition probabilities are conditioned on headwords .Using a training corpus , the exemplary parser counts up the number of times a specific headword is modified by a rule and the number of times it is n't .","label":"CompareOrContrast","metadata":{},"score":"46.397602"}{"text":"At run - time , the goodness function is quickly computed using these pre - computed probabilities ( which may be \" counts \" ) .Conditioning on Headwords .Consider parse trees 90 and 92 shown in .FIG .5 a .","label":"CompareOrContrast","metadata":{},"score":"46.404343"}{"text":"Thus while human judgement is not straightforward and it is difficult to create a Pan - Chinese lexicon manually , it is observed that combining simple clustering methods with the appropriate data sources appears to be a promising approach toward its automatic construction .","label":"CompareOrContrast","metadata":{},"score":"46.411903"}{"text":"A goodness function may be calculated using a generative grammar approach .Each sentence has a top - down derivation consisting of a sequence of rule applications ( transitions ) .The probability of the parse tree is defined to be the product of the probabilities of the transitions .","label":"CompareOrContrast","metadata":{},"score":"46.52526"}{"text":"A goodness function may be calculated using a generative grammar approach .Each sentence has a top - down derivation consisting of a sequence of rule applications ( transitions ) .The probability of the parse tree is defined to be the product of the probabilities of the transitions .","label":"CompareOrContrast","metadata":{},"score":"46.52526"}{"text":"One line of research focuses on the amount of evidence that infants need to generalize principles that are either found or not found among human languages .The other line focuses on how infants generalize from input that has at least two possible structural descriptions .","label":"CompareOrContrast","metadata":{},"score":"46.5517"}{"text":"Most of these algorithms avoid the known hardness results by defining parameters beyond the ... .Koo , Terry ; Globerson , Amir ; Carreras Prez , Xavier ; Collins , Michael ( 2007 ) Conference report Open Access .This paper provides an algorithmic framework for learning statistical models involving directed spanning trees , or equivalently non - projective dependency structures .","label":"CompareOrContrast","metadata":{},"score":"46.613293"}{"text":"For each parse tree t in the training corpus .For each node n in t .End loop .End loop .Run - Time Phase .Given the counts computed in the training phase ( described above ) , the goodness measure for a given parse tree may be calculated quickly and efficiently .","label":"CompareOrContrast","metadata":{},"score":"46.767456"}{"text":"While recent progress in machine learning has mainly focused on designing flexible and powerful input representations , this paper addresses the complementary ... \" .Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence .","label":"CompareOrContrast","metadata":{},"score":"46.807877"}{"text":"In order to initialize the SLM , all that is needed is the syntactic portion of the annotated training data .In that case , a general purpose parser can be used to generate a syntactic tree bank from which the SLM parameters can be initialized .","label":"CompareOrContrast","metadata":{},"score":"46.82859"}{"text":"The samples above show a clause ( denoted by underscore ) modifying a noun , adjective , and adverb respectively .This is generally a rare construction , however it is common for constructing comparatives .For a comparative structure like this , all other post - modifiers are reduced in probability .","label":"CompareOrContrast","metadata":{},"score":"46.84143"}{"text":"We introduce a new smoothing method , dubbed Stupid Backoff , that is inexpensive to train on large data sets and approaches the quality of Kneser - Ney Smoothing as the amount of training data increases .We present an extension of phrase - based statistical machine translation models that enables the straight - forward integration of additional annotation at the word - level --- may it be linguistic markup or automatically generated word classes .","label":"CompareOrContrast","metadata":{},"score":"46.904945"}{"text":"Semantic schema is often used for many different purposes .For example , semantic schema serves as the specification for a language - enabled application .In other words , once a semantic schema is defined , grammar and application logic development can proceed simultaneously according to the semantic schema .","label":"CompareOrContrast","metadata":{},"score":"47.05294"}{"text":"A node 's syntactic history is the relevant grammatical environment that a node finds itself in .It may include the history of transitions that occur above the node .For example , is the node below a NREL , PRPRT , PTPRT , RELCL , or AVPVP ?","label":"CompareOrContrast","metadata":{},"score":"47.114998"}{"text":"A node 's syntactic history is the relevant grammatical environment that a node finds itself in .It may include the history of transitions that occur above the node .For example , is the node below a NREL , PRPRT , PTPRT , RELCL , or AVPVP ?","label":"CompareOrContrast","metadata":{},"score":"47.114998"}{"text":"i . )Count . word .i . and .word . k . before .word .i . )Count . word . k . )This conditional probability is much more accurate than Prob(word i ) .This technique is commonly used by conventionally speech recognizers to predict what words are likely to follow a given speech fragment .","label":"CompareOrContrast","metadata":{},"score":"47.140953"}{"text":"i . )Count . word .i . and .word . k . before .word .i . )Count . word . k . )This conditional probability is much more accurate than Prob(word i ) .This technique is commonly used by conventionally speech recognizers to predict what words are likely to follow a given speech fragment .","label":"CompareOrContrast","metadata":{},"score":"47.140953"}{"text":"During test or run time , one embodiment of the present invention constrains the parser with the semantic schema such that the parser only considers parses having structures which do not violate the structures in the semantic schema .In addition , the output of one embodiment of the present invention is a desired number of semantic parse trees which are summed over all parse trees that have the same semantic annotation .","label":"CompareOrContrast","metadata":{},"score":"47.223442"}{"text":"The evaluation showed that PDMM is more effective than PMM .We address the problem of smoothing translation probabilities in a bilingual N - gram - based statistical machine translation system .It is proposed to project the bilingual tuples onto a continuous space and to estimate the translation probabilities in this representation .","label":"CompareOrContrast","metadata":{},"score":"47.258957"}{"text":"Experimental results are presented for composite translations computed from large numbers of different research systems as well as a set of translation systems derived from one of the best - ranked machine translation engines in the 2006 NIST machine translation evaluation .","label":"CompareOrContrast","metadata":{},"score":"47.263153"}{"text":"In addition , the lemma of the headword can be used in place of the headword .PredParamRule Probability and SynBigram Probability .As described above , the probability of a parse tree is the product of the probabilities of each node .","label":"CompareOrContrast","metadata":{},"score":"47.268486"}{"text":"In addition , the lemma of the headword can be used in place of the headword .PredParamRule Probability and SynBigram Probability .As described above , the probability of a parse tree is the product of the probabilities of each node .","label":"CompareOrContrast","metadata":{},"score":"47.268486"}{"text":"Each transition probability within the tree is conditioned on highly predicative linguistic phenomena .Such phenomena include headwords , \" phrase levels , \" \" syntactic biagrams , \" and \" syntactic history \" .Herein , the term \" linguistic features \" is used to generically describe transitions , headwords , phrase levels , and syntactic history .","label":"CompareOrContrast","metadata":{},"score":"47.27549"}{"text":"Query segmentation is the process of taking a user 's search - engine query and dividing the tokens into individual phrases or semantic units .Identification of these query segments can potentially improve both document retrieval precision , by first returning pages which contain the exact query segments , and document retrieval recall , by allowing query expansion or substitution via the segmented units .","label":"CompareOrContrast","metadata":{},"score":"47.281258"}{"text":"The probabilities of the exemplary parser are conditioned on transition name , headword , phrase level , and syntactic history .In other words , there may be two transitions with the same structure that have different probabilities because their transition names are different .","label":"CompareOrContrast","metadata":{},"score":"47.29738"}{"text":"The probabilities of the exemplary parser are conditioned on transition name , headword , phrase level , and syntactic history .In other words , there may be two transitions with the same structure that have different probabilities because their transition names are different .","label":"CompareOrContrast","metadata":{},"score":"47.29738"}{"text":"For this example , the transition probabilities are conditioned on segtype : .Prob(parse ) .Prob . parse . ) i .Prob .n .i . ) i .Prob . trans .n .i . ) segtype .","label":"CompareOrContrast","metadata":{},"score":"47.328205"}{"text":"The analysis grammar build a phrase up by first producing an HW from a word .This is the head word of the phrase .It then enforces an order of levels by attaching modifiers of the headword in increasing phrase level order .","label":"CompareOrContrast","metadata":{},"score":"47.391563"}{"text":"The analysis grammar build a phrase up by first producing an HW from a word .This is the head word of the phrase .It then enforces an order of levels by attaching modifiers of the headword in increasing phrase level order .","label":"CompareOrContrast","metadata":{},"score":"47.391563"}{"text":"( transition , headword , phrase level , syntactic history , segtype ) .( headword , phrase level , syntactic history , segtype ) .( modifying headword , transition , headword ) .( transition , headword ) .In the exemplary implementation of the parser , this is done by creating four multi - dimensional arrays - one for each set of counts .","label":"CompareOrContrast","metadata":{},"score":"47.40809"}{"text":"( transition , headword , phrase level , syntactic history , segtype ) .( headword , phrase level , syntactic history , segtype ) .( modifying headword , transition , headword ) .( transition , headword ) .In the exemplary implementation of the parser , this is done by creating four multi - dimensional arrays - one for each set of counts .","label":"CompareOrContrast","metadata":{},"score":"47.40809"}{"text":"That is , whenever a constituent with the same history is generated a second time , it is discarded if its probability is lower than the original version .I .. \" ...This article considers approaches which rerank the output of an existing probabilistic parser .","label":"CompareOrContrast","metadata":{},"score":"47.41014"}{"text":"We describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order .The resulting system is used as a preprocessor for both training and test sentences , transforming Chinese sentences to be much closer to English in terms of their word order .","label":"CompareOrContrast","metadata":{},"score":"47.43623"}{"text":"Its space requirements fall significantly below lossless information - theoretic lower bounds but it produces false positives with some quantifiable probability .Here we present a general framework for deriving smoothed language model probabilities from BFs .We investigate how a BF containing n -gram statistics can be used as a direct replacement for a conventional n -gram model .","label":"CompareOrContrast","metadata":{},"score":"47.631035"}{"text":"Each transition probability within the tree is conditioned on highly predicative linguistic phenomena .Such phenomena include headwords , \" phrase levels . \" \" syntactic biagrams , \" and \" syntactic history \" .Herein , the term \" linguistic features \" is used to generically describe transitions , headwords , phrase levels , and syntactic history .","label":"CompareOrContrast","metadata":{},"score":"47.639145"}{"text":"DeSR implements an incremental deterministic Shift / Reduce parsing algorithm , using specific rules to handle non - projective dependencies .For the multilingual track we adopted a second order averaged perceptron and performed feature selection to tune a feature model for each language .","label":"CompareOrContrast","metadata":{},"score":"47.724964"}{"text":"where X ranges over all the nodes in the parse tree .This represents the statistical goodness measure ( SGM ) of the exemplary parser .This may be divided into to two parts .For convenience , the first probability will be called the predictive - parameter - and - rule probability or simply \" PredParamRule Probability \" and the second probability will be called the \" SynBigram Probability \" .","label":"CompareOrContrast","metadata":{},"score":"47.854683"}{"text":"where X ranges over all the nodes in the parse tree .This represents the statistical goodness measure ( SGM ) of the exemplary parser .This may be divided into to two parts .For convenience , the first probability will be called the predictive - parameter - and - rule probability or simply \" PredParamRule Probability \" and the second probability will be called the \" SynBigram Probability \" .","label":"CompareOrContrast","metadata":{},"score":"47.854683"}{"text":"Again , \" after \" is relevant to creation of a parse tree and the ordering of the application of the grammar rules .The term does not relate to the order of standard writing or reading .For more complex noun phrases , the grammarian building a set of rules has some options .","label":"CompareOrContrast","metadata":{},"score":"47.857117"}{"text":"Again , \" after \" is relevant to creation of a parse tree and the ordering of the application of the grammar rules .The term does not relate to the order of standard writing or reading .For more complex noun phrases , the grammarian building a set of rules has some options .","label":"CompareOrContrast","metadata":{},"score":"47.857117"}{"text":"The training corpus used to determine conditional probabilities must necessarily be a subset of this set of infinite combinations .Thus , the sparse data problem results in poor probabilities with a given combination when the training corpus did not include that given combination .","label":"CompareOrContrast","metadata":{},"score":"47.88507"}{"text":"The training corpus used to determine conditional probabilities must necessarily be a subset of this set of infinite combinations .Thus , the sparse data problem results in poor probabilities with a given combination when the training corpus did not include that given combination .","label":"CompareOrContrast","metadata":{},"score":"47.88507"}{"text":"The exemplary ranking parser is operational with numerous other general purpose or special purpose computing system environments or configurations .The exemplary ranking parser may be described in the general context of computer - executable instructions , such as program modules , being executed by a computer .","label":"CompareOrContrast","metadata":{},"score":"47.94373"}{"text":"The exemplary ranking parser is operational with numerous other general purpose or special purpose computing system environments or configurations .The exemplary ranking parser may be described in the general context of computer - executable instructions , such as program modules , being executed by a computer .","label":"CompareOrContrast","metadata":{},"score":"47.94373"}{"text":"We investigate methods to improve the recall in coreference resolution by also trying to resolve those definite descriptions where no earlier mention of the referent shares the same lexical head ( coreferent bridging ) .The problem , which is notably harder than identifying coreference relations among mentions which have the same lexical head , has been tackled with several rather different approaches , and we attempt to provide a meaningful classification along with a quantitative comparison .","label":"CompareOrContrast","metadata":{},"score":"47.95826"}{"text":"Further , the semantic schema is language independent , in the sense that it does not specify the linguistic expressions used to express a concept .Therefore , it is used not only for language - enabling applications , but also for integrating inputs from multi - modalities , such as mouse click events .","label":"CompareOrContrast","metadata":{},"score":"48.132515"}{"text":"We also present an analysis of what is and is not learned by our system .This paper describes ETK ( Ensemble of Transformation based Keys ) a new algorithm for inducing search keys for name filtering .ETK has the low computational cost and ability to filter by phonetic similarity characteristic of phonetic keys such as Soundex , but is adaptable to alternative similarity models .","label":"CompareOrContrast","metadata":{},"score":"48.206932"}{"text":"The result might be : .Using the PCFG represented by Table 2 above , the probability of a parse tree can be computed below as follows : .Prob .S . )Prob .s .np .vp . )","label":"CompareOrContrast","metadata":{},"score":"48.53074"}{"text":"The result might be : .Using the PCFG represented by Table 2 above , the probability of a parse tree can be computed below as follows : .Prob .S . )Prob .s .np .vp . )","label":"CompareOrContrast","metadata":{},"score":"48.53074"}{"text":"We exploit methods to fine - tune the classifier and investigate a variety of features of different types .We rely on automatic MT evaluation metrics to approximate human judgements in our experiments .Experimental results show that our system can achieve 0.85 precision at 0.89 recall , excluding exact matches .","label":"CompareOrContrast","metadata":{},"score":"48.544167"}{"text":"The NLP parser takes a phrase and builds for the computer a representation of the syntax of the phrase that the computer can understand .A parser may produce multiple different representations for a given phrase .The representation makes explicit the role each word plays and the relationships between the words , much in the same way as grade school children diagram sentences .","label":"CompareOrContrast","metadata":{},"score":"48.57878"}{"text":"The NLP parser takes a phrase and builds for the computer a representation of the syntax of the phrase that the computer can understand .A parser may produce multiple different representations for a given phrase .The representation makes explicit the role each word plays and the relationships between the words , much in the same way as grade school children diagram sentences .","label":"CompareOrContrast","metadata":{},"score":"48.57878"}{"text":"The technology of opinion extraction allows users to retrieve and analyze people 's opinions scattered over Web documents .We define an opinion unit as a quadruple consisting of the opinion holder , the subject being evaluated , the part or the attribute in which it is evaluated , and the value of the evaluation that expresses a positive or negative assessment .","label":"CompareOrContrast","metadata":{},"score":"48.652878"}{"text":"Recent work from our lab suggests that infants not only keep close track of statistical properties of their input , but they use their statistical sensitivity to select among hypotheses about the underlying structures that might have given rise to those statistics .","label":"CompareOrContrast","metadata":{},"score":"48.673058"}{"text":"If so , then the process examines the next tree at 312 and then loops back through blocks 306 - 310 .If all trees of a phrase have been examined , then the exemplary parser ranks each parse tree based upon their SGM at 314 .","label":"CompareOrContrast","metadata":{},"score":"48.75174"}{"text":"If so , then the process examines the next tree at 312 and then loops back through blocks 306 - 310 .If all trees of a phrase have been examined , then the exemplary parser ranks each parse tree based upon their SGM at 314 .","label":"CompareOrContrast","metadata":{},"score":"48.75174"}{"text":"This multiple rules can build different syntactic relationships .This is an example of how this model may work : Start at the top node .There is a rule that produces the constituents below .Record this rule .Pick the leftmost of the children .","label":"CompareOrContrast","metadata":{},"score":"48.752502"}{"text":"This multiple rules can build different syntactic relationships .This is an example of how this model may work : Start at the top node .There is a rule that produces the constituents below .Record this rule .Pick the leftmost of the children .","label":"CompareOrContrast","metadata":{},"score":"48.752502"}{"text":"Inspired by work in discriminative dependency parsing , the key idea in our approach is to allow highly ... .Carreras Prez , Xavier ; Surdeanu , Mihai ; Mrquez Villodre , Llus ( 2010 ) Conference report Open Access .We describe an online learning dependency parser for the CoNLL - X Shared Task , based on the bottom - up projective algorithm of Eisner ( 2000 ) .","label":"CompareOrContrast","metadata":{},"score":"48.918995"}{"text":"Textual records of business - oriented conversations between customers and agents need to be analyzed properly to acquire useful business insights that improve productivity .For such an analysis , it is critical to identify appropriate textual segments and expressions to focus on , especially when the textual data consists of complete transcripts , which are often lengthy and redundant .","label":"CompareOrContrast","metadata":{},"score":"48.991234"}{"text":"Tree - adjoining grammars ( TAG ) have been proposed as a formalism for generation based on the intuition that the extended domain of syntactic locality that TAGs provide should aid in localizing semantic dependencies as well , in turn serving as an aid to generation from semantic representations .","label":"CompareOrContrast","metadata":{},"score":"48.99622"}{"text":"6 a shows a parse done in accordance with an exemplary grammar .The parse tree of .FIG .6 b shows a parse tree that includes a null transition .FIG .7 shows fragments of a pair of typical parse trees and illustrates the use of syntactic history .","label":"CompareOrContrast","metadata":{},"score":"49.183807"}{"text":"6 a shows a parse done in accordance with an exemplary grammar .The parse tree of .FIG .6 b shows a parse tree that includes a null transition .FIG .7 shows fragments of a pair of typical parse trees and illustrates the use of syntactic history .","label":"CompareOrContrast","metadata":{},"score":"49.183807"}{"text":"The exemplary parser of a NLP system employs a goodness function to rank the possible grammatically correct parses of an utterance .The goodness function of the exemplary parser is highly accurate in representing the intended meaning of a speaker .It also has reasonable training data requirements .","label":"CompareOrContrast","metadata":{},"score":"49.23217"}{"text":"The exemplary parser of a NLP system employs a goodness function to rank the possible grammatically correct parses of an utterance .The goodness function of the exemplary parser is highly accurate in representing the intended meaning of a speaker .It also has reasonable training data requirements .","label":"CompareOrContrast","metadata":{},"score":"49.23217"}{"text":"Morphological analysis and disambiguation are crucial stages in a variety of natural language processing applications , especially when languages with complex morphology are concerned .We present a system which disambiguates the output of a morphological analyzer for Hebrew .It consists of several simple classifiers and a module which combines them under linguistically motivated constraints .","label":"CompareOrContrast","metadata":{},"score":"49.29909"}{"text":"P .p .i . k .W . k .T . k . )P .p .i . k . h .h .It is worth noting that if the binary branching structure developed by the parser were always right - branching and the POStag and non - terminal label vocabularies were mapped to a single type then the model would be equivalent to a trigram language model .","label":"CompareOrContrast","metadata":{},"score":"49.30346"}{"text":"claim 4 . receiving language - usage probabilities based upon appearances of instances of combinations of linguistic features within a training corpus ; . calculating the probability at the node based upon linguistic features of the node and the language - usage probabilities .","label":"CompareOrContrast","metadata":{},"score":"49.41083"}{"text":"Such phenomena include headwords , \" phrase level \" , and \" syntactic history , \" and \" modifying headwords . \" BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1 is a schematic illustration of an exemplary natural language processing system .","label":"CompareOrContrast","metadata":{},"score":"49.410896"}{"text":"Such phenomena include headwords , \" phrase level \" , and \" syntactic history , \" and \" modifying headwords . \" BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1 is a schematic illustration of an exemplary natural language processing system .","label":"CompareOrContrast","metadata":{},"score":"49.410896"}{"text":"To do this , the exemplary parser uses the counts from the training phase ( described above and shown in .FIG .10 ) .At 308 in .FIG .11 , the exemplary parser calculates probability ( i.e. , SGM ) of the tree as a product of the probabilities of the nodes in the tree .","label":"CompareOrContrast","metadata":{},"score":"49.511524"}{"text":"Scalable term selection is proposed to optimize the term set at a given dimensionality according to an expected average vector length .Discriminability and coverage are separately measured ; by adjusting the ratio of their weights in a combined criterion , the expected average vector length can be reached , which means a good compromise between the specificity and the exhaustivity of the term subset .","label":"CompareOrContrast","metadata":{},"score":"49.531815"}{"text":"HashTBO made it possible to ship a trigram contextual speller in Microsoft Office 2007 .In morphologically rich languages , should morphological and syntactic disambiguation be treated sequentially or as a single problem ?We describe several efficient , probabilistically - interpretable ways to apply joint inference to morphological and syntactic disambiguation using lattice parsing .","label":"CompareOrContrast","metadata":{},"score":"49.612732"}{"text":"To reduce the cost of training data construction , our method accepts training examples in which complete word - by - word alignment labels are missing , but instead only the boundaries of coordinated conjuncts are marked .We report promising empirical results in detecting and disambiguating coordinated noun phrases in the GENIA corpus , despite a relatively small number of training examples and minimal features are employed .","label":"CompareOrContrast","metadata":{},"score":"49.638687"}{"text":"Sentence compression holds promise for many applications ranging from summarisation to subtitle generation and information retrieval .The task is typically performed on isolated sentences without taking the surrounding context into account , even though most applications would operate over entire documents .","label":"CompareOrContrast","metadata":{},"score":"49.63957"}{"text":"( This is much easier to describe using code . )This is what is meant by top - down and left to right .This produces a unique representation of the tree .The Problem .Given the ambiguity that exists in natural languages , many sentences have multiple syntactic interpretations .","label":"CompareOrContrast","metadata":{},"score":"49.658768"}{"text":"( This is much easier to describe using code . )This is what is meant by top - down and left to right .This produces a unique representation of the tree .The Problem .Given the ambiguity that exists in natural languages , many sentences have multiple syntactic interpretations .","label":"CompareOrContrast","metadata":{},"score":"49.658768"}{"text":"It may include information that appears elsewhere in the tree .For example , whether the headword of a sibling node is singular or plural .The specifics of what it relevant is dependent upon the specifics of the grammar ( i.e. , rewrite rules or transitions ) being used .","label":"CompareOrContrast","metadata":{},"score":"49.716476"}{"text":"It may include information that appears elsewhere in the tree .For example , whether the headword of a sibling node is singular or plural .The specifics of what it relevant is dependent upon the specifics of the grammar ( i.e. , rewrite rules or transitions ) being used .","label":"CompareOrContrast","metadata":{},"score":"49.716476"}{"text":"The input to an algorithm that learns a binary classifier normally consists of two sets of examples , where one set consists of positive examples of the concept to be learned , and the other set consists of negative examples .However , it is often the case that the available training data are an incomplete set of positive examples , and a set of unlabeled examples , some of which are positive and some of which are negative .","label":"CompareOrContrast","metadata":{},"score":"49.718826"}{"text":"The Bradley - Terry model for obtaining individual skill from paired comparisons has been popular in many areas .In machine learning , this model is related to multi - class probability estimates by coupling all pairwise classification results .Error correcting output codes ( ECOC ) are a general framework to decompose a multi - class problem to several binary problems .","label":"CompareOrContrast","metadata":{},"score":"49.745693"}{"text":"In order to incorporate HTML structure on the graph , three types of cliques are defined based on the HTML tree structure .We propose a method with Conditional Random Fields ( CRFs ) to categorize the nodes on the graph .","label":"CompareOrContrast","metadata":{},"score":"49.77097"}{"text":"Our approach extends WordNet with around 1.5 million meaning links for 800,000 words in over 200 languages , drawing on evidence extracted from a variety of resources including existing ( monolingual ) wordnets , ( mostly bilingual ) translation dictionaries , and parallel corpora .","label":"CompareOrContrast","metadata":{},"score":"49.78263"}{"text":"Experimental results on sentence compression bring significant improvements over a state - of - the - art model .Many emerging applications require documents to be repeatedly updated .Such documents include newsfeeds , webpages , and shared community resources such as Wikipedia .","label":"CompareOrContrast","metadata":{},"score":"49.863052"}{"text":"Each tree represents a grammatically valid parse of the phrase .If there is only one valid parse , there is no need to rank it for apparent reasons .Thus , this process may jump ahead to blocks 316 to report the results and 318 to end the process .","label":"CompareOrContrast","metadata":{},"score":"49.889915"}{"text":"Each tree represents a grammatically valid parse of the phrase .If there is only one valid parse , there is no need to rank it for apparent reasons .Thus , this process may jump ahead to blocks 316 to report the results and 318 to end the process .","label":"CompareOrContrast","metadata":{},"score":"49.889915"}{"text":"Such applications include speech recognition , handwriting recognition , grammar checking , spell checking , formulating database searches , and language translation .The core of a NLP system is its parser .Generally , a parser breaks an utterance ( such as a phrase or sentence ) down into its component parts with an explanation of the form , function , and syntactical relationship of each part .","label":"CompareOrContrast","metadata":{},"score":"49.985687"}{"text":"The standard approach ( using unconditional probability ) for computing Prob(word i ) is to take a training corpus and count up the number of times the word appears .This formula represents this approach : .Prob . word .i . )","label":"CompareOrContrast","metadata":{},"score":"49.992287"}{"text":"The standard approach ( using unconditional probability ) for computing Prob(word i ) is to take a training corpus and count up the number of times the word appears .This formula represents this approach : .Prob . word .i . )","label":"CompareOrContrast","metadata":{},"score":"49.992287"}{"text":"The probability for a given node is the probability that from the node one would take a specific transition , given the linguistic features .where .n X : is the X th node in a parse tree .n y & n z : are the Y th and Z th nodes and children of the X th node .","label":"CompareOrContrast","metadata":{},"score":"50.01228"}{"text":"Evaluation on a list of 500 proper names shows that the method achieves high precision and recall , and outperforms commercial machine translation systems .It has been widely observed that different NLP applications require different sense granularities in order to best exploit word sense distinctions , and that for many applications WordNet senses are too fine - grained .","label":"CompareOrContrast","metadata":{},"score":"50.044453"}{"text":"Parses were assigned a score based upon statistical information and heuristic rules .These scores were often called \" POD \" scores .Since this hodgepodge approach employs heuristics and does not use a unifying methodology for calculating the goodness measure of parses , there are unpredictable and unanticipated results that incorrectly rank the parses .","label":"CompareOrContrast","metadata":{},"score":"50.132988"}{"text":"The savings can be quite substantial ( up to 90 % ) and cause no reduction in BLEU score .In some cases , an improvement in BLEU is obtained at the same time although the effect is less pronounced if state - of - the - art phrasetable smoothing is employed .","label":"CompareOrContrast","metadata":{},"score":"50.321457"}{"text":"We then describe and analyze a new boosting algorithm for combining preferences called RankBoost .We also describe an efficient implementation of the algorithm for certain natural cases .We discuss two experiments we carried out to assess the performance of RankBoost .","label":"CompareOrContrast","metadata":{},"score":"50.32667"}{"text":"Such applications include speech recognition , handwriting recognition , grammar checking , spell checking , formulating database searches , and language translation .The core of a NLP system is its parser .Generally , a parser breaks an utterance ( such as a phrase or sentence ) down into its component parts with an it explanation of the form , function , and syntactical relationship of each part .","label":"CompareOrContrast","metadata":{},"score":"50.422577"}{"text":"( headword , phrase level , syntactic history , segtype ) ; .( modifying headword , transition , headword ) ; and .( transition , headword ) .A method as recited in . claim 10 , wherein the calculating comprises using PredParamRule Probability formula to calculate the probability at the node .","label":"CompareOrContrast","metadata":{},"score":"50.440136"}{"text":"One of the principal bottlenecks in applying learning techniques to classification problems is the large amount of labeled training data required .Especially for images and video , providing training data is very expensive in terms of human time and effort .","label":"CompareOrContrast","metadata":{},"score":"50.59016"}{"text":"One of the principal bottlenecks in applying learning techniques to classification problems is the large amount of labeled training data required .Especially for images and video , providing training data is very expensive in terms of human time and effort .","label":"CompareOrContrast","metadata":{},"score":"50.59016"}{"text":"1 Introduction In this paper , we present a robust parsing approach called supertagging that integrates the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques .The idea underlying the approach is that the ... . ... ing and briefly discuss Feature - based Lexicalized Tree - Adjoining Grammars ( LTAGs ) as a representative of the class of lexicalized grammars . by Yves Schabes , Anne Abeille , Aravind K. Joshi - IN PROCEEDINGS OF THE 12 TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL LINGUISTICS ( COLING&apos;88 , 1988 . \" ...","label":"CompareOrContrast","metadata":{},"score":"50.65864"}{"text":"Since these are highly predicative of the contextually correct parse , this PredParamRule Probability is a significantly more accurate goodness function than conventional techniques .The SynBigram Probability computes the probability of a syntactic bigram .Syntactic bigrams are two - word collocation .","label":"CompareOrContrast","metadata":{},"score":"50.675774"}{"text":"Since these are highly predicative of the contextually correct parse , this PredParamRule Probability is a significantly more accurate goodness function than conventional techniques .The SynBigram Probability computes the probability of a syntactic bigram .Syntactic bigrams are two - word collocation .","label":"CompareOrContrast","metadata":{},"score":"50.675774"}{"text":"To determine which tree was most probable using the conventional Transition Probability Approach ( TPA ) , which is described above in the background section , the number of occurrences of VPwNPrl and VPwAVPr in the corpus is counted .If VPwNPrl occurred most often , the conventional TPA 's goodness function would rank Tree 90 of .","label":"CompareOrContrast","metadata":{},"score":"50.792446"}{"text":"If there is no sibling go up to the parent and visit the parents right sibling .Keep going up and to the right until an unvisited node is found .If all nodes have been visited then we are done .","label":"CompareOrContrast","metadata":{},"score":"50.878613"}{"text":"If there is no sibling go up to the parent and visit the parents right sibling .Keep going up and to the right until an unvisited node is found .If all nodes have been visited then we are done .","label":"CompareOrContrast","metadata":{},"score":"50.878613"}{"text":"1 and by any program 960 - 962 or operating system 958 in .FIG .12 .The operating environments are only examples of suitable operating environments and are not intended to suggest any limitation as to the scope of use of functionality of the ranking parser described herein .","label":"CompareOrContrast","metadata":{},"score":"50.879066"}{"text":"1 and by any program 960 - 962 or operating system 958 in .FIG .12 .The operating environments are only examples of suitable operating environments and are not intended to suggest any limitation as to the scope of use of functionality of the ranking parser described herein .","label":"CompareOrContrast","metadata":{},"score":"50.879066"}{"text":"We have explored these ideas in the context of Lexicalized Tree - Adjoining Grammar ( LTAG ) framework .The supertags in LTAG combine both phrase structure information and dependency information in a single representation .Supertag disambiguation results in a representation that is effectively a parse ( almost parse ) , and the parser needs ' only ' combine the individual supertags .","label":"CompareOrContrast","metadata":{},"score":"50.901566"}{"text":"The Bradley - Terry model for obtaining individual skill from paired comparisons has been popular in many areas .In machine learning , this model is related to multi - class probability estimates by coupling all pairwise classification results .Error correcting output codes ( ECOC ) are a general f ... \" .","label":"CompareOrContrast","metadata":{},"score":"50.999474"}{"text":"Methodologies in ontological semantics are sets of techniques and instructions for acquiring and .NLP - friendliness does not mean just an aspect of formality - it has also to do with literal friendliness ...A natural language parse ranker of a natural language processing ( NLP ) system employs a goodness function to rank the possible grammatically valid parses of an utterance .","label":"CompareOrContrast","metadata":{},"score":"51.031998"}{"text":"X . ) sh .n .X . ) segtype .n .X . )Formula .To simplify Formula 2 , it is noted that not all the parameters are independent .In particular , trn(n X ) and pl(n X ) imply pl(n Y ) and pl(n Z ) .","label":"CompareOrContrast","metadata":{},"score":"51.04219"}{"text":"For example , the segtype of node 54 b in .Node - Associated Functional Notation .In this document , a functional notation is used to refer to the information associated with a node .For example , if a variable \" n \" represents a node in the tree , then \" hw(n ) \" is the headword of node \" n. \" .","label":"CompareOrContrast","metadata":{},"score":"51.07762"}{"text":"For example , the segtype of node 54 b in .Node - Associated Functional Notation .In this document , a functional notation is used to refer to the information associated with a node .For example , if a variable \" n \" represents a node in the tree , then \" hw(n ) \" is the headword of node \" n. \" .","label":"CompareOrContrast","metadata":{},"score":"51.07762"}{"text":"Parses were assigned a score based upon statistical information and heuristic rules .These scores were often called \" POD \" scores .Since this hodgepodge approach employs heuristics and does not use a lo unifying methodology for calculating the goodness measure of parses , there are unpredictable and unanticipated results that incorrectly rank the parses .","label":"CompareOrContrast","metadata":{},"score":"51.257454"}{"text":"The interesting observations might inspire further investigations .Active learning is a promising way to solve the knowledge bottleneck problem faced by supervised word sense disambiguation ( WSD ) methods .Unfortunately , in real - world data , the distribution of the senses of a word is often skewed , which causes a problem for learning methods for WSD .","label":"CompareOrContrast","metadata":{},"score":"51.512104"}{"text":"Because , the amount of training data required is astronomical .First , it requires that the tagged training corpus contain all the sentences that the parser is likely to ever encounter .Second , the sentences must appear in the correct ratios corresponding to their appearance within the normal usage of the natural language .","label":"CompareOrContrast","metadata":{},"score":"51.522552"}{"text":"Because , the amount of training data required is astronomical .First , it requires that the tagged training corpus contain all the sentences that the parser is likely to ever encounter .Second , the sentences must appear in the correct ratios corresponding to their appearance within the normal usage of the natural language .","label":"CompareOrContrast","metadata":{},"score":"51.522552"}{"text":"However , this approach , by itself , produces inaccurate results because the likelihood that a transition will apply is highly dependent upon the current linguistic context , but this approach does not consider the current linguistic context .This approach simply considers occurrences of specific transitions ( trans i ) .","label":"CompareOrContrast","metadata":{},"score":"51.530445"}{"text":"However , this approach , by itself , produces inaccurate results because the likelihood that a transition will apply is highly dependent upon the current linguistic context , but this approach does not consider the current linguistic context .This approach simply considers occurrences of specific transitions ( trans i ) .","label":"CompareOrContrast","metadata":{},"score":"51.530445"}{"text":"i P(P i , W ) ) .This is indicated by blocks 320 and 322 in .FIG .9 .Also , however , ranking component 316 can sum the probability of a semantic parse over all of the parses P that yield the same semantic parse S , and then choose the top N semantic parses with the highest associated probabilities .","label":"CompareOrContrast","metadata":{},"score":"51.559402"}{"text":"To do this , the 8 exemplary parser uses the counts from the training phase ( described above and shown in .FIG .10 ) .At 308 in .FIG .11 , the exemplary parser calculates probability ( i.e. , SGM ) of the tree as a product of the probabilities of the nodes in the tree .","label":"CompareOrContrast","metadata":{},"score":"51.60734"}{"text":"It is well - known that domain specific language models perform well in automatic speech recognition .Domain specific language and translation models in statistical machine translations perform well .However , there are two problems with using domain specific models .","label":"CompareOrContrast","metadata":{},"score":"51.80111"}{"text":"Given a prediction message , Crystal first identifies which party the message predicts to win and then aggregates prediction analysis results of a large amount of opinions to project the election results .We collect past election prediction messages from the Web and automatically build a gold standard .","label":"CompareOrContrast","metadata":{},"score":"51.83852"}{"text":"FIG .4 a would be ninety percent .If the parse of parse tree 70 is the correct parse , then this example provide good results .Note that , the exact sentence had to occur multiple times within the corpus to provide such good results .","label":"CompareOrContrast","metadata":{},"score":"52.051052"}{"text":"FIG .4 a would be ninety percent .If the parse of parse tree 70 is the correct parse , then this example provide good results .Note that , the exact sentence had to occur multiple times within the corpus to provide such good results .","label":"CompareOrContrast","metadata":{},"score":"52.051052"}{"text":"To determine which tree was most probable using the conventional Transition Probability Approach ( TPA ) , which is described above in the background section , the number of occurrences of VPwNPr1 and VPwAVPr in the corpus is counted .If VPwNPr1 occurred most often , the conventional TPA 's goodness function would rank Tree 90 of .","label":"CompareOrContrast","metadata":{},"score":"52.114708"}{"text":"The particular method used for decoding is not important to the present invention and any of several known methods for decoding may be used .The most probable sequence of hypothesis words is illustratively provided to a confidence measure module 422 .","label":"CompareOrContrast","metadata":{},"score":"52.163628"}{"text":"FIG .3 shows a depth first tree walk of the parse tree .The sequence of the walk is shown by the directional path 66 with circled reference points .The order of the stops along the path is numbered from 1 to 14 by the circled reference points .","label":"CompareOrContrast","metadata":{},"score":"52.397247"}{"text":"FIG .3 shows a depth first tree walk of the parse tree .The sequence of the walk is shown by the directional path 66 with circled reference points .The order of the stops along the path is numbered from 1 to 14 by the circled reference points .","label":"CompareOrContrast","metadata":{},"score":"52.397247"}{"text":"However , within each such grouping , the rules can be further subdivided into multiple levels .These levels are called \" phrase levels \" herein .These phrase levels are highly predicative of whether a transition will occur .A null transition is utilized for each phrase level to account for no modification from one level to the next .","label":"CompareOrContrast","metadata":{},"score":"52.43046"}{"text":"However , within each such grouping , the rules can be further subdivided into multiple levels .These levels are called \" phrase levels \" herein .These phrase levels are highly predicative of whether a transition will occur .A null transition is utilized for each phrase level to account for no modification from one level to the next .","label":"CompareOrContrast","metadata":{},"score":"52.43046"}{"text":"Phrase Level Defined .To define the phrase levels for a given segtype , rules that create the given segtype are grouped into levels .All the rules at a given level modify the segtype in the same way ( e.g. , add modifiers to the left ) .","label":"CompareOrContrast","metadata":{},"score":"52.466484"}{"text":"Phrase Level Defined .To define the phrase levels for a given segtype , rules that create the given segtype are grouped into levels .All the rules at a given level modify the segtype in the same way ( e.g. , add modifiers to the left ) .","label":"CompareOrContrast","metadata":{},"score":"52.466484"}{"text":"Our primary goal is the labeling of syntactic nodes with specific argument labels that preserve the similarity of roles such as the window in ... \" .This paper describes our approach to the development of a Proposition Bank , which involves the addition of semantic information to the Penn English Treebank .","label":"CompareOrContrast","metadata":{},"score":"52.484226"}{"text":"But this method does not work well for web query spelling correction , because there is no lexicon that can cover the vast amount of terms occurring across the web .Recent work showed that using search query logs helps to solve this problem to some extent .","label":"CompareOrContrast","metadata":{},"score":"52.60275"}{"text":"For each node n in t .RuleCountNumerator(trn(n ) , hw(n ) , pl(n ) , sh(n ) , .End loop .End loop .Run - time Phase .Given the counts computed in the training phase ( described above ) , the goodness measure for a given parse tree may be calculated quickly and efficiently .","label":"CompareOrContrast","metadata":{},"score":"52.96559"}{"text":"The resulting IE system achieves good performance on the MUC-4 terrorism corpus and ProMed disease outbreak stories .This approach requires only a few seed extraction patterns and a collection of relevant and irrelevant documents for training .This paper proposes a tree kernel with context - sensitive structured parse tree information for re - lation extraction .","label":"CompareOrContrast","metadata":{},"score":"53.07614"}{"text":"This \" training corpus \" may also be called \" training data . \"Thus , the probabilities are empirically derived from analyzing a training corpus .Various approaches exist for doing this .One of the simplest approaches is to use an unconditional probability formula like this : .","label":"CompareOrContrast","metadata":{},"score":"53.114025"}{"text":"This \" training corpus \" may also be called \" training data . \"Thus , the probabilities are empirically derived from analyzing a training corpus .Various approaches exist for doing this .One of the simplest approaches is to use an unconditional probability formula like this : .","label":"CompareOrContrast","metadata":{},"score":"53.114025"}{"text":"If every pair of words in our lexicon is considered and a record of how often that pairs appear is kept , the probability of a specific word pairing is : .Prob . word .i . word . k . appears . before .","label":"CompareOrContrast","metadata":{},"score":"53.192535"}{"text":"If every pair of words in our lexicon is considered and a record of how often that pairs appear is kept , the probability of a specific word pairing is : .Prob . word .i . word . k . appears . before .","label":"CompareOrContrast","metadata":{},"score":"53.192535"}{"text":"This translates into constraints on the order in which the rules can be applied .In other words , some rules must run before other rules .The SGM of the exemplary parser implements phrase levels to make this set of constraints explicit .","label":"CompareOrContrast","metadata":{},"score":"53.2436"}{"text":"This translates into constraints on the order in which the rules can be applied .In other words , some rules must run before other rules .The SGM of the exemplary parser implements phrase levels to make this set of constraints explicit .","label":"CompareOrContrast","metadata":{},"score":"53.2436"}{"text":"1 may be a parse ranker 34 in .FIG .1 may be a device implementing the exemplary parser within a NLP system 20 .Alternatively still , instructions to implement the exemplary parser may be on a computer readable medium .","label":"CompareOrContrast","metadata":{},"score":"53.257782"}{"text":"1 may be a parse ranker 34 in .FIG .1 may be a device implementing the exemplary parser within a NLP system 20 .Alternatively still , instructions to implement the exemplary parser may be on a computer readable medium .","label":"CompareOrContrast","metadata":{},"score":"53.257782"}{"text":"Affect : .The samples above show a clause ( denoted by underscore ) modifying a noun , adjective , and adverb respectively .This is generally a rare construction , however it is common for constructing comparatives .For a comparative structure like this , all other post - modifiers are reduced in probability .","label":"CompareOrContrast","metadata":{},"score":"53.34772"}{"text":"However , in this case , a different rule , at a different phrase level ( Topicalization ) , is used to attach the object to the verb than the usual VPwNPp .If Topicalization is used then the probability of VPwNPp must be lowered when the parser get to the phrase level it operates on .","label":"CompareOrContrast","metadata":{},"score":"53.360435"}{"text":"However , in this case , a different rule , at a different phrase level ( Topicalization ) , is used to attach the object to the verb than the usual VPwNPp .If Topicalization is used then the probability of VPwNPp must be lowered when the parser get to the phrase level it operates on .","label":"CompareOrContrast","metadata":{},"score":"53.360435"}{"text":"FIG .3 is one illustrative syntactic parse of the example input sentence .The vertical line in the input sentence illustrates the place at which processing is to commence .The SLM percolates a headword up to each node in the syntactic parse , wherein the headword is a word that most closely defines that constituent of the sentence .","label":"CompareOrContrast","metadata":{},"score":"53.4439"}{"text":"The type - based models perform better than the models which use tokens for selecting the classes .Furthermore , the models which use the automatically acquired thesaurus entries produced the best results .The correlation for the thesaurus models is stronger than any of the individual features used in previous research on the same dataset .","label":"CompareOrContrast","metadata":{},"score":"53.60696"}{"text":"For approach ( ii ) we use Platt scaling to get probability estimates which are then adjusted using Lemma 1 .For approach ( iii ) we run libSVM twice .The first run uses Platt scaling to get probability ... . by Alessandro Moschitti , Daniele Pighin , Roberto Basili - Computational Linguistics , 2008 . \" ...","label":"CompareOrContrast","metadata":{},"score":"53.663116"}{"text":"The results for boundary recognition , classification , and re - ranking stages provide systematic evidence about the significant impact of tree kernels on the overall accuracy , especially when the amount of training data is small .As a conclusive result , tree kernels allow for a general and easily portable feature engineering method which is applicable to a large family of natural language processing tasks . by Tzu - kuo Huang , Ruby C. Weng , Chih - jen Lin - Journal of Machine Learning Research . \" ...","label":"CompareOrContrast","metadata":{},"score":"53.746216"}{"text":"Our method compares favorably with state - of - the - art algorithms that recover WH - traces .Recent studies focussed on the question whether less - configurational languages like German are harder to parse than English , or whether the lower parsing scores are an artifact of treebank encoding schemes and data structures , as claimed by Kbler et al .","label":"CompareOrContrast","metadata":{},"score":"53.74631"}{"text":"Using the straw man approach , the probability of a parse tree is defined to be : .Prob . parse . )Count . parse . )Total . of . trees . in .the .training .corpus .","label":"CompareOrContrast","metadata":{},"score":"53.86595"}{"text":"Using the straw man approach , the probability of a parse tree is defined to be : .Prob . parse . )Count . parse . )Total . of . trees . in .the .training .corpus .","label":"CompareOrContrast","metadata":{},"score":"53.86595"}{"text":"Automatically summarizing vast amounts of on - line quantitative data with a short natural language paragraph has a wide range of real - world applications .However , this specific task raises a number of difficult issues that are quite distinct from the generic task of language generation : conciseness , ... \" .","label":"CompareOrContrast","metadata":{},"score":"53.871475"}{"text":"This output is one or more of parses 38 ranked from most to least goodness .Foundational Concepts .Three concepts form the foundation for understanding the invention described herein : statistics , linguistics , and computational linguistics .Statistics is the branch of mathematics that deals with the relationships among and between groups of measurements , and with the relevance of similarities and differences in those relationships .","label":"CompareOrContrast","metadata":{},"score":"53.885307"}{"text":"This output is one or more of parses 38 ranked from most to least goodness .Foundational Concepts .Three concepts form the foundation for understanding the invention described herein : statistics , linguistics , and computational linguistics .Statistics is the branch of mathematics that deals with the relationships among and between groups of measurements , and with the relevance of similarities and differences in those relationships .","label":"CompareOrContrast","metadata":{},"score":"53.885307"}{"text":"In the exemplary parser , the transition probabilities are conditioned on syntactic history as well as headwords .Using a training corpus , the exemplary parser counts up how often VPwNPrl occurs in a passive construction with a mono - transitive verb and finds that it never occurs .","label":"CompareOrContrast","metadata":{},"score":"53.91299"}{"text":"One feature of the present invention uses the parsing capabilities of a structured language model in the information extraction process .During training , the structured language model is first initialized with syntactically annotated training data .The model is then trained by generating parses on semantically annotated training data enforcing annotated constituent boundaries .","label":"CompareOrContrast","metadata":{},"score":"53.938168"}{"text":"One feature of the present invention uses the parsing capabilities of a structured language model in the information extraction process .During training , the structured language model is first initialized with syntactically annotated training data .The model is then trained by generating parses on semantically annotated training data enforcing annotated constituent boundaries .","label":"CompareOrContrast","metadata":{},"score":"53.938168"}{"text":"W ... \" .We propose a translation recommendation framework to integrate Statistical Machine Translation ( SMT ) output with Translation Memory ( TM ) systems .The framework recommends SMT outputs to a TM user when it predicts that SMT outputs are more suitable for post - editing than the hits provided by the TM .","label":"CompareOrContrast","metadata":{},"score":"53.95784"}{"text":"In the exemplary parser , the transition probabilities are conditioned on syntactic history as well as headwords .Using a training corpus , the exemplary parser counts up how often VPwNPr1 occurs in a passive construction with a mono - transitive verb and finds that it never occurs .","label":"CompareOrContrast","metadata":{},"score":"53.996223"}{"text":"The reordering approach improved the BLEU score for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data .We also conducted a series of experiments to analyze the accuracy and impact of different types of reordering rules .","label":"CompareOrContrast","metadata":{},"score":"54.04708"}{"text":"We compare V - measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions , using simulated clustering results .Finally , we use V - measure to evaluate two clustering tasks : document clustering and pitch accent type clustering .","label":"CompareOrContrast","metadata":{},"score":"54.075462"}{"text":"At run - time , the goodness function is quickly computed using these pre - computed probabilities ( which may be \" counts \" ) .Conditioning on Headwords .Consider parse trees 90 and 92 shown in .FIG .5 a. Assume the two parse trees are identical except for the transition that created the top - most VP ( verb phrase ) .","label":"CompareOrContrast","metadata":{},"score":"54.092068"}{"text":"One feature of the present invention uses the parsing capabilities of a structured language model in the information extraction process .During training , the structured language model is first initialized with syntactically annotated training data .Applying a structured language model to information extraction US 7805302 B2 .","label":"CompareOrContrast","metadata":{},"score":"54.144997"}{"text":"The parser is evaluated on the OVIS and WSJ corpora , and shows improvements on efficiency , parse accuracy and testset likelihood .Friday , June 29 , 2007 .A lexical analogy is a pair of word - pairs that share a similar semantic relation .","label":"CompareOrContrast","metadata":{},"score":"54.205605"}{"text":"The tenth CoNLL ( CoNLL - X ) saw a shared task on Multilingual Dependency Parsing .Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .","label":"CompareOrContrast","metadata":{},"score":"54.24644"}{"text":"For this task , we compare the performance of RankBoost to the individual search strategies .The second experiment is a collaborative - filtering task for making movie recommendations .Here , we present results comparing RankBoost to nearest - neighbor and regression algorithms . by Ioannis Tsochantaridis , Thorsten Joachims , Thomas Hofmann , Yasemin Altun - JOURNAL OF MACHINE LEARNING RESEARCH , 2005 . \" ...","label":"CompareOrContrast","metadata":{},"score":"54.488945"}{"text":"Confidence measure module 422 then provides the sequence of hypothesis words to an output module 424 along with identifiers indicating which words may have been improperly identified .Those skilled in the art will recognize that confidence measure module 422 is not necessary for the practice of the present invention .","label":"CompareOrContrast","metadata":{},"score":"54.50997"}{"text":"Using a set of one - vs - all Support Vector Machines ( SVMs ) , we evaluate these LTAG - based features .Our experiments show that LTAG - based features can improve SRL accuracy significantly .When compared with the best known set of features that are used in state of the art SRL systems we obtain an improvement in F - score from 82.34 % to 85.25 % .","label":"CompareOrContrast","metadata":{},"score":"54.56958"}{"text":"We describe the application of kernel methods to Natural Language Processing ( NLP ) problems .In many NLP tasks the objects being modeled are strings , trees , graphs or other discrete structures which require some mechanism to convert them into feature vectors .","label":"CompareOrContrast","metadata":{},"score":"54.573418"}{"text":"We describe the application of kernel methods to Natural Language Processing ( NLP ) problems .In many NLP tasks the objects being modeled are strings , trees , graphs or other discrete structures which require some mechanism to convert them into feature vectors .","label":"CompareOrContrast","metadata":{},"score":"54.573418"}{"text":"modhw(n ) is the modifying headword of node n .Annotated Parse Tree .A parse tree can be annotated with information computed during the parsing process .A common form of this is the lexicalized parse tree where each node is annotated with its headword .","label":"CompareOrContrast","metadata":{},"score":"54.6012"}{"text":"modhw(n ) is the modifying headword of node n .Annotated Parse Tree .A parse tree can be annotated with information computed during the parsing process .A common form of this is the lexicalized parse tree where each node is annotated with its headword .","label":"CompareOrContrast","metadata":{},"score":"54.6012"}{"text":"x . )Count .hw .n .x . ) pl .n .x . ) sh .n .x . ) segtype .n .x . )Formula .Moreover , the SynBigram Probability formula ( Formula 11 ) may be calculated as follows : .","label":"CompareOrContrast","metadata":{},"score":"54.655632"}{"text":"X . )Count .hw .n .X . ) pl .n .X . ) sh .n .X . ) segtype .n .X . )Formula .Moreover , the SynBigram Probability formula ( Formula 11 ) may be calculated as follows : .","label":"CompareOrContrast","metadata":{},"score":"54.655632"}{"text":"To predict elec - tion results , we apply SVM - based super - vised learning .To improve performance , we propose a novel technique which generalizes n - gram feature patterns .Experimental results show that Crystal significantly outperforms several baselines as well as a non - generalized n - gram ap - proach .","label":"CompareOrContrast","metadata":{},"score":"54.666893"}{"text":"A method as recited in .claim 1 , wherein the computing comprises counting appearances of instances of combinations of linguistic features within the training corpus .A computer - readable storage medium having computer - executable instructions that , when executed by a computer , performs the method as recited in . claim 1 .","label":"CompareOrContrast","metadata":{},"score":"54.735527"}{"text":"The parse ranker 34 mathematically measures the \" goodness \" of each parse and ranks them . \" Goodness \" is a measure of the likelihood that such a parse represents the intended meaning of the human speaker ( or writer ) .","label":"CompareOrContrast","metadata":{},"score":"54.848843"}{"text":"The parse ranker 34 mathematically measures the \" goodness \" of each parse and ranks them . \" Goodness \" is a measure of the likelihood that such a parse represents the intended meaning of the human speaker ( or writer ) .","label":"CompareOrContrast","metadata":{},"score":"54.848843"}{"text":"These nodes are internal and may be further expanded .Each non - terminal node has immediate children , which form a branch ( i.e. , \" local tree \" ) .Each branch corresponds to the application of a transition .","label":"CompareOrContrast","metadata":{},"score":"54.93495"}{"text":"In parsing we would have training examples fs i ; t i g where each s i is a sentence and each t i is the cor ... . by Paul Kingsbury , Martha Palmer - In Language Resources and Evaluation , 2002 . \" ...","label":"CompareOrContrast","metadata":{},"score":"55.01598"}{"text":"What is known is the information associated with the node and/or with any node previously encountered in the tree walk .For example , the properties of the node , its headword , phrase level , syntactic history , and segtype .","label":"CompareOrContrast","metadata":{},"score":"55.229404"}{"text":"What is known is the information associated with the node and/or with any node previously encountered in the tree walk .For example , the properties of the node , its headword , phrase level , syntactic history , and segtype .","label":"CompareOrContrast","metadata":{},"score":"55.229404"}{"text":"RELATED APPLICATIONS .This application is a continuation of and claims priority to U.S. patent application Ser .No . 09/620,745 , filed Jul. 20 , 2000 , the disclosure of which is incorporated by reference herein .TECHNICAL FIELD .This invention relates to ranking parses produced by a parser for a natural language processing system .","label":"CompareOrContrast","metadata":{},"score":"55.48586"}{"text":"FIG .11 , it will be appreciated that structured language model 420 can perform its speech recognition language model duties in order to recognize the speech input by speaker 400 and then perform feature extraction parsing on the recognized speech , as discussed above .","label":"CompareOrContrast","metadata":{},"score":"55.63974"}{"text":"FIG .9 , the conditional probability of exemplary parser is : .Prob . parse . )X .Prob .n .X . )X .Prob . trn .n .X . ) hw .n .","label":"CompareOrContrast","metadata":{},"score":"55.754242"}{"text":"The SLM is now described in a bit greater detail for the sake of completeness .The model assigns a probability P(W , T ) to every sentence W and its every possible binary parse T. The terminals of T are the words of W with part of speech tags ( POStags ) , and the nodes of T are annotated with phrase headwords and non - terminal labels .","label":"CompareOrContrast","metadata":{},"score":"55.89768"}{"text":"The Penn Tree Bank is the focus of most of those working on probabilistic grammars .The Penn Tree Bank is annotated with parts of speech for the words and minimal phrase names and brackets .The syntactic relationships between constituents are not given .","label":"CompareOrContrast","metadata":{},"score":"55.931965"}{"text":"The Penn Tree Bank is the focus of most of those working on probabilistic grammars .The Penn Tree Bank is annotated with parts of speech for the words and minimal phrase names and brackets .The syntactic relationships between constituents are not given .","label":"CompareOrContrast","metadata":{},"score":"55.931965"}{"text":"An implementation of the exemplary ranking parser may be described in the general context of computer - executable instructions , such as program modules , executed by one or more computers or other devices .Generally , program modules include routines , programs , objects , components , data structures , etc . that perform particular tasks or implement particular abstract data types .","label":"CompareOrContrast","metadata":{},"score":"55.981213"}{"text":"An implementation of the exemplary ranking parser may be described in the general context of computer - executable instructions , such as program modules , executed by one or more computers or other devices .Generally , program modules include routines , programs , objects , components , data structures , etc . that perform particular tasks or implement particular abstract data types .","label":"CompareOrContrast","metadata":{},"score":"55.981213"}{"text":"Sample sentences : .Had I seen such chaos ?Would he be home before 4 a.m ?Did he pass exams ?Affect : .A question can be formed by inverting the subject and an ' auxiliary ' verb .","label":"CompareOrContrast","metadata":{},"score":"55.987034"}{"text":"Sample sentences : .Had I seen such chaos ?Would he be home before 4 a.m ?Did he pass exams ?Affect : .A question can be formed by inverting the subject and an ' auxiliary ' verb .","label":"CompareOrContrast","metadata":{},"score":"55.987034"}{"text":"n .Z . ) hw .n .X . ) pl .n .X . ) sh .n .X . ) segtype .n .X . )OR .Formula .A .Prob . parse . )","label":"CompareOrContrast","metadata":{},"score":"56.073936"}{"text":"FIG .5 a does not use headword annotation , but the parse tree of .FIG .5 b does .FIGS .6 a and 6 b show fragments of two pairs of typical parse trees .The parse tree of .","label":"CompareOrContrast","metadata":{},"score":"56.107925"}{"text":"FIG .5 a does not use headword annotation , but the parse tree of .FIG .5 b does .FIGS .6 a and 6 b show fragments of two pairs of typical parse trees .The parse tree of .","label":"CompareOrContrast","metadata":{},"score":"56.107925"}{"text":"1992 ) .They are important for a few reasons .Many systems applied to part - ofspeech tagging , speech recognition and other language or speech tasks also fall into this class of model .Second , a partic ... . \" ...","label":"CompareOrContrast","metadata":{},"score":"56.303444"}{"text":"Because of this , there is no obvious way to describe a hierarchy by defining in what order modifiers are to be attached to the head .Because there is not hierarchy , there is not place to put null transitions .","label":"CompareOrContrast","metadata":{},"score":"56.33522"}{"text":"Because of this , there is no obvious way to describe a hierarchy by defining in what order modifiers are to be attached to the head .Because there is not hierarchy , there is not place to put null transitions .","label":"CompareOrContrast","metadata":{},"score":"56.33522"}{"text":"Ontological semantics is an integrated complex of theories , methodologies , descriptions and implementations .In ontological semantics , a theory is viewed as a set of statements determin ... \" .This book introduces ontological semantics , a comprehensive approach to the treatment of text meaning by computer .","label":"CompareOrContrast","metadata":{},"score":"56.350975"}{"text":"Conditioning on phrases levels means that any parse tree that violates the phrase level constraints can be eliminated ( given probability equal to zero ) by the exemplary parser .Modeling the Syntactic Modification of Individual Words .In the exemplary parser , the use of phrase levels and null transitions accurately model the syntactic way that words \" want \" to be modified .","label":"CompareOrContrast","metadata":{},"score":"56.439682"}{"text":"Conditioning on phrases levels means that any parse tree that violates the phrase level constraints can be eliminated ( given probability equal to zero ) by the exemplary parser .Modeling the Syntactic Modification of Individual Words .In the exemplary parser , the use of phrase levels and null transitions accurately model the syntactic way that words \" want \" to be modified .","label":"CompareOrContrast","metadata":{},"score":"56.439682"}{"text":"The language model probability assignment for the word at position k+1 in the input sentence is made using : .P .w . k .W . k . )T . k .S . k .P .w . k .","label":"CompareOrContrast","metadata":{},"score":"56.477337"}{"text":"We participated in the CoNLL Shared Task-2007 and evaluated our system for ten languages .We got an average multilingual labeled attachment score of 74.54 % ( with 65.50 % being the average and 80.32 % the highest ) and an average multilingual unlabeled attachment score of 80.30 % ( with 71.13 % being the average and 86.55 % the highest ) .","label":"CompareOrContrast","metadata":{},"score":"56.4981"}{"text":"We expect that our method could be further improved via well - tuned parameter validations for different languages . by Ting - fan Wu , Chih - Jen Lin , Ruby C. Weng - Journal of Machine Learning Research , 2003 . \" ...","label":"CompareOrContrast","metadata":{},"score":"56.610638"}{"text":"FIG .5 b to be much less than Tree 96 of .FIG .5 b. .Phrase Level .Phrases ( e.g. , noun phrases or verb phrases ) have a natural structure .The job of the grammar ( i.e. , grammar rules ) is to build this structure .","label":"CompareOrContrast","metadata":{},"score":"56.670784"}{"text":"FIG .5 b to be much less than Tree 96 of .FIG .5 b. .Phrase Level .Phrases ( e.g. , noun phrases or verb phrases ) have a natural structure .The job of the grammar ( i.e. , grammar rules ) is to build this structure .","label":"CompareOrContrast","metadata":{},"score":"56.670784"}{"text":"10 is a data flow diagram also illustrating the operation of the present system during run - time or test - time , in accordance with one embodiment of the present invention .FIG .11 is a block diagram of a speech recognition system employing a structured language model in accordance with one embodiment of the present invention .","label":"CompareOrContrast","metadata":{},"score":"56.733856"}{"text":"Regardless , the training phase ( or some portion thereof ) is performed , at least momentarily , before the run - time phase .This is because the training phase provides the foundation for the run - time phase to base its SGM calculations .","label":"CompareOrContrast","metadata":{},"score":"56.77115"}{"text":"Their ability to automatically induce features results in multilingual parsing which is robust enough to achieve accuracy well above the average for each individual language in the multilingual track of the CoNLL-2007 shared task .This robustness led to the third best overall average labeled attachment score in the task , despite using no discriminative methods .","label":"CompareOrContrast","metadata":{},"score":"56.906143"}{"text":"An apparatus as recited in . claim 19 , wherein the determiner calculates the probability at the node by using both PredParamRule Probability and SynBigram Probability formulas .A data structure for use with a computer having a processor and a memory , said structure comprising : . a corpus comprising one or more phrases in a natural language ; . parse trees having hierarchical nodes , each tree representing at least one syntactically valid parse of each phase in a subset of the corpus ; . wherein each node as an associated probability , wherein the associated probability of a node is based upon linguistic features of such node and language - usage probabilities derived from appearances of instances of combinations of linguistic features within a training corpus ; . wherein PredParamRule Probability formula is used to calculate a probability associated with a node .","label":"CompareOrContrast","metadata":{},"score":"57.082848"}{"text":"FIG .7A illustrates one example sentence of annotated training data that can be used to train the system , and .FIG .8 is a flow diagram illustrating the training procedure .In order to train the system , the SLM is first initialized with syntactic knowledge using annotated training data .","label":"CompareOrContrast","metadata":{},"score":"57.206207"}{"text":"Bailly , Raphal ; Carreras Prez , Xavier ; Luque , Franco M. ; Quattoni , Ariadna Julieta ( Association for Computational Linguistics , 2013 ) Conference lecture Open Access .We derive a spectral method for unsupervised learning ofWeighted Context Free Grammars .","label":"CompareOrContrast","metadata":{},"score":"57.30429"}{"text":"However , a larger and more accurate training corpus is necessary .Making the Training Phase More Efficient .As described , the training phase has two parts : the preparation and the calculation .However , it should be clear to anyone of ordinary skill in the art that the training phase may be accomplished in one part by merging the steps of each part into a single pass over the training corpus .","label":"CompareOrContrast","metadata":{},"score":"57.39"}{"text":"However , a larger and more accurate training corpus is necessary .Making the Training Phase More Efficient .As described , the training phase has two parts : the preparation and the calculation .However , it should be clear to anyone of ordinary skill in the art that the training phase may be accomplished in one part by merging the steps of each part into a single pass over the training corpus .","label":"CompareOrContrast","metadata":{},"score":"57.39"}{"text":"When building the parse tree for a noun phrase , the determiner ( e.g. , \" the \" ) is attached after the adjectives describing the noun .For example , \" the red book \" is correct , but \" red the book \" is not correct .","label":"CompareOrContrast","metadata":{},"score":"57.447002"}{"text":"When building the parse tree for a noun phrase , the determiner ( e.g. , \" the \" ) is attached after the adjectives describing the noun .For example , \" the red book \" is correct , but \" red the book \" is not correct .","label":"CompareOrContrast","metadata":{},"score":"57.447002"}{"text":"DTG involve two composition operations called subsertion and sister - adjunction .The most distinctive feature of DTG is that , unlike TAG , there is complete uniformity in the way that the two DTG operatio ... \" .DTG are designed to share some of the advantages of TAG while overcoming some of its limitations .","label":"CompareOrContrast","metadata":{},"score":"57.474586"}{"text":"noun . )Prob . noun .ants . )However , this approach does not define a very accurate goodness function .Alone , a PCFG is generally poor at ranking parses correctly .A PCFG prefers common constructions in a language over less common ones .","label":"CompareOrContrast","metadata":{},"score":"57.488937"}{"text":"noun . )Prob . noun .ants . )However , this approach does not define a very accurate goodness function .Alone , a PCFG is generally poor at ranking parses correctly .A PCFG prefers common constructions in a language over less common ones .","label":"CompareOrContrast","metadata":{},"score":"57.488937"}{"text":"Each non - terminal node has immediate children , which form a branch ( i.e. , \" local tree \" ) .Each branch corresponds to the application of a transition .For example , \" np \" 54 b can be further expanded into a \" noun \" by application of the \" np_noun \" transition .","label":"CompareOrContrast","metadata":{},"score":"57.5035"}{"text":"The exemplary parser overcomes the limitations of conventional syntactic bigram approaches by further conditioning the goodness measure on independent probability characteristics .In particular , those characteristics are represented by the PredParamRule Probability formula ( Formula 10 ) .As a review , the following is a known about calculating conditional probabilities by counting appearances in a training corpus : .","label":"CompareOrContrast","metadata":{},"score":"57.513992"}{"text":"The exemplary parser overcomes the limitations of conventional syntactic bigram approaches by further conditioning the goodness measure on independent probability characteristics .In particular , those characteristics are represented by the PredParamRule Probability formula ( Formula 10 ) .As a review , the following is a known about calculating conditional probabilities by counting appearances in a training corpus : .","label":"CompareOrContrast","metadata":{},"score":"57.513992"}{"text":"Regardless , the training phase ( or some portion thereof ) is performed , at least momentarily , before the run - time phase .This is because the training phase provides the foundation for the ran - time phase to base its SGM calculations .","label":"CompareOrContrast","metadata":{},"score":"57.54197"}{"text":"In this paper we study spectral learning methods for non - deterministic split head - automata grammars , a powerful hidden - state formalism for dependency parsing .We present a learning algorithm that , like other spectral ... .Balle Pigem , Borja de ; Carreras Prez , Xavier ; Luque , Franco M. ; Quattoni , Ariadna Julieta ( 2013 - 10 - 07 ) Article Restricted access - publisher 's policy .","label":"CompareOrContrast","metadata":{},"score":"57.828495"}{"text":"We describe kernels for various natural ... \" .We describe the application of kernel methods to Natural Language Processing ( NLP ) problems .In many NLP tasks the objects being modeled are strings , trees , graphs or other discrete structures which require some mechanism to convert them into feature vectors .","label":"CompareOrContrast","metadata":{},"score":"58.10354"}{"text":"Yet SMT translation quality still obviously suffers from inaccurate lexical choice .In this paper , we address this problem by investigating a new strategy for integrating WSD into an SMT system , that performs fully phrasal multi - word disambiguation .","label":"CompareOrContrast","metadata":{},"score":"58.208588"}{"text":"The tokenizer 28 identifies the words in the textual string 22 , looks them up in a dictionary , makes records for the parts of speech ( POS ) of a word , and passes these to the searcher .The searcher 30 in cooperation with the grammar rules interpreter generates multiple grammatically correct parses of the textual string .","label":"CompareOrContrast","metadata":{},"score":"58.26107"}{"text":"The tokenizer 28 identifies the words in the textual string 22 , looks them up in a dictionary , makes records for the parts of speech ( POS ) of a word , and passes these to the searcher .The searcher 30 in cooperation with the grammar rules interpreter generates multiple grammatically correct parses of the textual string .","label":"CompareOrContrast","metadata":{},"score":"58.26107"}{"text":"In order to compensate for the low recall , we used massive collection of HTML documents .Thus , we could prepare enough polar sentence corpus .This paper discusses automatic determination of case in Arabic .This task is a major source of errors in full diacritization of Arabic .","label":"CompareOrContrast","metadata":{},"score":"58.284515"}{"text":"trn .n . ) hw .n . ) pl .n . ) sh .n . ) segtype .n . ) RuleCountDenominator .hw .n . ) pl .n . ) sh .n . ) s . egtype .","label":"CompareOrContrast","metadata":{},"score":"58.569733"}{"text":"As another example , consider the following grammar that builds verb phrases .This grammar supports verbs , noun phrases , and adjective phrases , but it has been simplified and does not support a range of other valid linguistic phenomena like adverbs , infinitive clauses , prepositional phrases , and conjunctions .","label":"CompareOrContrast","metadata":{},"score":"58.595604"}{"text":"As another example , consider the following grammar that builds verb phrases .This grammar supports verbs , noun phrases , and adjective phrases , but it has been simplified and does not support a range of other valid linguistic phenomena like adverbs , infinitive clauses , prepositional phrases , and conjunctions .","label":"CompareOrContrast","metadata":{},"score":"58.595604"}{"text":"Acknowledgement Many thanks to Amit Dubey and Yuval Krymolowski , the other two organizers of the shared task , for discussions , converting treebanks , writing software and helping with the papers . \" ...This paper presents a new approach to phrase - level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions .","label":"CompareOrContrast","metadata":{},"score":"58.62905"}{"text":"Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks , including the ability to relax strong independence assumptions made in those models .Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models ( MEMMs ) and other discriminative Markov models based on directed graphical models , which can be biased towards states with few successor states .","label":"CompareOrContrast","metadata":{},"score":"58.737816"}{"text":"Log - linear and maximum - margin models are two commonly - used methods in supervised machine learning , and are frequently used in structured prediction problems .Efficient learning of parameters in these models is therefore ... .Lluis Martorell , Xavier ; Carreras Prez , Xavier ; Mrquez Villodre , Llus ( 2013 - 05 )","label":"CompareOrContrast","metadata":{},"score":"58.7668"}{"text":"As a test , we used MavenRank to identify the most influential members of the US Senate using data from the US Congressional Record and used committee ranking to evaluate the output .Our results show that MavenRank scores are largely driven by committee status in most topics , but can capture speaker centrality in topics where speeches are used to indicate ideological position instead of influence legislation .","label":"CompareOrContrast","metadata":{},"score":"58.800278"}{"text":"With the synthetic bilingual corpus , we can build an SMT system even if there is no real bilingual corpus .In our experiments using BLEU as a metric , the system achieves a relative improvement of 11.7 % over the best RBMT system that is used to produce the synthetic bilingual corpora .","label":"CompareOrContrast","metadata":{},"score":"58.86785"}{"text":"In particular , we present a novel method for combining morphological and distributional information for seed selection .Experimental results demonstrate that our approach works well for English and Bengali , thus providing suggestive evidence that it is applicable to both morphologically impoverished langauges and highly inflectional langauges .","label":"CompareOrContrast","metadata":{},"score":"58.981537"}{"text":"Since the probability of a transition occurring can not be mathematically derived , the standard approach is to estimate the probabilities based upon a training corpus .A training corpus is a body of sentences and phrases that are intended to represent \" typical \" human speech in a natural language .","label":"CompareOrContrast","metadata":{},"score":"58.98198"}{"text":"Since the probability of a transition occurring can not be mathematically derived , the standard approach is to estimate the probabilities based upon a training corpus .A training corpus is a body of sentences and phrases that are intended to represent \" typical \" human speech in a natural language .","label":"CompareOrContrast","metadata":{},"score":"58.98198"}{"text":"FIG .1 .However , the present invention can be carried out on a server , a computer devoted to message handling , or on a distributed system in which different portions of the present invention are carried out on different parts of the distributed computing system .","label":"CompareOrContrast","metadata":{},"score":"59.118423"}{"text":"Building A Large Annotated Corpus of English : The Penn Tree Bank , Computational Linguistics , 19(2):313 - 330(1993 ) ) .The next step in training is to train the model by constraining parses generated during the training to enforce the annotated constituent boundaries .","label":"CompareOrContrast","metadata":{},"score":"59.19969"}{"text":"claim 22 , wherein the combinations of linguistic features comprises : .( transition , headword , phrase level , syntactic history , segtype ) ; .( headword , phrase level , syntactic history , segtype ) ; .( modifying headword , transition , headword ) ; and .","label":"CompareOrContrast","metadata":{},"score":"59.386"}{"text":"In ontological semantics , a theory is viewed as a set of statements determining the format of descriptions of the phenomena with which the theory deals .A theory is associated with a methodology used to obtain the descriptions .Implementations are computer systems that use the descriptions to solve specific problems in text processing .","label":"CompareOrContrast","metadata":{},"score":"59.401672"}{"text":"We demonstrate the effectiveness of our technique largely surpassing both the random and most frequent baselines and outperforming current state - of - the - art unsupervised approaches on a benchmark ontology available in the literature .To date , work on Non - Local Dependencies ( NLDs ) has focused almost exclusively on English and it is an open research question how well these approaches migrate to other languages .","label":"CompareOrContrast","metadata":{},"score":"59.422295"}{"text":"( transition , headword , phrase level , syntactic history , segtype ) ; .( headword , phrase level , syntactic history , segtype ) ; .( modifying headword , transition , headword ) ; or .( transition , headword ) .","label":"CompareOrContrast","metadata":{},"score":"59.45227"}{"text":"\" has been simplified to be \" . . .we found useful . . . \" .FIG .6 b shows a parse tree 110 representing a parse of this sentence .The rule VPwNP1 at transition 114 requires the second constituent 116 to have PL 3 .","label":"CompareOrContrast","metadata":{},"score":"59.554085"}{"text":"In this article , we propose several kernel functions to model parse tree properties in kernelbased machines , for example , perceptrons or support vector machines .In particular , we define different kinds of tree kernels as general approaches to feature engineering in SRL .","label":"CompareOrContrast","metadata":{},"score":"59.59242"}{"text":"We learn our model using a Monte Carlo EM algorithm and present quantitative results validating the model .We present a maximally streamlined approach to learning HMM - based acoustic models for automatic speech recognition .In our approach , an initial monophone , single - Gaussian HMM is iteratively refined using a split - merge EM procedure which makes no assumptions about subphone structure or context - dependent structure and which uses only a single Gaussian per HMM state .","label":"CompareOrContrast","metadata":{},"score":"59.593018"}{"text":"All 109 final papers were allowed 9 pages plus bibliography .In a separate track , 22 specially designated short papers reported results in the CoNLL Shared Task competition , an annual tradition .9 of these were presented as short talks .","label":"CompareOrContrast","metadata":{},"score":"59.647255"}{"text":"\" has been simplified to be \" . . .we found useful . . . \" .FIG .6 b shows a parse tree 110 representing a parse of this sentence .The rule VPwNPl at transition 114 requires the second constituent 116 to have PL 3 .","label":"CompareOrContrast","metadata":{},"score":"59.689205"}{"text":"Word Order and Phrases .Words do not occur in just any order .Languages have constraints on the word order .Generally , words are organized into phrases , which are groupings of words that are clumped as a unit .","label":"CompareOrContrast","metadata":{},"score":"59.87057"}{"text":"Word Order and Phrases .Words do not occur in just any order .Languages have constraints on the word order .Generally , words are organized into phrases , which are groupings of words that are clumped as a unit .","label":"CompareOrContrast","metadata":{},"score":"59.87057"}{"text":"7B illustrates joint syntactic and semantic labels .FIG .8 is a flow diagram illustrating one embodiment of the process of training a structured language model in accordance with the present invention .FIG .9 is a flow diagram illustrating one embodiment of the operation of the structured language model during run - time , or test - time , in accordance with one embodiment of the present invention .","label":"CompareOrContrast","metadata":{},"score":"60.048687"}{"text":"The feature extraction module 408 produces a stream of feature vectors that are each associated with a frame of the speech signal .If the input signal is a training signal , this series of feature vectors is provided to a trainer 410 , which uses the feature vectors and a training text 412 to train an acoustic model 414 .","label":"CompareOrContrast","metadata":{},"score":"60.08117"}{"text":"Figure 5 shows the result of parsing with our combined model , using ... . \" ...This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm .We show how the algorithms can be efficiently applied to exponential sized representations of parse trees , such as the \" all subtrees \" ( DOP ) representation described by ( Bod 9 ... \" .","label":"CompareOrContrast","metadata":{},"score":"60.08275"}{"text":"hw(n ) is the headword of node n . segtype(n ) is the segtype of node n . trans(n ) is the transition ( rewrite rule ) associated with node n ( e.g. , the rules under the heading \" Transition \" in Table 1 ) .","label":"CompareOrContrast","metadata":{},"score":"60.15952"}{"text":"hw(n ) is the headword of node n . segtype(n ) is the segtype of node n . trans(n ) is the transition ( rewrite rule ) associated with node n ( e.g. , the rules under the heading \" Transition \" in Table 1 ) .","label":"CompareOrContrast","metadata":{},"score":"60.15952"}{"text":"FIGS . 1 and 12 illustrate examples of a suitable operating environments 920 in .FIG .1 and 930 in .FIG .12 ) in which the exemplary ranking parser may be implemented .Specifically , the exemplary ranking parser is implemented by the parse ranker 34 in .","label":"CompareOrContrast","metadata":{},"score":"60.382874"}{"text":"FIGS . 1 and 12 illustrate examples of a suitable operating environments 920 in .FIG .1 and 930 in .FIG .12 ) in which the exemplary ranking parser may be implemented .Specifically , the exemplary ranking parser is implemented by the parse ranker 34 in .","label":"CompareOrContrast","metadata":{},"score":"60.382874"}{"text":"where .a 1 , a 2 , . . .a g are the features of a parent node .b 1 , b 2 , . . .b g are the features of a left child .c 1 , c 2 , . . .","label":"CompareOrContrast","metadata":{},"score":"60.42729"}{"text":"where .a 1 , a 2 , . . .a g are the features of a parent node .b 1 , b 2 , . . .b g are the features of a left child .c 1 , c 2 , . . .","label":"CompareOrContrast","metadata":{},"score":"60.42729"}{"text":"The interpolated model achieves an absolute improvement of 0.0245 BLEU score ( 13.1 % relative ) as compared with the individual model trained on the real bilingual corpus .This paper investigates why the HMMs estimated by Expectation - Maximization ( EM ) produce such poor results as Part - of - Speech ( POS ) taggers .","label":"CompareOrContrast","metadata":{},"score":"60.432064"}{"text":"Herein , an utterance is equivalent to a phrase .A phase is a sequence of words intended to have meaning .In addition , a sentence is understood to be one or more phrases .In addition , references herein to a human speaker include a writer and speech includes writing .","label":"CompareOrContrast","metadata":{},"score":"60.462692"}{"text":"Herein , an utterance is equivalent to a phrase .A phase is a sequence of words intended to have meaning .In addition , a sentence is understood to be one or more phrases .In addition , references herein to a human speaker include a writer and speech includes writing .","label":"CompareOrContrast","metadata":{},"score":"60.462692"}{"text":"FIG .2 illustrates a CFG parse tree 50 of a phrase ( or sentence ) .This tree - like representation of the sentence \" flies like ants \" is deconstructed using a CFG set of rewrite rules ( i.e , transitions ) .","label":"CompareOrContrast","metadata":{},"score":"60.478973"}{"text":"FIG .2 illustrates a CFG parse tree 50 of a phrase ( or sentence ) .This tree - like representation of the sentence \" flies like ants \" is deconstructed using a CFG set of rewrite rules ( i.e , transitions ) .","label":"CompareOrContrast","metadata":{},"score":"60.478973"}{"text":"Our overall conclusion is that at least two measures , MI and PE , seem to differentiate MWEs from non - MWEs .We then investigate the influence of the size and quality of different corpora , using the BNC and the Web search engines Google and Yahoo .","label":"CompareOrContrast","metadata":{},"score":"60.54866"}{"text":"x .y . )Count .y . )When the known event is predicative of the outcome , knowing the conditional probabilities is better than knowing just the unconditional probability .For example , assume that a man is in a casino playing the following game .","label":"CompareOrContrast","metadata":{},"score":"60.597363"}{"text":"x .y . )Count .y . )When the known event is predicative of the outcome , knowing the conditional probabilities is better than knowing just the unconditional probability .For example , assume that a man is in a casino playing the following game .","label":"CompareOrContrast","metadata":{},"score":"60.597363"}{"text":"8 shows an annotated parse tree 140 of a parse of this sample sentence .As can be seen in .FIG .8 , the \" topicalization \" feature is propagated past the verb \" like \" to the verb \" visit .","label":"CompareOrContrast","metadata":{},"score":"60.645927"}{"text":"8 shows an annotated parse tree 140 of a parse of this sample sentence .As can be seen in .FIG .8 , the \" topicalization \" feature is propagated past the verb \" like \" to the verb \" visit .","label":"CompareOrContrast","metadata":{},"score":"60.645927"}{"text":"Computational linguistics is the analytic study of human natural language within computer science to mathematically represent language rules such as grammar , syntax , and semantics .Statistics .Probability .The expression \" Prob(x ) \" is the probability of event x occurring .","label":"CompareOrContrast","metadata":{},"score":"60.659985"}{"text":"Computational linguistics is the analytic study of human natural language within computer science to mathematically represent language rules such as grammar , syntax , and semantics .Statistics .Probability .The expression \" Prob(x ) \" is the probability of event x occurring .","label":"CompareOrContrast","metadata":{},"score":"60.659985"}{"text":"The joint probability P(W , T ) of a word sequence W and a complete parse T can be broken into : .P .W .T . ) k .n .[ .P .w . k .","label":"CompareOrContrast","metadata":{},"score":"60.792225"}{"text":"The system of .claim 4 and further comprising : . a ranking component ranking the candidate semantic parse trees generated by the structured language model .The system of . claim 5 wherein the ranking component ranks each candidate semantic parse tree by summing over all generated candidate semantic parse trees .","label":"CompareOrContrast","metadata":{},"score":"60.9436"}{"text":"Trigram language models are compessed using a Golomb coding method inspired by the original Unix spell program .Compression methods trade off space , time and accuracy ( loss ) .The proposed HashTBO method optimizes space at the expense of time and accuracy .","label":"CompareOrContrast","metadata":{},"score":"60.972954"}{"text":"The .p . k .p .N . k . k .The model is based on three probabilities , each illustratively estimated using deleted interpolation and parameterized ( approximated ) as follows : .P .w . k .","label":"CompareOrContrast","metadata":{},"score":"61.028374"}{"text":"The structured language model is set out in greater detail in Ciprian Chelba and Frederick Jelinek , Structured Language Modeling , Computer Speech and Language , 14(4):283 - 332 , October 2000 ; and Chelba , Exploiting Syntactic Structure for Natural Language Modeling , Ph.D. Dissertation , Johns Hopkins University , 2000 .","label":"CompareOrContrast","metadata":{},"score":"61.048626"}{"text":"Context Free Grammar ( CFG ) .The nature of the rewrite rules is that a certain syntactic category ( e.g , noun , np , vp , pp ) can be rewritten as one or more other syntactic categories or words .","label":"CompareOrContrast","metadata":{},"score":"61.09072"}{"text":"Context Free Grammar ( CFG ) .The nature of the rewrite rules is that a certain syntactic category ( e.g , noun , np , vp , pp ) can be rewritten as one or more other syntactic categories or words .","label":"CompareOrContrast","metadata":{},"score":"61.09072"}{"text":"Therefore , the novel concepts of the present invention can easily be practiced regardless of whether the input is actually text , or speech .BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1 is a block diagram of one embodiment of a computer environment in which the present invention can be practiced .","label":"CompareOrContrast","metadata":{},"score":"61.1724"}{"text":"The use of synchronous TAGs for generation provides solutions to several problems with previous approaches to TAG generation .Furthermore , the semantic monotonicity requirement previously advocated for generation gram- mars as a computational aid is seen to be an inherent property of synchronous TAGs . \" ...","label":"CompareOrContrast","metadata":{},"score":"61.23197"}{"text":"FIG .10 shows a methodological implementation of the training phase of the exemplary parser .The training phase has two parts : the preparation part and the computation part .The preparation part is performed before the computation part .During the preparation part , a training corpus is created at 200 .","label":"CompareOrContrast","metadata":{},"score":"61.358955"}{"text":"FIG .10 shows a methodological implementation of the training phase of the exemplary parser .The training phase has two parts : the preparation part and the computation part .The preparation part is performed before the computation part .During the preparation part , a training corpus is created at 200 .","label":"CompareOrContrast","metadata":{},"score":"61.358955"}{"text":"This means that the rule can be applied to an NP that is at the highest NP is level and to a VP that is at level three .The result of running the rule is to create a VP at level four .","label":"CompareOrContrast","metadata":{},"score":"61.47457"}{"text":"Part of speech tagging is a fundamental component in many NLP systems .When taggers developed in one domain are used in another domain , the performance can degrade considerably .We present a method for developing taggers for new domains without requiring POS annotated text in the new domain .","label":"CompareOrContrast","metadata":{},"score":"61.587574"}{"text":"This means that the rule can be applied to an NP that is at the highest NP level and to a VP that is at level three .The result of running the rule is to create a VP at level four .","label":"CompareOrContrast","metadata":{},"score":"61.597908"}{"text":"Smoothing probabilities is most important for tasks with a limited amount of training material .We consider here the Btec task of the 2006 Iwslt evaluation .Improvements in all official automatic measures are reported when translating from Italian to English .","label":"CompareOrContrast","metadata":{},"score":"61.60208"}{"text":"After the training corpus is created in the preparation part , the computation part of the training phase begins at 206 .This part computes all the counts , at 206 , used in the PredParamRule Probability and SynBigram Probability .Those of ordinary skill in the art can use formula 2 directly , or any simplification thereof .","label":"CompareOrContrast","metadata":{},"score":"61.63531"}{"text":"After the training corpus is created in the preparation part , the computation part of the training phase begins at 206 .This part computes all the counts , at 206 , used in the PredParamRule Probability and SynBigram Probability .Those of ordinary skill in the art can use formula 2 directly , or any simplification thereof .","label":"CompareOrContrast","metadata":{},"score":"61.63531"}{"text":"A modifying headword is the headword of a sub - phrase within a phrase where the sub - phrase modifies the main headword of the main phrase .Assume a phrase ( P ) has a headword ( hwP ) and a modifying sub - phrase ( M ) within the P that modifies hwP.","label":"CompareOrContrast","metadata":{},"score":"61.69099"}{"text":"A modifying headword is the headword of a sub - phrase within a phrase where the sub - phrase modifies the main headword of the main phrase .Assume a phrase ( P ) has a headword ( hwP ) and a modifying sub - phrase ( M ) within the P that modifies hwP.","label":"CompareOrContrast","metadata":{},"score":"61.69099"}{"text":"( transition , headword , phase level , syntactic history , segtype ) ; .( headword , phase level , syntactic history , segtype ) ; .( modifying headword , transition , headword ) ; or ( transition , headword ) .","label":"CompareOrContrast","metadata":{},"score":"61.722015"}{"text":"For example , assume that one wished to determine the probability of rolling a three given a weighted die .The probability may be 1/6 ( as it would be with a fair die ) , but it is likely to be more or less than that depending upon how the die is weighted .","label":"CompareOrContrast","metadata":{},"score":"61.734962"}{"text":"For example , assume that one wished to determine the probability of rolling a three given a weighted die .The probability may be 1/6 ( as it would be with a fair die ) , but it is likely to be more or less than that depending upon how the die is weighted .","label":"CompareOrContrast","metadata":{},"score":"61.734962"}{"text":"In the same way , other thing can be grouped together , such as vegetables , fluids , meat , etc .Therefore , instead of dealing with hundreds of words , we can replace them with tens of clusters .Other alternatives include storing probabilities instead of counts and clustering headwords .","label":"CompareOrContrast","metadata":{},"score":"61.76454"}{"text":"In the same way , other thing can be grouped together , such as vegetables , fluids , meat , etc .Therefore , instead of dealing with hundreds of words , we can replace them with tens of clusters .Other alternatives include storing probabilities instead of counts and clustering headwords .","label":"CompareOrContrast","metadata":{},"score":"61.76454"}{"text":"The computer is equally likely to consider the word \" saw \" as a noun as it is a verb , in either sentence .A NLP system assists the computer in distinguishing how words are used in different contexts and in applying rules to construct syntactical and meaning representations .","label":"CompareOrContrast","metadata":{},"score":"61.838158"}{"text":"The computer is equally likely to consider the word \" saw \" as a noun as it is a verb , in either sentence .A NLP system assists the computer in distinguishing how words are used in different contexts and in applying rules to construct syntactical and meaning representations .","label":"CompareOrContrast","metadata":{},"score":"61.838158"}{"text":"Recognizing polarity requires a list of polar words and phrases .For the purpose of building such lexicon automatically , a lot of studies have investigated ( semi- ) unsupervised method of learning polarity of words and phrases .In this paper , we explore to use structural clues that can extract polar sentences from Japanese HTML documents , and build lexicon from the extracted polar sentences .","label":"CompareOrContrast","metadata":{},"score":"61.925163"}{"text":"12 is an example of a computing operating environment capable of implementing the exemplary ranking parser for NLP .DETAILED DESCRIPTION .The following description sets forth a specific embodiment of the ranking parser for natural language processing ( NLP ) that incorporates elements recited in the appended claims .","label":"CompareOrContrast","metadata":{},"score":"62.25486"}{"text":"12 is an example of a computing operating environment capable of implementing the exemplary ranking parser for NLP .DETAILED DESCRIPTION .The following description sets forth a specific embodiment of the ranking parser for natural language processing ( NLP ) that incorporates elements recited in the appended claims .","label":"CompareOrContrast","metadata":{},"score":"62.25486"}{"text":"Tracking both hw(n Y ) and hw(n Z ) is not particularly valuable because one of them is the same as hw(n X ) .The one that is not the same is the modifying headword .The notation modhw(n X ) to refer to this modifying headword .","label":"CompareOrContrast","metadata":{},"score":"62.400948"}{"text":"Tracking both hw(n Y ) and hw(n Z ) is not particularly valuable because one of them is the same as hw(n X ) .The one that is not the same is the modifying headword .The notation modhw(n X ) to refer to this modifying headword .","label":"CompareOrContrast","metadata":{},"score":"62.400948"}{"text":"If the headword is included into the probability calculations , the goodness function is more likely to pick the correct parse .In particular , instead of just counting up all occurrences of VPwNPr1 and VPwAVPr in the corpus , a count is made of how often these rules appear with the headword \" smiled . \" In doing so , it likely to be discovered that there are very few instances of VPwNPr1 occurring with the headword \" smiled . \"","label":"CompareOrContrast","metadata":{},"score":"62.408806"}{"text":"11 shows a speech recognition system in which one or more of the information extraction techniques of the present invention can be used to extract information ( frame and slots ) from a natural language speech input signal .In .FIG .","label":"CompareOrContrast","metadata":{},"score":"62.446777"}{"text":"Since human language is inherently imprecise , rarely is one parse one hundred percent ( 100 % ) correct and the others never correct .Therefore , a parser typically ranks the parses from most likely to be correct to least likely to be correct .","label":"CompareOrContrast","metadata":{},"score":"62.47827"}{"text":"Since human language is inherently imprecise , rarely is one parse one hundred percent ( 100 % ) correct and the others never correct .Therefore , a parser typically ranks the parses from most likely to be correct to least likely to be correct .","label":"CompareOrContrast","metadata":{},"score":"62.47827"}{"text":"FIG .10 is a flowchart illustrating the methodology of an implementation of the training phase of the exemplary parser .FIG .11 is a flowchart illustrating the methodology of an implementation of the run - time phase of the exemplary parser .","label":"CompareOrContrast","metadata":{},"score":"62.535416"}{"text":"FIG .10 is a flowchart illustrating the methodology of an implementation of the training phase of the exemplary parser .FIG .11 is a flowchart illustrating the methodology of an implementation of the run - time phase of the exemplary parser .","label":"CompareOrContrast","metadata":{},"score":"62.535416"}{"text":"If the headword is included into the probability calculations , the goodness function is more likely to pick the correct parse .In particular , instead of just counting up all occurrences of VPwNPrl and VPwAVPr in the corpus , a count is made of how often these rules appear with the headword \" smiled . \" In doing so , it likely to be discovered that there are very few instances of VPwNPrl occurring with the headword \" smiled . \"","label":"CompareOrContrast","metadata":{},"score":"62.546936"}{"text":"trn .n . ) hw .n . ) pl .n . ) sh .n . ) segtype .n . ) RuleCountDenominator .hw .n . ) pl .n . ) sh .n . ) segtype .","label":"CompareOrContrast","metadata":{},"score":"62.720985"}{"text":"Since any transition has a probability of 1 or lower , the more transitions in a sentence or phrase implies a smaller goodness measure .Therefore , when calculating the goodness measure using conventional approaches , the sentence \" I want nut \" would be preferred over the sentence \" I want a nut .","label":"CompareOrContrast","metadata":{},"score":"62.72867"}{"text":"Since any transition has a probability of 1 or lower , the more transitions in a sentence or phrase implies a smaller goodness measure .Therefore , when calculating the goodness measure using conventional approaches , the sentence \" I want nut \" would be preferred over the sentence \" I want a nut .","label":"CompareOrContrast","metadata":{},"score":"62.72867"}{"text":"FIG .11 shows a methodological implementation of the run - time phase of the exemplary parser .At 300 , a parse of a phrase is initiated .An application , such as a grammar checker , may initiate such a parse by the exemplary parser .","label":"CompareOrContrast","metadata":{},"score":"62.825798"}{"text":"FIG .11 shows a methodological implementation of the run - time phase of the exemplary parser .At 300 , a parse of a phrase is initiated .An application , such as a grammar checker , may initiate such a parse by the exemplary parser .","label":"CompareOrContrast","metadata":{},"score":"62.825798"}{"text":"The null transition at 112 is used to move the VP(2 ) to be a VP(3 ) .The null transition can be explicitly represented in the parse tree ( as shown in .FIG .6 b ) or be implicit .","label":"CompareOrContrast","metadata":{},"score":"63.00981"}{"text":"The null transition at 112 is used to move the VP ( 2 ) to be a VP ( 3 ) .The null transition can be explicitly represented in the parse tree ( as shown in .FIG .6 b ) or be implicit .","label":"CompareOrContrast","metadata":{},"score":"63.00981"}{"text":": Each entry stores the count for a different combination of the transition , headword , phrase level , syntactic history , and segtype .These counts are used in the numerator of the PredParamRule Probability .RuleCountDenominator : Each entry stores the count for a different combination of headword , phrase level , syntactic history , and segtype .","label":"CompareOrContrast","metadata":{},"score":"63.024693"}{"text":": Each entry stores the count for a different combination of the transition , headword , phrase level , syntactic history , and segtype .These counts are used in the numerator of the PredParamRule Probability .RuleCountDenominator : Each entry stores the count for a different combination of headword , phrase level , syntactic history , and segtype .","label":"CompareOrContrast","metadata":{},"score":"63.024693"}{"text":"The frame ( or root ) level and the slot ( or leaf ) level .During parsing , SLM 310 discards any hypothesis parses which violate this structure .This is indicated by block 304 .When parsing is complete , SLM 310 will illustratively have parsed a desired number of parse trees 314 .","label":"CompareOrContrast","metadata":{},"score":"63.063374"}{"text":"Tags from Present Day English source text are projected to Middle English text using alignments on parallel Biblical text .We explore the use of multiple alignment approaches and a bigram tagger to reduce the noise in the projected tags .Finally , we train a maximum entropy tagger on the output of the bigram tagger on the target Biblical text and test it on tagged Middle English text .","label":"CompareOrContrast","metadata":{},"score":"63.21692"}{"text":"Experiments show that this wordnet has a high level of precision and coverage , and that it can be useful in applied tasks such as cross - lingual text classification . ... 4.1 Feature Computation For each candidate me ... . by Alessandro Moschitti , Daniele Pighin , Roberto Basili - IN PROCEEDINGS OF CONLL - X , 2006 . \" ...","label":"CompareOrContrast","metadata":{},"score":"63.3636"}{"text":"Unknown words are a well - known hindrance to natural language applications .In particular , they drastically impact machine translation quality .An easy way out commercial translation systems usually offer their users is the possibility to add unknown words and their translations into a dedicated lexicon .","label":"CompareOrContrast","metadata":{},"score":"63.39491"}{"text":"BigramCountNumerator : Each entry stores the count for a different combination of modifying headword , transition , and headword .These counts are used in the numerator of the SynBigram Probability .BigramCountDenominator : Each entry stores the count for a different combination of transition and headword .","label":"CompareOrContrast","metadata":{},"score":"63.77292"}{"text":"No . 10/929,167 and U.S. Pat .No .6,952,666 are incorporated by reference herein .BACKGROUND .In general , a computer is a digital machine that uses precise languages with absolute values , such as \" on \" , \" off \" , \" 1 \" , \" 0 \" , \" 3 + 4 \" , \" AND \" , and \" XOR \" .","label":"CompareOrContrast","metadata":{},"score":"63.798283"}{"text":"The parser also receives grammar rules 24 .These rules attempt to codify and interpret the actual grammar rules of a particular natural language , such as English .Alternatively , these rules may be stored in memory within the parser .","label":"CompareOrContrast","metadata":{},"score":"63.83909"}{"text":"The parser also receives grammar rules 24 .These rules attempt to codify and interpret the actual grammar rules of a particular natural language , such as English .Alternatively , these rules may be stored in memory within the parser .","label":"CompareOrContrast","metadata":{},"score":"63.83909"}{"text":"The 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ( EMNLP - CoNLL 2007 ) .The main track of the conference received 398 paper submissions ( not counting 21 that were withdrawn or rejected without review ) .","label":"CompareOrContrast","metadata":{},"score":"63.87831"}{"text":"This paper reports on the benefits of large - scale statistical language modeling in machine translation .A distributed infrastructure is proposed which we use to train on up to 2 trillion tokens , resulting in language models having up to 300 billion n - grams .","label":"CompareOrContrast","metadata":{},"score":"64.11961"}{"text":"alized composition and coordination ) can be easily implemented using such deduction parsing methods .3 The increased expressive power of ... . by Srinivas Bangalore , Aravind K. Joshi - Computational Linguistics , 1999 . \" ... this paper , we have proposed novel methods for robust parsing that integrate the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques .","label":"CompareOrContrast","metadata":{},"score":"64.31654"}{"text":"BigramCountNumerator : Each entry stores the count for a different combination of modifying headword , transition , and headword .These counts are used in the numerator of the Synfigram Probability .BigramCountDenominator : Each entry stores the count for a different combination of transition and headword .","label":"CompareOrContrast","metadata":{},"score":"64.59534"}{"text":"Thus , SLM 400 can be used in this embodiment not only to assist in the recognition of speech , but also to perform information extraction from speech .It can thus be seen that the present invention employs a structured language model in information extraction .","label":"CompareOrContrast","metadata":{},"score":"64.64923"}{"text":"i . )i .Count . trans .n .i . ) segtype .n .i . )Count . segtype .n .i . )Where .n i : is the i th node . segtype(n i ): is the segtype of n i .","label":"CompareOrContrast","metadata":{},"score":"65.21229"}{"text":"2 it is node 54 a ) , which is the start of the parse tree .Segtypes .A non - terminal node has a type that is called its \" segtype . \"In .FIG .2 , each non - terminal node 54 a - g is labeled with its segtype .","label":"CompareOrContrast","metadata":{},"score":"65.480316"}{"text":"Each non - terminal node in the parse tree is created via the application of some rewrite rule .For example , in .The tree 50 has a non - terminal node 54 a designated as the starting node and it is labeled \" s. \" .","label":"CompareOrContrast","metadata":{},"score":"65.49696"}{"text":"Yifan He , Yanjun Ma , Josef Genabith , Andy Way , Centre Next , Generation Localisation . \" ...We propose a translation recommendation framework to integrate Statistical Machine Translation ( SMT ) output with Translation Memory ( TM ) systems .","label":"CompareOrContrast","metadata":{},"score":"65.508415"}{"text":"i .Count . trans .n .i . ) segtype .n .i . )Count . segtype .n .i . )Where .n i : is the i th node . segtype(n i ): is the segtype of n i .","label":"CompareOrContrast","metadata":{},"score":"65.51454"}{"text":"The first method we discuss is based on a feature selection me ... . by Michael Collins , Nigel Duffy - Advances in Neural Information Processing Systems 14 , 2001 . \" ...We describe the application of kernel methods to Natural Language Processing ( NLP ) problems .","label":"CompareOrContrast","metadata":{},"score":"65.63548"}{"text":"Count . event . )Total . number . of . events .Prob . trans .i . )Count . times . trans .i . appears . )Total . of . transitions . in .the .training .","label":"CompareOrContrast","metadata":{},"score":"65.68776"}{"text":"Count . event . )Total . number . of . events .Prob . trans .i . )Count . times . trans .i . appears . )Total . of . transitions . in .the .training .","label":"CompareOrContrast","metadata":{},"score":"65.68776"}{"text":"The theory of ontological semantics is built as a society of microtheories covering such diverse ground as specific language phenomena , world knowledge organization , processing heuristics and issues relating to knowledge representation and implementation system architecture .The theory briefly sketched above is a top - level microtheory , the ontological semantics theory per se .","label":"CompareOrContrast","metadata":{},"score":"65.72816"}{"text":"In the past , this has been the situation because of limited processing capabilities of typical computers and because of the inherent difficulties of modeling imprecise human language within a digital computer .However , as typical computing power increases , natural language processing systems are being used by computers to \" understand \" imprecise human language .","label":"CompareOrContrast","metadata":{},"score":"65.79015"}{"text":"In the past , this has been the situation because of limited processing capabilities of typical computers and because of the inherent difficulties of modeling imprecise human language within a digital computer .However , as typical computing power increases , natural language processing systems are being used by computers to \" understand \" imprecise human language .","label":"CompareOrContrast","metadata":{},"score":"65.79015"}{"text":"The estimated domain ( sub - corpus ) specific language and translation models are used for the translation .The IWSLT05 Japanese to English evaluation set that we used in our experiments gave 2.7 points ( 52.4 to 55.1 ) higher Blue score using this method .","label":"CompareOrContrast","metadata":{},"score":"65.8751"}{"text":"BACKGROUND OF THE INVENTION .The present invention relates to machine understanding of textual or speech inputs .More specifically , the present invention relates to the task of information extraction in the machine understanding process .Natural language user interfaces to computers attempt to allow the user to operate a computer simply by inputting commands or directions to the computer in a natural language .","label":"CompareOrContrast","metadata":{},"score":"65.89809"}{"text":"FIG .2 illustrates a template or frame 200 in accordance with one example .Frame 200 defines an action \" schedule meeting \" and that is the label 202 of frame 200 .Frame 200 also includes a plurality of slots .","label":"CompareOrContrast","metadata":{},"score":"65.9471"}{"text":"This null transition can be applied to a VP at phrase level three and create a VP at phrase level four . \"PL_XP_Max \" in a phrase level indicator means the highest phrase level that occurs for a given segtype .","label":"CompareOrContrast","metadata":{},"score":"66.09867"}{"text":"This null transition can be applied to a VP at phrase level three and create a VP at phrase level four . \"PL_XP_Max \" in a phrase level indicator means the highest phrase level that occurs for a given segtype .","label":"CompareOrContrast","metadata":{},"score":"66.09867"}{"text":"FIGS .4 a and 4 b show examples of two of the seven valid parses of this sentence .For the parse tree 70 of .FIG .4 a , the object \" time \" 74 moves in a way that is similar to an arrow .","label":"CompareOrContrast","metadata":{},"score":"66.10695"}{"text":"FIGS .4 a and 4 b show examples of two of the seven valid parses of this sentence .For the parse tree 70 of .FIG .4 a , the object \" time \" 74 moves in a way that is similar to an arrow .","label":"CompareOrContrast","metadata":{},"score":"66.10695"}{"text":"more preferable is a probabilistic confidence score ( e.g. 90 % confidence ) which is better understood by post - editors and translators .Platt 's method estimates the posterior probability with a sigmod function , as in ( 4 ) : ( 3 ... Quattoni , Ariadna Julieta ; Carreras Prez , Xavier ; Torralba , Antonio ( Springer , 2012 ) Conference report Restricted access - publisher 's policy .","label":"CompareOrContrast","metadata":{},"score":"66.17125"}{"text":"Thus , the PredParamRule Probability formula ( Formula 10 ) may be calculated as follows : .PredParamRule .Probability .Prob . trn .n .x . ) hw .n .x . ) pl .n .x . ) sh .","label":"CompareOrContrast","metadata":{},"score":"66.199936"}{"text":"Thus , the PredParamRule Probability formula ( Formula 10 ) may be calculated as follows : .PredParamRule .Probability .Prob . trn .n .X . ) hw .n .X . ) pl .n .X . ) sh .","label":"CompareOrContrast","metadata":{},"score":"66.199936"}{"text":"The results of the experiments show that , contrary to Kbler et al .( 2006 ) , the question whether or not German is harder to parse than English remains undecided .In this paper , we study the problem of automatically segmenting written text into paragraphs .","label":"CompareOrContrast","metadata":{},"score":"66.33201"}{"text":"This paper presents a method for categorizing named entities in Wikipedia .In Wikipedia , an anchor text is glossed in a linked HTML text .We formalize named entity categorization as a task of categorizing anchor texts with linked HTML texts which glosses a named entity .","label":"CompareOrContrast","metadata":{},"score":"66.550476"}{"text":"A data structure as recited in .claim 22 , wherein both PredParamRule Probability and SynBigram Probability formulas are used to calculate a probability associated with a node .The structure as recited in .claim 22 , wherein the subset of the corpus includes all phrases in the corpus .","label":"CompareOrContrast","metadata":{},"score":"66.56041"}{"text":"On average , the man should expect to make $ 35 for every $ 6 that he bets .As another example , consider the problem of predicting what the next word in a stream of text will be .E.g. , is the next word after \" home \" more likely to be \" table \" or \" run .","label":"CompareOrContrast","metadata":{},"score":"66.58194"}{"text":"On average , the man should expect to make $ 35 for every $ 6 that he bets .As another example , consider the problem of predicting what the next word in a stream of text will be .E.g. , is the next word after \" home \" more likely to be \" table \" or \" run .","label":"CompareOrContrast","metadata":{},"score":"66.58194"}{"text":"The third is illustrated by numeral 238 and is the S_schedule_ScheduleMeeting label .This step thus enriches the non - terminal and pre - terminal labels of the resulting parses with the semantic tags ( both frame and slot ) present in the semantic parse .","label":"CompareOrContrast","metadata":{},"score":"66.74249"}{"text":"In the multilingual exercise of the CoNLL-2007 shared task ( Nivre et al.,2007 ) , our system obtains the best accuracy for English , and the second best accuracies for Basque and Czech .We present our system used in the CoNLL 2007 shared task on multilingual parsing .","label":"CompareOrContrast","metadata":{},"score":"66.822495"}{"text":"Probability .Prob . mod .hw .n .X . ) trn .n .X . ) hw .n .X . )Count . mod .hw .n .X . ) trn .n .","label":"CompareOrContrast","metadata":{},"score":"66.95167"}{"text":"The present invention is an information extraction system that utilizes a structured language model .The system can be implemented on a computing device and as a method .FIG .1 illustrates an example of a suitable computing system environment 100 on which the invention may be implemented .","label":"CompareOrContrast","metadata":{},"score":"66.95926"}{"text":"1 shows a NLP parser 20 of a typical NLP system .The parser 20 has four key components : .Tokenizer 28 ; .Grammar Rules Interpreter 26 ; .Searcher 30 ; and .Parse Ranker 34 .The parser 20 receives a textual string 22 .","label":"CompareOrContrast","metadata":{},"score":"66.96936"}{"text":"1 shows a NLP parser 20 of a typical NLP system .The parser 20 has four key components : .Tokenizer 28 ; .Grammar Rules Interpreter 26 ; .Searcher 30 ; and .Parse Ranker 34 .The parser 20 receives a textual string 22 .","label":"CompareOrContrast","metadata":{},"score":"66.96936"}{"text":"It may also be implemented by a device within a NLP device .For example , a parse ranker 34 in .FIG .1 may be a program module implementing the exemplary parser within a NLP program system 20 .Alternatively , the parse ranker 34 in .","label":"CompareOrContrast","metadata":{},"score":"67.013"}{"text":"It may also be implemented by a device within a NLP device .For example , a parse ranker 34 in .FIG .1 may be a program module implementing the exemplary parser within a NLP program system 20 .Alternatively , the parse ranker 34 in .","label":"CompareOrContrast","metadata":{},"score":"67.013"}{"text":"Additionally , we compare the maximum a - posteriori decision rule and the minimum Bayes risk decision rule .We show that not only from a theoretical point but also in terms of translation quality the minimum Bayes risk decision rule is preferable .","label":"CompareOrContrast","metadata":{},"score":"67.16509"}{"text":"By way of example , and not limitation , .FIG .12 illustrates remote application programs 989 as residing on a memory device of remote computer 982 .It will be appreciated that the network connections shown and described are exemplary and other means of establishing a communications link between the computers may be used .","label":"CompareOrContrast","metadata":{},"score":"67.16649"}{"text":"By way of example , and not limitation , .FIG .12 illustrates remote application programs 989 as residing on a memory device of remote computer 982 .It will be appreciated that the network connections shown and described are exemplary and other means of establishing a communications link between the computers may be used .","label":"CompareOrContrast","metadata":{},"score":"67.16649"}{"text":"The answer is to run an experiment .The die is thrown many times and the number of rolls where \" 3 \" is rolled is counted .This data is called the \" training data \" .It is sometimes called the \" training corpus . \"","label":"CompareOrContrast","metadata":{},"score":"67.45334"}{"text":"For example , in .The tree 50 has a non - terminal node 54 a designated as the starting node and it is labeled \" s. \" .In general , the order of the children in each branch generates the word order of the sentence , and the tree has a single root node ( in .","label":"CompareOrContrast","metadata":{},"score":"67.59792"}{"text":"Bailly , Raphal ; Carreras Prez , Xavier ; Quattoni , Ariadna Julieta ( 2012 ) Conference report Open Access .Finite - State Transducers ( FST ) are a standard tool for modeling paired inputoutput sequences and are used in numerous applications , ranging from computational biology to natural language processing .","label":"CompareOrContrast","metadata":{},"score":"67.702324"}{"text":"We found that our method could benefit from the two - preprocessing stages .To speed up training , in this year , we employ the MFN - SVM ( modified finite - Newton method support vector machines ) which can be learned in linear time .","label":"CompareOrContrast","metadata":{},"score":"67.81724"}{"text":"Iwould like toacknowledge the following people for their contribution to my education : I thank my advisor Mitch Marcus , who gave me the intellectual freedom to pursue what I believed to be the best way to approach natural language processing , and also gave me direction when necessary .","label":"CompareOrContrast","metadata":{},"score":"67.98521"}{"text":"Bailly , Raphal ; Carreras Prez , Xavier ; Quattoni , Ariadna Julieta ( 2013 ) Conference report Open Access .Finite - State Transducers ( FST ) are a standard tool for modeling paired input output sequences and are used in numerous applications , ranging from computational biology to natural language processing .","label":"CompareOrContrast","metadata":{},"score":"67.99606"}{"text":"In general , a computer is a digital machine that uses precise languages with absolute values , such as \" on \" , \" off \" , \" 1 \" , \" 0 \" , \" 3 + 4 \" , \" AND \" , and \" XOR \" .","label":"CompareOrContrast","metadata":{},"score":"68.048645"}{"text":"claim 7 further comprising storing the syntactic history for each node .A method as recited in .claim 7 , wherein the syntactic history may indicate one or more of the following syntactic phenomena : . passive verb phrase ; .","label":"CompareOrContrast","metadata":{},"score":"68.06601"}{"text":"We present experiments showing that multilingual , parallel text in Spanish , French , Russian , and Chinese can be utilized in this framework to improve translation performance on an Arabic - to - English task .Automatic word alignment is the problem of automatically annotating parallel text with translational correspondence .","label":"CompareOrContrast","metadata":{},"score":"68.07215"}{"text":"The visitation is typically from top of the tree ( i.e. , the start node ) to the bottom of the tree ( i.e. , terminal nodes ) .Such visitation is typically done from left - to - right to correspond to the order of reading / writing in English , but may be done from right - to - left .","label":"CompareOrContrast","metadata":{},"score":"68.412415"}{"text":"As shown above in Table 4 on the right - hand side of each rule , each constituent is associated with a particular phrase level that is required for that constituent .Specifically , the number in parenthesis indicates the phrase level of the constituent ( e.g. , \" VP(4 ) \" ) .","label":"CompareOrContrast","metadata":{},"score":"68.5936"}{"text":"As shown above in Table 4 on the right - hand side of each rule , each constituent is associated with a particular phrase level that is required for that constituent .Specifically , the number in parenthesis indicates the phrase level of the constituent ( e.g. , \" VP ( 4 ) \") .","label":"CompareOrContrast","metadata":{},"score":"68.5936"}{"text":"The answer is to ran an experiment .The die is thrown many times and the number of rolls where \" 3 \" is rolled is counted .This data is called the \" training data \" .It is sometimes called the \" training corpus . \"","label":"CompareOrContrast","metadata":{},"score":"68.733475"}{"text":"The visitation is typically from top of the tree ( i.e. , the start node ) to the bottom of the tree ( i.e. , terminal nodes ) .Such visitation is typically done from left - to - right to correspond to the order of reading / writing in English , but may is be done from right - to - left .","label":"CompareOrContrast","metadata":{},"score":"68.82937"}{"text":"FIG .2 it is node 54 a ) , which is the start of the parse tree .Segtypes .A non - terminal node has a type that is called its \" segtype . \"In .FIG .2 , each non - terminal node 54 a - g is labeled with its segtype .","label":"CompareOrContrast","metadata":{},"score":"68.86214"}{"text":"Inclusions from other languages can be a significant source of errors for monolingual parsers .We show this for English inclusions , which are sufficiently frequent to present a problem when parsing German .We describe an annotation - free approach for accurately detecting such inclusions , and develop two methods for interfacing this approach with a state - of - the - art parser for German .","label":"CompareOrContrast","metadata":{},"score":"68.93913"}{"text":"i . s .t .SEM .P .i . )S .P .P .i .W . ) . . .This is indicated by block 324 in .FIG .9 .One additional advantage of the present invention is that the SLM is not simply a parser , but is actually a language model that can be used in a speech recognition system .","label":"CompareOrContrast","metadata":{},"score":"68.94975"}{"text":"x . ) segtype .n .x . )Count . trn .n .x . ) hw .n .x . ) pl .n .x . ) sh .n .x . ) segtype .","label":"CompareOrContrast","metadata":{},"score":"69.00757"}{"text":"X . ) segtype .n .X . )Count . trn .n .X . ) hw .n .X . ) pl .n .X . ) sh .n .X . ) segtype .","label":"CompareOrContrast","metadata":{},"score":"69.00757"}{"text":"These digital values are provided to a frame constructor 406 , which , in one embodiment , groups the values into 25 millisecond frames that start 10 milliseconds apart .These \" frames \" are not the same as the frames or templates used in information extraction , but are just portions of the digitized speech signal .","label":"CompareOrContrast","metadata":{},"score":"69.07414"}{"text":"PDMM is an expansion of an existing probabilistic generative model : Parametric Mixture Model(PMM ) by hierarchical Bayes model .PMM models multiple - topic documents by mixing model parameters of each single topic with an equal mixture ratio .PDMM models multiple - topic documents by mixing model parameters of each single topic with mixture ratio following Dirichlet distribution .","label":"CompareOrContrast","metadata":{},"score":"69.24913"}{"text":"The most distinctive feature of DTG is that , unlike TAG , there is complete uniformity in the way that the two DTG operations relate lexical items : subsertion always corresponds to complementation and sister - adjunction to modification .Furthermore , DTG , unlike TAG , can provide a uniform analysis for whmovement in English and Kashmiri , despite the fact that the wh element in Kashmiri appears in sentence - second position , and not sentence - initial position as in English . \" ...","label":"CompareOrContrast","metadata":{},"score":"69.325195"}{"text":"A method as recited in .claim 4 , wherein the calculating comprises using PredParamRule Probability formula to calculate the probability at the node .A method as recited in .claim 4 , wherein the calculating comprises using both PredParamRule Probability and SynBigram Probability formulas to calculate the probability at the node .","label":"CompareOrContrast","metadata":{},"score":"69.34071"}{"text":"the . experiment .In general , the accuracy of the estimate increases as the amount of training data increases .Theoretically , the estimates increase in accuracy as the amount of training data increases .Conditional Probability .Conditional probabilities are used when there is additional information known about an event that affects the likelihood of the outcome .","label":"CompareOrContrast","metadata":{},"score":"69.368774"}{"text":"the . experiment .In general , the accuracy of the estimate increases as the amount of training data increases .Theoretically , the estimates increase in accuracy as the amount of training data increases .Conditional Probability .Conditional probabilities are used when there is additional information known about an event that affects the likelihood of the outcome .","label":"CompareOrContrast","metadata":{},"score":"69.368774"}{"text":"For example , consider the noun \" nut .\" You would never see a sentence such as ' I want nut . 'or ' Nut is on the table . 'The word \" nut \" wants a determiner such as \" a \" or \" the \" .","label":"CompareOrContrast","metadata":{},"score":"69.53235"}{"text":"For example , consider the noun \" nut .\" You would never see a sentence such as ' I want nut . 'or ' Nut is on the table . 'The word \" nut \" wants a determiner such as \" a \" or \" the \" .","label":"CompareOrContrast","metadata":{},"score":"69.53235"}{"text":"Computer Readable Media .An implementation of the exemplary ranking parser may be stored on or transmitted across some form of computer readable media .Computer readable media can be any available media that can be accessed by a computer .By way of example , and not limitation , computer readable media may comprise computer storage media and communications media .","label":"CompareOrContrast","metadata":{},"score":"69.595566"}{"text":"Computer Readable Media .An implementation of the exemplary ranking parser may be stored on or transmitted across some form of computer readable media .Computer readable media can be any available media that can be accessed by a computer .By way of example , and not limitation , computer readable media may comprise computer storage media and communications media .","label":"CompareOrContrast","metadata":{},"score":"69.595566"}{"text":"X .Prob . trn .n .X . ) hw .n .Y . ) pl .n .Y . ) sh .n .Y . ) hw .n .Z . ) pl .n .","label":"CompareOrContrast","metadata":{},"score":"69.86566"}{"text":"Linguistics is the scientific study of language .It endeavors to answer the question - what is language and how it is represented in the mind ?Linguistics focuses on describing and explaining language .Linguistics focuses on languages ' syntax ( sentence and phrase structures ) , morphology ( word formation ) , and semantics ( meaning ) .","label":"CompareOrContrast","metadata":{},"score":"70.08478"}{"text":"Linguistics is the scientific study of language .It endeavors to answer the question - what is language and how it is represented in the mind ?Linguistics focuses on describing and explaining language .Linguistics focuses on languages ' syntax ( sentence and phrase structures ) , morphology ( word formation ) , and semantics ( meaning ) .","label":"CompareOrContrast","metadata":{},"score":"70.08478"}{"text":"The logical connections depicted in .FIG .1 include a local area network ( LAN ) 171 and a wide area network ( WAN ) 173 , but may also include other networks .Such networking environments are commonplace in offices , enterprise - wide computer networks , intranets and the Internet .","label":"CompareOrContrast","metadata":{},"score":"70.47922"}{"text":"This is indicated by block 300 in .FIG .9 .Next , the trained SLM 310 parses the input data as indicated by block 302 .During parsing , SLM 310 accesses the semantic application schema 312 .In doing so , SLM 310 enforces the template structure in schema 312 during the parsing operation .","label":"CompareOrContrast","metadata":{},"score":"70.67035"}{"text":"Communication media typically embodies computer readable instructions , data structures , program modules or other data in a modulated data signal such as a carrier WAV or other transport mechanism and includes any information delivery media .The term \" modulated data signal \" means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal .","label":"CompareOrContrast","metadata":{},"score":"70.70148"}{"text":"Other input devices ( not shown ) may include a microphone , joystick , game pad , satellite dish , serial port , scanner , or the like .A monitor 972 or other type of display device is also connected to bus 936 via an interface , such as a video adapter 974 .","label":"CompareOrContrast","metadata":{},"score":"70.83692"}{"text":"Other input devices ( not shown ) may include a microphone , joystick , game pad , satellite dish , serial port , scanner , or the like .A monitor 972 or other type of display device is also connected to bus 936 via an interface , such as a video adapter 974 .","label":"CompareOrContrast","metadata":{},"score":"70.83692"}{"text":"In the exemplary implementation of the parser , the arrays initially contain all zeros .All nodes in the training corpus are examined and the corresponding entries in the arrays are incremented .At 208 , the results are stored .At 210 , the process ends .","label":"CompareOrContrast","metadata":{},"score":"70.873825"}{"text":"In the exemplary implementation of the parser , the arrays initially contain all zeros .All nodes in the training corpus are examined and the corresponding entries in the arrays are incremented .At 208 , the results are stored .At 210 , the process ends .","label":"CompareOrContrast","metadata":{},"score":"70.873825"}{"text":"Communication media typically embodies computer readable instructions , data structures , program modules , or other data in a modulated data signal such as carrier wave or other transport mechanism and included any information delivery media .The term \" modulated data signal \" means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal .","label":"CompareOrContrast","metadata":{},"score":"70.97652"}{"text":"Communication media typically embodies computer readable instructions , data structures , program modules , or other data in a modulated data signal such as carrier wave or other transport mechanism and included any information delivery media .The term \" modulated data signal \" means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal .","label":"CompareOrContrast","metadata":{},"score":"70.97652"}{"text":"i . )Total . of .words . in .the .training .corpus .Better results are achieved by using conditional probabilities .In English , words do n't appear in a random order .For example , \" Table red the is \" is highly unlikely but \" The table is red \" is common .","label":"CompareOrContrast","metadata":{},"score":"71.03278"}{"text":"i . )Total . of .words . in .the .training .corpus .Better results are achieved by using conditional probabilities .In English , words do n't appear in a random order .For example , \" Table red the is \" is highly unlikely but \" The table is red \" is common .","label":"CompareOrContrast","metadata":{},"score":"71.03278"}{"text":"FIG .9 shows a portion of a parse tree 150 and visually illustrates what is known and unknown at a node 152 .What is known is above line 154 because it has already been processed .Below line 154 is what is unknown because it has not been processed .","label":"CompareOrContrast","metadata":{},"score":"71.09048"}{"text":"FIG .9 shows a portion of a parse tree 150 and visually illustrates what is known and unknown at a node 152 .What is known is above line 154 because it has already been processed .Below line 154 is what is unknown because it has not been processed .","label":"CompareOrContrast","metadata":{},"score":"71.09048"}{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"CompareOrContrast","metadata":{},"score":"71.32468"}{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"CompareOrContrast","metadata":{},"score":"71.32468"}{"text":"FIG .12 , the system memory includes computer readable media in the form of volatile , such as random access memory ( RAM ) 940 , and/or non - volatile memory , such as read only memory ( ROM ) 938 .","label":"CompareOrContrast","metadata":{},"score":"71.61393"}{"text":"FIG .12 , the system memory includes computer readable media in the form of volatile , such as random access memory ( RAM ) 940 , and/or non - volatile memory , such as read only memory ( ROM ) 938 .","label":"CompareOrContrast","metadata":{},"score":"71.61393"}{"text":"In Table 1 below the name of the transitions are provided under the \" Name \" heading .The actual transitions are provided under the \" Transition \" heading .Table 1 provides an example of transitions being used to parse a sentence like \" Swat flies like ants \" : .","label":"CompareOrContrast","metadata":{},"score":"71.626915"}{"text":"In Table 1 below the name of the transitions are provided under the \" Name \" heading .The actual transitions are provided under the \" Transition \" heading .Table 1 provides an example of transitions being used to parse a sentence like \" Swat flies like ants \" : .","label":"CompareOrContrast","metadata":{},"score":"71.626915"}{"text":"pl(n X ): is the phrase level of n X .sh(n X ): is the syntactic history of n X .segtype(n X ): is the segtype of n X .modhw(n X ): is the modifying headword of n X .","label":"CompareOrContrast","metadata":{},"score":"71.75419"}{"text":"pl(n X ): is the phrase level of n X .sh(n X ): is the syntactic history of n X .segtype(n X ): is the segtype of n X .modhw(n X ): is the modifying headword of n X .","label":"CompareOrContrast","metadata":{},"score":"71.75419"}{"text":"Thus , \" I want a nut \" is preferred over \" I want nut . \"Although \" I want a nut \" appears , at a surface level , to have more transitions and thus should have a lower goodness measure using conventional approaches .","label":"CompareOrContrast","metadata":{},"score":"71.8106"}{"text":"Thus , \" I want a nut \" is preferred over \" I want nut . \"Although \" I want a nut \" appears , at a surface level , to have more transitions and thus should have a lower goodness measure using conventional approaches .","label":"CompareOrContrast","metadata":{},"score":"71.8106"}{"text":"In a dependency representation , every node in the tree structure is a surface word ( i.e. , there are no abstrac ... . by Paul Kingsbury , Martha Palmer - In Language Resources and Evaluation , 2002 . \" ...This paper describes our approach to the development of a Proposition Bank , which involves the addition of semantic information to the Penn English Treebank .","label":"CompareOrContrast","metadata":{},"score":"72.03821"}{"text":"Prob .x .y . )Prob .x .y . )Prob .y . )Count .x .y . )Total . of . events .Count .y . )Total . of . events .","label":"CompareOrContrast","metadata":{},"score":"72.06801"}{"text":"Prob .x .y . )Prob .x .y . )Prob .y . )Count .x .y . )Total . of . events .Count .y . )Total . of . events .","label":"CompareOrContrast","metadata":{},"score":"72.06801"}{"text":"For example , using a six - sided fair die with the sides labeled 1 - 6 , the probability of rolling a three is 1/6 .Estimating Probabilities using Training Data .The probability of events using a randomly shuffled deck or fair die can be mathematically derived .","label":"CompareOrContrast","metadata":{},"score":"72.16951"}{"text":"For example , using a six - sided fair die with the sides labeled 1 - 6 , the probability of rolling a three is 1/6 .Estimating Probabilities using Training Data .The probability of events using a randomly shuffled deck or fair die can be mathematically derived .","label":"CompareOrContrast","metadata":{},"score":"72.16951"}{"text":"First , VerbtoVP always runs to create the initial verb phrase .Then , post modifiers are added using PredAdj and/or VPwNPr1 .Then \" have \" and quantifiers can be added .Next , the subject is added using SubjAJP or VPwNP1 .","label":"CompareOrContrast","metadata":{},"score":"72.33067"}{"text":"Prob . trn .n .X . ) hw .n .X . ) pl .n .X . ) sh .n .X . ) segtype .n .X . )Prob . mod .hw .","label":"CompareOrContrast","metadata":{},"score":"72.76799"}{"text":"First , VerbtoVP always runs to create the initial verb phrase .Then , post modifiers are added using PredAdj and/or VPwNPrl .Then \" have \" and quantifiers can be added .Next , the subject is added using SubjAJP or VPwNPl .","label":"CompareOrContrast","metadata":{},"score":"72.982086"}{"text":"FIGS .4 a and 4 b illustrate two exemplary parse trees of two of seven syntactically valid parses of sample phrase , \" time flies like an arrow . \" FIGS .5 a and 5 b show fragments of two pairs of typical parse trees .","label":"CompareOrContrast","metadata":{},"score":"73.07756"}{"text":"FIGS .4 a and 4 b illustrate two exemplary parse trees of two of seven syntactically valid parses of sample phrase , \" time flies like an arrow . \" FIGS .5 a and 5 b show fragments of two pairs of typical parse trees .","label":"CompareOrContrast","metadata":{},"score":"73.07756"}{"text":"It may store it , report it , return it , display it , or the like .The run - time process ends at 318 .This process may be described by the following exemplary pseudocode : .For each node n in the parse tree .","label":"CompareOrContrast","metadata":{},"score":"73.18775"}{"text":"It may store it , report it , return it , display it , or the like .The run - time process ends at 318 .This process may be described by the following exemplary pseudocode : .For each node n in the parse tree .","label":"CompareOrContrast","metadata":{},"score":"73.18775"}{"text":"By way of example only , .The drives and their associated computer storage media discussed above and illustrated in .FIG .1 , provide storage of computer readable instructions , data structures , program modules and other data for the computer 110 .","label":"CompareOrContrast","metadata":{},"score":"73.39676"}{"text":"Probability .Prob . modhw .n .x . ) trn .n .x . ) hw .n .x . )Count . modhw .n .x . ) trn .n .x . ) hw .","label":"CompareOrContrast","metadata":{},"score":"73.39828"}{"text":"Among the major phrase types are noun phrases , verb phrases , prepositional phrases , and adjective phrases .Headword .The headword is the key word in a phrase .This is because it determines the syntactic character of a phrase .","label":"CompareOrContrast","metadata":{},"score":"73.56037"}{"text":"Among the major phrase types are noun phrases , verb phrases , prepositional phrases , and adjective phrases .Headword .The headword is the key word in a phrase .This is because it determines the syntactic character of a phrase .","label":"CompareOrContrast","metadata":{},"score":"73.56037"}{"text":"5 a highest .This may be correct , but often it will be wrong since it will choose Tree 90 of .FIG .5 a regardless of the linguistic context in which the rules appear .For example , assume that the headword was \" smiled \" .","label":"CompareOrContrast","metadata":{},"score":"73.73116"}{"text":"5 a highest .This may be correct , but often it will be wrong since it will choose Tree 90 of .FIG .5 a regardless of the linguistic context in which the rules appear .For example , assume that the headword was \" smiled \" .","label":"CompareOrContrast","metadata":{},"score":"73.73116"}{"text":"This paper describes our approach to the development of a Proposition Bank , which involves the addition of semantic information to the Penn English Treebank .Our primary goal is the labeling of syntactic nodes with specific argument labels that preserve the similarity of roles such as the window in John broke the window and the window broke .","label":"CompareOrContrast","metadata":{},"score":"73.74869"}{"text":"Generally , program modules include routines , programs , objects , components , data structures , etc . that perform particular tasks or implement particular abstract data types .The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network .","label":"CompareOrContrast","metadata":{},"score":"74.02811"}{"text":"SymbolB may be \" np \" for noun phrase and symbolC may be \" vp \" for verb phrase .The \" np \" and \" vp \" symbols may be further broken down until the actual words in the sentence are represented by symbolB , symbolC , etc . .","label":"CompareOrContrast","metadata":{},"score":"74.051994"}{"text":"SymbolB may be \" np \" for noun phrase and symbolC may be \" vp \" for verb phrase .The \" np \" and \" vp \" symbols may be further broken down until the actual words in the sentence are represented by symbolB , symbolC , etc . .","label":"CompareOrContrast","metadata":{},"score":"74.051994"}{"text":"This approach uses collocations to compute a goodness function .A collocation is two or more words in some adjacent ordering or syntactic relationship .Examples of such include : \" strong tea \" , \" weapons of mass destruction \" , \" make up \" , \" the rich and powerful \" , \" stiff breeze \" , and \" broad daylight . \"","label":"CompareOrContrast","metadata":{},"score":"74.070496"}{"text":"This approach uses collocations to compute a goodness function .A collocation is two or more words in some adjacent ordering or syntactic relationship .Examples of such include : \" strong tea \" , \" weapons of mass destruction \" , \" make up \" , \" the rich and powerful \" , \" stiff breeze \" , and \" broad daylight . \"","label":"CompareOrContrast","metadata":{},"score":"74.070496"}{"text":"This is the role of linguistics .Part of Speech .Linguists group words of a language into classes , which show similar syntactic behavior , and often a typical semantic type .These word classes are otherwise called \" syntactic \" or \" grammatical categories \" , but more commonly still by the traditional names \" part of speech \" ( POS ) .","label":"CompareOrContrast","metadata":{},"score":"74.12253"}{"text":"This is the role of linguistics .Part of Speech .Linguists group words of a language into classes , which show similar syntactic behavior , and often a typical semantic type .These word classes are otherwise called \" syntactic \" or \" grammatical categories \" , but more commonly still by the traditional names \" part of speech \" ( POS ) .","label":"CompareOrContrast","metadata":{},"score":"74.12253"}{"text":"The parse represented by the parse tree 70 of .FIG .4 a appears in nine of those times .In addition , the parse represented by the parse tree 80 of .FIG .4 b appears only once .","label":"CompareOrContrast","metadata":{},"score":"74.41803"}{"text":"The parse represented by the parse tree 70 of .FIG .4 a appears in nine of those times .In addition , the parse represented by the parse tree 80 of .FIG .4 b appears only once .","label":"CompareOrContrast","metadata":{},"score":"74.41803"}{"text":"FIG .7 shows two parse trees , 120 and 130 , for the same verb phrase .In tree 120 , the verb has a direct object as represented by NP at 122 .In tree 130 , the verb does not take a direct object .","label":"CompareOrContrast","metadata":{},"score":"74.69327"}{"text":"FIG .7 shows two parse trees , 120 and 130 , for the same verb phrase .In tree 120 , the verb has a direct object as represented by NP at 122 .In tree 130 , the verb does not take a direct object .","label":"CompareOrContrast","metadata":{},"score":"74.69327"}{"text":"For example , \" I gave Bill the ball , \" and \" I promised Bill the money \" Computational Linguistics .Transitions ( i.e. , Rewrite Rules ) .The regularities of a natural language 's word order and grammar are often captured by a set of rules called \" transitions \" or \" rewrite rules .","label":"CompareOrContrast","metadata":{},"score":"75.467636"}{"text":"For example , \" I gave Bill the ball , \" and \" I promised Bill the money \" Computational Linguistics .Transitions ( i.e. , Rewrite Rules ) .The regularities of a natural language 's word order and grammar are often captured by a set of rules called \" transitions \" or \" rewrite rules .","label":"CompareOrContrast","metadata":{},"score":"75.467636"}{"text":"Syntactic Features .Syntactic features are distinctive properties of a word relating to how the word is used syntactically .For example , the syntactic features of a noun include whether it is singular ( e.g. cat ) or plural ( e.g. cats ) and whether it is countable ( e.g. five forks ) or uncountable ( e.g. air ) .","label":"CompareOrContrast","metadata":{},"score":"76.03736"}{"text":"Syntactic Features .Syntactic features are distinctive properties of a word relating to how the word is used syntactically .For example , the syntactic features of a noun include whether it is singular ( e.g. cat ) or plural ( e.g. cats ) and whether it is countable ( e.g. five forks ) or uncountable ( e.g. air ) .","label":"CompareOrContrast","metadata":{},"score":"76.03736"}{"text":"The syntactic and semantic behavior of a headword is often independent of its inflectional morphology .For example , the verbs \" walks \" , \" walking \" , and \" walked \" are derived from the verb \" walk \" .","label":"CompareOrContrast","metadata":{},"score":"76.577225"}{"text":"The syntactic and semantic behavior of a headword is often independent of its inflectional morphology .For example , the verbs \" walks \" , \" walking \" , and \" walked \" are derived from the verb \" walk \" .","label":"CompareOrContrast","metadata":{},"score":"76.577225"}{"text":"A careful error analysis suggests that when we account for annotation errors in the gold standard , the error rate drops to 0.9 % , with the hand - written rules outperforming the machine learning - based system .We present in this paper methods to improve HMM - based part - of - speech ( POS ) tagging of Mandarin .","label":"CompareOrContrast","metadata":{},"score":"76.70674"}{"text":"The tree 50 includes a set of terminal nodes 52 a - 52 c .These nodes are at the end of each branch of the tree and can not be further expanded .For example , \" like \" 52 b can not be expanded any further because it is the word itself .","label":"CompareOrContrast","metadata":{},"score":"76.94014"}{"text":"The tree 50 includes a set of terminal nodes 52 a - 52 c .These nodes are at the end of each branch of the tree and can not be further expanded .For example , \" like \" 52 b can not be expanded any further because it is the word itself .","label":"CompareOrContrast","metadata":{},"score":"76.94014"}{"text":"For example , the object of the verb \" drink \" is more likely to be \" coffee \" or \" water \" than \" house \" .As described above in the background section , this is a conventional technique to calculate a goodness measure .","label":"CompareOrContrast","metadata":{},"score":"76.94205"}{"text":"For example , the object of the verb \" drink \" is more likely to be \" coffee \" or \" water \" than \" house \" .As described above in the background section , this is a conventional technique to calculate a goodness measure .","label":"CompareOrContrast","metadata":{},"score":"76.94205"}{"text":"FIG .3 shows an example of such a lexicalized parse tree 60 .( For the purposes of this example , directional path 66 with circled reference points is ignored . )FIG .3 is a parse tree of one of many parses of the sentence , \" swat flies like ants . \" Terminal nodes 62 a - d , which are the words of the sentence , are not annotated .","label":"CompareOrContrast","metadata":{},"score":"76.975815"}{"text":"FIG .3 shows an example of such a lexicalized parse tree 60 .( For the purposes of this example , directional path 66 with circled reference points is ignored . )FIG .3 is a parse tree of one of many parses of the sentence , \" swat flies like ants . \" Terminal nodes 62 a - d , which are the words of the sentence , are not annotated .","label":"CompareOrContrast","metadata":{},"score":"76.975815"}{"text":"Thus , Formula 7 may be approximated by removing sh(n X ) .The result is Formula 8 .This is a reasonable simplification .However , it should be clear to anyone of ordinary skill in the art that this simplification is not necessary .","label":"CompareOrContrast","metadata":{},"score":"77.07999"}{"text":"Thus , Formula 7 may be approximated by removing sh(n X ) .The result is Formula 8 .This is a reasonable simplification .However , it should be clear to anyone of ordinary skill in the art that this simplification is not necessary .","label":"CompareOrContrast","metadata":{},"score":"77.07999"}{"text":"FIG .12 illustrates an example of a suitable computing environment 920 on which the exemplary ranking parser may be implemented .The exemplary computing environment 920 may be a computing environment comprising or utilizing a NLP system .Exemplary computing environment 920 is only one example of a suitable computing environment and is not intended to suggest any limitation as to the scope of use or functionality of the invention .","label":"CompareOrContrast","metadata":{},"score":"77.17194"}{"text":"FIG .12 illustrates an example of a suitable computing environment 920 on which the exemplary ranking parser may be implemented .The exemplary computing environment 920 may be a computing environment comprising or utilizing a NLP system .Exemplary computing environment 920 is only one example of a suitable computing environment and is not intended to suggest any limitation as to the scope of use or functionality of the invention .","label":"CompareOrContrast","metadata":{},"score":"77.17194"}{"text":"The process of attempting to understand what the user has expressed is commonly referred to as natural language understanding ( NLU ) or , if the input modality being used by the user is speech , the process is referred to as spoken language understanding ( SLU ) .","label":"CompareOrContrast","metadata":{},"score":"77.69511"}{"text":"The English student recognizes that the word \" use \" is a verb , the word \" a \" is an adjective , and the word \" saw \" is a noun .Notice that the word \" saw \" is used in the two sentences as different parts of speech - a verb and a noun - which an English speaking person realizes .","label":"CompareOrContrast","metadata":{},"score":"77.78707"}{"text":"The English student recognizes that the word \" use \" is a verb , the word \" a \" is an adjective , and the word \" saw \" is a noun .Notice that the word \" saw \" is used in the two sentences as different parts of speech - a verb and a noun - which an English speaking person realizes .","label":"CompareOrContrast","metadata":{},"score":"77.78707"}{"text":"At 202 , the parser examines each parse tree of the \" correctly \" parsed sentences ( and phrases ) of the corpus .To examine the corpus , a depth - first tree walk is performed on each parse tree .","label":"CompareOrContrast","metadata":{},"score":"77.91186"}{"text":"At 202 , the parser examines each parse tree of the \" correctly \" parsed sentences ( and phrases ) of the corpus .To examine the corpus , a depth - first tree walk is performed on each parse tree .","label":"CompareOrContrast","metadata":{},"score":"77.91186"}{"text":"By conditioning on DOMODAL_FRONTING the exemplary parser knows to expect this disagreement .There are restrictions on what can be between the fronted verb and the subject .One would not normally say \" Did frequently he pass exams ? \"However , one could say \" Frequently he passed exams .","label":"CompareOrContrast","metadata":{},"score":"78.06004"}{"text":"n .Y . ) sh .n .Y . ) hw .n .Z . ) pl .n .Z . ) sh .n .Z . ) hw .n .X . ) pl .","label":"CompareOrContrast","metadata":{},"score":"78.5281"}{"text":"This means that its headword is \" ants .\" The parse tree 60 in .FIG .3 is also annotated with the names of the transitions between nodes .For example , the transition name \" vp_verbvp \" is listed between node 64 f and node 64 h. .","label":"CompareOrContrast","metadata":{},"score":"78.55214"}{"text":"This means that its headword is \" ants .\" The parse tree 60 in .FIG .3 is also annotated with the names of the transitions between nodes .For example , the transition name \" vp_verbvp \" is listed between node 64 f and node 64 h. .","label":"CompareOrContrast","metadata":{},"score":"78.55214"}{"text":"SubjPQuant specifies subject post - quantifiers on a verb phrase .For example , in \" we all found useful the guidelines \" is [ NP all][VP found useful the guidelines].\" all \" is a subject post - qualifier .","label":"CompareOrContrast","metadata":{},"score":"78.98427"}{"text":"SubjPQuant specifies subject post - quantifiers on a verb phrase .For example , in \" we all found useful the guidelines \" is [ NP all][VP found useful the guidelines].\" all \" is a subject post - qualifier .","label":"CompareOrContrast","metadata":{},"score":"78.98427"}{"text":"Prob . event . )Count . event . )Total . number . of . events .Prob . roll . is . a .Count . number . of .rolls . in .the . experiment . )Total . of .","label":"CompareOrContrast","metadata":{},"score":"79.509186"}{"text":"Prob . event . )Count . event . )Total . number . of . events .Prob . roll . is . a .Count . number . of .rolls . in .the . experiment . )Total . of .","label":"CompareOrContrast","metadata":{},"score":"79.509186"}{"text":"bigram_prob .BigramCountNumerator . modhw .n . ) trn .n . ) hw .n . )BigramCountDenominator . trn .n . ) hw .n . )End loop .Alternatives .Computing More Precise Bigrams .Above , Formula 7 was simplified into Formula 8 by removing the syntactic history of n X ( i.e. , sh(n X ) ) .","label":"CompareOrContrast","metadata":{},"score":"79.51518"}{"text":"bigram_prob .BigramCountNumerator . modhw .n . ) trn .n . ) hw .n . )BigramCountDenominator . trn .n . ) hw .n . )End loop .Alternatives .Computing More Precise Bigrams .Above , Formula 7 was simplified into Formula 8 by removing the syntactic history of n X ( i.e. , sh(n X ) ) .","label":"CompareOrContrast","metadata":{},"score":"79.51518"}{"text":"FIG .12 are a local area network ( LAN ) 977 and a general wide area network ( WAN ) 979 .Such networking environments are commonplace in offices , enterprise - wide computer networks , intranets , and the Internet .","label":"CompareOrContrast","metadata":{},"score":"79.527855"}{"text":"FIG .12 are a local area network ( LAN ) 977 and a general wide area network ( WAN ) 979 .Such networking environments are commonplace in offices , enterprise - wide computer networks , intranets , and the Internet .","label":"CompareOrContrast","metadata":{},"score":"79.527855"}{"text":"FIG .7A have been annotated with semantic labels , those labels are added to the syntactic labels to make a joint syntactic and semantic label .The first such label is indicated by numeral 234 and is the NP_Smith_attendee label .","label":"CompareOrContrast","metadata":{},"score":"79.7242"}{"text":"By conditioning on DOMODAL_FRONTING the exemplary parser knows to expect this disagreement .There are restrictions on what can be between the fronted verb and the subject .One would not normally say \" Did frequently he pass exams ? 'However , one could say \" Frequently he passed exams .","label":"CompareOrContrast","metadata":{},"score":"80.2799"}{"text":"Seldom would he be home before 4 a.m. .Rarely did he pass exams .Affect : .Compare the first sentence to the kernel representation : \" I had never seen such chaos .\" When the negation ( \" never \" ) is fronted for focusing purposes there must be subject - verb inversion : note the ' had ' before the ' I ' .","label":"CompareOrContrast","metadata":{},"score":"80.505104"}{"text":"Seldom would he be home before 4 a.m. .Rarely did he pass exams .Affect : .Compare the first sentence to the kernel representation : \" I had never seen such chaos .\" When the negation ( \" never \" ) is fronted for focusing purposes there must be subject - verb inversion : note the ' had ' before the ' I ' .","label":"CompareOrContrast","metadata":{},"score":"80.505104"}{"text":"FIG .8 .The constituent boundaries for the \" attendee \" semantic label 224 are the words \" John \" and \" Smith \" .Therefore , during the training step 230 , any parses generated by the SLM which do not combine \" John \" and \" Smith \" together as a single constituent will not be considered .","label":"CompareOrContrast","metadata":{},"score":"80.628456"}{"text":"Affect : .The verb in the passive construction ( e.g. , \" lick \" , \" detonate \" , \" bore \" ) in the great majority of cases does not take a syntactic object as a post - modifier .","label":"CompareOrContrast","metadata":{},"score":"80.726204"}{"text":"Affect : .The verb in the passive construction ( e.g. , \" lick \" , \" detonate \" , \" bore \" ) in the great majority of cases does not take a syntactic object as a post - modifier .","label":"CompareOrContrast","metadata":{},"score":"80.726204"}{"text":"The House rolls a pair of dice .If the man bets and if the dice sum to 12 , the man gets $ 35 , otherwise the man loses his bet .Since the probability of rolling two die that sum to 12 is 1/36 , the man should expect to lose money playing this game .","label":"CompareOrContrast","metadata":{},"score":"81.05966"}{"text":"The House rolls a pair of dice .If the man bets and if the dice sum to 12 , the man gets $ 35 , otherwise the man loses his bet .Since the probability of rolling two die that sum to 12 is 1/36 , the man should expect to lose money playing this game .","label":"CompareOrContrast","metadata":{},"score":"81.05966"}{"text":"Of course , it should be noted that ranking component 316 can be integrally formed with SLM component 310 , or it can be a separate component .In ranking parse trees 314 , ranking component 316 can rank in a number of different ways .","label":"CompareOrContrast","metadata":{},"score":"81.335655"}{"text":"I thank all of my thesis committee members : John La erty from Carnegie Mellon University , Aravind Joshi , Lyle Ungar , and Mark Liberman , for their extremely valuable suggestions and comments about my thesis research .I thank Mike Collins , Jason Eisner , and Dan Melamed , with whom I 've had many stimulating and impromptu discussions in the LINC lab .","label":"CompareOrContrast","metadata":{},"score":"81.85992"}{"text":"FIG .5 b illustrate the same parses shown in trees 90 and 92 in .FIG .5 a , but the headword \" smiled \" is noted .English - speaking humans know that Tree 94 of .FIG .","label":"CompareOrContrast","metadata":{},"score":"82.48853"}{"text":"FIG .5 b illustrate the same parses shown in trees 90 and 92 in .FIG .5 a , but the headword \" smiled \" is noted .English - speaking humans know that Tree 94 of .FIG .","label":"CompareOrContrast","metadata":{},"score":"82.48853"}{"text":"FIG .4 b , the insects called \" time flies \" 84 enjoy the arrow object ; just as one would say \" Fruit flies like a meal .\" Either parse could be what the speaker intended .In addition , five other 15 syntactically valid parses may represent the meaning that the speaker intended .","label":"CompareOrContrast","metadata":{},"score":"83.14451"}{"text":"The information extraction process will also desirably associate the phrase \" John Smith \" with the concept of \" meeting attendee \" and the word \" Saturday \" with the concept of \" meeting day \" .Current approaches used for information extraction require handwritten grammars , usually context free grammars ( CFGs ) .","label":"CompareOrContrast","metadata":{},"score":"83.25493"}{"text":"Such interfaces ( such as spoken language interfaces ) are sometimes one of the only interfaces practicable as opposed to other traditional methods of input , such as keyboards and mice .In natural language interfaces , the user speaks or otherwise interacts with the computer ( which can be a PDA , a desktop computer , a telephone , etc . ) and asks the computer to carryout certain actions .","label":"CompareOrContrast","metadata":{},"score":"83.58269"}{"text":"b .b . g .Prob .c . a . a .g .b .b .g . )Prob .c . a . a .g .b .b .g .c .Prob .","label":"CompareOrContrast","metadata":{},"score":"83.71251"}{"text":"b .b . g .Prob .c . a . a .g .b .b .g . )Prob .c . a . a .g .b .b .g .c .Prob .","label":"CompareOrContrast","metadata":{},"score":"83.71251"}{"text":"FIG .7A , frame label 222 identifies the \" schedule meeting \" action .Semantic slot labels 224 and 226 correspond to slots for the frame .Slot label 224 is the \" attendee \" slot and , as annotated , corresponds to the input word sequence ( or constituent ) \" John Smith \" .","label":"CompareOrContrast","metadata":{},"score":"83.77551"}{"text":"Neither should the computing environment 100 be interpreted as having any dependency or requirement relating to any one or combination of components illustrated in the exemplary operating environment 100 .The invention is operational with numerous other general purpose or special purpose computing system environments or configurations .","label":"CompareOrContrast","metadata":{},"score":"83.80268"}{"text":"The audio signals detected by microphone 402 are converted into electrical signals that are provided to analog - to - digital converter 404 .A - to - D converter 404 converts the analog signal from microphone 402 into a series of digital values .","label":"CompareOrContrast","metadata":{},"score":"83.83197"}{"text":"Now suppose that the man had a fairy godmother that could whisper in his ear and tell him whether one of the die rolled was going to be a six .With the fairy godmother 's help , the man can make money on the game .","label":"CompareOrContrast","metadata":{},"score":"84.26724"}{"text":"Now suppose that the man had a fairy godmother that could whisper in his ear and tell him whether one of the die rolled was going to be a six .With the fairy godmother 's help , the man can make money on the game .","label":"CompareOrContrast","metadata":{},"score":"84.26724"}{"text":"The basic idea is to find the probability of two words of being in a syntactic relationship to each other , regardless of where those words appear in the sentence .The words may be adjacent ( e.g. , \" I drink coffee . \" ) , but need not be ( e.g. , \" I love to drink hot black coffee . \" )","label":"CompareOrContrast","metadata":{},"score":"84.53297"}{"text":"The basic idea is to find the probability of two words of being in a syntactic relationship to each other , regardless of where those words appear in the sentence .The words may be adjacent ( e.g. , \" I drink coffee . \" ) , but need not be ( e.g. , \" I love to drink hot black coffee . \" )","label":"CompareOrContrast","metadata":{},"score":"84.53297"}{"text":"This can be used to create a goodness function based on \" syntactic bigrams .\" If the following four sentences appeared in the training corpus , all four would provide evidence that \" coffee \" is often the object of the verb \" to drink \" : .","label":"CompareOrContrast","metadata":{},"score":"84.98034"}{"text":"This can be used to create a goodness function based on \" syntactic bigrams .\" If the following four sentences appeared in the training corpus , all four would provide evidence that \" coffee \" is often the object of the verb \" to drink \" : .","label":"CompareOrContrast","metadata":{},"score":"84.98034"}{"text":"FIG .1 , for example , hard disk drive 141 is illustrated as storing operating system 144 , application programs 145 , other program modules 146 , and program data 147 .Note that these components can either be the same as or different from operating system 134 , application programs 135 , other program modules 136 , and program data 137 .","label":"CompareOrContrast","metadata":{},"score":"85.55609"}{"text":"This example helps explain \" topicalization .\" Start , for example , with the phrase \" I eat nematodes .\" The object of the verb , nematodes , can be put before the subject .This leads to \" Nematodes I eat . \" The movement of the object is called topicalization , and hence the rule name .","label":"CompareOrContrast","metadata":{},"score":"85.75653"}{"text":"This example helps explain \" topicalization .\" Start , for example , with the phrase \" I eat nematodes .\" The object of the verb , nematodes , can be put before the subject .This leads to \" Nematodes I eat . \" The movement of the object is called topicalization , and hence the rule name .","label":"CompareOrContrast","metadata":{},"score":"85.75653"}{"text":"FIG .4 b , the insects called \" time flies \" 84 enjoy the arrow object ; just as one would say \" Fruit flies like a meal .\" Either parse could be what the speaker intended .In addition , five other syntactically valid parses may represent the meaning that the speaker intended .","label":"CompareOrContrast","metadata":{},"score":"85.81958"}{"text":"Saturday , June 30 , 2007 .In the last decade , there have been significant developments in the design of approximate randomized algorithms for high - dimensional data .These include : hashing - based algorithms for similarity search problems , computing succinct approximate \" sketches \" of high - dimensional objects , etc .","label":"CompareOrContrast","metadata":{},"score":"86.0323"}{"text":"However , if a passive construction is being built , those are not allowed as transition below the passive and the verb that is passivized .( E.g. , The bomb must be had detonated . )However , the converse is not true , passive must follow progressive , perfect , or modal .","label":"CompareOrContrast","metadata":{},"score":"86.82733"}{"text":"However , if a passive construction is being built , those are not allowed as transition below the passive and the verb that is passivized .( E.g. , The bomb must be had detonated . )However , the converse is not true , passive must follow progressive , perfect , or modal .","label":"CompareOrContrast","metadata":{},"score":"86.82733"}{"text":"Prob .b . a . a .g . )Prob .b . a . a .g .b .Prob .b . a . a .g .b .b .Prob .b .g . a . a .","label":"CompareOrContrast","metadata":{},"score":"87.20143"}{"text":"Prob .b . a . a .g . )Prob .b . a . a .g .b .Prob .b . a . a .g .b .b .Prob .b .g . a . a .","label":"CompareOrContrast","metadata":{},"score":"87.20143"}{"text":"He melted .He had melted .He had been melted .To see an example of null transitions , consider the phrase : . \"Surprising , we found useful the guidelines . \"Notice that this phrase differs from the similar phrase used above in that \" . . .","label":"CompareOrContrast","metadata":{},"score":"87.23129"}{"text":"He melted .He had melted .He had been melted .To see an example of null transitions , consider the phrase : . \"Surprising , we found useful the guidelines . \"Notice that this phrase differs from the similar phrase used above in that \" . . .","label":"CompareOrContrast","metadata":{},"score":"87.23129"}{"text":"No .10/929,167 , filed Aug. 30 , 2004 , which itself is a U.S. patent application Ser .No . 09/620,745 , filed on Jul. 20 , 2000 and maturing into U.S. Pat .No .6,952,666 .Accordingly , this application claims priority to Jul. 20 , 2000 .","label":"CompareOrContrast","metadata":{},"score":"87.29808"}{"text":"2 is an illustration of a typical parse tree representing a syntactically valid parse of sample phrase , \" flies like ants . \"FIG .3 is another illustration of a typical parse tree representing a syntactically valid parse of sample phrase , \" swat flies like ants .","label":"CompareOrContrast","metadata":{},"score":"87.473335"}{"text":"2 is an illustration of a typical parse tree representing a syntactically valid parse of sample phrase , \" flies like ants . \"FIG .3 is another illustration of a typical parse tree representing a syntactically valid parse of sample phrase , \" swat flies like ants .","label":"CompareOrContrast","metadata":{},"score":"87.473335"}{"text":"For instance , a sentence in a natural language text read as follows : .Betty saw a bird .However , in the context of other sentences , the same words might assume different parts of speech .Consider the following sentence : .","label":"CompareOrContrast","metadata":{},"score":"87.61868"}{"text":"For instance , a sentence in a natural language text read as follows : .Betty saw a bird .However , in the context of other sentences , the same words might assume different parts of speech .Consider the following sentence : .","label":"CompareOrContrast","metadata":{},"score":"87.61868"}{"text":"A natural language processing ( NLP ) system is typically a computer - implemented software system , which intelligently derives meaning and context from an input string of natural language text . \"Natural languages \" are the imprecise languages that are spoken by humans ( e.g. , English , French , Japanese ) .","label":"CompareOrContrast","metadata":{},"score":"87.90445"}{"text":"A natural language processing ( NLP ) system is typically a computer - implemented software system , which intelligently derives meaning and context from an input string of natural language text . \"Natural languages \" are the imprecise languages that are spoken by humans ( e.g. , English , French , Japanese ) .","label":"CompareOrContrast","metadata":{},"score":"87.90445"}{"text":"9 illustrates the operation of the SLM parser during test or run time .FIG .10 is a data flow diagram illustrating a SLM 310 and a ranking component 316 .FIGS . 9 and 10 will be described in conjunction with one another .","label":"CompareOrContrast","metadata":{},"score":"87.91118"}{"text":"Since computers are tools for human use , input devices and input processing system are needed for humans to use the computer tools .Since it is generally easier to train humans to conform to the digital requirements of computers than vice versa , humans have used precise input is interfaces such as a keyboard and a mouse .","label":"CompareOrContrast","metadata":{},"score":"88.075554"}{"text":"Since computers are tools for human use , input devices and input processing system are needed for humans to use the computer tools .Since it is generally easier to train humans to conform to the digital requirements of computers than vice versa , humans have used precise input interfaces such as a keyboard and a mouse .","label":"CompareOrContrast","metadata":{},"score":"88.136246"}{"text":"For example , in \" John hit the ball \" [ NP John ] [ VP hit the ball ] where John is the subject .An APSG is similar to a CFG in that there are rules that look like CFG rules .","label":"CompareOrContrast","metadata":{},"score":"88.19264"}{"text":"With reference to .FIG .1 , an exemplary system for implementing the invention includes a general purpose computing device in the form of a computer 110 .Components of computer 110 may include , but are not limited to , a processing unit 120 , a system memory 130 , and a system bus 121 that couples various system components including the system memory to the processing unit 120 .","label":"CompareOrContrast","metadata":{},"score":"88.904686"}{"text":"RAM 132 typically contains data and/or program modules that are immediately accessible to and/or presently being operated on by processing unit 120 .By way of example , and not limitation , .FIG .1 illustrates operating system 134 , application programs 135 , other program modules 136 , and program data 137 .","label":"CompareOrContrast","metadata":{},"score":"89.282074"}{"text":"In contrast , when in the active form , the mono - transitive verb \" hit \" takes a direct object .For example , \" I hit the ball \" in the active form has a direct object \" ball \" to the verb \" hit \" , but \" the ball was hit \" in the passive form has no direct object to \" hit . \" English - speaking humans know that tree 120 will never occur .","label":"CompareOrContrast","metadata":{},"score":"89.39046"}{"text":"In contrast , when in the active form , the mono - transitive verb \" hit \" takes a direct object .For example , \" I hit the ball \" in the active form has a direct object \" ball \" to the verb \" hit \" , but \" the ball was hit \" in the passive form has no direct object to \" hit . \" English - speaking humans know that tree 120 will never occur .","label":"CompareOrContrast","metadata":{},"score":"89.39046"}{"text":"Computer 930 typically includes a variety of computer readable media .Such media may be any available media that is accessible by computer 930 , and it includes both volatile and non - volatile media , removable and non - removable media .","label":"CompareOrContrast","metadata":{},"score":"89.39285"}{"text":"Computer 930 typically includes a variety of computer readable media .Such media may be any available media that is accessible by computer 930 , and it includes both volatile and non - volatile media , removable and non - removable media .","label":"CompareOrContrast","metadata":{},"score":"89.39285"}{"text":"FIG .12 , the computing environment 920 includes a general - purpose computing device in the form of a computer 930 .The components of computer 920 may include , by are not limited to , one or more processors or processing units 932 , a system memory 934 , and a bus 936 that couples various system components including the system memory 934 to the processor 932 .","label":"CompareOrContrast","metadata":{},"score":"89.7843"}{"text":"FIG .12 , the computing environment 920 includes a general - purpose computing device in the form of a computer 930 .The components of computer 920 may include , by are not limited to , one or more processors or processing units 932 , a system memory 934 , and a bus 936 that couples various system components including the system memory 934 to the processor 932 .","label":"CompareOrContrast","metadata":{},"score":"89.7843"}{"text":"Imperative .Sample sentences : .Go to your room now .Pour three ounces of the gin into the vermouth .Please pass the trisodium phosphate .Affect : .Sentences usually have subjects .Imperatives do n't .However , it is less likely that ' tensed ' clauses within sentences lack subjects .","label":"CompareOrContrast","metadata":{},"score":"89.94049"}{"text":"Imperative .Sample sentences : .Go to your room now .Pour three ounces of the gin into the vermouth .Please pass the trisodium phosphate .Affect : .Sentences usually have subjects .Imperatives do n't .However , it is less likely that ' tensed ' clauses within sentences lack subjects .","label":"CompareOrContrast","metadata":{},"score":"89.94049"}{"text":"8 shows a typical parse tree of a sample sentence , \" Graceland , I like to visit .\" This figure illustrates the \" topicalization \" syntactic phenomenon .FIG .9 shows a fragment of a genericized parse tree .","label":"CompareOrContrast","metadata":{},"score":"90.03122"}{"text":"8 shows a typical parse tree of a sample sentence , \" Graceland , I like to visit .\" This figure illustrates the \" topicalization \" syntactic phenomenon .FIG .9 shows a fragment of a genericized parse tree .","label":"CompareOrContrast","metadata":{},"score":"90.03122"}{"text":"Computer 110 typically includes a variety of computer readable media .Computer readable media can be any available media that can be accessed by computer 110 and includes both volatile and nonvolatile media , removable and non - removable media .By way of example , and not limitation , computer readable media may comprise computer storage media and communication media .","label":"CompareOrContrast","metadata":{},"score":"90.10039"}{"text":"Combinations of any of the above should also be included within the scope of computer readable media .The system memory 130 includes computer storage media in the form of volatile and/or nonvolatile memory such as read only memory ( ROM ) 131 and random access memory ( RAM ) 132 .","label":"CompareOrContrast","metadata":{},"score":"91.06259"}{"text":"FIG .8 .One example of annotated training data is that shown in .FIG .7A which contains the example sentence \" Schedule a meeting with John Smith on Saturday . \" for which a parse tree has been built ( as shown in .","label":"CompareOrContrast","metadata":{},"score":"91.52744"}{"text":"Computer 930 may operate in a networked environment using logical connections to one or more remote computers , such as a remote computer 982 .Remote computer 982 may include many or all of the elements and features described herein relative to computer 930 .","label":"CompareOrContrast","metadata":{},"score":"92.131195"}{"text":"Computer 930 may operate in a networked environment using logical connections to one or more remote computers , such as a remote computer 982 .Remote computer 982 may include many or all of the elements and features described herein relative to computer 930 .","label":"CompareOrContrast","metadata":{},"score":"92.131195"}{"text":"I drink black coffee .I love to drink hot black coffee .I drink , on most days of the week , coffee in the morning .However , because of the huge potential number of word combinations , this approach requires a hefty training corpus .","label":"CompareOrContrast","metadata":{},"score":"92.15193"}{"text":"I drink black coffee .I love to drink hot black coffee .I drink , on most days of the week , coffee in the morning .However , because of the huge potential number of word combinations , this approach requires a hefty training corpus .","label":"CompareOrContrast","metadata":{},"score":"92.15193"}{"text":"Sample sentences : ( These are a small subset of the types of comparative constructions in English . )The artichoke has more brains than the rock does .The limpet is more beautiful than any defoliant can be .She worked harder than Filbert thought was possible .","label":"CompareOrContrast","metadata":{},"score":"92.466255"}{"text":"RAM 940 typically contains data and/or program modules that are immediately accessible to and/or presently be operated on by processor 932 .Computer 930 may further include other removable / non - removable , volatile / non - volatile computer storage media .","label":"CompareOrContrast","metadata":{},"score":"92.65581"}{"text":"RAM 940 typically contains data and/or program modules that are immediately accessible to and/or presently be operated on by processor 932 .Computer 930 may further include other removable / non - removable , volatile / non - volatile computer storage media .","label":"CompareOrContrast","metadata":{},"score":"92.65581"}{"text":"The hard disk drive 944 , magnetic disk drive 946 , and optical disk drive 950 are each connected to bus 936 by one or more interfaces 954 .The drives and their associated computer - readable media provide nonvolatile storage of computer readable instructions , data structures , program modules , and other data for computer 930 .","label":"CompareOrContrast","metadata":{},"score":"93.03073"}{"text":"The hard disk drive 944 , magnetic disk drive 946 , and optical disk drive 950 are each connected to bus 936 by one or more interfaces 954 .The drives and their associated computer - readable media provide nonvolatile storage of computer readable instructions , data structures , program modules , and other data for computer 930 .","label":"CompareOrContrast","metadata":{},"score":"93.03073"}{"text":"The sparse data problem occurs when there is not enough data in a training corpus to distinguish between events that never occur versus events that are possible but just did n't happen to occur in the training corpus .If in the training corpus the pair of words \" gigantic \" and \" car \" never appears , it would be wrong to conclude that it is impossible in the English language to have the words \" gigantic \" and \" car \" together .","label":"CompareOrContrast","metadata":{},"score":"93.168945"}{"text":"The sparse data problem occurs when there is not enough data in a training corpus to distinguish between events that never occur versus events that are possible but just did n't happen to occur in the training corpus .If in the training corpus the pair of words \" gigantic \" and \" car \" never appears , it would be wrong to conclude that it is impossible in the English language to have the words \" gigantic \" and \" car \" together .","label":"CompareOrContrast","metadata":{},"score":"93.168945"}{"text":"\" VPwNPl specifies a subject to a verb phrase .For example , in \" John hit the ball \" [ NP John ] [ VP hit the ball ] where John is the subject .An APSG is similar to a CFG in that there are rules that look like CFG rules .","label":"CompareOrContrast","metadata":{},"score":"93.512085"}{"text":"For example , if the last word in a stream is \" the \" or \" a \" , then the next word is usually either an adjective or noun , but is rarely a verb .If the last word in a stream is \" clever \" , then the next word is likely to be an animate noun like \" boy \" or \" dog \" and not likely to be an inanimate noun like \" stone .","label":"CompareOrContrast","metadata":{},"score":"94.68866"}{"text":"For example , if the last word in a stream is \" the \" or \" a \" , then the next word is usually either an adjective or noun , but is rarely a verb .If the last word in a stream is \" clever \" , then the next word is likely to be an animate noun like \" boy \" or \" dog \" and not likely to be an inanimate noun like \" stone .","label":"CompareOrContrast","metadata":{},"score":"94.68866"}{"text":"By way of example , and not limitation , .FIG .1 illustrates remote application programs 185 as residing on remote computer 180 .It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used .","label":"CompareOrContrast","metadata":{},"score":"94.835846"}{"text":"\" In one set of grammar rules , the structure may be like this : .( The ( red ( toy ( with the loud siren ) ) ) ) .All prepositional phrases ( e.g. \" with the loud siren \" ) are attached to noun first ; adjectives are attached next , and finally the determiner ( \" the \" ) is added last .","label":"CompareOrContrast","metadata":{},"score":"95.77518"}{"text":"\" In one set of grammar rules , the structure may be like this : .( The ( red ( toy ( with the loud siren ) ) ) ) .All prepositional phrases ( e.g. \" with the loud siren \" ) are attached to noun first ; adjectives are attached next , and finally the determiner ( \" the \" ) is added last .","label":"CompareOrContrast","metadata":{},"score":"95.77518"}{"text":"This step in the process is commonly referred to as information extraction .Take as an example a user input sentence ( where the user says or types or handwrites ) \" Schedule a meeting with John Smith on Saturday \" .","label":"CompareOrContrast","metadata":{},"score":"95.80357"}{"text":"In Tree 90 of .VPwNPrl is used to add an object to a verb .For example , \" John hit the ball \" or \" They elected the pope . \"In Tree 92 of .VPwAVPr is used when an adverbial phrase modifies a verb .","label":"CompareOrContrast","metadata":{},"score":"95.97569"}{"text":"noun .np . )Prob . noun .wat . )Prob . np .noun . )Prob . noun . files . )Prob . vp . verb .np . )Prob . verb .like . )","label":"CompareOrContrast","metadata":{},"score":"96.00722"}{"text":"Smiled \" is rarely transitive and rarely takes a direct object .In other words , \" She smiled the ball \" is incorrect because someone can not \" smile \" a \" ball . \"Although , it is correct to say , \" She smiled the most \" because the \" most \" is not an object of \" smiled . \"","label":"CompareOrContrast","metadata":{},"score":"96.17027"}{"text":"T . k .P .t . k .W . k .T . k . w . k . )i .N . k .P .p .i . k .W . k .T . k . w . k . t . k .","label":"CompareOrContrast","metadata":{},"score":"96.401955"}{"text":"T . k . )W . k .T . k . )W . k .T . k . )P .W . k .T . k . )T . k .S . k .","label":"CompareOrContrast","metadata":{},"score":"97.13939"}{"text":"FIG .12 , is a specific implementation of a WAN via the Internet .Over the Internet , computer 930 typically includes a modem 978 or other means for establishing communications over the Internet 980 .Modem 978 , which may be internal or external , is connected to bus 936 via interface 970 .","label":"CompareOrContrast","metadata":{},"score":"97.59099"}{"text":"FIG .12 , is a specific implementation of a WAN via the Internet .Over the Internet , computer 930 typically includes a modem 978 or other means for establishing communications over the Internet 980 .Modem 978 , which may be internal or external , is connected to bus 936 via interface 970 .","label":"CompareOrContrast","metadata":{},"score":"97.59099"}{"text":"Smiled \" is rarely transitive and rarely takes a direct object .In other words , \" She smiled the ball \" is incorrect because someone can not \" smile \" a \" ball . \"Although , it is correct to say , \" She smiled the most \" because the \" most \" is not an object of is \" smiled . \"","label":"CompareOrContrast","metadata":{},"score":"97.66414"}{"text":"For each phenomenon , sample sentences are provided and a general description ( or examples ) of why such phenomenon affects a phrase or sentence .Passive .Sample sentences : .The cabbage was licked by the rabbit .The bomb must have been detonated from a distance .","label":"CompareOrContrast","metadata":{},"score":"98.18911"}{"text":"For each phenomenon , sample sentences are provided and a general description ( or examples ) of why such phenomenon affects a phrase or sentence .Passive .Sample sentences : .The cabbage was licked by the rabbit .The bomb must have been detonated from a distance .","label":"CompareOrContrast","metadata":{},"score":"98.18911"}{"text":"Intransitive verbs do not take an object .For example , \" John laughed , \" and \" Bill walked , \" .Mono - transitive verbs take a single direct object .For example , \" I hit the ball \" , .","label":"CompareOrContrast","metadata":{},"score":"98.3134"}{"text":"Intransitive verbs do not take an object .For example , \" John laughed , \" and \" Bill walked , \" .Mono - transitive verbs take a single direct object .For example , \" I hit the ball \" , .","label":"CompareOrContrast","metadata":{},"score":"98.3134"}{"text":"Suppose the kernel phrase is \" The boy ate the banana .\" If so , then : .Perfect : \" The boy had eaten the banana .\" Progressive : \" The boy was eating the banana .\" Passive : \" The banana was eaten by the boy .","label":"CompareOrContrast","metadata":{},"score":"98.3659"}{"text":"Suppose the kernel phrase is \" The boy ate the banana .\" If so , then : .Perfect : \" The boy had eaten the banana .\" Progressive : \" The boy was eating the banana .\" Passive : \" The banana was eaten by the boy .","label":"CompareOrContrast","metadata":{},"score":"98.3659"}{"text":"VPwNPr1 is used to add an object to a verb .For example , \" John hit the ball \" or \" They elected the pope . \"In Tree 92 of .VPwAVPr is used when an adverbial phrase modifies a verb .","label":"CompareOrContrast","metadata":{},"score":"99.28546"}{"text":"In other words , a sentence has more than one grammatically valid structure ( \" syntactic interpretation \" ) and as a result , may have more than one reasonable meaning ( \" semantic interpretation \" ) .A classic example of this is the sentence , \" time flies like an arrow .","label":"CompareOrContrast","metadata":{},"score":"99.36168"}{"text":"In other words , a sentence has more than one grammatically valid structure ( \" syntactic interpretation \" ) and as a result , may have more than one reasonable meaning ( \" semantic interpretation \" ) .A classic example of this is the sentence , \" time flies like an arrow .","label":"CompareOrContrast","metadata":{},"score":"99.36168"}{"text":"Comparative .Sample sentences : ( These are a small subset of the types of comparative constructions in English . )The artichoke has more brains than the rock does .The limpet is more beautiful than any defoliant can be .","label":"CompareOrContrast","metadata":{},"score":"99.56517"}{"text":"In the embodiment shown in .FIG .2 , the slots for frame 200 include an \" attendee \" slot 204 , a \" date \" slot 206 and a \" location \" slot 208 .The job of the information extraction component is to identify frame 202 , and fill in the appropriate slots from a natural language user input such as the input sentence \" Schedule a meeting with John Smith on Saturday . \" In accordance with one embodiment of the present invention , frames 200 for applications correspond to a two - level structure such as that shown in .","label":"CompareOrContrast","metadata":{},"score":"100.08861"}{"text":"It can be seen from .FIG .3 that the headwords that immediately precede the word \" on \" are \" schedule meeting \" .Thus , it can be appreciated that the probability of seeing the word \" on \" after the words \" schedule meeting \" is much greater than the probability of seeing the word \" on \" after the words \" John Smith \" .","label":"CompareOrContrast","metadata":{},"score":"100.21177"}{"text":"In addition to the monitor , computers may also include other peripheral output devices such as speakers 197 and printer 196 , which may be connected through an output peripheral interface 190 .The computer 110 may operate in a networked environment using logical connections to one or more remote computers , such as a remote computer 180 .","label":"CompareOrContrast","metadata":{},"score":"100.83305"}{"text":"When used in a WAN networking environment , the computer typically includes a modem 978 or other means for establishing communications over the WAN 979 .The modem 978 , which may be internal or external , may be connected to the system bus 936 via the user input interface 970 , or other appropriate mechanism .","label":"CompareOrContrast","metadata":{},"score":"101.04611"}{"text":"When used in a WAN networking environment , the computer typically includes a modem 978 or other means for establishing communications over the WAN 979 .The modem 978 , which may be internal or external , may be connected to the system bus 936 via the user input interface 970 , or other appropriate mechanism .","label":"CompareOrContrast","metadata":{},"score":"101.04611"}{"text":"In a verb phrase , it is the main verb .For example , in the noun phrase \" red book \" , the headword is \" book . \"Similarly , for the verb phrase \" going to the big store \" , the headword is \" going . \"","label":"CompareOrContrast","metadata":{},"score":"102.12492"}{"text":"In a verb phrase , it is the main verb .For example , in the noun phrase \" red book \" , the headword is \" book . \"Similarly , for the verb phrase \" going to the big store \" , the headword is \" going . \"","label":"CompareOrContrast","metadata":{},"score":"102.12492"}{"text":"Another set of grammar rules might structure it this way : .( ( The ( red toy ) ) ( with the loud siren ) ) .However , as long as a grammar clearly defines the structure of noun phrases , there exist constraints on the order of the rules .","label":"CompareOrContrast","metadata":{},"score":"102.26983"}{"text":"Another set of grammar rules might structure it this way : .( ( The ( red toy ) ) ( with the loud siren ) ) .However , as long as a grammar clearly defines the structure of noun phrases , there exist constraints on the order of the rules .","label":"CompareOrContrast","metadata":{},"score":"102.26983"}{"text":"Topicalization Of Verb Object .Sample sentences : .Graceland , I love to visit .This book I must read .Affect : .In both ' Graceland , I love to visit . ' and ' I love to visit Graceland . '","label":"CompareOrContrast","metadata":{},"score":"102.967636"}{"text":"Topicalization Of Verb Object .Sample sentences : .Graceland , I love to visit .This book I must read .Affect : .In both ' Graceland , I love to visit . ' and ' I love to visit Graceland . '","label":"CompareOrContrast","metadata":{},"score":"102.967636"}{"text":"A user may enter commands and information into the computer 110 through input devices such as a keyboard 162 , a microphone 163 , and a pointing device 161 , such as a mouse , trackball or touch pad .Other input devices ( not shown ) may include a joystick , game pad , satellite dish , scanner , or the like .","label":"CompareOrContrast","metadata":{},"score":"103.90362"}{"text":"For example , if the phrase is \" The red bear growled at me \" , the headword is \" growled , \" the modifying phrase is \" the red bear , \" and the modifying headword is \" bear .\" If the phrase is \" running to the store \" , then the headword is \" running \" , the is modifying phrase is \" to the store \" , and the modifying headword is \" to .","label":"CompareOrContrast","metadata":{},"score":"104.462875"}{"text":"For example , if the phrase is \" The red bear growled at me \" , the headword is \" growled , \" the modifying phrase is \" the red bear , \" and the modifying headword is \" bear .\" If the phrase is \" running to the store \" , then the headword is \" running \" , the modifying phrase is \" to the store \" , and the modifying headword is \" to .","label":"CompareOrContrast","metadata":{},"score":"104.484085"}{"text":"In English , the Passive marking is closest to the verb , followed by the Progressive , followed by the Perfect .You can not say : The banana been being had eaten by the boy .Negative Polarity .Sample sentences : .","label":"CompareOrContrast","metadata":{},"score":"104.60637"}{"text":"In English , the Passive marking is closest to the verb , followed by the Progressive , followed by the Perfect .You can not say : The banana been being had eaten by the boy .Negative Polarity .Sample sentences : .","label":"CompareOrContrast","metadata":{},"score":"104.60637"}{"text":"T . k .P .w . k . h .h .P .t . k . w . k .W . k .T . k .P .t . k . w . k . h .","label":"CompareOrContrast","metadata":{},"score":"104.82625"}{"text":"In general , syntactic bigrams requires a great deal of training data .To make the statistics gathering more tractable , words could be grouped into clusters with similar distributional properties .One can eat : bananas , apples , pears , peaches , plums , apricots .","label":"CompareOrContrast","metadata":{},"score":"105.999405"}{"text":"In general , syntactic bigrams requires a great deal of training data .To make the statistics gathering more tractable , words could be grouped into clusters with similar distributional properties .One can eat : bananas , apples , pears , peaches , plums , apricots .","label":"CompareOrContrast","metadata":{},"score":"105.999405"}{"text":"When used in a WAN networking environment , the computer 110 typically includes a modem 172 or other means for establishing communications over the WAN 173 , such as the Internet .The modem 172 , which may be internal or external , may be connected to the system bus 121 via the user input interface 160 , or other appropriate mechanism .","label":"CompareOrContrast","metadata":{},"score":"106.01394"}{"text":"For example , assume that an input sentence is the same as discussed above \" Schedule a meeting with John Smith on Saturday . \"Assume also that the last recognized word is \" Smith \" such that the next word to be recognized will be \" on \" .","label":"CompareOrContrast","metadata":{},"score":"106.8136"}{"text":"Propagated Syntactic History .Syntactic history can be propagated down many levels of the tree .Take , for example , the sample sentence , \" Graceland , I love to visit .\" The thing ( \" Graceland \" ) that \" I \" love to visit is stated before it is revealed the \" I \" loves to visit anything .","label":"CompareOrContrast","metadata":{},"score":"108.06243"}{"text":"Propagated Syntactic History .Syntactic history can be propagated down many levels of the tree .Take , for example , the sample sentence , \" Graceland , I love to visit .\" The thing ( \" Graceland \" ) that \" I \" love to visit is stated before it is revealed the \" I \" loves to visit anything .","label":"CompareOrContrast","metadata":{},"score":"108.06243"}{"text":"noun .np . )Prob . noun .wat . )Prob . np .noun . )Prob . noun .flies . )Prob . vp . verb .np . )Prob . verb .like . )","label":"CompareOrContrast","metadata":{},"score":"117.730194"}