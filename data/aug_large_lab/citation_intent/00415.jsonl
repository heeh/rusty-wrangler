{"text":"[ Sahami et al . , 2006 ] use the Web as a corpus to measure the semantic similarity between pairs of short text fragments ( search requests ) , thus gaining automatic requests expansion and offering alternative requests .For this purpose , they retrieve the contexts of the pairs of short texts from the content of the documents returned after searching , and they then compare the most frequent words from these documents .","label":"Background","metadata":{},"score":"18.59476"}
{"text":"Our approach is based on performing series of queries against a Web search engine and analyzing the returned excerpts of texts ( snippets ) in order to extract contextual semantic information which we use to measure the semantic similarity between pairs of words and thus to approximate synonymy .","label":"Background","metadata":{},"score":"24.656994"}
{"text":"This paper presents a constructioninspecific model of multiword expression decomposability based on latent semantic analysis .We use latent semantic analysis to determine the similarity between a multiword expression and its constituent words , and claim that higher similarities indicate great ... \" .","label":"Background","metadata":{},"score":"25.359676"}
{"text":"This paper presents a new approach for measuring semantic similarity / distance between words and concepts .It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data .","label":"Background","metadata":{},"score":"26.369038"}
{"text":"We show that this task can be done using bilingual parallel corpora , a much more commonly available resource .Using alignment techniques from phrasebased statistical machine translation , we show how paraphrases in one language can be identified using a phrase in another language as a pivot .","label":"Background","metadata":{},"score":"27.76282"}
{"text":"Two techniques are employed : ( 1 ) simple string edit distance , and ( 2 ) a heuristic strategy ... \" .We investigate unsupervised techniques for acquiring monolingual sentence - level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web - based news sources .","label":"Background","metadata":{},"score":"28.555182"}
{"text":"For simplicity , we have used the hierarchical clustering techniques : single- and complete - linkage , and we showed that the latter is a more suitable technique from our performance measurements ( i. e. recall and precision ) compared with manually grouping different contexts of similar meaning .","label":"Background","metadata":{},"score":"29.090363"}
{"text":"This framework allows us to investigate the key factors in similarity of asymmetry , the relative influence of different contexts and the extent to which words share a context ( Chapter 4 ) .Finally , we consider the application of distributional similarity in language modelling ( Chapter 5 ) and as a predictor of semantic similarity using human judgements of similarity and a spelling correction task ( Chapter 6 ) . .","label":"Background","metadata":{},"score":"29.613354"}
{"text":"This motivates us to use the Web as a source of local context information for measuring semantic similarity between pair of words .We will describe a method for extraction of local context from the Web ( web context ) , similar to the one described in [ Nakov et al . , 2007a].","label":"Background","metadata":{},"score":"29.992308"}
{"text":"When tested on a common data set of word pair similarity ratings , the proposed approach outperforms other computational models . es that rely exclusively on the corpus datathemselves .With the introduction of machine readable dictionaries , lexicons , thesauri , and taxonomies , these manually built pseudo - knowledge bases provide a natural framework for organising ... .","label":"Background","metadata":{},"score":"29.997295"}
{"text":"After a description of the experiment details , the techniques are discussed with particular ... \" .This paper describes an experiment that attempts to compare a range of existing collocation extraction techniques as well as the implementation of a new technique based on tests for lexical substitutability .","label":"Background","metadata":{},"score":"30.395647"}
{"text":"In its essence , our method is also based on context retrieval and comparison , but we use the Web as a corpus for measuring semantic similarity and in this way we do not depend on other linguistic resources ( e.g. , large text corpora ) .","label":"Background","metadata":{},"score":"30.521923"}
{"text":"We report initial results over the recently released Discourse GraphBank ( Wolf and Gibson , 2005 ) .Our approach considers , and determines the contributions of , a variety of syntactic and lexico - semanti ... \" .In this paper we consider the problem of identifying and classifying discourse coherence relations .","label":"Background","metadata":{},"score":"30.568493"}
{"text":"Our algorithm is based on an extended version of Harris ' Distributional Hypothesis , which states that words that occurred in the same contexts tend to be similar .Instead of using this hypothesis on words , we apply it to paths in the dependency trees of a parsed corpus .","label":"Background","metadata":{},"score":"31.280159"}
{"text":"We also present an evaluation methodology for automatically measuring the precision and recall of discovered senses .Categories and Subject Descriptors H.3.3 [ Information Storage and Retrieval ] : Information Search and Retrieval --- Clustering . ...e same contexts tend to be similar .","label":"Background","metadata":{},"score":"31.822515"}
{"text":"The problem persists in our work as well .As a context for a given word in the first language , the set of all its probable trans- lations in the other language are used .Then the semantic similarity between the two words is measured as a smilarity between their contexts .","label":"Background","metadata":{},"score":"32.183243"}
{"text":"We start by introducing the concept of lexical distributional similarity .We discuss potential applications , which can be roughly divided into distributional or language modelling applications and semantic applications , and methods of evaluation ( Chapter 2 ) .We look at existing measures of distributional similarity and carry out an empirical comparison of fifteen of these measures , paying particular attention to the effects of word frequency ( Chapter 3 ) .","label":"Background","metadata":{},"score":"32.201923"}
{"text":"We start by introducing the concept of lexical distributional similarity .We discuss potential applications , which can be roughly divided into distributional or language modelling applications and semantic applications , and methods of evaluation ( Chapter 2 ) .We look at existing measures of distributional similarity and carry out an empirical comparison of fifteen of these measures , paying particular attention to the effects of word frequency ( Chapter 3 ) .","label":"Background","metadata":{},"score":"32.201923"}
{"text":"Given a pair of words , we extract their local contexts from the snippets returned by the search engine and we measure the semantic similarity between these words by calculating the similarity between their local contexts .Finally , the measured similarity is used to determine whether the words are likely to be synonyms or not .","label":"Background","metadata":{},"score":"32.783794"}
{"text":"OF THE 41ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS , 2003 . \" ...Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data .We describe a new approach which involves clustering subcategorization frame ( SCF ) distributions using the Information Bottleneck and nearest neighbour methods .","label":"Background","metadata":{},"score":"33.053413"}
{"text":"We identify a set of lexico - syntactic patterns that are ... \" .We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text .Two goals motivate the approach : ( i ) avoidante of the need for pre - encoded knowledge and ( ii ) applicability across a wide range of text .","label":"Background","metadata":{},"score":"33.31917"}
{"text":"Applying the semi - supervised clustering algorithm to the task of discovering verb classes , we obtain precision scores of 54 % and 37 % and corresponding recall scores of 53 % and 38 % for our two test sets .These are large improvements over the baseline . ; We next investigate the task of learning surface paraphrases , i.e. , paraphrases that do not require the use of a syntactic interpretation .","label":"Background","metadata":{},"score":"33.3585"}
{"text":"Applying the semi - supervised clustering algorithm to the task of discovering verb classes , we obtain precision scores of 54 % and 37 % and corresponding recall scores of 53 % and 38 % for our two test sets .These are large improvements over the baseline . ; We next investigate the task of learning surface paraphrases , i.e. , paraphrases that do not require the use of a syntactic interpretation .","label":"Background","metadata":{},"score":"33.3585"}
{"text":"Our method is a natural ex - tension of those proposed in ( Brown et al . , 1992 ) and ( Li and Abe , 1996 ) , and overcomes their draw - backs while retaining their advantages .We then coinbined this clustering method with the disam - I)iguation method of ( Li and Abe , 1995 ) to derive a disambiguation method that makes use of both auto - matically constructed thesauruses and a hand - made thesaurus .","label":"Background","metadata":{},"score":"33.452293"}
{"text":"In this paper , we present an unsupervised algorithm for discovering inference rules from text .Our algorithm is based on an extended version of Harris ' Distributional Hypothesis , which states that words that occurred in the same contexts tend to be similar .","label":"Background","metadata":{},"score":"33.48188"}
{"text":"The disadvantage of this approach is that it requires a big parallel corpus , which can be unavailable .It will also not work for uncommon words , which are almost not met in the corpus .[ Hagiwara et al . , 2007 ] propose to measure semantic similarity using local contexts extended with indirect retrieval of additional contextual words .","label":"Background","metadata":{},"score":"33.565315"}
{"text":"LRA extends the VSM approach in three ways : ( 1 ) patterns are derived automatically from the corpus , ( 2 ) Singular Value Decomposition is used to smooth the frequency data , and ( 3 ) synonyms are used to reformulate word pairs .","label":"Background","metadata":{},"score":"33.691277"}
{"text":"The idea of retrieving information from text snippets returned by a Web search engine is used in [ Chen et al . , 2006].In this approach , context words are completely ignored ( except for X and Y ) and their semantics are not used .","label":"Background","metadata":{},"score":"34.09201"}
{"text":"There are many formaliza ... . \" ...This paper investigates the use of clustering techniques in word - sense classification , which identifies different contexts that a word was used with the same or similar sense .For simplicity , we have used the hierarchical clustering techniques : single- and complete - linkage , and we showed that the la ... \" .","label":"Background","metadata":{},"score":"34.13414"}
{"text":"This paper presents a tree - to - tree transduction method for sentence compression .Our model is based on synchronous tree substitution grammar , a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches .","label":"Background","metadata":{},"score":"34.175526"}
{"text":"We use latent semantic analysis to determine the similarity between a multiword expression and its constituent words , and claim that higher similarities indicate greater decomposability .We test the model over English noun - noun compounds and verb - particles , and evaluate its correlation with similarities and hyponymy values in WordNet .","label":"Background","metadata":{},"score":"34.19937"}
{"text":"In this model , a connectivity matrix based on intra - sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences .Our system , based on LexRank ranked in rst place in more than one task in the recent DUC 2004 evaluation .","label":"Background","metadata":{},"score":"34.595493"}
{"text":"They can be used to extract lexical and syntactic paraphrase pairs and to generate new , unseen sentences that express the same meaning as the sentences in the input sets .Our FSAs can also predict the correctness of alternative semantic renderings , which may be used to evaluate the quality of translations . by Jeff Mitchell , Mirella Lapata - In Proceedings of ACL-08 : HLT , 2008 . \" ...","label":"Background","metadata":{},"score":"34.63156"}
{"text":"The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency . by Dragomir R. Radev - Journal of Artificial Intelligence Research ( JAIR , 2004 . \" ...We introduce a stochastic graph - based method for computing relative importance of textual units for Natural Language Processing .","label":"Background","metadata":{},"score":"34.84092"}
{"text":"In the present paper , we set the objective to design an algorithm for automatic extraction of pairs of synonyms from a text corpus .The results can be used to create linguistic resources , such as general and domain - specific thesauri and lexicons .","label":"Background","metadata":{},"score":"34.87063"}
{"text":"Under this framework , we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task .Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments . .","label":"Background","metadata":{},"score":"34.872894"}
{"text":"However , there are relatively few work reported in grouping concordances with similar meaning , particularly for Chinese .Yarowsky [ 5 ] was advocating one sense per collocation ( or concordance ) so tha ... . \" ...We propose an information - theoretic clustering approach that incorporates a pre - known partition of the data , aiming to identify common clusters that cut across the given partition .","label":"Background","metadata":{},"score":"35.082245"}
{"text":"The proposed approach yields improvements over state - of - the - art systems on a benchmark dataset . ... amount of context for each instance ( usually a sentence or two surrounding the sentence containing the target word ) .For instances containing more than one occurrence of the target word , we disambiguate the first occurrence .","label":"Background","metadata":{},"score":"35.587837"}
{"text":"The differences between manually identified groups of different contexts are measured in terms of recall and precision at about 80 % , which are not very different from the average recall and precision performance of complete - linkage clustering at 80 % and 75 % , respectively .","label":"Background","metadata":{},"score":"35.65212"}
{"text":"This framework allows us to investigate the key factors in similarity of asymmetry , the relative influence of different contexts and the extent to which words share a context ( Chapter 4 ) .Finally , we consider the application of distributional similarity in language modelling ( Chapter 5 ) and as a predictor of semantic similarity using human judgements of similarity and a spelling correction task ( Chapter 6 ) . by Ben Wellner , James Pustejovsky , Catherine Havasi , Anna Rumshisky , Roser Saur√≠ - In Proceedings of 7th SIGDIAL Workshop on Discourse and Dialogue , 2006 . \" ...","label":"Background","metadata":{},"score":"35.775837"}
{"text":".. lexicon , although it is a common locution .Semantic Relatedness Information .These approaches attempt to infer relationships among lexical terms by looking at very large text samples and determining which ones ar ... . by Dekang Lin - In Proceedings of the 15th International Conference on Machine Learning , 1998 . \" ...","label":"Background","metadata":{},"score":"35.86852"}
{"text":"[ 7 ] Harris , Z. ( 1954 ) . \"Distributional structure \" .Word , 10 , pages 146 - 162 .[ 8 ] Lin D. ( 1998 ) . \"Automatic Retrieval and Clustering of Similar Words \" .","label":"Background","metadata":{},"score":"35.89042"}
{"text":"Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase .The summary sentences , while less readily alignable , retain more of the non - trivial alternations that are of greatest interest learning paraphrase relationships . ... from thousands of news sources over an extended period .","label":"Background","metadata":{},"score":"35.967438"}
{"text":"In the language modeling task , a similarity - based model is used to improve probability estimates for unseen bigrams in a back - off language model .The similaritybased method yields a 20 % perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech - recognition error .","label":"Background","metadata":{},"score":"36.23855"}
{"text":"Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data .We describe a new approach which involves clustering subcategorization frame ( SCF ) distributions using the Information Bottleneck and nearest neighbour methods .In contrast to previous work , we particularly focus on clustering polysemic verbs .","label":"Background","metadata":{},"score":"36.294106"}
{"text":"We address the problem of clustering words ( or con - structing a thesaurus ) based on co - occurrence data , and using the acquired word classes to improve the accuracy of syntactic disambiguation .We view this problem as that of estimating a joint probability dis - tribution specifying the joint probabilities of word pairs , such as noun verb pairs .","label":"Background","metadata":{},"score":"36.407486"}
{"text":"\" Introduction to the Special Issue on the Web as Corpus \" , Computational Linguistics , 29(3):333 - 347 .[14 ] Inkpen D. ( 2007 ) .\" Near - synonym Choice in an Intelligent Thesaurus \" .In Proceedings of the NAACL - HLT , New York , USA .","label":"Background","metadata":{},"score":"36.81126"}
{"text":"retrieval of information from text snippets returned by the search engine .They automa- tically discover lexico - syntactic templates for semantically related and unrelated words using WordNet , and they train a support vector machine ( SVM ) classifier .The learned templates are used for extracting information from the text fragments returned by the search engine .","label":"Background","metadata":{},"score":"36.825375"}
{"text":"The similaritybased methods perform up to 40 % better on this particular task . \" ...We address the problem of clustering words ( or con - structing a thesaurus ) based on co - occurrence data , and using the acquired word classes to improve the accuracy of syntactic disambiguation .","label":"Background","metadata":{},"score":"36.86078"}
{"text":"We introduce a stochastic graph - based method for computing relative importance of textual units for Natural Language Processing .We test the technique on the problem of Text Summarization ( TS ) .Extractive TS relies on the concept of sentence salience to identify the most important sentences in a doc ... \" .","label":"Background","metadata":{},"score":"36.86705"}
{"text":"The algorithm measures the semantic similarity between pairs of words by comparing their local contexts extracted from the Web by series of queries against the Google search engine .The results show 11pt average precision of 63.16 % .Keywords Automatic synonym acquisition , semantic similarity measure , Web as a corpus , Web mining .","label":"Background","metadata":{},"score":"37.03889"}
{"text":"We describe a method for discovering these patterns and suggest that other lexical relations will also he acquirable iu this way .A subset of the acquisitiou algorithm is implemented and the results are used to augment and critique the structure of a large hand - built thesaurus .","label":"Background","metadata":{},"score":"37.188873"}
{"text":"This thesis is concerned with the measurement and application of lexical distributional similarity .Two words are said to be distributionally similar if they appear in similar contexts .This loose definition , however , has led to many measures being proposed or adopted from fields such as geometry , statistics , Information Retrieval ( IR ) and Information Theory .","label":"Background","metadata":{},"score":"37.212067"}
{"text":"We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments , and contrast the quality with paraphrases extracted from automatic alignments . ... nce that an answer is correct ( Ibrahim et al . , 2003 ) .","label":"Background","metadata":{},"score":"37.241096"}
{"text":"Our focus is on methods that seek to address the new challenges raised by sentiment - aware applications , as compared to those that are already present in more traditional fact - based analysis .We include materialon summarization of evaluative text and on broader issues regarding privacy , manipulation , and economic impact that the development of opinion - oriented information - access services gives rise to .","label":"Background","metadata":{},"score":"37.614815"}
{"text":"However , the nature of language is such that many word combinations are infrequent and do not occur in any given corpus .In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on \" most similar \" words .","label":"Background","metadata":{},"score":"37.91834"}
{"text":"These systems rely on lexical features like unigrams , bigrams , and co - occurrences .Tools . \" ...An important part of our information - gathering behavior has always been to find out what other people think .With the growing availability and popularity of opinion - rich resources such as online review sites and personal blogs , new opportunities and challenges arise as people now can , and do , active ... \" .","label":"Background","metadata":{},"score":"38.122063"}
{"text":"This thesis is concerned with the measurement and application of lexical distributional similarity .Two words are said to be distributionally similar if they appear in similar contexts .This loose definition , however , has led to many measures being proposed or adopted from fields such as geometry , s ... \" .","label":"Background","metadata":{},"score":"38.14644"}
{"text":"\" Finding Synonyms Using Automatic Word Alignment and Measures of Distributional Similarity \" .In Proceedings of COLING / ACL 2006 , Sydney , Australia .[11 ] Och F. , Ney H. ( 2003 ) .\"A Systematic Comparison of Various Statistical Alignment Models \" .","label":"Background","metadata":{},"score":"38.14704"}
{"text":"However , they often include many rare senses while missing corpus / domain - specific senses .We present a clustering algorithm called CBC ( Clustering By Committee ) that automatically discovers word senses from text .It initially discovers a set of tight clusters called committees that are well scattered in the similarity space .","label":"Background","metadata":{},"score":"38.21586"}
{"text":"The similarity measure allows us to construct a thesaurus using a parsed corpus .We then present a new evaluation methodology for the automatically constructed th ... \" .greatest challenges in natural language learning .We first define a word similarity measure based on the distributional pattern of words .","label":"Background","metadata":{},"score":"38.33612"}
{"text":"Annual Meeting of the Association for Computational Linguistics , 1993 . \" ...We describe and evaluate experimentally a method for clustering words according to their dis- tribution in particular syntactic contexts .Words are represented by the relative frequency distributions of contexts in which they appear , and relative entropy between those distributions is used as the si ... \" .","label":"Background","metadata":{},"score":"38.52302"}
{"text":"Our hypothesis is that synonyms should have higher level of semantic similarity compared to nonsynonyms .The results of our experiments show that this expectation is true in most cases .In this paper , we show that it is possible ( with a minimum human intervention ) to extract automatically all pairs of synonyms from a list of terms built from a terminological text .","label":"Background","metadata":{},"score":"38.56076"}
{"text":"I. . ... similarity measures to cluster related tags to optimize the computations .Begelman et al .[ 25 ] present an algorithm for the automated clustering of tags on the basis of tag co - occurrences in order to facilitate more ... . by Oren Glickman , Rosie Jones - In AAAI 1999 Workshop on Machine Learning for Information Extraction , 1999 . \" ...","label":"Background","metadata":{},"score":"38.644882"}
{"text":"Inventories of manually compiled dictionaries usually serve as a source for word senses .However , they often include many rare senses while missing corpus / domain - specific senses .We present a clustering algorithm called CBC ( Clustering By Committee ) that automatically discovers word senses from text ... \" .","label":"Background","metadata":{},"score":"38.69928"}
{"text":"We investigate how neural network embeddings perform on this task , compared to dependency - based vector space models , and evaluate a range of similarity measures on hyponym gener - ation .A new asymmetric similarity mea - sure and a combination approach are de - scribed , both of which significantly im - prove precision .","label":"Background","metadata":{},"score":"38.76616"}
{"text":"The use of crowdsourced annotations from the Web gives rise to trust issues .We propose an algorithm that , by making use of a combination of subjective logic , semantic relatedness measures and clustering , automates the process of evaluation for annotations represented by means of the Open Annotation ontology .","label":"Background","metadata":{},"score":"39.070004"}
{"text":"They can be used to extract lexical and syntactic paraphrase pairs and to generate new , unseen sente ... \" .We describe a syntax - based algorithm that automatically builds Finite State Automata ( word lattices ) from semantically equivalent translation sets .","label":"Background","metadata":{},"score":"39.125984"}
{"text":"A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning .Sense induction is thus typically viewed as an unsupervised clustering problem where the aim i ... \" .Sense induction seeks to automatically identify word senses directly from a corpus .","label":"Background","metadata":{},"score":"39.423042"}
{"text":"Examples of monolingual parallel corpora that have been used are multiple tra ... . by Bo Pang , Kevin Knight , Daniel Marcu - In Proceedings of HLT / NAACL , 2003 . \" ...We describe a syntax - based algorithm that automatically builds Finite State Automata ( word lattices ) from semantically equivalent translation sets .","label":"Background","metadata":{},"score":"39.72276"}
{"text":"We proceed by assigning words to their most similar clusters .After assigning an element to a cluster , we remove their overlapping features from the element .This allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses .","label":"Background","metadata":{},"score":"39.755352"}
{"text":"Recently , robust graphbased methods for NLP have also been gaining a lot of interest , e.g. , in word clustering ( Bre ...Scientific article : We present an original algorithm for automatic acquisition of synonyms from text .The algorithm measures the semantic similarity between pairs of words by comparing their local contexts extracted from the Web by series of queries against the Google search engine .","label":"Background","metadata":{},"score":"39.75541"}
{"text":"As a result , we obtained a list of 4,371 word pairs ordered in descending order by their similarity .We measure the accuracy by precision and recall , which come from information retrieval .We use this as a base for comparison with the other algorithms .","label":"Background","metadata":{},"score":"39.916306"}
{"text":"Previous work has used monolingual parallel corpora to extract and generate paraphrases .We show that this task can be done using bilingual parallel corpora , a much more commonly available resource .Using alignment techniques from phrasebased statistical machine translation , we show how paraphrases ... \" .","label":"Background","metadata":{},"score":"40.032433"}
{"text":"The system is trained on large volumes of sentence pairs automatically extracted from clustered news articles available on the World Wide Web .Alignment Error Rate ( AER ) is mea ... \" .We apply statistical machine translation ( SMT ) tools to generate novel paraphrases of input sentences in the same language .","label":"Background","metadata":{},"score":"40.14103"}
{"text":"The method is more complicated than the one we propose and requires extra resources for training the SVM .An interesting approach for finding synonyms and lexicalizations from the Web is described in [ Sanchez et al . , 2005].","label":"Background","metadata":{},"score":"40.443172"}
{"text":"Our contributions are three - fold : an empirical comparison of a broad range of measures ; a classification of similarity functions based on the information that they incorporate ; a ... \" .We study distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences .","label":"Background","metadata":{},"score":"40.61766"}
{"text":"The words used to find synonyms come as a list .It is possible to process all words in the text or some subset of them .For example , in order to avoid unnecessary computati- ons , we can use grammatical glossary to filter out words belonging to different parts of speech .","label":"Background","metadata":{},"score":"40.71289"}
{"text":"This causes significant errors during synonyms extraction and can be seen from the obtained results .Fixing this problem would be the most important challenge in our future work .Related Work Most of the automatic synonym extraction methods are based on distributional hypothe- sis , that semantically related words appear in close contexts [ Harris , 1954].","label":"Background","metadata":{},"score":"40.76539"}
{"text":"The first method is based on longest common subsequence between a candidate translation and a set of reference translations .Longest common subsequence takes into account sentence level structure simila ... \" .In this paper we describe two new objective automatic evaluation methods for machine translation .","label":"Background","metadata":{},"score":"40.82647"}
{"text":"Our approach considers , and determines the contributions of , a variety of syntactic and lexico - semantic features .We achieve 81 % accuracy on the task of discourse relation type classification and 70 % accuracy on relation identification . ...","label":"Background","metadata":{},"score":"40.86621"}
{"text":"They then search the Web for the longest multiword terms extracted from the taxonomy after removing the target keyword and assume that synonyms should be found on the same position where the original keyword was .The approach is quite original , but addresses a different problem : find possible synonyms for a given word .","label":"Background","metadata":{},"score":"40.965565"}
{"text":"Grammatical relations derived from a single top - ranked tree for each sentence ( headword , modifier , and relation type ) were used for feature construction .4.2 Modal Parsing and Temporal Ordering of ... . by Anna Korhonen , Ted Briscoe - IN PROCEEDINGS OF THE HLT / NAACL WORKSHOP ON COMPUTATIONAL LEXICAL SEMANTICS , 2004 . \" ...","label":"Background","metadata":{},"score":"40.9896"}
{"text":"Essentially , if two paths tend to link the same set of words , we hypothesize that their meanings are similar .We use examples to show that our system discovers many inference rules easily missed by humans . ...r such rules .","label":"Background","metadata":{},"score":"41.231335"}
{"text":"The n ... \" .We propose an information - theoretic clustering approach that incorporates a pre - known partition of the data , aiming to identify common clusters that cut across the given partition .In the standard clustering setting the formation of clusters is guided by a single source of feature information .","label":"Background","metadata":{},"score":"41.249992"}
{"text":"Our algorithm works by generalizing the syntactic paths between corresponding anchors in aligned sentence pairs .Compared to previous work , structural paraphrases generated by our algorithm tend to be much longer on average , and are capable of capturing long - distance dependencies .","label":"Background","metadata":{},"score":"41.551487"}
{"text":"In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence ( IJCAI-03 ) , pages 805 - 810 , Acapulco , Mexico , 2003 .[ Barzilay and McKeown , 2001 ] R. Barzilay and K. McKe - own .Extracting paraphrases from a parallel corpus .","label":"Background","metadata":{},"score":"41.69534"}
{"text":"We test the technique on the problem of Text Summarization ( TS ) .Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents .Salience is typically dened in terms of the presence of particular important words or in terms of similarity to a centroid pseudo - sentence .","label":"Background","metadata":{},"score":"42.058033"}
{"text":"The errorrecognition system , ALEK , performs with about 80 % precision and 20 % recall . \" ...This paper describes the use of WordNet in a new technique for collocation extraction .The approach is based on restrictions on the possible substitutions for synonyms within candidate phrases .","label":"Background","metadata":{},"score":"42.06282"}
{"text":"We present an informationtheoretic definition of similarity that is applicable as long as there is a probabilistic model .We demonstrate how our definition can be used to measure the similarity in a number of different domains . \" ... greatest challenges in natural language learning .","label":"Background","metadata":{},"score":"42.196247"}
{"text":"Sentence compression is the task of producing a summary at the sentence level .This paper focuses on three aspects of this task which have not received detailed treatment in the literature : training requirements , scalability , and automatic evaluation .","label":"Background","metadata":{},"score":"42.416138"}
{"text":"Despite the persistence of this theory , however , there is widespread agreement about its empirical shortcomings ( McCawley , 1968 ; Fodor , 1977 ) .As an alternative , some critics of the Katz - Fodor theory ( e.g. ( Johnson - Laird , 1983 ) ) have abandoned the treatment of selectional constraints as semantic , instead treating them as indistinguishable from inferences made on the basis of factual knowledge .","label":"Background","metadata":{},"score":"42.453056"}
{"text":"Evaluating text categorization .In Proceedings of the Speech and Natural Language Workshop , Asilomar , pages 312 - 318 , 1991 .[Lin , 1998 ] D. Lin .Automatic retrieval and clustering of similar words .In Proceedings of the 36th Annual Meet - ing of the Association for Computational Linguistics and the 17th International Conference on Computational Lin - guistics ( COLING - ACL ' 98 ) , pages 768 - 774 , Montreal , Canada , 1998 .","label":"Background","metadata":{},"score":"42.549713"}
{"text":"LRA achieves state - of - the - art results , reaching human - level performance on the analogy questions and significantly exceeding VSM performance on both tasks .Keywords : . analogies , semantic relations , vector space model , noun - modifier expressions , latent relational analysis .","label":"Background","metadata":{},"score":"42.57769"}
{"text":"This is followed by a discussion on the relative strengths and weaknesses of the techniques with reference to the results obtained .Since there is no general agreement on the exact nature of collocation , evaluating techniques with reference to any single standard is somewhat controversial .","label":"Background","metadata":{},"score":"42.642902"}
{"text":"Words are represented by the relative frequency distributions of contexts in which they appear , and relative entropy between those distributions is used as the similarity measure for clustering .Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership .","label":"Background","metadata":{},"score":"42.75482"}
{"text":"This paper describes the use of WordNet in a new technique for collocation extraction .The approach is based on restrictions on the possible substitutions for synonyms within candidate phrases .Following a general discussion of collocations and their applications , current extraction methods are briefly described .","label":"Background","metadata":{},"score":"42.767784"}
{"text":"We report first on a framework for implementing and evaluating such models .We then go on to report on the implementation of some techniques for using statistical models acquired from corpus data to infer the meaning of verb - particle constructions . by Martin Chodorow , Claudia Leacock - In Proceedings of NAACL'00 , 2000 . \" ...","label":"Background","metadata":{},"score":"42.79807"}
{"text":"Semantic Similarity Measured through Reverse Context When we extract the Web context for a given word , often semantically unrelated words fall in .Removing such words from the context is expected to improve accuracy when measuring semantic similarity with Web contexts [ Nakov et al . , 2007b].","label":"Background","metadata":{},"score":"42.831593"}
{"text":"Measuring Semantic Similarity by Latent Relational Analysis .Abstract .This paper introduces Latent Relational Analysis ( LRA ) , a method for measuring semantic similarity .LRA measures similarity in the semantic relations between two pairs of words .When two pairs have a high degree of relational similarity , they are analogous .","label":"Background","metadata":{},"score":"43.20692"}
{"text":"We show that these paraphrases can be used to learn surface patterns for relation extraction .The extraction patterns obtained by using the paraphrases are not only more precise ( more than 80 % precision for both our test relations ) , but also have higher relative recall compared to a state - of - the - art baseline .","label":"Background","metadata":{},"score":"43.243725"}
{"text":"We show that these paraphrases can be used to learn surface patterns for relation extraction .The extraction patterns obtained by using the paraphrases are not only more precise ( more than 80 % precision for both our test relations ) , but also have higher relative recall compared to a state - of - the - art baseline .","label":"Background","metadata":{},"score":"43.243725"}
{"text":"Participants are usually asked to rate the paraphrase pairs using a nominal scale ( e.g. , definitely similar , sometimes similar , never similar ) .In our experiments , w .. by Chris Quirk , Chris Brockett , William Dolan - In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing , 2004 . \" ...","label":"Background","metadata":{},"score":"43.310265"}
{"text":"Experiments and Results The experiments we preformed focus on studying and analyzing our algorithms for measuring semantic similarity extracted from the Web and their usage for the automatic discovery of synonyms .We performed experiments without and with using the reverse context and TF.IDF weighting and with various thresholds for the minimal frequency of the words in the context .","label":"Background","metadata":{},"score":"43.453773"}
{"text":"We start with extracting a list of all terms from the text that are interesting from a linguist 's point of view .We can also use a subset of them , e.g. nouns only , or all words in the text .","label":"Background","metadata":{},"score":"43.6652"}
{"text":"Algorithms for finding similar words assume the Distributional Hypothesis , which states that words that occurred in the same contexts tend to have similar meanings ( Harris ... . \" ...Selectional constraints are limitations on the applicability of predicates to arguments .","label":"Background","metadata":{},"score":"43.686226"}
{"text":"To our knowledge the first one ever .We then present a method for automatically learning the contexts in which quasi - paraphrases obtained from a corpus are mutually replaceable .For this purpose , we use Relational Selectional Preferences ( RSPs ) that specify the selectional preferences of the syntactic arguments of phrases ( usually verbs or verb phrases ) .","label":"Background","metadata":{},"score":"43.844505"}
{"text":"To our knowledge the first one ever .We then present a method for automatically learning the contexts in which quasi - paraphrases obtained from a corpus are mutually replaceable .For this purpose , we use Relational Selectional Preferences ( RSPs ) that specify the selectional preferences of the syntactic arguments of phrases ( usually verbs or verb phrases ) .","label":"Background","metadata":{},"score":"43.844505"}
{"text":"The lexicon is provided freely for research use , along with a script which can be used to filter and build sub - lexicons suited for different natural language processing ( NLP ) purposes .Documentation is also provided which explains each sub - lexicon option and evaluates its accuracy . \" ...","label":"Background","metadata":{},"score":"44.076508"}
{"text":"International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics . \" ...Sentence compression is the task of producing a summary at the sentence level .This paper focuses on three aspects of this task which have not received detailed treatment in the literature : training requirements , scalability , and automatic evaluation .","label":"Background","metadata":{},"score":"44.094845"}
{"text":"The largest and the most widely deployed classification in English is Levin 's ( 1993 ) taxonomy of verbs and their classes .While this resource is attractive in being exte ... \" .Lexical - semantic verb classifications have proved useful in supporting various natural language processing ( NLP ) tasks .","label":"Background","metadata":{},"score":"44.36734"}
{"text":"Therefore , extracting synonyms by measuring semantic simila- rity only is not possible without human intervention , but our experiments show that this intervention could be minimal .Semantic Similarity Measured by Contexts The algorithm for measuring semantic similarity between two words is based on analysis of the local context in which these words appear and follows the idea that words appearing in similar context should have similar meanings .","label":"Background","metadata":{},"score":"44.650414"}
{"text":"We then apply the learned ISPs to the task of filtering incorrect inferences .Learning directionality allows us to differentiate the strong ( bidirectional ) from the weak ( unidirectional ) paraphrases .We show that the directionality of the quasi - paraphrases can be learned with 48 % accuracy .","label":"Background","metadata":{},"score":"44.740868"}
{"text":"We then apply the learned ISPs to the task of filtering incorrect inferences .Learning directionality allows us to differentiate the strong ( bidirectional ) from the weak ( unidirectional ) paraphrases .We show that the directionality of the quasi - paraphrases can be learned with 48 % accuracy .","label":"Background","metadata":{},"score":"44.740868"}
{"text":"Our hypothesis is that synonyms , being words with very similar meanings , should have higher semantic similarity than pairs of nonsynonyms .Since our semantic similarity measures to what extent two words have a similar mea- nings , it is possible to get inaccurate results for some pairs of words and incorrectly to classify them as synonyms .","label":"Background","metadata":{},"score":"44.77758"}
{"text":"Extractive TS relies on the concept of sentence salience to identify the most important sentences in a doc ... \" .We introduce a stochastic graph - based method for computing relative importance of textual units for Natural Language Processing .We test the technique on the problem of Text Summarization ( TS ) .","label":"Background","metadata":{},"score":"44.859047"}
{"text":"Previous definitions of similarity are tied to a particular application or a form of knowledge representation .We present an informationtheoretic definition of similarity that is applicable as long as there is a probabilistic model .We demonstrate ... \" .Similarity is an important and widely used concept .","label":"Background","metadata":{},"score":"44.861053"}
{"text":"Capturing the variability of language , they play an important role in many natural language applications including question answering , machine translation , and multi - document summarization .In linguistics , paraphrases are characterized by approximate conceptual equivalence .Since no automated semantic interpretation systems available today can identify conceptual equivalence , paraphrases are difficult to acquire without human effort .","label":"Background","metadata":{},"score":"44.979706"}
{"text":"Capturing the variability of language , they play an important role in many natural language applications including question answering , machine translation , and multi - document summarization .In linguistics , paraphrases are characterized by approximate conceptual equivalence .Since no automated semantic interpretation systems available today can identify conceptual equivalence , paraphrases are difficult to acquire without human effort .","label":"Background","metadata":{},"score":"44.979706"}
{"text":"There is evidence from cognitive science that relational similarity is fundamental to many cognitive and linguistic tasks ( e.g. , analogical reasoning ) .In the Vector Space Model ( VSM ) approach to measuring relational similarity , the similarity between two pairs is calculated by the cosine of the angle between the vectors that represent the two pairs .","label":"Background","metadata":{},"score":"45.007523"}
{"text":"We also show how the parser can be used to parse questions for Question Answering .The accuracy of the original parser on questions is very poor , and we propose a novel technique for porting the parser to a new domain , by creating new labelled data at the lexical category level only .","label":"Background","metadata":{},"score":"45.28578"}
{"text":"Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word 's contexts into different classes , each representing a word sense .Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words .","label":"Background","metadata":{},"score":"45.38057"}
{"text":"Two words are said to be distributionally similar if they appear in similar contexts .This loose definition , however , has led to many measures being proposed or adopted from fields such as geometry , statistics , Information Retrieval ( IR ) and Information Theory .","label":"Background","metadata":{},"score":"45.44884"}
{"text":"We rely only on distributional similarity to learn paraphrases from this corpus .To scale paraphrase acquisition to this large corpus , we apply only simple POS tagging and randomized algorithms .We build a paraphrase resource containing more than 2.5 million phrases .","label":"Background","metadata":{},"score":"45.842697"}
{"text":"We rely only on distributional similarity to learn paraphrases from this corpus .To scale paraphrase acquisition to this large corpus , we apply only simple POS tagging and randomized algorithms .We build a paraphrase resource containing more than 2.5 million phrases .","label":"Background","metadata":{},"score":"45.842697"}
{"text":"Novel Association Measures Using Web Search with Double Checking \" .In Proceedings of the COLING / ACL 2006 , Sydney , Australia , pages 1009 - 1016 .[16 ] Sahami M. , Heilman T. ( 2006 ) . \"","label":"Background","metadata":{},"score":"46.05701"}
{"text":"Accurate dependency recovery has recently been reported for a number of wide - coverage statistical parsers using Combinatory Categorial Grammar ( CCG ) .However , overall figures give no indication of a parser 's performance on specific constructions , nor how suitable a parser is for specific applications .","label":"Background","metadata":{},"score":"46.106144"}
{"text":"To achieve this , a human - authored compression corpus has been created and our study highlights potential problems with the automatically gathered compression corpora currently used .Finally , we assess whether automatic evaluation measures can be used to determine compression quality . by Anna Korhonen , Yuval Krymolowski , Ted Briscoe - In Proceedings of LREC , 2006 . \" ...","label":"Background","metadata":{},"score":"46.1472"}
{"text":"We present an approach for automatically learning paraphrases from aligned monolingual corpora .Our algorithm works by generalizing the syntactic paths between corresponding anchors in aligned sentence pairs .Compared to previous work , structural paraphrases generated by our algorithm tend to be muc ... \" .","label":"Background","metadata":{},"score":"46.24935"}
{"text":"In these papers , the contexts are defined based on predefined grammatical relations that are retrieved from a language corpus .They also take into account the similarity between the retrieved contexts .The main problem of all the above methods is the difficulty to distinguish synonyms from other semantically similar pairs of words such as hyponyms , hypernyms , anto- nyms , etc .","label":"Background","metadata":{},"score":"46.64715"}
{"text":"We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents . anguage processing ( NLP ) has moved to a very firm mathematical foundation .Recently , robust graphbased methods for NLP have also been gaining a lot of interest , e.g. , in word clustering ( Bre ... . by Bill Dolan , Chris Quirk , Chris Brockett - In Proceedings of the 20th International Conference on Computational Linguistics , 2004 . \" ...","label":"Background","metadata":{},"score":"46.81388"}
{"text":"by Stephen Clark , Mark Steedman - In Proceedings of the EMNLP Conference , 2004 . \" ...Accurate dependency recovery has recently been reported for a number of wide - coverage statistical parsers using Combinatory Categorial Grammar ( CCG ) .","label":"Background","metadata":{},"score":"46.887268"}
{"text":"Complete source code and documentation for the Duluth systems that participated in the Senseval-3 ( 2004 ) comparative exercise among word sense disambiguation systems .This includes supervised lexical sample systems based on the Duluth Senseval-2 systems , and a new unsupervised lexical sample system .","label":"Background","metadata":{},"score":"46.90982"}
{"text":"2.2.12 Collocation Extraction There are various definitions for the term collocation .Collocations , in this sense , can be extracted by considering relative word combination and unigram frequencies ( BerryRogghe , 1973 ; ...Tools . \" ...We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text .","label":"Background","metadata":{},"score":"47.15063"}
{"text":"The definition of the exact nature of a collocation varies from one researcher to the next .More specifically , Smadja ( 1993 ) identifies four characteristics of collocations that have implications for machine applications , namely that ... . by Darren Pearce - IN THIRD INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION , LAS , 2002 . \" ...","label":"Background","metadata":{},"score":"47.428062"}
{"text":"In this way , the semantic information is enriched and thus the accuracy of measuring semantic simi- larity is improved .The only disadvantage is that this approach of retrieving context from the Web is too expensive because of the high number of search queries needed to retrieve the indirect context words .","label":"Background","metadata":{},"score":"47.560364"}
{"text":"We approach this problem as one of statistical machine translation ( SMT ) , within the noisy channel model of Brown et al .( 1993 ) .That is , we seek to identify the optimal paraphra ... . \" ...","label":"Background","metadata":{},"score":"47.690056"}
{"text":"Tools . by Trevor Cohn , Mirella Lapata - Journal of Artificial Intelligence Research , 2009 . \" ...This paper presents a tree - to - tree transduction method for sentence compression .Our model is based on synchronous tree substitution grammar , a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches .","label":"Background","metadata":{},"score":"47.818375"}
{"text":"Similarity involving attributes and relations : Judgments of similarity and difference are not inverses .Psychological Science , 1(1 ) : 64 - 69 , 1990 .[Nastase and Szpakowicz , 2003 ] V. Nastase and S. Szpako - wicz .","label":"Background","metadata":{},"score":"48.044167"}
{"text":"by Jay J. Jiang , David W. Conrath - Proc of 10th International Conference on Research in Computational Linguistics , ROCLING'97 , 1997 . \" ...This paper presents a new approach for measuring semantic similarity / distance between words and concepts .","label":"Background","metadata":{},"score":"48.08766"}
{"text":"Deterministic annealing is used to find lowest distortion sets of clusters : as the an-nealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchi- cal \" soft \" clustering of the data .Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held - out test data . \" ...","label":"Background","metadata":{},"score":"48.45771"}
{"text":"These techniques have been applied to word sense discrimination , email categorization , and name discrimination .Collocation Identification .NSP allows you to identify word n - grams in large corpora using standard tests of association such as Fisher 's exact test , the log likelihood ratio , Pearson 's chi - squared text , and the Dice Coefficient .","label":"Background","metadata":{},"score":"48.766216"}
{"text":"\" Improved Word Alignments Using the Web as a Corpus \" .In Proceedings of RANLP'2007 , pages 400 - 405 , Borovetz , Bulgaria .[ 3 ] Nakov S. , Nakov P. , Paskaleva E. ( 2007b ) .\" Cognate or False Friend ?","label":"Background","metadata":{},"score":"48.795166"}
{"text":"Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions .Under this framework , we introduce a wide range of composition models which ... \" .This paper proposes a framework for representing the meaning of phrases and sentences in vector space .","label":"Background","metadata":{},"score":"48.879173"}
{"text":"[ Salton and McGill , 1983 ] G. Salton and M.J. McGill .In - troduction to Modern Information Retrieval .McGraw - Hill , New York , 1983 .[ Turney et al . , 2003 ] P.D. Turney , M.L. Littman , J. Bigham , and V. Shnayder .","label":"Background","metadata":{},"score":"49.408417"}
{"text":"h exceeds significantly the precision of 53 % on word type as reported by Sch√ºtze ( 1995 ) on a related task . 4.3 Word Sense Induction The task of word sense induction ( WSI ) is to find the different senses of a word .","label":"Background","metadata":{},"score":"49.43332"}
{"text":"The system was developed and tested using essay - length responses to prompts on the Test of English as a Foreign Language ( TOEFL ) .The errorrecognition system , ALEK , performs ... \" .We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora .","label":"Background","metadata":{},"score":"49.47654"}
{"text":"The more the words are , the better the algorithm is .Table 4 shows an excerpt of the results obtained using the SIM algorithm and their corresponding precision and recall .Precision and recall obtained by the SIM algorithm .The results of all evaluated algorithms are given in Table 5 .","label":"Background","metadata":{},"score":"49.588097"}
{"text":"In Proceedings of the Workshop on Acquisition and Management of Multilingual Lexicons , held in conjunction with RANLP'2007 , pages 55 - 62 , Borovetz , Bulgaria .[ 4 ] Sparck - Jones K. ( 1972 ) .\"A Statistical Interpretation of Term Specificity and its Application in Retrieval \" .","label":"Background","metadata":{},"score":"49.69807"}
{"text":"Some sentences can be quite long , and it is not clear what part of them contains the context of the given word .Most linguists consider only the so called local context of the given word in a sentence which consists of few words before and after that word .","label":"Background","metadata":{},"score":"49.763687"}
{"text":"We also lay out a general prescription for an IE system in a new domain , employing existing components and technologies where possible .The goal is a system that can be adapted to a new domain with minimal human intervention ( say by someone who may be a domain expert but need not be a computational linguist ) .","label":"Background","metadata":{},"score":"49.988953"}
{"text":"Much of this personal information will be considered private , and therefore mechanisms which allow users to control the dissemination of these data are vital .Location information is a particularly useful form of context in ubiquitous computing , yet its unconditional distribution can be very invasive . \" ...","label":"Background","metadata":{},"score":"50.11925"}
{"text":"..It is worth noting at this point that there are several well - known measures from the NLP literature that we have omitted from our experiments .It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distribut ... .","label":"Background","metadata":{},"score":"50.408737"}
{"text":"While most work has been done on hyponym detection ( and the rela ... . ... range from multiple sequence alignment [ 10 ] , to gene expression [ 2 ] , to galaxy formation [ 56].Other applications include image registration , protein - protein interaction networks and VLSI circuit design .","label":"Background","metadata":{},"score":"50.512085"}
{"text":"Using a sequence of 10 such queries , we can obtain up to 4 . 1,000 query results ( Google sets explicit limits to never return more than 1,000 results ) .Each result contains a title and an excerpt ( snippet ) of text containing the word we searched for .","label":"Background","metadata":{},"score":"50.5637"}
{"text":"In Proceedings of the International Conference on Re - cent Advances in Natural Language Processing ( RANLP-03 ) , Borovets , Bulgaria , pages 482 - 489 , 2003 .[ Turney and Littman , 2005 ] P.D. Turney and M.L. Littman .","label":"Background","metadata":{},"score":"50.809082"}
{"text":"In Fifth International Workshop on Computational Seman - tics ( IWCS-5 ) , Tilburg , The Netherlands , pages 285 - 301 , 2003 .[ Rosario and Hearst , 2001 ] B. Rosario and M. Hearst .Clas - sifying the semantic relations in noun - compounds via a domain - specific lexical hierarchy .","label":"Background","metadata":{},"score":"50.81317"}
{"text":"Abstract - Cultural heritage institutions and multimedia archives often delegate the task of annotating their collections of artifacts to Web users .The use of crowdsourced annotations from the Web gives rise to trust issues .We propose an algorithm that , by making use of a combination of subjective l ... \" .","label":"Background","metadata":{},"score":"50.81632"}
{"text":"Experimental results on sentence compression bring significant improvements over a state - of - the - art model . re engineering and parameter optimization experiments .We measured F1 over directed and labeled dependency relations .Note that we could extract dependencies directly from the output of our model since it generates trees in addition to strings .","label":"Background","metadata":{},"score":"50.821712"}
{"text":"In this dissertation , I suggest that an answer to this question lies in the representation of conceptual . ... , or a task requires more abstraction than solely lexical correspondences permit .In the sections that follow , I discuss the extension of lexical relationships to class - based relationships , and consider the ... . by Lillian Lee - In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics , 1999 . \" ...","label":"Background","metadata":{},"score":"50.85305"}
{"text":"Such words do not bring semantic information about the searched word and should be omitted because they only distort the results .Then we go through the extracted words sequences and when we find the target word or one of its forms , we take 3 words before and after it ( the number 3 here we call context size ) .","label":"Background","metadata":{},"score":"50.866634"}
{"text":"Inference rules are extremely important in many fields such as n ... \" .Inference rules are extremely important in many fields such as natural language processing , information retrieval , and artificial intelligence in general .Our algorithm is based on an extended version of Harris 's Distributional Hypothesis , which states that words that occurred in the same contexts tend to be similar .","label":"Background","metadata":{},"score":"50.974663"}
{"text":"Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo - sentence .We consider a new approach , LexRank , for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences .","label":"Background","metadata":{},"score":"51.024338"}
{"text":"In this paper we survey such methods and assess to what extent they can help create a complete IE system that can be easily adapted to ... \" .All components of a typical IE system have been the object of some machine learning research , motivated by the need to improve time taken to transfer to new domains .","label":"Background","metadata":{},"score":"51.109287"}
{"text":"In Proceedings of 15th International World Wide Web Conference , Edinburgh , Scotland .[17 ] Bollegala D. , Matsuo Y. , Ishizuka M. ( 2007 ) . \"Measuring Semantic Similarity between Words Using Web Search Engines \" , In Proceedings of the 16th International World Wide Web Conference ( WWW2007 ) , Banff , Canada , pages 757 - 766 .","label":"Background","metadata":{},"score":"51.137074"}
{"text":"Our system , based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation .In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations .","label":"Background","metadata":{},"score":"51.16596"}
{"text":"Our algorithm is general and can be applied to many languages .It does not require resources that are hard to find .The only resource that is not publicly available for any language is the grammatical dictionary .It is good to have it for highly inflectional languages like Russian , but this is less important for languages like English .","label":"Background","metadata":{},"score":"51.654587"}
{"text":"Method for Automatic Synonyms Extraction Our algorithm for automatic extraction of synonyms from a list of words is based on measuring the semantic similarity between pairs of words by querying a Web search engine ( e.g. , Google ) and analyzing the returned results .","label":"Background","metadata":{},"score":"52.516327"}
{"text":"In learning the context and directionality of quasi - paraphrases , we have encountered the need for semantic concepts : Both RSPs and ISPs are defined in terms of semantic concepts .For learning these semantic concepts from text , we use a semi - supervised clustering algorithm HMRF - KMeans .","label":"Background","metadata":{},"score":"53.215046"}
{"text":"In learning the context and directionality of quasi - paraphrases , we have encountered the need for semantic concepts : Both RSPs and ISPs are defined in terms of semantic concepts .For learning these semantic concepts from text , we use a semi - supervised clustering algorithm HMRF - KMeans .","label":"Background","metadata":{},"score":"53.215046"}
{"text":"While this resource is attractive in being extensive enough for some NLP use , it is not comprehensive .In this paper , we present a substantial extension to Levin 's taxonomy which incorporates 57 novel classes for verbs not covered ( comprehensively ) by Levin .","label":"Background","metadata":{},"score":"53.31307"}
{"text":"Since commercial and news sites are typically ranked higher by Google , the top 1,000 results are not a representative sample of all texts on the Web .Our algorithms assume that words appearing in similar con- texts are similar , but this does not directly mean that they are synonyms .","label":"Background","metadata":{},"score":"53.560226"}
{"text":"We can formalize the above ideas by assigning frequency vectors formed out of the words in the local contexts of the target words and measure the similarity between these vectors .For example for the words painter and painting , we could have the frequency vectors of the words in their contexts as shown in table 1 ( with abridgments ) .","label":"Background","metadata":{},"score":"53.587555"}
{"text":"Modifying the threshold of the frequency used in the reverse context lookup has little impact on the accuracy .Lower threshold causes a lower precision at the beginning of the list and an overall higher recall .Higher thresholds improve the precision for the top 40 - 50 pairs , but lowers the overall recall .","label":"Background","metadata":{},"score":"53.905975"}
{"text":"The task of detecting and generating hy - ponyms is at the core of semantic under - standing of language , and has numerous practical applications .We investigate how neural network embeddings perform on this task , compared to dependency - based vector space models , and evaluate a range of similarity measu ... \" .","label":"Background","metadata":{},"score":"53.990242"}
{"text":"Respectively , the graphic of SIM+TFIDF is located above the graphic of the SIM algorithm most of the time .Applying the reverse context lookup technique improves the SIM algorithm by increasing its precision for the top 40 - 50 pairs in the list ( algorithms REV2 - REV7 ) , but overall decreases the recall .","label":"Background","metadata":{},"score":"54.27234"}
{"text":"If we search the Web for the first three words , we shall convince ourselves that painting appears often in their contexts .However , if we search for the last three words , we will find that in their contexts painting almost does not appear .","label":"Background","metadata":{},"score":"54.303047"}
{"text":"For the other algorithms 11pt average precision is not defined ( their recall never reaches 100 % ) .Discussion In table 5 , we can see that the proposed algorithms arrange most of the synonyms at the beginning of the produced ordered lists of pairs .","label":"Background","metadata":{},"score":"54.440807"}
{"text":"References [ 1 ] Hearst M. ( 1991 ) .\" Noun Homograph Disambiguation Using Local Context in Large Text Corpora \" .In Proceedings of the 7th Annual Conference of the University of Waterloo Centre for the New OED and Text Research , Oxford , England , pages 1 - 22 .","label":"Background","metadata":{},"score":"54.47755"}
{"text":"For the top 200 pairs , almost all synonyms are listed ( recall 96 % ) , but the precision drops to 24 % .The SIM algorithm lists almost all synonyms in the first 100 results ( which are only 2.11 % of all 4,371 pairs in the list ) .","label":"Background","metadata":{},"score":"54.48069"}
{"text":"[5 ] Salton G. , McGill M. ( 1983 ) , Introduction to Modern Information Retrieval , McGraw - Hill , New York .[ 6 ] Paskaleva E. ( 2002 ) .\" Processing Bulgarian and Russian Resources in Unified Format \" .","label":"Background","metadata":{},"score":"54.573425"}
{"text":"[ 12 ] Hagiwara –ú. , Ogawa Y. , Toyama K. ( 2007 ) .\"Effectiveness of Indirect Dependency for Automatic Synonym Acquisition \" .In Proceedings of CoSMo 2007 Workshop , held in conjuction with CONTEXT 2007 , Roskilde , Denmark .","label":"Background","metadata":{},"score":"54.624878"}
{"text":"With the growing availability and popularity of opinion - rich resources such as online review sites and personal blogs , new opportunities and challenges arise as people now can , and do , actively use information technologies to seek out and understand the opinions of others .","label":"Background","metadata":{},"score":"54.917576"}
{"text":"The dictionary contains about 1,500,000 wordforms and about 100,000 lemmata .Each dictionary entry contains wordform , cor- responding lemma , followed by morphological and grammatical information .Contains 507 words ( prepositi- ons , pronouns , conjunctions , particles , interjections and some adverbs ) .","label":"Background","metadata":{},"score":"55.121002"}
{"text":"These devices will perform actions based on the context of their users , and therefore ubiquitous systems will gather , collate and distribute much more per ... \" .The field of ubiquitous computing envisages an era when the average consumer owns hundreds or thousands of mobile and embedded computing devices .","label":"Background","metadata":{},"score":"55.23821"}
{"text":"We discuss several methods to compute centrality using the similarity graph .The results show that degree - based methods ( including LexRank ) outperform both centroid - based methods and other systems participating in DUC in most of the cases .","label":"Background","metadata":{},"score":"55.251152"}
{"text":".. ch rules .In this paper , we present an unsupervised algorithm , DIRT , for Discovery of Inference Rules from Text .Algorithms for finding similar words assume the Distributional Hypothesis , which states that words that occurred in the same contexts tend to have similar meanings [ 7].","label":"Background","metadata":{},"score":"55.354134"}
{"text":"We use examples to show that our system discovers many inference rules easily missed by humans . ... we present an unsupervised algorithm , DIRT , for Discovering Inference Rules from Text .Algorithms for finding similar words assume the Distributional Hypothesis , which states that words that occurred in the same contexts tend to have similar meanings ( Harris , 1985 ) .","label":"Background","metadata":{},"score":"55.355965"}
{"text":"This paper describes a distributional approach to the semantics of verb - particle constructions ( e.g. put up , make off ) .We report first on a framework for implementing and evaluating such models .We then go on to report on the implementation of some techniques for using statistical models acq ... \" .","label":"Background","metadata":{},"score":"55.82093"}
{"text":"ACM SIGIR Forum , 32(2 ) : 14 - 15 , 1998 .[Dagan and Glickman , 2004 ] I. Dagan and O. Glickman , Probabilistic textual entailment : Generic applied model - ing of language variability .In Learning Methods for Text Understanding and Mining , Grenoble , France , 2004 .","label":"Background","metadata":{},"score":"56.06891"}
{"text":"Alignment Error Rate ( AER ) is measured to gauge the quality of the resulting corpus .A monotone phrasal decoder generates contextual replacements .Human evaluation shows that this system outperforms baseline paraphrase generation techniques and , in a departure from previous work , offers better coverage and scalability than the current best - of - breed paraphrasing approaches . ...","label":"Background","metadata":{},"score":"56.145138"}
{"text":"Select the SEEK icon to attempt to find the referenced article .If it does not appear to be in cogprints you will be forwarded to the paracite service .Poorly formated references will probably not work .[ Banerjee and Pedersen , 2003 ] S. Banerjee and T. Pedersen .","label":"Background","metadata":{},"score":"56.69462"}
{"text":"While humans appear to use inference rules such as \" X writes Y \" implies \" X is the author of Y \" in answering questions , such rules are generally unavailable ... \" .One of the main challenges in question - answering is the potential mismatch between the expressions in questions and the expressions in texts .","label":"Background","metadata":{},"score":"57.39904"}
{"text":"It was shown that if words ' frequency is weighted according to their importance , the search quality improves [ Sparck - Jones , 1972].After that , we compute IDF[wi ] by dividing the total number of documents indexed by Google ( we assume they are about eight billions ) to the number of occurrences in Google of wi .","label":"Background","metadata":{},"score":"57.61563"}
{"text":"We then present a new evaluation methodology for the automatically constructed the- saurus .The evaluation results show that the the- saurus is significantly closer to WordNet than Roget Thesaurus is . ... alternatives .We also constructed several other thesauri using the same corpus , but with the similarity measures in Figure 1 .","label":"Background","metadata":{},"score":"58.039383"}
{"text":"Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co - occurring insequence n - grams automatically .The second method relaxes strict n - gram matching to skipbigram matching .Skip - bigram is any pair of words in their sentence order .","label":"Background","metadata":{},"score":"58.44834"}
{"text":"[ 9 ] Curran J. , Moens M. ( 2002 ) .\" Improvements in –êutomatic –¢hesaurus –ïxtraction \" .In Proceedings of the Workshop on Unsupervised Lexical Acquisition , SIGLEX 2002 , Philadelphia , USA , pages 59 - 67 .","label":"Background","metadata":{},"score":"58.95056"}
{"text":"Machine Learning , in press , 2005 . \" ...One of the main challenges in question - answering is the potential mismatch between the expressions in questions and the expressions in texts .While humans appear to use inference rules such as \" X writes Y \" implies \" X is the author of Y \" in answering questions , such rules are generally unavailable ... \" .","label":"Background","metadata":{},"score":"58.95278"}
{"text":"Matrix Computations .Third edition .Johns Hop - kins University Press , Baltimore , MD , 1996 .[ Landauer and Dumais , 1997 ] T.K. Landauer and S.T. Du - mais .A solution to Plato 's problem : The latent semantic analysis theory of the acquisition , induction , and repre - sentation of knowledge .","label":"Background","metadata":{},"score":"59.269424"}
{"text":"While humans appear to use inference rules such as \" X writes Y \" implies \" X is the author of Y \" in answering questions , such rules are generally unavailable to question - answering systems due to the inherent difficulty in constructing them .","label":"Background","metadata":{},"score":"59.496063"}
{"text":"The results show that degree - based methods ( including LexRank ) outperform both centroid - based methods and other systems participating in DUC in most of the cases .Furthermore , the LexRank with threshold method outperforms the other degree - based techniques including continuous LexRank .","label":"Background","metadata":{},"score":"59.802227"}
{"text":"In our case , the algorithms for synonyms extraction using the Web as a corpus return a list of pairs of words and some of them are synonyms while other are not .We compute precision @ n by dividing the number of synonyms in the first n pairs by the number n. We compute recall @ n by dividing the total number of synonyms that exist in the data set ( 50 ) by the number of synonyms in the first n pairs .","label":"Background","metadata":{},"score":"59.892338"}
{"text":"What are Cushman and Wakefield known for ?What are pomegranates ?What is hybridization ?Wh ... . \" ...This thesis is concerned with the measurement and application of lexical distributional similarity .Two words are said to be distributionally similar if they appear in similar contexts .","label":"Background","metadata":{},"score":"60.341347"}
{"text":"This extensive lexicon was acquired automatically from five corpora and the Web using the current version of the comprehensive subcategorizatio ... \" .We introduce a large computational subcategorization lexicon which includes subcategorization frame ( SCF ) and frequency information for 6,397 English verbs .","label":"Background","metadata":{},"score":"60.871735"}
{"text":"We compute the similarity between the vectors as the cosine in the n - dimensional Euclidean space .Thus we obtain a number between 0 and 1 , which is a numerical mea- sure for the semantic similarity between two words ( higher value means more similar words ) .","label":"Background","metadata":{},"score":"60.92003"}
{"text":"We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation .Results show that edit distance data is cleaner and more easily - aligned than the heuristic data , with an overall alignment error rate ( AER ) of 11.58 % on a similarly - extracted test set .","label":"Background","metadata":{},"score":"61.225746"}
{"text":"After a detailed definition of the algorithm and a discussion of its strengths and weaknesses , the performance of Chinese Whispers is measured on Natural Language Processing ( NLP ) problems as diverse as language separation , acquisition of syntactic word classes and word sense disambiguation .","label":"Background","metadata":{},"score":"62.68129"}
{"text":"Frequency vectors for the terms \" painter \" and \" painting \" .Most of these words are semantically related to painting , but some of them are not .If we take the word painting and a sufficiently large number of sentences containing that word ( e.g. , 1,000 ) and we extract from them all the words from its local context , we could expect that the most frequently appearing words to be semantically related to painting .","label":"Background","metadata":{},"score":"63.710114"}
{"text":"In the performed experiments we process Russian texts used for teaching students studying fine arts .We chose Russian and fine arts terminology because of the high volume of such texts available in Internet and the great number of full synonyms in this domain .","label":"Background","metadata":{},"score":"64.62361"}
{"text":"We apply lemmatization ( replace all words with their basic form ) , e.g. replace paintings with painting .For this purpose , we use a rich grammatical dictionary of Russian .Now we have all words which appear in the local Web context of the target word and their corresponding frequencies ( frequency vectors ) .","label":"Background","metadata":{},"score":"65.87412"}
{"text":"Since the paraphrases are learned from a large broad - coverage corpus , our patterns are domain - independent , making the task of moving to new domains very easy .LEARNING PARAPHRASES FROM TEXT by Rahul Bhagat A Dissertation Presented to the FACULTY OF THE GRADUATE SCHOOL UNIVERSITY OF SOUTHERN CALIFORNIA In Partial Fulfillment of the Requirements for the Degree DOCTOR OF PHILOSOPHY ( COMPUTER SCIENCE ) August 2009 Copyright 2009 Rahul Bhagat Unsupervised Corpus Based Clustering of Similar Contexts .","label":"Background","metadata":{},"score":"67.05003"}
{"text":"Workshop on Graph Based Methods for Natural Language Processing , 2006 . \" ...We introduce Chinese Whispers , a randomized graph - clustering algorithm , which is time - linear in the number of edges .After a detailed definition of the algorithm and a discussion of its strengths and weaknesses , the performance of Chinese Whispers is measured on Natural Language Processing ( NLP ) pro ... \" .","label":"Background","metadata":{},"score":"67.8342"}
{"text":"We evaluated the SIM algorithm also using \" 11-pt average precision \" , a widely used metric in information retrieval which combines precision and recall in a single number 9 .[ Salton , 1983].11-point average precision is computed by averaging the values in 11 points respectively for recall of 0 % , 10 % , 20 % , ... and 100 % .","label":"Background","metadata":{},"score":"68.44572"}
{"text":"[ Lesk , 1986 ] M.E. Lesk .Automatic sense disambiguation using machine readable dictionaries : How to tell a pine cone from a ice cream cone .In Proceedings of ACM SIGDOC ' 86 , pages 24 - 26 , 1986 .","label":"Background","metadata":{},"score":"68.87149"}
{"text":"Artificial Intelligence Research and Development , Volume 131 , 2005 .","label":"Background","metadata":{},"score":"68.87453"}
{"text":"Instead of precision and recall , in table 5 , we give the number of synonyms found in the first 1 , 5 , 10 , 20 , 30 , 40 , 50 , 100 and 200 results .The best values are given in bold .","label":"Background","metadata":{},"score":"70.07605"}
{"text":"We selected only terms that occur in Google at least 5,000 times in order to have a statistical precision .the Web ( e.g. , just 5 times ) can not be analyzed statistically because the extracted context will be too small and not enough meaningful .","label":"Background","metadata":{},"score":"70.22721"}
{"text":"The resulting algorithmic framework was applied successfully to synthetic data , as well as to identifying text - based cross - religion correspondences .LEARNING PARAPHRASES FROM TEXT by Rahul Bhagat A Dissertation Presented to the FACULTY OF THE GRADUATE SCHOOL UNIVERSITY OF SOUTHERN CALIFORNIA In Partial Fulfillment of the Requirements for the Degree DOCTOR OF PHILOSOPHY ( COMPUTER SCIENCE ) August 2009 Copyright 2009 Rahul Bhagat .","label":"Background","metadata":{},"score":"70.63002"}
{"text":"[ Berry , 1992 ] M.W. Berry .Large scale singular value com - putations .International Journal of Supercomputer Ap - plications , 6(1 ) : 13 - 49 , 1992 .[ Clarke et al . , 1998 ] C.L.A. Clarke , G.V. Cormack , and C.R. Palmer .","label":"Background","metadata":{},"score":"71.45831"}
{"text":"TF.IDF Weighting In information retrieval , TF.IDF weighting is a common technique for improving the search quality .The number TF.IDF ( term frequency times inverted document frequen- cy ) is a statistical measure that shows how important is a certain word for a given 5 . document in a set of documents .","label":"Background","metadata":{},"score":"71.465096"}
{"text":"When computing the co - occurrence frequency vector we can ignore words that occur in the co - occurrence frequency vector infrequently ( e.g. , three times or less ) because this could have happened by chance .By modifying this parameter ( frequency threshold ) , we can affect the accuracy of the results .","label":"Background","metadata":{},"score":"72.82858"}
{"text":"Automatic Acquisition of Synonyms Using the Web as a Corpus Svetlin Nakov1 1 Sofia University \" St. Kliment Ohridski \" , Department of Mathematics and Informatics , 5 James Baucher Bvld . , Sofia , Bulgaria , nakov [ at ] fmi.uni-sofia .","label":"Background","metadata":{},"score":"74.54051"}
{"text":"Accidentally found words like guide and Jane should appear quite rarely if we take a sufficiently large set of arbitrary sentences .Now let us take two words and extract the frequently appearing words in their local contexts taken from sufficiently large number of sentences .","label":"Background","metadata":{},"score":"75.5622"}
{"text":"According to the influential theo ... \" .Selectional constraints are limitations on the applicability of predicates to arguments .For example , the statement \" The number two is blue \" may be syntactically well formed , but at some level it is anomalous - BLUE is not a predicate that can be applied to numbers .","label":"Background","metadata":{},"score":"77.29062"}
{"text":"Let F(x , y ) be number of appearances of y in the Web context of x. Consider some word w and all the words wi from its Web context along with their frequencies F(w , wi ) .Now let us extract from the Web for each word wi the number of reverse occurrences F(wi , w ) of the word w in the context of wi ( reverse context ) .","label":"Background","metadata":{},"score":"79.11528"}
{"text":"Below we compare the algorithms in more detail and we discuss what causes the errors .Comparison of the Algorithms Figure 1 shows the precision / recall curves for the algorithms RAND , SIM , SIM+TFIDF and REV4 .Figure 1 .","label":"Background","metadata":{},"score":"79.371475"}
{"text":"Free price quotes from local exterior & interior painting contractors .List of titles and text snippets returned by Google for the word \" painting \" .In the titles and snippets returned by the search engine , we first convert all letters to lowercase and we extract all words .","label":"Background","metadata":{},"score":"80.180786"}
{"text":"Abstract .In many applications of natural language processing ( NLP ) it is necessary to determine the likelihood of a given word combination .For example , a speech recognizer may need to determine which of the two word combinations \" eat a peach \" and \" eat a beach \" is more likely .","label":"Background","metadata":{},"score":"81.18637"}
{"text":"Since the paraphrases are learned from a large broad - coverage corpus , our patterns are domain - independent , making the task of moving to new domains very easy .LEARNING PARAPHRASES FROM TEXT by Rahul Bhagat A Dissertation Presented to the FACULTY OF THE GRADUATE SCHOOL UNIVERSITY OF SOUTHERN CALIFORNIA In Partial Fulfillment of the Requirements for the Degree DOCTOR OF PHILOSOPHY ( COMPUTER SCIENCE ) August 2009 Copyright 2009 Rahul Bhagat LEARNING PARAPHRASES FROM TEXT by Rahul Bhagat A Dissertation Presented to the FACULTY OF THE GRADUATE SCHOOL UNIVERSITY OF SOUTHERN CALIFORNIA In Partial Fulfillment of the Requirements for the Degree DOCTOR OF PHILOSOPHY ( COMPUTER SCIENCE ) August 2009 Copyright 2009 Rahul Bhagat .","label":"Background","metadata":{},"score":"84.65577"}
{"text":"In many applications of natural language processing ( NLP ) it is necessary to determine the likelihood of a given word combination .For example , a speech recognizer may need to determine which of the two word combinations \" eat a peach \" and \" eat a beach \" is more likely .","label":"Background","metadata":{},"score":"84.88005"}
{"text":"The major SIM algorithm starts well with 5 correct synonyms and precision @ 5 of 100 % .For the top 10 ranked pairs and in the first 20 pairs the precision remains very high : 80 % .For the top 40 pairs , the algorithm lists 56 % of all the synonyms and its precision is still 70 % .","label":"Background","metadata":{},"score":"85.26017"}
{"text":"Painting - Wikipedia , the free encyclopedia Painting , meant literally , is the practice of applying color to a surface ( support ) such as , e.g. paper , canvas , wood , glass , lacquer or concrete . ...Painting - Exterior & Interior House Painters - Faux Finishing ...","label":"Background","metadata":{},"score":"87.59803"}
{"text":"For words not appearing in given context , we assume frequency 0 .Therefore , we obtain the frequency vectors ( with abridgments ) shown in Table 2 . vector 1 vector 2 word ( painter ) ( painting ) painter 422 98 painting 262 461 paint 202 91 art 167 188 gallery 94 183 famous 84 205 buy 72 386 big 56 176 expensive 3 345 camera 0 2 Table 2 .","label":"Background","metadata":{},"score":"88.76303"}
{"text":"The output of these programs is a ranked list of similar words to each word .For example , [ 12 ] outputs the following similar words for wine and suit : wine : beer , white wine , red wine , Chardon ... . ... 8.6 powerful tool 8.4 powerful storms 8.3 powerful minority 8.1 powerful neighbor A similar list can be computed on word pairs that have relationships other than immediate neighbor .","label":"Background","metadata":{},"score":"96.52078"}
