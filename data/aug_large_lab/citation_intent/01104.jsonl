{"text":"They measure similarity between two syntactic trees in terms of their sub - structures ( e.g. [ Collins and Duffy , 2002 ] ) .These approaches have given optimal results [ Moschitti , 2004 ] when introducing syntactic information in the task of Predicate Argument Classification .","label":"Background","metadata":{},"score":"29.499104"}{"text":"We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm , and we give experimental results on the ATIS corpus of parse trees . ... lems .The method is derived by the transformation from ranking problems to a margin - based classification problem in [ 8].","label":"Background","metadata":{},"score":"29.53125"}{"text":"The appropriate output transformation for a given task can be selected by applying a hill - climbing approach to held - out data .On the NP Chunking task , our hill - climbing system finds a model structure that outperforms both first - order and second - order models with the same input feature set .","label":"Background","metadata":{},"score":"29.776297"}{"text":"This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this i ... \" .","label":"Background","metadata":{},"score":"30.605833"}{"text":"We expect that our method could be further improved via well - tuned parameter validations for different languages . \" ...Discriminative reranking is one method for constructing high - performance statistical parsers ( Collins , 2000 ) .A discriminative reranker requires a source of candidate parses for each sentence .","label":"Background","metadata":{},"score":"31.196632"}{"text":"Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand - annotated training data . \" ...Discriminative reranking is one method for constructing high - performance statistical parsers ( Collins , 2000 ) .","label":"Background","metadata":{},"score":"31.3496"}{"text":"This paper describes a simple yet novel method for constructing sets of 50-best parses based on a co ... \" .Discriminative reranking is one method for constructing high - performance statistical parsers ( Collins , 2000 ) .A discriminative reranker requires a source of candidate parses for each sentence .","label":"Background","metadata":{},"score":"31.62263"}{"text":"In particular , it allows one to efficiently learn a model which discriminates among the entire space of parse trees , as opposed to reranking the top few candidates .Our models can condition on arbitrary features of input sentences , thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness .","label":"Background","metadata":{},"score":"31.66644"}{"text":"The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .","label":"Background","metadata":{},"score":"31.790703"}{"text":"The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .","label":"Background","metadata":{},"score":"31.790703"}{"text":"The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .","label":"Background","metadata":{},"score":"31.790703"}{"text":"The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .","label":"Background","metadata":{},"score":"31.790703"}{"text":"Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers .We sh ... \" .In this paper , we propose a machine learning algorithm for shallow semantic parsing , extending the work of Gildea and Jurafsky ( 2002 ) , Surdeanu et al .","label":"Background","metadata":{},"score":"31.814928"}{"text":"This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence .","label":"Background","metadata":{},"score":"32.065575"}{"text":"We show how the algorithms can be efficiently applied to exponential sized representations of parse trees , such as the \" all subtrees \" ( DOP ) representation described by ( Bod 98 ) , or a representation tracking all sub - fragments of a tagged sentence .","label":"Background","metadata":{},"score":"32.09884"}{"text":"We show how the algorithms can be efficiently applied to exponential sized representations of parse trees , such as the \" all subtrees \" ( DOP ) representation described by ( Bod 98 ) , or a representation tracking all sub - fragments of a tagged sentence .","label":"Background","metadata":{},"score":"32.09884"}{"text":"( 2005 ) , obtain very high accuracy on standard dependency parsing tasks and can be trained and applied without marginalization , ' ' summing trees ' ' permits some alternative techniques of interest .Using the summing algorithm , we present experimental results on four nonprojective languages , for maximum conditional likelihood estimation , minimum Bayes - risk parsing , and hidden variable training .","label":"Background","metadata":{},"score":"32.191"}{"text":"We present several improvements to unlexicalized parsing with hierarchically state - split PCFGs .First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar without a treebank .","label":"Background","metadata":{},"score":"32.285206"}{"text":"A notable gap in research on statistical dependency parsing is a proper conditional probability distribution over nonprojective dependency trees for a given sentence .We exploit the Matrix Tree Theorem ( Tutte , 1984 ) to derive an algorithm that efficiently sums the scores of all nonprojective trees in a sentence , permitting the definition of a conditional log - linear model over trees .","label":"Background","metadata":{},"score":"32.37909"}{"text":"However , increasing a model 's order can lead to an increase in the number of model parameters , making the model more susceptible to sparse data problems .This paper shows how the notion of output transformation can be used to explore a variety of alternative model structures .","label":"Background","metadata":{},"score":"32.566513"}{"text":"This paper shows how finite approximations of long distance dependency ( LDD ) resolution can be obtained automatically for wide - coverage , robust , probabilistic Lexical - Functional Grammar ( LFG ) resources acquired from treebanks .We extract LFG subcategorisation frames and paths linking LDD reentrancies from f - structures generated automatically for the Penn - II treebank trees and use them in an LDD resolution algorithm to parse new text .","label":"Background","metadata":{},"score":"32.57856"}{"text":"Evaluation on the ACE RDC corpora shows that our dynamic context - sensitive tree span is much more suitable for relation extraction than SPT and our tree kernel outperforms the state - of - the - art Collins and Duffy 's convolution tree kernel .","label":"Background","metadata":{},"score":"32.748512"}{"text":"Finally , it shows that feature - based and tree kernel - based methods much complement each other and the composite kernel can well integrate both flat and structured features .Syntactic reordering approaches are an effective method for handling word - order differences between source and target languages in statistical machine translation ( SMT ) systems .","label":"Background","metadata":{},"score":"32.836994"}{"text":"A second model then attempts to improve upon this i ... \" .This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .","label":"Background","metadata":{},"score":"32.936325"}{"text":"We argue that bootstrapping a parser is most promising when the model uses a rich set of redundant features , as in recent models for scoring dependency parses ( McDonald et al . , 2005 ) .Drawing on Abney 's ( 2004 ) analysis of the Yarowsky algorithm , we perform bootstrapping by entropy regularization : we maximize a linear combination of conditional likelihood on labeled data and confidence ( negative Renyi entropy ) on unlabeled data .","label":"Background","metadata":{},"score":"33.18834"}{"text":"In this paper we present new experiments to test this claim .We use the PARSEVAL metric , the Leaf - Ancestor metric as well as a dependency - based evaluation , and present novel approaches measuring the effect of controlled error insertion on treebank trees and parser output .","label":"Background","metadata":{},"score":"34.059074"}{"text":"This view leads to a single algorithmic framework for the three problems .We prove worst case loss bounds for various algorithms for both the realizable case and the non - realizable case .The end result is new algorithms and accompanying loss bounds for hinge - loss regression and uniclass .","label":"Background","metadata":{},"score":"34.124077"}{"text":"First , it automatically determines a dy - namic context - sensitive tree span for relation extraction by extending the widely - used Shortest Path - enclosed Tree ( SPT ) to include necessary context information outside SPT .Second , it proposes a context - sensitive convolution tree kernel , which enumerates both context - free and context - sensitive sub - trees by considering their ancestor node paths as their contexts .","label":"Background","metadata":{},"score":"34.37747"}{"text":"We show in particular how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications . \" ...In this paper , we propose a machine learning algorithm for shallow semantic parsing , extending the work of Gildea and Jurafsky ( 2002 ) , Surdeanu et al .","label":"Background","metadata":{},"score":"34.555183"}{"text":"A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence .The strength of our approach is that it allows a tree to be represented as an arbitrary set of features , without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account .","label":"Background","metadata":{},"score":"34.604694"}{"text":"We describe an incremental parser that was trained to minimize cost over sentences rather than over individual parsing actions .This is an attempt to use the advantages of the two top - scoring systems in the CoNLL - X shared task .","label":"Background","metadata":{},"score":"35.151196"}{"text":"Tools . \" ...This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .","label":"Background","metadata":{},"score":"35.152084"}{"text":"We show how partition functions and marginals for directed spanning trees can be computed by an adaptation of Kirchhoff 's Matrix - Tree Theorem .To demonstrate an application of the method , we perform experiments which use the algorithm in training both log - linear and max - margin dependency parsers .","label":"Background","metadata":{},"score":"35.267883"}{"text":"We present and evaluate here several methods that integrate LSA - based information with a standard language model : a semantic cache , partial reranking , and different forms of interpolation .We found that all methods show significant improvements , compared to the 4-gram baseline , and most of them to a simple cache model as well .","label":"Background","metadata":{},"score":"35.417206"}{"text":"We evaluate the proposed algorithms on the 2007 CONLL Shared Task , and report errors analysis .Experimental results show that the system score is better than the average score among the participating systems .In the paper we describe a dependency parser that uses exact search and global learning ( Crammer et al . , 2006 ) to produce labelled dependency trees .","label":"Background","metadata":{},"score":"35.661343"}{"text":"More generally , we consider problems involving multiple dependent output variables , structured output spaces , and classification problems with class attributes .In order to accomplish this , we propose to appropriately generalize the well - known notion of a separation margin and derive a corresponding maximum - margin formulation .","label":"Background","metadata":{},"score":"35.78518"}{"text":"This paper explores a parsimonious approach to Data - Oriented Parsing .While allowing , in principle , all possible subtrees of trees in the treebank to be productive elements , our approach aims at finding a manageable subset of these trees that can accurately describe empirical distributions over phrase - structure trees .","label":"Background","metadata":{},"score":"35.813408"}{"text":"We investigate for the first time how factors such as training data size , corpus ( e.g. , Br ... \" .We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .","label":"Background","metadata":{},"score":"35.83323"}{"text":"In addition , we utilize the RankBoost - based reranking algorithm to rerank the N - best outputs of the HMM - based tagger using various $ n$-gram , morphological , and dependency features .Two methods are proposed to improve the generalization performance of the reranking algorithm .","label":"Background","metadata":{},"score":"36.139923"}{"text":"Discriminative reranking is one method for constructing high - performance statistical parsers ( Collins , 2000 ) .A discriminative reranker requires a source of candidate parses for each sentence .This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse - to - fine generative parser ( Charniak , 2000 ) .","label":"Background","metadata":{},"score":"36.184437"}{"text":"This view leads to a single algorithmic framework for the three problems .We prove worst case loss bounds for various algorithms for both the realizable case and the non - realizable case .The end result is new alg ... \" .","label":"Background","metadata":{},"score":"36.194267"}{"text":"This factorization provides conceptual simplicity , straightforward opportunities for separately improving the component models , and a level of performance comparable to similar , non - factored models .This pruning is done for efficiency ; the question is whether it is hurting accuracy .","label":"Background","metadata":{},"score":"36.475464"}{"text":"We can not use non - local features due to concerns about complexity with current major methods of sequence labeling such as CRFs .We propose a new perceptron algorithm that can use non - local features .Our algorithm allows the use of all types of non - local features whose values are determined from the sequence and the labels .","label":"Background","metadata":{},"score":"36.62315"}{"text":"In this paper , we describe a two - stage multilingual dependency parser used for the multilingual track of the CoNLL 2007 shared task .The system consists of two components : an unlabeled dependency parser using Gibbs sampling which can incorporate sentence - level ( global ) features as well as token - level ( local ) features , and a dependency relation labeling module based on Support Vector Machines .","label":"Background","metadata":{},"score":"36.676643"}{"text":"Second , we compare various inference procedures for state - split PCFGs from the standpoint of risk minimization , paying particular attention to their practical tradeoffs .Finally , we present multilingual experiments which show that parsing with hierarchical state - splitting is fast and accurate in multiple languages and domains , even without any language - specific tuning . .","label":"Background","metadata":{},"score":"36.865974"}{"text":"We introduce a new method for the reranking task , based on the boosting approach to ranking problems described in Freund et al .( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .","label":"Background","metadata":{},"score":"36.906494"}{"text":"We introduce a new method for the reranking task , based on the boosting approach to ranking problems described in Freund et al .( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .","label":"Background","metadata":{},"score":"36.906494"}{"text":"In this paper , we present a three - step multi - lingual dependency parser based on a deterministic shift - reduce parsing algorithm .Different from last year , we separate the root - parsing strategy as sequential labeling task and try to link the neighbor word dependences via a near neighbor parsing .","label":"Background","metadata":{},"score":"37.330727"}{"text":"For example suppose we want to implement a re - ranker based on tree kernel and flat features .We need to compare pairs of instances .The following line contains a pair of PASs and a pair of feature vectors that correspond to two target predicate argument structures .","label":"Background","metadata":{},"score":"37.46897"}{"text":"This framework integrates multiple MT systems ' output at the word- , phrase- and sentence- levels .By boosting common word and phrase translation pairs , pruning unused phrases , and exploring decoding paths adopted by other MT systems , this framework achieves better translation quality with much less re - decoding time .","label":"Background","metadata":{},"score":"37.480274"}{"text":"We introduce a general framework for answer extraction which exploits semantic role annotations in the FrameNet paradigm .We view semantic role assignment as an optimization problem in a bipartite graph and answer extraction as an instance of graph matching .Experimental results on the TREC datasets demonstrate improvements over state - of - the - art models .","label":"Background","metadata":{},"score":"37.98265"}{"text":"This article documents a large set of heretofore unpublished details Collins used in his parser , such that , along with Collins ' ( 1999 ) thesis , this article contains all information necessary to duplicate Collins ' benchmark results .Indeed , these as - yet - unpublished details account for an 11 % relative increase in error from an implementation including all details to a clean - room implementation of Collins ' model .","label":"Background","metadata":{},"score":"38.008316"}{"text":"We also give an overview of the parsing approaches that participants took and the results that they achieved .Finally , we try to draw general conclusions about multi - lingual parsing : What makes a particular language , treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser ?","label":"Background","metadata":{},"score":"38.119984"}{"text":"We present a comparative error analysis of the two dominant approaches in data - driven dependency parsing : global , exhaustive , graph - based models , and local , greedy , transition - based models .We show that , in spite of similar performance overall , the two models produce different types of errors , in a way that can be explained by theoretical properties of the two models .","label":"Background","metadata":{},"score":"38.169945"}{"text":"In parsing we would have training examples fs i ; t i g where e .. \" ...This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm .We show how the algorithms can be efficiently applied to exponential sized representations of parse trees , such as the \" all subtrees \" ( DOP ) representation described by ( Bod 9 ... \" .","label":"Background","metadata":{},"score":"38.229225"}{"text":"To demonstrate the efficiency , scalability and accuracy of these algorithms , we present experiments on Bikel 's i ... \" .We discuss the relevance of k - best parsing to recent applications in natural language processing , and develop efficient algorithms for k - best trees in the framework of hypergraph parsing .","label":"Background","metadata":{},"score":"38.312763"}{"text":"Our results provide the first known empirical evidence that lexical semantics are indeed useful for SMT , despite claims to the contrary .This paper presents a tree - to - tree transduction method for text rewriting .Our model is based on synchronous tree substitution grammar , a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches .","label":"Background","metadata":{},"score":"38.368553"}{"text":"Our formulation uses a factorization analogous to the standard dynamic programs for parsing .In particular , it allows one to efficiently learn a model which discriminates ... \" .We present a novel discriminative approach to parsing inspired by the large - margin criterion underlying support vector machines .","label":"Background","metadata":{},"score":"38.36987"}{"text":"We describe new lookup algorithms for hierarchical phrase - based translation that reduce the empirical computation time by nearly two orders of magnitude , making on - the - fly lookup feasible for source phrases with gaps .This paper presents an empirical study on how different selections of input translation systems affect translation quality in system combination .","label":"Background","metadata":{},"score":"38.37066"}{"text":"The strength of our approach is that it allows a tree to be represented as an arbitrary set of features , without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account .","label":"Background","metadata":{},"score":"38.768726"}{"text":"However , the use of forest and vector set make slightly slower the new version so if you just need a fast computation of one tree kernel please use SVM - LIGHT - TK1.0 .This article considers approaches which rerank the output of an existing probabilistic parser .","label":"Background","metadata":{},"score":"39.055016"}{"text":".. or a temporal .This statistical technique of labeling predicate argument operates on the output of the probabilistic parser reported ... . \" ...This article documents a large set of heretofore unpublished details Collins used in his parser , such that , along with Collins ' ( 1999 ) thesis , this article contains all information necessary to duplicate Collins ' benchmark results .","label":"Background","metadata":{},"score":"39.219128"}{"text":"This paper presents a novel unsupervised support vector machine ( U - SVM ) classifier for answer selection , which is independent of language and does not require hand - tagged training pairs .The key ideas are the following : 1 . unsupervised learning of training data for the classifier by clustering web search results ; and 2 . selecting the answer from the candidates by classifying the question .","label":"Background","metadata":{},"score":"39.38218"}{"text":"A key idea is to introduce non - standard CCG combinators that relax certain parts of the grammar --- for example allowing flexible word order , or insertion of lexical items --- with learned costs .We also present a new , online algorithm for inducing a weighted CCG .","label":"Background","metadata":{},"score":"39.39405"}{"text":"Deterministic parsing has emerged as an effective alternative for complex parsing algorithms which search the entire search space to get the best probable parse tree .In this paper , we present an online large margin based training framework for deterministic parsing using Nivre 's Shift - Reduce parsing algorithm .","label":"Background","metadata":{},"score":"39.418034"}{"text":"We show how a kernel over trees can be applied to parsing using the voted perceptron algorithm , and we give experimental results on the ATIS corpus of parse trees . ... ion from ranking problems to a margin - based classification problem in [ 8].","label":"Background","metadata":{},"score":"39.525383"}{"text":"This paper describes a corpus - based study of plural descriptions , and proposes a psycholinguistically - motivated algorithm for plural reference generation .The descriptive strategy is based on partitioning , and incorporates corpus - derived heuristics .An exhaustive evaluation shows that the output closely matches human data .","label":"Background","metadata":{},"score":"39.74197"}{"text":"Our approach recovers non - local dependencies at the level of Lexical - Functional Grammar f - structures , using automatically acquired subcategorisation frames and f - structure paths linking antecedents and traces in NLDs .Currently our algorithm achieves 92.2 % f - score for trace insertion and 84.3 % for antecedent recovery evaluating on gold - standard CTB trees , and 64.7 % and 54.7 % , respectively , on CTBtrained state - of - the - art parser output trees .","label":"Background","metadata":{},"score":"39.844894"}{"text":"Collins ( 2000 ) uses a technique based on boosting algorithms for machine learning that reranks n - best output from model 2 in t .. \" ...The problem of combining preferences arises in several applications , such as combining the results of different search engines .","label":"Background","metadata":{},"score":"39.84643"}{"text":"( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .","label":"Background","metadata":{},"score":"40.01981"}{"text":"( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .","label":"Background","metadata":{},"score":"40.01981"}{"text":"In these results , the generative model performs significantly better than the others , and does about equally well at assigning part - of - speech tags . \" ...Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .","label":"Background","metadata":{},"score":"40.271164"}{"text":"Our results suggest that our bootstrapping methods have considerable potential , and could be used to semi - automate an approach based on incremental manual annotation .In this paper , we consider the computational modelling of human plausibility judgements for verb - relation - argument triples , a task equivalent to the computation of selectional preferences .","label":"Background","metadata":{},"score":"40.434906"}{"text":"In this paper we address the problem of multiple citation concept alignment by combining and modifying the CRF based pairwise word alignment system of Blunsom & Cohn ( 2006 ) and a posterior decoding based multiple sequence alignment algorithm of Schwartz & Pachter ( 2007 ) .","label":"Background","metadata":{},"score":"40.533283"}{"text":"In common with other approaches to sequence modeling using perceptrons , and in contrast with comparable generative models , this model permits and transparently exploits arbitrary features of input strings .The simplicity of perceptron training lends more versatility than comparable approaches , allowing the model to be applied to a variety of problem types for which a learned edit model might be useful .","label":"Background","metadata":{},"score":"40.551575"}{"text":"We treat the graph as a Markov chain and compute a word - specific stationary distribution via a generalized PageRank algorithm .Semantic relatedness of a word pair is scored by a novel divergence measure , ZKL , that outperforms existing measures on certain classes of distributions .","label":"Background","metadata":{},"score":"40.62567"}{"text":"We introduce an approximate inference method using Tree - based Reparameterization ( TRP ) to reduce computational cost .In experiments , our proposed model obtained significant improvements compare to baseline models that use Support Vector Machines .We introduce a technique for identifying the most salient participants in a discussion .","label":"Background","metadata":{},"score":"40.67905"}{"text":"We present experiments with a dependency parsing model defined on rich factors .Our model represents dependency trees with factors that include three types of relations between the tokens of a dependency and their children .We extend the projective parsing algorithm of Eisner ( 1996 ) for our case , and train models using the averaged perceptron .","label":"Background","metadata":{},"score":"40.786987"}{"text":"Three different classifiers are trained to predict weighted soft - constraints on parts of the complex output .From these constraints , a standard weighted constraint satisfaction problem can be formed , the solution to which is a valid dependency tree .","label":"Background","metadata":{},"score":"40.787724"}{"text":"However , these methods fail when the correct output is pruned away in the first pass .Closest to our proposal are gradient - descent methods that adjust the parameters of all of the local classifiers ... . ... ns , or quotation marks ) and have the same label 15 as a constituent in the treebank parse .","label":"Background","metadata":{},"score":"40.816025"}{"text":"A more complete description of the FrameNet project can be found in Baker , Fillmore , and Lowe ( 1998 ) and Johnson et al .( 2002 ) , and the ramifications for automatic classification are discussed more ... . \" ...","label":"Background","metadata":{},"score":"40.995483"}{"text":"Finally , we show a qualitative evaluation of the results of automatically adding extracted MWEs to existing linguistic resources .We argue that such a process improves qualitatively , if a more compositional approach to grammar / lexicon automated extension is adopted .","label":"Background","metadata":{},"score":"41.010445"}{"text":"This paper focuses on the domain estimation problem in statistical machine translations .In the proposed method , a training corpus , which is a bilingual corpus , is automatically clustered to sub - corpuses .Each sub - corpus is regarded as a domain .","label":"Background","metadata":{},"score":"41.03024"}{"text":"This paper presents a new approach to phrase - level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions .With this approach , the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions , achieving results that are significantly better than baseline . ...","label":"Background","metadata":{},"score":"41.085526"}{"text":"However , previous approaches ignore this dependency .We propose a novel approach for this task , namely training Semi Markov models discriminatively using a Max - Margin method .This method allows us to model the sequence dependency of the problem and to incorporate properties of a whole paragraph , such as coherence , which can not be used in previous methods .","label":"Background","metadata":{},"score":"41.24193"}{"text":"An adaptation technique is used to avoid this problem .The second problem is domain estimation .For adaptation , the domain must be given in advance .However , in many cases , the domain is not given or changes dynamically .","label":"Background","metadata":{},"score":"41.308437"}{"text":"Significant improvement over the previous results in the literature is reported as well as a new benchmark dataset is introduced .Semi - supervised algorithms perform better than their supervised version by a wide margin especially when the amount of labeled data is limited .","label":"Background","metadata":{},"score":"41.44297"}{"text":"We also address the issue whether a corpus annotated by means of AL -- using a particular classifier and a particular feature set -- can be re - used to train classifiers different from the ones employed by AL , supplying alternative feature sets as well .","label":"Background","metadata":{},"score":"41.45234"}{"text":"In the extreme case where there is so much tr ... .This represents a 13 % decrease in error rate over the best single - parser results on this corpus [ 9].The major technical innova- tion is the use of a \" maximum - entropy - inspired \" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events .","label":"Background","metadata":{},"score":"41.61622"}{"text":"The evaluation showed that PDMM is more effective than PMM .We address the problem of smoothing translation probabilities in a bilingual N - gram - based statistical machine translation system .It is proposed to project the bilingual tuples onto a continuous space and to estimate the translation probabilities in this representation .","label":"Background","metadata":{},"score":"41.63614"}{"text":"We achieve average results , which is partly due to difficulties in mapping to the dependency representation used for the shared task .Following ( Blitzer et al . , 2006 ) , we present an application of structural correspondence learning to non - projective dependency parsing ( McDonald et al . , 2005 ) .","label":"Background","metadata":{},"score":"41.678757"}{"text":"We define the objective function of our hybrid model , which is written in log - linear form , by discriminatively combining discriminative structured predictor(s ) with generative model(s ) that incorporate unlabeled data .Then , unlabeled data is used in a generative manner to increase the sum of the discriminant functions for all outputs during the parameter estimation .","label":"Background","metadata":{},"score":"41.89705"}{"text":"The idea of behind syntactic tree kernels is graphically described by the following figure : .As 3 structures ( out of 5 ) are completely identical the similarity is equal to 3 .This kind of similarity has been shown useful to improve the ranking of the m best syntactic parse trees [ Collins and Duffy , 2002].","label":"Background","metadata":{},"score":"41.926918"}{"text":"Our error analysis for this task suggests that the primary source of error are differences in annotation guidelines among treebanks .Our suspicions are supported by the observation that no team was able to improve target domain performance substantially over a state of the art baseline .","label":"Background","metadata":{},"score":"42.138527"}{"text":"We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process ( HDP ) .Our HDP - PCFG model allows the complexity of the grammar to grow as more training data is available .In addition to presenting a fully Bayesian model for the PCFG , we also develop an efficient variational inference procedure .","label":"Background","metadata":{},"score":"42.15949"}{"text":"We analyze the effect of resampling techniques , including under - sampling and over - sampling used in active learning .Experimental results show that under - sampling causes negative effects on active learning , but over - sampling is a relatively good choice .","label":"Background","metadata":{},"score":"42.166107"}{"text":"This limits the sparseness of the kernels applied to the whole tree .Given two objects , O 1 and O 2 , described by two sets of trees , and , several kernels can be defined on : . -Two types of tree kernels : .","label":"Background","metadata":{},"score":"42.39691"}{"text":"In the domain adaptation track , we use two models to parse unlabeled data in the target domain to supplement the labeled out - of - domain training set , in a scheme similar to one iteration of co - training .","label":"Background","metadata":{},"score":"42.674423"}{"text":"We integrate these probabilities into the framework of fully - lexicalized parsing based on large - scale case frames .This approach simultaneously addresses two tasks of coordination disambiguation : the detection of coordinate conjunctions and the scope disambiguation of coordinate structures .","label":"Background","metadata":{},"score":"42.746277"}{"text":"The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence .","label":"Background","metadata":{},"score":"43.132877"}{"text":"When tested on a corpus of Wikipedia articles , our hierarchically informed model predicts the correct insertion paragraph more accurately than baseline methods .In this paper we consider the problem of automatically identifying the arguments of discourse connectives ( e.g. and , because , nevertheless ) in the Penn Discourse TreeBank(PDTB ) .","label":"Background","metadata":{},"score":"43.14773"}{"text":"We find that the use of a decision tree improves on the basic approach only for the deep parser - based approach .We also show that combining both the shallow and deep decision tree features is effective .Our evaluation is carried out using a large test set of grammatical and ungrammatical sentences .","label":"Background","metadata":{},"score":"43.215935"}{"text":"For our models and training sets , more peaked measures of confidence , measured by Renyi entropy , outperformed smoother ones .We discuss how our feature set could be extended with cross - lingual or cross - domain features , to incorporate knowledge from parallel or comparable corpora during bootstrapping .","label":"Background","metadata":{},"score":"43.228683"}{"text":"The increasing use of large open - domain document sources is exacerbating the problem of ambiguity in named entities .This paper explores the use of a range of syntactic and semantic features in unsupervised clustering of documents that result from ad hoc queries containing names .","label":"Background","metadata":{},"score":"43.230904"}{"text":"Our best result , 91.44 % accuracy , reflects a 25 % reduction in error rate compared with the previous state of the art .We present a new approach to automatic summarization based on neural nets , called NetSum .We extract a set of features from each sentence that helps identify its importance in the document .","label":"Background","metadata":{},"score":"43.373413"}{"text":"Tree - based modeling still lacks many of the standard tools taken for granted in ( finite - state ) string - based modeling .The theory of tree transducer automata provides a possible framework to draw on , as it has been worked out in an extensive literature .","label":"Background","metadata":{},"score":"43.377357"}{"text":"We demonstrate significant gains using features derived from a dependency parse representation over those derived from a constituency - based tree parse .By also capturing inter - argument dependencies using a log - linear re - ranking model we achieve very promising results on this difficult task identifying both arguments correctly for over 74 % of the connectives on held - out test data using gold - standard parses .","label":"Background","metadata":{},"score":"43.423653"}{"text":"We present a novel generative model for natural language tree structures in which semantic ( lexical dependency ) and syntactic ( PCFG ) structures are scored with separate models .This factorization provides conceptual simplicity , straightforward opportunities for separately improving the component mod ... \" .","label":"Background","metadata":{},"score":"43.44906"}{"text":"We also point out the high variance in all of these estimators , and that they require many more iterations to approach convergence than usually thought .This paper describes a probabilistic model for coordination disambiguation integrated into syntactic and case structure analysis .","label":"Background","metadata":{},"score":"43.60452"}{"text":"The first stage consists in tuning a single - parser system for each language by optimizing parameters of the parsing algorithm , the feature model , and the learning algorithm .The second stage consists in building an ensemble system that combines six different parsing strategies , extrapolating from the optimal parameters settings for each language .","label":"Background","metadata":{},"score":"43.658394"}{"text":"Finally , we investigate when to stop active learning , and adopt two strategies , max - confidence and min - error , as stopping conditions for active learning .According to experimental results , we suggest a prediction solution by considering max - confidence as the upper bound and min - error as the lower bound of stopping conditions .","label":"Background","metadata":{},"score":"43.66774"}{"text":"Koo , Terry ; Carreras PÃ©rez , Xavier ; Collins , Michael ( 2008 )Conference report Open Access .We present a simple and effective semisupervised method for training dependency parsers .We focus on the problem of lexical representation , introducing features that incorporate word clusters derived from a large unannotated ... .","label":"Background","metadata":{},"score":"43.685913"}{"text":"The system has been used to parse many well known corpora in order to produce data for lexical acquisition efforts ; it has also been used as a component in an open - domain question answering project .The performance of the system is competitive with that of statistical parsers using highly lexicalised parse selection models .","label":"Background","metadata":{},"score":"43.75949"}{"text":"This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution .We investigate Gibbs Sampling ( GS ) and Variational Bayes ( VB ) estimators and show that VB converges faster than GS for this task and that VB significantly improves 1-to-1 tagging accuracy over EM .","label":"Background","metadata":{},"score":"43.825455"}{"text":"We apply the proposed approach to enhance opinion sum - marization in a two - stage framework .Experimental results show that the proposed approach effectively ( 1 ) discriminates low - quality reviews from high - quality ones and ( 2 ) enhances the task of opinion summarization by detecting and filtering low - quality reviews .","label":"Background","metadata":{},"score":"43.915108"}{"text":"Parser actions are determined by a classifier , based on features that represent the current state of the parser .We apply this parsing framework to both tracks of the CoNLL 2007 shared task , in each case taking advantage of multiple models trained with different learners .","label":"Background","metadata":{},"score":"43.998093"}{"text":"In phrase - based models , this problem can be addressed by storing the training data in memory and using a suffix array as an efficient index to quickly lookup and extract rules on the fly .Hierarchical phrase - based translation introduces the added wrinkle of source phrases with gaps .","label":"Background","metadata":{},"score":"44.03633"}{"text":"The parser first identifies dependencies using a discriminative classifier and then labels those dependencies as a sequence labeling problem .The features for two stages are proposed .For four languages have different values of ROOT , we design some special features for ROOT labeler .","label":"Background","metadata":{},"score":"44.05493"}{"text":"Based on an extension to Harris 's distributional hypothesis , we use selectional preferences to gather evidence of inference directionality and plausibility .Experiments show empirical evidence that our approach can classify inference rules significantly better than several baselines .This paper assesses the role of multi - label classification in modelling polysemy for language acquisition tasks .","label":"Background","metadata":{},"score":"44.08834"}{"text":"Recent efforts have tried to overcome this issue by using statistics from speech lattices instead of only the 1-best transcripts ; however , these efforts have invariably used the classical vector space retrieval model .This paper presents a novel approach to lattice - based spoken document retrieval using statistical language models : a statistical model is estimated for each document , and probabilities derived from the document models are directly used to measure relevance .","label":"Background","metadata":{},"score":"44.259193"}{"text":"We not only analyze the effect of the unpublished details , but also reanalyze the effect of certain well - known details , revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought .","label":"Background","metadata":{},"score":"44.547295"}{"text":"We describe the application of kernel methods to Natural Language Processing ( NLP ) problems .In many NLP tasks the objects being modeled are strings , trees , graphs or other discrete structures which require some mechanism to convert them into feature vectors .","label":"Background","metadata":{},"score":"44.56481"}{"text":"We describe the application of kernel methods to Natural Language Processing ( NLP ) problems .In many NLP tasks the objects being modeled are strings , trees , graphs or other discrete structures which require some mechanism to convert them into feature vectors .","label":"Background","metadata":{},"score":"44.56481"}{"text":"The results were achieved by using only information about heads and daughters as features to guide the parser which obeys strict incrementality .A memory - based learner was used to predict the next action of the parser .This paper presents an online algorithm for dependency parsing problems .","label":"Background","metadata":{},"score":"44.59909"}{"text":"They are important for a few reasons .First , at present the best performing parsers on the WSJ treebank ( Ratnaparkhi 1997 ; Charniak 1997 , 1999 ; Collins 1997 , 1999 ) are all cases of history - based mo .. \" ...","label":"Background","metadata":{},"score":"44.6325"}{"text":"Furthermore , the combination of parse trees can compensate for the reordering errors caused by single parse tree .Finally , experimental results show that the performance of our system is superior to that of the state - of - the - art phrase - based SMT system .","label":"Background","metadata":{},"score":"44.667522"}{"text":"It is based on : ( 1 ) an extended set of features ; and ( ... \" .In this paper we present a novel , customizable IE paradigm that takes advantage of predicate - argument structures .We also introduce a new way of automatically identifying predicate argument structures , which is central to our IE paradigm .","label":"Background","metadata":{},"score":"44.746162"}{"text":"We present an information extraction system that decouples the tasks of finding relevant regions of text and applying extraction patterns .We create a self - trained relevant sentence classifier to identify relevant regions , and use a semantic affinity measure to automatically learn domain - relevant extraction patterns .","label":"Background","metadata":{},"score":"44.90021"}{"text":"We present two techniques for training the MST parser : tree - normalized and graph - normalized conditional training .We describe the reranker features which include non - projective edge attributes .We provide an analysis of the errors made by our system and suggest changes to the models and features that might rectify the current system .","label":"Background","metadata":{},"score":"44.9249"}{"text":"This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension .The paper proposes a simple informationtheoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel , incremental , probabi ... \" .","label":"Background","metadata":{},"score":"44.98777"}{"text":"Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .In co ... \" .We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .","label":"Background","metadata":{},"score":"45.04017"}{"text":"This paper discusses the statistical theory underlying various parameter - estimation methods , and gives algorithms which depend on alternatives to ( smoothed ) maximumlikelihood estimation .We first give an overview of results from statistical learning theory .We then show how important concepts from the classification literature -- specifically , generalization results based on margins on training data -- can be derived for parsing models .","label":"Background","metadata":{},"score":"45.07837"}{"text":"Vector sets , multiple feature vectors over multiple feature spaces can be specified in the input .This allows us to use different kernels with different feature subsets .Given two objects , O 1 and O 2 , described by two sets of feature vectors , and , several kernels can be defined : . - Tree forests , a set of trees over multiple feature spaces can be specified in the input .","label":"Background","metadata":{},"score":"45.101402"}{"text":"In particular , we demonstrated in Petrov et al .( 2006 ) that a hierarchically split PCFG could exceed the accuracy of lexic ... . by Ben Taskar , Dan Klein , Michael Collins , Daphne Koller , Christopher Manning - In Proceedings of EMNLP , 2004 . \" ...","label":"Background","metadata":{},"score":"45.191914"}{"text":"Most of these algorithms avoid the known hardness results by defining parameters beyond the ... .Koo , Terry ; Globerson , Amir ; Carreras PÃ©rez , Xavier ; Collins , Michael ( 2007 ) Conference report Open Access .This paper provides an algorithmic framework for learning statistical models involving directed spanning trees , or equivalently non - projective dependency structures .","label":"Background","metadata":{},"score":"45.20553"}{"text":"Our approach is based on the analysis of the paths between two protein names in the dependency parse trees of the sentences .Given two dependency trees , we define two separate similarity functions ( kernels ) based on cosine similarity and edit distance among the paths between the protein names .","label":"Background","metadata":{},"score":"45.41426"}{"text":"The online method adapts the translation model by redistributing the weight of each predefined submodels .Information retrieval model is used for the weighting scheme in both methods .Experimental results show that without using any additional resource , both methods can improve SMT performance significantly .","label":"Background","metadata":{},"score":"45.52437"}{"text":"The deep processing approach uses the XLE LFG parser and English grammar : two versions are presented , one which uses the XLE directly to perform the classification , and another one which uses a decision tree trained on features consisting of the XLE 's output statistics .","label":"Background","metadata":{},"score":"45.658905"}{"text":"The paper reports a hybridization experiment , where an existing ML dependency parser ( LingPars ) , was allowed access to Constraint Grammar analyses provided by a rule - based parser ( EngGram ) for the same data .Descriptive compatibility issues and their influence on performance are discussed , such as tokenization problems , category bundling and dependency head conventions .","label":"Background","metadata":{},"score":"45.80593"}{"text":"Carreras PÃ©rez , Xavier ; Collins , Michael ; Koo , Terry ( Coling 2008 Organizing Committee , 2008 )Conference report Open Access .We describe a parsing approach that makes use of the perceptron algorithm , in conjunction with dynamic programming methods , to recover full constituent - based parse trees .","label":"Background","metadata":{},"score":"45.86608"}{"text":"Subtree kernel ( ST ) [ Vishwanathan and Smola , 2001 ; Moschitti , EACL 2006 ] . - Embedded combinations of Trees and vectors : . sequential summation , the kernels between corresponding pairs of trees and/or vectors in the input sequence are summed together .","label":"Background","metadata":{},"score":"45.984787"}{"text":"In this study we show that analogical learning offers as well an elegant and effective solution to the problem of identifying potential translations of unknown words .We present a probabilistic model of diachronic phonology in which individual word forms undergo stochastic edits along the branches of a phylogenetic tree .","label":"Background","metadata":{},"score":"46.026047"}{"text":"We present a detailed case study of this learni ... \" .this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance .","label":"Background","metadata":{},"score":"46.12861"}{"text":"We report statistics on TEXTRUNNER 's 11,000,000 highest probability tuples , and show that they contain over 1,000,000 concrete facts and over 6,500,000 more abstract assertions . \" ...We present several improvements to unlexicalized parsing with hierarchically state - split PCFGs .","label":"Background","metadata":{},"score":"46.137897"}{"text":"A fundamental problem in statistical parsing is the choice of criteria and algorithms used to estimate the parameters in a model .The predominant approach in computational linguistics has been to use a parametric model with some variant of maximum - likelihood estimation .","label":"Background","metadata":{},"score":"46.288723"}{"text":"A fundamental problem in statistical parsing is the choice of criteria and algorithms used to estimate the parameters in a model .The predominant approach in computational linguistics has been to use a parametric model with some variant of maximum - likelihood estimation .","label":"Background","metadata":{},"score":"46.288723"}{"text":"The issues of consistency of argument structure across both polysemous and synonymous verbs are also discussed and we present our actual guidelines for these types of phenomena , along with numerous examples of tagged sentences and verb frames .We conclude with a summary of the current status of annotation process .","label":"Background","metadata":{},"score":"46.318985"}{"text":"Syntactic parsers are among of the most useful tools for Natural Language Processing ( NLP ) applications .However , how to exploit the syntactic parse tree information in NLP tasks is considered an open problem .For example , the learning models for automatic Word Sense Disambiguation or Coreference Resolution would benefit from syntactic tree features but their design and selection is not an easy task .","label":"Background","metadata":{},"score":"46.34784"}{"text":"We describe our submission to the domain adaptation track of the CoNLL07 shared task in the open class for systems using external resources .Our main finding was that it was very difficult to map from the annotation scheme used to prepare training and development data to one that could be used to effectively train and adapt the RASP system unlexicalised parse ranking model .","label":"Background","metadata":{},"score":"46.45278"}{"text":"We describe kernels for various natural ... \" .We describe the application of kernel methods to Natural Language Processing ( NLP ) problems .In many NLP tasks the objects being modeled are strings , trees , graphs or other discrete structures which require some mechanism to convert them into feature vectors .","label":"Background","metadata":{},"score":"46.61731"}{"text":"The disambiguation component employs a vector space model and a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document , as well as the agreement among the category tags associated with the candidate entities .","label":"Background","metadata":{},"score":"46.635468"}{"text":"We extract effective expressions from the important segments to define various viewpoints .In text mining a viewpoint defines the important associations between key entities and it is crucial that the correct viewpoints are identified .We show the effectiveness of the method by using real datasets from a car rental service center .","label":"Background","metadata":{},"score":"46.697067"}{"text":"The input format and the new options are compatible with those of the original SVM - light 5.01 .Moreover , combination models between tree kernels and feature vectors are available .Software Features .-Fast Kernel Computation [ Moschitti , EACL 2006 ] ( already available since the previous version ) .","label":"Background","metadata":{},"score":"46.800095"}{"text":"Three binary linear classifiers were trained to predict the existence of a preposition , etc , on unlabeled data and we used singular value decomposition to induce new features .During the training , the parser was trained with these additional features in addition to these described in ( McDonald et al . , 2005 ) .","label":"Background","metadata":{},"score":"46.8273"}{"text":"We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation .We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result .","label":"Background","metadata":{},"score":"46.96408"}{"text":"This proposal subsumes and clarifies findings that high - constraint contexts can facilitate lexical processing , and connects these findings to well - known models of parallel constraint - based comprehension .In addition , the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension , including the reversal of locality - based difficulty patterns in syntactically constrained contexts , and conditions under which increased ambiguity facilitates processing .","label":"Background","metadata":{},"score":"47.126385"}{"text":"While recent progress in machine learning has mainly focused on designing flexible and powerful input representations , this paper addresses the complementary ... \" .Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence .","label":"Background","metadata":{},"score":"47.25013"}{"text":"The first method we discuss is based on a feature selection me ... . by Michael Collins , Nigel Duffy - Advances in Neural Information Processing Systems 14 , 2001 . \" ...We describe the application of kernel methods to Natural Language Processing ( NLP ) problems .","label":"Background","metadata":{},"score":"47.326744"}{"text":"Rather than using syntactic features to augment existing statistical classifiers ( as in previous work ) , we build on the idea that questions and their ( correct ) answers relate to each other via loose but predictable syntactic transformations .We propose a probabilistic quasi - synchronous grammar , inspired by one proposed for machine translation ( D. Smith and Eisner , 2006 ) , and parameterized by mixtures of a robust non - lexical syntax / alignment model with a(n optional ) lexical - semantics - driven log - linear model .","label":"Background","metadata":{},"score":"47.464035"}{"text":"We present a maximum - likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently , using as examples several problems in natural language processing . \" ... this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .","label":"Background","metadata":{},"score":"47.51069"}{"text":"To obtain the sequential summation K s ( of tree and vector kernels ) previously defined , we can set the option \" -t 5 -T 1 -W S -V S -C + \" .Considering the default values , this is equivalent to use \" -t 5 -C + \" .","label":"Background","metadata":{},"score":"47.51291"}{"text":"In deterministic approaches to this task , dependency trees are constructed by series of actions of attaching a bunsetsu chunk to one of the nodes in the tree being constructed .Conventional techniques select the node based on whether the new bunsetsu chunk and each node in the trees are in a parent - child relation or not .","label":"Background","metadata":{},"score":"47.586678"}{"text":"Collins , Michael ; Globerson , Amir ; Koo , Terry ; Carreras PÃ©rez , Xavier ; Bartlett , Peter ( 2008 - 08 )Article Open Access .Log - linear and maximum - margin models are two commonly - used methods in supervised machine learning , and are frequently used in structured prediction problems .","label":"Background","metadata":{},"score":"47.650185"}{"text":"Therefore , we use ancestor - descendant relation in addition to parent - child relation , so that the added redundancy helps errors be corrected .Experimental results show that the proposed method achieves higher accuracy .We propose a sequence - alignment based method for detecting and disambiguating coordinate conjunctions .","label":"Background","metadata":{},"score":"47.779434"}{"text":"The second approach combines unsupervised hidden markov modelling with language models .Empirical evaluation of both systems pointed out that the hidden markov model managed best to learn the task of segmenting and labelling biological field book entries from a derived database only .","label":"Background","metadata":{},"score":"47.87429"}{"text":"Its space requirements fall significantly below lossless information - theoretic lower bounds but it produces false positives with some quantifiable probability .Here we present a general framework for deriving smoothed language model probabilities from BFs .We investigate how a BF containing n -gram statistics can be used as a direct replacement for a conventional n -gram model .","label":"Background","metadata":{},"score":"47.88053"}{"text":"Using the WordNet hierarchy , we embed the construction of Abney and Light in the topic model and show that automatically learned domains improve WSD accuracy compared to alternative contexts .This paper focuses on the evaluation of methods for the automatic acquisition of Multiword Expressions ( MWEs ) for robust grammar engineering .","label":"Background","metadata":{},"score":"47.902176"}{"text":"Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .They only determine parsing actions stepwisely by a trained classifier .To globally model parsing actions of all steps that are taken on the input sentence , we propose two kinds of probabilistic parsing action models that can compute the probability of the whole dependency tree .","label":"Background","metadata":{},"score":"47.97735"}{"text":"It is well - known that domain specific language models perform well in automatic speech recognition .Domain specific language and translation models in statistical machine translations perform well .However , there are two problems with using domain specific models .","label":"Background","metadata":{},"score":"48.07103"}{"text":"We briefly describe each model , highlighting points where they differ .We include a quantitative comparison of the phrase pairs that each model has to work with , as well as the reasons why some phrase pairs are not learned by the syntax - based model .","label":"Background","metadata":{},"score":"48.111828"}{"text":"We propose ( a ) a lexical affinity model where words struggle to modify each other , ( b ) a sense tagging model where words fluctuate randomly in their selectional prefe ... \" .After presenting a novel O(nÂ³ ) parsing algorithm for dependency grammar , we develop three contrasting ways to stochasticize it .","label":"Background","metadata":{},"score":"48.198696"}{"text":"To characterise the arguments in a given grammatical relationship we experiment with three models of selectional preference .Two use WordNet and one uses the entries from a distributional thesaurus as classes for representation .In previous work on selectional preference acquisition , the classes used for representation are selected according to the coverage of argument tokens rather than being selected according to the coverage of argument types .","label":"Background","metadata":{},"score":"48.271797"}{"text":"Key enablers of this high performance are features derived from previous natural language processing work in noun compound bracketing .For example , token association features beyond simple N - gram counts provide powerful indicators of segmentation .We present two machine learning approaches to information extraction from semi - structured documents that can be used if no annotated training data are available , but there does exist a database filled with information derived from the type of documents to be processed .","label":"Background","metadata":{},"score":"48.287254"}{"text":"We also compare the translation accuracy for all variations .We achieved a state of the art performance in statistical machine translation by using a large number of features with an online large - margin training algorithm .The millions of parameters were tuned only on a small development set consisting of less than 1 K sentences .","label":"Background","metadata":{},"score":"48.407143"}{"text":"DeSR implements an incremental deterministic Shift / Reduce parsing algorithm , using specific rules to handle non - projective dependencies .For the multilingual track we adopted a second order averaged perceptron and performed feature selection to tune a feature model for each language .","label":"Background","metadata":{},"score":"48.538857"}{"text":"In contrast with previous work , we are able to split various terminals to different degrees , as appropriate to the actual complexity in the data .Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .","label":"Background","metadata":{},"score":"48.612278"}{"text":"We address the problem of training the free parameters of a statistical machine translation system .We show significant improvements over a state - of - the - art minimum error rate training baseline on a large Chinese - English translation task .","label":"Background","metadata":{},"score":"48.638947"}{"text":"The Penn Treebank has recently implemented a new syntactic annotation scheme , designed to highlight aspects of predicate - argument structure .This paper discusses the implementation of crucial aspects of this new annotation scheme .It incorporates a more consistent treatment of a wide range of gramma ... \" .","label":"Background","metadata":{},"score":"48.659775"}{"text":"HashTBO made it possible to ship a trigram contextual speller in Microsoft Office 2007 .In morphologically rich languages , should morphological and syntactic disambiguation be treated sequentially or as a single problem ?We describe several efficient , probabilistically - interpretable ways to apply joint inference to morphological and syntactic disambiguation using lattice parsing .","label":"Background","metadata":{},"score":"48.702766"}{"text":"Furthermore , we explore the performance of information drawn from different levels of linguistic description , using feature sets based on morphology , syntax , semantics , and n -gram distribution .Finally , we demonstrate that ensemble classifiers are a powerful and adequate way to combine different types of linguistic evidence : a simple , majority voting ensemble classifier improves the accuracy from 62.5 % ( best single classifier ) to 84 % .","label":"Background","metadata":{},"score":"48.70371"}{"text":"Standard thesaurus - based measures of word pair similarity are based on only a single path between those words in the thesaurus graph .By contrast , we propose a new model of lexical semantic relatedness that incorporates information from every explicit or implicit path connecting the two words in the entire graph .","label":"Background","metadata":{},"score":"48.812195"}{"text":"We present a new generative alignment model which avoids these structural limitations , and show that it is effective when trained using both unsupervised and semi - supervised training methods .Experiments show strong improvements in word alignment accuracy and usage of the generated alignments in hierarchical and phrasal SMT systems increases the BLEU score .","label":"Background","metadata":{},"score":"48.884953"}{"text":"Each of the two types of kernels can or can not be normalized according to a command line parameter .More formally : .K t can be either the SST or the ST kernel whereas k b can be one of the traditional kernels on feature vectors , e.g. gaussian or polynomial kernel .","label":"Background","metadata":{},"score":"48.93291"}{"text":"By extending a recent model , we obtain a completely corpus - driven model for this task which achieves significant correlations with human judgements .It rivals or exceeds deeper , resource - driven models while exhibiting higher coverage .Moreover , we show that our model can be combined with deeper models to obtain better predictions than from either model alone .","label":"Background","metadata":{},"score":"49.166664"}{"text":"We present the idea of estimating semantic distance in one , possibly resource - poor , language using a knowledge source in another , possibly resource - rich , language .We do so by creating cross - lingual distributional profiles of concepts , using a bilingual lexicon and a bootstrapping algorithm , but without the use of any sense - annotated data or word - aligned corpora .","label":"Background","metadata":{},"score":"49.271454"}{"text":"After motivating the need for explicit predicate argument structure labels , we briefly discuss the theoretical considerations of predicate argument structure and the need to maintain consistency across syntactic alternations .The issues of consistency of argument structure across both polysemous and synonymous verbs are also discussed and we present our actual guidelines for these types of phenomena , along with numerous examples of tagged sentences and verb frames .","label":"Background","metadata":{},"score":"49.29718"}{"text":"This paper addresses the problem of detecting low - quality product reviews .Three types of biases in the existing evaluation standard of product reviews are discovered .To assess the quality of product reviews , a set of specifications for judging the quality of reviews is first defined .","label":"Background","metadata":{},"score":"49.44729"}{"text":"We first give a formal framework for the problem .We then describe and analyze a new boosting ... \" .The problem of combining preferences arises in several applications , such as combining the results of different search engines .This work describes an efficient algorithm for combining multiple preferences .","label":"Background","metadata":{},"score":"49.714252"}{"text":"Our model is inspired by theories of local coherence and formulated within the framework of Integer Linear Programming .Experimental results show significant improvements over a state - of - the - art discourse agnostic approach .Shallow semantic parsing , the automatic identification and labeling of sentential constituents , has recently received much attention .","label":"Background","metadata":{},"score":"49.744434"}{"text":"We report on experiments over a 9,000,000 Web page corpus that compare TEXTRUNNER with KNOWITALL , a state - of - the - art Web IE system .TEXTRUNNER achieves an error reduction of 33 % on a comparable set of extractions .","label":"Background","metadata":{},"score":"49.7604"}{"text":"In this paper , we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured .We also give an overview of the parsing approaches that participants took and the results that they achieved .","label":"Background","metadata":{},"score":"49.894855"}{"text":"Sentence compression holds promise for many applications ranging from summarisation to subtitle generation and information retrieval .The task is typically performed on isolated sentences without taking the surrounding context into account , even though most applications would operate over entire documents .","label":"Background","metadata":{},"score":"49.94761"}{"text":"Current phrase - based SMT technologies are good at capturing local reordering but not global reordering .This paper introduces syntactic knowledge to improve global reordering capability of SMT system .Syntactic knowledge such as boundary words , POS information and dependencies is used to guide phrase reordering .","label":"Background","metadata":{},"score":"49.96466"}{"text":"We describe an approach to improve Statistical Machine Translation ( SMT ) performance using multi - lingual , parallel , sentence - aligned corpora in several bridge languages .Our approach consists of a simple method for utilizing a bridge language to create a word alignment system and a procedure for combining word alignment systems from multiple bridge languages .","label":"Background","metadata":{},"score":"49.974678"}{"text":"In text categorization , term selection is an important step for the sake of both categorization accuracy and computational efficiency .Different dimensionalities are expected under different practical resource restrictions of time or space .Traditionally in text categorization , the same scoring or ranking criterion is adopted for all target dimensionalities , which considers both the discriminability and the coverage of a term , such as $ \\chi^2 $ or IG .","label":"Background","metadata":{},"score":"49.981552"}{"text":"We present a method and a tool , OntoLearn , aimed at the extraction of domain ontologies from Web sites , and more generally from documents shared among the members of virtual organizations .OntoLearn first extracts a domain terminology from available documents .","label":"Background","metadata":{},"score":"50.272537"}{"text":"We present a method and a tool , OntoLearn , aimed at the extraction of domain ontologies from Web sites , and more generally from documents shared among the members of virtual organizations .OntoLearn first extracts a domain terminology from available documents .","label":"Background","metadata":{},"score":"50.272537"}{"text":"The proposed method has important applications in areas such as computational biology , natural language processing , information retrieval / extraction , and optical character recognition .Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach . by Koby Crammer , Ofer Dekel , Shai Shalev - Shwartz , Yoram Singer - JMLR , 2006 . \" ...","label":"Background","metadata":{},"score":"50.276268"}{"text":"This method generates 50-best lists that are of substantially higher quality than previously obtainable . ...m search , keeping some large number of possibilities to extend by adding the next word , and then re - pruning .","label":"Background","metadata":{},"score":"50.310333"}{"text":"There has been little effort reported on this in the research community .We argue that semantics is important for record extraction or finer - grained language processing tasks .We derive a data record template including semantic language models from unstructured text and represent them with a discourse level Conditional Random Fields ( CRF ) model .","label":"Background","metadata":{},"score":"50.332344"}{"text":"The resulting IE system achieves good performance on the MUC-4 terrorism corpus and ProMed disease outbreak stories .This approach requires only a few seed extraction patterns and a collection of relevant and irrelevant documents for training .This paper proposes a tree kernel with context - sensitive structured parse tree information for re - lation extraction .","label":"Background","metadata":{},"score":"50.356117"}{"text":"In this paper , a novel method is proposed for use of web search results to improve the existing query spelling correction models solely based on query logs by leveraging the rich information on the web related to the query and its top - ranked candidate .","label":"Background","metadata":{},"score":"50.415756"}{"text":"In this paper , we define the tasks of the different tracks and describe how the data sets were created from existing treebanks for ten languages .In addition , we characterize the different approaches of the participating systems , report the test results , and provide a first analysis of these results .","label":"Background","metadata":{},"score":"50.540535"}{"text":"Experimental results on sentence compression bring significant improvements over a state - of - the - art model .Many emerging applications require documents to be repeatedly updated .Such documents include newsfeeds , webpages , and shared community resources such as Wikipedia .","label":"Background","metadata":{},"score":"50.75424"}{"text":"We introduce a new smoothing method , dubbed Stupid Backoff , that is inexpensive to train on large data sets and approaches the quality of Kneser - Ney Smoothing as the amount of training data increases .We present an extension of phrase - based statistical machine translation models that enables the straight - forward integration of additional annotation at the word - level --- may it be linguistic markup or automatically generated word classes .","label":"Background","metadata":{},"score":"50.75882"}{"text":"Suzuki , Jun ; Isozaki , Hideki ; Carreras PÃ©rez , Xavier ; Collins , Michael ( 2009 ) Conference report Open Access .This paper describes an empirical study of high - performance dependency parsers based on a semi - supervised learning approach .","label":"Background","metadata":{},"score":"50.813324"}{"text":"We present results that show that incorporating lexical and structural semantic information is effective for word sense disambiguation .We evaluated the method by using precise information from a large treebank and an ontology automatically created from dictionary sentences .Exploiting these information improves precision 2 - 3 % , especially 5.7 % for verb , over a model using only bag of words and n - gram features .","label":"Background","metadata":{},"score":"50.83825"}{"text":"In task ( 1 ) , cross - lingual measures are superior to conventional monolingual measures based on a wordnet .In task ( 2 ) , cross - lingual measures are able to solve more problems correctly , and despite scores being affected by many tied answers , their overall performance is again better than the best monolingual measures .","label":"Background","metadata":{},"score":"50.925182"}{"text":"Our proposal takes advantage of the one - sided error guarantees of the BF and simple inequalities that hold between related $ n$-gram statistics in order to further reduce the BF storage requirements and the error rate of the derived probabilities .","label":"Background","metadata":{},"score":"51.00904"}{"text":"Scalable term selection is proposed to optimize the term set at a given dimensionality according to an expected average vector length .Discriminability and coverage are separately measured ; by adjusting the ratio of their weights in a combined criterion , the expected average vector length can be reached , which means a good compromise between the specificity and the exhaustivity of the term subset .","label":"Background","metadata":{},"score":"51.178944"}{"text":"Finally , a general - purpose ontology , WordNet , is trimmed and enriched with the detected domain concepts .The major novel aspect of this approach is semantic interpretation , that is , the association of a complex concept with a complex term .","label":"Background","metadata":{},"score":"51.220123"}{"text":"We describe a robust accurate domain - independent approach to statistical parsing incorporated into the new release of the ANLT toolkit , and publicly available as a research tool .The system has been used to parse many well known corpora in order to produce data for lexical acquisition efforts ; it ha ... \" .","label":"Background","metadata":{},"score":"51.694344"}{"text":"Instead of collecting more and more parallel training corpora , this paper aims to improve SMT performance by exploiting full potential of the existing parallel corpora .Two kinds of methods are proposed : offline data optimization and online model optimization .","label":"Background","metadata":{},"score":"51.72062"}{"text":"In our approach , proper nouns are expanded into new queries aimed at maximizing the probability of retrieving transliterations from existing search engines .The method involves learning the sublexical relationships between names and their transliterations .At run - time , a given name is automatically extended into queries with relevant morphemes , and transliterations in the returned search snippets are extracted and ranked .","label":"Background","metadata":{},"score":"51.73665"}{"text":"We focus on two important subtasks of opinion extraction : ( a ) extracting aspect - evaluation relations , and ( b ) extracting aspect - of relations , and we approach each task using methods which combine contextual and statistical clues .","label":"Background","metadata":{},"score":"51.895134"}{"text":"Carreras PÃ©rez , Xavier ; Surdeanu , Mihai ; MÃ rquez Villodre , LluÃ­s ( 2010 ) Conference report Open Access .We describe an online learning dependency parser for the CoNLL - X Shared Task , based on the bottom - up projective algorithm of Eisner ( 2000 ) .","label":"Background","metadata":{},"score":"51.95253"}{"text":"Morphological analysis and disambiguation are crucial stages in a variety of natural language processing applications , especially when languages with complex morphology are concerned .We present a system which disambiguates the output of a morphological analyzer for Hebrew .It consists of several simple classifiers and a module which combines them under linguistically motivated constraints .","label":"Background","metadata":{},"score":"51.956062"}{"text":"To describe the linguistic phenomenon of the former task , i.e. the syntactic / semantic relation between a predicate and the semantic roles of its arguments , we need to extract features from some subparts of a syntactic tree .For example given the following sentence : .","label":"Background","metadata":{},"score":"52.022133"}{"text":"Many probabilistic models for natural language are now written in terms of hierarchical tree structure .Tree - based modeling still lacks many of the standard tools taken for granted in ( finite - state ) string - based modeling .The theory of tree transducer automata provides a possible framework to ... \" .","label":"Background","metadata":{},"score":"52.09675"}{"text":"- Source code ( It is available with the make files for Windows DevC++ and Linux gcc ) .- Example data ( it contains the PropBank Argument 0 as positive class and Argument 1 as negative class ) .It is possible to design our own kernel by using weights for both trees and vectors and combining them in very different way as the following example illustrates : .","label":"Background","metadata":{},"score":"52.14081"}{"text":"Bailly , RaphaÃ«l ; Carreras PÃ©rez , Xavier ; Luque , Franco M. ; Quattoni , Ariadna Julieta ( Association for Computational Linguistics , 2013 ) Conference lecture Open Access .We derive a spectral method for unsupervised learning ofWeighted Context Free Grammars .","label":"Background","metadata":{},"score":"52.273804"}{"text":"In particular , we wish to determine the best location in a text for a given piece of new information .For this process to succeed , the insertion algorithm should be informed by the existing document structure .Lengthy real - world texts are often hierarchically organized into chapters , sections , and paragraphs .","label":"Background","metadata":{},"score":"52.280987"}{"text":"Using the RankNet learning algorithm , we train a pair - based sentence ranker to score every sentence in the document and identify the most important sentences .We apply our system to documents gathered from CNN.com , where each document includes highlights and an article .","label":"Background","metadata":{},"score":"52.534424"}{"text":"In this paper , we try to address this gap and explore the problem of book summarization .We introduce a new data set specifically designed for the evaluation of systems for book summarization , and describe summarization techniques that explicitly account for the length of the documents .","label":"Background","metadata":{},"score":"52.685696"}{"text":"In addition , we present work on experiments with named entities and other multi - word units , showing a statistically significant improvement of generation accuracy .Given multiple translations of the same source sentence , how to combine them to produce a translation that is better than any single system output ?","label":"Background","metadata":{},"score":"52.721478"}{"text":"This paper defines a generative probabilistic model of parse trees , which we call PCFG - LA .This model is an extension of PCFG in which non - terminal symbols are augmented with latent variables .Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG - LA model using an E ... \" .","label":"Background","metadata":{},"score":"52.815536"}{"text":"These category labels are used as features in a CRF - based NE tagger .We demonstrate using the CoNLL 2003 dataset that the Wikipedia category labels extracted by such a simple method actually improve the accuracy of NER .This paper presents a large - scale system for the recognition and semantic disambiguation of named entities based on information extracted automatically from a large encyclopedic collection and Web search results , over a space of more than 1.4 million entities .","label":"Background","metadata":{},"score":"52.938286"}{"text":"The savings can be quite substantial ( up to 90 % ) and cause no reduction in BLEU score .In some cases , an improvement in BLEU is obtained at the same time although the effect is less pronounced if state - of - the - art phrasetable smoothing is employed .","label":"Background","metadata":{},"score":"52.9511"}{"text":"Semantic inference is a core component of many natural language applications .In response , several researchers have developed algorithms for automatically learning inference rules from textual corpora .However , these rules are often either imprecise or underspecified in directionality .","label":"Background","metadata":{},"score":"53.0311"}{"text":"Our approach combines a set of hand - written patterns together with a probabilistic model .Because the patterns heavily utilize regular expressions , the pertinent tree structures are covered using a limited number of patterns .The probabilistic model is essentially a probabilistic context - free grammar ( PCFG ) approach with the patterns acting as the terminals in production rules .","label":"Background","metadata":{},"score":"53.215202"}{"text":"This process ... .Carreras PÃ©rez , Xavier ; Collins , Michael ( 2009 ) Conference report Open Access .We describe a novel approach for syntaxbased statistical MT , which builds on a variant of tree adjoining grammar ( TAG ) .","label":"Background","metadata":{},"score":"53.26777"}{"text":"We perform both identification and resolution automatically , with two sets of easily computable features .Experimental results show that our proposed learning approach achieves anaphoric zero pronoun resolution accuracy comparable to a previous state - of - the - art , heuristic rule - based approach .","label":"Background","metadata":{},"score":"53.44851"}{"text":"That is , whenever a constituent with the same history is generated a second time , it is discarded if its probability is lower than the original version .I .. \" ...This article considers approaches which rerank the output of an existing probabilistic parser .","label":"Background","metadata":{},"score":"53.492393"}{"text":"In this paper we study spectral learning methods for non - deterministic split head - automata grammars , a powerful hidden - state formalism for dependency parsing .We present a learning algorithm that , like other spectral ... .Balle Pigem , Borja de ; Carreras PÃ©rez , Xavier ; Luque , Franco M. ; Quattoni , Ariadna Julieta ( 2013 - 10 - 07 ) Article Restricted access - publisher 's policy .","label":"Background","metadata":{},"score":"53.590927"}{"text":"In particular , it is important to see if the classification could accommodate new words from heterogeneous data sources , and whether simple similarity measures and clustering methods could cope with such variation .We use the cosine function for similarity and test it on automatically classifying 120 target words from four regions , using different datasets for the extraction of feature vectors .","label":"Background","metadata":{},"score":"53.62462"}{"text":"Using a set of one - vs - all Support Vector Machines ( SVMs ) , we evaluate these LTAG - based features .Our experiments show that LTAG - based features can improve SRL accuracy significantly .When compared with the best known set of features that are used in state of the art SRL systems we obtain an improvement in F - score from 82.34 % to 85.25 % .","label":"Background","metadata":{},"score":"53.658943"}{"text":"Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .The tenth CoNLL ( CoNLL - X ) saw a shared task on Multilingual Dependency Parsing .","label":"Background","metadata":{},"score":"53.660297"}{"text":"Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers .We show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the AQUAINT corpus . by Michael Strube , Simone Paolo Ponzetto - In Proceedings of the 21st national conference on Artificial intelligence , 2006 . \" ... Wikipedia provides a knowledge base for computing word relatedness in a more structured fashion than a search engine and with more coverage than WordNet .","label":"Background","metadata":{},"score":"53.82433"}{"text":"During label prediction , the system automatically selects for each feature an appropriate level of smoothing .We report on several experiments that we conducted with our system .In the shared task evaluation , it scored better than average .We present Pro3Gres , a deep - syntactic , fast dependency parser that combines a hand - written competence grammar with probabilistic performance disambiguation and that has been used in the biomedical domain .","label":"Background","metadata":{},"score":"53.83085"}{"text":"Thus while human judgement is not straightforward and it is difficult to create a Pan - Chinese lexicon manually , it is observed that combining simple clustering methods with the appropriate data sources appears to be a promising approach toward its automatic construction .","label":"Background","metadata":{},"score":"53.950123"}{"text":".. rning deserves further study .There are many different ways one could try to construct a language learner .In [ 65 ] , a selforganizing language learner is proposed to be used for language modelling .In this work we take a different approach , namely starting with a s .. \" ...","label":"Background","metadata":{},"score":"54.06953"}{"text":"In order to capture inherent relations occurring in corpus texts that can be critical in real - world applications , many NP relations are included in the set of grammatical relations used .We provide a comparison of our system with Minipar and the Link parser .","label":"Background","metadata":{},"score":"54.40177"}{"text":"The experiments are carried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser .We use a generative history - based model to predict the most likely derivation of a dependency parse .","label":"Background","metadata":{},"score":"54.425476"}{"text":"We present experimental results from the CoNLL 2003 named entity recognition ( NER ) task to demonstrate the performance of the proposed algorithm .In this paper , we address a unique problem in Chinese language processing and report on our study on extending a Chinese thesaurus with region - specific words , mostly from the financial domain , from various Chinese speech communities .","label":"Background","metadata":{},"score":"54.512497"}{"text":"State - of - the - art performance on Hebrew Treebank parsing is demonstrated using the new method .The benefits of joint inference are modest with the current component models , but appear to increase as components themselves improve .This paper proposes a new bootstrapping approach to unsupervised part - of - speech induction for resource - scarce languages .","label":"Background","metadata":{},"score":"54.5141"}{"text":"In this study , we present a system that generates lexical analogies automatically from text data .Our system discovers semantically related pairs of words by using dependency relations , and applies novel machine learning algorithms to match these word - pairs to form lexical analogies .","label":"Background","metadata":{},"score":"54.53563"}{"text":"We consistently observed significant improvements on several test sets in multiple languages covering different genres .This paper proposes a method using the existing Rule - based Machine Translation ( RBMT ) system as a black box to produce synthetic bilingual corpus , which will be used as training data for the Statistical Machine Translation ( SMT ) system .","label":"Background","metadata":{},"score":"54.573807"}{"text":".. by Aoife Cahill , Michael Burke , Josef Van Genabith , Andy Way - In Proceedings of the 42nd Meeting of the ACL , 2004 . \" ...This paper shows how finite approximations of long distance dependency ( LDD ) resolution can be obtained automatically for wide - coverage , robust , probabilistic Lexical - Functional Grammar ( LFG ) resources acquired from treebanks .","label":"Background","metadata":{},"score":"54.698956"}{"text":"The interesting observations might inspire further investigations .Active learning is a promising way to solve the knowledge bottleneck problem faced by supervised word sense disambiguation ( WSD ) methods .Unfortunately , in real - world data , the distribution of the senses of a word is often skewed , which causes a problem for learning methods for WSD .","label":"Background","metadata":{},"score":"54.8349"}{"text":"We show that the proposed model performs at least as well as an approach based on statistical machine translation on two problems of name transliteration , and provide evidence that the combination of the two approaches promises further improvement .In this paper we propose an instance based method for lexical entailment and apply it to automatic ontology population from text .","label":"Background","metadata":{},"score":"54.878937"}{"text":"This model is an extension of PCFG in which non - terminal symbols are augmented with latent variables .Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG - LA model using an EM - algorithm .","label":"Background","metadata":{},"score":"54.898228"}{"text":"This system outperforms previou ... \" .We describe a parsing system based upon a language model for English that is , in turn , based upon assigning probabilities to possible parses for a sentence .This model is used in a parsing system by finding the parse for the sentence with the highest probability .","label":"Background","metadata":{},"score":"55.0742"}{"text":"Experimental results using the TREC dataset are shown to significantly outperform strong state - of - the - art baselines .Previous machine learning techniques for answer selection in question answering ( QA ) have required question - answer training pairs .","label":"Background","metadata":{},"score":"55.144875"}{"text":"This paper presents a novel approach for exploiting the global context for the task of word sense disambiguation ( WSD ) .This is done by using topic features constructed using the latent dirichlet allocation ( LDA ) algorithm on unlabeled data .","label":"Background","metadata":{},"score":"55.273563"}{"text":"This manual labor scales linearly with the number of target relations .This paper introduces Open IE ( OIE ) , a new extraction paradigm where the system makes a single data - driven pass over its corpus and extracts a large set of relational tuples without requiring any human input .","label":"Background","metadata":{},"score":"55.48677"}{"text":"Smoothing probabilities is most important for tasks with a limited amount of training material .We consider here the Btec task of the 2006 Iwslt evaluation .Improvements in all official automatic measures are reported when translating from Italian to English .","label":"Background","metadata":{},"score":"55.64892"}{"text":"The tenth CoNLL ( CoNLL - X ) saw a shared task on Multilingual Dependency Parsing .Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .","label":"Background","metadata":{},"score":"55.776955"}{"text":"Moreover , the expected input is a parse - tree this means that a pre - terminal ( the lowest non - terminal in the tree ) is always followed by only one leaf .The \" svm_classify \" and the \" svm_learn \" commands maintain the original svm - light format : .","label":"Background","metadata":{},"score":"55.911728"}{"text":"But this method does not work well for web query spelling correction , because there is no lexicon that can cover the vast amount of terms occurring across the web .Recent work showed that using search query logs helps to solve this problem to some extent .","label":"Background","metadata":{},"score":"55.913567"}{"text":"Semantic interpretation is based on a new word sense disambiguation algorithm , called structural semantic interconnections . ouses and Dedicated Web Sites in fact , ontology learning papers regard domain terms as concepts .In order to tag texts with the appropriate semantic role they use a training set of 50,000 sentences manually annotated within the FrameNet semantic labeling project .","label":"Background","metadata":{},"score":"56.06273"}{"text":"Shifting to a new domain requires the user to name the target relations and to ma ... \" .Traditionally , Information Extraction ( IE ) has focused on satisfying precise , narrow , pre - specified requests from small homogeneous corpora ( e.g. , extract the location and time of seminars from a set of announcements ) .","label":"Background","metadata":{},"score":"56.229843"}{"text":"Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks , including the ability to relax strong independence assumptions ... \" .We present conditional random fields , a framework for building probabilistic models to segment and label sequence data .","label":"Background","metadata":{},"score":"56.373695"}{"text":"We present a system for identifying the semantic relationships , or semantic roles , filled by constituents of a sentence within a semantic frame .Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand - annotated training data .","label":"Background","metadata":{},"score":"56.39672"}{"text":"Experimental results are presented for composite translations computed from large numbers of different research systems as well as a set of translation systems derived from one of the best - ranked machine translation engines in the 2006 NIST machine translation evaluation .","label":"Background","metadata":{},"score":"56.533127"}{"text":"Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . .","label":"Background","metadata":{},"score":"56.581284"}{"text":"Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . .","label":"Background","metadata":{},"score":"56.581284"}{"text":"Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . .","label":"Background","metadata":{},"score":"56.581284"}{"text":"In both the English all - words task and the English lexical sample task , the method achieved significant improvement over the simple naive Bayes classifier and higher accuracy than the best offical scores on Senseval-3 for both task .We develop latent Dirichlet allocation with WordNet ( LDAWN ) , an unsupervised probabilistic topic model that includes word sense as a hidden variable .","label":"Background","metadata":{},"score":"56.78726"}{"text":"Textual records of business - oriented conversations between customers and agents need to be analyzed properly to acquire useful business insights that improve productivity .For such an analysis , it is critical to identify appropriate textual segments and expressions to focus on , especially when the textual data consists of complete transcripts , which are often lengthy and redundant .","label":"Background","metadata":{},"score":"56.90709"}{"text":"The type - based models perform better than the models which use tokens for selecting the classes .Furthermore , the models which use the automatically acquired thesaurus entries produced the best results .The correlation for the thesaurus models is stronger than any of the individual features used in previous research on the same dataset .","label":"Background","metadata":{},"score":"56.96203"}{"text":"1992 ) .They are important for a few reasons .Many systems applied to part - ofspeech tagging , speech recognition and other language or speech tasks also fall into this class of model .Second , a partic ... . \" ...","label":"Background","metadata":{},"score":"57.235332"}{"text":"We also present an analysis of what is and is not learned by our system .This paper describes ETK ( Ensemble of Transformation based Keys ) a new algorithm for inducing search keys for name filtering .ETK has the low computational cost and ability to filter by phonetic similarity characteristic of phonetic keys such as Soundex , but is adaptable to alternative similarity models .","label":"Background","metadata":{},"score":"57.7408"}{"text":"This lexicon provides the initial lexical probabilities for EM training of a HMM model .We evaluate the method by applying it in the Biology domain and show that we achieve results that are comparable with some taggers developed for this domain .","label":"Background","metadata":{},"score":"57.763306"}{"text":"We present a method for improving word alignment for statistical syntax - based machine translation that employs a syntactically informed alignment model closer to the translation model than commonly - used word alignment models .This leads to extraction of more useful linguistic patterns and improved BLEU scores on translation experiments in Chinese and Arabic .","label":"Background","metadata":{},"score":"57.772053"}{"text":"Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . \" ...","label":"Background","metadata":{},"score":"58.49888"}{"text":"Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models ( MEMMs ) and other discriminative Markov models based on directed graphical models , which can be biased towards states with few successor states .We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural - language data .","label":"Background","metadata":{},"score":"58.509033"}{"text":"We then describe and analyze a new boosting algorithm for combining preferences called RankBoost .We also describe an efficient implementation of the algorithm for certain natural cases .We discuss two experiments we carried out to assess the performance of RankBoost .","label":"Background","metadata":{},"score":"58.609306"}{"text":"Recent work from our lab suggests that infants not only keep close track of statistical properties of their input , but they use their statistical sensitivity to select among hypotheses about the underlying structures that might have given rise to those statistics .","label":"Background","metadata":{},"score":"58.617218"}{"text":"One line of research focuses on the amount of evidence that infants need to generalize principles that are either found or not found among human languages .The other line focuses on how infants generalize from input that has at least two possible structural descriptions .","label":"Background","metadata":{},"score":"58.868095"}{"text":"We investigate methods to improve the recall in coreference resolution by also trying to resolve those definite descriptions where no earlier mention of the referent shares the same lexical head ( coreferent bridging ) .The problem , which is notably harder than identifying coreference relations among mentions which have the same lexical head , has been tackled with several rather different approaches , and we attempt to provide a meaningful classification along with a quantitative comparison .","label":"Background","metadata":{},"score":"59.070446"}{"text":"We describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order .The resulting system is used as a preprocessor for both training and test sentences , transforming Chinese sentences to be much closer to English in terms of their word order .","label":"Background","metadata":{},"score":"59.161385"}{"text":"Eraall : brill@cs.jhu.edu .Word sense disambiguation , a problem which once seemed out of reach for systems without a great deal of hand cr ... . \" ...We describe a parsing system based upon a language model for English that is , in turn , based upon assigning probabilities to possible parses for a sentence .","label":"Background","metadata":{},"score":"59.25046"}{"text":"Trigram language models are compessed using a Golomb coding method inspired by the original Unix spell program .Compression methods trade off space , time and accuracy ( loss ) .The proposed HashTBO method optimizes space at the expense of time and accuracy .","label":"Background","metadata":{},"score":"59.44118"}{"text":"Most current word prediction systems make use of n - gram language models ( LM ) to estimate the probability of the following word in a phrase .In the past years there have been many attempts to enrich such language models with further syntactic or semantic information .","label":"Background","metadata":{},"score":"59.4553"}{"text":"Evaluation on a list of 500 proper names shows that the method achieves high precision and recall , and outperforms commercial machine translation systems .It has been widely observed that different NLP applications require different sense granularities in order to best exploit word sense distinctions , and that for many applications WordNet senses are too fine - grained .","label":"Background","metadata":{},"score":"59.560074"}{"text":"Their ability to automatically induce features results in multilingual parsing which is robust enough to achieve accuracy well above the average for each individual language in the multilingual track of the CoNLL-2007 shared task .This robustness led to the third best overall average labeled attachment score in the task , despite using no discriminative methods .","label":"Background","metadata":{},"score":"59.597343"}{"text":"Our overall conclusion is that at least two measures , MI and PE , seem to differentiate MWEs from non - MWEs .We then investigate the influence of the size and quality of different corpora , using the BNC and the Web search engines Google and Yahoo .","label":"Background","metadata":{},"score":"59.64028"}{"text":"In scientific literature , sentences that cite related work can be a valuable resource for applications such as summarization , synonym identification , and entity extraction .In order to determine which equivalent entities are discussed in the various citation sentences , we propose aligning the words within these sentences according to semantic similarity .","label":"Background","metadata":{},"score":"59.82891"}{"text":"We also show that our techniques can be applied to full - scale parsing applications by demonstrating its effectiveness in learning state - split grammars .We explore the use of Wikipedia as external knowledge to improve named entity recognition ( NER ) .","label":"Background","metadata":{},"score":"59.888947"}{"text":"For this task , we compare the performance of RankBoost to the individual search strategies .The second experiment is a collaborative - filtering task for making movie recommendations .Here , we present results comparing RankBoost to nearest - neighbor and regression algorithms . by Ioannis Tsochantaridis , Thorsten Joachims , Thomas Hofmann , Yasemin Altun - JOURNAL OF MACHINE LEARNING RESEARCH , 2005 . \" ...","label":"Background","metadata":{},"score":"59.944054"}{"text":"With the synthetic bilingual corpus , we can build an SMT system even if there is no real bilingual corpus .In our experiments using BLEU as a metric , the system achieves a relative improvement of 11.7 % over the best RBMT system that is used to produce the synthetic bilingual corpora .","label":"Background","metadata":{},"score":"59.97206"}{"text":"We train a discriminative classifier over a wide variety of features derived from WordNet structure , corpus - based evidence , and evidence from other lexical resources .Our learned similarity measure outperforms previously proposed automatic methods for sense clustering on the task of predicting human sense merging judgments , yielding an absolute F - score improvement of 4.1 % on nouns , 13.6 % on verbs , and 4.0 % on adjectives .","label":"Background","metadata":{},"score":"60.36157"}{"text":"To reduce the cost of training data construction , our method accepts training examples in which complete word - by - word alignment labels are missing , but instead only the boundaries of coordinated conjuncts are marked .We report promising empirical results in detecting and disambiguating coordinated noun phrases in the GENIA corpus , despite a relatively small number of training examples and minimal features are employed .","label":"Background","metadata":{},"score":"60.57036"}{"text":"In order to incorporate HTML structure on the graph , three types of cliques are defined based on the HTML tree structure .We propose a method with Conditional Random Fields ( CRFs ) to categorize the nodes on the graph .","label":"Background","metadata":{},"score":"60.63607"}{"text":"In parsing we would have training examples fs i ; t i g where each s i is a sentence and each t i is the cor ... . by Paul Kingsbury , Martha Palmer - In Language Resources and Evaluation , 2002 . \" ...","label":"Background","metadata":{},"score":"60.90972"}{"text":"Our method compares favorably with state - of - the - art algorithms that recover WH - traces .Recent studies focussed on the question whether less - configurational languages like German are harder to parse than English , or whether the lower parsing scores are an artifact of treebank encoding schemes and data structures , as claimed by KÃ¼bler et al .","label":"Background","metadata":{},"score":"60.980515"}{"text":"Lexical chains have been successfully employed to evaluate lexical cohesion of text segments and to predict topic boundaries .Our approach is based in the notion of semantic cohesion .It uses spectral embedding to estimate semantic association between content nouns over a span of multiple text segments .","label":"Background","metadata":{},"score":"61.05122"}{"text":"by Mihai Surdeanu , Sanda Harabagiu , John Williams , Paul Aarseth - IN PROCEEDINGS OF ACL 2003 , 2003 . \" ...In this paper we present a novel , customizable IE paradigm that takes advantage of predicate - argument structures .","label":"Background","metadata":{},"score":"61.387512"}{"text":"We compare V - measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions , using simulated clustering results .Finally , we use V - measure to evaluate two clustering tasks : document clustering and pitch accent type clustering .","label":"Background","metadata":{},"score":"61.434753"}{"text":"Part of speech tagging is a fundamental component in many NLP systems .When taggers developed in one domain are used in another domain , the performance can degrade considerably .We present a method for developing taggers for new domains without requiring POS annotated text in the new domain .","label":"Background","metadata":{},"score":"61.531982"}{"text":"Our primary goal is the labeling of syntactic nodes with specific argument labels that preserve the similarity of roles such as the window in ... \" .This paper describes our approach to the development of a Proposition Bank , which involves the addition of semantic information to the Penn English Treebank .","label":"Background","metadata":{},"score":"61.783012"}{"text":"Figure 5 shows the result of parsing with our combined model , using ... . \" ...This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm .We show how the algorithms can be efficiently applied to exponential sized representations of parse trees , such as the \" all subtrees \" ( DOP ) representation described by ( Bod 9 ... \" .","label":"Background","metadata":{},"score":"61.862244"}{"text":"To predict elec - tion results , we apply SVM - based super - vised learning .To improve performance , we propose a novel technique which generalizes n - gram feature patterns .Experimental results show that Crystal significantly outperforms several baselines as well as a non - generalized n - gram ap - proach .","label":"Background","metadata":{},"score":"62.32202"}{"text":"Introduction We present a statistical parser that induces its grammar and probabilities from a hand - parsed corpus ( a tree - bank ) .Parsers induced from corpora are of interest both as simply exercises in machine learning and also because they are often the best parsers obtainable by any method .","label":"Background","metadata":{},"score":"62.71006"}{"text":"Despite its simplicity , our best grammar achieves an F1 of 90.2 % on the Penn Treebank , higher than fully lexicalized systems . ...e into smaller steps ) .In this paper , we investigate the learning of a grammar consistent with a treebank at the level of evaluation symbols ... . by Michele Banko , Michael J Cafarella , Stephen Soderland , Matt Broadhead , Oren Etzioni - IN IJCAI , 2007 . \" ...","label":"Background","metadata":{},"score":"62.865623"}{"text":"This paper reports on the benefits of large - scale statistical language modeling in machine translation .A distributed infrastructure is proposed which we use to train on up to 2 trillion tokens , resulting in language models having up to 300 billion n - grams .","label":"Background","metadata":{},"score":"63.090096"}{"text":"Tools . by Adam L. Berger , Stephen A. Della Pietra , Vincent J. Della Pietra - COMPUTATIONAL LINGUISTICS , 1996 . \" ...The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .","label":"Background","metadata":{},"score":"63.127647"}{"text":"Unknown words are a well - known hindrance to natural language applications .In particular , they drastically impact machine translation quality .An easy way out commercial translation systems usually offer their users is the possibility to add unknown words and their translations into a dedicated lexicon .","label":"Background","metadata":{},"score":"63.32431"}{"text":"The technology of opinion extraction allows users to retrieve and analyze people 's opinions scattered over Web documents .We define an opinion unit as a quadruple consisting of the opinion holder , the subject being evaluated , the part or the attribute in which it is evaluated , and the value of the evaluation that expresses a positive or negative assessment .","label":"Background","metadata":{},"score":"63.74998"}{"text":"The parser is evaluated on the OVIS and WSJ corpora , and shows improvements on efficiency , parse accuracy and testset likelihood .Friday , June 29 , 2007 .A lexical analogy is a pair of word - pairs that share a similar semantic relation .","label":"Background","metadata":{},"score":"63.77234"}{"text":"The Proposition Bank project takes a practical approach to semantic representation , adding a layer of predicate - argument information , or semantic role labels , to the syntactic structures of the Penn Treebank .The resulting resource can be thought of as shallow , in that it does not represent coreference , quantification , and many other higher - order phenomena , but also broad , in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated .","label":"Background","metadata":{},"score":"63.872192"}{"text":"The best results on this dataset are obtained by integrating Google , WordNet and Wikipedia based measures .We also show that including Wikipedia improves the performance of an NLP application processing naturally occurring texts . ...n. No relatedness score is computed for pairs including pronouns .","label":"Background","metadata":{},"score":"64.55852"}{"text":"Acknowledgement Many thanks to Amit Dubey and Yuval Krymolowski , the other two organizers of the shared task , for discussions , converting treebanks , writing software and helping with the papers . \" ...This paper presents a new approach to phrase - level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions .","label":"Background","metadata":{},"score":"64.61842"}{"text":"Query segmentation is the process of taking a user 's search - engine query and dividing the tokens into individual phrases or semantic units .Identification of these query segments can potentially improve both document retrieval precision , by first returning pages which contain the exact query segments , and document retrieval recall , by allowing query expansion or substitution via the segmented units .","label":"Background","metadata":{},"score":"64.79703"}{"text":"Given a prediction message , Crystal first identifies which party the message predicts to win and then aggregates prediction analysis results of a large amount of opinions to project the election results .We collect past election prediction messages from the Web and automatically build a gold standard .","label":"Background","metadata":{},"score":"65.30586"}{"text":"Yet SMT translation quality still obviously suffers from inaccurate lexical choice .In this paper , we address this problem by investigating a new strategy for integrating WSD into an SMT system , that performs fully phrasal multi - word disambiguation .","label":"Background","metadata":{},"score":"65.32189"}{"text":", CoNLL 2006].In case we liked to use only feature vectors we could write them as follows : .However , the original SVM - light input format can be used without any changes as the next two lines associated with two instances illustrate : . -1 1:1 21:2.742439465642236E-4 23:1 30:1 36:1 39:1 41:1 46:1 49:1 66:1 152:1 274:1 333:1 .","label":"Background","metadata":{},"score":"65.555786"}{"text":"In order to compensate for the low recall , we used massive collection of HTML documents .Thus , we could prepare enough polar sentence corpus .This paper discusses automatic determination of case in Arabic .This task is a major source of errors in full diacritization of Arabic .","label":"Background","metadata":{},"score":"66.07284"}{"text":"The 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ( EMNLP - CoNLL 2007 ) .The main track of the conference received 398 paper submissions ( not counting 21 that were withdrawn or rejected without review ) .","label":"Background","metadata":{},"score":"66.082954"}{"text":"Bailly , RaphaÃ«l ; Carreras PÃ©rez , Xavier ; Quattoni , Ariadna Julieta ( 2012 ) Conference report Open Access .Finite - State Transducers ( FST ) are a standard tool for modeling paired inputoutput sequences and are used in numerous applications , ranging from computational biology to natural language processing .","label":"Background","metadata":{},"score":"66.36031"}{"text":"Tools . by Martha Palmer , Paul Kingsbury , Daniel Gildea - Computational Linguistics , 2005 . \" ...The Proposition Bank project takes a practical approach to semantic representation , adding a layer of predicate - argument information , or semantic role labels , to the syntactic structures of the Penn Treebank .","label":"Background","metadata":{},"score":"66.88924"}{"text":"Additionally , we compare the maximum a - posteriori decision rule and the minimum Bayes risk decision rule .We show that not only from a theoretical point but also in terms of translation quality the minimum Bayes risk decision rule is preferable .","label":"Background","metadata":{},"score":"66.93801"}{"text":"Bailly , RaphaÃ«l ; Carreras PÃ©rez , Xavier ; Quattoni , Ariadna Julieta ( 2013 ) Conference report Open Access .Finite - State Transducers ( FST ) are a standard tool for modeling paired input output sequences and are used in numerous applications , ranging from computational biology to natural language processing .","label":"Background","metadata":{},"score":"67.11517"}{"text":"The reordering approach improved the BLEU score for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data .We also conducted a series of experiments to analyze the accuracy and impact of different types of reordering rules .","label":"Background","metadata":{},"score":"67.33817"}{"text":"Inclusions from other languages can be a significant source of errors for monolingual parsers .We show this for English inclusions , which are sufficiently frequent to present a problem when parsing German .We describe an annotation - free approach for accurately detecting such inclusions , and develop two methods for interfacing this approach with a state - of - the - art parser for German .","label":"Background","metadata":{},"score":"67.50975"}{"text":"The input format has changed since previous version as sets of objects have to be specified : .Note that ( a ) two begin trees , i.e. \" .encode the empty tree ( useful as placeholder ) , ( b ) two begin vectors , i.e. \" .","label":"Background","metadata":{},"score":"67.79459"}{"text":"In particular , we present a novel method for combining morphological and distributional information for seed selection .Experimental results demonstrate that our approach works well for English and Bengali , thus providing suggestive evidence that it is applicable to both morphologically impoverished langauges and highly inflectional langauges .","label":"Background","metadata":{},"score":"68.15404"}{"text":"Lluis Martorell , Xavier ; Carreras PÃ©rez , Xavier ; MÃ rquez Villodre , LluÃ­s ( 2013 - 05 )Article Restricted access - publisher 's policy .In this paper we introduce a joint arc - factored model for syntactic and semantic dependency parsing .","label":"Background","metadata":{},"score":"68.379196"}{"text":"Quattoni , Ariadna Julieta ; Carreras PÃ©rez , Xavier ; Torralba , Antonio ( Springer , 2012 ) Conference report Restricted access - publisher 's policy .Since their introduction , ranking SVM models have become a powerful tool for training content - based retrieval systems .","label":"Background","metadata":{},"score":"69.2187"}{"text":"We demonstrate the effectiveness of our technique largely surpassing both the random and most frequent baselines and outperforming current state - of - the - art unsupervised approaches on a benchmark ontology available in the literature .To date , work on Non - Local Dependencies ( NLDs ) has focused almost exclusively on English and it is an open research question how well these approaches migrate to other languages .","label":"Background","metadata":{},"score":"69.30769"}{"text":"Recognizing polarity requires a list of polar words and phrases .For the purpose of building such lexicon automatically , a lot of studies have investigated ( semi- ) unsupervised method of learning polarity of words and phrases .In this paper , we explore to use structural clues that can extract polar sentences from Japanese HTML documents , and build lexicon from the extracted polar sentences .","label":"Background","metadata":{},"score":"70.14198"}{"text":"We found that our method could benefit from the two - preprocessing stages .To speed up training , in this year , we employ the MFN - SVM ( modified finite - Newton method support vector machines ) which can be learned in linear time .","label":"Background","metadata":{},"score":"70.281494"}{"text":"This paper discusses the implementation of crucial aspects of this new annotation scheme .INTRODUCTION During the first phase of the The Penn Treebank project [ 10 ] , ending in December 1992 , 4.5 million words of text were tagged for part - of - speech , with about two - thirds of this material also annotated with a skeletal syntactic bracketing .","label":"Background","metadata":{},"score":"70.53938"}{"text":"In a dependency representation , every node in the tree structure is a surface word ( i.e. , there are no abstrac ... . by Paul Kingsbury , Martha Palmer - In Language Resources and Evaluation , 2002 . \" ...This paper describes our approach to the development of a Proposition Bank , which involves the addition of semantic information to the Penn English Treebank .","label":"Background","metadata":{},"score":"70.62947"}{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"Background","metadata":{},"score":"70.87583"}{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"Background","metadata":{},"score":"70.87583"}{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"Background","metadata":{},"score":"70.87583"}{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"Background","metadata":{},"score":"70.87583"}{"text":"As a test , we used MavenRank to identify the most influential members of the US Senate using data from the US Congressional Record and used committee ranking to evaluate the output .Our results show that MavenRank scores are largely driven by committee status in most topics , but can capture speaker centrality in topics where speeches are used to indicate ideological position instead of influence legislation .","label":"Background","metadata":{},"score":"71.236984"}{"text":"This paper presents a method for categorizing named entities in Wikipedia .In Wikipedia , an anchor text is glossed in a linked HTML text .We formalize named entity categorization as a task of categorizing anchor texts with linked HTML texts which glosses a named entity .","label":"Background","metadata":{},"score":"71.23953"}{"text":"Tags from Present Day English source text are projected to Middle English text using alignments on parallel Biblical text .We explore the use of multiple alignment approaches and a bigram tagger to reduce the noise in the projected tags .Finally , we train a maximum entropy tagger on the output of the bigram tagger on the target Biblical text and test it on tagged Middle English text .","label":"Background","metadata":{},"score":"71.28035"}{"text":"The results of the experiments show that , contrary to KÃ¼bler et al .( 2006 ) , the question whether or not German is harder to parse than English remains undecided .In this paper , we study the problem of automatically segmenting written text into paragraphs .","label":"Background","metadata":{},"score":"71.93659"}{"text":"We learn our model using a Monte Carlo EM algorithm and present quantitative results validating the model .We present a maximally streamlined approach to learning HMM - based acoustic models for automatic speech recognition .In our approach , an initial monophone , single - Gaussian HMM is iteratively refined using a split - merge EM procedure which makes no assumptions about subphone structure or context - dependent structure and which uses only a single Gaussian per HMM state .","label":"Background","metadata":{},"score":"72.05901"}{"text":"In the multilingual exercise of the CoNLL-2007 shared task ( Nivre et al.,2007 ) , our system obtains the best accuracy for English , and the second best accuracies for Basque and Czech .We present our system used in the CoNLL 2007 shared task on multilingual parsing .","label":"Background","metadata":{},"score":"72.172195"}{"text":"The estimated domain ( sub - corpus ) specific language and translation models are used for the translation .The IWSLT05 Japanese to English evaluation set that we used in our experiments gave 2.7 points ( 52.4 to 55.1 ) higher Blue score using this method .","label":"Background","metadata":{},"score":"72.43487"}{"text":"All 109 final papers were allowed 9 pages plus bibliography .In a separate track , 22 specially designated short papers reported results in the CoNLL Shared Task competition , an annual tradition .9 of these were presented as short talks .","label":"Background","metadata":{},"score":"72.652176"}{"text":"The interpolated model achieves an absolute improvement of 0.0245 BLEU score ( 13.1 % relative ) as compared with the individual model trained on the real bilingual corpus .This paper investigates why the HMMs estimated by Expectation - Maximization ( EM ) produce such poor results as Part - of - Speech ( POS ) taggers .","label":"Background","metadata":{},"score":"72.70767"}{"text":"We participated in the CoNLL Shared Task-2007 and evaluated our system for ten languages .We got an average multilingual labeled attachment score of 74.54 % ( with 65.50 % being the average and 80.32 % the highest ) and an average multilingual unlabeled attachment score of 80.30 % ( with 71.13 % being the average and 86.55 % the highest ) .","label":"Background","metadata":{},"score":"73.056854"}{"text":"Iwould like toacknowledge the following people for their contribution to my education : I thank my advisor Mitch Marcus , who gave me the intellectual freedom to pursue what I believed to be the best way to approach natural language processing , and also gave me direction when necessary .","label":"Background","metadata":{},"score":"73.22417"}{"text":"Iwould like toacknowledge the following people for their contribution to my education : I thank my advisor Mitch Marcus , who gave me the intellectual freedom to pursue what I believed to be the best way to approach natural language processing , and also gave me direction when necessary .","label":"Background","metadata":{},"score":"73.22417"}{"text":"is used to specify that the first vector ( after trees ) is empty .For example , if we liked to experiment with different trees for question classification , given the question \" What does S.O.S stand for ? \" , we may use the following forest : .","label":"Background","metadata":{},"score":"73.54532"}{"text":"PDMM is an expansion of an existing probabilistic generative model : Parametric Mixture Model(PMM ) by hierarchical Bayes model .PMM models multiple - topic documents by mixing model parameters of each single topic with an equal mixture ratio .PDMM models multiple - topic documents by mixing model parameters of each single topic with mixture ratio following Dirichlet distribution .","label":"Background","metadata":{},"score":"73.89797"}{"text":".. ations are used to define grammatical roles .The original treebanks , in particular the Penn Treebank , were for English , and provided only phrase structure trees , and hence this is the native ou ... . by Slav Petrov , Leon Barrett , Romain Thibaux , Dan Klein - In ACL ' 06 , 2006 . \" ...","label":"Background","metadata":{},"score":"74.15956"}{"text":"INT'L CONF .ON LANGUAGE RESOURCES AND EVALUATION ( LREC , 2006 . \" ...This paper describes a system for extracting typed dependency parses of English sentences from phrase structure parses .In order to capture inherent relations occurring in corpus texts that can be critical in real - world applications , many NP relations are included in the set of grammatical relations ... \" .","label":"Background","metadata":{},"score":"75.19498"}{"text":"We present experiments showing that multilingual , parallel text in Spanish , French , Russian , and Chinese can be utilized in this framework to improve translation performance on an Arabic - to - English task .Automatic word alignment is the problem of automatically annotating parallel text with translational correspondence .","label":"Background","metadata":{},"score":"75.513"}{"text":"Wikipedia provides a knowledge base for computing word relatedness in a more structured fashion than a search engine and with more coverage than WordNet .In this work we present experiments on using Wikipedia for computing semantic relatedness and compare it to WordNet on various benchmarking datasets .","label":"Background","metadata":{},"score":"77.118164"}{"text":"The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .","label":"Background","metadata":{},"score":"78.71141"}{"text":"This paper describes our approach to the development of a Proposition Bank , which involves the addition of semantic information to the Penn English Treebank .Our primary goal is the labeling of syntactic nodes with specific argument labels that preserve the similarity of roles such as the window in John broke the window and the window broke .","label":"Background","metadata":{},"score":"78.8888"}{"text":"A careful error analysis suggests that when we account for annotation errors in the gold standard , the error rate drops to 0.9 % , with the hand - written rules outperforming the machine learning - based system .We present in this paper methods to improve HMM - based part - of - speech ( POS ) tagging of Mandarin .","label":"Background","metadata":{},"score":"81.93109"}{"text":"I thank all of my thesis committee members : John La erty from Carnegie Mellon University , Aravind Joshi , Lyle Ungar , and Mark Liberman , for their extremely valuable suggestions and comments about my thesis research .I thank Mike Collins , Jason Eisner , and Dan Melamed , with whom I 've had many stimulating and impromptu discussions in the LINC lab .","label":"Background","metadata":{},"score":"86.86655"}{"text":"I thank all of my thesis committee members : John La erty from Carnegie Mellon University , Aravind Joshi , Lyle Ungar , and Mark Liberman , for their extremely valuable suggestions and comments about my thesis research .I thank Mike Collins , Jason Eisner , and Dan Melamed , with whom I 've had many stimulating and impromptu discussions in the LINC lab .","label":"Background","metadata":{},"score":"86.86655"}{"text":"Saturday , June 30 , 2007 .In the last decade , there have been significant developments in the design of approximate randomized algorithms for high - dimensional data .These include : hashing - based algorithms for similarity search problems , computing succinct approximate \" sketches \" of high - dimensional objects , etc .","label":"Background","metadata":{},"score":"94.54633"}