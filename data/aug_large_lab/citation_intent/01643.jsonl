{"text":"Deterministic parsing algorithms for building labeled dependency graphs ( Kudo and Matsumoto,2002 ; Yamada and Matsumoto , 2003 ; Nivre,2003 ) .History - based models for predicting the next parser action at nondeterministic choice points ( Black et al . , 1992 ; Magerman , 1995 ; Ratnaparkhi , 1997 ; Collins , 1999 ) .","label":"Background","metadata":{},"score":"27.82928"}{"text":"This paper describes a simple yet novel method for constructing sets of 50-best parses based on a co ... \" .Discriminative reranking is one method for constructing high - performance statistical parsers ( Collins , 2000 ) .A discriminative reranker requires a source of candidate parses for each sentence .","label":"Background","metadata":{},"score":"28.972218"}{"text":"A discriminative reranker requires a source of candidate parses for each sentence .This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse - to - fine generative parser ( Charniak , 2000 ) .","label":"Background","metadata":{},"score":"31.565243"}{"text":"Finally , we present multilingual experiments which show that parsing with hierarchical state - splitting is fast and accurate in multiple languages and domains , even without any language - specific tuning . ... g ( Matsuzaki et al . , 2005 ; Petrov et al . , 2006 ) .","label":"Background","metadata":{},"score":"32.079914"}{"text":"Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand - annotated training data . \" ...Discriminative reranking is one method for constructing high - performance statistical parsers ( Collins , 2000 ) .","label":"Background","metadata":{},"score":"33.305405"}{"text":"We evaluate using dependency - based gold standards ( DCU 105 , PARC 700 , CBS 500 and dependencies for WSJ Section 22 ) and use the Approximate Randomization Test ( Noreen 1989 ) to test the statistical significance of the results .","label":"Background","metadata":{},"score":"33.442924"}{"text":"Because the conditioning is local , efficient polynomial time parsing algorithms exist for computing inside , outside , and Viterbi parses .PFGs can produce probabilities of strings , making them potentially useful for language modeling .Precision and recall results are comparable to the state of the art with words , and the best reported without words . 1 Introduction Recently , many researchers have worked on statistical parsing techniques which try to capture additional context beyond that of simple probabilistic context - free grammars ( PCFGs ) , including Magerman ( 1995 ) , Charniak ( 1996 ) , Collins ( 1996 ; 1997 ) , ... . ... is somewhat inelegant ; also , for the probabilities to sum to one , it requires an additional step of normalization , which they appear not to have implemented .","label":"Background","metadata":{},"score":"34.425842"}{"text":"Because the conditioning is local , efficient polynomial time parsing algorithms exist for computing inside , outside , and Viterbi parses .PFGs can produce probabilities of strings , making them potentially useful for language modeling .Precision and recall results are comparable to the state of the art with words , and the best reported without words . 1 Introduction Recently , many researchers have worked on statistical parsing techniques which try to capture additional context beyond that of simple probabilistic context - free grammars ( PCFGs ) , including Magerman ( 1995 ) , Charniak ( 1996 ) , Collins ( 1996 ; 1997 ) , ... . ... is somewhat inelegant ; also , for the probabilities to sum to one , it requires an additional step of normalization , which they appear not to have implemented .","label":"Background","metadata":{},"score":"34.425842"}{"text":"We compare various PCFG and history - based parsers ( based on Collins , 1999 ; Charniak , 2000 ; Bikel , 2002 ) to find a baseline parsing system that fits best into our automatic dependency structure annotation technique .This combined system of syntactic parser and dependency structure annotation is compared to two hand - crafted , deep constraint - based parsers ( Carroll and Briscoe 2002 ; Riezler et al .","label":"Background","metadata":{},"score":"34.752052"}{"text":"A second model then attempts to improve upon this i ... \" .This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .","label":"Background","metadata":{},"score":"35.068966"}{"text":"We investigate the precise relationship between these two formalisms , showing that , while they define the same classes of probabilis- tic languages , they appear to impose different inductive biases . ... fining the probability of a parse tree as the probability that a certain top - down stochastic generative process produces that tree .","label":"Background","metadata":{},"score":"36.075"}{"text":"We investigate the precise relationship between these two formalisms , showing that , while they define the same classes of probabilis- tic languages , they appear to impose different inductive biases . ... fining the probability of a parse tree as the probability that a certain top - down stochastic generative process produces that tree .","label":"Background","metadata":{},"score":"36.075"}{"text":"This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this i ... \" .","label":"Background","metadata":{},"score":"36.45624"}{"text":"They are important for a few reasons .First , at present the best performing parsers on the WSJ treebank ( Ratnaparkhi 1997 ; Charniak 1997 , 1999 ; Collins 1997 , 1999 ) are all cases of history - based mo .. \" ...","label":"Background","metadata":{},"score":"36.491985"}{"text":"A second group of papers does parsing by a sequence of independent , discriminative decisions , either greedily or with use of a small beam ( Ratnaparkhi , 1997 ; Henderson , 2004 ) .This paper extends th ...CSCI - GA.2590 - Natural Language Processing - Spring 2013 Prof. Grishman .","label":"Background","metadata":{},"score":"37.119606"}{"text":"Discriminative reranking is one method for constructing high - performance statistical parsers ( Collins , 2000 ) .A discriminative reranker requires a source of candidate parses for each sentence .This paper describes a simple yet novel method for constructing sets of 50-best parses based on a co ... \" .","label":"Background","metadata":{},"score":"38.175762"}{"text":"This system outperforms previou ... \" .We describe a parsing system based upon a language model for English that is , in turn , based upon assigning probabilities to possible parses for a sentence .This model is used in a parsing system by finding the parse for the sentence with the highest probability .","label":"Background","metadata":{},"score":"38.359436"}{"text":"This represents a 13 % decrease in error rate over the best single - parser results on this corpus [ 9].The major technical innova- tion is the use of a \" maximum - entropy - inspired \" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events .","label":"Background","metadata":{},"score":"40.087597"}{"text":"It could play a key role in NLP tasks like Information Extraction , Question Answering and Summarization .We propose a machine learning algorithm for semantic role parsing , extending the work of Gildea and Jurafsky ( 2002 ) , Surdeanu et al .","label":"Background","metadata":{},"score":"40.32789"}{"text":"The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence .","label":"Background","metadata":{},"score":"41.061882"}{"text":"Reference ... \" ...We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .","label":"Background","metadata":{},"score":"41.313965"}{"text":"( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .","label":"Background","metadata":{},"score":"41.683205"}{"text":"It makes it easy to describe parsers that compute a wide variety of interesting quantities , including the inside and outside probabilities , as well as related quantities such as Viterbi probabilities and n - best lists .We also present three novel uses for the inside and outside probabilities .","label":"Background","metadata":{},"score":"41.97766"}{"text":"We introduce a new method for the reranking task , based on the boosting approach to ranking problems described in Freund et al .( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .","label":"Background","metadata":{},"score":"42.37389"}{"text":"When these probabilities are multiplied together and normalized , they produce the probabili ... \" .Probabilistic Context - Free Grammars ( PCFGs ) and variations on them have recently become some of the most common formalisms for parsing .It is common with PCFGs to compute the inside and outside probabilities .","label":"Background","metadata":{},"score":"42.518"}{"text":"Unfortunately , existing algorithms are both computationally intensive and difficult to implement .Previous algorithms are expensive due to two factors : the exponential number of rules that mus ... \" .Excellent results have been reported for DataOriented Parsing ( DOP ) of natural language texts ( Bod , 1993c ) .","label":"Background","metadata":{},"score":"42.819443"}{"text":"Statistical Parsers , cont'd .Evaluating Parsers .Constituent Parsers : the accuracy of constituent parsers is stated in terms of labeled constituent recall / precision / F - measure when compared to a standard parse .Comparison against a standard parse is feasible because UPenn parses have become such a widely used standard .","label":"Background","metadata":{},"score":"43.017956"}{"text":"A Fundamental Algorithm for Dependency Parsing .In Proceedings of the 39th Annual ACM Southeast Conference , pp .95 - 102 .Fan , R.-E. , Chang , K.-W. , Hsieh , C.-J. , Wang , X.-R. and Lin , C.-J. LIBLINEAR :","label":"Background","metadata":{},"score":"43.12857"}{"text":"Second , we compare various inference procedures for state - split PCFGs from the standpoint of risk minimization , paying particular attention to their practical tradeoffs .Finally , we present multilingual experiments which show that parsing with hierarchical state - splitting is fast and accurate in multiple languages and domains , even without any language - specific tuning . .","label":"Background","metadata":{},"score":"43.614952"}{"text":"Beam search keeps the top beam - width states .Equivalent states can be merged ( Huang and Sagae 2010 ) .Easy - First .Easy - first parsers are deterministic bottom - up parsers .In contrast to transition - based parsers , they do not necessarily build their structures from left to right ; at each step they select the best pair of neighbors to link .","label":"Background","metadata":{},"score":"44.73789"}{"text":"Many probabilistic models for natural language are now written in terms of hierarchical tree structure .Tree - based modeling still lacks many of the standard tools taken for granted in ( finite - state ) string - based modeling .The theory of tree transducer automata provides a possible framework to ... \" .","label":"Background","metadata":{},"score":"44.78252"}{"text":"We also give an overview of the parsing approaches that participants took and the results that they achieved .Finally , we try to draw general conclusions about multi - lingual parsing : What makes a particular language , treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser ?","label":"Background","metadata":{},"score":"45.183968"}{"text":"In particular , we demonstrated in Petrov et al .( 2006 ) that a hierarchically split PCFG could exceed the accuracy of lexic ... . by Ben Taskar , Dan Klein , Michael Collins , Daphne Koller , Christopher Manning - In Proceedings of EMNLP , 2004 . \" ...","label":"Background","metadata":{},"score":"45.229095"}{"text":"First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar without a treebank .In our experiments , hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy .","label":"Background","metadata":{},"score":"45.300026"}{"text":"49 - 56 .Ratnaparkhi , A. ( 1997 ) .A linear observed time statistical parser based on maximum entropy models .In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pp . 1 - 10 .","label":"Background","metadata":{},"score":"45.363224"}{"text":"In particular , it allows one to efficiently learn a model which discriminates among the entire space of parse trees , as opposed to reranking the top few candidates .Our models can condition on arbitrary features of input sentences , thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness .","label":"Background","metadata":{},"score":"45.508507"}{"text":"Previous algorithms are expensive due to two factors : the exponential number of rules that must be generated and the use of a Monte Carlo p arsing algorithm .In this paper we solve the first problem by a novel reduction of the DOP model toga small , equivalent probabilistic context - free grammar .","label":"Background","metadata":{},"score":"45.688076"}{"text":"We present several improvements to unlexicalized parsing with hierarchically state - split PCFGs .First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar without a treebank .","label":"Background","metadata":{},"score":"45.68965"}{"text":"Black , E. , F. Jelinek , J. D. Lafferty , D. M. Magerman , R. L. Mercer and S. Roukos ( 1992 ) .Towards history - based grammars : Using richer models for probabilistic parsing .In Proceedings of the 5th DARPA Speech and Natural Language Workshop , pp .","label":"Background","metadata":{},"score":"45.709976"}{"text":"This paper presents a new approach to phrase - level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions .With this approach , the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions , achieving results that are significantly better than baseline . ...","label":"Background","metadata":{},"score":"45.805805"}{"text":"81 - 124 .Abstract .In this article , we revisit the experiments in Preiss ( 2003 ) and Kaplan et al .( 2004 ) , this time using the sophisticated automatic LFG f - structure annotation methodologies of Cahill et al .","label":"Background","metadata":{},"score":"45.90784"}{"text":"First , we compare a complete sentence in a stack with ... .by Eduard Hovy , Chin - yew Lin , Liang Zhou - Proceedings of DUC-2005 , 2005 . \" ...In this paper we introduce Basic Elements , a new way of automating the evaluation of text summaries .","label":"Background","metadata":{},"score":"46.075287"}{"text":"First , we compare a complete sentence in a stack with ... .by Eduard Hovy , Chin - yew Lin , Liang Zhou - Proceedings of DUC-2005 , 2005 . \" ...In this paper we introduce Basic Elements , a new way of automating the evaluation of text summaries .","label":"Background","metadata":{},"score":"46.075287"}{"text":"Tree - based modeling still lacks many of the standard tools taken for granted in ( finite - state ) string - based modeling .The theory of tree transducer automata provides a possible framework to draw on , as it has been worked out in an extensive literature .","label":"Background","metadata":{},"score":"46.17828"}{"text":"In these results , the generative model performs significantly better than the others , and does about equally well at assigning part - of - speech tags . \" ...Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .","label":"Background","metadata":{},"score":"46.221733"}{"text":"The traditional use of these probabilities is to improve the probabilities of grammar rules .In this thesis we show that these values are useful for solving many other problems in Statistical Natural Language Processing .We give a framework for describing parsers .","label":"Background","metadata":{},"score":"46.57604"}{"text":"Our formulation uses a factorization analogous to the standard dynamic programs for parsing .In particular , it allows one to efficiently learn a model which discriminates ... \" .We present a novel discriminative approach to parsing inspired by the large - margin criterion underlying support vector machines .","label":"Background","metadata":{},"score":"46.81775"}{"text":"An Improved Oracle for Dependency Parsing with Online Reordering .In Proceedings of the 11th International Conference on Parsing Technologies ( IWPT ) , 73 - 76 .Natural Language Parsing as Statistical Pattern Recognition ( 1994 ) .Tools . by Joshua Goodman - IN PROCEEDINGS OF THE 34TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS , 1996 . \" ...","label":"Background","metadata":{},"score":"46.84232"}{"text":"The parsing algorithm is quite simple : we initialize pending to the sequence of words in the sentence .A score function , based on a linear combination of features around i and i+1 , assigns a score to each possible action ; we choose the action with the highest score .","label":"Background","metadata":{},"score":"46.899414"}{"text":"We reformulate the task as a combined chunking and classification problem , thus allowing our algorithm to be applied to new languages or genres of text for which statistical syntactic parsers may not be available . \" ...Probabilistic Context - Free Grammars ( PCFGs ) and variations on them have recently become some of the most common formalisms for parsing .","label":"Background","metadata":{},"score":"47.11063"}{"text":"LIBLINEAR --A Library for Large Linear Classification ( Fan et al . , 2008 ) .MaltParser can also be turned into a phrase structure parser that recovers both continuous and discontinuous phrases with both phrase labels and grammatical functions ( Hall and Nivre , 2008a ; Hall and Nivre , 2008b ) .","label":"Background","metadata":{},"score":"47.570625"}{"text":"Cahill , Aoife and Burke , Michael and O'Donovan , Ruth and Riezler , Stefan and van Genabith , Josef and Way , Andy ( 2008 )Wide - coverage deep statistical parsing using automatic dependency structure annotation .Computational Linguistics , 34 ( 1 ) .","label":"Background","metadata":{},"score":"47.580524"}{"text":"Shay , 2009 . \" ...We present a family of priors over probabilistic grammar weights , called the shared logistic normal distribution .This family extends the partitioned logistic normal distribution , enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar , prov ... \" .","label":"Background","metadata":{},"score":"47.997257"}{"text":"This has led to concer ... \" .Statistical parsers trained and tested on the Penn Wall Street Journal ( WSJ ) treebank have shown vast improvements over the last 10 years .Much of this improvement , however , is based upon an ever - increasing number of features to be trained on ( typically ) the WSJ treebank data .","label":"Background","metadata":{},"score":"48.092247"}{"text":"This family extends the partitioned logistic normal distribution , enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar , providing a new way to encode prior knowledge about an unknown grammar .We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors .","label":"Background","metadata":{},"score":"48.12037"}{"text":"We present a maximum - likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently , using as examples several problems in natural language processing . \" ... this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .","label":"Background","metadata":{},"score":"48.471954"}{"text":"Fast Dependency Parsers .Dependency parses can be generated easily from constituent parses , so we can generate a constituent parse using a CKY parser in time n 3 and then convert it to a dependency parse .In the past few years , there has been considerable interest in producing dependency parses directly and quickly .","label":"Background","metadata":{},"score":"48.55354"}{"text":"However , most parsing algorithms , including the Viterbi algorithm , attempt to optimize the same metric , namely the probability of getting th ... \" .Many different metrics exist for evaluating parsing results , including Viterbi , Crossing Brackets Rate , Zero Crossing Brackets Rate , and several others .","label":"Background","metadata":{},"score":"48.69996"}{"text":"The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .","label":"Background","metadata":{},"score":"48.783394"}{"text":"The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .","label":"Background","metadata":{},"score":"48.783394"}{"text":"This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension .The paper proposes a simple informationtheoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel , incremental , probabi ... \" .","label":"Background","metadata":{},"score":"48.870975"}{"text":"This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension .The paper proposes a simple informationtheoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel , incremental , probabi ... \" .","label":"Background","metadata":{},"score":"48.870975"}{"text":"This paper shows how finite approximations of long distance dependency ( LDD ) resolution can be obtained automatically for wide - coverage , robust , probabilistic Lexical - Functional Grammar ( LFG ) resources acquired from treebanks .We extract LFG subcategorisation frames and paths linking LDD reentrancies from f - structures generated automatically for the Penn - II treebank trees and use them in an LDD resolution algorithm to parse new text .","label":"Background","metadata":{},"score":"49.482292"}{"text":"We report statistics on TEXTRUNNER 's 11,000,000 highest probability tuples , and show that they contain over 1,000,000 concrete facts and over 6,500,000 more abstract assertions . \" ...We present several improvements to unlexicalized parsing with hierarchically state - split PCFGs .","label":"Background","metadata":{},"score":"49.58381"}{"text":"We could also introduce new variables , e.g. , nonterminal refinements ( Matsuzaki et al . , 2005 ) , or secondary links Mij ( not constrai ... . by Jin - dong Kim , Tomoko Ohta , Sampo Pyysalo , Yoshinobu Kano - In Proceedings of Natural Language Processing in Biomedicine ( BioNLP )","label":"Background","metadata":{},"score":"49.74908"}{"text":"One notable exception is Brill 's TransformationBased Error Driven system ( Brill , 1993 ) , which induces a set of transformations designed to maximize the Consistent ... . by Aoife Cahill , Michael Burke , Josef Van Genabith , Andy Way - In Proceedings of the 42nd Meeting of the ACL , 2004 . \" ...","label":"Background","metadata":{},"score":"49.786896"}{"text":"In particular we note the effects of two comparatively recent techniques for parser improvement .Then a reranking phase uses more detailed features , features which would ( mostly ) be ... . \" ...We formulate dependency parsing as a graphical model with the novel ingredient of global constraints .","label":"Background","metadata":{},"score":"49.968853"}{"text":"We apply this idea to dependency and constituent parsing , generating results that surpass state - of - theart ... \" .We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers .","label":"Background","metadata":{},"score":"50.326866"}{"text":"Both probabilistic context - free grammars ( PCFGs ) and shift - reduce probabilistic pushdown automata ( PPDAs ) have been used for language modeling and maximum likelihood parsing .We investigate the precise relationship between these two formalisms , showing that , while they define the same classes of pr ... \" .","label":"Background","metadata":{},"score":"50.43211"}{"text":"Both probabilistic context - free grammars ( PCFGs ) and shift - reduce probabilistic pushdown automata ( PPDAs ) have been used for language modeling and maximum likelihood parsing .We investigate the precise relationship between these two formalisms , showing that , while they define the same classes of pr ... \" .","label":"Background","metadata":{},"score":"50.43211"}{"text":"In this paper we introduce Basic Elements , a new way of automating the evaluation of text summaries .We show that this method correlates better with human judgments than any other automated procedure to date , and overcomes the subjectivity / variability problems of manual methods that require humans to preprocess summaries to be evaluated .","label":"Background","metadata":{},"score":"50.445183"}{"text":"In this paper we introduce Basic Elements , a new way of automating the evaluation of text summaries .We show that this method correlates better with human judgments than any other automated procedure to date , and overcomes the subjectivity / variability problems of manual methods that require humans to preprocess summaries to be evaluated .","label":"Background","metadata":{},"score":"50.445183"}{"text":"We propose ( a ) a lexical affinity model where words struggle to modify each other , ( b ) a sense tagging model where words fluctuate randomly in their selectional prefe ... \" .After presenting a novel O(n³ ) parsing algorithm for dependency grammar , we develop three contrasting ways to stochasticize it .","label":"Background","metadata":{},"score":"50.477646"}{"text":"Unlike ( Collins , 1999 ; Johnson , 2002 ) , in our approach resolution of LDDs is done at f - structure ( attribute - value structure representations of basic predicate - argument or dependency structure ) without empty productions , traces and coindexation in CFG parse trees . ... tation .","label":"Background","metadata":{},"score":"50.509003"}{"text":"Hall , J. , J. Nivre and J. Nilsson ( 2006 ) .Discriminative Classifiers for Deterministic Dependency Parsing .In Proceedings of the 21stInternational Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics , pp .","label":"Background","metadata":{},"score":"50.62719"}{"text":"Broad coverage , high quality parsers are available for only a handful of languages .A prerequisite for developing broad coverage parsers for more languages is the annotation of text with the desired linguistic representations ( also known as \" treebanking \" ) .","label":"Background","metadata":{},"score":"50.63141"}{"text":"Statistical Dependency Analysis with Support Vector Machines .In Proceedings of the 8th International Workshop on Parsing Technologies ( IWPT ) , pp .195 - 206 .Hall , J. and J. Nivre ( 2008a )A Dependency - Driven Parser for German Dependency and Constituency Representations .","label":"Background","metadata":{},"score":"50.676025"}{"text":"Graph - based parsers make an exhaustive search of possible dependency structures , seeking the highest - scoring tree .The score of a tree is the product ( or sum ) of the scores of the individual arcs ; the score of an arc may represent its probability ( as for a probabilistic constituent grammar ) or some other linear combination of features .","label":"Background","metadata":{},"score":"50.827965"}{"text":".. by Rebecca Hwa , Philip Resnik , Amy Weinberg , Clara Cabezas , Okan Kolak - Natural Language Engineering , 2005 . \" ...Broad coverage , high quality parsers are available for only a handful of languages .A prerequisite for developing broad coverage parsers for more languages is the annotation of text with the desired linguistic representations ( also known as \" treebanking \" ) .","label":"Background","metadata":{},"score":"51.054085"}{"text":"3.3.3 Questions about Word Identities .Instead , we view the word identities as a further refinement of the POS tags .We start the clustering algorithm wit ... . ... ssible values of P w i c(w i i\\Gamman+1 ) are bucketed .","label":"Background","metadata":{},"score":"51.215843"}{"text":"Nivre , J. ( 2006 ) Inductive Dependency Parsing .Springer .Nivre , J. , Hall , J. and Nilsson , J. ( 2004 )Memory - Based Dependency Parsing .In Ng , H. T. and Riloff , E. ( eds . )","label":"Background","metadata":{},"score":"51.472534"}{"text":"Such worries have merit .The standard \" Charniak parser \" checks in at a labeled precisionrecall f - measure of 89.7 % on the Penn WSJ test set , but only 82.9 % on the test set from the Brown treebank corpus .","label":"Background","metadata":{},"score":"51.537224"}{"text":"Building this kind of resource is expensive and labor - intensive .This work proposes to use sample selection to find helpful training examples and reduce human effort spent on annotating less informative ones .We consider several criteria for predicting whether unlabeled data might be a helpful training example .","label":"Background","metadata":{},"score":"51.56736"}{"text":"Building this kind of resource is expensive and labor - intensive .This work proposes to use sample selection to find helpful training examples and reduce human effort spent on annotating less informative ones .We consider several criteria for predicting whether unlabeled data might be a helpful training example .","label":"Background","metadata":{},"score":"51.56736"}{"text":"Darpa Speech and Natural Language Workshop , 1994 . \" ... Parser development is generally viewed as a primarily linguistic enterprise .A grammarian examines sentences , skillfully extracts the linguistic generalizations evident in the data , and writes grammar rules which cover the language .","label":"Background","metadata":{},"score":"51.579315"}{"text":"In this paper , we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured .We also give an overview of the parsing approaches that participants took and the results that they achieved .","label":"Background","metadata":{},"score":"52.107582"}{"text":"We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .In addition , we introduce two novel smoothing techniques , one a variation of Jelinek - Mercer smoothing and one a very simple linear interpolation technique , both of which outperform existing methods . ... her applications as well , e.g. prepositional phrase attachment ( Collins & Brooks , 1995 ) , part - of - speech tagging ( Church , 1988 ) , and stochastic pars - S. Whenever data sparsity is an issue , smoothing can help performance , and data sparsity is almost always an issue in statistical modeling .","label":"Background","metadata":{},"score":"52.14586"}{"text":"This method generates 50-best lists that are of substantially higher quality than previously obtainable . ...m search , keeping some large number of possibilities to extend by adding the next word , and then re - pruning .","label":"Background","metadata":{},"score":"52.31965"}{"text":"In 2008 the shared task was dedicated to the joint parsing of syntactic and semantic depe ... \" .The Conference on Computational Natural Language Learning is accompanied every year by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting .","label":"Background","metadata":{},"score":"52.359795"}{"text":"The strength of our approach is that it allows a tree to be represented as an arbitrary set of features , without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account .","label":"Background","metadata":{},"score":"52.393654"}{"text":"On the task of assigning semantic labels to the PropBank ( Kingsbury , Palmer , & Marcus , 2002 ) corpus , our final system has a precision of 84 % and a recall of 75 % , which are the best results currently reported for this task .","label":"Background","metadata":{},"score":"52.542"}{"text":"This proposal subsumes and clarifies findings that high - constraint contexts can facilitate lexical processing , and connects these findings to well - known models of parallel constraint - based comprehension .In addition , the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension , including the reversal of locality - based difficulty patterns in syntactically constrained contexts , and conditions under which increased ambiguity facilitates processing .","label":"Background","metadata":{},"score":"52.679703"}{"text":"This proposal subsumes and clarifies findings that high - constraint contexts can facilitate lexical processing , and connects these findings to well - known models of parallel constraint - based comprehension .In addition , the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension , including the reversal of locality - based difficulty patterns in syntactically constrained contexts , and conditions under which increased ambiguity facilitates processing .","label":"Background","metadata":{},"score":"52.679703"}{"text":"Text is first converted into a sequence of part - of - speech tags .Next a Markov model is used to give the most likely sequence of phrase breaks for the input part - of - speech tags .In the Markov model , states represent types of phrase break and the transitions between states represent the likelihoods of sequences of phrase types occurring .","label":"Background","metadata":{},"score":"53.230804"}{"text":"In co ... \" .We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .","label":"Background","metadata":{},"score":"53.31121"}{"text":"We present a new formalism , probabilistic feature grammar ( PFG ) .PFGs combine most of the best properties of several other formalisms , including those of Collins , Magerman , and Charniak , and in experiments have comparable or better performance .","label":"Background","metadata":{},"score":"53.46827"}{"text":"We present a new formalism , probabilistic feature grammar ( PFG ) .PFGs combine most of the best properties of several other formalisms , including those of Collins , Magerman , and Charniak , and in experiments have comparable or better performance .","label":"Background","metadata":{},"score":"53.46827"}{"text":"We present a new formalism , probabilistic feature grammar ( PFG ) .PFGs combine most of the best properties of several other formalisms , including those of Collins , Magerman , and Charniak , and in experiments have comparable or better performance .","label":"Background","metadata":{},"score":"53.46827"}{"text":"We present a new formalism , probabilistic feature grammar ( PFG ) .PFGs combine most of the best properties of several other formalisms , including those of Collins , Magerman , and Charniak , and in experiments have comparable or better performance .","label":"Background","metadata":{},"score":"53.46827"}{"text":"Transition - based parsers .Transition - based parsers ( also called shift - reduce parsers ) are deterministic left - to - right parses .They are similar to the parsers used for programming languages .Given an input sequence and a stack , at each step the parser can push the next word onto the stack or link the top item on the stack with the next word in the input .","label":"Background","metadata":{},"score":"53.632587"}{"text":"non - parallel , multilingual corpus . 1 Introduction Probabilistic grammars have become an important tool in natural language processing .An attractive property of probabilistic grammars is that the ... . by Fei Wu , Daniel S. Weld - in Proc . 48th Annu .","label":"Background","metadata":{},"score":"53.713467"}{"text":"Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .In co ... \" .We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .","label":"Background","metadata":{},"score":"53.756447"}{"text":"Our algorithm is based on Support Vector Machines which we show give large improvement in performance over earlier classifiers .We show performance improvements through a number of new features designed to improve generalization to unseen data , such as automatic clustering of verbs .","label":"Background","metadata":{},"score":"53.834156"}{"text":"This naive grammar ... . \" ...We present several improvements to unlexicalized parsing with hierarchically state - split PCFGs .First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar wi ... \" .","label":"Background","metadata":{},"score":"53.944633"}{"text":"In this work , we present 1 . an effective method for pruning in split PCFGs 2 . a comparison of objective functions for infe ... . \" ...The l - bfgs limited - memory quasi - Newton method is the algorithm of choice for optimizing the parameters of large - scale log - linear models with L2 regularization , but it can not be used for an L1-regularized loss due to its non - differentiability whenever some parameter is zero .","label":"Background","metadata":{},"score":"53.948044"}{"text":"We also present a proof that owl - qn is guaranteed to converge to a globally optimal parameter vector . \" ...Statistical parsers trained and tested on the Penn Wall Street Journal ( WSJ ) treebank have shown vast improvements over the last 10 years .","label":"Background","metadata":{},"score":"53.961147"}{"text":"MaltParser can be characterized as a data - driven parser - generator .While a traditional parser - generator constructs a parser given a grammar , a data - driven parser - generator constructs a parser given a treebank .MaltParser is an implementation of inductive dependency parsing , where the syntactic analysis of a sentence amounts to the derivation of a dependency structure , and where inductive machine learning is used to guide the parser at nondeterministic choice points ( Nivre , 2006 ) .","label":"Background","metadata":{},"score":"53.98758"}{"text":"We present a detailed case study of this learni ... \" .this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance .","label":"Background","metadata":{},"score":"54.06414"}{"text":"Dependency Parsers : the accuracy of dependency parsers is generally stated in terms of the fraction of tokens for which the proper head and dependency label is assigned ; unlabeled dependency may also be reported ( see , for example , Nivre and Scholz ) .","label":"Background","metadata":{},"score":"54.098297"}{"text":"The classifier is trained by converting each dependency tree to a transition sequence which generates that tree .This is a linear - time ( O(n ) ) algorithm .Making deterministic decisions with limited look - ahead limits the accuracy of the parser .","label":"Background","metadata":{},"score":"54.314384"}{"text":"4 The parallel corpus is aligned at the word level using the GIZA++ implementation of the IBM statistical translation models ( Brown et al . ... . by Sameer Pradhan , Kadri Hacioglu , Valerie Krugler , Wayne Ward , James H. Martin , Daniel Jurafsky , 2005 . \" ...","label":"Background","metadata":{},"score":"54.68676"}{"text":"This paper presents an algorithm for automatically assigning phrase breaks to unrestricted text for use in a text - to - speech synthesizer .Text is first converted into a sequence of part - of - speech tags .Next a Markov model is used to give the most likely sequence of phrase breaks for the input part - of ... \" .","label":"Background","metadata":{},"score":"54.805347"}{"text":"In 2008 the shared task was dedicated to the joint parsing of syntactic and semantic dependencies .In this paper , we define the shared task and describe how the data sets were created .Furthermore , we report and analyze the results and describe the approaches of the participating systems . .","label":"Background","metadata":{},"score":"54.98477"}{"text":"This partitions local subtrees of depth one ( corresponding to CFG rules ) into left and right contexts ( relative to head ) .The annotation al .. \" ...Interactive spoken dialogue provides many new challenges for natural language understanding systems .","label":"Background","metadata":{},"score":"55.064262"}{"text":"In particular , we show that the reranking parser described in Charniak and Johnson ( 2005 ) improves performance of the parser on Brown to 85.2 % .Furthermore , use of the self - training techniques described in ( Mc - Closky et al . , 2006 ) raise this to 87.8 % ( an error reduction of 28 % ) again without any use of labeled Brown data .","label":"Background","metadata":{},"score":"55.39863"}{"text":"Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . .","label":"Background","metadata":{},"score":"55.72306"}{"text":"Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . .","label":"Background","metadata":{},"score":"55.72306"}{"text":", 2004 ; Hall et al . , 2006 ) .MaltParser allows users to define feature models of arbitrary complexity .MaltParser currently includes two machine learning packages ( thanks to Sofia Cassel for her work on LIBLINEAR ) : .","label":"Background","metadata":{},"score":"55.814777"}{"text":"As a parsing algorithm , BP is both asymptotically and empirically efficient .E ... \" .We formulate dependency parsing as a graphical model with the novel ingredient of global constraints .We show how to apply loopy belief propagation ( BP ) , a simple and effective tool for approximate learning and inference .","label":"Background","metadata":{},"score":"56.22"}{"text":"The best setup correctly identifies 79 % of breaks in the test corpus .© 1998 Academic Press Limited 1 . ... cause syntactic parses themselves are unhelpful .These have been shown to significantly outperform rule - driven parsers .","label":"Background","metadata":{},"score":"56.661472"}{"text":"The Penn Treebank has recently implemented a new syntactic annotation scheme , designed to highlight aspects of predicate - argument structure .This paper discusses the implementation of crucial aspects of this new annotation scheme .It incorporates a more consistent treatment of a wide range of gramma ... \" .","label":"Background","metadata":{},"score":"56.707973"}{"text":"In Bunt , H. , Merlo , P. and Nivre , J. ( eds . )New Trends in Parsing Technology .Springer .Nivre , J. ( 2009 ) Non - Projective Dependency Parsing in Expected Linear Time .In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP , 351 - 359 .","label":"Background","metadata":{},"score":"56.7407"}{"text":"For each sentence , for each processing step , if the scoring function selects a valid reduction , we perform that reduction and continue .If it selects an invalid action , we reduce the weights associated with the invalid action and increase the weights associated with the valid action .","label":"Background","metadata":{},"score":"56.85151"}{"text":"The issues of consistency of argument structure across both polysemous and synonymous verbs are also discussed and we present our actual guidelines for these types of phenomena , along with numerous examples of tagged sentences and verb frames .We conclude with a summary of the current status of annotation process .","label":"Background","metadata":{},"score":"56.86151"}{"text":"This process entails identifying groups of words in a sentence ... \" .The natural language processing community has recently experienced a growth of interest in domain independent shallow semantic parsing - the process of assigning a WHO did WHAT to WHOM , WHEN , WHERE , WHY , HOW etc . structure to plain text .","label":"Background","metadata":{},"score":"57.2918"}{"text":"The paper presents the design and implementation of the BioNLP'09 Shared Task , and reports the final results with analysis .The shared task consists of three sub - tasks , each of which addresses bio - molecular event extraction at a different level of specificity .","label":"Background","metadata":{},"score":"57.360497"}{"text":"The paper presents the design and implementation of the BioNLP'09 Shared Task , and reports the final results with analysis .The shared task consists of three sub - tasks , each of which addresses bio - molecular event extraction at a different level of specificity .","label":"Background","metadata":{},"score":"57.360497"}{"text":"While prior feature - based dynamic programming parsers have restricted training and evaluation to artificially short sentences , we present the first general , featurerich discriminative parser , based on a conditional random field model , which has been successfully scaled to the full WSJ parsing data .","label":"Background","metadata":{},"score":"57.374763"}{"text":"Corpus - based statistical parsing relies on using large quantities of annotated text as training examples .Building this kind of resource is expensive and labor - intensive .This work proposes to use sample selection to find helpful training examples and reduce human effort spent on annotating less inf ... \" .","label":"Background","metadata":{},"score":"57.471024"}{"text":"Corpus - based statistical parsing relies on using large quantities of annotated text as training examples .Building this kind of resource is expensive and labor - intensive .This work proposes to use sample selection to find helpful training examples and reduce human effort spent on annotating less inf ... \" .","label":"Background","metadata":{},"score":"57.471024"}{"text":"We present a system for identifying the semantic relationships , or semantic roles , filled by constituents of a sentence within a semantic frame .Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand - annotated training data .","label":"Background","metadata":{},"score":"57.587254"}{"text":"In order to obtain linguistically adequate CCG analyses , and to eliminate noise and inconsistencies in the original annotation , an extensive analysis of the constructions and annotations in the Penn Treebank was called for , and a substantial number of changes to the Treebank were necessary .","label":"Background","metadata":{},"score":"57.800243"}{"text":"In order to obtain linguistically adequate CCG analyses , and to eliminate noise and inconsistencies in the original annotation , an extensive analysis of the constructions and annotations in the Penn Treebank was called for , and a substantial number of changes to the Treebank were necessary .","label":"Background","metadata":{},"score":"57.800243"}{"text":"In contrast with previous work , we are able to split various terminals to different degrees , as appropriate to the actual complexity in the data .Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .","label":"Background","metadata":{},"score":"58.350906"}{"text":"By choosing a parsing algorithm appropriate for the evaluation metric , better performance can be achieved .We present two new algorithms : the \" Labelled Recall Algorithm , \" which maximizes the expected Labelled Recall Rate , and the \" Bracketed Recall Algorithm , \" which maximizes the Bracketed Recall Rate .","label":"Background","metadata":{},"score":"58.678757"}{"text":"Chang , C.-C. and C.-J. Lin ( 2001 ) .LIBSVM : A Library for Support Vector Machines .[ pdf ] .Collins , M. ( 1999 ) .Head - Driven Statistical Models for Natural Language Parsing .Ph . D. thesis , University of Pennsylvania .","label":"Background","metadata":{},"score":"58.735268"}{"text":"Speed is O(n log n ) -- computing the max at each of n steps .Dominant time is for feature calculation , which is O(n ) .Overall speed - up .Improvements in speed due to the shift from CKY and graph - based models over the past few years have been dramatic , moving from 4/sentences per second ( e.g. , Charniak parser ) to 75 sentences per second ( Tratz / Hovy easy - first parser ) with little change in parse accuracy .","label":"Background","metadata":{},"score":"59.103577"}{"text":"Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .The tenth CoNLL ( CoNLL - X ) saw a shared task on Multilingual Dependency Parsing .","label":"Background","metadata":{},"score":"59.23128"}{"text":"Tools . by Slav Petrov , Leon Barrett , Romain Thibaux , Dan Klein - In ACL ' 06 , 2006 . \" ...We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .","label":"Background","metadata":{},"score":"59.47708"}{"text":"Kudo , T. and Y. Matsumoto ( 2002 ) .Japanese Dependency Analysis Using Cascaded Chunking .In Proceedings of the Sixth Workshop on Computational Language Learning ( CoNLL ) , pp .63 - 69 .Magerman , D. M. ( 1995 ) .","label":"Background","metadata":{},"score":"59.57882"}{"text":"In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics ( ACL ) , pp .276 - 283 .Nivre , J. ( 2003 ) .An Efficient Algorithm for Projective Dependency Parsing .In Proceedings of the 8th International Workshop on Parsing Technologies ( IWPT 03 ) , pp .","label":"Background","metadata":{},"score":"59.655365"}{"text":"Using ithe optimizations , experiments yield a 97 % crossing brackets rate and 88 % zero crossing brackets rate .This differs significantly from the results reported by Bod , and is compara- ble to results from a duplication of Pereira and Schabes 's ( 1992 ) experiment on the same data .","label":"Background","metadata":{},"score":"59.689056"}{"text":"Comput .Linguist . , 2010 . \" ...Information - extraction ( IE ) systems seek to distill semantic relations from naturallanguage text , but most systems use supervised learning of relation - specific examples and are thus limited by the availability of training data .","label":"Background","metadata":{},"score":"59.711823"}{"text":"Parser development is generally viewed as a primarily linguistic enterprise .A grammarian examines sentences , skillfully extracts the linguistic generalizations evident in the data , and writes grammar rules which cover the language .The grammarian . ... the 1100 sentences .","label":"Background","metadata":{},"score":"59.867577"}{"text":".. by Aoife Cahill , Michael Burke , Josef Van Genabith , Andy Way - In Proceedings of the 42nd Meeting of the ACL , 2004 . \" ...This paper shows how finite approximations of long distance dependency ( LDD ) resolution can be obtained automatically for wide - coverage , robust , probabilistic Lexical - Functional Grammar ( LFG ) resources acquired from treebanks .","label":"Background","metadata":{},"score":"60.06655"}{"text":"The Conference on Computational Natural Language Learning is accompanied every year by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting .In 2008 the shared task was dedicated to the joint parsing of syntactic and semantic depe ... \" .","label":"Background","metadata":{},"score":"60.07663"}{"text":"We present an algorithm Orthant - Wise Limited - memory Quasi - Newton ( owlqn ) , based on l - bfgs , that can efficiently optimize the L1-regularized log - likelihood of log - linear models with millions of parameters .","label":"Background","metadata":{},"score":"60.102783"}{"text":"Despite its simplicity , our best grammar achieves an F1 of 90.2 % on the Penn Treebank , higher than fully lexicalized systems . ...e into smaller steps ) .In this paper , we investigate the learning of a grammar consistent with a treebank at the level of evaluation symbols ... . by Michele Banko , Michael J Cafarella , Stephen Soderland , Matt Broadhead , Oren Etzioni - IN IJCAI , 2007 . \" ...","label":"Background","metadata":{},"score":"60.918114"}{"text":"We report on experiments over a 9,000,000 Web page corpus that compare TEXTRUNNER with KNOWITALL , a state - of - the - art Web IE system .TEXTRUNNER achieves an error reduction of 33 % on a comparable set of extractions .","label":"Background","metadata":{},"score":"61.119423"}{"text":"In this paper , we define the shared task and describe how the data sets were created .Furthermore , we report and analyze the results and describe the approaches of the participating systems . ... dependent relations rather than phrases , the task of the conversion procedure is to identify and label the head - dependent pairs .","label":"Background","metadata":{},"score":"61.14917"}{"text":"The de ... . by E Jelinek , J. Lafferty , D. Magerman , R. Mercer , A. Ratnaparkhi , S. Roukos - Proc .Darpa Speech and Natural Language Workshop , 1994 . \" ... Parser development is generally viewed as a primarily linguistic enterprise .","label":"Background","metadata":{},"score":"61.36048"}{"text":"Hall , J. and J. Nivre ( 2008b )Parsing Discontinuous Phrase Structure with Grammatical Functions .In Proceedings of the 6th International Conference on Natural Language Processing ( GoTAL 2008 ) , August 25 - 27 , 2008 , Gothenburg , Sweden .","label":"Background","metadata":{},"score":"61.536804"}{"text":"The words that are replaced or repeated are no longer part of the intended utterance , and so need to be identified .Segmenting turns and resolving repairs are strongly intertwined with a third task : identifying discourse markers .Because of the interactions , and interactions with POS tagging and speech recognition , we need to address these tasks together and early on in the processing stream .","label":"Background","metadata":{},"score":"61.64071"}{"text":"Eraall : brill@cs.jhu.edu .Word sense disambiguation , a problem which once seemed out of reach for systems without a great deal of hand cr ... . \" ...We describe a parsing system based upon a language model for English that is , in turn , based upon assigning probabilities to possible parses for a sentence .","label":"Background","metadata":{},"score":"61.737404"}{"text":"In this article , we explore using parallel text to help solving the problem of creating syntactic annotation in more languages .The central idea is to annotate the English side of a parallel corpus , project the analysis to the second language , and then train a stochastic analyzer on the resulting noisy annotations .","label":"Background","metadata":{},"score":"62.04721"}{"text":"This paper defines a generative probabilistic model of parse trees , which we call PCFG - LA .This model is an extension of PCFG in which non - terminal symbols are augmented with latent variables .Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG - LA model using an E ... \" .","label":"Background","metadata":{},"score":"62.405373"}{"text":"That is , whenever a constituent with the same history is generated a second time , it is discarded if its probability is lower than the original version .I .. \" ...This article considers approaches which rerank the output of an existing probabilistic parser .","label":"Background","metadata":{},"score":"62.56691"}{"text":"The l - bfgs limited - memory quasi - Newton method is the algorithm of choice for optimizing the parameters of large - scale log - linear models with L2 regularization , but it can not be used for an L1-regularized loss due to its non - differentiability whenever some parameter is zero .","label":"Background","metadata":{},"score":"62.86248"}{"text":"Introduction We present a statistical parser that induces its grammar and probabilities from a hand - parsed corpus ( a tree - bank ) .Parsers induced from corpora are of interest both as simply exercises in machine learning and also because they are often the best parsers obtainable by any method .","label":"Background","metadata":{},"score":"62.98261"}{"text":"The tenth CoNLL ( CoNLL - X ) saw a shared task on Multilingual Dependency Parsing .Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .","label":"Background","metadata":{},"score":"63.233128"}{"text":"We extract LFG subcategorisation frames and paths linking LDD reentrancie ... \" .This paper shows how finite approximations of long distance dependency ( LDD ) resolution can be obtained automatically for wide - coverage , robust , probabilistic Lexical - Functional Grammar ( LFG ) resources acquired from treebanks .","label":"Background","metadata":{},"score":"63.62657"}{"text":"This model is an extension of PCFG in which non - terminal symbols are augmented with latent variables .Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG - LA model using an EM - algorithm .","label":"Background","metadata":{},"score":"63.67273"}{"text":"Discriminative feature - based methods are widely used in natural language processing , but sentence parsing is still dominated by generative methods .While prior feature - based dynamic programming parsers have restricted training and evaluation to artificially short sentences , we present the first gene ... \" .","label":"Background","metadata":{},"score":"64.42172"}{"text":"Tools . by Adam L. Berger , Stephen A. Della Pietra , Vincent J. Della Pietra - COMPUTATIONAL LINGUISTICS , 1996 . \" ...The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .","label":"Background","metadata":{},"score":"64.61757"}{"text":"By solving these simultaneously , we obtain better results on each task than addressing them separately .Our model is able to identify 72 % of turn - internal intonational boundaries with a precision of 71 % , 97 % of discourse markers with 96 % precision , and detect and correct 66 % of repairs with 74 % precision . .","label":"Background","metadata":{},"score":"65.15794"}{"text":"In order to capture inherent relations occurring in corpus texts that can be critical in real - world applications , many NP relations are included in the set of grammatical relations used .We provide a comparison of our system with Minipar and the Link parser .","label":"Background","metadata":{},"score":"65.29062"}{"text":"Contact .Introduction .MaltParser is a system for data - driven dependency parsing , which can be used to induce a parsing model from treebank data and to parse new data using an induced model .MaltParser is developed by Johan Hall , Jens Nilsson and Joakim Nivre at Växjö University and Uppsala University , Sweden .","label":"Background","metadata":{},"score":"65.4121"}{"text":"1992 ) .They are important for a few reasons .Many systems applied to part - ofspeech tagging , speech recognition and other language or speech tasks also fall into this class of model .Second , a partic ... . \" ...","label":"Background","metadata":{},"score":"65.57908"}{"text":"The Stanford Parser is used to derive dependencies from CJ50 and gold parse trees .Figure 8 shows the detailed P / R curves .We can see that although today ... .by Jenny Rose Finkel , Alex Kleeman , Christopher D. Manning - In Proc .","label":"Background","metadata":{},"score":"65.97573"}{"text":"We find that sample selection can significantly reduce the size of annotated training corpora and that uncertainty is a robust predictive criterion that can be easily applied to different learning models . ource sentence is a short one , the decoder will never be able to find it , for the hypotheses leading to it have been pruned permanently .","label":"Background","metadata":{},"score":"66.71142"}{"text":"We find that sample selection can significantly reduce the size of annotated training corpora and that uncertainty is a robust predictive criterion that can be easily applied to different learning models . ource sentence is a short one , the decoder will never be able to find it , for the hypotheses leading to it have been pruned permanently .","label":"Background","metadata":{},"score":"66.71142"}{"text":"We use separate buckets for each n - gram model being interpolated .In performing this bucketing , we create an array containing how many n - grams occur for each value of P w i c(w i i\\Gamman+1 ) up to ... . \" ...","label":"Background","metadata":{},"score":"66.96669"}{"text":"Acknowledgement Many thanks to Amit Dubey and Yuval Krymolowski , the other two organizers of the shared task , for discussions , converting treebanks , writing software and helping with the papers . \" ...This paper presents a new approach to phrase - level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions .","label":"Background","metadata":{},"score":"67.61405"}{"text":"Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .On the other hand , our grammars are much more compact and substantially more accurate than previous work on automatic annotation .Despite its simplicity , our best grammar achieves an F1 of 90.2 % on the Penn Treebank , higher than fully lexicalized systems . ... reebank , higher than fully lexicalized systems .","label":"Background","metadata":{},"score":"67.93676"}{"text":"Even with second - order features or latent variables , which would make exact parsing considerably slower or NP - hard , BP needs only O(n3 ) time with a small constant factor .Furthermore , such features significantly improve parse accuracy over exact first - order methods .","label":"Background","metadata":{},"score":"67.99739"}{"text":"Information - extraction ( IE ) systems seek to distill semantic relations from naturallanguage text , but most systems use supervised learning of relation - specific examples and are thus limited by the availability of training data .Open IE systems such as TextRunner , on the other hand , aim to handle the unbounded number of relations found on the Web .","label":"Background","metadata":{},"score":"68.09206"}{"text":"These are not common in English , but are much more common in languages with freer word order .Trees with such crossing edges are termed non - projective dependency parses .We will discuss three general approaches : graph - based , transition - based , and easy - first .","label":"Background","metadata":{},"score":"68.113754"}{"text":"On WSJ15 , we attain a state - of - the - art F - score of 90.9 % , a 14 % relative reduction in error over previous models , while being two orders of magnitude faster .On sentences of length 40 , our system achieves an F - score of 89.0 % , a 36 % relative reduction in error over a generative baseline . ...","label":"Background","metadata":{},"score":"68.47248"}{"text":"This manual labor scales linearly with the number of target relations .This paper introduces Open IE ( OIE ) , a new extraction paradigm where the system makes a single data - driven pass over its corpus and extracts a large set of relational tuples without requiring any human input .","label":"Background","metadata":{},"score":"68.82909"}{"text":"Shifting to a new domain requires the user to name the target relations and to ma ... \" .Traditionally , Information Extraction ( IE ) has focused on satisfying precise , narrow , pre - specified requests from small homogeneous corpora ( e.g. , extract the location and time of seminars from a set of announcements ) .","label":"Background","metadata":{},"score":"68.904884"}{"text":".. rning deserves further study .There are many different ways one could try to construct a language learner .In [ 65 ] , a selforganizing language learner is proposed to be used for language modelling .In this work we take a different approach , namely starting with a s .. Documentation .","label":"Background","metadata":{},"score":"69.61496"}{"text":"The previous versions 0.1 - 0.4 of MaltParser were implemented in C. The Java implementation ( version 1.0.0 and later releases ) replaces the C implementation ( version 0 .x ) and MaltParser 0.x will not be supported and updated any more .","label":"Background","metadata":{},"score":"70.298965"}{"text":"The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .","label":"Background","metadata":{},"score":"70.567604"}{"text":"The grammarian ... \" .Parser development is generally viewed as a primarily linguistic enterprise .A grammarian examines sentences , skillfully extracts the linguistic generalizations evident in the data , and writes grammar rules which cover the language .The grammarian . ... the 1100 sentences .","label":"Background","metadata":{},"score":"70.81117"}{"text":"This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar ( CCG ) derivations augmented with local and long - range word - word dependencies .The resulting corpus , CCGbank , includes 99.4 % of the sentences in the Penn Treebank .","label":"Background","metadata":{},"score":"72.393456"}{"text":"This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar ( CCG ) derivations augmented with local and long - range word - word dependencies .The resulting corpus , CCGbank , includes 99.4 % of the sentences in the Penn Treebank .","label":"Background","metadata":{},"score":"72.393456"}{"text":"This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar ( CCG ) derivations augmented with local and long - range word - word dependencies .The resulting corpus , CCGbank , includes 99.4 % of the sentences in the Penn Treebank .","label":"Background","metadata":{},"score":"72.393456"}{"text":"This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar ( CCG ) derivations augmented with local and long - range word - word dependencies .The resulting corpus , CCGbank , includes 99.4 % of the sentences in the Penn Treebank .","label":"Background","metadata":{},"score":"72.393456"}{"text":"In a dependency representation , every node in the tree structure is a surface word ( i.e. , there are no abstrac ... . by Paul Kingsbury , Martha Palmer - In Language Resources and Evaluation , 2002 . \" ...This paper describes our approach to the development of a Proposition Bank , which involves the addition of semantic information to the Penn English Treebank .","label":"Background","metadata":{},"score":"74.65184"}{"text":"This paper discusses the implementation of crucial aspects of this new annotation scheme .INTRODUCTION During the first phase of the The Penn Treebank project [ 10 ] , ending in December 1992 , 4.5 million words of text were tagged for part - of - speech , with about two - thirds of this material also annotated with a skeletal syntactic bracketing .","label":"Background","metadata":{},"score":"74.69617"}{"text":"This paper presents WOE , an open IE system which improves dramatically on TextRunner 's precision and recall .The key to WOE 's performance is a novel form of self - supervised learning for open extractors - using heuristic matches between Wikipedia infobox attribute values and corresponding sentences to construct training data .","label":"Background","metadata":{},"score":"74.85637"}{"text":".. ations are used to define grammatical roles .The original treebanks , in particular the Penn Treebank , were for English , and provided only phrase structure trees , and hence this is the native ou ... . by Slav Petrov , Leon Barrett , Romain Thibaux , Dan Klein - In ACL ' 06 , 2006 . \" ...","label":"Background","metadata":{},"score":"77.56903"}{"text":"Eve ... \" .Interactive spoken dialogue provides many new challenges for natural language understanding systems .One of the most critical challenges is simply determining the speaker 's intended utterances : both segmenting a speaker 's turn into utterances and determining the intended words in each utterance .","label":"Background","metadata":{},"score":"77.823845"}{"text":"This paper describes our approach to the development of a Proposition Bank , which involves the addition of semantic information to the Penn English Treebank .Our primary goal is the labeling of syntactic nodes with specific argument labels that preserve the similarity of roles such as the window in John broke the window and the window broke .","label":"Background","metadata":{},"score":"78.825165"}{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"Background","metadata":{},"score":"79.27017"}{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"Background","metadata":{},"score":"79.27017"}{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"Background","metadata":{},"score":"79.27017"}{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"Background","metadata":{},"score":"79.27017"}{"text":"WOE can operate in two modes : when restricted to POS tag features , it runs as quickly as TextRunner , but when set to use dependency - parse features its precision and recall rise even higher . ... h recall .","label":"Background","metadata":{},"score":"80.48543"}{"text":"INT'L CONF .ON LANGUAGE RESOURCES AND EVALUATION ( LREC , 2006 . \" ...This paper describes a system for extracting typed dependency parses of English sentences from phrase structure parses .In order to capture inherent relations occurring in corpus texts that can be critical in real - world applications , many NP relations are included in the set of grammatical relations ... \" .","label":"Background","metadata":{},"score":"82.23935"}{"text":"I thank all of my thesis committee members : John La erty from Carnegie Mellon University , Aravind Joshi , Lyle Ungar , and Mark Liberman , for their extremely valuable suggestions and comments about my thesis research .I thank Mike Collins , Jason Eisner , and Dan Melamed , with whom I 've had many stimulating and impromptu discussions in the LINC lab .","label":"Background","metadata":{},"score":"82.50072"}{"text":"I thank all of my thesis committee members : John La erty from Carnegie Mellon University , Aravind Joshi , Lyle Ungar , and Mark Liberman , for their extremely valuable suggestions and comments about my thesis research .I thank Mike Collins , Jason Eisner , and Dan Melamed , with whom I 've had many stimulating and impromptu discussions in the LINC lab .","label":"Background","metadata":{},"score":"82.50072"}{"text":"Iwould like toacknowledge the following people for their contribution to my education : I thank my advisor Mitch Marcus , who gave me the intellectual freedom to pursue what I believed to be the best way to approach natural language processing , and also gave me direction when necessary .","label":"Background","metadata":{},"score":"82.83091"}{"text":"Iwould like toacknowledge the following people for their contribution to my education : I thank my advisor Mitch Marcus , who gave me the intellectual freedom to pursue what I believed to be the best way to approach natural language processing , and also gave me direction when necessary .","label":"Background","metadata":{},"score":"82.83091"}{"text":"The shared task was run over 12 weeks , drawing initial interest from 42 teams .Of these teams , 24 submitted final results .The evaluation results are encouraging , indicating that state - of - the - art performance is approaching a practically applicable level and revealing some remaining challenges . ... parsers . \" ...","label":"Background","metadata":{},"score":"84.56044"}{"text":"Dependency Parsing of Turkish .Computational Linguistics 34(3 ) , 357 - 389 .Nivre , J. ( 2008 ) Algorithms for Deterministic Incremental Dependency Parsing .Computational Linguistics 34(4 ) , 513 - 553 .Hall , J. , Nilsson , J. and Nivre , J. ( 2010 ) Single Malt or Blended ?","label":"Background","metadata":{},"score":"86.22009"}{"text":"Instead he was born in London Ontario , where he grew up on a strawberry farm .He attended the University of Waterloo where he re - ceived a Bachelors of Mathematics with a joint degree in Pure Mathematics and Com - puter Science in the spring of 1987 .","label":"Background","metadata":{},"score":"90.49672"}{"text":"Instead he was born in London Ontario , where he grew up on a strawberry farm .He attended the University of Waterloo where he re - ceived a Bachelors of Mathematics with a joint degree in Pure Mathematics and Com - puter Science in the spring of 1987 .","label":"Background","metadata":{},"score":"90.49672"}{"text":"What better way to wipe the slate clear than by going to graduate school at the University of Toronto , but not without first spending the sum - mer in Europe .After spending two months in countries where he could n't speak the language , Peter became fascinated by language , and so decided to give computational linguistics a try .","label":"Background","metadata":{},"score":"98.81467"}{"text":"Peter Heeman was born October 22 , 1963 , and much to his dismay his parents had already moved away from Toronto .Instead he was born in London Ontario , where he grew up on a strawberry farm .He attended the University of Waterloo where he re - ceived a Bachelors of Mathematics with a joint degree in Pu ... \" .","label":"Background","metadata":{},"score":"98.96335"}{"text":"Peter Heeman was born October 22 , 1963 , and much to his dismay his parents had already moved away from Toronto .Instead he was born in London Ontario , where he grew up on a strawberry farm .He attended the University of Waterloo where he re - ceived a Bachelors of Mathematics with a joint degree in Pu ... \" .","label":"Background","metadata":{},"score":"98.96335"}{"text":"What better way to wipe the slate clear than by going to graduate school at the University of Toronto , but not without first spending the sum - mer in Europe .After spending two months in countries where he could n't speak the language , Peter became fascinated by language , and so decided to give computational linguistics a try . \" ...","label":"Background","metadata":{},"score":"100.460144"}