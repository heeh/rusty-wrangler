{"text":"Transition - based dependency parsers are often forced to make attachment decisions at a point when only partial information about the relevant graph configuration is available .In this paper , we describe a model that takes into account complete structures as they become available to rescore the elements of a beam , combining the advantages of transition - based and graph - based approaches .","label":"Uses","metadata":{},"score":"30.991243"}{"text":"The system consists of two components : an unlabeled dependency parser using Gibbs sampling which can incorporate sentence - level ( global ) features as well as token - leve ... \" .In this paper , we describe a two - stage multilingual dependency parser used for the multilingual track of the CoNLL 2007 shared task .","label":"Uses","metadata":{},"score":"31.600796"}{"text":"The system consists of two components : an unlabeled dependency parser using Gibbs sampling which can incorporate sentence - level ( global ) features as well as token - leve ... \" .In this paper , we describe a two - stage multilingual dependency parser used for the multilingual track of the CoNLL 2007 shared task .","label":"Uses","metadata":{},"score":"31.600796"}{"text":"We look at two strategies and provide convergence bounds for a particular mode of distributed structured perceptron training based on iterative parameter mixing ( or averaging ) .We present experiments on two structured prediction problems - namedentity recognition and dependency parsing - to highlight the efficiency of this method . ... converged models .","label":"Uses","metadata":{},"score":"34.73633"}{"text":"We focus on one of the simplest and most efficient architectures , based on a deterministic shift - reduce algorithm , trained with the perceptron .By adopting second - order feature maps , the primal form of the perce ... \" .","label":"Uses","metadata":{},"score":"34.85279"}{"text":"We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative improvement over the baseline approach that uses a fixed context window of adjacent words .","label":"Uses","metadata":{},"score":"35.311638"}{"text":"We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative improvement over the baseline approach that uses a fixed context window of adjacent words .","label":"Uses","metadata":{},"score":"35.311638"}{"text":"We show that , in spite of similar performance overall , the two models produce different types of errors , in a w ... \" .We present a comparative error analysis of the two dominant approaches in datadriven dependency parsing : global , exhaustive , graph - based models , and local , greedy , transition - based models .","label":"Uses","metadata":{},"score":"36.708828"}{"text":"The parsing methodology is based on three essential components : .Deterministic parsing algorithms for building labeled dependency graphs ( Kudo and Matsumoto,2002 ; Yamada and Matsumoto , 2003 ; Nivre,2003 ) .History - based models for predicting the next parser action at nondeterministic choice points ( Black et al . , 1992 ; Magerman , 1995 ; Ratnaparkhi , 1997 ; Collins , 1999 ) .","label":"Uses","metadata":{},"score":"36.937904"}{"text":"A Fundamental Algorithm for Dependency Parsing .In Proceedings of the 39th Annual ACM Southeast Conference , pp .95 - 102 .Fan , R.-E. , Chang , K.-W. , Hsieh , C.-J. , Wang , X.-R. and Lin , C.-J. LIBLINEAR :","label":"Uses","metadata":{},"score":"36.962646"}{"text":"We focus on one of the simplest and most efficient architectures , based on a deterministic shift - reduce algorithm , trained with the perceptron .By adopting second - order feature maps , the primal form of the perceptron produces models with comparable accuracy to more complex architectures , with no need for approximations .","label":"Uses","metadata":{},"score":"38.02154"}{"text":"Experimental results show that the global features are useful in all the languages . ... mines unlabeled dependency structures only , and we attach dependency relation labels using Support Vector Machines afterwards . \" ...We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .","label":"Uses","metadata":{},"score":"38.257324"}{"text":"Experimental results show that the global features are useful in all the languages . ... mines unlabeled dependency structures only , and we attach dependency relation labels using Support Vector Machines afterwards . \" ...We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .","label":"Uses","metadata":{},"score":"38.257324"}{"text":"To demonstrate an application of the method , we perform experiments which use the algorithm in training both log - linear and max - margin dependency parsers .The new training methods give improvements in accuracy over perceptron - trained models . ... linear and max - margin training to be applied via the framework developed in this paper .","label":"Uses","metadata":{},"score":"38.533356"}{"text":"Beam search keeps the top beam - width states .Equivalent states can be merged ( Huang and Sagae 2010 ) .Easy - First .Easy - first parsers are deterministic bottom - up parsers .In contrast to transition - based parsers , they do not necessarily build their structures from left to right ; at each step they select the best pair of neighbors to link .","label":"Uses","metadata":{},"score":"38.65851"}{"text":", 2005b ) .Recently Nivre and McDonald ( 2008 ) used the output of one dependency parser to provide features for another .We show that this is an example of stacked learning , in which a second predictor is trained to improve the performance of the first .","label":"Uses","metadata":{},"score":"38.73416"}{"text":"We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .We apply the framework to word segmentation , joint segmentation and POStagging , dependency parsing , and phrase - structure parsing .","label":"Uses","metadata":{},"score":"39.024277"}{"text":"We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .We apply the framework to word segmentation , joint segmentation and POStagging , dependency parsing , and phrase - structure parsing .","label":"Uses","metadata":{},"score":"39.024277"}{"text":"While the synchronised derivations allow different structures to be built for the semantic non - planar graphs and syntactic dependency trees , useful statistical dependencies between these structures are modeled using latent variables .The resulting synchronous parser achieves competitive performance on the CoNLL-2008 shared task , achieving relative error reduction of 12 % in semantic F score over previously proposed synchronous models that can not process non - planarity online . ... ivre and Nilsson , 2005].","label":"Uses","metadata":{},"score":"39.528946"}{"text":"A typical approach in graph - based dependency parsing has been to assume a factorized model , where local features are used but a global function is optimized ( McDonald et al ., 2005b ) .Recently ... \" .We explore a stacked framework for learning to predict dependency structures for natural language sentences .","label":"Uses","metadata":{},"score":"39.85043"}{"text":"To determine why , we analyzed the time usage of a dependency parser .We illustrate that the mapping of the features onto thei ... \" .In addition to a high accuracy , short parsing and training times are the most important properties of a parser .","label":"Uses","metadata":{},"score":"40.309174"}{"text":"In addition , we demonstrate that our method also improves performance when small amounts of training data are available , and can roughly halve the amount of supervised data required to reach a desired level of performance .The idea of combining word clusters with discriminative learning has been previously explored by Miller et al .","label":"Uses","metadata":{},"score":"40.380394"}{"text":"In this paper , we show how these results can be exploited to improve parsing accuracy by integrating a graph ... \" .Previous studies of data - driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference .","label":"Uses","metadata":{},"score":"40.479168"}{"text":"In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"Uses","metadata":{},"score":"40.759438"}{"text":"In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"Uses","metadata":{},"score":"40.759438"}{"text":"The first stage is based on the unlabeled dependency parsing models described by McDonald and Pereira ( 2006 ) augmented with morphological features for a subset of the languages .The second stage takes the ... \" .We present a two - stage multilingual dependency parser and evaluate it on 13 diverse languages .","label":"Uses","metadata":{},"score":"40.949448"}{"text":"The beam - search decoder only requires the syntactic processing task to be broken into a sequence of decisions , such that , at each stage in the process , the decoder is able to consider the top - n candidates and generate all possibilities for the next stage .","label":"Uses","metadata":{},"score":"41.35411"}{"text":"The beam - search decoder only requires the syntactic processing task to be broken into a sequence of decisions , such that , at each stage in the process , the decoder is able to consider the top - n candidates and generate all possibilities for the next stage .","label":"Uses","metadata":{},"score":"41.35411"}{"text":"Transition - Based Natural Language Parsing with Dependency and Constituency Representations .Acta Wexionensia , No 152/2008 , Computer Science , Växjö University ( PhD Thesis ) [ pdf ] .Nivre , J. ( 2008 ) Algorithms for Deterministic Incremental Dependency Parsing .","label":"Uses","metadata":{},"score":"41.407654"}{"text":"Information about different options can be found on the LIBLINEAR web site .Prediction strategy .From version 1.1 of MaltParser it is possible to choose different prediction strategies .Previously , MaltParser ( version 1.0.4 and earlier ) combined the prediction of the transition with the prediction of the arc label into one complex prediction with one feature model .","label":"Uses","metadata":{},"score":"41.44561"}{"text":"The dependency parsing approach presented here extends the existing body of work mainly in four ways : 1 .Although stepwise 1 dependency parsing has commonly been performed using parsing algo1 Stepw ... . \" ...Perceptron training is widely applied in the natural language processing community for learning complex structured models .","label":"Uses","metadata":{},"score":"41.505814"}{"text":"Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"Uses","metadata":{},"score":"41.59188"}{"text":"Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"Uses","metadata":{},"score":"41.59188"}{"text":"To determine why , we analyzed the time usage of a dependency parser .We illustrate that the mapping of the features onto their weights in the support vector machine is the major factor in time complexity .To resolve this problem , we implemented the passive - aggressive perceptron algorithm as a Hash Kernel .","label":"Uses","metadata":{},"score":"42.048866"}{"text":"The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph .We report results on the CoNLL - X shared task ( Buchholz et al . , 2006 ) data sets and present an error analysis . .","label":"Uses","metadata":{},"score":"42.57501"}{"text":"Finally , we demonstrate the efficacy of a semi - supervised extension .The key idea that enables this is an application of the predict - self idea for unsupervised learning . \" ...This paper describes an empirical study of high - performance dependency parsers based on a semi - supervised learning approach .","label":"Uses","metadata":{},"score":"43.25168"}{"text":"Unlike previous approaches , our framework does not require full projected parses , allowing partial , approximate transfer through linear expectation constraints on the space of distributions over trees .We consider several types of constraints that range from generic dependency conservation to language - specific annotation rules for auxiliary verb analysis .","label":"Uses","metadata":{},"score":"43.44886"}{"text":"Unlike previous approaches , our framework does not require full projected parses , allowing partial , approximate transfer through linear expectation constraints on the space of distributions over trees .We consider several types of constraints that range from generic dependency conservation to language - specific annotation rules for auxiliary verb analysis .","label":"Uses","metadata":{},"score":"43.44886"}{"text":"The results show a significant improvement in precision for both topic relevance and opinion relevance . ...Results We performed a few experiments using the TREC 2006 Blog topics n .. \" ...Transition - based dependency parsers are often forced to make attachment decisions at a point when only partial information about the relevant graph configuration is available .","label":"Uses","metadata":{},"score":"43.666016"}{"text":"This paper describes an empirical study of high - performance dependency parsers based on a semi - supervised learning approach .We describe an extension of semisupervised structured conditional models ( SS - SCMs ) to the dependency parsing problem , whose framework is originally proposed in ( Suzuki and Isozaki , 2008 ) .","label":"Uses","metadata":{},"score":"43.82437"}{"text":"We provide experimental evaluations on the Penn Treebank . ... , or build a single tree by means of shift - reduce parsing actions ( Yamada & Matsumoto , 2003 ) .These parsers process the sentence sequentially , hence their efficiency makes them suitable for processing large amounts of text , as required , for example , in information retrieval applications .","label":"Uses","metadata":{},"score":"44.15967"}{"text":"To process non - planarity online , the semantic transition - based parser u ... \" .This paper investigates a generative history - based parsing model that synchronises the derivation of non - planar graphs representing semantic dependencies with the derivation of dependency trees representing syntactic structures .","label":"Uses","metadata":{},"score":"44.309673"}{"text":"[ pdf ] .Nilsson J. , J. Nivre and J. Hall .( 2007 )Generalizing Graph Transformations in Data - Driven Dependency Parsing .In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics ( ACL ) , Prauge , Czech Republic , pp .","label":"Uses","metadata":{},"score":"44.35402"}{"text":"This paper presents novel improvements to the induction of translation lexicons from monolingual corpora using multilingual dependency parses .We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative ... \" .","label":"Uses","metadata":{},"score":"44.459286"}{"text":"This paper presents novel improvements to the induction of translation lexicons from monolingual corpora using multilingual dependency parses .We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative ... \" .","label":"Uses","metadata":{},"score":"44.459286"}{"text":"For example with the Nivre arc - eager algorithm , it is possible to first predict the transition ; if the transition is SHIFT or REDUCE the nondeterminism is resolved , but if the predicted transition is RIGHT - ARC or LEFT - ARC the parser continues to predict the arc label .","label":"Uses","metadata":{},"score":"44.476658"}{"text":"Hall , J. , J. Nivre and J. Nilsson ( 2006 ) .Discriminative Classifiers for Deterministic Dependency Parsing .In Proceedings of the 21stInternational Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics , pp .","label":"Uses","metadata":{},"score":"44.662994"}{"text":"[ ps ] .Nivre , J. , J. Hall and J. Nilsson ( 2004 ) .Memory - Based Dependency Parsing .In Ng , H. T. and Riloff , E. ( eds . )Proceedings of the Eighth Conference on Computational Natural Language Learning ( CoNLL ) , May 6 - 7 , 2004 , Boston , Massachusetts , pp .","label":"Uses","metadata":{},"score":"44.800804"}{"text":"..METU - Sabancı treebank ( Atalay et al . , 2003 ; Oflazer et al . , 2003 ) from the CoNLL shared task in 2006 .Whenever using CoNLL shared task data , we used the first 80 % of the data d .. \" ...","label":"Uses","metadata":{},"score":"45.164776"}{"text":"..METU - Sabancı treebank ( Atalay et al . , 2003 ; Oflazer et al . , 2003 ) from the CoNLL shared task in 2006 .Whenever using CoNLL shared task data , we used the first 80 % of the data d .. \" ...","label":"Uses","metadata":{},"score":"45.164776"}{"text":"The Planar algorithm ( Gómez - Rodríguez and Nivre , 2010 ) is a linear - time algorithm limited to planar dependency structures , the set of structures that do not contain any crossing links .It works in a similar way to Nivre 's algorithm in arc - eager mode , but with more fine - grained transitions .","label":"Uses","metadata":{},"score":"45.261173"}{"text":"The Planar algorithm ( Gómez - Rodríguez and Nivre , 2010 ) is a linear - time algorithm limited to planar dependency structures , the set of structures that do not contain any crossing links .It works in a similar way to Nivre 's algorithm in arc - eager mode , but with more fine - grained transitions .","label":"Uses","metadata":{},"score":"45.261173"}{"text":"There has been a long history in combinatorial optimization of methods that exploit structure in complex problems , using methods such as dual decomposition or Lagrangian relaxation ( Lemaréchal , 200 ... . \" ...We formulate the problem of nonprojective dependency parsing as a polynomial - sized integer linear program .","label":"Uses","metadata":{},"score":"45.60279"}{"text":"With MaltParser 1.1 and later versions it is possible to divide the prediction of the parser action into several predictions .For example with the Nivre arc - eager algorithm , it is possible to first predict the transition ; if the transition is SHIFT or REDUCE the nondeterminism is resolved , but if the predicted transition is RIGHT - ARC or LEFT - ARC the parser continues to predict the arc label .","label":"Uses","metadata":{},"score":"45.841995"}{"text":"The model parameters are learned in a max - margin framework by employing a linear programming relaxation .We evaluate the performance of our parser on data in several natural languages , achieving improvements over existing state - of - the - art methods . \" ...","label":"Uses","metadata":{},"score":"46.110268"}{"text":"Experiments on twelve languages show that stacking transition - based and graphbased parsers improves performance over existing state - of - the - art dependency parsers . by Terry Koo , Amir Globerson , Xavier Carreras , Michael Collins - In EMNLP - CoNLL , 2007 . \" ...","label":"Uses","metadata":{},"score":"46.35484"}{"text":"Fast Dependency Parsers .Dependency parses can be generated easily from constituent parses , so we can generate a constituent parse using a CKY parser in time n 3 and then convert it to a dependency parse .In the past few years , there has been considerable interest in producing dependency parses directly and quickly .","label":"Uses","metadata":{},"score":"46.593117"}{"text":"We present a simple and effective semisupervised method for training dependency parsers .We focus on the problem of lexical representation , introducing features that incorporate word clusters derived from a large unannotated corpus .We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank , and we show that the cluster - based features yield substantial gains in performance across a wide range of conditions .","label":"Uses","metadata":{},"score":"46.597847"}{"text":"We discuss how the general framework is applied to each of the problems studied in this article , making comparisons with alternative learning and decoding algorithms .We also show how the comparability of candidates considered by the beam is an important factor in the performance .","label":"Uses","metadata":{},"score":"46.768036"}{"text":"We discuss how the general framework is applied to each of the problems studied in this article , making comparisons with alternative learning and decoding algorithms .We also show how the comparability of candidates considered by the beam is an important factor in the performance .","label":"Uses","metadata":{},"score":"46.768036"}{"text":"Our experiments confirm that the online algorithms are much faster than the batch algorithms in practice .We describe how the EG updates factor in a convenient way for structured prediction problems , allowing the algorithms to be . ... in McDonald et al .","label":"Uses","metadata":{},"score":"46.822098"}{"text":"While discriminative methods , such as those presented in McDonald et al .( 2005b ) , obtain very high accuracy on standard dependency parsing tasks and can be trained and applied without marginalization , \" summing trees \" permits some alternative techniques of interest .","label":"Uses","metadata":{},"score":"46.834774"}{"text":"We apply this parsing framework to both tracks of the CoNLL 2007 shared task , in each case taking advantage of multiple models trained with different learners .In the multilingual track , we train three LR models for each of the ten languages , and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme .","label":"Uses","metadata":{},"score":"46.870186"}{"text":"This simple framework performs surprisingly well , giving accuracy results competitive with the state - of - the - art on all the tasks we consider .The computational simplicity of the decoder and training algorithm leads to significantly higher test speeds and lower training times than their main alternatives , including log - linear and large - margin training algorithms and dynamic - programming for decoding .","label":"Uses","metadata":{},"score":"47.244316"}{"text":"This simple framework performs surprisingly well , giving accuracy results competitive with the state - of - the - art on all the tasks we consider .The computational simplicity of the decoder and training algorithm leads to significantly higher test speeds and lower training times than their main alternatives , including log - linear and large - margin training algorithms and dynamic - programming for decoding .","label":"Uses","metadata":{},"score":"47.244316"}{"text":"We exploit the Matrix Tree Theorem ( Tutte , 1984 ) to derive an algorithm that efficiently sums the scores of all nonprojective trees i ... \" .A notable gap in research on statistical dependency parsing is a proper conditional probability distribution over nonprojective dependency trees for a given sentence .","label":"Uses","metadata":{},"score":"47.24891"}{"text":"Transition - based parsers .Transition - based parsers ( also called shift - reduce parsers ) are deterministic left - to - right parses .They are similar to the parsers used for programming languages .Given an input sequence and a stack , at each step the parser can push the next word onto the stack or link the top item on the stack with the next word in the input .","label":"Uses","metadata":{},"score":"47.644325"}{"text":"We describe an adaptation and application of a search - based structured prediction algorithm \" Searn \" to unsupervised learning problems .We show that it is possible to reduce unsupervised learning to supervised learning and demonstrate a high - quality unsupervised shift - reduce parsing model .","label":"Uses","metadata":{},"score":"47.867905"}{"text":"We describe an adaptation and application of a search - based structured prediction algorithm \" Searn \" to unsupervised learning problems .We show that it is possible to reduce unsupervised learning to supervised learning and demonstrate a high - quality unsupervised shift - reduce parsing model .","label":"Uses","metadata":{},"score":"47.867905"}{"text":"Tools . by Terry Koo , Xavier Carreras , Michael Collins - In Proc .ACL / HLT , 2008 . \" ...We present a simple and effective semisupervised method for training dependency parsers .We focus on the problem of lexical representation , introducing features that incorporate word clusters derived from a large unannotated corpus .","label":"Uses","metadata":{},"score":"48.087425"}{"text":"Nivre , J. ( 2006 ) Inductive Dependency Parsing .Springer .Nivre , J. , Hall , J. and Nilsson , J. ( 2004 )Memory - Based Dependency Parsing .In Ng , H. T. and Riloff , E. ( eds . )","label":"Uses","metadata":{},"score":"48.149822"}{"text":"Parser actions are determined by a classifier , based on features that represent the current state of the parser .We apply this pars ... \" .We present a data - driven variant of the LR algorithm for dependency parsing , and extend it with a best - first search for probabilistic generalized LR dependency parsing .","label":"Uses","metadata":{},"score":"48.274437"}{"text":"This analysis leads to new directions for parser development . ... otated corpus .The advantage of such models is that they are easily ported to any domain or language in which annotated resources exist .The first is what Buchholz and Marsi ( 2006 ) call the \" all - pairs \" approach , where every possible arc is considered in the ... . by Kenji Sagae - In Proceedings of the Eleventh Conference on Computational Natural Language Learning , 2007 . \" ...","label":"Uses","metadata":{},"score":"48.668106"}{"text":"Statistical Dependency Analysis with Support Vector Machines .In Proceedings of the 8th International Workshop on Parsing Technologies ( IWPT ) , pp .195 - 206 .Hall , J. and J. Nivre ( 2008a )A Dependency - Driven Parser for German Dependency and Constituency Representations .","label":"Uses","metadata":{},"score":"49.26617"}{"text":"We formulate the problem of nonprojective dependency parsing as a polynomial - sized integer linear program .Our formulation is able to handle non - local output features in an efficient manner ; not only is it compatible with prior knowledge encoded as hard constraints , it can also learn soft constraints from data .","label":"Uses","metadata":{},"score":"49.382515"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"Uses","metadata":{},"score":"49.493168"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"Uses","metadata":{},"score":"49.493168"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"Uses","metadata":{},"score":"49.493168"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"Uses","metadata":{},"score":"49.493168"}{"text":"In this paper we adopt a simplified version of this approach , where we introduce a single new action .Although the resulting parser is not powerful enough to parse all non - planar structures , this s .. \" ...In addition to a high accuracy , short parsing and training times are the most important properties of a parser .","label":"Uses","metadata":{},"score":"49.947433"}{"text":"In order to replicate the behavior of older versions , use the following settings : . Covington .Covington 's algorithm ( Covington 2001 ) is a quadratic - time algorithm for unrestricted dependency structures , which proceeds by trying to link each new token to each preceding token .","label":"Uses","metadata":{},"score":"50.876633"}{"text":"( 2007 ) , resulting in 2,500,554 features .The training data consists of 2,306 sentences ( 58,771 tokens ) .To evaluate validation error , we use 1,000 sentences ( 30,563 tokens ) and report accuracy ( rate of correct edges in a predicted parse t .. by Ryan Mcdonald - Proceedings of the Conference on Empirical Methods in Natural Language Processing and Natural Language Learning , 2007 . \" ...","label":"Uses","metadata":{},"score":"51.17653"}{"text":"49 - 56 .Ratnaparkhi , A. ( 1997 ) .A linear observed time statistical parser based on maximum entropy models .In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pp . 1 - 10 .","label":"Uses","metadata":{},"score":"51.225376"}{"text":"Yet , various grammar parameters are expected to be correlated because the elements in language they represent share linguistic properties .In this paper , we suggest an alternative to the Dirichlet prior , a family of logistic normal distributions .We derive an inference algorithm for this family of distributions and experiment with the task of dependency grammar induction , demonstrating performance improvements with our priors on a set of six treebanks in different natural languages .","label":"Uses","metadata":{},"score":"51.391426"}{"text":"Yet , various grammar parameters are expected to be correlated because the elements in language they represent share linguistic properties .In this paper , we suggest an alternative to the Dirichlet prior , a family of logistic normal distributions .We derive an inference algorithm for this family of distributions and experiment with the task of dependency grammar induction , demonstrating performance improvements with our priors on a set of six treebanks in different natural languages .","label":"Uses","metadata":{},"score":"51.391426"}{"text":"In this paper , we ... . by Michael Collins , Amir Globerson , Terry Koo , Xavier Carreras , Peter L. Bartlett , 2008 . \" ...Log - linear and maximum - margin models are two commonly - used methods in supervised machine learning , and are frequently used in structured prediction problems .","label":"Uses","metadata":{},"score":"51.9647"}{"text":"By letting one model generate features for the other , we consistently improve accuracy for both models , resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL - X shared task . ...","label":"Uses","metadata":{},"score":"52.01605"}{"text":"An Improved Oracle for Dependency Parsing with Online Reordering .In Proceedings of 11th International Conference on Parsing Technologies ( IWPT ) , Paris , France , pp . to appear .Nivre , J. ( 2009 ) Non - Projective Dependency Parsing in Expected Linear Time .","label":"Uses","metadata":{},"score":"52.532814"}{"text":"We then describe and analyze two families of such algorithms : stack - based and list - based algorithms .In the former family , which is restricted to projective dependency structures , we describe an arc - eager and an arc - standard variant ; in the latter family , we present a projective and a nonprojective variant .","label":"Uses","metadata":{},"score":"52.86093"}{"text":"The tree with the maximal probability is outputted .The experiments are carried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser . ... arried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser .","label":"Uses","metadata":{},"score":"52.90934"}{"text":"The tree with the maximal probability is outputted .The experiments are carried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser . ... arried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser .","label":"Uses","metadata":{},"score":"52.90934"}{"text":"This command will create a new directory test containing the following files : .Description .conllx.xml .XML document describing the data format .NivreEager.xml .XML document containing the feature model specification .odm0.libsvm.moo , odm0.libsvm.map .The LIBSVM model that is used for predicting the next parsing action .","label":"Uses","metadata":{},"score":"52.971798"}{"text":"Black , E. , F. Jelinek , J. D. Lafferty , D. M. Magerman , R. L. Mercer and S. Roukos ( 1992 ) .Towards history - based grammars : Using richer models for probabilistic parsing .In Proceedings of the 5th DARPA Speech and Natural Language Workshop , pp .","label":"Uses","metadata":{},"score":"53.03442"}{"text":"In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics ( ACL ) , pp .276 - 283 .Nivre , J. ( 2003 ) .An Efficient Algorithm for Projective Dependency Parsing .In Proceedings of the 8th International Workshop on Parsing Technologies ( IWPT 03 ) , pp .","label":"Uses","metadata":{},"score":"53.287704"}{"text":"This has lead to a higher accuracy .We could further increase the parsing and training speed with a parallel feature extraction and a parallel parsing algorithm .We are convinced that the Hash Kernel and the parallelization can be applied successful to other NLP applications as well such as transition based dependency parsers , phrase structrue parsers , and machine translation . by Massimiliano Ciaramita - Proc . of the 12th International Workshop on Parsing Technologies ( IWPT , 2007 . \" ...","label":"Uses","metadata":{},"score":"53.482384"}{"text":"LIBLINEAR --A Library for Large Linear Classification ( Fan et al . , 2008 ) .MaltParser can also be turned into a phrase structure parser that recovers both continuous and discontinuous phrases with both phrase labels and grammatical functions ( Hall and Nivre , 2008a ; Hall and Nivre , 2008b ) .","label":"Uses","metadata":{},"score":"53.541126"}{"text":"E ... \" .We formulate dependency parsing as a graphical model with the novel ingredient of global constraints .We show how to apply loopy belief propagation ( BP ) , a simple and effective tool for approximate learning and inference .","label":"Uses","metadata":{},"score":"54.112255"}{"text":"[ ps ] .Nivre , J. ( 2004 ) .Incrementality in Deterministic Dependency Parsing .In Incremental Parsing : Bringing Engineering and Cognition Together .Workshop at ACL-2004 , Barcelona , Spain , July 25 , 2004 .[ pdf ] .","label":"Uses","metadata":{},"score":"54.320114"}{"text":"The most common strategy uses the swap transition ( Nivre , 2009 ; Nivre et al . , 2009 ) , an alternative solution uses two planes and a switch transition to switch between the two planes ( G .. \" ... Abstract .","label":"Uses","metadata":{},"score":"54.3541"}{"text":"Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars .Nevertheless , it has been shown that such algorithms , combined with treebank - induced classifiers , can be used to build highly accurate disambiguating parsers , in particular for dependency - based syntactic representations .","label":"Uses","metadata":{},"score":"54.52698"}{"text":"To parse all the sentences in the PDT , one must use a non - projectiv ... .by Ryan McDonald , Kevin Lerman , Fernando Pereira - IN PROCEEDINGS OF THE CONFERENCE ON COMPUTATIONAL NATURAL LANGUAGE LEARNING ( CONLL , 2006 . \" ...","label":"Uses","metadata":{},"score":"54.640366"}{"text":"Kudo , T. and Y. Matsumoto ( 2002 ) .Japanese Dependency Analysis Using Cascaded Chunking .In Proceedings of the Sixth Workshop on Computational Language Learning ( CoNLL ) , pp .63 - 69 .Magerman , D. M. ( 1995 ) .","label":"Uses","metadata":{},"score":"54.807846"}{"text":"In the max - margin case , O ( 1 ε ) EG updates are required to reach a given accuracy ε in the dual ; in contrast , for log - linear models only O(log ( 1/ε ) ) updates are required .","label":"Uses","metadata":{},"score":"54.866013"}{"text":"Since data is processed as soon as it becomes available , processing delay is minimized improving data throughput .The processing modules can be written in C++ or in Python and can be combined using few lines of Python scripts to produce full NLP applications .","label":"Uses","metadata":{},"score":"55.01528"}{"text":"Inductive Dependency Parsing .MaltParser can be characterized as a data - driven parser - generator .While a traditional parser - generator constructs a parser given a grammar , a data - driven parser - generator constructs a parser given a treebank .","label":"Uses","metadata":{},"score":"55.142883"}{"text":"The parsing algorithm is quite simple : we initialize pending to the sequence of words in the sentence .A score function , based on a linear combination of features around i and i+1 , assigns a score to each possible action ; we choose the action with the highest score .","label":"Uses","metadata":{},"score":"55.296597"}{"text":"An Improved Oracle for Dependency Parsing with Online Reordering .In Proceedings of the 11th International Conference on Parsing Technologies ( IWPT ) , 73 - 76 .","label":"Uses","metadata":{},"score":"55.345325"}{"text":"INPUT .Input data in both learning and parsing mode , such as part - of - speech tags or word forms .DEPENDENCY_EDGE_LABEL .Column containing a dependency label .If the parser is to learn to produce labeled dependency graphs , these must be present in learning mode .","label":"Uses","metadata":{},"score":"55.566093"}{"text":"Journal of Machine Learning Research 9 , 1871 - 1874 .Hall , J. ( 2008 )Transition - Based Natural Language Parsing with Dependency and Constituency Representations .Acta Wexionensia , No 152/2008 , Computer Science , Växjö University ( PhD Thesis ) .","label":"Uses","metadata":{},"score":"55.604492"}{"text":"Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .The tenth CoNLL ( CoNLL - X ) saw a shared task on Multilingual Dependency Parsing .","label":"Uses","metadata":{},"score":"55.743702"}{"text":"It is possible to define your own feature model specification using the description above and using the --guide - features option to specify the feature model specification file .LIBLINEAR .Prediction strategy .From version 1.1 of MaltParser it is possible to choose different prediction strategies .","label":"Uses","metadata":{},"score":"55.779274"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"Uses","metadata":{},"score":"55.916428"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"Uses","metadata":{},"score":"55.916428"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"Uses","metadata":{},"score":"55.916428"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"Uses","metadata":{},"score":"55.916428"}{"text":"Graph - based parsers make an exhaustive search of possible dependency structures , seeking the highest - scoring tree .The score of a tree is the product ( or sum ) of the scores of the individual arcs ; the score of an arc may represent its probability ( as for a probabilistic constituent grammar ) or some other linear combination of features .","label":"Uses","metadata":{},"score":"55.9172"}{"text":"Speed is O(n log n ) -- computing the max at each of n steps .Dominant time is for feature calculation , which is O(n ) .Overall speed - up .Improvements in speed due to the shift from CKY and graph - based models over the past few years have been dramatic , moving from 4/sentences per second ( e.g. , Charniak parser ) to 75 sentences per second ( Tratz / Hovy easy - first parser ) with little change in parse accuracy .","label":"Uses","metadata":{},"score":"55.94629"}{"text":"The classifier is trained by converting each dependency tree to a transition sequence which generates that tree .This is a linear - time ( O(n ) ) algorithm .Making deterministic decisions with limited look - ahead limits the accuracy of the parser .","label":"Uses","metadata":{},"score":"56.01426"}{"text":"Hall , J. and J. Nivre ( 2008b )Parsing Discontinuous Phrase Structure with Grammatical Functions .In Ranta , A. and Nordström , B. ( eds . )In Proceedings of the 6th International Conference on Natural Language Processing ( GoTAL 2008 ) , LNAI 5221 , Springer - Verlag , August 25 - 27 , 2008 , Gothenburg , Sweden , pp . 169 - 180 .","label":"Uses","metadata":{},"score":"56.04026"}{"text":"2-Planar .The 2-Planar algorithm ( Gómez - Rodríguez and Nivre , 2010 ) is a linear - time algorithm that can be used to parse 2-planar dependency structures , i.e. , those whose links may be coloured with two colours in such a way that no two same - coloured links cross .","label":"Uses","metadata":{},"score":"56.299633"}{"text":"2-Planar .The 2-Planar algorithm ( Gómez - Rodríguez and Nivre , 2010 ) is a linear - time algorithm that can be used to parse 2-planar dependency structures , i.e. , those whose links may be coloured with two colours in such a way that no two same - coloured links cross .","label":"Uses","metadata":{},"score":"56.299633"}{"text":"[ pdf ] Documentation .Resources .Contact .Introduction .MaltParser is a system for data - driven dependency parsing , which can be used to induce a parsing model from treebank data and to parse new data using an induced model .","label":"Uses","metadata":{},"score":"56.440285"}{"text":"The second extension is to apply the approach to secondorder parsing models , such as those described in ( Carreras , 2007 ) , using a twostage semi - supervised learning approach .We demonstrate the effectiveness of our proposed methods on dependency parsing experiments using two widely used test collections : the Penn Treebank for English , and the Prague Dependency Treebank . \" ...","label":"Uses","metadata":{},"score":"56.460876"}{"text":"LIBLINEAR :A library for large linear classification .Journal of Machine Learning Research 9 , 1871 - 1874 .Hall , J. ( 2008 )Transition - Based Natural Language Parsing with Dependency and Constituency Representations .Acta Wexionensia , No 152/2008 , Computer Science , Växjö University ( PhD Thesis ) .","label":"Uses","metadata":{},"score":"56.46709"}{"text":"A Tanl pipeline can be processed in parallel on a cluster of computers by means of a modified version of Hadoop streaming .We present the architecture , its modules and some sample applications . ... trees .The module takes as input a stream of vectors of tokens , and produces a stream of sentences .","label":"Uses","metadata":{},"score":"56.56423"}{"text":"We show how partition functions and marginals for directed spanning trees can be computed by an adaptation of Kirchhoff 's Matrix - Tree ... \" .This paper provides an algorithmic framework for learning statistical models involving directed spanning trees , or equivalently non - projective dependency structures .","label":"Uses","metadata":{},"score":"56.596756"}{"text":"Hall , J. and J. Nivre ( 2008b )Parsing Discontinuous Phrase Structure with Grammatical Functions .In Ranta , A. and Nordstöm , B. ( eds . )In Proceedings of the 6th International Conference on Natural Language Processing ( GoTAL 2008 ) , LNAI 5221 , Springer - Verlag , August 25 - 27 , 2008 , Gothenburg , Sweden , pp . 169 - 180 .","label":"Uses","metadata":{},"score":"56.76797"}{"text":"Perceptron training is widely applied in the natural language processing community for learning complex structured models .Like all structured prediction learning frameworks , the structured perceptron can be costly to train as training complexity is proportional to inference , which is frequently non - linear in example sequence length .","label":"Uses","metadata":{},"score":"57.028847"}{"text":"[ pdf ] .Hall , J. and Nivre , J. ( 2008 )Parsing Discontinuous Phrase Structure with Grammatical Functions .In Ranta , A. and Nordström , B. ( eds . )In Proceedings of the 6th International Conference on Natural Language Processing ( GoTAL 2008 ) , LNAI 5221 , Springer - Verlag , August 25 - 27 , 2008 , Gothenburg , Sweden , pp . 169 - 180 .","label":"Uses","metadata":{},"score":"57.084435"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"Uses","metadata":{},"score":"57.51877"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"Uses","metadata":{},"score":"57.51877"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"Uses","metadata":{},"score":"57.51877"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"Uses","metadata":{},"score":"57.51877"}{"text":", 2004 ; Hall et al . , 2006 ) .MaltParser allows users to define feature models of arbitrary complexity .MaltParser currently includes two machine learning packages ( thanks to Sofia Cassel for her work on LIBLINEAR ) : .","label":"Uses","metadata":{},"score":"57.55094"}{"text":"Contact .Publications .Nivre , J. ( 2003 ) .An Efficient Algorithm for Projective Dependency Parsing .In Proceedings of the 8th International Workshop on Parsing Technologies ( IWPT 03 ) , Nancy , France , 23 - 25 April 2003 , pp .","label":"Uses","metadata":{},"score":"57.71486"}{"text":"Log - linear and maximum - margin models are two commonly - used methods in supervised machine learning , and are frequently used in structured prediction problems .Efficient learning of parameters in these models is therefore an important problem , and becomes a key factor when learning from very large data sets .","label":"Uses","metadata":{},"score":"57.820786"}{"text":"Springer .Nilsson , J. , Löwe W. , Hall , J. and Nivre , J. ( 2009 )Natural Language Parsing for Fact Extraction from Source Code .In Proceedings of 17th IEEE International Conference on Program Comprehension , Vancouver , Canada , pp .","label":"Uses","metadata":{},"score":"58.055145"}{"text":"The reduce on switch option can be used to change the specific behaviour of Switch transitions , while the planar root handling option can be employed to change the algorithm 's behavior with respect to root tokens .The 2-Planar algorithm uses three data structures : .","label":"Uses","metadata":{},"score":"58.30522"}{"text":"The reduce on switch option can be used to change the specific behaviour of Switch transitions , while the planar root handling option can be employed to change the algorithm 's behavior with respect to root tokens .The 2-Planar algorithm uses three data structures : .","label":"Uses","metadata":{},"score":"58.30522"}{"text":"During learning , the configuration is created and stored in a configuration file with the file suffix .mco .This configuration file can later be reused whenever the trained model is used to parse new data .Potentially there can be several types of configuration , but MaltParser 1.8.1 only knows one type : the Single Malt configuration ( singlemalt ) .","label":"Uses","metadata":{},"score":"58.423504"}{"text":"Hall , J. and J. Nivre ( 2008b )Parsing Discontinuous Phrase Structure with Grammatical Functions .In Proceedings of the 6th International Conference on Natural Language Processing ( GoTAL 2008 ) , August 25 - 27 , 2008 , Gothenburg , Sweden .","label":"Uses","metadata":{},"score":"58.549644"}{"text":"Propagation .Since MaltParser 1.4 it is possible to propagate column values towards the root of the dependency graph when a labeled transition is performed .The propagation is managed by a propagation specification file formatted in XML with the following attributes : .","label":"Uses","metadata":{},"score":"58.570675"}{"text":"Propagation .Since MaltParser 1.4 it is possible to propagate column values towards the root of the dependency graph when a labeled transition is performed .The propagation is managed by a propagation specification file formatted in XML with the following attributes : .","label":"Uses","metadata":{},"score":"58.570675"}{"text":"Parse data with your parsing model .We have now created a parsing model that we can use for parsing new sentences from the same language .It is important that unparsed sentences are formatted according to the format that was used during training ( except that the output columns for head and dependency relation are missing ) .","label":"Uses","metadata":{},"score":"58.580563"}{"text":"Parse data with your parsing model .We have now created a parsing model that we can use for parsing new sentences from the same language .It is important that unparsed sentences are formatted according to the format that was used during training ( except that the output columns for head and dependency relation are missing ) .","label":"Uses","metadata":{},"score":"58.580563"}{"text":"( Text , sectionn 14.7 ) .Dependency Parsers : the accuracy of dependency parsers is generally stated in terms of the fraction of tokens for which the proper head and dependency label is assigned ; unlabeled dependency may also be reported ( see , for example , Nivre and Scholz ) .","label":"Uses","metadata":{},"score":"58.716637"}{"text":"Figure 1 summarizes the system architecture .We detail the parsing All authors contributed equally to this work . ...The parser processes input tokens advancing on the input from left to right with Shift actions and accumulates processed tokens on a stack with ... . \" ...","label":"Uses","metadata":{},"score":"58.82048"}{"text":"Such features can help accuracy - as we show .Hence we seek approximations .We will show how BP 's \" message - passing \" discipline offers a principled way for higher - order features to incrementally adjust the numerical edge weights that are fed to ... . by Terry Koo , Alexander M. Rush , Michael Collins , Tommi Jaakkola , David Sontag - In Proc . of EMNLP , 2010 . \" ...","label":"Uses","metadata":{},"score":"58.927055"}{"text":"In Bunt , H. , Merlo , P. and Nivre , J. ( eds . )New Trends in Parsing Technology .Springer .Nivre , J. ( 2009 ) Non - Projective Dependency Parsing in Expected Linear Time .In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP , 351 - 359 .","label":"Uses","metadata":{},"score":"58.942"}{"text":"Nilsson , J. , Löwe W. , Hall , J. and Nivre , J. ( 2009 )Parsing Formal Languages using Natural Language Parsing Techniques .In Proceedings of 11th International Conference on Parsing Technologies ( IWPT ) , Paris , France , pp . to appear .","label":"Uses","metadata":{},"score":"59.18051"}{"text":"The prediction strategy -gdsT.TRANS;A.DEPREL , A.HEADREL , A.PHRASE , A.ATTACH tells the parser to first predict the transition T.TRANS and if it is a left or right arc transition it continues to predict the sublabels A.DEPREL , A.HEADREL , A.PHRASE and A.ATTACH in that order .","label":"Uses","metadata":{},"score":"59.25633"}{"text":"( If one of the address functions is undefined , a null - value is returned . )This feature function can be used to define features over the dependency graph predicted by another parser and given as input to MaltParser .","label":"Uses","metadata":{},"score":"59.298996"}{"text":"( If one of the address functions is undefined , a null - value is returned . )This feature function can be used to define features over the dependency graph predicted by another parser and given as input to MaltParser .","label":"Uses","metadata":{},"score":"59.298996"}{"text":"Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext .","label":"Uses","metadata":{},"score":"59.342915"}{"text":"99 - 106 .[ pdf ] .Nivre , J. , J. Hall , J. Nilsson , G. Eryigit and S. Marinov ( 2006 ) .Labeled Pseudo - Projective Dependency Parsing with Support Vector Machines .In Proceedings of the Tenth Conference on Computational Natural Language Learning ( CoNLL ) .","label":"Uses","metadata":{},"score":"59.343887"}{"text":"Nivre 's algorithm uses two data structures : .A stack Stack of partially processed tokens , where Stack[i ] is the i+1th token from the top of the stack , with the top being Stack[0 ] .A list Input of remaining input tokens , where Input[i ] is the i+1th token in the list , with the first token being Input[0 ] .","label":"Uses","metadata":{},"score":"59.370483"}{"text":"Nivre 's algorithm uses two data structures : .A stack Stack of partially processed tokens , where Stack[i ] is the i+1th token from the top of the stack , with the top being Stack[0 ] .A list Input of remaining input tokens , where Input[i ] is the i+1th token in the list , with the first token being Input[0 ] .","label":"Uses","metadata":{},"score":"59.370483"}{"text":"We propose a new algorithm for approximate MAP inference on factor graphs , by combining augmented Lagrangian optimization with the dual decomposition method .Each slave subproblem is given a quadratic penalty , which pushes toward faster consensus than in previous subgradient approaches .","label":"Uses","metadata":{},"score":"59.47999"}{"text":"We propose a new algorithm for approximate MAP inference on factor graphs , by combining augmented Lagrangian optimization with the dual decomposition method .Each slave subproblem is given a quadratic penalty , which pushes toward faster consensus than in previous subgradient approaches .","label":"Uses","metadata":{},"score":"59.47999"}{"text":"Covington 's algorithm ( Covington 2001 ) is a quadratic - time algorithm for unrestricted dependency structures , which proceeds by trying to link each new token to each preceding token .It can be run in a projective ( -a covproj ) mode , where the linking operation is restricted to projective dependency structures , or in a non - projective ( -a covnonproj ) mode , allowing non - projective ( but acyclic ) dependency structures .","label":"Uses","metadata":{},"score":"59.540985"}{"text":"FEATURE MODEL .Outputs the content of the feature specification file .INTERFACE .Information about the interface to the learner , in this case LIBSVM .SETTINGS .All settings of specific learner options , in this case LIBSVM .Unpack a configuration .","label":"Uses","metadata":{},"score":"59.647034"}{"text":"Classifier ... \" .This paper describes the DeSRL system , a joined effort of Yahoo !Research Barcelona and Università di Pisa for the CoNLL-2008 Shared Task ( Surdeanu et al . , 2008 ) .The system is characterized by an efficient pipeline of linear complexity components , each carrying out a different sub - task .","label":"Uses","metadata":{},"score":"59.942642"}{"text":"INPUT .Input data in both learning and parsing mode , such as part - of - speech tags or word forms .DEPENDENCY_EDGE_LABEL .Denote that the column contain a dependency label .If the parser is to learn to produce labeled dependency graph , these must be present in learning mode .","label":"Uses","metadata":{},"score":"60.0507"}{"text":"Hall , J. ( 2006 )MaltParser : An Architecture for Labeled Inductive Dependency Parsing .Licentiate thesis , Växjö University .[ pdf ] .Nivre , J. ( 2006 ) Inductive Dependency Parsing .Springer .Nilsson , J. , J. Nivre and J. Hall .","label":"Uses","metadata":{},"score":"60.13549"}{"text":"[ pdf ] .Hall , J. , J. Nilsson , J. Nivre , G. Eryigit , B. Megyesi , M. Nilsson and M. Saers ( 2007 ) .Single Malt or Blended ?A Study in Multilingual Parser Optimization .In Proceedings of the CoNLL Shared Task Session of EMNLP - CoNLL 2007 , 933 - -939 .","label":"Uses","metadata":{},"score":"60.35289"}{"text":"We show how it can efficiently handle problems with ( possibly global ) structural constraints via simple sort operations .Experiments on synthetic and real - world data show that our approach compares favorably with the state - of - the - art .","label":"Uses","metadata":{},"score":"60.361355"}{"text":"Hall , J. and J. Nivre ( 2008 )A Dependency - Driven Parser for German Dependency and Constituency Representations .In Proceedings of the ACL Workshop on Parsing German ( PaGe08 ) , June 20 , 2008 , Columbus , Ohio , US , pp .","label":"Uses","metadata":{},"score":"60.53969"}{"text":"There are seven dependency graph address functions : . head .Returns the head of the graph node if defined ; otherwise , a null - value . ldep .Returns the leftmost ( left ) dependent of the graph node if defined ; otherwise , a null - value . rdep .","label":"Uses","metadata":{},"score":"60.617584"}{"text":"There are seven dependency graph address functions : . head .Returns the head of the graph node if defined ; otherwise , a null - value . ldep .Returns the leftmost ( left ) dependent of the graph node if defined ; otherwise , a null - value . rdep .","label":"Uses","metadata":{},"score":"60.617584"}{"text":"A Dependency - Driven Parser for German Dependency and Constituency Representations .In Proceedings of the ACL Workshop on Parsing German ( PaGe08 ) , June 20 , 2008 , Columbus , Ohio , US , pp .x - x .","label":"Uses","metadata":{},"score":"60.659664"}{"text":"A Dependency - Driven Parser for German Dependency and Constituency Representations .In Proceedings of the ACL Workshop on Parsing German ( PaGe08 ) , June 20 , 2008 , Columbus , Ohio , US , pp .x - x .","label":"Uses","metadata":{},"score":"60.659664"}{"text":"..But first of all , we need to define the notion of a dependency graph a little more precisely . \" ...This paper describes the DeSRL system , a joined effort of Yahoo !Research Barcelona and Università di Pisa for the CoNLL-2008 Shared Task ( Surdeanu et al . , 2008 ) .","label":"Uses","metadata":{},"score":"60.71342"}{"text":"This robustness led to the third best overall average labeled attachment score in the task , despite using no discriminative methods .We also demonstrate that the parser is quite fast , and can provide even faster parsing times without much loss of accuracy . \" ...","label":"Uses","metadata":{},"score":"60.817444"}{"text":"This robustness led to the third best overall average labeled attachment score in the task , despite using no discriminative methods .We also demonstrate that the parser is quite fast , and can provide even faster parsing times without much loss of accuracy . \" ...","label":"Uses","metadata":{},"score":"60.817444"}{"text":"In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP , 351 - 359 .Nivre , J. , Kuhlmann , M. and Hall , J. ( 2009 )","label":"Uses","metadata":{},"score":"61.038857"}{"text":"In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP , 351 - 359 .Nivre , J. , Kuhlmann , M. and Hall , J. ( 2009 )","label":"Uses","metadata":{},"score":"61.038857"}{"text":"However , parsing accuracies for Arabic usually lag behind non - semitic languages .Moreover , whil ... \" ...We formulate dependency parsing as a graphical model with the novel ingredient of global constraints .We show how to apply loopy belief propagation ( BP ) , a simple and effective tool for approximate learning and inference .","label":"Uses","metadata":{},"score":"61.222656"}{"text":"The Projective Stack algorithm uses essentially the same transitions as the arc - standard version of Nivre 's algorithm and is limited to projective dependency trees .The Eager and Lazy Stack algorithms in addition make use of a swap transition , which makes it possible to derive arbitrary non - projective dependency trees .","label":"Uses","metadata":{},"score":"61.22731"}{"text":"In addition , there are two options , allow shift and allow root , that controls the behavior of Covington 's algorithm .Covington 's algorithm uses four data structures : .A list Left of partially processed tokens , where Left[i ] is the i+1th token in the list , with the first token being Left[0 ] .","label":"Uses","metadata":{},"score":"61.250797"}{"text":"In Proceedings of the 21stInternational Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics ( COLING - ACL ) Main ConferencePoster Sessions , 316 - 323 .[ pdf ] .Nivre , J. , J. Hall and J. Nilsson ( 2006 )","label":"Uses","metadata":{},"score":"61.330402"}{"text":"By running experiments , which allows other programs to train a parser model or parse with a parser model .IO - handling is done by MaltParser .By first initializing a parser model and then calling the method parse ( ) for each sentence that should be parsed by MaltParser .","label":"Uses","metadata":{},"score":"61.40118"}{"text":"Tools . by Kuzman Ganchev , Jennifer Gillenwater , Ben Taskar - In ACL - IJCNLP , 2009 . \" ...Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext .","label":"Uses","metadata":{},"score":"61.44866"}{"text":"The concurrent interface uses a more \" light - weighted \" parser and hopefully supports almost all features .One know exception is feature propagation is not supported in the new \" light - weighted \" parser .To compile the examples in srcex / org / maltparser / examples .","label":"Uses","metadata":{},"score":"61.60333"}{"text":"First predicts the transition ( T.TRANS ) and if the transition does not require any arc label then the nondeterminism is resolved , but if the predicted transition requires an arc label then the parser continues to predict the arc label .","label":"Uses","metadata":{},"score":"61.765175"}{"text":"First predicts the transition ( T.TRANS ) and if the transition does not require any arc label then the nondeterminism is resolved , but if the predicted transition requires an arc label then the parser continues to predict the arc label .","label":"Uses","metadata":{},"score":"61.765175"}{"text":"The tenth CoNLL ( CoNLL - X ) saw a shared task on Multilingual Dependency Parsing .In this paper , we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured .","label":"Uses","metadata":{},"score":"61.775906"}{"text":"Even with second - order features or latent variables , which would make exact parsing considerably slower or NP - hard , BP needs only O(n3 ) time with a small constant factor .Furthermore , such features significantly improve parse accuracy over exact first - order methods .","label":"Uses","metadata":{},"score":"61.865143"}{"text":"To globally model parsing actions of all steps that are taken on the inpu ... \" .Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .They only determine parsing actions stepwisely by a trained classifier .","label":"Uses","metadata":{},"score":"62.08965"}{"text":"To globally model parsing actions of all steps that are taken on the inpu ... \" .Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .They only determine parsing actions stepwisely by a trained classifier .","label":"Uses","metadata":{},"score":"62.08965"}{"text":"Finally , we try to draw general conclusions about multi - lingual parsing : What makes a particular language , treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser ?Acknowledgement Many thanks to Amit Dubey and Yuval Krymolowski , the other two organizers of the shared task , for discussions , converting treebanks , writing software and helping with the papers . \" ...","label":"Uses","metadata":{},"score":"62.29755"}{"text":"Unfortunately the sentence in Figure 1(b ) is highly unusual in its amount of dependency conservation .To get a feel for the typical case , we used off - the - shelf parsers ( McDonald et al . , 2005 ) for E .. by Ivan Titov , James Henderson - IN PROCEEDINGS OF CONLL-2007 SHARED TASK .","label":"Uses","metadata":{},"score":"62.56119"}{"text":"Unfortunately the sentence in Figure 1(b ) is highly unusual in its amount of dependency conservation .To get a feel for the typical case , we used off - the - shelf parsers ( McDonald et al . , 2005 ) for E .. by Ivan Titov , James Henderson - IN PROCEEDINGS OF CONLL-2007 SHARED TASK .","label":"Uses","metadata":{},"score":"62.56119"}{"text":"The bottom half specifies that DEPREL values should be copied to the VALENCY field of the head , whenever an arc labeled by one of the labels listed in the FOR parameter is created .Provided that these labels denote valency - bound functions , this will have the effect of propagating information about satisfaction of valency constraints to the head .","label":"Uses","metadata":{},"score":"62.652542"}{"text":"The bottom half specifies that DEPREL values should be copied to the VALENCY field of the head , whenever an arc labeled by one of the labels listed in the FOR parameter is created .Provided that these labels denote valency - bound functions , this will have the effect of propagating information about satisfaction of valency constraints to the head .","label":"Uses","metadata":{},"score":"62.652542"}{"text":"CSCI - GA.2590 - Natural Language Processing - Spring 2013 Prof. Grishman .Lecture 12 Outline .Statistical Parsers , cont'd .Evaluating Parsers .Constituent Parsers : the accuracy of constituent parsers is stated in terms of labeled constituent recall / precision / F - measure when compared to a standard parse .","label":"Uses","metadata":{},"score":"62.77541"}{"text":"MaltParser uses history - based feature models for predicting the next action in the deterministic derivation of a dependency structure , which means that it uses features of the partially built dependency structure together with features of the ( tagged ) input string .","label":"Uses","metadata":{},"score":"63.012592"}{"text":"MaltParser uses history - based feature models for predicting the next action in the deterministic derivation of a dependency structure , which means that it uses features of the partially built dependency structure together with features of the ( tagged ) input string .","label":"Uses","metadata":{},"score":"63.012592"}{"text":"The system participated in the closed challenge ranking third in the complete problem evaluation with the following scores : 82.06 labeled macro F1 for the overall task , 86.6 labeled attachment for syntactic dependencies , and 77.5 labeled F1 for semantic dependencies .","label":"Uses","metadata":{},"score":"63.07967"}{"text":"In Proceedings of the 21stInternational Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics ( COLING / ACL ) , Sydney , Australia , pp .257 - 264 .[ pdf ] .Hall , J. , J. Nivre and J. Nilsson ( 2006 )","label":"Uses","metadata":{},"score":"63.202003"}{"text":"Based on this i ... \" .Abstract .This paper explores the idea that non - projective dependency parsing can be conceived as the outcome of two interleaved processes , one that sorts the words of a sentence into a canonical order , and one that performs strictly projective dependency parsing on the sorted input .","label":"Uses","metadata":{},"score":"63.54493"}{"text":"They also permit the use of well - understood , generalpurpose learning algorithms .There has been an increased interest in using probabilistic grammars in the Bayesian setting .To date , most of the literature has focused on using a Dirichlet prior .","label":"Uses","metadata":{},"score":"63.773613"}{"text":"They also permit the use of well - understood , generalpurpose learning algorithms .There has been an increased interest in using probabilistic grammars in the Bayesian setting .To date , most of the literature has focused on using a Dirichlet prior .","label":"Uses","metadata":{},"score":"63.773613"}{"text":"Unfortunately , this procedure suffers from severe numerical problems in the low - temperature setting , which prevents its use in DD - Acc where the temperature must be set as O(ɛ/(n log n ) ) .Finally , n .. Documentation .","label":"Uses","metadata":{},"score":"63.87105"}{"text":"Their symbolic component is amenable to inspection by humans , while their probabilistic component helps resolve ambiguity .They also permit the use of well - understood , generalpurpose learn ... \" .Probabilistic grammars offer great flexibility in modeling discrete sequential data like natural language text .","label":"Uses","metadata":{},"score":"63.90339"}{"text":"Their symbolic component is amenable to inspection by humans , while their probabilistic component helps resolve ambiguity .They also permit the use of well - understood , generalpurpose learn ... \" .Probabilistic grammars offer great flexibility in modeling discrete sequential data like natural language text .","label":"Uses","metadata":{},"score":"63.90339"}{"text":"In Proceedings of the 11th International Conference on Parsing Technologies ( IWPT'09 ) .Nivre , J. and J. Nilsson ( 2005 )Pseudo - Projective Dependency Parsing .In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics , pp .","label":"Uses","metadata":{},"score":"63.936905"}{"text":"In Proceedings of the 11th International Conference on Parsing Technologies ( IWPT'09 ) .Nivre , J. and J. Nilsson ( 2005 )Pseudo - Projective Dependency Parsing .In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics , pp .","label":"Uses","metadata":{},"score":"63.936905"}{"text":"There are two ways to call the MaltParserService : .By running experiments , which allows other programs to train a parser model or parse with a parser model .IO - handling is done by MaltParser .By first initializing a parser model and then calling the method parse ( ) for each sentence that should be parsed by MaltParser .","label":"Uses","metadata":{},"score":"63.98198"}{"text":"Chang , C.-C. and Lin , C.-J. ( 2001 )LIBSVM : a library for support vector machines .Fan , R.-E. , Chang , K.-W. , Hsieh , C.-J. , Wang , X.-R. and Lin , C.-J. ( 2008 )LIBLINEAR :","label":"Uses","metadata":{},"score":"64.012344"}{"text":"In Human Language Technologies 2007 : The Conference of the North American Chapter of the Association for Computational Linguistics ; Proceedings of the Main Conference , pp .396 - 403 [ pdf ] .Nivre , J. , J. Hall , J. Nilsson , A. Chanev , G. Eryigit , S. Kübler , S. Marinov and E. Marsi ( 2007 ) .","label":"Uses","metadata":{},"score":"64.13699"}{"text":"Type .Description .Address function .There are two types of address functions : parsing algorithm specific functions and dependency graph functions .The parsing algorithm specific functions have the form Data - structure[i ] , where Data - structure is a data structure used by a specific parsing algorithm and i is an offset from the start position in this data structure .","label":"Uses","metadata":{},"score":"64.40149"}{"text":"Type .Description .Address function .There are two types of address functions : parsing algorithm specific functions and dependency graph functions .The parsing algorithm specific functions have the form Data - structure[i ] , where Data - structure is a data structure used by a specific parsing algorithm and i is an offset from the start position in this data structure .","label":"Uses","metadata":{},"score":"64.40149"}{"text":"It is possible to define your own feature model specification using the description above and using the --guide - features option to specify the feature model specification file .Learner .MaltParser can be used with different learning algorithms to induce classifiers from training data .","label":"Uses","metadata":{},"score":"64.421104"}{"text":"It continues with information about the learning models that are created , in this case only one LIBSVM model .It then saves the symbol table and all options ( which can not be changed later during parsing ) and stores everything in a configuration file named test.mco .","label":"Uses","metadata":{},"score":"64.725685"}{"text":"It continues with information about the learning models that are created , in this case only one LIBSVM model .It then saves the symbol table and all options ( which can not be changed later during parsing ) and stores everything in a configuration file named test.mco .","label":"Uses","metadata":{},"score":"64.725685"}{"text":"MaltParser have seven pre - defined flow charts that describe what tasks MaltPasrer should perform .These seven flow charts are : .Name .Description . learn .Creates a Single Malt configuration and induces a parsing model from input data . parse .","label":"Uses","metadata":{},"score":"65.13957"}{"text":"Chang , C.-C. and C.-J. Lin ( 2001 ) .LIBSVM : A Library for Support Vector Machines .[ pdf ] .Collins , M. ( 1999 ) .Head - Driven Statistical Models for Natural Language Parsing .Ph . D. thesis , University of Pennsylvania .","label":"Uses","metadata":{},"score":"65.35266"}{"text":"Configuration .The purpose of the configuration is to gather information about all settings and files into one file .During learning , the configuration is created and stored in a configuration file with the file suffix .mco .This configuration file can later be reused whenever the trained model is used to parse new data .","label":"Uses","metadata":{},"score":"65.58953"}{"text":"Flow chart .MaltParser have seven pre - defined flow charts that describe what tasks MaltPasrer should perform .These seven flow charts are : .Name .Description . learn .Creates a Single Malt configuration and induces a parsing model from input data . parse .","label":"Uses","metadata":{},"score":"65.74275"}{"text":"These are not common in English , but are much more common in languages with freer word order .Trees with such crossing edges are termed non - projective dependency parses .We will discuss three general approaches : graph - based , transition - based , and easy - first .","label":"Uses","metadata":{},"score":"65.87744"}{"text":"Creates a configuration and projectivizes input data without inducing a parsing model .Get configuration information .Sometimes it is useful to get information about a configuration , for instance , to know which settings have been used when creating the configuration .","label":"Uses","metadata":{},"score":"65.911194"}{"text":"Creates a configuration and projectivizes input data without inducing a parsing model .Get configuration information .Sometimes it is useful to get information about a configuration , for instance , to know which settings have been used when creating the configuration .","label":"Uses","metadata":{},"score":"65.911194"}{"text":"Just like Nivre 's algorithm , the Planar algorithm uses two data structures : .A stack Stack of partially processed tokens , where Stack[i ] is the i+1th token from the top of the stack , with the top being Stack[0 ] .","label":"Uses","metadata":{},"score":"66.07225"}{"text":"Just like Nivre 's algorithm , the Planar algorithm uses two data structures : .A stack Stack of partially processed tokens , where Stack[i ] is the i+1th token from the top of the stack , with the top being Stack[0 ] .","label":"Uses","metadata":{},"score":"66.07225"}{"text":"For each sentence , for each processing step , if the scoring function selects a valid reduction , we perform that reduction and continue .If it selects an invalid action , we reduce the weights associated with the invalid action and increase the weights associated with the valid action .","label":"Uses","metadata":{},"score":"66.25275"}{"text":"The example data sets are formatted according to the CoNLL data format .Note that these data sets are very small and that you need more training data to create a useful parsing model .To train a default parsing model with MaltParser type the following at the command line prompt : .","label":"Uses","metadata":{},"score":"66.53699"}{"text":"We consider generative and di ... \" .Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext .","label":"Uses","metadata":{},"score":"66.70595"}{"text":"Nivre .Nivre 's algorithm ( Nivre 2003 , Nivre 2004 ) is a linear - time algorithm limited to projective dependency structures .It can be run in arc - eager ( -a nivreeager ) or arc - standard ( -a nivrestandard ) mode .","label":"Uses","metadata":{},"score":"66.78544"}{"text":"Nivre .Nivre 's algorithm ( Nivre 2003 , Nivre 2004 ) is a linear - time algorithm limited to projective dependency structures .It can be run in arc - eager ( -a nivreeager ) or arc - standard ( -a nivrestandard ) mode .","label":"Uses","metadata":{},"score":"66.78544"}{"text":"Same as DEPENDENCY_EDGE_LABEL , used by MaltParser version 1.0 - 1.1 .PHRASE_STRUCTURE_EDGE_LABEL .Column containing a phrase structure edge label .PHRASE_STRUCTURE_NODE_LABEL .Column containing a phrase category label .SECONDARY_EDGE_LABEL .Column containing a secondary edge label .HEAD .","label":"Uses","metadata":{},"score":"66.830444"}{"text":"Combines the prediction of the transition ( T.TRANS ) and the arc label ( A.DEPREL ) .This is the default setting of MaltParser 1.1 and was the only setting available for previous versions of MaltParser .T.TRANS , A.DEPREL .First predicts the transition ( T.TRANS ) and continues to predict the arc label ( A.DEPREL ) if the transition requires an arc label .","label":"Uses","metadata":{},"score":"66.8869"}{"text":"Combines the prediction of the transition ( T.TRANS ) and the arc label ( A.DEPREL ) .This is the default setting of MaltParser 1.1 and was the only setting available for previous versions of MaltParser .T.TRANS , A.DEPREL .First predicts the transition ( T.TRANS ) and continues to predict the arc label ( A.DEPREL ) if the transition requires an arc label .","label":"Uses","metadata":{},"score":"66.8869"}{"text":"Tanl pipelines are data driven , i.e. each stage pulls data from the preceding stage and transforms them for use by the next stage .Since data is processed as s ... \" .Tanl ( Natural Language Text Analytics ) is a suite of tools for text analytics based on the software architecture paradigm of data pipelines .","label":"Uses","metadata":{},"score":"67.36847"}{"text":"We generalize the evaluation to other word - types , and show that the performance can be increased to 18 % relative by preserving part - of - speech equivalencies during translation .We further differentiate ourselves from previous work by conducting a second evaluation which examines the accuracy of translating all word types , rather than just nouns .","label":"Uses","metadata":{},"score":"67.38797"}{"text":"We generalize the evaluation to other word - types , and show that the performance can be increased to 18 % relative by preserving part - of - speech equivalencies during translation .We further differentiate ourselves from previous work by conducting a second evaluation which examines the accuracy of translating all word types , rather than just nouns .","label":"Uses","metadata":{},"score":"67.38797"}{"text":"Covington 's algorithm uses four data structures : .A list Left of partially processed tokens , where Left[i ] is the i+1th token in the list , with the first token being Left[0 ] .A list Right of remaining input tokens , where Right[i ] is the i+1th token in the list , with the first token being Right[0 ] .","label":"Uses","metadata":{},"score":"67.38933"}{"text":"With the availabi ... . \" ...The task of paraphrasing is inherently familiar to speakers of all languages .Moreover , the task of automatically generating or extracting semantic equivalences for the various units of language- words , phrases , and sentences - is an important part of natural language processing ( NLP ) and is being inc ... \" .","label":"Uses","metadata":{},"score":"67.39099"}{"text":"The Stack algorithms are described in Nivre ( 2009 ) and Nivre , Kuhlmann and Hall ( 2009 ) .The Stack algorithms use three data structures : .A stack Stack of partially processed tokens , where Stack[i ] is the i+1th token from the top of the stack , with the top being Stack[0 ] .","label":"Uses","metadata":{},"score":"67.46385"}{"text":"The Stack algorithms are described in Nivre ( 2009 ) and Nivre , Kuhlmann and Hall ( 2009 ) .The Stack algorithms use three data structures : .A stack Stack of partially processed tokens , where Stack[i ] is the i+1th token from the top of the stack , with the top being Stack[0 ] .","label":"Uses","metadata":{},"score":"67.46385"}{"text":"The parsing model gets its name from the configuration name , which is specified by the option flag -c without the file suffix .mco .The configuration name is a name of your own choice .The option flag -i tells the parser where to find the input data .","label":"Uses","metadata":{},"score":"67.591095"}{"text":"However , parsing accuracies for Arabic usually lag behind non - semitic languages .Moreover , whil ...Tools . by Kuzman Ganchev , Jennifer Gillenwater , Ben Taskar - In ACL - IJCNLP , 2009 . \" ...Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .","label":"Uses","metadata":{},"score":"67.81561"}{"text":"Partial trees .Since MaltParser 1.4 it is possible to parse with partial trees , i.e. , sentences may be input with a partial dependency structure , a subgraph of a complete dependency tree .To parse with partial trees you need to do the following : .","label":"Uses","metadata":{},"score":"67.83492"}{"text":"Partial trees .Since MaltParser 1.4 it is possible to parse with partial trees , i.e. , sentences may be input with a partial dependency structure , a subgraph of a complete dependency tree .To parse with partial trees you need to do the following : .","label":"Uses","metadata":{},"score":"67.83492"}{"text":"Here you can see the basic usage and options .To get all available options : .Train a parsing model .Now we are ready to train our first parsing model .In the directory examples / data there are two data files talbanken05_train . conll and talbanken05_test .","label":"Uses","metadata":{},"score":"67.97878"}{"text":"Stack .The Projective ( -a stackproj )Stack algorithm uses essentially the same transitions as the arc - standard version of Nivre 's algorithm and is limited to projective dependency trees .The Eager ( -a stackeager ) and Lazy ( -a stacklazy ) Stack algorithms in addition make use of a swap transition , which makes it possible to derive arbitrary non - projective dependency trees .","label":"Uses","metadata":{},"score":"68.00043"}{"text":"s132 J. Nivre et al .Matthias Trautner Kromann , Alberto Lavelli , Haitao Liu , Yuji Matsumoto , Ryan McDonald , Kemal Oflazer , Petya Osenova , Kiril Simov , Yannick Versley , ... . \" ...Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars .","label":"Uses","metadata":{},"score":"68.027985"}{"text":"Maps a feature value onto a new set of values and takes as arguments a feature specification and one or more arguments that control the mapping .There is one feature map function : .Split .Splits the feature value into a set of feature values .","label":"Uses","metadata":{},"score":"68.18918"}{"text":"Maps a feature value onto a new set of values and takes as arguments a feature specification and one or more arguments that control the mapping .There is one feature map function : .Split .Splits the feature value into a set of feature values .","label":"Uses","metadata":{},"score":"68.18918"}{"text":"We explored a single stage approach to opinion mining , retrieving opinionated documents ranked with a special ranking function which exploits an index enriched with opinion tags .A set of subjective words are used as tags for identifying opinionated sentences .","label":"Uses","metadata":{},"score":"68.325645"}{"text":"Moreover , the task of automatically generating or extracting semantic equivalences for the various units of language- words , phrases , and sentences - is an important part of natural language processing ( NLP ) and is being increasingly employed to improve the performance of several NLP applications .","label":"Uses","metadata":{},"score":"68.61435"}{"text":"Example : .This feature function returns the number of words occurring between the token on top of the stack and the first token in the input buffer , with discrete categories 0 , 1 , 2 - 4 and 5- .","label":"Uses","metadata":{},"score":"68.62309"}{"text":"Example : .This feature function returns the number of words occurring between the token on top of the stack and the first token in the input buffer , with discrete categories 0 , 1 , 2 - 4 and 5- .","label":"Uses","metadata":{},"score":"68.62309"}{"text":"MaltParser 1.1 and MaltParser 1.2 can be turned into a phrase structure parser that recovers both continuous and discontinuous phrases with both phrase labels and grammatical functions .Note : The implementation of phrase structure parsing has been removed in later releases of MaltParser .","label":"Uses","metadata":{},"score":"68.66"}{"text":"The option --singlemalt - use_partial_tree need to be set to true by using the command line flag -up true .The two data columns should look like these : .Note : To benefit from the partial dependency structure , the parser model should also be trained on partial trees .","label":"Uses","metadata":{},"score":"68.7232"}{"text":"The option --singlemalt - use_partial_tree need to be set to true by using the command line flag -up true .The two data columns should look like these : .Note : To benefit from the partial dependency structure , the parser model should also be trained on partial trees .","label":"Uses","metadata":{},"score":"68.7232"}{"text":"( 2004 ) ( for English ) , using a different parsing algorithm first presented in Nivre ( 2003 ) . by Joakim Nivre , Ryan Mcdonald - In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies ( ACL-08 : HLT , 2008 . \" ...","label":"Uses","metadata":{},"score":"69.12432"}{"text":"It is possible to define your own input / output format and then supply the data format specification file with the format option .Currently , MaltParser only supports tab - separated data files , which means that a sentence in a data file in the CoNLL data format could look like this : .","label":"Uses","metadata":{},"score":"69.3078"}{"text":"In addition , we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action , using data from thirteen languages .We show that all four algorithms give competitive accuracy , although the non - projective list - based algorithm generally outperforms the projective algorithms for languages with a non - negligible proportion of non - projective constructions .","label":"Uses","metadata":{},"score":"69.66245"}{"text":"We decompose the problem into three subtasks : parsing , predicate identification and classification ( PIC ) , and argument identification and classification ( AIC ) .We address each of these subtasks with separate components without backward feedback between sub - tasks .","label":"Uses","metadata":{},"score":"69.950485"}{"text":"The results show that all three systems achieve competitive performance , with a best labeled attachment score over 88 % .All three parsers benefit from the use of automatically derived lemmas , while morphological features seem to be less important .","label":"Uses","metadata":{},"score":"69.95224"}{"text":"The results show that all three systems achieve competitive performance , with a best labeled attachment score over 88 % .All three parsers benefit from the use of automatically derived lemmas , while morphological features seem to be less important .","label":"Uses","metadata":{},"score":"69.95224"}{"text":"Unpack a configuration .This command will create a new directory test containing the following files : .File .All distinct symbols in the training data , divided into different columns .For example , the column POSTAG in the CoNLL format has its own symbol table with all distinct values occurring in the training data .","label":"Uses","metadata":{},"score":"70.01584"}{"text":"Takes three arguments , an address function , a relation name , and a normalization string , and returns the number of nodes having the specified relation to the node identified by the address function .Valid relation names are ldeps and rdeps and deps ( for left dependent , right dependent and dependent , respectively ) .","label":"Uses","metadata":{},"score":"70.0253"}{"text":"Hall , J. , Nilsson , J. and Nivre , J. ( 2009 ) Single Malt or Blended ?A Study in Multilingual Parser Optimization .In Bunt , H. , Merlo , P. and Nivre , J. ( eds . )","label":"Uses","metadata":{},"score":"70.075806"}{"text":"The latter specification format should be saved in a text file where the file name must end with the file suffix .par .Below you can see an example of the new XML format ( Nivre arc - eager default feature model ) : .","label":"Uses","metadata":{},"score":"70.12692"}{"text":"The latter specification format should be saved in a text file where the file name must end with the file suffix .par .Below you can see an example of the new XML format ( Nivre arc - eager default feature model ) : .","label":"Uses","metadata":{},"score":"70.12692"}{"text":"The head column defines the unlabeled structure of a dependency graph and is also output data of the parser in parsing mode . type .Defines the data type of the column and/or its treatment during learning and parsing : .STRING .","label":"Uses","metadata":{},"score":"70.261856"}{"text":"Same as DEPENDENCY_EDGE_LABEL , used by MaltParser version 1.0 - 1.1 .PHRASE_STRUCTURE_EDGE_LABEL .Denote that the column contain a phrase structure edge label .PHRASE_STRUCTURE_NODE_LABEL .Denote that the column contain a phrase category label .SECONDARY_EDGE_LABEL .Denote that the column contain a secondary edge label .","label":"Uses","metadata":{},"score":"70.26381"}{"text":"You can start to optimize the feature model by using this file examples / covnonproj_ps.xml .We use the Covington non - projective parsing algorithm , because it is capable of parsing non - projective dependency graphs ( a discontinuous phrase structure will result in a non - projective dependency graph ) .","label":"Uses","metadata":{},"score":"70.738235"}{"text":"Below you can see an example of a propagation specification file : .The top half specifies that POSTAG values should be copied to the CJ - POSTAG field of the head , whenever an arc with the label CJ ( for conjunct ) is created .","label":"Uses","metadata":{},"score":"70.91412"}{"text":"Below you can see an example of a propagation specification file : .The top half specifies that POSTAG values should be copied to the CJ - POSTAG field of the head , whenever an arc with the label CJ ( for conjunct ) is created .","label":"Uses","metadata":{},"score":"70.91412"}{"text":"MaltParser 1.1 and later versions can be turned into a phrase structure parser that recovers both continuous and discontinuous phrases with both phrase labels and grammatical functions .Each edge label in the dependency graph is a quadruple consisting of four sublabels ( DEPREL , HEADREL , PHRASE , ATTACH ) .","label":"Uses","metadata":{},"score":"70.98048"}{"text":"Maven repository .Since version 1.7 , MaltParser is also available via the official Maven repository . org.maltparser maltparser 1.8.1 .MaltParser optimization .MaltParser is a fairly complex system with many parameters that need to be optimized .Simply using the system out of the box with default settings is therefore likely to result in suboptimal performance .","label":"Uses","metadata":{},"score":"71.00867"}{"text":"mco .The configuration name is a name of your own choice .The option flag -i tells the parser where to find the input data .The last option flag -m specifies the processing mode learn ( as opposed to parse ) , since in this case we want to induce a model by using the default learning method ( LIBSVM ) .","label":"Uses","metadata":{},"score":"71.12698"}{"text":"For more information about how to use MaltParserService , please see the examples provided in the directory examples / apiexamples / srcex .References .Chang , C.-C. and Lin , C.-J. ( 2001 )LIBSVM : a library for support vector machines .","label":"Uses","metadata":{},"score":"71.18698"}{"text":"Finally , the character encoding can be specified with the charset option and this option is used by MaltParser to define the java class Charset .Parsing Algorithm .Any deterministic parsing algorithm compatible with the MaltParser architecture can be implemented in the MaltParser package .","label":"Uses","metadata":{},"score":"72.00302"}{"text":"which will create a configuration based on the same setting except the parsing algorithm is now nivreeager instead of nivrestandard .If you want to create a configuration that has the same settings as the option file with command - line options , you need to type : .","label":"Uses","metadata":{},"score":"72.12999"}{"text":"Element .Description . experiment .All other elements must be enclosed by an experiment element . optioncontainer .It is possible to have one or more option containers , but MaltParser 1.4.1 only uses the first option container .Later releases may make use of multiple option containers , for instance , to build ensemble systems . optiongroup .","label":"Uses","metadata":{},"score":"72.30817"}{"text":"Example : .InputArcDir(PHEAD , Stack[0 ] ) .InputTable .Takes two arguments , a column name and an address function , and returns the column value for the node identified by the address function .The column name must correspond to a new column defined in a propagation specification and the address function must return a token node in the input string .","label":"Uses","metadata":{},"score":"72.40513"}{"text":"The information is grouped into different categories : .Category .Description .CONFIGURATION .The name and type of the configuration and the date when it was created .SYSTEM .Information about the system that was used when creating the configuration , such as processor , operating system and version of Java Runtime Environment ( JRE ) .","label":"Uses","metadata":{},"score":"72.72571"}{"text":"We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data .Results indicate that ( a ) MST - parser performs better on Hebrew data than Malt - Parser , and ( b ) both parsers do not make good use of morphological information when parsing Hebrew . ... s on Hebrew dependency parsing .","label":"Uses","metadata":{},"score":"72.7343"}{"text":"We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data .Results indicate that ( a ) MST - parser performs better on Hebrew data than Malt - Parser , and ( b ) both parsers do not make good use of morphological information when parsing Hebrew . ... s on Hebrew dependency parsing .","label":"Uses","metadata":{},"score":"72.7343"}{"text":"It is possible to have one or more option containers , but MaltParser 1.8.1 only uses the first option container .Later releases may make use of multiple option containers , for instance , to build ensemble systems . optiongroup .There can be one or more option group elements within an option container .","label":"Uses","metadata":{},"score":"72.82155"}{"text":"To differentiate the feature model when using sequential prediction you can specify two submodels for T.TRANS and A.DEPREL .Here is a truncated example : .When using branching prediction it is possible to use three submodels ( T.TRANS , RA.A.DEPREL and LA.A.DEPREL ) , where RA denotes the right arc model and LA the left arc model : .","label":"Uses","metadata":{},"score":"72.96278"}{"text":"To differentiate the feature model when using sequential prediction you can specify two submodels for T.TRANS and A.DEPREL .Here is a truncated example : .When using branching prediction it is possible to use three submodels ( T.TRANS , RA.A.DEPREL and LA.A.DEPREL ) , where RA denotes the right arc model and LA the left arc model : .","label":"Uses","metadata":{},"score":"72.96278"}{"text":"Natural Language Engineering , 13(2 ) , 95 - 135 .[ pdf ] .Hall , J. , J. Nivre and J. Nilsson .( 2007 )A hybrid constituency - dependency parser for Swedish .In Proceedings of NODALIDA-2007 , Tartu , Estonia , pp .","label":"Uses","metadata":{},"score":"73.17444"}{"text":"Is a shorter version of Command - line option group and option name and can only be used when the option name is unambiguous .Option file .The option settings are specified in a option file , formatted in XML .","label":"Uses","metadata":{},"score":"73.38966"}{"text":"Is a shorter version of Command - line option group and option name and can only be used when the option name is unambiguous .Option file .The option settings are specified in a option file , formatted in XML .","label":"Uses","metadata":{},"score":"73.38966"}{"text":"most languages are projective .In Figure 8 An example Chinese dependency tree .Although non - projec ... . \" ...Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .","label":"Uses","metadata":{},"score":"73.547745"}{"text":"most languages are projective .In Figure 8 An example Chinese dependency tree .Although non - projec ... . \" ...Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .","label":"Uses","metadata":{},"score":"73.547745"}{"text":"Takes three arguments , two address functions and a normalization string , and returns the string distance ( number of intervening words ) between the words identified by the address functions .The list must start with 0 and be sorted in ascending order .","label":"Uses","metadata":{},"score":"74.33658"}{"text":"Takes three arguments , two address functions and a normalization string , and returns the string distance ( number of intervening words ) between the words identified by the address functions .The list must start with 0 and be sorted in ascending order .","label":"Uses","metadata":{},"score":"74.33658"}{"text":"These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .We present a new measure , Neutral Edge Direction ( NED ) , and show that it greatly reduces this undesired phenomenon .","label":"Uses","metadata":{},"score":"74.72451"}{"text":"These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .We present a new measure , Neutral Edge Direction ( NED ) , and show that it greatly reduces this undesired phenomenon .","label":"Uses","metadata":{},"score":"74.72451"}{"text":"Takes three arguments , an address function , a relation name , and a normalization string , and returns the number of nodes having the specified relation to the node identified by the address function .Valid relation names are ldep , rdep and dep ( for left dependent , right dependent and dependent , respectively ) .","label":"Uses","metadata":{},"score":"74.862076"}{"text":"Returns the next left ( same - side ) sibling of the graph node if defined ; otherwise , a null - value . rsib .Returns the next right ( same - side ) sibling of the graph node if defined ; otherwise , a null - value .","label":"Uses","metadata":{},"score":"74.99378"}{"text":"Returns the next left ( same - side ) sibling of the graph node if defined ; otherwise , a null - value . rsib .Returns the next right ( same - side ) sibling of the graph node if defined ; otherwise , a null - value .","label":"Uses","metadata":{},"score":"74.99378"}{"text":"All option settings that can not be changed during parsing . symboltables.sym .All distinct symbols in the training data , divided into different columns .For example , the column POSTAG in the CoNLL format has its own symbol table with all distinct values occurring in the training data . test_singlemalt . info .","label":"Uses","metadata":{},"score":"75.01511"}{"text":"We apply the new transition - based parser on typologically different languages such as English , Chinese , Czech , and German and report competitive labeled and unlabeled attachment scores . ... restricted to projective dependency trees and used pseudo - projective parsing ( Kahane et al .","label":"Uses","metadata":{},"score":"75.07513"}{"text":"If you want to create a configuration that has the same settings as the option file with command - line options , you need to type : .To parse using one of the three configurations you simply type : .Configuration .","label":"Uses","metadata":{},"score":"75.24321"}{"text":"We focus on parsing algorithms for nonprojective head automata , a generalization of head - automata models to non - projective structures .The dual decomposition algorithms are simple and efficient , relying on standa ... \" .This paper introduces algorithms for nonprojective parsing based on dual decomposition .","label":"Uses","metadata":{},"score":"75.33501"}{"text":"If no lexical child can be found , then take the rightmost nonterminal child .Another example is CAT : AVP r r[LABEL : HD CAT : AVP ] , which first searches for an outgoing edge label HD if the parent nonterminal is labeled AVP .","label":"Uses","metadata":{},"score":"75.41942"}{"text":"The --graph - head_rules option ( -ghr flag ) specifies the URL or the path to a file that contains a list of head rules .MaltParser API .Other programs can invoke Maltparser in various ways , but the easiest way is to use the org.maltparser.","label":"Uses","metadata":{},"score":"75.80059"}{"text":"( If the address function is undefined , a null - value is returned . )Example : .OutputColumn(DEPREL , Stack[0 ] ) .InputArc .Takes three arguments , a column name and two address functions , and returns LEFT , RIGHT or NULL depending on whether the column value defines a left - pointing , right - pointing or no arc between the two nodes identified by the address functions .","label":"Uses","metadata":{},"score":"75.80323"}{"text":"( If the address function is undefined , a null - value is returned . )Example : .OutputColumn(DEPREL , Stack[0 ] ) .InputArc .Takes three arguments , a column name and two address functions , and returns LEFT , RIGHT or NULL depending on whether the column value defines a left - pointing , right - pointing or no arc between the two nodes identified by the address functions .","label":"Uses","metadata":{},"score":"75.80323"}{"text":"Dependency Parsing of Turkish .Computational Linguistics 34(3 ) , 357 - 389 .Nivre , J. ( 2008 ) Algorithms for Deterministic Incremental Dependency Parsing .Computational Linguistics 34(4 ) , 513 - 553 .Hall , J. , Nilsson , J. and Nivre , J. ( 2010 ) Single Malt or Blended ?","label":"Uses","metadata":{},"score":"75.83152"}{"text":"In Proceedings of the fifth international conference on Language Resources and Evaluation ( LREC2006 ) , May 24 - 26 , 2006 , Genoa , Italy , pp .2216 - 2219 [ pdf ] .Nivre , J. ( 2007 ) .","label":"Uses","metadata":{},"score":"75.875656"}{"text":"( See Nivre & Nilsson ( 2005 ) for more details concerning the encoding schemes . )A dependency file can be projectivized using the head encoding by typing : .There is one additional option for the projectivization called covered_root , which is mainly used for handling dangling punctuation .","label":"Uses","metadata":{},"score":"75.88035"}{"text":"( See Nivre & Nilsson ( 2005 ) for more details concerning the encoding schemes . )A dependency file can be projectivized using the head encoding by typing : .There is one additional option for the projectivization called covered_root , which is mainly used for handling dangling punctuation .","label":"Uses","metadata":{},"score":"75.88035"}{"text":"Returns the predecessor of the graph node in the linear order of the input string if defined ; otherwise , a null - value . succ .Returns the successor of the graph node in the linear order of the input string if defined ; otherwise , a null - value .","label":"Uses","metadata":{},"score":"76.06863"}{"text":"Returns the predecessor of the graph node in the linear order of the input string if defined ; otherwise , a null - value . succ .Returns the successor of the graph node in the linear order of the input string if defined ; otherwise , a null - value .","label":"Uses","metadata":{},"score":"76.06863"}{"text":"Returns the proper leftmost descendant of the graph node if defined ; otherwise , a null - value . rdesc .Returns the rightmost descendant of the graph node if defined ; otherwise , a null - value . prdesc .Returns the proper rightmost descendant of the graph node if defined ; otherwise , a null - value .","label":"Uses","metadata":{},"score":"76.174286"}{"text":"Returns the proper leftmost descendant of the graph node if defined ; otherwise , a null - value . rdesc .Returns the rightmost descendant of the graph node if defined ; otherwise , a null - value . prdesc .Returns the proper rightmost descendant of the graph node if defined ; otherwise , a null - value .","label":"Uses","metadata":{},"score":"76.174286"}{"text":"To run MaltParser with the above option file type : . xml .This command will create a configuration file example1.mco based on the settings in the option file .It is possible to override the options by command - line options , for example : . xml -a nivreeager .","label":"Uses","metadata":{},"score":"76.80105"}{"text":"The column elements have three attributes : .Attribute .Description . name .The column name .Note that the column name can be used by an option and within a feature model specification as an identifier of the column . category .","label":"Uses","metadata":{},"score":"76.91438"}{"text":"The column elements have three attributes : .Attribute .Description . name .The column name .Note that the column name can be used by an option and within a feature model specification as an identifier of the column . category .","label":"Uses","metadata":{},"score":"76.91438"}{"text":"MaltParser 1.0.0 and later releases constitute a complete reimplementation of MaltParser in Java and are distributed with an open source license .The previous versions 0.1 - 0.4 of MaltParser were implemented in C. The Java implementation ( version 1.0.0 and later releases ) replaces the C implementation ( version 0 .","label":"Uses","metadata":{},"score":"76.927315"}{"text":"It will perform a left - to - right search to find the leftmost lexical child .If no lexical child can be found , the head - child of the phrase will be the leftmost phrase child and the lexical head will be the lexical child of the head child recursively .","label":"Uses","metadata":{},"score":"77.23988"}{"text":"The projecitivization and deprojectivization ( below ) , including the encoding schemes , are know as pseudo - projective transformations and are described in more detail in Nivre & Nilsson ( 2005 ) .The only difference compared to Nivre & Nilsson is that it is the most deeply nested non - projective arc that is lifted first , not the shortest one .","label":"Uses","metadata":{},"score":"77.3347"}{"text":"The projecitivization and deprojectivization ( below ) , including the encoding schemes , are know as pseudo - projective transformations and are described in more detail in Nivre & Nilsson ( 2005 ) .The only difference compared to Nivre & Nilsson is that it is the most deeply nested non - projective arc that is lifted first , not the shortest one .","label":"Uses","metadata":{},"score":"77.3347"}{"text":"MaltParser can also be used to deprojectivize a projective file containing pseudo - projective encoding , with or without involving parsing , where it is assumed that the configuration pproj contains the same encoding scheme as during projectivization .It could look like this : .","label":"Uses","metadata":{},"score":"77.780365"}{"text":"Returns the ancestor of the graph node if defined ; otherwise , a null - value .panc .Returns the proper ancestor of the graph node if defined ; otherwise , a null - value .ldesc .Returns the leftmost descendant of the graph node if defined ; otherwise , a null - value .","label":"Uses","metadata":{},"score":"78.22238"}{"text":"Returns the ancestor of the graph node if defined ; otherwise , a null - value .panc .Returns the proper ancestor of the graph node if defined ; otherwise , a null - value .ldesc .Returns the leftmost descendant of the graph node if defined ; otherwise , a null - value .","label":"Uses","metadata":{},"score":"78.22238"}{"text":"Version of MaltParser and when it was built .SETTINGS .All option settings divided into several categories .DEPENDENCIES .In some cases the parser self - corrects when an illegal combination of options is specified or some option is missing .","label":"Uses","metadata":{},"score":"78.732666"}{"text":"Version of MaltParser and when it was built .SETTINGS .All option settings divided into several categories .DEPENDENCIES .In some cases the parser self - corrects when an illegal combination of options is specified or some option is missing .","label":"Uses","metadata":{},"score":"78.732666"}{"text":"In practice , however , this will probably have little impact for the parsing accuracy .Deprojectivize input data .MaltParser can also be used to deprojectivize a projective file containing pseudo - projective encoding , with or without involving parsing , where it is assumed that the configuration pproj contains the same encoding scheme as during projectivization .","label":"Uses","metadata":{},"score":"78.795715"}{"text":"MaltParser outputs the following information : . 1 0s 5 MB . 10 0s 6 MB 32 0s 8 MB Creating Liblinear model odm0.liblinear.moo - Read all training instances .- Train a parser model using LibLinear .- Optimize the memory usage - Save the Liblinear model odm0.liblinear.moo Learning time : 00:00:01 ( 1290 ms ) Finished : Fri May 02 23:45:19 CEST 2014 .","label":"Uses","metadata":{},"score":"78.849976"}{"text":"The value returned is ( a category corresponding to ) the greatest integer in the normalization string that is smaller than or equal to the exact number .Example : .This feature function returns the number of left dependents of the token on top of the stack , with discrete categories 0 , 1 , 2 - 4 and 5- .","label":"Uses","metadata":{},"score":"78.97827"}{"text":"The value returned is ( a category corresponding to ) the greatest integer in the normalization string that is smaller than or equal to the exact number .Example : .This feature function returns the number of left dependents of the token on top of the stack , with discrete categories 0 , 1 , 2 - 4 and 5- .","label":"Uses","metadata":{},"score":"78.97827"}{"text":"The following specification defines a feature the value of which the part - of - speech of the top token of the stack and the next input token are merged into one feature value .Merge(InputColumn(POSTAG , Stack[0 ] ) , InputColumn(POSTAG , Input[0 ] ) ) .","label":"Uses","metadata":{},"score":"79.16438"}{"text":"The following specification defines a feature the value of which the part - of - speech of the top token of the stack and the next input token are merged into one feature value .Merge(InputColumn(POSTAG , Stack[0 ] ) , InputColumn(POSTAG , Input[0 ] ) ) .","label":"Uses","metadata":{},"score":"79.16438"}{"text":"The dual decomposition algorithms are simple and efficient , relying on standard dynamic programming and minimum spanning tree algorithms .They provably solve an LP relaxation of the non - projective parsing problem .Empirically the LP relaxation is very often tight : for many languages , exact solutions are achieved on over 98 % of test sentences .","label":"Uses","metadata":{},"score":"79.27217"}{"text":"To train a default parsing model with MaltParser type the following at the command line prompt : .This line tells MaltParser to create a parsing model named test.mco ( also know as a Single Malt configuration file ) from the data in the file examples / data / talbanken05_train.conll .","label":"Uses","metadata":{},"score":"79.53192"}{"text":"Here is an example ( examples / optionexample . xml ) : .To run MaltParser with the above option file type : . xml .This command will create a configuration file example1.mco based on the settings in the option file .","label":"Uses","metadata":{},"score":"79.60078"}{"text":"The CoNLL data format specification file looks like this : .A data format specification file has two types of XML elements .First , there is the dataformat element with the attribute name , which gives the data format a name .","label":"Uses","metadata":{},"score":"79.65166"}{"text":"The CoNLL data format specification file looks like this : .A data format specification file has two types of XML elements .First , there is the dataformat element with the attribute name , which gives the data format a name .","label":"Uses","metadata":{},"score":"79.65166"}{"text":"In indexing the collection , we recovered the relevant content from the blog permalink pages , exploiting HTML metadata about the generator and heuristics to remove irrelevant parts from the body .The index also contains information about the occurrence of opinionated words , extracted from an analysis of WordNet glosses .","label":"Uses","metadata":{},"score":"79.820984"}{"text":"LIBSVM .LIBSVM ( Chang and Lin 2001 ) is a machine learning package for support vector machines with different kernels .Information about different options can be found on the LIBSVM web site .LIBLINEAR .LIBLINEAR ( Fan et al .","label":"Uses","metadata":{},"score":"79.928375"}{"text":"Recent work done in manual and automatic construction of paraphrase corpora is also examined .We also discuss the strategies used for evaluating paraphrase generation techniques and briefly explore some future trends in paraphrase generation .this disparity could be that paraphrasing is not an application in and of itself .","label":"Uses","metadata":{},"score":"80.00328"}{"text":"The file contains several head - finding rules ( one per row ) .The third column is a priority list of children .For example the first row CAT : AA r r[LABEL : HD ] indicates that the parser should first perform a right - to - left search for an outgoing edge with a label HD if the parent nonterminal is labeled AA .","label":"Uses","metadata":{},"score":"80.32525"}{"text":"Description .CONFIGURATION .The name and type of the configuration and the date when it was created .SYSTEM .Information about the system that was used when creating the configuration , such as processor , operating system and version of Java Runtime Environment ( JRE ) .","label":"Uses","metadata":{},"score":"80.3304"}{"text":"Merge three feature value into one feature value .The following specification defines a feature the value of which the part - of - speech of the three next input token are merged into one feature value .Merge3(InputColumn(POSTAG , Input[0 ] ) , InputColumn(POSTAG , Input[1 ] ) , InputColumn(POSTAG , Input[2 ] ) ) .","label":"Uses","metadata":{},"score":"80.42946"}{"text":"Merge three feature value into one feature value .The following specification defines a feature the value of which the part - of - speech of the three next input token are merged into one feature value .Merge3(InputColumn(POSTAG , Input[0 ] ) , InputColumn(POSTAG , Input[1 ] ) , InputColumn(POSTAG , Input[2 ] ) ) .","label":"Uses","metadata":{},"score":"80.42946"}{"text":"Deterministic Dependency Parsing of English Text .In Proceedings of COLING 2004 , Geneva , Switzerland , August 23 - 27 , 2004 .[ pdf ] .Nivre , J. and J. Nilsson ( 2005 )Pseudo - Projective Dependency Parsing .","label":"Uses","metadata":{},"score":"80.52118"}{"text":"FOR .A subset of values that can be copied ( other values will not be copied ) .If empty then all values will be copied .OVER .A subset of dependency labels that allow propagation when a labeled transition is performed .","label":"Uses","metadata":{},"score":"80.70514"}{"text":"FOR .A subset of values that can be copied ( other values will not be copied ) .If empty then all values will be copied .OVER .A subset of dependency labels that allow propagation when a labeled transition is performed .","label":"Uses","metadata":{},"score":"80.70514"}{"text":"_ P IP _ 2 IP _ _ .Finally , the character encoding can be specified with the charset option and this option is used by MaltParser to define the java class Charset .Parsing Algorithm .Any deterministic parsing algorithm compatible with the MaltParser architecture can be implemented in the MaltParser package .","label":"Uses","metadata":{},"score":"80.78814"}{"text":"Now we are ready to train our first parsing model .In the directory examples / data there are two data files talbanken05_train . conll and talbanken05_test .conll , which contain very small portions of the Swedish treebank Talbanken05 .The example data sets are formatted according to the CoNLL data format .","label":"Uses","metadata":{},"score":"81.40637"}{"text":"The linear time complexity of the stack - based algorithms gives them an advantage with respect to efficiency both in learning and in parsing , but the projective list - based algorithm turns out to be equally efficient in practice .Moreover , when the projective algorithms are used to implement pseudo - projective parsing , they sometimes become less efficient in parsing ( but not in learning ) than the non - projective list - based algorithm .","label":"Uses","metadata":{},"score":"81.9745"}{"text":"A list Lookahead , which is a suffix of the buffer containing all nodes that have not been on Stack , where Lookahead[i ] is the i+1th token from the start of Lookahead .Note that it is only the swap transition that can move nodes from Stack back to the buffer , which means that for the Projective Stack algorithm Input will always be empty and Lookahead will always contain all the nodes in the buffer .","label":"Uses","metadata":{},"score":"82.20756"}{"text":"A list Lookahead , which is a suffix of the buffer containing all nodes that have not been on Stack , where Lookahead[i ] is the i+1th token from the start of Lookahead .Note that it is only the swap transition that can move nodes from Stack back to the buffer , which means that for the Projective Stack algorithm Input will always be empty and Lookahead will always contain all the nodes in the buffer .","label":"Uses","metadata":{},"score":"82.20756"}{"text":"MaltParser 1.4.1 ----------------------------------------------------------------------------- MALT ( Models and Algorithms for Language Technology ) Group Vaxjo University and Uppsala University Sweden -----------------------------------------------------------------------------Started : Sun Jun 27 15:58:46 CEST 2010 Data Format : file:////home / jha / dev / eclipse / malt / MaltParser / test2/conllx . xml Transition system : Arc - Eager Parser configuration : Nivre with NORMAL root handling Feature model : NivreEager.xml Learner : libsvm Oracle : Arc - Eager . 1 0s 3 MB . 10 1s 2 MB 32 1s 3 MB Creating LIBSVM model odm0.libsvm.mod Learning time : 00:00:03 ( 3500 ms ) Finished : Sun Jun 27 15:58:50 CEST 2010 .","label":"Uses","metadata":{},"score":"82.56088"}{"text":"Prefix .The following specification defines a feature the value of which is the four - character prefix of the word form ( FORM ) of the next input token .Prefix(InputColumn(FORM , Input[0 ] ) , 4 ) .Merge .","label":"Uses","metadata":{},"score":"82.89581"}{"text":"Prefix .The following specification defines a feature the value of which is the four - character prefix of the word form ( FORM ) of the next input token .Prefix(InputColumn(FORM , Input[0 ] ) , 4 ) .Merge .","label":"Uses","metadata":{},"score":"82.89581"}{"text":"Note that command line option settings override the settings in the option file if options are specified twice .Option file .An option file is useful when you have many options that differ from the default value , as is often the case when you are training a parsing model .","label":"Uses","metadata":{},"score":"82.89932"}{"text":"Note that command line option settings override the settings in the option file if options are specified twice .Option file .An option file is useful when you have many options that differ from the default value , as is often the case when you are training a parsing model .","label":"Uses","metadata":{},"score":"82.89932"}{"text":"Suffix .Extract the suffix of a feature value ( only InputColumn ) with a suffix length n .The following specification defines a feature the value of which is the four - character suffix of the word form ( FORM ) of the next input token .","label":"Uses","metadata":{},"score":"83.10745"}{"text":"Suffix .Extract the suffix of a feature value ( only InputColumn ) with a suffix length n .The following specification defines a feature the value of which is the four - character suffix of the word form ( FORM ) of the next input token .","label":"Uses","metadata":{},"score":"83.10745"}{"text":"Note that is is only the encoding schemes head , path and head+path that actively try to recover the non - projective arcs .Input and output format .The format and encoding of the input and output data is controlled by the format , reader , writer and charset options in the input and output option group .","label":"Uses","metadata":{},"score":"83.45921"}{"text":"For instance , 14.4 % of section 23 is tagged differently by ( 1 ) and ( 2 ) 8 .5 The Neutral Edge Direction ( NED ) Me ... . by Shay B. Cohen , Noah A. Smith , Alex Clark , Dorota Glowacka , Colin De La Higuera , Mark Johnson , John Shawe - taylor . \" ...","label":"Uses","metadata":{},"score":"83.86477"}{"text":"For instance , 14.4 % of section 23 is tagged differently by ( 1 ) and ( 2 ) 8 .5 The Neutral Edge Direction ( NED ) Me ... . by Shay B. Cohen , Noah A. Smith , Alex Clark , Dorota Glowacka , Colin De La Higuera , Mark Johnson , John Shawe - taylor . \" ...","label":"Uses","metadata":{},"score":"83.86477"}{"text":"It is possible to projectivize an input file , with or without involving parsing .All non - projective arcs in the input file are replaced by projective arcs by applying a lifting operation .The lifts are encoded in the dependency labels of the lifted arcs .","label":"Uses","metadata":{},"score":"84.015"}{"text":"To parse type the following : .Controlling MaltParser .MaltParser can be controlled by specifying values for a range of different options .The values for these option can be specified in different ways : .Method .Description .Example .","label":"Uses","metadata":{},"score":"84.72456"}{"text":"To parse type the following : .Controlling MaltParser .MaltParser can be controlled by specifying values for a range of different options .The values for these option can be specified in different ways : .Method .Description .Example .","label":"Uses","metadata":{},"score":"84.72456"}{"text":"InputArc(PHEAD , Stack[0 ] , Input[0 ] ) .InputArcDir .The column name must correspond to an input column of integer type in the data format and the address function must return a token node in the input string .( If the address function is undefined , a null - value is returned . )","label":"Uses","metadata":{},"score":"84.84334"}{"text":"InputArc(PHEAD , Stack[0 ] , Input[0 ] ) .InputArcDir .The column name must correspond to an input column of integer type in the data format and the address function must return a token node in the input string .( If the address function is undefined , a null - value is returned . )","label":"Uses","metadata":{},"score":"84.84334"}{"text":"Given that you have training data in the file train.negra formatted as above and a feature specification file , type the following at the command line prompt : .This command will create testps.mco containing a parser model for parsing phrase structure .","label":"Uses","metadata":{},"score":"85.37874"}{"text":"A feature function takes at least one address function as input and returns a feature value defined in terms of the input arguments .There are seven feature functions available : .InputColumn .Takes two arguments , a column name and an address function , and returns the column value for the node identified by the address function .","label":"Uses","metadata":{},"score":"85.40697"}{"text":"A feature function takes at least one address function as input and returns a feature value defined in terms of the input arguments .There are seven feature functions available : .InputColumn .Takes two arguments , a column name and an address function , and returns the column value for the node identified by the address function .","label":"Uses","metadata":{},"score":"85.40697"}{"text":"This section contains a short guide to get familiar with MaltParser .We start by running MaltParser without any arguments by typing the following at the command line prompt ( it is important that you are in the malt-1.4.1 directory ) : .","label":"Uses","metadata":{},"score":"86.295395"}{"text":"option .An option group can consist of one or more option .The element option has two attributes : name that corresponds to an option name and value that is the value of the option .Please consult the description of all available options to see all legal option names and values .","label":"Uses","metadata":{},"score":"86.742836"}{"text":"The file deprojectivized.conll will contain the deprojectivized data .Note that is is only the encoding schemes head , path and head+path that actively try to recover the non - projective arcs .Input and output format .The format and encoding of the input and output data is controlled by the format , reader , writer and charset options in the input and output option group .","label":"Uses","metadata":{},"score":"86.85359"}{"text":"Name .Description . FROM .The data column from which the values are copied .TO .The data column to which the values are copied .This data column should not exist in the data format and the values are interpreted as sets .","label":"Uses","metadata":{},"score":"87.18172"}{"text":"Name .Description . FROM .The data column from which the values are copied .TO .The data column to which the values are copied .This data column should not exist in the data format and the values are interpreted as sets .","label":"Uses","metadata":{},"score":"87.18172"}{"text":"An inactive stack ( InactiveStack ) of partially processed tokens that may be linked on the other plane , where InactiveStack[i ] is the i+1th token from the top of the stack , with the top being InactiveStack[0 ] .A list Input of remaining input tokens , where Input[i ] is the i+1th token in the list , with the first token being Input[0 ] .","label":"Uses","metadata":{},"score":"88.47503"}{"text":"An inactive stack ( InactiveStack ) of partially processed tokens that may be linked on the other plane , where InactiveStack[i ] is the i+1th token from the top of the stack , with the top being InactiveStack[0 ] .A list Input of remaining input tokens , where Input[i ] is the i+1th token in the list , with the first token being Input[0 ] .","label":"Uses","metadata":{},"score":"88.47503"}{"text":"The attribute groupname specifies the option group name ( see description of all available options ) .option .An option group can consist of one or more option .The element option has two attributes : name that corresponds to an option name and value that is the value of the option .","label":"Uses","metadata":{},"score":"89.01446"}{"text":"For more information about how to use MaltParserService , please see the examples provided in the directory examples / apiexamples / srcex / org / maltparser / examples / old .To compile the old examples ( srcex / org / maltparser / examples / old ) used by MaltParser-1.7.2 and previous versions of MaltParser . javac -d classes -cp .","label":"Uses","metadata":{},"score":"89.32868"}{"text":"To parse type the following : .The input file must contain four columns : WORD , LEMMA , POS , MORPH .A test file can look like this : . ''Head - finding rules .It is possible to define your own head - finding rules in a file .","label":"Uses","metadata":{},"score":"90.280235"}{"text":"MaltParser 1.4.1 ----------------------------------------------------------------------------- MALT ( Models and Algorithms for Language Technology ) Group Vaxjo University and Uppsala University Sweden -----------------------------------------------------------------------------Usage : java -jar malt.jar -f . html .Here you can see the basic usage and options .To get all available options : .","label":"Uses","metadata":{},"score":"90.47087"}{"text":"MaltParser API .From version MaltParser-1.8 there is a new interface to MaltParser located in org.maltparser.concurrent and contains following classes : .org.maltparser.concurrent.ConcurrentMaltParserModel .org.maltparser.concurrent.ConcurrentMaltParserService .org.maltparser.concurrent.ConcurrentUtils .This interface can only be used during parsing time and can hopefully be used in a multi - threaded environment .","label":"Uses","metadata":{},"score":"90.84581"}{"text":"Example : .InputTable(CJ - POSTAG , Stack[0 ] ) .Exists .Takes an address function as argument and returns TRUE if the address function returns an existing node ( and FALSE otherwise ) .Example : . Exists(ldep(Stack[0 ] ) ) .","label":"Uses","metadata":{},"score":"91.11867"}{"text":"CONFIGURATION Configuration name : test Configuration type : singlemalt Created : Sun Jul 15 11:59:37 CEST 2010 SYSTEM Operating system architecture : amd64 Operating system name : Linux JRE vendor name : Sun Microsystems Inc.The information is grouped into different categories : .","label":"Uses","metadata":{},"score":"91.32049"}{"text":"Projectivize input data .It is possible to projectivize an input file , with or without involving parsing .All non - projective arcs in the input file are replaced by projective arcs by applying a lifting operation .The lifts are encoded in the dependency labels of the lifted arcs .","label":"Uses","metadata":{},"score":"91.40471"}{"text":"Example : .InputArc(PHEAD , Stack[0 ] ) .Exists .Takes an address function as argument and returns TRUE if the address function returns an existing node ( and FALSE otherwise ) .Example : . Exists(ldep(Stack[0 ] ) ) .","label":"Uses","metadata":{},"score":"92.01216"}{"text":"IGNORE .The column value will be ignored and therefore will not be present in the output file . type .Defines the data type of the column and/or its treatment during learning and parsing : .STRING .The column value will be used as a string value in the feature model .","label":"Uses","metadata":{},"score":"92.051575"}{"text":"/data / swemalt - mini / swedish - swap . xml .Note that swemalt - mini . swemalt - mini . java -cp classes : . /maltparser-1.8.1.jar org.maltparser.examples.ParseSentence1 java -cp classes : . /maltparser-1.8.1.jar org.maltparser.examples.ParseSentence2 java -cp classes : . /maltparser-1.8.1.jar org.maltparser.examples.","label":"Uses","metadata":{},"score":"92.38753"}{"text":"Start using MaltParser .This section contains a short guide to get familiar with MaltParser .We start by running MaltParser without any arguments by typing the following at the command line prompt ( it is important that you are in the maltparser-1.8.1 directory ) : .","label":"Uses","metadata":{},"score":"93.918076"}{"text":"( If the address function is undefined , a null - value is returned . )Example : .InputColumn(POSTAG , Stack[0 ] ) .OutputColumn .Takes two arguments , a column name and an address function , and returns the column value for the node identified by the address function .","label":"Uses","metadata":{},"score":"94.44083"}{"text":"( If the address function is undefined , a null - value is returned . )Example : .InputColumn(POSTAG , Stack[0 ] ) .OutputColumn .Takes two arguments , a column name and an address function , and returns the column value for the node identified by the address function .","label":"Uses","metadata":{},"score":"94.44083"}{"text":"Uses the option flag with a dash ( - ) before the option flag and a blank between the option flag and the value . -c test .Command - line option group and option name .Uses both the option group name and the option name to specify the option , with two dashes ( -- ) before the option group name and one dash ( - ) to separate the option group name and the option name .","label":"Uses","metadata":{},"score":"94.62936"}{"text":"Uses the option flag with a dash ( - ) before the option flag and a blank between the option flag and the value . -c test .Command - line option group and option name .Uses both the option group name and the option name to specify the option , with two dashes ( -- ) before the option group name and one dash ( - ) to separate the option group name and the option name .","label":"Uses","metadata":{},"score":"94.62936"}{"text":"/maltparser-1.8.1 . jar : . java .To run the examples you first need to create a Swedish parser model swemalt - mini .mco by using MaltParser : . java -jar . /maltparser-1.8.1.jar -w output -c swemalt - mini -i .","label":"Uses","metadata":{},"score":"95.184135"}{"text":"This , in turn , results in lots of ( unnecessary ) lifts , and can be avoided by using the covered_root flag -pcr .This option has four values : none , left , right and head .For the last three values , tokens like dangling punctuation are then attached to one of the tokens connected by the shortest arc covering the token , either the leftmost ( left ) , rightmost ( right ) , or head ( head ) token of the covering arc .","label":"Uses","metadata":{},"score":"95.34973"}{"text":"This , in turn , results in lots of ( unnecessary ) lifts , and can be avoided by using the covered_root flag -pcr .This option has four values : none , left , right and head .For the last three values , tokens like dangling punctuation are then attached to one of the tokens connected by the shortest arc covering the token , either the leftmost ( left ) , rightmost ( right ) , or head ( head ) token of the covering arc .","label":"Uses","metadata":{},"score":"95.34973"}{"text":"ConcurrentExample1 java -cp classes : . /maltparser-1.8.1.jar org.maltparser.examples.ConcurrentExample2 java -cp classes : . /maltparser-1.8.1.jar org.maltparser.examples.ConcurrentExample3 .Old MaltParserService interface .Before MaltParser-1.8 there was another interface to MaltParser .Note that this interface can only be used in a single - threaded environment and the interface does n't use the light - weighted parser .","label":"Uses","metadata":{},"score":"96.541016"}{"text":"Intent mining is a special kind of document analysis whose goal is to assess the attitude of the document author with respect to a given subject .Opinion mining is a kind of intent mining where the attitude is a positive or negative opinion .","label":"Uses","metadata":{},"score":"96.62214"}{"text":"Intent mining is a special kind of document analysis whose goal is to assess the attitude of the document author with respect to a given subject .Opinion mining is a kind of intent mining where the attitude is a positive or negative opinion .","label":"Uses","metadata":{},"score":"96.62214"}{"text":"The column value will be used as an integer value in the feature model .BOOLEAN .The column value will be used as a boolean value in the feature model .REAL .The column value will be used as a real value in the feature model . default .","label":"Uses","metadata":{},"score":"98.71919"}{"text":"TrainingExperiment java -cp classes : . /maltparser-1.8.1.jar org.maltparser.examples.old.ParsingExperiment java -cp classes : . /maltparser-1.8.1.jar org.maltparser.examples.old.ParseSentence1 java -cp classes : . /maltparser-1.8.1.jar org.maltparser.examples.old.ParseSentence2 java -cp classes : . /maltparser-1.8.1.jar org.maltparser.examples.old.ParseSentence3 .Other programs can invoke Maltparser in various ways , but the easiest way is to use the org.maltparser.","label":"Uses","metadata":{},"score":"99.331055"}{"text":"The dependency relation DEPREL is the grammatical function of the highest nonterminal of which the dependent is the lexical head .The attachment ATTACH is a non - negative integer that encodes the attachment level of the highest nonterminal of which it is the lexical head .","label":"Uses","metadata":{},"score":"101.93315"}{"text":"To run the old examples .java -cp classes : . /maltparser-1.8.1.jar org.maltparser.examples.old.ReadWriteCoNLL ./data / talbanken05_test.conll out.conll ./appdata / dataformat / conllx .xml UTF-8 java -cp classes : . /maltparser-1.8.1.jar org.maltparser.examples.old.CreateDependencyGraph java -cp classes : . /maltparser-1.8.1.jar org.maltparser.examples.old.","label":"Uses","metadata":{},"score":"102.74418"}{"text":"The column value will be ignored and therefore will not be present in the output file . default .The default output for columns that have the column type IGNORE .It is possible to define your own input / output format and then supply the data format specification file with the format option .","label":"Uses","metadata":{},"score":"111.55791"}{"text":"INTEGER .The column value will be stored as an integer value .BOOLEAN .The column value will be stored as a boolean value .ECHO .The column value will be stored as an integer value , but can not be used in the definition of features .","label":"Uses","metadata":{},"score":"111.795715"}{"text":"Covington projective .Left , Right .Covington non - projective .Left , Right , LeftContext , RightContext .Stack projective .Stack , Input , Lookahead .Planar .Stack , Input . 2-Planar .ActiveStack , InactiveStack , Input .","label":"Uses","metadata":{},"score":"119.810844"}{"text":"Covington projective .Left , Right .Covington non - projective .Left , Right , LeftContext , RightContext .Stack projective .Stack , Input , Lookahead .Planar .Stack , Input . 2-Planar .ActiveStack , InactiveStack , Input .","label":"Uses","metadata":{},"score":"119.810844"}