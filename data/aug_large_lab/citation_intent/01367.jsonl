{"text":"Learning of a Decision Boundary with and without Queries ; A Bayesian .Approach to On - line Learning ; Optimal Perceptron Learning : an On - line .Bayesian Approach .Optimization .Cichocki , A. and Unbehauen , R. ( 1993 ) .","label":"Background","metadata":{},"score":"30.286974"}{"text":"Machine learning techniques like Naïve Bayesian , C4.5 , Decision trees etc are helpful for mobile devices .Some important machine learning applications for mobile devices include Sensor based activity recognition , Mobile text categorization , Malware detection on mobile devices , Language understanding etc .","label":"Background","metadata":{},"score":"32.164284"}{"text":"Working Session on Learning , pages 81 - 92 , 1988 .Kohavi , R. and Sommerfield , D. Oblivious decision trees , graphs , and top - down pruning .In Proc . 14th Intl .Joint Conf . on Artificial Intelligence , pages 1071 - 1077 , Montreal , 1995 .","label":"Background","metadata":{},"score":"32.996605"}{"text":"Direct experimental comparisons with the other learning algorithms show that our nearest neighbor algorithm is comparable or superior ... . by David Opitz , Richard Maclin - Journal of Artificial Intelligence Research , 1999 . \" ...An ensemble consists of a set of individually trained classifiers ( such as neural networks or decision trees ) whose predictions are combined when classifying novel instances .","label":"Background","metadata":{},"score":"33.91542"}{"text":"his book will be of interest even to beginners in neural nets .Chapter headings : Introduction and Examples ; Statistical Decision Theory ; .Linear Discriminant Analysis ; Flexible Discriminants ; Feed - forward Neural .Networks ; Non - parametric Methods ; Tree - structured Classifiers ; Belief .","label":"Background","metadata":{},"score":"34.23678"}{"text":"The wake - sleep algorithm for unsupervised neural networks .Science , 268 , 1158 - 1161 .Hinton , G. E. , Sallans , B. , & Ghahramani , Z. ( 1999 ) .A hierarchical community of experts .In M. I. Jordan ( Ed . ) , Learning in graphical models .","label":"Background","metadata":{},"score":"34.440166"}{"text":"It is used in classification and regression related tasks [ 3].A decision tree works with input data and uses decision rules for future predictions .Decision trees work with rules that are well understood by humans and used in knowledge system such as database .","label":"Background","metadata":{},"score":"34.705997"}{"text":"Machine Learning , 11 , 63 - 90 .Iba , W. , & Langley , P. ( 1992 ) .Induction of one - level decision trees .Proceedings of the Ninth International Conference on Machine Learning ( pp .233 - 240 ) .","label":"Background","metadata":{},"score":"35.50014"}{"text":"Elsevier Science Publishers .Langley , P. , Iba , W. , & Thompson , K. ( 1992 ) .An analysis of Bayesian classifiers .Proceedings of the Tenth National Conference on Artificial Intelligence ( pp .223 - 228 ) .","label":"Background","metadata":{},"score":"35.697865"}{"text":"Heterogeneous Sources of Partial Domain Knowledge ; Approximation of .Differential Equations Using Neural Networks ; Fynesse : A Hybrid Architecture . for Self - Learning Control ; Data Mining Techniques for Designing Neural .Network Time Series Predictors ; Extraction of Decision Trees from Artificial .","label":"Background","metadata":{},"score":"36.164932"}{"text":"Of these two , Kecman ( 2001 ) provides clearer explanations and better . diagrams , but Cherkassky and Mulier ( 1998 ) are better organized have an .excellent section on unsupervised learning , especially self - organizing maps .I have been tempted to add both of these books to the \" best \" list , but I . have not done so because I think VC theory is of doubtful practical utility .","label":"Background","metadata":{},"score":"36.21471"}{"text":"Basic Bayesian decision theory relevant to the problem of learning classification rules is reviewed , then a Bayesian framework for such learning is presented .The framework has three components : the hypothesis space , the learning protocol , and criteria for successful learning .","label":"Background","metadata":{},"score":"36.392746"}{"text":"Chapter headings : 1 .Introduction 2 .The Perceptron 3 .Linear .Autoassociative Memories 4 .Linear Heteroassociative Memories 5 .Error .Backpropagation 6 .Useful References .Bayesian learning .Neal , R. M. ( 1996 )Bayesian Learning for Neural Networks , New York : .","label":"Background","metadata":{},"score":"36.427063"}{"text":"Advances in neural information processing systems 8 ' ( pp .479 - 485 ) .Duda , R. , & Hart , P. ( 1973 ) .Pattern classification and scene analysis .Wiley .Efron , B. , & Tibshirani , R. ( 1993 ) .","label":"Background","metadata":{},"score":"36.618393"}{"text":"Nearest Neighbor Models ; Adaptive Maps ; The BSB Model : A Simple Nonlinear .Autoassociative Neural Network ; Associative Computation ; Teaching Arithmetic . to a Neural Network .Hagan , M.T. , Demuth , H.B. , and Beale , M. ( 1996 ) , Neural Network Design , .","label":"Background","metadata":{},"score":"36.73431"}{"text":"Finally , by studying neural networks in addition to decision trees we can examine how Bagging and Boosti ... . by Paul E. Utgoff , Neil C. Berkman , Jeffery A. Clouse , Doug Fisher - Machine Learning , 1996 .The ability to restructure a decision tree efficiently enables a variety of approaches to decision tree induction that would otherwise be prohibitively expensive .","label":"Background","metadata":{},"score":"36.820774"}{"text":"Artificial Intelligence , 56 , 71 - 113 .Neal , R. ( 1993 ) .Probabilistic inference using Markov chain Monte Carlo methods .( Technical Report CRG - TR-93 - 1 ) .Toronto : Department of Computer Science , University of Toronto .","label":"Background","metadata":{},"score":"36.85234"}{"text":"Morgan Kaufmann .Kwok , S.W. , & Carter , C. ( 1990 ) .Multiple decision trees .In R.D. Schachter , T.S. Levitt , L.N. Kanal , & J.F. Lemmer ( Eds . ) , Uncertainty in Artificial Intelligence ( pp .","label":"Background","metadata":{},"score":"36.882866"}{"text":"The Nineth European Conference on Machine Learning ,Poster Papers ' ( pp .78 - 87 ) .Kohavi , R. , & Kunz , C. ( 1997 ) .Option decision trees with majority votes .In D. Fisher ( Ed . ) , Machine Learning : Proceedings of theFourteenth International Conference ( pp .","label":"Background","metadata":{},"score":"36.996574"}{"text":"There are many supervised learning algorithms as decision trees , K - Nearest Neighbor ( KNN ) , Support Vector Machines ( SVM ) and Random Forests [ 4].Unsupervised learning In Supervised learning there is an outcome or output variable to guide the learning process whereas unsupervised learning works with models without predefined classes or examples [ 5].","label":"Background","metadata":{},"score":"37.3804"}{"text":"After you have read Smith ( 1996 ) or Weiss and Kulikowski ( 1991 ) , consult .Reed and Marks for practical details on training MLPs ( other types of neural .nets such as RBF networks are barely even mentioned ) .","label":"Background","metadata":{},"score":"37.44426"}{"text":"MACHINE LEARNING ALGORITHMS There are various machine learning algorithms depending upon the domain of their application [ 1].The following Machine learning algorithms are considered to give readers a basic understanding of machine learning algorithms . A. Decision Tree A decision tree is a classifier that works with recursive partition of the instance space .","label":"Background","metadata":{},"score":"37.94481"}{"text":"The analysis is done by interpreting a protocol as a .. \" ...This article describes an approach to combining symbolic and connectionist approaches to machine learning .A three - stage framework is presented and the research of several groups is reviewed with respect to this framework .","label":"Background","metadata":{},"score":"38.009613"}{"text":"This article describes an approach to combining symbolic and connectionist approaches to machine learning .A three - stage framework is presented and the research of several groups is reviewed with respect to this framework .The first stage involves the insertion of symbolic knowledge into neural networks , the second addresses the refinement of this prior knowledge in its neural representation , while the third concerns the extraction of the refined symbolic knowledge .","label":"Background","metadata":{},"score":"38.378845"}{"text":"Programming ; Temporal Processing Using Feedforward Networks ; Neurodynamics ; .Dynamically Driven Recurrent Networks .Kecman , V. ( 2001 ) , Learning and Soft Computing : Support Vector Machines , .Neural Networks , and Fuzzy Logic Models , Cambridge , MA : The MIT Press ; .","label":"Background","metadata":{},"score":"38.471825"}{"text":"Jordan , M. I. , Ghahramani , Z. , & Saul , L. K. ( 1997 ) .Hidden Markov decision trees .In M. C. Mozer , M. I. Jordan , & T. Petsche ( Eds . ) , Advances in neural information processing systems 9 .","label":"Background","metadata":{},"score":"38.58102"}{"text":"A tutorial on learning with Bayesian networks .In M. I. Jordan ( Ed . ) , Learning in graphical models .Cambridge , MA : MIT Press .Henrion , M. ( 1991 ) .Search - based methods to bound diagnostic probabilities in very large belief nets .","label":"Background","metadata":{},"score":"38.619987"}{"text":"Journal of Artificial Intelligence Research , 4 , 61 - 76 .Saul , L. K. , & Jordan , M. I. ( 1996 ) .Exploiting tractable substructures in intractable networks .In D. S. Touretzky , M. C. Mozer , & M. E. Hasselmo ( Eds . ) , Advances in neural information processing systems 8 .","label":"Background","metadata":{},"score":"38.628136"}{"text":"A second , perhaps more satisfying , approach is to . provide a forum that encourages the regular production -- . and perusal -- of high - quality survey articles .This is .especially useful in an inter - disciplinary , evolving field . such as neural networks .","label":"Background","metadata":{},"score":"38.99659"}{"text":"Detection , and Bayesian Decision Theory ; Extracting Rules from Networks ; .Appendix : Representation Comparisons .Cloete , I. , and Zurada , J.M. ( 2000 ) , Knowledge - Based Neurocomputing , .Cambridge , MA : The MIT Press , ISBN 0 - 262 - 03274 - 0 .","label":"Background","metadata":{},"score":"39.097702"}{"text":"In this way a constrained device can evaluate an SVM .Whereas decision trees are classifiers that can be properly converted into a rule set that can be further evaluated .The performance of Decision trees can be improved with boosting strategy .","label":"Background","metadata":{},"score":"39.26426"}{"text":"Sebag , M. , & Rouveirol , C. ( 1997 ) .Tractable induction and classification in first order logic via stochastic matching .Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence ( pp .888 - 893 ) .","label":"Background","metadata":{},"score":"39.300205"}{"text":"Pao , Y. H. ( 1989 ) .Adaptive Pattern Recognition and Neural Networks .Addison - Wesley Publishing Company , Inc. ( ISBN 0 - 201 - 12584 - 6 ) .Comments from readers of comp.ai.neural-nets : \" An excellent book that ties . together classical approaches to pattern recognition with Neural Nets .","label":"Background","metadata":{},"score":"39.330482"}{"text":"A Bayesian account and its implications .In D. Heckerman , H. Mannila , D. Pregibon , & R. Uthurusamy ( Eds . ) , Proceedings of the Third International Conference on Knowledge Discovery and Data Mining ( pp .155 - 158 ) .","label":"Background","metadata":{},"score":"39.491493"}{"text":"Computational Intelligence , 5 , 142 - 150 .Dechter , R. ( 1999 ) .Bucket elimination : A unifying framework for probabilistic inference .In M. I. Jordan ( Ed . ) , Learning in graphical models .Cambridge , MA : MIT Press .","label":"Background","metadata":{},"score":"39.790512"}{"text":"History .Knowledge , rules , and expert systems .Learning theory .Object oriented programming .On - line and incremental learning .Optimization .Pulsed / Spiking networks .Recurrent .Reinforcement learning .Speech recognition .Statistics .Time - series forecasting .","label":"Background","metadata":{},"score":"39.982277"}{"text":"Top - down induction of first order logical decision trees .Artificial Intelligence , 101 , 285 - 297 .Blum , A. , & Kalai , A. ( 1998 ) .A note on learning from multiple - instance examples .","label":"Background","metadata":{},"score":"40.034485"}{"text":"Baggi ... \" .An ensemble consists of a set of individually trained classifiers ( such as neural networks or decision trees ) whose predictions are combined when classifying novel instances .Previous research has shown that an ensemble is often more accurate than any of the single classifiers in the ensemble .","label":"Background","metadata":{},"score":"40.117332"}{"text":"Time Series Prediction : .Forecasting the Future and Understanding the Past , Reading , MA : .Addison - Wesley , ISBN 0201626020 .Unsupervised learning .Kohonen , T. ( 1995/1997 ) , Self - Organizing Maps , 1st ed .","label":"Background","metadata":{},"score":"40.21351"}{"text":"He has wisely avoided the temptation to try to cover everything and has .therefore omitted interesting topics like reinforcement learning , Hopfield .networks , and Boltzmann machines in order to focus on the types of neural .networks that are most widely used in practical applications .","label":"Background","metadata":{},"score":"40.32994"}{"text":"Neural networks and the bias / variance dilemma .Neural Computation , 4 , 1 - 48 .Good , I.J. ( 1965 ) .The estimation of probabilities : An essay on modern bayesian methods .M.I.T. Press .Holte , R.C. ( 1993 ) .","label":"Background","metadata":{},"score":"40.4048"}{"text":"Saad , D. , ed .( 1998 ) , On - Line Learning in Neural Networks , Cambridge : .Cambridge University Press , ISBN 0 - 521 - 65263 - 4 .Articles : Introduction ; On - line Learning and Stochastic Approximations ; .","label":"Background","metadata":{},"score":"40.42846"}{"text":"Gascuel , O. and Caraux , G. Statistical significance in inductive learning .In Proc . of the European Conf . on Artificial Intelligence ( ECAI ) , pages 435 - 439 , New York , 1992 .Wiley .Hildebrand , D. Statistical Thinking for Behavioral Scientists .","label":"Background","metadata":{},"score":"40.436768"}{"text":"A view of the EM algorithm that justifies incremental , sparse , and other variants .In M. I. Jordan ( Ed . ) , Learning in graphical models .Cambridge , MA : MIT Press .Parisi , G. ( 1988 ) .","label":"Background","metadata":{},"score":"40.478043"}{"text":"Learning ; Annealed On - line Learning in Multilayer Neural Networks ; On - line .Learning of Prototypes and Principal Components ; On - line Learning with .Time - Correlated Examples ; On - line Learning from Finite Training Sets ; .","label":"Background","metadata":{},"score":"40.776466"}{"text":"Performance can be improved by good prediction accuracy and good sensitivity .Hence , the choice of most appropriate algorithm plays a vital role designing mobile learning applications .[ 10 ] [ 11 ] [ 12 ] [ 13 ] V. CONCLUSION This paper presents a review of machine learning techniques .","label":"Background","metadata":{},"score":"40.818344"}{"text":"We introduce a nearest neighbor algorithm for learning in domains with symbolic features .Our algorithm calculates distance tables that allow it to produce real - valued distances between instances , and attaches weights to the instances to further modify the structure of feature space .","label":"Background","metadata":{},"score":"40.84893"}{"text":"This approach overcomes problems that arise when using imperfect domain theories to build explanations and addresses the problem of choosing a good initial neural network configuration .Empirical results show that the hybrid system more accurately l .. Abstract .This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models ( Bayesian networks and Markov random fields ) .","label":"Background","metadata":{},"score":"40.87404"}{"text":"New York : ACM Press .Maron .O. , & Lozano - Pérez , T. ( 1998 ) .A framework for multiple - instance learning .Advances in Neural Information Processing Systems , 10 .MIT Press .Maron , O. , & Ratan , A. L. ( 1998 ) .","label":"Background","metadata":{},"score":"40.953434"}{"text":"Machine learning is an area where both symbolic and neural approaches have been heavily investigated .However , there has been little research into the synergies achievable by combining these two learning paradigms .A hybrid approach that combines the symbolically - oriented explanation - based learning paradigm with the neural back - propagation algorithm is described .","label":"Background","metadata":{},"score":"41.07649"}{"text":"Sharkey , A.J.C. ( 1999 ) , Combining Artificial Neural Nets : Ensemble and .Modular Multi - Net Systems , London : Springer , ISBN : 185233004X .Connectionism .Elman , J.L. , Bates , E.A. , Johnson , M.H. , Karmiloff - Smith , A. , and Parisi , D. .","label":"Background","metadata":{},"score":"41.08478"}{"text":"Rachlin , John , Simon Kasif , Steven Salzberg , David W. Aha , ( 1994 ) .Towards a Better Understanding of Memory - Based and Bayesian Classifiers .In Proceedings of the Eleventh International Machine Learning Conference .New Brunswick , NJ : Morgan Kaufmann , pp .","label":"Background","metadata":{},"score":"41.321007"}{"text":"One of the . ... hods applied to the distribution of attribute values , or instances such as in ESEL ( Michalski , and Larson , 1978 ) . 2.3 Methods for Building more General Learning Algorithms As has been shown in Shaffer ( 1994 ) and Rao , Gordon , and Spears , ( 1995 ) there is no hope for building a universal learner .","label":"Background","metadata":{},"score":"41.402866"}{"text":"Cliffs , NJ .Levine , D. S. ( 2000 ) .Introduction to Neural and Cognitive Modeling . 2nd . ed . , Lawrence Erlbaum : Hillsdale , N.J. .Comments from readers of comp.ai.neural-nets : \" Highly recommended \" .","label":"Background","metadata":{},"score":"41.494003"}{"text":"Also , there is little information on data .preparation , for which Smith ( 1996 ) and Masters ( 1993 ; see below ) should be . consulted .There is some elementary calculus , but not enough that it should .","label":"Background","metadata":{},"score":"41.511307"}{"text":"for Empirical Classifier Selection ; The Maximum Likelihood Principle ; .Parametric Classification ; Generalized Linear Discrimination ; Complexity .Regularization ; Condensed and Edited Nearest Neighbor Rules ; Tree .Classifiers ; Data - Dependent Partitioning ; Splitting the Data ; The .","label":"Background","metadata":{},"score":"41.570457"}{"text":"Additional Information : Unfortunately that article is not available there .The best introductory book for business executives .Bigus , J.P. ( 1996 ) , Data Mining with Neural Networks : Solving Business .Problems -- from Application Development to Decision Support , NY : .","label":"Background","metadata":{},"score":"41.739418"}{"text":"Networks and Fuzzy Approximation ; Neural Knowledge Processing in Expert .Systems .Learning theory .Wolpert , D.H. , ed .( 1995 )The Mathematics of Generalization : The .Proceedings of the SFI / CNLS Workshop on Formal Approaches to Supervised .","label":"Background","metadata":{},"score":"41.8069"}{"text":"Their coverage of initialization methods , . constructive networks , pruning , and regularization methods is unusually .thorough .Unlike the vast majority of books on neural nets , this one has .lots of really informative graphs .The chapter on generalization assessment . is slightly weak , which is why you should read Smith ( 1996 ) or Weiss and .","label":"Background","metadata":{},"score":"41.927494"}{"text":"Meth .Inform .Med .Smyth , P. , Heckerman , D. , & Jordan , M. I. ( 1997 ) .Probabilistic independence networks for hidden Markov probability models .Neural Computation , 9 , 227 - 270 .Waterhouse , S. , MacKay , D. J. C. , & Robinson , T. ( 1996 ) .","label":"Background","metadata":{},"score":"41.97651"}{"text":"Factorial Hidden Markov models .Machine Learning , 29 , 245 - 273 .Gilks , W. , Thomas , A. , & Spiegelhalter , D. ( 1994 ) .A language and a program for complex Bayesian modelling .The Statistician , 43 , 169 - 178 .","label":"Background","metadata":{},"score":"41.984375"}{"text":"Neural Network Architectures : An Introduction .Van .Nostrand Reinhold : New York .Comments from readers of comp.ai.neural-nets : \" Like Wasserman 's book , .Dayhoff 's book is also very easy to understand \" .Freeman , James ( 1994 ) .","label":"Background","metadata":{},"score":"42.31445"}{"text":"An empirical analysis of likelihood - Weighting simulation on a large , multiply connected medical belief network .Computers and Biomedical Research , 24 , 453 - 475 .Shwe , M. A. , Middleton , B. , Heckerman , D. E. , Henrion , M. , Horvitz , E. J. , Lehmann , H. P. , & Cooper , G. F. ( 1991 ) .","label":"Background","metadata":{},"score":"42.507072"}{"text":"Computer Vision , Graphics , and Image Processing , Vol .37 , pp .54 - 115 .Kibler , Dennis , and David W. Aha , ( 1987 ) .Learning representative exemplars of concepts : An initial case study .","label":"Background","metadata":{},"score":"42.5596"}{"text":"Uncertainty and Artificial Intelligence : Proceedings of the Tenth Conference .San Mateo , CA : Morgan Kaufmann .Shenoy , P. P. ( 1992 ) .Valuation - based systems for Bayesian decision analysis .Operations Research , 40 , 463 - 484 .","label":"Background","metadata":{},"score":"42.608307"}{"text":"We use scatterplots that graphically show how AdaBoost reweights instances , emphasizing not only \" hard \" areas but also outliers and noise . classification boosting Bagging decision trees Naive - Bayes mean - squared error .Breiman , L. ( 1996a ) .","label":"Background","metadata":{},"score":"42.625366"}{"text":"In this paper we evaluate these methods on 23 data sets using both neural networks and decision trees as our classification algorithm .Our results clearly indicate a number of conclusions .First , while Bagging is almost always more accurate than a single classifier , it is sometimes much less accurate than Boosting .","label":"Background","metadata":{},"score":"42.742023"}{"text":"A large number of paradigms are presented .On the .negative side the book is very shallow .Best used as a complement to other . books \" .Wasserman , P.D. ( 1993 ) .Advanced Methods in Neural Computing .","label":"Background","metadata":{},"score":"42.78016"}{"text":"In D. Fisher ( Ed . ) , Machine Learning : Proceedings of the Fourteenth International Conference ( pp .254 - 262 ) .Morgan Kaufmann .Oliver , J. , & Hand , D. ( 1995 ) .On pruning and averaging decision trees .","label":"Background","metadata":{},"score":"42.90798"}{"text":"The proposed theory is based on the concept of structured measurement device , which is motivated by the formal model of inductive learning and is a far - reaching generalization of the concept of classical measurement device . by R. S. Michalski , A. Rosenfeld , Y. Aloimonos , Z. Duric , M. A. Maloof , Q. Zhang , 1996 . \" ...","label":"Background","metadata":{},"score":"42.914898"}{"text":"We then introduce variational methods , which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient .Inference in the simpified model provides bounds on probabilities of interest in the original model .","label":"Background","metadata":{},"score":"43.079414"}{"text":"The method has been experimentally compared to k - nearest neighbor , a statistical pattern recognition technique , and artificial neural networks .Experimental results demonstrate strong advantages of the AQ methodology over the other methods .Specifically , the method has higher predictive accuracy and faster learning and recognition rates .","label":"Background","metadata":{},"score":"43.08686"}{"text":"Apparently there is a new edition I have n't seen yet : .Smith , M. ( 1996 ) .Neural Networks for Statistical Modeling , Boston : .International Thomson Computer Press , ISBN 1 - 850 - 32842 - 0 .","label":"Background","metadata":{},"score":"43.19368"}{"text":"Processing ; Simple Matrix Operations ; The Linear Associator : Background and .Foundations ; The Linear Associator : Simulations ; Early Network Models : The .Perceptron ; Gradient Descent Algorithms ; Representation of Information ; .Applications of Simple Associators : Concept Formation and Object Motion ; .","label":"Background","metadata":{},"score":"43.420986"}{"text":"Jaakkola , T. S. , & Jordan , M. I. ( 1996 ) .Computing upper and lower bounds on likelihoods in intractable networks .Uncertainty and Artificial Intelligence : Proceedings of the Twelth Conference .San Mateo , CA : Morgan Kaufmann .","label":"Background","metadata":{},"score":"43.426746"}{"text":"Kohavi , R. ( 1995a ) .A study of cross - validation and bootstrap for accuracy estimation and model selection .In C.S. Mellish ( Ed . ) , Proceedings of the 14th International Joint Conference on Artificial Intelligence ( pp .","label":"Background","metadata":{},"score":"43.58181"}{"text":"Vapnik , V. ( 1995 ) .The nature of statistical learning theory .New York : Springer - Verlag .Zucker , J.-D. , & Ganascia , J.-G. Changes of representation for efficient learning in structural domains .Proceedings of the Thirteenth International Conference on Machine Learning ( pp .","label":"Background","metadata":{},"score":"43.793854"}{"text":"It is the state of art in machine learning techniques .Support Vector Machines work on decision planes or hyper planes that define decision boundaries .A decision plane is a plane that separates a set of objects having different class memberships .","label":"Background","metadata":{},"score":"43.81858"}{"text":"In D. S. Touretzky , J. Elman , T. Sejnowski , & G. E. Hinton ( Eds . ) , Proceedings of the 1990 Connectionist Models Summer School .San Mateo , CA : Morgan Kaufmann .Solving Multiple - Instance Problem : A Lazy Learning Approach .","label":"Background","metadata":{},"score":"43.963036"}{"text":"Kanazawa , K. , Koller , D. , & Russell , S. ( 1995 ) .Stochastic simulation algorithms for dynamic probabilistic networks .Uncertainty and Artificial Intelligence : Proceedings of the Eleventh Conference .San Mateo , CA : Morgan Kaufmann .","label":"Background","metadata":{},"score":"44.0848"}{"text":"Bottou , P. , & Vapnik , V. ( 1992 ) .Local learning algorithms .Neural Computation , 4 , 888 - 900 .Dasarathy , B.V. ( 1991 ) .Nearest neighbor norms : NN pattern classification techniques .Los Alamitos , CA : IEEE Computer Society Press .","label":"Background","metadata":{},"score":"44.095387"}{"text":"This paper describes several phenomena that can , if ignored , invalidate an experimental comparison .These phenomena and the conclusions that follow apply not only to classification , but to computational experiments in almost any aspect of data mining .The paper also discusses why comparative analysis is more important in evaluating some types of algorithms than for others , and provides some suggestions about how to avoid the pitfalls suffered by many experimental studies .","label":"Background","metadata":{},"score":"44.14038"}{"text":"Outline the four key questions that must be answered when designing a machine learning algorithm .Give an example of an answer for each question .Define the following algorithms : ( a real question would just ask for one of these ) Find - S List - Then - Eliminate ( Version Space ) Candidate Elimination ( Version Space )","label":"Background","metadata":{},"score":"44.44268"}{"text":"International Journal of Human - Computer Studies , 42 , 647 - 666 .Jensen , F. V. , & Jensen , F. ( 1994 ) .Optimal junction trees .Uncertainty and Artificial Intelligence : Proceedings of the Tenth Conference .","label":"Background","metadata":{},"score":"44.500973"}{"text":"Proceedings of the Tenth International Conference on Machine Learning ( pp .73 - 80 ) .Morgan Kaufmann .Dietterich , T.G. ( 1998 ) .Approximate statistical tests for comparing supervised classification learning algorithms .Neural Computation , 10 ( 7 ) .","label":"Background","metadata":{},"score":"44.532944"}{"text":"References .Aha , D. Generalizing from case studies : A case study .In Proc .Ninth Intl .Workshop on Machine Learning , pages 1 - 10 , San Mateo , CA , 1992 .Morgan Kaufmann .Cochran , W. and Cox , G. Experimental Designs .","label":"Background","metadata":{},"score":"44.591995"}{"text":"Kohavi , R. , & Wolpert , D.H. ( 1996 ) .Bias plus variance decomposition for zero - one loss functions .In L. Saitta ( Ed . ) , Machine Learning : Proceedings of the Thirteenth International Conference ( pp .","label":"Background","metadata":{},"score":"44.621918"}{"text":"\" Chapter headings : Statistical Pattern Recognition ; Probability Density .Estimation ; Single - Layer Networks ; The Multi - layer Perceptron ; Radial Basis .Functions ; Error Functions ; Parameter Optimization Algorithms ; .Pre - processing and Feature Extraction ; Learning and Generalization ; Bayesian .","label":"Background","metadata":{},"score":"44.85785"}{"text":"Quinlan , J.R. ( 1993 ) .C4.5 : programs for machine learning .San Mateo , California : Morgan Kaufmann .Quinlan , J.R. ( 1994 ) .Comparing connectionist and symbolic learning methods .In S.J. Hanson , G.A. Drastal , & R.L. Rivest ( Eds . ) , Computational learning theory and natural learning systems ( Vol .","label":"Background","metadata":{},"score":"44.90853"}{"text":"Chapter headings : Supervised Learning ; Single - Layer Networks ; MLP .Representational Capabilities ; Back - Propagation ; Learning Rate and Momentum ; .Weight - Initialization Techniques ; The Error Surface ; Faster Variations of .Back - Propagation ; Classical Optimization Techniques ; Genetic Algorithms and .","label":"Background","metadata":{},"score":"44.92575"}{"text":"Horvitz , E. J. , Suermondt , H. J. , & Cooper , G. F. ( 1989 ) .Bounded conditioning : Flexible inference for decisions under scarce resources .Conference on Uncertainty in Artificial Intelligence : Proceedings of the Fifth Conference .","label":"Background","metadata":{},"score":"44.981712"}{"text":"Advances in Neural Information Processing Systems , 6 , 216 - 223 .San Mateo : Morgan Kaufmann .Dietterich , T.G. , Lathrop , R. H. , & Lozano - Pérez , T. ( 1997 ) .Solving the multiple - instance problem with axis - parallel rectangles .","label":"Background","metadata":{},"score":"45.033836"}{"text":"ISBN :0262201070 .Mozer , M.C. , Jordan , M.I. , and Petsche , T. , eds .( 1997 ) Advances in .Neural Information Processing Systems 9 , Cambridge , MA : The MIT Press , .ISBN : 0262100657 .","label":"Background","metadata":{},"score":"45.177464"}{"text":"Levenberg - Marquardt , etc . ?Their approach makes clear the intimate . connections between fuzzy systems , neural networks , and statistical methods . such as B - spline regression .The best comparison of NNs with other classification methods .","label":"Background","metadata":{},"score":"45.204113"}{"text":"Some of the datasets were used in a prediction contest and are described . in detail in the book \" Time series prediction : Forecasting the future and .understanding the past \" , edited by Weigend / Gershenfield , Proceedings .","label":"Background","metadata":{},"score":"45.300186"}{"text":"430 - 437 ) .Morgan Kaufmann .Pazzani , M. , Merz , C. , Murphy , P. , Ali , K. , Hume , T. , & Brunk , C. ( 1994 ) .Reducing misclassification costs .Machine Learning : Proceedings of the Eleventh International Conference .","label":"Background","metadata":{},"score":"45.38659"}{"text":"Recent advances in Mobile technologies are the areas of application of machinelearning techniques .T.Mitchell , Machine Learning , MIT Press , 1997 .Alpaydin Ethem , Introduction to Machine Learning , MIT Press , Second Edition , 2004 .L.Rokach and O.Maimon .","label":"Background","metadata":{},"score":"45.409157"}{"text":"Biological learning and neurophysiology .Koch , C. , and Segev , I. , eds .( 1998 )Methods in Neuronal Modeling : From .Ions to Networks , 2nd ed . , Cambridge , MA : The MIT Press , ISBN .","label":"Background","metadata":{},"score":"45.410126"}{"text":"Cherkassky , V. , Friedman , J.H. , and Wechsler , H. , eds .( 1991 )From .Statistics to Neural Networks : Theory and Pattern Recognition Applications , .NY : Springer , ISBN 0 - 387 - 58199 - 5 .","label":"Background","metadata":{},"score":"45.57621"}{"text":"Technical report , Oregon State University , Corvallis , OR , 1996 .Everitt , B. The Analysis of Contingency Tables .Chapman and Hall , London . , 1977 .Fayyad , U.M. and Irani , K.B. Multi - interval discretization of continuous valued attributes for classification learning .","label":"Background","metadata":{},"score":"45.664635"}{"text":"Ridgeway , G. , Madigan , D. , & Richardson , T. ( 1998 ) .Interpretable boosted naive bayes classification .Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining .Schaffer , C. ( 1994 ) .","label":"Background","metadata":{},"score":"45.705273"}{"text":"1986 - 1991 , fuzzy logic , genetic algorithms , artificial life , evolutionary .biology , and many Project Gutenberg and Wiretap e - texts .AI CD - ROM .Network Cybernetics Corporation produces the \" AI CD - ROM \" .","label":"Background","metadata":{},"score":"45.921253"}{"text":"Husmeier , D. ( 1999 ) , Neural Networks for Conditional Probability .Estimation : Forecasting Beyond Point Predictions , Berlin : Springer Verlag , .ISBN 185233095 .Fuzzy logic and neurofuzzy systems .See also \" General ( including SVMs and Fuzzy Logic ) \" .","label":"Background","metadata":{},"score":"45.97036"}{"text":"Programming ; Monte Carlo Methods ; Temporal - Difference Learning ; A Unified .View ; Eligibility Traces ; Generalization and Function Approximation ; .Planning and Learning ; Dimensions of Reinforcement Learning ; Case Studies .Bertsekas , D. P. and Tsitsiklis , J. N. ( 1996 ) , Neuro - Dynamic .","label":"Background","metadata":{},"score":"45.980545"}{"text":"We show that in language learning , contrary to received wisdom , keeping exceptional training instances in memory can be beneficial for generalization accuracy .We investigate this phenomenon empirically on a selection of benchmark natural language processing tasks : grapheme - to - phoneme conversion , part - of - speech tagging , prepositional - phrase attachment , and base noun phrase chunking .","label":"Background","metadata":{},"score":"46.051376"}{"text":"Neural Networks , 9 , 1996 .Qian , N. and Sejnowski , T. Predicting the secondary structure of globular proteins using neural network models .Journal of Molecular Biology , 202:65 - 884 , 1988 .Raftery , A. Bayesian model selection in social research ( with discussion by Andrew Gelman , Donald B. Rubin , and Robert M. Hauser ) .","label":"Background","metadata":{},"score":"46.08602"}{"text":"Jensen , F. V. ( 1996 ) .An introduction to Bayesian networks .London : UCL Press .Jordan , M. I. ( 1994 ) .A statistical approach to decision tree modeling .In M. Warmuth ( Ed . ) , Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory .","label":"Background","metadata":{},"score":"46.1026"}{"text":"Annealed theories of learning .In J.-H. Oh , C. Kwon , S. Cho ( Eds . ) , Neural networks : The statistical mechanics perspectives .Singapore : World Scientific .Shachter , R. D. , Andersen , S. K. , & Szolovits , P. ( 1994 ) .","label":"Background","metadata":{},"score":"46.138557"}{"text":"Experiments comparing the ID3 symbolic learning algorithm with ... \" .Abstract Despite the fact that many symbolic and neural network ( connectionist ) learning algorithms address the same problem of learning from classified examples , very little is known regarding their comparative strengths and weaknesses .","label":"Background","metadata":{},"score":"46.354687"}{"text":"Morgan Kaufmann .Kong , E.B. , & Dietterich , T.G. ( 1995 ) .Error - correcting output coding corrects bias and variance .In A. Prieditis & S. Russell ( Eds . ) , Machine Learning : Proceedings of the Twelfth International Conference ( pp .","label":"Background","metadata":{},"score":"46.45615"}{"text":"The Annals of Mathematical Statistics , 41 , 164 - 171 .Bishop , C. M. , Lawrence , N. , Jaakkola , T. S. , & Jordan , M. I. ( 1998 ) .Approximating posterior distributions in belief networks using mixtures .","label":"Background","metadata":{},"score":"46.52688"}{"text":"Cambridge , MA : MIT Press .Jaakkola , T. S. , & Jordan , M. I. ( 1997b ) .Bayesian logistic regression : A variational approach .In D. Madigan & P. Smyth ( Eds . ) , Proceedings of the 1997 Conference on Artificial Intelligence and Statistics .","label":"Background","metadata":{},"score":"46.53701"}{"text":"The ability to restructure a decision tree efficiently enables a variety of approaches to decision tree induction that would otherwise be prohibitively expensive .Two such approaches are described here , one being incremental tree induction ( ITI ) , and the other being non - incremental tree induction using a measure of tree quality instead of test quality ( DMTI ) .","label":"Background","metadata":{},"score":"46.544636"}{"text":"book for you .In summary , this is the best starting point for the outsider and/or . beginner ... a truly excellent text .Smith , M. ( 1996 ) .Neural Networks for Statistical Modeling , NY : Van Nostrand .","label":"Background","metadata":{},"score":"46.556202"}{"text":"Distributed Processing : Computational Models of Cognition and Perception .( software manual ) .The MIT Press .Comments from readers of comp.ai.neural-nets : \" Written in a tutorial style , . and includes 2 diskettes of NN simulation programs that can be compiled on .","label":"Background","metadata":{},"score":"46.643105"}{"text":"Dayan , P. , Hinton , G. E. , Neal , R. , & Zemel , R. S. ( 1995 ) .The Helmholtz Machine .Neural Computation , 7 , 889 - 904 .Dean , T. , & Kanazawa , K. ( 1989 ) .","label":"Background","metadata":{},"score":"46.841373"}{"text":"financial forecasting , is to reserve two cases for the validation set !My comments apply only to the text of the above books .I have not examined .or attempted to compile the code .An impractical guide to neural nets .","label":"Background","metadata":{},"score":"46.945847"}{"text":"Cohen ( 1995 ) will probably be of particular interest to researchers in . neural nets and machine learning ( see also the review of Cohen 's book by Ron .Kohavi in the International Journal of Neural Systems , which can be found .","label":"Background","metadata":{},"score":"46.951653"}{"text":"Complex Systems , 1 , 995 - 1019 .Rockafellar , R. ( 1972 ) .Convex analysis .Princeton University Press .Rustagi , J. ( 1976 ) .Variational methods in statistics .New York : Academic Press .Sakurai , J. ( 1985 ) .","label":"Background","metadata":{},"score":"47.028366"}{"text":"Morgan Kaufmann .Kohavi , R. ( 1995b ) .Wrappers for performance enhancement and oblivious decision graphs .Ph.D. thesis , Stanford University , Computer Science department .STAN - CS - TR-95 - 1560 .Kohavi , R. , Becker , B. , & Sommerfield , D. ( 1997 ) .","label":"Background","metadata":{},"score":"47.17334"}{"text":"Kohavi , R. , & Sahami , M. ( 1996 ) .Error - based and entropy - based discretization of continuous features .Proceedings of the Second International Conference on Knowledge Discovery and Data Mining ( pp .114 - 119 ) .","label":"Background","metadata":{},"score":"47.238106"}{"text":"Neural Computation : A Beginner 's .Guide .Lawrence Earlbaum Associates : London .Comments from readers of comp.ai.neural-nets : \" Short user - friendly . introduction to the area , with a non - technical flavour .Apparently . accompanies a software package , but I have n't seen that yet \" .","label":"Background","metadata":{},"score":"47.390194"}{"text":"Subject : Conferences and Workshops on Neural .Networks ?o The journal \" Neural Networks \" has a list of conferences , workshops and . meetings in each issue .o Conferences , workshops , and other events concerned with neural networks , . inductive learning , genetic algorithms , data mining , agents , applications . of AI , pattern recognition , vision , and related fields . are listed at .","label":"Background","metadata":{},"score":"47.397976"}{"text":"An Algorithm for a Selective Nearest Neighbor Decision Rule .IEEE Transactions on Information Theory , Vol . 21 , No . 6 , pp .665 - 669 .Skalak , D. B. , ( 1994 ) .Prototype and Feature Selection by Sampling and Random Mutation Hill Climbing Algorithsm .","label":"Background","metadata":{},"score":"47.59917"}{"text":"Architectures and Techniques for Knowledge - Based Neurocomputing ; Symbolic .Knowledge Representation in Recurrent Neural Networks : Insights from .Theoretical Models of Computation ; A Tutorial on Neurocomputing of .Structures ; Structural Learning and Rule Discovery ; VL[subscript 1]ANN : .","label":"Background","metadata":{},"score":"47.610752"}{"text":"Saul , L. K. , & Jordan , M. I. ( 1999 ) .A mean field learning algorithm for unsupervised neural networks .In M. I. Jordan ( Ed . ) , Learning in graphical models .Cambridge , MA : MIT Press .","label":"Background","metadata":{},"score":"47.62377"}{"text":"Error - correcting output codes : A general method for improving multiclass inductive learning programs .Proceedings of the Ninth National Conference on Artificial Intelligence ( AAAI-91 ) ( pp .572 - 577 ) .Domingos , P. ( 1997 ) .","label":"Background","metadata":{},"score":"47.65187"}{"text":"We provide explanations for both results in terms of the properties of the natural language processing tasks and the learning algorithms . \" ...This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context .","label":"Background","metadata":{},"score":"47.82449"}{"text":"Mobile learning applications include language translation , game playing , intelligent agents etc .An architecture for designing an Intelligent System on mobile device was discussed by Jawad et al .[21].This design makes use of sensor networks . A. Chaudhary et al Int .","label":"Background","metadata":{},"score":"47.840137"}{"text":"Includes Werbos 's 1974 Harvard Ph.D. thesis , Beyond .Regression .Kohonen , T. ( 1984/1989 ) , Self - organization and Associative Memory , 1st ed .1988 , 3rd ed .1989 , NY : Springer .Additional Information : Book is out of print .","label":"Background","metadata":{},"score":"47.866676"}{"text":"network is able to generalize \" !Chester , M. ( 1993 ) .Neural Networks : A Tutorial , Englewood Cliffs , NJ : PTR .Prentice Hall .Additional Information : Seems to be out of print .Shallow , sometimes confused , especially with regard to Kohonen networks .","label":"Background","metadata":{},"score":"48.009712"}{"text":"and Electronic Networks .Academic Press .( ISBN 0 - 12 - 781881 - 2 ) .Comments from readers of comp.ai.neural-nets : \" Covers quite a broad range of . topics ( collection of articles / papers ) .","label":"Background","metadata":{},"score":"48.01476"}{"text":"Networks : Theory and Applications , NY : Wiley , ISBN 0 - 471 - 05436 - 4 .Van Hulle , M.M. ( 2000 ) , Faithful Representations and Topographic Maps : .From Distortion- to Information - Based Self - Organization , NY : Wiley , ISBN .","label":"Background","metadata":{},"score":"48.06887"}{"text":"Consequences and Detection of Misspecified Nonlinear Regression Models ; .Maximum Likelihood Estimation of Misspecified Models ; Some Results for Sieve .Estimation with Dependent Observations .Time - series forecasting .Weigend , A.S. and Gershenfeld , N.A. , eds .","label":"Background","metadata":{},"score":"48.12629"}{"text":"The purpose of the study is to improve our understanding of why and when these algorithms , which use perturbation , reweighting , and combination techniques , affect classification error .We provide a bias and variance decomposition of the error to show how different methods and variants influence these two terms .","label":"Background","metadata":{},"score":"48.132977"}{"text":"Redwood City , CA : Addison - Wesley .Pearl , J. ( 1988 ) .Probabilistic reasoning in intelligent systems : Networks of plausible inference .San Mateo , CA : Morgan Kaufmannn .Peterson , C. , & Anderson , J. R. ( 1987 ) .","label":"Background","metadata":{},"score":"48.1872"}{"text":"Tesauro , G. , Touretzky , D. , and Leen , T. , eds .( 1995 ) Advances in Neural .Information Processing Systems 7 , Cambridge , MA : The MIT Press , ISBN : .Touretzky , D. S. , Mozer , M.C. , and Hasselmo , M.E. , eds .","label":"Background","metadata":{},"score":"48.194504"}{"text":"There are two important characteristics of using Random forests .The first characteristic is that the generalization error converges with the increase in number of trees increases and second characteristic is that this type of learning does not suffer from over fitting [ 10].","label":"Background","metadata":{},"score":"48.27835"}{"text":"Reinforcement learning .Sutton , R.S. , and Barto , A.G. ( 1998 ) , Reinforcement Learning : An .Introduction , The MIT Press , ISBN : 0 - 262193 - 98 - 1 .Chapter headings : The Problem ; Introduction ; Evaluative Feedback ; The .","label":"Background","metadata":{},"score":"48.333633"}{"text":"2.2.2Classification ( Discriminant Analysis ) .2.2.3 Regression .2.2.4 Stochastic Approximation .2.2.5 Solving Problems with Finite Data .2.2.6 Nonparametric Methods . 2.3 Adaptive Learning : Concepts and Inductive Principles .2.3.1 Philosophy , Major Concepts , and Issues .","label":"Background","metadata":{},"score":"48.374077"}{"text":"Part I , .Background , introduces several basic neural models , explains how the present . study of brain theory and neural networks integrates brain theory , . artificial intelligence , and cognitive psychology , and provides a tutorial .on the concepts essential for understanding neural networks as dynamic , . adaptive systems .","label":"Background","metadata":{},"score":"48.37435"}{"text":"Mobile devices offer a wide range of services above several wireless network access technologies [ 19].Mobile devices are prune to threats and machine learning techniques like K - Nearest Neighbor , Bayesian Networks , Random Forests are useful in intrusion detection for mobile devices [ 19].","label":"Background","metadata":{},"score":"48.408176"}{"text":"Overfitting ; Cross Validation ; Preparing the Data ; Representing Variables ; .Using the Model .Weiss , S.M. and Kulikowski , C.A. ( 1991 ) , Computer Systems That Learn , .Morgan Kaufmann .ISBN 1 - 55860 - 065 - 5 .","label":"Background","metadata":{},"score":"48.4581"}{"text":"However , there is much to be gained by utilizing the capability to reason nearly correctly .In the presented EBL - ANN algorithm , a \" roughly - correct \" explanatory capability leads to the acquisition of a classification rule that is almost correct .","label":"Background","metadata":{},"score":"48.479668"}{"text":"Zurada , Jacek M. ( 1992 ) .Introduction To Artificial Neural Systems .Hardcover , 785 Pages , 317 Figures , ISBN 0 - 534 - 95460-X , 1992 , PWS Publishing .Company , Price : $ 56.75 ( includes shipping , handling , and the ANS software . diskette ) .","label":"Background","metadata":{},"score":"48.497543"}{"text":"Recently .it has published a special issue on connectionist approaches . to symbolic reasoning .The journal regularly publishes . issues devoted to genetic algorithms as well .Title : INTELLIGENCE - The Future of Computing .Published by : Intelligence .","label":"Background","metadata":{},"score":"48.513393"}{"text":"Maximum - likelihood from incomplete data via the EM algorithm .Journal of the Royal Statistical Society , B39 , 1 - 38 .Draper , D. L. , & Hanks , S. ( 1994 ) .Localized partial evaluation of belief networks .","label":"Background","metadata":{},"score":"48.5468"}{"text":"Fausett , L. ( 1994 ) , Fundamentals of Neural Networks : Architectures , .Algorithms , and Applications , Englewood Cliffs , NJ : Prentice Hall , ISBN .Also published as a Prentice Hall International Edition , ISBN .Sample software ( source code listings in C and Fortran ) is . included in an Instructor 's Manual .","label":"Background","metadata":{},"score":"48.567757"}{"text":"( 1998 ) Advances in .Neural Information Processing Systems 10 , Cambridge , MA : The MIT Press , .ISBN : 0262100762 .Kearns , M.S. , Solla , S.A. , amd Cohn , D.A. , eds .( 1999 ) Advances in .","label":"Background","metadata":{},"score":"48.570114"}{"text":"Intl .Joint Conf . on Artificial Intelligence , pages 1022 - 1027 , Chambery , France , 1993 .Morgan Kaufmann .Feelders , A. and Verkooijen , W. Which method learns most from the data ?In Prelim .Papers Fifth Intl .","label":"Background","metadata":{},"score":"48.572388"}{"text":"It gives brief study of recent areas of applications of machine learning techniques for mobile devices .It also presents some performance measures for a machine learning algorithms .It is somewhat not an easy task directly to state that one learning technique performs better than the other one .","label":"Background","metadata":{},"score":"48.575554"}{"text":"Unconstrained Optimization and Learning Algorithms ; Neural Networks for .Linear , Quadratic Programming and Linear Complementarity Problems ; A Neural .Network Approach to the On - Line Solution of a System of Linear Algebraic ; .Equations and Related Problems ; Neural Networks for Matrix Algebra Problems ; .","label":"Background","metadata":{},"score":"48.647537"}{"text":"In other words it implies computer programs use their past experience of problem solving tasks to improve their performance .But description does not include the concept of acquisition of knowledge for the stated computer programs .Alpaydin [ 3 ] defines Machine learning as \" the capability of the computer program to acquire or develop new knowledge or skills from existing or nonexisting examples for the sake of optimizing performance criterion \" .","label":"Background","metadata":{},"score":"48.647587"}{"text":"Holte , R. Very simple classification rules perform well on most commonly used datasets .Machine Learning , 11(1):63 - 90 , 1993 .Jensen , D. Knowledge discovery through induction with randomization testing .In G. Piatetsky - Shapiro , editor , Proc .","label":"Background","metadata":{},"score":"48.69231"}{"text":"Fung , R. & Favero , B. D. ( 1994 ) .Backward simulation in Bayesian networks .Uncertainty and Artificial Intelligence : Proceedings of the Tenth Conference .San Mateo , CA : Morgan Kaufmann .Galland , C. ( 1993 ) .","label":"Background","metadata":{},"score":"48.700737"}{"text":"collaboration between the authors .The lessons learned form a case study .that demonstrates how hybrid systems can be developed to combine neural .networks with more traditional statistical approaches .The book illustrates .both the advantages and limitations of neural networks in the framework of a . statistical system .","label":"Background","metadata":{},"score":"48.75504"}{"text":"These Computational components or nodes are connected via links that have weights .A set of data ( input - output pairs ) is used for training the Neural Network .Single - layer ANN or Perceptrons work with algorithms like Gaussian maximum - likelihood classifiers .","label":"Background","metadata":{},"score":"48.76461"}{"text":"Machine learning techniques are also helpful in network management and surveillance domains [ 13].Machine learning techniques like reinforcement learning and instance - based learning perform better in mobile devices .These techniques are also useful for mobile music platform for different categories of music expressions [ 15].","label":"Background","metadata":{},"score":"48.802048"}{"text":"Segmentation and Recognition .Anthony , M. , and Bartlett , P.L. ( 1999 ) , Neural Network Learning : .Theoretical Foundations , Cambridge : Cambridge University Press , ISBN . 0 - 521 - 57353-X. Vapnik , V.N. ( 1998 )","label":"Background","metadata":{},"score":"48.91243"}{"text":"Chapman & Hall .Elkan , C. ( 1997 ) .Boosting and naive bayesian learning ( Technical Report ) .San Diego : Department of Computer Science and Engineering , University of California .Fayyad , U.M. , & Irani , K.B. ( 1993 ) .","label":"Background","metadata":{},"score":"48.93132"}{"text":"Wettschereck , D. and Dietterich , T. An experimental comparison of the nearest - neighbor and nearest - hyperrectangle algorithms .Machine Learning , 19(1):5 - 28 , 1995 .Wolpert , D. On the connection between in - sample testing and generalization error .","label":"Background","metadata":{},"score":"48.936195"}{"text":"The best popular introduction to NNs .Hinton , G.E. ( 1992 ) , \" How Neural Networks Learn from Experience \" , Scientific .American , 267 ( September ) , 144 - 151 ( page numbers are for the US edition ) .","label":"Background","metadata":{},"score":"48.95215"}{"text":"Multicriteria filtering methods based on concordance and non - discordance principles .Annals of Operations Research , 80 , 137 - 165 .Bussum , The Netherlands : Baltzer Science Publishers .Ruffo , G. ( 2000 ) .Learning single and multiple instance decision tree for computer security applications .","label":"Background","metadata":{},"score":"48.9532"}{"text":"Results show that editing exceptional instances ( with low typicality or low class prediction strength ) tends to harm generalization accuracy .In a second series of experiments we compare memory - based learning and decision - tree learning methods on the same selection of tasks , and find that decision - tree learning often performs worse than memory - based learning .","label":"Background","metadata":{},"score":"48.963623"}{"text":"Chapter headings : Historical and Biological Aspects ; Neural Networks ; Fuzzy .Systems ; Modelling Neuro - Fuzzy Systems ; Cooperative Neuro - Fuzzy Systems ; .Hybrid Neuro - Fuzzy Systems ; The Generic Fuzzy Perceptron ; NEFCON - .","label":"Background","metadata":{},"score":"49.034405"}{"text":"These tools are needed to help human analysts make sense of the increasing amount of complex data available electronically from do ... \" .Machine learning ( ML ) algorithms are increasingly being pressed into service to help users understand and detect patterns or regularities found in large amounts of data .","label":"Background","metadata":{},"score":"49.05792"}{"text":"well worth reading even for people who have no interest in programming .Chapter headings : Foundations ; Classification ; Autoassociation ; Time - Series .Prediction ; Function Approximation ; Multilayer Feedforward Networks ; Eluding .Local Minima I : Simulated Annealing ; Eluding Local Minima II : Genetic .","label":"Background","metadata":{},"score":"49.11195"}{"text":"It does n't really say much about design , but this book provides formulas and . examples in excruciating detail for a wide variety of networks .It also .includes some mathematical background material .Chapter headings : Neuron Model and Network Architectures ; An Illustrative .","label":"Background","metadata":{},"score":"49.116264"}{"text":"Anderson , J.A. , and Rosenfeld , E. , eds .( 1998 ) , Talking Nets : An Oral .History of Neural Networks , Cambridge , MA : The MIT Press , ISBN .Knowledge , rules , and expert systems .","label":"Background","metadata":{},"score":"49.142303"}{"text":"that the reader has the basic mathematical literacy required for an .undergraduate science degree , and using these tools he explains everything .from scratch .Before introducing the multilayer perceptron , for example , he . lays a solid foundation of basic statistical concepts .","label":"Background","metadata":{},"score":"49.226288"}{"text":"Transformations for Neural ; Networks ; Supervised Hebbian Learning ; .Performance Surfaces and Optimum Points ; Performance Optimization ; .Widrow - Hoff Learning ; Backpropagation ; Variations on Backpropagation ; .Associative Learning ; Competitive Networks ; Grossberg Network ; Adaptive .","label":"Background","metadata":{},"score":"49.28945"}{"text":"d )It provides clear classification and traces fields important for prediction or classification .The limitations of using decision tree learning are a. This learning tool is not suitable for prediction of continuous attribute .b. It does not perform well with multiple classes and small data .","label":"Background","metadata":{},"score":"49.3059"}{"text":"Architectures ; Interpreting Weights : How Does This Thing Work ; Probabilistic .Neural Networks ; Functional Link Networks ; Hybrid Networks ; Designing the .Training Set ; Preparing Input Data ; Fuzzy Data and Processing ; Unsupervised .Training ; Evaluating Performance of Neural Networks ; Confidence Measures ; .","label":"Background","metadata":{},"score":"49.376976"}{"text":"What is a hybrid learning algorithm ?Give three examples of hybrid learning methods , explain how each works , and discuss the advantages of each . \" ...In this paper , we describe a method for learning shape descriptions of objects in x - ray images .","label":"Background","metadata":{},"score":"49.38417"}{"text":"Applying Divide and Conquer to Large Scale Pattern Recognition Tasks ; .Forecasting the Economy with Neural Nets : A Survey of Challenges and .Solutions ; How to Train Neural Networks .Arbib , M.A. , ed .( 1995 ) , The Handbook of Brain Theory and Neural .","label":"Background","metadata":{},"score":"49.407005"}{"text":"This technique takes the instances of solutions from the problems solved earlier and tries to solve step by step new problems by using these cases .Each solution available is termed as a case [ 8].A new case is defined by the initial description of the problem .","label":"Background","metadata":{},"score":"49.552727"}{"text":"Machine learning is the study of computer algorithms , applying these algorithms machines improve automatically with experience .In other words it is the ability of the computer or mobile program to acquire or develop new knowledge or skills from examples for optimizing the performance .","label":"Background","metadata":{},"score":"49.63002"}{"text":"Irvine , CA : Morgan Kaufmann , pp .24 - 30 .Michalski , Ryszard S. , Robert E. Stepp , and Edwin Diday , ( 1981 ) .A Recent Advance in Data Analysis : Clustering Objects into Classes Characterized by Conjunctive Concepts .","label":"Background","metadata":{},"score":"49.698845"}{"text":"Systems 2 , San Mateo , CA : Morgan Kaufmann , ISBN : 1558601007 .Lippmann , R.P. , Moody , J.E. , and Touretzky , D. S. , eds .( 1991 ) Advances . in Neural Information Processing Systems 3 , San Mateo , CA : Morgan .","label":"Background","metadata":{},"score":"49.747845"}{"text":"Ellis .Horwood , Ltd. , Chichester .Comments from readers of comp.ai.neural-nets : \" Gives the AI point of view \" .Zornetzer , S. F. , Davis , J. L. and Lau , C. ( 1990 ) .","label":"Background","metadata":{},"score":"49.766647"}{"text":"sift through it all to come away with anything useful .Simpson , P. K. ( 1990 ) .Artificial Neural Systems : Foundations , Paradigms , .Applications and Implementations .Pergamon Press : New York .Comments from readers of comp.ai.neural-nets : \" Contains a very useful 37 .","label":"Background","metadata":{},"score":"49.803745"}{"text":"The best book on neurofuzzy systems .The best comparison of NNs with other classification methods .Other notable books .Introductory .Bayesian learning .Biological learning and neurophysiology .Collections .Combining networks .Connectionism .Feedforward networks .Fuzzy logic and neurofuzzy systems .","label":"Background","metadata":{},"score":"49.8702"}{"text":"Detailed Table of Contents : .Learning and Soft Computing : Rationale , Motivations , Needs , Basics . 1.1 Examples of Applications in Diverse Fields .1.2 Basic Tools of Soft Computing : Neural Networks , Fuzzy Logic Systems , and Support Vector Machines .","label":"Background","metadata":{},"score":"49.880104"}{"text":"Machine Learning : Proceedings of the Eleventh International Conference ( pp .259 - 265 ) .Morgan Kaufmann .Schapire , R.E. ( 1990 ) .The strength of weak learnability .Machine Learning , 5 ( 2 ) , 197 - 227 .","label":"Background","metadata":{},"score":"49.887665"}{"text":"Berlin : Springer - Verlag , ISBN 3540620176 .Deco , G. and Obradovic , D. ( 1996 ) , An Information - Theoretic Approach to .Neural Computing , NY : Springer - Verlag , ISBN 0 - 387 - 94666 - 7 .","label":"Background","metadata":{},"score":"49.907887"}{"text":"2 Problem Statement , Classical Approaches , and Adaptive Learning .2.1 Formulation of the Learning Problem .2.1.1 Role of the Learning Machine .2.1.2 Common Learning Tasks .2.1.3 Scope of the Learning Problem Formulation . 2.2Classical Approaches .","label":"Background","metadata":{},"score":"49.98735"}{"text":"However , if you are especially interested in VC theory and .support vector machines , then both of these books can be highly recommended .To help you choose between them , a detailed table of contents is provided .below for each book .","label":"Background","metadata":{},"score":"49.99199"}{"text":"38 % ) tend to favor symbolic learning algorithms .We suggest how classification algorith ... . \" ...Machine learning is an area where both symbolic and neural approaches have been heavily investigated .However , there has been little research into the synergies achievable by combining these two learning paradigms .","label":"Background","metadata":{},"score":"49.996986"}{"text":"3.4 Linear Discriminants .3.4.1 The Normality Assumption and Discriminant Functions .3.4.2 Logistic Regression .3.5 Nearest Neighbor Methods .3.6 Feature Selection . 3.7 Error Rate Analysis . 3.8 Bibliographical and Historical Remarks .4 Neural Nets .4.1 Introduction and Overview . 4.2 Perceptrons .","label":"Background","metadata":{},"score":"50.00907"}{"text":"In this framework , neural networks ( and in particular , . multilayer perceptrons or MLPs ) have been restricted to well - defined . subtasks of the whole system , i.e. , HMM emission probability estimation and . feature extraction .","label":"Background","metadata":{},"score":"50.086136"}{"text":"Redwood City , CA : Addison - Wesley .Saul , L. K. , & Jordan , M. I. ( 1994 ) .Learning in Boltzmann trees .Neural Computation , 6 , 1173 - 1183 .Saul , L. K. , Jaakkola , T. S. , & Jordan , M. I. ( 1996 ) .","label":"Background","metadata":{},"score":"50.136093"}{"text":"Hinton , G. E. , & van Camp , D. ( 1993 ) .Keeping neural networks simple by minimizing the description length of the weights .Proceedings of the 6th Annual Workshop on Computational Learning Theory .New York , NY : ACM Press .","label":"Background","metadata":{},"score":"50.182087"}{"text":"New York : North - Holland , pp .33 - 56 .Mitchell , Tom M. , ( 1980 ) .The Need for Biases in Learning Generalizations .in J. W. Shavlik & T. G. Dietterich ( Eds . ) , Readings in Machine Learning .","label":"Background","metadata":{},"score":"50.36994"}{"text":"We strongly believe in the central role of inductive learning processes , around which , we think , all other ( intelligent ) biological processes have evolved .In this paper we outline a ( computational ) theory of vision completely built around the inductive learning processes for all levels in vision .","label":"Background","metadata":{},"score":"50.522102"}{"text":"This technique makes use of greedy search method .Propositional learning and Relational learning [ 8 ] are two ways of rule induction- .H. Genetic Algorithms and Genetic Programming These are a type of evolutionary computing models that are used for problem solving and work on the principle of biological evolution like natural selection .","label":"Background","metadata":{},"score":"50.681343"}{"text":"ISBN : 1558600337 .NIPS : .Touretzky , D.S. , ed .( 1989 ) , Advances in Neural Information Processing .Systems 1 , San Mateo , CA : Morgan Kaufmann , ISBN : 1558600159 .Touretzky , D. S. , ed .","label":"Background","metadata":{},"score":"50.74447"}{"text":"TYPES OF MACHINE LEARNING Machine learning can be categorized into major groups as supervised , unsupervised machine learning and reinforcement learning as shown in Fig . 1 .These groups represent how the learning method works . A. Chaudhary et al Int .","label":"Background","metadata":{},"score":"50.791985"}{"text":"Overall , backpropagation performs slightly better than the other two algorithms in terms of classification accuracy on new examples , but takes much longer to train .Experimental results suggest that backpropagation can work significantly better on data sets containing numerical data .","label":"Background","metadata":{},"score":"50.79316"}{"text":"Comments from readers of comp.ai.neural-nets : \" Cohesive and comprehensive .book on neural nets ; as an engineering - oriented introduction , but also as a . research foundation .Thorough exposition of fundamentals , theory and . applications .","label":"Background","metadata":{},"score":"50.815353"}{"text":"Analysis indicates that the performance of the Boosting methods is dependent on the characteristics of the data set being examined .In fact , further results show that Boosting ensembles may overfit noisy data sets , thus decreasing its performance .Finally , consistent with previous studies , our work suggests that most of the gain in an ensemble 's performance comes in the first few classifiers combined ; however , relatively large gains can be seen up to 25 classifiers when Boosting decision trees .","label":"Background","metadata":{},"score":"50.817905"}{"text":"Variational methods for inference and estimation in graphical models .Unpublished doctoral dissertation , Massachusetts Institute of Technology , Cambridge , MA .Jaakkola , T. S. , & Jordan , M. I. ( 1997a ) .Recursive algorithms for approximating probabilities in graphical models .","label":"Background","metadata":{},"score":"50.915962"}{"text":"ftp://mitpress.mit.edu/pub/Intro-to-NeuralNets/ .Anderson provides an accessible introduction to the AI and .neurophysiological sides of NN research , although the book is weak regarding .practical aspects of using NNs .Chapter headings : Properties of Single Neurons ; Synaptic Integration and .","label":"Background","metadata":{},"score":"50.94404"}{"text":"Classification , a.k.a .Risk Estimation , via Penalized Log Likelihood and .Smoothing Spline Analysis of Variance ; Current Research ; Preface to .Simplifying Neural Networks by Soft Weight Sharing ; Simplifying Neural .Networks by Soft Weight Sharing ; Error - Correcting Output Codes : A General .","label":"Background","metadata":{},"score":"50.953285"}{"text":"This field has evolved from the field of Artificial Intelligence , which aims to imitate intelligent abilities of humans by machines .The advent of Mobile technology has fostered development of Machine learning .This paper presents a review on machine learning and presents a brief study on different machine learning techniques along with their applications on mobile devices .","label":"Background","metadata":{},"score":"51.016342"}{"text":"Minsky , M.L. , and Papert , S.A. ( 1969/1988 )Perceptrons , Cambridge , MA : The .MIT Press , 1st ed .1969 , expanded edition 1988 ISBN 0 - 262 - 63111 - 3 .Werbos , P.J. ( 1994 ) , The Roots of Backpropagation , NY : John Wiley & Sons , .","label":"Background","metadata":{},"score":"51.118294"}{"text":"7.5.2 Constrained Topological Mapping .7.6 Empirical Comparisons .7.6.1 Experimental Setup .7.6.2 Summary of Experimental Results .7.7 Combining Predictive Models . 7.8 Summary .References . 8 Classification . 8.1 Statistical Learning Theory formulation .8.2 Classical Formulation . 8.3","label":"Background","metadata":{},"score":"51.1335"}{"text":"Oxford : Oxford University Press , ISBN : 0198524323 .Chapter headings : Introduction ; Pattern association memory ; Autoassociation . memory ; Competitive networks , including self - organizing maps ; .Error - correcting networks : perceptrons , the delta rule , backpropagation of . error in multilayer networks , and reinforcement learning algorithms ; The .","label":"Background","metadata":{},"score":"51.15226"}{"text":"23 - 37 ) .Springer - Verlag , To appear in Journal of Computer and System Sciences .Freund , Y. , & Schapire , R.E. ( 1996 ) .Experiments with a new boosting algorithm .In L. Saitta ( Ed . ) , Machine Learning : Proceedings of the Thirteenth National Conference ( pp .","label":"Background","metadata":{},"score":"51.168"}{"text":"Give an example of a concept that could not be learned by this type of artificial neural network .A multi - layer perceptron with sigmoid units can learn ( using an algorithm like backpropagation ) concepts that can not be learned by artificial neural networks that lack hidden units or sigmoid activation functions .","label":"Background","metadata":{},"score":"51.251015"}{"text":"Long , P.M. , & Tan , L. ( 1996 ) .PAC - learning axis aligned rectangles with respect to product distributions from multiple - instance examples .Proceedings of the Ninth Annual Conference on Computational Learning Theory ( pp .","label":"Background","metadata":{},"score":"51.32727"}{"text":"We measure tree sizes and show an interesting positive correlation between the increase in the average tree size in AdaBoost trials and its success in reducing the error .We compare the mean - squared error of voting methods to non - voting methods and show that the voting methods lead to large and significant reductions in the mean - squared errors .","label":"Background","metadata":{},"score":"51.48104"}{"text":"Cover , T. , & Thomas , J. ( 1991 ) .Elements of information theory .New York : John Wiley .Dagum , P. , & Luby , M. ( 1993 ) .Approximating probabilistic inference in Bayesian belief networks is NP - hard .","label":"Background","metadata":{},"score":"51.532814"}{"text":"Figure 1 .Machine learning types Supervised learning It consists of algorithms that reason or learn from externally supplied instances to result in a general hypothesis that makes prediction about future instances [ 1].Reinforcement learning The machine communicates with its surroundings by producing certain actions in reinforcement learning .","label":"Background","metadata":{},"score":"51.544422"}{"text":"one - dimensional polynomials and only later applied to neural networks .An . impressive aspect of this book is that it takes the reader all the way from .the simplest linear models to the very latest Bayesian multilayer neural .","label":"Background","metadata":{},"score":"51.659767"}{"text":"An artificial neural network uses gradient descent to search for a local minimum in weight space .How is a local minimum different from the global minimum ?Why does n't gradient descent find the global minimum ?A concept is represented in C4.5 format with the following files .","label":"Background","metadata":{},"score":"51.707108"}{"text":"How would you represent the following concepts in a decision tree : A OR B A AND NOT B ( A AND B ) OR ( C OR NOT D ) 7 .What problem does reduced - error pruning address ?","label":"Background","metadata":{},"score":"51.712822"}{"text":"Langley , P. , & Sage , S. ( 1997 ) .Scaling to domains withmany irrelevant features .In R. Greiner ( Ed . ) , Computational learning theory and natural learning systems ( Vol .MIT Press .Oates , T. , & Jensen , D. ( 1997 ) .","label":"Background","metadata":{},"score":"51.809364"}{"text":"Abdi , H. , Valentin , D. , and Edelman , B. ( 1999 ) , Neural Networks , Sage .University Papers Series on Quantitative Applications in the Social .Sciences , 07 - 124 , Thousand Oaks , CA : Sage , ISBN 0 - 7619 - 1440 - 4 .","label":"Background","metadata":{},"score":"51.934204"}{"text":"15 , pp .445 - 456 ) .MIT Press .Quinlan , J.R. ( 1996 ) .Bagging , boosting , and c4.5 .Proceedings of the Thirteenth National Conference on Artificial Intelligence ( pp .725 - 730 ) .","label":"Background","metadata":{},"score":"51.964787"}{"text":"How do you translate a decision tree into a corresponding set of rules ?What mechanism was suggested in class for dealing with continuous - valued attributes in a decision tree ?What mechanism was suggested in class for dealing with missing attribute values in a decision tree ?","label":"Background","metadata":{},"score":"51.997665"}{"text":"without ever addressing the underlying fundamentals ( ' why it works ' ) - . important basic concepts such as clustering , principal components or . gradient descent are not treated .It 's also full of errors , and unhelpful . diagrams drawn with what appears to be PCB board layout software from the . '","label":"Background","metadata":{},"score":"52.014404"}{"text":"This is an excellent classic work on neural nets from the perspective of . physics covering a wide variety of networks .Comments from readers of comp.ai.neural-nets : \" My first impression is that this one is by far the .","label":"Background","metadata":{},"score":"52.048378"}{"text":"184 - 191 .Mohri , Takao , and Hidehiko Tanaka , ( 1994 ) . \"An Optimal Weighting Criterion of Case Indexing for both Numeric and Symbolic Attributes .In D. W. Aha ( Ed . ) , Case - Based Reasoning :","label":"Background","metadata":{},"score":"52.079987"}{"text":"Handbook of Neural Computing .Applications .Academic Press .ISBN : 0 - 12 - 471260 - 6 .( 451 pages ) .Comments from readers of comp.ai.neural-nets : \" They cover a broad area \" ; . \"","label":"Background","metadata":{},"score":"52.131157"}{"text":"San Mateo , CA : Morgan Kaufmann .Hinton , G. E. , & Sejnowski , T. ( 1986 ) .Learning and relearning in Boltzmann machines .In D. E. Rumelhart & J. L. McClelland ( Eds . ) , Parallel distributed processing ( Vol .","label":"Background","metadata":{},"score":"52.237377"}{"text":"6.1.5 Fuzzy Inference .6.1.6 Zadeh 's Compositional Rule of Inference .6.1.7 Defuzzification . 6.2 Mathematical Similarities between Neural Networks and Fuzzy Logic Models .6.3 Fuzzy Additive Models .Problems .Simulation Experiments .Case Studies .7.1 Neural Networks - Based Adaptive Control .","label":"Background","metadata":{},"score":"52.353672"}{"text":"Hence machine learns the given function and it is capable to work for extrapolated examples as well .In this type of learning machine develops the capability to forecast or learn new concept or given hypothesis on the basis of examples that are provided .","label":"Background","metadata":{},"score":"52.456406"}{"text":"Bathe , K. J. ( 1996 ) .Finite element procedures .Englewood Cliffs , NJ : Prentice - Hall .Baum , L. E. , Petrie , T. , Soules , G. , & Weiss , N. ( 1970 ) .","label":"Background","metadata":{},"score":"52.49563"}{"text":"Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case .graphical models Bayesian networks belief networks probabilistic inference approximate inference variational methods mean field methods hidden Markov models Boltzmann machines neural networks .Share .","label":"Background","metadata":{},"score":"52.54034"}{"text":"AAAI Press .Jensen , D. Labeling space : A tool for thinking about significance testing in knowledge discovery .Office of Technology Assessment , U.S. Congress , 1995 .Kibler , D. and Langley , P. Machine learning as an experimental science .","label":"Background","metadata":{},"score":"52.571655"}{"text":".. s ?We will argue in this section that the answer to these related questions is \" no . \" \" ...This paper describes work in the StatLog project comparing classification algorithms on large real - world problems .The algorithms compared were from : symbolic learning ( CART , C4.5 , NewID , AC 2 , ITrule , Cal5 , CN2 ) , statistics ( Naive Bayes , k - nearest neighbor , kernel density , linear discriminant , qua ... \" .","label":"Background","metadata":{},"score":"52.631252"}{"text":"the entire training set of L cases -- see What are cross - validation and .bootstrapping ?Also , there are L leave - one - out networks , not L-1 .While Swingler has some knowldege of statistics , his expertise is not . sufficient for him to detect that certain articles on neural nets are . statistically nonsense .","label":"Background","metadata":{},"score":"52.82761"}{"text":"About this Article .Title .An Empirical Comparison of Voting Classification Algorithms : Bagging , Boosting , and Variants \" ...In the past , nearest neighbor algorithms for learning from examples have worked best in domains in which all features had numeric values .","label":"Background","metadata":{},"score":"52.841805"}{"text":"Cameron - Jones , R. M. , ( 1995 ) .Instance Selection by Encoding Length Heuristic with Random Mutation Hill Climbing .In Proceedings of the Eighth Australian Joint Conference on Artificial Intelligence , pp .99 - 106 .Carpenter , Gail A. , and Stephen Grossberg , ( 1987 ) .","label":"Background","metadata":{},"score":"52.86268"}{"text":"Chapter headings : The Bayes Error ; Inequalities and Alternate Distance .Measures ; Linear Discrimination ; Nearest Neighbor Rules ; Consistency ; Slow .Rates of Convergence ; Error Estimation ; The Regular Histogram Rule ; Kernel .Rules ; Consistency of the k - Nearest Neighbor Rule ; Vapnik - Chervonenkis .","label":"Background","metadata":{},"score":"52.86765"}{"text":"Berkeley : Statistics Department , University of California .Breiman , L. ( 1996b ) .Bagging predictors .Machine Learning , 24 , 123 - 140 .Breiman , L. ( 1997 ) .Arcing the edge ( Technical Report 486 ) .","label":"Background","metadata":{},"score":"53.049545"}{"text":"It is a well known technique because it requires less input data for further analysis .This is the greatest advantage of this technique as compared to other learning techniques which require extensive input , detailed examination of intermediate results and explanation of results are required [ 8].","label":"Background","metadata":{},"score":"53.12512"}{"text":"research on computational approaches to learning .The journal .publishes articles reporting substantive research results on a .wide range of learning methods applied to a variety of task . domains .The ideal paper will make a theoretical contribution . supported by a computer implementation .","label":"Background","metadata":{},"score":"53.147007"}{"text":"This paper describes a method for learning shape descriptions of 2D objects in x - ray images .The descriptions are induced from shape examples using the AQ15c inductive learning system .The method has been experimentally compared to k - nearest neighbor , a statistical pattern recognition technique , and ... \" .","label":"Background","metadata":{},"score":"53.16436"}{"text":"Domingos , P. , & Pazzani , M. ( 1997 ) .Beyond independence : Conditions for the optimality of the simple Bayesian classifier .Machine Learning , 29 ( 2/3 ) , 103 - 130 .Drucker , H. , & Cortes , C. ( 1996 ) .","label":"Background","metadata":{},"score":"53.18804"}{"text":"In D. S. Touretzky , M. C. Mozer , & M. E. Hasselmo ( Eds . ) , Advances in neural information processing systems 8 .Cambridge , MA : MIT Press .Williams , C. K. I. , & Hinton , G. E. ( 1991 ) .","label":"Background","metadata":{},"score":"53.20728"}{"text":"UCI repository of machine learning databases .Cestnik , B. ( 1990 ) .Estimating probabilities : A crucial task in machine learning .In L.C. Aiello ( Ed . ) , Proceedings of the Ninth European Conference on Artificial Intelligence ( pp .","label":"Background","metadata":{},"score":"53.20996"}{"text":"Self - organizing and Matching Nets , Applications , Implementations , Appendix ) .Books with Source Code ( C , C++ ) .Blum , Adam ( 1992 ) , Neural Networks in C++ , Wiley .Review by Ian Cresswell .","label":"Background","metadata":{},"score":"53.229393"}{"text":"The statistical and neural - network methods perform the best on this particular problem and we discuss a potential reason for this ob- served difference .We also discuss the role of bias in machine ] earning and its importance in explaining performance differences observed on specific problems . by Jude W. Shavlik , Raymond J. Mooney , Geoffrey G. Towell - Machine Learning , 1991 . \" ...","label":"Background","metadata":{},"score":"53.307175"}{"text":"In symbolic domains , a more sophisticated treatment of t ... \" .In the past , nearest neighbor algorithms for learning from examples have worked best in domains in which all features had numeric values .In such domains , the examples can be treated as points and distance metrics can use standard definitions .","label":"Background","metadata":{},"score":"53.33504"}{"text":"Study of On - line Learning ; On - line Learning in Switching and Drifting .Environments with Application to Blind Source Separation ; Parameter .Adaptation in Stochastic Optimization ; Optimal On - line Learning in .Multilayer Neural Networks ; Universal Asymptotics in Committee Machines with .","label":"Background","metadata":{},"score":"53.406116"}{"text":"ISBN : 0262112450 .Solla , S.A. , Leen , T. , and Müller , K.-R. , eds .( 2000 ) Advances in Neural .Information Processing Systems 12 , Cambridge , MA : The MIT Press , ISBN : .","label":"Background","metadata":{},"score":"53.437443"}{"text":"conventional approaches , it is shown that MLP probability estimation can .improve recognition performance .Other approaches are discussed , though .there is no such unequivocal experimental result for these methods .Connectionist Speech Recognition : A Hybrid Approach is of use to anyone . intending to use neural networks for speech recognition or within the . framework provided by an existing successful statistical approach .","label":"Background","metadata":{},"score":"53.44505"}{"text":"orbitofrontal cortex ; Cortical networks for invariant pattern recognition ; .Motor systems : cerebellum and basal ganglia ; Cerebral neocortex .Schmajuk , N.A. ( 1996 )Animal Learning and Cognition : A Neural Network .Approach , Cambridge : Cambridge University Press , ISBN 0521456967 .","label":"Background","metadata":{},"score":"53.47574"}{"text":"Chapter headings : Introduction ; Learning Processes ; Single Layer .Perceptrons ; Multilayer Perceptrons ; Radial - Basis Function Networks ; Support .Vector Machines ; Committee Machines ; Principal Components Analysis ; .Self - Organizing Maps ; Information - Theoretic Models ; Stochastic Machines And .","label":"Background","metadata":{},"score":"53.56494"}{"text":"Lauderdale , FL .Jaakkola , T. S. , & Jordan , M. I. ( 1999a ) .Improving the mean field approximation via the use of mixture distributions .In M. I. Jordan ( Ed . ) , Learning in graphical models .","label":"Background","metadata":{},"score":"53.594906"}{"text":"Problems ; Neural Networks for Estimation , Identification and Prediction ; .Neural Networks for Discrete and Combinatorial Optimization Problems .Pulsed / Spiking networks .Maass , W. , and Bishop , C.M. , eds .( 1999 ) Pulsed Neural Networks , .","label":"Background","metadata":{},"score":"53.64982"}{"text":"Ch . 2 Simple Neural Nets for Pattern Classification , 2.1 General Discussion , . 2.2 Hebb Net , 2.3 Perceptron , 2.4 Adaline ; .Ch . 3 Pattern Association , 3.1 Training Algorithms for Pattern Association , .3.2 Heteroassociative Memory Neural Network , 3.3 Autoassociative Net , 3.4 .","label":"Background","metadata":{},"score":"53.69548"}{"text":"5.3.1 Neural Network Construction Algorithms .5.3.2 Classification and Regression Trees ( CART ) .5.4 Feature Selection , Optimization , and Stat .Learning Th . 5.5 Summary .References .6 Methods for Data Reduction and Dim .Reduction .","label":"Background","metadata":{},"score":"53.71881"}{"text":"8.1.6 Fletcher - Reeves Method .8.1.7 Polak - Ribiere Method .8.1.8 Two Specialized Algorithms for a Sum - of - Error - Squares Error Function .Gauss - Newton Method .Levenberg - Marquardt Method .8.2 Genetic Algorithms and Evolutionary Computing .","label":"Background","metadata":{},"score":"53.769913"}{"text":"6.3.1 Discrete Principal Curves and Self - org .Map Alg .6.3.2 Statistical Interpretation of the SOM Method .6.3.3 Flow - through Version of the SOM and Learning Rate Schedules .6.3.4 SOM Applications and Modifications .6.3.5 Self - supervised MLP .","label":"Background","metadata":{},"score":"53.79447"}{"text":"Structure Models with Multidimensional Latent Variables ; Artificial Neural .Networks and Multivariate Statistics .White , H. ( 1992b ) , Artificial Neural Networks : Approximation and Learning .Theory , Blackwell , ISBN : 1557863296 .Articles : There Exists a Neural Network That Does Not Make Avoidable .","label":"Background","metadata":{},"score":"53.91392"}{"text":"New York : .John Wiley and Sons , Inc. .Additional Information : One has to search .Franco Insana comments : . is some formulation of backprop theory .McClelland , J. L. and Rumelhart , D. E. ( 1988 ) .","label":"Background","metadata":{},"score":"54.09559"}{"text":"In other words the agent / machine receives some evaluation of actions such as a scalar reward or punishment from its surroundings .Figure 4 .Reinforcement Learning Figure 2 .Supervised Machine Learning This type of learning works with examples .","label":"Background","metadata":{},"score":"54.13916"}{"text":"Generalizations of the PAC Model for Neural Net and Other Learning .Applications ; The Relationship Between PAC , the Statistical Physics .Framework , the Bayesian Framework , and the VC Framework ; Statistical Physics .Models of Supervised Learning ; On Exhaustive Learning ; A Study of .","label":"Background","metadata":{},"score":"54.18088"}{"text":"7.1.2 Indirect Learning Architecture .7.1.3 Specialized Learning Architecture .7.1.4 Adaptive Backthrough Control . 7.2 Financial Time Series Analysis .7.3 Computer Graphics .7.3.1 One - Dimensional Morphing .7.3.2Multidimensional Morphing .7.3.3 Radial Basis Function Networks for Human Animation .","label":"Background","metadata":{},"score":"54.27374"}{"text":"Neural Information Processing Systems 5 , San Mateo , CA : Morgan Kaufmann , .ISBN : 1558602747 .Cowan , J.D. , Tesauro , G. , and Alspector , J. , eds .( 1994 ) Advances in .Neural Information Processing Systems 6 , San Mateo , CA : Morgan Kaufman , .","label":"Background","metadata":{},"score":"54.293587"}{"text":"be sensitized to the issues that can affect successful completion of such . applications . \" Bigus succeeds in explaining NNs at a practical , intuitive , . and necessarily shallow level without formulas -- just what the SBE needs .This book is far better than Caudill and Butler ( 1990 ) , a popular but .","label":"Background","metadata":{},"score":"54.344902"}{"text":"Advances at the Interface , Oxford : Oxford University Press , ISBN .Articles : Flexible Discriminant and Mixture Models ; Neural Networks for .Unsupervised Learning Based on Information Theory ; Radial Basis Function .Networks and Statistics ; Robust Prediction in Many - parameter Models ; Density .","label":"Background","metadata":{},"score":"54.368126"}{"text":"Automatic Kernel Rules ; Automatic Nearest Neighbor Rules ; Hypercubes and .Discrete Spaces ; Epsilon Entropy and Totally Bounded Sets ; Uniform Laws of .Large Numbers ; Neural Networks ; Other Error Estimates ; Feature Extraction .The best books on neurofuzzy systems .","label":"Background","metadata":{},"score":"54.38697"}{"text":"Theory , and Methods , NY : John Wiley & Sons ; ISBN : 0 - 471 - 15493 - 8 .Detailed Table of Contents : . 1 Introduction . 1.1 Learning and Statistical Estimation .1.2 Statistical Dependency and Causality . 1.3 Characterization of Variables . 1.4 Characterization of Uncertainty .","label":"Background","metadata":{},"score":"54.420403"}{"text":"Networks , 2011 .Rajkamal , Mobile Computing , Oxford University Press , 2007 .Jawad Oubaha , Ahmed Habbani and Mohammed Elkoutbi , \" Mobile Intellient System ( MIS ) and a multi - criteria in MPLS networks \" , Internationla Journla of Next Generation Networks ( IJNGN ) , 2:4 , pp .","label":"Background","metadata":{},"score":"54.495586"}{"text":"There are also .brief chapters on data preparation and diagnostic plots , topics usually . ignored in elementary NN books .Only feedforward nets are covered in any .detail .Chapter headings : Mapping Functions ; Basic Concepts ; Error Derivatives ; .","label":"Background","metadata":{},"score":"54.52856"}{"text":"I .Random Forest Random forests models are used for classification purpose .Random forests are a combination of tree forecasters such that every tree depends upon the values of an independently sampled random vector where same distribution is used for all trees in the forest .","label":"Background","metadata":{},"score":"54.588066"}{"text":"Ch . 4 Neural Networks Based on Competition , 4.1 Fixed - Weight Competitive .Nets , 4.2 Kohonen Self - Organizing Maps , 4.3 Learning Vector Quantization , . 4.4 Counterpropagation ; .Ch .5 Adaptive Resonance Theory , 5.1 Introduction , 5.2 Art1 , 5.3 Art2 ; .","label":"Background","metadata":{},"score":"54.671726"}{"text":"The best of such neural net \" cookbooks \" is probably .Haykin 's ( 1999 ) second edition .Among conceptually - integrated books , there are two excellent books that use .the Vapnil - Chervonenkis theory as a unifying theme , and provide strong .","label":"Background","metadata":{},"score":"54.697052"}{"text":"These models are based on the Darwin theory \" the survival of the fittest \" relying on the fitness function where the best solutions are selected from a set of individuals .The individuals that are found fit have good chance of their selection .","label":"Background","metadata":{},"score":"54.701797"}{"text":"Chapter 4 describes a variety of NNs and .what they are good for .Chapter 5 goes into practical issues of training and . testing NNs .Chapters 6 and 7 explain how to use the results from NNs .Chapter 8 discusses intelligent agents .","label":"Background","metadata":{},"score":"54.746162"}{"text":"2.4.3 The Nonlinear Classifier .2.4.4 Regression by Support Vector .Machines .Problems .Simulation Experiments .Single - Layer Networks .3.1 The Perceptron .3.1.1The Geometry of Perceptron Mapping .3.1.2 Convergence Theorem and .Perceptron Learning Rule .","label":"Background","metadata":{},"score":"54.761322"}{"text":"Menlo Park , CA : AIII Press , pp .123 - 127 .Papadimitriou , Christos H. , and Jon Louis Bentley , ( 1980 ) .A Worst - Case Analysis of Nearest Neighbor Searching by Projection .Lecture Notes in Computer Science , Vol . 85 , Automata Languages and Programming , pp .","label":"Background","metadata":{},"score":"55.00814"}{"text":"San Mateo , CA : Morgan Kaufmann .Frey , B. , Hinton , G. E. , & Dayan , P. ( 1996 ) .Does the wake - sleep algorithm learn good density estimators ?In D. S. Touretzky , M. C. Mozer , & M. E. Hasselmo ( Eds . ) , Advances in neural information processing systems 8 .","label":"Background","metadata":{},"score":"55.03401"}{"text":"Chan , P. , Stolfo , S. , & Wolpert , D. ( 1996 ) .Integrating multiple learned models for improving and scaling machine learning algorithms .AAAI Workshop .Craven , M.W. , & Shavlik , J.W. ( 1993 ) .","label":"Background","metadata":{},"score":"55.064434"}{"text":"Attribute - value learning versus inductive logic programming : The missing links .Proceedings of the Eighth International Conference on Inductive Logic Programming ( pp . 1 - 8 ) .Springer - Verlag .Dietterich , T.G. , Jain , A. , Lathrop , R. H. , & Lozano - Pérez , T. ( 1994 ) .","label":"Background","metadata":{},"score":"55.074318"}{"text":"Give the input and output vectors for each of the data points shown above .What are the advantages and disadvantages of your representation ?How does a k - Nearest Neighbor learner make predictions about new data points ?How does a distance - weighted k - Nearest Neighbor learner differ from a standard k - Nearest Neighbor learner ?","label":"Background","metadata":{},"score":"55.107594"}{"text":"Swingler , K. ( 1996 ) , Applying Neural Networks : A Practical Guide , London : .Academic Press .Review by Ian Cresswell .( For a review of the text , see \" The Worst \" below . )","label":"Background","metadata":{},"score":"55.26391"}{"text":"Renals , Steve , and Richard Rohwer , ( 1989 ) .Phoneme Classification Experiments Using Radial Basis Functions .In Proceedings of the IEEE International Joint Conference on Neural Networks ( IJCNN'89 ) , Vol . 1 , pp .461 - 467 .","label":"Background","metadata":{},"score":"55.35014"}{"text":"Buntine , W. ( 1992a ) .Learning classification trees .Statistics and Computing , 2 ( 2 ) , 63 - 73 .Buntine , W. ( 1992b ) .A theory of learning classification rules .Ph.D. thesis , University of Technology , Sydney , School of Computing Science .","label":"Background","metadata":{},"score":"55.39495"}{"text":"pricing , customer ranking , and sales forecasting .Bigus provides generally sound advice .He briefly discusses overfitting and .overtraining without going into much detail , although I think his advice on .p. 57 to have at least two training cases for each connection is somewhat . lenient , even for noise - free data .","label":"Background","metadata":{},"score":"55.42125"}{"text":"Addison - Wesley , ISBN : 0 - 201 - 56629-X. Helps the reader make his own NNs .The mathematica code for the programs in .Freeman , J.A. and Skapura , D.M. ( 1991 ) .Neural Networks : Algorithms , .","label":"Background","metadata":{},"score":"55.424156"}{"text":"Jaakkola , T. S. , & Jordan , M. I. ( 1999b ) .Variational methods and the QMR - DT database .Journal of Artificial Intelligence Research , 10 , 291 - 322 .Jensen , C. S. , Kong , A. , & Kjærulff , U. ( 1995 ) .","label":"Background","metadata":{},"score":"55.463394"}{"text":"Self - Organizing Neural Networks , Cambridge , MA : The MIT Press , ISBN .Articles on ART , BAM , SOMs , counterpropagation , etc . .Nilsson , N.J. ( 1965/1990 ) , Learning Machines , San Mateo , CA : Morgan .","label":"Background","metadata":{},"score":"55.47555"}{"text":"The AQ15c learning method is shown to have distinct advantages over the aforementioned techniques in terms of higher or comprable classification accuracy , learning and recognition time , and understandability of learned concepts .This approach is well - suited for recognizing objects that can be isolated in the image using histogram and thresholding techniques and that have little internal structure .","label":"Background","metadata":{},"score":"55.51009"}{"text":"why various methods behave as they do , or under what conditions a method .will or will not work well .It has no discussion of efficient training .methods such as RPROP or conventional numerical optimization techniques .And , most egregiously , it has no explanation of overfitting and .","label":"Background","metadata":{},"score":"55.53132"}{"text":"4.3.2 Number of Neurons in a Hidden Layer , or the Bias - Variance Dilemma .4.3.3 Type of Activation Functions in a Hidden Layer and the Geometry of Approximation .4.3.4 Weights Initialization .4.3.5 Error Function for Stopping Criterion at Learning .","label":"Background","metadata":{},"score":"55.53742"}{"text":"Variations , 6.3 Theoretical Results ; .Ch . 7 A Sampler of Other Neural Nets , 7.1 Fixed Weight Nets for Constrained .Optimization , 7.2 A Few More Nets that Learn , 7.3 Adaptive Architectures , . 7.4 Neocognitron ; Glossary .","label":"Background","metadata":{},"score":"55.547455"}{"text":"The method has been experimentally compared to k - nearest neighbor , a statistical pattern recognition technique , ... \" .In this paper , we describe a method for learning shape descriptions of objects in x - ray images .The descriptions are induced from shape examples using the AQ15c inductive learning system .","label":"Background","metadata":{},"score":"55.658302"}{"text":"It works with understandable rules .b )It performs classification without much of tedious computation . A. Chaudhary et al Int .Journal of Engineering Research and Applications ISSN : 2248 - 9622 , Vol . 3 , Issue 6 , Nov - Dec 2013 , pp.913 - 917 c )","label":"Background","metadata":{},"score":"55.70774"}{"text":"Introduction ( 2nd ed . )Berlin , Heidelberg , New York : Springer - Verlag .ISBN .( DOS 3.5 \" disk included . )Book Webpage ( Publisher ) : .Comments from readers of comp.ai.neural-nets : \" The book was developed out of .","label":"Background","metadata":{},"score":"55.77313"}{"text":"Neuro - Fuzzy Function Approximation ; Neural Networks and Fuzzy Prolog ; Using .Neuro - Fuzzy Systems .General ( including SVMs and Fuzzy Logic ) .Many books on neural networks , machine learning , etc . , present various .","label":"Background","metadata":{},"score":"55.813305"}{"text":"generalization is skimpy compared to the books by Weiss and .Kulikowski or Smith listed above .If you 're new to neural nets and you do n't want to be swamped by .bogus ideas , huge amounts of intimidating looking mathematics , a .","label":"Background","metadata":{},"score":"55.82415"}{"text":"Briefly covers at a very elementary level feedforward nets , linear and .nearest - neighbor discriminant analysis , trees , and expert sytems , . emphasizing practical applications .For a book at this level , it has an .unusually good chapter on estimating generalization error , including .","label":"Background","metadata":{},"score":"55.863632"}{"text":"higher mathematical level .Ripley also covers a variety of methods that are . not discussed , or discussed only briefly , by Bishop , such as tree - based .methods and belief networks .While Ripley is best appreciated by people with .","label":"Background","metadata":{},"score":"55.985508"}{"text":"Plunkett , K. , and Elman , J.L. ( 1997 ) , Exercises in Rethinking Innateness : A .Handbook for Connectionist Simulations , Cambridge , MA : The MIT Press , ISBN : .Chapter headings : Introduction and overview ; The methodology of simulations ; .","label":"Background","metadata":{},"score":"55.987793"}{"text":"Neural Networks ( Source code and executables for many different platforms . including Unix , DOS , and Macintosh .ANN development tools , example .networks , sample data , tutorials .A complete collection of Neural Digest . is included as well . )","label":"Background","metadata":{},"score":"56.00403"}{"text":"Masters , T. ( 1995 ) Advanced Algorithms for Neural Networks : A C++ .Sourcebook , NY : John Wiley and Sons , ISBN 0 - 471 - 10588 - 0 .Additional Information : One has to search .Clear explanations of conjugate gradient and Levenberg - Marquardt . optimization algorithms , simulated annealing , kernel regression ( GRNN ) and . discriminant analysis ( PNN ) , Gram - Charlier networks , dimensionality .","label":"Background","metadata":{},"score":"56.034245"}{"text":"8.2.2 Mechanism of Genetic Algorithms .Mathematical Tools of Soft Computing .9.1 Systems of Linear Equations .9.2 Vectors and Matrices . 9.3 Linear Algebra and Analytic Geometry .9.4 Basics of Multivariable Analysis . 9.5 Basics from Probability Theory .","label":"Background","metadata":{},"score":"56.081947"}{"text":"On p. 110 , Swingler . reports an article that attempts to apply bootstrapping to neural nets , but .this article is also obviously wrong to anyone familiar with bootstrapping .While Swingler can not be blamed entirely for accepting these articles at .","label":"Background","metadata":{},"score":"56.123566"}{"text":"167 - 181 .Wettschereck , Dietrich , David W. Aha , and Takao Mohri , ( 1995 ) .A Review and Comparative Evaluation of Feature Weighting Methods for Lazy Learning Algorithms .Technical Report AIC-95 - 012 .Washington , D.C. : Naval Research Laboratory , Navy Center for Applied Research in Artificial Intelligence . A. Chaudhary et al Int .","label":"Background","metadata":{},"score":"56.201744"}{"text":"For each of the algorithms above , show how it works on a specific problem ( examples of these may be found in the book or in the notes ) .Why is inductive bias important for a machine learning algorithm ?","label":"Background","metadata":{},"score":"56.274635"}{"text":"Learning Long - Term Dependencies in NARX Recurrent Neural Networks ; .Oscillation Responses in a Chaotic Recurrent Network ; .Lessons from Language Learning ; .Recurrent Autoassociative Networks : Developing Distributed Representations . of Hierarchically Structured Sequences by Autoassociation ; .","label":"Background","metadata":{},"score":"56.348152"}{"text":"The report covers work done on the following projects : ( 1)The Multi - level Image Sampling and Trans ... \" .This report briefly reviews research progress on vision through learning conducted as a collaborative effort of the GMU Machine Learning and Inference Laboratory and the UMD Computer Vision Laboratory . \" ...","label":"Background","metadata":{},"score":"56.45512"}{"text":"References .10 Fuzzy Systems .10.1 Terminology , Fuzzy Sets , and Operations .10.2 Fuzzy Inference Systems and Neurofuzzy Systems . 10.2.1Fuzzy Inference Systems . 10.2.2Equivalent Basis Function Representation .10.2.3Learning Fuzzy Rules from Data .10.3 Applications in Pattern Recognition . 10.3.1","label":"Background","metadata":{},"score":"56.502663"}{"text":"We present two variants of the K - nearest neighbor algorithm , called Bayesian - KNN and Citation - KNN , solving the multiple - instance problem .Experiments on the Drug discovery benchmark data show that both algorithms are competitive with the best ones conceived in the concept learning framework .","label":"Background","metadata":{},"score":"56.54677"}{"text":"Papers . emphasizing mathematical results should ideally seek to put these results . in the context of algorithm design , however purely theoretical papers will . be considered .Other papers in the areas of cultural algorithms , artificial .life , molecular computing , evolvable hardware , and the use of simulated .","label":"Background","metadata":{},"score":"56.55351"}{"text":"8.3.1 Regression - Based Methods .8.3.2 Tree - Based Methods .8.3.3 Nearest Neighbor and Prototype Methods .8.3.4 Empirical Comparisons . 8.4 Summary .References .9 Support Vector Machines .9.1 Optimal Separating Hyperplanes .9.2 High Dimensional Mapping and Inner Product Kernels . 9.3 Support Vector Machine for Classification .","label":"Background","metadata":{},"score":"56.564125"}{"text":"Blackwells , Oxford , UK , 1995 .Sejnowski , T. and Rosenberg , C. Parallel networks that learn to pronounce English text .Complex Systems , 1:145 - 168 , 1987 .Shavlik , J. , Mooney , R. and Towell , G. Symbolic and neural learning algorithms : An experimental comparison .","label":"Background","metadata":{},"score":"56.613937"}{"text":"As opposed to traditional supervised learning , multiple - instance learning concerns the problem of classifying a bag of instances , given bags that are labeled by a teacher as being overall positive or negative .Current research mainly concentrates on adapting traditional concept learning to solve this problem .","label":"Background","metadata":{},"score":"56.63701"}{"text":"Journal of Engineering Research and Applications ISSN : 2248 - 9622 , Vol . 3 , Issue 6 , Nov - Dec 2013 , pp.913 - 917 Neural Network .It works on condition - action rules , decision trees or same kind of knowledge structures .","label":"Background","metadata":{},"score":"56.67738"}{"text":"Cohen , P.R. and Jensen , D. Overfitting explained .In Prelim .Papers Sixth Intl .Workshop on Artificial Intelligence and Statistics , pages 115 - 122 , January 1997 .Denton , F. Data mining as an industry .Review of Economics and Statistics , 67:124 - 127 , 1985 .","label":"Background","metadata":{},"score":"56.695824"}{"text":"Experimental results demonstrate strong advantages of the AQ methodology over the other methods .Specifically , the method has higher predictive accuracy and faster learning and recognition rates .AQ 's representation language , VL , was better suited for this problem , which can be seen by examining the empirical results and the learned rules .","label":"Background","metadata":{},"score":"56.80821"}{"text":"6.3.2 Incremental Learning .6.4 Future Prospects for Improved Learning Methods . 6.5 Bibliographical and Historical Remarks .7 Expert Systems .7.1 Introduction and Overview .7.1.1 Why Build Expert Systems ?New vs. Old Knowledge . 7.2 Estimating Error Rates for Expert Systems . 7.3 Complexity of Knowledge Bases .","label":"Background","metadata":{},"score":"56.83564"}{"text":"5.3.4 Optimal Subset Selection by Linear .Programming .Problems .Simulation Experiments .Fuzzy Logic Systems .6.1 Basics of Fuzzy Logic Theory .6.1.1 Crisp ( or Classic ) and Fuzzy Sets .6.1.2 Basic Set Operations .6.1.3 Fuzzy Relations .","label":"Background","metadata":{},"score":"56.985683"}{"text":"Cohen , P.R. ( 1995 ) , Empirical Methods for Artificial Intelligence , .Cambridge , MA : The MIT Press .Subject : Databases for experimentation with NNs ?UCI machine learning database .A large collection of data sets accessible via anonymous FTP at .","label":"Background","metadata":{},"score":"57.04193"}{"text":"About this Article .Title .On Comparing Classifiers : Pitfalls to Avoid and a Recommended Approach","label":"Background","metadata":{},"score":"57.16187"}{"text":"Estimating the Weight Decay Parameter ; Controling the Hyperparameter Search . in MacKay 's Bayesian Neural Network Framework ; Adaptive Regularization in .Neural Network Modeling ; Large Ensemble Averaging ; Square Unit Augmented , .Radially Extended , Multilayer Perceptrons ; A Dozen Tricks with Multitask .","label":"Background","metadata":{},"score":"57.16378"}{"text":"While the authors realize that backpropagation networks can suffer from .local minima , they mistakenly think that counterpropagation has some kind of . global optimization ability ( p. 202 ) : .Unlike the backpropagation network , a counterpropagation network . can not be fooled into finding a local minimum solution .","label":"Background","metadata":{},"score":"57.17653"}{"text":"Medsker , L.R. , and Jain , L.C. , eds .( 2000 ) , Recurrent Neural Networks : .Design and Applications , Boca Raton , FL : CRC Press , ISBN 0 - 8493 - 7181 - 3 .Articles : .","label":"Background","metadata":{},"score":"57.179276"}{"text":"The methodology and initial experimental results are discussed , along with comparisons to k - nearest neighbor and to feed - forward neural networks .The AQ15c learning method is shown ... \" .This paper describes a method for applying AQ15c to learning shape descriptions of 2D bloblike objects in x - ray images .","label":"Background","metadata":{},"score":"57.269108"}{"text":"Speech recognition .Bourlard , H.A. , and Morgan , N. ( 1994 ) , Connectionist Speech Recognition : A .Hybrid Approach , Boston : Kluwer Academic Publishers , ISBN : 0792393961 .From The Publisher : Describes the theory and implementation of a method to . incorporate neural network approaches into state - of - the - art continuous . speech recognition systems based on Hidden Markov Models ( HMMs ) to improve .","label":"Background","metadata":{},"score":"57.311325"}{"text":"Chapter 1 introduces data mining and data warehousing , and sketches some .applications thereof .Chapter 2 is the semi - obligatory .philosophico - historical discussion of AI and NNs and is well - written , .although the SBE in a hurry may want to skip it .","label":"Background","metadata":{},"score":"57.540573"}{"text":"Neural Computing : Theory & Practice .Van Nostrand .Reinhold : New York .( ISBN 0 - 442 - 20743 - 3 ) .This is not as bad as some books on NNs .It provides an elementary account . of the mechanics of a variety of networks .","label":"Background","metadata":{},"score":"57.583305"}{"text":"Proceedings of the Fifteenth International Conference on Machine Learning .San Francisco : Morgan Kaufmann .Maron , O. ( 1998 ) .Learning from ambiguity .Doctoral dissertation , Department of Electrical Engineering and Computer Science , Massachusetts Institute of Technology .","label":"Background","metadata":{},"score":"57.64114"}{"text":"It is a simple graphical model where non - terminal nodes represent tests on one or more attributes and terminal nodes give decision outcomes .This tree consists of one root , branches , internal nodes and leaves .Each node corresponds with a certain feature or characteristic or feature and the branches correspond with a range of values or decision outcomes .","label":"Background","metadata":{},"score":"57.783035"}{"text":"Feature subset selection using the wrapper model : Overfitting and dynamic search space topology .The First International Conference on Knowledge Discovery and Data Mining ( pp .192 - 197 ) .Kohavi , R. , Sommerfield , D. , & Dougherty , J. ( 1997 ) .","label":"Background","metadata":{},"score":"57.800236"}{"text":"How does a Radial Basis Function network work ?How does a kernel function work ?Give an example of a domain theory expressed as predicates .How would that domain theory be converted into a corresponding neural network by KBANN ( show the structure and weights of the resulting network ) .","label":"Background","metadata":{},"score":"57.85354"}{"text":"21 - 29 ) .San Francisco : Morgan Kaufmann .Bergadano , F. , Giordana , A. , & Saitta , L. ( 1991 ) .Machine learning : An integrated framework and its application .Chichester , UK : Ellis Horwood .","label":"Background","metadata":{},"score":"57.863342"}{"text":"getting something better ) .The Worst .How not to use neural nets in any programming language .Blum , Adam ( 1992 ) , Neural Networks in C++ , NY : Wiley .Welstead , Stephen T. ( 1994 ) , Neural Network and Fuzzy Logic .","label":"Background","metadata":{},"score":"57.89846"}{"text":"ISBN 0 - 13 - 124991 - 6 .Kosko 's new book is a big improvement over his older neurofuzzy book and . makes an excellent sequel to Brown and Harris ( 1994 ) .Nauck , D. , Klawonn , F. , and Kruse , R. ( 1997 ) , Foundations of Neuro - Fuzzy .","label":"Background","metadata":{},"score":"57.94491"}{"text":"nets are presented in a clear step by step manner in plain English .Equally , the mathematics is introduced in a relatively gentle manner .There are no unnecessary complications or diversions from the main . theme .The examples that are used to demonstrate the various algorithms are . detailed but ( perhaps necessarily ) simple .","label":"Background","metadata":{},"score":"57.95876"}{"text":"For some sound information on choosing the number . of hidden units , see How many hidden units should I use ?Choosing the number of hidden units is one important aspect of getting good .generalization , which is the most crucial issue in neural network training .","label":"Background","metadata":{},"score":"57.99124"}{"text":"Twelve datasets were used : five from image analysis , three from medicine , and two each from engineering and finance .We found that which algorithm performed best depended critically on the dataset investigated .We therefore developed a set of dataset descriptors to help decide which algorithms are suited to particular datasets .","label":"Background","metadata":{},"score":"58.138382"}{"text":"The reader will come away from this book with a .grossly oversimplified view of NNs and no concept whatsoever of how to use .NNs for practical applications .Comments from readers of comp.ai.neural-nets : \" Wasserman flatly enumerates .","label":"Background","metadata":{},"score":"58.23645"}{"text":"or disciplines with interests in Machine Learning .Researchers , .practitioners , and users of Machine Learning in academia , industry , and . government are encouraged to join the list to discuss and exchange ideas . regarding any aspect of Machine Learning , e.g. , various learning . algorithms , data pre - processing , variable selection mechanism , instance .","label":"Background","metadata":{},"score":"58.346832"}{"text":"Sidelines .Devroye , L. , Györfi , L. , and Lugosi , G. ( 1996 ) , A Probabilistic Theory of .Pattern Recognition , NY : Springer , ISBN 0 - 387 - 94618 - 7 , vii+636 pages .","label":"Background","metadata":{},"score":"58.43363"}{"text":"Morgan Kaufman , pp .293 - 301 .Wess , Stefan , Klaus - Dieter Althoff and Guido Derwand , ( 1994 ) .Using k -d Trees to Improve the Retrieval Step in Case - Based Reasoning .Stefan Wess , Klaus - Dieter Althoff , & M. M. Richter ( Eds . ) , Topics in Case - Based Reasoning .","label":"Background","metadata":{},"score":"58.486824"}{"text":"Moody , J.E. , Hanson , S.J. , and Lippmann , R.P. , eds .( 1992 ) Advances in .Neural Information Processing Systems 4 , San Mateo , CA : Morgan Kaufmann , .ISBN : 1558602224 .Hanson , S.J. , Cowan , J.D. , and Giles , C.L. eds .","label":"Background","metadata":{},"score":"58.487827"}{"text":"Necessary and Sufficient Conditions for Uniform Convergence of Means to .Their Expectations ; .Necessary and Sufficient Conditions for Uniform One - Sided Convergence of .Means to Their Expectations ; .Comments and Bibliographical Remarks .Object oriented programming .","label":"Background","metadata":{},"score":"58.522144"}{"text":"The descriptions are induced from shape examples using the AQ15c inductive learning system .The method has been experimentally compared to k - nearest neighbor , a statistical pattern recognition technique , and ... \" .This paper describes a method for learning shape descriptions of 2D objects in x - ray images .","label":"Background","metadata":{},"score":"58.568123"}{"text":"that the network is guaranteed to find the correct response ( or the .nearest stored response ) to an input , no matter what .But even though they acknowledge the problem of local minima , the authors . are ignorant of the importance of initial weight values ( p. 186 ) : .","label":"Background","metadata":{},"score":"58.595657"}{"text":"This book is much better than Vapnik 's The Nature of Statistical Learning .Theory .Chapter headings : .Introduction : The Problem of Induction and Statistical Inference ; .Two Approaches to the Learning Problem ; .Appendix : Methods for Solving Ill - Posed Problems ; .","label":"Background","metadata":{},"score":"58.72237"}{"text":"Smith is not a statistician , but he has made an impressive effort to convey . statistical fundamentals applied to neural networks .The book has entire .brief chapters on overfitting and validation ( early stopping and .split - sample validation , which he incorrectly calls cross - validation ) , .","label":"Background","metadata":{},"score":"58.740875"}{"text":"Merz , C. J. , & Murphy , P. M. ( 1996 ) .UCI repository of machine learning databases .Irvine , CA : Department of Information and Computer Science , University of California .Neal , R. ( 1992 ) .","label":"Background","metadata":{},"score":"58.77814"}{"text":"Basic Nonlinear Optimization Methods . 8.1 Classical Methods .8.1.1 Newton - Raphson Method .8.1.2 Variable Metric or Quasi - Newton Methods .8.1.3 Davidon - Fletcher - Powel Method .8.1.4 Broyden - Fletcher - Go1dfarb - Shano Method .","label":"Background","metadata":{},"score":"58.805054"}{"text":"Masters , T. ( 1994 ) , Signal and Image Processing with Neural Networks : A .C++ Sourcebook , NY : Wiley , ISBN 0 - 471 - 04963 - 8 .Additional Information : One has to search .The best intermediate textbooks on NNs .","label":"Background","metadata":{},"score":"58.892136"}{"text":"Simulation Experiments .Support Vector Machines .2.1 Risk Minimization Principles and the Concept of Uniform Convergence . 2.2The VC Dimension . 2.3Structural Risk Minimization .2.4 Support Vector Machine Algorithms .2.4.1Linear Maximal Margin Classifier for Linearly Separable Data .","label":"Background","metadata":{},"score":"58.918247"}{"text":"Subject : How to benchmark learning methods ?1995 workshop on NN benchmarking .The page contains pointers to various .papers on proper benchmarking methodology and to various sources of . datasets .Benchmark studies require some familiarity with the statistical design and . analysis of experiments .","label":"Background","metadata":{},"score":"58.93735"}{"text":"Freund , Y. ( 1996 ) .Boosting a weak learning algorithm by majority .Information and Computation , 121 ( 2 ) , 256 - 285 .Freund , Y. , & Schapire , R.E. ( 1995 ) .A decision - theoretic generalization of on - line learning and an application to boosting .","label":"Background","metadata":{},"score":"58.9412"}{"text":"An intelligent system performing this detection task can be used to assist airport security personnel with luggage screening . by M. A. Maloof , R. S. Michalski , An Initial Study - Department of Computer Science , George Mason University , 1994 . \" ...","label":"Background","metadata":{},"score":"58.97036"}{"text":"Neural Networks for Pattern Recognition , Oxford : .Oxford University Press .ISBN 0 - 19 - 853849 - 9 ( hardback ) or 0 - 19 - 853864 - 2 .( paperback ) , xvii+482 pages .This is definitely the best book on feedforward neural nets for readers .","label":"Background","metadata":{},"score":"58.974964"}{"text":"Triangulation of graphs - Algorithms giving small total state space .( Research Report R-90 - 09 ) .Department of Mathematics and Computer Science , Aalborg University , Denmark .Kjærulff , U. ( 1994 ) .Reduction of computational complexity in Bayesian networks through removal of weak dependences .","label":"Background","metadata":{},"score":"59.031414"}{"text":"There are two categories of software that provide SVM training algorithms .The first category is specialized software whose prime objective is to have an SVM solver .LIBSVM [ 6 ] and SVMlight [ 6 ] are two popular software of SVM for this category .","label":"Background","metadata":{},"score":"59.08604"}{"text":"10.3.2Fuzzy Clustering .10.4 Summary .References .Appendix A : Review of Nonlinear Optimization .Appendix B : Eigenvalues and Singular Value Decomposition .History .Hebb , D.O. ( 1949 ) , The Organization of Behavior , NY : Wiley .","label":"Background","metadata":{},"score":"59.104713"}{"text":"Unsupervised Machine Learning In this category there is no supervisor available and learning depends upon the guidance obtained heuristically by the system testing different data samples or environment .This learning is related to decision theory ( in statistics and management science ) and control theory ( in engineering ) .","label":"Background","metadata":{},"score":"59.12993"}{"text":"Just the functionality of these .subjects is described ; enough to get you started .Lots of references are .given to more elaborate descriptions .Easy to read , no extensive . mathematical background necessary .\"Zeidenberg .M. ( 1990 ) .","label":"Background","metadata":{},"score":"59.39766"}{"text":"Statistical Perspective ; Some Asymptotic Results for Learning in Single .Hidden Layer Feedforward Networks ; Connectionist Nonparametric Regression : .Multilayer Feedforward Networks Can Learn Arbitrary Mappings ; Nonparametric .Estimation of Conditional Quantiles Using Neural Networks ; On Learning the .","label":"Background","metadata":{},"score":"59.45317"}{"text":"They both have little or no discussion of generalization , . validation , and overfitting .Neither provides any valid advice on choosing .the number of hidden nodes .If you have ever wondered where these stupid . \" rules of thumb \" that pop up frequently come from , here 's a source for one . of them : . \"","label":"Background","metadata":{},"score":"59.526688"}{"text":"1.2.2 Basics of Fuzzy Logic Modeling . 1.3 Basic Mathematics of Soft Computing .1.3.1 Approximation of Multivariate Functions .1.3.2 Nonlinear Error Surface and Optimization . 1.4 Learning and Statistical Approaches to Regression and Classification .1.4.1 Regression .1.4.2 Classification .","label":"Background","metadata":{},"score":"59.528114"}{"text":"2.8.2 Unrepresentative Samples . 2.9 How Close to the Truth ? 2.10 Common Mistakes in Performance Analysis .2.11 Bibliographical and Historical Remarks .3 Statistical Pattern Recognition . 3.1 Introduction and Overview . 3.2 A Few Sample Applications . 3.3 Bayesian Classifiers .","label":"Background","metadata":{},"score":"59.70194"}{"text":"T. Caelli , W.F. Bischof , Machine Learning and Image Interpretation , Plenum Press , 1997 .Asa Ben - Hur , Jason Weston , A user 's guide to Support Vector Machines .Richard P. Lippmann , \" An introduction with computing with Neural Nets \" , ASSP IEEE 4:2 , pp .","label":"Background","metadata":{},"score":"59.767963"}{"text":"If you do n't have a good grasp of higher level math , .this book can be really tough to get through .The best advanced textbook covering NNs .Ripley , B.D. ( 1996 ) Pattern Recognition and Neural Networks , Cambridge : .","label":"Background","metadata":{},"score":"59.83259"}{"text":"The introduction to OO that is provided trivialises the area and . demonstrates a distinct lack of direction and/or understanding .The public interfaces to classes are overspecified and the design relies .upon the flawed neuron / layer / network model .","label":"Background","metadata":{},"score":"59.851925"}{"text":"Flexer , A. Statistical evaluation of neural network experiments : Minimum requirements and current practice .In R. Trappl , editor , Cybernetics and Systems ' 96 : Proc . 13thEuropean Meeting on Cybernetics and Systems Res . , pages 1005 - 1008 .","label":"Background","metadata":{},"score":"59.863197"}{"text":"Neural and Statistical Classification , Ellis Horwood .Author 's Webpage : .Other notable books .Introductory .Anderson , J.A. ( 1995 ) , An Introduction to Neural Networks , Cambridge , MA : .The MIT Press , ISBN 0 - 262 - 01144 - 1 .","label":"Background","metadata":{},"score":"59.876022"}{"text":"Many second - rate books treat neural nets as mysterious .black boxes , but Reed and Marks open up the box and provide genuine insight . into the way neural nets work .One problem with the book is that the terms \" validation set \" and \" test set \" .","label":"Background","metadata":{},"score":"59.91111"}{"text":"Network , 4 , 355 - 379 .Ghahramani , Z. , & Hinton , G. E. ( 1996 ) .Switching state - space models .( Technical Report CRG - TR-96 - 3 ) .Toronto : Department of Computer Science , University of Toronto .","label":"Background","metadata":{},"score":"59.943207"}{"text":"This makes the database quite unsuitable for training .neural networks , since for proper generalisation several instances of the . same subject are required .However , it is still useful for use as testing . set on a trained network .","label":"Background","metadata":{},"score":"60.01329"}{"text":"taught by the authors to Physics students .The book comes together with a .PC - diskette .The book is divided into three parts : ( 1 ) Models of Neural .Networks ; describing several architectures and learing rules , including the . mathematics .","label":"Background","metadata":{},"score":"60.07675"}{"text":"restatement of Kolmogorov 's theorem is possible , and Kurkova did not claim . to prove any such restatement .Swingler omits the crucial details that .Kurkova used two hidden layers , staircase - like activation functions ( not .ordinary sigmoidal functions such as the logistic ) in the first hidden .","label":"Background","metadata":{},"score":"60.20797"}{"text":"3.4.2 Model Selection via Resampling .3.4.3 Bias - variance Trade - off .3.4.4 Example of Model Selection .3.5 Summary .References .4 Statistical Learning Theory .4.1 Conditions for Consistency and Convergence of ERM . 4.2Growth Function and VC - Dimension .","label":"Background","metadata":{},"score":"60.238724"}{"text":"recognition , both with standard and neural network approaches , as well as .other pattern recognition and/or neural network researchers .This book is . also suitable as a text for advanced courses on neural networks or speech .processing .","label":"Background","metadata":{},"score":"60.24811"}{"text":"Abstract .An important component of many data mining projects is finding a good classification algorithm , a process that requires very careful thought about experimental design .If not done very carefully , comparative studies of classification and other types of algorithms can easily result in statistically invalid conclusions .","label":"Background","metadata":{},"score":"60.27079"}{"text":"The descriptions are induced from shape examples using the AQ15c inductive learning system .The method has been experimentally compared to k - nearest neighbor , a statistical pattern recognition technique , and artificial neural networks .Experimental results demonstrate strong advantages of the AQ methodology over the other methods .","label":"Background","metadata":{},"score":"60.304127"}{"text":"7.6 Future : Combined Learning and Expert Systems . 7.7 Bibliographical and Historical Remarks .Reed , R.D. , and Marks , R.J , II ( 1999 ) , Neural Smithing : Supervised Learning . in Feedforward Artificial Neural Networks , Cambridge , MA : The MIT Press , .","label":"Background","metadata":{},"score":"60.374603"}{"text":"The attempt to rationalise differing types of Neural Network into a single .very shallow but wide class hierarchy is naive .The general use of the ' float ' data type would cause serious hassle if this . software could possibly be extended to use some of the more sensitive .","label":"Background","metadata":{},"score":"60.489456"}{"text":"Orr , G.B. , and Mueller , K.-R. , eds .( 1998 ) , Neural Networks : Tricks of the .Trade , Berlin : Springer , ISBN 3 - 540 - 65311 - 2 .Articles : Efficient BackProp ; Early Stopping - But When ?","label":"Background","metadata":{},"score":"60.499065"}{"text":"Proceedings of the 13th International Joint Conference on Artificial Intelligence ( pp .1022 - 1027 ) .Morgan Kaufmann Publishers .Freund , Y. ( 1990 ) .Boosting a weak learning algorithm by majority .Proceedings of the Third Annual Workshop on Computational Learning Theory ( pp .","label":"Background","metadata":{},"score":"60.54046"}{"text":"2.5.2 Train - and Test Error Rate Estimation .2.5.3 Resampling Techniques .2.5.4 Finding the Right Complexity Fit . 2.6Getting the Most Out of the Data .2.7 Classifier Complexity and Feature Dimensionality .2.7.1 Expected Patterns of Classifier Behavior . 2.8 What Can Go Wrong ?","label":"Background","metadata":{},"score":"60.54673"}{"text":"Influencing Generalization ; Generalization Prediction and Assessment ; .Heuristics for Improving Generalization ; Effects of Training with Noisy .Inputs ; Linear Regression ; Principal Components Analysis ; Jitter .Calculations ; Sigmoid - like Nonlinear Functions .The best books on using and programming NNs .","label":"Background","metadata":{},"score":"60.602867"}{"text":"Rosenblatt , F. ( 1962 ) , Principles of Neurodynamics , NY : Spartan Books .Out . of print .Anderson , J.A. , and Rosenfeld , E. , eds .( 1988 ) , Neurocomputing : .Foundatons of Research , Cambridge , MA : The MIT Press , ISBN 0 - 262 - 01097 - 6 .","label":"Background","metadata":{},"score":"60.638496"}{"text":"2 How to Estimate the True Performance of a Learning System .2.1 The Importance of Unbiased Error Rate Estimation .What is an Error ?2.2.1 Costs and Risks . 2.3Apparent Error Rate Estimates . 2.4 Too Good to Be True : Overspecialization . 2.5 True Error Rate Estimation .","label":"Background","metadata":{},"score":"60.66971"}{"text":"The . data sets in this repository include the ' nettalk ' data , ' two spirals ' , .protein structure prediction , vowel recognition , sonar signal .classification , and a few others .Proben1 .Proben1 is a collection of 12 learning problems consisting of real data .","label":"Background","metadata":{},"score":"60.850784"}{"text":"San Mateo , CA : Morgan Kaufmann .MacKay , D. J. C. ( 1997 ) .Ensemble learning for hidden Markov models .Unpublished manuscript .Cambridge : Department of Physics , University of Cambridge .McEliece , R. J. , MacKay , D. J. C. , & Cheng , J.-F. Turbo decoding as an instance of Pearl 's \" belief propagation algorithm . \"","label":"Background","metadata":{},"score":"60.876717"}{"text":"Error ( MSE ) Algorithms ; Unsupervised Learning ; The Distributed Method and .Radial Basis Functions ; Computational Learning Theory and the BRD Algorithm ; .Constructive Algorithms ; Backpropagation ; Backpropagation : Variations and .Applications ; Simulated Annealing and Boltzmann Machines ; Expert Systems and .","label":"Background","metadata":{},"score":"61.10287"}{"text":"1 Overview of Learning Systems . 1.1 What is a Learning System ? 1.2 Motivation for Building Learning Systems . 1.3 Types of Practical Empirical Learning Systems .1.3.1 Common Theme : The Classification Model .1.3.2 Let the Data Speak . 1.4 What 's New in Learning Methods .","label":"Background","metadata":{},"score":"61.245792"}{"text":"Recurrent Neural Networks for Optimization : The State of the Art ; .Efficient Second - Order Learning Algorithms for Discrete - Time Recurrent .Neural Networks ; .Designing High Order Recurrent Networks for Bayesian Belief Revision ; .Equivalence in Knowledge Representation : Automata , Recurrent Neural .","label":"Background","metadata":{},"score":"61.50617"}{"text":"5 Machine Learning : Easily Understood Decision Rules .5.1 Introduction and Overview .5.2 Decision Trees .5.2.1 Finding the Perfect Tree .5.2.2 The Incredible Shrinking Tree .5.2.3 Limitations of Tree Induction Methods .5.3 Rule Induction .5.3.1 Predictive Value Maximization .","label":"Background","metadata":{},"score":"61.7704"}{"text":"Books for the Beginner .Not - quite - so - introductory Literature .Books with Source Code ( C , C++ ) .The Worst .Journals and magazines about Neural Networks ?Conferences and Workshops on Neural Networks ?Neural Network Associations ?","label":"Background","metadata":{},"score":"61.7992"}{"text":"what it has is very interesting and much of it is not found in other texts .The emphasis is on statistical proofs of universal consistency for a wide .variety of methods , including histograms , ( k ) nearest neighbors , kernels .","label":"Background","metadata":{},"score":"61.872097"}{"text":"Aha , D. W. ( Ed . )Lazy Learning .Dordrecht , The Netherlands : Kluwer Academic Publishers .Auer , P. ( 1997 ) .On learning from multi - instance examples : Empirical evaluation of a theoretical approach .","label":"Background","metadata":{},"score":"61.94178"}{"text":"Bary , Italy : Morgan Kaufmann .Zucker , J.-D. , & Ganascia , J.-G. Learning structurally indeterminate clauses .Proceedings of the Eighth International Conference on Inductive Logic Programming ( pp .235 - 244 ) .Springer - Verlag .","label":"Background","metadata":{},"score":"61.9442"}{"text":"Keywords : decision tree , incremental induction , direct metric , binary test , example incorporation , missing value , tree transposition , installed test , virtual pruning , update cost .Introduction Decision tree induction offers a highly practical method for generalizing from examples whose class membership is known .","label":"Background","metadata":{},"score":"61.977478"}{"text":"o On p. 109 , Swingler describes leave - one - out cross - validation , which he .ascribes to Hecht - Neilsen .But Swingler concludes , \" the method provides .you with L minus 1 networks to choose from ; none of which has been . validated properly , \" completely missing the point that cross - validation .","label":"Background","metadata":{},"score":"61.980183"}{"text":"An Empirical Comparison of Voting Classification Algorithms : Bagging , Boosting , and Variants .Abstract .Methods for voting classification algorithms , such as Bagging and AdaBoost , have been shown to be very successful in improving the accuracy of certain classifiers for artificial and real - world datasets .","label":"Background","metadata":{},"score":"62.01071"}{"text":"The sp ... \" .This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context .The algorithms tested include statistical , neural - network , decision - tree , rule - based , and case - based classification techniques .","label":"Background","metadata":{},"score":"62.030354"}{"text":"Areej Shhab , Gongde Guo , Daniel Neagu , \" A study on applications of Machine Learning Techniques in Data Mining \" , 22nd BNCOD Workshop , 2005 .Nigel Williams , Sebastian Zander , Grenville Armitage , \" A preliminary Performance comparison of Five Machine Learning Algorithms for practical IP traffic flow classification \" , ACM SIGCOMM Computer Communication Review , 36:5 , 2006 .","label":"Background","metadata":{},"score":"62.05584"}{"text":"C++ Neural Networks and Fuzzy Logic .MIS : Press , ISBN 1 - 55828 - 298-x , US $ 45 incl . disks .Covers a wider variety of networks than Masters ( 1993 ) , but is shallow and . lacks Masters 's insight into practical issues of using NNs .","label":"Background","metadata":{},"score":"62.268814"}{"text":"Blum offers some profound advice on choosing inputs : . \"The next step is to pick as many input factors as possible that . might be related to [ the target].Blum also shows a deep understanding of statistics : . \" A statistical model is simply a more indirect way of learning . correlations .","label":"Background","metadata":{},"score":"62.30293"}{"text":"Rumelhart , Hinton , and Williams .Anderson , J. A. , Pellionisz , A. and Rosenfeld , E. ( Eds ) .Neurocomputing 2 : Directions for Research .The MIT Press : Cambridge , MA .Carpenter , G.A. , and Grossberg , S. , eds .","label":"Background","metadata":{},"score":"62.43245"}{"text":"elementary properties of error functions and optimization algorithms .For . example , in their discussion of the delta rule , the authors seem oblivious . to the differences between batch and on - line training , and they attribute .magical properties to the algorithm ( p. 71 ) : .","label":"Background","metadata":{},"score":"62.486454"}{"text":"Backpropagation occasionally outperforms the other two systems when given relatively small amounts of training data .It is slightly more accurate than ID3 when examples are noisy or incompletely specified .Finally , backpropagation more effectively utilizes a & quot;distributed & quot ; output encoding .","label":"Background","metadata":{},"score":"62.54973"}{"text":"Alpaydin [ 3 ] states the two important factors for growth in this field are elimination of tedious human effort and reduction of cost .Thus , machine learning algorithms work better in improving the efficiency and accuracy of intelligent decisions making process by intelligent computer programmes .","label":"Background","metadata":{},"score":"62.65259"}{"text":"o Deterministic chaos in an X - ray pulsar ?( Her X-1 ) .Miscellaneous Images .CityU Image Processing Lab : .Lenna 97 : A Complete Story of Lenna : . StatLib .University has a large collection of data sets , many of which can be used .","label":"Background","metadata":{},"score":"62.67774"}{"text":"Neural Networks in the Capital Markets .Chichester , England : John Wiley and Sons , Inc. .Additional Information : One has to search .Franco Insana comments : . chapters , which could be a valuable reference source for any .","label":"Background","metadata":{},"score":"62.690002"}{"text":"Keywords Mobile phones ; CART , Bluetooth , ANN , GSM , k - NN , Mobile computers , Mobile Intelligent System , PDA , Pocket PC , Wireless networks , 3G+ . I. INTRODUCTION The field of Machine learning has developed from the field of Artificial Intelligence that aims to follow intelligent capabilities of humans by machines .","label":"Background","metadata":{},"score":"62.734093"}{"text":"( already included in print ) on a 5.25 \" disk .The author claims that his work provides an ' Object Oriented Framework ... ' .This can best be put in his own terms ( Page 137 ) : . ...","label":"Background","metadata":{},"score":"62.748367"}{"text":"Caudill , M. and Butler , C. ( 1990 ) .Naturally Intelligent Systems .MIT Press : .Cambridge , Massachusetts .( ISBN 0 - 262 - 03156 - 6 ) .The authors try to translate mathematical formulas into English .","label":"Background","metadata":{},"score":"62.756"}{"text":"Specific aspects include self organizing systems , neurobiological .connections , network dynamics and architecture , speech recognition , . electronic and photonic implementation , robotics and controls .Includes Letters concerning new research results .( Note : Remarks are from journal announcement ) .","label":"Background","metadata":{},"score":"62.863064"}{"text":"Boosting the margin : A new explanation for the effectiveness of voting methods .In D. Fisher ( Ed . ) , Machine Learning : Proceedings of the Fourteenth International Conference ( pp .322 - 330 ) .Morgan Kaufmann .","label":"Background","metadata":{},"score":"63.083763"}{"text":"This solved case becomes a sample solution to the defined problem .When this solution is known , it is implemented practically to the real world problems and therefore it is tested .This testing process is termed as revision of the problem .","label":"Background","metadata":{},"score":"63.235603"}{"text":"4.2.2How Good Is a Linear Separation Network ? 4.3 Multilayer Neural Networks .4.3.1 Back - Propagation .4.3.2 The Practical Application of Back - Propagation . 4.4 Error Rate and Complexity Fit Estimation .4.5 Improving on Standard Back - Propagation . 4.6","label":"Background","metadata":{},"score":"63.25052"}{"text":"B. k - Nearest Neighbor ( k - NN )This is also known as k - NN or Instance Based learning which a type of supervised learning algorithm .k - NN works by simply storing in memory current training dataset , when a new query is fired , a set of related instances that show resemblance or neighbors are retrieved from memory .","label":"Background","metadata":{},"score":"63.424316"}{"text":"Neural Network Gradient Factors ; Avoiding Roundoff Error in Backpropagating .Derivatives ; Transformation Invariance in Pattern Recognition - Tangent .Distance and Tangent Propagation ; Combining Neural Networks and .Context - Driven Search for On - Line , Printed Handwriting Recognition in the .","label":"Background","metadata":{},"score":"63.436825"}{"text":"Contents : .Ch . 1 Introduction , 1.1 Why Neural Networks and Why Now ? , 1.2 What Is a .Neural Net ? , 1.3 Where Are Neural Nets Being Used ? , 1.4 How Are Neural .Networks Used ? , 1.5 Who Is Developing Neural Networks ? , 1.6 When Neural Nets .","label":"Background","metadata":{},"score":"63.50541"}{"text":"MLP with one hidden layer .Actually , Hecht - Neilsen , says \" the direct .usefulness of this result is doubtful , because no constructive method for .developing the [ output activation ] functions is known . \"Then Swingler . implies that V. Kurkova ( 1991 , \" Kolmogorov 's theorem is relevant , \" Neural .","label":"Background","metadata":{},"score":"63.85199"}{"text":"The Best .The best of the best .The best popular introduction to NNs .The best introductory book for business executives .The best elementary textbooks .The best books on using and programming NNs .The best intermediate textbooks on NNs .","label":"Background","metadata":{},"score":"63.897808"}{"text":"XX , Reading , MA : Addison - Wesley , ISBN : 0201409836 .Articles : The Status of Supervised Learning Science circa 1994 - The Search . for a Consensus ; Reflections After Refereeing Papers for NIPS ;The Probably .","label":"Background","metadata":{},"score":"63.942772"}{"text":"A mobile device has limited computational and memory capacity which restricts the application of machine learning algorithms that work with large data set .b )A mobile learning environment exhibits concept drift , for e.g. the concept being learned changes as the user 's mode of operation changes .","label":"Background","metadata":{},"score":"64.17201"}{"text":"ISSN # : 0129 - 0657 ( IJNS ) .Remark : The International Journal of Neural Systems is a quarterly . journal which covers information processing in natural .and artificial neural systems .Contributions include research papers , . reviews , and Letters to the Editor - communications under 3,000 .","label":"Background","metadata":{},"score":"64.357895"}{"text":"139 - 140 he uncritically reports .a method that allegedly obtains error bars by doing a simple linear . regression on the target vs. output scores .To a trained statistician , this .method is obviously wrong ( and , as usual in this book , the formula for .","label":"Background","metadata":{},"score":"64.36239"}{"text":"Control , NY : Prentice Hall , ISBN 0 - 13 - 134453 - 6 .Additional Information : Additional page at : . abstract can be found at : . Brown and Harris rely on the fundamental insight that that a fuzzy system is . a nonlinear mapping from an input space to an output space that can be . parameterized in various ways and therefore can be adapted to data using the .","label":"Background","metadata":{},"score":"64.37849"}{"text":"The file is about 1.8 MB . and unpacks into about 20 MB .Delve : Data for Evaluating Learning in Valid Experiments .Delve is a standardised , copyrighted environment designed to evaluate the . performance of learning methods .Delve makes it possible for users to .","label":"Background","metadata":{},"score":"64.407974"}{"text":"The partition is defined by selectin ... . by Walter Daelemans , Antal van den Bosch , Jakub Zavrel - MACHINE LEARNING , SPECIAL ISSUE ON NATURAL LANGUAGE LEARNING , 1999 . \" ...We show that in language learning , contrary to received wisdom , keeping exceptional training instances in memory can be beneficial for generalization accuracy .","label":"Background","metadata":{},"score":"64.43864"}{"text":"6 Which Technique is Best ?6.1 What 's Important in Choosing a Classifier ?6.1.1 Prediction Accuracy .6.1.2 Speed of Learning and Classification .6.1.3 Explanation and Insight . 6.2So , How Do I Choose a Learning System ? 6.3 Variations on the Standard Problem .","label":"Background","metadata":{},"score":"64.47646"}{"text":"6.1.1 Optimal Source Coding in Vector Quantization .6.1.2 Generalized Lloyd Algorithm .6.1.3 Clustering and Vector Quantization .6.1.4 EM Algorithm for VQ and Clustering . 6.2 Dimensionality Reduction : Statistical Methods .6.2.1 Linear Principal Components .6.2.2 Principal Curves and Surfaces .","label":"Background","metadata":{},"score":"64.50777"}{"text":"AI - CD - ROM ( see question \" Other sources of information \" ) .Time series .Santa Fe Competition .Various datasets of time series ( to be used for prediction learning . problems ) are available for anonymous ftp from ftp.santafe.edu in .","label":"Background","metadata":{},"score":"64.58244"}{"text":"Rethinking Innateness : A Connectionist Perspective on Development , .Cambridge , MA : The MIT Press , ISBN : 026255030X. Chapter headings : New perspectives on development ; Why connectionism ?Ontogenetic development : A connectionist synthesis ; The shape of change ; .","label":"Background","metadata":{},"score":"64.66861"}{"text":"Cambridge , MA : The MIT Press , ISBN 0 - 262 - 07145 - 2 .Chapter headings : ; Introduction and Important Definitions ; Representation .Issues ; Perceptron Learning and the Pocket Algorithm ; Winner - Take - All Groups .","label":"Background","metadata":{},"score":"64.78415"}{"text":"For anyone who wants to do active research in the field I consider it .quite inadequate \" ; \" Okay , but too shallow \" ; \" Quite easy to understand \" ; \" The . best bedtime reading for Neural Networks .","label":"Background","metadata":{},"score":"64.8394"}{"text":"Self - Organizing Neural Network .Bidirectional Associative Memory .Appendix A Support Classes .Appendix B Listings .References and Suggested Reading .However , you will learn very little about NNs other than elementary .programming techniques from Rogers .","label":"Background","metadata":{},"score":"64.91459"}{"text":"Remark : Statistical mechanics aspects of neural networks .( mostly Hopfield models ) .Title : Physical Review A : Atomic , Molecular and Optical Physics .Publish : The American Physical Society ( Am .Inst . of Physics ) .","label":"Background","metadata":{},"score":"64.9359"}{"text":"The . files have been collected from AI bulletin boards , Internet archive . sites , University computer deptartments , and other government and . civilian AI research organizations .Network Cybernetics Corporation . intends to release annual revisions to the AI CD - ROM to keep it up to . date with current developments in the field .","label":"Background","metadata":{},"score":"65.121376"}{"text":"SV Machines for Pattern Recognition ; ( includes examples of digit . recognition ) .SV Machines for Function Approximations , Regression Estimation , and .Signal Processing ; ( includes an example of positron emission tomography ) .Necessary and Sufficient Conditions for Uniform Convergence of .","label":"Background","metadata":{},"score":"65.439644"}{"text":"The performance criteria of any machine learning algorithm depend upon its Prediction accuracy , Sensitivity and Specificity [ 14].The Prediction accuracy is a measure of accuracy of prediction or classification .The sensitivity of an algorithm is a measure of how an algorithm classifies correctly the positive instances [ 14].","label":"Background","metadata":{},"score":"65.44623"}{"text":"Covers NN , genetic algorithms , fuzzy systems , wavelets , chaos . and other advanced computing approaches , as well as molecular . computing and nanotechnology .Title : Journal of Physics A : Mathematical and General .Publish : Inst . of Physics , Bristol .","label":"Background","metadata":{},"score":"65.64277"}{"text":"Stacked generalization .Neural Networks , 5 , 241 - 259 .Wolpert , D.H. ( 1994 ) .The relationship between PAC , the statistical physics framework , the Bayesian framework , and the VC framework .In D.H. Wolpert ( Ed . ) , The mathematics of generalization .","label":"Background","metadata":{},"score":"66.046555"}{"text":"rather than practical applications of NNs .The Best .The best of the best .Bishop ( 1995 ) is clearly the single best book on artificial NNs .This book .excels in organization and choice of material , and is a close runner - up to .","label":"Background","metadata":{},"score":"66.12103"}{"text":"Press , ISBN 0 - 12 - 479040 - 2 , US $ 45 incl . disks .Masters has written three exceptionally good books on NNs ( the two others . are listed below ) .He combines generally sound practical advice with some . basic statistical knowledge to produce a programming text that is far .","label":"Background","metadata":{},"score":"66.15756"}{"text":"7.3.1 Local Polynomial Estimators and Splines .7.3.2Radial Basis Function Networks .7.3.3 Orthogonal Basis Functions and Wavelets . 7.4 Adaptive Dictionary Methods .7.4.1Additive Methods and Projection Pursuit Regression .7.4.2 Multilayer Perceptrons and Backpropagation .7.4.3 Multivariate Adaptive Regression Splines . 7.5 Adaptive Kernel Methods and Local Risk Minimization .","label":"Background","metadata":{},"score":"66.3497"}{"text":"There is no mention of .training , validation , and test sets , or of other methods for estimating .generalization error .There is no practical advice on the important issue of .choosing the number of hidden units .","label":"Background","metadata":{},"score":"66.50678"}{"text":"Magazine . ; May 1989 IEEE Trans .Circuits and Systems . ; .July 1988 IEEE Trans .Acoust .Speech Signal Process .Title : The Journal of Experimental and Theoretical Artificial Intelligence .Publish : Taylor & Francis , Ltd. .","label":"Background","metadata":{},"score":"66.549286"}{"text":"Bigus says .( p. xv ) , \" For business executives , managers , or computer professionals , this .book provides a thorough introduction to neural network technology and the . issues related to its application without getting bogged down in complex . math or needless details .","label":"Background","metadata":{},"score":"66.57006"}{"text":"Nets .Addison - Wesley Publishing Company , Inc. ( ISBN 0 - 201 - 52376 - 0 ) .Lots of applications without technical details , lots of hype , lots of goofs , .no formulas .Muller , B. , Reinhardt , J. , Strickland , M. T. ( 1995 ) .","label":"Background","metadata":{},"score":"66.59627"}{"text":"Edgar , G.A. ( 1995 ) .Measure , topology , and fractal geometry ( 3rd print ) .Springer - Verlag .Garfield , E. ( 1979 ) .Citation indexing : Its theory and application in science , technology , and humanities .","label":"Background","metadata":{},"score":"66.71167"}{"text":"The .Delve learning methods and evaluation procedures are well documented , . such that meaningful comparisons can be made .The data collection . includes not only isolated data sets , but \" families \" of data sets in . which properties of the data , such as number of inputs and degree of .","label":"Background","metadata":{},"score":"66.79062"}{"text":"3.2.1 Representational Capabilities of the Adaline .3.2.2 Weights Learning for a Linear Processing Unit .Problems .Simulation Experiments .Multilayer Perceptrons .4.1 The Error Backpropagation Algorithm . 4.2The Generalized Delta Rule .4.3 Heuristics or Practical Aspects of the Error Backpropagation Algorithm .","label":"Background","metadata":{},"score":"66.79097"}{"text":"We observed that Arc - x4 behaves differently than AdaBoost if reweighting is used instead of resampling , indicating a fundamental difference .Voting variants , some of which are introduced in this paper , include : pruning versus no pruning , use of probabilistic estimates , weight perturbations ( Wagging ) , and backfitting of data .","label":"Background","metadata":{},"score":"66.845215"}{"text":"This is an interesting feature from the point of view of user because a mobile user spends very less time in system setting and more time in enjoying the benefits .But designing SVMs on a constrained device is a tedious task .","label":"Background","metadata":{},"score":"66.88576"}{"text":"Many . illustrations and intuitive examples .Winner among NN textbooks at a senior .UG / first year graduate level-[175 problems].\" Contents : Intro , Fundamentals . of Learning , Single - Layer & Multilayer Perceptron NN , Assoc .","label":"Background","metadata":{},"score":"67.0058"}{"text":"The on - line delta ] rule always takes the most efficient route from .the current position of the weight vector to the \" ideal \" position , . based on the current input pattern .The delta rule not only minimizes .","label":"Background","metadata":{},"score":"67.053925"}{"text":"Rogers , Joey ( 1996 ) , Object - Oriented Neural Networks in C++ , Academic .Press , ISBN 0125931158 .Contents : .Introduction .Object - Oriented Programming Review .Neural - Network Base Classes .ADALINE Network .","label":"Background","metadata":{},"score":"67.107414"}{"text":"Calculus of Variations ; Principal Components .Hertz , J. , Krogh , A. , and Palmer , R. ( 1991 ) .Introduction to the Theory of .Neural Computation .Redwood City , CA : Addison - Wesley , ISBN 0 - 201 - 50395 - 6 .","label":"Background","metadata":{},"score":"67.28209"}{"text":"Mohammed Alzaabi , Jawad Berri , Mohammed Jamal Zemerly , \" Web - based Architecture for Mobile learning \" , International Journal for Infonomics(IJI ) 3:1 , March 2010 .Jihoon Yang , Prashant Pai , Vasant Honavar , Les Miller , \" Mobile Intelligent Agents for Document Classification and Retrieval : A Machine Learning Approach \" , Citeseerx , 2011 .","label":"Background","metadata":{},"score":"67.5273"}{"text":"Processing : Explorations in the Microstructure of Cognition , Volumes 1 & 2 , .Cambridge , MA : The MIT Press ISBN 0 - 262 - 63112 - 1 .Hecht - Nielsen , R. ( 1990 ) , Neurocomputing , Reading , MA : Addison - Wesley , .","label":"Background","metadata":{},"score":"67.59287"}{"text":"Mobile learning is a novel and useful form of learning as it has all the strengths of elearning and bridges the gap of time and space in classroom learning [ 16].To enhance English learning [ 16 ] presented a Personalized Intelligent Mobile Learning System ( PIMS ) .","label":"Background","metadata":{},"score":"67.59503"}{"text":"You can post , read , and reply messages on the Web .Or you can choose to .receive messages as individual emails , daily summaries , daily full - text .digest , or read them on the Web only .","label":"Background","metadata":{},"score":"67.60678"}{"text":"Additional Information : Seems to be out of print .A good book for beginning programmers who want to learn how to write NN . programs while avoiding any understanding of what NNs do or why they do it .Gately , E. ( 1996 ) .","label":"Background","metadata":{},"score":"67.82811"}{"text":"Technology : .CEDAR CD - ROM 1 : Database of Handwritten Cities , States , ZIP Codes , .Digits , and Alphabetic Characters .AI - CD - ROM .Time series .Financial data .USENIX Faces .Linguistic Data Consortium .","label":"Background","metadata":{},"score":"67.84394"}{"text":"Mobile agents are agents that traverse in a network from one host to other for successful completion of different tasks [ 18].A mobile agent is an intelligent decision maker concerning its itinerary and updates the decision(s ) as per the available information as it traverses from one host to another [ 18].","label":"Background","metadata":{},"score":"68.037384"}{"text":"Along with the .data comes a technical report describing a set of rules and conventions .for performing and reporting benchmark tests and their results .Accessible via anonymous FTP on ftp.cs.cmu.edu [ 128.2.206.173 ] as ./afs / cs / project / connect / bench / contrib / prechelt / proben1 . tar.gz . and also .","label":"Background","metadata":{},"score":"68.05884"}{"text":"Computer Codes ; explanation about the demonstration programs .First part . gives a nice introduction into neural networks together with the formulas .Together with the demonstration programs a ' feel ' for neural networks can be .developed . \"","label":"Background","metadata":{},"score":"68.34908"}{"text":"There is also considerable material on validation and cross - validation .The . authors say , \" We did not scar the pages with backbreaking simulations or .quick - and - dirty engineering solutions \" ( p. 7 ) .","label":"Background","metadata":{},"score":"68.36582"}{"text":"Conditions for Consistency of Empirical Risk Minimization Principle ; .Bounds on the Risk for Indicator Loss Functions ; .Appendix : Lower Bounds on the Risk of the ERM Principle ; .Bounds on the Risk for Real - Valued Loss Functions ; .","label":"Background","metadata":{},"score":"68.54747"}{"text":"upon a free sample copy ) , I think that the journal succeeds . very well .The highest density of interesting articles I . have found in any journal .( Note : Remarks supplied by kehoe@csufres .CSUFresno .EDU ) .","label":"Background","metadata":{},"score":"68.56496"}{"text":"( Note : remark provided by J.R.M. Smits \" anjos@sci.kun.nl \" ) .Title : Computer Simulations in Brain Science .Title : Internation Journal of Neuroscience .Title : Neural Network Computation .Remark : Possibly the same as \" Neural Computation \" .","label":"Background","metadata":{},"score":"68.63638"}{"text":"PIMS used fuzzy item response theory .PIMS was implemented successfully on Personal Digital Assistant ( PDA ) to furnish personalized mobile learning for improving the reading ability of readers .PIMS also enriched English vocabulary of learners .The advancement of mobile technologies and efficient access to multimedia resources are important contributions that led to development of mobile learning systems [ 17].","label":"Background","metadata":{},"score":"68.6492"}{"text":"hidden units , saying that , \" Kurkova was able to restate Kolmogorov 's theorem . in terms of a set of sigmoidal functions .\" If Kolmogorov 's theorem , or .Hecht - Nielsen 's adaptation of it , could be restated in terms of known . sigmoid activation functions in the ( single ) hidden and output layers , then .","label":"Background","metadata":{},"score":"68.72615"}{"text":"Freq . : Quarterly ( vol . 1 in May 1997 ) .Remark : The IEEE Transactions on Evolutionary Computation will publish archival . journal quality original papers in evolutionary computation and related .areas , with particular emphasis on the practical application of the . techniques to solving real problems in industry , medicine , and other . disciplines .","label":"Background","metadata":{},"score":"69.09985"}{"text":"Nostrand Reinhold : New York ( ISBN : 0 - 442 - 00461 - 3 ) .Comments from readers of comp.ai.neural-nets : \" Several neural network topics . are discussed e.g. Probalistic Neural Networks , Backpropagation and beyond , . neural control , Radial Basis Function Networks , Neural Engineering .","label":"Background","metadata":{},"score":"69.296005"}{"text":"somewhere between the input layer size ... and the output layer size . ... \" Blum , p. 60 .( John Lazzaro tells me he recently \" reviewed a paper that cited this rule of . thumb -- and referenced this book !","label":"Background","metadata":{},"score":"69.34955"}{"text":"References .5 Nonlinear Optimization Strategies . 5.1 Stochastic Approximation Methods .5.1.1 Linear Parameter Estimation .5.1.2 Backpropagation Training of MLP Networks .5.2 Iterative Methods .5.2.1 Expectation - Maximization Methods for Density Est . .5.2.2 Generalized Inverse Training of MLP Networks .","label":"Background","metadata":{},"score":"69.36034"}{"text":"Morgan Kaufmann .Friedman , J.H. ( 1997 ) .On bias , variance , 0/1-loss , and the curse of dimensionality .Data Mining and Knowledge Discovery , 1 ( 1 ) , 55 - 77 .ftp://playfair.stanford.edu / pub / friedman / curse.ps .","label":"Background","metadata":{},"score":"69.41643"}{"text":"Articles : Spiking Neurons ; Computing with Spiking Neurons ; Pulse - Based .Computation in VLSI Neural Networks ; Encoding Information in Neuronal .Activity ; Building Silicon Nervous Systems with Dendritic Tree Neuromorphs ; .A Pulse - Coded Communications Infrastructure ; Analog VLSI Pulsed Networks for .","label":"Background","metadata":{},"score":"69.60388"}{"text":"Not everyone likes his .C++ code ( the usual complaint is that the code is not sufficiently OO ) but , . unlike the code in some other books , Masters 's code has been successfully . compiled and run by some readers of comp.ai.neural-nets .","label":"Background","metadata":{},"score":"69.69733"}{"text":"Management Program at the National Science Foundation , and is intended to .expand the current UCI Machine Learning Database Repository to datasets .that are orders of magnitude larger and more complex .The neural - bench Benchmark collection .ftp://ftp.boltz.cs.cmu.edu / pub / neural - bench/. In case of problems or if .","label":"Background","metadata":{},"score":"69.7791"}{"text":"Problems .Simulation Experiments .Radial Basis Function Networks .5.1 Ill - Posed Problems and the Regularization Technique .5.2 Stabilizers and Basis Functions .5.3 Generalized Radial Basis Function Networks .5.3.1 Moving Centers Learning .5.3.2 Regularization with Nonradial Basis Functions .","label":"Background","metadata":{},"score":"69.818344"}{"text":"for further cusomization .A complete & lucid explanation of the code .but pretty weak on the principles , theory , and application of neural .networks .Great as a code source , disappointing as a neural network . tutorial .","label":"Background","metadata":{},"score":"69.87372"}{"text":"Universal Approximation Using Feedforward Networks with Non - sigmoid Hidden .Layer Activation Functions ; Approximating and Learning Unknown Mappings .Using Multilayer Feedforward Networks with Bounded Weights ; Universal .Approximation of an Unknown Mapping and Its Derivatives ; Neural Network .","label":"Background","metadata":{},"score":"69.904755"}{"text":"it to random values .It wo n't matter what those values are , as long .as they are not all the same and not equal to 1 .Like most introductory books , this one neglects the difficulties of getting .","label":"Background","metadata":{},"score":"69.95172"}{"text":"The network class has 59 methods . in its public section .Lack of planning is evident for the construction of a class hierarchy .This code is without doubt written by a rushed C programmer .Whilst it would . require a C++ compiler to be successfully used , it lacks the tight .","label":"Background","metadata":{},"score":"70.00168"}{"text":"The images .are mostly 96x128 greyscale frontal images and are stored in ascii files .in a way that makes it easy to convert them to any usual graphic format .( GIF , PCX , PBM etc . ) .","label":"Background","metadata":{},"score":"70.077835"}{"text":"Remark : Statistical mechanics of neural networks .Title : Information Sciences .Publish : North Holland ( Elsevier Science ) .Freq . : Monthly .ISSN : 0020 - 0255 .Editor : Paul P. Wang ; Department of Electrical Engineering ; Duke University ; .","label":"Background","metadata":{},"score":"70.28519"}{"text":"This makes it difficult to retrieve by ftp even a small .part of the database , as you have to get each one individually .A solution , as Barbara proposed me , would be to compress the whole set of . images ( in separate files of , say , 100 images ) and maintain them as a . specific archive for research on face processing , similar to the ones .","label":"Background","metadata":{},"score":"70.39531"}{"text":"The LDC catalog includes .pronunciation lexicons , varied lexicons , broadcast speech , microphone . speech , mobile - radio speech , telephone speech , broadcast text , . conversation text , newswire text , parallel text , and varied text , at .","label":"Background","metadata":{},"score":"70.80905"}{"text":"The process of crossover or mutation forms the new individuals .In crossover process , a combination of the genetic makeup of the two solution candidates results in creation of new individuals .In mutation process , some randomly chosen portions of genetic information are varied to obtain a new individual .","label":"Background","metadata":{},"score":"70.85255"}{"text":"See the part 1 of this posting for full information .what it is all about .Part 1 : Introduction .Part 2 : Learning .Part 3 : Generalization .Part 4 : Books , data , etc . .","label":"Background","metadata":{},"score":"71.34401"}{"text":"of part III through an introductory \" Meta - Map \" and twenty - three road maps , . each of which tours all the Part III articles on the chosen theme .Touretzky , D. , Hinton , G , and Sejnowski , T. , eds . , ( 1989 ) Proceedings of the .","label":"Background","metadata":{},"score":"71.43302"}{"text":"Subject : Journals and magazines about Neural Networks ?[ to be added : comments on speed of reviewing and publishing , .whether they accept TeX format or ASCII by e - mail , etc . ] . A. Dedicated Neural Network Journals : .","label":"Background","metadata":{},"score":"71.514854"}{"text":"If you are new to the field , read it from cover . to cover .If you have lots of experience with NNs , it 's an excellent . reference .If you do n't know calculus , take a class .","label":"Background","metadata":{},"score":"71.55214"}{"text":"Cost / Yr : $ 10 for Members belonging to participating IEEE societies .Freq . : Quarterly ( vol . 1 in March 1990 ) .Remark : Devoted to the science and technology of neural networks . which disclose significant technical knowledge , exploratory . developments and applications of neural networks from biology to . software to hardware .","label":"Background","metadata":{},"score":"71.66043"}{"text":"Title : NEURAL COMPUTING SURVEYS .Publish : Lawrence Erlbaum Associates .Address : 10 Industrial Avenue , Mahwah , NJ 07430 - 2262 , USA .Freq . :Yearly .Cost / Yr : Free on - line .ISSN # : 1093 - 7609 .","label":"Background","metadata":{},"score":"71.693245"}{"text":"give the author(s ) a chance to make a rebuttal or . concurrence .Sometimes , as I 'm sure you can imagine , things .get pretty lively .Their reviewers are called something like .Behavioral and Brain Associates , and I believe they have to . be nominated by current associates , and should be fairly .","label":"Background","metadata":{},"score":"71.915855"}{"text":"References . 7 Methods for Regression .7.1 Taxonomy : Dictionary versus Kernel Representation . 7.2Linear Estimators .7.2.1Estimation of Linear Models and Equivalence of Representations .7.2.2Analytic Form of Cross - validation .7.2.3 Estimating Complexity of Penalized Linear Models .","label":"Background","metadata":{},"score":"71.9913"}{"text":"NY : John Wiley & Sons , ISBN 0 - 471 - 93010 - 5 .( hardbound ) , 526 pages , $ 57.95 .Additional Information : One has to search .Chapter headings : Mathematical Preliminaries of Neurocomputing ; .","label":"Background","metadata":{},"score":"72.11554"}{"text":"4.2.2VC - Dim .for Classification and Regression Problems .4.2.3 Examples of Calculating VC - Dimension .4.3 Bounds on the Generalization .4.3.1 Classification .4.3.2 Regression .4.3.3 Generalization Bounds and Sampling Theorem . 4.4 Structural Risk Minimization .","label":"Background","metadata":{},"score":"72.32669"}{"text":"( For a review of Blum 's source code , see \" Books with Source Code \" above . )Both Blum and Welstead contribute to the dangerous myth that any idiot can .use a neural net by dumping in whatever data are handy and letting it train .","label":"Background","metadata":{},"score":"72.38495"}{"text":"ISO-9660 format CD - ROM and contains a large assortment of software . related to artificial intelligence , artificial life , virtual reality , and .other topics .Programs for OS/2 , MS - DOS , Macintosh , UNIX , and other . operating systems are included .","label":"Background","metadata":{},"score":"72.5105"}{"text":"Keywords : . multiple - instance problem , multiple - instance learning , lazing learning , nearest neighbor .References in Article .Select the SEEK icon to attempt to find the referenced article .If it does not appear to be in cogprints you will be forwarded to the paracite service .","label":"Background","metadata":{},"score":"72.633606"}{"text":"o There is dangerous misinformation on p. 55 , where Swingler says , \" If a .data set contains no noise , then there is no risk of overfitting as there . is nothing to overfit .\" It is true that overfitting is more common with .","label":"Background","metadata":{},"score":"72.87359"}{"text":"Kurkova later estimated the number of units required for uniform .approximation within an error epsilon as nm(m+1 ) in the first hidden . layer and m^2(m+1)^n in the second hidden layer , where n is the number . which f increases distances . \"","label":"Background","metadata":{},"score":"72.89412"}{"text":"Even more dangerous is .the statement on p. 28 that \" Any pair of variables with high covariance are .dependent , and one may be chosen to be discarded . \"Although high .correlations can be used to identify redundant inputs , it is incorrect to .","label":"Background","metadata":{},"score":"72.98593"}{"text":"M3 Competition .3003 time series from the M3 Competition can be found at .The numbers of series of various types are given in the following table : .Interval Micro Industry Macro Finance Demog Other Total .Yearly 146 102 83 58 245 11 645 .","label":"Background","metadata":{},"score":"73.01721"}{"text":"Other contributions are typically published within nine months .The journal presents a fresh undogmatic attitude towards this .multidisciplinary field and aims to be a forum for novel ideas and . improved understanding of collective and cooperative phenomena with . computational capabilities .","label":"Background","metadata":{},"score":"73.119545"}{"text":"The original sources of most data sets . can be accessed via associated links .A compressed tar file containing .all data sets is available .NIST special databases of the National Institute Of Standards .And Technology : .Several large databases , each delivered on a CD - ROM .","label":"Background","metadata":{},"score":"73.28443"}{"text":"Astronomical Time Series .Miscellaneous Images .StatLib .Part 5 : Free software .Part 6 : Commercial software .Part 7 : Hardware and miscellaneous .Subject : Books and articles about Neural Networks ?The following search engines will search many bookstores for new and used . books and return information on availability , price , and shipping charges : .","label":"Background","metadata":{},"score":"73.472496"}{"text":"London : Academic Press .( For a review of the source code , see \" Books with Source Code \" above . )This book has lots of good advice liberally sprinkled with errors , incorrect .formulas , some bad advice , and some very serious mistakes .","label":"Background","metadata":{},"score":"73.61206"}{"text":"Monthly 474 334 312 145 111 52 1428 .Other 4 0 0 29 0 141 174 .Total 828 519 731 308 413 204 3003 .Rob Hyndman 's Time Series Data Library .A collection of over 500 time series on subjects including agriculture , . chemistry , crime , demography , ecology , economics & finance , health , . hydrology & meteorology , industry , physics , production , sales , simulated .","label":"Background","metadata":{},"score":"73.614044"}{"text":"an introduction to some of the things that NNs can do .editions of this book .One comes with disks for the IBM PC , the other comes .with disks for the Macintosh \" .McCord Nelson , M. and Illingworth , W.T. ( 1990 ) .","label":"Background","metadata":{},"score":"73.64207"}{"text":"Training Algorithms for Recurrent Neural Nets that Eliminate the Need for .Computation of Error Gradients with Application to Trajectory Production .Problem ; .Training Recurrent Neural Networks for Filtering and Control ; .Remembering How to Behave : Recurrent Neural Networks for Adaptive Robot .","label":"Background","metadata":{},"score":"73.67683"}{"text":"Submitted articles reviewed by two technical referees paper 's . interdisciplinary format and accessability . \"Also Viewpoints and .Reviews commissioned by the editors , abstracts ( with reviews ) of . articles published in other journals , and book reviews .","label":"Background","metadata":{},"score":"73.784515"}{"text":"and \" Notable \" sections will do a search using AddAll .There are many on - line bookstores , such as : .The neural networks reading group at the University of Illinois at .Urbana - Champaign , the Artifical Neural Networks and Computational Brain .","label":"Background","metadata":{},"score":"74.06091"}{"text":"In retain process important experience is retained for reusing it in future and case base is enhanced by a newly learned case or by updating of some existing cases . D. Artificial Neural Network ( ANN ) ANN is based on Biological nervous system .","label":"Background","metadata":{},"score":"74.43848"}{"text":"Upper Saddle River , NJ : Prentice Hall , ISBN 0 - 13 - 273350 - 1 .The second edition is much better than the first , which has been described . as a core - dump of Haykin 's brain .","label":"Background","metadata":{},"score":"74.67044"}{"text":"Title : Neural Network News .Publish : AIWeek Inc. .Address : Neural Network News , 2555 Cumberland Parkway , Suite 299 , .Atlanta , GA 30339 USA .Tel : ( 404 ) 434 - 2187 .Freq . :","label":"Background","metadata":{},"score":"74.96623"}{"text":"Results on these .data can be projected to overall ZIP Code recogni- . tion performance .+ image format documentation and software included .System requirements are a 5.25 \" CD - ROM drive with software to read .ISO-9660 format .","label":"Background","metadata":{},"score":"75.064804"}{"text":"than twice the number of input units . \"Furthermore , constructing a counter . example to Swingler 's advice is trivial : use one input and one output , where .the output is the sine of the input , and the domain of the input extends .","label":"Background","metadata":{},"score":"75.13545"}{"text":"learn nothing , while beginners will be unable to separate the useful .information from the dangerous .For example , there is a chapter on \" Data . encoding and re - coding \" that would be very useful to beginners if it were . accurate , but the formula for the standard deviation is wrong , and the .","label":"Background","metadata":{},"score":"75.31895"}{"text":"SVMs perform wonderful in bioinformatics in DNA sequencing and modeling protein structure .SVM learning is used for data analysis and patterns identification that are useful for classification and regression analysis .SVMs make use of a ( nonlinear ) mapping function ( Φ ) that transforms input data space to data in feature space in such a way as to make a problem linearly separable .","label":"Background","metadata":{},"score":"75.475975"}{"text":"Yet this chapter is so .egregiously misleading that the book has earned a place on \" The Worst \" list .A detailed criticism of this chapter , along with some other sections of the .Other . chapters of the book are reviewed in the November , 1997 , issue of Scientific .","label":"Background","metadata":{},"score":"75.51447"}{"text":"Have the authors never heard that \" a picture is worth a thousand words \" ?What few diagrams they have ( such as the one on p. 74 ) tend to be confusing .Their jargon is peculiar even by NN standards ; for example , they refer to . target values as \" mentor inputs \" ( p. 66 ) .","label":"Background","metadata":{},"score":"75.556"}{"text":"Appendix : Estimating Functions on the Basis of Indirect Measurements ; .Stochastic Ill - Posed Problems ; .Estimating the Values of Functions at Given Points ; .Perceptrons and Their Generalizations ; .The Support Vector Method for Estimating Indicator Functions ; .","label":"Background","metadata":{},"score":"75.74903"}{"text":"Subject : Mailing lists , BBS , CD - ROM ?See also \" Other NN links ? \" in Part 7 of the FAQ .Machine Learning mailing list .The Machine Learning mailing list is an unmoderated mailing list intended .","label":"Background","metadata":{},"score":"76.02896"}{"text":"ISSN # : 1370 - 4621 .Remark : The aim of the journal is to rapidly publish new ideas , original . developments and work in progress .Neural Processing Letters . covers all aspects of the Artificial Neural Networks field .","label":"Background","metadata":{},"score":"76.09087"}{"text":"73 . and 170 that RBF networks have advantages over backprop networks for .nonstationary inputs -- perhaps he is using the word \" nonstationary \" in a . sense different from the statistical meaning of the term .There are other . things in the book that I would quibble with , but I did not find any of the . flagrant errors that are common in other books on NN applications such as .","label":"Background","metadata":{},"score":"76.20462"}{"text":"There are . only a small number of minor criticisms that can be made about this . one .More space should have been given to backprop and its variants .because of the practical importance of such methods .And while the .","label":"Background","metadata":{},"score":"76.41652"}{"text":"UCI KDD Archive . encompasses a wide variety of data types , analysis tasks , and application .areas .The primary role of this repository is to serve as a benchmark . testbed to enable researchers in knowledge discovery and data mining to . scale existing and future data analysis algorithms to very large and . complex data sets .","label":"Background","metadata":{},"score":"76.46071"}{"text":"The book is exceptionally well organized , .presenting topics in a logical progression ideal for conceptual . understanding .Geoffrey Hinton writes in the foreword : . \" Bishop is a leading researcher who has a deep understanding of the material .","label":"Background","metadata":{},"score":"76.58669"}{"text":"Title : The Behavioral and Brain Sciences .Publish : Cambridge University Press .Remark : ( Remarks by Don Wunsch .This is a delightful journal that encourages discussion on a . variety of controversial topics .I have especially enjoyed .","label":"Background","metadata":{},"score":"76.634155"}{"text":"Once a . paper is accepted for publication , authors are invited to e - mail .the LaTeX source file of their paper in order to expedite publication .Title : International Journal of Neurocomputing .Publish : Elsevier Science Publishers , Journal Dept . ; PO Box 211 ; .","label":"Background","metadata":{},"score":"76.64463"}{"text":"The main thing is that I liked .the articles I read .Title : International Journal of Applied Intelligence .Publish : Kluwer Academic Publishers .Remark : first issue in 1990 ( ? )Title : International Journal of Modern Physics C .","label":"Background","metadata":{},"score":"76.65082"}{"text":"Yogesh Singh , Pradeep Kumar Bhatia , Omprakash Sangwan,\"A Review of studies on Machine Learning Techniques \" , International Journal of Computer Science and Security 1:1 , 2007 .L. Breiman , Random Forests , Kluwer Academic Publishers , Vol .Sandor Dornbush , Jesse English , Tim Oates , Zary Segall , Anupam Joshi , \" XPod : A Human Activity Aware Learning Mobile Music Player \" , Proceedings of conference on Advances in Ambient Intelligence , pp .","label":"Background","metadata":{},"score":"76.661545"}{"text":"Title : International Journal of Neural Networks .Publish : Learned Information .Freq . : Quarterly ( vol . 1 in 1989 ) .Cost / Yr : 90 pounds .ISSN # : 0954 - 9889 .Remark : The journal contains articles , a conference report ( at least the .","label":"Background","metadata":{},"score":"76.82678"}{"text":"Autoassociation ; Generalization ; Translation invariance ; Simple recurrent .networks ; Critical points in learning ; Modeling stages in cognitive .development ; Learning the English past tense ; The importance of starting .small .Feedforward networks .Fine , T.L. ( 1999 ) Feedforward Neural Network Methodology , NY : Springer , .","label":"Background","metadata":{},"score":"76.82842"}{"text":"It is intended to .what 's known on an unfamiliar topic .Title : IEEE Transactions on Neural Networks .Publish : Institute of Electrical and Electronics Engineers ( IEEE ) .Address : IEEE Service Cemter , 445 Hoes Lane , P.O. Box 1331 , Piscataway , NJ , . 08855 - 1331 USA .","label":"Background","metadata":{},"score":"76.83902"}{"text":"In 1995 will go to floppy disc - based .publishing with databases + , \" the equivalent to 50 pages per issue are .planned . \"Often focuses on specific topics : e.g. , August , 1994 contains two . articles : \" Economics , Times Series and the Market , \" and \" Finite Particle .","label":"Background","metadata":{},"score":"76.94453"}{"text":"This technique is useful in solving either regression type problems or classification type problems .CART analysis is basically designs a tree , which is different from other conventional data analysis methods .It is a process of designing a tree and finally selecting an optimal tree that fits the required information . F. Support Vector Machine ( SVM )","label":"Background","metadata":{},"score":"76.985306"}{"text":"( 300 ppi 1-bit ) .This database is intended to encourage research in . off - line handwriting recognition by providing access to handwriting . samples digitized from envelopes in a working post office .Specifications of the database include : .","label":"Background","metadata":{},"score":"77.02533"}{"text":"CONNECTIONISTS is a moderated mailing list for discussion of technical . issues relating to neural computation , and dissemination of professional .announcements such as calls for papers , book announcements , and .electronic preprints .CONNECTIONISTS is focused on meeting the needs of .","label":"Background","metadata":{},"score":"77.157715"}{"text":"Grossberg ( separate papers , not collaborations ) a few years . back .They have a really neat concept : they get a paper , . then invite a number of noted scientists in the field to .praise it or trash it .","label":"Background","metadata":{},"score":"77.28118"}{"text":"The structured forms used in this database are 12 . different forms from the 1988 , IRS 1040 Package X. These include Forms .1040 , 2106 , 2441 , 4562 , and 6251 together with Schedules A , B , C , D , E , F . and SE .","label":"Background","metadata":{},"score":"77.35669"}{"text":"A user 's intolerance of any hindrance that machine learning techniques may introduce in the normal mobile phone operation .d ) Therefore , a study of different machine learning techniques on different parameters is of much importance in order to pave a way for future works in this direction .","label":"Background","metadata":{},"score":"77.44377"}{"text":"2.3.3 Inductive Principles . 2.4 Summary .References .3 Regularization Framework .3.1 Curse and Complexity of Dimensionality .3.2 Function Approx . and Characterization of Complexity . 3.3 Penalization .3.3.1 Parametric Penalties .3.3.2 Nonparametric Penalties .3.4 Model Selection ( Complexity Control ) .","label":"Background","metadata":{},"score":"77.4827"}{"text":"Artificial Intelligence and Cognitive Research .Publish : Carfax Publishing .Address : Europe : Carfax Publishing Company , PO Box 25 , Abingdon , Oxfordshire .OX14 3UE , UK .USA : Carfax Publishing Company , PO Box 2025 , Dunnellon , Florida .","label":"Background","metadata":{},"score":"77.499985"}{"text":"comes out soon !For more information , see The best intermediate textbooks on .NNs below .If you have questions on feedforward nets that are n't answered by Bishop , . try Masters ( 1993 ) or Reed and Marks ( 1999 ) for practical issues or Ripley .","label":"Background","metadata":{},"score":"77.57143"}{"text":"Cost / Yr : USA and Canada $ 249 , Elsewhere $ 299 .Remark : Commercial Newsletter .Title : Network : Computation in Neural Systems .Publish : IOP Publishing Ltd .Address : Europe : IOP Publishing Ltd , Techno House , Redcliffe Way , Bristol .","label":"Background","metadata":{},"score":"78.00954"}{"text":"when there are more training cases than weights .There is an example of . such overfitting under How many hidden layers should I use ?o Regarding the use of added noise ( jitter ) in training , Swingler says on .","label":"Background","metadata":{},"score":"78.212234"}{"text":"Astronomical Time Series .Prepared by Paul L. Hertz ( Naval Research Laboratory ) & Eric D. Feigelson .( Pennsyvania State University ) : .o Detection of variability in photon counting observations 1 .( QSO1525 + 337 ) .","label":"Background","metadata":{},"score":"79.06966"}{"text":"( Note : Remarks are from journal CFP ) .Title : International Journal of Neural Systems .Publish : World Scientific Publishing .Address : USA : World Scientific Publishing Co. , 1060 Main Street , River Edge , .NJ 07666 .","label":"Background","metadata":{},"score":"79.14028"}{"text":"automatically generated by a computer in order to make the data available .without the danger of distributing privileged tax information .In . addition to the images the database includes 5,590 answer files , one for .each image .Each answer file contains an ASCII representation of the data .","label":"Background","metadata":{},"score":"79.38044"}{"text":"The Linguistic Data Consortium ( URL : . universities , companies and government research laboratories .It creates , . collects and distributes speech and text databases , lexicons , and other .resources for research and development purposes .The University of .","label":"Background","metadata":{},"score":"79.42365"}{"text":"From The Publisher : The heart of the book , part III , comprises of 267 .original articles by leaders in the various fields , arranged alphabetically . by title .Parts I and II , written by the editor , are designed to help .","label":"Background","metadata":{},"score":"79.53865"}{"text":"Murphy , P.M. UCI repository of machine learning databases - a machine - readable data repository .Maintained at the Department of Information and Computer Science , University of California , Irvine .Anonymous FTP from ics.uci.edu in the directory pub / machine - learning - databases , 1995 .","label":"Background","metadata":{},"score":"79.58272"}{"text":"Each image file takes approximately 25K.According to the archive administrator , Barbara L. Dijker .However , the image files are stored in separate directories corresponding . to the Internet site to which the person represented in the image . belongs , with each directory containing a small number of images ( two in .","label":"Background","metadata":{},"score":"79.639534"}{"text":"collegues who want to know NN basics , but who never plan to implement . anything .An excellent book to give your manager . \" Not - quite - so - introductory Literature .Kung , S.Y. ( 1993 ) .","label":"Background","metadata":{},"score":"79.710205"}{"text":"k - NN considers more than one neighbor at a time while classifying hence it is known as K - Nearest Neighbor .This nearest neighbor is determined in terms of Euclidean distance which measures the dissimilarities between examples represented as vector inputs and some related measures .","label":"Background","metadata":{},"score":"79.94379"}{"text":"p. 8 .Blum at least mentions some important issues , however simplistic his advice . may be .Welstead just ignores them .What Welstead gives you is code -- vast .amounts of code .I have no idea how anyone could write that much code for a . simple feedforward NN .","label":"Background","metadata":{},"score":"80.126656"}{"text":"Simulation of Spiking Neural Networks ; Populations of Spiking Neurons ; .Collective Excitation Phenomena and Their Applications ; Computing and .Learning with Dynamic Synapses ; Stochastic Bit - Stream Neural Networks ; .Hebbian Learning of Pulse Timing in the Barn Owl Auditory System .","label":"Background","metadata":{},"score":"80.49385"}{"text":"What a relief !As a broad introductory text this is without any doubt .the best currently available in its area .It does n't include source .code of any kind ( normally this is badly written and compiler . specific ) .","label":"Background","metadata":{},"score":"81.00936"}{"text":"+ extracted from live mail in a working U.S. Post .Office .+ word images in the test set supplied with dic- .tionaries of postal words that simulate partial .recognition of the corresponding ZIP Code .+ digit images included in test set that simulate .","label":"Background","metadata":{},"score":"81.75243"}{"text":"Title : Neural Computation .Publish : MIT Press .Address : MIT Press Journals , 55 Hayward Street Cambridge , .MA 02142 - 9949 , USA , Phone : ( 617 ) 253 - 2889 .Freq . : Quarterly ( vol . 1 in 1989 ) .","label":"Background","metadata":{},"score":"82.21041"}{"text":"Database 9 .o NIST Binary Image Databases of Census Miniforms ( MFDB ) .o NIST Mated Fingerprint Card Pairs 2 ( MFCP 2 ) .o NIST Scoring Package Release 1.0 .o NIST FORM - BASED HANDPRINT RECOGNITION SYSTEM .","label":"Background","metadata":{},"score":"82.52968"}{"text":"Samples of the data can be found by ftp on sequoyah.ncsl.nist.gov in . directory /pub / data A more complete description of the available .databases can be obtained from the same host as ./pub / databases / catalog . txt .","label":"Background","metadata":{},"score":"82.53072"}{"text":"Address : Kluwer Academic Publishers .P.O. Box 358 .Accord Station .Hingham , MA 02018 - 0358 USA .Freq . :Monthly ( 8 issues per year ; increasing to 12 in 1993 ) .Cost / Yr : Individual $ 140 ( 1992 ) ; Member of AAAI or CSCSI $ 88 .","label":"Background","metadata":{},"score":"82.552704"}{"text":"ZIP Codes , Digits , and Alphabetic Characters .The Center Of Excellence for Document Analysis and Recognition ( CEDAR ) .State University of New York at Buffalo announces the availability of .CEDAR CDROM 1 : USPS Office of Advanced Technology The database contains . handwritten words and ZIP Codes in high resolution grayscale ( 300 ppi .","label":"Background","metadata":{},"score":"82.71272"}{"text":"Subject : comp.ai.neural-nets FAQ , Part 4 of 7 : Books , data , etc . .Followup - To : comp.ai.neural-nets .Date : 30 Dec 2002 21:40:01 GMT .Organization : SAS Institute Inc. , Cary , NC , USA .","label":"Background","metadata":{},"score":"82.78288"}{"text":"How to benchmark learning methods ?Databases for experimentation with NNs ?UCI machine learning database .UCI KDD Archive .The neural - bench Benchmark collection .Proben1 .Delve : Data for Evaluating Learning in Valid Experiments .Bilkent University Function Approximation Repository .","label":"Background","metadata":{},"score":"82.78842"}{"text":"Each image is stored . in bi - level black and white raster format .The images in this database .appear to be real forms prepared by individuals but the images have been .automatically derived and synthesized using a computer and contain no . \" real \" tax data .","label":"Background","metadata":{},"score":"82.95992"}{"text":"Address : WINNERS , c / o Judith Dayhoff , 11141 Georgia Ave . , Suite 206 , .Wheaton , MD 20902 .Phone : 301 - 933 - 9000 .European Neural Network Society ( ENNS ) .ENNS membership includes subscription to \" Neural Networks \" , the official . journal of the society .","label":"Background","metadata":{},"score":"82.96951"}{"text":"money order , bank draft , or credit card from : .Network Cybernetics Corporation ; . 4201Wingren Road Suite 202 ; .Irving , TX 75062 - 2763 ; .Tel 214/650 - 2002 ; .Fax 214/650 - 1929 ; .","label":"Background","metadata":{},"score":"83.31543"}{"text":"list .o NIST Binary Images of Printed Digits , Alphas , and Text .o NIST Structured Forms Reference Set of Binary Images .o NIST Binary Images of Handwritten Segmented Characters .o NIST 8-bit Gray Scale Images of Fingerprint Image Groups .","label":"Background","metadata":{},"score":"83.55704"}{"text":"Swingler addresses many important practical issues , and often provides good . practical advice .But the peculiar combination of much good advice with some .extremely bad advice , a few examples of which are provided above , could .easily seduce a beginner into thinking that the book as a whole is reliable .","label":"Background","metadata":{},"score":"83.6613"}{"text":"Central Neural System Electronic Bulletin Board .Supported by : Wesley R. Elsberry .3027 Macaulay Street .San Diego , CA 92106 .Email : welsberr@inia.cls.org .Many MS - DOS PD and shareware simulations , source code , benchmarks , . demonstration packages , information files ; some Unix , Macintosh , Amiga . related files .","label":"Background","metadata":{},"score":"83.88138"}{"text":"( Note : remarks by Osamu Saito \" saito@nttica .NTT.JP \") .Title : Neural Networks Today .Remark : I found this title in a bulletin board of october last year .It was a message of Tim Pattison , timpatt@augean .","label":"Background","metadata":{},"score":"84.48918"}{"text":"Nate Derbinsky , Georg Essl , \" Cognitive Architecture in Mobile Music Interactions \" , NIME'11 , Oslo , Norway May - June 2011 .Chih - Ming Chen , Shih - Hsun Hsu , \" Personalized Intelligent Mobile Learning System for Supporting Effective English Learning \" , Educational Technology and Society , 11:3 , pp .","label":"Background","metadata":{},"score":"84.520996"}{"text":"It is a matter of great .fortune that such software is unlikely to be reusable and will therefore , .like all good dinosaurs , disappear with the passage of time .The irony is that there is a card in the back of the book asking the .","label":"Background","metadata":{},"score":"84.619156"}{"text":"o Periodicity in a gamma ray burster ( GRB790305 ) .o Solar cycles in sunspot numbers ( Sun ) .o Deconvolution of sources in a scanning operation ( HEAO A-1 ) .o Fractal time variability in a seyfert galaxy ( NGC5506 ) .","label":"Background","metadata":{},"score":"84.7867"}{"text":"This statement makes no sense as it stands ( it would make more sense if .\" general \" were changed to \" smooth \" ) , but it could certainly encourage a . beginner to use far too much jitter -- see What is jitter ?","label":"Background","metadata":{},"score":"84.845825"}{"text":"The whole compressed .database would take some 30 megabytes of disk space .I encourage anyone . willing to host this database in his / her site , available for anonymous . ftp , to contact her for details ( unfortunately I do n't have the resources . to set up such a site ) .","label":"Background","metadata":{},"score":"85.03815"}{"text":"Mr Blum has not only contributed a masterpiece of NN inaccuracy but also .seems to lack a fundamental understanding of Object Orientation .The excessive use of virtual methods ( see page 32 for example ) , the . inclusion of unnecessary ' friend ' relationships ( page 133 ) and a penchant .","label":"Background","metadata":{},"score":"85.77239"}{"text":"1022 Hougang Avenue 1 # 05 - 3520 , Singapore 1953 , Rep. of Singapore .Tel : 382 5663 .Freq : bi - monthly .Eds : H. Herrmann , R. Brower , G.C. Fox and S Nose .Title : Machine Learning .","label":"Background","metadata":{},"score":"85.83876"}{"text":"In a generous sense the code is free and the author does n't claim any .expertise in software engineering .It works in a limited sense but would be .difficult to extend and/or reuse .It 's fine for demonstration purposes in a . stand - alone manner and for use with the book concerned .","label":"Background","metadata":{},"score":"85.876526"}{"text":"conditioning : data and theories ; Cognitive mapping ; Attentional processes ; .Storage and retrieval processes ; Configural processes ; Timing ; Operant . conditioning and animal communication : data , theories , and networks ; Animal .cognition : data and theories ; Place learning and spatial navigation ; Maze . learning and cognitive mapping ; Learning , cognition , and the hippocampus : . data and theories ; Hippocampal modulation of learning and cognition ; The . character of the psychological law .","label":"Background","metadata":{},"score":"86.12179"}{"text":"An intelligent system performing this detection task can be used to assist airport security personnel with luggage screening .INTRODUCTION This paper concerns the development of a methodology for shape learning and recognition , and its application to learning symbolic descriptions of blasting caps in x - ray images under varying perceptual conditions .","label":"Background","metadata":{},"score":"86.19186"}{"text":"The stereotypical business executive ( SBE ) does not want to know how or why .NNs work -- he ( SBEs are usually male ) just wants to make money .The SBE may . know what an average or percentage is , but he is deathly afraid of .","label":"Background","metadata":{},"score":"86.252365"}{"text":"Image format . documentation and example software are also provided .The uncompressed .database totals approximately 5.9 gigabytes of data .NIST special database 3 : Binary Images of Handwritten Segmented .Characters ( HWSC ) .Contains 313,389 isolated character images segmented from the 2,100 . full - page images distributed with \" NIST Special Database 1 \" . digits , 44,951 upper - case , and 45,313 lower - case character images .","label":"Background","metadata":{},"score":"86.88253"}{"text":"Title : Biological Cybernetics ( Kybernetik ) .Publish : Springer Verlag .Remark : Monthly ( vol . 1 in 1961 ) .Title : Various IEEE Transactions and Magazines .Publish : IEEE .Remark : Primarily see IEEE Trans . on System , Man and Cybernetics ; .","label":"Background","metadata":{},"score":"86.94812"}{"text":"The use of public data fields within classes ( loss of encapsulation ) .Classes with no protected or private sections .Little or no use of inheritance and/or run - time polymorphism .Use of floats not doubles ( a common mistake ) to store values for . connection weights .","label":"Background","metadata":{},"score":"87.22452"}{"text":"Individual $ 65 , Institution $ 175 .ISSN # : 0893 - 6080 .Remark : Official Journal of International Neural Network Society ( INNS ) , .European Neural Network Society ( ENNS ) and Japanese Neural .Network Society ( JNNS ) .","label":"Background","metadata":{},"score":"87.54923"}{"text":"The Delve web page is .Bilkent University Function Approximation Repository .A repository of data sets collected mainly by searching resources on the .data sets are used for the experimental analysis of function .approximation techniques and for training and demonstration by machine .","label":"Background","metadata":{},"score":"87.66709"}{"text":"o NIST Test Data 1 : Binary Images of Hand - Printed Segmented Characters .o NIST Machine - Print Database of Gray Scale and Binary Images .o NIST 8-Bit Gray Scale Images of Mated Fingerprint Card Pairs .","label":"Background","metadata":{},"score":"87.97402"}{"text":"FaceSaver archive and may discontinue that service if it becomes a . burden .This means that people should not download more than maybe 10 .faces at a time from uunet .A last remark : each file represents a different person ( except for .","label":"Background","metadata":{},"score":"88.66435"}{"text":"Next part is part 5 ( of 7 ) .Previous part is part 3 .Warren S. Sarle SAS Institute Inc.The opinions expressed here .( 919 ) 677 - 8000 Cary , NC 27513 , USA those of SAS Institute .","label":"Background","metadata":{},"score":"88.874985"}{"text":"NJ 07666 .Tel : ( 201 ) 487 9655 ; Europe : World Scientific Publishing . Co. Ltd. , 57 Shelton Street , London WC2H 9HE , England .Tel : ( 0171 ) 836 0888 ; Asia : World Scientific Publishing Co.","label":"Background","metadata":{},"score":"89.0169"}{"text":"o Detection of variability in photon counting observations 3 ( SN1987A ) .o Detecting orbital and pulsational periodicities in stars 1 ( binaries ) .o Detecting orbital and pulsational periodicities in stars 2 ( variables ) .o Cross - correlation of two time series 1 ( Sun ) .","label":"Background","metadata":{},"score":"89.039116"}{"text":"The prices of the databases are between US$ 250 and 1895 If you wish to .order a database , please contact : Standard Reference Data ; National .Institute of Standards and Technology ; 221/A323 ; Gaithersburg , MD 20899 ; .","label":"Background","metadata":{},"score":"89.33634"}{"text":"Tel : ( 0171 ) 836 0888 ; Asia : World Scientific Publishing Co.Pte . Ltd. , .1022 Hougang Avenue 1 # 05 - 3520 , Singapore 1953 , Rep. of Singapore .Tel : 382 5663 .Freq . : Quarterly ( Vol . 1 in 1990 ) .","label":"Background","metadata":{},"score":"89.33872"}{"text":"The one serious drawback of this book is that it is more than one page long .and may therefore tax the attention span of the SBE .But any SBE who .succeeds in reading the entire book should learn enough to be able to hire a . good NN expert to do the real work .","label":"Background","metadata":{},"score":"89.43907"}{"text":"o 5632 city words .o 4938 state words .o 9454 ZIP Codes .+ 300 ppi binary handwritten characters and digits : .o 27,837 mixed alphas and numerics segmented .from address blocks .o 21,179 digits segmented from ZIP Codes .","label":"Background","metadata":{},"score":"89.816376"}{"text":"He understands profit and loss but does not want to waste his .time learning things involving complicated math , such as high - school . algebra .For further information on the SBE , see the \" Dilbert \" comic strip .","label":"Background","metadata":{},"score":"90.05646"}{"text":"Acknowledgments The authors would like to thank the many people whose useful discussions contributed to this work : Jerzy Bala , Eric Bloedorn , Ibrahim Imam , and Ali Hadjarian .The authors wish to also thank those who read preliminary drafts of this p ..","label":"Background","metadata":{},"score":"90.3544"}{"text":"MACHINE LEARNING TECHNIQUES FOR MOBILE DEVICES : A STUDY Machine learning techniques are useful for mobile devices .XPod Mobile MP3 player was developed by Sandor et al .[ 11].XPod automated song selection process as per user interest or liking .","label":"Background","metadata":{},"score":"90.60532"}{"text":"Data sets include : .o Fluctuations in a far - infrared laser .o Physiological data of patients with sleep apnea ; .o High frequency currency exchange rate data ; .o Intensity of a white dwarf star ; .","label":"Background","metadata":{},"score":"91.045204"}{"text":"because one of the inputs has a high standard deviation .The most ludicrous thing I 've found in the book is the claim that .Hecht - Neilsen used Kolmogorov 's theorem to show that \" you will never require .","label":"Background","metadata":{},"score":"91.57822"}{"text":"And it 's below $ 30 for the paperback .\" ; \" Well . written , theoretical ( but not overwhelming ) \" ; It provides a good balance of .model development , computational algorithms , and applications .The . mathematical derivations are especially well done \" ; \" Nice mathematical . analysis on the mechanism of different learning algorithms \" ; \" It is NOT for .","label":"Background","metadata":{},"score":"92.176834"}{"text":"USENIX Faces .The USENIX faces archive is a public database , accessible by ftp , that . can be of use to people working in the fields of human face recognition , . classification and the like .It currently contains 5592 different faces .","label":"Background","metadata":{},"score":"92.24785"}{"text":"Freq . : Quarterly ( vol . 1 in 1989 ) .Title : Neural Processing Letters .Publish : Kluwer Academic publishers .Address : P.O. Box 322 , 3300AH Dordrecht , The Netherlands .Freq : 6 issues / year ( vol . 1 in 1994 ) .","label":"Background","metadata":{},"score":"92.94769"}{"text":"( Note : remark provided by J.R.M. Smits \" anjos@sci.kun.nl \") .Title : Sixth Generation Systems ( formerly Neurocomputers ) .Publish : Gallifrey Publishing .Address : Gallifrey Publishing , PO Box 155 , Vicksburg , Michigan , 49097 , USA .","label":"Background","metadata":{},"score":"94.81975"}{"text":"NIST special database 2 : Structured Forms Reference Set ( SFRS ) .The NIST database of structured forms contains 5,590 full page images of . simulated tax forms completed using machine print .THERE IS NO REAL TAX .DATA IN","label":"Background","metadata":{},"score":"94.852875"}{"text":"Linguistic Data Consortium .University of Pennsylvania .3615 Market Street , Suite 200 .Philadelphia , PA 19104 - 2608 .Tel ( 215 ) 898 - 0464 Fax ( 215 ) 573 - 2175 .Email : ldc@ldc.upenn.edu .Otago Speech Corpus .","label":"Background","metadata":{},"score":"95.88413"}{"text":"The uncompressed database totals approximately 2.75 gigabytes of . image data and includes image format documentation and example software .The system requirements for all databases are a 5.25 \" CD - ROM drive with . software to read ISO-9660 format .","label":"Background","metadata":{},"score":"96.25257"}{"text":"The application considered is detecting blasting caps in x - ray images of luggage .An intelligent system performing this detection task can be used to assist airport security personnel with luggage screening . . ..To make this paper self - contained , we begin by characterizing very briefly the description language .","label":"Background","metadata":{},"score":"96.47667"}{"text":"clearly stated that it is supplied as an extra -- almost as an afterthought .This may be a wise move .Although not as bad as other ( even commercial ) implementations , the code .provided lacks proper OO structure and is typical of C++ written in a C . style .","label":"Background","metadata":{},"score":"96.84872"}{"text":"ISSN # : 0899 - 7667 .Remark : Combination of Reviews ( 10,000 words ) , Views ( 4,000 words ) .and Letters ( 2,000 words ) .I have found this journal to be of . outstanding quality .","label":"Background","metadata":{},"score":"96.89462"}{"text":"There are many excellent books about NNs by Timothy Masters ( listed .elsewhere in the FAQ ) that provide C++ code for NNs .If you simply want code .that works , these books should satisfy your needs .If you want code that . exemplifies the highest standards of object oriented design , you will be . disappointed by Masters .","label":"Background","metadata":{},"score":"97.05584"}{"text":"Services 500 Sunnyside Blvd . , Woodbury , NY 11797 - 2999 .Freq . : Quarterly ( 1st issue 1990 ) .Cost / Yr : USA : $ 180 , Europe : 110 pounds .Remark : Description : \" a forum for integrating theoretical and experimental . findings across relevant interdisciplinary boundaries .","label":"Background","metadata":{},"score":"97.42769"}{"text":"Freq .Monthly plus four special reports each year ( 1st issue : May , 1984 ) .ISSN # : 1042 - 4296 .Editor : Edward Rosenfeld .Cost / Yr : $ 395 ( USA ) , US$ 450 ( elsewhere ) .","label":"Background","metadata":{},"score":"97.45055"}{"text":"Approved : news - answers - request@MIT.EDU .Expires : 3 Feb 2003 21:40:00 GMT .NNTP -Posting - Date : 30 Dec 2002 21:40:01 GMT .Keywords : frequently asked questions , answers .Xref : cs.uu.nl comp.ai.neural-nets:66901 comp.answers:52478 news.answers:241488 View main headers .","label":"Background","metadata":{},"score":"97.762985"}{"text":"International Neural Network Society ( INNS ) .INNS membership includes subscription to \" Neural Networks \" , the official . journal of the society .Membership is $ 55 for non - students and $ 45 for .students per year .","label":"Background","metadata":{},"score":"97.90828"}{"text":"Publish : Pergamon Press .Address : Pergamon Journals Inc. , Fairview Park , Elmsford , .New York 10523 , USA and Pergamon Journals Ltd. .Headington Hill Hall , Oxford OX3 , 0BW , England .Freq .: 10 issues / year ( vol . 1 in 1988 ) .","label":"Background","metadata":{},"score":"97.95767"}{"text":"Washington , MD 20749 .International Student Society for Neural Networks .( ISSNNets ) .Membership is $ 5 per year .Address : ISSNNet , Inc. , P.O. Box 15661 , .Boston , MA 02215 USA .Women In Neural Network Research and technology .","label":"Background","metadata":{},"score":"98.251854"}{"text":"Freq . : ? ( 1st issue Jan 1989 ) .Remark : For submission information , please contact either of the editors : .Eric Dietrich Chris Fields .PACSS - Department of Philosophy Box 30001/3CRL .SUNY Binghamton New Mexico State University .","label":"Background","metadata":{},"score":"98.2666"}{"text":"Australia : Carfax Publishing Company , Locked Bag 25 , Deakin , .ACT 2600 , Australia .Freq . : Quarterly ( vol . 1 in 1989 ) .Cost / Yr : Personal rate : . 48 pounds ( EC ) 66 pounds ( outside EC ) US$ 118 ( USA and Canada ) .","label":"Background","metadata":{},"score":"98.77606"}{"text":"Bad science writing .Dewdney , A.K. ( 1997 ) , Yes , We Have No Neutrons : An Eye - Opening Tour . through the Twists and Turns of Bad Science , NY : Wiley .This book , allegedly an expose of bad science , contains only one chapter of .","label":"Background","metadata":{},"score":"99.95423"}{"text":"Publish : Institute of Electrical and Electronics Engineers ( IEEE ) .Address : IEEE Service Cemter , 445 Hoes Lane , P.O. Box 1331 , Piscataway , NJ , . 08855 - 1331 USA .Tel : ( 201 ) 981 - 0060 .","label":"Background","metadata":{},"score":"100.42618"}{"text":"Technologies .Title : JNNS Newsletter ( Newsletter of the Japan Neural Network Society ) .Publish : The Japan Neural Network Society .Freq . : Quarterly ( vol . 1 in 1989 ) .Remark : ( IN JAPANESE LANGUAGE ) Official Newsletter of the Japan Neural .","label":"Background","metadata":{},"score":"100.938354"}{"text":"Freq . : Quarterly .Publish : Springer Verlag .Cost / yr : 120 Pounds .Remark : Is the journal of the Neural Computing Applications Forum .Publishes original research and other information . in the field of practical applications of neural computing .","label":"Background","metadata":{},"score":"102.34685"}{"text":"UK pounds for students ) per year .Address : ENNS Membership , Centre for .Neural Networks , King 's College London , Strand , London WC2R 2LS , United .Kingdom .Japanese Neural Network Society ( JNNS ) .","label":"Background","metadata":{},"score":"102.94714"}{"text":"Freq .Monthly ( 1st issue January , 1987 ) .ISSN # : 0893 - 1585 .Editor : Derek F. Stubbs .Cost / Yr : $ 79 ( USA , Canada ) , US$ 95 ( elsewhere ) .","label":"Background","metadata":{},"score":"103.65872"}{"text":"FF per year ; Activities : newsletter , conference ( every year ) , list of .Neurosciences et Sciences de l'Ingenieur ( NSI ) .Biology & Computer Science Activity : conference ( every year )Address : .NSI - TIRF / INPG 46 avenue Felix Viallet 38031 Grenoble Cedex FRANCE .","label":"Background","metadata":{},"score":"104.153496"}{"text":"SNN ( Foundation for Neural Networks ) .The Foundation for Neural Networks ( SNN ) is a university based non - profit . organization that stimulates basic and applied research on neural .networks in the Netherlands .Every year SNN orgines a symposium on Neural .","label":"Background","metadata":{},"score":"107.058495"}{"text":"by Eric E. Bloedorn , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ Kenneth De Jong , _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _","label":"Background","metadata":{},"score":"107.071304"}{"text":"Additional Information : The Webpage includes errata and additional . information , which has n't been available at publishing time , for this book .Brian Ripley 's book is an excellent sequel to Bishop ( 1995 ) .Ripley starts .","label":"Background","metadata":{},"score":"112.04167"}{"text":"Tamagawa University ; 6 - 1 - 1 , Tamagawa Gakuen , Machida City , Tokyo ; 194 .JAPAN ; Phone : +81 427 28 3457 , Fax : +81 427 28 3597 .Association des Connexionnistes en THese ( ACTH ) .","label":"Background","metadata":{},"score":"116.62225"}{"text":"Reviews provided by other authors as cited below are copyrighted by those authors , who by submitting the reviews for the FAQ give permission for the review to be reproduced as part of the FAQ in any of the ways specified in part 1 of the FAQ .","label":"Background","metadata":{},"score":"141.60645"}