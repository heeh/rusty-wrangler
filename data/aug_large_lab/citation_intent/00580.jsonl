{"text":"234 .Brants , Thorsten , and Alex Franz .Web 1 T 5-gram Version 1 LDC2006T13 .DVD .Philadelphia : Linguistic Data Consortium , 2006 .Introduction .Web 1 T 5-gram Version 1 , contributed by Google Inc. , contains English word n - grams and their observed frequency counts .","label":"Background","metadata":{},"score":"31.579594"}{"text":"The length of the n - grams ranges from unigrams to seven - grams .The n - grams were extracted from publicly accessible web pages that were crawled by Google in July 2007 .This data set contains only n - grams that appear at least 20 times in the processed sentences .","label":"Background","metadata":{},"score":"36.568356"}{"text":"This data is expected to be useful for statistical language modeling , e.g. , for machine translation or speech recognition , as well as for other uses .Source Data .The n - gram counts were generated from approximately 1 trillion word tokens of text from publicly accessible Web pages .","label":"Background","metadata":{},"score":"42.456306"}{"text":"The recent availability of large collections of text such as the Google 1 T 5-gram corpus ( Brants and Franz , 2006 ) and the Gigaword corpus of newswire ( Graff , 2003 ) have made it possible to build language models that incorporate counts of billions of n - grams .","label":"Background","metadata":{},"score":"42.78942"}{"text":"Data Collection .N - gram counts were generated from approximately 883 billion word tokens of text from publicly accessible web pages .This data set contains only n - grams that appeared at least 40 times in the processed sentences .","label":"Background","metadata":{},"score":"43.160713"}{"text":"The N ... \" .We propose a novel method for using the World Wide Web to acquire trigram estimates for statistical language modeling .We submit an N - gram as a phrase query to web search engines .The search engines return the number of web pages containing the phrase , from which the N - gram count is estimated .","label":"Background","metadata":{},"score":"43.99414"}{"text":"This data set contains only n - grams that appeared at least 40 times in the processed sentences .Less frequent n - grams were discarded .While the aim was to identify and collect pages from the specific target languages only , it is likely that some text from other languages may be in the final data .","label":"Background","metadata":{},"score":"45.581894"}{"text":"Figure 1 shows that the fraction of unique n - grams inc .. \" ...Typical statistical machine translation systems are batch trained with a given training data and their performances are largely influenced by the amount of data .With the growth of the available data across different domains , it is computationally demanding to perform batch training every time when ... \" .","label":"Background","metadata":{},"score":"46.870354"}{"text":"For the majority of our tasks , we find that simple , unsupervised models perform better when n - gram counts are obtained from the Web rather than from a large corpus .In some cases , performance can be improved further by using backoff or interpolation techniques that combine Web counts and corpus counts .","label":"Background","metadata":{},"score":"47.049896"}{"text":"That is , there is no data like more data .Performance scales log - linearly with the number of parameters in the model ( the number of unique N - grams ) .The w ... \" .We use web - scale N - grams in a base NP parser that correctly analyzes 95.4 % of the base NPs in natural text .","label":"Background","metadata":{},"score":"47.994995"}{"text":"( ICASSP'01 , 2001 . \" ...We propose a novel method for using the World Wide Web to acquire trigram estimates for statistical language modeling .We submit an N - gram as a phrase query to web search engines .","label":"Background","metadata":{},"score":"48.030785"}{"text":"The length of the n - grams ranges from unigrams ( single words ) to 5-grams .This data should be useful for statistical language modeling ( e.g. , segmentation , machine translation ) as well as for other uses .","label":"Background","metadata":{},"score":"48.138508"}{"text":"The length of the n - grams ranges from unigrams ( single words ) to five - grams .The n - gram counts were generated from approximately one hundred billion word tokens of text for each language , or approximately one trillion total tokens .","label":"Background","metadata":{},"score":"48.535076"}{"text":"However , few existing studies have studied the practical application of this approach .In this paper we propose two improvements to the use of synonym substitution for encoding hidden bits of information .First , we use the Web 1 T Google n - gram corpus for checking the applicability of a synonym in context , and we evaluate this method using data from the SemEval lexical substitution task .","label":"Background","metadata":{},"score":"49.855804"}{"text":"We show that in ... \" .In this paper , we systematically assess the value of using web - scale N - gram data in state - of - the - art supervised NLP classifiers .We compare classifiers that include or exclude features for the counts of various N - grams , where the counts are obtained from a web - scale auxiliary corpus .","label":"Background","metadata":{},"score":"50.323944"}{"text":"Our information theoretical analysis shows that queries seem to be composed in a way most similar to how authors summarize documents in anchor texts or titles , offering a quantitative explanation to the observations in past work .We apply these web scale n - gram language models to three search query processing ( SQP ) tasks : query spelling correction , query bracketing and long query segmentation .","label":"Background","metadata":{},"score":"52.285065"}{"text":"In spite of their well known limitations , most notably their use of very local contexts , n - gram language models remain an essential component of many Natural Language Processing applications , such as Automatic Speech Recognition or Statistical Machine Translation .","label":"Background","metadata":{},"score":"52.76621"}{"text":"Bold # s are not statistically significant worse than exact model .Their resulting model contained 300 million unique n - grams .It ... . \" ...Inferring attributes of discourse participants has been treated as a batch - processing task : data such as all tweets from a given author are gathered in bulk , processed , analyzed for a particular feature , then reported as a result of academic interest .","label":"Background","metadata":{},"score":"53.18994"}{"text":"Kudo , Taku , and Hideto Kazawa .Japanese Web N - gram Version 1 LDC2009T08 .Web Download .Philadelphia : Linguistic Data Consortium , 2009 .Introduction .Japanese Web N - gram Version 1 , Linguistic Data Consortium ( LDC ) catalog number LDC2009T08 and isbn 1 - 58563 - 510 - 3 , was created by Google Inc.","label":"Background","metadata":{},"score":"53.455025"}{"text":"Philadelphia : Linguistic Data Consortium , 2010 .Introduction .Chinese Web 5-gram Version 1 , Linguistic Data Consortium ( LDC ) catalog number LDC2010T06 and isbn 1 - 58563 - 539 - 1 , was created by researchers at Google Inc.","label":"Background","metadata":{},"score":"54.458984"}{"text":"To address semantic ambiguities in coreference resolution , we use Web n - gram features that capture a range of world knowledge in a diffuse but robust way .Specifically , we exploit short - distance cues to hypernymy , semantic compatibility , and semantic context , as well as general lexical co - occurrence .","label":"Background","metadata":{},"score":"54.939568"}{"text":"We include dependency model results using PMI as the association measure ; results were lower with the adjacency model .As in - domain data , we use Vadas and Curran ( 20 ... . \" ...We use web - scale N - grams in a base NP parser that correctly analyzes 95.4 % of the base NPs in natural text .","label":"Background","metadata":{},"score":"55.073357"}{"text":"Data .Before the n - grams were collected , the web pages were converted into UTF-8 encoding , normalized into Unicode Normalization Form KC ( see below ) , and split into sentences .Ill - formed sentences were filtered out , and the remaining sentences were segmented into \" words \" .","label":"Background","metadata":{},"score":"56.280327"}{"text":"Assigning a preferred reading to it is therefore unwise .In . \" ...In this paper , we systematically assess the value of using web - scale N - gram data in state - of - the - art supervised NLP classifiers .","label":"Background","metadata":{},"score":"56.41709"}{"text":"That is , there is no data like more data .Performance scales log - linearly with the number of parameters in the model ( the number of unique N - grams ) .The web - scale N - grams are particularly helpful in harder cases , such as NPs that contain conjunctions . ... corresponds to the more highly associated pair .","label":"Background","metadata":{},"score":"56.621582"}{"text":"Previous work demonstrated that Web counts can be used to approximate bigram counts , suggesting that Web - based frequencies should be useful for a wide variety of Natural Language Processing ( NLP ) tasks .However , only a limited number of tasks have so far been tested using Web - scale data sets .","label":"Background","metadata":{},"score":"56.63085"}{"text":"Previous work demonstrated that Web counts can be used to approximate bigram counts , suggesting that Web - based frequencies should be useful for a wide variety of Natural Language Processing ( NLP ) tasks .However , only a limited number of tasks have so far been tested using Web - scale data sets .","label":"Background","metadata":{},"score":"56.63085"}{"text":"Consequently linguistic knowledge beyond the lexicon level can be integrated in a recognizer .The LOB corpus contains 500 English texts , each consisting of about 2,000 words .These texts are of quit ... . \" ...This thesis is concerned with the measurement and application of lexical distributional similarity .","label":"Background","metadata":{},"score":"57.084557"}{"text":"Some work has been done to combine insights from these two frameworks .A recent succe ... \" .The phrase - based and N - gram - based SMT frameworks complement each other .While the former is better able to memorize , the latter provides a more principled model that captures dependencies across phrasal boundaries .","label":"Background","metadata":{},"score":"58.521736"}{"text":"We als ... \" .In this paper we present IAM - OnDB- a new large online handwritten sentences database .It is publicly available and consists of text acquired via an electronic interface from a whiteboard .The database contains about 86 K word instances from an 11 K dictionary written by more than 200 writers .","label":"Background","metadata":{},"score":"58.52947"}{"text":"More importantly , when operating on new domains , or when labeled training data is not plentiful , we show that using web - scale N - gram features is essential for achieving robust performance . ... on between word pairs in the NC .","label":"Background","metadata":{},"score":"58.64182"}{"text":"These methods use substantially less space than all known approaches and allow n - gram probabilities or counts to be retrieved in constant time , at speeds comparable to modern language modeling toolkits .Our basic ... \" .We present three novel methods of compactly storing very large n - gram language models .","label":"Background","metadata":{},"score":"58.719315"}{"text":"Our basic approach generates an explicit minimal perfect hash function , that maps all n - grams in a model to distinct integers to enable storage of associated values .Extensions of this approach exploit distributional characteristics of n - gram data to reduce storage costs , including variable length coding of values and the use of tiered structures that partition the data for more efficient storage .","label":"Background","metadata":{},"score":"59.223938"}{"text":"These non - parametric learning algorithms are based on storing and combining frequency counts of word subsequences of different lengths , e.g. , 1 , 2 and 3 for 3-grams .\\ )Furthermore , a new observed sequence typically will have occurred rarely or not at all in the training set .","label":"Background","metadata":{},"score":"59.229805"}{"text":"A word N - gram is a sequence of words of length N with an associated probability of occurrence .N - gram probabilities are usually estimated from natural language corpora .They are utilized in the reco ... . by Marcus Liwicki , Horst Bunke - In Proc .","label":"Background","metadata":{},"score":"59.40319"}{"text":"This paper presents a new method of using statistical models to estimate the reading difficulty of Web pages .Language Models are used to represent the content typically associated with different readability levels .Reading level classifiers are created as linear combinations of a language model and ... \" .","label":"Background","metadata":{},"score":"59.789368"}{"text":"Previous work demonstrated that web counts can be used to approximate bigram frequencies , and thus should be useful for a wide variety of NLP tasks .So far , only two generation tasks ( candidate selection for machine translation and confusion - set disambiguation ) have been tested using web - scale data ... \" .","label":"Background","metadata":{},"score":"59.959915"}{"text":"This is much more than the number of operations typically involved in computing probability predictions for n - gram models .Several researchers have developed techniques to speed - up either probability prediction ( when using the model ) or estimating gradients ( when training the model ) .","label":"Background","metadata":{},"score":"59.974968"}{"text":"Figure 1 : Example of 2-dimensional distributed representation for words obtained in ( Blitzer et al 2005 ) .In ( Bengio et al 2001 , Bengio et al 2003 ) , it was demonstrated how distributed representations for symbols could be combined with neural network probability predictions in order to surpass standard n - gram models on statistical language modeling tasks .","label":"Background","metadata":{},"score":"60.69716"}{"text":"We discuss the properties of such estimates , and methods to interpolate them with traditional corpus based trigram estimates .We show that the interpolated models improve speech recognition word error rate significantly over a small test set .The second direction is to acquire more training data .","label":"Background","metadata":{},"score":"60.95632"}{"text":"So far , only two generation tasks ( candidate selection for machine translation and confusion - set disambiguation ) have been tested using web - scale data sets .The present paper investigates if these results generalize to tasks covering both syntax and semantics , both generation and analysis , and a larger range of n - grams .","label":"Background","metadata":{},"score":"60.982864"}{"text":"Optimizing the latter remains a difficult challenge .In addition , it could be argued that using a huge training set ( e.g. , all the text in the Web ) , one could get n - gram based language models that appear to capture semantics correctly .","label":"Background","metadata":{},"score":"61.412483"}{"text":"Web 1 T 5-gram , 10 European Languages Version 1 LDC2009T25 .Web Download .Philadelphia : Linguistic Data Consortium , 2009 .Introduction .Web 1 T 5-gram , 10 European Languages Version 1 was created by Google , Inc.","label":"Background","metadata":{},"score":"61.729713"}{"text":"A recent successful attempt showed the advantage of using phrasebased search on top of an N - gram - based model .We probe this question in the reverse direction by investigating whether integrating N - gram - based translation and reordering models into a phrase - based decoder helps overcome the problematic phrasal independence assumption .","label":"Background","metadata":{},"score":"62.39841"}{"text":"This paper presents methods to combine large language models trained from diverse text sources and applies them to a stateof - art French - English and Arabic - English machine translation system .We show gains of over 2 BLEU points over a strong baseline by using continuous space language models in re - ranking . \" ...","label":"Background","metadata":{},"score":"62.537155"}{"text":"Conf . on Document Analysis and Recognition , 2005 . \" ...In this paper we present IAM - OnDB- a new large online handwritten sentences database .It is publicly available and consists of text acquired via an electronic interface from a whiteboard .","label":"Background","metadata":{},"score":"62.628593"}{"text":"We present a novel beam - search decoder for grammatical error correction .The decoder iteratively generates new hypothesis corrections from current hypotheses and scores them based on features of grammatical correctness and fluency .These features include scores from discriminative classifiers for specific error categories , such as articles and prepositions .","label":"Background","metadata":{},"score":"62.82181"}{"text":"For the 1.5 billion n - grams of Gigaword , for example , we can store full count information at a cost of 1.66 bytes per n - gram ( around 30 % of the cost when using the current stateof - the - art approach ) , or quantized counts for 1.41 bytes per n - gram .","label":"Background","metadata":{},"score":"63.27349"}{"text":"We report on efforts to build large - scale translation systems for eight European language pairs .We achieve most gains from the use of larger training corpora and basic modeling , but also show promising results from integrating more linguistic annotation . .","label":"Background","metadata":{},"score":"63.40628"}{"text":"Computational issues and applications .The experiments have been mostly on small corpora , where training a neural network language model is easier , and show important improvements on both log - likelihood and speech recognition accuracy .Resampling techniques may be used to train the neural network language model on corpora of several hundreds of millions of words ( Schwenk and Gauvain 2004 ) .","label":"Background","metadata":{},"score":"63.514027"}{"text":"There is no data like more data , performance improves log - linearly with the number of parameters ( unique N - grams ) .More importantly , when operating on new domains , we show that using web - derived selectional preferences is essential for achieving robust performance . ... le corpus : one is the web , which is the largest data set that is available for NLP ( Keller and Lapata , 2003 ) .","label":"Background","metadata":{},"score":"63.617126"}{"text":"In this paper we consider bigram language models .N -gram language models are based on the observation that we are often able to guess the next word when we are reading a given text .In other words , the probability of a word is highly depending on ... . \" ... Interpolated Kneser - Ney is one of the best smoothing methods for n - gram language models .","label":"Background","metadata":{},"score":"63.714355"}{"text":"We argue that Web - based models should therefore be used as a baseline for , rather than an alternative to , standard supervised models . \" ...Semantic relationships among words and phrases are often marked by explicit syntactic or lexical clues that help recognize such relationships in texts .","label":"Background","metadata":{},"score":"64.04202"}{"text":"This paper presents an efficient low - memory method for constructing high - order approximate n - gram frequency counts .The method is based on a deterministic streaming algorithm which efficiently computes approximate frequency counts over a stream of data while employing a small memory footprint .","label":"Background","metadata":{},"score":"64.05773"}{"text":"This paper presents an efficient low - memory method for constructing high - order approximate n - gram frequency counts .The method is based on a deterministic streaming algorithm which efficiently computes approximate frequency counts over a stream of data while employing a small memory footprint .","label":"Background","metadata":{},"score":"64.05773"}{"text":"We present an efficient low - memory method for constructing high - order approximate n - gram frequency counts .The method is based on a deterministic streaming algorithm which efficiently computes ... \" .In this paper , we explore a streaming algorithm paradigm to handle large amounts of data for NLP problems .","label":"Background","metadata":{},"score":"64.287094"}{"text":"If a human were to choose the features of a word , he might pick grammatical features like gender or plurality , as well as semantic features like animate \" or invisible .With a neural network language model , one relies on the learning algorithm to discover these features , and the features are continuous - valued ( making the optimization problem involved in learning much simpler ) .","label":"Background","metadata":{},"score":"64.3441"}{"text":"We study the linguistic phenomenon of informal words in the domain of Chi - nese microtext and present a novel method for normalizing Chinese informal words to their formal equivalents .We formal - ize the task as a classification problem and propose rule - based and statistical fea - tures to model three plausible channels that explain the connection between for - mal and informal pairs .","label":"Background","metadata":{},"score":"64.43901"}{"text":"Size matters : ( 1 ) is a million words , ( 2 ) is potentially billions of words and ( 3 ) is potentially trillions of words .The unannotated monolingual data is helpful when the ambiguity can be resolved through associations among the lexical items .","label":"Background","metadata":{},"score":"64.57822"}{"text":"We add one to all counts for ... . \" ...We present a novel beam - search decoder for grammatical error correction .The decoder iteratively generates new hypothesis corrections from current hypotheses and scores them based on features of grammatical correctness and fluency .","label":"Background","metadata":{},"score":"64.9191"}{"text":"We propose a novel interpretation of interpolated Kneser - Ney as approxima ... \" .Interpolated Kneser - Ney is one of the best smoothing methods for n - gram language models .Previous explanations for its superiority have been based on intuitive and empirical justifications of specific properties of the method .","label":"Background","metadata":{},"score":"65.45685"}{"text":"This study is made possible by the development of several novel Neural Network Language Model architectures , which can easily fare with such large context windows .We experimentally observed that extending the context size yields clear gains in terms of perplexity and that the n - gram assumption is statistically reasonable as long as n is sufficiently high , and that efforts should be focused on improving the estimation procedures for such large models . ... nada , June 8 , 2012 . \" ...","label":"Background","metadata":{},"score":"65.52397"}{"text":"Each word corresponds to a point in a feature space .One can imagine that each dimension of that space corresponds to a semantic or grammatical characteristic of words .The hope is that functionally similar words get to be closer to each other in that space , at least along some directions .","label":"Background","metadata":{},"score":"65.676506"}{"text":"Additional preprocessing techniques are intro - duced , which significantly increase the word recognition rate .For classification , Hidden Markov Models are used together with a statistical language model .In writer inde - pendent experiments we achieved word recognition rates of 67.3 % on the test set when no language model is used , and 70.8 % by including a language model .","label":"Background","metadata":{},"score":"65.740234"}{"text":"In this paper we present an approach to person name disambiguation that clusters documents on the basis of textual features using cosine similarity and a machinely learned meta similarity measure .The approach achieves an F - measure of B - Cubed Precision and Recall of 0.74 1 on the Clustering Subtask for WePS-2 .","label":"Background","metadata":{},"score":"66.03134"}{"text":"The performance measured by BLEU is at least as comparable to the traditional batch training method .Furthermore , each phrase table is trained separately in each domain , and while computational overhead is significantly reduced by training them in parallel .","label":"Background","metadata":{},"score":"66.0855"}{"text":"This loose definition , however , has led to many measures being proposed or adopted from fields such as geometry , s ... \" .This thesis is concerned with the measurement and application of lexical distributional similarity .Two words are said to be distributionally similar if they appear in similar contexts .","label":"Background","metadata":{},"score":"66.15682"}{"text":"Language Models are used to represent the content typically associated with different readability levels .Reading level classifiers are created as linear combinations of a language model and surface linguistic features .Experiments show that this new method is more accurate than the widely used Flesch - Kincaid readability formula KEYWORDS Readability , Flesch - Kincaid , Unigram Language Model , EM .","label":"Background","metadata":{},"score":"67.20177"}{"text":"Inferring attributes of discourse participants has been treated as a batch - processing task : data such as all tweets from a given author are gathered in bulk , processed , analyzed for a particular feature , then reported as a result of academic interest .","label":"Background","metadata":{},"score":"67.461975"}{"text":"We generate improved word alignment of the training data by incorporating an unsupervised transliteration mining module to GIZA++ and build a phrase - based machine tran ... \" .This paper describes QCRI - MES 's submission on the English - Russian dataset to the Eighth Workshop on Statistical Machine Translation .","label":"Background","metadata":{},"score":"67.68181"}{"text":"The method is based on a deterministic streaming algorithm which efficiently computes ... \" .In this paper , we explore a streaming algorithm paradigm to handle large amounts of data for NLP problems .We present an efficient low - memory method for constructing high - order approximate n - gram frequency counts .","label":"Background","metadata":{},"score":"67.96914"}{"text":"word 's surrounding context .The attributes of the classifier are the log counts of different fillers occurring in the context patterns .We apply add - one smoothing to all counts .Every classifier also has bias features ( for every class ) .","label":"Background","metadata":{},"score":"67.99284"}{"text":"We propose a new general framework for distributional similarity based on the context of lexical substitutability , which me measure using the IR concepts of precision and recall .This framework allows us to investigate the key factors in similarity of asymmetry , the relative influence of different contexts and the extent to which words share a context ( Chapter 4 ) .","label":"Background","metadata":{},"score":"68.01036"}{"text":"Counts from large corpora ( like the web ) can be powerful syntactic cues .Past work has used web counts to help resolve isolated ambiguities , such as binary noun - verb PP attachments and noun compound bracketings .In this work , we first present a method for generating web count features that address t ... \" .","label":"Background","metadata":{},"score":"68.09623"}{"text":"The system proposed in this paper uses state - of - the - art normalization and feat ... \" .In this paper we present an on - line recognition sys - tem for handwritten texts acquired from a whiteboard .This input modality has received relatively little atten - tion in the handwriting recognition community in the past .","label":"Background","metadata":{},"score":"68.11279"}{"text":"( Uszkoreit and Brants , 2008 ) used partially classbased LMs together with word - based LMs to improve SMT performance despite the large size of the word - based models used . \" ...The phrase - based and N - gram - based SMT frameworks complement each other .","label":"Background","metadata":{},"score":"68.378075"}{"text":"Those web pages requiring user authentication , pages containing \" noarchive \" or \" noindex \" meta tags , and pages under other special restrictions were excluded from the final release .While the aim was to process only Japanese pages , the corpus may contain some pages in other languages due to language detection errors .","label":"Background","metadata":{},"score":"69.0086"}{"text":"Because many different combinations of feature values are possible , a very large set of possible meanings can be represented compactly , allowing a model with a comparatively small number of parameters to fit a large training set .The dominant methodology for probabilistic language modeling since the 1980 's has been based on n - gram models ( Jelinek and Mercer , 1980;Katz 1987 ) .","label":"Background","metadata":{},"score":"69.46154"}{"text":"We show that this method easily scales to billion - word monolingual corpora using a conventional ( 4 GB RAM ) desktop machine .Statistical machine translation experimental results corroborate that the resulting high - n approximate language model is as effective as conventional higher order language models .","label":"Background","metadata":{},"score":"69.73932"}{"text":"We then integrate our features into full - scale dependency and constituent parsers .We show relative error reductions of 7.0 % over the second - order dependency parser of McDonald and Pereira ( 2006 ) , 9.2 % over the constituent parser of Petrov et al .","label":"Background","metadata":{},"score":"69.81627"}{"text":"We show that this method easily scales to billion - word monolingual corpora using a conventional ( 8 GB RAM ) desktop machine .Statistical machine translation experimental results corroborate that the resulting high - n approximate small language model is as effective as models obtained from other count pruning methods . ... count - based pruning on SMT performance using EAN corpus .","label":"Background","metadata":{},"score":"69.8205"}{"text":"With the growth of the available data across different domains , it is computationally demanding to perform batch training every time when new data comes .In face of the problem , we propose an efficient phrase table combination method .In particular , we train a Bayesian phrasal inversion transduction grammars for each domain separately .","label":"Background","metadata":{},"score":"70.390625"}{"text":"A distributed representation is opposed to a local representation , in which only one neuron ( or very few ) is active at each time , i.e. , as with grandmother cells .One can view n - gram models as a mostly local representation : only the units associated with the specific subsequences of the input sequence are turned on .","label":"Background","metadata":{},"score":"70.43705"}{"text":"We train separate classifiers with monolingual and bilingual features and iteratively improve them via co - training .The co - trained classifier achieves close to 96 % accuracy on Treebank data and makes 20 % fewer errors than a supervised system trained with Treebank annotations .","label":"Background","metadata":{},"score":"70.4669"}{"text":"Our aim is to investigate the properties which make a good measure of lexical distributional similarity .We start by introducing the concept of lexical distributional similarity .We discuss potential applications , which can be roughly divided into distributional or language modelling applications and semantic applications , and methods of evaluation ( Chapter 2 ) .","label":"Background","metadata":{},"score":"70.62543"}{"text":"While the aim was to identify and collect only Chinese language pages , some text from other languages is incidentally included in the final data .Data collection took place in March 2008 ; no text that was created on or after April 1 , 2008 was used to develop this corpus .","label":"Background","metadata":{},"score":"70.76485"}{"text":"Data .The input encoding of documents was automatically detected , and all text was converted to UTF8 .The following table contains statistics for the entire release .File sizes ( entire corpus ) : approximately 27.9 GB compressed ( bzip2 ) text files .","label":"Background","metadata":{},"score":"70.957855"}{"text":"For this year 's workshop , we present a system for English - Spanish translation .Output N - best lists were rescored via a target Neural Network Language Model , yielding improvements in the final translation quality as measured by BLEU and TER . \" ...","label":"Background","metadata":{},"score":"70.969925"}{"text":"( 3 )a. [ [ backup compiler ] disk ] b .. by Jian Huang , Xiaolong Li , Jianfeng Gao , Kuansan Wang , Jiangbo Miao , Fritz Behr - In Proceedings of WWW 2010 . \" ...It has been widely observed that search queries are composed in a very different style from that of the body or the title of a document .","label":"Background","metadata":{},"score":"71.32315"}{"text":"..Background 40 p(x)logq(x ) ( 2.3 ) Such an evaluat ... . by Marcus Liwicki , Horst Bunke - in Tenth International Workshop on Frontiers in Handwriting Recognition , 2006 . \" ...In this paper we present an on - line recognition sys - tem for handwritten texts acquired from a whiteboard .","label":"Background","metadata":{},"score":"71.40361"}{"text":"However , naive implementations of the above equations yield predictors that are too slow for large scale natural language applications .Schwenk and Gauvain ( 2004 ) were able to build systems in which the neural network component took less than 5 % of real - time ( the duration of the speech being analyzed ) .","label":"Background","metadata":{},"score":"71.81015"}{"text":"Le Hai Son , Re Allauzen , Fran√ßois Yvon - Proc . of NAACLHLT 2012 Workshop . \" ...In spite of their well known limitations , most notably their use of very local contexts , n - gram language models remain an essential component of many Natural Language Processing applications , such as Automatic Speech Recognition or Statistical Machine Translation .","label":"Background","metadata":{},"score":"71.900925"}{"text":"In addition to the computational challenges briefly described above , several weaknesses of the neural network language model are being worked on by researchers in the field .One of them is the representation of a fixed - size context .To represent longer - term context , one may employ a recurrent network formulation , which learns a representation of context that summarizes the past word sequence in a way that preserves information predictive of the future .","label":"Background","metadata":{},"score":"71.92395"}{"text":"We trained domain - specific language models separately and then linearly interpolated them using SRILM toolkit ( Stolke , 2002 ) with weights opLanguage Pair Cased Uncased Spanish - English 25.25 26.36 ( ... . by Hassan Sajjad , Svetlana Smekalova , Nadir Durrani , Er Fraser , Helmut Schmid - In Proceedings of the Eighth Workshop on Statistical Machine Translation , 2013 . \" ...","label":"Background","metadata":{},"score":"71.938934"}{"text":"..or each sents file considering the set of unigrams occurring in sentences . \" ...We present a simple technique for learning better SVMs using fewer training examples .Rather than using the standard SVM regularization , we regularize toward low weight - variance .","label":"Background","metadata":{},"score":"72.04522"}{"text":"For example , we can collect the cooccurrence statistics of an anaphor with various candidate antecedents to ju ... . \" ...Resolving coordination ambiguity is a classic hard problem .This paper looks at coordination disambiguation in complex noun phrases ( NPs ) .","label":"Background","metadata":{},"score":"72.44676"}{"text":"We show that using smoothed language models yields significant accuracy gains for query bracketing for instance , compared to using web counts as in the literature .We also demonstrate that applying web - scale language models can have marked accuracy advantage over smaller ones . .","label":"Background","metadata":{},"score":"72.556244"}{"text":"By examining many samples of human - produced translation , SMT algorithms automatically learn how to translate .SMT has made tremendous strides in less than two decades , and many popular tec ... \" .Statistical machine translation ( SMT ) treats the translation of natural language as a machine learning problem .","label":"Background","metadata":{},"score":"72.59317"}{"text":"Systems that analyze such nominals must compensate for the lack of surface cl ... \" .Semantic relationships among words and phrases are often marked by explicit syntactic or lexical clues that help recognize such relationships in texts .Within complex nominals , however , few overt clues are available .","label":"Background","metadata":{},"score":"72.59913"}{"text":"It has been widely observed that search queries are composed in a very different style from that of the body or the title of a document .Many techniques explicitly accounting for this language style discrepancy have shown promising results for information retrieval , yet a large scale analysis on the extent of the language differences has been lacking .","label":"Background","metadata":{},"score":"73.09708"}{"text":"Figure 2 : Architecture of neural net language model introduced in ( Bengio et al 2001 ) .Note that the gradient on most of \\(C\\ ) is zero ( and need not be computed or used ) for most of the columns of \\(C\\ : \\ ) only those corresponding to words in the input subsequence have a non - zero gradient .","label":"Background","metadata":{},"score":"73.09811"}{"text":"This recognizer is based on Hidden Markov Models ( HMMs ) .In our experiments we show that by using larger training sets we can significantly increase the word recognition rate .This recognizer may serve as a benchmark reference for future research .","label":"Background","metadata":{},"score":"73.25295"}{"text":"In this paper , we present a novel approach which incorporates the web - derived selectional preferences to improve statistical dependency parsing .Conventional selectional preference learning methods have usually focused on word - to - class relations , e.g. , a verb selects as its subject a given nominal c ... \" .","label":"Background","metadata":{},"score":"73.28753"}{"text":"We explore systems trained u ... \" .Resolving coordination ambiguity is a classic hard problem .This paper looks at coordination disambiguation in complex noun phrases ( NPs ) .Parsers trained on the Penn Treebank are reporting impressive numbers these days , but they do n't do very well on this problem ( 79 % ) .","label":"Background","metadata":{},"score":"73.49759"}{"text":"We develop a novel method in which words are the vertices in a graph , synonyms are linked by edges , and the bits assigned to a word are determined by a vertex colouring algorithm .This method ensures that each word encodes a unique sequence of bits , without cutting out large number of synonyms , and thus maintaining a reasonable embedding capacity .","label":"Background","metadata":{},"score":"73.82768"}{"text":"Statistical machine translation experimental results corroborate that the resulting high - n approximate language model is as effective as conventional higher order language models .Liu , Fang , Meng Yang , and Dekang Lin .Chinese Web 5-gram Version 1 LDC2010T06 .","label":"Background","metadata":{},"score":"73.984116"}{"text":"As opposed to past explanations , our interpretation can recover exactly the formulation of interpolated Kneser - Ney , and performs better than interpolated Kneser - Ney when a better inference procedure is used . ...994 ; Berger et al .","label":"Background","metadata":{},"score":"74.24027"}{"text":"References .Jelinek , F. and Mercer , R.L. ( 1980 ) Interpolated Estimation of Markov Source Parameters from Sparse Data .Pattern Recognition in Practice , Gelsema E.S. and Kanal L.N. eds , North - Holland .pp .381 - 397 .","label":"Background","metadata":{},"score":"74.56529"}{"text":"However , in most cases , web - based models fail to outperform more sophisticated state - of - theart models trained on small corpora .We argue that web - based models should therefore be used as a baseline for , rather than an alternative to , standard models .","label":"Background","metadata":{},"score":"74.59483"}{"text":"In particular , the reason ... . byAobo Wang , Min - yen Kan , Daniel Andrade , Takashi Onishi , Kai Ishikawa . \" ...We study the linguistic phenomenon of informal words in the domain of Chi - nese microtext and present a novel method for normalizing Chinese informal words to their formal equivalents .","label":"Background","metadata":{},"score":"74.69729"}{"text":"We show that our approaches are simple to implement and can easily be combined with pruning and quantization to achieve additional reductions in the size of the language model .or tasks such as machine translation and speech recognition have shown that increasing the size of the model is a constructive way of improving the performance on those tasks .","label":"Background","metadata":{},"score":"74.76698"}{"text":"We describe the English Lexical Simplification task at SemEval-2012 .This is the first time such a shared task has been organized and its goal is to provide a framework for the evaluation of systems for lexical simplification and foster research on context - aware lexical simplification approaches .","label":"Background","metadata":{},"score":"75.150375"}{"text":"We describe the English Lexical Simplification task at SemEval-2012 .This is the first time such a shared task has been organized and its goal is to provide a framework for the evaluation of systems for lexical simplification and foster research on context - aware lexical simplification approaches .","label":"Background","metadata":{},"score":"75.150375"}{"text":"In both cases , we filter out section headings , references , tables , etc .Table 1 gives an overview of the data sets .4 Experiments and Results This section reports experimental results of our system on the HOO - HELDOUT and the HOO - TEST data ... . \" ...","label":"Background","metadata":{},"score":"75.46454"}{"text":"One way is to load the system with lexical semantics for nouns or adjectives .This merely shifts the problem elsewhere : how do we define the lexical se- mantics and build large semantic lexicons ?Another way is to find constructions similar to a given complex nominal , for which the relationships are already known .","label":"Background","metadata":{},"score":"75.644714"}{"text":"Alternatively , the language model is incrementally updated by using a succinct data structure with a interpolation technique ( Levenberg and Osborne , 2009 ; Levenberg et al . , 2011 ) .In the case of th ...Tools . \" ...","label":"Background","metadata":{},"score":"75.75355"}{"text":"Table 1 gives an overview of the data sets . byLorenza Romano , Krisztian Buza , Lars Schmidt - thieme , Claudio Giuliano . \" ...In this paper we present an approach to person name disambiguation that clusters documents on the basis of textual features using cosine similarity and a machinely learned meta similarity measure .","label":"Background","metadata":{},"score":"76.066925"}{"text":"For tuning , we use a variation of PRO which provides better weights by optimizing BLEU+1 at corpus - level .We transliterate out - of - vocabulary words in a postprocessing step by using a transliteration system built on the transliteration pairs extracted using an unsupervised transliteration mining system .","label":"Background","metadata":{},"score":"76.34132"}{"text":"Sequences of numbers separated by slashes ( e.g. in dates ) form one token .Sequences that look like urls or email addresses form one token .Data Sizes .File sizes : approx .24 GB compressed ( gzip'ed ) text files Number of tokens : 1,024,908,267,229 Number of sentences : 95,119,665,584 Number of unigrams : 13,588,391 Number of bigrams : 314,843,401 Number of trigrams : 977,069,902 Number of fourgrams : 1,313,818,354 Number of fivegrams : 1,176,470,663 .","label":"Background","metadata":{},"score":"77.08154"}{"text":"The input encoding of documents was automatically detected , and all text was converted to UTF8 .Tokenization .The data was tokenized in a manner similar to the tokenization of the Wall Street Journal portion of the Penn Treebank .Notable exceptions include the following : .","label":"Background","metadata":{},"score":"77.1331"}{"text":"This paper describes the system developed in collabaration between UCH and UPV for the 2010 WMT .For this year 's workshop , we present a system for English - Spanish translation .Output N - best lists were rescored via a target Neural Network Language Model , yielding improvements in the final translation ... \" .","label":"Background","metadata":{},"score":"77.15571"}{"text":"We show that under certain common formulations , the batchprocessing analytic framework can be decomposed into a sequential series of updates , using as an example the task of gender classification .Once in a streaming framework , and motivated by large data sets generated by social media services , we present novel results in approximate counting , showing its applicability to space efficient streaming classification .","label":"Background","metadata":{},"score":"77.37404"}{"text":"The input character encoding of documents was automatically detected , and all text was converted to UTF-8 .The data was tokenized by an automatic tool , and all continuous Chinese character sequences were processed by the segmenter . \" ...We report on efforts to build large - scale translation systems for eight European language pairs .","label":"Background","metadata":{},"score":"77.6971"}{"text":"Past work has used web counts to help resolve isolated ambiguities , such as binary noun - verb PP attachments and noun compound bracketings .In this work , we first present a method for generating web count features that address the full range of syntactic attachments .","label":"Background","metadata":{},"score":"77.80518"}{"text":"2000 ; Lafferty et al .2001 ) .Even hierarchical Bayesian models have been applied to language modelling - MacKay and Peto ( 1994 ) have proposed one based on Dirichlet distributions .Our model is a natural generalizat ...A language model is a function , or an algorithm for learning such a function , that captures the salient statistical characteristics of the distribution of sequences of words in a natural language , typically allowing one to make probabilistic predictions of the next word given preceding ones .","label":"Background","metadata":{},"score":"77.91823"}{"text":"The notion of simplicity is biased towards non - native speakers of English .Out of nine participating systems , the best scoring ones combine contextdependent and context - independent information , with the strongest individual contribution given by the frequency of the substitute regardless of its context . ... process of randomization is such that is allows the occurrence of ties .","label":"Background","metadata":{},"score":"78.59371"}{"text":"Hence the study of threeword query is of much practical importance .The noun compound bracketing task can be summarized as : given a three - word noun phrase ( NP ) 11 n1n2n3 , determine the sub - NP structure either as left or right bracketing .","label":"Background","metadata":{},"score":"78.83623"}{"text":"We develop a model of ambiguity , and a method of applying it , which represent a novel approach to the problem described here .Our model is based on the notion that human perception is the only valid criterion for judging ambiguity .","label":"Background","metadata":{},"score":"79.037964"}{"text":"A large literature on techniques to smooth frequency counts of subsequences has given rise to a number of algorithms and variants .Distributed representations .The idea of distributed representation has been at the core of the revival of artificial neural network research in the early 1980 's , best represented by the connectionist bringing together computer scientists , cognitive psychologists , physicists , neuroscientists , and others .","label":"Background","metadata":{},"score":"79.1555"}{"text":"The following is an example of the 3-gram data contained this corpus : . ceramics collectables collectibles 55 ceramics collectables fine 130 ceramics collected by 52 ceramics collectible pottery 50 ceramics collectibles cooking 45 ceramics collection , 144 ceramics collection .","label":"Background","metadata":{},"score":"79.23805"}{"text":"The design of statistical language learners therefore involves answering two questions : ( i ) Which of the multitude of possible language models will most accurately reflect the properties necessary to a given task ?( ii )What will constitute a sufficient volume of training data ?","label":"Background","metadata":{},"score":"79.24722"}{"text":"Our decoder achieves an F1 correction score significantly higher than all previous published scores on the Helping Our Own ( HOO ) shared task data set . ... odels .We crawl all non - OCR documents from the anthology , except those documents that overlap with the HOO data .","label":"Background","metadata":{},"score":"79.32895"}{"text":"The method is based on a deterministic streaming algorithm which efficiently computes approximate frequency counts over a stream of data while employing a small memory footprint .We show that this method easily scales to billion - word monolingual corpora using a conventional ( 8 GB RAM ) desktop machine .","label":"Background","metadata":{},"score":"79.53665"}{"text":"A fundamental obstacle to progress in this direction has to do with the diffusion of gradients through long chains of non - linear transformations , making it difficult to learn long - term dependencies ( Bengio et al 1994 ) in sequential data .","label":"Background","metadata":{},"score":"80.30532"}{"text":"Samples .For an example of the data in this corpus please examine this sample file .Copyright .Portions ¬© 2009 Google Inc. , ¬© 2009 Trustees of the University of Pennsylvania Tools . \" ...To address semantic ambiguities in coreference resolution , we use Web n - gram features that capture a range of world knowledge in a diffuse but robust way .","label":"Background","metadata":{},"score":"81.36353"}{"text":"We present a simple technique for learning better SVMs using fewer training examples .Rather than using the standard SVM regularization , we regularize toward low weight - variance .Our new SVM objective remains a convex quadratic function of the weights , and is therefore computationally no harder to optimize than a standard SVM .","label":"Background","metadata":{},"score":"81.46513"}{"text":"\" ...Statistical language learning research takes the view that many traditional natural language processing tasks can be solved by training probabilistic models of language on a sufficient volume of training data .The design of statistical language learners therefore involves answering two questions : ( i ... \" .","label":"Background","metadata":{},"score":"81.62481"}{"text":"The method requires only a corpus of documents and an index connecting the documents to genes . by Horst Bunke - In Proc .7th Int .Conf . on Document Analysis and Recognition , 2003 . \" ...This paper review the state of the art in o#-line Roman cursive han dw iting recognition .","label":"Background","metadata":{},"score":"81.8683"}{"text":"\\ )Vector \\(C_k\\ ) contains the learned features for word \\(k\\ .Let us denote \\(\\theta\\ ) for the concatenation of all the parameters .The capacity of the model is controlled by the number of hidden units \\(h\\ ) and by the number of learned word features \\(d\\ .","label":"Background","metadata":{},"score":"82.61458"}{"text":"An early discussion can also be found in the Parallel Distributed Processing book ( 1986 ) , a landmark of the connectionist approach .For example , with \\(m\\ ) binary features , one can describe up to \\(2^m\\ ) different objects .","label":"Background","metadata":{},"score":"82.77202"}{"text":"By leveraging some assistant data , the dependency parsing model can direct ...Tools . \" ...This paper presents methods to combine large language models trained from diverse text sources and applies them to a stateof - art French - English and Arabic - English machine translation system .","label":"Background","metadata":{},"score":"83.770584"}{"text":"Conventional selectional preference learning methods have usually focused on word - to - class relations , e.g. , a verb selects as its subject a given nominal class .This paper extends previous work to wordto - word selectional preferences by using webscale data .","label":"Background","metadata":{},"score":"84.163765"}{"text":"The method is based on a deterministic streaming algorithm which efficiently computes approximate frequency counts over a stream of data while employing a small memory footprint .We sh ... \" .This paper presents an efficient low - memory method for constructing high - order approximate n - gram frequency counts .","label":"Background","metadata":{},"score":"84.211136"}{"text":"Regarding the second , exploration of the design space has so far proceeded without an adequate answer .The goal of this thesis is to advance the exploration of the statistical language learning design space .The first of these contributions is called the meaning distributions theory .","label":"Background","metadata":{},"score":"84.455414"}{"text":"The neural network learns to map that sequence of feature vectors to a prediction of interest , such as the probability distribution over the next word in the sequence .The advantage of this distributed representation approach is that it allows the model to generalize well to sequences that are not in the set of training word sequences , but that are similar in terms of their features , i.e. , their distributed representation .","label":"Background","metadata":{},"score":"84.68654"}{"text":"This method allows compression while retaining O(1 ) access to query the model .The first step in the compression is to encode the ranks array using ... . \" ...In this paper , we explore a streaming algorithm paradigm to handle large amounts of data for NLP problems .","label":"Background","metadata":{},"score":"85.2324"}{"text":"For a discussion of shallow vs deep architectures , see ( Bengio and LeCun 2007 ) .Whereas current models have two or three layers , theoretical research on deep architectures suggests that representing high - level semantic abstractions efficiently may require deeper networks .","label":"Background","metadata":{},"score":"85.44168"}{"text":"Katz , S.M. ( 1987 )Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer .IEEE Transactions on Acoustics , Speech and Signal Processing 3:400 - 401 .Hinton , G.E. ( 1989 )Connectionist Learning Procedures .","label":"Background","metadata":{},"score":"85.80794"}{"text":"This dissertation is an investigation into how ambiguity should be classified for authors and readers of text , and how this process can be automated .Usually , authors and readers disambiguate ambiguity , either consciously or unconsciously .However , disambiguation is not always appropriate .","label":"Background","metadata":{},"score":"86.01486"}{"text":"This dissertation is an investigation into how ambiguity should be classified for authors and readers of text , and how this process can be automated .Usually , authors and readers disambiguate ambiguity , either consciously or unconsciously .However , disambiguation is not always appropriate .","label":"Background","metadata":{},"score":"86.01486"}{"text":"When the number of input variables increases , the number of required examples can grow exponentially .The curse of dimensionality arises when a huge number of different combinations of values of the input variables must be discriminated from each other , and the learning algorithm needs at least one example per relevant combination of values .","label":"Background","metadata":{},"score":"86.15907"}{"text":"Motivation : Many experimental and algorithmic approaches in biology generate groups of genes that need to be examined for related functional properties .For example , gene expression profiles are frequently organized into clusters of genes that may share functional properties .","label":"Background","metadata":{},"score":"86.77693"}{"text":"Motivation : Many experimental and algorithmic approaches in biology generate groups of genes that need to be examined for related functional properties .For example , gene expression profiles are frequently organized into clusters of genes that may share functional properties .","label":"Background","metadata":{},"score":"86.77693"}{"text":"This taski ... \" .This paper review the state of the art in o#-line Roman cursive han dw iting recognition .The input provided to an o#-line han iting recognition system is an image of a digit , aw ord , or - more generally - some text , and the system produces , as output , an ASCII transcription of the input .","label":"Background","metadata":{},"score":"87.18613"}{"text":"Along the way , we present a taxonomy of some different approaches within these areas .We conclude with an overview of evaluation and notes on future directions . by Xiaojin Zhu , Ronald Rosenfeld - Acoustics , Speech , and Signal Processing , 2001 .","label":"Background","metadata":{},"score":"88.54088"}{"text":"Another idea is to decompose the probability computation hierarchically , using a tree of binary probabilistic decisions , so as to replace \\(O(N)\\ ) computations by \\(O(\\log N)\\ ) computations ( Morin and Bengio 2005 ) .Yet another idea is to replace the exact gradient by a stochastic estimator obtained using a Monte - Carlo sampling technique ( Bengio and Senecal 2008 ) .","label":"Background","metadata":{},"score":"89.13587"}{"text":"Learning Distributed Representations of Concepts .Proceedings of the Eighth Annual Conference of the Cognitive Science Society:1 - 12 .Rumelhart , D. E. and McClelland , J. L ( 1986 ) Parallel Distributed Processing : Explorations in the Microstructure of Cognition .","label":"Background","metadata":{},"score":"89.33378"}{"text":"One of the major transformations used in Linguistic Steganography is synonym substitution .However , few existing studies have studied the practical application of this approach .In this paper we propose two impro ... \" .Linguistic Steganography is concerned with hiding information in natural language text .","label":"Background","metadata":{},"score":"92.33417"}{"text":"This is particularly dangerous if they do not realise that other readings are possible .Misunderstandings may then occur .This is particularly serious in the field of requirements engineering .If requirements are misunderstood , systems may be built incorrectly , and this can prove very costly .","label":"Background","metadata":{},"score":"92.99044"}{"text":"This paper describes the submission of the National University of Singapore ( NUS ) to the Helping Our Own ( HOO ) Pilot Shared Task .Our system targets spelling , article , and preposition errors in a sequential processing pipeline . ... the HOO data4 .","label":"Background","metadata":{},"score":"94.13884"}{"text":"CIKM ' 01 , Nov 5 - 10 , 2001 , Atlanta , GA .Copyright 2001 ACM 1 - 58113 - 000 - 0/00/0000 ... $5.00 .Our second hypothesis was that statistical language models could capture the content information related to reading difficulty .","label":"Background","metadata":{},"score":"94.30767"}{"text":"Typically , preprocessing , normalization , feature extraction , classification , and postprocessing operations are required .We 'll survey the state of the art , analyze recent trends , and try to identify challenges for future research in this field . .","label":"Background","metadata":{},"score":"96.21071"}{"text":"Prospective informal - formal pairs are further classified by a supervised binary classifier to identify correct pairs .In the classification model , we incorporate both rule - based and statistical fea ... . \" ...This paper describes the submission of the National University of Singapore ( NUS ) to the Helping Our Own ( HOO ) Pilot Shared Task .","label":"Background","metadata":{},"score":"100.333496"}{"text":"SMT has made tremendous strides in less than two decades , and many popular techniques have only emerged within the last few years .This survey presents a tutorial overview of state - of - the - art SMT at the beginning of 2007 .","label":"Background","metadata":{},"score":"101.733795"}{"text":"Copyright .Portions ¬© 2006 Google Inc. , ¬© 2006 Trustees of the University of Pennsylvania","label":"Background","metadata":{},"score":"117.02048"}{"text":"212 ceramics community for 61 ceramics companies .53 ceramics companies consultants 173 ceramics company !4432 ceramics company , 133 ceramics company .92 ceramics company 41 ceramics company facing 145 ceramics company in 181 ceramics company started 137 ceramics company that 87 ceramics component ( 76 ceramics composed of 85 ceramics composites ferrites 56 ceramics composition as 41 ceramics computer graphics 51 ceramics computer imaging 52 ceramics consist of 92 .","label":"Background","metadata":{},"score":"125.226364"}