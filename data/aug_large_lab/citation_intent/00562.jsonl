{"text":"This article demonstrates these two applications using the Penn Treebank .In a robustness evaluation experiment , two state - of - the - art statistical parsers are evaluated on an ungrammatical version of Sect .23 of the Wall Street Journal ( WSJ ) portion of the Penn treebank .","label":"Uses","metadata":{},"score":"39.368973"}
{"text":"We introduce a new method for the reranking task , based on the boosting approach to ranking problems described in Freund et al .( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .","label":"Uses","metadata":{},"score":"39.926918"}
{"text":"These can then be used , for example , in text segmentation and text summarization , or in testing hypotheses about domain adaptation [ Plank and van Noord , 2011 ] .This new resource created from the TIPSTER files employs the same file structure and conventions used in the Penn TreeBank and the PDTB 2.0 .","label":"Uses","metadata":{},"score":"41.715645"}
{"text":"Experiments .The experiments were conducted by randomly splitting the Wall St. Journal corpus into a training and testing in roughly 90/10 proportion .There are several model parameters that need to be set .This maybe due to overfitting in the cpds for the larger contexts , but I did not investigate an alternative estimate smoothing or tree induction method .","label":"Uses","metadata":{},"score":"45.556404"}
{"text":"They are important for a few reasons .First , at present the best performing parsers on the WSJ treebank ( Ratnaparkhi 1997 ; Charniak 1997 , 1999 ; Collins 1997 , 1999 ) are all cases of history - based mo .. \" ...","label":"Uses","metadata":{},"score":"45.745735"}
{"text":", 1998 ) .All of them use words and tags surrounding a word in a small window ( 1 - 3 on either side ) to assign a tags to all words in a sentence .Probabilistic methods , like Hidden Markov Models and maximum entropy models , learn a joint distribution over tags and words in a sentence , and then select the tags that are most likely given the sequence of words in a sentence .","label":"Uses","metadata":{},"score":"46.350662"}
{"text":"The papers associated with the participating systems can be found in the reference section below .Related information .References .This reference section contains two parts : first the papers from the shared task session at CoNLL-2001 and then the other related publications .","label":"Uses","metadata":{},"score":"46.936954"}
{"text":"A file of updates and further information is available via anonymous FTP from ftp.cis.upenn.edu , in pub / treebank / doc / update.cd2 .The PTB project selected 2,499 stories from a three year Wall Street Journal ( WSJ ) collection of 98,732 stories for syntactic annotation .","label":"Uses","metadata":{},"score":"48.50197"}
{"text":"Also included are tagged and parsed data from Department of Energy abstracts , IBM computer manuals , MUC-3 and ATIS .In addition , the CD - ROM includes source code for programs that were used by the PTB project in creating portions of the data .","label":"Uses","metadata":{},"score":"49.095196"}
{"text":"The Penn Treebank has recently implemented a new syntactic annotation scheme , designed to highlight aspects of predicate - argument structure .This paper discusses the implementation of crucial aspects of this new annotation scheme .It incorporates a more consistent treatment of a wide range of gramma ... \" .","label":"Uses","metadata":{},"score":"49.13971"}
{"text":"Data .The Penn Treebank ( PTB ) project selected 2,499 stories from a three year Wall Street Journal ( WSJ ) collection of 98,732 stories for syntactic annotation .These 2,499 stories have been distributed in both Treebank-2 ( LDC1999T42 ) and Treebank-3 ( LDC1999T42 ) releases of PTB .","label":"Uses","metadata":{},"score":"49.699715"}
{"text":"The input of the programs should consist of a file which is the same as the test data but which contains an additional final column which holds the results of that should be evaluated .Results .Six systems have participated in the CoNLL-2001 shared task .","label":"Uses","metadata":{},"score":"50.626755"}
{"text":"The model uses decision trees based on tags of surrounding words and other features of a word to predict its tag .To tag an entire sentence , the tags of individual words are iteratively reassigned through a process of statistical relaxation .","label":"Uses","metadata":{},"score":"50.73405"}
{"text":"not aware of any separate study of human performance ( the best systems do almost 96 % against test data , so human consistency probably at least that high ) .Looking ahead .The best performance on the baseNP and chunking tasks was obtained using a Support Vector Machine method .","label":"Uses","metadata":{},"score":"51.86727"}
{"text":"In these results , the generative model performs significantly better than the others , and does about equally well at assigning part - of - speech tags . \" ...Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .","label":"Uses","metadata":{},"score":"52.112442"}
{"text":"To learn the tree structures we use greedy hill - climbing with Bayesian scoring to evaluate next candidates ( Chickering et al . , 1997 ) .The remaining words are either unambiguous or there is not enough data to learn contextualized cpds .","label":"Uses","metadata":{},"score":"53.285217"}
{"text":"This paper discusses the implementation of crucial aspects of this new annotation scheme .INTRODUCTION During the first phase of the The Penn Treebank project [ 10 ] , ending in December 1992 , 4.5 million words of text were tagged for part - of - speech , with about two - thirds of this material also annotated with a skeletal syntactic bracketing .","label":"Uses","metadata":{},"score":"53.76747"}
{"text":"The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .","label":"Uses","metadata":{},"score":"53.824585"}
{"text":"A tag next to the open bracket denotes the type of the clause .In the CoNLL-2001 shared task , the goal is to identify clauses in text .Training and test data for this task are available .This data consists of the same partitions of the Wall Street Journal part ( WSJ ) of the Penn Treebank as the widely used data for noun phrase chunking : sections 15 - 18 as training data ( 211727 tokens ) and section 20 as test data ( 47377 tokens ) .","label":"Uses","metadata":{},"score":"55.128136"}
{"text":"The baseline results were produced by a system which only put clause brackets around sentences .All of the participating systems outperformed the baseline .Most systems obtained an F - rate between 62 and 68 .One performed below the rest [ Ham01 ] but has not used all training data .","label":"Uses","metadata":{},"score":"55.3383"}
{"text":"( 1993 ) proposed a probabilistic model for combining these features : .This model makes the approximation that those features are independent given the tag to keep the number of parameters small , but ignores certain correlations , for example , between capitalized and unknown .","label":"Uses","metadata":{},"score":"56.013313"}
{"text":"The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence .","label":"Uses","metadata":{},"score":"56.68241"}
{"text":"Note : at the workshop some of the participants have presented results that are different from the ones mentioned in their paper .Whenever possible an update of the paper with the improved results is available alongside the original version .[","label":"Uses","metadata":{},"score":"56.74927"}
{"text":"Original Treebank Release .This CD - ROM contains over 1.6 million words of hand - parsed material from the Dow Jones News Service , plus an additional one million words tagged for part - of - speech .This material is a subset of the language model corpus for the DARPA CSR large - vocabulary speech recognition project .","label":"Uses","metadata":{},"score":"57.430393"}
{"text":"Summary thoughts on POS tagging .the Penn tag set is now the standard for assessing English tagging , but it forces annotators to make some hard decisions , and has an interannotator error of around 3 % .there are many supervised learning methods which can get 96 - 97 % accuracy on held - out Wall Street Journal data ; we looked at HMMs , TBL , and MaxEnt ; the error in the Penn Treebank probably masks differences between the methods .","label":"Uses","metadata":{},"score":"57.57289"}
{"text":"The contexts for TBL rules included words , part - of - speech assignments , and prior IOB tags .Results can be scored based on the correct assignment of tags , or on recall and precision of complete baseNPs .The latter is normally used as the metric , since it corresponds to the actual objective -- different tag sets can be used as an intermediate representation .","label":"Uses","metadata":{},"score":"58.39978"}
{"text":"So if we start with highly accurate assignments to the surrounding tags , we can accurately predict the tag for the current word .By iteratively reassigning tags based on the current assignment of other tags , and keeping track of the most common assignments , we can infer the most likely tags for each word .","label":"Uses","metadata":{},"score":"58.457047"}
{"text":"Learning methods for text chunking .In his paper on POS tagging , Church also described a method for finding base noun phrases .He conducted an informal test ( 243 NPs ) and reported very good results ( 238 correct ) .","label":"Uses","metadata":{},"score":"58.55946"}
{"text":"In : Proceedings of CoNLL-2001 , Toulouse , France , 2001 .original : [ ps ] [ pdf ] [ bibtex ] [ system output ] update : paper not available [ system output ] .[ PG01 ] Jon D. Patrick and Ishaan Goyal , Boosted Decision Graphs for NLP Learning Tasks .","label":"Uses","metadata":{},"score":"58.62313"}
{"text":"This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this i ... \" .","label":"Uses","metadata":{},"score":"58.771233"}
{"text":"Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .The tenth CoNLL ( CoNLL - X ) saw a shared task on Multilingual Dependency Parsing .","label":"Uses","metadata":{},"score":"59.097332"}
{"text":"This project investigated a novel combination of statistical methods to define a flexible , though implicit , probability distribution for prediction of Part - of - Speech tags .The model can be improved upon in several ways .It is possible that using surrounding words , not just tags may be advantageous .","label":"Uses","metadata":{},"score":"59.118153"}
{"text":"Among the most successful current approaches to tagging are probabilistic models such as HMMs ( Weischedel et al . , 1993 ) and maximum entropy ( Ratnaparkhi , 1996 ) , and rule - based techniques such as transformation learning ( Brill , 1995 ) .","label":"Uses","metadata":{},"score":"59.688446"}
{"text":"ZPar is fast , processing above 50 sentences per second using the standard Penn Treebank ( Wall Street Journal ) data .Selected Publications .Yue Zhang and Stephen Clark .Syntactic Processing Using the Generalized Perceptron and Beam Search .In Computational Linguistics , 37(1 ) , March .","label":"Uses","metadata":{},"score":"60.14371"}
{"text":"Ramshaw and Marcus adapted the TBL method which had been introduced by Brill for POS tagging .They pointed out that one - level bracketing can be restated as a word tagging task .For NP chunking , they used 3 tags : I ( inside a baseNP ) , O ( outside a baseNP , ) , and B ( the start of a baseNP which immediately follows another baseNP ) .","label":"Uses","metadata":{},"score":"61.536644"}
{"text":"We also give an overview of the parsing approaches that participants took and the results that they achieved .Finally , we try to draw general conclusions about multi - lingual parsing : What makes a particular language , treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser ?","label":"Uses","metadata":{},"score":"62.162476"}
{"text":"Ratnaparkhi , 1996 finds that distribution of tags for the word \" about \" ( as well as several others ) is fairly different for different annotators of the dataset , suggesting that there is a real limit to the level of achievable accuracy .","label":"Uses","metadata":{},"score":"62.30954"}
{"text":"One million words of 1989 Wall Street Journal material annotated in Treebank II style .A small sample of ATIS-3 material annotated in Treebank II style .The Treebank bracketing style is designed to allow the extraction of simple predicate / argument structure .","label":"Uses","metadata":{},"score":"62.498547"}
{"text":"Microsoft Technical Report MSR - TR-00 - 16 .[ Marcus et al . , 1994 ] Mitchel P. Marcus , Beatrice Santorini , and Mary Ann Marcinkiewicz .Building a large annotaded corpus of English : the Penn Treebank .Computational Linguistics 19(2):313 - 330 .","label":"Uses","metadata":{},"score":"62.601234"}
{"text":"Furhter morphological features can be used for tagging of unknown words .Recent work by Brill et al .( 1998 ) showed that combining several different state - of - the - art taggers ( HMM , MaxEnt , Transformation ) in a classifier ensemble can achieve performance of up to 97.2 % percent .","label":"Uses","metadata":{},"score":"62.768654"}
{"text":"129 - 145 .ISSN 1433 - 2833 .Abstract .This article describes how a treebank of ungrammatical sentences can be created from a treebank of well - formed sentences .The treebank creation procedure involves the automatic introduction of frequently occurring grammatical errors into the sentences in an existing treebank , and the minimal transformation of the original analyses in the treebank so that they describe the newly created ill - formed sentences .","label":"Uses","metadata":{},"score":"62.79017"}
{"text":"( Several models were tried , including max , min , product and mixture , but this one seemed to work best . )Since test data contains words not seen in the training data , we must predict tags for unknown words .","label":"Uses","metadata":{},"score":"62.969772"}
{"text":"Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . .","label":"Uses","metadata":{},"score":"63.27085"}
{"text":"We present a detailed case study of this learni ... \" .this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance .","label":"Uses","metadata":{},"score":"63.692326"}
{"text":"Lots of people have tried text chunking , using many different learning methods .The CoNLL-2000 shared task ( organized by the Special Interest Group of the ACL on Computational Natural Language Learning ) included both full chunking and noun phrase ( group ) chunking .","label":"Uses","metadata":{},"score":"63.817352"}
{"text":"300-page style manual for Treebank-2 bracketing , as well as the part - of - speech tagging guidelines .The contents of the previous Treebank CD - ROM ( Version 0.5 ) , with cleaner versions of the WSJ , Brown Corpus , and ATIS material ( annotated in Treebank-1 style ) .","label":"Uses","metadata":{},"score":"63.930477"}
{"text":"While these materials have not all been converted to the newer bracketing style , they have been cleaned up to remove problems that had appeared in the earlier release .The contents of Treebank Release 2 are as follows : .One million words of 1989 Wall Street Journal material annotated in Treebank-2 style .","label":"Uses","metadata":{},"score":"64.715744"}
{"text":"References .[ Brill , 1995 ] Eric Brill .Transformation - based error - driven learning and natural language processing : A case study in part of speech tagging .Computational Linguistics 21:543 - 565 .[ Brill , 1998 ] Eric Brill and Jun Wu .","label":"Uses","metadata":{},"score":"64.79374"}
{"text":"The shared task consists of three parts : identifying clause start positions , recognizing clause end positions and building complete clauses .We have not used clauses labeled with FRAG or RRC , and all clause labels have been converted to S. The goal of this task is to come forward with machine learning methods which after a training phase can recognize the clause segmentation of the test data as well as possible .","label":"Uses","metadata":{},"score":"64.979164"}
{"text":"The core part of PCEDT 1.0 is a Czech translation of 21,600 English sentences from the Wall Street Journal , which are part of the Penn Treebank corpus .Sentences of the Czech translation were automatically morphologically annotated and parsed into two levels ( analytical and tectogrammatical ) of dependency structures introduced in the theory of Functional Generative Description and closely related to the Prague Dependency Treebank project .","label":"Uses","metadata":{},"score":"65.02042"}
{"text":"A breakdown by error type is provided for both parsers .A second experiment retrains both parsers using an ungrammatical version of WSJ Sections 2 - 21 .This experiment indicates that an ungrammatical treebank is a useful resource in improving parser robustness to grammatical errors , but that the correct combination of grammatical and ungrammatical training data has yet to be determined .","label":"Uses","metadata":{},"score":"65.33894"}
{"text":"This system outperforms previou ... \" .We describe a parsing system based upon a language model for English that is , in turn , based upon assigning probabilities to possible parses for a sentence .This model is used in a parsing system by finding the parse for the sentence with the highest probability .","label":"Uses","metadata":{},"score":"65.59277"}
{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"Uses","metadata":{},"score":"66.20773"}
{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"Uses","metadata":{},"score":"66.20773"}
{"text":"ftp://ftp.cs.columbia.edu/pub/cs4999/brill94.ps .[ CM03 ] Xavier Carreras and Lluís Màrquez , Phrase Recognition by Filtering and Ranking with Perceptrons .In \" Proceedings of the International Conference on Recent Advances in Natural Language Processing , RANLP-2003 \" , Borovets , Bulgaria , 2003 .","label":"Uses","metadata":{},"score":"66.524536"}
{"text":"A heldout ( development and evaluation ) set of 515 sentence pairs was selected and manually annotated on tectogrammatical level in both Czech and English ; for the purposes of quantitative evaluation , this set has been retranslated from Czech into English by four different translation companies .","label":"Uses","metadata":{},"score":"66.54911"}
{"text":"Institute For Research in Cognitive Science .Data and meta - data relevant to understanding as texts the files in the Penn TreeBank ( LDC Catalog entry LDC99T42 ) and the Penn Discourse TreeBank ( LDC Catalog entry LDC99T42 ) , can be found in the the TIPSTER WSJ corpus ( LDC Catalog entry LDC93T3A ) .","label":"Uses","metadata":{},"score":"66.605995"}
{"text":"ftp://ftp.cis.upenn.edu/pub/chunker/wvlcbook.ps.gz .[Rij79 ] C.J. van Rijsbergen , \" Information Retrieval \" .Buttersworth , 1979 .Biography .My research interest includes machine learning based natural language processing , web information extraction and financial market prediction .For natural language processing , I work on natural language parsing and generation ( in particular for English and Chinese ) , as well as machine translation .","label":"Uses","metadata":{},"score":"66.71544"}
{"text":"About thirty different suffixes we distinguished , of which around twenty actually ended up being used by the induced decision tree .Tagging .Test sentences are tagged one at a time .N .n . select most commonly sampled tag for each T i .","label":"Uses","metadata":{},"score":"66.91851"}
{"text":"In contrast , transformation - based method starts with an initial assignment of tags to words using the most common tag regardless of context .It then learns a list of \" rewrite \" rules that can be successively applied to the tags of the sentence to correct the initial errors .","label":"Uses","metadata":{},"score":"67.00464"}
{"text":"Software . ZPar : statistical multi - language parser , with language - specific support for Chinese and English . ZPar gives state - of - the - art speed and accuracies for Chinese and English on standard Penn Chinese Treebank and Penn Treebank test data .","label":"Uses","metadata":{},"score":"67.01407"}
{"text":"First , the initial samples must be discarded and sequential samples are not independent , so samples are actually counted after a short burn - in phase and with counts incremented every several iterations .Second , tags do not have to be sampled sequentially , and indeed , performance is improved when a random order is used .","label":"Uses","metadata":{},"score":"67.87243"}
{"text":"Treebanks gone bad : parser evaluation and retraining using a treebank of ungrammatical sentences .Foster , Jennifer ( 2007 ) Treebanks gone bad : parser evaluation and retraining using a treebank of ungrammatical sentences .International Journal of Document Analysis and Recognition ( IJDAR ) , 10 ( 3 - 4 ) .","label":"Uses","metadata":{},"score":"68.04422"}
{"text":"This information can be accessed indirectly using map files that pair each Penn TreeBank file name ( eg , wsj_0005 ) with its corresponding index in the TIPSTER WSJ corpus ( eg , 891031 - 0011 ) .While these map files are publically available via the LDC catalog entry for the Penn TreeBank , the LDC will provide a convenient meta - data / data package we have prepared , to current and future license holders of the Penn Discourse TreeBank .","label":"Uses","metadata":{},"score":"68.40761"}
{"text":"We propose ( a ) a lexical affinity model where words struggle to modify each other , ( b ) a sense tagging model where words fluctuate randomly in their selectional prefe ... \" .After presenting a novel O(n³ ) parsing algorithm for dependency grammar , we develop three contrasting ways to stochasticize it .","label":"Uses","metadata":{},"score":"68.783585"}
{"text":"ACL 1998 .[Chickering et al . , 1997 ] David Chickering , David Heckerman , Christopher Meek .A Bayesian Approach to Learning Bayesian Networks with Local Structure .Microsoft Technical Report MSR - TR-97 - 07 .[Heckerman et al . , 2000 ] David Chickering , Christopher Meek , Robert Rounthwaite , Carl Kadie .","label":"Uses","metadata":{},"score":"68.798615"}
{"text":"However , although the learning phase uses corpus statistics to induce rules , tagging itself is deterministic .Hence , it is impossible to compare likelihood of different tagging assignments or output k - best , etc . .This project investigates a probabilistic method of exploiting this high accuracy of tagging most words to bootstrap tagging of difficult ones .","label":"Uses","metadata":{},"score":"69.02238"}
{"text":"( bracketing guidelines for Treebank II Style Penn Treebank Project , section 8.1 , p.l35 ) .Modifier sharing , however , is sometimes hard for people to judge and is not always consistently annotated in the Treebank .This limits the maximum performance of any Treebank - based NP tagger .","label":"Uses","metadata":{},"score":"69.08681"}
{"text":"CMPR02 ] Xavier Carreras , Luís Màrquez , Vasin Punyakanok and Dan Roth , Learning and Inference for Clause Identification .In \" Proceedings of the 13th European Conference on Machine Learning \" , ECML'02 , Helsinki , Finland , 2002 .","label":"Uses","metadata":{},"score":"69.22331"}
{"text":"[Lef98 ] built a rule - based algorithm for finding clauses in English and Portuguese texts .[Ora00 ] used memory - based learning techniques for finding clauses in the Susanne corpus .His system included a rule - based post - processing phase for improving clause recognition performance .","label":"Uses","metadata":{},"score":"69.73505"}
{"text":"We present a maximum - likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently , using as examples several problems in natural language processing . \" ... this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .","label":"Uses","metadata":{},"score":"69.95848"}
{"text":"Ratnaparkhi points out that MaxEnt has the advantage of allowing specialized features ( like TBL ) and providing probabilities ( like an HMM ) .As an example of specialized features , he tried using conjunctions of features for difficult words , but found very little gain .","label":"Uses","metadata":{},"score":"69.95915"}
{"text":"However , POS tagging is a simpler task than full syntactic parsing , since no attempt is made to create a tree - structured model of the sentence .With the recent availability of large manually tagged corpora , many researchers have proposed statistical and rule - based techniques that automatically learn to label unseen text with POS tags .","label":"Uses","metadata":{},"score":"70.252754"}
{"text":"( Without lexical information , they got about 90.5 % recall and precision . )R&M mention two major sources of error ( and these are also error sources for simple finite - state patterns for baseNP ) : participles and conjunction .","label":"Uses","metadata":{},"score":"70.46262"}
{"text":"With a larger rule set , using Constraint Grammar rules , Voutilainen reports recall of 98%+ with precison of 95 - 98 % for noun chunks .Why do text chunking ?Partial parsing can be much faster , more robust , yet may be sufficient for many applications ( IE , QA ) .","label":"Uses","metadata":{},"score":"70.65976"}
{"text":"Lecture 3 .Part of speech tagging using maximum entropy .Ratnaparkhi used MaxEnt for POS tagging .In general , all features which occurred at least 10 times were kept .The search algorithm used was a beam search , keeping the N best tag sequences at each word .","label":"Uses","metadata":{},"score":"70.68667"}
{"text":"In addition , many words are rare , so parameter estimation is unreliable because of sparsity of the data .Since many words only appear rarely and most words appear overwhelmingly with one tag , we should devote more attention to predicting tags for the common and difficult to tag words .","label":"Uses","metadata":{},"score":"71.04856"}
{"text":"Here is an example of a sentence and its clauses obtained from Wall Street Journal section 15 of the Penn Treebank [ MSM93 ] : .( S The deregulation of railroads and trucking companies ( SBAR that ( S began in 1980 ) ) enabled ( S shippers to bargain for transportation ) . )","label":"Uses","metadata":{},"score":"71.49243"}
{"text":"Xavier Carreras has reported errors in the test data set testb3 which concerned the presence of duplicate clauses : ( S(S words S)S ) .These clauses have been removed on August 3 , 2003 .Here are the results of the systems that participated in shared task for the corrected test data set : .","label":"Uses","metadata":{},"score":"71.631485"}
{"text":"Computational Linguistics 21:543 - 565 .PCEDT 1.0 is a corpus of Czech - English parallel resources suitable for experiments in machine translation , with a special emphasis on dependency - based ( structural ) translation ( with evaluation data provided for Czech - to - English systems ) .","label":"Uses","metadata":{},"score":"71.93909"}
{"text":"There have been some earlier studies in identifying clauses .[Abn90 ] used a clause filter as a part of his CASS parser .It consists of two parts : one for recognizing basic clauses and one for repairing difficult cases ( clauses without subjects and clauses with additional VPs ) .","label":"Uses","metadata":{},"score":"72.018524"}
{"text":"The meta - data for a Penn TreeBank file comprise the set of header fields from its TIPSTER entry , explained as follows in the TIPSTER WSJ sample text : .AN : unique identifier for the article .HL : the column name ( for regular features such as . , Marketing & Media , Technology ) , its headline and by - line .","label":"Uses","metadata":{},"score":"72.132576"}
{"text":"I thank all of my thesis committee members : John La erty from Carnegie Mellon University , Aravind Joshi , Lyle Ungar , and Mark Liberman , for their extremely valuable suggestions and comments about my thesis research .I thank Mike Collins , Jason Eisner , and Dan Melamed , with whom I 've had many stimulating and impromptu discussions in the LINC lab .","label":"Uses","metadata":{},"score":"73.20493"}
{"text":"HMM methods learn a joint distribution over both words and tags of a sentence by making conditional independence assumptions ( limited horizon dependence for states and independence of words given their tags ) that are only rough approximations .It is plausible that perhaps we can achieve higher accuracy at predicting the tags if we focused on somehow learning just the conditional probability of the tags given the words .","label":"Uses","metadata":{},"score":"73.42672"}
{"text":"Eraall : brill@cs.jhu.edu .Word sense disambiguation , a problem which once seemed out of reach for systems without a great deal of hand cr ... . \" ...We describe a parsing system based upon a language model for English that is , in turn , based upon assigning probabilities to possible parses for a sentence .","label":"Uses","metadata":{},"score":"73.5441"}
{"text":"Treebank-2 includes the raw text for each story .Three \" map \" files are available in a compressed file ( pennTB_tipster_wsj_map.tar .gz ) as an additional download for users who have licensed Treebank-2 and provide the relation between the 2,499 PTB filenames and the corresponding WSJ DOCNO strings in TIPSTER .","label":"Uses","metadata":{},"score":"73.59843"}
{"text":"The train and test data consist of four columns separated by spaces .Each word has been put on a separate line and there is an empty line after each sentence .The first column contains the current word , the second a part - of - speech tag derived by the Brill tagger , the third a chunk tag generated by a chunker [ TKS00 ] and the fourth a corresponding clause tag extracted from the Penn Treebank .","label":"Uses","metadata":{},"score":"73.74124"}
{"text":"HL : Technology & Health:@ Asbestos Once Used in Kent Filters Led@ To Workers ' Cancer Deaths , Group Says@ ----@ By Anne Newman@ Staff Reporter ofThe Wall Street Journal .There are two sorts of data related to text structure : . indications as to where errors were made in creating Penn TreeBank files from TIPSTER entries , such that more than a single WSJ article from a TIPSTER file was included in a single Penn TreeBank file . indications as to where separator symbols appear in the body of the original WSJ article , indicating that it consists of a sequence of texts from different sources .","label":"Uses","metadata":{},"score":"73.76273"}
{"text":"Before coming to SUTD , I worked as a postdoctoral research associate at University of Cambridge .I received my PhD degree from University of Oxford , working on statistical Chinese processing for my thesis .I received my MSc degree from University of Oxford , working on statistical machine translation from Chinese to English by parsing .","label":"Uses","metadata":{},"score":"74.532906"}
{"text":"A maximum Entropy Model for Part - Of - Speech Tagging .In EMNLP 1 , pp .133 - 142 .[ Weischedel et al , 1993 ] Ralph Weischedel , Marie Meteer , Richar Schwartz , Lance Ramshaw and Jeff Palmucci .","label":"Uses","metadata":{},"score":"74.57156"}
{"text":"The PTB file lacks such separators .Both these meta - data and data can be of value to discourse researchers .The meta - data can , for example , enable the texts to be distinguished by genre ( news reports , editorials , etc .","label":"Uses","metadata":{},"score":"75.6387"}
{"text":"This level of performance , although not quite state - of - the - art , is quite reasonable .Some words are very difficult to classify correctly , perhaps due to the limited context window and linguistic depth of this model and other current state - of - the - art models .","label":"Uses","metadata":{},"score":"75.71643"}
{"text":"Three \" map \" files are available in a compressed file ( pennTB_tipster_wsj_map.tar .gz ) as an additional download for users who have licensed Treebank-2 and provide the relation between the 2,499 PTB filenames and the corresponding WSJ DOCNO strings in TIPSTER .","label":"Uses","metadata":{},"score":"76.55447"}
{"text":"In \" Proceedings of the ECAI ' 96Workshop on Extended finite state models of language \" , ECAI ' 96 , Budapest , Hungary , 1996 .[ RM95 ] Lance A. Ramshaw and Mitchell P. Marcus , Text Chunking Using Transformation - Based Learning .","label":"Uses","metadata":{},"score":"77.179695"}
{"text":"baseNP chunking is a task for which people ( with some linguistics training ) can write quite good rules fairly quickly .This raises the practical question of whether we should be using machine learning at all .Clearly if there is already a large relevant resource , it makes sense to learn from it .","label":"Uses","metadata":{},"score":"77.36687"}
{"text":"In : Proceedings of CoNLL-2001 , Toulouse , France , 2001 .original : [ abstract ] [ ps ] [ pdf ] [ bibtex ] update : [ abstract ] [ ps ] [ pdf ] [ bibtex ] sheets : [ ps ] [ pdf ] .","label":"Uses","metadata":{},"score":"77.456055"}
{"text":"Introduction We present a statistical parser that induces its grammar and probabilities from a hand - parsed corpus ( a tree - bank ) .Parsers induced from corpora are of interest both as simply exercises in machine learning and also because they are often the best parsers obtainable by any method .","label":"Uses","metadata":{},"score":"77.974625"}
{"text":"Ngai and Yarowsky addressed this question : Tools . by Adam L. Berger , Stephen A. Della Pietra , Vincent J. Della Pietra - COMPUTATIONAL LINGUISTICS , 1996 . \" ...The concept of maximum entropy can be traced back along multiple threads to Biblical times .","label":"Uses","metadata":{},"score":"78.30946"}
{"text":"Iwould like toacknowledge the following people for their contribution to my education : I thank my advisor Mitch Marcus , who gave me the intellectual freedom to pursue what I believed to be the best way to approach natural language processing , and also gave me direction when necessary .","label":"Uses","metadata":{},"score":"78.553185"}
{"text":"The PTB Project Release 2 CD - ROM features the new PTB-2 bracketing style , which is designed to allow the extraction of simple predicate / argument structure .Over one million words of text are provided with this bracketing applied , along with a complete style manual explaining the bracketing and new versions of tools for searching and treating bracketed data .","label":"Uses","metadata":{},"score":"78.66139"}
{"text":"[ Ham01 ] James Hammerton , Clause identification with Long Short - Term Memory .In : Proceedings of CoNLL-2001 , Toulouse , France , 2001 .[ ps ] [ pdf ] [ bibtex ] [ system output ] .[","label":"Uses","metadata":{},"score":"78.74506"}
{"text":".. rning deserves further study .There are many different ways one could try to construct a language learner .In [ 65 ] , a selforganizing language learner is proposed to be used for language modelling .In this work we take a different approach , namely starting with a s ..","label":"Uses","metadata":{},"score":"78.7524"}
{"text":"For pdf copies of the documentation files , please go to addenda for a list of the files available .Marcus , Mitchell , Beatrice Santorini , and Mary Ann Marcinkiewicz .Treebank-2 LDC95T7 .Web Download .Philadelphia : Linguistic Data Consortium , 1995 .","label":"Uses","metadata":{},"score":"80.6846"}
{"text":"In this paper we des ... \" .The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .","label":"Uses","metadata":{},"score":"80.90852"}
{"text":"A sequential Gibbs sampler instantiates the variables to arbitrary initial values and loops over them , sampling from the conditionals .It can be shown ( Heckerman et al . , 2000 ) that if conditionals are positive , the process converges to a unique stationary distribution .","label":"Uses","metadata":{},"score":"81.94826"}
{"text":"Here is an example : .In this example , the fourth column contains the clause tags for part 1 , 2 and 3 of the shared task separated by slashes .In the third column , the O chunk tag is used for tokens which are not part of any chunk .","label":"Uses","metadata":{},"score":"81.95708"}
{"text":"HL : Canadian Pig Herd Shrinks DD : 10/30/89 SO : WALL STREET JOURNAL ( J ) CO : CANDA DATELINE : OTTAWA ARTICLEBREAK : 212 . .213DOCNO : 891030 - 0155 .HL : Who 's News:@ American Federal Savings Bank of Duval County DD : 10/30/89 SO : WALL STREET JOURNAL ( J ) CO : AMJX WNEWS Clause Identification .","label":"Uses","metadata":{},"score":"82.11519"}
{"text":"Top words according to such a criterion are the ones that are commonly reported as difficult : that , about , up , 's , etc . .Learning .In addition , there are usually many context - specific independencies in the conditional probability distribution ( cpd ) , e.g. given that the next tag is comma , it does not matter what the tag after the next tag is .","label":"Uses","metadata":{},"score":"82.989624"}
{"text":"original : [ abstract ] [ ps ] [ pdf ] [ bibtex ] [ system output ] update : paper not available [ system output ] .[ Bri94 ] Eric Brill , Some Advances in Rule - Based Part of Speech Tagging .","label":"Uses","metadata":{},"score":"83.078094"}
{"text":"All software is provided \" as is . \"( We have learned since publication that the tgrep source code provided on the cd - rom is not readily portable , and compiling tgrep requires modification of the source files .The CD - ROM does include a pre - compiled program file for tgrep , built for use on Sun sparc systems . )","label":"Uses","metadata":{},"score":"83.33037"}
{"text":"As the manual for tectogrammatical annotation of English gets created , the proportion of manually annotated data will increase .Updates .There are no updates available at this time .Copyright .Portions © 1988 - 1989 Dow Jones & Company , Inc. , © 1993 - 1996 Reader 's Digest , © 1991 - 1995 Lidové noviny , © 2004 Milan Svoboda , © 2002 - 2004 Center for Computational Linguistics , Charles University in Prague , © 2004 Trustees of the University of Pennsylvania","label":"Uses","metadata":{},"score":"83.60897"}
{"text":"All the files corresponding to a given section ( XX ) are in sub - directory XX .The tarball distribution contains the 25 sub - directories 00 through 24 .Each individual file starts with the meta - data from its corresponding article in the TIPSTER corpus , followed by a list headed SBREAKS of the byte positions of section breaks present in the file .","label":"Uses","metadata":{},"score":"84.54502"}
{"text":"DOCNO : 891102 - 0087 .HL : Letters to the Editor:@ Brutal World of Life on the Streets DD : 11/02/89 SO : WALL STREET JOURNAL ( J )SBREAKS : 1988 .Those files that , in error , contain more than one article from the TIPSTER corpus have two copies of the above data separated by ARTICLEBREAK , as for example wsj_0545 : .","label":"Uses","metadata":{},"score":"85.47389"}
{"text":"Text Chunking ( J&M sec .What is text chunking ?Text chunking subsumes a range of tasks .The simplest is finding ' noun groups ' or ' base NPs ' ... non - recursive noun phrases up to the head ( for English ) .","label":"Uses","metadata":{},"score":"85.97264"}
{"text":"In particular , the suffix of a word is often a good predictor ( e.g. -tion , -ed , -ly , -ing ) .Capitalization and whether the word comes after a period or quotation marks are also indicative .In addition , numbers are rarely seen in the training data , but often can be easily classified as such ( note that numbers can also act as list markers ) .","label":"Uses","metadata":{},"score":"86.13411"}
{"text":"The rules for conjoined NPs are complicated by the bracketing rules of the Penn Tree Bank .Conjoined prenominal nouns are generally treated as part of a single baseNP : \" brick and mortar university \" ( with \" brick and mortar \" modifying \" university \" ) .","label":"Uses","metadata":{},"score":"87.15108"}
{"text":"DATELINE : normally the location where the article was filed , but sometimes has very unexpected contents .GV : Branch of Government or Government Agency mentioned in the article .While files usually have an AN field and two DD fields ( dates in different formats ) , the other fields may or may not be present .","label":"Uses","metadata":{},"score":"87.4574"}
{"text":"In : Proceedings of CoNLL-2001 , Toulouse , France , 2001 .[ ps ] [ pdf ] [ bibtex ] [ system output ] .[ Dej01 ] Hervé Déjean , Using ALLiS for Clausing .In : Proceedings of CoNLL-2001 , Toulouse , France , 2001 .","label":"Uses","metadata":{},"score":"90.021545"}
{"text":"So where the TIPSTER file has : .Technology & Health : @ Asbestos Once Used in Kent Filters Led @To Workers ' Cancer Deaths , Group Says @ ---- @By Anne Newman @Staff Reporter ofThe Wall Street Journal .","label":"Uses","metadata":{},"score":"91.668945"}
{"text":"Introduction .Part - of - speech tagging consists of labeling each word in a sentence by its appropriate part of speech , e.g. verb , noun , adjective , adverb .Since many words can act as multiple parts of speech , tagging requires disambiguating the use of the word in the context of a particular sentence .","label":"Uses","metadata":{},"score":"92.36594"}
{"text":"The included Czech - English translation dictionary consists of 46,150 translation pairs in its lemmatized version and 496,673 pairs of word forms , where for each entry - translation pair all corresponding word form pairs have been generated .Also included is an English - Czech dictionary provided by Milan Svoboda under GNU / FDL license ; this dictionary contains multi - word translations in 115,929 translation pairs .","label":"Uses","metadata":{},"score":"92.464516"}
{"text":"[ NP He ] [ VP reckons ] [ NP the current account deficit ] [ VP will narrow ] [ PP to ] [ NP only $ 1.8 billion ] [ PP in ] [ NP September ] .In any case , the chunks are non - recursive structures which can potentially be handled by finite - state methods .","label":"Uses","metadata":{},"score":"93.49601"}
