{"text":"It also shows how to extend the approach to other cases , including CFGs , link grammars ( whose original parsing algorithm is quite similar ) , and the composition of a bilexical dependency grammar with a finite - state transducer .","label":"Future","metadata":{},"score":"27.234934"}{"text":"The experimental work was reported in two papers : .In subsequent papers I developed the algorithmic point further .Others had been using O(n 5 ) algorithms for bilexical parsing models , whereas the above papers had found an O(n 3 ) algorithm : .","label":"Future","metadata":{},"score":"31.374605"}{"text":"The experiments also highlight the benefits of our data selection method .Attempts have also been made to extend beyond n - gram dependencies by exploiting ( hidden ) syntax structure ( Chelba and Jelinek , 2000 ) and semantic or topical dependencies ( Khudanpur and Wu , 2000 ) .","label":"Future","metadata":{},"score":"32.56688"}{"text":"We show that the automatically induced latent variable grammars of Petrov et al .( 2006 ) vary widely in their underlying representations , depending on their EM initialization point .We use this to our advantage , combining multiple automatically learned grammars into an unweighted product model , which ... \" .","label":"Future","metadata":{},"score":"34.08719"}{"text":"This paper describes two lexical decoding combined models which are based on a stochastic category - based model and a probabilistic model of word distribution into linguistic categories .In the rst combined model , the stochastic category - based model is a Stochastic ContextFree Grammar , and in the second combined model , the stochastic categorybased model is a n - gram model .","label":"Future","metadata":{},"score":"35.503532"}{"text":"Of the many recent publications covered , some are surely , sadly , not destined to make a substantive impact on the field .The book also occasionally exhibits excessive reluctance to extract principles .One example of this reticence is its treatment of the work of Chelba and Jelinek ( 1998 ) ; although the text hails this paper as ' ' the first clear demonstration of a probabilistic parser outperforming a trigram model ' ' ( pg .","label":"Future","metadata":{},"score":"35.969666"}{"text":"( 2006 ) vary widely in their underlying representations , depending on their EM initialization point .We use this to our advantage , combining multiple automatically learned grammars into an unweighted product model , which gives significantly improved performance over state - ofthe - art individual grammars .","label":"Future","metadata":{},"score":"38.070183"}{"text":"It is often easy to think of features which might be useful in discriminating between candidate trees for a sentence , but much more difficult to alter the model to take these features into account .In the second part of the talk I will review more recent work on learning methods which promise to be considerably more flexible in incorporating features .","label":"Future","metadata":{},"score":"39.462227"}{"text":"At the time , empirical techniques to natural language processing were on the rise -- that year , Computational Linguistics published a special issue on such methods -- and Charniak 's text was the first to treat the emerging field .Nowadays , the revolution has become the establishment ; for instance , in 1998 , nearly half the papers in Computational Linguistics concerned empirical methods ( Hirschberg , 1998 ) .","label":"Future","metadata":{},"score":"41.714653"}{"text":"In 2008 the shared task was dedicated to the joint parsing of syntactic and semantic depe ... \" .The Conference on Computational Natural Language Learning is accompanied every year by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting .","label":"Future","metadata":{},"score":"42.263035"}{"text":"First , we compare a complete sentence in a stack with ... .by Eduard Hovy , Chin - yew Lin , Liang Zhou - Proceedings of DUC-2005 , 2005 . \" ...In this paper we introduce Basic Elements , a new way of automating the evaluation of text summaries .","label":"Future","metadata":{},"score":"42.593575"}{"text":"First , we compare a complete sentence in a stack with ... .by Eduard Hovy , Chin - yew Lin , Liang Zhou - Proceedings of DUC-2005 , 2005 . \" ...In this paper we introduce Basic Elements , a new way of automating the evaluation of text summaries .","label":"Future","metadata":{},"score":"42.593575"}{"text":"The ... \" .Low - dimensional representations for lexical co - occurrence data have become increasingly important in alleviating the sparse data problem inherent in natural language processing tasks .This work presents a distributed latent variable model for inducing these low - dimensional representations .","label":"Future","metadata":{},"score":"42.692497"}{"text":"This paper successfully reanalyzes \" iterative \" metrical stress within OTP , not using GA at all .The new analysis is arguably better : it explains typological gaps that were previously mysterious or unremarked .One would like to compile phonological grammars into finite - state transducers ( FSTs ) .","label":"Future","metadata":{},"score":"42.694786"}{"text":"In this paper we introduce Basic Elements , a new way of automating the evaluation of text summaries .We show that this method correlates better with human judgments than any other automated procedure to date , and overcomes the subjectivity / variability problems of manual methods that require humans to preprocess summaries to be evaluated .","label":"Future","metadata":{},"score":"43.21102"}{"text":"In this paper we introduce Basic Elements , a new way of automating the evaluation of text summaries .We show that this method correlates better with human judgments than any other automated procedure to date , and overcomes the subjectivity / variability problems of manual methods that require humans to preprocess summaries to be evaluated .","label":"Future","metadata":{},"score":"43.21102"}{"text":"Experiments on the UPenn Treebank corpus are reported .These experiments have been carried out in terms of the test set perplexity and the word error rate in a speech recognition experiment . \" ... application .Furthermore , other structure knowledge such as context free grammar and link grammar [ 10 , 11 ] is introduced to language models for improving their performances .","label":"Future","metadata":{},"score":"43.34323"}{"text":"Experiments on the UPenn Treebank corpus are reported .These experiments have been carried out in terms of the test set perplexity and the word error rate in a speech recognition experiment . \" ... application .Furthermore , other structure knowledge such as context free grammar and link grammar [ 10 , 11 ] is introduced to language models for improving their performances .","label":"Future","metadata":{},"score":"43.34323"}{"text":"In this pa - per , we describe a number of extensions to this rule - based tagger .First , we describe a method for expressing lexical re - lations in tagging that stochastic taggers are currently unable to express .","label":"Future","metadata":{},"score":"43.362934"}{"text":"Bilexical context - free grammars ( 2-LCFGs ) have proved to be accurate models for statistical natural language parsing .A 2-LCFG is splittable if the left arguments of a lexical head are always independent of the right arguments , and vice versa .","label":"Future","metadata":{},"score":"43.56466"}{"text":"This work has been motivated by two long term goals : to understand how humans learn language and to build programs that can understand language .Using a representation that makes the relevant features explicit is a prerequisite for successful learning and understanding .","label":"Future","metadata":{},"score":"43.600624"}{"text":"This work has been motivated by two long term goals : to understand how humans learn language and to build programs that can understand language .Using a representation that makes the relevant features explicit is a prerequisite for successful learning and understanding .","label":"Future","metadata":{},"score":"43.600624"}{"text":"Finally , we show how the tagger can be extended into a k - best tagger , where multiple tags can be assigned to words in some cases of uncertainty . \" ...The Conference on Computational Natural Language Learning is accompanied every year by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting .","label":"Future","metadata":{},"score":"43.64042"}{"text":"Much of the recent success of these methods has been due to the incorporation of lexically conditioned parameters .I will discuss the importance of head words and dependency parameters , and also the use of estimation methods such as decision trees or maximum entropy methods .","label":"Future","metadata":{},"score":"43.684395"}{"text":"Building this kind of resource is expensive and labor - intensive .This work proposes to use sample selection to find helpful training examples and reduce human effort spent on annotating less informative ones .We consider several criteria for predicting whether unlabeled data might be a helpful training example .","label":"Future","metadata":{},"score":"43.872826"}{"text":"Building this kind of resource is expensive and labor - intensive .This work proposes to use sample selection to find helpful training examples and reduce human effort spent on annotating less informative ones .We consider several criteria for predicting whether unlabeled data might be a helpful training example .","label":"Future","metadata":{},"score":"43.872826"}{"text":"Jelinek , Frederick .Statistical Methods for Speech Recognition .MIT Press .Jurafsky , Daniel and James Martin .In press .Speech and Language Processing .Prentice Hall .Mooney , Raymond J. 1996 .Comparative experiments on disambiguating word senses : An illustration of the role of bias in machine learning .","label":"Future","metadata":{},"score":"43.88934"}{"text":"We investigate the precise relationship between these two formalisms , showing that , while they define the same classes of probabilis- tic languages , they appear to impose different inductive biases . ... fining the probability of a parse tree as the probability that a certain top - down stochastic generative process produces that tree .","label":"Future","metadata":{},"score":"44.24872"}{"text":"We investigate the precise relationship between these two formalisms , showing that , while they define the same classes of probabilis- tic languages , they appear to impose different inductive biases . ... fining the probability of a parse tree as the probability that a certain top - down stochastic generative process produces that tree .","label":"Future","metadata":{},"score":"44.24872"}{"text":"We carry out systema ... \" .We extend discriminative n - gram language modeling techniques originally proposed for automatic speech recognition to a statistical machine translation task .In this context , we propose a novel data selection method that leads to good models using a fraction of the training data .","label":"Future","metadata":{},"score":"45.14314"}{"text":"In this work , we focus on prosodic breaks .Based on decision trees , ...Subsequent discussion of those proposals resulted in senseval , the first evaluation exercise for word sense disambiguation ( Kilgarriff and Palmer forthcoming ) .This article is a revised and extended version of our 1997 workshop paper , reviewing its observations and proposals and discussing them in light of the senseval exercise .","label":"Future","metadata":{},"score":"45.157185"}{"text":"However , the lack of an underlying set of principles driving the presentation has the unfortunate consequence of obscuring some important connections .For example , classification is not treated in a unified way : Chapter 7 introduces two supervised classification algorithms , but several popular and important techniques , including decision trees and k -nearest - neighbor , are deferred until Chapter 16 .","label":"Future","metadata":{},"score":"45.2181"}{"text":"Yi Su , Frederick Jelinek , Sanjeev Khudanpur - in Proc .Interspeech , 2007 . \" ...The random forest language model ( RFLM ) has shown encouraging results in several automatic speech recognition ( ASR ) tasks but has been hindered by practical limitations , notably the space - complexity of RFLM estimation from large amounts of data .","label":"Future","metadata":{},"score":"45.467888"}{"text":"This implies that our scheme may be useful as a novel method for PCFG induction .1 Introduction Like its non - stochastic brethren , probabilistic parsing has been based upon context - free grammars ( CFGs ) , and for similar reasons : CFGs support a simple and efficien ... . ... that tag given the two previous tags . ) 8 What would correspond to a good improvement in the cross - entropy of the tag sequence ?","label":"Future","metadata":{},"score":"45.492706"}{"text":"We discuss challenges such a design faces and describe our solutions that scale well to large tagsets and corpora .We propose two fine - grain tagsets and evaluate our model using these tags , as well as POS tags and SuperARV tags in a speech recognition task and discuss future directions . ... and time constraints .","label":"Future","metadata":{},"score":"45.500328"}{"text":"These data show that monolingual sense distinctions at most levels of granularity can be effectively captured by translations into some ... . ... her than verb tokens ( Dorr and Jones 1996a , 1996b ) .Theseld has narrowed down approaches , but only a little .","label":"Future","metadata":{},"score":"45.643814"}{"text":"Computer science .Issue Date : .Nederhof , M J & Satta , G 2011 , ' Splittability of bilexical context - free grammars is undecidable ' Computational Linguistics , vol 37 , no .4 , pp .867 - 879 . , 10.1162/COLI_a_00079 .","label":"Future","metadata":{},"score":"46.133286"}{"text":"Because the conditioning is local , efficient polynomial time parsing algorithms exist for computing inside , outside , and Viterbi parses .PFGs can produce probabilities of strings , making them potentially useful for language modeling .Precision and recall results are comparable to the state of the art with words , and the best reported without words . 1 Introduction Recently , many researchers have worked on statistical parsing techniques which try to capture additional context beyond that of simple probabilistic context - free grammars ( PCFGs ) , including Magerman ( 1995 ) , Charniak ( 1996 ) , Collins ( 1996 ; 1997 ) , ... . ... is somewhat inelegant ; also , for the probabilities to sum to one , it requires an additional step of normalization , which they appear not to have implemented .","label":"Future","metadata":{},"score":"46.18453"}{"text":"Because the conditioning is local , efficient polynomial time parsing algorithms exist for computing inside , outside , and Viterbi parses .PFGs can produce probabilities of strings , making them potentially useful for language modeling .Precision and recall results are comparable to the state of the art with words , and the best reported without words . 1 Introduction Recently , many researchers have worked on statistical parsing techniques which try to capture additional context beyond that of simple probabilistic context - free grammars ( PCFGs ) , including Magerman ( 1995 ) , Charniak ( 1996 ) , Collins ( 1996 ; 1997 ) , ... . ... is somewhat inelegant ; also , for the probabilities to sum to one , it requires an additional step of normalization , which they appear not to have implemented .","label":"Future","metadata":{},"score":"46.18453"}{"text":"However , for the SLM it is possible to generalize the CKY chart parsing algorithm to obtain a chart which allows the direct computation of language model probabilities thus rendering the stacks unnecessary .An analysis of the behavior of the SLM leads to a generalization of the Inside - Outside algorithm and thus to rigorous EM type re - estimation of the SLM parameters .","label":"Future","metadata":{},"score":"46.476242"}{"text":"In this paper , a new part - of - speech tagging method based on neural networks ( Net-7h . qger ) is presented and its performance is compared to that of a 11IvlM - tagger ( Cutting ct al . , 1992 ) anti a trigrambased tagger ( Kempe , 1993 ) .","label":"Future","metadata":{},"score":"46.683823"}{"text":"It seems likely that the same situation exists now ; there is , currently , no other comprehensive reference for statistical NLP .Luckily , this big book takes its responsibilities seriously , and the authors are to be commended for their efforts .","label":"Future","metadata":{},"score":"46.738358"}{"text":"Based on decision lists , the algorithm incorporates both local syntactic patterns and more distant collocational evidence , combining the strengths of decision trees , N - gram taggers an ... \" .This chapter presents a statistical decision procedure for lexical ambiguity resolution in text - to - speech synthesis .","label":"Future","metadata":{},"score":"47.16323"}{"text":"Tools . \" ... Interpolated Kneser - Ney is one of the best smoothing methods for n - gram language models .Previous explanations for its superiority have been based on intuitive and empirical justifications of specific properties of the method .","label":"Future","metadata":{},"score":"47.322388"}{"text":"We show that for restricted model sizes this model gives better cross - entropy and speech recognition results than the conventional n - gram models , and also better recognition results than non - clustered varigram models built with another recently introduced method .","label":"Future","metadata":{},"score":"47.32608"}{"text":"In this approach , ( discrete ) words and documents are mapped onto a single ( continuous ) semantic vector space of comparatively low dimension , in which familiar clustering techniques can be applied .Applications include information retrieval , automatic semantic classification , and semantic language modeling .","label":"Future","metadata":{},"score":"47.37878"}{"text":"One of my colleagues , a computational chemist with a background in statistical physics , recently became interested in applying methods from statistical NLP to protein modeling .4 In particular , we briefly discussed the notion of using probabilistic context - free grammars for modeling long - distance dependencies .","label":"Future","metadata":{},"score":"47.438282"}{"text":"The above algorithms find word - to - word dependencies in order to let the parser evaluate whether the dependencies are plausible .Usually this evaluation considers the two words being linked .But there are other ways to evalute a dependency : .","label":"Future","metadata":{},"score":"47.482643"}{"text":"We have an accompanying library of techniques for learning model parameters .To design the language , the compiler , and the machine learning library , we have had to embed the various techniques we know into a common framework .This has been fruitful and enlightening work , and we all use the result to get our research done .","label":"Future","metadata":{},"score":"47.525146"}{"text":"Meanwhile , we show empirically that a finer grained prosodic break is needed for language modeling .Experimental results showed that given prosodic breaks , we were able to reduce the LM perplexity by a significant margin , suggesting a prosodic N - best rescoring approach for ASR .","label":"Future","metadata":{},"score":"47.64074"}{"text":"A hybrid language model is defined as a combination of a word - based n - gram , which is used to capture the local relation ... \" .Abstract .This paper explores the use of initial Stochastic Context - Free Grammars ( SCFG ) obtained from a treebank corpus for the learning of SCFG by means of estimation algorithms .","label":"Future","metadata":{},"score":"47.82732"}{"text":"A hybrid language model is defined as a combination of a word - based n - gram , which is used to capture the local relation ... \" .Abstract .This paper explores the use of initial Stochastic Context - Free Grammars ( SCFG ) obtained from a treebank corpus for the learning of SCFG by means of estimation algorithms .","label":"Future","metadata":{},"score":"47.82732"}{"text":"Though Chomsky argued convincingly that such approaches can not fully model grammaticality judgments on word sequences , there are reasons to use them : .Many other linguistic phenomena do appear to be genuinely finite - state : phonology , morphology , subcategorization ( the right - hand sides of phrase - structure rules ) , and syntactic chunking .","label":"Future","metadata":{},"score":"48.673016"}{"text":"Index Terms : random forest language model , large - scale training , data scaling , speech recognition . ... popularity because of its simplicity and surprisingly good performance .Several new models have been proposed over the years with varying degrees of success .","label":"Future","metadata":{},"score":"48.847496"}{"text":"Implicit in all these comments is the belief that a mathematical foundation for statistical natural language processing can exist and will eventually develop .The authors , as cited above , maintain that this is not currently the case , and they might well be right .","label":"Future","metadata":{},"score":"49.313244"}{"text":"This talk will discuss the problem of machine learning applied to natural language parsing : given a set of example sentence / tree pairs , the task is to learn a function from sentences to trees which generalizes well to new sentences .","label":"Future","metadata":{},"score":"49.31534"}{"text":"In Chinese , shallow parsing has gotten some promising results [ 13 , 14].However , due to the lacks of the fine - annotated corpus ( such as Treebanks ) and competitive syntactic parser , it is infeasible to build a language model that depends on the complete parsing technique such as Chelba [ 9].","label":"Future","metadata":{},"score":"49.356537"}{"text":"In Chinese , shallow parsing has gotten some promising results [ 13 , 14].However , due to the lacks of the fine - annotated corpus ( such as Treebanks ) and competitive syntactic parser , it is infeasible to build a language model that depends on the complete parsing technique such as Chelba [ 9].","label":"Future","metadata":{},"score":"49.356556"}{"text":"We propose a joint language mod ... . by John Blitzer , Amir Globerson , Fernando Pereira - IN PROCEEDINGS OF THE INTERNATIONAL WORKSHOP ON ARTIFICIAL INTELLIGENCE AND STATISTICS , 2005 . \" ...Low - dimensional representations for lexical co - occurrence data have become increasingly important in alleviating the sparse data problem inherent in natural language processing tasks .","label":"Future","metadata":{},"score":"49.370888"}{"text":"Most of this work tries to use statistical prediction to help users cope with information overload .The lawyers uglified the writing , though .The final version of this book review appears in Computational Linguistics 26(2 ) , pp 277 - 279 .","label":"Future","metadata":{},"score":"49.45144"}{"text":"Of course , encyclopedias cover many subjects , too ; a good text not only contains information , but arranges it in an edifying way .In organizing the book , the authors have ' ' decided against attempting to present Statistical NLP as homogeneous in terms of mathematical tools and theories ' ' ( pg . xxx ) , asserting that a unified theory , though desirable , does not currently exist .","label":"Future","metadata":{},"score":"49.774216"}{"text":"Because of their large - span nature , such LSA language models are well suited to complement conventional n - grams .An integrative formulation is proposed for harnessing this synergy , in which the latent semantic information is used to suitably adjust the standard n - gram probability .","label":"Future","metadata":{},"score":"49.98501"}{"text":"This combines two finite - state traditions : algebraic ( hand - built expert systems ) and statistical ( empirically trainable systems ) .A leisurely journal paper on this work is coming soon .In their general form , some of the ideas were adapted from my thesis .","label":"Future","metadata":{},"score":"50.06968"}{"text":"Finally , several chapters are devoted to specific problems , among them lexicon acquisition , word sense disambiguation , parsing , machine translation , and information retrieval . 2 ( The companion website contains further useful material , including links to programs and a list of errata . )","label":"Future","metadata":{},"score":"50.131226"}{"text":"The book 's suppression of computational steps in its presentations , combined with some unfortunate typographical errors , risks leaving the reader with neither the ability nor the confidence to develop EM formulations in his or her own work .Finally , if FSNLP had been organized around a set of theories , it could have been more focused .","label":"Future","metadata":{},"score":"50.33214"}{"text":"Furthermore , it makes unsupervised log - linear estimation tractable , which allows the beneficial incorporation of arbitrary features such as spelling .Finite - state automata and their stochastic cousins , Hidden Markov Models , are respectively standard topics in the CS and EE curricula .","label":"Future","metadata":{},"score":"50.37887"}{"text":"Interpolated Kneser - Ney is one of the best smoothing methods for n - gram language models .Previous explanations for its superiority have been based on intuitive and empirical justifications of specific properties of the method .We propose a novel interpretation of interpolated Kneser - Ney as approximate inference in a hierarchical Bayesian model consisting of Pitman - Yor processes .","label":"Future","metadata":{},"score":"50.483173"}{"text":"This talk will attempt to cover ( very briefly ! )Weighted automata and transducers are successfully used in many text and speech processing applications .They give a unifying framework for the construction and representation of the components of speech recognition , speech synthesis and spoken - dialogue systems .","label":"Future","metadata":{},"score":"50.576424"}{"text":"Darpa Speech and Natural Language Workshop , 1994 . \" ... Parser development is generally viewed as a primarily linguistic enterprise .A grammarian examines sentences , skillfully extracts the linguistic generalizations evident in the data , and writes grammar rules which cover the language .","label":"Future","metadata":{},"score":"50.629395"}{"text":"Darpa Speech and Natural Language Workshop , 1994 . \" ... Parser development is generally viewed as a primarily linguistic enterprise .A grammarian examines sentences , skillfully extracts the linguistic generalizations evident in the data , and writes grammar rules which cover the language .","label":"Future","metadata":{},"score":"50.629395"}{"text":"If probabilistic context - free grammars are t ... \" .In this paper we describe a new algorithm that achieves the required computation in at most a constant times k3-steps .ven word string wlw2 ...Wk ?That is , which sequence of rewrite rules resulting in wlw2 ...","label":"Future","metadata":{},"score":"51.03694"}{"text":"Two papers reported the state of the work as of 1991 : .The bilexical idea ultimately proved to be my most important contribution to that project , and it so dramatically improved performance that it continued to influence my direction .","label":"Future","metadata":{},"score":"51.23523"}{"text":"The Conference on Computational Natural Language Learning is accompanied every year by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting .In 2008 the shared task was dedicated to the joint parsing of syntactic and semantic dependencies .","label":"Future","metadata":{},"score":"51.25078"}{"text":"Directional evaluation is also attractive on empirical grounds ( it naturally describes iterative phenomena without using GA ) and on aesthetic grounds ( it resembles constraint ranking ) .Comprehension and Compilation in Optimality Theory ( ACL 2002 ) unifies directional constraints with other proposals for changing the constraint evaluation strategy .","label":"Future","metadata":{},"score":"51.270145"}{"text":"In this article , however , we show the negative result that splittability of 2-LCFGs is undecidable . \" ...Lexical decoding is the obtaining of the most probable sequence of categories associated to a sequence of words .This paper describes two lexical decoding combined models which are based on a stochastic category - based model and a probabilistic model of word distribution into linguistic categories ... \" .","label":"Future","metadata":{},"score":"51.787334"}{"text":"This led to a probabilistic dependency parser that I built for fun in 1994 , which was interesting for both its cubic - time algorithm and its bilexical parameterization .Unfortunately I did n't bother to evaluate it on a sufficient volume of training and test data until 1996 , spurred by the strong results of my friend and colleague Mike Collins , who was working along similar lines .","label":"Future","metadata":{},"score":"51.802555"}{"text":"Corpus - based statistical parsing relies on using large quantities of annotated text as training examples .Building this kind of resource is expensive and labor - intensive .This work proposes to use sample selection to find helpful training examples and reduce human effort spent on annotating less inf ... \" .","label":"Future","metadata":{},"score":"52.598114"}{"text":"Corpus - based statistical parsing relies on using large quantities of annotated text as training examples .Building this kind of resource is expensive and labor - intensive .This work proposes to use sample selection to find helpful training examples and reduce human effort spent on annotating less inf ... \" .","label":"Future","metadata":{},"score":"52.598114"}{"text":"We present a scalable joint language model designed to utilize fine - grain syntactic tags .We discuss challenges such a design faces and describe our solutions that scale well to large tagsets and corpora .We advocate the use of relatively simple tags that do not require deep linguistic knowledge of ... \" .","label":"Future","metadata":{},"score":"52.607788"}{"text":"The talk will be loosely based on the survey paper : . \"Two decades of Statistical Language Modeling : Where Do We Go From Here ? \" , Roni Rosenfeld , Proceedings of the IEEE 88(8 ) , August 2000 pdf ( 98 KB ) postscript ( 113 KB ) .","label":"Future","metadata":{},"score":"52.844215"}{"text":"These systems rely heavily on domain - specific , handcrafted knowledge to handle the myriad syntactic , semantic , and pragmatic ambiguities that pervade virtually all aspects of sentence analysis .Not surprisingly , however , generating this knowledge for new domain ... . \" ...","label":"Future","metadata":{},"score":"52.913254"}{"text":"This design decision enables the authors to place attractive yet accessible topics early in the book .For instance , word sense disambiguation , a problem students seem to find quite intuitive , is presented a full two chapters before hidden Markov models , even though HMM 's are considered a basic technology in statistical NLP .","label":"Future","metadata":{},"score":"53.23529"}{"text":"The model is based on the Factorial Hidden Markov Model ( FHMM ) with distributed hidden states representing partof - speech and noun phrase se ... \" .We present new statistical models for jointly labeling multiple sequences and apply them to the combined task of partof - speech tagging and noun phrase chunking .","label":"Future","metadata":{},"score":"53.36196"}{"text":"Contrastive Estimation : Training Log - Linear Models on Unlabeled Data ( ACL 2005 , with Noah A. Smith ) shows dramatic gains on unsupervised part - of - speech tagging .Instead of maximizing the likelihood of the example strings , it tries to make them more likely than other , superficially similar strings in the same \" neighborhood .","label":"Future","metadata":{},"score":"53.415848"}{"text":"In this paper , we define the shared task and describe how the data sets were created .Furthermore , we report and analyze the results and describe the approaches of the participating systems . ... dependent relations rather than phrases , the task of the conversion procedure is to identify and label the head - dependent pairs .","label":"Future","metadata":{},"score":"53.47639"}{"text":"Statistical language models used in large vocabulary speech recognition have to properly encapsulate the various constraints , both local and global , present in the language .While usual n - gram modeling can readily capture local constraints , it has been more difficult to handle global constraints , like long - term semantic dependencies , within an adequate data - driven formalism .","label":"Future","metadata":{},"score":"53.503914"}{"text":"As with syntactic models , however , scalability is again an issue .In this case , computational complexity is demanding , and if one has to choose between RFs with less data and KN smoothing with more ... . \" ...","label":"Future","metadata":{},"score":"53.57216"}{"text":"We develop a language model using probabilistic context - free grammars ( PCFGs ) that is \" pseudo context - sensitive \" in that the probability that a non - terminal N expands using a rule r depends on N 's parent .","label":"Future","metadata":{},"score":"53.63652"}{"text":"We develop a language model using probabilistic context - free grammars ( PCFGs ) that is \" pseudo context - sensitive \" in that the probability that a non - terminal N expands using a rule r depends on N 's parent .","label":"Future","metadata":{},"score":"53.63652"}{"text":"In summer 2002 , a team at the Johns Hopkins CLSP Summer Workshop investigated \" tree - to - tree \" translation ( on dependency trees ) .That is , the training and decoding methods are given parse trees rather than sentences , and pay attention to syntax .","label":"Future","metadata":{},"score":"53.66958"}{"text":"Similarly , someone teaching a course on statistical NLP will appreciate the large number of topics FSNLP covers , allowing the tailoring of a syllabus to individual interests .And for those entering the field , the book records ' ' folklore ' ' knowledge that is typically acquired only by word of mouth or bitter experience , such as techniques for coping with computational underflow .","label":"Future","metadata":{},"score":"53.7313"}{"text":"Unlike the linear interpo ... \" . application .Furthermore , other structure knowledge such as context free grammar and link grammar [ 10 , 11 ] is introduced to language models for improving their performances .To combine statistical information from multiple sources , maximum entropy ( ME ) LM is presented [ 12].","label":"Future","metadata":{},"score":"53.919147"}{"text":"Unlike the linear interpo ... \" . application .Furthermore , other structure knowledge such as context free grammar and link grammar [ 10 , 11 ] is introduced to language models for improving their performances .To combine statistical information from multiple sources , maximum entropy ( ME ) LM is presented [ 12].","label":"Future","metadata":{},"score":"53.919147"}{"text":"The proposed model combines trigram and structure knowledge of base phrase in which trigram is used to capture the local relation between words , while structure knowledge \" ...Lexical decoding is the obtaining of the most probable sequence of categories associated to a sequence of words .","label":"Future","metadata":{},"score":"54.019756"}{"text":"We demonstrate that this joint labeling approach , by enabling information sharing between tagging / chunking subtasks , outperforms the traditional method of tagging and chunking in succession .Further , we extend this into a novel model , Switching FHMM , to allow for explicit modeling of cross - sequence dependencies based on linguistic knowledge .","label":"Future","metadata":{},"score":"54.305347"}{"text":"In order to obtain linguistically adequate CCG analyses , and to eliminate noise and inconsistencies in the original annotation , an extensive analysis of the constructions and annotations in the Penn Treebank was called for , and a substantial number of changes to the Treebank were necessary .","label":"Future","metadata":{},"score":"54.511093"}{"text":"In order to obtain linguistically adequate CCG analyses , and to eliminate noise and inconsistencies in the original annotation , an extensive analysis of the constructions and annotations in the Penn Treebank was called for , and a substantial number of changes to the Treebank were necessary .","label":"Future","metadata":{},"score":"54.511093"}{"text":"I 'm not above engineering tweaks , but I do try to do them in an elegant and general way .There are many recurring techniques that appear in NLP and more generally in AI .This is true both at the high level of algorithms and at the low level of implementation .","label":"Future","metadata":{},"score":"54.9645"}{"text":"Text corpora which are tagged with part - o[-speech information are useful in many areas of linguistic research .In this paper , a new part - of - speech tagging method based on neural networks ( Net-7h . qger ) is presented and its performance is compared to that of a 11IvlM - tagger ( Cutting ct al . , 1992 ) anti ... \" .","label":"Future","metadata":{},"score":"55.1628"}{"text":"However , most of the benefit was due to using a varigram model instead of a full n - gram model .Their language model outperformed traditional n - gram model in both perplexity evaluation and speech recognition .An n - gram ... .","label":"Future","metadata":{},"score":"55.165405"}{"text":"+ a 10-minute talk -- that 's all it deserves until the team gets real empirical results ) .Grammar Learning .In my thesis work ( \" transformational smoothing \" ) , I took up the question of learning deep structure , showing how to learn the probabilities of transformations or lexical redundancy rules that could turn one lexicalized context - free rule ( or other lexical entry ) into another : .","label":"Future","metadata":{},"score":"55.398163"}{"text":"The random forest language model ( RFLM ) has shown encouraging results in several automatic speech recognition ( ASR ) tasks but has been hindered by practical limitations , notably the space - complexity of RFLM estimation from large amounts of data .","label":"Future","metadata":{},"score":"55.800354"}{"text":"It involves looking for the \" best \" parameters for a grammar .But how do we define \" best \" ?And how do we find the globally \" best \" parameters when the objective function has many local maxima ?","label":"Future","metadata":{},"score":"55.901688"}{"text":"Such approximations can also be reasonable theories of human performance , since in practice humans can not process more than a couple of levels of center - embedding .Finite - state machines can implement common engineering models for many tasks : speech recognition , machine translation , segmentation , normalization , part - of - speech tagging , and other markup .","label":"Future","metadata":{},"score":"56.316727"}{"text":"These notes are not comprehensive , though they include some more advanced topics and at least provide useful further references !We will present some general pseudo - Bayesian rule to aggregate estimators and show that it satisfies a non - asymptotic oracle inequality .","label":"Future","metadata":{},"score":"56.31715"}{"text":"We present a new formalism , probabilistic feature grammar ( PFG ) .PFGs combine most of the best properties of several other formalisms , including those of Collins , Magerman , and Charniak , and in experiments have comparable or better performance .","label":"Future","metadata":{},"score":"56.443382"}{"text":"We present a new formalism , probabilistic feature grammar ( PFG ) .PFGs combine most of the best properties of several other formalisms , including those of Collins , Magerman , and Charniak , and in experiments have comparable or better performance .","label":"Future","metadata":{},"score":"56.443382"}{"text":"We present a new formalism , probabilistic feature grammar ( PFG ) .PFGs combine most of the best properties of several other formalisms , including those of Collins , Magerman , and Charniak , and in experiments have comparable or better performance .","label":"Future","metadata":{},"score":"56.443382"}{"text":"We present a new formalism , probabilistic feature grammar ( PFG ) .PFGs combine most of the best properties of several other formalisms , including those of Collins , Magerman , and Charniak , and in experiments have comparable or better performance .","label":"Future","metadata":{},"score":"56.443382"}{"text":"Indubitably so ; the question is , is this it ?Foundations of Statistical Natural Language Processing ( henceforth FSNLP ) is certainly ambitious in scope .Scattered throughout are also topics fundamental to doing good experimental work in general , such as hypothesis testing , cross - validation , and baselines .","label":"Future","metadata":{},"score":"56.509064"}{"text":"Since I 'm interested not only in competence grammars , but also in robust techniques for comprehension and acquisition , statistical models form an important part of my repertoire .I tend to think of the world as a big parametric probability distribution .","label":"Future","metadata":{},"score":"56.68014"}{"text":"However , OT grammars - and even the very simple OTP grammars - are more powerful than FSTs .This makes them linguistically suspect as well as inconvenient .Directional Constraint Evaluation in Optimality Theory ( COLING 2000 ) offers a solution .","label":"Future","metadata":{},"score":"56.731453"}{"text":"Sanjeev Khudanpur ( Johns Hopkins University ) .Maximum Entropy Techniques and Exponential Models in SLM / SCL .Maximum Entropy methods provide elegant means for estimating complex probabilistic models from sparse data .They have been applied to various problems encountered in the processing of natural language including statistical language modeling , part - of - speech tagging , parsing , text segmentation and classification and machine translation .","label":"Future","metadata":{},"score":"56.92516"}{"text":"Exploiting syntactic structure for language modeling .In ACL 36/COLING 17 , pages 225 - 231 .Hirschberg , Julia .\" Every time I fire a linguist , my performance goes up , \" and other myths of the statistical natural language processing revolution .","label":"Future","metadata":{},"score":"56.99248"}{"text":"Both probabilistic context - free grammars ( PCFGs ) and shift - reduce probabilistic pushdown automata ( PPDAs ) have been used for language modeling and maximum likelihood parsing .We investigate the precise relationship between these two formalisms , showing that , while they define the same classes of pr ... \" .","label":"Future","metadata":{},"score":"57.189728"}{"text":"Both probabilistic context - free grammars ( PCFGs ) and shift - reduce probabilistic pushdown automata ( PPDAs ) have been used for language modeling and maximum likelihood parsing .We investigate the precise relationship between these two formalisms , showing that , while they define the same classes of pr ... \" .","label":"Future","metadata":{},"score":"57.189728"}{"text":"For example , the key concept of separating training and test data ( failure to do so being regarded in the community as a ' ' cardinal sin ' ' ( pg .206 ) ) appears as a subsection of the chapter on n -gram language modeling .","label":"Future","metadata":{},"score":"57.268303"}{"text":"Despite its simplicity , a product of eight automatically learned grammars improves parsing accuracy from 90.2 % to 91.8 % on English , and from 80.3 % to 84.5 % on German . \" ...We extend discriminative n - gram language modeling techniques originally proposed for automatic speech recognition to a statistical machine translation task .","label":"Future","metadata":{},"score":"57.48147"}{"text":"The grouping of topics in this paragraph , while convenient , does not correspond to the order of presentation in the book .Indeed , the way in which one thinks about a subject need not be the organization that is best for teaching it , a point to which we will return later .","label":"Future","metadata":{},"score":"57.66439"}{"text":"My first real parsing research , summers 1989 - 1992 , was a collaboration with Mark A. Jones at AT&T Bell Labs .This was early in the days of probabilistic NLP .The two of us eventually built a probabilistic left - to - right LFG parser , with a hand - built domain grammar and history - based probabilities trained from data .","label":"Future","metadata":{},"score":"57.862694"}{"text":"However , the talk will be considerably more technical , and will be specifically designed to rapidly introduce statisticians and mathematicians to the field .In this talk I present an adaptive algorithm for estimating the conditional probability of a letter knowing the past and give an \" oracle \" inequality for its conditional Kullback - Leibler risk .","label":"Future","metadata":{},"score":"58.028557"}{"text":"In recent work , my students and I have been trying to build a unified perspective on the techniques that we use in this field .Our Dyna programming language lets you write down models and algorithms at a high level .","label":"Future","metadata":{},"score":"58.069405"}{"text":"The proposed model combines trigram and structure knowledge of base phrase in which trigram is used to capture the local relation between words , while structure knowledge","label":"Future","metadata":{},"score":"58.2853"}{"text":"\" Most finite - state constructions can be easily adapted to weighted machines , but minimization is a bit harder .Since Feb. 2001 I have served as President of the ACL 's Special Interest Group for Computational Phonology ( SIGPHON ) .","label":"Future","metadata":{},"score":"58.31101"}{"text":"We give a brief introduction to the theory of weighted transducers and describe some of these algorithms .The automata used in language modeling for speech synthesis , speech understanding and speech recognition represent regular languages .Recent work in the theory of weighted automata shows that the same weighted automata as those currently used in speech processing can be used to recognize efficiently non - trivial classes of context - free languages .","label":"Future","metadata":{},"score":"58.3638"}{"text":"Most recent research in trainable part of speech taggers has explored stochastic tagging .While these taggers obtain high accuracy , linguistic information is captured indirectly , typi - cally in tens of thousands of lexical and contextual probabili - ties .","label":"Future","metadata":{},"score":"58.620308"}{"text":"Most recent research in trainable part of speech taggers has explored stochastic tagging .While these taggers obtain high accuracy , linguistic information is captured indirectly , typi - cally in tens of thousands of lexical and contextual probabili - ties .","label":"Future","metadata":{},"score":"58.620308"}{"text":"It will be argued that the main issues in using maximum entropy techniques are ( i ) the selection of features or linear constraints which the model should be required to satisfy and ( ii ) the enormous computation involved in obtaining model parameters given these constraints .","label":"Future","metadata":{},"score":"58.934937"}{"text":"In this paper , we propose the application of another sampling technique : the Perfect Sampling ( PS ) .The experiment has shown a reduction of 30 % in the perplexity of the WSME model over the trigram model and a reduc- tion of 2 % over the WSME model trained with MCMC . by Diego Linares , José - miguel Benedí , Joan - andreu Sánchez , Javeriana Cali . \" ... Abstract .","label":"Future","metadata":{},"score":"59.05698"}{"text":"In this paper , we propose the application of another sampling technique : the Perfect Sampling ( PS ) .The experiment has shown a reduction of 30 % in the perplexity of the WSME model over the trigram model and a reduc- tion of 2 % over the WSME model trained with MCMC . by Diego Linares , José - miguel Benedí , Joan - andreu Sánchez , Javeriana Cali . \" ... Abstract .","label":"Future","metadata":{},"score":"59.05698"}{"text":"Furthermore , we report and analyze the results and describe the approaches of the participating systems . ... dependent relations rather than phrases , the task of the conversion procedure is to identify and label the head - dependent pairs .","label":"Future","metadata":{},"score":"59.26128"}{"text":"Frederick Jelinek ( Department of Electrical and Computer Engineering , Johns Hopkins University ) jelinek@jhu.edu .Parser - Driven Language Modeling .he talk will describe two current approaches to parser - driven language modeling .A bottom - up approach due to Chelba and Jelinek ( the Structured Language Model - SLM ) and a top - down approach due to Brian Roark .","label":"Future","metadata":{},"score":"59.31498"}{"text":"A parser that is biased toward short dependencies can be faster and sometimes more accurate , as shown by experiments .In fact , placing a hard limit on dependency length allows O(n ) ( linear - time ) parsing , by modifying the algorithms above .","label":"Future","metadata":{},"score":"59.503464"}{"text":"Lillian Lee is an assistant professor in the Computer Science Department at Cornell University .Together with John Lafferty , she has led two AAAI tutorials on statistical methods in natural language processing .She received the Stephen and Marilyn Miles Excellence in Teaching Award in 1999 from Cornell 's College of Engineering .","label":"Future","metadata":{},"score":"61.921127"}{"text":"We find that sample selection can significantly reduce the size of annotated training corpora and that uncertainty is a robust predictive criterion that can be easily applied to different learning models . ource sentence is a short one , the decoder will never be able to find it , for the hypotheses leading to it have been pruned permanently .","label":"Future","metadata":{},"score":"62.13784"}{"text":"We find that sample selection can significantly reduce the size of annotated training corpora and that uncertainty is a robust predictive criterion that can be easily applied to different learning models . ource sentence is a short one , the decoder will never be able to find it , for the hypotheses leading to it have been pruned permanently .","label":"Future","metadata":{},"score":"62.13784"}{"text":"We give experimental results showing that , beginning with a high - performance PCFG , one can develop a pseudo PCSG that yields significant performance gains .Analysis shows that the benefits from the context - sensitive statistics are localized , suggesting that we can use them to extend the original PCFG .","label":"Future","metadata":{},"score":"62.311153"}{"text":"And how can a speaker manage to sift through an infinite set to find the best pronunciation ?The two questions are related .Only after pinning down the formalism can one say much about computation .So I tried to do both : .","label":"Future","metadata":{},"score":"62.35852"}{"text":"Statistical Language Models estimate the distribution of various natural language phenomena for the purpose of speech recognition and other language technologies .Since the first significant model was proposed in 1980 , many attempts have been made to improve the state of the art .","label":"Future","metadata":{},"score":"62.57229"}{"text":"Grammar learning is always a great problem .Unfortunately , in OT , it is hard ( coNP - complete ) even to check whether a given grammar generates the observed pronunciation .The learning problem of finding such a grammar is even harder .","label":"Future","metadata":{},"score":"62.923836"}{"text":"The Maximum Entropy principle ( ME ) is an ap- propriate framework for combining information of a diverse nature from several sources into the same language model .In order to incorporate long - distance information into the ME framework in a language model , a Whole Sentence Maximum Entropy Language Model ( WSME ) could be used .","label":"Future","metadata":{},"score":"64.42905"}{"text":"The Maximum Entropy principle ( ME ) is an ap- propriate framework for combining information of a diverse nature from several sources into the same language model .In order to incorporate long - distance information into the ME framework in a language model , a Whole Sentence Maximum Entropy Language Model ( WSME ) could be used .","label":"Future","metadata":{},"score":"64.42905"}{"text":"Foundations of Statistical Natural Language Processing Christopher D. Manning and Hinrich Schütze Stanford University and Xerox PARC Cambridge , MA : The MIT Press , 1999 , xxxvii + 680 pp .Hardbound , ISBN 0 - 262 - 13360 - 1 , $ 60.00 .","label":"Future","metadata":{},"score":"64.44019"}{"text":"Mark Johnson ( Department of Cognitive and Linquistic Sciences , Brown University ) mj@lx.cog.brown.edu .An Introduction to Probabilistic Grammars and their Applications pdf postscript .This talk surveys the roles of grammars in computational linguistics , and gives a high - level description of HMMs , PCFGs and stochastic unification grammars .","label":"Future","metadata":{},"score":"65.054276"}{"text":"Although Hastings constructions , especially the Gibbs sampler ( aka the heat bath algorithm ) , still predominate , there have also been some useful advances in the design of algorithms , such as reversible jumps .The talk provides an introduction to MCMC for Bayesian computation .","label":"Future","metadata":{},"score":"65.17235"}{"text":"Lexical attraction is defined as the likelihood of such relations .I introduce a new class of probabilistic language models named lexical attraction models which can represent long distance relations between words and I formalize this new class of models using information theory .","label":"Future","metadata":{},"score":"65.28404"}{"text":"Normal - Form Parsing .In unrelated work , en route to grad school I devised a normal - form parsing algorithm for Combinatory Categorial Grammar ( CCG ) , whose correctness I proved the following year .Rather than falling prey to \" spurious ambiguity \" ( the bane of CCG ) , it was guaranteed to find exactly one parse from every semantic equivalence class : .","label":"Future","metadata":{},"score":"65.47377"}{"text":"Lexical decoding is the obtaining of the most probable sequence of categories associated to a sequence of words .This paper describes two lexical decoding combined models which are based on a stochastic category - based model and a probabilistic model of word distribution into linguistic categories .","label":"Future","metadata":{},"score":"65.591034"}{"text":"Clustering ( unsupervised classification ) undergoes the same disjointed treatment , appearing both in Chapter 7 and 14 .On a related note , the level of mathematical detail fluctuates in certain places .In general , the book tends to present helpful calculations ; however , some derivations that would provide crucial motivation and clarification have been omitted .","label":"Future","metadata":{},"score":"65.74829"}{"text":"Relations are more powerful than languages or functions .Learning of finite - state machines seems particularly important .Currently I 'm focusing on the case of hand - built probabilistic machines with learnable parameters .Besides giving the first EM algorithm for the basic case of learning independent arc probabilities , I showed how my solution generalized to more complex parameterizations .","label":"Future","metadata":{},"score":"65.78004"}{"text":"I gave him Charniak ( 1993 ) .In the interim , the second edition of Allen 's book did include some material on probabilistic methods , and much of Jelinek 's Statistical Methods for Speech Recognition ( 1997 ) concerns language processing .","label":"Future","metadata":{},"score":"66.11444"}{"text":"The estimation processes of the models are described in detail .Finally , experiments on the Wall Street Journal corpus are reported . \" ...The Maximum Entropy principle ( ME ) is an ap- propriate framework for combining information of a diverse nature from several sources into the same language model .","label":"Future","metadata":{},"score":"66.26752"}{"text":"Finally , experiments on the Wall Street Journal corpus are reported . \" ...The Maximum Entropy principle ( ME ) is an ap- propriate framework for combining information of a diverse nature from several sources into the same language model .","label":"Future","metadata":{},"score":"66.60907"}{"text":"Most of the material we plan to explain can be found in written form in the following communication : . O. Catoni Data compression and adaptive histograms , International Conference on Foundations of Computational Mathematics in honor of Professor Steve Smale 's 70th Birthday July 13 - 17 2000 .","label":"Future","metadata":{},"score":"67.65436"}{"text":"The availability of on - line corpora is rapidly changing the field of natural language processing ( NLP ) from one dominated by theoretical models of often very specific linguistic phenomena to one guided by computational models that simultaneously account for a wide variety of phenomena that occur in real - world text .","label":"Future","metadata":{},"score":"67.993515"}{"text":"The algorithm is applied to 7 major types of ambiguity where context can be used to choose a word 's pronunciation . ... versesHe covered the hull with lead ) .In general , we will resolve the part - of - speech ambiguitysrst , and then resolve the additional semantic ambiguity if present .","label":"Future","metadata":{},"score":"68.10501"}{"text":"Our target language is Finnish , and in order to evade the problems of its rich morphology , we use sub - word units , morphs , as model units instead of the words .In the proposed model we apply incremental growing and clustering of the morph n - gram histories .","label":"Future","metadata":{},"score":"68.33812"}{"text":"This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar ( CCG ) derivations augmented with local and long - range word - word dependencies .The resulting corpus , CCGbank , includes 99.4 % of the sentences in the Penn Treebank .","label":"Future","metadata":{},"score":"68.41493"}{"text":"This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar ( CCG ) derivations augmented with local and long - range word - word dependencies .The resulting corpus , CCGbank , includes 99.4 % of the sentences in the Penn Treebank .","label":"Future","metadata":{},"score":"68.41493"}{"text":"This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar ( CCG ) derivations augmented with local and long - range word - word dependencies .The resulting corpus , CCGbank , includes 99.4 % of the sentences in the Penn Treebank .","label":"Future","metadata":{},"score":"68.41493"}{"text":"This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar ( CCG ) derivations augmented with local and long - range word - word dependencies .The resulting corpus , CCGbank , includes 99.4 % of the sentences in the Penn Treebank .","label":"Future","metadata":{},"score":"68.41493"}{"text":"The original Metropolis ( 1953 ) construction , generalized by Hastings ( 1970 ) , was used extensively in the analysis of interacting particle systems such as Ising and Potts models .Subsequent developments included cluster and multigrid algorithms in the 1980 's and simulated tempering and coupling from the past in the 1990 's .","label":"Future","metadata":{},"score":"68.66771"}{"text":"340 ) is akin to snubbing computer science because it only deals with zeros and ones .Allen , James .Natural Language Understanding .Benjamin Cummins , second edition .Charniak , Eugene .Statistical Language Learning .MIT Press .","label":"Future","metadata":{},"score":"68.892815"}{"text":"Parser development is generally viewed as a primarily linguistic enterprise .A grammarian examines sentences , skillfully extracts the linguistic generalizations evident in the data , and writes grammar rules which cover the language .The grammarian . ... the 1100 sentences .","label":"Future","metadata":{},"score":"69.1601"}{"text":"Parser development is generally viewed as a primarily linguistic enterprise .A grammarian examines sentences , skillfully extracts the linguistic generalizations evident in the data , and writes grammar rules which cover the language .The grammarian . ... the 1100 sentences .","label":"Future","metadata":{},"score":"69.1601"}{"text":"This work concerns building n - gram language models that are suitable for large vocabulary speech recognition in devices that have a restricted amount of memory and space available .Our target language is Finnish , and in order to evade the problems of its rich morphology , we use sub - word units , morph ... \" .","label":"Future","metadata":{},"score":"72.70345"}{"text":"The presentation will conclude with a discussion of intrinsic trade - offs and open issues in this emerging area of statistical language modeling .Markov chain Monte Carlo ( MCMC ) refers to a collection of methods for closely approximating integrals with respect to awkward , often very high - dimensional , probability distributions .","label":"Future","metadata":{},"score":"74.38379"}{"text":"Finite - state machines are a pleasure to work with : they are efficient , flexible , scalable , and easy to design .The finite - state machines used for language are generalizations of the usual FSAs and HMMs .They can produce output as well as reading input , and they can do so nondeterministically .","label":"Future","metadata":{},"score":"76.2298"}{"text":"The intersection of these constraints is the set of probability functions which are consistent with all the information sources .The function with the highest entropy within that set is the ME solution .ME takes all the previous words as possible features , so training ME model is computational challenging and sometimes almost infeasible .","label":"Future","metadata":{},"score":"76.87358"}{"text":"The intersection of these constraints is the set of probability functions which are consistent with all the information sources .The function with the highest entropy within that set is the ME solution .ME takes all the previous words as possible features , so training ME model is computational challenging and sometimes almost infeasible .","label":"Future","metadata":{},"score":"76.87361"}{"text":"I can not help but remember , in concluding , that I once read a review that said something like the following : ' 'I know you 're going to see this movie .It does n't matter what my review says .","label":"Future","metadata":{},"score":"78.14563"}{"text":"Temperamentally I 'm more of a mathematician than an engineer .That is not a value judgment about engineering , nor a scientific judgment that human cognition is neat rather than scruffy .It 's just a research style .It means that I prefer formalisms and algorithms that are clean enough to be easily communicated , analyzed , and modified .","label":"Future","metadata":{},"score":"81.768616"}{"text":"OT has eclipsed previous approaches to phonology .It says that when you 're trying to pronounce a word ( \" resign \" or \" resignation \" ) , you consider all possible pronunciations and pick the best one .A grammar in OT simply specifies the criteria on which pronunciations are judged , and especially the relative importance of those criteria , which are known as violable constraints .","label":"Future","metadata":{},"score":"82.253296"}{"text":"This is because the \" transformation models \" that I developed in the thesis can be regarded as probabilistic finite - state automata .Most arcs are epsilon - arcs ( hence there are many epsilon - cycles ) , and the arc probabilities have a log - linear parameterization .","label":"Future","metadata":{},"score":"87.89653"}{"text":"Instead he was born in London Ontario , where he grew up on a strawberry farm .He attended the University of Waterloo where he re - ceived a Bachelors of Mathematics with a joint degree in Pure Mathematics and Com - puter Science in the spring of 1987 .","label":"Future","metadata":{},"score":"88.854706"}{"text":"Instead he was born in London Ontario , where he grew up on a strawberry farm .He attended the University of Waterloo where he re - ceived a Bachelors of Mathematics with a joint degree in Pure Mathematics and Com - puter Science in the spring of 1987 .","label":"Future","metadata":{},"score":"88.854706"}{"text":"This document was generated by editing the output of the LaTeX 2 HTML translator Version 98.1p1 release ( March 2nd , 1998 ) .Copyright © 1993 , 1994 , 1995 , 1996 , 1997 , Nikos Drakos , Computer Based Learning Unit , University of Leeds .","label":"Future","metadata":{},"score":"90.26991"}{"text":"What better way to wipe the slate clear than by going to graduate school at the University of Toronto , but not without first spending the sum - mer in Europe .After spending two months in countries where he could n't speak the language , Peter became fascinated by language , and so decided to give computational linguistics a try .","label":"Future","metadata":{},"score":"91.67445"}{"text":"Peter Heeman was born October 22 , 1963 , and much to his dismay his parents had already moved away from Toronto .Instead he was born in London Ontario , where he grew up on a strawberry farm .He attended the University of Waterloo where he re - ceived a Bachelors of Mathematics with a joint degree in Pu ... \" .","label":"Future","metadata":{},"score":"92.40669"}{"text":"Peter Heeman was born October 22 , 1963 , and much to his dismay his parents had already moved away from Toronto .Instead he was born in London Ontario , where he grew up on a strawberry farm .He attended the University of Waterloo where he re - ceived a Bachelors of Mathematics with a joint degree in Pu ... \" .","label":"Future","metadata":{},"score":"92.40669"}{"text":"What better way to wipe the slate clear than by going to graduate school at the University of Toronto , but not without first spending the sum - mer in Europe .After spending two months in countries where he could n't speak the language , Peter became fascinated by language , and so decided to give computational linguistics a try . \" ...","label":"Future","metadata":{},"score":"93.891914"}