{"text":"Although the generalization performance of generative models can often be improved by ' training them discriminatively ' , they can then no longer make use of unlabelled data .In an attempt to gain the benefit of both generative and discriminative approaches , heuristic procedure have been proposed [ 2 , 3 ] which interpolate between these two extremes by taking a convex combination of the generative and discriminative objective functions .","label":"Future","metadata":{},"score":"29.599997"}{"text":"This paper proposes a method for learning a discriminative parser for machine translation reordering using only aligned parallel text .This is done by treating the parser 's derivation tree as a latent variable in a model that is trained to maximize reordering accuracy .","label":"Future","metadata":{},"score":"31.542107"}{"text":"This paper proposes a method for learning a discriminative parser for machine translation reordering using only aligned parallel text .This is done by treating the parser 's derivation tree as a latent variable in a model that is trained to maximize reordering accuracy .","label":"Future","metadata":{},"score":"31.542107"}{"text":"We apply this technique to part - of - speech induction , grammar induction , word alignment , and word segmentation , incorporating a few linguistically - motivated features into the standard generative model for each task .These feature - enhanced models each outperform their basic counterparts by a substantial margin , and even compete with and surpass more complex state - of - the - art models . ... references of heads .","label":"Future","metadata":{},"score":"34.19348"}{"text":"Unlike existing preordering models , we train feature - rich discriminative classifiers that directly predict the target - side word order .Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long - distance reorderings using the structure of the parse tree , while utilizing a discriminative model with a rich set of features , including lexical features .","label":"Future","metadata":{},"score":"34.566525"}{"text":"Our method does not assume any knowledge about the target language ( in particular no tagging dictionary is assumed ) , making it applicable to a wide array of resource - poor languages .We use graph - based label propagation for cross - lingual knowledge transfer and use the projected labels as features in an unsupervised model ( Berg - Kirkpatrick et al .","label":"Future","metadata":{},"score":"34.609127"}{"text":"Latent variable grammars take an observed ( coarse ) treebank and induce more fine - grained grammar categories , that are better suited for modeling the syntax of natural languages .Estimation can be done in a generative or a discriminative framework , and results in the best published parsing accuracies over a wide range of syntactically divergent languages and domains .","label":"Future","metadata":{},"score":"34.718334"}{"text":"From this viewpoint , generative and discriminative models correspond to specific choices for the prior over parameters .As well as giving a principled interpretation of ' discriminative training ' , this approach opens door to very general ways of interpolating between generative and discriminative extremes through alternative choices of prior .","label":"Future","metadata":{},"score":"35.793343"}{"text":"Therefore , this paper explores the application of a framebased discriminative training objective for adaptation .It presents evaluations for supervised as well as for unsupervised adaption on the 1993 WSJ adaptation tests of native and non - native speakers .Relative improvements in word error rate of up to 25 % could be measured compared to the MLLR adapted recognition systems .","label":"Future","metadata":{},"score":"35.938866"}{"text":"Our results show that , when the supply of labelled training data is limited , the optimum performance corresponds to a balance between the purely generative and the purely discriminative .uch interest in ' discriminative training ' of generative models [ 2 , 3 , 12 ] with a view to improving classification accuracy .","label":"Future","metadata":{},"score":"36.032944"}{"text":"The primary reason for its difficulty is that in order to induce plausible grammars , the underlying model must be capable of representing the intricacies of language while also ensuring that it can be readily learned from data .The majority of existing work on grammar induction has favoured model simplicity ( and thus learnability ) over representational capacity by using context free grammars and first order dependency grammars , which are not sufficiently expressive to model many common linguistic constructions .","label":"Future","metadata":{},"score":"36.39843"}{"text":"We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .We propose using corpus - level statistics for lexicon learning decisions .","label":"Future","metadata":{},"score":"36.466087"}{"text":"In addition , our discriminative approach integrally admits features beyond local tree configurations .We present a multi - scale training method along with an efficient CKY - style dynamic program .On a variety of domains and languages , this method produces the best published parsing accuracies with the smallest reported grammars .","label":"Future","metadata":{},"score":"36.7325"}{"text":"We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system .We use a corpus of weakly - labeled reference reorderings to guide parser training .Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress .","label":"Future","metadata":{},"score":"37.527153"}{"text":"We describe a method for prediction of linguistic structure in a language for which only unlabeled data is available , using annotated data from a set of one or more helper languages .Our approach is based on a model that locally mixes between supervised models from the helper languages .","label":"Future","metadata":{},"score":"37.565727"}{"text":"We describe a method for prediction of linguistic structure in a language for which only unlabeled data is available , using annotated data from a set of one or more helper languages .Our approach is based on a model that locally mixes between supervised models from the helper languages .","label":"Future","metadata":{},"score":"37.565727"}{"text":"In particular we note the effects of two comparatively recent techniques for parser improvement .Then a reranking phase uses more detailed features , features which would ( mostly ) be ... . \" ...We formulate dependency parsing as a graphical model with the novel ingredient of global constraints .","label":"Future","metadata":{},"score":"37.692772"}{"text":"In our model , arbitrary , nonindependent features may be freely incorporated , thereby overcoming the inherent limitation of generative models , which require that features be sensitive to the conditional independencies of the generative process .However , unlike previous work on discriminative modeling of word alignment ( which also permits the use of arbitrary features ) , the parameters in our models are learned from unannotated parallel sentences , rather than from supervised word alignments .","label":"Future","metadata":{},"score":"37.77117"}{"text":"In our model , arbitrary , nonindependent features may be freely incorporated , thereby overcoming the inherent limitation of generative models , which require that features be sensitive to the conditional independencies of the generative process .However , unlike previous work on discriminative modeling of word alignment ( which also permits the use of arbitrary features ) , the parameters in our models are learned from unannotated parallel sentences , rather than from supervised word alignments .","label":"Future","metadata":{},"score":"37.77117"}{"text":"Unlike previous work , our final model does not require any additional resources at run - time .Compared to a state - of - the - art approach , we achieve more than 20 % relative error reduction .Additionally , we annotate a corpus of search queries with part - of - speech tags , providing a resource for future work on syntactic query analysis .","label":"Future","metadata":{},"score":"37.89233"}{"text":"We apply this idea to dependency and constituent parsing , generating results that surpass state - of - theart ... \" .We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers .","label":"Future","metadata":{},"score":"37.96006"}{"text":"Our approach is based on a model that locally mixes between supervised models from the helper languages .Parallel dat ... \" .We describe a method for prediction of linguistic structure in a language for which only unlabeled data is available , using annotated data from a set of one or more helper languages .","label":"Future","metadata":{},"score":"37.9989"}{"text":"Our model produces state - of - theart results on the task of unsupervised grammar induction , improving over the best previous work by almost 10 percentage points . \" ...We present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages .","label":"Future","metadata":{},"score":"38.35205"}{"text":"We show that the automatically induced latent variable grammars of Petrov et al .2006 vary widely in their underlying representations , depending on their EM initialization point .We use this to our advantage , combining multiple automatically learned grammars into an unweighted product model , which gives significantly improved performance over state - of - the - art individual grammars .","label":"Future","metadata":{},"score":"38.480583"}{"text":"To resolve conflicts in shift - reduce parsing , we propose a maximum entropy model trained on the derivation graph of training data .As our approach combines the merits of phrase - based and string - todependency models , it achieves significant improvements over the two baselines on the NIST Chinese - English datasets .","label":"Future","metadata":{},"score":"38.644325"}{"text":"Unlike previous approaches , our framework does not require full projected parses , allowing partial , approximate transfer through linear expectation constraints on the space of distributions over trees .We consider several types of constraints that range from generic dependency conservation to language - specific annotation rules for auxiliary verb analysis .","label":"Future","metadata":{},"score":"38.75553"}{"text":"Unlike previous approaches , our framework does not require full projected parses , allowing partial , approximate transfer through linear expectation constraints on the space of distributions over trees .We consider several types of constraints that range from generic dependency conservation to language - specific annotation rules for auxiliary verb analysis .","label":"Future","metadata":{},"score":"38.75553"}{"text":"Our best results show a 26-fold speedup compared to a sequential C implementation .We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data .We first demonstrate that delexicalized parsers can be directly transferred between languages , producing significantly higher accuracies than unsupervised parsers .","label":"Future","metadata":{},"score":"39.234283"}{"text":"We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative improvement over the baseline approach that uses a fixed context window of adjacent words .","label":"Future","metadata":{},"score":"39.40661"}{"text":"We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative improvement over the baseline approach that uses a fixed context window of adjacent words .","label":"Future","metadata":{},"score":"39.40661"}{"text":"( 1993 ) .In our model , arbitrary , nonindependent features may be freely incorporated , thereby overcoming the inherent limitation of generative models , which requ ... \" .We introduce a discriminatively trained , globally normalized , log - linear variant of the lexical translation models proposed by Brown et al .","label":"Future","metadata":{},"score":"39.44439"}{"text":"( 1993 ) .In our model , arbitrary , nonindependent features may be freely incorporated , thereby overcoming the inherent limitation of generative models , which requ ... \" .We introduce a discriminatively trained , globally normalized , log - linear variant of the lexical translation models proposed by Brown et al .","label":"Future","metadata":{},"score":"39.44439"}{"text":"Previous sentence segmentation systems have typically been very local , using low - level prosodic and lexical features to independently decide whether or not to segment at each word boundary position .In this work , we leverage global syntactic information from a syn- tactic parser , which is better able to capture long distance depen- dencies .","label":"Future","metadata":{},"score":"39.48672"}{"text":"Second , how can we efficiently infer optimal structures within them ?Hierarchical coarse - to - fine methods address both questions .Coarse - to - fine approaches exploit a sequence of models which introduce complexity gradually .At the top of the sequence is a trivial model in which learning and inference are both cheap .","label":"Future","metadata":{},"score":"39.909473"}{"text":"Nonetheless , the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks .We demonstrate that log - linear grammars with latent variables can be practically trained using discriminative methods .Central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectations to be efficiently approximated in a gradient - based procedure .","label":"Future","metadata":{},"score":"39.98137"}{"text":"We highlight the use of this resource via two experiments , including one that reports competitive accuracies for unsupervised grammar induction without gold standard part - of - speech tags .We present an online learning algorithm for training structured prediction models with extrinsic loss functions .","label":"Future","metadata":{},"score":"40.017075"}{"text":"Because each refinement introduces only limited complexity , both learning and inference can be done in an incremental fashion .In this dissertation , we describe several coarse - to - fine systems .In the domain of syntactic parsing , complexity is in the grammar .","label":"Future","metadata":{},"score":"40.02527"}{"text":"We then present a novel procedure for feature selection , which exploits discrepancies between the existing model and the training corpus .We demonstrate our ideas by constructing and analyzing competitive models in the Switchboard domain , incorporating lexical and syntact ... . ... lity or semantic coherence is awkward to encode in a conditional framework .","label":"Future","metadata":{},"score":"40.24344"}{"text":"Finally , we conduct a multi - lingual evaluation that demonstrates the robustness of the overall structured neural approach , as well as the benefits of the extensions proposed in this work .Our research further demonstrates the breadth of the applicability of neural network methods to dependency parsing , as well as the ease with which new features can be added to neural parsing models .","label":"Future","metadata":{},"score":"40.526817"}{"text":"To limit the model 's complexity we employ a Bayesian non - parametric prior which biases the model towards a sparse grammar with shallow productions .We demonstrate the model 's efficacy on supervised phrase - structure parsing , where we induce a latent segmentation of the training treebank , and on unsupervised dependency grammar induction .","label":"Future","metadata":{},"score":"40.61923"}{"text":"Our approach combines the strengths of lexical reordering and syntactic preordering models by pe ... \" .We present a simple and novel classifier - based preordering approach .Unlike existing preordering models , we train feature - rich discriminative classifiers that directly predict the target - side word order .","label":"Future","metadata":{},"score":"40.674652"}{"text":"In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"Future","metadata":{},"score":"40.85354"}{"text":"In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"Future","metadata":{},"score":"40.85354"}{"text":"Inducing a grammar from text has proven to be a notoriously challenging learning task despite decades of research .The primary reason for its difficulty is that in order to induce plausible grammars , the underlying model must be capable of representing the intricacies of language while also ensuring ... \" .","label":"Future","metadata":{},"score":"40.932056"}{"text":"The application of discriminative techniques for adaptation will be described .Finally , possible future avenues of research will be given . ing to the transcription wr of observation Or .MLE also offers the theoretical advantage that if certain modeling assumptions hold , no other training criteria will do better ; MLE is a minimum variance , consistent estimator of the true model para ... . \" ...","label":"Future","metadata":{},"score":"40.977386"}{"text":"The system consists of two components : an unlabeled dependency parser using Gibbs sampling which can incorporate sentence - level ( global ) features as well as token - leve ... \" .In this paper , we describe a two - stage multilingual dependency parser used for the multilingual track of the CoNLL 2007 shared task .","label":"Future","metadata":{},"score":"41.000835"}{"text":"The system consists of two components : an unlabeled dependency parser using Gibbs sampling which can incorporate sentence - level ( global ) features as well as token - leve ... \" .In this paper , we describe a two - stage multilingual dependency parser used for the multilingual track of the CoNLL 2007 shared task .","label":"Future","metadata":{},"score":"41.000835"}{"text":"A prerequisite for developing broad coverage parsers for more languages is the annotation of text with the desired linguistic representations ( also known as \" treebanking \" ) .However , syntactic annotation is a labor in ... \" .Broad coverage , high quality parsers are available for only a handful of languages .","label":"Future","metadata":{},"score":"41.047165"}{"text":"We describe a novel approach for inducing unsupervised part - of - speech taggers for languages that have no labeled training data , but have translated text in a resource - rich language .Our method does not assume any knowledge about the target language ( in particular no tagging dictionary is assumed ) , making it applicable to a wide array of resource - poor languages .","label":"Future","metadata":{},"score":"41.057228"}{"text":"The annotations are produced automatically with statistical models that are specifically adapted to historical text .The corpus will facilitate the study of linguistic trends , especially those related to the evolution of syntax .Syntactic analysis of search queries is important for a variety of information- retrieval tasks ; however , the lack of annotated data makes training query analysis models difficult .","label":"Future","metadata":{},"score":"41.17141"}{"text":"Our first- , second- , and third - order models achieve accuracies comparable to those of their unpruned counterparts , while exploring only a fraction of the search space .We observe speed - ups of up to two orders of magnitude compared to exhaustive search .","label":"Future","metadata":{},"score":"41.18825"}{"text":"We show how features can easily be added to standard generative models for unsupervised learning , without requiring complex new training methods .In particular , each component multinomial of a generative model can be turned into a miniature logistic regression model if feature locality permits .","label":"Future","metadata":{},"score":"41.241745"}{"text":"We show how features can easily be added to standard generative models for unsupervised learning , without requiring complex new training methods .In particular , each component multinomial of a generative model can be turned into a miniature logistic regression model if feature locality permits .","label":"Future","metadata":{},"score":"41.241745"}{"text":"A second group of papers does parsing by a sequence of independent , discriminative decisions , either greedily or with use of a small beam ( Ratnaparkhi , 1997 ; Henderson , 2004 ) .This paper extends th ...Syntactic parsing is a fundamental problem in computational linguistics and natural language processing .","label":"Future","metadata":{},"score":"41.265854"}{"text":"Standard inference can be used at test time .Our approach is able to scale to very large problems and yields significantly improved target domain accuracy .It is well known that parsing accuracies drop significantly on out - of - domain data .","label":"Future","metadata":{},"score":"41.26713"}{"text":"However , for large - scale applications such as object recognition , hand labelling of data is expensive , and there is much interest in semi - supervised techniques ba ... \" .When labelled training data is plentiful , discriminative techniques are widely used since they give excellent generalization performance .","label":"Future","metadata":{},"score":"41.318794"}{"text":"This paper presents novel improvements to the induction of translation lexicons from monolingual corpora using multilingual dependency parses .We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative ... \" .","label":"Future","metadata":{},"score":"41.341606"}{"text":"This paper presents novel improvements to the induction of translation lexicons from monolingual corpora using multilingual dependency parses .We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative ... \" .","label":"Future","metadata":{},"score":"41.341606"}{"text":"This simple framework performs surprisingly well , giving accuracy results competitive with the state - of - the - art on all the tasks we consider .The computational simplicity of the decoder and training algorithm leads to significantly higher test speeds and lower training times than their main alternatives , including log - linear and large - margin training algorithms and dynamic - programming for decoding .","label":"Future","metadata":{},"score":"41.352356"}{"text":"This simple framework performs surprisingly well , giving accuracy results competitive with the state - of - the - art on all the tasks we consider .The computational simplicity of the decoder and training algorithm leads to significantly higher test speeds and lower training times than their main alternatives , including log - linear and large - margin training algorithms and dynamic - programming for decoding .","label":"Future","metadata":{},"score":"41.352356"}{"text":"We present experiments with sequence models on part - of - speech tagging and named entity recognition tasks , and with syntactic parsers on dependency parsing and machine translation reordering tasks .Low - latency solutions for syntactic parsing are needed if parsing is to become an integral part of user - facing natural language applications .","label":"Future","metadata":{},"score":"41.44574"}{"text":"Finally , we present multilingual experiments which show that parsing with hierarchical state - splitting is fast and accurate in multiple languages and domains , even without any language - specific tuning . ... g ( Matsuzaki et al . , 2005 ; Petrov et al . , 2006 ) .","label":"Future","metadata":{},"score":"41.637894"}{"text":"Some of this improvement is from training , but more than half is from parsing with induced constraints , in inference .Punctuation - aware decoding works with existing ( even already - trained ) parsing models and always increased accuracy in our experiments . ... mplementation , extension , understanding and debugging . \" ...","label":"Future","metadata":{},"score":"41.81594"}{"text":"However , syntactic annotation is a labor intensive and time - consuming process , and it is difficult to find linguistically annotated text in sufficient quantities .In this article , we explore using parallel text to help solving the problem of creating syntactic annotation in more languages .","label":"Future","metadata":{},"score":"41.847794"}{"text":"Recently , Sutskever et al .( 2014 ) presented a task - agnostic method for learning to map input sequences to output sequences that achieved strong results on a large scale machine translation problem .In this work , we show that precisely the same sequence - to - sequence method achieves results that are close to state - of - the - art on syntactic constituency parsing , whilst making almost no assumptions about the structure of the problem .","label":"Future","metadata":{},"score":"41.920776"}{"text":"We also show that our techniques can be applied to full - scale parsing applications by demonstrating its effectiveness in learning state - split grammars .Treebank parsing can be seen as the search for an optimally refined grammar consistent with a coarse training treebank .","label":"Future","metadata":{},"score":"42.186317"}{"text":"We present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages .Our method uses a single set of manually - specified language - independent rules that identify syntactic dependencies between pairs of syntactic categories that commonly occur across languages .","label":"Future","metadata":{},"score":"42.198772"}{"text":"In particular , we introduce a linkage structure over the latent themes to encode the dependencies of the patches .This structure enforces the semantic connections among the patches by facilitating better clustering of the themes .As a result , our models for object categories tend to be more discriminative than the ones obtained under the independent patch assumption .","label":"Future","metadata":{},"score":"42.45253"}{"text":"Their symbolic component is amenable to inspection by humans , while their probabilistic component helps resolve ambiguity .They also permit the use of well - understood , generalpurpose learn ... \" .Probabilistic grammars offer great flexibility in modeling discrete sequential data like natural language text .","label":"Future","metadata":{},"score":"42.465126"}{"text":"Their symbolic component is amenable to inspection by humans , while their probabilistic component helps resolve ambiguity .They also permit the use of well - understood , generalpurpose learn ... \" .Probabilistic grammars offer great flexibility in modeling discrete sequential data like natural language text .","label":"Future","metadata":{},"score":"42.465126"}{"text":"Their symbolic component is amenable to inspection by humans , while their probabilistic component helps resolve ambiguity .They also permit the use of well - understood , generalpurpose learn ... \" .Probabilistic grammars offer great flexibility in modeling discrete sequential data like natural language text .","label":"Future","metadata":{},"score":"42.465126"}{"text":"The basic theory of discriminative training will be discussed and an explanation of maximum mutual information ( MMI ) given .Common problems inherent to discriminative training will be explored as well as practicalities associated with implementing discriminative training for large vocabulary recognition .","label":"Future","metadata":{},"score":"42.47956"}{"text":"However , this previous work relied on translation grammars constructed using standard generative word alignment processes .7 Future Work While we have demonstrated that this ... . \" ...Syntax - based translation models should in principle be efficient with polynomially - sized search space , but in practice they are often embarassingly slow , partly due to the cost of language model integration .","label":"Future","metadata":{},"score":"42.510345"}{"text":"Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"Future","metadata":{},"score":"42.58199"}{"text":"Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"Future","metadata":{},"score":"42.58199"}{"text":"However , it is well known , that discriminative training objectives outperform Maximum Likelihood training approaches , espe ... \" .Maximum Likelihood Linear Regression ( MLLR ) has become the most popular approach for adapting speakerindependent Hidden Markov Models to a specic speaker 's characteristics .","label":"Future","metadata":{},"score":"42.75065"}{"text":"Instead , the phylogenetic prior couples languages at a parameter level .Joint induction in the multilingual model substantially outperforms independent learning , with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages .Across eight languages , the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1 % and reaching as high as 39 % . ... inally given as simple multinomial distributions with one parameter per outcome . \" ...","label":"Future","metadata":{},"score":"42.913876"}{"text":"To globally model parsing actions of all steps that are taken on the inpu ... \" .Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .They only determine parsing actions stepwisely by a trained classifier .","label":"Future","metadata":{},"score":"43.02499"}{"text":"To globally model parsing actions of all steps that are taken on the inpu ... \" .Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .They only determine parsing actions stepwisely by a trained classifier .","label":"Future","metadata":{},"score":"43.02499"}{"text":"The dissertation shows first that the performance degradation occuring as speech becomes more conversational is severe and is partially attributable to differences in the acoustic realizations of sentences .Hypothesizing that the quantifiably wider range of .Such parameters might ... . ... gated which consider the likelihood of competing hypotheses and explicitly model the performance metrics in the criteria .","label":"Future","metadata":{},"score":"43.126644"}{"text":"In contrast with most existing models , our approach captures the intrinsic uncertainty in the number and identity o ... \" .Motivated by the problem of learning to detect and recognize objects with minimal supervision , we develop a hierarchical probabilistic model for the spatial structure of visual scenes .","label":"Future","metadata":{},"score":"43.219654"}{"text":"In our method the first , monolingual view consists of supervised predictors learned separately for each language .The second , bilingual view consists of log - linear predictors learned over both languages on bilingual text .Our training procedure estimates the parameters of the bilingual model using the output of the monolingual model , and we show how to combine the two models to account for dependence between views .","label":"Future","metadata":{},"score":"43.358234"}{"text":"We discuss how the general framework is applied to each of the problems studied in this article , making comparisons with alternative learning and decoding algorithms .We also show how the comparability of candidates considered by the beam is an important factor in the performance .","label":"Future","metadata":{},"score":"43.458862"}{"text":"We discuss how the general framework is applied to each of the problems studied in this article , making comparisons with alternative learning and decoding algorithms .We also show how the comparability of candidates considered by the beam is an important factor in the performance .","label":"Future","metadata":{},"score":"43.458862"}{"text":"However , we explain that both algorithms optimize the wrong objectives and prove that there are fundamental disconnects between the likelihoods of sentences , best parses , and true parses , beyond the wellestablished discrepancies between likelihood , accuracy and extrinsic performance . ... erarchical syntactic structure from natural language text .","label":"Future","metadata":{},"score":"43.549652"}{"text":"Our system relies on new methods for discriminative training with partially labeled data .We combine a margin - sensitive approach for data - mining hard negative examples with a formalism we call latent SVM .A latent SVM is a reformulation of MI - SVM in terms of latent variables .","label":"Future","metadata":{},"score":"43.592453"}{"text":"er we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .These parameters correspond to local cases wher ...Tools . \" ...We describe a novel approach for inducing unsupervised part - of - speech taggers for languages that have no labeled training data , but have translated text in a resource - rich language .","label":"Future","metadata":{},"score":"43.59493"}{"text":"From this unified representation , the decoder can extract not only the 1- or k - best translations , but also alignments to a reference , or the quantities necessary to drive discriminative training using gradient - based or gradient - free optimization techniques .","label":"Future","metadata":{},"score":"43.653572"}{"text":"Starting from a mono - phone model , we learn increasingly refined models that capture phone internal structures , as well as context - dependent variations in an automatic way .Our approaches reduces error rates compared to other baseline approaches , while streamlining the learning procedure .","label":"Future","metadata":{},"score":"43.775963"}{"text":"Experimental results show that the global features are useful in all the languages . ... mines unlabeled dependency structures only , and we attach dependency relation labels using Support Vector Machines afterwards . \" ...We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .","label":"Future","metadata":{},"score":"43.976997"}{"text":"Experimental results show that the global features are useful in all the languages . ... mines unlabeled dependency structures only , and we attach dependency relation labels using Support Vector Machines afterwards . \" ...We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .","label":"Future","metadata":{},"score":"43.976997"}{"text":"However , HMMs are based on a series of assumptions some of which are known to be poor .In particular , the assumption that successive speech frames are conditionally independent given the discrete state that generated them is not a good assumption for speech recognition .","label":"Future","metadata":{},"score":"43.99234"}{"text":"In this paper , we introduce basic valence frames and le ... \" .Unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts .Traditionally , the unsupervised models have been kept simple due to tractability and data sparsity concerns .","label":"Future","metadata":{},"score":"43.99504"}{"text":"Previous work has shown that FD training can give better results than maximum mutual information ( MMI ) training for small tasks .The use of FD for much larger tasks required the development of a technique to be able to rapidly find the most likely set of Gaussians for each frame in the system .","label":"Future","metadata":{},"score":"44.207626"}{"text":"Then , sub - lexical features are used to rerank the sub - lexical hypotheses .We explore features , similar to syntactic features , on sub - lexical units to reveal the implicit morpho - syntactic information conveyed by these units .","label":"Future","metadata":{},"score":"44.271217"}{"text":"However , such models complicate inference by not being conjugate .Instead , we appeal to tree - based extensions of the Dirichlet distribution , which has been used to induce correlation in semantic on ... . \" ...Dependency parsing is a central NLP task .","label":"Future","metadata":{},"score":"44.416355"}{"text":"We discuss our background assumptions , describe an initial study on the \" projectability \" of syntactic relations , and then present two experiments in which stochastic parsers are developed with minimal human intervention via projection from English . \" ...This paper describes discriminative language modeling for a large vocabulary speech recognition task .","label":"Future","metadata":{},"score":"44.439278"}{"text":", 2010a ) , a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures .These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .","label":"Future","metadata":{},"score":"44.476376"}{"text":", 2010a ) , a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures .These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .","label":"Future","metadata":{},"score":"44.476376"}{"text":"We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al ., 2010a ... \" .Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .","label":"Future","metadata":{},"score":"44.645966"}{"text":"We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al ., 2010a ... \" .Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .","label":"Future","metadata":{},"score":"44.645966"}{"text":"We present an approach to multilingual grammar induction that exploits a phylogeny - structured model of parameter drift .Our method does not require any translated texts or token - level alignments .Instead , the phylogenetic prior couples languages at a parameter level .","label":"Future","metadata":{},"score":"44.7379"}{"text":"We present an approach to multilingual grammar induction that exploits a phylogeny - structured model of parameter drift .Our method does not require any translated texts or token - level alignments .Instead , the phylogenetic prior couples languages at a parameter level .","label":"Future","metadata":{},"score":"44.7379"}{"text":"Unlike previous work on projecting syntactic resources , we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers .The projected parsers from our system result in state - of - the - art performance when compared to previously studied unsupervised and projected parsing systems across eight different languages .","label":"Future","metadata":{},"score":"44.74228"}{"text":"First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar without a treebank .In our experiments , hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy .","label":"Future","metadata":{},"score":"44.79786"}{"text":"First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar without a treebank .In our experiments , hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy .","label":"Future","metadata":{},"score":"44.79786"}{"text":"Klein and Manning 's ( 2004 ) Dependency Model with Valence ( DMV ) was the first to beat a simple parsing heuristic - the right - branching baseline .Despite recent advances , unsupervised parsers lag far behind their supervised counterparts .","label":"Future","metadata":{},"score":"44.80709"}{"text":"Specifically , an ini- tial hypothesis lattice is constrcuted using local features .Candidate sentences are then assigned syntactic language model scores .These global syntactic scores are combined with local low - level scores in a log - linear model .","label":"Future","metadata":{},"score":"44.904335"}{"text":"We extend and improve upon recent work in structured training for neural network transition - based dependency parsing .We do this by experimenting with novel features , additional transition systems and by testing on a wider array of languages .In particular , we introduce set - valued features to encode the predicted morphological properties and part - of - speech confusion sets of the words being parsed .","label":"Future","metadata":{},"score":"44.936085"}{"text":"3 Haghighi and Klein ( 2006 ) achieve higher accuracies by making use of labeled prototypes .We do not use any external information .5.2 Grammar Induction Feat ... . \" ...Unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts .","label":"Future","metadata":{},"score":"45.060177"}{"text":"We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .","label":"Future","metadata":{},"score":"45.27881"}{"text":"We could also introduce new variables , e.g. , nonterminal refinements ( Matsuzaki et al . , 2005 ) , or secondary links Mij ( not constrai ... . by Jin - dong Kim , Tomoko Ohta , Sampo Pyysalo , Yoshinobu Kano - In Proceedings of Natural Language Processing in Biomedicine ( BioNLP )","label":"Future","metadata":{},"score":"45.35283"}{"text":"We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .We apply the framework to word segmentation , joint segmentation and POStagging , dependency parsing , and phrase - structure parsing .","label":"Future","metadata":{},"score":"45.390182"}{"text":"We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .We apply the framework to word segmentation , joint segmentation and POStagging , dependency parsing , and phrase - structure parsing .","label":"Future","metadata":{},"score":"45.390182"}{"text":"In this work we address the problem of unsupervised part - of - speech induction by bringing together several strands of research into a single model .We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman - Yor processes prior , providing an elegant and principled means of incorporating lexical characteristics .","label":"Future","metadata":{},"score":"45.513115"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"Future","metadata":{},"score":"45.561302"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"Future","metadata":{},"score":"45.561302"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"Future","metadata":{},"score":"45.561302"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"Future","metadata":{},"score":"45.561302"}{"text":"The model is formally a latent variable CRF grammar over trees , learned by iteratively splitting grammar productions ( not categories ) .Different regions of the grammar are refined to different degrees , yielding grammars which are three orders of magnitude smaller than the single - scale baseline and 20 times smaller than the split - and - merge grammars of Petrov et al .","label":"Future","metadata":{},"score":"45.6719"}{"text":"The resulting grammars are extremely compact com- pared to other high - performance parsers , yet the parser gives the best published accuracies on several languages , as well as the best generative parsing numbers in English .In addi- tion , we give an associated coarse - to - fine inference scheme which vastly improves inference time with no loss in test set accuracy .","label":"Future","metadata":{},"score":"45.737297"}{"text":"For example , noun phrases might be split into subcategories for subjects and objects , singular and plural , and so on .This splitting process admits an efficient incremental inference scheme which reduces parsing times by orders of magnitude .Furthermore , it produces the best parsing accuracies across an array of languages , in a fully language - general fashion .","label":"Future","metadata":{},"score":"45.75483"}{"text":"As the algorithm generates dependency trees for partial translations left - to - right in decoding , it allows for efficient integration of both n - gram and dependency language models .To resolve conflicts in s ... \" .We introduce a shift - reduce parsing algorithm for phrase - based string - todependency translation .","label":"Future","metadata":{},"score":"45.8483"}{"text":"This robustness led to the third best overall average labeled attachment score in the task , despite using no discriminative methods .We also demonstrate that the parser is quite fast , and can provide even faster parsing times without much loss of accuracy . \" ...","label":"Future","metadata":{},"score":"45.934704"}{"text":"This robustness led to the third best overall average labeled attachment score in the task , despite using no discriminative methods .We also demonstrate that the parser is quite fast , and can provide even faster parsing times without much loss of accuracy . \" ...","label":"Future","metadata":{},"score":"45.934704"}{"text":"Instead of using a model based on salient image fragments , we show that object class detection is also possible using only the object 's boundary .To this end , we develop a novel learning technique to extract class - discriminative boundary fragments .","label":"Future","metadata":{},"score":"46.022606"}{"text":"Comput .Linguist . , 2010 . \" ...Information - extraction ( IE ) systems seek to distill semantic relations from naturallanguage text , but most systems use supervised learning of relation - specific examples and are thus limited by the availability of training data .","label":"Future","metadata":{},"score":"46.6715"}{"text":"We show that addressing this task directly , using probabilistic finite - state methods , produces better results than relying on the local predictions of a current best unsupervised parser , Seginer 's ( 2007 ) CCL .These finite - state models are combined in a cascade to produce more general ( full - sentence ) constituent structures ; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English , German and Chinese .","label":"Future","metadata":{},"score":"46.763153"}{"text":"Our method does not require any translated texts or token - level alignments .Instead , the phylogenetic prior couples languages at a parameter level .Joint induction in the multiling ... \" .We present an approach to multilingual grammar induction that exploits a phylogeny - structured model of parameter drift .","label":"Future","metadata":{},"score":"46.904305"}{"text":"We also automatically refine the syntactic categories given in our coarsely tagged input .Across six languages our approach outperforms state - of - theart unsupervised methods by a significant margin . by Valentin I. Spitkovsky , Hiyan Alshawi , Daniel Jurafsky - IN NAACL - HLT . \" ...","label":"Future","metadata":{},"score":"47.01809"}{"text":"Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .In co ... \" .We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .","label":"Future","metadata":{},"score":"47.155514"}{"text":"Finally , we present multilingual experiments which show that parsing with hierarchical state - splitting is fast and accurate in multiple languages and domains , even without any language - specific tuning .This work describes systems for detecting semantic categories present in news video .","label":"Future","metadata":{},"score":"47.202644"}{"text":"We generalize the evaluation to other word - types , and show that the performance can be increased to 18 % relative by preserving part - of - speech equivalencies during translation .We further differentiate ourselves from previous work by conducting a second evaluation which examines the accuracy of translating all word types , rather than just nouns .","label":"Future","metadata":{},"score":"47.329678"}{"text":"We generalize the evaluation to other word - types , and show that the performance can be increased to 18 % relative by preserving part - of - speech equivalencies during translation .We further differentiate ourselves from previous work by conducting a second evaluation which examines the accuracy of translating all word types , rather than just nouns .","label":"Future","metadata":{},"score":"47.329678"}{"text":"The length of this thesis including footnotes and appendices does not exceed 29,500 words and includes no more than 40 figures .1 Systems which automatically transcribe carefully dictated speech are now commercially available , but their performance degrades dramatically when the speaking style of users becomes more relaxed or conversational .","label":"Future","metadata":{},"score":"47.60748"}{"text":"The beam - search decoder only requires the syntactic processing task to be broken into a sequence of decisions , such that , at each stage in the process , the decoder is able to consider the top - n candidates and generate all possibilities for the next stage .","label":"Future","metadata":{},"score":"47.633614"}{"text":"The beam - search decoder only requires the syntactic processing task to be broken into a sequence of decisions , such that , at each stage in the process , the decoder is able to consider the top - n candidates and generate all possibilities for the next stage .","label":"Future","metadata":{},"score":"47.633614"}{"text":"We combine a margin - sensitive approach for data mining hard negative examples with a formalism we call latent SVM .A latent SVM , like a hidden CRF , leads to a non - convex training problem .However , a latent SVM is semi - convex and the training problem becomes convex once latent information is specified for the positive examples .","label":"Future","metadata":{},"score":"47.664238"}{"text":"Our methods result in state - of - the - art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .","label":"Future","metadata":{},"score":"47.685486"}{"text":"We present several improvements to unlexicalized parsing with hierarchically state - split PCFGs .First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar wi ... \" .","label":"Future","metadata":{},"score":"47.746468"}{"text":"Tools . by Kuzman Ganchev , Jennifer Gillenwater , Ben Taskar - In ACL - IJCNLP , 2009 . \" ...Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext .","label":"Future","metadata":{},"score":"47.844856"}{"text":"By performing Latent Dirichlet Allocation using a block of preceding text , we achieve a topic - conditioned RNNLM .This approach has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data subsets .","label":"Future","metadata":{},"score":"47.853607"}{"text":"[19 ] ) .Boosting is used to select discriminative combinations of boundary fragments ( weak detectors ) to form a strong \" Boundary - Fragment - Model \" ( BFM ) detector .The generative aspect of the model is used to determine an approximate segmentation . ... posed from codebook entries , possibly arising from different source images .","label":"Future","metadata":{},"score":"47.936275"}{"text":"The new model is computationally more efficient , and more naturally suited to modeling global sentential phenomena , than the conditional exponential ( e.g. Maximum Entropy ) models proposed to date .Using the model is straightforward .Training the model requires sampling from an exponential distribution .","label":"Future","metadata":{},"score":"48.026947"}{"text":"INTRODUCTION Previous research has shown that the accuracy of a speech recognition system trained using Maximum Likelihood Estimation ( MLE ) can often be improved further using discriminative training .All such techniques normally give much greater improvements in recognition accuracy on the training data than on the test set except wh ... . \" ...","label":"Future","metadata":{},"score":"48.20428"}{"text":"Information - extraction ( IE ) systems seek to distill semantic relations from naturallanguage text , but most systems use supervised learning of relation - specific examples and are thus limited by the availability of training data .Open IE systems such as TextRunner , on the other hand , aim to handle the unbounded number of relations found on the Web .","label":"Future","metadata":{},"score":"48.39479"}{"text":"Using this model in the pre - ordering framework results in significant gains in translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods . ... ing available in the source language .However , as building a parser for each sour ... . \" ...","label":"Future","metadata":{},"score":"48.475815"}{"text":"State space models are based on a continuous state vector evolving through time according to a state evo- . ... ative training yields better performance than ML .In comparison to ML training , the discriminative methods require recognition run ... . \" ...","label":"Future","metadata":{},"score":"48.562645"}{"text":"Despite its simplicity , a product of eight automatically learned grammars improves parsing accuracy from 90.2 % to 91.8 % on English , and from 80.3 % to 84.5 % on German .Pruning can massively accelerate the computation of feature expectations in large models .","label":"Future","metadata":{},"score":"48.602074"}{"text":"The perceptron algorithm has the benefit of automatically selecting a relatively small feature set in just a couple of passes over the training data .However , using the feature set output from the perceptron algorithm ( initialized with their weights ) , CRF training provides an additional 0.5 % reduction in word error rate , for a total 1.8 % absolute reduction from the baseline of 39.2 % . by Ronald Rosenfeld , Stanley F. Chen , Xiaojin Zhu - Computers , Speech and Language , 2001 . \" ...","label":"Future","metadata":{},"score":"48.603416"}{"text":"We describe experiments on learning latent variable grammars for various German treebanks , using a language - agnostic statistical approach .In our method , a minimal initial grammar is hierarchically refined using an adaptive split - and - merge EM procedure , giving compact , accurate grammars .","label":"Future","metadata":{},"score":"48.775528"}{"text":"Instead of using a model based on salient image fragments , we show that object class detection is also possible using only the object 's boundary .To this end , we develop a novel learning technique t ... \" .Abstract .","label":"Future","metadata":{},"score":"49.160866"}{"text":"Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext .","label":"Future","metadata":{},"score":"49.230103"}{"text":"The penalty for a recognition failure is often small : if two con- figurations are confused , they are often similar to each other , and the illusion works well enough , for instance , to drive a graphics animation of the moving hand .","label":"Future","metadata":{},"score":"49.2481"}{"text":"Left - to - right decoding , which generates the target string in order , can improve decoding efficiency by simplifying the language model evaluation .This paper presents a novel left to right decoding algorithm for tree - to - string translation , using a bottom - up parsing strategy and dynamic future cost estimation for each partial translation .","label":"Future","metadata":{},"score":"49.621845"}{"text":"Yet , various grammar parameters are expected to be correlated because the elements in language they represent share linguistic properties .In this paper , we suggest an alternative to the Dirichlet prior , a family of logistic normal distributions .We derive an inference algorithm for this family of distributions and experiment with the task of dependency grammar induction , demonstrating performance improvements with our priors on a set of six treebanks in different natural languages .","label":"Future","metadata":{},"score":"49.643066"}{"text":"Yet , various grammar parameters are expected to be correlated because the elements in language they represent share linguistic properties .In this paper , we suggest an alternative to the Dirichlet prior , a family of logistic normal distributions .We derive an inference algorithm for this family of distributions and experiment with the task of dependency grammar induction , demonstrating performance improvements with our priors on a set of six treebanks in different natural languages .","label":"Future","metadata":{},"score":"49.643066"}{"text":"Yet , various grammar parameters are expected to be correlated because the elements in language they represent share linguistic properties .In this paper , we suggest an alternative to the Dirichlet prior , a family of logistic normal distributions .We derive an inference algorithm for this family of distributions and experiment with the task of dependency grammar induction , demonstrating performance improvements with our priors on a set of six treebanks in different natural languages .","label":"Future","metadata":{},"score":"49.643066"}{"text":"Parser quality is usually evaluated by comparing its output to a gold standard whose annotations are linguistically motivated .However , there are cases in which ... . by Shay B. Cohen , Noah A. Smith , Alex Clark , Dorota Glowacka , Colin De La Higuera , Mark Johnson , John Shawe - taylor . \" ...","label":"Future","metadata":{},"score":"49.692726"}{"text":"In this paper , we relax the independence assumption and model explicitly the inter - dependency of the local regions .Similarly to previous work , we represent images as a collection of patches , each of which belongs to a latent \" theme \" that is shared across images as well as categories .","label":"Future","metadata":{},"score":"49.718983"}{"text":"..The first grammar induction models to surpass a trivial baseline concentrated on the task of inducing unlabelled bracketings f .. \" ...In this paper , we develop multilingual supervised latent Dirichlet allocation ( MLSLDA ) , a probabilistic generative model that allows insights gleaned from one language 's data to inform how the model captures properties of other languages .","label":"Future","metadata":{},"score":"49.721107"}{"text":"Across eight languages , the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1 % and reaching as high as 39 % . ...focus is not the DMV model itself , which is well - studied , but rather the prior which couples the various languages ' parameters .","label":"Future","metadata":{},"score":"49.739445"}{"text":"We present several models to this end ; in particular a partially observed conditional random field model , where coupled token and type constraints provide a partial signal for training .Averaged across eight previously studied Indo - European languages , our model achieves a 25 % relative error reduction over the prior state of the art .","label":"Future","metadata":{},"score":"49.852287"}{"text":"Despite the much simplified training process , our acoustic model achieves state - of - the - art results on phone classification ( where it outperforms almost all other methods ) and competitive performance on phone recognition ( where it outperforms standard CD triphone / subphone / GMM approaches ) .","label":"Future","metadata":{},"score":"49.9702"}{"text":"We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process ( HDP ) .Our HDP - PCFG model allows the complexity of the grammar to grow as more training data is available .In addition to presenting a fully Bayesian model for the PCFG , we also develop an efficient variational inference procedure .","label":"Future","metadata":{},"score":"50.05332"}{"text":"Unlike that work which used 3D annotations of keypoints , we use only 2D annotations which are much easier for naive human annotators .The main algorithmic contribution is in how we use the pattern of poselet activations .Individual poselet activations are noisy , but considering the spatial context of each can provide vital disambiguating information , just as object detection can be improved by considering the detection scores of nearby objects in the scene .","label":"Future","metadata":{},"score":"50.227104"}{"text":"On full - scale treebank parsing experiments , the discriminative latent models outperform both the comparable generative latent models as well as the discriminative non - latent baselines .We present a maximally streamlined approach to learning HMM - based acoustic models for automatic speech recognition .","label":"Future","metadata":{},"score":"50.406822"}{"text":"Unfortunately the sentence in Figure 1(b ) is highly unusual in its amount of dependency conservation .To get a feel for the typical case , we used off - the - shelf parsers ( McDonald et al . , 2005 ) for E .. by Ivan Titov , James Henderson - IN PROCEEDINGS OF CONLL-2007 SHARED TASK .","label":"Future","metadata":{},"score":"50.417576"}{"text":"Unfortunately the sentence in Figure 1(b ) is highly unusual in its amount of dependency conservation .To get a feel for the typical case , we used off - the - shelf parsers ( McDonald et al . , 2005 ) for E .. by Ivan Titov , James Henderson - IN PROCEEDINGS OF CONLL-2007 SHARED TASK .","label":"Future","metadata":{},"score":"50.417576"}{"text":"The paper presents the design and implementation of the BioNLP'09 Shared Task , and reports the final results with analysis .The shared task consists of three sub - tasks , each of which addresses bio - molecular event extraction at a different level of specificity .","label":"Future","metadata":{},"score":"50.5147"}{"text":"The paper presents the design and implementation of the BioNLP'09 Shared Task , and reports the final results with analysis .The shared task consists of three sub - tasks , each of which addresses bio - molecular event extraction at a different level of specificity .","label":"Future","metadata":{},"score":"50.5147"}{"text":"Discriminative feature - based methods are widely used in natural language processing , but sentence parsing is still dominated by generative methods .While prior feature - based dynamic programming parsers have restricted training and evaluation to artificially short sentences , we present the first gene ... \" .","label":"Future","metadata":{},"score":"50.639153"}{"text":"Via an oracle experiment , we show that the upper bound on accuracy of a CCG parser is significantly lowered when its search space is pruned using a supertagger , though the supertagger also prunes many bad parses .Inspired by this analysis , we design a single model with both supertagging and parsing features , rather than separating them into distinct models chained together in a pipeline .","label":"Future","metadata":{},"score":"50.760784"}{"text":"Our generative self - trained grammars reach F scores of 91.6 on the WSJ test set and surpass even discriminative reranking systems without self - training .Additionally , we show that multiple self - trained grammars can be combined in a product model to achieve even higher accuracy .","label":"Future","metadata":{},"score":"50.77807"}{"text":"Since most of these systems were developed and tested using data from the WSJ corpus , we compare their generalization abilities by testing on both WSJ and the multilingual Multext - East corpus .Finally , we introduce the idea of evaluating systems based on their ability to produce cluster prototypes that are useful as input to a prototype - driven learner .","label":"Future","metadata":{},"score":"50.799915"}{"text":"We present a \" parts and structure \" model for object category recognition that can be learnt efficiently and in a weakly - supervised manner : the model is learnt from example images containing category instances , without requiring segmentation from background clutter .","label":"Future","metadata":{},"score":"50.976955"}{"text":"We present a \" parts and structure \" model for object category recognition that can be learnt efficiently and in a weakly - supervised manner : the model is learnt from example images containing category instances , without requiring segmentation from background clutter .","label":"Future","metadata":{},"score":"50.976955"}{"text":"To manage this complexity , we translate into target language clusterings of increasing vocabulary size .This approach gives dramatic speed - ups while additionally increasing final translation quality .The intersection of tree transducer - based translation models with n - gram language models results in huge dynamic programs for machine translation decoding .","label":"Future","metadata":{},"score":"51.21942"}{"text":"The optimal choice of feature types ( whose repertoire includes interest points , curves and regions ) is made automatically .In recognition , the model may be applied efficiently in a complete manner , bypassing the need for feature detectors , to give the globally optimal match within a query image .","label":"Future","metadata":{},"score":"51.3502"}{"text":"Furthermore several variations to the MMIE training scheme are introduced with the a .. by Julia A. Lasserre - In CVPR ' 06 : Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition , 2006 . \" ...","label":"Future","metadata":{},"score":"51.428818"}{"text":"The algorithm uses a similarity graph to encourage similar n - grams to have similar POS tags .We demonstrate the efficacy of our approach on a domain adaptation task , where we assume that we have access to large amounts of unlabeled data from the target domain , but no additional labeled data .","label":"Future","metadata":{},"score":"51.546337"}{"text":"We consider generative and di ... \" .Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext .","label":"Future","metadata":{},"score":"51.854267"}{"text":"n word hypotheses and using sub - lexical features on sub - lexical hypotheses allow us to investigate this interaction .However , obtaining linguistic information from sub - lexical hypotheses is not trivial , since the morphological and syntactic tools can not be directly applied to sub - lexical units .","label":"Future","metadata":{},"score":"51.96782"}{"text":"While prior feature - based dynamic programming parsers have restricted training and evaluation to artificially short sentences , we present the first general , featurerich discriminative parser , based on a conditional random field model , which has been successfully scaled to the full WSJ parsing data .","label":"Future","metadata":{},"score":"52.03247"}{"text":"Combining multiple grammars that were self - trained on disjoint sets of unlabeled data results in a final test accuracy of 92.5\\% on the WSJ test set and 89.6\\% on our Broadcast News test set .This work shows how to improve state - of - the - art monolingual natural language processing models using unannotated bilingual text .","label":"Future","metadata":{},"score":"52.083534"}{"text":"We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences .Given this fixed network representation , we learn a final layer using the structured perceptron with beam - search decoding .On the Penn Treebank , our parser reaches 94.26 % unlabeled and 92.41 % labeled attachment accuracy , which to our knowledge is the best accuracy on Stanford Dependencies to date .","label":"Future","metadata":{},"score":"52.105087"}{"text":"Our experimental results on the Wall Street Journal corpus show that we obtain significant reductions in perplexity compared to the state - of - the - art baseline trigram model with Good - Turing and Kneser - Ney smoothings . binations because the linear additive form is too blunt to capture subtleties in each of the component models .","label":"Future","metadata":{},"score":"52.150726"}{"text":"The refined poselet activations are then clustered into mutually consistent hypotheses where consistency is based on empirically determined spatial keypoint distributions .Finally , bounding boxes are predicted for each person hypothesis and shape masks are aligned to edges in the image to provide a segmentation .","label":"Future","metadata":{},"score":"52.18467"}{"text":"It also outperforms the best results in the 2007 challenge in ten out of twenty categories .The system relies heavily on deformable parts .While deformable part models have become quite popular , their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge .","label":"Future","metadata":{},"score":"52.194675"}{"text":"In contrast with previous work , we are able to split various terminals to different degrees , as appropriate to the actual complexity in the data .Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .","label":"Future","metadata":{},"score":"52.46828"}{"text":"Primary acoustic , speech , and vision systems were trained to discriminate instances of the categories .Higher - level systems exploited correlations among the categories , incorporated sequential context , and combined the joint evidence from the three information sources .","label":"Future","metadata":{},"score":"52.487064"}{"text":"A mixture grammar fit with the EM algorithm shows improvement over a single PCFG , both in parsing accuracy and in test data likelihood .We argue that this improvement comes from the learning of specialized grammars that capture non - local correlations .","label":"Future","metadata":{},"score":"52.560745"}{"text":"It has not been submitted in whole or part for a degree at any other university .The length of this thesis including footnotes and appendices does ... \" .Declaration This dissertation is the result of my own work and includes nothing which is the outcome of work done in collaboration , except where stated .","label":"Future","metadata":{},"score":"52.649426"}{"text":"Basic sub - lexical n - gram features result in 0.6 % reduction over the baseline and morpho - syntactic features yield an additional 0.4 % reduction on the test set .Index Terms - language modeling , automatic speech recognition , discriminative training , sub - lexical recognition units 1 . .","label":"Future","metadata":{},"score":"52.70559"}{"text":"With 100 K unlabeled and 2 K labeled questions , uptraining is able to improve parsing accuracy to 84 % , closing the gap between in - domain and out - of - domain performance .We study self - training with products of latent variable grammars in this paper .","label":"Future","metadata":{},"score":"52.706863"}{"text":"The first , Baby Steps , bootstraps itself via iterated learning of increasingly longer sentences and requires no initialization .Th ... \" .We present three approaches for unsupervised grammar induction that are sensitive to data complexity and apply them to Klein and Manning 's Dependency Model with Valence .","label":"Future","metadata":{},"score":"52.787773"}{"text":"Currently the most popular acoustic model for speech recognition is the hidden Markov model ( HMM ) .However , HMMs are based on a series of assumptions some of which are known to be poor .In particular , the assumption that successive speech frames are conditionally independent given the discrete stat ... \" .","label":"Future","metadata":{},"score":"52.821453"}{"text":"As auxiliary results , we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder .Travatar is available under the LGPL at . \" ...We present a simple and novel classifier - based preordering approach .","label":"Future","metadata":{},"score":"53.04109"}{"text":"This paper presents WOE , an open IE system which improves dramatically on TextRunner 's precision and recall .The key to WOE 's performance is a novel form of self - supervised learning for open extractors - using heuristic matches between Wikipedia infobox attribute values and corresponding sentences to construct training data .","label":"Future","metadata":{},"score":"53.0453"}{"text":"We present a novel approach which employs a randomized sequence of pruning masks .Formally , we apply auxiliary variable MCMC sampling to generate this sequence of masks , thereby gaining theoretical guarantees about convergence .Because each mask is generally able to skip large portions of an underlying dynamic program , our approach is particularly compelling for high - degree algorithms .","label":"Future","metadata":{},"score":"53.15404"}{"text":"They also permit the use of well - understood , generalpurpose learning algorithms .There has been an increased interest in using probabilistic grammars in the Bayesian setting .To date , most of the literature has focused on using a Dirichlet prior .","label":"Future","metadata":{},"score":"53.15828"}{"text":"They also permit the use of well - understood , generalpurpose learning algorithms .There has been an increased interest in using probabilistic grammars in the Bayesian setting .To date , most of the literature has focused on using a Dirichlet prior .","label":"Future","metadata":{},"score":"53.15828"}{"text":"They also permit the use of well - understood , generalpurpose learning algorithms .There has been an increased interest in using probabilistic grammars in the Bayesian setting .To date , most of the literature has focused on using a Dirichlet prior .","label":"Future","metadata":{},"score":"53.15828"}{"text":"This paper gives an overview of discriminative training as it pertains to the speech recognition problem .The basic theory of discriminative training will be discussed and an explanation of maximum mutual information ( MMI ) given .Common problems inherent to discriminative training will be explored a ... \" .","label":"Future","metadata":{},"score":"53.170322"}{"text":"Recurrent neural network language models ( RNNLMs ) have recently demonstrated state - of - the - art performance across a variety of tasks .In this paper , we improve their performance by providing a contextual real - valued input vector in association with each word .","label":"Future","metadata":{},"score":"53.192017"}{"text":"Recurrent neural network language models ( RNNLMs ) have recently demonstrated state - of - the - art performance across a variety of tasks .In this paper , we improve their performance by providing a contextual real - valued input vector in association with each word .","label":"Future","metadata":{},"score":"53.192017"}{"text":"When Brown et al .( 1993 ) wan ... . \" ...We consider a new subproblem of unsupervised parsing from raw text , unsupervised partial parsing - the unsupervised version of text chunking .We show that addressing this task directly , using probabilistic finite - state methods , produces better results than relying on the local predictions of a current ... \" .","label":"Future","metadata":{},"score":"53.21813"}{"text":"However , parsing accuracies for Arabic usually lag behind non - semitic languages .Moreover , whil ...Tools . by Kuzman Ganchev , Jennifer Gillenwater , Ben Taskar - In ACL - IJCNLP , 2009 . \" ...Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .","label":"Future","metadata":{},"score":"53.299824"}{"text":"This paper describes the application of a discriminative HMM parameter estimation technique called Frame Discrimination ( FD ) , to medium and large vocabulary continuous speech recognition .Previous work has shown that FD training can give better results than maximum mutual information ( MMI ) training ... \" .","label":"Future","metadata":{},"score":"53.394234"}{"text":"non - parallel , multilingual corpus . 1 Introduction Probabilistic grammars have become an important tool in natural language processing .An attractive property of probabilistic grammars is that the ... . by Fei Wu , Daniel S. Weld - in Proc . 48th Annu .","label":"Future","metadata":{},"score":"53.55376"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"Future","metadata":{},"score":"53.640816"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"Future","metadata":{},"score":"53.640816"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"Future","metadata":{},"score":"53.640816"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"Future","metadata":{},"score":"53.640816"}{"text":"In particular , we show that the reranking parser described in Charniak and Johnson ( 2005 ) improves performance of the parser on Brown to 85.2 % .Furthermore , use of the self - training techniques described in ( Mc - Closky et al . , 2006 ) raise this to 87.8 % ( an error reduction of 28 % ) again without any use of labeled Brown data .","label":"Future","metadata":{},"score":"53.83967"}{"text":"The tree with the maximal probability is outputted .The experiments are carried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser . ... arried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser .","label":"Future","metadata":{},"score":"53.90129"}{"text":"The tree with the maximal probability is outputted .The experiments are carried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser . ... arried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser .","label":"Future","metadata":{},"score":"53.90129"}{"text":"Compositional systems are distinct from these models in that they are non - Markovian .On the one hand this makes computation substantially more difficult , ... . \" ...Abstract .Bourdev and Malik ( ICCV 09 ) introduced a new notion of parts , poselets , constructed to be tightly clustered both in the configuration space of keypoints , as well as in the appearance space of image patches .","label":"Future","metadata":{},"score":"53.993736"}{"text":"Participants were to build a single parsing system that is robust to domain changes and can handle noisy text that is commonly encountered on the web .There was a constituency and a dependency parsing track and 11 sites submitted a total of 20 systems .","label":"Future","metadata":{},"score":"54.114365"}{"text":"Decoding algorithms for syntax based machine translation suffer from high computational complexity , a consequence of intersecting a language model with a context free grammar .Left - to - right decoding , which generates the target string in order , can improve decoding efficiency by simplifying the langu ... \" .","label":"Future","metadata":{},"score":"54.222862"}{"text":"State - of - the - art natural language processing models are anything but compact .Syntactic parsers have huge grammars , machine translation systems have huge transfer tables , and so on across a range of tasks .With such complexity come two challenges .","label":"Future","metadata":{},"score":"54.262276"}{"text":"In this work , we present 1 . an effective method for pruning in split PCFGs 2 . a comparison of objective functions for infe ... . \" ...The l - bfgs limited - memory quasi - Newton method is the algorithm of choice for optimizing the parameters of large - scale log - linear models with L2 regularization , but it can not be used for an L1-regularized loss due to its non - differentiability whenever some parameter is zero .","label":"Future","metadata":{},"score":"54.35927"}{"text":"We present cdec , an open source framework for decoding , aligning with , and training a number of statistical machine translation models , including word - based models , phrase - based models , and models based on synchronous context - free grammars .","label":"Future","metadata":{},"score":"54.419678"}{"text":"This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function . by Pedro Felzenszwalb , David Mcallester , Deva Ramanan - In IEEE Conference on Computer Vision and Pattern Recognition ( CVPR-2008 , 2008 . \" ...","label":"Future","metadata":{},"score":"54.4473"}{"text":"The results show that all three systems achieve competitive performance , with a best labeled attachment score over 88 % .All three parsers benefit from the use of automatically derived lemmas , while morphological features seem to be less important .","label":"Future","metadata":{},"score":"54.82476"}{"text":"The results show that all three systems achieve competitive performance , with a best labeled attachment score over 88 % .All three parsers benefit from the use of automatically derived lemmas , while morphological features seem to be less important .","label":"Future","metadata":{},"score":"54.82476"}{"text":"The best accuracies were in the 80 - 84\\% range for F1 and LAS ; even part - of - speech accuracies were just above 90\\% .Coarse - to - fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy .","label":"Future","metadata":{},"score":"54.855396"}{"text":"As a parsing algorithm , BP is both asymptotically and empirically efficient .E ... \" .We formulate dependency parsing as a graphical model with the novel ingredient of global constraints .We show how to apply loopy belief propagation ( BP ) , a simple and effective tool for approximate learning and inference .","label":"Future","metadata":{},"score":"55.006546"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"Future","metadata":{},"score":"55.09163"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"Future","metadata":{},"score":"55.09163"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"Future","metadata":{},"score":"55.09163"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"Future","metadata":{},"score":"55.09163"}{"text":"UPPARSE , the software used for the experiments in this paper , is available under an open - sourc ... . \" ...In this work we address the problem of unsupervised part - of - speech induction by bringing together several strands of research into a single model .","label":"Future","metadata":{},"score":"55.173557"}{"text":"Computers fail to track these in fast video , but sleight of hand fools humans as well : what happens too quickly we just can not see .We show a 3D tracker for these types of motions that relies on the recognition of familiar configurations in 2D images ( classification ) , and fills the gaps in - between ( interpolation ) .","label":"Future","metadata":{},"score":"55.342964"}{"text":"Thus , although these newer methods have introduced potentially useful machine learning techniques , they should not be assumed to provide the best performance for unsupervised POS induction .In addi ... . \" ...Dependency parsing is a central NLP task .","label":"Future","metadata":{},"score":"55.417393"}{"text":"most languages are projective .In Figure 8 An example Chinese dependency tree .Although non - projec ... . \" ...Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .","label":"Future","metadata":{},"score":"55.44781"}{"text":"most languages are projective .In Figure 8 An example Chinese dependency tree .Although non - projec ... . \" ...Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .","label":"Future","metadata":{},"score":"55.44781"}{"text":"In cdec , the only model - specific logic is confined to the first step in the process where an input string ( or lattice , e .. \" ...Via an oracle experiment , we show that the upper bound on accuracy of a CCG parser is significantly lowered when its search space is pruned using a supertagger , though the supertagger also prunes many bad parses .","label":"Future","metadata":{},"score":"55.512638"}{"text":"Shay , 2009 . \" ...We present a family of priors over probabilistic grammar weights , called the shared logistic normal distribution .This family extends the partitioned logistic normal distribution , enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar , prov ... \" .","label":"Future","metadata":{},"score":"55.55813"}{"text":"There are a number of options for model training , and tuning includes advanced options such as hypergraph MERT , and training of sparse features through online learning .The training pipeline is modeled after that of the popular Moses decoder , so users familiar with Moses should be able to get started quickly .","label":"Future","metadata":{},"score":"56.14599"}{"text":"Similarly , modeling of semantic coherence was attempted in the conditional exponential model o .. by Shaojun Wang , Shaomin Wang , Russell Greiner , Dale Schuurmans , Li Cheng - In Proceedings of ICML 2005 , 2005 . \" ...We present a directed Markov random field ( MRF ) model that combines n - gram models , probabilistic context free grammars ( PCFGs ) and probabilistic latent semantic analysis ( PLSA ) for the purpose of statistical language modeling .","label":"Future","metadata":{},"score":"56.25345"}{"text":"Tools . by Chris Dyer , Adam Lopez , Juri Ganitkevitch , Jonathan Weese , Hendra Setiawan , Ferhan Ture , Vladimir Eidelman , Phil Blunsom , Philip Resnik - In Proceedings of ACL System Demonstrations , 2010 . \" ...We present cdec , an open source framework for decoding , aligning with , and training a number of statistical machine translation models , including word - based models , phrase - based models , and models based on synchronous context - free grammars .","label":"Future","metadata":{},"score":"56.300026"}{"text":"Parallel data is not used , allowing the technique to be applied even in domains where human - translated texts are unavailable .We obtain state - of - theart performance for two tasks of structure prediction : unsupervised part - of - speech tagging and unsupervised dependency parsing . ... parsing .","label":"Future","metadata":{},"score":"56.362602"}{"text":"They provided an average relative improvement of 10 % compared to ordinary unsupervised MLLR .I .. \" ...This paper investigates the use of discriminative training techniques for large vocabulary speech recogntion with training datasets up to 265 hours .Techniques for improving lattice - based Maximum Mutual Information Estimation ( MMIE ) training are described and compared to Frame Discrimination ( FD ) .","label":"Future","metadata":{},"score":"56.382927"}{"text":"Our preordering approach has several advantages .First , be - nsubj ROOT attr det amod It was a real whirlwind .NN VBD ...Tools . by Rebecca Hwa , Philip Resnik , Amy Weinberg , Clara Cabezas , Okan Kolak - Natural Language Engineering , 2005 . \" ...","label":"Future","metadata":{},"score":"56.48589"}{"text":"..METU - Sabanc treebank ( Atalay et al . , 2003 ; Oflazer et al . , 2003 ) from the CoNLL shared task in 2006 .Whenever using CoNLL shared task data , we used the first 80 % of the data d .. \" ...","label":"Future","metadata":{},"score":"56.56697"}{"text":"..METU - Sabanc treebank ( Atalay et al . , 2003 ; Oflazer et al . , 2003 ) from the CoNLL shared task in 2006 .Whenever using CoNLL shared task data , we used the first 80 % of the data d .. \" ...","label":"Future","metadata":{},"score":"56.56697"}{"text":"However there may be no common target that all parts predict reliably .Each poselet makes good predict ... . \" ... \" Bag of words \" models have enjoyed much attention and achieved good performances in recent studies of object categorization .","label":"Future","metadata":{},"score":"56.66034"}{"text":"Applied to a dataset of partially labeled street scenes , we show that the TDP 's inclusion of spatial structure improves detection performance , and allows unsupervised discovery of object categories . ... stances .In addition , it seems difficult to generalize the fixed set of constellation parts to problems where the number of objects is uncertain .","label":"Future","metadata":{},"score":"56.747"}{"text":"Behavior Control : Finally we show how all these elements can be incorporated into a goal keeping robot .We develop simple behaviors that can be used in a layered architecture and enable the robot to block most balls that are being shot at the goal .","label":"Future","metadata":{},"score":"56.753815"}{"text":"By avoiding the chain rule , the model treats each sentence as a \" bag of features \" , where features are arbitrary computable properties of the sentence .The new model is computationally m ... \" .We introduce an exponential language model which models a whole sentence or utterance as a single unit .","label":"Future","metadata":{},"score":"56.871025"}{"text":"The following sections describes commonly used discriminative criteria and associated optimisation schemes for discriminative traini ...Tools . by Pedro F. Felzenszwalb , Ross B. Girshick , David McAllester , Deva Ramanan . \" ...We describe an object detection system based on mixtures of multiscale deformable part models .","label":"Future","metadata":{},"score":"56.926914"}{"text":"This ' universal ' treebank is made freely available in order to facilitate research on multilingual dependency parsing .We consider the construction of part - of - speech taggers for resource - poor languages .Recently , manually constructed tag dictionaries from Wiktionary and dictionaries projected via bitext have been used as type constraints to overcome the scarcity of annotated data in this setting .","label":"Future","metadata":{},"score":"57.032356"}{"text":"Unli ... \" .In this paper , we describe a unified probabilistic framework for statistical language modeling -- the latent maximum entropy principle -- which can effectively incorporate various aspects of natural language , such as local word interaction , syntactic structure and semantic document information .","label":"Future","metadata":{},"score":"57.10116"}{"text":"Syntax - based translation models should in principle be efficient with polynomially - sized search space , but in practice they are often embarassingly slow , partly due to the cost of language model integration .Experiments show that , with comparable translation quality , our tree - to - string system ( in Python ) can run more than 30 times faster than the phrase - based system Moses ( in C++ ) . ... ld potentially be combined to further speed up decoding .","label":"Future","metadata":{},"score":"57.2558"}{"text":"Such worries have merit .The standard \" Charniak parser \" checks in at a labeled precisionrecall f - measure of 89.7 % on the Penn WSJ test set , but only 82.9 % on the test set from the Brown treebank corpus .","label":"Future","metadata":{},"score":"57.363327"}{"text":"Alternatively , parts can be learned by clustering visually similar image patches [ 1 , 11 ] but this approach does not exploit ... . by Erik B. Sudderth , Antonio Torralba , William T. Freeman , Alan S. Willsky - Advances in Neural Information Processing Systems 18 , 2005 . \" ...","label":"Future","metadata":{},"score":"57.484833"}{"text":"Our system achieves a two - fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge .It also outperforms the best results in the 2007 challenge ... \" .This paper describes a discriminatively trained , multiscale , deformable part model for object detection .","label":"Future","metadata":{},"score":"57.581802"}{"text":"Although HMMs are a natural choice to warp the time axis and model the temporal phenomena in the speech signal , their conditional independence properties limit their ... \" .Abstract - Acoustic modeling based on hidden Markov models ( HMMs ) is employed by state - of - the - art stochastic speech recognition systems .","label":"Future","metadata":{},"score":"57.58785"}{"text":"It is more accurate than standard inside - outside re - estimation ( classic EM ) , significantly faster , and simpler .Our experiments with Klein and Manning 's Dependency Model with Valence ( DMV ) attain state - of - the - art p ... \" .","label":"Future","metadata":{},"score":"57.763268"}{"text":"The new Viewer adds three features for more powerful search : wildcards , morphological inflections , and capitalization .These additions allow the discovery of patterns that were previously difficult to find and further facilitate the study of linguistic trends in printed text .","label":"Future","metadata":{},"score":"57.807217"}{"text":"This has led to concer ... \" .Statistical parsers trained and tested on the Penn Wall Street Journal ( WSJ ) treebank have shown vast improvements over the last 10 years .Much of this improvement , however , is based upon an ever - increasing number of features to be trained on ( typically ) the WSJ treebank data .","label":"Future","metadata":{},"score":"57.91452"}{"text":"This family extends the partitioned logistic normal distribution , enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar , providing a new way to encode prior knowledge about an unknown grammar .We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors .","label":"Future","metadata":{},"score":"57.948074"}{"text":"Our grammar inducer is trained on the Wall Street Journal ( WSJ ) and achieves 59.5 % accuracy out - of - domain ( Brown sentences with 100 or fewer words ) , more than 6 % higher than the previous best results .","label":"Future","metadata":{},"score":"57.975807"}{"text":"We further apply the model to the Wall Street Journal speech recognition task , where we observe improvements in word - error - rate . ... ets , with the goal of making subsets containing data on only one topic .","label":"Future","metadata":{},"score":"58.022526"}{"text":"We describe efficient algorithms for marginalization , inference and normalization in our extended models .We then present experimental results for our approach on the Wall Street Journal corpus . . ..However , doing so explicitly is not always possible , and even if attempted , sparse data problems almost always immediately arise in such complex models .","label":"Future","metadata":{},"score":"58.48654"}{"text":"There are several aspects of the model that we wish to improve and investigate .Acknowledgments We are very grateful for suggestions from and discussions with Michael Isar ... . by Andreas Opelt , Andrew Zisserman - In Proceedings IEEE Conf . on Computer Vision and Pattern Recognition ( CVPR06 , 2006 . \" ... Abstract .","label":"Future","metadata":{},"score":"58.49362"}{"text":"The l - bfgs limited - memory quasi - Newton method is the algorithm of choice for optimizing the parameters of large - scale log - linear models with L2 regularization , but it can not be used for an L1-regularized loss due to its non - differentiability whenever some parameter is zero .","label":"Future","metadata":{},"score":"59.040985"}{"text":"In this paper , we develop multilingual supervised latent Dirichlet allocation ( MLSLDA ) , a probabilistic generative model that allows insights gleaned from one language 's data to inform how the model captures properties of other languages .MLSLDA accomplishes this by jointly modeling two aspects of text : how multilingual concepts are clustered into thematically coherent topics and how topics associated with text connect to an observed regression variable ( such as ratings on a sentiment scale ) .","label":"Future","metadata":{},"score":"59.24359"}{"text":"By examining the distributions of the latent themes for each object category , we construct an object taxonomy using the 101 object classes from the Caltech 101 datasets .een proposed for object categorization .There are also some discriminative part - based models that aim to find the boundaries among categories ( e.g.[10 , 18 , 23 ] ) .","label":"Future","metadata":{},"score":"59.458668"}{"text":"Techniques for improving lattice - based Maximum Mutual Information Estimation ( MMIE ) training are described and compared to Frame Discrimination ( FD ) .An objective function which is an interpolation of MMIE and standard Maximum Likelihood Estimation ( MLE ) is also discussed .","label":"Future","metadata":{},"score":"59.496155"}{"text":"On CCGbank we achieve a labelled dependency F - measure of 88.8 % on gold POS tags , and 86.7 % on automatic part - of - speeoch tags , the best reported results for this task . ... of combining models to avoid the pipeline problem ( Felzenszwalb and McAllester , 2007 ) is very much in line with much recent work in NLP . \" ...","label":"Future","metadata":{},"score":"59.50305"}{"text":"Unlik ... \" .Abstract .Bourdev and Malik ( ICCV 09 ) introduced a new notion of parts , poselets , constructed to be tightly clustered both in the configuration space of keypoints , as well as in the appearance space of image patches .","label":"Future","metadata":{},"score":"59.715954"}{"text":"It is more accurate than standard inside - outside re - estimation ( classic EM ) , significantly faster , and simpler .This generalizes to the Brown corpus , our held - out set , where accuracy reaches 50.8 % - a 7.5 % gain over previous best results .","label":"Future","metadata":{},"score":"59.742794"}{"text":"In this paper , a new acoustic modeling paradigm based on augmented conditional random fields ( ACRFs ) is investigated and developed .This paradigm addresses some limitations of HMMs while maintaining many of the aspects which have made them successful .","label":"Future","metadata":{},"score":"59.77"}{"text":"ICASSP , 2010 . \" ...This paper investigates syntactic and sub - lexical features in Turkish discriminative language models ( DLMs ) .DLM is a featurebased language modeling approach .It reranks the ASR output with discriminatively trained feature parameters .","label":"Future","metadata":{},"score":"59.777504"}{"text":"Tools . \" ...This paper describes , and evaluates on a large scale , the lattice based framework for discriminative training of large vocabulary speech recognition systems based on Gaussian mixture hidden Markov models ( HMMs ) .The paper concentrates on the maximum mutual information estimation ( MMIE ) criterion whi ... \" .","label":"Future","metadata":{},"score":"59.805935"}{"text":"Across eight European languages , our approach results in an average absolute improvement of 10.4 % over a state - of - the - art baseline , and 16.7 % over vanilla hidden Markov models induced with the Expectation Maximization algorithm .","label":"Future","metadata":{},"score":"60.00942"}{"text":"Even with second - order features or latent variables , which would make exact parsing considerably slower or NP - hard , BP needs only O(n3 ) time with a small constant factor .Furthermore , such features significantly improve parse accuracy over exact first - order methods .","label":"Future","metadata":{},"score":"60.342957"}{"text":"The paper concentrates on the maximum mutual information estimation ( MMIE ) criterion which has been used to train HMM systems for conversational telephone speech transcription using up to 265 hours of training data .These experiments represent the largest - scale application of discriminative training techniques for speech recognition of which the authors are aware , and have led to significant reductions in word error rate for both triphone and quinphone HMMs compared to our best models trained using maximum likelihood estimation .","label":"Future","metadata":{},"score":"60.44464"}{"text":"We present a directed Markov random field ( MRF ) model that combines n - gram models , probabilistic context free grammars ( PCFGs ) and probabilistic latent semantic analysis ( PLSA ) for the purpose of statistical language modeling .We generalize various smoothing techniques to alleviate the sparseness of n - gram counts in cases where there are hidden variables .","label":"Future","metadata":{},"score":"60.450066"}{"text":"Across eight European languages , our approach results in an average absolute improvement of 10.4 % over a state - of - the - art baseline , and 16.7 % over vanilla hidden Markov models induced with the Expectation Maximization algorithm . \" ...","label":"Future","metadata":{},"score":"60.830338"}{"text":"We show that dependency parsers have more difficulty parsing questions than constituency parsers .In particular , deterministic shift - reduce dependency parsers , which are of highest interest for practical applications because of their linear running time , drop to 60 % labeled accuracy on a question test set .","label":"Future","metadata":{},"score":"60.93885"}{"text":"Joint representation and modeling of morphological and lexical items reduces the OOV rate and provides smooth probability estimates while keeping the predictive power of whole words .Speech recognition and machine translation experiments in dialectal - Arabic show improvements over word and morpheme based trigram language models .","label":"Future","metadata":{},"score":"61.179165"}{"text":"We also present a proof that owl - qn is guaranteed to converge to a globally optimal parameter vector . \" ...Statistical parsers trained and tested on the Penn Wall Street Journal ( WSJ ) treebank have shown vast improvements over the last 10 years .","label":"Future","metadata":{},"score":"61.789505"}{"text":"This method substantially exceeds Klein and Manning 's published scores and achieves 39.4 % accuracy on Section 23 ( all sentences ) of the Wall Street Journal corpus .The second , Less is More , uses a low - complexity subset of the available data : sentences up to length 15 .","label":"Future","metadata":{},"score":"61.847816"}{"text":"In an empirical evaluation we show that our model consistently out - performs the current state - of - the - art across 10 languages . ...MM ) and also with the character LM ( 1HMM - LM ) .Starred entries denote results reported in CGS10 .","label":"Future","metadata":{},"score":"61.92173"}{"text":"On WSJ15 , we attain a state - of - the - art F - score of 90.9 % , a 14 % relative reduction in error over previous models , while being two orders of magnitude faster .On sentences of length 40 , our system achieves an F - score of 89.0 % , a 36 % relative reduction in error over a generative baseline . ...","label":"Future","metadata":{},"score":"62.05308"}{"text":"Despite its simplicity , our best grammar achieves an F1 of 90.2 % on the Penn Treebank , higher than fully lexicalized systems . ... reebank , higher than fully lexicalized systems .However , as demonstrated in Charniak ( 1996 ) and Klein and Manning ( 2003 ) , a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well .","label":"Future","metadata":{},"score":"62.308365"}{"text":"Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .On the other hand , our grammars are much more compact and substantially more accurate than previous work on automatic annotation .Despite its simplicity , our best grammar achieves an F1 of 89.9 % on the Penn Treebank , higher than most fully lexicalized systems .","label":"Future","metadata":{},"score":"62.489227"}{"text":"Ball Tracking :The reliable tracking of the ball is vital in robot soccer .Therefore a Kalman - filter based system for estimating the ball position and velocity in the presence of occlusions is developped . -Sensor Fusion : The robot perceives its environment through several independent sensors ( camera , odometer , etc . ) , which have different delays .","label":"Future","metadata":{},"score":"62.550217"}{"text":"It provides an open - source C++ implementation for the entire forest - to - string MT pipeline , including rule extraction , tuning , decoding , and evaluation .There are a number of options for ... \" .In this paper we describe Travatar , a forest - to - string machine translation ( MT ) engine based on tree transducers .","label":"Future","metadata":{},"score":"62.639954"}{"text":"We obtain state - of - theart performance for two tasks of structure prediction : unsupervised part - of - speech tagging and unsupervised dependency parsing . ... en used to initialize learning of the target language 's model using standard unsupervised parameter estimation . by Valentin I. Spitkovsky , Hiyan Alshawi , Daniel Jurafsky , Christopher D. Manning . \" ...","label":"Future","metadata":{},"score":"62.73618"}{"text":"This paper investigates syntactic and sub - lexical features in Turkish discriminative language models ( DLMs ) .DLM is a featurebased language modeling approach .It reranks the ASR output with discriminatively trained feature parameters .Syntactic information is incorporated into DLM as part - of - speech ( PoS ) tag n - gram features and head - to - head dependency relations .","label":"Future","metadata":{},"score":"62.764275"}{"text":"Many different methods have been proposed , yet comparisons are difficult to make since there is little consensus on evaluation framework , and many papers evaluate against only one or two competitor systems .Here we evaluate seven different POS induction systems spanning nearly 20 years of work , using a variety of measures .","label":"Future","metadata":{},"score":"63.122078"}{"text":"In most previous works using the \" bag of words \" ... \" .\" Bag of words \" models have enjoyed much attention and achieved good performances in recent studies of object categorization .In most of these works , local patches are modeled as basic building blocks of an image , analogous to words in text documents .","label":"Future","metadata":{},"score":"63.167633"}{"text":"Acoustic context modeling is explicitly integrated to handle the sequential phenomena of the speech signal .We present an efficient framework for estimating these models that ensures scalability and generality .In the TIMIT . ... m which utilizes second - order information to adapt the gradient step sizes [ 44].","label":"Future","metadata":{},"score":"63.5506"}{"text":"The models are encoded as deterministic weighted finite state automata ... \" .This paper describes discriminative language modeling for a large vocabulary speech recognition task .We contrast two parameter estimation methods : the perceptron algorithm , and a method based on conditional random fields ( CRFs ) .","label":"Future","metadata":{},"score":"63.570137"}{"text":"Our scene model is based on the transformed Dirichlet process ( TDP ) , a novel extension of the hierarchical DP in which a set of stochastically transformed mixture components are shared between multiple groups of data .For visual scenes , mixture components describe the spatial structure of visual features in an object - centered coordinate frame , while transformations model the object positions in a particular image .","label":"Future","metadata":{},"score":"63.66445"}{"text":"We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data .Results indicate that ( a ) MST - parser performs better on Hebrew data than Malt - Parser , and ( b ) both parsers do not make good use of morphological information when parsing Hebrew . ... s on Hebrew dependency parsing .","label":"Future","metadata":{},"score":"64.05318"}{"text":"We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data .Results indicate that ( a ) MST - parser performs better on Hebrew data than Malt - Parser , and ( b ) both parsers do not make good use of morphological information when parsing Hebrew . ... s on Hebrew dependency parsing .","label":"Future","metadata":{},"score":"64.05318"}{"text":"While deformable part models have become quite popular , their ... \" .We describe an object detection system based on mixtures of multiscale deformable part models .Our system is able to represent highly variable object classes and achieves state - of - the - art results in the PASCAL object detection challenges .","label":"Future","metadata":{},"score":"64.0829"}{"text":"We present an algorithm Orthant - Wise Limited - memory Quasi - Newton ( owlqn ) , based on l - bfgs , that can efficiently optimize the L1-regularized log - likelihood of log - linear models with millions of parameters .","label":"Future","metadata":{},"score":"64.47542"}{"text":"While parsing algorithms can be used to parse partial translations in phrase - based decoding , the se ... .Despite these successful efforts , challenges still remain for both directions .While parsing algorithms can be used to parse partial translations in phrase - based decoding , the se ... . \" ...","label":"Future","metadata":{},"score":"64.66728"}{"text":"Across various hierarchical encoding schemes and for multiple language pairs , we show speed - ups of up to 50 times over single - pass decoding while improving BLEU score .Moreover , our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram - to - trigram decoder .","label":"Future","metadata":{},"score":"65.08112"}{"text":"Our linguistic analysis confirms the strong connection between English punctuation and phrase boundaries in the Penn Treebank .However , approaches that naively include punctuation marks in the grammar ( as if they were words ) do not perform well with Klein and Manning 's Dependency Model with Valence ( DMV ) .","label":"Future","metadata":{},"score":"65.529785"}{"text":"Abstract - Language modeling for an inflected language such as Arabic poses new challenges for speech recognition and machine translation due to its rich morphology .Rich morphology results in large increases in out - of - vocabulary ( OOV ) rate and poor language model parameter estimation in the absence ... \" .","label":"Future","metadata":{},"score":"65.63787"}{"text":"Meanwhile , Graphics Processor Units ( GPUs ) have become widely available , offering the opportunity to alleviate this bottleneck by exploiting the fine - grained data parallelism found in the CKY algorithm .In this paper , we explore the design space of parallelizing the dynamic programming computations carried out by the CKY algorithm .","label":"Future","metadata":{},"score":"66.63817"}{"text":"WOE can operate in two modes : when restricted to POS tag features , it runs as quickly as TextRunner , but when set to use dependency - parse features its precision and recall rise even higher . ... h recall .","label":"Future","metadata":{},"score":"67.26776"}{"text":"However , parsing accuracies for Arabic usually lag behind non - semitic languages .Moreover , whil ...Tools . by Slav Petrov , Leon Barrett , Romain Thibaux , Dan Klein - In ACL ' 06 , 2006 . \" ...","label":"Future","metadata":{},"score":"67.84634"}{"text":"Leapfrog , our third heuristic , combines Less is More with Baby Steps by mixing their models of shorter sentences , then rapidly ramping up exposure to the full training set , driving up accuracy to 45.0 % .These trends generalize to the Brown corpus ; awareness of data complexity may improve other parsing models and unsupervised algorithms . .","label":"Future","metadata":{},"score":"69.20686"}{"text":"To facilitate future research in unsupervised induction of syntactic structure and to standardize best - practices , we propose a tagset that consists of twelve universal part - of - speech categories .In addition to the tagset , we develop a mapping from 25 different treebank tagsets to this universal set .","label":"Future","metadata":{},"score":"70.28097"}{"text":"Part - of - speech ( POS ) induction is one of the most popular tasks in research on unsupervised NLP .Many different methods have been proposed , yet comparisons are difficult to make since there is little consensus on evaluation framework , and many papers evaluate against only one or two competitor syste ... \" .","label":"Future","metadata":{},"score":"70.39116"}{"text":"The Stanford Parser is used to derive dependencies from CJ50 and gold parse trees .Figure 8 shows the detailed P / R curves .We can see that although today ... .by Jenny Rose Finkel , Alex Kleeman , Christopher D. Manning - In Proc .","label":"Future","metadata":{},"score":"71.06449"}{"text":".. \" ...We show how punctuation can be used to improve unsupervised dependency parsing .Our linguistic analysis confirms the strong connection between English punctuation and phrase boundaries in the Penn Treebank .However , approaches that naively include punctuation marks in the grammar ( as if they were wo ... \" .","label":"Future","metadata":{},"score":"71.467476"}{"text":"The shared task was run over 12 weeks , drawing initial interest from 42 teams .Of these teams , 24 submitted final results .The evaluation results are encouraging , indicating that state - of - the - art performance is approaching a practically applicable level and revealing some remaining challenges . ... parsers . \" ...","label":"Future","metadata":{},"score":"72.11029"}{"text":".. es into the language model .So far , the use of morphology for language modeling has been largely limited to segmenting words into morphemes to build a morpheme based language model . by Shaojun Wang , Dale Schuurmans , Fuchun Peng , Yunxin Zhao - In IEEE International Conference on Acoustics , Speech , and Signal Processing ( ICASSP03 , 2003 . \" ...","label":"Future","metadata":{},"score":"72.46031"}{"text":"We present extensive experiments on 22 language pairs , including preordering into English from 7 other languages .We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task .For languages from different families the improvements often exceed 2 BLEU .","label":"Future","metadata":{},"score":"72.64867"}{"text":"Rich morphology results in large increases in out - of - vocabulary ( OOV ) rate and poor language model parameter estimation in the absence of large quantities of data .In this study , we present a joint morphological - lexical language model ( JMLLM ) that takes advantage of Arabic morphology .","label":"Future","metadata":{},"score":"73.00803"}{"text":"We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task .For languages from different families the improvements often exceed 2 BLEU .Many of these gains are also significant in human evaluations .We present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages : German , English , Swedish , Spanish , French and Korean .","label":"Future","metadata":{},"score":"75.75142"}{"text":"We present a new edition of the Google Books Ngram Corpus , which describes how often words and phrases were used over a period of five centuries , in eight languages ; it reflects 6 % of all books ever published .","label":"Future","metadata":{},"score":"76.59927"}{"text":"These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .We present a new measure , Neutral Edge Direction ( NED ) , and show that it greatly reduces this undesired phenomenon .","label":"Future","metadata":{},"score":"77.50262"}{"text":"These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .We present a new measure , Neutral Edge Direction ( NED ) , and show that it greatly reduces this undesired phenomenon .","label":"Future","metadata":{},"score":"77.50262"}{"text":"For instance , 14.4 % of section 23 is tagged differently by ( 1 ) and ( 2 ) 8 .5 The Neutral Edge Direction ( NED ) Me ... . by Shay B. Cohen , Noah A. Smith , Alex Clark , Dorota Glowacka , Colin De La Higuera , Mark Johnson , John Shawe - taylor . \" ...","label":"Future","metadata":{},"score":"82.98453"}{"text":"For instance , 14.4 % of section 23 is tagged differently by ( 1 ) and ( 2 ) 8 .5 The Neutral Edge Direction ( NED ) Me ... . by Shay B. Cohen , Noah A. Smith , Alex Clark , Dorota Glowacka , Colin De La Higuera , Mark Johnson , John Shawe - taylor . \" ...","label":"Future","metadata":{},"score":"82.98453"}{"text":"ion window .Finer scale features are captured by part templates that can be moved with respect to the detection window .HOG Representation We follow the construction in [ 5 ] to define a dense representation of an image at a particular resolution .","label":"Future","metadata":{},"score":"85.14384"}{"text":"This Master 's thesis describes parts of the control software used by the soccer robots of the Free University of Berlin , the so called FU - Fighters .The FU - Fighters compete in the Middle Sized League of RoboCup and reached the semi - finals during the 2004 RoboCup World Cup in Lisbon , Portugal .","label":"Future","metadata":{},"score":"98.459854"}