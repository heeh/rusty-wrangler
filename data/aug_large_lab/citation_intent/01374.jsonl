{"text":"( Others have induced more basic text analysis tools . )No knowledge was used except for the sentence - alignment of the corpus and the original English parser ; given these resources , the building of a foreign - language parser was completely unsupervised .","label":"Background","metadata":{},"score":"34.315964"}{"text":"However , it seems reasonable to expect that the usefulness of the models trained using LinkSet would be limited by its ability to assign meaningful correspondences between the English and foreign versions of a training sentence .Inducing a Monolingual Parser through Bilingual Parsing .","label":"Background","metadata":{},"score":"35.310455"}{"text":"After using this corpus as input to W OLFIE 12 , the learned lexicon was compared to the original lexicon , and weighted precision and weighted recall of the learned lexicon were measured .Precision measures the percentage of the lexicon entries ( i.e. , word - meaning pairs ) that the system learns that are correct .","label":"Background","metadata":{},"score":"36.195805"}{"text":"Here is a concise summary of the conclusions detailed in the chapters above : .A statistical model of the relationship between the syntactic structures of two different languages can be effectively learned from a bilingual corpus by an unsupervised learning technique , even when syntactic annotations are available for only one of the languages .","label":"Background","metadata":{},"score":"36.904045"}{"text":"Typically , the training data must consist of real sentences annotated with structural information of the kind the parser will eventually generate .Unfortunately , annotating these sentences can require a huge amount of work by language experts , comparable to that required to develop a rule - based grammar .","label":"Background","metadata":{},"score":"36.937355"}{"text":"A third advantage of this technique is its versatility .Training data for a statistical parser could come not only from a single existing parser , but from various sources , if desired .For example , a few hand - coded examples could be added to introduce a new grammatical construct not found in the original parser , or the output of two or three parsers with different strengths and weaknesses could be combined to produce a new parser that combines their strengths .","label":"Background","metadata":{},"score":"37.46351"}{"text":"Secondly , the use of real sentences in training actually adds information to the system , enabling to make use of lexical and distributional knowledge ignored by the original rule - based system , which is especially useful in ambiguous cases .","label":"Background","metadata":{},"score":"38.098488"}{"text":"Bilingual Parsing .When given a bilingual corpus where a parser is already available for the opposite language ( such as English ) , this could be done in principle by word - aligning the corpus and ' ' copying ' ' the structure across the alignments .","label":"Background","metadata":{},"score":"38.20318"}{"text":"Thus a rule - based parser can be parlayed into a new parser with all the advantages of a statistical technique , without the need for a language expert .However , this technique has several advantages .Statistical parsing is more robust than rule - based methods , and can respond to difficult inputs with graceful degradation rather than sudden failure .","label":"Background","metadata":{},"score":"38.62623"}{"text":"They also have the advantage of being easier to build and to customize , because they do not require the work of a language expert to carefully design a grammar and patiently encode a dictionary .Instead , given an appropriate framework , all but the most basic grammar rules can be learned automatically from data , resulting in a huge savings in time and effort , especially if an existing parsing system is being ported to a new language or domain .","label":"Background","metadata":{},"score":"38.96589"}{"text":"By using these multi - level meaning representations we demonstrate the learning of more complex representations than those in the geography database domain : none of the hand - built meanings for phrases in that lexicon had functors embedded in arguments .","label":"Background","metadata":{},"score":"39.348602"}{"text":"The exact definition of this specialized grammar is conceptualized as a series of transformations applied to the English structure to arrive at the foreign structure .These transformations constrain the range of structures considered possible for the foreign sentence .Then word - to - word translation probabilities to generate the actual foreign sentence from the transformed English one assign a probability to each possible structure , allowing the most probable to be selected .","label":"Background","metadata":{},"score":"39.62928"}{"text":"In order to build a statistical translation system that accurately models the hierarchical structure of natural language , it would be very useful to be able to train a model of a structured foreign sentence given a structured English sentence .This would require designing a model capable of capturing the relevant features of the relationship between structures in two languages .","label":"Background","metadata":{},"score":"40.081955"}{"text":"Training this model with a bilingual corpus , we will test its ability to align sentences by comparing the most probable alignments according to the model , as compared to human - annotated alignments .Finally , we will use these models together in two different ways .","label":"Background","metadata":{},"score":"40.1059"}{"text":"To test this ability to quickly build a working parser for a foreign language , we used two bilingual corpora ( French -- English and Chinese -- English ) to automatically build monolingual parsers for French and Chinese .We then tested these parsers by parsing test sets in each language , and comparing the structures assigned by the new parsers against hand - bracketings of the same test sentences .","label":"Background","metadata":{},"score":"40.221626"}{"text":"While many translation systems use phrase - to - phrase alignment , these approaches typically deal only with contiguous phrases .As our experiment shows , a simple extension to the LinkSet model can enable it to handle discontiguous phrases .( While a relatively straightforward extension of the LinkSet model , it is still significant enough a change to be implemented with a quick fix to the existing code base ; hence the narrowly focused experiment . )","label":"Background","metadata":{},"score":"40.263416"}{"text":"To get weighted precision and recall measures , we then weight the results for each pair by the word 's frequency in the entire corpus ( not just the training corpus ) .This models how likely we are to have learned the correct meaning for an arbitrarily chosen word in the corpus .","label":"Background","metadata":{},"score":"40.362717"}{"text":"Because it is expensive to annotate examples by hand , we used an existing parser to analyze sentences collected from news articles , and used its output as training data .Although this automatically - generated training set has significantly more errors than one would expect from a hand - labeled training set , the Lynx parser is able to score as well as the Link parser at constituent bracketing .","label":"Background","metadata":{},"score":"40.42182"}{"text":"In principle , these models of English sentence structure and of foreign sentence structure given English sentence structure can be composed to generate a model of foreign sentence structure Pr(F ) , which can then be used to parse foreign sentences monolingually .","label":"Background","metadata":{},"score":"40.46957"}{"text":"However , it can be approximated .Once we build a model of foreign structure , we can parse foreign sentences the same way we do English : .Following the noisy - channel interpretation of statistical translation that has become standard , we pretend that the foreign sentence f is a transformed version of some ' ' original ' ' English sentence e .","label":"Background","metadata":{},"score":"40.474323"}{"text":"With this combined model , the best translation of a foreign sentence f can in principle be found .In practice , the sum over all English structures and over all foreign structures consistent with the given foreign sentence is infeasible to calculate .","label":"Background","metadata":{},"score":"40.69775"}{"text":"The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet Agirre , E. and Rigau , G. ( 1996 ) .","label":"Background","metadata":{},"score":"40.89411"}{"text":"However , our case study suggests that for any particular language pair , progress could quickly be made from a quick and dirty induced parser to a higher - quality parser through focused incremental improvements such as the one we have demonstrated .","label":"Background","metadata":{},"score":"41.34406"}{"text":"Combine the new scores with those from the translation system .Rank the hypothesis list by the interpolated scores .The top - ranked candidate is now output as the English translation of the foreign sentence .To test this idea , we trained the Lynx and LinkSet parsers using a Chinese -- English corpus , and used an existing statistical translation system to generate 100-best lists of English translations for another corpus of Chinese sentences , for each of which four reference translations were available .","label":"Background","metadata":{},"score":"41.402313"}{"text":"We now show that our algorithm scales up well with more lexicon items to learn , more ambiguity , and more synonymy .These factors are difficult to control when using real data as input .Also , there are no large corpora available that are annotated with semantic parses .","label":"Background","metadata":{},"score":"41.567947"}{"text":"We will test this new parser by comparing its accuracy in bracketing unseen foreign sentences with respect to human - annotated bracketings .Second , we will enhance an automatic translation system by using these structured models to reevaluate the hypotheses it generates , comparing the resulting translations against the original system 's output using standard evaluation metrics such as the BLEU and NIST scores .","label":"Background","metadata":{},"score":"41.572235"}{"text":"A new method of inducing a parser for a foreign language given a bilingual corpus and an English parser , implemented as a combination of the Lynx and LinkSet parsers .A new method of incorporating syntactic structure into a statistical translation model , demonstrated by reranking decoder outputs using the Lynx and LinkSet parsers .","label":"Background","metadata":{},"score":"41.821632"}{"text":"This experiment involved the following steps : . # deriving a lexicon from the WordNet data files which contains all possible semantic tags for each noun , adjective , adverb and verb .Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored .","label":"Background","metadata":{},"score":"41.876663"}{"text":"This experiment involved the following steps : . # deriving a lexicon from the WordNet data files which contains all possible semantic tags for each noun , adjective , adverb and verb .Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored .","label":"Background","metadata":{},"score":"41.876663"}{"text":"This experiment involved the following steps : . # deriving a lexicon from the WordNet data files which contains all possible semantic tags for each noun , adjective , adverb and verb .Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored .","label":"Background","metadata":{},"score":"41.876663"}{"text":"This experiment involved the following steps : . # deriving a lexicon from the WordNet data files which contains all possible semantic tags for each noun , adjective , adverb and verb .Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored .","label":"Background","metadata":{},"score":"41.876663"}{"text":"This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .","label":"Background","metadata":{},"score":"42.01399"}{"text":"This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .","label":"Background","metadata":{},"score":"42.01399"}{"text":"This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .","label":"Background","metadata":{},"score":"42.01399"}{"text":"This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .","label":"Background","metadata":{},"score":"42.01399"}{"text":"This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .","label":"Background","metadata":{},"score":"42.01399"}{"text":"This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .","label":"Background","metadata":{},"score":"42.01399"}{"text":"This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .","label":"Background","metadata":{},"score":"42.01399"}{"text":"This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .","label":"Background","metadata":{},"score":"42.01399"}{"text":"This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .","label":"Background","metadata":{},"score":"42.01399"}{"text":"This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .","label":"Background","metadata":{},"score":"42.01399"}{"text":"This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .","label":"Background","metadata":{},"score":"42.01399"}{"text":"First , we automatically induce a parser for a foreign language across a bilingual corpus .Then we improve the output of an automatic translation system by incorporating syntactic structure into its translation and language models .Thesis Statement .My thesis statement has 4 parts : .","label":"Background","metadata":{},"score":"42.69232"}{"text":"Such corpora are not usually available anyway , especially if the two languages must be annotated using the same formalism .Solving these would allow us to build both a parser for the foreign language and a statistical translation system between that language and English that takes into account the syntactic structure of each sentence .","label":"Background","metadata":{},"score":"42.71437"}{"text":"Integrating syntax structure into the statistical models used for automatic translation can increase the quality of translated output .The use of syntax - based models significantly improves the overall ranking of hypotheses according to the standard NIST evaluation of each hypothesis .","label":"Background","metadata":{},"score":"42.716522"}{"text":"In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .This representation can then be applied to new instances in order to disambiguate them .","label":"Background","metadata":{},"score":"42.91908"}{"text":"In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .This representation can then be applied to new instances in order to disambiguate them .","label":"Background","metadata":{},"score":"42.91908"}{"text":"In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .This representation can then be applied to new instances in order to disambiguate them .","label":"Background","metadata":{},"score":"42.91908"}{"text":"In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .This representation can then be applied to new instances in order to disambiguate them .","label":"Background","metadata":{},"score":"42.91908"}{"text":"Even when I am only using 50 sents of the corpus for training its still taking about 10 minutes to finish all 100 iterations .So how did you train your ClassifierBasedPOSTagger on the whole corpus without waiting 1 week ?I hope , my English is not too bad and you understand me .","label":"Background","metadata":{},"score":"43.085045"}{"text":"To parse a sentence is to resolve it into its component parts and describe its grammatical structure .Applications that benefit from syntactic parsing include corpus analysis , question answering , natural - language command execution , rule - based automatic translation , and summarization .","label":"Background","metadata":{},"score":"43.085823"}{"text":"The overview above serves as an outline of this thesis .We will first introduce a model of English sentence structure , along with a method for using it to parse English sentences .Using training data derived from an existing parser , we will train the model , and then test this new statistical parser by comparing its ability to bracket sentence constituents with that of the other parser , as measured against human - annotated bracketings .","label":"Background","metadata":{},"score":"43.341618"}{"text":"The meaning of a word in a particular usage can only be determined by examining its context .This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : .","label":"Background","metadata":{},"score":"43.8061"}{"text":"However , the models used in most statistical translation systems are much too simple to capture to syntactic structure of each sentence , often resulting in output that is ungrammatical or has the wrong meaning .Because important information is carried not only in the choice of words , but in their grammatical relationships , it is necessary to incorporate syntactic structure into translation models in order to meaning to be conveyed accurately and grammatically .","label":"Background","metadata":{},"score":"43.84691"}{"text":"In this corpus , both the sentences and their representations are completely artificial , and the sentence representation is a variable - free representation , as suggested by the work of Jackendoff ( 1990 ) and others .For each corpus discussed below , a random lexicon mapping words to simulated meanings was first constructed .","label":"Background","metadata":{},"score":"43.924835"}{"text":"Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet Agirre , E. and Rigau , G. ( 1996 ) .","label":"Background","metadata":{},"score":"44.016243"}{"text":"This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .","label":"Background","metadata":{},"score":"44.214996"}{"text":"This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .","label":"Background","metadata":{},"score":"44.214996"}{"text":"This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .","label":"Background","metadata":{},"score":"44.214996"}{"text":"This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .","label":"Background","metadata":{},"score":"44.214996"}{"text":"This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .","label":"Background","metadata":{},"score":"44.214996"}{"text":"This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .","label":"Background","metadata":{},"score":"44.214996"}{"text":"This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .","label":"Background","metadata":{},"score":"44.214996"}{"text":"This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .","label":"Background","metadata":{},"score":"44.214996"}{"text":"This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .","label":"Background","metadata":{},"score":"44.214996"}{"text":"One of the reasons the Lynx parser is able to do so well is that it gains the benefit of the grammatical and distributional knowledge implicit in the sentences used for training , even though the analysis of these sentences by the Link parser is fallible .","label":"Background","metadata":{},"score":"44.380623"}{"text":"Weighted precision at 1650 examples drops to 65.4 % from the previous level of 99.3 % , and weighted recall to 58.9 % from 99.3 % .The full learning curve is shown in Figure 17 .A quick comparison to Siskind 's performance on this corpus confirmed that his system achieved comparable performance , showing that with current methods , this is close to the best performance that we are able to obtain on this more difficult corpus .","label":"Background","metadata":{},"score":"44.46135"}{"text":"A statistical model of the relationship between the syntactic structures of two different languages can be effectively learned from a bilingual corpus by an unsupervised learning technique , even when syntactic annotations are available for only one of the languages .Using a bilingual corpus and an existing parser in one language , a new parser can be automatically induced for the other language , without the aid of a language expert .","label":"Background","metadata":{},"score":"44.4934"}{"text":"In these cases , correct analysis may require lexical and distributional knowledge not found in hand - crafted grammar rules .Instead of attempting to encode this knowledge manually , which would be far too difficult , researchers have turned to corpus - based statistical techniques , in which lexical and distributional knowledge is gathered from large corpora of real human - generated sentences .","label":"Background","metadata":{},"score":"44.53968"}{"text":"Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored . constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .","label":"Background","metadata":{},"score":"44.81703"}{"text":"Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored . constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .","label":"Background","metadata":{},"score":"44.81703"}{"text":"Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored . constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .","label":"Background","metadata":{},"score":"44.81703"}{"text":"Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored . constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .","label":"Background","metadata":{},"score":"44.81703"}{"text":"Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored . constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .","label":"Background","metadata":{},"score":"44.81703"}{"text":"Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored . constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .","label":"Background","metadata":{},"score":"44.81703"}{"text":"Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored . constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .","label":"Background","metadata":{},"score":"44.81703"}{"text":"Since ancient times , the ability to translate from one language to another has been valued .Automatic translation is now a reality , though for open - domain translation , automated systems can not yet come anywhere near a bilingual human in quality .","label":"Background","metadata":{},"score":"45.32113"}{"text":"To appear in Journal of Natural Language Engineering , 4(3 ) .use large lexicons ( generally machine readable dictionaries ) and the information associated with the senses ( such as part - of - speech tags , topical guides and selectional preferences ) to indicate the correct sense .","label":"Background","metadata":{},"score":"45.39411"}{"text":"To appear in Journal of Natural Language Engineering , 4(3 ) .use large lexicons ( generally machine readable dictionaries ) and the information associated with the senses ( such as part - of - speech tags , topical guides and selectional preferences ) to indicate the correct sense .","label":"Background","metadata":{},"score":"45.39411"}{"text":"This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky [ 13 ] .","label":"Background","metadata":{},"score":"45.402252"}{"text":"This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky [ 13 ] .","label":"Background","metadata":{},"score":"45.402252"}{"text":"This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky [ 13 ] .","label":"Background","metadata":{},"score":"45.402252"}{"text":"This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky [ 13 ] .","label":"Background","metadata":{},"score":"45.402252"}{"text":"This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky [ 13 ] .","label":"Background","metadata":{},"score":"45.402252"}{"text":"This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky [ 13 ] .","label":"Background","metadata":{},"score":"45.402252"}{"text":"This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky [ 13 ] .","label":"Background","metadata":{},"score":"45.402252"}{"text":"These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .","label":"Background","metadata":{},"score":"45.58412"}{"text":"These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .","label":"Background","metadata":{},"score":"45.58412"}{"text":"These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .","label":"Background","metadata":{},"score":"45.58412"}{"text":"These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .","label":"Background","metadata":{},"score":"45.58412"}{"text":"These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .","label":"Background","metadata":{},"score":"45.58412"}{"text":"These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .","label":"Background","metadata":{},"score":"45.58412"}{"text":"These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .","label":"Background","metadata":{},"score":"45.58412"}{"text":"These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .","label":"Background","metadata":{},"score":"45.58412"}{"text":"These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .","label":"Background","metadata":{},"score":"45.58412"}{"text":"These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .","label":"Background","metadata":{},"score":"45.58412"}{"text":"These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .","label":"Background","metadata":{},"score":"45.58412"}{"text":"These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .","label":"Background","metadata":{},"score":"45.58412"}{"text":"These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .","label":"Background","metadata":{},"score":"45.58412"}{"text":"In these tests , we use a lexicon containing 1000 words and using 250 conceptual symbols .We generated both a corpus with no ambiguity , and one from a lexicon with ambiguity and synonymy similar to that found in the WordNet database [ Beckwith et al.1991 ] ; the ambiguity there is approximately 1.68 meanings per word and the synonymy 1.3 words per meaning .","label":"Background","metadata":{},"score":"45.64795"}{"text":"Then you train the brill tagger using the same corpus & your initial tagger .I am a bit confused about your evaluation method .It seems like you evaluated the default NLTK tagger on the brown corpus without any tag conversion ...","label":"Background","metadata":{},"score":"45.902565"}{"text":"Thus when confronted with a sentence that would be considered ungrammatical by a more rigid grammar , a stochastic syntax model simply assigns a lower score , nevertheless selecting the structure most probable under the circumstances .This generalization yields a great increase in robustness , allowing performance to degrade gracefully on more difficult sentences .","label":"Background","metadata":{},"score":"45.91165"}{"text":"While this training is a fully automated process , given a human - generated but unannotated text corpus ( a plentiful resource ) , it is a semi - supervised learning process because the explicit grammar knowledge exercised by the Link parser acts as a kind of supervisor .","label":"Background","metadata":{},"score":"45.931786"}{"text":"In order to enable computers to handle the complexities of natural language , and of many other phenomena as well , we need the combined power of theoretical knowledge and real - world statistics .Creating tools to bring these together should pay off not only in natural language processing , but in a variety of fields .","label":"Background","metadata":{},"score":"46.113235"}{"text":"A rough - grain approximation of expectation maximization training does not improve the model .However , a more fine - grained EM approach might work much better .The induced parser can be incrementally improved through cleaning of the translation lexicon and adjustment of the model to handle common structures missed during the original training .","label":"Background","metadata":{},"score":"46.213814"}{"text":"( 1996 ) which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .A similarity matrix is thus formed which is subject to cluster analysis to determine groups of semantically related instances of terms .","label":"Background","metadata":{},"score":"46.239357"}{"text":"( 1996 ) which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .A similarity matrix is thus formed which is subject to cluster analysis to determine groups of semantically related instances of terms .","label":"Background","metadata":{},"score":"46.239357"}{"text":"In some sense it could be considered a relief , considering the huge computational cost involved .However , perhaps a better EM technique would yield improvement , since our EM training was a fairly crude approximation of the ideal parameter estimation method , with sentence - level rather than feature - level granularity .","label":"Background","metadata":{},"score":"46.498444"}{"text":"To train this transformation model , an EM approach can be used , updating the likelihood of each transformation according to the probability of each sentence structure that uses it .In addition to improving the quality of foreign structures inferred , this transformational model can then be used in a translation system .","label":"Background","metadata":{},"score":"46.593987"}{"text":"decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .This experiment involved the following steps : . # deriving a lexicon from the WordNet data files which contains all possible semantic tags for each noun , adjective , adverb and verb .","label":"Background","metadata":{},"score":"46.8245"}{"text":"decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .This experiment involved the following steps : . # deriving a lexicon from the WordNet data files which contains all possible semantic tags for each noun , adjective , adverb and verb .","label":"Background","metadata":{},"score":"46.8245"}{"text":"In this thesis we incorporated linguistic knowledge into statistical models of language structure and of the relationship between different languages ' structures .The linguistic knowledge appears in the guise of model structure and of the selection and dependencies of model parameters .","label":"Background","metadata":{},"score":"46.98442"}{"text":"Finally , we show the change in performance with increasing ambiguity and increasing synonymy , holding the number of words and conceptual symbols constant .Figure 18 shows the weighted precision and recall with 1050 training examples for increasing levels of ambiguity , holding the synonymy level constant .","label":"Background","metadata":{},"score":"47.22329"}{"text":"This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .","label":"Background","metadata":{},"score":"47.24541"}{"text":"This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .","label":"Background","metadata":{},"score":"47.24541"}{"text":"This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .","label":"Background","metadata":{},"score":"47.24541"}{"text":"This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .","label":"Background","metadata":{},"score":"47.24541"}{"text":"This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .","label":"Background","metadata":{},"score":"47.24541"}{"text":"This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .","label":"Background","metadata":{},"score":"47.24541"}{"text":"Approaches .Knowledge based .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet [ 5 ] and LDOCE [ 6 ] .","label":"Background","metadata":{},"score":"48.02209"}{"text":"Approaches .Knowledge based .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet [ 5 ] and LDOCE [ 6 ] .","label":"Background","metadata":{},"score":"48.02209"}{"text":"Approaches .Knowledge based .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet [ 5 ] and LDOCE [ 6 ] .","label":"Background","metadata":{},"score":"48.02209"}{"text":"Approaches .Knowledge based .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet [ 5 ] and LDOCE [ 6 ] .","label":"Background","metadata":{},"score":"48.02209"}{"text":"Approaches .Knowledge based .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet [ 5 ] and LDOCE [ 6 ] .","label":"Background","metadata":{},"score":"48.02209"}{"text":"Approaches .Knowledge based .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet [ 5 ] and LDOCE [ 6 ] .","label":"Background","metadata":{},"score":"48.02209"}{"text":"Approaches .Knowledge based .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet [ 5 ] and LDOCE [ 6 ] .","label":"Background","metadata":{},"score":"48.02209"}{"text":"In all four cases , the LinkSet bilingual parser was able to align the words of bilingual sentence pairs significantly better than a baseline technique which took the most probable alignment according to the word - translation model without any structural information .","label":"Background","metadata":{},"score":"48.354668"}{"text":"For each sentence , sorting the 100 translation hypotheses by score yields a hypothesis ranking .We ranked the hypotheses according to the original decoder score , the Lynx score , the LinkSet score , and by combinations of these scores .","label":"Background","metadata":{},"score":"48.471657"}{"text":"For the easier large corpus , the maximum average of weighted precision and recall was 85.6 % , at 8100 training examples , while for the harder corpus , the maximum average was 63.1 % at 8600 training examples .Next : Active Learning Up : Evaluation of WOLFIE Previous : LICS versus Fracturing is being used in a textual context .","label":"Background","metadata":{},"score":"48.632866"}{"text":"This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .","label":"Background","metadata":{},"score":"48.6593"}{"text":"This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .","label":"Background","metadata":{},"score":"48.6593"}{"text":"So I want to assign correct POS - tags to a set of non - standard words .It 's clear about tagging single words .But when it comes to multiple - word phrases , I 'm not sure about the solution .","label":"Background","metadata":{},"score":"49.012886"}{"text":"Integrating syntax structure into the statistical models used for automatic translation can increase the quality of translated output .Overview .This section gives a high - level theoretical overview of the models developed throughout this thesis .Beginning with the structural model of English sentences used in the Lynx parser we then introduce the model of the relationship between English and a foreign language used in the LinkSet bilingual parser .","label":"Background","metadata":{},"score":"49.223648"}{"text":"However , we were able to address this problem fairly easily by narrowing the overly broad word - translation model for those specific words and by allowing some English words to effectively align to more than one foreign word .The problem of discontiguous many - to - one alignment is quite general and may be at the root of many misaligned phrases .","label":"Background","metadata":{},"score":"49.488388"}{"text":"My testing seems to confirm this , I get 57 % accuracy on a subset of the Brown corpus using the default POS tagger without any conversions , which is similar to your results .I am new to this , so perhaps I misunderstood something , please let me know !","label":"Background","metadata":{},"score":"49.80701"}{"text":"In the original formulation , the LinkSet model was unable to account for this phrase properly , because of its one - to - one alignment model .While the LinkSet model can , through the use of null links , deal reasonably well with contiguous multi - word phrases aligned to single words in the other language , a non - contiguous phrase presented a problem .","label":"Background","metadata":{},"score":"49.93594"}{"text":"Begin with a foreign sentence needing translation into English .Get an N - best list from some other translation system .Rescore each translation hypothesis as follows : .Parse the candidate using the Lynx parser , obtaining the most probable structure and its probability .","label":"Background","metadata":{},"score":"49.978268"}{"text":"The main point of this work has been the integration of syntactic structure into the statistical models used in translation --- both monolingual language models and models of translation from one language to another .Through the development of the Lynx and LinkSet models and parsers , we have taken a big step in that direction , and shown that adding models of syntactic structure can improve the output of a state - of - the - art statistical translation system .","label":"Background","metadata":{},"score":"50.076923"}{"text":"As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .With the rise of statistical methods in CL in the 1990s , WSD became one of the main focus ' of supervised learning techniques .","label":"Background","metadata":{},"score":"50.120705"}{"text":"natural language processing : A case study in part of speech tagging .Computational Linguistics , 21(4):543 - 566 , December 1995 .The problem is that words often have more than one meaning , sometimes fairly similar and sometimes completely different .","label":"Background","metadata":{},"score":"50.284157"}{"text":"Expectation maximization training improves the model .Careful model design is important in order to avoid the complementary hazards of computational complexity and data sparsity .Using a bilingual corpus and an existing parser in one language , a new parser can be automatically induced for the other language , without the aid of a language expert .","label":"Background","metadata":{},"score":"50.28856"}{"text":"The French version , on the other hand , is almost the reverse of the Chinese : While failing to improve very much over its baseline , it does quite well overall , with high precision and recall .The French baseline scores higher than the English baseline , perhaps because the French sentences are more consistently right - branching than English or Chinese , or perhaps because the hand - annotator favored right - branching interpretations of the sentences .","label":"Background","metadata":{},"score":"50.312595"}{"text":"use large lexicons ( generally machine readable dictionaries ) and the information associated with the senses ( such as part - of - speech tags , topical guides and selectional preferences ) to indicate the correct sense .Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .","label":"Background","metadata":{},"score":"50.577675"}{"text":"use large lexicons ( generally machine readable dictionaries ) and the information associated with the senses ( such as part - of - speech tags , topical guides and selectional preferences ) to indicate the correct sense .Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .","label":"Background","metadata":{},"score":"50.577675"}{"text":"use large lexicons ( generally machine readable dictionaries ) and the information associated with the senses ( such as part - of - speech tags , topical guides and selectional preferences ) to indicate the correct sense .Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .","label":"Background","metadata":{},"score":"50.577675"}{"text":"use large lexicons ( generally machine readable dictionaries ) and the information associated with the senses ( such as part - of - speech tags , topical guides and selectional preferences ) to indicate the correct sense .Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .","label":"Background","metadata":{},"score":"50.577675"}{"text":"As I ca n't fully rely on it , I think I could define some simple grammar rules manually and if they do n't return results , a trained chunker could be used .I 'm new to NLTK and I would appreciate any suggestions to the overall process .","label":"Background","metadata":{},"score":"50.82083"}{"text":"In Proceedings of the 14th International Conference on Computational Linguistics ( COLING-92 ) , pages 454 - 460 , Nantes , France , 1992 .This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .","label":"Background","metadata":{},"score":"50.913235"}{"text":"In Proceedings of the 14th International Conference on Computational Linguistics ( COLING-92 ) , pages 454 - 460 , Nantes , France , 1992 .This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .","label":"Background","metadata":{},"score":"50.913235"}{"text":"This improvement was limited by the small space available for improvement of a state - of - the - art statistical translation system through reranking of translation hypotheses .These are the main contributions of this thesis : .A new technique for training a statistical link grammar parser from examples , implemented as the Lynx parser .","label":"Background","metadata":{},"score":"51.236702"}{"text":"These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .A good example of this is Luk 's system [ 12 ] which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .","label":"Background","metadata":{},"score":"51.342632"}{"text":"These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .A good example of this is Luk 's system [ 12 ] which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .","label":"Background","metadata":{},"score":"51.342632"}{"text":"These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .A good example of this is Luk 's system [ 12 ] which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .","label":"Background","metadata":{},"score":"51.342632"}{"text":"These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .A good example of this is Luk 's system [ 12 ] which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .","label":"Background","metadata":{},"score":"51.342632"}{"text":"These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .A good example of this is Luk 's system [ 12 ] which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .","label":"Background","metadata":{},"score":"51.342632"}{"text":"These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .A good example of this is Luk 's system [ 12 ] which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .","label":"Background","metadata":{},"score":"51.342632"}{"text":"These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .A good example of this is Luk 's system [ 12 ] which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .","label":"Background","metadata":{},"score":"51.342632"}{"text":"[ 2 ] However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .","label":"Background","metadata":{},"score":"51.345795"}{"text":"[ 2 ] However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .","label":"Background","metadata":{},"score":"51.345795"}{"text":"[ 2 ] However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .","label":"Background","metadata":{},"score":"51.345795"}{"text":"[ 2 ] However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .","label":"Background","metadata":{},"score":"51.345795"}{"text":"[ 2 ] However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .","label":"Background","metadata":{},"score":"51.345795"}{"text":"[ 2 ] However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .","label":"Background","metadata":{},"score":"51.345795"}{"text":"[ 2 ] However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .","label":"Background","metadata":{},"score":"51.345795"}{"text":"In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .In 1975 Kelly and Stone E.F. Kelly and P.J. Stone .Computer Recognition of English Word Senses , Amsterdam : North - Holland . published a book explicitly listing their rules for disambiguation of word senses .","label":"Background","metadata":{},"score":"51.375526"}{"text":"In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .In 1975 Kelly and Stone E.F. Kelly and P.J. Stone .Computer Recognition of English Word Senses , Amsterdam : North - Holland . published a book explicitly listing their rules for disambiguation of word senses .","label":"Background","metadata":{},"score":"51.375526"}{"text":"In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .In 1975 Kelly and Stone E.F. Kelly and P.J. Stone .Computer Recognition of English Word Senses , Amsterdam : North - Holland . published a book explicitly listing their rules for disambiguation of word senses .","label":"Background","metadata":{},"score":"51.375526"}{"text":"Meanwhile , the LinkSet score is able to bring a small improvement when added to the decoder score , but not as much as the Lynx score .For long sentences , especially those 80 words and longer , the lines converge .","label":"Background","metadata":{},"score":"51.49783"}{"text":"Our approach , which implicitly incorporates knowledge into the design of an over - arching statistical model , is in many ways more elegant because it has a simple probabilistic interpretation and is very flexible in its ability to be trained on new data without special programming .","label":"Background","metadata":{},"score":"51.529182"}{"text":"[ 1 ] The problem is that words often have more than one meaning , sometimes fairly similar and sometimes completely different .The meaning of a word in a particular usage can only be determined by examining its context .This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : .","label":"Background","metadata":{},"score":"51.53587"}{"text":"[ 1 ] The problem is that words often have more than one meaning , sometimes fairly similar and sometimes completely different .The meaning of a word in a particular usage can only be determined by examining its context .This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : .","label":"Background","metadata":{},"score":"51.53587"}{"text":"[ 1 ] The problem is that words often have more than one meaning , sometimes fairly similar and sometimes completely different .The meaning of a word in a particular usage can only be determined by examining its context .This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : .","label":"Background","metadata":{},"score":"51.53587"}{"text":"[ 1 ] The problem is that words often have more than one meaning , sometimes fairly similar and sometimes completely different .The meaning of a word in a particular usage can only be determined by examining its context .This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : .","label":"Background","metadata":{},"score":"51.53587"}{"text":"[ 1 ] The problem is that words often have more than one meaning , sometimes fairly similar and sometimes completely different .The meaning of a word in a particular usage can only be determined by examining its context .This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : .","label":"Background","metadata":{},"score":"51.53587"}{"text":"[ 1 ] The problem is that words often have more than one meaning , sometimes fairly similar and sometimes completely different .The meaning of a word in a particular usage can only be determined by examining its context .This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : .","label":"Background","metadata":{},"score":"51.53587"}{"text":"Unfortunately , parsing is a hard problem , and constructing a new parsing system that works reasonably well typically requires a large effort by language experts , either in designing a grammar or in annotating training data .However , by training the Lynx parser ) using data annotated automatically by the LinkSet bilingual parser , a quick - and - dirty parser can be constructed with minimal effort and without much need for language expertise .","label":"Background","metadata":{},"score":"51.730656"}{"text":"The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .","label":"Background","metadata":{},"score":"51.85644"}{"text":"The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .","label":"Background","metadata":{},"score":"51.85644"}{"text":"The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .","label":"Background","metadata":{},"score":"51.85644"}{"text":"The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .","label":"Background","metadata":{},"score":"51.85644"}{"text":"The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .","label":"Background","metadata":{},"score":"51.85644"}{"text":"The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .","label":"Background","metadata":{},"score":"51.85644"}{"text":"The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .","label":"Background","metadata":{},"score":"51.85644"}{"text":"After chunking , it should be easy to split up any word with \" _ \" in it .An alternative solution is to transform your list of phrases into a corpus of tagged & chunked phrases , then train a tagger and chunker on it .","label":"Background","metadata":{},"score":"51.94264"}{"text":"Our experiments have shown that scores assigned by the Lynx and LinkSet parsers to translation decoder output can produce a significant improvement in the ranking of the hypotheses over the decoder 's original ranking .However , when only the number - one hypothesis is taken into account , no significant gains were made in the end - to - end performance of the translation system .","label":"Background","metadata":{},"score":"52.028328"}{"text":"Meaning representations were generated using a set of ' ' conceptual symbols ' ' that combined to form the meaning for each word .The number of conceptual symbols used in each lexicon will be noted when we describe each corpus below .","label":"Background","metadata":{},"score":"52.039295"}{"text":"Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .","label":"Background","metadata":{},"score":"52.04322"}{"text":"Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .","label":"Background","metadata":{},"score":"52.04322"}{"text":"Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .","label":"Background","metadata":{},"score":"52.04322"}{"text":"Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .","label":"Background","metadata":{},"score":"52.04322"}{"text":"Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .","label":"Background","metadata":{},"score":"52.04322"}{"text":"Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .","label":"Background","metadata":{},"score":"52.04322"}{"text":"Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .","label":"Background","metadata":{},"score":"52.04322"}{"text":"Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .","label":"Background","metadata":{},"score":"52.04322"}{"text":"Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .","label":"Background","metadata":{},"score":"52.04322"}{"text":"# computing a HMM model based on the training corpus , runnig the tagger on the test corpus and comparing the results with the original tags in the test corpus .The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .","label":"Background","metadata":{},"score":"52.14837"}{"text":"# computing a HMM model based on the training corpus , runnig the tagger on the test corpus and comparing the results with the original tags in the test corpus .The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .","label":"Background","metadata":{},"score":"52.14837"}{"text":"# computing a HMM model based on the training corpus , runnig the tagger on the test corpus and comparing the results with the original tags in the test corpus .The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .","label":"Background","metadata":{},"score":"52.14837"}{"text":"# computing a HMM model based on the training corpus , runnig the tagger on the test corpus and comparing the results with the original tags in the test corpus .The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .","label":"Background","metadata":{},"score":"52.14837"}{"text":"with high levels of accuracy . E. Brill .Transformation - based error - driven learning . and .natural language processing : A case study in part of speech tagging .Computational Linguistics , 21(4):543 - 566 , December 1995 .","label":"Background","metadata":{},"score":"52.327026"}{"text":"Perhaps something like training on brown romance and testing against science_fiction , and/or different combinations of brown corpus categories .At any rate , that does n't detract from your excellent work here , which definitely elegantly showcases what can be done with different taggers under different constraints , but it 's perhaps something to consider once nltk.pos_tag's source is published .","label":"Background","metadata":{},"score":"52.37978"}{"text":"However , these rules are often too rigid to accommodate real - world utterances , which often bend the rules of ' ' correct ' ' grammar in creative ways , or include minor errors , but are still easily comprehensible by human listeners .","label":"Background","metadata":{},"score":"52.75927"}{"text":"Transformation - based error - driven learning and natural language processing : A case study in part of speech tagging .Computational Linguistics , 21(4):543 - 566 , December 1995 .The problem is that words often have more than one meaning , sometimes fairly similar and sometimes completely different .","label":"Background","metadata":{},"score":"52.876656"}{"text":"Transformation - based error - driven learning and natural language processing : A case study in part of speech tagging .Computational Linguistics , 21(4):543 - 566 , December 1995 .The problem is that words often have more than one meaning , sometimes fairly similar and sometimes completely different .","label":"Background","metadata":{},"score":"52.876656"}{"text":"I have these questions - 1 ) How will I use Brill to solve this situation ( using my pre - tagged corpus as the Brill 's temp .corpus ) .2 ) What is pre - coded corpus from NLTK ( in Brill ) .","label":"Background","metadata":{},"score":"53.125977"}{"text":"The Lynx parser uses a generative model Pr(E ) of a structured English sentence E , a syntactic tree structure including words and labels .Parsing a given English sentence string e is the same as finding the most likely tree that yields the same surface string .","label":"Background","metadata":{},"score":"53.187454"}{"text":"Yarowsky reports that the system correctly classifies senses 96 % of the time .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .","label":"Background","metadata":{},"score":"53.200363"}{"text":"Yarowsky reports that the system correctly classifies senses 96 % of the time .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .","label":"Background","metadata":{},"score":"53.200363"}{"text":"Yarowsky reports that the system correctly classifies senses 96 % of the time .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .","label":"Background","metadata":{},"score":"53.200363"}{"text":"Yarowsky reports that the system correctly classifies senses 96 % of the time .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .","label":"Background","metadata":{},"score":"53.200363"}{"text":"Yarowsky reports that the system correctly classifies senses 96 % of the time .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .","label":"Background","metadata":{},"score":"53.200363"}{"text":"Yarowsky reports that the system correctly classifies senses 96 % of the time .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .","label":"Background","metadata":{},"score":"53.200363"}{"text":"The remainder of the words ( the remaining 5 % ) had the empty meaning to simulate function words .In addition , the functors in each meaning could have a depth of up to two and an arity of up to two .","label":"Background","metadata":{},"score":"53.21267"}{"text":"word .first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .The resolution of a word 's syntactic ambiguity has largely been solved . in .language processing by part - of - speech taggers which predict .","label":"Background","metadata":{},"score":"53.2436"}{"text":"While this would probably increase computational and memory requirements , a cleverly integrated system could probably accomplish this kind of training without massively increasing the ( already quite high ) processing burden for training .Such a system would probably give better results ; whether they would be significantly better is hard to say without having time to actually do the experiment .","label":"Background","metadata":{},"score":"53.349667"}{"text":"If a model will not work well , we would want to discover that fact before investing months of effort in building a new translation decoder .If a reranking experiment shows that a new model has good potential , the effort of building a decoder that can take advantage of its strengths may then be justified .","label":"Background","metadata":{},"score":"53.39993"}{"text":"In Machine Translation . of .Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .In 1975 .Kelly .E.F. Kelly and P.J. Stone .Computer Recognition of English Word Senses , Amsterdam : North - Holland . published a book explicitly listing their rules for disambiguation of word senses .","label":"Background","metadata":{},"score":"53.43932"}{"text":"Each score is the F 1 combination of precision and recall , and the improvement is the margin by which Lynx does better than the baseline .Unsurprisingly , the English version of the parser is most able to exceed the performance of its baseline .","label":"Background","metadata":{},"score":"53.45401"}{"text":"Sounds like you 're clear the overall structure , though you may want to think about removing the stemming & stop word filtering steps , as transforming chunks & words can often change the meaning .But that depends on your needs .","label":"Background","metadata":{},"score":"53.812714"}{"text":"An alternate way of combining knowledge with statistical modeling is to incorporate plenty of small statistical models into what is structurally a knowledge - based system , which could also be pictured as a rule - based system in which statistical models govern the application of each rule .","label":"Background","metadata":{},"score":"54.097885"}{"text":"This shows that the Lynx parser compares favorably with an established parser , which is good .However , this result is made more meaningful by the observation that the Link parser is the source of all the training data for the Lynx parser .","label":"Background","metadata":{},"score":"54.15384"}{"text":"In 1975 .Kelly and Stone .Computer Recognition of English Word Senses , Amsterdam : North - Holland . published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .","label":"Background","metadata":{},"score":"54.45308"}{"text":"In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 In modern WSD systems , the senses of a word are typically taken from some specified dictionary .","label":"Background","metadata":{},"score":"54.481033"}{"text":"In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 In modern WSD systems , the senses of a word are typically taken from some specified dictionary .","label":"Background","metadata":{},"score":"54.481033"}{"text":"Translation .In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .Computer Recognition of English Word Senses , Amsterdam : North - Holland .Word sense disambiguation using conceptual density .","label":"Background","metadata":{},"score":"54.497242"}{"text":"Translation .In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .Computer Recognition of English Word Senses , Amsterdam : North - Holland .Word sense disambiguation using conceptual density .","label":"Background","metadata":{},"score":"54.497242"}{"text":"Translation .In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .Computer Recognition of English Word Senses , Amsterdam : North - Holland .Word sense disambiguation using conceptual density .","label":"Background","metadata":{},"score":"54.497242"}{"text":"Translation .In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .Computer Recognition of English Word Senses , Amsterdam : North - Holland .Word sense disambiguation using conceptual density .","label":"Background","metadata":{},"score":"54.497242"}{"text":"Translation .In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .Computer Recognition of English Word Senses , Amsterdam : North - Holland .Word sense disambiguation using conceptual density .","label":"Background","metadata":{},"score":"54.497242"}{"text":"Translation .In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .Computer Recognition of English Word Senses , Amsterdam : North - Holland .Word sense disambiguation using conceptual density .","label":"Background","metadata":{},"score":"54.497242"}{"text":"Translation .In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .Computer Recognition of English Word Senses , Amsterdam : North - Holland .Word sense disambiguation using conceptual density .","label":"Background","metadata":{},"score":"54.497242"}{"text":"The resolution of a word 's syntactic ambiguity has largely been solved in language processing by part - of - speech taggers which predict the syntactic category of words in text with high levels of accuracy . E. Brill .Transformation - based error - driven learning and natural language processing : A case study in part of speech tagging .","label":"Background","metadata":{},"score":"54.57187"}{"text":"The resolution of a word 's syntactic ambiguity has largely been solved . in .language processing by part - of - speech taggers which predict .the . syntactic category of words . in text .with high levels of accuracy . E. Brill .","label":"Background","metadata":{},"score":"54.585197"}{"text":"Figure 16 shows the weighted precision and recall curves for this initial test .This demonstrates good scalability to a slightly larger corpus and lexicon than that of the U.S. geography query domain .A second corpus was generated from a second lexicon , also of 100 words using 25 conceptual symbols , but increasing the ambiguity to 1.25 meanings per word .","label":"Background","metadata":{},"score":"54.671593"}{"text":"Admittedly , the Lynx parser does not score better than its source of training data , only about the same .However , on 30 % of the sentences evaluated , the Lynx parser scores better than the Link parser , showing that it actually does have a significant improvement in the ability to parse some sentences , offset by inferior performance on some other sentences .","label":"Background","metadata":{},"score":"54.70621"}{"text":"This method allows various models to be compared on the translation task without requiring a whole new translation system to be built each time .Thus , a reranking approach will tend to underestimate the potential of a new model because its range of outputs is artificially constrained .","label":"Background","metadata":{},"score":"54.899723"}{"text":"In Proceedings of COLING'96 and LDOCE J. Guthrie , L. Guthrie , Y. Wilks and H. Aidinejad , Subject - Dependent Co - Occurrence and Word Sense Disambiguation , ACL-91 , pp .146 - 152 .The information in these resources has been used in several ways , for example Wilks and Stevenson Y. Wilks and M. Stevenson .","label":"Background","metadata":{},"score":"54.968166"}{"text":"In Proceedings of COLING'96 and LDOCE J. Guthrie , L. Guthrie , Y. Wilks and H. Aidinejad , Subject - Dependent Co - Occurrence and Word Sense Disambiguation , ACL-91 , pp .146 - 152 .The information in these resources has been used in several ways , for example Wilks and Stevenson Y. Wilks and M. Stevenson .","label":"Background","metadata":{},"score":"54.968166"}{"text":"Increasing the level of synonymy does not effect the results as much as increasing the level of ambiguity , which is as we expected .On the other hand , increasing the level of synonymy does not have the potential to mislead the learner .","label":"Background","metadata":{},"score":"55.08966"}{"text":"In order to economically test the efficacy of the syntax - based Lynx and LinkSet models in a translation system , we use the models to assign scores to the translation hypotheses generated by a state - of - the - art statistical translation system .","label":"Background","metadata":{},"score":"55.49321"}{"text":"Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , [ 9 ] decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .","label":"Background","metadata":{},"score":"55.522636"}{"text":"Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , [ 9 ] decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .","label":"Background","metadata":{},"score":"55.522636"}{"text":"Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , [ 9 ] decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .","label":"Background","metadata":{},"score":"55.522636"}{"text":"Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , [ 9 ] decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .","label":"Background","metadata":{},"score":"55.522636"}{"text":"Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , [ 9 ] decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .","label":"Background","metadata":{},"score":"55.522636"}{"text":"Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , [ 9 ] decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .","label":"Background","metadata":{},"score":"55.522636"}{"text":"Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , [ 9 ] decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .","label":"Background","metadata":{},"score":"55.522636"}{"text":"And your english is great , I would have assumed you 're a native if you had n't mentioned anything .Jacob .I tried different things now , but without any results .I am facing different issues : . - scipy - algorithms do not work for some reason - there are no binary releases of megam for windows ... and I was n't able to compile it on my machine .","label":"Background","metadata":{},"score":"55.52444"}{"text":"In the rule - based Link parser , any constructions not anticipated by the designers of the grammar can only be covered through null links .There is a sharp dividing line between what is considered grammatical and what is not , which can cause sentences with a missing word to be analyzed strangely or not at all .","label":"Background","metadata":{},"score":"55.686543"}{"text":"What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching technique [ 10 ] which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .","label":"Background","metadata":{},"score":"55.861145"}{"text":"What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching technique [ 10 ] which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .","label":"Background","metadata":{},"score":"55.861145"}{"text":"What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching technique [ 10 ] which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .","label":"Background","metadata":{},"score":"55.861145"}{"text":"What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching technique [ 10 ] which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .","label":"Background","metadata":{},"score":"55.861145"}{"text":"What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching technique [ 10 ] which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .","label":"Background","metadata":{},"score":"55.861145"}{"text":"What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching technique [ 10 ] which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .","label":"Background","metadata":{},"score":"55.861145"}{"text":"What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching technique [ 10 ] which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .","label":"Background","metadata":{},"score":"55.861145"}{"text":"In other words , you define a few high - precision regular expressions , and then rely on the trained chunker to find / recall chunks the manual chunker misses .The other thing I recommend is if you 're using a treebank trained chunker , then you should use a treebank trained tagger .","label":"Background","metadata":{},"score":"55.867523"}{"text":"The only solution that comes to mind is to create a Part - of - Speech Tagged Word Corpus and assign \" custom tags \" to words that I need and then pass them to the chunker , but I 'm not sure about such approach .","label":"Background","metadata":{},"score":"56.39936"}{"text":"Under Zipf 's Law , the occurrence frequency of a word is inversely proportional to its ranking by occurrence .We started with a baseline corpus generated from a lexicon of 100 words using 25 conceptual symbols and no ambiguity or synonymy ; 1949 sentence - meaning pairs were generated .","label":"Background","metadata":{},"score":"56.41475"}{"text":"In Table 4 , we show the point at which a standard precision of 75 % was first reached for each level of ambiguity .Note , however , that we only measured accuracy after each set of 100 training examples , so the numbers in the table are approximate .","label":"Background","metadata":{},"score":"56.593887"}{"text":"The problem is that words often have more than one meaning , sometimes fairly similar and sometimes completely different .The meaning . of a .word in a particular usage can only be determined by examining its context . . .","label":"Background","metadata":{},"score":"56.595497"}{"text":"Any remaining minor node may be deleted .The remaining minor nodes are repartitioned into left and right sets .On either side , a new minor node may be inserted .Inserted nodes correspond to null links .The probability of a foreign sentence tree given an English sentence tree is the product over all nodes in the English tree of the probabilities of applying these transformations in a way that results in the foreign sentence tree .","label":"Background","metadata":{},"score":"56.60323"}{"text":"Did you do any form of tag normalization ?No , I did not try Brill with a MaxentClassifier tagger .It probably would give another percent of accuracy , but I do n't think that 's what the pre - trained tagger for pos_tag does , as the repr of the tagger is from the ClassifierBasedTagger .","label":"Background","metadata":{},"score":"56.651398"}{"text":"However , the best option ( but also the most time consuming ) is to create your own tagged & chunked corpus , then train a tagger & chunker on that .I recommend a bootstrap approach , where you 'd use an existing tagger & chunker to create an initial corpus , then go in and hand - correct before training a custom tagger & chunker .","label":"Background","metadata":{},"score":"56.75399"}{"text":"In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .This takes a small number of seed definitions of the senses of some word ( the seeds could be WordNet synsets or definitions from some lexicon ) and uses these to classify the \" obvious \" cases in a corpus .","label":"Background","metadata":{},"score":"56.755936"}{"text":"In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .This takes a small number of seed definitions of the senses of some word ( the seeds could be WordNet synsets or definitions from some lexicon ) and uses these to classify the \" obvious \" cases in a corpus .","label":"Background","metadata":{},"score":"56.755936"}{"text":"In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .This takes a small number of seed definitions of the senses of some word ( the seeds could be WordNet synsets or definitions from some lexicon ) and uses these to classify the \" obvious \" cases in a corpus .","label":"Background","metadata":{},"score":"56.755936"}{"text":"In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .This takes a small number of seed definitions of the senses of some word ( the seeds could be WordNet synsets or definitions from some lexicon ) and uses these to classify the \" obvious \" cases in a corpus .","label":"Background","metadata":{},"score":"56.755936"}{"text":"In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .This takes a small number of seed definitions of the senses of some word ( the seeds could be WordNet synsets or definitions from some lexicon ) and uses these to classify the \" obvious \" cases in a corpus .","label":"Background","metadata":{},"score":"56.755936"}{"text":"In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .This takes a small number of seed definitions of the senses of some word ( the seeds could be WordNet synsets or definitions from some lexicon ) and uses these to classify the \" obvious \" cases in a corpus .","label":"Background","metadata":{},"score":"56.755936"}{"text":"The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .To appear in Journal of Natural Language Engineering , 4(3 ) .Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .","label":"Background","metadata":{},"score":"56.839554"}{"text":"The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .To appear in Journal of Natural Language Engineering , 4(3 ) .Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .","label":"Background","metadata":{},"score":"56.839554"}{"text":"The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .To appear in Journal of Natural Language Engineering , 4(3 ) .Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .","label":"Background","metadata":{},"score":"56.839554"}{"text":"The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .To appear in Journal of Natural Language Engineering , 4(3 ) .Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .","label":"Background","metadata":{},"score":"56.839554"}{"text":"The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .To appear in Journal of Natural Language Engineering , 4(3 ) .Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .","label":"Background","metadata":{},"score":"56.839554"}{"text":"The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .To appear in Journal of Natural Language Engineering , 4(3 ) .Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .","label":"Background","metadata":{},"score":"56.839554"}{"text":"The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .To appear in Journal of Natural Language Engineering , 4(3 ) .Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .","label":"Background","metadata":{},"score":"56.839554"}{"text":"Discussions .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .","label":"Background","metadata":{},"score":"57.08677"}{"text":"Discussions .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .","label":"Background","metadata":{},"score":"57.08677"}{"text":"Discussions .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .","label":"Background","metadata":{},"score":"57.08677"}{"text":"Discussions .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .","label":"Background","metadata":{},"score":"57.08677"}{"text":"Discussions .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .","label":"Background","metadata":{},"score":"57.08677"}{"text":"Discussions .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .","label":"Background","metadata":{},"score":"57.08677"}{"text":"Discussions .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .","label":"Background","metadata":{},"score":"57.08677"}{"text":"You 're correct about the default tagger , and i should have explained that the low accuracy on brown was due to the different tag sets .I was more concerned with evaluating training methods , so I included the default tagger as a reference to compare to , in order to demonstrate that it 's possible to train a tagger that 's just as good or better . is being used in a textual context .","label":"Background","metadata":{},"score":"57.324615"}{"text":"Thanks a lot !I 've got a question but to be more precise , let me explain a bit : I 'm working on a solution that extracts meaningful pieces of information from text ( phrases , names ... ) that will be used for further text mining stages .","label":"Background","metadata":{},"score":"57.61419"}{"text":"The figure above shows the average distance between the rankings of various systems and the NIST - score ranking , grouped by sentence length .The sentences were grouped by length into bins of width five .Six different system combinations were compared : the original decoder score , the Lynx score , both of these together , and each of these three with the LinkSet score .","label":"Background","metadata":{},"score":"57.757774"}{"text":"This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet Agirre , E. and Rigau , G. ( 1996 ) .Word sense disambiguation using conceptual density .","label":"Background","metadata":{},"score":"57.76989"}{"text":"This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet Agirre , E. and Rigau , G. ( 1996 ) .Word sense disambiguation using conceptual density .","label":"Background","metadata":{},"score":"57.76989"}{"text":"This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet Agirre , E. and Rigau , G. ( 1996 ) .Word sense disambiguation using conceptual density .","label":"Background","metadata":{},"score":"57.76989"}{"text":"This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet Agirre , E. and Rigau , G. ( 1996 ) .Word sense disambiguation using conceptual density .","label":"Background","metadata":{},"score":"57.76989"}{"text":"Monolingual Probabilistic Parsing .The results are shown in this table , which shows the number and percentage of sentences for which the method in each column beats the method in each row .The Lynx parser analyzes sentences for syntactic structure by finding the most probable structure consistent with the given text , according to a statistical model of structured sentences .","label":"Background","metadata":{},"score":"57.979782"}{"text":"With the rise of statistical methods in CL in the 1990s , WSD became one of the main focus ' of supervised learning techniques .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .","label":"Background","metadata":{},"score":"58.11592"}{"text":"With the rise of statistical methods in CL in the 1990s , WSD became one of the main focus ' of supervised learning techniques .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .","label":"Background","metadata":{},"score":"58.11592"}{"text":"With the rise of statistical methods in CL in the 1990s , WSD became one of the main focus ' of supervised learning techniques .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .","label":"Background","metadata":{},"score":"58.11592"}{"text":"With the rise of statistical methods in CL in the 1990s , WSD became one of the main focus ' of supervised learning techniques .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .","label":"Background","metadata":{},"score":"58.11592"}{"text":"Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .For example , does it make sense to describe an overall percentage accuracy for a WSD system or does evaluation require specific comparison of system performance on a word by word basis .","label":"Background","metadata":{},"score":"58.148224"}{"text":"These days WordNet is the usual dictionary in question .WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .Contents .One of the first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .","label":"Background","metadata":{},"score":"58.167336"}{"text":"That might almost explain the remaining gap .Also , I am willing to wager heavily that the primary reason nltk.pos_tag has such a high error rate on brown is because the tags are substantially different between brown and treebank , more so than any difference in the actual corpus material .","label":"Background","metadata":{},"score":"58.185474"}{"text":"It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .","label":"Background","metadata":{},"score":"58.68293"}{"text":"It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .","label":"Background","metadata":{},"score":"58.68293"}{"text":"It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .","label":"Background","metadata":{},"score":"58.68293"}{"text":"It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .","label":"Background","metadata":{},"score":"58.68293"}{"text":"It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .","label":"Background","metadata":{},"score":"58.68293"}{"text":"It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .","label":"Background","metadata":{},"score":"58.68293"}{"text":"It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .","label":"Background","metadata":{},"score":"58.68293"}{"text":"It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .","label":"Background","metadata":{},"score":"58.68293"}{"text":"It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .","label":"Background","metadata":{},"score":"58.68293"}{"text":"It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .","label":"Background","metadata":{},"score":"58.68293"}{"text":"It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .","label":"Background","metadata":{},"score":"58.68293"}{"text":"First , a new statistical parser is presented .This parser , titled Lynx , is trained from sentences annotated automatically by another parser , and achieves results comparable to the original parser .Second , a statistical model of the relationship between the syntactic structure of two languages is presented , along with an unsupervised training algorithm , titled LinkSet .","label":"Background","metadata":{},"score":"59.048008"}{"text":"1975 . . .Computer Recognition of English Word Senses , Amsterdam : North - Holland . published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .","label":"Background","metadata":{},"score":"59.071312"}{"text":"A similarity matrix is thus formed which is subject to cluster analysis to determine groups of semantically related instances of terms .Another example is the work of Pedersen T. Pedersen and R. Bruce .Distinguishing word senses in untagged text .","label":"Background","metadata":{},"score":"59.208187"}{"text":"A similarity matrix is thus formed which is subject to cluster analysis to determine groups of semantically related instances of terms .Another example is the work of Pedersen T. Pedersen and R. Bruce .Distinguishing word senses in untagged text .","label":"Background","metadata":{},"score":"59.208187"}{"text":"A similarity matrix is thus formed which is subject to cluster analysis to determine groups of semantically related instances of terms .Another example is the work of Pedersen T. Pedersen and R. Bruce .Distinguishing word senses in untagged text .","label":"Background","metadata":{},"score":"59.208187"}{"text":"A similarity matrix is thus formed which is subject to cluster analysis to determine groups of semantically related instances of terms .Another example is the work of Pedersen T. Pedersen and R. Bruce .Distinguishing word senses in untagged text .","label":"Background","metadata":{},"score":"59.208187"}{"text":"Thinking that postag was probably trained on the full treebank corpus , I did the same , and re - evaluated : .The result was 98.08 % accuracy .So the remaining 2 % difference must be due to the MaxentClassifier being more accurate than the naive bayes classifier , and/or the use of a different feature detector .","label":"Background","metadata":{},"score":"59.406647"}{"text":"146 - 152 .The information in these resources has been used in several ways , for example Wilks and Stevenson Y. Wilks and M. Stevenson .The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .","label":"Background","metadata":{},"score":"59.45388"}{"text":"146 - 152 .The information in these resources has been used in several ways , for example Wilks and Stevenson Y. Wilks and M. Stevenson .The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .","label":"Background","metadata":{},"score":"59.45388"}{"text":"146 - 152 .The information in these resources has been used in several ways , for example Wilks and Stevenson Y. Wilks and M. Stevenson .The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .","label":"Background","metadata":{},"score":"59.45388"}{"text":"146 - 152 .The information in these resources has been used in several ways , for example Wilks and Stevenson Y. Wilks and M. Stevenson .The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .","label":"Background","metadata":{},"score":"59.45388"}{"text":"Using Bayes ' rule , we can turn this around and factor out the Pr ( f ) that would appear in the denominator , since it does not depend on e .So far we have described translation of sentence strings , ignoring syntactic structure .","label":"Background","metadata":{},"score":"59.694046"}{"text":"These days .[[ . WordNet . ] ] is the usual dictionary in question .WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .The SENSEVAL conferences have attempted to put Word Sense Disambiguation on an empirically measurable basis by hosting evaluations in which a given corpus of tagged word senses are created using . ] ] 's senses and participants attempt to recognize those senses after tuning their systems with a corpus of training data .","label":"Background","metadata":{},"score":"59.998"}{"text":"In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 .","label":"Background","metadata":{},"score":"60.26711"}{"text":"In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 .","label":"Background","metadata":{},"score":"60.26711"}{"text":"In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 .","label":"Background","metadata":{},"score":"60.26711"}{"text":"In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 .","label":"Background","metadata":{},"score":"60.26711"}{"text":"In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 .","label":"Background","metadata":{},"score":"60.26711"}{"text":"In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 .","label":"Background","metadata":{},"score":"60.26711"}{"text":"In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 .","label":"Background","metadata":{},"score":"60.26711"}{"text":"In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 The previous section showed that W OLFIE successfully learns lexicons for a natural corpus and a realistic task .","label":"Background","metadata":{},"score":"60.468723"}{"text":"Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .","label":"Background","metadata":{},"score":"60.474785"}{"text":"Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .","label":"Background","metadata":{},"score":"60.474785"}{"text":"Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .","label":"Background","metadata":{},"score":"60.474785"}{"text":"Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .","label":"Background","metadata":{},"score":"60.474785"}{"text":"Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .","label":"Background","metadata":{},"score":"60.474785"}{"text":"Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .","label":"Background","metadata":{},"score":"60.474785"}{"text":"If a highly accurate selector could be trained to choose which parser has a better analysis of each sentence , a gain of almost 5 percentage points is possible .Why is the Lynx parser able to score better than its source of training data on so many sentences ?","label":"Background","metadata":{},"score":"60.531513"}{"text":"To evaluate the system - generated rankings , we measured the rank - order distance between each system 's ranking for each sentence and the corresponding rankings given by BLEU score and by NIST score .We used the Kendall rank correlation (  ) as a measure of agreement between rankings ; this distance is the number of adjacent pairs that must be swapped in order to bring the rankings into agreement .","label":"Background","metadata":{},"score":"61.198368"}{"text":"Another example is the work of Pedersen [ 11 ] who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .","label":"Background","metadata":{},"score":"61.31019"}{"text":"Another example is the work of Pedersen [ 11 ] who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .","label":"Background","metadata":{},"score":"61.31019"}{"text":"Another example is the work of Pedersen [ 11 ] who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .","label":"Background","metadata":{},"score":"61.31019"}{"text":"Another example is the work of Pedersen [ 11 ] who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .","label":"Background","metadata":{},"score":"61.31019"}{"text":"Another example is the work of Pedersen [ 11 ] who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .","label":"Background","metadata":{},"score":"61.31019"}{"text":"Another example is the work of Pedersen [ 11 ] who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .","label":"Background","metadata":{},"score":"61.31019"}{"text":"Another example is the work of Pedersen [ 11 ] who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .","label":"Background","metadata":{},"score":"61.31019"}{"text":"Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , Segond , F. , Schiller , A. , Grefenstette , G. , and Chanod , J. ( 1997 ) .An experiment in semantic tagging using hidden markov model tagging .","label":"Background","metadata":{},"score":"61.36711"}{"text":"Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , Segond , F. , Schiller , A. , Grefenstette , G. , and Chanod , J. ( 1997 ) .An experiment in semantic tagging using hidden markov model tagging .","label":"Background","metadata":{},"score":"61.36711"}{"text":"In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 is being used in a textual context .","label":"Background","metadata":{},"score":"61.424953"}{"text":"A good example of this is Luk 's system A. Luk .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 . which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .","label":"Background","metadata":{},"score":"61.62786"}{"text":"A good example of this is Luk 's system A. Luk .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 . which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .","label":"Background","metadata":{},"score":"61.62786"}{"text":"A good example of this is Luk 's system A. Luk .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 . which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .","label":"Background","metadata":{},"score":"61.62786"}{"text":"A good example of this is Luk 's system A. Luk .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 . which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .","label":"Background","metadata":{},"score":"61.62786"}{"text":"A good example of this is Luk 's system A. Luk .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 . which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .","label":"Background","metadata":{},"score":"61.62786"}{"text":"A good example of this is Luk 's system A. Luk .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 . which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .","label":"Background","metadata":{},"score":"61.62786"}{"text":"For the brown corpus , I trained on 2/3 of the reviews , lore , and romance categories , and tested against the remaining 1/3 .For conll2000 , I used the standard train.txt vs test.txt .And for treebank , I again used a 2/3 vs 1/3 split .","label":"Background","metadata":{},"score":"62.608246"}{"text":"language processing by part - of - speech taggers which predict .the . syntactic category of words . in text .with high levels of accuracy . E. Brill .Transformation - based error - driven learning . and .natural language processing : A case study in part of speech tagging .","label":"Background","metadata":{},"score":"62.745895"}{"text":"Distinguishing word senses in untagged text .In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .","label":"Background","metadata":{},"score":"62.985554"}{"text":"Distinguishing word senses in untagged text .In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .","label":"Background","metadata":{},"score":"62.985554"}{"text":"However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .Another approach is to use Hidden Markov Models which have proved very successful in part - of - speech tagging .","label":"Background","metadata":{},"score":"63.025352"}{"text":"However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .Another approach is to use Hidden Markov Models which have proved very successful in part - of - speech tagging .","label":"Background","metadata":{},"score":"63.025352"}{"text":"However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .Another approach is to use Hidden Markov Models which have proved very successful in part - of - speech tagging .","label":"Background","metadata":{},"score":"63.025352"}{"text":"However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .Another approach is to use Hidden Markov Models which have proved very successful in part - of - speech tagging .","label":"Background","metadata":{},"score":"63.025352"}{"text":"It should be noted that unsupervised disambiguation can not actually label specific terms as a referring to a specific concept : that would require more information than is available .What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .","label":"Background","metadata":{},"score":"63.080925"}{"text":"It should be noted that unsupervised disambiguation can not actually label specific terms as a referring to a specific concept : that would require more information than is available .What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .","label":"Background","metadata":{},"score":"63.080925"}{"text":"Hi Oli , .Yes , the default MaxentClassifier algorithm is unfortunately slow .If you create a custom training function that calls MaxentClassifier.train with different parameters , you can speed it up .I generally set min_lldelta to 0.01 for the default algorithm and often stop the iterations before it gets to 10 iterations by using Ctrl - C. Great idea on using the feature extractor from the default tagger .","label":"Background","metadata":{},"score":"63.57412"}{"text":"What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching techniqueRadford et al .","label":"Background","metadata":{},"score":"63.584053"}{"text":"What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching techniqueRadford et al .","label":"Background","metadata":{},"score":"63.584053"}{"text":"What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching techniqueRadford et al .","label":"Background","metadata":{},"score":"63.584053"}{"text":"What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching techniqueRadford et al .","label":"Background","metadata":{},"score":"63.584053"}{"text":"Y. Bar - Hillel .Language and Information .Addison - Wesley , 1964 .However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .","label":"Background","metadata":{},"score":"63.64151"}{"text":"Y. Bar - Hillel .Language and Information .Addison - Wesley , 1964 .However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .","label":"Background","metadata":{},"score":"63.64151"}{"text":"Y. Bar - Hillel .Language and Information .Addison - Wesley , 1964 .However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .","label":"Background","metadata":{},"score":"63.64151"}{"text":"In modern WSD systems , the senses of a word are typically taken from some specified dictionary .In earlier systems the senses were more typically generic senses selected by the originators of the system .These days [ [ WordNet ] ] is the usual dictionary in question .","label":"Background","metadata":{},"score":"63.69022"}{"text":"In modern WSD systems , the senses of a word are typically taken from some specified dictionary .In earlier systems the senses were more typically generic senses selected by the originators of the system .These days [ [ WordNet ] ] is the usual dictionary in question .","label":"Background","metadata":{},"score":"63.69022"}{"text":"I am not able to test the stuff , because the default - algo is toooo slow .But thank you for the hint with ctrl+c .I am also a bit confused how to init the ClassifierBasedPOSTagger .It would be great , if you could post your code for your testing . thanks in advance !","label":"Background","metadata":{},"score":"63.77214"}{"text":"I am not able to test the stuff , because the default - algo is toooo slow .But thank you for the hint with ctrl+c .I am also a bit confused how to init the ClassifierBasedPOSTagger .It would be great , if you could post your code for your testing . thanks in advance !","label":"Background","metadata":{},"score":"63.77214"}{"text":"WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .Contents .One of the first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .","label":"Background","metadata":{},"score":"63.94061"}{"text":"WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .Contents .One of the first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .","label":"Background","metadata":{},"score":"63.94061"}{"text":"WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .Contents .One of the first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .","label":"Background","metadata":{},"score":"63.94061"}{"text":"WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .Contents .One of the first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .","label":"Background","metadata":{},"score":"63.94061"}{"text":"WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .Contents .One of the first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .","label":"Background","metadata":{},"score":"63.94061"}{"text":"In modern WSD systems , the senses of a word are typically taken from some specified dictionary .These days [ [ WordNet ] ] is the usual dictionary in question .WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .","label":"Background","metadata":{},"score":"63.997433"}{"text":"In modern WSD systems , the senses of a word are typically taken from some specified dictionary .These days [ [ WordNet ] ] is the usual dictionary in question .WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .","label":"Background","metadata":{},"score":"63.997433"}{"text":"Decision lists [ 14 ] are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .The learning proceeds in this way until all corpus instances are classified .","label":"Background","metadata":{},"score":"64.0865"}{"text":"Decision lists [ 14 ] are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .The learning proceeds in this way until all corpus instances are classified .","label":"Background","metadata":{},"score":"64.0865"}{"text":"Decision lists [ 14 ] are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .The learning proceeds in this way until all corpus instances are classified .","label":"Background","metadata":{},"score":"64.0865"}{"text":"Decision lists [ 14 ] are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .The learning proceeds in this way until all corpus instances are classified .","label":"Background","metadata":{},"score":"64.0865"}{"text":"Decision lists [ 14 ] are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .The learning proceeds in this way until all corpus instances are classified .","label":"Background","metadata":{},"score":"64.0865"}{"text":"Decision lists [ 14 ] are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .The learning proceeds in this way until all corpus instances are classified .","label":"Background","metadata":{},"score":"64.0865"}{"text":"Decision lists [ 14 ] are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .The learning proceeds in this way until all corpus instances are classified .","label":"Background","metadata":{},"score":"64.0865"}{"text":"In earlier systems the senses were more typically generic senses selected by the originators of the system .These days WordNet is the usual dictionary in question .WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .","label":"Background","metadata":{},"score":"64.21331"}{"text":"Y. Bar - Hillel .Language and Information .Addison - Wesley , 1964 .However , the situation is not as bad as Bar - Hillel feared , there have been several advances in .word . sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .","label":"Background","metadata":{},"score":"64.27644"}{"text":"Now you can use train_set for training the classifier with the same feature_detector as the pos_tag ( ) .Right ?I would like to test it , but I got a problem .When I use the simple example , u mentioned in your post : .","label":"Background","metadata":{},"score":"64.51976"}{"text":"Without it the tagger will never consult its backoff .I do n't use a backoff tagger with a classifier tagger , as anything else is only going to be less accurate .Yes , I found that cutoff_prob parameter later and did some experiments , coming to the same conclusion as you : a backoff tagger with a classifier based tagger generally does n't help .","label":"Background","metadata":{},"score":"64.98343"}{"text":"WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .One of the first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .","label":"Background","metadata":{},"score":"65.24066"}{"text":"WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .One of the first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .","label":"Background","metadata":{},"score":"65.24066"}{"text":"An experiment in semantic tagging using hidden markov model tagging .In Vossen , P. , Adriaens , G. , Calzolari , N. , Sanfilippo , A. , and Wilks , Y. , editors , Proceedings of the ACL / EACL'97 Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources .","label":"Background","metadata":{},"score":"65.59156"}{"text":"An experiment in semantic tagging using hidden markov model tagging .In Vossen , P. , Adriaens , G. , Calzolari , N. , Sanfilippo , A. , and Wilks , Y. , editors , Proceedings of the ACL / EACL'97 Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources .","label":"Background","metadata":{},"score":"65.59156"}{"text":"An experiment in semantic tagging using hidden markov model tagging .In Vossen , P. , Adriaens , G. , Calzolari , N. , Sanfilippo , A. , and Wilks , Y. , editors , Proceedings of the ACL / EACL'97 Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources .","label":"Background","metadata":{},"score":"65.59156"}{"text":"An experiment in semantic tagging using hidden markov model tagging .In Vossen , P. , Adriaens , G. , Calzolari , N. , Sanfilippo , A. , and Wilks , Y. , editors , Proceedings of the ACL / EACL'97 Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources .","label":"Background","metadata":{},"score":"65.59156"}{"text":"An experiment in semantic tagging using hidden markov model tagging .In Vossen , P. , Adriaens , G. , Calzolari , N. , Sanfilippo , A. , and Wilks , Y. , editors , Proceedings of the ACL / EACL'97 Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources .","label":"Background","metadata":{},"score":"65.59156"}{"text":"An experiment in semantic tagging using hidden markov model tagging .In Vossen , P. , Adriaens , G. , Calzolari , N. , Sanfilippo , A. , and Wilks , Y. , editors , Proceedings of the ACL / EACL'97 Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources .","label":"Background","metadata":{},"score":"65.59156"}{"text":"An experiment in semantic tagging using hidden markov model tagging .In Vossen , P. , Adriaens , G. , Calzolari , N. , Sanfilippo , A. , and Wilks , Y. , editors , Proceedings of the ACL / EACL'97 Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources .","label":"Background","metadata":{},"score":"65.59156"}{"text":"An experiment in semantic tagging using hidden markov model tagging .In Vossen , P. , Adriaens , G. , Calzolari , N. , Sanfilippo , A. , and Wilks , Y. , editors , Proceedings of the ACL / EACL'97 Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources .","label":"Background","metadata":{},"score":"65.59156"}{"text":"An experiment in semantic tagging using hidden markov model tagging .In Vossen , P. , Adriaens , G. , Calzolari , N. , Sanfilippo , A. , and Wilks , Y. , editors , Proceedings of the ACL / EACL'97 Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources .","label":"Background","metadata":{},"score":"65.59156"}{"text":"An experiment in semantic tagging using hidden markov model tagging .In Vossen , P. , Adriaens , G. , Calzolari , N. , Sanfilippo , A. , and Wilks , Y. , editors , Proceedings of the ACL / EACL'97 Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources .","label":"Background","metadata":{},"score":"65.59156"}{"text":"An experiment in semantic tagging using hidden markov model tagging .In Vossen , P. , Adriaens , G. , Calzolari , N. , Sanfilippo , A. , and Wilks , Y. , editors , Proceedings of the ACL / EACL'97 Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources .","label":"Background","metadata":{},"score":"65.59156"}{"text":"Statistical methods are effective for many natural language processing tasks , including automatic translation and parsing .This thesis brings these two applications together in two ways , using translation to aid parser construction and using parsing to improve translation quality .","label":"Background","metadata":{},"score":"65.72621"}{"text":"The thing is that POS - tagger can not work perfectly due to unknown words and words or phrases that have special meaning ( names of trademarks , companies , products and so on ) .I want to use categorized phrases / words lists in order to parse information more efficiently .","label":"Background","metadata":{},"score":"65.75408"}{"text":"In earlier systems the senses were more typically generic senses selected by the originators of the system .These days [ [ WordNet ] ] is the usual dictionary in question .WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .","label":"Background","metadata":{},"score":"65.865295"}{"text":"In that case , stick with a simpler tagger that 's nearly as accurate and orders of magnitude faster .Share this : .Post navigation .Did you try a Brill tagger with the MaxEnt classifier as the initial tagger ?","label":"Background","metadata":{},"score":"66.04352"}{"text":"word . sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .first introduced by Warren Weaver in 1949 W. Weaver .In Machine Translation . of .","label":"Background","metadata":{},"score":"66.14207"}{"text":"# constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .# computing a HMM model based on the training corpus , runnig the tagger on the test corpus and comparing the results with the original tags in the test corpus .","label":"Background","metadata":{},"score":"66.21538"}{"text":"# constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .# computing a HMM model based on the training corpus , runnig the tagger on the test corpus and comparing the results with the original tags in the test corpus .","label":"Background","metadata":{},"score":"66.21538"}{"text":"who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .","label":"Background","metadata":{},"score":"66.73078"}{"text":"who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .","label":"Background","metadata":{},"score":"66.73078"}{"text":"who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .","label":"Background","metadata":{},"score":"66.73078"}{"text":"who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .","label":"Background","metadata":{},"score":"66.73078"}{"text":"These days [ [ WordNet ] ] is the usual dictionary in question .WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .","label":"Background","metadata":{},"score":"66.95455"}{"text":"Hopefully the NLTK leaders will publish the training method so we can all know for sure .This was run with python 2.6.4 on an Athlon 64 Dual Core 4600 + with 3 G RAM , but the important thing is the relative times . braubt is over 246 times faster than cpos !","label":"Background","metadata":{},"score":"67.923874"}{"text":"The brackets are used by the BracketParseCorpusReader ( used by treebank ) to define noun phrases .This method can definitely work , but it may take a lot more effort on your part to tag & chunk every phrases .Can we use nltk pos_tag without training ... is it already trained on Treebank Corpus ? ?","label":"Background","metadata":{},"score":"68.14441"}{"text":"It also takes much longer to train and tag ( more details below ) and so may not be worth the tradeoff in efficiency .Using brill tagger will nearly always increase the accuracy of your initial tagger , but not by much .","label":"Background","metadata":{},"score":"68.17598"}{"text":"Contents .One of the first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .The resolution of a word 's syntactic ambiguity has largely been solved in language processing by part - of - speech taggers which predict the syntactic category of words in text with high levels of accuracy .","label":"Background","metadata":{},"score":"68.20639"}{"text":"The real point of testing against brown was to illustrate the importance of using the right training data , and I think that came across loud and clear . tdflatline .Actually , I 'm not sure if you have conclusively demonstrated the importance of training data .","label":"Background","metadata":{},"score":"68.76543"}{"text":"This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : . # The boy leapt from the bank into the cold water .","label":"Background","metadata":{},"score":"69.03649"}{"text":"This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : . # The boy leapt from the bank into the cold water .","label":"Background","metadata":{},"score":"69.03649"}{"text":"Word Sense Disambiguation ( WSD ) is the process of identifying the sense of a polysemic word .In modern WSD systems , the senses of a word are typically taken from some specified dictionary .These days WordNet is the usual dictionary in question .","label":"Background","metadata":{},"score":"69.64232"}{"text":"first introduced by Warren Weaver in 1949 W. Weaver .In Machine Translation . of .Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .In 1975 .Kelly and Stone .","label":"Background","metadata":{},"score":"69.74448"}{"text":"An early sceptic was Bar - Hillel who famously proclaimed that \" sense ambiguity could not be resolved . by .electronic computer either current or imaginable \" .Y. Bar - Hillel .Language and Information .Addison - Wesley , 1964 .","label":"Background","metadata":{},"score":"69.747604"}{"text":"So the lesson is : do not use a classifier based tagger if speed is an issue .Here 's the code for timing postag .You can do the same thing for any other pickled tagger by replacing nltk.tag ._ POS_TAGGER with a nltk.data accessible path with a . pickle suffix for the load method .","label":"Background","metadata":{},"score":"69.967445"}{"text":"That 's not the solution that I like .I 'm sure Python or NLTK provide some suitable functionality .Could you suggest a better way to do such things ?I would appreciate any thoughts .Hi Max , .","label":"Background","metadata":{},"score":"70.8195"}{"text":"In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 Word sense disambiguation ( WSD ) is the ability of software to distinguish what sense of a word is being used in a textual context .","label":"Background","metadata":{},"score":"70.96269"}{"text":"Additional trials , in which varying weights were used for each system , gave almost identical results .All of these best combinations include the Lynx score .This chart shows that among these systems , the Lynx score is the best approximation of the ranking given by the NIST scores , and that no combination of systems significantly outperforms the unadorned Lynx score .","label":"Background","metadata":{},"score":"71.04134"}{"text":"The problem is that words often have more than one meaning , sometimes fairly similar and sometimes completely different .The meaning of a word in a particular usage can only be determined by examining its context .This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : . # The boy leapt from the bank into the cold water .","label":"Background","metadata":{},"score":"71.326965"}{"text":"I do n't use a backoff tagger with a classifier tagger , as anything else is only going to be less accurate .Dannii .You said : \" A ClassifierBasedPOSTagger does not need a backoff tagger , since cpos accuracy is exactly the same as for craubt across all corpora .","label":"Background","metadata":{},"score":"71.656815"}{"text":"word in a particular usage can only be determined by examining its context . . .This is , in general , a trivial task for the human language processing system , for .# The boy leapt from the bank into the cold water .","label":"Background","metadata":{},"score":"71.959076"}{"text":"The meaning . of a .word in a particular usage can only be determined by examining its context . . .This is , in general , a trivial task for the human language processing system , for .# The boy leapt from the bank into the cold water .","label":"Background","metadata":{},"score":"72.01433"}{"text":"Computer Recognition of English Word Senses , Amsterdam : North - Holland . published a book explicitly listing their rules for disambiguation of word senses .Revision as of 04:20 , 25 June 2012 .Word Sense Disambiguation ( WSD ) is the process of identifying the sense of a polysemic word .","label":"Background","metadata":{},"score":"72.58485"}{"text":"Revision as of 04:20 , 25 June 2012 .Word Sense Disambiguation ( WSD ) is the process of identifying the sense of a polysemic word .In modern WSD systems , the senses of a word are typically taken from some specified dictionary .","label":"Background","metadata":{},"score":"72.84568"}{"text":"Revision as of 04:20 , 25 June 2012 .Word Sense Disambiguation ( WSD ) is the process of identifying the sense of a polysemic word .In modern WSD systems , the senses of a word are typically taken from some specified dictionary .","label":"Background","metadata":{},"score":"72.84568"}{"text":"Revision as of 04:20 , 25 June 2012 .Word Sense Disambiguation ( WSD ) is the process of identifying the sense of a polysemic word .In modern WSD systems , the senses of a word are typically taken from some specified dictionary .","label":"Background","metadata":{},"score":"72.84568"}{"text":"Revision as of 04:20 , 25 June 2012 .Word Sense Disambiguation ( WSD ) is the process of identifying the sense of a polysemic word .In modern WSD systems , the senses of a word are typically taken from some specified dictionary .","label":"Background","metadata":{},"score":"72.84568"}{"text":"Revision as of 04:20 , 25 June 2012 .Word Sense Disambiguation ( WSD ) is the process of identifying the sense of a polysemic word .In modern WSD systems , the senses of a word are typically taken from some specified dictionary .","label":"Background","metadata":{},"score":"72.84568"}{"text":"Oli .Thanks for your fast answer , Jacob !I tried different things now , but without any results .I am facing different issues : . - scipy - algorithms do not work for some reason - there are no binary releases of megam for windows ... and I was n't able to compile it on my machine .","label":"Background","metadata":{},"score":"73.3797"}{"text":"Iykeln .Hi Jacob , thanks for the info I have gotten so far from your articles .I have two corpora .One is pre - tagged in some fashion and the other a gold std .I wish to use transformation - based learning to improve the pre - tagged corpus , which brill fits in .","label":"Background","metadata":{},"score":"73.50654"}{"text":"In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987","label":"Background","metadata":{},"score":"74.6508"}{"text":"We immediately recognise that in the first sentence bank refers . to .the edge . of a . river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .","label":"Background","metadata":{},"score":"75.71121"}{"text":"CAN ANYONE PLEASE SUGGEST ME THE BEST TAGGING METHOD .I HAVE TO MAKE A PROJECT .ANYONE PLZZZZZZZZZZZZZ HELP .Hi Max , I just updated the infochimps page to list all the tags .The VB+ tags are pretty rare , but most of the rest are fairly common in the brown corpus .","label":"Background","metadata":{},"score":"75.890045"}{"text":"We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .","label":"Background","metadata":{},"score":"75.924095"}{"text":"We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .","label":"Background","metadata":{},"score":"75.924095"}{"text":"We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .","label":"Background","metadata":{},"score":"75.924095"}{"text":"There 's also a significant difference in the file size of the pickled taggers ( trained on treebank ) : .Fin .I think there 's a lot of room for experimentation with classifier based taggers and their feature detectors .","label":"Background","metadata":{},"score":"76.1416"}{"text":"Thanks .I 'm not sure what you mean by pre - tagged .A corpus is either tagged or not tagged .In order to use a brill tagger , you must have an initial tagger , such as a UnigramTagger .","label":"Background","metadata":{},"score":"78.21255"}{"text":"Dannii .You said : \" A ClassifierBasedPOSTagger does not need a backoff tagger , since cpos accuracy is exactly the same as for craubt across all corpora .\" This is probably because you did n't set the classifier 's cutoff_prob parameter .","label":"Background","metadata":{},"score":"79.24874"}{"text":"We immediately recognise that in the first sentence bank refers . to .the edge of a river and in the second . to a . building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .","label":"Background","metadata":{},"score":"79.36878"}{"text":"The problem of WSD was first introduced by Warren Weaver in 1949 [ 3 ] .In 1975 Kelly and Stone [ 4 ] published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .","label":"Background","metadata":{},"score":"79.717064"}{"text":"The problem of WSD was first introduced by Warren Weaver in 1949 [ 3 ] .In 1975 Kelly and Stone [ 4 ] published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .","label":"Background","metadata":{},"score":"79.717064"}{"text":"The problem of WSD was first introduced by Warren Weaver in 1949 [ 3 ] .In 1975 Kelly and Stone [ 4 ] published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .","label":"Background","metadata":{},"score":"79.717064"}{"text":"The problem of WSD was first introduced by Warren Weaver in 1949 [ 3 ] .In 1975 Kelly and Stone [ 4 ] published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .","label":"Background","metadata":{},"score":"79.717064"}{"text":"The problem of WSD was first introduced by Warren Weaver in 1949 [ 3 ] .In 1975 Kelly and Stone [ 4 ] published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .","label":"Background","metadata":{},"score":"79.717064"}{"text":"The problem of WSD was first introduced by Warren Weaver in 1949 [ 3 ] .In 1975 Kelly and Stone [ 4 ] published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .","label":"Background","metadata":{},"score":"79.717064"}{"text":"The problem of WSD was first introduced by Warren Weaver in 1949 [ 3 ] .In 1975 Kelly and Stone [ 4 ] published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .","label":"Background","metadata":{},"score":"79.717064"}{"text":"electronic computer either current or imaginable \" .Y. Bar - Hillel .Language and Information .Addison - Wesley , 1964 .However , the situation is not as bad as Bar - Hillel feared , there have been several advances in .","label":"Background","metadata":{},"score":"81.66886"}{"text":"I have n't tried any of scipy algorithms ; all I can recommend is to be sure numpy and scipy are correctly installed .That 's too bad about megam , maybe someone on the mailing list can help out .I 've been thinking about doing an evaluation of each of the training algorithms for speed and memory consumption .","label":"Background","metadata":{},"score":"84.17909"}{"text":"I have n't tried any of scipy algorithms ; all I can recommend is to be sure numpy and scipy are correctly installed .That 's too bad about megam , maybe someone on the mailing list can help out .I 've been thinking about doing an evaluation of each of the training algorithms for speed and memory consumption .","label":"Background","metadata":{},"score":"84.17909"}{"text":"The van pulled up outside the bank and three masked men got out .We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .","label":"Background","metadata":{},"score":"84.22781"}{"text":"The van pulled up outside the bank and three masked men got out .We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .","label":"Background","metadata":{},"score":"84.22781"}{"text":"The van pulled up outside the bank and three masked men got out .We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .","label":"Background","metadata":{},"score":"84.22781"}{"text":"The van pulled up outside the bank and three masked men got out .We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .","label":"Background","metadata":{},"score":"84.22781"}{"text":"The van pulled up outside the bank and three masked men got out .We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .","label":"Background","metadata":{},"score":"84.22781"}{"text":"The van pulled up outside the bank and three masked men got out .We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .","label":"Background","metadata":{},"score":"84.22781"}{"text":"The van pulled up outside the bank and three masked men got out .We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .","label":"Background","metadata":{},"score":"84.22781"}{"text":"The problem of WSD was first introduced by Warren Weaver in 1949 W. Weaver .In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press . . . .","label":"Background","metadata":{},"score":"84.45114"}{"text":"Oli .Hi Jacob , . as I mentioned on the google - group , your post is very interesting .Thanks for it .I am still seeking for the real implementation of pos_tag ( ) .You mentioned a different feature_detector .","label":"Background","metadata":{},"score":"86.611786"}{"text":"# The boy leapt from the bank into the cold water .# The van pulled up outside the bank and three masked men got out .We immediately recognise that in the first sentence bank refers . to .the edge . of a . river and in the second to a building .","label":"Background","metadata":{},"score":"88.416016"}{"text":"Max .Hi Jacob , .Thanks for your fast reply !I was away ( on vacation ) but now I 'm back to work .I 'm following your recommendations , also learned NLTK Trainer and using for training .","label":"Background","metadata":{},"score":"88.68483"}{"text":"This way using my own POS tagger as the first in the chain will tag \" General_Electric \" as a single noun and chunking stage may be more successful .Also I 'll have to somehow remember that \" General_Electric \" refers to the original \" General Electric \" and belongs to \" Companies \" list .","label":"Background","metadata":{},"score":"91.506836"}{"text":"A minor node is selected ( from among those listed at the current major node ) to head the current major node .The selected node may be replaced with one of its descendants .( This is called head - deletion . )","label":"Background","metadata":{},"score":"93.367516"}{"text":"In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 The problem of WSD was first introduced by Warren Weaver in 1949 W. Weaver .","label":"Background","metadata":{},"score":"94.24521"}