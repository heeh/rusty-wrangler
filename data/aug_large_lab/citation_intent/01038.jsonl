{"text":"Conventional HMM based word alignment models include states that indicate ( position , word ) pairs from which a word in one language results in a corresponding word in another language ( or an alternate word , such as a synonym , in the same language ) .","label":"Background","metadata":{},"score":"25.308037"}
{"text":"As noted above , the word alignment modeler provides word - dependent transition models for use with HMM based alignment models .However , conventional transition models that only depend on source word positions ( i.e. , the ( position , word ) pairs described in Section 2.2 ) are generally not as accurate as might otherwise be desired .","label":"Background","metadata":{},"score":"25.715118"}
{"text":"Note that word emission models for use in HMMs are known to those skilled in the art , and will not be described in detail herein .Initial training of these word - dependent transition models is performed using sets of training data comprising known parallel phrases ( i.e. , matched phrase or sentences having approximately the same meaning ) .","label":"Background","metadata":{},"score":"28.262217"}
{"text":", 1996 ) .( Toutanova et al . , 2002 ) and ( Lopez and Resnik , 2005 ) presented a variety of refinements of the HMM model particularly effective for low data conditions .The common thread connecting these works is their reliance on the 1-to - N approximation , w .. \" ...","label":"Background","metadata":{},"score":"28.915442"}
{"text":"1 . 1.1System Overview : .As noted above , the word alignment modeler provides various techniques for aligning a source phrase to a target phrase using learned word - dependent transition models in HMM based probabilistic word alignment models .","label":"Background","metadata":{},"score":"29.722416"}
{"text":"Heuristics are then to combine these two word alignments to produce a final word - to - word mapping between a source sentence or phrase and a target sentence or phrase .Separate HMM based alignment models are used for each of these two passes , with the one being constructed on the source phrase to emit observations of the target phrase , and the other being constructed on the target side to emit observations of the source phrase , respectively .","label":"Background","metadata":{},"score":"29.939243"}
{"text":"Since the use of these contexts alone causes data sparsity problems , we develop a decision tree algorithm for clustering the contexts based on optimisation of t ... \" .We introduce alignment models for Machine Translation that take into account the context of a source word when determining its translation .","label":"Background","metadata":{},"score":"30.22707"}
{"text":"Heuristics are then used to combine these two word alignments to produce a final word - to - word mapping between a source sentence or phrase and a target sentence or phrase .Separate HMM based alignment models are used for each of these two passes , with the one being constructed on the source phrase to emit observations of the target phrase , and the other being constructed on the target side to emit observations of the source phrase , respectively .","label":"Background","metadata":{},"score":"30.27581"}
{"text":"In other words , the HMM is built on the source side , while observations are generated on the target side .In general , this alignment observation is accomplished by evaluating 340 the word alignment model to align the source phrase to the target phrase .","label":"Background","metadata":{},"score":"30.506298"}
{"text":"The model predicts block neighbors to carry out a phrasebased translation that explicitly handles local phrase re - ordering .We use a maximum likelihood criterion ... \" .In this paper , we present a novel training method for a localized phrase - based prediction model for statistical machine translation ( SMT ) .","label":"Background","metadata":{},"score":"31.209719"}
{"text":"Efficient estimation and alignment procedures for word and phrase alignment HMMs are developed for the alignment of parallel text .The development of these models is motivated by an analysis of the desirable features of IBM Model 4 , one of the original and most effective models for word alignment .","label":"Background","metadata":{},"score":"31.313583"}
{"text":"This mapping is typically generated using probabilistic word alignment modeling .A number of classical approaches to word alignment are based on Hidden Markov Model ( HMM ) based alignment models .Although HMM based word alignment approaches generally provide good translation performance , one weakness of conventional HMM based approaches is the use of coarse transition models which generally assume that word transition probabilities depend only on a jump width from a last model state to a next model state .","label":"Background","metadata":{},"score":"31.355492"}
{"text":"Phrase Order and Phrase Translation in Statistical Machine Translation , 2005 .X. Zhu .Phrase Order and Phrase Translation in Statistical Machine Translation , 2005 A word alignment modeler uses probabilistic learning techniques to train \" word - dependent transition models \" for use in constructing phrase level Hidden Markov Model ( HMM ) based word alignment models .","label":"Background","metadata":{},"score":"31.529575"}
{"text":"HMM word and phrase alignment for statistical machine translation .Deng , Y and Byrne , WJ ( 2005 ) HMM word and phrase alignment for statistical machine translation .In : Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing ( HLT - EMNLP'05 ) , 2005 - 10- to -- , Vancouver , Canada pp . 169 - 176 .","label":"Background","metadata":{},"score":"31.548384"}
{"text":"However , when increasing τ to a larger value , a stronger prior is applied , thereby resulting in a more robust word - dependent transition model that has been observed to significantly outperform the models used in conventional HMM based word alignment models .","label":"Background","metadata":{},"score":"32.64898"}
{"text":"As a result , phrase alignment models outperform classical word - level models in both generative and discriminative settings .This result is fundamental to the field : the models proposed in this thesis address a general , language - independent alignment problem that arises in all state - of - the - art statistical machine translation systems in use today .","label":"Background","metadata":{},"score":"33.722492"}
{"text":"Use the HMM alignment models to induce phrase translations under its statistical models .Phrase - pair induction can generate richer inventories of phrase translations than can be extracted from Viterbi alignments .Edit the C++ source code to implement your own estimation and alignment procedures .","label":"Background","metadata":{},"score":"33.92913"}
{"text":"In other words , as described in further detail in Section 2.4.2 , the tuning parameter is varied to control the contribution of the prior distribution in model training in order to tune word alignment performance of the corresponding word - dependent transition model .","label":"Background","metadata":{},"score":"34.179092"}
{"text":"We report on investigations into hierarchical phrase - based translation grammars based on rules extracted from posterior distributions over alignments of the parallel text .Rather than restrict rule extraction to a single alignment , such as Viterbi , we instead extract rules based on posterior distributions provided by the HMM word - to - word alignment model .","label":"Background","metadata":{},"score":"34.47802"}
{"text":"We report on investigations into hierarchical phrase - based translation grammars based on rules extracted from posterior distributions over alignments of the parallel text .Rather than restrict rule extraction to a single alignment , such as Viterbi , we instead extract rules based on posterior distributions provided by the HMM word - to - word alignment model .","label":"Background","metadata":{},"score":"34.47802"}
{"text":"Related schemes have modeled self - transition probability separately from other transition probabilities to address cases where there is no analog for a particular word in the language to which a phrase is being translated .Further adaptations of such schemes include using a word - to - phrase HMM in which a source word dependent phrase length model is used to improve translation results .","label":"Background","metadata":{},"score":"34.63961"}
{"text":"2.0 Operation Overview : .The above - described program modules are employed for implementing various embodiments of the word alignment modeler .As summarized above , the word alignment modeler provides various techniques for learning probabilistic word - dependent transition models for use in aligning a source phrase to a target phrase via phrase level HMM based word alignment models .","label":"Background","metadata":{},"score":"34.90178"}
{"text":"Abstract .A word alignment modeler uses probabilistic learning techniques to train \" word - dependent transition models \" for use in constructing phrase level Hidden Markov Model ( HMM ) based word alignment models .HMM based word alignment models are then used for various word alignment and machine translation tasks .","label":"Background","metadata":{},"score":"34.90412"}
{"text":"FIG .3 , the word alignment modeler begins operation by receiving 300 training data 200 comprising one or more sets of parallel texts .As described above , these parallel texts are basically two sets of texts , either in different languages or in the same language , that have known matches between words and phrases in each of the parallel texts .","label":"Background","metadata":{},"score":"34.910988"}
{"text":"The cumulative result of this thesis is to establish model - based phrase alignment as the most effective approach to acquiring phrasal translations .Only phrase alignment models are able to incorporate statistical signals about multi - word constructions into alignment decisions and score coherent phrasal analyses of full sentence pairs .","label":"Background","metadata":{},"score":"35.040386"}
{"text":"In this paper we present ... . \" ...Word alignment is the problem of annotating parallel text with translational correspondence .Previous generative word alignment models have made structural assumptions such as the 1-to-1 , 1-to - N , or phrase - based consecutive word assumptions , while previous discriminative models have either made such ... \" .","label":"Background","metadata":{},"score":"35.07463"}
{"text":"Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation ( MLE ) , the parameters of the IHMM are estimated indirectly from a variety of sources including word semantic similarity , word surface similarity , and a distance - based distortion penalty .","label":"Background","metadata":{},"score":"35.81436"}
{"text":"This paper presents a syntax - driven approach to question answering , specifically the answer - sentence selection problem for short - answer questions .Rather than using syntactic features to augment existing statistical classifiers ( as in previous work ) , we build on the idea that questions and their ( co ... \" .","label":"Background","metadata":{},"score":"35.832886"}
{"text":"Previous generative word alignment models have made structural assumptions such as the 1-to-1 , 1-to - N , or phrase - based consecutive word assumptions , while previous discriminative models have either made such an assumption directly or used features derived from a generative model making one of these assumptions .","label":"Background","metadata":{},"score":"35.879025"}
{"text":"We compare and contrast the strengths and weaknesses of a syntax - based machine translation model with a phrase - based machine translation model on several levels .We briefly describe each model , highlighting points where they differ .We include a quantitative comparison of the phrase pairs that each ... \" .","label":"Background","metadata":{},"score":"36.069122"}
{"text":"Next , given the source phrase , the word alignment modeler uses a HMM model construction module 250 to construct a phrase level HMM based word alignment model using the source phrase and the learned word - dependent transition models and other HMM components 220 .","label":"Background","metadata":{},"score":"36.24096"}
{"text":"The processes summarized above are illustrated by the general system diagram of .FIG .2 .In particular , the system diagram of .FIG .1 illustrates the interrelationships between program modules for implementing various embodiments of the word alignment modeler , as described herein .","label":"Background","metadata":{},"score":"36.96804"}
{"text":"Experimental results using the TREC dataset are shown to significantly outperform strong state - of - the - art baselines .We can also observe a trend in recent work in textual entailment that more emphasis is put on explicit learning of the syntactic graph mapping between the entail ... . \" ...","label":"Background","metadata":{},"score":"37.139736"}
{"text":"This paper presents a simple and robust consensus decoding approach for combining multiple Machine Translation ( MT ) system outputs .A consensus network is constructed from an N - best list by aligning the hypotheses against an alignment reference , where the alignment is based on minimising the transla ... \" .","label":"Background","metadata":{},"score":"37.328747"}
{"text":"In this paper , we describe a mixture extension of the HMM alignment model and the derivation of Viterbi alignments to feed a state - of - the - art phrase - based system .Experiments carried out on the Europarl and News Commentary corpora show the potential interest and limitations of mixture modelling . ...","label":"Background","metadata":{},"score":"37.662567"}
{"text":"Further , as described above , in the case of sparse data , i.e. , rarely used words , the prior probabilities can be adjusted using a tuning parameter to improve overall model performance .Once learned , the word - dependent transition models 220 are then used in combination with input source phrases 320 to construct 330 phrase level HMM based word alignment models for each input phrase .","label":"Background","metadata":{},"score":"37.671356"}
{"text":"These tasks include , for example , word alignment tasks and translating phrases or sentences from one language to another language , or from one language to alternate phrases or sentences in the same language .Clearly , similar training data represented by parallel texts in other languages would be used to train the word - dependent transition models depending upon which language pairs are to be translated .","label":"Background","metadata":{},"score":"37.685524"}
{"text":"FIG .2 . 2.1 Operational Details of the Word Alignment Modeler : .In general , initial training of word - dependent transition models is based on one or more sets of training data comprising parallel texts in two languages of interest .","label":"Background","metadata":{},"score":"37.74169"}
{"text":"A novel and robust approach to improving statistical machine translation fluency is developed within a minimum Bayesrisk decoding framework .By segmenting translation lattices according to confidence measures over the maximum likelihood translation hypothesis we are able to focus on regions with potential translation errors .","label":"Background","metadata":{},"score":"38.251347"}
{"text":"Such tasks include , for example , word alignment tasks , and translating phrases or sentences from one language ( i.e. , a \" source phrase \" ) to a corresponding \" target phrase \" in another language .Similarly , a source phrase in one language can also be aligned to an alternate phrase or sentence in the same language given appropriate training of the word - dependent transition models .","label":"Background","metadata":{},"score":"38.29805"}
{"text":"Each case is handled separately , with source phrases ( in the language from which source phrases will be translated ) being used to construct the HMM based word alignment models which then emit the corresponding target phrases .The following paragraphs detail specific operational and alternate embodiments of the word alignment modeler described herein .","label":"Background","metadata":{},"score":"38.303467"}
{"text":"A consensus network is constructed from an N - best list by aligning the hypotheses against an alignment reference , where the alignment is based on minimising the translation edit rate ( TER ) .The Minimum Bayes Risk ( MBR ) decoding technique is investigated for the selection of an appropriate alignment reference .","label":"Background","metadata":{},"score":"38.39431"}
{"text":"It automatically learns aligned tree sequence pairs with mapping probabilities from word - aligned biparsed parallel texts .Compared with previous models , it not only captures non - syntactic phrases and discontinuous phrases with linguistically structured features , but also supports multi - level structure reordering of tree typology with larger span .","label":"Background","metadata":{},"score":"38.398567"}
{"text":"These alignment models can also be used to generate posterior statistics over collections of parallel text , and this is used to refine and extend phrase translation tables with a resulting improvement in translation quality .We address the problem of extracting bilingual chunk pairs from parallel text to create training sets for statistical machine translation .","label":"Background","metadata":{},"score":"38.426666"}
{"text":"A method for estimating an alignment between words in a source phrase and words in a target phrase for constructing a word alignment model , comprising : . providing at least one set of probabilistic word - dependent transition models , said word - dependent transition models having been automatically learned from at least one training data set comprising known parallel texts representing a source language and a target language ; . each word - dependent transition model models source word transition probabilities as a combination of a self - jump probability of a particular source language word and probabilities of jumping from that word to other particular source language words ; . providing a source phrase in the source language and selecting a corresponding word - dependent transition model for each word in the source phrase ; . constructing a Hidden Markov Model ( HMM ) on the source phrase from the probabilistic word - dependent transition models for each word of the source phrase in combination with other HMM components including word emission models ; . evaluating the HMM to determine an alignment between the source phrase and a target phrase in the target language ; and .","label":"Background","metadata":{},"score":"38.510254"}
{"text":"We briefly describe each model , highlighting points where they differ .We include a quantitative comparison of the phrase pairs that each model has to work with , as well as the reasons why some phrase pairs are not learned by the syntax - based model .","label":"Background","metadata":{},"score":"38.556557"}
{"text":"Possible unfavorable items are n : m mapping objects , such as paraphrases , non - literal translations , and multiword expressions .This paper presents a pre - processing method which detects such unfavorable items before supplying them to the word aligner under the assumption that their frequency is low , such as below 5 percent .","label":"Background","metadata":{},"score":"38.667496"}
{"text":"Rather than using syntactic features to augment existing statistical classifiers ( as in previous work ) , we build on the idea that questions and their ( correct ) answers relate to each other via loose but predictable syntactic transformations .We propose a probabilistic quasi - synchronous grammar , inspired by one proposed for machine translation ( D. Smith and Eisner , 2006 ) , and parameterized by mixtures of a robust nonlexical syntax / alignment model with a(n optional ) lexical - semantics - driven log - linear model .","label":"Background","metadata":{},"score":"38.732403"}
{"text":"A computer - readable storage device having computer executable instructions stored thereon for determining a probabilistic mapping between a source phrase and a target phrase , comprising instructions for : . providing an automatically learned word - dependent transition model for each source language word in a training set comprising known parallel texts in a source language and a target language ; . each word - dependent transition model models source word transition probabilities as a combination of a self - jump probability of a particular source language word and probabilities of jumping from that word to other particular source language words ; . receiving a source phrase in the source language ; . selecting a corresponding one of the word - dependent transition models for each word in the source phrase ; . constructing a source phrase based Hidden Markov Model ( HMM ) including the selected word - dependent transition models ; and . determining a probabilistic mapping between the source phrase and a target phrase in the target language by evaluating the HMM .","label":"Background","metadata":{},"score":"38.750687"}
{"text":"We evaluate the performance of our approach on a Chinese - to - English machine translation task , and report a 12.2 % relative increase in BLEU score over a state - of - the art phrasebased SMT system . ... fined as the problem of determining a translational correspondence at word level given a parallel corpus of aligned sentences .","label":"Background","metadata":{},"score":"38.781853"}
{"text":"We introduce a simple method to pack words for statistical word alignment .Our goal is to simplify the task of automatic word alignment by packing several consecutive words together when we believe they correspond to a single word in the opposite language .","label":"Background","metadata":{},"score":"38.80555"}
{"text":"We introduce a simple method to pack words for statistical word alignment .Our goal is to simplify the task of automatic word alignment by packing several consecutive words together when we believe they correspond to a single word in the opposite language .","label":"Background","metadata":{},"score":"38.80555"}
{"text":"The model leverages on the strengths of both phrase - based and linguistically syntax - based method .It automatically learns aligned tree ... \" .This paper presents a translation model that is based on tree sequence alignment , where a tree sequence refers to a single sequence of subtrees that covers a phrase .","label":"Background","metadata":{},"score":"38.873844"}
{"text":"Our approach consists of a simple method for utilizing a bridge language to create a word alignment system and a procedure for com ... \" .We describe an approach to improve Statistical Machine Translation ( SMT ) performance using multi - lingual , parallel , sentence - aligned corpora in several bridge languages .","label":"Background","metadata":{},"score":"38.907784"}
{"text":"This paper compares several translation representations for a synchronous context - free grammar parse including CFGs / hypergraphs , finite - state automata ( FSA ) , and pushdown automata ( PDA ) .The representation choice is shown to determine the form and complexity of target LM intersection and shortest - path algorithms that follow .","label":"Background","metadata":{},"score":"38.9506"}
{"text":"Therefore , in order to improve the transition model in HMM based translation scenarios , the word alignment modeler extends transition probabilities to be word - dependent so that the probability of jumping from state a j .- to a j not only depends on a j .","label":"Background","metadata":{},"score":"39.377815"}
{"text":"In additional embodiments , the word alignment modeler addresses data sparsity problems in word - dependent transition modeling by using probabilistic learning techniques , such as , for example , Bayesian learning , to estimate word - dependent transition model parameters by maximum a posteriori ( MAP ) training .","label":"Background","metadata":{},"score":"39.489098"}
{"text":"In general , the HMM model learning module 210 models each word in the training data set by automatically learning probabilistic models ( i.e. , word - dependent transition models along with other HMM components such as word emission models 220 ) for each word .","label":"Background","metadata":{},"score":"39.537437"}
{"text":"In the case where the source and target phrases are in different languages , the alignment of the source phrase to the target phrase has the effect of translating the source phrase from a first language into a corresponding target sentence in a second language .","label":"Background","metadata":{},"score":"39.608154"}
{"text":", Extensions to HMM - based Statistical Word Alignment Models , Proceedings of the ACL-02 conference on Empirical methods in natural language processing - vol .10 , Date : 2002 , pp .87 - 94 , Association for Computational Linguistics , USA .","label":"Background","metadata":{},"score":"39.71592"}
{"text":"Using a variational Bayes training procedure , we learn the latent structure of translation equivalence through the induction of synchronous grammar categories for phrasal translations , showing improvements in translation performance over maximum likelihood models .As the model is non - parametric , the HDP prior will provide a bias towards parameter distributions using as many , or as few , non - terminals as necessary to model the training data .","label":"Background","metadata":{},"score":"39.81034"}
{"text":"Finally , we show that the best way to exploit source - totarget and target - to - source alignment models is to build two separate systems and combine their output translation lattices . \" ...This article describes the use of pushdown automata ( PDA ) in the context of statistical machine translation and alignment under a synchronous context - free grammar .","label":"Background","metadata":{},"score":"40.19154"}
{"text":"An indirect hidden Markov model ( IHMM ) is proposed to address the synonym matching and word ordering issues in hypothesis alignment .Unlike traditional HMMs whose parameters are ... \" .This paper presents a new hypothesis alignment method for combining outputs of multiple machine translation ( MT ) systems .","label":"Background","metadata":{},"score":"40.383102"}
{"text":"We assess this experimentally on a large - scale Chinese - to - English alignment and translation task .In translation , we propose a two - pass decoding strategy involving a weaker language model in the first - pass to address the results of PDA complexity analysis .","label":"Background","metadata":{},"score":"40.639847"}
{"text":"F. Zuo .An HTK + OpenFst ASR Decoder , 2008 ( joint with Prof. P.C. Woodland ) .L. Feng .Limited domain synthesis , 2008 . D. Herath .Lexical Context in HMM Word Alignment , 2008 . A. Murugesan .","label":"Background","metadata":{},"score":"40.832237"}
{"text":"We show that shallow - n grammars , low - level rule catenation , and other search constraints can help to match the power of the translation system to specific language pairs . ... program and portions of the newswire sections of MT02 through MT05 .","label":"Background","metadata":{},"score":"41.169437"}
{"text":"We find that the use of WFSTs rather than k - best lists requires less pruning in translation search , resulting in fewer search errors , better parameter optimization , and improved translation performance .The direct generation of translation lattices in the target language can improve subsequent rescoring procedures , yielding further gains when applying long - span language models and Minimum Bayes Risk decoding .","label":"Background","metadata":{},"score":"41.188065"}
{"text":"It is designed mainly for building statistical machine translation systems , but can be exploited in other multilingual applications .It provides computationally efficient alignment and estimation procedures that can be used for the unsupervised alignment of parallel text collections in a language independent fashion .","label":"Background","metadata":{},"score":"41.250416"}
{"text":"One advantage of learning both sets of word - dependent transition models from the training data is that each language can be represented as either a source or a target , thereby allowing alignments from either language as a source to the other language as a target .","label":"Background","metadata":{},"score":"41.27413"}
{"text":"As such , important knowledge of jumping from a particular source word to another position , e.g. , jumping forward ( monotonic alignment ) or jumping backward ( non - monotonic alignment ) , is not modeled .Further , these types of translation schemes do not adequately address the problem of data sparsity in detailed word transition modeling .","label":"Background","metadata":{},"score":"41.343765"}
{"text":"storing the probabilistic mapping between the source phrase and the target phrase as an entry in a learned word alignment model .The process of .claim 16 further comprising using Bayesian learning for estimating parameters of the word - dependent transition models using maximum a posteriori ( MAP ) training for words that are sparsely represented in the pair of parallel texts of the selected pair of unique source and target languages .","label":"Background","metadata":{},"score":"41.364655"}
{"text":"HMM - based models are developed for the alignment of words and phrases in bitext .The models are formulated so that alignment and parameter estimation can be performed efficiently .We find that Chinese - English word alignment performance is comparable to that of IBM Model-4 even over large training bitexts .","label":"Background","metadata":{},"score":"41.40045"}
{"text":"This alignment - based approach to acquiring phrasal translations gives rise to statistical models of phrase alignment .The cumulative result of this thesis is to establish model - based phrase alignment as the most effective approach to acquiring phrasal translations .","label":"Background","metadata":{},"score":"41.436226"}
{"text":"Finally , we show that the best way to exploit source - totarget and target - to - source alignment models is to build two separate systems and combine their output translation lattices . \" ...A novel and robust approach to improving statistical machine translation fluency is developed within a minimum Bayesrisk decoding framework .","label":"Background","metadata":{},"score":"41.450554"}
{"text":"FIG .1 , the HMM based word alignment model is built on the source side ( i.e. , the Chinese phrase in this example ) , while observations are built on the target side ( i.e. , the English phrase in this example ) .","label":"Background","metadata":{},"score":"42.069183"}
{"text":"The first procedure is a now - standard dynamic programming alignment model which we use to generate an initial coarse alignment of the parallel text .The second procedure is a divisive clustering parallel text alignment procedure which we use to refine the first - pass alignments .","label":"Background","metadata":{},"score":"42.157898"}
{"text":"Consequently , in the above example , there is no requirement for the number of French words , J , to equal the number of English words , I. .Given this basic setup , and assuming a translation from the English sentence into the French sentence , using HMM based word alignment , a HMM is built at English side , i.e. , each ( position , word ) pair , ( a j , e a .","label":"Background","metadata":{},"score":"42.80587"}
{"text":"We report an empirical study of n - gram posterior probability confidence measures for statistical machine translation ( SMT ) .We first describe an efficient and practical algorithm for rapidly computing n - gram posterior probabilities from large translation word lattices .","label":"Background","metadata":{},"score":"42.879444"}
{"text":"We present a novel method for inducing synchronous context free grammars ( SCFGs ) from a corpus of parallel string pairs .SCFGs can model equivalence between strings in terms of substitutions , insertions and deletions , and the reordering of sub - strings .","label":"Background","metadata":{},"score":"43.119827"}
{"text":"Again , it should be noted that the two languages can be the same language so long as each text includes parallel sentences .The following sections describe training and use of word - dependent transition models that are trained using these types of parallel text data sets .","label":"Background","metadata":{},"score":"43.150124"}
{"text":"MPhil Students Supervised .M. Horvat .Lagrangian Relaxation for String Regeneration , 2013 .T. Foreman .Model extrapolation for HMM - based speech synthesis , 2010 ( joint with Dr. M. Gibson ) .Z. Raeesy .Model Interpolation for Speech Synthesis , 2009 ( joint with Dr. M. Gibson ) . A. Waite .","label":"Background","metadata":{},"score":"43.270416"}
{"text":"Experimental results show that our hierarchical alignment approach works very well even if OCR output has a high recognition error rate .Finally , we evaluate the performance of a commercial OCR engine over a large dataset of books based on the alignment results .","label":"Background","metadata":{},"score":"43.517204"}
{"text":"Our combined SMT system using the . \" ...We present a novel method for inducing synchronous context free grammars ( SCFGs ) from a corpus of parallel string pairs .SCFGs can model equivalence between strings in terms of substitutions , insertions and deletions , and the reordering of sub - strings .","label":"Background","metadata":{},"score":"44.04519"}
{"text":"The system incorporates a phrase - based model of string generation that aims to take unordered bags of words and produce fluent , grammatical sentences .We describe the generation grammars and introduce parsing procedures that address the computational complexity of generation under permutation of phrases .","label":"Background","metadata":{},"score":"44.13481"}
{"text":"See Sections 2.2 through 2.4 for further details regarding these points .Note that in one embodiment , there may be word - dependent transition models available for multiple languages , depending upon the training data 200 that was provided for training purposes , as described above .","label":"Background","metadata":{},"score":"44.43181"}
{"text":"However , these measures do not give any details about the nature of translation errors .Therefore some analysis of the generated output is needed in order to identify the main prob - lems and to focus the research efforts .On the other hand , human evaluation is a time consuming and expensive task .","label":"Background","metadata":{},"score":"44.484634"}
{"text":"In translation , we propose a two - pass decoding strategy involving a weaker language model in the first - pass to address the results of PDA complexity analysis .We study in depth the experimental conditions and tradeoffs in which HiPDT can achieve state - of - the - art performance for large - scale SMT .","label":"Background","metadata":{},"score":"44.790833"}
{"text":"This thesis focuses on acquiring translations of phrases : contiguous sequences of a few words that encapsulate enough context to be translatable , but recur frequently in large corpora .We automatically identify phrase - level translations that are contained within human - translated sentences by partitioning each sentence into phrases and aligning phrases across languages .","label":"Background","metadata":{},"score":"44.874557"}
{"text":"In the case where the source and target phrases are in the same language , the alignment of the source phrase to the target phrase has the effect of creating an alternate version of the source phrase in the same language as the source phrase .","label":"Background","metadata":{},"score":"44.898293"}
{"text":"FIG .2 represent alternate embodiments of the word alignment modeler described herein , and that any or all of these alternate embodiments , as described below , may be used in combination with other alternate embodiments that are described throughout this document .","label":"Background","metadata":{},"score":"44.96019"}
{"text":"Broadly characterized , statistical MT systems translate an input document by matching fragments of its contents to examples in a parallel corpus , and then stitching together the translations of those fragments into a coherent document in an output language .The central challenge of this approach is to distill example translations into reusable parts : fragments of sentences that we know how to translate robustly and are likely to recur .","label":"Background","metadata":{},"score":"45.01307"}
{"text":"One advantage of this method of training is that it can use the large number of hypotheses encoded in a translation lattice as training data .We demonstrate that the MERT line optimisation can be modelled as computing the shortest distance in a weighted finite - state transducer using a tropical polynomial semiring .","label":"Background","metadata":{},"score":"45.115555"}
{"text":"Experimental results on the NIST MT-2005 Chinese - English translation task show that our method statistically significantly outperforms the baseline systems . ... translations .i1 3.2 Tree Sequence Translation Model Figure 3 : Two examples of translation rules tax - based methods .","label":"Background","metadata":{},"score":"45.20059"}
{"text":"The availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple systems .This paper describes three different approaches to MT system combination .These combination methods operate on sentence , phrase and word level exploiting information from - best lists , system scores and target - to - source phrase alignments .","label":"Background","metadata":{},"score":"45.205437"}
{"text":"dependent .2.4 Probabilistic Learning for Word - Dependent Transition Models : .In general , a number of probabilistic learning techniques can be applied for learning probabilistic models .In one embodiment , the word alignment modeler uses Bayesian learning techniques including Maximum a Posteriori ( MAP ) training for learning the word - dependent transition models .","label":"Background","metadata":{},"score":"45.342094"}
{"text":"FIG .2 , various embodiments of the word alignment modeler begin operation by receiving one or more sets of training data 200 via a training data input module 205 .As noted above , each training data set represents a pair of known parallel texts , such as the aforementioned Hansard Corpus , comprising matching or parallel sentences or phrases in a desired source language and a desired target language .","label":"Background","metadata":{},"score":"45.530525"}
{"text":"Note that this concept is discussed in further detail in Section 2.3 with respect to Equation ( 8) .Further , in one embodiment , a tuning parameter adjustment module 215 is used to improve word - dependent transition models for the case where the training data 200 includes sparse ( i.e. , rarely used ) words .","label":"Background","metadata":{},"score":"45.55621"}
{"text":"In adding syntax to statistical MT , there is a tradeoff between taking advantage of linguistic analysis , versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data .A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment .","label":"Background","metadata":{},"score":"45.97046"}
{"text":"FIG .1 due to the relatively short length of the source phrase in this example ) .In view of the jumps illustrated in the HMM - based word alignment model of .FIG .1 , evaluation of the HMM - based word alignment model results in a probabilistic alignment of the Chinese phrase to the English phrase , as follows : .","label":"Background","metadata":{},"score":"46.103535"}
{"text":"Many modifications and variations are possible in light of the above teaching .Further , it should be noted that any or all of the aforementioned alternate embodiments may be used in any combination desired to form additional hybrid embodiments of the word alignment modeler .","label":"Background","metadata":{},"score":"46.36422"}
{"text":"Gene ... \" .This article describes the use of pushdown automata ( PDA ) in the context of statistical machine translation and alignment under a synchronous context - free grammar .We use PDAs to com - pactly represent the space of candidate translations generated by the grammar when applied to an input sentence .","label":"Background","metadata":{},"score":"46.539246"}
{"text":"Koehn , et al . , \" Statistical Phrase - Based Translation \" , Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - vol .1 , Date : 2003 , pp .","label":"Background","metadata":{},"score":"46.66809"}
{"text":"Koehn , et al . , \" Statistical Phrase - Based Translation \" , Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - vol .1 , Date : 2003 , pp .","label":"Background","metadata":{},"score":"46.66809"}
{"text":"In particular , .FIG .3 provides an exemplary operational flow diagram which illustrates operation of several embodiments of the word alignment modeler .Note that .FIG .3 is not intended to be an exhaustive representation of all of the various embodiments of the word alignment modeler described herein , and that the embodiments represented in .","label":"Background","metadata":{},"score":"47.224365"}
{"text":"We use PDAs to compactly represent the space of candidate translations generated by the grammar when applied to an input sentence .General - purpose PDA algorithms for replacement , composition , shortest path , and expansion are presented .We describe HiPDT , a hierarchical phrase - based decoder using the PDA representation and these algorithms .","label":"Background","metadata":{},"score":"47.270718"}
{"text":"Alignment behavior is analyzed and compared to human - generated reference alignments , and the ability of these models to capture different types of alignment phenomena is evaluated .In analyzing alignment performance , Chinese - English word alignments are shown to be comparable to those of IBM Model-4 even when models are trained over large parallel texts .","label":"Background","metadata":{},"score":"47.700203"}
{"text":"A novel stochastic gradient descent ( SGD ) training algorithm is presented that can easily handle millions of features .Moreover , when viewing SMT as a block generation process , it becomes quite similar to sequential natural language annotation problems such as part - of - speech tagging , phrase chunking , or shallow parsing .","label":"Background","metadata":{},"score":"47.73985"}
{"text":"Experimental results are presented primarily based on three - way combination of Chinese - English translation outputs , and also presents results for six - way system combination .It is shown that worthwhile improvements in translation performance can be obtained using the methods discussed .","label":"Background","metadata":{},"score":"47.913445"}
{"text":"Toutanova , et al ., Extensions to HMM - based Statistical Word Alignment Models , Proceedings of the ACL-02 conference on Empirical methods in natural language processing - vol .10 , Date : 2002 , pp .87 - 94 , Association for Computational Linguistics , USA .","label":"Background","metadata":{},"score":"48.04457"}
{"text":"We believe this is the first work to automatically align a whole book without using any book structure information .The alignment process works by breaking up the problem of aligning two long sequences into the problem of aligning many smaller subsequences .","label":"Background","metadata":{},"score":"48.065"}
{"text":"On the other hand , for the same word , when a small tuning parameter , τ , is used , a weak prior distribution is applied , and the transition probability is more dependent on the training data of that word .","label":"Background","metadata":{},"score":"48.07101"}
{"text":"Generate word - to - word and word - to - phrase alignments of parallel text .MTTK can generate Viterbi alignments of parallel text ( both training text and other texts ) under the supported alignment models .Extract word - to - word translation tables from aligned bitext and from the estimated models .","label":"Background","metadata":{},"score":"48.29164"}
{"text":"However , these measures do not give any details about the nature of translation errors .Therefore some analysis of the generated output is needed in order to identify the main problems and to focus the research efforts .On the other hand , human evaluation is a time consuming and expensive task .","label":"Background","metadata":{},"score":"48.37912"}
{"text":"Train statistical models for parallel text alignment .The following models are supported : .Parallelize your model training procedures .If you have multiple CPUs available , you can partition your translation training texts into subsets , thus speeding up iterative parameter re - estimation procedures and reducing the amount of memory needed in training .","label":"Background","metadata":{},"score":"48.528057"}
{"text":"The process of .claim 16 further comprising instructions for generating a translation from the source phrase to the target language by using the probabilistic mapping between the source phrase and the target phrase to construct a string of words in the target language .","label":"Background","metadata":{},"score":"48.949158"}
{"text":"p .i .i .I . )p . if .i .p .p .i .i .I . ) otherwise .Equation .Then , when training a conventional HMM based transition model , the model parameters , Λ , are usually estimated through maximum likelihood ( ML ) training , as illustrated by Equation 4 , where : . ML . arg .","label":"Background","metadata":{},"score":"49.19823"}
{"text":"l .I .Equation .2.4.2 Setting Hyper - Parameters for the Prior Distribution : .Next , substituting Equation ( 14 ) into Equation ( 13 ) , the MAP based word - dependent transition model training formula is derived , as illustrated by Equation ( 15 ) .","label":"Background","metadata":{},"score":"49.443623"}
{"text":"Once the phrase alignment module 250 has evaluated the HMM based alignment model , the resulting target phrase is provided to a target phrase output module 265 to be output .In the simplest case , only alignment data between the source phrase and the resulting target phrase is provided by the target phrase output module 265 .","label":"Background","metadata":{},"score":"49.5147"}
{"text":"We assess these grammars in terms of their expressive power , measured by their ability to align the parallel text from which their rules are extracted , and the quality of the translations they yield .In Chinese - to - English translation , we find that rule extraction from posteriors gives translation improvements .","label":"Background","metadata":{},"score":"49.78293"}
{"text":"We assess these grammars in terms of their expressive power , measured by their ability to align the parallel text from which their rules are extracted , and the quality of the translations they yield .In Chinese - to - English translation , we find that rule extraction from posteriors gives translation improvements .","label":"Background","metadata":{},"score":"49.78293"}
{"text":"l .i .Equation .As noted above , a sentence or phrase in one language may not have the same number of words when translated to another language .Therefore , a \" null \" word is used to generate a word in the target language that does not correspond to a word in the sentence in the first language .","label":"Background","metadata":{},"score":"49.819324"}
{"text":"The quality of chunk pairs are measured by the performance of machine translation systems trained from them .We show practical benefits of divisive clustering as well as how system performance can be improved by exploiting portions of the parallel text that otherwise would have to be discarded .","label":"Background","metadata":{},"score":"49.893944"}
{"text":"At the other extreme , whole sentences can be translated without much context , but rarely repeat , and so can not be recycled to build new translations .This thesis focuses on acquiring translations of phrases : contiguous sequences of a few words that encapsulate enough context to be translatable , but recur frequently in large corpora .","label":"Background","metadata":{},"score":"49.919464"}
{"text":"Evaluation of machine translation output is an important but difficult task .Over the last years , a variety of automatic evalua - tion measures have been studied , some of them like Word Error Rate ( WER ) , Posi - tion Independent Word Error Rate ( PER ) and BLEU and NIST scores have become widely used tools ... \" .","label":"Background","metadata":{},"score":"49.923817"}
{"text":"In this case , initial word - dependent transition model training would use a same - language equivalent of the Hansard Corpus , or the like , or be can constructed using other learning methods .However , it should also be noted that word alignment can be used for tasks other that translation tasks .","label":"Background","metadata":{},"score":"49.928177"}
{"text":"We also compare the translation accuracy for all variations .A primary difference is the use of syntax trees on the target side , rather than sequences of w .. by Min Zhang , Hongfei Jiang , Aiti Aw , Haizhou Li , Chew Lim Tan , Sheng Li - In Proceedings of ACL , 2008 . \" ...","label":"Background","metadata":{},"score":"50.0029"}
{"text":"In addition to the just described benefits , other advantages of the word alignment modeler will become apparent from the detailed description that follows hereinafter when taken in conjunction with the accompanying drawing figures .DESCRIPTION OF THE DRAWINGS .The specific features , aspects , and advantages of the present invention will become better understood with regard to the following description , appended claims , and accompanying drawings where : .","label":"Background","metadata":{},"score":"50.100517"}
{"text":"Next , the resulting English target phrase would then be used as a source phrase in a second pass of HMM based modeling for aligning the English phrase to a Chinese target phrase .The results of these two passes are generally not in perfect agreement due to the probabilistic nature of the HMM based alignment process .","label":"Background","metadata":{},"score":"50.26208"}
{"text":"We obtain substantial improvements in performance for translation from Chinese and Arabic to English .Chiang ( 2005 ) distinguishes statistical MT approaches that are \" syntactic \" in a formal sense , going beyond the finite - state underpinnings of phrasebased models , from approaches ... . by Antti - veikko I. Rosti , Necip Fazil Ayan , Bing Xiang , Spyros Matsoukas , Richard Schwartz , Bonnie J. Dorr - In Proceedings of the North American Chapter of the Association for Computational Linguistics Human Language Technologies , 2007 . \" ...","label":"Background","metadata":{},"score":"50.440628"}
{"text":"These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge .The availability of such a variety o ... \" .Currently there are several approaches to machine translation ( MT ) based on different paradigms ; e.g. , phrasal , hierarchical and syntax - based .","label":"Background","metadata":{},"score":"50.70665"}
{"text":"We introduce an easy to implement and general purpose solution to tackle this problem : we store SMT models as a set of key - value pairs in an HFile .We apply this strategy to two specific tasks : test set hierarchical phrase - based rule filtering and n - gram count filtering for language model lattice rescoring .","label":"Background","metadata":{},"score":"50.792656"}
{"text":"This may be viewed as equivalent to the problem of aligning two large ( easily a million long ) sequences .The problem is further complicated by OCR errors as well as the possibility of large chunks of missing material in one of the sequences .","label":"Background","metadata":{},"score":"50.812668"}
{"text":"2 provides an exemplary architectural flow diagram that illustrates program modules for implementing various embodiments of a word alignment modeler , as described herein .FIG .3 illustrates a general system flow diagram that illustrates exemplary methods for implementing various embodiments of the word alignment modeler , as described herein .","label":"Background","metadata":{},"score":"50.987244"}
{"text":"When combined with our previous work on forest - based decoding , it achieves a 2.5 BLEU points improvement over the baseline , and even outperforms the hierarchical system of Hiero by 0.7 points . ...t list will only capture a tiny fraction of the whole space .","label":"Background","metadata":{},"score":"51.082985"}
{"text":"Align document translation pairs at the sentence or sub - sentence level , sometimes known as chunking .This is a useful pre - processing step to prepare collections of translations for use in estimating the parameters of complex alignment models .","label":"Background","metadata":{},"score":"51.180508"}
{"text":"4.0 Exemplary Operating Environments : .FIG .4 and .FIG .5 illustrate two examples of suitable computing environments on which various embodiments and elements of a word alignment modeler , as described herein , may be implemented .For example , .","label":"Background","metadata":{},"score":"51.292496"}
{"text":"We use a maximum likelihood criterion to train a log - linear block bigram model which uses real - valued features ( e.g. a language model score ) as well as binary features based on the block identities themselves ( e.g. block bigram features ) .","label":"Background","metadata":{},"score":"51.4478"}
{"text":"We show that our contextdependent models lead to an improvement in alignment quality , and an increase in translation quality when the alignments are used in Arabic - English and Chinese - English translation . by Mei Yang University , Mei Yang - In Proceedings of the European Chapter of the ACL , 2006 . \" ...","label":"Background","metadata":{},"score":"51.597492"}
{"text":"Over the last years , a variety of automatic evaluation measures have been studied , some of them like Word Error Rate ( WER ) , Position Independent Word Error Rate ( PER ) and BLEU and NIST scores have become widely used tools f ... \" .","label":"Background","metadata":{},"score":"51.856674"}
{"text":"The current dominant practice only uses 1-best trees , which adversely affects the rule set quality due to parsing errors .So we propose a novel approach which extracts rules from a packed forest that compactly encodes exponentially many parses .","label":"Background","metadata":{},"score":"52.18312"}
{"text":"Hierarchical phrase - based translation grammars extracted from alignment posterior probabilities .In Proceedings of the Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , Cambridge , MA , 2010 .Graeme Blackwood , Adrià de Gispert , William Byrne .","label":"Background","metadata":{},"score":"52.284134"}
{"text":"1.3 Word Alignment Example : .These unique transition models are described herein using the term \" word - dependent transition models . \" In general , .FIG .1 provides a simple example of word alignment between a parallel sentence pair of corresponding English and Chinese sentences , given initial word - dependent transition model training using known English - Chinese parallel text data .","label":"Background","metadata":{},"score":"52.365967"}
{"text":"Therefore , in various embodiments , the parameter τ is varied control the contribution of the prior distribution in model training in order to tune word alignment performance of the word - dependent transition model .For example , in various tested embodiments , the value of τ was varied in a range from 0 to 100,000 .","label":"Background","metadata":{},"score":"52.686478"}
{"text":"Alignments from a word in the source phrase to a word in the target phrase ( i.e. , 105 aligns to 120 ) ; .Jumps from a word in the source phrase to a \" null \" ( i.e. , 105 to 150 ) where a word such as \" the \" in the target phrase does not have an equivalent in the language of the source phrase ; .","label":"Background","metadata":{},"score":"52.746834"}
{"text":"For print OCR [ 3 ] created groundtruth by using a machine readable description to print the document and then match ... . by Gonzalo Iglesias , Cyril Allauzen , William Byrne , Adrià Gispert , Michael Riley . \" ...This paper compares several translation representations for a synchronous context - free grammar parse including CFGs / hypergraphs , finite - state automata ( FSA ) , and pushdown automata ( PDA ) .","label":"Background","metadata":{},"score":"52.76088"}
{"text":"j .i . a .j .i .d .f .J .e .I .Equation .The probability mass for transitions having a jump width larger than 6 is uniformly divided .In addition two separate sets of distortion parameters were used for transitioning into the first state of the HMM model , and for transitioning out of the last state of the HMM model , respectively .","label":"Background","metadata":{},"score":"53.03346"}
{"text":"As defined herein the learned \" word - dependent transition models \" provide a probabilistic model wherein for each unique source word in the training data , a self - transition probability is modeled in combination with a probability of jumping from that particular word to a different word .","label":"Background","metadata":{},"score":"53.124897"}
{"text":"Comprehensive n - gram precision and word coverage measurements are presented for a variety of different language pairs , domains and conditions .We analyze the effect on reference precision of using single or multiple references , and compare the precision of posteriors computed from k - best lists to those computed over the full evidence space of the lattice .","label":"Background","metadata":{},"score":"53.250298"}
{"text":"Thus , going back to the English to French translation example described above , the new probabilistic formulation can be illustrated by Equation ( 8) , where : . p .f .J .e .I . ) a .","label":"Background","metadata":{},"score":"53.30931"}
{"text":"Efficient Path Counting Transducers for Minimum Bayes - Risk Decoding of Statistical Machine Translation Lattices .Proc .Annual Meeting of the Association for Computational Linguistics ( ACL ) 2010 Mikko Kurimo , et al .Personalising speech - to - speech translation in the EMIME project .","label":"Background","metadata":{},"score":"53.83216"}
{"text":"The method of . claim 1 further comprising using Bayesian learning for estimating parameters of the word - dependent transition models using maximum a posteriori ( MAP ) training for words that are sparsely represented in the training data set .The method of . claim 2 further comprising an adjustable tuning parameter for adjusting a prior distribution of the word - dependent transition model for one or more of the sparsely represented words .","label":"Background","metadata":{},"score":"53.865387"}
{"text":"FIG .5 , this figure shows a general system diagram showing a simplified computing device .Such computing devices can be typically be found in devices having at least some minimum computational capability in combination with a communications interface , including , for example , cell phones PDA 's , dedicated media players ( audio and/or video ) , etc .","label":"Background","metadata":{},"score":"54.244896"}
{"text":"j .a .j .I . )I .p . a .j .a .j .I . )Equation .Finally , after training , Viterbi decoding is used to find a best alignment sequence , â 1 J , as illustrated by Equation ( 7 ) : . a .","label":"Background","metadata":{},"score":"54.6067"}
{"text":"i .I .e . a .j .e . )Pr . a .j .i . a .j .i .d .f .J .e .I .Equation .However , for many non - frequent or sparse words ( i.e. , words that are rarely used ) , the data samples for c(d ; e ) are very limited .","label":"Background","metadata":{},"score":"54.67733"}
{"text":"The central challenge of this approach is to distill example translations into reusable parts : fragments of sentences that we know how to translate robustly and are likely to recur .Individual words are certainly common enough to recur , but they often can not be translated correctly in isolation .","label":"Background","metadata":{},"score":"54.71064"}
{"text":"3 are provided only for purposes of explanation .Further , it should be noted that any boxes and interconnections between boxes that are represented by broken or dashed lines in .FIG .3 represent optional or alternate embodiments of the word alignment modeler described herein , and that any or all of these optional or alternate embodiments , as described below , may be used in combination with other alternate embodiments that are described throughout this document .","label":"Background","metadata":{},"score":"55.06408"}
{"text":"Chinese - to - English translation experiments using HiFST and HiPDT , FSA and PDA - based decoders , are presented using admissible ( or exact ) search , possible for HiFST with compact SCFG rulesets and HiPDT with compact LMs .","label":"Background","metadata":{},"score":"55.20024"}
{"text":"For example , assume that training data 200 is available to learn word - dependent transition models 220 for aligning English to French phrases , and French to German phrases .The result is a two stage alignment from English source phrases to German target phrases .","label":"Background","metadata":{},"score":"55.481743"}
{"text":"W. Yuan .Small Language Models for Large Problems , 2007 . D. Sharkov .Phrasal Models in Stochastic Machine Translation , 2006 .Y.Q. Liu .Phrasal Language Models in Stochastic Machine Translation , 2006 .J. Smith .Phrasal Language Models in Stochastic Machine Translation , 2005 .","label":"Background","metadata":{},"score":"55.546204"}
{"text":"While MT systems are in wide use today , the problem of producing human - quality translations remains unsolved .Statistical approaches have substantially improved the quality of MT systems by effectively exploiting parallel corpora : large collections of documents that have been translated by people , and therefore naturally occur in both the input and output languages .","label":"Background","metadata":{},"score":"55.584274"}
{"text":"Training and evaluation of techniques for handwriting recognition and retrieval is a challenge given that it is difficult to create large ground - truthed datasets .This is especially true for historical handwritten datasets .In many instances the ground truth has to be created by manually transcribing each word , which is a very labor intensive process .","label":"Background","metadata":{},"score":"55.648487"}
{"text":"Consequently , in recent years there has been a renewed interest in improving the ' naturalness ' of ASR and SMT output , even in systems that produce good WER and BLEU scores .In this talk , the perceived ' naturalness ' of ASR and SMT transcriptions will be considered in the context of on - going debates about grammaticality and acceptability .","label":"Background","metadata":{},"score":"55.96615"}
{"text":"MTTK is a collection of software tools for the alignment of parallel text for use in Statistical Machine Translation .The toolkit was written by Yonggang Deng in the course of his Ph.D. at The Johns Hopkins University Center for Language and Speech Processing .","label":"Background","metadata":{},"score":"56.26582"}
{"text":"Categories and Subject Descriptors : I.2.7 [ Artificial Intelligence ] : Natural Language Processing - statistical machine translation ; G.3 [ Probability and Statistics ] : Statistical computing- stochastic gradient descent \" ...We report on investigations into hierarchical phrase - based translation grammars based on rules extracted from posterior distributions over alignments of the parallel text .","label":"Background","metadata":{},"score":"56.382008"}
{"text":"tems , standard MERT ( Och , 2003 ) iterative parameter estimation under IBM BLEU4 is performed on the development set .We then follow standard heuristics ( Chiang , 2007 ) and filtering strategies ( Iglesias et al . , 2009b ) to extract hierarchical phrases from the ... . by Christoph Tillmann , Tong Zhang - ACM Transactions Speech Language Processing , 2007 . \" ...","label":"Background","metadata":{},"score":"56.618477"}
{"text":"We extract hierarchical rules from the aligned parallel texts using the constraints developed by Chiang ( 2007 ) .We further filter the extracted rules by count and pattern as described by Iglesias e .. \" ...Mixture modelling is a standard technique for density estimation , but its use in statistical machine translation ( SMT ) has just started to be explored .","label":"Background","metadata":{},"score":"56.66727"}
{"text":"This assumption gives rise to the following probabilistic formulation for translating a French sentence to an English sentence : . p .f .J .e .I . ) a .J .j .J .[ .p . a .","label":"Background","metadata":{},"score":"56.761086"}
{"text":"We propose a backoff model for phrasebased machine translation that translates unseen word forms in foreign - language text by hierarchical morphological abstractions at the word and the phrase level .The model is evaluated on the Europarl corpus for German - English and Finnish - English translation and shows improvements over state - of - the - art phrase - based models . by Maja Popović , Hermann Ney , Adrià De Gispert , José B. Mariño , Deepa Gupta , Marcello Federico , Patrik Lambert , Rafael Banchs - In Proc . of NAACL Workshop on Statistical Machine Translation , 2006 . \" ...","label":"Background","metadata":{},"score":"56.93332"}
{"text":"It is shown that the output produced by the CCG - based system is considerably improved if the N - best generated hypotheses are rescored and reranked using Ngram - based techniques .Dr. Yue Zhang will take up position as Assistant Professor at Singapore University of Technology and Design with effect from July 2012 .","label":"Background","metadata":{},"score":"57.28917"}
{"text":"Our system incorporates dependency language models , large n - gram language models , and minimum Bayes risk decoding .December 2013 - Collaboration with Michael Riley and Cyril Allauzen of Google Research to appear in Computational Linguistics .Pushdown Automata in Statistical Machine Translation .","label":"Background","metadata":{},"score":"57.30611"}
{"text":"This task --- so simple in its specification , and yet so rich in its complexities --- has challenged computer science researchers for 60 years .While MT systems are in wide use today , the problem of producing human - quality translations remains unsolved .","label":"Background","metadata":{},"score":"57.496082"}
{"text":"We describe HiPDT , a hierarchical phrase - based decoder using the PDA representation and these algorithms .We contrast the complexity of this decoder with a de - coder based on a finite state automata representation , showing that PDAs provide a more suitable framework to achieve exact decoding for larger synchronous context - free grammars and smaller language models .","label":"Background","metadata":{},"score":"57.53224"}
{"text":"Marcus Tomalin gave a seminar titled ' In Search of ' Natural ' Speech : Grammaticality , Acceptability , and Speech Technology ' at the Edinburgh Linguistics Circle , March 2012 .Although state - of - the - art large vocabulary Automatic Speech Recognition ( ASR ) and Statistical Machine Translation ( SMT ) systems often achieve impressive Word Error Rates ( WERs ) and BLEU scores respectively , end - users frequently consider the word sequences output by such systems to be ' unnatural ' .","label":"Background","metadata":{},"score":"57.90426"}
{"text":"Direct phrase pair induction under the model is described and shown to improve translation performance .MTTK is released under the Open Source Educational Community License .You 're free to use , modify , and distribute MTTK under the terms of this license .","label":"Background","metadata":{},"score":"57.96778"}
{"text":"This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description .This Summary is not intended to identify key features or essential features of the claimed subject matter , nor is it intended to be used as an aid in determining the scope of the claimed subject matter .","label":"Background","metadata":{},"score":"58.255543"}
{"text":"We show that phrase structures in Penn Treebank style parses are not optimal for syntaxbased machine translation .We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploi ... \" .","label":"Background","metadata":{},"score":"58.30272"}
{"text":"However , it is only necessary to train one set of word - dependent transition models 220 to align a desired source phrase to a desired target phrase .Given the learned word - dependent transition models 220 , the word alignment modeler then continues operation by receiving a source phrase ( in the desired source language ) that is input via a source phrase input module 225 .","label":"Background","metadata":{},"score":"58.50986"}
{"text":"Parallel corpora are made by human beings .However , as an MT system is an aggregation of state - of - the - art NLP technologies without any intervention of human beings , it is unavoidable that quite a few sentence pairs are beyond its analysis and that will therefore not contribute to the system .","label":"Background","metadata":{},"score":"58.914505"}
{"text":"Parallel corpora are made by human beings .However , as an MT system is an aggregation of state - of - the - art NLP technologies without any intervention of human beings , it is unavoidable that quite a few sentence pairs are beyond its analysis and that will therefore not contribute to the system .","label":"Background","metadata":{},"score":"58.914505"}
{"text":"4 is a general system diagram depicting a general - purpose computing device constituting an exemplary system for implementing various embodiments of the word alignment modeler , as described herein .FIG .5 is a general system diagram depicting a general computing device having simplified computing and I / O capabilities for use in implementing various embodiments of the word alignment modeler , as described herein .","label":"Background","metadata":{},"score":"59.1957"}
{"text":"Hierarchical Phrase - based Translation with Weighted Finite State Transducers .Invited Presentation and Best Thesis Award for 2010 .15th Annual Conference of the European Association for Machine Translation , Leuven , Belgium , March 2011 .G. Blackwood .Lattice Rescoring Methods for Statistical Machine Translation .","label":"Background","metadata":{},"score":"59.198475"}
{"text":"The final translation is obtained by consensus decoding that combines hypotheses obtained using all bridge language word alignments .We present experiments showing that multilingual , parallel text in Spanish , French , Russian , and Chinese can be utilized in this framework to improve translation performance on an Arabic - to - English task . ... sed SMT system ( Och and Ney , 2003 ; Fraser and Marcu , 2006b ) .","label":"Background","metadata":{},"score":"59.45186"}
{"text":"The decoder is implemented with standard Weighted Finite - State Transducer ( WFST ) operations as an alternative to the well - known cube pruning procedure .We find that the use of WFSTs ra ... \" .In this article we describe HiFST , a lattice - based decoder for hierarchical phrase - based translation and alignment .","label":"Background","metadata":{},"score":"59.96698"}
{"text":"Changing any step ( including the scanning process ) can affect OCR performance and hence a good automatic statistical evaluation of OCR performance on book length material is needed .Evaluating OCR performance on the entire book is non - trivial .","label":"Background","metadata":{},"score":"60.183674"}
{"text":"Yonggang Deng , Ph.D. Electrical and Computer Engineering , JHU , 2005 Bitext Alignment for Statistical Machine Translation Thesis defense [ . pdf ] , Dissertation [ . pdf ] Now at IBM T.J. Watson Research .Stavros Tsakalidis , Ph.D. Electrical and Computer Engineering , JHU , 2005 Linear Transforms in Automatic Speech Recognition : Discriminative Estimation Procedures and Integration of Diverse Acoustic Data Thesis defense [ . pdf ] , Dissertation [ . pdf ] Now at BBN Technologies .","label":"Background","metadata":{},"score":"60.268105"}
{"text":"Abstract .Training and evaluation of techniques for handwriting recognition and retrieval is a challenge given that it is difficult to create large ground - truthed datasets .This is especially true for historical handwritten datasets .In many instances the ground truth has to be created by manually t ... \" .","label":"Background","metadata":{},"score":"60.412495"}
{"text":"j .J .[ .p . a .j .a .j .e . a .j .I . )p .f .j .e . a .j . ) . ]Equation . - , e a . j-","label":"Background","metadata":{},"score":"60.585228"}
{"text":"FIG .5 represent alternate embodiments of the simplified computing device , and that any or all of these alternate embodiments , as described below , may be used in combination with other alternate embodiments that are described throughout this document .","label":"Background","metadata":{},"score":"60.89508"}
{"text":"These transcriptions were created for other purposes and hence correspondence at the word , line , or sentence level may not be available .To be useful for training and evaluation , a word level correspondence must be available between the segmented handwritten word images and the ASCII transcriptions .","label":"Background","metadata":{},"score":"60.999878"}
{"text":"These systems employ more than just the surface - level information used by the state - of - the - art phrase - based translation systems .Combined with the latest advances in phrase - based translation systems , it has become more attractive to take advantage of the various ... . by Steve Deneefe , Kevin Knight , Wei Wang , Daniel Marcu - In Proc .","label":"Background","metadata":{},"score":"61.05184"}
{"text":"February , 2011 .Adrià de Gispert , Gonzalo Iglesias , Graeme Blackwood , Eduardo Banga , William Byrne .Hierarchical Phrase - based Translation with Weighted Finite State Transducers and Shallow - N Grammars .Computational Linguistics 36(3):505 - 533 .","label":"Background","metadata":{},"score":"61.254387"}
{"text":"Mixture modelling is a standard technique for density estimation , but its use in statistical machine translation ( SMT ) has just started to be explored .One of the main advantages of this technique is its capability to learn specific probability distributions that better fit subsets of the training dataset .","label":"Background","metadata":{},"score":"61.365498"}
{"text":"We exploit a series of binarization methods to restructure the Penn Treebank style trees such that syntactified phrases smaller than Penn Treebank constituents can be acquired and exploited in translation .We find that by employing the EM algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result . ... algorithm for determining the binarization of a parse tree among a set of alternative binarizations gives us the best translation result .","label":"Background","metadata":{},"score":"61.365894"}
{"text":"ML . i .i .e .I . )c .i .i .e . )l .I .c .l .i .e . )Equation .c .d .e . ) j .","label":"Background","metadata":{},"score":"61.57498"}
{"text":"We investigate methods for improving the quality of such translations by making use of part - ofspeech information and maximum entropy modeling .Results for translations from English into Spanish and Catalan are presented on the LC - STAR corpus which consists of spontaneously spoken dialogues in the domain of appointment scheduling and travel planning . by Jamie Brunning , Adrià De Gispert , William Byrne - In Proceedings of Human Language Technologies : The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics , 2009 . \" ...","label":"Background","metadata":{},"score":"62.179466"}
{"text":"a .j .I . )p .f .j .e . a .j . ) . ]Equation .p .i .i .I . )c .i .i .l .I .","label":"Background","metadata":{},"score":"62.565918"}
{"text":"Self jumps from a word in the source phrase to itself ( i.e. , 110 to 110 ) , where the word in the source phrase ( 110 ) aligns to two or more words ( 140 and 145 ) in the target phrase ; and .","label":"Background","metadata":{},"score":"62.58999"}
{"text":"The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network .In a distributed computing environment , program modules may be located in both local and remote computer storage media including memory storage devices .","label":"Background","metadata":{},"score":"63.02021"}
{"text":"FIG .5 may also include other components , such as , for example one or more input devices 540 ( analogous to the input devices described with respect to .FIG .4 ) .The simplified computing device of .","label":"Background","metadata":{},"score":"63.035637"}
{"text":"In the following description of the preferred embodiments of the present invention , reference is made to the accompanying drawings , which form a part hereof , and in which is shown by way of illustration specific embodiments in which the invention may be practiced .","label":"Background","metadata":{},"score":"63.051918"}
{"text":"Tools . \" ...Translation rule extraction is a fundamental problem in machine translation , especially for linguistically syntax - based systems that need parse trees from either or both sides of the bitext .The current dominant practice only uses 1-best trees , which adversely affects the rule set quality due to par ... \" .","label":"Background","metadata":{},"score":"63.055946"}
{"text":"FIG .1 , a Chinese source phrase , \" wancheng zhaosheng gongzuo \" denoted by Chinese characters .( characters 105 , 110 , and 115 , respectively ) , aligns to a target English phrase \" finish the task of recruiting students \" ( words 120 , 125 , 130 , 135 , 140 , and 145 , respectively ) .","label":"Background","metadata":{},"score":"64.26909"}
{"text":"In yet another embodiment , the target phrase is output as a voice or speech output 275 by using conventional voice synthesis techniques to render a voice approximation of the text string represented by the target phrase .Finally , in another embodiment , the target phrase provided via the target phrase output module 265 is provided back to the source phrase input module 225 as a new source phrase input .","label":"Background","metadata":{},"score":"65.16043"}
{"text":"Mei Yang , Katrin Kirchhoff - in \" Proceedings of the 21stInternational Conference on Computational Linguistics , 2006 . \" ...We propose a backoff model for phrasebased machine translation that translates unseen word forms in foreign - language text by hierarchical morphological abstractions at the word and the phrase level .","label":"Background","metadata":{},"score":"65.1694"}
{"text":"4 .The logical connections depicted in .FIG .4 include a local area network ( LAN ) 471 and a wide area network ( WAN ) 473 , but may also include other networks .Such networking environments are commonplace in offices , enterprise - wide computer networks , intranets , and the Internet .","label":"Background","metadata":{},"score":"65.36428"}
{"text":"claim 9 further comprising instructions for using Bayesian learning for estimating parameters of the word - dependent transition models using maximum a posteriori ( MAP ) training for words that are sparsely represented in the training set .The computer - readable storage device of . claim 10 further comprising instructions for providing an adjustable tuning parameter for adjusting a prior distribution of the word - dependent transition models for sparsely represented words .","label":"Background","metadata":{},"score":"65.642746"}
{"text":"Hierarchical phrase - based translation with weighted finite state transducers .Google , Inc , Mountain View , CA , USA , April 2010 .William Byrne .FAUST project overview .ICT - FP7 Language Technology Days , Luxembourg , March 2010 .","label":"Background","metadata":{},"score":"65.64426"}
{"text":"5 also includes storage 560 that is either removable 570 and/or non - removable 580 ( analogous to the storage devices described above with respect to .FIG .4 ) .The foregoing description of the word alignment modeler has been presented for the purposes of illustration and description .","label":"Background","metadata":{},"score":"65.77565"}
{"text":"The length of this thesis including appendices , references , footnotes , tables and equations is approximately 53,000 words and it contains 56 figures and 58 tables .iSummary Modern statistical mach ... . \" ...We report on investigations into hierarchical phrase - based translation grammars based on rules extracted from posterior distributions over alignments of the parallel text .","label":"Background","metadata":{},"score":"65.95762"}
{"text":"Fast Hiero grammars .DARPA GALE PI Meeting , Scottsdale , AZ , USA , April 2010 .William Byrne .Hierarchical phrase - based translation with weighted finite state transducers .Columbia University , New York , NY , USA , April 2010 .","label":"Background","metadata":{},"score":"66.33617"}
{"text":"claim 16 further comprising providing a user interface for entering the received source phrase in the selected language .Description .BACKGROUND .Technical Field .Related Art .Word alignment is an important step in typical approaches to statistical machine translation .","label":"Background","metadata":{},"score":"67.14543"}
{"text":"Therefore , in one embodiment , in order to address this issue , a maximum a posteriori ( MAP ) framework is applied .Specifically , in MAP training , an appropriate prior distribution , g , is used to incorporate prior knowledge into the model parameter estimation , as illustrated by Equation ( 11 ) : .","label":"Background","metadata":{},"score":"67.60668"}
{"text":"Proceedings of the International Conference on Computational Linguistics ( COLING ) 2010 Juan Pino , Gonzalo Iglesias , Adrià de Gispert , Graeme Blackwood , Jamie Brunning and William Byrne .The CUED HiFST System for the WMT10 Translation Shared Task .","label":"Background","metadata":{},"score":"67.88951"}
{"text":"Unless otherwise stated , performance was measured using TER and BLEU scores based on detokenised lowercase translations .System 2004 TER BLEU 2005 TER BLEU ISI Phrase 55.63 35.89 56.792 33.85 Hiero ... . \" ...In adding syntax to statistical MT , there is a tradeoff between taking advantage of linguistic analysis , versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data .","label":"Background","metadata":{},"score":"67.99214"}
{"text":"The method of . claim 1 further comprising providing the source phrase by receiving a text based document .The method of . claim 1 further comprising providing the source phrase through speech recognition of a speech signal .The method of . claim 1 further comprising selecting a language of the source language from a set of available source languages corresponding to one set of the learned probabilistic word - dependent transition models .","label":"Background","metadata":{},"score":"68.137665"}
{"text":"3.0 Operation : .The processes described above with respect to .FIG .2 and in further view of the detailed description provided above in Sections 1 and 2 are illustrated by the general operational flow diagram of .FIG .","label":"Background","metadata":{},"score":"68.301155"}
{"text":"The computer - readable storage device of .claim 9 further comprising instructions for performing speech recognition of a speech signal for receiving the source phrase .The computer - readable storage device of .claim 9 further comprising instructions for receiving the source phrase via a text input device .","label":"Background","metadata":{},"score":"68.704735"}
{"text":"February 2012 -- Article to appear in Speech Communication .Impacts of machine translation and speech synthesis on speech - to - speech translation Kei Hashimoto , Junichi Yamagishi , William Byrne , Simon King , Keiichi Tokuda .Graeme Blackwood ( CUED PhD 2010 ) starts as Research Staff Member in the Machine Translation Group at the IBM T.J. Watson Research Center on the 3rd of October .","label":"Background","metadata":{},"score":"69.10239"}
{"text":"These include the Million Book Project , the Google Book project and similar efforts from Yahoo and Microsoft .Content - based on line book retrieval usually requires first converting printed text into machine readable ( e.g. ASCII ) text using an optical character recognition ( OCR ) engine and then doing full text search on the results .","label":"Background","metadata":{},"score":"69.20332"}
{"text":"Very little work has been done on the alignment of handwritten data to transcripts .Here , a novel Hidden Markov Model based automatic alignment algorithm is described and tested .The algorithm produces an average alignment accuracy of about 72.8 % when aligning whole pages at a time on a set of 70 pages of the George Washington collection .","label":"Background","metadata":{},"score":"70.2768"}
{"text":"July 2012 FSMNLP paper on links between LMERT and tropical polynomials .Lattice - based minimum error rate training using weighted finite - state transducers with tropical polynomial weights . A. Waite , G. Blackwood , W. Byrne .10thInternational Workshop on Finite State Methods and Natural Language Processing ( FSMNLP 2012 ) , Donostia - San Sebastian , Spain , July 2012 .","label":"Background","metadata":{},"score":"70.33469"}
{"text":"4 , the processing unit(s ) 510 illustrated in .FIG .5 may be specialized ( and inexpensive ) microprocessors , such as a DSP , a VLIW , or other micro - controller rather than the general - purpose processor unit of a PC - type computer or the like , as described above .","label":"Background","metadata":{},"score":"70.71968"}
{"text":"p .f .J .e .I .Equation . and where an efficient Expectation - Maximization algorithm can be used to optimize Λ iteratively until convergence .c .d . ) j .J .i .I .","label":"Background","metadata":{},"score":"70.83623"}
{"text":"Item Type : .Conference or Workshop Item ( UNSPECIFIED ) .Subjects : .UNSPECIFIED Tools . by Adria de Gispert , Gonzalo Iglesias , Graeme Blackwood , Eduardo R. Banga , William Byrne - IN PROCEEDINGS OF HLT / NAACL , 2010 . \" ...","label":"Background","metadata":{},"score":"72.07293"}
{"text":"Annual Meeting of the Association for Computational Linguistics ( ACL ) 2010 ( Demo session ) Phrase Alignment Models for Statistical Machine Translation .John Sturdy DeNero .EECS Department University of California , Berkeley Technical Report No .UCB / EECS-2010 - 161 December 16 , 2010 .","label":"Background","metadata":{},"score":"72.40076"}
{"text":"This dissertation is the result of my own work and includes nothing which is the outcome of work done in collaboration except where specifically indicated in the text .It has not been submitted in whole or in part for a degree at any other university .","label":"Background","metadata":{},"score":"73.255615"}
{"text":"This dissertation is the result of my own work and includes nothing which is the outcome of work done in collaboration except where specifically indicated in the text .It has not been submitted in whole or in part for a degree at any other university .","label":"Background","metadata":{},"score":"73.255615"}
{"text":"By way of example , and not limitation , .FIG .4 illustrates remote application programs 485 as residing on memory device 481 .It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used .","label":"Background","metadata":{},"score":"73.600876"}
{"text":"When translating from languages with hardly any inflectional morphology like English into morphologically rich languages , the English word forms often do not contain enough information for producing the correct fullform in the target language .We investigate methods for improving the quality ... \" .","label":"Background","metadata":{},"score":"73.63017"}
{"text":"EMIME project overview .European Commission Information Society Conference ( ICT 2010 ) , Brussels , Belgium , September 2010 . A. de Gispert .Hierarchical Phrase - Based Translation with weighted finite state transducers .Talk at IST / INESC - id , Lisbon ( Portugal ) , July 2010 .","label":"Background","metadata":{},"score":"73.68081"}
{"text":"claim 9 further comprising instructions for generating a translation from the source phrase to the target language by using the probabilistic mapping between the source phrase and a target phrase to construct a string of words in the target language .The computer - readable storage device of .","label":"Background","metadata":{},"score":"73.89679"}
{"text":"Rory Waite , Ph.D. Cambridge University Department of Engineering , 2015 The Geometry of Statistical Machine Translation [ . pdf ] Now with SDL Research .Juan Pino , Ph.D. Cambridge University Department of Engineering , 2015 Refinements in Hierarchical Phrase - Based Translation Systems [ . pdf ] Now with Facebook .","label":"Background","metadata":{},"score":"73.92705"}
{"text":"max .p .f .J .e .I . g .e .I . )Equation .The conjugate prior distribution of these transition probabilities is a Dirichlet distribution taking the form illustrated by Equation ( 12 ) , where : . g .","label":"Background","metadata":{},"score":"74.10342"}
{"text":"The invention is operational with numerous other general purpose or special purpose computing system environments or configurations .The invention may be described in the general context of computer - executable instructions , such as program modules , being executed by a computer in combination with hardware modules , including components of a microphone array 498 .","label":"Background","metadata":{},"score":"74.65849"}
{"text":"i .i .e . i .I . )e .I . )i .I .p .i .i .e . i .I . )v .i .i .Equation .p .","label":"Background","metadata":{},"score":"74.787155"}
{"text":"A number of projects are creating searchable digital libraries of printed books .These include the Million Book Project , the Google Book project and similar efforts from Yahoo and Microsoft .Content - based on line book retrieval usually requires first converting printed text into machine readable ( e. ... \" .","label":"Background","metadata":{},"score":"75.45627"}
{"text":"5 may also include other optional components , such as , for example one or more output devices 550 ( analogous to the output devices described with respect to .FIG .4 ) .Finally , the simplified computing device of .","label":"Background","metadata":{},"score":"75.48471"}
{"text":"Talk at DARPA GALE PI Meeting , Arlington , VA ( USA ) , May 2011 . A. de Gispert .Hierarchical Phrase - Based Translation at University of Cambridge .Talk at Google Research Labs , Mountain View , CA ( USA ) , May 2011 .","label":"Background","metadata":{},"score":"75.91552"}
{"text":"For example , while the Chinese phrase consists of three words ( 105 , 110 , and 115 ) , the English phrase consists of six words ( 120 , 125 , 130 , 135 , 140 , and 145 ) .","label":"Background","metadata":{},"score":"76.14526"}
{"text":"One example of parallel text training data is the well known \" Hansard Corpus \" which consists of parallel texts in English and Canadian French , drawn from official records of the proceedings of the Canadian Parliament over a period of years .","label":"Background","metadata":{},"score":"76.300385"}
{"text":"Graeme Blackwood , Ph.D. Cambridge University Department of Engineering , 2010 Lattice Rescoring Methods for Statistical Machine Translation [ . pdf ] Now Research Staff Member in the Machine Translation Group at the IBM T.J. Watson Research Center .Jamie Brunning , Ph.D. Cambridge University Department of Engineering , 2010 Alignment Models and Algorithms for Statistical Machine Translation [ . pdf ] .","label":"Background","metadata":{},"score":"76.39252"}
{"text":"William Byrne .Hierarchical phrase - based translation with weighted finite state transducers .Keynote speech at IWSLT 2010 , Paris ( France ) , December 2010 .William Byrne .Hierarchical phrase - based translation with weighted finite state transducers .","label":"Background","metadata":{},"score":"77.27908"}
{"text":"William Byrne .Recent research in statistical machine translation .Winton Capital Management Internal Research Conference , November 2010 .Invited presentation .William Byrne .Hierarchical phrase - based translation with weighted finite state transducers .Dublin Computational Linguistics Research Seminar , Dublin , Ireland , November 2010 .","label":"Background","metadata":{},"score":"78.484215"}
{"text":"In particular , as illustrated by .FIG .5 , the computational capability is generally illustrated by processing unit(s ) 510 ( roughly analogous to processing units 420 described above with respect to .FIG .4 ) .Note that in contrast to the processing unit(s ) 420 of the general computing device of .","label":"Background","metadata":{},"score":"78.612526"}
{"text":"An example of this concept is that the word \" the \" in the English language does not have an equivalent word in the Chinese language .This idea is further addressed below in Section 1.3 with respect to the discussion of .","label":"Background","metadata":{},"score":"78.75261"}
{"text":"Minimum Bayes - Risk Lattice Rescoring Methods for Statistical Machine Translation .Natural Language Processing Seminar , Computer Lab , University of Cambridge .May 2011 .G. Blackwood .Lattice Rescoring Methods for Statistical Machine Translation .Talk at SRI , Menlo Park , CA ( USA ) , April 2011 .","label":"Background","metadata":{},"score":"79.202835"}
{"text":"Vaibhava Goel , Ph.D. Biomedical Engineering , JHU , 2001 Minimum Bayes - Risk Automatic Speech Recognition Now at IBM T.J. Watson Research .Asela Gunawardana , Ph.D. Electrical and Computer Engineering , JHU , 2001 The Information Geometry of EM Variants for Speech and Image Processing Now at Microsoft Research .","label":"Background","metadata":{},"score":"80.553116"}
{"text":"Ed Hughes from the Department of Pure Maths and Mathematical Statistics will work with Rory Waite on SMT system optimisation using techniques from tropical geometry .Ed will be sponsored by the DPMMS Post Master 's Consultancy Scheme .Data availability and distributed computing techniques have allowed statistical machine translation ( SMT ) researchers to build larger models .","label":"Background","metadata":{},"score":"80.76366"}
{"text":"Shankar Kumar , Ph.D. Electrical and Computer Engineering , JHU , 2005 Minimum Bayes - Risk Techniques in Automatic Speech Recognition and Machine Translation Thesis defense [ . pdf ] , Dissertation [ . pdf ] Now at Google , Inc. .","label":"Background","metadata":{},"score":"81.069595"}
{"text":"J . arg .max .a .J .j .J .[ .p . a .j .a .j .I . )p .f .j .e . a .j . ) . ]","label":"Background","metadata":{},"score":"81.41382"}
{"text":"MAP .i .i .e .I . )c .i .i .e . )p .i .i .I . )l .I .c .l .i .e . )","label":"Background","metadata":{},"score":"81.77958"}
{"text":"Automatic post - editors ( APEs ) enable the re - use of black box machine translation ( MT ) systems for a variety of tasks where different aspects of translation are important .We test the APEs on two different MT systems and across two different genres .","label":"Background","metadata":{},"score":"83.481384"}
{"text":"Hierarchical Phrase - Based Translation at University of Cambridge .Talk at Barcelona Media Innovation Centre , Barcelona , Catalonia ( Spain ) , July 2011 . A. de Gispert .Hierarchical Phrase - Based Translation at University of Cambridge .Talk at Catalonia Research Group on Accessibility and Ambient Intelligence ( CaiaC ) , Universitat Autònoma de Catalunya , Bellaterra , Catalonia ( Spain ) , July 2011 . A. de Gispert .","label":"Background","metadata":{},"score":"84.03593"}
{"text":"FIG .4 , for example , hard disk drive 441 is illustrated as storing operating system 444 , application programs 445 , other program modules 446 , and program data 447 .Note that these components can either be the same as or different from operating system 434 , application programs 435 , other program modules 436 , and program data 437 .","label":"Background","metadata":{},"score":"84.70683"}
{"text":"i .i .e .I . )c .i .i .e . )v .i .i .l .I .c .l .i .e . )l .I .v .","label":"Background","metadata":{},"score":"88.22724"}
{"text":"By way of example only , .The drives and their associated computer storage media discussed above and illustrated in .FIG .4 , provide storage of computer readable instructions , data structures , program modules and other data for the computer 410 .","label":"Background","metadata":{},"score":"90.35276"}
{"text":"RAM 432 typically contains data and/or program modules that are immediately accessible to and/or presently being operated on by processing unit 420 .By way of example , and not limitation , .FIG .4 illustrates operating system 434 , application programs 435 , other program modules 436 , and program data 437 .","label":"Background","metadata":{},"score":"91.034935"}
{"text":"July 2011 -- Paper to be presented at the Conference on Empirical Methods in Natural Language Processing ( EMNLP'11 ) -- joint work with Google Research .Hierarchical Phrase - based Translation Representations .Gonzalo Iglesias , William Byrne , Adrià de Gispert , Department of Engineering , University of Cambridge Cyril Allauzen , Michael Riley , Google Research .","label":"Background","metadata":{},"score":"93.48175"}
{"text":"FIG .4 , an exemplary system for implementing the invention includes a general - purpose computing device in the form of a computer 410 .Components of computer 410 may include , but are not limited to , a processing unit 420 , a system memory 430 , and a system bus 421 that couples various system components including the system memory to the processing unit 420 .","label":"Background","metadata":{},"score":"94.17516"}
{"text":"A user may enter commands and information into the computer 410 through input devices such as a keyboard 462 and pointing device 461 , commonly referred to as a mouse , trackball , or touch pad .Other input devices ( not shown ) may include a joystick , game pad , satellite dish , scanner , radio receiver , and a television or broadcast video receiver , or the like .","label":"Background","metadata":{},"score":"98.22635"}
{"text":"When used in a WAN networking environment , the computer 410 typically includes a modem 472 or other means for establishing communications over the WAN 473 , such as the Internet .The modem 472 , which may be internal or external , may be connected to the system bus 421 via the user input interface 460 , or other appropriate mechanism .","label":"Background","metadata":{},"score":"102.38515"}
{"text":"4 illustrates an example of a suitable computing system environment 400 on which the invention may be implemented .The computing system environment 400 is only one example of a suitable computing environment and is not intended to suggest any limitation as to the scope of use or functionality of the invention .","label":"Background","metadata":{},"score":"103.107445"}
{"text":"In addition to the monitor , computers may also include other peripheral output devices such as a printer 496 , which may be connected through an output peripheral interface 495 .The computer 410 may operate in a networked environment using logical connections to one or more remote computers , such as a remote computer 480 .","label":"Background","metadata":{},"score":"103.57209"}
{"text":"Computer 410 typically includes a variety of computer readable media .Computer readable media can be any available media that can be accessed by computer 410 and includes both volatile and nonvolatile media , removable and non - removable media .The system memory 430 includes computer storage media in the form of volatile and/or nonvolatile memory such as read only memory ( ROM ) 431 and random access memory ( RAM ) 432 .","label":"Background","metadata":{},"score":"112.87865"}
{"text":"EndNote citation : .","label":"Background","metadata":{},"score":"115.462845"}
