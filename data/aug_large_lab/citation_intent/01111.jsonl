{"text":"The concept of Maximum Entropy can be traced back along multiple threads to Biblical times .However , not until the late of 21st century has computer become powerful enough to handle complex problems with statistical modeling technique like Maxent .Maximum Entropy was first introduced to NLP area by Berger , et al ( 1996 ) and Della Pietra , et al .","label":"Uses","metadata":{},"score":"30.527523"}
{"text":"This link is to the Maximum Entropy Modeling Toolkit , for parameter estimation and prediction for maximum entropy models in discrete domains .The software comes with documentation , and was used as the basis of the 1996 Johns Hopkins workshop on language modelling .","label":"Uses","metadata":{},"score":"37.262257"}
{"text":"Four iterative parameter estimation algorithms are compared on several NLP tasks .L - BFGS is observed to be the most effective parameter estimation method for Maximum Entropy model , much better than IIS and GIS .( Wallach 02 ) reported similar results on parameter estimation of Conditional Random Fields .","label":"Uses","metadata":{},"score":"37.790077"}
{"text":"That is to say , when characterizing some unknown events with a statistical model , we should always choose the one that has Maximum Entropy .Maximum Entropy Modeling has been successfully applied to Computer Vision , Spatial Physics , Natural Language Processing and many other fields .","label":"Uses","metadata":{},"score":"38.409317"}
{"text":"The second question is how one can estimate NLP systems ' performance when gold standard on the test data does not exist .To answer the question , we extend the parsing prediction model in ( Ravi et al . , 2008 ) to provide prediction for word segmentation and POS tagging as well .","label":"Uses","metadata":{},"score":"39.245533"}
{"text":"The nonparametric model , such as kernel density method , moving block bootstrapping method , or K - nearest neighbor resampling method , does not make assumptions about the probability distribution or dependence forms and provides an alternative for stochastic simulation [ Vogel and Shallcross , 1996 ; Sharma et al . , 1997 ; Prairie et al . , 2007 ; Nowak et al . , 2010 ] .","label":"Uses","metadata":{},"score":"39.449814"}
{"text":"Inducing Features of Random Fields .Technical Report CMU - CS95 - 144 , School of Computer Science , Carnegie - Mellon University .Statistical Machine Translation .The work on maximum entropy and parsing led to the first purely statistically based translation system .","label":"Uses","metadata":{},"score":"39.52987"}
{"text":"Extensive results and benchmarks are provided , as well as a number of practical algorithms for modeling conditional data using maxent .A connection between conditional maxent models and Markov random fields -- a popular modeling technique in computer vision -- is drawn in the final section .","label":"Uses","metadata":{},"score":"41.30144"}
{"text":"However , the ability of the commonly used parametric copulas to model dependences in higher dimensions is rather restricted [ Kao and Govindaraju , 2008 ; Chui and Wu , 2009 ] .In this study , we propose the maximum entropy copula for multisite monthly streamflow simulation in which the rank correlation in higher dimensions among monthly streamflows at different sites can be modeled .","label":"Uses","metadata":{},"score":"42.357327"}
{"text":", 2009 ; Biber & Gray , 2010 ) , but the most interesting usages apply the divergence to a machine learning system .Despite the fact that authors have shown that a divergence ( Van Asch & Daelemans , 2010 ; Plank , 2011 ) or a linear combination of divergences ( McClosky , 2010 ) can be successfully used to link the sim ... . \" ...","label":"Uses","metadata":{},"score":"43.142326"}
{"text":"Tools . by Adam L. Berger , Stephen A. Della Pietra , Vincent J. Della Pietra - COMPUTATIONAL LINGUISTICS , 1996 . \" ...The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .","label":"Uses","metadata":{},"score":"43.207386"}
{"text":"These results show that the dependence structure of the monthly streamflow at each site and between different sites can be preserved relatively well .Figure 2 . Conclusions .[ 25 ] The maximum entropy copula method is proposed for the multisite monthly streamflow simulation and shown to be capable of modeling the rank correlation of monthly streamflows at different sites .","label":"Uses","metadata":{},"score":"43.836876"}
{"text":"A geometric interpretation of Darroch and Ratcli 's generalized iterative scaling ( 1989 ) .Tools . by Adam L. Berger , Stephen A. Della Pietra , Vincent J. Della Pietra - COMPUTATIONAL LINGUISTICS , 1996 . \" ...The concept of maximum entropy can be traced back along multiple threads to Biblical times .","label":"Uses","metadata":{},"score":"44.319687"}
{"text":"The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non - Markovian and have a large number of parameters that must be estimated .","label":"Uses","metadata":{},"score":"44.4371"}
{"text":"What is Maximum Entropy Modeling .In his famous 1957 paper , Ed .T. Jaynes wrote : Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge , and leads to a type of statistical inference which is called the maximum entropy estimate .","label":"Uses","metadata":{},"score":"44.451332"}
{"text":"Instead , we apply the principle of Maximum Entropy ( ME ) .Each information source gives rise to a set of constraints , to be imposed on the combined estimate .The intersection of these constraints is the set of probability functions which are consistent with all the information sources .","label":"Uses","metadata":{},"score":"44.483624"}
{"text":"[5 ] Jelinek , F. , Mercer , R.L. , 1980 .Interpolated estimation of Markov source parameters from sparse data .In : Gelsema , E.S. , Kanal , L.N. ( Eds . ) , Pattern Recognition in Practice .","label":"Uses","metadata":{},"score":"44.718853"}
{"text":"We present a maximum - likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently , using as examples several problems in natural language processing . \" ... this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .","label":"Uses","metadata":{},"score":"46.395817"}
{"text":"v .c . u .v . )d .u . d .v .[ 9 ] With the moment constraints up to order m and pairwise product constraint in equation ( 4 ) , the maximum entropy copula density function can be obtained as [ Chui and Wu , 2009 ; Chu , 2011 ] .","label":"Uses","metadata":{},"score":"46.622726"}
{"text":"Hidden Markov Models and related formalisms : the Viterbi and Baum - Welch algorithms , integration with semantic hierarchies , language modeling , smoothing , and Maximum Entropy Markov Models .The noisy channel model ( basics of information theory ) : entropy , the Kullback - Leibler divergence , and mutual information .","label":"Uses","metadata":{},"score":"46.853447"}
{"text":"Deterministic annealing is used to find lowest distortion sets of clusters : as the an-nealing parameter increases , existing clusters become unstable and subdivide , yielding a hierarchi- cal \" soft \" clustering of the data .Clusters are used as the basis for class models of word coocurrence , and the models evaluated with respect to held - out test data . .","label":"Uses","metadata":{},"score":"47.85184"}
{"text":"Whole Sentence Language Model ) with sampling based training .Now seems to be part of scipy .Stanford Classifer is another open source implementation of Maximum Entropy Model in java , suitable for NLP tagging and parsing tasks .NLTK includes a maxent classifier written entirely in Python .","label":"Uses","metadata":{},"score":"47.867615"}
{"text":"Adam Berger , Stephen Della Pietra , and Vincent Della Pietra Computational Linguistics , ( 22 - 1 ) , March 1996 ; .The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .","label":"Uses","metadata":{},"score":"48.723923"}
{"text":"Since then , Maximum Entropy technique ( and the more general framework Random Fields ) has enjoyed intensive research in NLP community .YASMET --Yet Another Simple Maximum Entropy Toolkit with Feature Selection .YASMET(2 ) --Yet Another Small MaxEnt Toolkit .","label":"Uses","metadata":{},"score":"48.740612"}
{"text":"Within the broad class of exponential models exists a family of distributions , maximum entropy models , with some interesting mathematical and philosophical properties .Though the concept of maximum entropy can be traced back along multiple threads to Biblical times , only recently have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .","label":"Uses","metadata":{},"score":"49.04117"}
{"text":"In these results , the generative model performs significantly better than the others , and does about equally well at assigning part - of - speech tags . \" ...Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .","label":"Uses","metadata":{},"score":"49.17414"}
{"text":"The singular value decomposition ( SVD ) and latent semantic indexing ( LSI ) .Julian Kupiec , 1992 .Robust Part - of - Speech Tagging Using a Hidden Markov Model .Computer Speech and Language 6 , pp .225 - 242 .","label":"Uses","metadata":{},"score":"49.92513"}
{"text":"Discriminative training of acoustic and language models .The discriminative training craze for acoustic modeling started with Maximum Mutual Information ( MMI ) in Peter Brown 's thesis .It was then continued at IBM and followed up with a paper on the extended Baum Welch algorithm that gives a recipe to optimize rational functions satisfying certain constraints .","label":"Uses","metadata":{},"score":"50.36455"}
{"text":"The point of this document is twofold : first , to motivate the improved iterative scaling algorithm for conditional models , and second , to do so in a way that is minimizes the mathematical burden on the reader .This tutorial explains how to build maximum entropy models for natural language applications such as information retrieval and speech recognition .","label":"Uses","metadata":{},"score":"50.709293"}
{"text":"Statistical NLP methods determine the likelihood of a word combination from its frequency in a training corpus .However , the nature of language is such that many word combinations are infrequent and do not occur in any given corpus .In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on \" most similar \" words .","label":"Uses","metadata":{},"score":"50.88492"}
{"text":"We present a maximum - likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently , using as examples several problems in natural language processing .Tools . by Jay J. Jiang , David W. Conrath - Proc of 10th International Conference on Research in Computational Linguistics , ROCLING'97 , 1997 . \" ...","label":"Uses","metadata":{},"score":"51.13503"}
{"text":"As a demonstration of the method , we describe its application to the problem of automatic word classification in natural language processing .An adaptive statistical language model is described , which successfullyintegrates long distance linguistic information with other knowledge sources .","label":"Uses","metadata":{},"score":"51.619343"}
{"text":"Although the experiments in this article are on natural language parsing ( NLP ) , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or natural language generation . .","label":"Uses","metadata":{},"score":"51.707275"}
{"text":"IBM was first to introduce hidden Markov models ( HMMs ) to the world of speech recognition .Although Rabiner 's tutorial 's on HMMs are more widely cited the IBM papers were first .The first paper is on the forward - backward algorithm : .","label":"Uses","metadata":{},"score":"51.72454"}
{"text":"Later , Rosenfeld and his group proposed a Whole Sentence Exponential Model that overcome the computation bottleneck of conditional ME model .You can find more on my SLM page .This dissertation discusses the application of maxent model to various Natural Language Dis - ambiguity tasks in detail .","label":"Uses","metadata":{},"score":"52.063217"}
{"text":"The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .","label":"Uses","metadata":{},"score":"52.299496"}
{"text":"Parametric disaggregation method is another type of parametric methods for multisite streamflow simulation that generally consists of two steps .It first generates aggregated streamflow ( e.g. , annual flow ) and then disaggregates or divides it into lower - level variables ( e.g. , monthly flow ) [ Valencia and Schaake , 1973 ; Stedinger and Vogel , 1984 ; Koutsoyiannis and Manetas , 1996 ] .","label":"Uses","metadata":{},"score":"52.300095"}
{"text":"i . g .i ( 7 ) .[ 13 ] These parameters can be estimated using the Newton Raphson iteration method [ Wu , 2003 ; Hao and Singh , 2011 ] .However , a high - dimensional integration is involved in the parameter estimation for the multisite simulation to obtain the value of λ 0 in equation ( 6 ) , which makes it even more complicated than the single - site streamflow simulation . , 1991 ] .","label":"Uses","metadata":{},"score":"52.594536"}
{"text":"proposed an ensemble method ( Reichart and Rappoport , 2007 ) .They regarded parses as being of high quality if 20 different parsers agreed .They used an SVM regression approach on the basis of text - based and parse - based features .","label":"Uses","metadata":{},"score":"52.918335"}
{"text":"In this paper we des ... \" .The concept of maximum entropy can be traced back along multiple threads to Biblical times .Only recently , however , have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition .","label":"Uses","metadata":{},"score":"53.16188"}
{"text":".. our corpus .The second question is how one can estimate NLP systems ' performance when gold standard on the test data does not exist .Our experiments show that the predicted scores are close to the real scores when tested on the CTB data .","label":"Uses","metadata":{},"score":"53.84237"}
{"text":"Suitable marginal distributions , such as kernel density , can be selected to model the properties of streamflow of each month , such as skewness and bimodal properties , which have been well documented [ Sharma et al . , 1997 ; Prairie et al . , 2007 ; Salas and Lee , 2010 ; Hao and Singh , 2012 ] .","label":"Uses","metadata":{},"score":"53.929123"}
{"text":"Current statistical parsers tend to perform well only on their training domain and nearby genres .While strong performance on a few related domains is sufficient for many situations , it is advantageous for parsers to be able to generalize to a wide variety of domains .","label":"Uses","metadata":{},"score":"54.533447"}
{"text":"Current statistical parsers tend to perform well only on their training domain and nearby genres .While strong performance on a few related domains is sufficient for many situations , it is advantageous for parsers to be able to generalize to a wide variety of domains .","label":"Uses","metadata":{},"score":"54.533447"}
{"text":"These works aim to predict the parser performance on a given target sentence .Ravi et al .( 2008 ) frame this as a regression problem .Kawahara and Uchimoto ( 2008 ) treat ... . \" ...Genre classification has been found to improve performance in many applications of statistical NLP , including language modeling for spoken language , domain adaptation of statistical parsers , and machine translation .","label":"Uses","metadata":{},"score":"54.82525"}
{"text":"We describe and evaluate experimentally a method for clustering words according to their dis- tribution in particular syntactic contexts .Words are represented by the relative frequency distributions of contexts in which they appear , and relative entropy between those distributions is used as the si ... \" .","label":"Uses","metadata":{},"score":"55.53274"}
{"text":"Specifically , the proposed measure is a combined approach that inherits the edge - based approach of the edge counting scheme , which is then enhanced by the node - based approach of the information content calculation .When tested on a common data set of word pair similarity ratings , the proposed approach outperforms other computational models .","label":"Uses","metadata":{},"score":"55.710236"}
{"text":"Some of the large margin classifier work was inspired by Povey 's work .Below are some of the older papers as well as selected papers from Povey 's IBM work .[ 12 ] Nadas , A. : A decision - theoretic formulation of a training problem in speech recognition and a comparison of training by unconditional versus conditional maximum likelihood .","label":"Uses","metadata":{},"score":"55.839767"}
{"text":"In this paper , we address two issues that are related to domain adaptation .The first question is how much genre variation will affect NLP systems ' performance .We investigate the effect of genre variation on the performance of three NLP tools , namely , word segmenter , POS tagger , and parser .","label":"Uses","metadata":{},"score":"56.545708"}
{"text":"Despite the persistence of this theory , however , there is widespread agreement about its empirical shortcomings ( McCawley , 1968 ; Fodor , 1977 ) .As an alternative , some critics of the Katz - Fodor theory ( e.g. ( Johnson - Laird , 1983 ) ) have abandoned the treatment of selectional constraints as semantic , instead treating them as indistinguishable from inferences made on the basis of factual knowledge .","label":"Uses","metadata":{},"score":"56.74614"}
{"text":"It is well known that parsing accuracy suffers when a model is applied to out - of - domain data .It is also known that the most beneficial data to parse a given domain is data that matches the domain ( Sekine , 1997 ; Gildea , 2001 ) .","label":"Uses","metadata":{},"score":"56.81183"}
{"text":"Notes .In ' 'A maximum entropy approach to natural language processing ' ' ( Computational Linguistics 22:1 , March 1996 ) , the appendix describes an approach to computing the gain of a single feature f .This note elaborates on the equations presented there ; in particular , we show how to derive equations ( 37 ) and ( 38 ) .","label":"Uses","metadata":{},"score":"56.87732"}
{"text":"[16 ] Povey , D. , Kanevsky , D. , Kingsbury , B. , Ramabhadran , B. , Saon , G. , Visweswariah , K.:Boosted MMI for model and feature - space discriminative training .In : Proceedings of the IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP-08 ) , Las Vegas , NV ( 2008 ) .","label":"Uses","metadata":{},"score":"56.946106"}
{"text":"We first describe an algorithm for converting the hierarchical structure of WordNet [ 13 ] ... \" .We discuss a method for augmenting and rearranging a structured lexicon in order to make it more suitable for a topic labeling task , by making use of lexical association information from a large text corpus .","label":"Uses","metadata":{},"score":"56.9582"}
{"text":"However , most ... \" .It is well known that parsing accuracy suffers when a model is applied to out - of - domain data .It is also known that the most beneficial data to parse a given domain is data that matches the domain ( Sekine , 1997 ; Gildea , 2001 ) .","label":"Uses","metadata":{},"score":"57.22725"}
{"text":"We present a maximum - likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently , using as examples several problems in natural language processing .Stephen Della Pietra , Vincent Della Pietra , and John Lafferty IEEE Transactions on Pattern Analysis and Machine Intelligence 19:4 , pp.380 - -393 , April , 1997 .","label":"Uses","metadata":{},"score":"57.67831"}
{"text":"Another must read paper on maxent .It deals with a more general frame work : Random Fields and proposes an Improved Iterative Scaling algorithm for estimating parameters of Random Fields .This paper gives theoretical background to Random Fields ( and hence Maxent model ) .","label":"Uses","metadata":{},"score":"57.787254"}
{"text":"Given consistent statistical evidence , a unique ME solution is guaranteed to exist , and an iterative algorithm exists which is guaranteed to converge to it .The ME framework is extremely general : any phenomenon that can be described in terms of statistics of the text can be readily incorporated .","label":"Uses","metadata":{},"score":"58.267487"}
{"text":"We take two popular dependency parsers - one graph - based and one transition - based - and compare results for both .Results show that using semisupervised learning in the form of self - training and co - training yields only very modest improvements in parsing accuracy .","label":"Uses","metadata":{},"score":"58.341167"}
{"text":"This article considers approaches which rerank the output of an existing probabilistic parser .The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this i ... \" .","label":"Uses","metadata":{},"score":"58.365494"}
{"text":"Such lexicon compilation requires highly reliable predicate - argument structures to practically contribute to Natural Language Processing ( NLP ) applications , such as paraphrasing , text entailment , and machine translation .We first apply chunking to raw corpora and then extract reliable chunks to ensure that high - quality predicate - argument structures are obtained from the chunks .","label":"Uses","metadata":{},"score":"58.512436"}
{"text":"In this dissertation , I suggest that an answer to this question lies in the representation of conceptual . \" ... Introduction An impressive array of statistical methods have been developed for word sense identification .They range from dictionary - based approaches that rely on definitions ( Vronis and Ide 1990 ; Wilks et al .","label":"Uses","metadata":{},"score":"58.797512"}
{"text":"The learning paradigm builds increasingly complex fields by allowing potential functions , or features , that are supported by increasingly large subgraphs .Each feature has a weight that is trained by minimizing the Kullback - Leibler divergence between the model and the empirical distribution of the training data .","label":"Uses","metadata":{},"score":"58.839333"}
{"text":"381 - 397 .[ 6 ] [ Berger et al . , 1996 ] Adam Berger , Stephen A. Della Pietra , and Vincent J. Della Pietra.1996 .A Maximum Entropy Approach to Natural Language Processing .Computational Linguistics , 22 ( 1):39 - 71 .","label":"Uses","metadata":{},"score":"58.983295"}
{"text":"In the language modeling task , a similarity - based model is used to improve probability estimates for unseen bigrams in a back - off language model .The similaritybased method yields a 20 % perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech - recognition error .","label":"Uses","metadata":{},"score":"59.11911"}
{"text":"From the sample , which constitutes an incomplete state of knowledge about the process , the modeling problem is to parlay this knowledge into a succinct , accurate representation of the process .We can then use this representation to make predictions of the future behavior of the process .","label":"Uses","metadata":{},"score":"59.414974"}
{"text":"MaxEnt and Exponential Models .This page contains pedagogically - oriented material on maximum entropy and exponential models .The emphasis is towards modelling of discrete - valued stochastic processes which arise in human language applications , such as language modelling .","label":"Uses","metadata":{},"score":"59.63115"}
{"text":"We propose ( a ) a lexical affinity model where words struggle to modify each other , ( b ) a sense tagging model where words fluctuate randomly in their selectional prefe ... \" .After presenting a novel O(n³ ) parsing algorithm for dependency grammar , we develop three contrasting ways to stochasticize it .","label":"Uses","metadata":{},"score":"59.65924"}
{"text":"The similaritybased methods perform up to 40 % better on this particular task . ...Sections 2.3.1 and 2.3.2 discuss two related information - theoretic functions , the KL divergence and the Jensen - Shannon divergence .Section 2.3.3 describes the L 1 norm , ... . by Ido Dagan , Lillian Lee , Fernando Pereira - In Proceedings of the Association for Computational Linguistics , 1997 . \" ...","label":"Uses","metadata":{},"score":"59.721397"}
{"text":"H . a .b . a .b .f .x .y . ) ln .f .x .y . )d .x .d .y ( 1 ) .Maximum Entropy Copula .[ 7 ] The maximum entropy copula has been developed based on the entropy theory [ Chui and Wu , 2009 ; Chu , 2011 ] .","label":"Uses","metadata":{},"score":"60.0018"}
{"text":"It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantifie ... \" .This paper presents a new approach for measuring semantic similarity / distance between words and concepts .","label":"Uses","metadata":{},"score":"60.408707"}
{"text":"Each year the Conference on Computational Natural Language Learning ( CoNLL ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems .The tenth CoNLL ( CoNLL - X ) saw a shared task on Multilingual Dependency Parsing .","label":"Uses","metadata":{},"score":"60.62533"}
{"text":"Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer .IEEE Transactions on Acoustics , Speech and Signal Processing , ASSP-35 ( 3 ) , pp 400 - 401 .Reinhard Kneser and Hermann Ney , 1995 .","label":"Uses","metadata":{},"score":"60.68158"}
{"text":"We present a method for acquiring reliable predicate - argument structures from raw corpora for automatic compilation of case frames .Such lexicon compilation requires highly reliable predicate - argument structures to practically contribute to Natural Language Processing ( NLP ) applications , such as par ... \" .","label":"Uses","metadata":{},"score":"60.821537"}
{"text":"We present a detailed case study of this learni ... \" .this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance .","label":"Uses","metadata":{},"score":"60.957962"}
{"text":"Parameter Estimation . , 2 m+ 1 ) in equation ( 5 ) have to be estimated .It has been shown that these Lagrange multipliers can be solved by finding the minimum of a convex function Γ expressed as [ Kapur , 1989 ] .","label":"Uses","metadata":{},"score":"61.325912"}
{"text":"The monthly streamflows at different sites are then generated by sampling from the conditional distribution .A case study for the generation of monthly streamflow at three sites in the Colorado River basin illustrates the application of the proposed method .Simulated streamflow from the maximum entropy copula is in satisfactory agreement with observed streamflow .","label":"Uses","metadata":{},"score":"61.579117"}
{"text":"For a copula density function c ( u , v ) , the entropy can be expressed as .W .c . u .v . ) log .c . u .v . )d .u . d .","label":"Uses","metadata":{},"score":"61.72171"}
{"text":"The focus in this tutorial is on the foundation common to the two algorithms : convex functions and their convenient properties .Where examples are called for , we draw from applications in human language technology .This note concerns the improved iterative scaling algorithm for computing maximum - likelihood estimates of the parameters of exponential models .","label":"Uses","metadata":{},"score":"61.72896"}
{"text":"Res ., 44 , W02415 , doi : 10.1029/2007WR006261 .Sharma , A. , and R. O'Neill ( 2002 ) , A nonparametric approach for representing interannual dependence in monthly streamflow sequences , Water Resour .Res . , 38 ( 7 ) , 1100 , doi : 10.1029/2001WR000953 .","label":"Uses","metadata":{},"score":"61.848892"}
{"text":"We study this problem as a new task - multiple source parser adaptation .Our system trains on corpora from many different domains .It learns not only statistics of those domains but quantitative measures of domain differences and how those differences affect parsing accuracy .","label":"Uses","metadata":{},"score":"62.276344"}
{"text":"The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data .Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach .","label":"Uses","metadata":{},"score":"62.422012"}
{"text":"Given a multidimensional space upon which a node represents a 2unique concept consisting of a certain amount of information , and an edge represents a direct association between two concepts , ... .by Fernando Pereira , Naftali Tishby , Lillian Lee - In Proceedings of the 31st","label":"Uses","metadata":{},"score":"62.46922"}
{"text":".. rning deserves further study .There are many different ways one could try to construct a language learner .In [ 65 ] , a selforganizing language learner is proposed to be used for language modelling .In this work we take a different approach , namely starting with a s ..","label":"Uses","metadata":{},"score":"62.700325"}
{"text":"When interfaced to SPHINX - II , Carnegie Mellon 's speech recognizer , it reduced its error rate by 10%--14 % .This thus illustrates the feasibility of incorporating many diverse knowledge sources in a single , unified statistical framework .Statistical modeling addresses the problem of modeling the behavior of a random process .","label":"Uses","metadata":{},"score":"62.741783"}
{"text":"Well , it 's time to have a look at this one .Edwin Thompson Jaynes presented some insightful results of maximum entropy principle in this 1957 paper published in Physics Reviews .This is also his first paper in information theory .","label":"Uses","metadata":{},"score":"62.74361"}
{"text":"14 ] Gopalakrishnan , P.S. Kanevsky , D. Nadas , A. Nahamoo , D. An inequality for rational functions with applications to some statistical estimation problems , IEEE Trans .Inform .Theo . 37 ( 1 ) , pp .107 - 113 , 1991 .","label":"Uses","metadata":{},"score":"62.77492"}
{"text":"[ .r . m .r .u .r .r . m .v .r . ) m .u .v . ) . ]d .u . d .v ( 6 ) .[ 10 ] The dependence structure in terms of the Spearman rank correlation can be modeled through the joint probability density function in equation ( 5 ) .","label":"Uses","metadata":{},"score":"62.784218"}
{"text":"At its ... \" .Genre classification has been found to improve performance in many applications of statistical NLP , including language modeling for spoken language , domain adaptation of statistical parsers , and machine translation .It has also been found to benefit retrieval of spoken or written docu - ments .","label":"Uses","metadata":{},"score":"62.78781"}
{"text":"WSJ data ( Petrov and Klein , 2007 ; Foster , 2010 ) .The parser uses the English signature list described in Attia et al ( 2010 ) to assign partof - speech tags to unknown words . \" ...Domain adaptation is an important task in order for NLP systems to work well in real applications .","label":"Uses","metadata":{},"score":"63.00663"}
{"text":"[ 4 ] Bahl , L.R. , Brown , P.F. , deSouza , P.V. , Mercer , R.L. , Nahamoo , D. , 1991 .A fast algorithm for deleted interpolation .In : Proc .Europ .Conf .Speech Comm .","label":"Uses","metadata":{},"score":"63.125713"}
{"text":"Dan Povey did this work for his dissertation before coming to IBM .At IBM he came up with new discriminative features ( feature space MPE ( fMPE ) ) as well as an altogether better criteria : boosted MMI ( bMMI ) .","label":"Uses","metadata":{},"score":"63.144714"}
{"text":"An word morphology application for English was developed . longer version .This paper applies ME technique to statistical language modeling task .More specifically , it builds a conditional Maximum Entropy model that incorporates traditional N - gram , distant N - gram and trigger pair features .","label":"Uses","metadata":{},"score":"63.415108"}
{"text":"This is a high - level tutorial on how to use MaxEnt for modelling discrete stochastic processes .The motivating example is the task of determining the most appropriate translation of a French word in context .The tutorial discusses the process of growing an exponential model by automatic feature selection ( \" inductive learning , \" if you will ) and also the task of estimating maximum - likelihood parameters for a model containing a fixed set of features .","label":"Uses","metadata":{},"score":"63.479195"}
{"text":"To extract information from further back in the document 's history , we propose and use trigger pairs as the basic information bearing elements .This allows the model to adapt its expectations to the topic of discourse .Next , statistical evidence from multiple sources must be combined .","label":"Uses","metadata":{},"score":"63.850708"}
{"text":"We also give an overview of the parsing approaches that participants took and the results that they achieved .Finally , we try to draw general conclusions about multi - lingual parsing : What makes a particular language , treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser ?","label":"Uses","metadata":{},"score":"64.7436"}
{"text":"[ 13 ] L.R. Bahl , P.F. Brown , P.V. de Souza , R.L. Mercer ( 1986 ) .Maximum Mutual Information Estimation of Hidden Markov Model Parameters for Speech Recognition , Proc .ICASSP 86 , pp .49 - 52 , Tokyo .","label":"Uses","metadata":{},"score":"64.74448"}
{"text":"No additional knowledge about the target domain is included .A more realistic approach assumes that only raw text from the target domain is available .This assumption lends itself well to semi - supervised learning methods since these utilize both labeled and unlabeled examples .","label":"Uses","metadata":{},"score":"65.17064"}
{"text":"We introduce a new method for the reranking task , based on the boosting approach to ranking problems described in Freund et al .( 1998 ) .We apply the boosting method to parsing the Wall Street Journal treebank .The method combined the log - likelihood under a baseline model ( that of Collins [ 1999 ] ) with evidence from an additional 500,000 features over parse trees that were not included in the original model .","label":"Uses","metadata":{},"score":"65.174774"}
{"text":"[ 11 ] The joint distribution in the higher dimension is of particular interest when the multivariate dependence structure has to be modeled .In this case , a multivariate entropy in equation ( 2 ) can be defined and then copula density function with the maximum entropy can be derived straightforward .","label":"Uses","metadata":{},"score":"65.282776"}
{"text":"532 - -556 .Publication Date : April 1976 .Pioneering Innovations in Language Modeling .The first successful language modeling smoothing algorithm , deleted interpolation , was invented at IBM .IBM also had the first application of the Maximum Entropy Principle to language modeling .","label":"Uses","metadata":{},"score":"65.43275"}
{"text":"Inform Theory , vol IT-20 , pp .248 - 287 , March 1974 .Here are two papers on the maximum likelihood approach to speech recognition : .[ 2 ] Maximum likelihood approach to continuous speech recognition .LR Bahl , F Jelinek , RL Mercer , IEEE Transactions on Pattern Analysis and Machine Intelligence 5:22 , 179 - 190 , 1983 .","label":"Uses","metadata":{},"score":"65.5149"}
{"text":"Alternatively , view our Knowledge Base articles for additional help .Your feedback is important to us , so please let us know if you have comments or ideas for improvement .Introduction .[ 2 ] For multisite streamflow simulation in a river basin , it is desired that statistical properties of streamflow at individual sites and dependence structure among different sites are preserved .","label":"Uses","metadata":{},"score":"65.658585"}
{"text":"[ 8 ] P. Brown , J. Cocke , S. Della Pietra , V. Della Pietra , F. Jelinek , R. Mercer , P. Roossin , A statistical approach to language translation , Proceedings of the 12th conference on Computational linguistics , p.71 - 76 , August 22 - 27 , 1988 .","label":"Uses","metadata":{},"score":"65.82294"}
{"text":"Suitable for text categorization and related NLP tasks .Here is another small maxent package in C++ with a BSD - like license , written by Dekang Lin .A must read paper on applying maxent technique to Natural Language Processing .","label":"Uses","metadata":{},"score":"66.290855"}
{"text":"The first submission , the highest ranked constituency parsing system , uses a combination of PCFG - LA product grammar parsing and self - training .In the second submission , also a constituency parsing ... \" .The DCU - Paris13 team submitted three systems to the SANCL 2012 shared task on parsing English web text .","label":"Uses","metadata":{},"score":"66.79317"}
{"text":"The Penn Treebank has recently implemented a new syntactic annotation scheme , designed to highlight aspects of predicate - argument structure .This paper discusses the implementation of crucial aspects of this new annotation scheme .It incorporates a more consistent treatment of a wide range of gramma ... \" .","label":"Uses","metadata":{},"score":"66.82282"}
{"text":"We also conclude that events that occur only once in the training set have major impact on similarity - based estimates . by Marti A. Hearst , Hinrich Schütze - Proc . of the Workshop on Extracting Lexical Knowledge , 1996 . \" ...","label":"Uses","metadata":{},"score":"67.13785"}
{"text":"We then use lexical cooccurrence statistics in combination with these categories to classify proper names , assign more specific senses to broadly defined terms , and classify new words into existing categories .We also describe how to use these statistics to assign schema - like information to the categories and show how the new categories improve a text - labeling algorithm .","label":"Uses","metadata":{},"score":"67.332954"}
{"text":"However , most previous work on domain adaptation relied on the implicit assumption that domains are somehow given .As more and more data becomes available , automatic ways to select data that is beneficial for a new ( unknown ) target domain are becoming attractive .","label":"Uses","metadata":{},"score":"67.44867"}
{"text":"They are important for a few reasons .First , at present the best performing parsers on the WSJ treebank ( Ratnaparkhi 1997 ; Charniak 1997 , 1999 ; Collins 1997 , 1999 ) are all cases of history - based mo .. \" ...","label":"Uses","metadata":{},"score":"67.99155"}
{"text":"The proposed methodology can also be applied to similar topics , such as rainfall simulation and geostatistical interpolation .The potential drawbacks would be that the marginal properties of the copula are approximated numerically and the sum of tributary flows adding up to the downstream flow can not be ensured with the current framework .","label":"Uses","metadata":{},"score":"68.01903"}
{"text":"This system outperforms previou ... \" .We describe a parsing system based upon a language model for English that is , in turn , based upon assigning probabilities to possible parses for a sentence .This model is used in a parsing system by finding the parse for the sentence with the highest probability .","label":"Uses","metadata":{},"score":"68.17619"}
{"text":"Self - training creates semi - supervised learners from existing supervised learners with minimal effort .We first show results on self - training for constituency parsing within a single domain .While self - training has failed here in the past , we present a simple modification which allows it to succeed , producing state - of - the - art results for English constituency parsing .","label":"Uses","metadata":{},"score":"68.41905"}
{"text":"Comparison with other machine learning technique ( Naive Bayes , Transform Based Learning , Decision Tree etc . ) was given .Ratnaparkhi also had a short introduction paper on ME .Abney applies Improved Iterative Scaling algorithm to parameters estimation of Attribute - Value grammars , which can not be corrected calculated by ERF method ( though it works on PCFG ) .","label":"Uses","metadata":{},"score":"68.62969"}
{"text":"Introduction An impressive array of statistical methods have been developed for word sense identification .They range from dictionary - based approaches that rely on definitions ( Vronis and Ide 1990 ; Wilks et al .1993 ) to corpus - based approaches that use only word cooccurrence frequencies extracted from large textual corpora ( Schfitze 1995 ; Dagan and Itai 1994 ) .","label":"Uses","metadata":{},"score":"68.66867"}
{"text":"You need GCC 2.9x to compile the source .link2 .MEGA Model Optimization Package .A recently appeared ME implementation by Hal Daumé III .The software features CG and LM - BFGS Optimization and is written in OCaml .Although I no longer use OCaml , I 'd say that 's a great language , and is worth learning .","label":"Uses","metadata":{},"score":"69.214676"}
{"text":"The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial ranking of these parses .A second model then attempts to improve upon this initial ranking , using additional features of the tree as evidence .","label":"Uses","metadata":{},"score":"69.318665"}
{"text":"[ 11 ] A. Berger , P. Brown , S. Della Pietra , V. Della Pietra , J. Gillett , J. Lafferty , H. Printz , L. Ures ( 1994 ) .The Candide system for machine translation .ARPA Workshop on Speech and Natural Language .","label":"Uses","metadata":{},"score":"69.33182"}
{"text":"Apologies for the inconvenience .Space Sciences and Space Physics .Abstract .[ 1 ] Synthetic streamflows at different sites in a river basin are needed for planning , operation , and management of water resources projects .Modeling the temporal and spatial dependence structure of monthly streamflow at different sites is generally required .","label":"Uses","metadata":{},"score":"69.42772"}
{"text":"Words are represented by the relative frequency distributions of contexts in which they appear , and relative entropy between those distributions is used as the similarity measure for clustering .Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership .","label":"Uses","metadata":{},"score":"69.99701"}
{"text":"McClosky et al .( 2010 ) coined the term multiple source domain adaptation .Similar to us , McClosky et al .( 2010 ) regard a target domain as mixture of source domains , b .. by Joseph Le Roux , Jennifer Foster , Joachim Wagner , Rasul Samad , Zadeh Kaljahi , Anton Bryl . \" ...","label":"Uses","metadata":{},"score":"70.23283"}
{"text":"[ 8 ] The constraints can be expressed as .c . u .v . ) g .i . u .v . )d .u . d .v .g .i .i .n ( 3 ) .","label":"Uses","metadata":{},"score":"70.564995"}
{"text":"Web - scale experiments show that the DMV , perhaps because it is unlexicalized , does not benefit from orders of magnitude more annotated but noisier data .Our model , trained on a single blog , generalizes to 53.3 % accuracy out - of - domain , against the Brown corpus - nearly 10 % higher than the previous published best .","label":"Uses","metadata":{},"score":"70.743484"}
{"text":"Scatterplots of the observed ( closed circle ) and simulated ( plus symbol ) monthly streamflow ( marginal ) for March and April at different sites .[ 24 ] Boxplots were used to display the observed and simulated statistics , and the performance was judged to be good when a statistic fell within the boxplot [ Nowak et al . , 2010 ; Salas and Lee , 2010 ] .","label":"Uses","metadata":{},"score":"70.80147"}
{"text":"Methodology .Entropy Concepts .[5 ] Let the joint probability density function ( PDF ) of two random variable X and Y on the interval [ a 1 , b 1 ] .× [ a 2 , b 2 ] be f ( x , y ) .","label":"Uses","metadata":{},"score":"71.12041"}
{"text":"In the second submission , also a constituency parsing system , the n - best lists of various parsing models are combined using an approximate sentence - level product model .The third system , the highest ranked system in the dependency parsing track , uses voting over dependency arcs to combine the output of three constituency parsing systems which have been converted to dependency trees .","label":"Uses","metadata":{},"score":"71.50527"}
{"text":"Furthermore , some of these approaches can be employed in other applications , such as computational biology and data mining .This course will explore important classes of probabilistic models of language and survey some of the common general techniques .Syllabus .","label":"Uses","metadata":{},"score":"71.6711"}
{"text":"Proceedings of the IEEE International Conference on Acoustics , Speech , and Signal Processing , 181 - 184 .Arthur Nadas , 1985 .On Turing 's formula for word probabilities .IEEE Transactions on Acoustics , Speech , and Signal Processing , Vol .","label":"Uses","metadata":{},"score":"71.919266"}
{"text":"In this paper , we address two issues that are related to domain adaptation .The first question is how much genre variation will affect NLP systems ' per ... \" .Domain adaptation is an important task in order for NLP systems to work well in real applications .","label":"Uses","metadata":{},"score":"72.41238"}
{"text":"The two traditions complement each other .Corpus - based approaches have the advantage of being generally applicable to new texts , domains , and corpora without needing costly and perhaps error - prone parsing or semantic analysis .They require only training corpora in which the sense distinctions have been marked , but therein lies their weakness .","label":"Uses","metadata":{},"score":"72.65894"}
{"text":"n tendencies into associations of words to certain hidden senses classes and associations between the classes themselves .More specifically , we model senses as probabilistic concepts or clusters c with corresponding clus ... . \" ...Selectional constraints are limitations on the applicability of predicates to arguments .","label":"Uses","metadata":{},"score":"72.671135"}
{"text":"From Figure 2 , it can be seen that for most months , the median of simulated statistics is within the boxplot .Box plots of the spatial dependence of the observed and simulated monthly streamflow of the same month between different sites are shown in Figure 2 ( right column ) .","label":"Uses","metadata":{},"score":"73.5863"}
{"text":"Eraall : brill@cs.jhu.edu .Word sense disambiguation , a problem which once seemed out of reach for systems without a great deal of hand cr ... . \" ...We describe a parsing system based upon a language model for English that is , in turn , based upon assigning probabilities to possible parses for a sentence .","label":"Uses","metadata":{},"score":"73.60667"}
{"text":"The similarity - based methods perform up to 40 % better on this particular task .We al ... \" .We compare four similarity - based estimation methods against back - off and maximum - likelihood estimation methods on a pseudo - word sense disambiguation task in which we controlled for both unigram and bigram frequency .","label":"Uses","metadata":{},"score":"73.8355"}
{"text":"This paper revisits an assump - tion that genre variation is continuous along multiple dimensions , and an early use of principal component analysis to find these dimensions .Results on a very heterogeneous corpus of post-1990s American English reveal four major dimensions , three of which echo those found in prior work and the fourth depending on features not used in the earlier study .","label":"Uses","metadata":{},"score":"74.01265"}
{"text":"Maxent models have been applied with success in astrophysics and medicine , among other fields .Group Name .Tab navigation .Here are some of the top contributions to speech recognition from IBM .The papers listed have been cited more than 10,000 citations times combined .","label":"Uses","metadata":{},"score":"74.56864"}
{"text":"The automatic disambiguation of word senses has been an interest and concern since the earliest days of computer treatment of language in the 1950 's .Sense disambiguation is an \" intermediate task \" ( Wilks and Stevenson , 1996 ) which is not an end in itself , but rather is necessary at one level o ... \" .","label":"Uses","metadata":{},"score":"75.134254"}
{"text":"These methods require labeled examples of syntactic structures to learn statistical patterns governing these structures .Labeled data typically requires expert annotators which makes it both time consuming and costly to produce .Furthermo ... \" .Current efforts in syntactic parsing are largely data - driven .","label":"Uses","metadata":{},"score":"75.187454"}
{"text":"Claude Elwood Shannon 's influential 1948 paper that laid the foundation of information theory and changed the whole world since then .I see no reason who has read the above papers does not want to read this one .Information Theory and Statistical Mechanics ( Jaynes , E. T. , 1957 )","label":"Uses","metadata":{},"score":"75.35675"}
{"text":"s .u . s .v .s .v .s .v .s . )[ 23 ] One hundred sequences of monthly streamflow ( marginal ) with the same length as the historical record ( 98 years ) were generated for each site with the simulation methodology .","label":"Uses","metadata":{},"score":"75.52408"}
{"text":"Introduction We present a statistical parser that induces its grammar and probabilities from a hand - parsed corpus ( a tree - bank ) .Parsers induced from corpora are of interest both as simply exercises in machine learning and also because they are often the best parsers obtainable by any method .","label":"Uses","metadata":{},"score":"75.861176"}
{"text":"[21 ] We illustrate the derivation of the joint probability density function for monthly streamflow at site 1 and 2 as an example .Denote the marginal probabilities of monthly streamflow for the month s at sites 1 and 2 as U s and V s .","label":"Uses","metadata":{},"score":"76.34667"}
{"text":"v . ) exp .[ .r . m .r .u .r .r . m .v .r . ) m .u .v . ) . ]Parameter λ 0 can be expressed as a function of other parameters as .","label":"Uses","metadata":{},"score":"77.38339"}
{"text":"Tested across six domains , our system outperforms all non - oracle baselines including the best domain - independent parsing model .Thus , we are able to demonstrate the value of customizing parsing models to specific domains . ... train models in many different domains but sidestep the problem of domain detection .","label":"Uses","metadata":{},"score":"77.5468"}
{"text":"[ 15 ] The simulation methodology to generate the monthly streamflow ( marginal ) at each site can be summarized as follows : . [ 16 ] ( 1 ) Initialize monthly streamflow at sites 1 , 2 , and 3 of the first month , i.e. , u 1 , v 1 , and w 1 , by assigning random values from historical records .","label":"Uses","metadata":{},"score":"78.217186"}
{"text":"Labeled data typically requires expert annotators which makes it both time consuming and costly to produce .Furthermore , once training data has been created for one textual domain , portability to similar domains is limited .This domain - dependence has inspired a large body of work since syntactic parsing aims to capture syntactic patterns across an entire language rather than just a specific domain .","label":"Uses","metadata":{},"score":"78.7192"}
{"text":"CS 775 : Seminar in Natural Language Understanding , Spring 2001 \" Statistical Natural Language Processing : Models and Methods \" .Wednesdays , 1:25 - 2:15 , Hollister 362 .Natural language processing ( NLP ) has been considered one of the \" holy grails \" for artificial intelligence ever since Turing proposed his famed \" imitation game \" ( the Turing Test ) .","label":"Uses","metadata":{},"score":"78.80746"}
{"text":"Sense disambiguation is an \" intermediate task \" ( Wilks and Stevenson , 1996 ) which is not an end in itself , but rather is necessary at one level or another to accomplish most natural language processing tasks .It is . by Lillian Lee , Fernando C. N. Pereira , Claire Cardie , Raymond Mooney - Machine Learning , 1999 . \" ... Abstract .","label":"Uses","metadata":{},"score":"79.30405"}
{"text":"For instance , starting from a corpus of English text with no linguistic knowledge whatsoever , the algorithms can automatically induce a set of rules for determining the appropriate meaning of a word in context .Since this inductive learning procedure is computationally taxing , we are also obliged to provide a set of heuristics to ease the computational burden .","label":"Uses","metadata":{},"score":"79.385376"}
{"text":"This paper discusses the implementation of crucial aspects of this new annotation scheme .INTRODUCTION During the first phase of the The Penn Treebank project [ 10 ] , ending in December 1992 , 4.5 million words of text were tagged for part - of - speech , with about two - thirds of this material also annotated with a skeletal syntactic bracketing .","label":"Uses","metadata":{},"score":"79.456406"}
{"text":"359 - -382 .Kenneth W. Church and William A. Gale , 1991 .A comparison of the enhanced Good - Turing and deleted estimation methods for estimating probabilities of English bigrams .Computer Speech and Language 5 , 19 - 54 .","label":"Uses","metadata":{},"score":"80.17325"}
{"text":"The spread pattern of simulated streamflow pairs generally matched that of observed streamflow pairs of the 2 months well .As an example , the monthly streamflow of March and April for site 3 shows a strong dependence ( Spearman correlation : 0.83 ) and most of the streamflow pairs spread along the diagonal .","label":"Uses","metadata":{},"score":"82.178406"}
{"text":"19 ] ( 4 ) Repeat step ( 3 ) to generate a sequence of monthly streamflows u 4 , ... , u t , v 4 , ... , v t and w 4 , ... , w t up to time t .","label":"Uses","metadata":{},"score":"83.37371"}
{"text":"We show how web mark - up can be used to improve unsupervised dependency parsing .Starting from raw bracketings of four common HTML tags ( anchors , bold , italics and underlines ) , we refine approximate partial phrase boundaries to yield accurate parsing constraints .","label":"Uses","metadata":{},"score":"84.451996"}
{"text":"We show how web mark - up can be used to improve unsupervised dependency parsing .Starting from raw bracketings of four common HTML tags ( anchors , bold , italics and underlines ) , we refine approximate partial phrase boundaries to yield accurate parsing constraints .","label":"Uses","metadata":{},"score":"84.451996"}
{"text":"To ensure the integration of the copula density function over all the space equates one , g 1 ( u , v ) can be specified as 1 .To model the dependence structure , the function g ( u , v ) can be specified in the form that is related to an association measure such that the expectation E ( g ( u , v ) ) becomes some linear form of rank correlation .","label":"Uses","metadata":{},"score":"84.9277"}
{"text":"The scatterplots of the rank of observed monthly streamflow and one sequence of simulated streamflow pairs from the copula at different sites for the same month of March are also shown in Figure 1 ( bottom ) .The simulated Spearman correlations are 0.59 , 0.59 , and 0.58 , which are relatively close to the observed Spearman correlation ( i.e. , 0.65 , 0.68 , and 0.67 ) .","label":"Uses","metadata":{},"score":"86.73818"}
{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"Uses","metadata":{},"score":"88.090775"}
{"text":"The best aspect of a research environment , in my opinion , is the abundance of bright people with whom you argue , discuss , and nurture your ideas .I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas .","label":"Uses","metadata":{},"score":"88.090775"}
{"text":"Tagging English Text with a Probabilistic Model .Computational Linguistics 20(2 ) , pp .155 - -171 .Ralph Weischedel , Marie Meteer , Richard Schwartz , Lance A. Ramshaw , and Jeff Palmucci , 1993 .Coping with ambiguity and unknown words through probabilistic models .","label":"Uses","metadata":{},"score":"88.6487"}
{"text":"Iwould like toacknowledge the following people for their contribution to my education : I thank my advisor Mitch Marcus , who gave me the intellectual freedom to pursue what I believed to be the best way to approach natural language processing , and also gave me direction when necessary .","label":"Uses","metadata":{},"score":"88.765465"}
{"text":"You can find out more about our use of cookies in About Cookies , including instructions on how to turn off cookies if you wish to do so .By continuing to browse this site you agree to us using cookies as described in About Cookies .","label":"Uses","metadata":{},"score":"89.83165"}
{"text":"For example , a speech recognizer may need to determine which of the two word combinations \" eat a peach \" and \" eat a beach \" is more likely .Statistical NLP met ... \" .Abstract .In many applications of natural language processing ( NLP ) it is necessary to determine the likelihood of a given word combination .","label":"Uses","metadata":{},"score":"90.27156"}
{"text":"The results show that an unsupervised technique based on topic models is effective - it outperforms random data selection on both languages examined , English and Dutch .Moreover , the technique works better than manually assigned labels gathered from meta - data that is available for English . ...","label":"Uses","metadata":{},"score":"90.36312"}
{"text":"I thank all of my thesis committee members : John La erty from Carnegie Mellon University , Aravind Joshi , Lyle Ungar , and Mark Liberman , for their extremely valuable suggestions and comments about my thesis research .I thank Mike Collins , Jason Eisner , and Dan Melamed , with whom I 've had many stimulating and impromptu discussions in the LINC lab .","label":"Uses","metadata":{},"score":"91.575455"}
{"text":"We present a number of semi - supervised parsing experiments on the Irish language carried out using a small seed set of manually parsed trees and a larger , yet still relatively small , set of unlabelled sentences .We take two popular dependency parsers - one graph - based and one transition - based - and ... \" .","label":"Uses","metadata":{},"score":"92.62583"}
{"text":"According to the influential theo ... \" .Selectional constraints are limitations on the applicability of predicates to arguments .For example , the statement \" The number two is blue \" may be syntactically well formed , but at some level it is anomalous - BLUE is not a predicate that can be applied to numbers .","label":"Uses","metadata":{},"score":"93.72072"}
{"text":"In the early 1960s , IBM developed and demonstrated the Shoebox -- a forerunner of today 's voice recognition systems .The device recognized digits and arithmetic commands and responded to them .The following is a 1961 Times article about the device .","label":"Uses","metadata":{},"score":"101.31779"}
{"text":"v .s .v .s . ) exp .i .i . u . s .i .i .i .v .s .i .i .i .v .s .i . u . s .","label":"Uses","metadata":{},"score":"101.66102"}
{"text":"IBM is not responsible for , and does not validate or confirm , the correctness or accuracy of any user content posted .IBM does not endorse any user content .User content does not represent the views or opinions of IBM .","label":"Uses","metadata":{},"score":"120.56738"}
