{"text":"Our approach involves building a probabilistic context - free grammar for each author and using this grammar as a language model for classification .We evaluate the performance of our method on a wide range of datasets to demonstrate its efficacy . ... 000 ; Holmes and Forsyth , 1995 ; Joachims , 1998 ; Mosteller and Wallace , 1984 ) .","label":"Background","metadata":{},"score":"38.498497"}{"text":"Our approach involves building a probabilistic context - free grammar for each author and using this grammar as a language model for classification .We evaluate the performance of our method on a wide range of datasets to demonstrate its efficacy . ... 000 ; Holmes and Forsyth , 1995 ; Joachims , 1998 ; Mosteller and Wallace , 1984 ) .","label":"Background","metadata":{},"score":"38.498497"}{"text":"Next , I present a PCFG induction model for grounded language learning that extends the model of Borschinger , Jones , and Johnson ( 2011 ) by utilizing a semantic lexicon .Our model overcomes such limitations by employing a semantic lexicon as the basic building block for PCFG rule generation .","label":"Background","metadata":{},"score":"39.442993"}{"text":"We present a Bayesian formulation for weakly - supervised learning of a Combinatory Categorial Grammar ( CCG ) supertagger with an HMM .We assume supervision in the form of a tag dictionary , and our prior encourages the use of cross - linguistically common category structures as well as transitions between tags that can combine locally according to CCG 's combinators .","label":"Background","metadata":{},"score":"40.33246"}{"text":"We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative improvement over the baseline approach that uses a fixed context window of adjacent words .","label":"Background","metadata":{},"score":"41.840393"}{"text":"We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative improvement over the baseline approach that uses a fixed context window of adjacent words .","label":"Background","metadata":{},"score":"41.840393"}{"text":"The system consists of two components : an unlabeled dependency parser using Gibbs sampling which can incorporate sentence - level ( global ) features as well as token - leve ... \" .In this paper , we describe a two - stage multilingual dependency parser used for the multilingual track of the CoNLL 2007 shared task .","label":"Background","metadata":{},"score":"42.429802"}{"text":"The system consists of two components : an unlabeled dependency parser using Gibbs sampling which can incorporate sentence - level ( global ) features as well as token - leve ... \" .In this paper , we describe a two - stage multilingual dependency parser used for the multilingual track of the CoNLL 2007 shared task .","label":"Background","metadata":{},"score":"42.429802"}{"text":"This builds on the intuitions of Klein and Manning 's ( 2002 ) \" constituent - context \" model , which demonstrated the value of modeling context , but has the advantage of being able to exploit the properties of CCG .","label":"Background","metadata":{},"score":"42.603943"}{"text":"We also introduce a modified closed - world assumption that significantly reduces the size of the ground network , thereby making inference feasible .Our approach is evaluated on the recognizing textual entailment task , and experiments demonstrate its dramatic impact on the efficiency of inference .","label":"Background","metadata":{},"score":"43.052906"}{"text":"We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .We apply the framework to word segmentation , joint segmentation and POStagging , dependency parsing , and phrase - structure parsing .","label":"Background","metadata":{},"score":"43.461582"}{"text":"We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .We apply the framework to word segmentation , joint segmentation and POStagging , dependency parsing , and phrase - structure parsing .","label":"Background","metadata":{},"score":"43.461582"}{"text":"We investigate this question for a space of matrix decomposition models which can express a variety of widely used models from unsupervised learning .To enable model selection , we organize these models into a context - free grammar which generates a wide variety of structures through the compositional application of a few simple rules .","label":"Background","metadata":{},"score":"44.46342"}{"text":"For future work , I propose to extend our PCFG induction model in several ways : improving the lexicon learning algorithm , discriminative re - ranking of top - k parses , and integrating the meaning representation language ( MRL ) grammar for extra structural information .","label":"Background","metadata":{},"score":"44.51304"}{"text":"This can be captured by the low - rank model GG + G. In a landmark paper , Olshausen and Field ( 1996 ) found that image patches are better modeled as a linear combination of a small number of components drawn from a larger dictionary .","label":"Background","metadata":{},"score":"44.69224"}{"text":"Specifically , we develop three infinite tree models , each of which enforces different independence assumptions , and for each model we define a simple direct assignment sampling inference procedure . ... ch of the left and right .As is standard , we used WSJ sections 2 - 21 for training , section 22 for development , and section 23 for testing .","label":"Background","metadata":{},"score":"45.1166"}{"text":"Specifically , we develop three infinite tree models , each of which enforces different independence assumptions , and for each model we define a simple direct assignment sampling inference procedure . ... ch of the left and right .As is standard , we used WSJ sections 2 - 21 for training , section 22 for development , and section 23 for testing .","label":"Background","metadata":{},"score":"45.1166"}{"text":"Unlike previous approaches , our framework does not require full projected parses , allowing partial , approximate transfer through linear expectation constraints on the space of distributions over trees .We consider several types of constraints that range from generic dependency conservation to language - specific annotation rules for auxiliary verb analysis .","label":"Background","metadata":{},"score":"45.29219"}{"text":"Unlike previous approaches , our framework does not require full projected parses , allowing partial , approximate transfer through linear expectation constraints on the space of distributions over trees .We consider several types of constraints that range from generic dependency conservation to language - specific annotation rules for auxiliary verb analysis .","label":"Background","metadata":{},"score":"45.29219"}{"text":"Steedman , M. ( with Baldridge , J. ) ( forthcoming ) .Combinatory categorial grammar .To appear in Non - transformational syntax ( eds .Borsley , R. & Borjars , K. ) .Available here .Clark , S. & Curran , J.R. ( 2007 ) .","label":"Background","metadata":{},"score":"45.30438"}{"text":"Steedman , M. ( with Baldridge , J. ) ( forthcoming ) .Combinatory categorial grammar .To appear in Non - transformational syntax ( eds .Borsley , R. & Borjars , K. ) .Available here .Clark , S. & Curran , J.R. ( 2007 ) .","label":"Background","metadata":{},"score":"45.30438"}{"text":"We investigate how to best train a parser on the French Treebank ( Abeillé et al . , 2003 ) , viewing the task as a trade - off between generalizability and interpretability .We compare , for French , a supervised lexicalized parsing algorithm with a semi - supervised unlexicalized algorithm ( Petrov et al . , 2006 ) along the lines of ( Crabbé and Candito , 2008 ) .","label":"Background","metadata":{},"score":"45.86986"}{"text":"We investigate how to best train a parser on the French Treebank ( Abeillé et al . , 2003 ) , viewing the task as a trade - off between generalizability and interpretability .We compare , for French , a supervised lexicalized parsing algorithm with a semi - supervised unlexicalized algorithm ( Petrov et al . , 2006 ) along the lines of ( Crabbé and Candito , 2008 ) .","label":"Background","metadata":{},"score":"45.86986"}{"text":"For this we exploit recurrent neural networks , specifically LSTMs , which have demonstrated state - of - the - art performance in image caption generation .Our LSTM model is trained on video - sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip .","label":"Background","metadata":{},"score":"45.92038"}{"text":"Experimental results show that the global features are useful in all the languages . ... mines unlabeled dependency structures only , and we attach dependency relation labels using Support Vector Machines afterwards . \" ...We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .","label":"Background","metadata":{},"score":"46.4662"}{"text":"Experimental results show that the global features are useful in all the languages . ... mines unlabeled dependency structures only , and we attach dependency relation labels using Support Vector Machines afterwards . \" ...We study a range of syntactic processing tasks using a general statistical framework that consists of a global linear model , trained by the generalized perceptron together with a generic beamsearch decoder .","label":"Background","metadata":{},"score":"46.4662"}{"text":"In this document , we describe our methods for learning these rules , estimating their associated weights , and performing probabilistic and logical inference to infer unseen relations .In the KBP SF task , our system was able to infer several unextracted relations , but its performance was limited by the base level extractor .","label":"Background","metadata":{},"score":"46.57047"}{"text":"However , it does not scale to problems with a large set of potential meanings for each sentence , such as the navigation instruction following task studied by Chen and Mooney ( 2011 ) .This paper presents an enhancement of the PCFG approach that scales to such problems with highly - ambiguous supervision .","label":"Background","metadata":{},"score":"46.672516"}{"text":"We find that this hypothesis only holds when it is applied to relevant dimensions .We propose a robust supervised approach that achieves accuracies of .84 and .85 on two existing datasets and that can be interpreted as selecting the dimensions that are relevant for distributional inclusion .","label":"Background","metadata":{},"score":"46.79909"}{"text":"They also permit the use of well - understood , generalpurpose learning algorithms .There has been an increased interest in using probabilistic grammars in the Bayesian setting .To date , most of the literature has focused on using a Dirichlet prior .","label":"Background","metadata":{},"score":"46.958168"}{"text":"They also permit the use of well - understood , generalpurpose learning algorithms .There has been an increased interest in using probabilistic grammars in the Bayesian setting .To date , most of the literature has focused on using a Dirichlet prior .","label":"Background","metadata":{},"score":"46.958168"}{"text":"We then describe first steps of an approach that uses this mapping to recast first - order semantics into the probabilistic models that are part of Statistical Relational AI .Specifically , we show how Discourse Representation Structures can be combined with distributional models for word meaning inside a Markov Logic Network and used to successfully perform inferences that take advantage of logical concepts such as negation and factivity as well as weighted information on word meaning in context .","label":"Background","metadata":{},"score":"47.04738"}{"text":"..METU - Sabancı treebank ( Atalay et al . , 2003 ; Oflazer et al . , 2003 ) from the CoNLL shared task in 2006 .Whenever using CoNLL shared task data , we used the first 80 % of the data d .. \" ...","label":"Background","metadata":{},"score":"47.061115"}{"text":"..METU - Sabancı treebank ( Atalay et al . , 2003 ; Oflazer et al . , 2003 ) from the CoNLL shared task in 2006 .Whenever using CoNLL shared task data , we used the first 80 % of the data d .. \" ...","label":"Background","metadata":{},"score":"47.061115"}{"text":"A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment .We present an approach that explores the tradeoff from the other direction , starting with a context - free translation model learned directly from aligned parallel text , and then adding soft constituent - level constraints based on parses of the source language .","label":"Background","metadata":{},"score":"47.09603"}{"text":"A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment .We present an approach that explores the tradeoff from the other direction , starting with a context - free translation model learned directly from aligned parallel text , and then adding soft constituent - level constraints based on parses of the source language .","label":"Background","metadata":{},"score":"47.09603"}{"text":"Our approach outperforms a baseline model trained with uniform priors by exploiting universal , intrinsic properties of the CCG formalism to bias the model toward simpler , more cross - linguistically common categories .ML ID : 310 .Natural Language Semantics using Probabilistic Logic [ Details ] [ PDF ] [ Slides ] Islam Beltagy October 2014 .","label":"Background","metadata":{},"score":"47.38837"}{"text":"Furthermore , we propose a novel approach to weighting rules using a curated lexical ontology like WordNet .The learned rules along with their parameters are then used to infer implicit information using a Bayesian Logic Program .Experimental evaluation on a machine reading testbed demonstrates the efficacy of the proposed methods .","label":"Background","metadata":{},"score":"47.61026"}{"text":"Empirically , we show that it yields substantial improvements over previous work that used similar biases to initialize an EM - based learner .Additional gains are obtained by further shaping the prior with corpus - specific information that is extracted automatically from raw text and a tag dictionary .","label":"Background","metadata":{},"score":"47.642925"}{"text":"Previous work has shown that learning sequence models for CCG tagging can be improved by using priors that are sensitive to the formal properties of CCG as well as cross - linguistic universals .We extend this approach to the task of learning a full CCG parser from weak supervision .","label":"Background","metadata":{},"score":"47.6559"}{"text":"Distributional models use contextual similarity to predict the ' ' graded ' ' semantic similarity of words and phrases but they do not adequately capture logical structure .In addition , there are a few recent attempts to combine both representations either on the logic side ( still , not a graded representation ) , or in the distribution side(not full logic ) .","label":"Background","metadata":{},"score":"47.71844"}{"text":"Our system handles overall sentence structure and phenomena like negation in the logic , then uses our Robinson resolution variant to query distributional systems about words and short phrases .Therefor , we use our system to evaluate distributional lexical entailment approaches .","label":"Background","metadata":{},"score":"47.81889"}{"text":"Here , we first develop an approach using BLPs to infer implicitly stated facts from natural language text .It involves learning uncertain common sense knowledge in the form of probabilistic first - order rules by mining a large corpus of automatically extracted facts using an existing rule learner .","label":"Background","metadata":{},"score":"47.906097"}{"text":"We investigate how to best train a parser on the French Treebank ( Abeillé et al . , 2003 ) , viewing the task as a trade - off between generalizability and interpretability .We compare , for French , a supervised lexicalized parsing algorithm w ... \" .","label":"Background","metadata":{},"score":"47.962017"}{"text":"We investigate how to best train a parser on the French Treebank ( Abeillé et al . , 2003 ) , viewing the task as a trade - off between generalizability and interpretability .We compare , for French , a supervised lexicalized parsing algorithm w ... \" .","label":"Background","metadata":{},"score":"47.962017"}{"text":"ML ID : 273 . \"Grounded \" language learning employs training data in the form of sentences paired with relevant but ambiguous perceptual contexts .Borschinger et al .( 2011 ) introduced an approach to grounded language learning based on unsupervised PCFG induction .","label":"Background","metadata":{},"score":"49.21156"}{"text":"This is quite different from representing them in standard first - order logic . 2 ) knowledge base construction in the form of weighted inference rules from different sources like WordNet , paraphrase collections , and lexical and phrasal distributional rules generated on the fly .","label":"Background","metadata":{},"score":"49.232544"}{"text":"Finally , we present a method of adapting discriminative reranking to grounded language learning in order to improve the performance of our proposed generative models .Although such generative models are easy to implement and are intuitive , it is not always the case that generative models perform best , since they are maximizing the joint probability of data and model , rather than directly maximizing conditional probability .","label":"Background","metadata":{},"score":"49.47518"}{"text":"However , it is an open question of how best to integrate it with uncertain , probabilistic knowledge , for example regarding word meaning .This paper describes the first steps of an approach to recasting first - order semantics into the probabilistic models that are part of Statistical Relational AI .","label":"Background","metadata":{},"score":"49.60208"}{"text":"In Computational linguistics , 33(4 ) , pp.493 - 552 .Available here .Cann , R. ( 1993 ) .Formal semantics .Cambridge University Press .( Google Books , UL , etc . )Bos , J. & Blackburn , P. ( 2004 ) .","label":"Background","metadata":{},"score":"49.65671"}{"text":"In Computational linguistics , 33(4 ) , pp.493 - 552 .Available here .Cann , R. ( 1993 ) .Formal semantics .Cambridge University Press .( Google Books , UL , etc . )Bos , J. & Blackburn , P. ( 2004 ) .","label":"Background","metadata":{},"score":"49.65671"}{"text":"This paper presents novel improvements to the induction of translation lexicons from monolingual corpora using multilingual dependency parses .We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative ... \" .","label":"Background","metadata":{},"score":"49.848946"}{"text":"This paper presents novel improvements to the induction of translation lexicons from monolingual corpora using multilingual dependency parses .We introduce a dependency - based context model that incorporates long - range dependencies , variable context sizes , and reordering .It provides a 16 % relative ... \" .","label":"Background","metadata":{},"score":"49.848946"}{"text":"We introduce a refinement algorithm that first learns a lexicon which is then used to remove parts of the graphs that are irrelevant .Experiments in a navigation domain shows that the algorithm successfully recovered over three quarters of the correct semantic content .","label":"Background","metadata":{},"score":"49.905045"}{"text":"This module provides an introduction to the formal syntax and semantics of natural language , in particular Montague - style compositional semantics using a Categorial Grammar model of syntax .Half of the module will focus on the theory of syntax , followed by an example of how recent advances in parsing technology allow such a theory to be implemented in practice , operating on naturally occurring text .","label":"Background","metadata":{},"score":"50.148598"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"Background","metadata":{},"score":"50.96804"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"Background","metadata":{},"score":"50.96804"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"Background","metadata":{},"score":"50.96804"}{"text":"We use a generative history - based model to predict the most likely derivation of a dependency parse .Our probabilistic model is based on Incremental Sigmoid Belief Networks , a recently proposed class of latent variable models for structure prediction .","label":"Background","metadata":{},"score":"50.96804"}{"text":"Next , I present an end - to - end deep network that can jointly model a sequence of video frames and a sequence of words .The second part of the proposal outlines a set of models to significantly extend work in this area .","label":"Background","metadata":{},"score":"50.981884"}{"text":"( 2008 ) .By applying Rule ( 1 ) to the sparse coding structure , we can represent their model in our framework as ( exp(GG+G )G)G+G. This model has been successful at capturing higher - level textural properties of a scene and has properties similar to complex cells in the primary visual cortex .","label":"Background","metadata":{},"score":"51.003365"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"Background","metadata":{},"score":"51.072296"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"Background","metadata":{},"score":"51.072296"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"Background","metadata":{},"score":"51.072296"}{"text":"We compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for French .The architectures are based on PCFGs with latent variables , graph - based dependency parsing and transition - based dependency parsing , respectively .","label":"Background","metadata":{},"score":"51.072296"}{"text":"Unlike previous methods , our approach can annotate arbitrary videos without requiring the expensive collection and annotation of a similar training video corpus .We evaluate our technique against a baseline that does not use text - mined knowledge and show that humans prefer our descriptions 61 percent of the time .","label":"Background","metadata":{},"score":"51.292633"}{"text":"Borschinger et al .'s approach works well in situations of limited ambiguity , such as in the sportscasting task .However , it does not scale well to highly ambiguous situations when there are large sets of potential meaning possibilities for each sentence , such as in the navigation instruction following task first studied by Chen and Mooney ( 2011 ) .","label":"Background","metadata":{},"score":"51.308067"}{"text":"Heckerman et al .[ 4 ] introduced the directed acyclic probabilistic entity - relationship ( DAPER ) model which is based on the entity - relationship model used to design relational databases .P rob R e M is built on the DAPER specification of a PRM , thus it requires the data to be modelled with a entity - relationship diagram .","label":"Background","metadata":{},"score":"51.340492"}{"text":"The code is available as is on github .Lise Getoor .Learning probabilistic relational models .In SARA ' 02 : Proceedings of the 4th Interna- tional Symposium on Abstraction , Reformulation , and Approximation , pages 322 - 323 .","label":"Background","metadata":{},"score":"51.506195"}{"text":"We combine logical and distributional representations of natural language meaning by transforming distributional similarity judgments into weighted inference rules using Markov Logic Networks ( MLNs ) .We show that this framework supports both judging sentence similarity and recognizing textual entailment by appropriately adapting the MLN implementation of logical connectives .","label":"Background","metadata":{},"score":"51.575485"}{"text":"In this paper , we present a novel approach for authorship attribution , the task of identifying the author of a document , using probabilistic context - free grammars .Our approach involves building a probabilistic context - free grammar for each author and using this grammar as a language model for class ... \" .","label":"Background","metadata":{},"score":"51.88652"}{"text":"In this paper , we present a novel approach for authorship attribution , the task of identifying the author of a document , using probabilistic context - free grammars .Our approach involves building a probabilistic context - free grammar for each author and using this grammar as a language model for class ... \" .","label":"Background","metadata":{},"score":"51.88652"}{"text":"We then develop an online rule learner that handles the concise , incomplete nature of natural - language text and learns first - order rules from noisy IE extractions .Finally , we develop a novel approach to calculate the weights of the rules using a curated lexical ontology like WordNet .","label":"Background","metadata":{},"score":"52.012985"}{"text":"Their symbolic component is amenable to inspection by humans , while their probabilistic component helps resolve ambiguity .They also permit the use of well - understood , generalpurpose learn ... \" .Probabilistic grammars offer great flexibility in modeling discrete sequential data like natural language text .","label":"Background","metadata":{},"score":"52.103416"}{"text":"Their symbolic component is amenable to inspection by humans , while their probabilistic component helps resolve ambiguity .They also permit the use of well - understood , generalpurpose learn ... \" .Probabilistic grammars offer great flexibility in modeling discrete sequential data like natural language text .","label":"Background","metadata":{},"score":"52.103416"}{"text":"Therefore , we extend BLPs to use logical abduction to construct Bayesian networks and call the resulting model Bayesian Abductive Logic Programs ( BALPs ) .In the second part of the dissertation , we apply BLPs to the task of machine reading , which involves automatic extraction of knowledge from natural language text .","label":"Background","metadata":{},"score":"52.35013"}{"text":"As a result , they are widely used in domains like social network analysis , biological data analysis , and natural language processing .Bayesian Logic Programs ( BLPs ) , which integrate both first - order logic and Bayesian networks are a powerful SRL formalism developed in the recent past .","label":"Background","metadata":{},"score":"52.394073"}{"text":"Yet , various grammar parameters are expected to be correlated because the elements in language they represent share linguistic properties .In this paper , we suggest an alternative to the Dirichlet prior , a family of logistic normal distributions .We derive an inference algorithm for this family of distributions and experiment with the task of dependency grammar induction , demonstrating performance improvements with our priors on a set of six treebanks in different natural languages .","label":"Background","metadata":{},"score":"52.394352"}{"text":"Yet , various grammar parameters are expected to be correlated because the elements in language they represent share linguistic properties .In this paper , we suggest an alternative to the Dirichlet prior , a family of logistic normal distributions .We derive an inference algorithm for this family of distributions and experiment with the task of dependency grammar induction , demonstrating performance improvements with our priors on a set of six treebanks in different natural languages .","label":"Background","metadata":{},"score":"52.394352"}{"text":"I build on recent \" deep \" machine learning approaches to develop video description models using a unified deep neural network with both convolutional and recurrent structure .This technique treats the video domain as another \" language \" and takes a machine translation approach using the deep network to translate videos to text .","label":"Background","metadata":{},"score":"52.723434"}{"text":"In adding syntax to statistical MT , there is a tradeoff between taking advantage of linguistic analysis , versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data .A number of previous efforts have tackled this tradeoff by starting with a commitment ... \" .","label":"Background","metadata":{},"score":"52.83482"}{"text":"All these approaches are evaluated on the two publicly available domains that have been actively used in many other grounded language learning studies .Our methods demonstrate consistently improved performance over those of previous studies in the domains with different languages ; this proves that our methods are language - independent and can be generally applied to other grounded learning problems as well .","label":"Background","metadata":{},"score":"52.908936"}{"text":"In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"Background","metadata":{},"score":"53.022125"}{"text":"In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"Background","metadata":{},"score":"53.022125"}{"text":"We outperform text - only models in two different evaluations , and demonstrate that low - level visual features are directly compatible with the existing model .( 2 ) We present a novel way to integrate visual features into the LDA model using unsupervised clusters of images .","label":"Background","metadata":{},"score":"53.203926"}{"text":"ML ID : 295 .Recent investigations into grounded models of language have shown that holistic views of language and perception can provide higher performance than independent views .In this work , we improve a two - dimensional multimodal version of Latent Dirichlet Allocation ( Andrews et al . , 2009 ) in various ways .","label":"Background","metadata":{},"score":"53.2042"}{"text":"On the Proper Treatment of Quantifiers in Probabilistic Logic Semantics [ Details ] [ PDF ] [ Slides ] Islam Beltagy and Katrin Erk In Proceedings of the 11th International Conference on Computational Semantics ( IWCS-2015 ) , London , UK , April 2015 .","label":"Background","metadata":{},"score":"53.206547"}{"text":"The tree with the maximal probability is outputted .The experiments are carried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser . ... arried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser .","label":"Background","metadata":{},"score":"53.687164"}{"text":"The tree with the maximal probability is outputted .The experiments are carried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser . ... arried on 10 languages , and the results show that our probabilistic parsing action models outperform the original deterministic dependency parser .","label":"Background","metadata":{},"score":"53.687164"}{"text":"Taking the MIN - GREEDY algorithm ( Ravi et al . , 2010 ) as a starting point , we improve it with several intuitive heuristics .We also define a simple HMM emission initialization that takes advantage of the tag dictionary and raw data to capture both the openness of a given tag and its estimated prevalence in the raw data .","label":"Background","metadata":{},"score":"53.712765"}{"text":"Undergraduate Honors Thesis , Computer Science Department , University of Texas at Austin .Semantic Parsing using Distributional Semantics and Probabilistic Logic [ Details ] [ PDF ][Poster ] Islam Beltagy and Katrin Erk and Raymond Mooney In Proceedings of ACL 2014 Workshop on Semantic Parsing ( SP-2014 ) , 7 - -11 , Baltimore , MD , June 2014 .","label":"Background","metadata":{},"score":"53.72141"}{"text":"But practical probabilistic logic frameworks usually assume a finite domain in which each entity corresponds to a constant in the logic ( domain closure assumption ) .They also assume a closed world where everything has a very low prior probability .","label":"Background","metadata":{},"score":"54.51603"}{"text":"Real - World Semi - Supervised Learning of POS - Taggers for Low - Resource Languages [ Details ] [ PDF ] Dan Garrette and Jason Mielens and Jason Baldridge In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL-2013 ) , 583 - -592 , Sofia , Bulgaria , August 2013 .","label":"Background","metadata":{},"score":"54.633705"}{"text":"Sentences and the on - the - fly ontology are represented in probabilistic logic .For inference , we use probabilistic logic frameworks like Markov Logic Networks ( MLN ) and Probabilistic Soft Logic ( PSL ) .This semantic parsing approach is evaluated on two tasks , Textual Entitlement ( RTE ) and Textual Similarity ( STS ) , both accomplished using inference in probabilistic logic .","label":"Background","metadata":{},"score":"54.640465"}{"text":"It is especially important in language grounding where the training data usually consist of language paired with an ambiguous perceptual context .Recent work by Chen and Mooney ( 2011 ) introduced a lexicon learning method that deals with ambiguous relational data by taking intersections of graphs .","label":"Background","metadata":{},"score":"54.724304"}{"text":"Unlike previous work , our approach works on out - of - domain actions : it does not require training videos of the exact activity .If it can not find an accurate prediction for a pre - trained model , it finds a less specific answer that is also plausible from a pragmatic standpoint .","label":"Background","metadata":{},"score":"54.988388"}{"text":"In this paper , we show how to formulate Textual Entailment ( RTE ) inference problems in probabilistic logic in a way that takes the domain closure and closed - world assumptions into account .We evaluate our proposed technique on three RTE datasets , on a synthetic dataset with a focus on complex forms of quantification , on FraCas and on one more natural dataset .","label":"Background","metadata":{},"score":"54.99345"}{"text":"Aims .This module provides an introduction to the formal syntax and semantics of natural language , in particular Montague - style compositional semantics using a Categorial Grammar model of syntax .Half of the module will focus on the theory of syntax , followed by an example of how recent advances in parsing technology allow such a theory to be implemented in practice , operating on naturally occurring text .","label":"Background","metadata":{},"score":"55.00728"}{"text":"However , constructing such corpora can be expensive and time - consuming due to the expertise it requires to annotate such data .In this thesis , we explore alternative ways of learning which do not rely on direct human supervision .","label":"Background","metadata":{},"score":"55.039024"}{"text":"The reported experiments Tools . \" ...In adding syntax to statistical MT , there is a tradeoff between taking advantage of linguistic analysis , versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data .A number of previous efforts have tackled this tradeoff by starting with a commitment ... \" .","label":"Background","metadata":{},"score":"55.064552"}{"text":"We present probabilistic generative models for learning such correspondences along with a reranking model to improve the performance further .We perform evaluations on the RoboCup sportscasting corpus , proving that our model is more effective than those proposed by previous researchers .","label":"Background","metadata":{},"score":"55.135498"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"Background","metadata":{},"score":"55.239056"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"Background","metadata":{},"score":"55.239056"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"Background","metadata":{},"score":"55.239056"}{"text":"We describe a newly available Hebrew Dependency Treebank , which is extracted from the Hebrew ( constituency ) Treebank .We establish some baseline unlabeled dependency parsing performance on Hebrew , based on two state - of - the - art parsers , MST - parser and MaltParser .","label":"Background","metadata":{},"score":"55.239056"}{"text":"First , we propose a number of new probabilistic script models leveraging recent advances in Neural Network training .These include recurrent sequence models with different hidden unit structure and Convolutional Neural Network models .Second , we propose integrating more lexical and linguistic information into events .","label":"Background","metadata":{},"score":"55.29903"}{"text":"This simple framework performs surprisingly well , giving accuracy results competitive with the state - of - the - art on all the tasks we consider .The computational simplicity of the decoder and training algorithm leads to significantly higher test speeds and lower training times than their main alternatives , including log - linear and large - margin training algorithms and dynamic - programming for decoding .","label":"Background","metadata":{},"score":"55.32692"}{"text":"This simple framework performs surprisingly well , giving accuracy results competitive with the state - of - the - art on all the tasks we consider .The computational simplicity of the decoder and training algorithm leads to significantly higher test speeds and lower training times than their main alternatives , including log - linear and large - margin training algorithms and dynamic - programming for decoding .","label":"Background","metadata":{},"score":"55.32692"}{"text":"Stamatatos et al .( 1999 ) and Luyckx and Daelemans ( 2008 ) use a combination of word - level statistics and part - of - speech counts or n - grams .Baayen ... .by Anna N. Rafferty , Christopher D. Manning - In ACL WorkShop on Parsing German , 2008 . \" ...","label":"Background","metadata":{},"score":"55.429955"}{"text":"Stamatatos et al .( 1999 ) and Luyckx and Daelemans ( 2008 ) use a combination of word - level statistics and part - of - speech counts or n - grams .Baayen ... .by Anna N. Rafferty , Christopher D. Manning - In ACL WorkShop on Parsing German , 2008 . \" ...","label":"Background","metadata":{},"score":"55.429955"}{"text":"Inducing Grammars from Linguistic Universals and Realistic Amounts of Supervision [ Details ] [ PDF ] Dan Garrette PhD Thesis , Department of Computer Science , The University of Texas at Austin , 2015 .Representing Meaning with a Combination of Logical Form and Vectors [ Details ] [ PDF ] Islam Beltagy and Stephen Roller and Pengxiang Cheng and Katrin Erk and Raymond J. Mooney arXiv preprint arXiv:1505.06816 [ cs .","label":"Background","metadata":{},"score":"55.44008"}{"text":"Previous work in computational linguistics on extracting lexical semantic information from unannotated corpora does not provide adequate representational flexibility and hence fails to capture the full extent of human conceptual knowledge .In this thesis I outline a family of probabilistic models capable of capturing important aspects of the rich organizational structure found in human language that can predict contextual variation , selectional preference and feature - saliency norms to a much higher degree of accuracy than previous approaches .","label":"Background","metadata":{},"score":"55.587532"}{"text":"However , it is not always possible to restrict the set of possible alignments to such limited numbers .Thus , we present another system that allows each sentence to be aligned to one of exponentially many connected subgraphs without explicitly enumerating them .","label":"Background","metadata":{},"score":"55.65024"}{"text":"NLP tasks differ in the semantic information they require , and at this time no single semantic representation fulfills all requirements .Logic - based representations characterize sentence structure , but do not capture the graded aspect of meaning .Distributional models give graded similarity ratings for words and phrases , but do not adequately capture overall sentence structure .","label":"Background","metadata":{},"score":"55.810326"}{"text":"We first present a system that learned to sportscast for RoboCup simulation games by observing how humans commentate a game .Using the simple assumption that people generally talk about events that have just occurred , we pair each textual comment with a set of events that it could be referring to .","label":"Background","metadata":{},"score":"55.850075"}{"text":"We present a holistic data - driven technique that generates natural - language descriptions for videos .We combine the output of state - of - the - art object and activity detectors with \" real - world \" knowledge to select the most probable subject - verb - object triplet for describing a video .","label":"Background","metadata":{},"score":"55.977043"}{"text":"We present a method for searching over this space of structures which mirrors the scientific discovery process .The learned structures can often decompose functions into interpretable components and enable long - range extrapolation on time - series datasets .Our structure search method outperforms many widely used kernels and kernel combination methods on a variety of prediction tasks .","label":"Background","metadata":{},"score":"56.165665"}{"text":"We discuss how the general framework is applied to each of the problems studied in this article , making comparisons with alternative learning and decoding algorithms .We also show how the comparability of candidates considered by the beam is an important factor in the performance .","label":"Background","metadata":{},"score":"56.166042"}{"text":"We discuss how the general framework is applied to each of the problems studied in this article , making comparisons with alternative learning and decoding algorithms .We also show how the comparability of candidates considered by the beam is an important factor in the performance .","label":"Background","metadata":{},"score":"56.166042"}{"text":"The parent and the child attribute of a dependency can be associated with different entities or relationships ; Getoor et al .refer to the path from child to parent attribute the slotchain whereas Heckerman et al .associate a more general constraint with the dependency instead .","label":"Background","metadata":{},"score":"56.217964"}{"text":"Inducing Grammars from Linguistic Universals and Realistic Amounts of Supervision [ Details ] [ PDF ] Dan Garrette PhD Thesis , Department of Computer Science , The University of Texas at Austin , 2015 .The best performing NLP models to date are learned from large volumes of manually - annotated data .","label":"Background","metadata":{},"score":"56.488388"}{"text":"Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"Background","metadata":{},"score":"56.526848"}{"text":"Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .","label":"Background","metadata":{},"score":"56.526848"}{"text":"Scripts represent knowledge of stereotypical event sequences that can aid text understanding .Initial statistical methods have been developed to learn probabilistic scripts from raw text corpora ; however , they utilize a very impoverished representation of events , consisting of a verb and one dependent argument .","label":"Background","metadata":{},"score":"56.620735"}{"text":"However , parsing accuracies for Arabic usually lag behind non - semitic languages .Moreover , whil ...The point of [ 1,2,3 ] is that , maybe , your data can be decomposed with a grammar like structure .What we usually study in datasets is whether or not they fit in a Robust PCA or NMF approach but one is seldom pushing the concept further .","label":"Background","metadata":{},"score":"56.724236"}{"text":"We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples , as well as atmospheric CO2 trends and airline passenger data .We also show that we can reconstruct standard covariances within our framework .Despite its importance , choosing the structural form of the kernel in nonparametric regression remains a black art .","label":"Background","metadata":{},"score":"56.789997"}{"text":"Scripts encode knowledge of prototypical sequences of events .We describe a Recurrent Neural Network model for statistical script learning using Long Short - Term Memory , an architecture which has been demonstrated to work well on a range of Artificial Intelligence tasks .","label":"Background","metadata":{},"score":"56.849865"}{"text":"Unlike previous work , we model the interactions between multiple entities in a script .Experiments on a large corpus using the task of inferring held - out events ( the \" narrative cloze evaluation \" ) demonstrate that modeling multi - argument events improves predictive accuracy .","label":"Background","metadata":{},"score":"56.941025"}{"text":"Unfortunately the sentence in Figure 1(b ) is highly unusual in its amount of dependency conservation .To get a feel for the typical case , we used off - the - shelf parsers ( McDonald et al . , 2005 ) for E .. by Ivan Titov , James Henderson - IN PROCEEDINGS OF CONLL-2007 SHARED TASK .","label":"Background","metadata":{},"score":"56.9812"}{"text":"Unfortunately the sentence in Figure 1(b ) is highly unusual in its amount of dependency conservation .To get a feel for the typical case , we used off - the - shelf parsers ( McDonald et al . , 2005 ) for E .. by Ivan Titov , James Henderson - IN PROCEEDINGS OF CONLL-2007 SHARED TASK .","label":"Background","metadata":{},"score":"56.9812"}{"text":"More sources can easily be added by mapping them to logical rules ; our system learns a resource - specific weight that counteract scaling differences between resources .3 ) inference , where we show how to solve the inference problems efficiently .","label":"Background","metadata":{},"score":"57.052757"}{"text":"First - order logic provides a powerful and flexible mechanism for representing natural language semantics .However , it is an open question of how best to integrate it with uncertain , weighted knowledge , for example regarding word meaning .This paper describes a mapping between predicates of logical form and points in a vector space .","label":"Background","metadata":{},"score":"57.056526"}{"text":"The beam - search decoder only requires the syntactic processing task to be broken into a sequence of decisions , such that , at each stage in the process , the decoder is able to consider the top - n candidates and generate all possibilities for the next stage .","label":"Background","metadata":{},"score":"57.067116"}{"text":"The beam - search decoder only requires the syntactic processing task to be broken into a sequence of decisions , such that , at each stage in the process , the decoder is able to consider the top - n candidates and generate all possibilities for the next stage .","label":"Background","metadata":{},"score":"57.067116"}{"text":"Unlike conventional reranking used in syntactic and semantic parsing , gold - standard reference trees are not naturally available in a grounded setting .Instead , we show how the weak supervision of response feedback ( e.g. successful task completion ) can be used as an alternative , experimentally demonstrating that its performance is comparable to training on gold - standard parse trees .","label":"Background","metadata":{},"score":"57.24202"}{"text":"Integrating Logical Representations with Probabilistic Information using Markov Logic [ Details ] [ PDF ] [ Slides ] Dan Garrette , Katrin Erk , Raymond Mooney In Proceedings of the International Conference on Computational Semantics , 105 - -114 , Oxford , England , January 2011 .","label":"Background","metadata":{},"score":"57.26541"}{"text":"Integrating Logical Representations with Probabilistic Information using Markov Logic [ Details ] [ PDF ] [ Slides ] Dan Garrette , Katrin Erk , Raymond Mooney In Proceedings of the International Conference on Computational Semantics , 105 - -114 , Oxford , England , January 2011 .","label":"Background","metadata":{},"score":"57.26541"}{"text":"We generalize the evaluation to other word - types , and show that the performance can be increased to 18 % relative by preserving part - of - speech equivalencies during translation .We further differentiate ourselves from previous work by conducting a second evaluation which examines the accuracy of translating all word types , rather than just nouns .","label":"Background","metadata":{},"score":"57.605125"}{"text":"We generalize the evaluation to other word - types , and show that the performance can be increased to 18 % relative by preserving part - of - speech equivalencies during translation .We further differentiate ourselves from previous work by conducting a second evaluation which examines the accuracy of translating all word types , rather than just nouns .","label":"Background","metadata":{},"score":"57.605125"}{"text":"While crowdsourcing offers a cheap alternative , quality control and scalability can become problematic .We discuss a novel annotation task that uses videos as the stimulus which discourages cheating .In addi- tion , our approach requires only monolingual speakers , thus making it easier to scale since more workers are qualified to contribute .","label":"Background","metadata":{},"score":"57.61241"}{"text":"An advantage of using probabilistic logic is that more rules can be added from more resources easily by mapping them to logical rules and weighting them appropriately .The last component is the inference , where we solve the probabilistic logic inference problem using an appropriate probabilistic logic tool like Markov Logic Network ( MLN ) , or Probabilistic Soft Logic ( PSL ) .","label":"Background","metadata":{},"score":"57.72816"}{"text":"\" We present a number of results improving the state of the art of learning statistical scripts for inferring implicit events .First , we demonstrate that incorporating multiple arguments into events , yielding a more complex event representation than is used in previous work , helps to improve a co - occurrence - based script system 's predictive power .","label":"Background","metadata":{},"score":"57.742786"}{"text":"And then there is this issue of model selection after all the computations are performed , What sort of criteria do you use ?At the other extreme , one can hold out a subset of the entries of the matrix and compute the mean squared error for predicting these entries .","label":"Background","metadata":{},"score":"57.839176"}{"text":"This work shows how linguistic universals intrinsic to the CCG formalism itself can be encoded as Bayesian priors to improve learning .ML ID : 321 .Combinatory Categorial Grammar ( CCG ) is a lexicalized grammar formalism in which words are associated with categories that specify the syntactic configurations in which they may occur .","label":"Background","metadata":{},"score":"57.87104"}{"text":"We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation .These kernels are derived by modelling a spectral density -- the Fourier transform of a kernel -- with a Gaussian mixture .","label":"Background","metadata":{},"score":"57.872406"}{"text":"We represent natural language semantics by combining logical and distributional information in probabilistic logic .We use Markov Logic Networks ( MLN ) for the RTE task , and Probabilistic Soft Logic ( PSL ) for the STS task .The system is evaluated on the SICK dataset .","label":"Background","metadata":{},"score":"58.333527"}{"text":"Research into non - parametric priors , such as the Dirichlet process , has enabled instead the use of infinite models , in which the number of hidden categories is not fixed , but can grow with the amount of training data .","label":"Background","metadata":{},"score":"58.35609"}{"text":"Research into non - parametric priors , such as the Dirichlet process , has enabled instead the use of infinite models , in which the number of hidden categories is not fixed , but can grow with the amount of training data .","label":"Background","metadata":{},"score":"58.35609"}{"text":"While there are few , they still exist .How could we expect the recursive approach of [ 3 ] to fit vision studies i.e. the type of work that Yi Ma et al usually investigate , from [ 3 ] : .","label":"Background","metadata":{},"score":"58.36439"}{"text":"Bos , J. ( 2008 ) .Wide - coverage semantic analysis with Boxer . 2ndConference on semantics in text processing .Available here .Garrette , D. , Erk , K. & Mooney , R. ( 2011 ) .Integrating logical representations with probabilistic information using Markov logic .","label":"Background","metadata":{},"score":"58.449486"}{"text":"Bos , J. ( 2008 ) .Wide - coverage semantic analysis with Boxer . 2ndConference on semantics in text processing .Available here .Garrette , D. , Erk , K. & Mooney , R. ( 2011 ) .Integrating logical representations with probabilistic information using Markov logic .","label":"Background","metadata":{},"score":"58.449486"}{"text":"I wonder if the MSE would be able to distinguish these modesl better if one were to use the solvers from the Matrix Factorization Jungle page .Which leads us to the following fleeting thought : How about picking solvers from the Jungle page and build a recursive grammar out of them ? would the exploration of the world through that grammar be competitive with the tools developed in [ 1 - 3 ] ( and for which no implementation is currently available ) ?","label":"Background","metadata":{},"score":"58.515457"}{"text":"Our results show that annotation of word types is the most important , provided a sufficiently capable semi - supervised learning infrastructure is in place to project type information onto a raw corpus .We also show that finite - state morphological analyzers are effective sources of type information when few labeled examples are available .","label":"Background","metadata":{},"score":"58.59275"}{"text":"In this dissertation , we demonstrate the efficacy of BLPs for inference and learning from incomplete data .Experimental comparison on various benchmark data sets on both tasks demonstrate the superior performance of BLPs over state - of - the - art methods .","label":"Background","metadata":{},"score":"58.610126"}{"text":"Next , text mining is employed to learn the correlations between these verbs and related objects .This knowledge is then used together with the outputs of an off - the - shelf object recognizer and the trained activity classifier to produce an improved activity recognizer .","label":"Background","metadata":{},"score":"58.832016"}{"text":"Available here .Bos , J. , Clark , S. , Curran , J.R. & Hockenmaier , J. ( 2004 ) .Wide - coverage semantic representations from a CCG parser .In Proceedings of COLING-04 , pp.1240 - 1246 , Geneva , Switzerland .","label":"Background","metadata":{},"score":"58.86232"}{"text":"Available here .Bos , J. , Clark , S. , Curran , J.R. & Hockenmaier , J. ( 2004 ) .Wide - coverage semantic representations from a CCG parser .In Proceedings of COLING-04 , pp.1240 - 1246 , Geneva , Switzerland .","label":"Background","metadata":{},"score":"58.86232"}{"text":"In order to ground the meanings of language in a real world situation , computational systems are trained with data in the form of natural language sentences paired with relevant but ambiguous perceptual contexts .With such ambiguous supervision , it is required to resolve the ambiguity between a natural language ( NL ) sentence and a corresponding set of possible logical meaning representations ( MR ) .","label":"Background","metadata":{},"score":"58.991447"}{"text":"We show that just a small amount of annotation input - even that which can be collected in just a few hours - can provide enormous advantages if we have learning algorithms that can appropriately exploit it .This work presents new algorithms , models , and approaches designed to learn grammatical information from weak supervision .","label":"Background","metadata":{},"score":"59.045998"}{"text":"ML ID : 285 .A Formal Approach to Linking Logical Form and Vector - Space Lexical Semantics [ Details ] [ PDF ] Dan Garrette , Katrin Erk , Raymond J. Mooney In Harry Bunt , Johan Bos , and Stephen Pulman , editors , Computing Meaning , 27 - -48 , Berlin , 2013 .","label":"Background","metadata":{},"score":"59.068157"}{"text":"Specifically , I will present two probabilistic generative models for learning such correspondences .The models are applied to two publicly available datasets in two different domains , sportscasting and navigation , and compared with previous work on the same data .","label":"Background","metadata":{},"score":"59.080788"}{"text":"The motivation for a combined system is to generate richer linguistic descriptions of images .Standalone vision systems are typically unable to generate linguistically rich descriptions .This approach combines abundant available language data to clean up noisy results from standalone vision systems .","label":"Background","metadata":{},"score":"59.1341"}{"text":"Previous topic .Next topic .Quick search .A Probabilistic Relational Model ( PRM ) , [ 1 ] , [ 2 ] , is a directed probabilistic graphical model used in the field of Statistical Relational Learning ( SRL ) .","label":"Background","metadata":{},"score":"59.38878"}{"text":"In this thesis , we investigate the use of ensemble learning to combine the output of existing individual Slot Filling systems .We are the first to employ Stacking , a type of ensemble learning algorithm for the task of ensembling Slot Filling systems for the KBP ESF and SFV tasks .","label":"Background","metadata":{},"score":"59.55608"}{"text":"Toward this goal , computational systems are trained with data in the form of natural language sentences paired with relevant but ambiguous perceptual contexts .With such ambiguous supervision , it is required to resolve the ambiguity between a natural language ( NL ) sentence and a corresponding set of possible logical meaning representations ( MR ) .","label":"Background","metadata":{},"score":"59.65164"}{"text":"We present a novel combination of standard activity classification , object recognition , and text mining to learn effective activity recognizers without ever explicitly labeling training videos .We cluster verbs used to describe videos to automatically discover classes of activities and produce a labeled training set .","label":"Background","metadata":{},"score":"60.255974"}{"text":"We demonstrate that our stacking approach outperforms the best system from the 2014 KBP - ESF competition as well as alternative ensembling methods employed in the 2014 KBP Slot Filler Validation task and several other ensembling baselines .Additionally , we demonstrate that including provenance information further increases the performance of stacking .","label":"Background","metadata":{},"score":"60.284904"}{"text":"Knowledge Base Population using Stacked Ensembles of Information Extractors [ Details ] [ PDF ] Vidhoon Viswanathan Masters Thesis , Department of Computer Science , The University of Texas at Austin , May 2015 .On the Proper Treatment of Quantifiers in Probabilistic Logic Semantics [ Details ] [ PDF ] [ Slides ] Islam Beltagy and Katrin Erk In Proceedings of the 11th International Conference on Computational Semantics ( IWCS-2015 ) , London , UK , April 2015 .","label":"Background","metadata":{},"score":"60.308884"}{"text":"ML ID : 291 .Real - World Semi - Supervised Learning of POS - Taggers for Low - Resource Languages [ Details ] [ PDF ] Dan Garrette and Jason Mielens and Jason Baldridge In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL-2013 ) , 583 - -592 , Sofia , Bulgaria , August 2013 .","label":"Background","metadata":{},"score":"60.377716"}{"text":"ML ID : 279 .Improving Video Activity Recognition using Object Recognition and Text Mining [ Details ] [ PDF ] [ Slides ] Tanvi S. Motwani and Raymond J. Mooney In Proceedings of the 20th European Conference on Artificial Intelligence ( ECAI-2012 ) , 600 - -605 , August 2012 .","label":"Background","metadata":{},"score":"60.45554"}{"text":"Sources of information include tag dictionaries , morphological analyzers , constituent bracketings , and partial tree annotations , as well as unannotated corpora .For example , we present algorithms that are able to combine faster - to - obtain type - level annotation with unannotated text to remove the need for slower - to - obtain token - level annotation .","label":"Background","metadata":{},"score":"60.566498"}{"text":"[Poster ] Islam Beltagy and Katrin Erk and Raymond Mooney In Proceedings of ACL 2014 Workshop on Semantic Parsing ( SP-2014 ) , 7 - -11 , Baltimore , MD , June 2014 .We propose a new approach to semantic parsing that is not constrained by a fixed formal ontology and purely logical inference .","label":"Background","metadata":{},"score":"60.566948"}{"text":"This paper presents a comparative study of probabilistic treebank parsing of German , using the Negra and TüBa - D / Z treebanks .Experiments with the Stanford parser , which uses a factored PCFG and dependency model , show that , contrary to previous claims for other parsers , lexicalization of PCFG models ... \" .","label":"Background","metadata":{},"score":"60.672676"}{"text":"This paper presents a comparative study of probabilistic treebank parsing of German , using the Negra and TüBa - D / Z treebanks .Experiments with the Stanford parser , which uses a factored PCFG and dependency model , show that , contrary to previous claims for other parsers , lexicalization of PCFG models ... \" .","label":"Background","metadata":{},"score":"60.672676"}{"text":"Objectives .understand how the syntax of natural language sentences can be modelled using a type - driven ( Combinatory ) Categorial Grammar ; .understand how a wide - coverage grammar of English can be constructed ; .understand how the meaning of natural language sentences can be modelled using a logical , model - theoretic approach ; .","label":"Background","metadata":{},"score":"60.718987"}{"text":"Objectives .understand how the syntax of natural language sentences can be modelled using a type - driven ( Combinatory ) Categorial Grammar ; .understand how a wide - coverage grammar of English can be constructed ; .understand how the meaning of natural language sentences can be modelled using a logical , model - theoretic approach ; .","label":"Background","metadata":{},"score":"60.718987"}{"text":"ML ID : 311 .Weakly - Supervised Grammar - Informed Bayesian CCG Parser Learning [ Details ] [ PDF ] [ Slides ] Dan Garrette , Chris Dyer , Jason Baldridge , Noah A. Smith In Proceedings of the Twenty - Ninth AAAI Conference on Artificial Intelligence ( AAAI-15 ) , Austin , TX , January 2015 .","label":"Background","metadata":{},"score":"60.996525"}{"text":"While a variety of semi - supervised methods exist for training from incomplete data , there are open questions regarding what types of training data should be used and how much is necessary .We discuss a series of experiments designed to shed light on such questions in the context of part - of - speech tagging .","label":"Background","metadata":{},"score":"61.008156"}{"text":"( 3 )We provide two novel ways to extend the bimodal models to support three or more modalities .We find that the three- , four- , and five - dimensional models significantly outperform models using only one or two modalities , and that nontextual modalities each provide separate , disjoint knowledge that can not be forced into a shared , latent structure .","label":"Background","metadata":{},"score":"61.081566"}{"text":"One of the key challenges in grounded language acquisition is resolving the intentions of the expressions .Typically the task involves identifying a subset of records from a list of candidates as the correct meaning of a sentence .While most current work assume complete or partial independence be- tween the records , we examine a scenario in which they are strongly related .","label":"Background","metadata":{},"score":"61.10986"}{"text":"ML ID : 301 .Probabilistic Soft Logic ( PSL ) is a recently developed framework for probabilistic logic .We use PSL to combine logical and distributional representations of natural - language meaning , where distributional information is represented in the form of weighted inference rules .","label":"Background","metadata":{},"score":"61.117455"}{"text":"Experiments show that our semantic representation can handle RTE and STS reasonably well .For the future work , our short - term goals are 1 . better RTE task representation and finite domain handling , 2 . adding more inference rules , precompiled and on - the - fly , 3 . generalizing the modified closed - world assumption , 4 . enhancing our inference algorithm for MLNs , and 5 . adding a weight learning step to better adapt the weights .","label":"Background","metadata":{},"score":"61.2306"}{"text":"To globally model parsing actions of all steps that are taken on the inpu ... \" .Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .They only determine parsing actions stepwisely by a trained classifier .","label":"Background","metadata":{},"score":"61.261192"}{"text":"To globally model parsing actions of all steps that are taken on the inpu ... \" .Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .They only determine parsing actions stepwisely by a trained classifier .","label":"Background","metadata":{},"score":"61.261192"}{"text":"Empirically , optimal k - best lists can be extracted significantly faster than with other approaches , over a range of grammar types . \" ...We present a novel approach to grammatical error correction based on Alternating Structure Optimization .As part of our work , we introduce the NUS Corpus of Learner English ( NUCLE ) , a fully annotated one million words corpus of learner English available for research purposes .","label":"Background","metadata":{},"score":"61.309803"}{"text":"Empirically , optimal k - best lists can be extracted significantly faster than with other approaches , over a range of grammar types . \" ...We present a novel approach to grammatical error correction based on Alternating Structure Optimization .As part of our work , we introduce the NUS Corpus of Learner English ( NUCLE ) , a fully annotated one million words corpus of learner English available for research purposes .","label":"Background","metadata":{},"score":"61.309803"}{"text":"The results show that all three systems achieve competitive performance , with a best labeled attachment score over 88 % .All three parsers benefit from the use of automatically derived lemmas , while morphological features seem to be less important .","label":"Background","metadata":{},"score":"61.378826"}{"text":"The results show that all three systems achieve competitive performance , with a best labeled attachment score over 88 % .All three parsers benefit from the use of automatically derived lemmas , while morphological features seem to be less important .","label":"Background","metadata":{},"score":"61.378826"}{"text":"To move beyond short video clips , I also outline models to process multi - activity movie videos , learning to jointly segment and describe coherent event sequences .I propose further extensions to take advantage of movie scripts and subtitle information to generate richer descriptions .","label":"Background","metadata":{},"score":"61.38483"}{"text":"We evaluate in two ways : first , we evaluate systems ' ability to infer held - out events from documents ( the \" Narrative Cloze \" evaluation ) ; second , we evaluate novel event inferences by collecting human judgments .","label":"Background","metadata":{},"score":"61.53162"}{"text":"With better natural language semantic representations , computers can do more applications more efficiently as a result of better understanding of natural text .However , no single semantic representation at this time fulfills all requirements needed for a satisfactory representation .","label":"Background","metadata":{},"score":"61.7854"}{"text":"We consider generative and di ... \" .Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext .","label":"Background","metadata":{},"score":"61.994976"}{"text":"We consider generative and di ... \" .Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext .","label":"Background","metadata":{},"score":"61.994976"}{"text":"The reported experiments Tools . by Kuzman Ganchev , Jennifer Gillenwater , Ben Taskar - In ACL - IJCNLP , 2009 . \" ...Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .","label":"Background","metadata":{},"score":"62.08056"}{"text":"Natural Language Semantics using Probabilistic Logic [ Details ] [ PDF ] [ Slides ] Islam Beltagy October 2014 .PhD proposal , Department of Computer Science , The University of Texas at Austin .Inclusive yet Selective : Supervised Distributional Hypernymy Detection [ Details ] [ PDF ] Stephen Roller and Katrin Erk and Gemma Boleda In Proceedings of the 25th International Conference on Computational Linguistics ( COLING 2014 ) , 1025 - -1036 , Dublin , Ireland , August 2014 .","label":"Background","metadata":{},"score":"62.20157"}{"text":"Identifying Phrasal Verbs Using Many Bilingual Corpora [ Details ] [ PDF ][Poster ] Karl Pichotta and John DeNero In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing ( EMNLP 2013 ) , 636 - -646 , Seattle , WA , October 2013 .","label":"Background","metadata":{},"score":"62.441723"}{"text":"Identifying Phrasal Verbs Using Many Bilingual Corpora [ Details ] [ PDF ][Poster ] Karl Pichotta and John DeNero In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing ( EMNLP 2013 ) , 636 - -646 , Seattle , WA , October 2013 .","label":"Background","metadata":{},"score":"62.441723"}{"text":"The generalization process is a knowledge - based analysis of the narrative 's causal structure which removes unnecessary details while maintaining the validity of the explanation .The resulting generalized set of actions is then stored as a new schema and used by the system to process narratives which were previously beyond its capabilities .","label":"Background","metadata":{},"score":"62.600918"}{"text":"Online Inference - Rule Learning from Natural - Language Extractions [ Details ] [ PDF ][Poster ] Sindhu Raghavan and Raymond J. Mooney In Proceedings of the 3rd Statistical Relational AI ( StaRAI-13 ) workshop at AAAI ' 13 , July 2013 .","label":"Background","metadata":{},"score":"62.662987"}{"text":"Much of the information conveyed in text must be inferred from what is explicitly stated since easily inferable facts are rarely mentioned .The proposed rule learner accounts for this phenomenon by learning rules in which the body of the rule contains relations that are usually explicitly stated , while the head employs a less - frequently mentioned relation that is easily inferred .","label":"Background","metadata":{},"score":"62.852318"}{"text":"This robustness led to the third best overall average labeled attachment score in the task , despite using no discriminative methods .We also demonstrate that the parser is quite fast , and can provide even faster parsing times without much loss of accuracy . \" ...","label":"Background","metadata":{},"score":"62.920372"}{"text":"This robustness led to the third best overall average labeled attachment score in the task , despite using no discriminative methods .We also demonstrate that the parser is quite fast , and can provide even faster parsing times without much loss of accuracy . \" ...","label":"Background","metadata":{},"score":"62.920372"}{"text":"This paper aims to provide some understanding and solid baseline numbers for the ... \" .Previous work on German parsing has provided confusing and conflicting results concerning the difficulty of the task and whether techniques that are useful for English , such as lexicalization , are effective for German .","label":"Background","metadata":{},"score":"63.39234"}{"text":"This paper aims to provide some understanding and solid baseline numbers for the ... \" .Previous work on German parsing has provided confusing and conflicting results concerning the difficulty of the task and whether techniques that are useful for English , such as lexicalization , are effective for German .","label":"Background","metadata":{},"score":"63.39234"}{"text":"However , such resources are extremely costly to produce , making them an unlikely option for building NLP tools in under - resourced languages or domains .This dissertation is concerned with reducing the annotation required to learn NLP models , with the goal of opening up the range of domains and languages to which NLP technologies may be applied .","label":"Background","metadata":{},"score":"63.411263"}{"text":"The section Using ProbReM illustrates the approach used in P rob R e M , and the example model is a walkthrough with an applied example .For theoretical background , we recommend Introduction to Statistical Relational Learning [ 3 ] by Lise Getoor & Ben Taskar for an excellent introduction to the different approaches introduced in the SRL field .","label":"Background","metadata":{},"score":"63.591297"}{"text":"Integrating Visual and Linguistic Information to Describe Properties of Objects [ Details ] [ PDF ] Calvin MacKenzie 2014 .Undergraduate Honors Thesis , Computer Science Department , University of Texas at Austin .Generating sentences from images has historically been performed with standalone Computer Vision systems .","label":"Background","metadata":{},"score":"63.614532"}{"text":"Our factor graph model combines these detection confidences with probabilistic knowledge mined from text corpora to estimate the most likely subject , verb , object , and place .Results on YouTube videos show that our approach improves both the joint detection of these latent , diverse sentence components and the detection of some individual components when compared to using the vision system alone , as well as over a previous n - gram language - modeling approach .","label":"Background","metadata":{},"score":"63.67919"}{"text":"Historically , unsupervised learning techniques have lacked a principled technique for selecting the number of unseen components .Research into non - parametric priors , such as the Dirichlet process , has enabled instead the use of infinite models , in which the number of hidden categories is not fixed , ... \" .","label":"Background","metadata":{},"score":"63.796257"}{"text":"Historically , unsupervised learning techniques have lacked a principled technique for selecting the number of unseen components .Research into non - parametric priors , such as the Dirichlet process , has enabled instead the use of infinite models , in which the number of hidden categories is not fixed , ... \" .","label":"Background","metadata":{},"score":"63.796257"}{"text":"For most people , watching a brief video and describing what happened ( in words ) is an easy task .For machines , extracting the meaning from video pixels and generating a sentence description is a very complex problem .The goal of my research is to develop models that can automatically generate natural language ( NL ) descriptions for events in videos .","label":"Background","metadata":{},"score":"64.06555"}{"text":"Inclusive yet Selective : Supervised Distributional Hypernymy Detection [ Details ] [ PDF ] Stephen Roller and Katrin Erk and Gemma Boleda In Proceedings of the 25th International Conference on Computational Linguistics ( COLING 2014 ) , 1025 - -1036 , Dublin , Ireland , August 2014 .","label":"Background","metadata":{},"score":"64.09555"}{"text":"In this paper , we adopt a hybrid approach that combines logic - based and distributional semantics through probabilistic logic inference in Markov Logic Networks ( MLNs ) .We focus on textual entailment ( RTE ) , a task that can utilize the strengths of both representations .","label":"Background","metadata":{},"score":"64.44116"}{"text":"However , parsing accuracies for Arabic usually lag behind non - semitic languages .Moreover , whil ...Tools . by Kuzman Ganchev , Jennifer Gillenwater , Ben Taskar - In ACL - IJCNLP , 2009 . \" ...Broad - coverage annotated treebanks necessary to train parsers do not exist for many resource - poor languages .","label":"Background","metadata":{},"score":"64.67707"}{"text":"Generative Models of Grounded Language Learning with Ambiguous Supervision [ Details ] [ PDF ] [ Slides ] Joohyun Kim Technical Report , PhD proposal , Department of Computer Science , The University of Texas at Austin , June 2012 .Fast Online Lexicon Learning for Grounded Language Acquisition [ Details ] [ PDF ] [ Slides ] David L. Chen In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics ( ACL-2012 ) , 430 - -439 , July 2012 .","label":"Background","metadata":{},"score":"64.84187"}{"text":"Using natural language to write programs is a touchstone problem for computational linguistics .We present an approach that learns to map natural - language descriptions of simple \" if - then \" rules to executable code .By training and testing on a large corpus of naturally - occurring programs ( called \" recipes \" ) and their natural language descriptions , we demonstrate the ability to effectively map language to code .","label":"Background","metadata":{},"score":"65.12254"}{"text":"Despite a recent push towards large - scale object recognition , activity recognition remains limited to narrow domains and small vocabularies of actions .In this paper , we tackle the challenge of recognizing and describing activities \" in - the - wild \" .","label":"Background","metadata":{},"score":"65.3218"}{"text":"In particular , [ 3 ] talks about performing some sort of automated recursive matrix factorization .The key word here is automated as is one explore many several composition of these factorization and see which one provides a good explanatory framework .","label":"Background","metadata":{},"score":"65.473854"}{"text":"English Slot Filling ( SF ) task .The UT Austin system builds upon the output of an existing relation extractor by augmenting relations that are explicitly stated in the text with ones that are inferred from the stated relations using probabilistic rules that encode commonsense world knowledge .","label":"Background","metadata":{},"score":"65.55647"}{"text":"Knowledge Base Construction , and 3 .Inference The input natural sentences of the RTE / STS task are mapped to logical form using Boxer which is a rule based system built on top of a CCG parser , then they are used to formulate the RTE / STS problem in probabilistic logic .","label":"Background","metadata":{},"score":"65.74687"}{"text":"Past work on learning part - of - speech taggers from tag dictionaries and raw data has reported good results , but the assumptions made about those dictionaries are often unrealistic : due to historical precedents , they assume access to information about labels in the raw and test sets .","label":"Background","metadata":{},"score":"66.496414"}{"text":"Relationships are either of type 1:n or m : n , therefore it is possible that attribute objects have multiple parent attribute objects for the same probabilistic dependency .Aggregation of the parent attribute objects is a common way to deal with this problem , P rob R e M allows to define generic aggregation functions ( avg , min , max ) .","label":"Background","metadata":{},"score":"67.25465"}{"text":"In this paper we introduce a new online algorithm that is an order of magnitude faster and surpasses the state - of - the - art results .We show that by changing the grammar of the formal meaning representation language and training on additional data collected from Amazon 's Mechanical Turk we can further improve the results .","label":"Background","metadata":{},"score":"67.637405"}{"text":"Lexicalized parsing focuses on identifying dependencies .w.o .GFs with GFs TüBa - D / Z 2611 2610 2197 Tiger 2535 2534 1592 T ..Existing k - best extraction methods can efficiently search for top derivations , but only after an exhaustive 1-best pass .","label":"Background","metadata":{},"score":"67.696304"}{"text":"Lexicalized parsing focuses on identifying dependencies .w.o .GFs with GFs TüBa - D / Z 2611 2610 2197 Tiger 2535 2534 1592 T ..Existing k - best extraction methods can efficiently search for top derivations , but only after an exhaustive 1-best pass .","label":"Background","metadata":{},"score":"67.696304"}{"text":"ML ID : 272 .Fast Online Lexicon Learning for Grounded Language Acquisition [ Details ] [ PDF ] [ Slides ] David L. Chen In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics ( ACL-2012 ) , 430 - -439 , July 2012 .","label":"Background","metadata":{},"score":"67.70331"}{"text":"( 2013 ) to handle word - level code - switching between multiple languages .Further , we enable our system to handle spelling variability , including now - obsolete shorthand systems used by printers .Our results show average relative character error reductions of 14 % across a variety of historical texts .","label":"Background","metadata":{},"score":"67.77524"}{"text":"Text Analysis Conference conducts English Slot Filling ( ESF ) and Slot Filler Validation ( SFV ) tasks as part of its KBP track to promote research in this area .Slot Filling systems are developed to do relation extraction for specific relation and entity types .","label":"Background","metadata":{},"score":"68.18585"}{"text":"Described video datasets are scarce , and most existing methods have been applied to toy domains with a small vocabulary of possible words .By transferring knowledge from 1.2M+ images with category labels and 100,000 + images with captions , our method is able to create sentence descriptions of open - domain videos with large vocabularies .","label":"Background","metadata":{},"score":"68.2421"}{"text":"The system processes short English narratives and from a single narrative acquires a new schema for a stereotypical set of actions .During the understanding process , the system constructs explanations for characters ' actions in terms of the goals they were meant to achieve .","label":"Background","metadata":{},"score":"68.51633"}{"text":"In plan recognition , the underlying cause or the top - level plan that resulted in the observed actions is not known or observed .Further , only a subset of the executed actions can be observed by the plan recognition system resulting in partially observed data .","label":"Background","metadata":{},"score":"68.734474"}{"text":"most languages are projective .In Figure 8 An example Chinese dependency tree .Although non - projec ... . \" ...Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .","label":"Background","metadata":{},"score":"68.79974"}{"text":"most languages are projective .In Figure 8 An example Chinese dependency tree .Although non - projec ... . \" ...Deterministic dependency parsers use parsing actions to construct dependencies .These parsers do not compute the probability of the whole dependency tree .","label":"Background","metadata":{},"score":"68.79974"}{"text":"Latent Variable Models of Distributional Lexical Semantics [ Details ] [ PDF ] Joseph Reisinger PhD Thesis , Department of Computer Science , University of Texas at Austin , May 2012 .In order to respond to increasing demand for natural language interfaces - and provide meaningful insight into user query intent - fast , scalable lexical semantic models with flexible representations are needed .","label":"Background","metadata":{},"score":"68.93025"}{"text":"This paper presents an approach for detecting promotional content in Wikipedia .By incorporating stylometric features , including features based on n - gram and PCFG language models , we demonstrate improved accuracy at identifying promotional articles , compared to using only lexical information and meta - features .","label":"Background","metadata":{},"score":"69.15698"}{"text":"Our experiments show that our approach outperforms two baselines trained on non - learner text and learner text , respectively .Our approach also outperforms two commercial grammar checking software packages . \" ...We investigate whether wording , stylistic choices , and online behavior can be used to predict the age category of blog authors .","label":"Background","metadata":{},"score":"69.51222"}{"text":"Our experiments show that our approach outperforms two baselines trained on non - learner text and learner text , respectively .Our approach also outperforms two commercial grammar checking software packages . \" ...We investigate whether wording , stylistic choices , and online behavior can be used to predict the age category of blog authors .","label":"Background","metadata":{},"score":"69.51222"}{"text":"We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets ( M - VAD and MPII - MD ) .ML ID : 319 .We present results on using stacking to ensemble multiple systems for the Knowledge Base Population English Slot Filling ( KBP - ESF ) task .","label":"Background","metadata":{},"score":"69.654465"}{"text":"ML ID : 274 .Generative Models of Grounded Language Learning with Ambiguous Supervision [ Details ] [ PDF ] [ Slides ] Joohyun Kim Technical Report , PhD proposal , Department of Computer Science , The University of Texas at Austin , June 2012 . \"","label":"Background","metadata":{},"score":"69.66372"}{"text":"ML ID : 253 .Generalizing Explanations of Narratives into Schemata [ Details ] [ PDF ] Raymond J. Mooney In Proceedings of the Third International Machine Learning Workshop , 126 - -128 , New Brunswick , New Jersey , 1985 .","label":"Background","metadata":{},"score":"69.72575"}{"text":"ML ID : 325 .Statistical Script Learning with Recurrent Neural Nets [ Details ] [ PDF ] [ Slides ] Karl Pichotta December 2015 .PhD proposal , Department of Computer Science , The University of Texas at Austin .Statistical Scripts are probabilistic models of sequences of events .","label":"Background","metadata":{},"score":"69.93898"}{"text":"Springer .Latent Variable Models of Distributional Lexical Semantics [ Details ] [ PDF ] Joseph Reisinger PhD Thesis , Department of Computer Science , University of Texas at Austin , May 2012 .Bayesian Logic Programs for Plan Recognition and Machine Reading [ Details ] [ PDF ] [ Slides ] Sindhu Raghavan PhD Thesis , Department of Computer Science , University of Texas at Austin , December 2012 .","label":"Background","metadata":{},"score":"70.34059"}{"text":"University of Texas at Austin KBP 2013 Slot Filling System : Bayesian Logic Programs for Textual Inference [ Details ] [ PDF ] Yinon Bentor and Amelia Harrison and Shruti Bhosale and Raymond Mooney In Proceedings of the Sixth Text Analysis Conference ( TAC 2013 ) , 2013 .","label":"Background","metadata":{},"score":"70.839134"}{"text":"Plan recognition is the task of predicting an agent 's top - level plans based on its observed actions .It is an abductive reasoning task that involves inferring cause from effect .In the first part of the dissertation , we develop an approach to abductive plan recognition using BLPs .","label":"Background","metadata":{},"score":"71.42773"}{"text":"We evaluate this semantic representation on two tasks , Recognizing Textual Entailment ( RTE ) and Semantic Textual Similarity ( STS ) .Doing RTE and STS better is an indication of a better semantic understanding .Our system has three main components , 1 .","label":"Background","metadata":{},"score":"71.962425"}{"text":"Grounded Language Learning Models for Ambiguous Supervision [ Details ] [ PDF ] [ Slides ] Joo Hyun Kim PhD Thesis , Department of Computer Science , University of Texas at Austin , December 2013 .Communicating with natural language interfaces is a long - standing , ultimate goal for artificial intelligence ( AI ) agents to pursue , eventually .","label":"Background","metadata":{},"score":"71.97635"}{"text":"We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data .Results indicate that ( a ) MST - parser performs better on Hebrew data than Malt - Parser , and ( b ) both parsers do not make good use of morphological information when parsing Hebrew . ... s on Hebrew dependency parsing .","label":"Background","metadata":{},"score":"72.10959"}{"text":"We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data .Results indicate that ( a ) MST - parser performs better on Hebrew data than Malt - Parser , and ( b ) both parsers do not make good use of morphological information when parsing Hebrew . ... s on Hebrew dependency parsing .","label":"Background","metadata":{},"score":"72.10959"}{"text":"However , much of the information conveyed in text must be inferred from what is explicitly stated since easily inferable facts are rarely mentioned .Human readers naturally use common sense knowledge and \" read between the lines \" to infer such implicit information from the explicitly stated facts .","label":"Background","metadata":{},"score":"72.92444"}{"text":"ML ID : 320 .Real - world videos often have complex dynamics ; and methods for generating open - domain video descriptions should be sensitive to temporal structure and allow both input ( sequence of frames ) and output ( sequence of words ) of variable length .","label":"Background","metadata":{},"score":"73.520584"}{"text":"Several real world tasks involve data that is uncertain and relational in nature .Traditional approaches like first - order logic and probabilistic models either deal with structured data or uncertainty , but not both .To address these limitations , statistical relational learning ( SRL ) , a new area in machine learning integrating both first - order logic and probabilistic graphical models , has emerged in the recent past .","label":"Background","metadata":{},"score":"73.96353"}{"text":"ML ID : 271 .Learning Language from Ambiguous Perceptual Context [ Details ] [ PDF ] [ Slides ] David L. Chen PhD Thesis , Department of Computer Science , University of Texas at Austin , May 2012 .Building a computer system that can understand human languages has been one of the long - standing goals of artificial intelligence .","label":"Background","metadata":{},"score":"74.646484"}{"text":"Our polyglot ranking approach integrates frequency statistics from translated corpora in 50 different languages .Our experimental evaluation demonstrates that combining statistical evidence from many parallel corpora using a novel ranking - oriented boosting algorithm produces a comprehensive set of English phrasal verbs , achieving performance comparable to a human - curated set .","label":"Background","metadata":{},"score":"74.78716"}{"text":"Adapting Discriminative Reranking to Grounded Language Learning [ Details ] [ PDF ] [ Slides ] Joohyun Kim and Raymond J. Mooney In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL-2013 ) , 218 - -227 , Sofia , Bulgaria , August 2013 .","label":"Background","metadata":{},"score":"74.81061"}{"text":"It is hard to draw a line between the core functionality of the framework and the functionality needed by a specific P rob R e M project .For this reason the source code will be provided as a folder and the framework has to be added to the PYTHONPATH of the P rob R e M project .","label":"Background","metadata":{},"score":"75.14577"}{"text":"By only observing how humans follow navigation instructions , the system was able to infer the corresponding hidden navigation plans and parse previously unseen instructions in new environments for both English and Chinese data .With the rise in popularity of crowdsourcing , we also present results on collecting additional training data using Amazon 's Mechanical Turk .","label":"Background","metadata":{},"score":"75.84429"}{"text":"The system does not use any prior language knowledge and was able to learn to sportscast in both English and Korean .Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans .","label":"Background","metadata":{},"score":"76.46667"}{"text":"ML ID : 316 .Knowledge Base Population using Stacked Ensembles of Information Extractors [ Details ] [ PDF ] Vidhoon Viswanathan Masters Thesis , Department of Computer Science , The University of Texas at Austin , May 2015 .The performance of relation extractors plays a significant role in automatic creation of knowledge bases from web corpus .","label":"Background","metadata":{},"score":"76.48918"}{"text":"If you actually liked it , here 's some good news : There are dot - to - dot books for gr ... .Why GenICam 3.0 deserves your attention - Some engineers love standards .They get all excited about reading them , and positively orgasmic when offered the chance to be on a standards committee .","label":"Background","metadata":{},"score":"76.6626"}{"text":"ML ID : 304 .Using Markov logic to integrate logical and distributional information in natural - language semantics results in complex inference problems involving long , complicated formulae .Current inference methods for Markov logic are ineffective on such problems .","label":"Background","metadata":{},"score":"76.883095"}{"text":"We present a novel approach to grammatical error correction based on Alternating Structure Optimization .As part of our work , we introduce the NUS Corpus of Learner English ( NUCLE ) , a fully annotated one million words corpus of learner English available for research purposes .","label":"Background","metadata":{},"score":"76.97136"}{"text":"We present a novel approach to grammatical error correction based on Alternating Structure Optimization .As part of our work , we introduce the NUS Corpus of Learner English ( NUCLE ) , a fully annotated one million words corpus of learner English available for research purposes .","label":"Background","metadata":{},"score":"76.97136"}{"text":"Let 's take for example Robust PCA . a factorization that is usually used for background subtraction work : what if after the decomposition of a data matrix M through robust PCA , e.g. .It will not have escaped the faithful reader of Nuit Blanche that Yi Ma and co - authors ( see [ 4 ] ) are building these models from scratch with an approach that is grounded in theoretical justification for the use of these transforms .","label":"Background","metadata":{},"score":"77.06731"}{"text":"Publications : Natural Language Processing .Natural Language Processing is a broad area that includes various approaches to building computational systems that understand and generate language , as well as categorization and analysis of text documents , and cognitive models of human language processing .","label":"Background","metadata":{},"score":"77.96387"}{"text":"Most work on weakly - supervised learning for part - of - speech taggers has been based on unrealistic assumptions about the amount and quality of training data .For this paper , we attempt to create true low - resource scenarios by allowing a linguist just two hours to annotate data and evaluating on the languages Kinyarwanda and Malagasy .","label":"Background","metadata":{},"score":"78.5715"}{"text":"understand how this approach to meaning can be combined with probabilistic inference ; . gain an appreciation of how syntactic and semantic theory can be implemented in practice .Assessment .Four ticked take - home tests or short practicals .Each ticked test is worth 5 % of the final assessment for the course .","label":"Background","metadata":{},"score":"78.606476"}{"text":"understand how this approach to meaning can be combined with probabilistic inference ; . gain an appreciation of how syntactic and semantic theory can be implemented in practice .Assessment .Four ticked take - home tests or short practicals .Each ticked test is worth 5 % of the final assessment for the course .","label":"Background","metadata":{},"score":"78.606476"}{"text":"[Poster ] Sindhu Raghavan and Raymond J. Mooney In Proceedings of the 3rd Statistical Relational AI ( StaRAI-13 ) workshop at AAAI ' 13 , July 2013 .Adapting Discriminative Reranking to Grounded Language Learning [ Details ] [ PDF ] [ Slides ] Joohyun Kim and Raymond J. Mooney In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL-2013 ) , 218 - -227 , Sofia , Bulgaria , August 2013 .","label":"Background","metadata":{},"score":"79.356415"}{"text":"These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .We present a new measure , Neutral Edge Direction ( NED ) , and show that it greatly reduces this undesired phenomenon .","label":"Background","metadata":{},"score":"79.81302"}{"text":"These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .We present a new measure , Neutral Edge Direction ( NED ) , and show that it greatly reduces this undesired phenomenon .","label":"Background","metadata":{},"score":"79.81302"}{"text":"Experiments with the Stanford parser , which uses a factored PCFG and dependency model , show that , contrary to previous claims for other parsers , lexicalization of PCFG models boosts parsing performance for both treebanks .The experiments also show that there is a big difference in parsing performance , when trained on the Negra and on the TüBa - D / Z treebanks .","label":"Background","metadata":{},"score":"80.10419"}{"text":"Experiments with the Stanford parser , which uses a factored PCFG and dependency model , show that , contrary to previous claims for other parsers , lexicalization of PCFG models boosts parsing performance for both treebanks .The experiments also show that there is a big difference in parsing performance , when trained on the Negra and on the TüBa - D / Z treebanks .","label":"Background","metadata":{},"score":"80.10419"}{"text":"The proposed .It learns sensible structures for datasets as diverse as image patches , motion capture , 20 Questions , and U.S. Senate votes , all using exactly the same code .Silicon - based millime ... .The State of Probabilistic Programming - For two weeks last July , I cocooned myself in a hotel in Portland , OR , living and breathing probabilistic programming as a \" student \" in the probabilistic p ..","label":"Background","metadata":{},"score":"80.56759"}{"text":"ML ID : 269 .Building a Persistent Workforce on Mechanical Turk for Multilingual Data Collection [ Details ] [ PDF ] [ Slides ] David L. Chen and William B. Dolan In Proceedings of The 3rd Human Computation Workshop ( HCOMP 2011 ) , August 2011 .","label":"Background","metadata":{},"score":"81.24685"}{"text":"ML ID : 317 .Representing Meaning with a Combination of Logical Form and Vectors [ Details ] [ PDF ] Islam Beltagy and Stephen Roller and Pengxiang Cheng and Katrin Erk and Raymond J. Mooney arXiv preprint arXiv:1505.06816 [ cs .","label":"Background","metadata":{},"score":"81.739456"}{"text":"We present the results of one of the largest linguistic data collection efforts to date using Mechanical Turk , yielding 85 K English sentences and more than 1k sentences for each of a dozen more languages .ML ID : 265 .","label":"Background","metadata":{},"score":"82.24628"}{"text":"Building a Persistent Workforce on Mechanical Turk for Multilingual Data Collection [ Details ] [ PDF ] [ Slides ] David L. Chen and William B. Dolan In Proceedings of The 3rd Human Computation Workshop ( HCOMP 2011 ) , August 2011 .","label":"Background","metadata":{},"score":"83.128044"}{"text":"The attributes used were split into two categories : color attributes and other attributes .Our proposed model was found to be statistically significantly more accurate than the vision system alone for both sets of attributes .ML ID : 302 .","label":"Background","metadata":{},"score":"83.82794"}{"text":"ML ID : 315 .Solving the visual symbol grounding problem has long been a goal of artificial intelligence .The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images .","label":"Background","metadata":{},"score":"84.09083"}{"text":"Our experimental evaluation proves that Stacking is useful for ensembling SF systems .We demonstrate new state - of - the - art results for KBP ESF task .Our proposed system achieves an F1 score of 47 .Given the complexity of developing Slot Filling systems from scratch , our promising results indicate that performance on Slot Filling tasks can be increased by ensembling existing systems in shorter timeframe .","label":"Background","metadata":{},"score":"84.310974"}{"text":"ML ID : 305 .This paper integrates techniques in natural language processing and computer vision to improve recognition and description of entities and activities in real - world videos .We propose a strategy for generating textual descriptions of videos by using a factor graph to combine visual detections with language statistics .","label":"Background","metadata":{},"score":"84.51125"}{"text":"Through experimentation with a range of y ... \" .We investigate whether wording , stylistic choices , and online behavior can be used to predict the age category of blog authors .Our hypothesis is that significant changes in writing style distinguish pre - social media bloggers from post - social media bloggers .","label":"Background","metadata":{},"score":"84.945984"}{"text":"Through experimentation with a range of y ... \" .We investigate whether wording , stylistic choices , and online behavior can be used to predict the age category of blog authors .Our hypothesis is that significant changes in writing style distinguish pre - social media bloggers from post - social media bloggers .","label":"Background","metadata":{},"score":"84.945984"}{"text":"Available here .© 2015 Computer Laboratory , University of Cambridge Information provided by Prof Ted Briscoe Syntax and Semantics of Natural Language .Principal lecturers : Prof Ted Briscoe , Dr Stephen Clark Taken by : MPhil ACS , Part III Code : L107 Hours : 16 Prerequisites : L100 Introduction to Natural Language Processing and R07 Introductory Logic for students who have not taken a course in logic before .","label":"Background","metadata":{},"score":"85.70953"}{"text":"We also show that internet writing characteristics are important features for age prediction , but that lexical content is also needed to produce significantly more accurate results .Our best results allow for 81.57 % accuracy . by Marie C , Benoît Crabbé , Djamé Seddah , Université Paris , Université Paris . \" ...","label":"Background","metadata":{},"score":"86.2932"}{"text":"We also show that internet writing characteristics are important features for age prediction , but that lexical content is also needed to produce significantly more accurate results .Our best results allow for 81.57 % accuracy . by Marie C , Benoît Crabbé , Djamé Seddah , Université Paris , Université Paris . \" ...","label":"Background","metadata":{},"score":"86.2932"}{"text":"We examine the performance of three techniques on three treebanks ( Negra , Tiger , and TüBa - D / Z ) : ( i ) Markovization , ( ii ) lexicalization , and ( iii ) state splitting .We additionally explore parsing with the inclusion of grammatical function information .","label":"Background","metadata":{},"score":"88.03232"}{"text":"We examine the performance of three techniques on three treebanks ( Negra , Tiger , and TüBa - D / Z ) : ( i ) Markovization , ( ii ) lexicalization , and ( iii ) state splitting .We additionally explore parsing with the inclusion of grammatical function information .","label":"Background","metadata":{},"score":"88.03232"}{"text":"Finally , we propose investigating the interface between models of event co - occurrence and coreference resolution , in particular by integrating script information into general coreference systems .ML ID : 326 .Natural Language Video Description using Deep Recurrent Neural Networks [ Details ] [ PDF ] [ Slides ] Subhashini Venugopalan November 2015 .","label":"Background","metadata":{},"score":"89.201675"}{"text":"Statistical Script Learning with Recurrent Neural Nets [ Details ] [ PDF ] [ Slides ] Karl Pichotta December 2015 .PhD proposal , Department of Computer Science , The University of Texas at Austin .Natural Language Video Description using Deep Recurrent Neural Networks [ Details ] [ PDF ] [ Slides ] Subhashini Venugopalan November 2015 .","label":"Background","metadata":{},"score":"89.44255"}{"text":"ML ID : 313 .Transcribing documents from the printing press era , a challenge in its own right , is more complicated when documents interleave multiple languages - a common feature of 16th century texts .Additionally , many of these documents precede consistent orthographic conventions , making the task even harder .","label":"Background","metadata":{},"score":"90.77465"}{"text":"One final take - home exam covering all the material taken at beginning of Easter Term .Final take - home exam will contribute 80 % to the final assessment .Questions set and marked by Professor Briscoe and Dr Clark .","label":"Background","metadata":{},"score":"92.420265"}{"text":"One final take - home exam covering all the material taken at beginning of Easter Term .Final take - home exam will contribute 80 % to the final assessment .Questions set and marked by Professor Briscoe and Dr Clark .","label":"Background","metadata":{},"score":"92.420265"}{"text":"Using these models , natural language systems will be able to infer a more comprehensive semantic relations , which in turn may yield improved systems for question answering , text classification , machine translation , and information retrieval .ML ID : 309 .","label":"Background","metadata":{},"score":"93.58121"}{"text":"Available here .© 2013 Computer Laboratory , University of Cambridge Information provided by Prof Ted Briscoe Publications : Natural Language Processing .Natural Language Processing is a broad area that includes various approaches to building computational systems that understand and generate language , as well as categorization and analysis of text documents , and cognitive models of human language processing .","label":"Background","metadata":{},"score":"97.31364"}{"text":"This comparison at least suggests that German is not harder to parse than its West - Germanic neighbor language English . ... ing an appropriate parsing model for German .Section 3 introduces the Negra and TüBa - D / Z treebanks and 2 German is not the first language for which this question has been raised .","label":"Background","metadata":{},"score":"98.350975"}{"text":"This comparison at least suggests that German is not harder to parse than its West - Germanic neighbor language English . ... ing an appropriate parsing model for German .Section 3 introduces the Negra and TüBa - D / Z treebanks and 2 German is not the first language for which this question has been raised .","label":"Background","metadata":{},"score":"98.350975"}{"text":"For instance , 14.4 % of section 23 is tagged differently by ( 1 ) and ( 2 ) 8 .5 The Neutral Edge Direction ( NED ) Me ... . by Shay B. Cohen , Noah A. Smith , Alex Clark , Dorota Glowacka , Colin De La Higuera , Mark Johnson , John Shawe - taylor . \" ...","label":"Background","metadata":{},"score":"100.339294"}{"text":"For instance , 14.4 % of section 23 is tagged differently by ( 1 ) and ( 2 ) 8 .5 The Neutral Edge Direction ( NED ) Me ... . by Shay B. Cohen , Noah A. Smith , Alex Clark , Dorota Glowacka , Colin De La Higuera , Mark Johnson , John Shawe - taylor . \" ...","label":"Background","metadata":{},"score":"100.339294"}{"text":"Verlag , 2000 .","label":"Background","metadata":{},"score":"114.01963"}