{"text":"This model can be estimated from a single training sequence , yet shares statistical strength between subse- quent symbol predictive distributions in such a way that predictive performance general- izes well .The model builds on a specific pa- rameterization of an unbounded - depth hier- archical Pitman - Yor process .","label":"Uses","metadata":{},"score":"55.46546"}{"text":"( Teh 2006 ) such that a latent shared , non - domain - specific language model as well as domain specific models are esti- mated together .We call the resulting model the dou- bly hierarchical Pitman - Yor process language model ( DHPYLM ) ( Section 3 ) .","label":"Uses","metadata":{},"score":"57.101433"}{"text":"Typically , preprocessing , normalization , feature extraction , classification , and postprocessing operations are required .We 'll survey the state of the art , analyze recent trends , and try to identify challenges for future research in this field . .","label":"Uses","metadata":{},"score":"61.59942"}{"text":"The most commonly used language models are very simple ( e.g. a Katz - smoothed trigram model ) .There are many improvements over this simple model however , including caching , clustering , higherorder n - grams , skipping models , and sentence - mixture models , all of which we will describe below .","label":"Uses","metadata":{},"score":"61.632877"}{"text":"The most commonly used language models are very simple ( e.g. a Katz - smoothed trigram model ) .There are many improvements over this simple model however , including caching , clustering , higherorder n - grams , skipping models , and sentence - mixture models , all of which we will describe below .","label":"Uses","metadata":{},"score":"61.632877"}{"text":"We use the first heuristic to develop a novel class - based language model that outperforms a baseline word trigram model by 28 % in perplexity and 1.9 % absolute in speech recognition word - error rate on Wall Street Journal data .","label":"Uses","metadata":{},"score":"62.295494"}{"text":"Related articles Cite Save .Sentence simplification as tree transduction D Feblowitz , D Kauchak - Proc . of the Second Workshop on Predicting ... , 2013 - aclweb.org ...The probability of the output tree 's yield , as given by an n - gram language model trained on the simple side of the training corpus using the IRSTLM Toolkit ( Federico et al . , 2008 ) .","label":"Uses","metadata":{},"score":"62.347363"}{"text":"The result is compiled models that are larger than the training models , but execute at 2 million characters per second on a desktop PC .Cross - entropy on held - out data shows these models to be state of the art in terms of performance . ... piled to a less compact but more efficient static representation . \" ...","label":"Uses","metadata":{},"score":"62.368477"}{"text":"Two decades of statistical language model- ing : where do we go from here ?In Proceedings of the IEEE , volume 88 , pages 1270 - 1278 , 2000 .SOUCorpus . Y. W. Teh .A hierarchical Bayesian language model based on Pitman - Yor processes .","label":"Uses","metadata":{},"score":"62.409584"}{"text":"Furthermore , we do not have simple frequency cut - offs .Parsimonious language m .. \" ...Language modeling is the art of determining the probability of a sequence of words .This is useful in a large variety of areas including speech recognition , optical character recognition , handwriting recognition , machine translation , and spelling correction ( Church , 1988 ; Brown et al . , 1990 ; Hull , 1 ... \" .","label":"Uses","metadata":{},"score":"62.703598"}{"text":"Furthermore , we do not have simple frequency cut - offs .Parsimonious language m .. \" ...Language modeling is the art of determining the probability of a sequence of words .This is useful in a large variety of areas including speech recognition , optical character recognition , handwriting recognition , machine translation , and spelling correction ( Church , 1988 ; Brown et al . , 1990 ; Hull , 1 ... \" .","label":"Uses","metadata":{},"score":"62.703598"}{"text":"Additionally , we investigate several ways of deriving continuous word representations for unknown words from those of known words .The resulting model significantly reduces perplexity on sparse - data tasks when compared to standard backoff models , standard neural language models , and factored language models .","label":"Uses","metadata":{},"score":"64.173256"}{"text":"2000 ; Lafferty et al .2001 ) .Even hierarchical Bayesian models have been applied to language modelling - MacKay and Peto ( 1994 ) have proposed one based on Dirichlet distributions .Our model is a natural generalizat ... \" ...","label":"Uses","metadata":{},"score":"64.60204"}{"text":"The system proposed in this paper uses state - of - the - art normalization and feat ... \" .In this paper we present an on - line recognition sys - tem for handwritten texts acquired from a whiteboard .This input modality has received relatively little atten - tion in the handwriting recognition community in the past .","label":"Uses","metadata":{},"score":"64.87627"}{"text":"Full - text preview .ucl.ac.uk Abstract In this paper we present a doubly hierarchi- cal Pitman - Yor process language model .Its bottom layer of hierarchy consists of multi- ple hierarchical Pitman - Yor process language models , one each for some number of do- mains .","label":"Uses","metadata":{},"score":"65.319046"}{"text":"Scalable Modified Kneser - Ney Language Model Estimation . ...SRILM and IRSTLM were run un- til the test machine ran out of RAM ( 64 GB ) . ...Cited by 26 Related articles All 9 versions Cite Save More .","label":"Uses","metadata":{},"score":"65.62306"}{"text":"Additional preprocessing techniques are intro - duced , which significantly increase the word recognition rate .For classification , Hidden Markov Models are used together with a statistical language model .In writer inde - pendent experiments we achieved word recognition rates of 67.3 % on the test set when no language model is used , and 70.8 % by including a language model .","label":"Uses","metadata":{},"score":"65.72961"}{"text":"Related articles All 3 versions Cite Save .Dynamically Shaping the Reordering Search Space of Phrase - Based Statistical Machine Translation .A Bisazza , M Federico - TACL , 2013 - transacl.org ...As proposed by Johnson et al .( 2007 ) , statistically improbable phrase pairs are removed from the translation model .","label":"Uses","metadata":{},"score":"65.81651"}{"text":"Page 7 .286 L. Wang et al . built using GIZA++ [ 26 ] and the training script of Moses .A 5-gram language model was trained using the IRSTLM toolkit [ 27 ] , exploiting improved Modified Kneser- Ney smoothing , and quantizing both , probabilities and back - off weights .","label":"Uses","metadata":{},"score":"65.92711"}{"text":"We conclude that for both small and moderately sized tasks , we obtain new state of the art results with combination of models , that is significantly better than performance of any individual model .Obtained perplexity reductions against Good - Turing trigram baseline are over 50 % and against modified Kneser - Ney smoothed 5-gram over 40 % .","label":"Uses","metadata":{},"score":"66.108"}{"text":"In ( Chen , 2009 ) , we show that for a variety of language models belonging to the exponential family , the test set cross - entropy of a model can be accurately predicted from its training set cross - entropy and its parameter values .","label":"Uses","metadata":{},"score":"66.35657"}{"text":"We built a trigram language model using the IRSTLM lan- guage modeling toolkit ( Federico et al . , 2008 ) .The advantage of this language model was that it con- tained both MSA and dialectal text .IRSTLM : an open source toolkit for handling large scale language models . ...","label":"Uses","metadata":{},"score":"66.51132"}{"text":"Related articles All 4 versions Cite Save .Improving Word Translation Disambiguation by Capturing Multiword Expressions with Dictionaries L Bungum , B Gambäck , A Lynum , E Marsi - NAACL HLT 2013 , 2013 - aclweb.org ... performance .The n - gram models were built using the IRSTLM toolkit ( Federico et al . , 2008 ; Bungum and Gambäck , 2012 ) on the DeWaC corpus ( Baroni and Kilgarriff , 2006 ) , using the stopword list from NLTK ( Loper and Bird , 2002 ) . ...","label":"Uses","metadata":{},"score":"66.6077"}{"text":"The toolkit supports creation and evaluation of a variety of language model types based on N - gram statistics , as well as several related tasks , such as statistical tagging and manipulation of N - best lists and word lattices .","label":"Uses","metadata":{},"score":"66.645325"}{"text":"The toolkit supports creation and evaluation of a variety of language model types based on N - gram statistics , as well as several related tasks , such as statistical tagging and manipulation of N - best lists and word lattices .","label":"Uses","metadata":{},"score":"66.645325"}{"text":"The baseline language model in t ..Tools . \" ...We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .","label":"Uses","metadata":{},"score":"67.01494"}{"text":"The baseline language model in t ..A Hierarchical Nonparametric Bayesian Approach to Statistical Language Model Domain Adaptation .In this paper we present a doubly hierarchi- cal Pitman - Yor process language model .Its bottom layer of hierarchy consists of multi- ple hierarchical Pitman - Yor process language models , one each for some number of do- mains .","label":"Uses","metadata":{},"score":"67.16092"}{"text":"Language Models are used to represent the content typically associated with different readability levels .Reading level classifiers are created as linear combinations of a language model and surface linguistic features .Experiments show that this new method is more accurate than the widely used Flesch - Kincaid readability formula KEYWORDS Readability , Flesch - Kincaid , Unigram Language Model , EM .","label":"Uses","metadata":{},"score":"67.22275"}{"text":"Y. W. Teh , M. I. Jordan , M. J. Beal , and D. M. Blei .Hi- erarchical Dirichlet processes .Journal of the American Statistical Association , 101(476):1566 - 1581 , 2006 .X. Zhu and R. Rosenfeld .modeling with the world wide web .","label":"Uses","metadata":{},"score":"67.22967"}{"text":"Language models based on a continuous word representation and neural network probability estimation have recently emerged as an alternative to the established backoff language models .At the same time , factored language models have been developed that use additional word information ( such as parts - of - speech , morphological classes , and syntactic features ) in conjunction with refined back - off strategies .","label":"Uses","metadata":{},"score":"67.788216"}{"text":"Differing provisions from the publisher 's actual policy or licence agreement may be applicable .\" Experiments on the AP News corpus showed that the novel hierarchical Pitman - Yor process language model produces results superior to hierarchical Dirichlet language models and n - gram LMs smoothed by interpolated Kneser - Ney ( IKN ) , and comparable to those smoothed by modified Kneser - Ney ( MKN ) [ 12].","label":"Uses","metadata":{},"score":"67.89005"}{"text":"..the interpolation weights are estimated by using the EM algorithm .4.4 Language model pruning Our system can produce an SLM given a memory constraint .The basic idea is to remove as many useless probabilities as possible without increasing the perplexity .","label":"Uses","metadata":{},"score":"68.01259"}{"text":"..the interpolation weights are estimated by using the EM algorithm .4.4 Language model pruning Our system can produce an SLM given a memory constraint .The basic idea is to remove as many useless probabilities as possible without increasing the perplexity .","label":"Uses","metadata":{},"score":"68.01259"}{"text":"Cache models work by interpolating simple language models derived from the recent context with more elaborate , context - independent models .We use the SRILM toolkit [ 28 ] to compute n - gram langua ... jleibund .exprjs - v0.0.3 - expression language for javascript ( in javascript ) like ognl .","label":"Uses","metadata":{},"score":"68.22047"}{"text":"Common practice uses lowerorder entries in an N - gram model to score the first few words of a fragment ; this violates assumptions made by common smoothing strategies , including Kneser - Ney .Instead , we use a unigram model to score the first word , a bigram for the second , etc .","label":"Uses","metadata":{},"score":"68.36605"}{"text":"We investigate a fractional Kneser - Ney smoothing approach to handle . \" ...Language model plays an important role in statistical machine translation systems .It is the key knowledge source to determine the right word order of the translation .","label":"Uses","metadata":{},"score":"68.412224"}{"text":"We show how to perform inference in such a model without truncation approximation and introduce fragmentation operators nec- essary to do predictive inference .We demon- strate the sequence memoizer by using it as a language model , achieving state - of - the - art results .","label":"Uses","metadata":{},"score":"68.47733"}{"text":"Intuitively this sharing results in the \" adaptation \" of a latent shared language model to each domain .We intro- duce a general formalism capable of describ- ing the overall model which we call the graph- ical Pitman - Yor process and explain how to perform Bayesian inference in it .","label":"Uses","metadata":{},"score":"68.52655"}{"text":"Intuitively this sharing results in the \" adaptation \" of a latent shared language model to each domain .We intro- duce a general formalism capable of describ- ing the overall model which we call the graph- ical Pitman - Yor process and explain how to perform Bayesian inference in it .","label":"Uses","metadata":{},"score":"68.52655"}{"text":"For applications that are tolerant of a certain class of relatively innocuous errors ( where unseen n - grams may be accepted as rare n - grams ) , we can reduce the latter cost to below 1 byte per n - gram . ... de the use of entropy pruning techniques ( Stolcke , 1998 ) or clustering ( Jelinek et al . , 1990 ; Goodman and Gao , 2000 ) to reduce the number of n - grams that must be stored .","label":"Uses","metadata":{},"score":"68.5341"}{"text":"Overall increasing the training data size from 360h to 2200h and optimising the training procedure reduced the word error rate on the DARPA / NIST 2003 eval set by about 20 % relative . ...n - grams are trained on different text corpora and then interpolated together with the interpolation weights optimised on a development test set .","label":"Uses","metadata":{},"score":"68.925095"}{"text":"Overall increasing the training data size from 360h to 2200h and optimising the training procedure reduced the word error rate on the DARPA / NIST 2003 eval set by about 20 % relative . ...n - grams are trained on different text corpora and then interpolated together with the interpolation weights optimised on a development test set .","label":"Uses","metadata":{},"score":"68.925095"}{"text":"Our first contribution is the development of a sensible construction for it .Our second contribu- tion is the development of a new class of nonparametric Bayesian models which we call graphical Pitman - Yor processes and the derivation of generic inference al- gorithms for them ( Section 4 ) .","label":"Uses","metadata":{},"score":"69.145256"}{"text":"Conversely , we show how to save memory by collapsing probability and backoff into a single value without changing sentence - level scores , at the expense of less accurate estimates for sentence fragments .These changes can be stacked , achieving better estimates with unchanged memory usage .","label":"Uses","metadata":{},"score":"69.425934"}{"text":"Our model enables rapid incremental language model adaptation via caching the fractional topic counts of word hypotheses decoded from previous speech utterances .Latent Dirichlet - Tree allocation models topic correlation in a tree - based hierarchy and thus addresses the model initialization issue .","label":"Uses","metadata":{},"score":"69.51849"}{"text":"This offers a principled approach to language model smoothing , embedding the power - law distribution for natural language .Experiments on the recognition of conversational speech in multiparty meetings demonstrate that by using hierarchical Bayesian language models , we are able to achieve significant reductions in perplexity and word error rate .","label":"Uses","metadata":{},"score":"69.60692"}{"text":"This offers a principled approach to language model smoothing , embedding the power - law distribution for natural language .Experiments on the recognition of conversational speech in multiparty meetings demonstrate that by using hierarchical Bayesian language models , we are able to achieve significant reductions in perplexity and word error rate .","label":"Uses","metadata":{},"score":"69.60692"}{"text":"HashTBO made it possible to ship a trigram contextual speller in Microsoft Office 2007 . \" ...We describe the implementation steps required to scale high - order character language models to gigabytes of training data without pruning .Our online models build character - level PAT trie structures on the fly using heavily data - unfolded implementations of an mutable daughter maps with a long intege ... \" .","label":"Uses","metadata":{},"score":"69.750145"}{"text":"The main advantage of these architectures is that they learn an embedding for words ( or other symbo ... \" .In recent years , variants of a neural network architecture for statistical language modeling have been proposed and successfully applied , e.g. in the language modeling component of speech recognizers .","label":"Uses","metadata":{},"score":"70.225815"}{"text":"The scheme can represent any standard n - gram model and is easily combined with existing model reduction techniques such as entropy - pruning .We demonstrate the space - savings of the scheme via machine translation experiments within a distributed language modeling framework .","label":"Uses","metadata":{},"score":"70.23043"}{"text":"The scheme can represent any standard n - gram model and is easily combined with existing model reduction techniques such as entropy - pruning .We demonstrate the space - savings of the scheme via machine translation experiments within a distributed language modeling framework .","label":"Uses","metadata":{},"score":"70.23043"}{"text":"Rather , we reuse the structure ( grammar ) and replace constituents to construct new sentences .Structured language model tries to model the structural information in natural language , especially the long - distance dependencies in a probabilistic framework .However , exploring and using structural information is computationally expensive , as the number of possible structures for a sentence is very large even with the constraint of a grammar .","label":"Uses","metadata":{},"score":"70.265366"}{"text":"This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution .We also point out the high variance in all of these estimators , and that they require many more iterations to approach conver - gence than usually thought .","label":"Uses","metadata":{},"score":"70.3996"}{"text":"We contribute two changes that trade between accuracy of these estimates and memory , holding sentence - level scores constant .Common practi ... \" .Approximate search algorithms , such as cube pruning in syntactic machine translation , rely on the language model to estimate probabilities of sentence fragments .","label":"Uses","metadata":{},"score":"70.540146"}{"text":"However , these models are extremely slow in comparison to the more commonly used n - gram models , both for training and recognition .As an alternative to an importance sampling method proposed to speed - up training , we introduce a hierarchical decomposition of the conditional probabilities that yields a speed - up of about 200 both during training and recognition .","label":"Uses","metadata":{},"score":"70.74728"}{"text":"Theoretical results suggest that in order to learn the kind of complicated functions that can represent highlevel abstractions ( e.g. in vision , language , and other AI - level tasks ) , one may need deep architectures .Deep architectures are composed of multiple levels of non - linear operations , such as in neural nets with many hidden layers or in complicated propositional formulae re - using many sub - formulae .","label":"Uses","metadata":{},"score":"70.93544"}{"text":"..Background 40 p(x)logq(x ) ( 2.3 ) Such an evaluat ... . by Marcus Liwicki , Horst Bunke - in Tenth International Workshop on Frontiers in Handwriting Recognition , 2006 . \" ...In this paper we present an on - line recognition sys - tem for handwritten texts acquired from a whiteboard .","label":"Uses","metadata":{},"score":"71.12964"}{"text":"For automatic speech recognition we rapidly adapt a language model on a source language .For statistical machine translation , we adapt a language model of a target language , a translation lexicon and a phrase table using a source text .","label":"Uses","metadata":{},"score":"71.296524"}{"text":"At the same time , they can be mapped into memory quickly and be searched directly in time linear in the length of the key , without the need to decompress the entire file .The overhead for local decompression during search is marginal . \" ...","label":"Uses","metadata":{},"score":"71.54909"}{"text":"Related articles All 2 versions Cite Save More .Applying Pairwise Ranked Optimisation to Improve the Interpolation of Translation Models .B Haddow - HLT - NAACL , 2013 - aclweb.org ...Schroeder , 2007 ) ) , and is implemented in popular language modelling tools like IRSTLM ( Federico et al . , 2008 ) and SRILM ( Stolcke , 2002 ) .","label":"Uses","metadata":{},"score":"71.558975"}{"text":"The PDP - based n - gram models correspond well to versions of Kneser - Ney smoothing ( Teh , 2006b ) , the state of the art method in applications .\" [ Show abstract ] [ Hide abstract ] ABSTRACT : Exact Bayesian network inference exists for Gaussian and multinomial distributions .","label":"Uses","metadata":{},"score":"71.63457"}{"text":"We then compare combinations of both methods .It is shown that a broadcast news language model can be compressed by up to 83 % to only 12.6Mb with no loss in performance on a broadcast news task .Compressing the language model further by quantization to 10.3Mb resulted in only a 0.4 % degradation in word error rate which is better than can be achieved through entropy - based pruning alone . ing all those elements in the list whose contribution to the language model entropy lies below some threshold [ 1].","label":"Uses","metadata":{},"score":"71.70813"}{"text":"The model can be pruned down to a smaller size by manipulating the statistics or the estimated model .This paper shows how an n - gram model can be built by ad ... \" .Traditionally , when building an n - gram model , we decide the span of the model history , collect the relevant statistics and estimate the model .","label":"Uses","metadata":{},"score":"72.0139"}{"text":"In this work , we present an efficient data structure and optimized algorithms specifically designed for iterative parameter tuning .With the ... \" .Despite the availability of better performing techniques , most language models are trained using popular toolkits that do not support perplexity optimization .","label":"Uses","metadata":{},"score":"72.152245"}{"text":"Trigram language models are normally consid ... \" .Trigram language models are compressed using a Golomb coding method inspired by the original Unix spell program .Compression methods trade off space , time and accuracy ( loss ) .The proposed HashTBO method optimizes space at the expense of time and accuracy .","label":"Uses","metadata":{},"score":"72.30759"}{"text":"In this paper , we exploi ... \" .Abstract - Traditional - gram language models are widely used in state - of - the - art large vocabulary speech recognition systems .This simple model suffers from some limitations , such as overfitting of maximum - likelihood estimation and the lack of rich contextual knowledge sources .","label":"Uses","metadata":{},"score":"72.310486"}{"text":"We present Tightly Packed Tries ( TPTs ) , a compact implementation of read - only , compressed trie structures with fast on - demand paging and short load times .We demonstrate the benefits of TPTs for storing n - gram back - off language models and phrase tables for statistical machine translation .","label":"Uses","metadata":{},"score":"72.558846"}{"text":"We present Tightly Packed Tries ( TPTs ) , a compact implementation of read - only , compressed trie structures with fast on - demand paging and short load times .We demonstrate the benefits of TPTs for storing n - gram back - off language models and phrase tables for statistical machine translation .","label":"Uses","metadata":{},"score":"72.558846"}{"text":"The .Page 8 .614 A Hierarchical Nonparametric Bayesian Approach to Statistical Language Model Domain Adaptation adding more data and another ( likely more expensive ) resource cost to acquire more in - domain training data .While the costs specific to each application domain are different , Fig .","label":"Uses","metadata":{},"score":"72.57921"}{"text":"Graphical Pitman - Yor pro- cesses form a general framework within which to ex- plore a large variety of language models while retaining the same inference engine .We intend to undertake a more detailed and thorough theoretical treatment of graphical Pitman - Yor processes .","label":"Uses","metadata":{},"score":"72.81799"}{"text":"This technique , provided by the IRSTLM toolkit , consists in the linear interpolation of the n - gram probabilities from all component LMs . ...Related articles All 3 versions Cite Save More . ...Related articles Cite Save . 01","label":"Uses","metadata":{},"score":"72.86417"}{"text":"This paper presents a new method of using statistical models to estimate the reading difficulty of Web pages .Language Models are used to represent the content typically associated with different readability levels .Reading level classifiers are created as linear combinations of a language model and ... \" .","label":"Uses","metadata":{},"score":"73.017685"}{"text":"MIT Press , 2007 .R. Iyer , M. Ostendorf , and H. Gish .Using out - of - domain data to improve in - domain language models .IEEE Sig- nal processing letters , 4:221 - 223 , 1997 .","label":"Uses","metadata":{},"score":"73.24194"}{"text":"By examining many samples of human - produced translation , SMT algorithms automatically learn how to translate .SMT has made tremendous strides in less than two decades , and many popular tec ... \" .Statistical machine translation ( SMT ) treats the translation of natural language as a machine learning problem .","label":"Uses","metadata":{},"score":"73.36551"}{"text":"The resulting clusterings are then used in training partially class - based language models .We show that combining them with wordbased n - gram models in the log - linear model of a state - of - the - art statistical machine translation system leads to improvements in translation quality as indicated by the BLEU score . ...","label":"Uses","metadata":{},"score":"73.634544"}{"text":"In a German - English Moses system with target - side syntax , improved estimates yielded a 63 % reduction in CPU time ; for a Hiero - style version , the reduction is 21 % .The compressed language model uses 26 % less RAM while equivalent search quality takes 27 % more CPU .","label":"Uses","metadata":{},"score":"73.8348"}{"text":"Language model plays an important role in statistical machine translation systems .It is the key knowledge source to determine the right word order of the translation .Increasing the order of n and the size of the training data improves the performance of the LM as shown by the suffix array language model and distributed language model systems .","label":"Uses","metadata":{},"score":"73.938095"}{"text":"Conf . on Document Analysis and Recognition , 2005 . \" ...In this paper we present IAM - OnDB- a new large online handwritten sentences database .It is publicly available and consists of text acquired via an electronic interface from a whiteboard .","label":"Uses","metadata":{},"score":"74.08589"}{"text":"Growing algorithm .Thus , the main differences to KP are the following : We modify the model after each -gram has been pruned , inst ... . \" ...We present three novel methods of compactly storing very large n - gram language models .","label":"Uses","metadata":{},"score":"74.253235"}{"text":"An Online Service for SUbtitling by MAchine G van Loenhout , A Walker , Y Georgakopoulou ... - 2013 - sumat - project . eu ... model building plus decoding .To build the language models we have used the state - of - the - art open - source IRSTLM toolkit [ Federico & Cettolo , 2007].","label":"Uses","metadata":{},"score":"74.47016"}{"text":"Minimum error rate ... Related articles Cite Save More .( 2 ) You 'd better run the baseline of Moses .( 3 ) Python 2.7 .Here we use irstlm to train language model , as this baseline suggest .","label":"Uses","metadata":{},"score":"74.6089"}{"text":"We discuss the properties of such estimates , and methods to interpolate them with traditional corpus based trigram estimates .We show that the interpolated models improve speech recognition word error rate significantly over a small test set .The second direction is to acquire more training data .","label":"Uses","metadata":{},"score":"74.87894"}{"text":"Finally , we applied a generalized ROVER algorithm to combine the N - best hypotheses from several systems based on different acoustic models .ts had been optimized for perplexity on prior evaluation data .Lattice expansion used an unpruned , trigram backoff LM ( 4.8 M bigrams , 11.5 M trigrams ) constructed in the same fashion .","label":"Uses","metadata":{},"score":"75.05204"}{"text":"Finally , we applied a generalized ROVER algorithm to combine the N - best hypotheses from several systems based on different acoustic models .ts had been optimized for perplexity on prior evaluation data .Lattice expansion used an unpruned , trigram backoff LM ( 4.8 M bigrams , 11.5 M trigrams ) constructed in the same fashion .","label":"Uses","metadata":{},"score":"75.05204"}{"text":"The main drawbacks of NLMs are computational complexity and the fact that only distributional information ( word context ) is used to generalize over words , where ... . \" ...We present a correlated bigram LSA approach for unsupervised LM adaptation for automatic speech recognition .","label":"Uses","metadata":{},"score":"75.2218"}{"text":"The global 5-gram LM smoothed through the improved Kneser - Ney technique is estimated on the target monolingual side of the parallel train- ing data using the IRSTLM toolkit ( Federico et al . , 2008 ) .Models are case - sensitive .","label":"Uses","metadata":{},"score":"75.23655"}{"text":"The method requires only a corpus of documents and an index connecting the documents to genes . by Horst Bunke - In Proc .7th Int .Conf . on Document Analysis and Recognition , 2003 . \" ...This paper review the state of the art in o#-line Roman cursive han dw iting recognition .","label":"Uses","metadata":{},"score":"75.55652"}{"text":"As opposed to past explanations , our interpretation can recover exactly the formulation of interpolated Kneser - Ney , and performs better than interpolated Kneser - Ney when a better inference procedure is used . \" ...In ( Chen , 2009 ) , we show that for a variety of language models belonging to the exponential family , the test set cross - entropy of a model can be accurately predicted from its training set cross - entropy and its parameter values .","label":"Uses","metadata":{},"score":"75.61582"}{"text":"To find an n - gram , one would start from the ( em ... . \" ...Trigram language models are compressed using a Golomb coding method inspired by the original Unix spell program .Compression methods trade off space , time and accuracy ( loss ) .","label":"Uses","metadata":{},"score":"75.620186"}{"text":"With the resulting implementation , we demonstrate the feasibility and effectiveness of such iterative techniques in language model estimation .Index Terms : language modeling , smoothing , interpolation 1 . ...[ 3 ] , most work in the field opts for simpler techniques with inferior results .","label":"Uses","metadata":{},"score":"75.771225"}{"text":"We investigate the extent to which alignment can be simulated using word sequences alone ( not syntactic structures ) .To this end , we interpolate a default language model with one calculated on the basis of a cached sentence .Experiments on sentences with the prepositional / double object alternation show that varying the weight given to the cache model varies the propensity to align .","label":"Uses","metadata":{},"score":"75.832344"}{"text":"Our online models build character - level PAT trie structures on the fly using heavily data - unfolded implementations of an mutable daughter maps with a long integer count interface .Terminal nodes are shared .Character 8-gram training runs at 200,000 characters per second and allows online tuning of hyperparameters .","label":"Uses","metadata":{},"score":"75.9258"}{"text":"Low - dimensional representations for lexical co - occurrence data have become increasingly important in alleviating the sparse data problem inherent in natural language processing tasks .This work presents a distributed latent variable model for inducing these low - dimensional representations .","label":"Uses","metadata":{},"score":"76.05861"}{"text":"Low - dimensional representations for lexical co - occurrence data have become increasingly important in alleviating the sparse data problem inherent in natural language processing tasks .This work presents a distributed latent variable model for inducing these low - dimensional representations .","label":"Uses","metadata":{},"score":"76.05861"}{"text":"This paper discusses the motivations and principles regarding learning algorithms for deep architectures , in particular those exploiting as building blocks unsupervised learning of single - layer models such as Restricted Boltzmann Machines , used to construct deeper models such as Deep Belief Networks . ... ently .","label":"Uses","metadata":{},"score":"76.22113"}{"text":"We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .In addition , we introduce two novel smoothing techniques , one a variation of Jelinek - Mercer smoothing and one a very simple linear interpolation technique , both of which outperform existing methods . ... he differences in performance seem to be less when cutoffs are used .","label":"Uses","metadata":{},"score":"76.33151"}{"text":"We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .In addition , we introduce two novel smoothing techniques , one a variation of Jelinek - Mercer smoothing and one a very simple linear interpolation technique , both of which outperform existing methods . ... he differences in performance seem to be less when cutoffs are used .","label":"Uses","metadata":{},"score":"76.33151"}{"text":"Our basic ... \" .We present three novel methods of compactly storing very large n - gram language models .These methods use substantially less space than all known approaches and allow n - gram probabilities or counts to be retrieved in constant time , at speeds comparable to modern language modeling toolkits .","label":"Uses","metadata":{},"score":"76.39345"}{"text":"This model provides a powerful solution to topic modeling and a flexible framework for the incorporation of other cues ... \" .Abstract .In this paper , we address the modeling of topic and role information in multiparty meetings , via a nonparametric Bayesian model called the hierarchical Dirichlet process .","label":"Uses","metadata":{},"score":"76.588844"}{"text":"Tools . \" ...We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling , including those described by Jelinek and Mercer ( 1980 ) , Katz ( 1987 ) , and Church and Gale ( 1991 ) .","label":"Uses","metadata":{},"score":"76.776566"}{"text":"In particular , single components of each n - gram are searched , via binary search , into blocks ... . by Vesa Siivola , Bryan L. Pellom - In Proceedings of 9th European Conference on Speech Communication and Technology , 2005 . \" ...","label":"Uses","metadata":{},"score":"76.865265"}{"text":"I. . ...e models [ 9 ] , and a Bayesian framework [ 10]-[12].In this paper , we propose the use of hierarchical Bayesian approaches [ 16 ] to better model of ... . \" ...In open - domain language exploitation applications , a wide variety of topics with swift topic shifts has to be captured .","label":"Uses","metadata":{},"score":"77.203964"}{"text":"In this . by Andrei Alex , Katrin Kirchhoff , Andrei Alex , Katrin Kirchhoff , 2006 . \" ...Language models based on a continuous word representation and neural network probability estimation have recently emerged as an alternative to the established backoff language models .","label":"Uses","metadata":{},"score":"77.28525"}{"text":"We propose a succinct randomized language model which employs a perfect hash function to encode fingerprints of n - grams and their associated probabilities , backoff weights , or other parameters .The scheme can represent any standard n - gram model and is easily combined with existing model reduction te ... \" .","label":"Uses","metadata":{},"score":"77.293976"}{"text":"We propose a succinct randomized language model which employs a perfect hash function to encode fingerprints of n - grams and their associated probabilities , backoff weights , or other parameters .The scheme can represent any standard n - gram model and is easily combined with existing model reduction te ... \" .","label":"Uses","metadata":{},"score":"77.293976"}{"text":"We address the scalability issue to large ... \" .We present a correlated bigram LSA approach for unsupervised LM adaptation for automatic speech recognition .The model is trained using efficient variational EM and smoothed using the proposed fractional Kneser - Ney smoothing which handles fractional counts .","label":"Uses","metadata":{},"score":"77.44559"}{"text":"( ICASSP'01 , 2001 . \" ...We propose a novel method for using the World Wide Web to acquire trigram estimates for statistical language modeling .We submit an N - gram as a phrase query to web search engines .","label":"Uses","metadata":{},"score":"77.60918"}{"text":"In this paper we investigate the extent to which Katz backoff language models can be compressed through a combination of parameter quantization ( width - wise compression ) and parameter pruning ( length - wise compression ) methods while preserving performance .","label":"Uses","metadata":{},"score":"77.8654"}{"text":"This paper shows how an n - gram model can be built by adding suitable sets of n - grams to a unigram model until desired complexity is reached .Very high order n - grams can be used in the model , since the need for handling the full unpruned model is eliminated by the proposed technique .","label":"Uses","metadata":{},"score":"77.90692"}{"text":"Proc . of the ... , 2013 - workshop2013.iwslt.org ... Translation and lexicalized reordering models were trained on the parallel training data ; 5-gram LMs with im- proved Kneser - Ney smoothing were estimated on the target side of the training parallel data with the IRSTLM toolkit [ 42]. ...","label":"Uses","metadata":{},"score":"78.16202"}{"text":"However , as it can take advantage of memory mapping on disk , the proposed implementation seems to scale - up much better to very large language models : decoding with a 289-million 5-gram language model runs in 2.1Gb of RAM . ... into a binary format to be efficiently loaded and accessed at runtime .","label":"Uses","metadata":{},"score":"78.20033"}{"text":"Cross - entropy and speech recognition In this section , we briefly examine how the performance of a language model meas ... . by Andreas Stolcke - IN PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING ( ICSLP 2002 , 2002 . \" ...","label":"Uses","metadata":{},"score":"78.58448"}{"text":"Cross - entropy and speech recognition In this section , we briefly examine how the performance of a language model meas ... . by Andreas Stolcke - IN PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING ( ICSLP 2002 , 2002 . \" ...","label":"Uses","metadata":{},"score":"78.58448"}{"text":"Marcello Federico , Nicola Bertoldi , and Mauro Cet- tolo .IRSTLM : an open source toolkit for handling large scale language models .In Proceed- ings of Interspeech , pages 1618 - 1621 . ...Cited by 3 Related articles All 8 versions Cite Save More .","label":"Uses","metadata":{},"score":"78.60424"}{"text":"In this paper we consider bigram language models .N -gram language models are based on the observation that we are often able to guess the next word when we are reading a given text .In other words , the probability of a word is highly depending on ... . \" ... Interpolated Kneser - Ney is one of the best smoothing methods for n - gram language models .","label":"Uses","metadata":{},"score":"78.60727"}{"text":"We describe SRI 's large vocabulary conversational speech recognition system as used in the March 2000 NIST Hub-5E evaluation .The system performs four recognition passes : ( 1 ) bigram recognition with phone - loop - adapted , within - word triphone acoustic models , ( 2 ) lattice generation with transcription - m ... \" .","label":"Uses","metadata":{},"score":"78.64257"}{"text":"We describe SRI 's large vocabulary conversational speech recognition system as used in the March 2000 NIST Hub-5E evaluation .The system performs four recognition passes : ( 1 ) bigram recognition with phone - loop - adapted , within - word triphone acoustic models , ( 2 ) lattice generation with transcription - m ... \" .","label":"Uses","metadata":{},"score":"78.64257"}{"text":"Equivalent test corpus perplexity could be had using a SOU - only HPYLM model by more than doubling the amount of SOU domain specific training words .In applica- tion domains where adding more out - of - domain data is significantly cheaper than acquiring more in - domain training data this could result in substantial savings .","label":"Uses","metadata":{},"score":"78.69647"}{"text":"Cited by 7 Related articles All 9 versions Cite Save More . ...The method is available in the IRSTLM toolkit ( Fed- erico et al . , 2008 ) .28 Page 3 . 3 Data for Development ...Cited by 3 Related articles All 2 versions Cite Save More . ...","label":"Uses","metadata":{},"score":"79.19165"}{"text":"[ 7 ] Marcello Federico , Nicola Bertoldi , Mauro Cettolo .2008 IRSTLM : an Open Source Toolkit for Handling Large Scale Language Models .In Proceedings of Interspeech 2008 , 1618 - 1621 . ...Cited by 1 Related articles Cite Save More .","label":"Uses","metadata":{},"score":"79.216255"}{"text":"Irstlm(L ( Op ) )Mn ?Irstlm(L ( On ) ) ...Cited by 1 Related articles All 2 versions Cite Save .FBK 's Machine Translation Systems for the IWSLT 2013 Evaluation Campaign N Bertoldi , MA Farajian , P Mathur , N Ruiz , M Federico ... - hlt.fbk.eu ... 2.4.1 .","label":"Uses","metadata":{},"score":"79.3692"}{"text":"MAP adaptation of stochastic grammars .and Language , 20:41 - 68 , 2006 .Computer Speech J. R. Bellegarda .Statistical language model adaptation : review and perspectives .Speech Communication , 42:93- 108 , 2004 .J. Carletta .Unleashing the killer corpus : experiences in creating the multi - everything AMI meeting corpus .","label":"Uses","metadata":{},"score":"79.6202"}{"text":"We apply parsimonious models at three stages of the retrieval process:1 ) at indexing time ; 2 ) at search time ; 3 ) at feedback time .Experimental results show that we are able to build models that are significantly smaller than standard models , but that still perform at least as well as the standard approaches . .","label":"Uses","metadata":{},"score":"79.65042"}{"text":"We apply parsimonious models at three stages of the retrieval process:1 ) at indexing time ; 2 ) at search time ; 3 ) at feedback time .Experimental results show that we are able to build models that are significantly smaller than standard models , but that still perform at least as well as the standard approaches . .","label":"Uses","metadata":{},"score":"79.65042"}{"text":"The N ... \" .We propose a novel method for using the World Wide Web to acquire trigram estimates for statistical language modeling .We submit an N - gram as a phrase query to web search engines .The search engines return the number of web pages containing the phrase , from which the N - gram count is estimated .","label":"Uses","metadata":{},"score":"79.80404"}{"text":"Cited by 4 Related articles All 12 versions Cite Save .Large - scale multiple language translation accelerator at the United Nations B Pouliquen , C Elizalde , M Junczys - Dowmunt ... - mtsummit2013.info ... Language models are being computed with the IRSTLM toolkit ( Federico et al . , 2008 ) .","label":"Uses","metadata":{},"score":"79.912155"}{"text":"Our aim is to investigate the properties which make a good measure of lexical distributional similarity .We start by introducing the concept of lexical distributional similarity .We discuss potential applications , which can be roughly divided into distributional or language modelling applications and semantic applications , and methods of evaluation ( Chapter 2 ) .","label":"Uses","metadata":{},"score":"80.05046"}{"text":"This taski ... \" .This paper review the state of the art in o#-line Roman cursive han dw iting recognition .The input provided to an o#-line han iting recognition system is an image of a digit , aw ord , or - more generally - some text , and the system produces , as output , an ASCII transcription of the input .","label":"Uses","metadata":{},"score":"80.208786"}{"text":"This paper focuses on the application and scalability of HPYLM on a practical large vocabulary ASR system .Experimental results on NIST RT06s evaluation meeting data verify that HPYLM is a competitive and promising language modeling technique , which consistently performs better than interpolated Kneser - Ney and modified Kneser - Ney n - gram LMs in terms of both perplexity and word error rate . .","label":"Uses","metadata":{},"score":"80.22384"}{"text":"A word N - gram is a sequence of words of length N with an associated probability of occurrence .N - gram probabilities are usually estimated from natural language corpora .They are utilized in the reco ... . by Marcus Liwicki , Horst Bunke - In Proc .","label":"Uses","metadata":{},"score":"80.23177"}{"text":"Related articles All 2 versions Cite Save More .Statistical sentiment analysis performance in Opinum B Bonev , G Ramírez - Sánchez , SO Rojas - arXiv preprint arXiv:1303.0446 , 2013 - arxiv.org ...In our setup we use the IRSTLM open - source library for building the language model . ...","label":"Uses","metadata":{},"score":"80.431305"}{"text":"In this paper , we show that some of the commonly used pruning methods do not take into account how removing an - gram should modify the backoff distributions in the state - of - the - art Kneser - Ney smoothing .","label":"Uses","metadata":{},"score":"80.62157"}{"text":"Obser ... . \" ...Theoretical results suggest that in order to learn the kind of complicated functions that can represent highlevel abstractions ( e.g. in vision , language , and other AI - level tasks ) , one may need deep architectures .","label":"Uses","metadata":{},"score":"80.643814"}{"text":"Improving Language Model Adaptation using Automatic Data Selection and Neural Network .S Jalalvand - RANLP , 2013 - aclweb.org ...On this data we trained a 4-gram back - off LM using the modified shift beta smoothing method as supplied by the IRSTLM toolkit ( Federico , 2008 ) .","label":"Uses","metadata":{},"score":"80.66368"}{"text":"rpus .Combining several N - grams can produce a model with a very large number of parameters , which is costly in decoding .In such cases N - grams are typically pruned .The same pruning parameters were applied to all models in our experiments .","label":"Uses","metadata":{},"score":"80.69748"}{"text":"rpus .Combining several N - grams can produce a model with a very large number of parameters , which is costly in decoding .In such cases N - grams are typically pruned .The same pruning parameters were applied to all models in our experiments .","label":"Uses","metadata":{},"score":"80.69748"}{"text":"Any opinions , findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA .To the best of our knowledge , our formulation in this paper is considered new to the research community .","label":"Uses","metadata":{},"score":"80.732796"}{"text":"SRILM is freely available for noncommercial purposes .The toolkit supports creation ... \" .SRILM is a collection of C++ libraries , executable programs , and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications .","label":"Uses","metadata":{},"score":"81.02874"}{"text":"SRILM is freely available for noncommercial purposes .The toolkit supports creation ... \" .SRILM is a collection of C++ libraries , executable programs , and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications .","label":"Uses","metadata":{},"score":"81.02874"}{"text":"This recognizer is based on Hidden Markov Models ( HMMs ) .In our experiments we show that by using larger training sets we can significantly increase the word recognition rate .This recognizer may serve as a benchmark reference for future research .","label":"Uses","metadata":{},"score":"81.07323"}{"text":"\" ...Statistical machine translation , as well as other areas of human language processing , have recently pushed toward the use of large scale n - gram language models .This paper presents efficient algorithmic and architectural solutions which have been tested within the Moses decoder , an open source toolk ... \" .","label":"Uses","metadata":{},"score":"81.08017"}{"text":"First , requiring no modification to the model , but po- tentially further improving test performance , we will experiment with mutiple domain adaptation by adding more than two corpora into the DHPYLM .Secondly the DHPYLM can integrated into topic models such that the bag - of - words assumption can be avoided .","label":"Uses","metadata":{},"score":"81.26415"}{"text":"We propose a new general framework for distributional similarity based on the context of lexical substitutability , which me measure using the IR concepts of precision and recall .This framework allows us to investigate the key factors in similarity of asymmetry , the relative influence of different contexts and the extent to which words share a context ( Chapter 4 ) .","label":"Uses","metadata":{},"score":"81.32368"}{"text":"As opposed to past explanations , our interpretation can recover exactly the formulation of interpolated Kneser - Ney , and performs better than interpolated Kneser - Ney when a better inference procedure is used . ...994 ; Berger et al .","label":"Uses","metadata":{},"score":"81.34128"}{"text":"The intersection of tree transducer - based translation models with n - gram language models results in huge dynamic programs for machine translation decoding .We propose a multipass , coarse - to - fine approach in which the language model complexity is incrementally introduced .","label":"Uses","metadata":{},"score":"81.36922"}{"text":"The intersection of tree transducer - based translation models with n - gram language models results in huge dynamic programs for machine translation decoding .We propose a multipass , coarse - to - fine approach in which the language model complexity is incrementally introduced .","label":"Uses","metadata":{},"score":"81.36922"}{"text":"The system incorporates two new kinds of acoustic model : triphone models conditioned on speaking rate , and an explicit joint model of within - word phone durations .We also obtained an unusually large improvement from modeling crossword pronunciation variants in \" multiword \" vocabulary items .","label":"Uses","metadata":{},"score":"81.40906"}{"text":"The system incorporates two new kinds of acoustic model : triphone models conditioned on speaking rate , and an explicit joint model of within - word phone durations .We also obtained an unusually large improvement from modeling crossword pronunciation variants in \" multiword \" vocabulary items .","label":"Uses","metadata":{},"score":"81.40906"}{"text":"In Proceedings of In- terspeech , Brisbane , Australie . ...Cited by 2 Related articles All 6 versions Cite Save More .Graph Model for Chinese Spell Checking Z Jia , P Wang , H Zhao - Sixth International Joint Conference on Natural ... , 2013 - aclweb.org ...","label":"Uses","metadata":{},"score":"81.53952"}{"text":"NAIST at 2013 CoNLL grammatical error correction shared task I Yoshimoto , T Kose , K Mitsuzawa , K Sakaguchi ... -CoNLL-2013 , 2013 - aclweb.org ... 515 as the alignment tool .The grow - diag - final heuristics was applied for phrase extraction .","label":"Uses","metadata":{},"score":"81.68076"}{"text":"3 we plot the \" baseline \" test perplexity for a HPYLM model trained on SOU corpus data alone ( this is the same baseline as was established in Fig . 2 ) .Tests were run for each combination of Brown and SOU training corpus sizes shown by the small crosses and absolute test perplexity improvement was inter- polated between these points to produce isosurfaces of test perplexity improvement .","label":"Uses","metadata":{},"score":"81.69195"}{"text":"To improve the n - gram language model , we also developed dynamic n - gram language model adaptation and discriminative language model to tackle issues with the standard n - gram language models and observed improvements in the translation qualities .","label":"Uses","metadata":{},"score":"81.80963"}{"text":"In fact , the lower ... . by E. W. D. Whittaker , B. Raj - the European Conference on Speech Communication and Technology , 2001 . \" ...In this paper we investigate the extent to which Katz backoff language models can be compressed through a combination of parameter quantization ( width - wise compression ) and parameter pruning ( length - wise compression ) methods while preserving performance .","label":"Uses","metadata":{},"score":"81.8647"}{"text":"Tento èlánek pojednává o empirické evaluaci a kombinaci pokroèilých technik jazykového modelování .Na¹e práce je prvním pokusem o kombinaci mnoha pokroèilých technik jazykového modelování .Abstrakt .We present results obtained with several advanced language modeling techniques , including class based model , cache model , maximum entropy model , structured language model , random forest language model and several types of neural network based language models .","label":"Uses","metadata":{},"score":"81.90112"}{"text":"In this paper we investigate the effects of applying such a technique to higherorder n - gram models trained on large corpora .We introduce a modi ... \" .In statistical language modeling , one technique to reduce the problematic effects of data sparsity is to partition the vocabulary into equivalence classes .","label":"Uses","metadata":{},"score":"82.10287"}{"text":"Section 5 compares the DH- PYLM to previous language model domain adapta- tion approaches and Section 6 reports on experiments showing the effectiveness of the new model .We start in the next section by reviewing language modeling and the HPYLM in particular .","label":"Uses","metadata":{},"score":"82.21382"}{"text":"Phrase - Based Machine Translation of Under - Resourced Languages A Drummer - people.cs.uct.ac.za ...The Moses toolkit was used along with Giza++ for alignment and IRSTLM for the language model .The researcher was unsuccessful in ... in the training pipeline .","label":"Uses","metadata":{},"score":"82.21716"}{"text":"[ Show abstract ] [ Hide abstract ] ABSTRACT : Traditional n -gram language models are widely used in state - of - the - art large vocabulary speech recognition systems .This simple model suffers from some limitations , such as overfitting of maximum - likelihood estimation and the lack of rich contextual knowledge sources .","label":"Uses","metadata":{},"score":"82.22623"}{"text":"This paper describes an LVCSR system for the conversational telephone speech ( CTS ) task trained on more than 2000 hours of data for which only approximate transcriptions were available .The challenges of dealing which such a large data set and the accuracy improvements over the small baseline system are discussed .","label":"Uses","metadata":{},"score":"82.246155"}{"text":"This paper describes an LVCSR system for the conversational telephone speech ( CTS ) task trained on more than 2000 hours of data for which only approximate transcriptions were available .The challenges of dealing which such a large data set and the accuracy improvements over the small baseline system are discussed .","label":"Uses","metadata":{},"score":"82.246155"}{"text":"In this paper we investigate the possibility of evaluating MT quality and fluency at the sentence level in the absence of reference translations .We measure the correlation between automatically - generated scores and human judgments , and we evaluate the performance of our system when used as a classifier for identifying highly dysfluent and illformed sentences .","label":"Uses","metadata":{},"score":"82.27962"}{"text":"This paper presents efficient algorithmic and architectural solutions which have been tested within the Moses decoder , an open source toolkit for statistical machine translation .Experiments are reported with a high performing baseline , trained on the Chinese - English NIST 2006 Evaluation task and running on a standard Linux 64-bit PC architecture .","label":"Uses","metadata":{},"score":"82.381546"}{"text":"Applying SLM techniques like trigram language models to Chinese is challenging because ( 1 ) there is no standard definition of words in Chinese ; ( 2 ) word boundaries are not marked by spaces ; and ( 3 ) there is a dearth of training data .","label":"Uses","metadata":{},"score":"82.650276"}{"text":"Applying SLM techniques like trigram language models to Chinese is challenging because ( 1 ) there is no standard definition of words in Chinese ; ( 2 ) word boundaries are not marked by spaces ; and ( 3 ) there is a dearth of training data .","label":"Uses","metadata":{},"score":"82.650276"}{"text":"In this paper , we describe how overly aggressive key - target resizing can sometimes prevent users from typing their desired text , violating basic user expectations about keyboard functionality .We propose an anchored key - target method which aims to provide an input method that is robust to errors while respecting usability principles .","label":"Uses","metadata":{},"score":"82.70114"}{"text":"The classifier uses linguistic features and has been trained to distinguish human translations from machine translations .We show that this approach also performs well in identifying dysfluent sentences . by Jakob Uszkoreit , Thorsten Brants - In ACL International Conference Proceedings , 2008 . \" ...","label":"Uses","metadata":{},"score":"82.85184"}{"text":"In ( Emami and Jelinek , 2005 ) a clustering algorithm is ... . ...w these clusterings are obtained and how much refinement is optimal for each pass .Note that unlikely in the parsing scenario Chapter 2 where the projection state space was obvious and we only needed to estimate the pa ... . by Asela Gunawardana , Tim Paek , Christopher Meek - Proc . IUI & apos;10 .","label":"Uses","metadata":{},"score":"82.85897"}{"text":"In this we have trained our language model using IRSTLM toolkit [ 9].Our transliteration system follows the steps which are represented in figure 1 .Example : ... Cited by 2 Related articles All 5 versions Cite Save .","label":"Uses","metadata":{},"score":"82.990204"}{"text":"Related articles Cite Save More .Robustness of Distant - Speech Recognition and Speaker Identification - Development of Baseline System G Potamianos , A Abad , A Brutti , M Hagmuller , G Kubin ... - 2013 - dirha.fbk.eu ... industrial applications .","label":"Uses","metadata":{},"score":"83.01506"}{"text":"net / projects/ irstlm/ 17consisting of entries through 2012 . ...Cited by 6 Related articles All 9 versions Cite Save More .To build the language models ( LM ) , we used the state - of - the - art open - source IRSTLM toolkit ( Federico and Cettolo , 2007 ) .","label":"Uses","metadata":{},"score":"83.2887"}{"text":"In Proceedings of the IEEE International Conference on Acoustics , Speech , and Sig- nal Processing , pages 586 - 589 , 1993 . H. Kucera and W. N. Francis .Computational analysis of present - day American English .Brown University Press , Providence , RI , 1967 .","label":"Uses","metadata":{},"score":"83.38568"}{"text":"Across various encoding schemes , and for multiple language pairs , we show speed - ups of up to 50 times over single - pass decoding while improving BLEU score .Moreover , our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram - to - trigram decoder . \" ... Interpolated Kneser - Ney is one of the best smoothing methods for n - gram language models .","label":"Uses","metadata":{},"score":"83.85133"}{"text":"Applying SLM techniques like trigram language models to Chinese is challenging because ( 1 ) there is no standard definition of words in Chinese ; ( 2 ) word boundaries are not marked by spaces ; and ( 3 ) there is a de ... \" .","label":"Uses","metadata":{},"score":"83.90051"}{"text":"Applying SLM techniques like trigram language models to Chinese is challenging because ( 1 ) there is no standard definition of words in Chinese ; ( 2 ) word boundaries are not marked by spaces ; and ( 3 ) there is a de ... \" .","label":"Uses","metadata":{},"score":"83.90051"}{"text":"Proc . of IWSLT , 2013 - eu - bridge . ...Cited by 2 Related articles All 7 versions Cite Save More . ...Related articles Cite Save .Context Dependent Bag of words generation SA Jadhav , DVLN Somayajulu ... - Advances in ... , 2013 - ieeexplore.ieee.org ... ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?","label":"Uses","metadata":{},"score":"84.06984"}{"text":"In Finnish speech recognition tests , the models trained by the growing method outperform the entropy pruned models of similar size .We need to encode which words actually make up the ngram .The tree contains all the n - grams of the model , regardless of the order of the n - gram .","label":"Uses","metadata":{},"score":"84.44847"}{"text":"1INTRODUCTIONConsider the problem of statistical natural language model domain adaptation .Statistical language mod- els typically have a very large number of parameters and thus need a large quantity of training data to pro- duce good estimates of those parameters .","label":"Uses","metadata":{},"score":"84.495834"}{"text":"It is easy , however , to obtain non - domain - specific data , e.g. text from the world wide web .Unfortunately models trained using such data are often ill - suited for domain - specific ap-Appearing in Proceedings of the 12thInternational Confe- rence on Artificial Intelligence and Statistics ( AISTATS ) 2009 , Clearwater Beach , Florida , USA .","label":"Uses","metadata":{},"score":"85.02391"}{"text":"Experiments on Finnish and English text corpora show that the proposed pruning algorithm provides considerable improvements over previous pruning algorithms on Kneser - Ney - smoothed models and is also better than the baseline entropy pruned Good - Turing smoothed models .","label":"Uses","metadata":{},"score":"85.34955"}{"text":"The improvements in the Finnish speech recognition over the other Kneser - Ney smoothed models are statistically significant , as well .Index Terms - Modeling , natural languages , smoothing methods , speech recognition .I. . ... probability of the training data given by the current model .","label":"Uses","metadata":{},"score":"85.42705"}{"text":"There are more open source statistical language modeling toolkits available like IRSTLM , RandLM and KenLM .D ... Related articles Cite Save More . ...Cited by 2 Related articles All 2 versions Cite Save .Unsupervised and Semi - supervised Myanmar Word Segmentation Approaches for Statistical Machine Translation YK Thu , A Finch , E Sumita , Y Sagisaka - saki.siit.tu.ac.th ... blog data[16].","label":"Uses","metadata":{},"score":"85.53847"}{"text":"We propose a novel interpretation of interpolated Kneser - Ney as approxima ... \" .Interpolated Kneser - Ney is one of the best smoothing methods for n - gram language models .Previous explanations for its superiority have been based on intuitive and empirical justifications of specific properties of the method .","label":"Uses","metadata":{},"score":"85.66895"}{"text":"We propose a novel interpretation of interpolated Kneser - Ney as approxima ... \" .Interpolated Kneser - Ney is one of the best smoothing methods for n - gram language models .Previous explanations for its superiority have been based on intuitive and empirical justifications of specific properties of the method .","label":"Uses","metadata":{},"score":"85.66895"}{"text":"Songfang Huang , Steve Renals - In Proceedings of IEEE ASRU International Conference , 2007 . \" ...In this paper we investigate the application of a hierarchical Bayesian language model ( LM ) based on the Pitman - Yor process for automatic speech recognition ( ASR ) of multiparty meetings .","label":"Uses","metadata":{},"score":"85.86195"}{"text":"Cited by 5 Related articles All 9 versions Cite Save More .Simple , readable sub - sentences .S Klerke , A Søgaard - ACL ( Student Research Workshop ) , 2013 - aclweb.org ... dsl .Generative and discriminative methods for online adaptation in smt K Wäschle , P Simianer , N Bertoldi ... - Proceedings of the ... , 2013 - wiki.cl.uni-heidelberg .","label":"Uses","metadata":{},"score":"85.94503"}{"text":"In this paper we investigate the possibility of evaluating MT quality and fluency at the sentence level in the absence of reference translations .We measure the correlation between automatically - generated scores and human judgments , and we evaluate the performance of our system when used a ... \" .","label":"Uses","metadata":{},"score":"86.22643"}{"text":"Related articles All 2 versions Cite Save More .Federico , M. , Bertoldi , N. , Cettolo , M. : IRSTLM : an Open Source Toolkit for Handling Large Scale Language Models . ...Cited by 2 Related articles All 4 versions Cite Save .","label":"Uses","metadata":{},"score":"86.29144"}{"text":"K Sakaguchi , Y Arase , M Komachi - ACL ( 2 ) , 2013 - aclweb.org ...Table 3 : Ratio of appropriate distractors ( RAD ) with a 95 % confidence interval and inter - rater agreement statistics ? model score trained on Google 1 T Web Corpus ( Brants and Franz , 2006 ) with IRSTLM toolkit12 . ... net / projects / irstlm / files / irstlm/ ... Related articles All 2 versions Cite Save More .","label":"Uses","metadata":{},"score":"86.54222"}{"text":"A number of ... Related articles Cite Save More .Omnifluent English - to - French and Russian - to - English systems for the 2013 Workshop on Statistical Machine Translation E Matusov , G Leusch - Proceedings of the Eighth Workshop on Statistical ... , 2013 - aclweb.org ...","label":"Uses","metadata":{},"score":"86.57002"}{"text":"As such , ... \" .We systematically investigate a new approach to estimating the parameters of language models for information retrieval , called parsimonious language models .Parsimonious language models explicitly address the relation between levels of language models that are typically used for smoothing .","label":"Uses","metadata":{},"score":"86.58031"}{"text":"As such , ... \" .We systematically investigate a new approach to estimating the parameters of language models for information retrieval , called parsimonious language models .Parsimonious language models explicitly address the relation between levels of language models that are typically used for smoothing .","label":"Uses","metadata":{},"score":"86.58031"}{"text":"Community - based post - editing of machine - translated content : monolingual vs. bilingual L Mitchell , J Roturier , S O'Brien - Machine Translation Summit XIV - accept.unige.ch ... the available monolingual English forum data ( approx .a million sentences ) .","label":"Uses","metadata":{},"score":"86.84413"}{"text":"ICASSP , 2005 . \" ...Typical systems for large vocabulary conversational speech recognition ( LVCSR ) have been trained on a few hundred hours of carefully transcribed acoustic training data .This paper describes an LVCSR system for the conversational telephone speech ( CTS ) task trained on more than 2000 hours of data for ... \" .","label":"Uses","metadata":{},"score":"86.89395"}{"text":"ICASSP , 2005 . \" ...Typical systems for large vocabulary conversational speech recognition ( LVCSR ) have been trained on a few hundred hours of carefully transcribed acoustic training data .This paper describes an LVCSR system for the conversational telephone speech ( CTS ) task trained on more than 2000 hours of data for ... \" .","label":"Uses","metadata":{},"score":"86.89395"}{"text":"Related articles All 6 versions Cite Save More .Efficient solutions for word reordering in German - English phrase - based statistical machine translation A Bisazza , M Federico - 8th Workshop on Statistical Machine Translation , 2013 - aclweb.org ... ley Aligner ( Liang et al . , 2006 ) .","label":"Uses","metadata":{},"score":"87.024826"}{"text":"An approx ... \" .In this paper we investigate the application of a hierarchical Bayesian language model ( LM ) based on the Pitman - Yor process for automatic speech recognition ( ASR ) of multiparty meetings .The hierarchical Pitman - Yor language model ( HPY - LM ) provides a Bayesian interpretation of LM smoothing .","label":"Uses","metadata":{},"score":"87.04818"}{"text":"Sources of training data suitable for language modeling of conversational speech are limited .In this paper , we show how training data can be supplemented with text from the web filtered to match the style and/or topic of the target recognition task , but also that it is possible to get bigger perfor ... \" .","label":"Uses","metadata":{},"score":"87.22197"}{"text":"Sources of training data suitable for language modeling of conversational speech are limited .In this paper , we show how training data can be supplemented with text from the web filtered to match the style and/or topic of the target recognition task , but also that it is possible to get bigger perfor ... \" .","label":"Uses","metadata":{},"score":"87.22197"}{"text":"This thesis addresses unsupervised topic adaptation in both monolingual and crossl ... \" .In open - domain language exploitation applications , a wide variety of topics with swift topic shifts has to be captured .Consequently , it is crucial to rapidly adapt all language components of a spoken language system .","label":"Uses","metadata":{},"score":"87.29411"}{"text":"Improving trigram language .Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .","label":"Uses","metadata":{},"score":"87.37029"}{"text":"In this paper , we investigate language modeling for ASR in multiparty meetings through the inclusion of richer knowledge i .. \" ...Abstract - Traditional - gram language models are widely used in state - of - the - art large vocabulary speech recognition systems .","label":"Uses","metadata":{},"score":"87.5805"}{"text":"Results obtained on 175 manually - labeled songs provided an increase in accuracy of about 2 % . by Carsten Brockmann , Jon Oberlander - In Proc . of the UM'05 Workshop on Adapting the Interaction , 2005 . \" ...For a successful and satisfying interaction , a dialogue participant may align their language to be more like that of their interlocutor .","label":"Uses","metadata":{},"score":"87.83223"}{"text":"MIKOLOV Tomá¹ , DEORAS Anoop , KOMBRINK Stefan , BURGET Luká¹ a ÈERNOCKÝ Jan .Empirical Evaluation and Combination of Advanced Language Modeling Techniques .In : Proceedings of Interspeech 2011 .Florence : International Speech Communication Association , 2011 , s. 605 - 608 .","label":"Uses","metadata":{},"score":"87.98628"}{"text":"Cited by 1 Related articles All 8 versions Cite Save More .This document is part of the Project \" Machine Translation Enhanced Computer Assisted Translation ( MateCat ) \" , funded by the 7th Framework Programme of the European Commission through grant agreement no . : 287688 .","label":"Uses","metadata":{},"score":"88.31883"}{"text":"We als ... \" .In this paper we present IAM - OnDB- a new large online handwritten sentences database .It is publicly available and consists of text acquired via an electronic interface from a whiteboard .The database contains about 86 K word instances from an 11 K dictionary written by more than 200 writers .","label":"Uses","metadata":{},"score":"88.50502"}{"text":"S. F. Chen and J. T. Goodman .An empirical study of smoothing techniques for language modeling .Technical Report TR-10 - 98 , Dept . of Comp .Sci . , Harvard , 1998 .S. Della Pietra , V. Della Pietra , R. Mercer , and S. Roukos .","label":"Uses","metadata":{},"score":"88.50521"}{"text":"In Proceedings of the IEEE International Conference on Acoustics , Speech , and Sig- nal Processing , pages 633 - 636 , 1992 .S. Goldwater , T. L. Griffiths , and M. Johnson .Interpolat- ing between types and tokens by estimating power law generators .","label":"Uses","metadata":{},"score":"88.70175"}{"text":"..The ability to approxima ... . by Djoerd Hiemstra , Stephen Robertson , Hugo Zaragoza - In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , 2004 . \" ...We systematically investigate a new approach to estimating the parameters of language models for information retrieval , called parsimonious language models .","label":"Uses","metadata":{},"score":"88.90108"}{"text":"..The ability to approxima ... . by Djoerd Hiemstra , Stephen Robertson , Hugo Zaragoza - In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , 2004 . \" ...We systematically investigate a new approach to estimating the parameters of language models for information retrieval , called parsimonious language models .","label":"Uses","metadata":{},"score":"88.90108"}{"text":"SMT has made tremendous strides in less than two decades , and many popular techniques have only emerged within the last few years .This survey presents a tutorial overview of state - of - the - art SMT at the beginning of 2007 .","label":"Uses","metadata":{},"score":"89.07712"}{"text":"Author Keywords source - channel key - target resizing , language model , touch model . ... odel predicts that it is very unlikely compared to the key ' s ' .The key - target outlines are shown in heavy lines . \" ...","label":"Uses","metadata":{},"score":"89.07934"}{"text":"Cited by 1 Related articles All 3 versions Cite Save .Quality Estimation Software Extensions L Specia , K Shah , E Avramidis - 2013 - qt21.eu Page 1 .FP7-ICT Coordination and Support Action ( CSA ) QTLaunchPad( No . 296347 )","label":"Uses","metadata":{},"score":"89.261826"}{"text":"This loose definition , however , has led to many measures being proposed or adopted from fields such as geometry , s ... \" .This thesis is concerned with the measurement and application of lexical distributional similarity .Two words are said to be distributionally similar if they appear in similar contexts .","label":"Uses","metadata":{},"score":"89.28075"}{"text":"The LM is built on the Academia Sinica corpus ( Emerson , 2005 ) with IRSTLM toolkit ( Federico et al . , 2008 ) .Irstlm : an open source toolkit for handling large scale language models . ...Related articles All 4 versions Cite Save More . ...","label":"Uses","metadata":{},"score":"89.71245"}{"text":"Cited by 2 Related articles All 2 versions Cite Save More .An English - to - Hungarian Morpheme - based Statistical Machine Translation System with Reordering Rules LJ Laki , A Novák , B Siklósi - ACL 2013 , 2013 - aclweb.org ... task .","label":"Uses","metadata":{},"score":"90.03418"}{"text":"ISSN 1990 - 9772 .Jazyk publikace : .Název publikace : .Empirical Evaluation and Combination of Advanced Language Modeling Techniques .Název ( cs ) : .Empirická evaluace a kombinace pokroèilých technik jazykového modelování .language modeling , neural networks , model combination , speech recognition .","label":"Uses","metadata":{},"score":"90.348305"}{"text":"Standard and factored language models are analyzed in terms of applicability to the chord recognition task .Pitch class profile vectors that represent harmonic information are extracted from the given audio ... \" .This paper focuses on automatic extraction of acoustic chord sequences from a musical piece .","label":"Uses","metadata":{},"score":"90.51229"}{"text":"To automatically achieve this , an unsupervised clustering approach ... Related articles All 4 versions Cite Save More .Identifying multilingual Wikipedia articles based on cross language similarity and activity KN Tran , P Christen - Proceedings of the 22nd ACM international ... , 2013 - dl.acm.org ...","label":"Uses","metadata":{},"score":"90.560135"}{"text":"Allow the user to set this probability ( IRSTLM)4 . net / apps / mediawiki / irstlm / index . ...Related articles Cite Save More . 3.3 Results ...Marcello Federico , Nicola Bertoldi , and Mauro Cet- tolo .IRSTLM : an Open Source Toolkit for Page 7 . ...","label":"Uses","metadata":{},"score":"90.612976"}{"text":"Consequently linguistic knowledge beyond the lexicon level can be integrated in a recognizer .The LOB corpus contains 500 English texts , each consisting of about 2,000 words .These texts are of quit ... . \" ...This thesis is concerned with the measurement and application of lexical distributional similarity .","label":"Uses","metadata":{},"score":"90.824936"}{"text":"IRSTLM : an open source toolkit for handling large scale language models .In Interspeech 2008 , pages 1618 - 1621 , Brisbane , Australia . ...Cited by 3 Related articles All 2 versions Cite Save More .Constrained grammatical error correction using Statistical Machine Translation Z Yuan , M Felice - CoNLL-2013 , 2013 - aclweb.org ... systems . ...","label":"Uses","metadata":{},"score":"91.18033"}{"text":"Along the way , we present a taxonomy of some different approaches within these areas .We conclude with an overview of evaluation and notes on future directions . by Xiaojin Zhu , Ronald Rosenfeld - Acoustics , Speech , and Signal Processing , 2001 .","label":"Uses","metadata":{},"score":"91.42047"}{"text":"We are concerned with LMs for multiparty meetings .The multimodal and interactive nature of gr ... . bySongfang Huang , Steve Renals - Proc . of Machine Learning for Multimodal Interaction ( MLMI'08 , 2008 . \" ... Abstract .","label":"Uses","metadata":{},"score":"91.48349"}{"text":"To estimate the fluency of the descriptions we use IRSTLM [ 6 ] which is based on n - gram statistics of TACoS. The final ...Cited by 3 Related articles All 9 versions Cite Save .System Description of BJTU - NLP MT for NTCIR-10 PatentMT P Wu , J Xu , Y Yin , Y Zhang - Proceedings of NTCIR , 2013 - research.nii.ac.jp ...","label":"Uses","metadata":{},"score":"91.58676"}{"text":"We find that the HMMs es - timated by EM generally assign a roughly equal number of word tokens to each hid - den state , while the empirical distribution of tokens ... \" .This paper investigates why the HMMs es - timated by Expectation - Maximization ( EM ) produce such poor results as Part - of - Speech ( POS ) taggers .","label":"Uses","metadata":{},"score":"92.10499"}{"text":"Extensions of this approach exploit distributional characteristics of n - gram data to reduce storage costs , including variable length coding of values and the use of tiered structures that partition the data for more efficient storage .We apply our approach to storing the full Google Web1 T n - gram set and all 1-to-5 grams of the Gigaword newswire corpus .","label":"Uses","metadata":{},"score":"92.12577"}{"text":"All 3 versions Cite Save More .Topic models for translation quality estimation for gisting purposes R Rubino , J de Souza , J Foster , L Specia - 2013 - doras.dcu.ie ... and SYSTRAN . ...Cited by 2 Related articles All 2 versions Cite Save .","label":"Uses","metadata":{},"score":"92.13052"}{"text":"In this paper we present generalized networks of Dirichlet distributions , and show how , using the two - parameter Poisson - Dirichlet distribution and Gibbs sampling , one can do approximate inference over them .This involves integrating out the proba - bility vectors but leaving auxiliary discrete count vectors in their place .","label":"Uses","metadata":{},"score":"92.139496"}{"text":"Cited by 5 Related articles All 16 versions Cite Save More .Parameter Optimization for Iterative Confusion Network Decoding in Weather - Domain Speech Recognition S Jalalvand , D Falavigna - eu - bridge .iCPE : A Hybrid Data Selection Model for SMT Domain Adaptation L Wang , DF Wong , LS Chao , Y Lu , J Xing - ...","label":"Uses","metadata":{},"score":"92.17073"}{"text":"It is entirely possible that two techniques that work well separately will not work well together , and , as we will show , even possible that some techniques will work better together than either one does by itself .In this ... . \" ...","label":"Uses","metadata":{},"score":"93.474396"}{"text":"It is entirely possible that two techniques that work well separately will not work well together , and , as we will show , even possible that some techniques will work better together than either one does by itself .In this ... . \" ...","label":"Uses","metadata":{},"score":"93.474396"}{"text":"Brocki - Intelligent Tools for Building a Scientific ... , 2013 - Springer ...Page 6 .494 D. Koržinek , K. Marasek , and ?Brocki Table 1 .Experiment results comparing our system to the Julius baseline using models from IRSTLM on a 30k and 60k vocabulary . ...","label":"Uses","metadata":{},"score":"93.686264"}{"text":"For LM adaptation , unigram and bigram LSA are integrated into the background N - gram LM via marginal adaptation and linear interpolation respectively .Experimental results on the Mandarin RT04 test set show that applying unigram and bigram LSA together yields 6%-8 % relative perplexity reduction and 2.5 % relative character error rate reduction which is statistically significant compared to applying only unigram LSA .","label":"Uses","metadata":{},"score":"93.78775"}{"text":"We present our modeling framework for topic and role on the AMI Meeting Corpus , and illustrate the effectiveness of the approach in the context of adapting a baseline language model in a large - vocabulary automatic speech recognition system for multiparty meetings .","label":"Uses","metadata":{},"score":"93.82811"}{"text":"IRSTLM : an open source toolkit for handling large scale language models . ...Related articles All 10 versions Cite Save .Edit Distance : A New Data Selection Criterion for Domain Adaptation in SMT . 3.3 Baseline System ... Related articles All 2 versions Cite Save More . ...","label":"Uses","metadata":{},"score":"94.0188"}{"text":"Pitch class profile vectors that represent harmonic information are extracted from the given audio signal .The resulting chord sequence is obtained by running a Viterbi decoder on trained hidden Markov models and subsequent lattice rescoring , applying the language model weight .","label":"Uses","metadata":{},"score":"94.41985"}{"text":"Abstract --gram models are the most widely used language models in large vocabulary continuous speech recognition .Since the size of the model grows rapidly with respect to the model order and available training data , many methods have been proposed for pruning the least relevant - grams from the model .","label":"Uses","metadata":{},"score":"94.504486"}{"text":"However , these efforts only support simple s .. by Vesa Siivola , Teemu Hirsimäki , Sami Virpioja - IEEE Trans .on Audio , Speech , and Language Processing . \" ...Abstract --gram models are the most widely used language models in large vocabulary continuous speech recognition .","label":"Uses","metadata":{},"score":"95.97484"}{"text":"In Proceed- ings of Interspeech , Brisbane , Australia .Najeh Hajlaoui and Andrei Popescu - Belis . ...Cited by 1 Related articles All 10 versions Cite Save .Statistical Machine Translation Model for English to Urdu Machine Translation RB Mishra - Artificial Intelligence and Soft Computing - researchgate.net ... translations .","label":"Uses","metadata":{},"score":"96.63609"}{"text":"A fast and flexible architecture for very large word n - gram datasets M Flor - Natural Language Engineering , 2013 - Cambridge Univ Press Page 1 .Natural Language Engineering 19 ( 1 ) : 61 - 93 .c Cambridge University Press 2012 doi:10.1017/S1351324911000349 61 A fast and flexible architecture for very large word n - gram datasets MICHAEL FLOR NLP and ...","label":"Uses","metadata":{},"score":"97.20722"}{"text":"Tools . by Mark Johnson - In : Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ( EMNLP - CoNLL ) .Prague , Czech Republic : Association for Computational Linguistics , 2007 . \" ...","label":"Uses","metadata":{},"score":"97.72268"}{"text":"Cited by 6 Related articles All 4 versions Cite Save .Statistical machine translation system for English to Urdu RB Mishra - International Journal of Advanced Intelligence ... , 2013 - Inderscience ...Modified Kneser - Ney discounting is used as smoothing scheme for training 5-gram language model .","label":"Uses","metadata":{},"score":"97.87038"}{"text":"IRSTLM : an open source toolkit for handling large scale language models . ...Cited by 1 Related articles Cite Save More .Rule Based Transliteration Scheme for English to Punjabi D Bhalla , N Joshi , I Mathur - arXiv preprint arXiv:1307.4300 , 2013 - arxiv.org ...","label":"Uses","metadata":{},"score":"98.262405"}{"text":"CIKM ' 01 , Nov 5 - 10 , 2001 , Atlanta , GA .Copyright 2001 ACM 1 - 58113 - 000 - 0/00/0000 ... $5.00 .Our second hypothesis was that statistical language models could capture the content information related to reading difficulty .","label":"Uses","metadata":{},"score":"98.34844"}{"text":"The phrase - based baseline decoder includes ...Cited by 1 Related articles All 11 versions Cite Save More .How hard is it to automatically translate phrasal verbs from English to French ? statmt .org / moses/ ? Baseline . ing the grow - diag - final heuristic .","label":"Uses","metadata":{},"score":"98.416245"}{"text":"Improved backing - off for m - gram language modeling .In Proceedings of the IEEE Interna- tional Conference on Acoustics Speech and Signal Pro- cessing , volume 1 , pages 181 - 184 , 1995 .R. Kneser and V. Steinbiss .","label":"Uses","metadata":{},"score":"98.442856"}{"text":"Related articles All 3 versions Cite Save More .Are ACT 's scores increasing with better translation quality ?N Hajlaoui - Are ACT \" s scores increasing with better translation ... , 2013 - infoscience.epfl.ch ...Marcello Federico , Nicola Bertoldi , and Mauro Cet- tolo .","label":"Uses","metadata":{},"score":"99.81643"}{"text":"Motivation : Many experimental and algorithmic approaches in biology generate groups of genes that need to be examined for related functional properties .For example , gene expression profiles are frequently organized into clusters of genes that may share functional properties .","label":"Uses","metadata":{},"score":"100.52768"}{"text":"Motivation : Many experimental and algorithmic approaches in biology generate groups of genes that need to be examined for related functional properties .For example , gene expression profiles are frequently organized into clusters of genes that may share functional properties .","label":"Uses","metadata":{},"score":"100.52768"}{"text":"Soft keyboards offer touch - capable mobile and tabletop devices many advantages such as multiple language support and space for larger graphical displays .On the other hand , because soft keyboards lack haptic feedback , users often produce more typing errors .","label":"Uses","metadata":{},"score":"100.650406"}{"text":"Soft keyboards offer touch - capable mobile and tabletop devices many advantages such as multiple language support and space for larger graphical displays .On the other hand , because soft keyboards lack haptic feedback , users often produce more typing errors .","label":"Uses","metadata":{},"score":"100.650406"}{"text":"\" We used the CRF sampler outlined in Section 5 with the addition of Metropolis - Hastings updates for the discount parameters ( Wood & Teh , 2009 ) .The discounts in the collapsed node restaurants are products of subsets of discount parameters making other approaches difficult .","label":"Uses","metadata":{},"score":"102.450516"}{"text":"For a successful and satisfying interaction , a dialogue participant may align their language to be more like that of their interlocutor .In the first part of this paper , we examine the alignment phenomenon from the viewpoint of personalityrelated , linguistic , sociolinguistic and psycholinguistic research , concluding that some people are stronger aligners than others .","label":"Uses","metadata":{},"score":"102.57978"}{"text":"biblio.unitn.it Page 1 . PhD Dissertation International Doctorate School in Information and Communication Technologies DISI - University of Trento Linguistically Motivated Reordering Modeling for Phrase - Based Statistical Machine Translation Arianna Bisazza Advisor : ... Cited by 1 Related articles All 3 versions Cite Save Tools . \" ...","label":"Uses","metadata":{},"score":"102.81259"}{"text":"Cited by 2 Related articles All 2 versions Cite Save .Model for English - Urdu Statistical Machine Translation A Ali , A Hussain , MK Malik - World Applied Sciences Journal , 2013 - idosi.org ...The model is trained on TrainSet using Moses Conclusion and Future Work : There are certain words in translation setup with language modeling toolkit IRSTLM .","label":"Uses","metadata":{},"score":"103.20694"}{"text":"Copyright 2009 by the authors . plications ( Rosenfeld 2000 ) .The phrase domain adap- tation describes procedures that take a model trained on a large amount of non - specific data and adapt it to work well for a specific domain for which less training data is available .","label":"Uses","metadata":{},"score":"103.956924"}{"text":"Wherever ...Cited by 1 Related articles All 5 versions Cite Save More . ...Cited by 1 Related articles All 9 versions Cite Save .Translating video content to natural language descriptions M Rohrbach , W Qiu , I Titov , S Thater ... - ... Vision ( ICCV ) , 2013 ... , 2013 - ieeexplore.ieee.org ... probability .","label":"Uses","metadata":{},"score":"106.33999"}{"text":"New dedicated ...Cited by 1 Related articles All 2 versions Cite Save More .The weights ... Related articles Cite Save More .Lexicon - supported OCR of eighteenth century Dutch books : a case study J de Does , K Depuydt - IS&T / SPIE ... , 2013 - proceedings.spiedigitallibrary.org ... 5th ACL - HLT Workshop on Language Technology for Cultural Heritage , Social Sciences and Humanities , 33 - 38 ( 2011 ) .","label":"Uses","metadata":{},"score":"109.89726"}{"text":"Automatically detected acoustic landmarks for assessing natural emotion from speech Hervé SIERRO herve.sierro@unifr.ch , BENEFRI Master Student Document , Image and Voice Analysis group University of Fribourg Thesis Supervisor : Dr. Fabien RINGEVAL ...Cited by 1 Related articles All 5 versions Cite Save More .","label":"Uses","metadata":{},"score":"110.03075"}{"text":"Machine Translation of Film Subtitles from English to Spanish J Isele - 2013 - mlta.uzh.ch Page 1 .Institut für Computerlinguistik Machine Translation of Film Subtitles from English to Spanish Combining a Statistical System with Rule - based Grammar Checking Masterarbeit der Philosophischen Fakultät der Universität Zürich Referent : Prof. Dr. M. Volk Verfasserin : ... Related articles Cite Save More .","label":"Uses","metadata":{},"score":"112.38159"}{"text":"Related articles Cite Save More .Computing n - gram statistics in MapReduce K Berberich , S Bedathur - ... of the 16th International Conference on ... , 2013 - dl.acm.org Page 1 .Computing n - Gram Statistics in MapReduce Klaus Berberich Max Planck Institute for Informatics Saarbrücken , Germany kberberi@mpi-inf.mpg.de Srikanta Bedathur Indraprastha Institute of Information Technology New Delhi , India bedathur@iiitd.ac.in ...","label":"Uses","metadata":{},"score":"115.8224"}