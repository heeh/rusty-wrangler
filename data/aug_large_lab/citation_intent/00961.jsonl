{"text":"In most applications , edge weights are computed through a complex data - modelling process and convey crucially important information for classifying nodes .Appearing in Proceedings of the 27 th Inte ... . \" ...We present a general learning - based approach for phrase - level sentiment analysis that adopts an ordinal sentiment scale and is explicitly compositional in nature .","label":"Uses","metadata":{},"score":"23.10611"}
{"text":"Later work by Pang et al .( Pang & Lee , 2004 ) extends the work in this paper by classifying document only on subjective sentences , and utilization of pair - wise interaction information between nearby sentences .( Pang & Lee , 2005 ) further extends the binary classification of sentiment to a multi - point scale ( multi - class classification ) .","label":"Uses","metadata":{},"score":"27.131752"}
{"text":"Pang and Lee ( 2004 ) introduce a hierarchical approach to classification .Specifically , they use the subjectivity classifier to extract subjective sentences from reviews to be used for polarity classification .Hierarchical models are quite common in the classification and general statistics and machine learning literatures .","label":"Uses","metadata":{},"score":"27.246002"}
{"text":"We could also build a class that read both models in from a file , or took both classifiers as parameters , or any number of other solutions in - between .By constructing a hierarchical classifier in any of these ways , it may be supplied to an evaluator .","label":"Uses","metadata":{},"score":"27.376204"}
{"text":"We could also build a class that reads both models in from a file , or takes both classifiers as parameters , or any number of other solutions in - between .By constructing a hierarchical classifier in any of these ways , it may be supplied to an evaluator .","label":"Uses","metadata":{},"score":"27.382988"}
{"text":"In ICCV , 2009 .Sentiment analysis is a growing field of research , driven by both commercial applications and academic interest .We show how to map discrete affective states into ordinal scales in these two dimensions , based on the psychological model of Russell 's circumplex model of affect and label a previously available corpus with multidimensional , real - valued annotations .","label":"Uses","metadata":{},"score":"27.86509"}
{"text":"We used a similar dataset , as released by the authors , and did our best to use the same libraries and pre - processing techniques .+ In addition to replicating Pang 's work as closely as we could , we extended the work by exploring an additional dataset , additional preprocessing techniques , and combining classifiers .","label":"Uses","metadata":{},"score":"28.083122"}
{"text":"For example , combining an adverb ... \" .We present a general learning - based approach for phrase - level sentiment analysis that adopts an ordinal sentiment scale and is explicitly compositional in nature .Thus , we can model the compositional effects required for accurate assignment of phrase - level sentiment .","label":"Uses","metadata":{},"score":"28.726486"}
{"text":"Our method can also be interpreted as conducting a constrained embedding in a transformed space , or a ranking on the graph .Experimental results on three real applications demonstrate the benefits of the proposed method over existing alternatives 1 . ...","label":"Uses","metadata":{},"score":"30.996517"}
{"text":"The main idea is to gather visual rationales alongside traditional image labels , in order to use human insight more fully to better train a discriminative classifier . \"Rationales \" are a way of getting more information from a single annotation by a human annotator : we ask the annotator not only for the image 's class label , but additionally for information about \" why \" they chose that label .","label":"Uses","metadata":{},"score":"31.244312"}
{"text":"For this , two diverse global inference paradigms are used : a supervised collective classification framework and an unsupervised optimization framework .Both approaches perform substantially better than baseline approaches , establishing the efficacy of the methods and the underlying discourse scheme .","label":"Uses","metadata":{},"score":"31.46861"}
{"text":"This follows the linguistic intuition that rich contextual information should be useful in these tasks .We present a framework which combines a ... \" .In this paper , we investigate how modeling content structure can benefit text analysis applications such as extractive summarization and sentiment analysis .","label":"Uses","metadata":{},"score":"31.669098"}
{"text":"This section covers a second form of sentiment analysis , namely determining if a sentence is \" objective \" or \" subjective \" ( again , as defined by the database curators ) .It follows pretty much the same pattern as the last example , with a slightly different data format , the addition of the classifier evaluation framework from com.aliasi.classify , and a step to compile the model to a file for later use .","label":"Uses","metadata":{},"score":"31.884813"}
{"text":"This section covers a second form of sentiment analysis , namely determining if a sentence is \" objective \" or \" subjective \" ( again , as defined by the database curators ) .It follows pretty much the same pattern as the last example , with a slightly different data format , the addition of the classifier evaluation framework from com.aliasi.classify , and a step to compile the model to a file for later use .","label":"Uses","metadata":{},"score":"31.884813"}
{"text":"For either case , we show how to map the response to synthetic contrast examples , and then exploit an existing large - margin learning technique to refine the decision boundary accordingly .Results on multiple scene categorization and human attractiveness tasks show the promise of our approach , which can more accurately learn complex categories with the explanations behind the label choices .","label":"Uses","metadata":{},"score":"32.478264"}
{"text":"We present a framework which combines a supervised text analysis application with the induction of latent content structure .Both of these elements are learned jointly using the EM algorithm .The induced content structure is learned from a large unannotated corpus and biased by the underlying text analysis task .","label":"Uses","metadata":{},"score":"32.5812"}
{"text":"When the number of categories reaches the magnitude of tens of thousands or higher , the conventional approach of using all the documents to train a two - way classifier per category is no longer computationally feasible .Liu et al .","label":"Uses","metadata":{},"score":"32.772034"}
{"text":"We particularly target subjective , perceptual labeling tasks .We first explain the notion of contrast examples as used in prior NLP rationales work [ 1 ] , and then define our two proposed forms of visual rationale .We define and use rationales in a support vector machine ( SVM ) based on the approach of Zaidan et al .","label":"Uses","metadata":{},"score":"32.955025"}
{"text":"Text mining , Discourse , Machine learning , Linguistic processing .CITATION .[ 1 ] A. Abbasi , H. Chen , and A. Salem , \" Sentiment Analysis in Multiple Languages : Feature Selection for Opinion Classification in Web Forums , \" ACM Trans .","label":"Uses","metadata":{},"score":"33.124203"}
{"text":"Our approach enhances machine learning algorithms with features generated from domain - specific and common - sense knowledge .This knowledge is represented by ontologies that contain hundreds of thousands of concepts , further enriched through controlled Web crawling .Prior to text categorization , a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning .","label":"Uses","metadata":{},"score":"33.317978"}
{"text":"Inspired by recent work on distributional approaches to compositionality , we model each word as a matrix and combine words using iterated matrix multiplication , which allows for the modeling of both additive and multiplicative semantic effects .Although the multiplication - based matrix - space framework has been shown to be a theoretically elegant way to model composition ( Rudolph and Giesbrecht , 2010 ) , training such models has to be done carefully : the optimization is nonconvex and requires a good initial starting point .","label":"Uses","metadata":{},"score":"33.484077"}
{"text":"Despite its simplicity , the result reported in this paper is much better than that in Turney_ACL_2001 .The improved classification accuracy demonstrates the powerfulness of automatic machine learning , but is also attributable to the availability of labeled data in Pang 's case .","label":"Uses","metadata":{},"score":"34.33046"}
{"text":"In this study we compared several feature representations for affect analysis , including learned n - grams and various automatically and manually crafted affect lexicons .We also proposed the support vector regression correlation ensemble ( SVRCE ) method for enhanced classification of affect intensities .","label":"Uses","metadata":{},"score":"34.507732"}
{"text":"Pang et al , EMNLP 2002 .From ScribbleWiki : Analysis of Social Media .An influential paper that 's the first to propose applying supervised machine learning techniques to the problem of sentiment classification , without using any prior knowledge .","label":"Uses","metadata":{},"score":"35.24328"}
{"text":"Thus the differences among these methods only come from their loss functions .\\ ] .\\ ] .\\ ] .Those experiments also showed that removing the regularization term in the objective functions of these classifiers resulted in significant performance degradation .","label":"Uses","metadata":{},"score":"35.338806"}
{"text":"For the actual test case of a domain - dependent review , the review 's rating is predicted by aggregating the scores of all opinions in the review and combining it with a domaindependent unigram model .The paper presents a constrained ridge regression algorithm for learning opinion scores .","label":"Uses","metadata":{},"score":"35.582645"}
{"text":"Even so , 85 % is higher than our standalone simple classifier , and 86.4 % is the best hierarchical performance reported by Pang and Lee using SVMs to classify polarity stacked on top of a naive Bayes subjectivity classifier .Inspecting the Code .","label":"Uses","metadata":{},"score":"35.833412"}
{"text":"This is because we do n't actually have an implementation of the Classifier interface ( see the next section for an illustration of how to create one ) .Instead , we 'll create classifications on our own and add them to the evaluation .","label":"Uses","metadata":{},"score":"36.45562"}
{"text":"The two ways of measuring performance are complementary to each other , and both are informative .Other evaluation metrics include utility functions defined over weighted TP , FP , TN and FN , and rank - based metrics like Mean Average Precision ( MAP ) which evaluates the ability of a system to rank categories for a given document .","label":"Uses","metadata":{},"score":"36.497032"}
{"text":"This work investigates design choices in modeling a discourse scheme for improving opinion polarity classification .For this , two diverse global inference paradigms are used : a supervised collective classification framework and an unsupervised optimization framework .Both approaches perform substant ... \" .","label":"Uses","metadata":{},"score":"36.538784"}
{"text":"This illustrates how the evaluator may be used without an embedded classifier -- cases are just added in terms of the first - best answer and the response classification .The meat of this implementation is in pulling out the subjective sentences .","label":"Uses","metadata":{},"score":"37.61894"}
{"text":"Abstract .We demonstrate that it is possible to perform automatic sentiment classification in the very noisy domain of customer feedback data .We show that by using large feature vectors in combination with feature reduction , we can train linear support vector machines that achieve high classification accuracy on data that present classification challenges even for a human annotator .","label":"Uses","metadata":{},"score":"37.685883"}
{"text":"For example , one might want to classify documents by sentiment ( Pang & Lee , 2002 ) : is a review positive or negative ?Or one might want to classify a message as deceptive or not .While representations and methods developed for topic - based classification are applicable to these classification problems as well , special considerations reflecting the nature of the classification task will probably lead to improved performance .","label":"Uses","metadata":{},"score":"37.884277"}
{"text":"The first line simply breaks the review into sentences .We then create a priority queue of objects ordered by score with a maximum size of the maximum number of sentences returned .Then we just iterate over the sentences and classify them with the subjectivity classifier we created in the constructor .","label":"Uses","metadata":{},"score":"37.927673"}
{"text":"The first line simply breaks the review into sentences .We then create a priority queue of objects ordered by score with a maximum size of the maximum number of sentences returned .Then we just iterate over the sentences and classify them with the subjectivity classifier we created in the constructor .","label":"Uses","metadata":{},"score":"37.927673"}
{"text":"This tutorial essentially reimplements the basic classifiers and then the hierarchical classification technique described in Bo Pang and Lillian Lee 's 2004 ACL paper \" A sentimental education . \"Downloading Training Corpora .Luckily for us , Lillian Lee and Bo Pang have provided annotated slices of movie review data for polarity ( both boolean and scalar ) , and subjectivity .","label":"Uses","metadata":{},"score":"37.965477"}
{"text":"Hierarchical Polarity Analysis .Pang and Lee ( 2004 ) introduce a hierarchical approach to classification .Specifically , they use the subjectivity classifier to extract subjective sentences from reviews to be used for polarity classification .Hierarchical models are quite common in the classification and general statistics and machine learning literatures .","label":"Uses","metadata":{},"score":"38.48477"}
{"text":"There are , of course , more efficient ways to write this code , and ways to refactor so that the cut - and - pasted code is also shared , but we leave those improvements as exercises to the reader .","label":"Uses","metadata":{},"score":"38.987637"}
{"text":"There are , of course , more efficient ways to write this code , and ways to refactor so that the cut - and - pasted code is also shared , but we leave those improvements as exercises to the reader .","label":"Uses","metadata":{},"score":"38.987637"}
{"text":"Experimental results over a range of datasets confirm improved performance compared to the bag of words document representation . ... , which are assumed to be relevant to the query ; for example , characteristic terms from such documents may be used for query expansion ( Xu and Croft , 1996 ) .","label":"Uses","metadata":{},"score":"39.04105"}
{"text":"18 - 27 , 1999 .What is Sentiment Analysis ?Sentiment analysis involves classifying text based on its sentiment .While this may mean many things , in this tutorial , we focus on two types of classification problem : .","label":"Uses","metadata":{},"score":"39.091484"}
{"text":"[ 9 ] G. Grefenstette , Y. Qu , D.A. Evans , and J.G. Shanahan , \" Validating the Coverage of Lexical Resources for Affect Analysis and Automatically Classifying New Words Along Semantic Axes , \" Proc .AAAI Spring Symp .","label":"Uses","metadata":{},"score":"39.20067"}
{"text":"There are lots of startups in this area and conferences .This tutorial covers assigning sentiment to movie reviews using language models .There are many other approaches to sentiment .One we use fairly often is sentence based sentiment with a logistic regression classifier .","label":"Uses","metadata":{},"score":"39.215504"}
{"text":"We test our classifiers on an external dataset to see how well they generalize .+ Sentiment analysis , broadly speaking , is the set of techniques that allows detection of emotional content in text .This has a variety of applications : it is commonly used by trading algorithms to process news articles , as well as by corporations to better respond to consumer service needs .","label":"Uses","metadata":{},"score":"39.22654"}
{"text":"Others use sentence cohesion ( Pang and Lee , 2004 ) , agreement / disagreement between speakers ( Thomas et al . , 2006 ; Bansal et al . , 2008 ) , or structural adjacency .In contrast , our work focuses on dis ... . \" ...","label":"Uses","metadata":{},"score":"39.8961"}
{"text":"\\ ) Empirical evaluations have shown the performance of those non - linear classifiers comparable to stronger linear classifiers ( Lewis , 1994 ; Yang , 1994 , 1999 ; Wiener et al . , 1995 ; Joachims , 1998 , Li & Yang , 2003 ) .","label":"Uses","metadata":{},"score":"39.94629"}
{"text":"While decision trees , logical rules , and instance - based rules have been explored for text classification , the most commonly used type of classification rules are linear rules .Often an additional feature that always takes value 1 is added to simulate a constant offset .","label":"Uses","metadata":{},"score":"40.303764"}
{"text":"While they perform well in many categorization tasks , these methods are inherently limited when faced with more complicated tasks where external knowledge is essential .Recently , there have been efforts to augment these basic features with external knowledge , including semi - supervised learning and transfer learning .","label":"Uses","metadata":{},"score":"40.354202"}
{"text":"It has been empirically observed that linear classifiers with proper regularization are often sufficient for solving practical text categorization problems , with performance comparable or better than non - linear classifiers .Furthermore , linear methods are generally computationally efficient , both at training as well as at classification .","label":"Uses","metadata":{},"score":"40.431717"}
{"text":"2544 - 2558 , 2010 .[ 15 ] O. Zaidan , J. Eisner , and C. Piatko , \" Using ' Annotator Rationales ' to Improve Machine Learning for Text Categorization , \" Proc .Human Language Technologies : Conf .","label":"Uses","metadata":{},"score":"40.478336"}
{"text":"In deriving our characterization , we obtain a simple randomized algorithm achieving the optimal mistake bound on any weighted graph .Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant amortized time and linear space .","label":"Uses","metadata":{},"score":"40.67763"}
{"text":"Text classification rules are typically evaluated using performance measures from information retrieval .Common metrics for text categorization evaluation include recall , precision , accuracy and error rate and F1 .Given a test set of N documents , a two - by - two contingency table with four cells can be constructed for each binary classification problem .","label":"Uses","metadata":{},"score":"40.71282"}
{"text":"[ 10 ] M. Thelwall , K. Buckley , G. Paltoglou , C. Di , and A. Kappas , \" Sentiment Strength Detection in Short Informal Text , \" J. Am .Soc .Information Science Technology , vol .61 , no .","label":"Uses","metadata":{},"score":"40.72629"}
{"text":"This mechanism uses a graph - based learning technique .We evaluate the effectiveness ( predictive accuracy ) of this mechanism against a large real - world data set .We also evaluate the computational cost of a J2ME implementation on a mobile phone . ... and scalable .","label":"Uses","metadata":{},"score":"40.72985"}
{"text":"Basic Polarity Analysis .We begin with a simple classification exercise , amounting to training and testing our basic classifiers on ( different slices of ) the boolean polarity data .The resulting classifier is able to judge whether a whole movie review is essentially positive or negative ( as defined by the data set curators ) .","label":"Uses","metadata":{},"score":"40.731277"}
{"text":"Basic Polarity Analysis .We begin with a simple classification exercise , amounting to training and testing our basic classifiers on ( different slices of ) the boolean polarity data .The resulting classifier is able to judge whether a whole movie review is essentially positive or negative ( as defined by the data set curators ) .","label":"Uses","metadata":{},"score":"40.731277"}
{"text":"Meeting Assoc .Computational Linguistics ( ACL ' 05 ) , pp .115 - 124 , 2005 .[ 6 ] T. Wilson , J. Wiebe , and P. Hoffmann , \" Recognizing Contextual Polarity in Phrase - Level Sentiment Analysis , \" Proc .","label":"Uses","metadata":{},"score":"40.795174"}
{"text":"Web - page Taxonomy ( 2004 ) exhibits a power law .The scale of real - world text classification applications , both in terms of the number of classes as well as the ( often highly unbalanced ) number of training examples , poses interesting research challenges .","label":"Uses","metadata":{},"score":"40.843414"}
{"text":"Chapter Assoc . for Computational Linguistics ( NAACL HLT ' 07 ) , pp .260 - 267 , 2007 .[19 ] S. Baccianella , A. Esuli , and S. Fabrizio , \" Sentiwordnet 3.0 : An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining , \" Proc .","label":"Uses","metadata":{},"score":"40.868015"}
{"text":"This performance is much better at 92 % accuracy than the polarity classification results we saw in the last section .After the basic accuracy report , the confusion matrix is presented in a format to provide easy inclusion into a spreadsheet or other graphing package such as gnuplot . .","label":"Uses","metadata":{},"score":"40.877872"}
{"text":"While movie reviews have been the most studied domain , sentiment analysis has been extended to a number of new domains , ranging from stock message boards to congressional floor debates [ 25 , 61].Res ... . by Jing Gao , Feng Liang , Wei Fan , Yizhou Sun , Jiawei Han - Advances in Neural Information Processing Systems ( NIPS , 2009 . \" ...","label":"Uses","metadata":{},"score":"40.99185"}
{"text":"218 - 229 , 2007 .[ 33 ] S. Gobron , J. Ahn , G. Paltoglou , M. Thelwall , and D. Thalmann , \" From Sentence to Emotion : A Real - Time Three - Dimensional Graphics Metaphor of Emotions Extracted from Text , \" Visual Computer , vol .","label":"Uses","metadata":{},"score":"41.113235"}
{"text":"Li F. , Yang Y. ( 2003 )A Loss Function Analysis for Classification Methods in Text Categorization .International Conference on Machine Learning ( ICML ) : 472 - 479 .Liu T. , Yang Y. , Wan H. , Zeng H. , Chen Z. , Ma W. ( 2005 ) Support vector machines classification with a very large - scale taxonomy .","label":"Uses","metadata":{},"score":"41.731865"}
{"text":"Using this training sample , a supervised learning algorithm aims to find the optimal classification rule , i.e. , a function mapping from the p - dimensional feature space to the one - dimensional class label .Optimal generally means that the classification rule can both accurately classify training documents and generalize well to new documents beyond the training set .","label":"Uses","metadata":{},"score":"41.904984"}
{"text":"Next comes the same accuracy report as we computed by hand in the last demo .This performance is much better at 92 % accuracy than the polarity classification results we saw in the last section .After the basic accuracy report , the confusion matrix is presented in a format to provide easy inclusion into a spreadsheet or other graphing package such as gnuplot . .","label":"Uses","metadata":{},"score":"42.11221"}
{"text":"In this paper , we study ensemble learning with output from multiple supervised and unsupervised models , a topic where little work has been done .Although unsupervised models , such as clustering , do not directly generate label prediction for each individual , they provide useful constraints for the joint prediction of a set of related objects .","label":"Uses","metadata":{},"score":"42.21569"}
{"text":"Coupled with the ability to generalize concepts using the ontology , this approach addresses two significant problems in natural language processing - synonymy and polysemy .Categorizing documents with the aid of knowledge - based features leverages information that can not be deduced from the training documents alone .","label":"Uses","metadata":{},"score":"42.307274"}
{"text":"A boosting - based system for text categorization .Machine Learning , 39(2/3):135 - 168 .Sebastiani F. ( 2002 ) Machine learning in automated text categorization .ACM Computing Surveys , 34(1):1 - 47 .Yang Y. , Liu X. ( 1999 )","label":"Uses","metadata":{},"score":"42.651855"}
{"text":"71 - 78 , 2004 .[ 10 ] G. Grefenstette , Y. Qu , J.G. Shanahan , and D.A. Evans , \" Coupling Niche Browsers and Affect Analysis for an Opinion Mining Application , \" Proc .12th Int'l Conf .","label":"Uses","metadata":{},"score":"42.698753"}
{"text":"Who 's Idea was This ?This tutorial essentially reimplements the basic classifiers and then the hierarchical classification technique described in Bo Pang and Lillian Lee 's 2004 ACL paper \" A sentimental education . \"Downloading Training Corpora .Luckily for us , Lillian Lee and Bo Pang have provided annotated slices of movie review data for polarity ( both boolean and scalar ) , and subjectivity .","label":"Uses","metadata":{},"score":"42.70547"}
{"text":"Contents .Instead of manually classifying documents or hand - crafting automatic classification rules , statistical text categorization uses machine learning methods to learn automatic classification rules based on human - labeled training documents .A specific version of this approach is the TF - IDF term weighting scheme .","label":"Uses","metadata":{},"score":"42.743816"}
{"text":"Another way of averaging is to sum over TP , FP , TN , FN and N over all the categories first , and then compute each of the above metrics .The resulted scores are called micro - averaged .Macro - averaging gives an equal weight to each category , and is often dominated by the system 's performance on rare categories ( the majority ) in a power - law like distribution .","label":"Uses","metadata":{},"score":"43.148674"}
{"text":"ACM Special Interest Group of Information Retrieval ( SIGIR ) : 42 - 49 .Zhang T and Oles F. ( 2001 )Text Categorization Based on Regularized Linear Classification Methods .Information Retrieval 4(1):5 - 31 .Analysis of affective intensities in computer mediated communication is important in order to allow a better understanding of online users ? emotions and preferences .","label":"Uses","metadata":{},"score":"43.190186"}
{"text":"Traditional supervised visual learning simply asks annotators \" what \" label an image should have .We propose an approach for image classification problems requiring subjective judgment that also asks \" why \" , and uses that information to enrich the learned model .","label":"Uses","metadata":{},"score":"43.1976"}
{"text":"79 - 86 , 2002 .[20 ] B. Pang and L. Lee , \" Seeing Stars : Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales , \" Proc .Ann .Meeting on Assoc . for Computational Linguistics ( ACL ' 05 ) , pp.115 - 124 , 2005 .","label":"Uses","metadata":{},"score":"43.310455"}
{"text":"We cast this ensemble task as an optimization problem on a bipartite graph , where the objective function favors the smoothness of the prediction over the graph , as well as penalizing deviations from the initial labeling provided by supervised models .","label":"Uses","metadata":{},"score":"43.35837"}
{"text":"Knowledge Discovery and Data Mining ( KDD ' 08 ) , pp .408 - 416 , 2008 .[46 ] J. Ahn , S. Gobron , Q. Silvestre , and D. Thalmann , \" Asymmetrical Facial Expressions Based on an Advanced Interpretation of Two - Dimensional Russell 's Emotional Model , \" Proc .","label":"Uses","metadata":{},"score":"44.03638"}
{"text":"Here , a string subjReview is the result of applying the method subjectiveSentences to the review .This extracts the subjective sentences and returns them as a string .We then create a classification using the filtered intput subjReview .Finally , we add it as a case to the evaluator .","label":"Uses","metadata":{},"score":"44.04933"}
{"text":"Ablation testing showed that the improved performance of SVRCE was attributable to its use of feature ensembles as well as affect correlation information .A brief case study was conducted to illustrate the utility of the features and techniques for affect analysis of large archives of online discourse .","label":"Uses","metadata":{},"score":"44.115044"}
{"text":"Their potential , however , is limited in applications which have no access to raw data but to the meta - level model output .In this paper , we study ensemble learni ... \" .Ensemble classifiers such as bagging , boosting and model averaging are known to have improved accuracy and robustness over a single model .","label":"Uses","metadata":{},"score":"44.268402"}
{"text":"To apply SSL algorithms on our problem , we ... . \" ...Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features .While they perform well in many categorization tasks , these methods are inherently limited when faced with more complicated tasks where external knowledge is ... \" .","label":"Uses","metadata":{},"score":"44.31117"}
{"text":"Known as an instance - based or lazy learning method , kNN uses the k nearest neighbors of each new document in the training set to estimate the local likelihood of each category for the new document .Just as in the case of linear classifiers , it is important to find the right trade - off between empirical risk and model complexity for non - linear classifiers as well .","label":"Uses","metadata":{},"score":"44.365417"}
{"text":"In the context of the phrase - level sentiment analysis task , our experimental results show statistically significant improvements in performance over a bagof - words model . ... k is different : we classify phrases according to a single ordinal scale that combines both polarity and strength .","label":"Uses","metadata":{},"score":"44.41749"}
{"text":"Note that the category supplied to the evaluator is only used for evaluation purposes ; the classifier will be used to perform classification on the input sentence without reference to the reference category .The resulting scored classification is then added as an evaluation case with the specified reference category for computing results .","label":"Uses","metadata":{},"score":"44.498646"}
{"text":"The optimal value of Î» is typically determined via cross - validation .To analyze the differences and similarities among popular classifiers , let us look at three of the more successful linear classification methods in text categorization as examples : linear SVM , ridge regression and regularized logistic regression .","label":"Uses","metadata":{},"score":"44.50161"}
{"text":"However , such a system is useless .For this reason , recall , precision and F1 are more commonly used instead of accuracy and error in text categorization evaluations .In multi - label classification , the simplest method for computing an aggregate score across categories is to average the scores of all binary task .","label":"Uses","metadata":{},"score":"44.680275"}
{"text":"Web page taxonomy .They found the divide - and - conquer strategy not only addressed the scaling issue but also yielded better classification performance because of local optimization of classifiers based on domains and sub - domains .Finally , there are interesting challenges in new applications of text classification .","label":"Uses","metadata":{},"score":"44.717373"}
{"text":"Here , we construct a file using the specified pattern and then read all of the data from the file .We then split on newlines to derive the sentences .The number of training instances is set to 90 % of the input data .","label":"Uses","metadata":{},"score":"44.72172"}
{"text":"Here , we construct a file using the specified pattern and then read all of the data from the file .We then split on newlines to derive the sentences .The number of training instances is set to 90 % of the input data .","label":"Uses","metadata":{},"score":"44.72172"}
{"text":"..Basically , this is just another precision - recall evaluation over aggregate data .+ We implement a series of classifiers ( Naive Bayes , Maximum Entropy , and SVM ) to distinguish positive and negative sentiment in critic and user reviews .","label":"Uses","metadata":{},"score":"44.75808"}
{"text":"Although the classifier is n't very confident on average , what happens when we sort the output by confidence and return answers we are most confident in first ?That 's the report that we find in the conditional one - versus - all report ( this time for the positive category ) : .","label":"Uses","metadata":{},"score":"45.225067"}
{"text":"Details .Published in .Proceeding of COLING-04 , the 20th International Conference on Computational Linguistics Text categorization ( a.k.a . text classification ) is the task of assigning predefined categories to free - text documents .It can provide conceptual views of document collections and has important applications in the real world .","label":"Uses","metadata":{},"score":"45.31394"}
{"text":"This is illustrated in the remaining new code .Here , a string subjReview is the result of applying the method subjectiveSentences to the review .This extracts the subjective sentences and returns them as a string .We then create a classification using the filtered intput subjReview .","label":"Uses","metadata":{},"score":"45.395977"}
{"text":"Evaluation on two travel expression translation tasks demonstrates improvements of up to 2.6 BLEU points absolute and 2.8 % in PER .Sentiment classification on customer feedback data : noisy data , large feature vectors , and the role of linguistic analysis .","label":"Uses","metadata":{},"score":"45.479877"}
{"text":"+ Pang applied the bag - of - words method to positive and negative sentiment classification , but the same method can be extended to various other domains , including topic classification .We additionally chose to work with a set of 5000 Yelp reviews , 1000 for each of their five \" star \" rating .","label":"Uses","metadata":{},"score":"45.530014"}
{"text":"We use 30 iterations of the Limited - Memory Variable Metric ( L - BFGS ) parameter estimation .Pang used the Improved Iterative Scaling ( IIS ) method , but L - BFGS , a method that was invented after their paper was published , was found to out - perform both IIS and generalized iterative scaling ( GIS ) , yet another parameter estimation method .","label":"Uses","metadata":{},"score":"45.69738"}
{"text":"Training .The next step is to train the polarity classifier .It does this in exactly the same way as in the basic polarity demo using the method train ( ) .This brings up the possibility of only training on the subjective sentences of the training data , but when we did this , we found it hurt rather than helped performance , so we do not include that technique here .","label":"Uses","metadata":{},"score":"45.97797"}
{"text":"Then the classifier is used to produce a classification for a review string in a single line .Next , the classification 's best category is extracted as the result category of classification .If the result category matches the test category , the number of correct classifications is incremented .","label":"Uses","metadata":{},"score":"46.241333"}
{"text":"Then the classifier is used to produce a classification for a review string in a single line .Next , the classification 's best category is extracted as the result category of classification .If the result category matches the test category , the number of correct classifications is incremented .","label":"Uses","metadata":{},"score":"46.241333"}
{"text":"The micro - averaged results weight by case , not by category , treating all cases as equal .This is calculated by summing the one - versus - all matrices and presenting the result as a precision - recall evaluation .","label":"Uses","metadata":{},"score":"46.244415"}
{"text":"The micro - averaged results weight by case , not by category , treating all cases as equal .This is calculated by summing the one - versus - all matrices and presenting the result as a precision - recall evaluation .","label":"Uses","metadata":{},"score":"46.244415"}
{"text":"When the evaluation loop is done , we just call the toString ( ) method on the evaluator to print out the results .Note that the category supplied to the evaluator is only used for evaluation purposes ; the classifier will be used to perform classification on the input sentence without reference to the reference category .","label":"Uses","metadata":{},"score":"46.32243"}
{"text":"The way to implement add - one smoothing over LingPipe 's naive Bayes implementation is to collect all of the tokens during the first training pass in a set .Then they may be added as training data for both categories .","label":"Uses","metadata":{},"score":"46.69996"}
{"text":"In many cases , however , a document may have more than one associated category in a classification scheme , e.g. , a journal article could belong to computational biology , machine learning and some sub - domains in both categories .","label":"Uses","metadata":{},"score":"46.77385"}
{"text":"505 - 519 , June 2010 .[40 ] S.S. Keerthi , S. Sundararajan , K.-W. Chang , C.-J. Hsieh , and C.-J. Lin , \" A Sequential Dual Method for Large Scale Multi - Class Linear SVMs , \" Proc .","label":"Uses","metadata":{},"score":"46.78857"}
{"text":"Even more interestingly , despite the motivating example , verbs under - performed all other tests , while still being consistently better than random .The tests ranged from 60\\% to 67\\% accuracy , even sometimes doing worse than the 64\\% accurate human - based classifier from Pang 2002 .","label":"Uses","metadata":{},"score":"47.01053"}
{"text":"/lib / commons - math-1.1 . jar \" PolarityHierarchical POLARITY_DIR .This produces the following output after a minute or so on my desktop : .What this is telling us is that the classifier was not at all confident about most of its decisions .","label":"Uses","metadata":{},"score":"47.133057"}
{"text":"Joachims , T. ( 1998 ) .Text categorization with Support Vector Machines : Learning with many relevant features .In Machine Learning : ECML-98 , Tenth European Conference on Machine Learning , pp .137 - -142 .Lewis D.L. , Yang Y. , Rose T.G. , Li F. ( 2004 ) : RCV1 : A New Benchmark Collection for Text Categorization Research .","label":"Uses","metadata":{},"score":"47.236908"}
{"text":"This method runs through the categories , of which there are two in this demo .It then creates a directory using the polarity data directory and the name of the category .This only works for this demo because the data is organized into directories by category .","label":"Uses","metadata":{},"score":"47.25605"}
{"text":"This method runs through the categories , of which there are two in this demo .It then creates a directory using the polarity data directory and the name of the category .This only works for this demo because the data is organized into directories by category .","label":"Uses","metadata":{},"score":"47.25605"}
{"text":"However , when using bigrams , the MaxEnt and SVM classifiers did significantly better , achieving 3 - 4\\% better accuracy with part of speech tagging when measuring frequency and presence information .+ Intuitively , adjectives like ' ' beautiful ' ' , ' ' wonderful ' ' , and ' ' great ' ' hold valuable sentiment information , so we trained our classifiers after filtering out only the adjectives within reviews .","label":"Uses","metadata":{},"score":"47.62078"}
{"text":"For the plot category , recall was slightly lower than precision .The one - versus - all report for the plot category continues with histograms of rank , average rank , conditional and joint probabilities , just as before , but broken out one - versus - all : .","label":"Uses","metadata":{},"score":"47.63975"}
{"text":"For the plot category , recall was slightly lower than precision .The one - versus - all report for the plot category continues with histograms of rank , average rank , conditional and joint probabilities , just as before , but broken out one - versus - all : .","label":"Uses","metadata":{},"score":"47.63975"}
{"text":"The evaluation code this time is even simpler with the use of the evaluator .mCategories[i ] .tok.gt9.5000 \" ) ; .The first line simply creates an evaluator from the classifier and the array of categories .The greyed out parts of the code are identical to that in the training method .","label":"Uses","metadata":{},"score":"47.804253"}
{"text":"Writing the Model to a File .The remaining code in the train ( ) method simply compiles the model to a file : .This actually transforms the format , with the resulting model being much faster at runtime .A more robust implementation would handle the close in a finally block that made sure the file output stream was closed to ensure no dangling file pointers were held .","label":"Uses","metadata":{},"score":"47.95952"}
{"text":"Writing the Model to a File .The remaining code in the train ( ) method simply compiles the model to a file : .This actually transforms the format , with the resulting model being much faster at runtime .A more robust implementation would handle the close in a finally block that made sure the file output stream was closed to ensure no dangling file pointers were held .","label":"Uses","metadata":{},"score":"47.95952"}
{"text":"..The average precision being much higher than the accuracy tells us that we are doing a good job ranking by confidence in that we tend to be more correct when we are more confident .Macro- and Micro - Averaged Results .","label":"Uses","metadata":{},"score":"48.183517"}
{"text":"..The average precision being much higher than the accuracy tells us that we are doing a good job ranking by confidence in that we tend to be more correct when we are more confident .Macro- and Micro - Averaged Results .","label":"Uses","metadata":{},"score":"48.183517"}
{"text":"We also implemented and tested the effect of term frequency - inverse document frequency ( TF - IDF ) on classification results .+ For our experiments , we worked with movie reviews .The dataset contains 1000 positive reviews and 1000 negative reviews , each labeled with their true sentiment .","label":"Uses","metadata":{},"score":"48.287586"}
{"text":"These are the kinds of reports produced for information retrieval tasks as used , for example , in the Text Retrieval Conference ( TREC ) . ...These include cumulative statistics from the interpolated and uninterpolated precision - recall ( PR ) and receiver operating characteristic ( ROC ) curves determined as described in the scored precision - recall evaluation documentation .","label":"Uses","metadata":{},"score":"48.52661"}
{"text":"These are the kinds of reports produced for information retrieval tasks as used , for example , in the Text Retrieval Conference ( TREC ) . ...These include cumulative statistics from the interpolated and uninterpolated precision - recall ( PR ) and receiver operating characteristic ( ROC ) curves determined as described in the scored precision - recall evaluation documentation .","label":"Uses","metadata":{},"score":"48.52661"}
{"text":"26 , no .3,article 12 , July 2008 .[ 4 ] K.J. Cherkauer , \" Human Expert - Level Performance on a Scientific Image Analysis Task by a System Using Combined Artificial Neural Networks , \" Working Notes of the AAAI Workshop Integrating Multiple Learned Models , P. Chan , ed . , pp .","label":"Uses","metadata":{},"score":"48.541336"}
{"text":"Suffice it to say here that this kappa value is well within the rule - of - thumb range expected for \" reliable \" classification .The next basic statistics report on information - theoretic measures about the marginal and conditional distributions produced by the training data and the results : .","label":"Uses","metadata":{},"score":"48.690582"}
{"text":"Suffice it to say here that this kappa value is well within the rule - of - thumb range expected for \" reliable \" classification .The next basic statistics report on information - theoretic measures about the marginal and conditional distributions produced by the training data and the results : .","label":"Uses","metadata":{},"score":"48.690582"}
{"text":"It 's identical to the first demo in the way that it steps through data , so we do n't repeat that code here .Construction : Reading in the Model .The constructor in this implementation does all of the work of the one in the basic polarity demo in setting up member variables ( indicated by ellipses ( ... ) ) .","label":"Uses","metadata":{},"score":"48.70409"}
{"text":"Information about document structure has the potential to greatly reduce this ambiguity .In addition to commonly used lexical features , this se ... . byLizhen Qu , For Informatics , Georgiana Ifrim , Gerhard Weikum - In COLING , 2010 . \" ...","label":"Uses","metadata":{},"score":"48.749527"}
{"text":"And , of course , each of these classifiers has a few parameters to tweak .Each model may also be pruned , for further control .Just do n't be fooled by overtrained a posteriori results on a test set .","label":"Uses","metadata":{},"score":"48.803432"}
{"text":"These averages are over the one - versus - all results , which are broken out category - by - category at the end of the report ( see below ) .The thing to remember is that the macro - averaged results average the one - versus - all results with each category weighted equally : .","label":"Uses","metadata":{},"score":"48.87731"}
{"text":"These averages are over the one - versus - all results , which are broken out category - by - category at the end of the report ( see below ) .The thing to remember is that the macro - averaged results average the one - versus - all results with each category weighted equally : .","label":"Uses","metadata":{},"score":"48.87731"}
{"text":"We implement a technique that reduces a review to 5 to 25 sentences .This will be the five most subjective sentences as ranked by conditional probability of the subjectivity model , as well as up to 20 more sentences if they are 50 % or more likely to be subjective according to the subjectivity model .","label":"Uses","metadata":{},"score":"48.927155"}
{"text":"+ We set out to replicate Pang 's work from 2002 on using classical knowledge - free supervised machine learning techniques to perform sentiment classification .They used the machine learning methods ( Naive Bayes , maximum entropy classification , and support vector machines ) , methods commonly used for topic classification , to explore the difference between and sentiment classification in documents .","label":"Uses","metadata":{},"score":"49.10827"}
{"text":"..Basically , this is just another precision - recall evaluation over aggregate data .Tools . \" ...Using mobile devices , such as smart phones , people may create and distribute different types of digital content ( e.g. , photos , videos ) .","label":"Uses","metadata":{},"score":"49.18343"}
{"text":"The meat of this implementation is in pulling out the subjective sentences .There are many ways this can be configured to run .We implement a technique that reduces a review to 5 to 25 sentences .This will be the five most subjective sentences as ranked by conditional probability of the subjectivity model , as well as up to 20 more sentences if they are 50 % or more likely to be subjective according to the subjectivity model .","label":"Uses","metadata":{},"score":"49.395927"}
{"text":"The constructor in this implementation does all of the work of the one in the basic polarity demo in setting up member variables ( indicated by ellipses ( ... ) ) .It also reads in the subjectivity model from a hardwired file named subjectivity.model : .","label":"Uses","metadata":{},"score":"49.43623"}
{"text":"..One - Versus - All Evaluations .Next up are one - versus - all evaluations for each category .These indicate performance on a category - by - category basis .This is n't so interesting in our case , because both categories perform about equally well , which is typical in two - category problems with roughly balanced false positives and false negatives .","label":"Uses","metadata":{},"score":"49.459023"}
{"text":"..One - Versus - All Evaluations .Next up are one - versus - all evaluations for each category .These indicate performance on a category - by - category basis .This is n't so interesting in our case , because both categories perform about equally well , which is typical in two - category problems with roughly balanced false positives and false negatives .","label":"Uses","metadata":{},"score":"49.459023"}
{"text":"In fact , this tells us that by setting a threshold other than 0.50 for classification , we could achieve an 86.4 % f - measure on this task .This looks substantially better than the 85 % reported above , but is not very significant as we indicate in the Appendix 1 : Confidence Intervals .","label":"Uses","metadata":{},"score":"49.471977"}
{"text":"We also saw strong positive trends across all test configurations , classifying reviews with more stars more positively .","label":"Uses","metadata":{},"score":"49.613205"}
{"text":"The evaluation code this time is even simpler with the use of the evaluator .Classification classification .mCategories[i ] .tok.gt9.5000 \" ) ; .The first line simply creates an evaluator from the classifier and the array of categories .","label":"Uses","metadata":{},"score":"49.6902"}
{"text":"What this is telling us is that the classifier was not at all confident about most of its decisions .When this is the case , classifiers tend to have a very high variance with respect to parameter settings .Although the classifier is n't very confident on average , what happens when we sort the output by confidence and return answers we are most confident in first ?","label":"Uses","metadata":{},"score":"49.980385"}
{"text":"Int'l Language Resources and Evaluation ( LREC ' 10 ) , 2010 .[ 24 ] A. Neviarouskaya , H. Prendinger , and M. Ishizuka , \" Textual Affect Sensing for Sociable and Expressive Online Communication , \" Proc .Second Int'l Conf .","label":"Uses","metadata":{},"score":"50.01446"}
{"text":"In this section , we step through the source found in the file src / PolarityBasic . java .The program reads the training directory location from the command line , trains a classifier on the training data , then evaluates the classifier on the test data .","label":"Uses","metadata":{},"score":"50.173313"}
{"text":"In this section , we step through the source found in the file src / PolarityBasic . java .The program reads the training directory location from the command line , trains a classifier on the training data , then evaluates the classifier on the test data .","label":"Uses","metadata":{},"score":"50.173313"}
{"text":"In NAACL - HLT , 2007 .[ 2 ] L. Fei - Fei and P. Perona .A Bayesian Hierarchical Model for Learning Natural Scene Categories .In CVPR , 2005 .[ 3 ] N. Kumar , A. C. Berg , P. N. Belhumeur , and S. K. Nayar .","label":"Uses","metadata":{},"score":"50.535248"}
{"text":"As usual , our main method constructs an instance using the command - line arguments , then runs it .If any errors are thrown , it prints their stack traces .Constructor to Marshal Arguments .Also following our standard operating procedure ( SOP ) , the constructor sets up the member variables using the command - line arguments : .","label":"Uses","metadata":{},"score":"50.61444"}
{"text":"As usual , our main method constructs an instance using the command - line arguments , then runs it .If any errors are thrown , it prints their stack traces .Constructor to Marshal Arguments .Also following our standard operating procedure ( SOP ) , the constructor sets up the member variables using the command - line arguments : .","label":"Uses","metadata":{},"score":"50.61444"}
{"text":"We first consider the 15 Scene Categories dataset [ 2 ] , which consists of 15 indoor and outdoor scene classes , with 200 - 400 images per class .Below is an example of the MTurk interface to gather the scene categories rationales ( click to enlarge ) : .","label":"Uses","metadata":{},"score":"50.63057"}
{"text":"Current phrase - based statistical machine translation systems process each test sentence in isolation and do not enforce global consistency constraints , even though the test data is often internally consistent with respect to topic or style .We propose a new consistency model for machine translation in the form of a graph - based semi - supervised learning algorithm that exploits similarities between training and test data and also similarities between different test sentences .","label":"Uses","metadata":{},"score":"50.827625"}
{"text":"The priority queue keeps the items in ranked order up to the specified maximum .The next batch of code creates a buffer into which to append the subjective sentences .We create an iterator over the elements in the priority queue , which returns the item in order of highest estimated probability of being subjective .","label":"Uses","metadata":{},"score":"50.83315"}
{"text":"The priority queue keeps the items in ranked order up to the specified maximum .The next batch of code creates a buffer into which to append the subjective sentences .We create an iterator over the elements in the priority queue , which returns the item in order of highest estimated probability of being subjective .","label":"Uses","metadata":{},"score":"50.83315"}
{"text":"First Int'l Conf .Affective Computing and Intelligent Interaction ( ACII ' 05 ) , pp.622 - 628 , 2005 .[19 ] B. Pang , L. Lee , and S. Vaithyanathain , \" Thumbs Up ?Sentiment Classification Using Machine Learning Techniques , \" Proc .","label":"Uses","metadata":{},"score":"50.932594"}
{"text":"We 'll step through the output ( which runs to more than a page ) a bite - sized chunk at a time ; the ellipses ( ... ) indicate that the report is continued .The first line of the report indicates the name of the categories .","label":"Uses","metadata":{},"score":"51.004993"}
{"text":"It is useful to differentiate text classification problems by the number of classes a document can belong to .If there are exactly two classes ( e.g. spam / non - spam ) , this is called a \" binary \" text classification problem .","label":"Uses","metadata":{},"score":"51.052906"}
{"text":"Otherwise , we append the next sentence .Finally , we return the result after trimming any residual whitespace ( probably just the final newline ; not a very efficient way to do this at all , but these data sets are miniscule ) .","label":"Uses","metadata":{},"score":"51.250313"}
{"text":"Otherwise , we append the next sentence .Finally , we return the result after trimming any residual whitespace ( probably just the final newline ; not a very efficient way to do this at all , but these data sets are miniscule ) .","label":"Uses","metadata":{},"score":"51.250313"}
{"text":"The final bit of this section of the report includes results about average performance from a ranked , scored , conditional and joint probability perspective .The average reference rank indicates the average position on the n - best list provided by a ranked classifier of the gold standard answer .","label":"Uses","metadata":{},"score":"51.41399"}
{"text":"The final bit of this section of the report includes results about average performance from a ranked , scored , conditional and joint probability perspective .The average reference rank indicates the average position on the n - best list provided by a ranked classifier of the gold standard answer .","label":"Uses","metadata":{},"score":"51.41399"}
{"text":"There there are 1000 test cases and a 92.1 % accuracy , leading to : .So for those results , our 95 % confidence interval is the narrower ( 90.7 , 93.5 ) .Pang and Lee used paired t - tests over cross - validated slices of data for significance , but we ca n't use that tighter technique to compare our results to theirs because we do n't have access to their results for the requisite pairing .","label":"Uses","metadata":{},"score":"51.540268"}
{"text":"There there are 1000 test cases and a 92.1 % accuracy , leading to : .So for those results , our 95 % confidence interval is the narrower ( 90.7 , 93.5 ) .Pang and Lee used paired t - tests over cross - validated slices of data for significance , but we ca n't use that tighter technique to compare our results to theirs because we do n't have access to their results for the requisite pairing .","label":"Uses","metadata":{},"score":"51.540268"}
{"text":"Multi - label and multi - class tasks are often handled by reducing them to \\(k\\ ) binary classification tasks , one for each category .For each such binary classification tasks , members of the respective category are designated as positive examples , while all others are designated as negative examples .","label":"Uses","metadata":{},"score":"51.72914"}
{"text":"Lee and Pang were generous enough to pre - slice the files into ten equally - sized slices which are distinguished by the third character of the file name .For instance , the file pos / cv362_15341.txt is a positive training instance in block 3 , whereas pos / cv532_6522.txt is a positive training instance in block 5 .","label":"Uses","metadata":{},"score":"51.75228"}
{"text":"Lee and Pang were generous enough to pre - slice the files into ten equally - sized slices which are distinguished by the third character of the file name .For instance , the file pos / cv362_15341.txt is a positive training instance in block 3 , whereas pos / cv532_6522.txt is a positive training instance in block 5 .","label":"Uses","metadata":{},"score":"51.75228"}
{"text":"We additionally supported the ability to use the full movie dataset as a training set and using the yelp dataset as a test set .+ There are several ways to construct a probability model for a set of document n - grams .","label":"Uses","metadata":{},"score":"51.780155"}
{"text":"How is it Done ?The high - level idea is to use LingPipe 's language classification framework to do two classification tasks : separating subjective from objective sentences , and separating positive from negative movie reviews .In the third section , we show how to build a hierarchical classifier by composing these models .","label":"Uses","metadata":{},"score":"51.91127"}
{"text":"Although Pang limited many of his tests to use only the 16165 most common ngrams , advanced processors have lifted this computational constraint , and so we additionally tested on all ngrams .We use a newer parameter estimation algorithm called Limited - Memory Variable Metric ( L - BFGS ) for maximum entropy classification .","label":"Uses","metadata":{},"score":"51.92406"}
{"text":"Any point on or inside the margin is referred to as a support vector , and the hyperplane , given by .+ For this paper , we use the PyML implementation of SVMs , which uses the liblinear optimizer to actually find the separating hyperplane .","label":"Uses","metadata":{},"score":"52.047886"}
{"text":"The metrics for binary - decisions are defined as : .Due to the often highly unbalance number of positive vs. negative examples , note that TN often dominates the accuracy and error of a system , leading to miss - interpretation of the results .","label":"Uses","metadata":{},"score":"52.54432"}
{"text":"The ensemble is combined with affect correlation information to enable better prediction of emotive intensities .Experiments were conducted on four test beds encompassing web forums , blogs , and online stories .The results revealed that learned n - grams were more effective than lexicon based affect representations .","label":"Uses","metadata":{},"score":"52.73905"}
{"text":"Distributional Semantics and Composit ... . \" ...Current phrase - based statistical machine translation systems process each test sentence in isolation and do not enforce global consistency constraints , even though the test data is often internally consistent with respect to topic or style .","label":"Uses","metadata":{},"score":"52.82859"}
{"text":"The maximum F(1)-measure indicates the best possible operating point achievable by setting a threshold .The BEP indicates the best score possible when precision is equal to recall .Note that the first set of results compares cases using their score , which is just their joint log probabilities ( divided by the number of characters , and thus expressed as negative cross - entropy rates ) .","label":"Uses","metadata":{},"score":"52.897648"}
{"text":"The maximum F(1)-measure indicates the best possible operating point achievable by setting a threshold .The BEP indicates the best score possible when precision is equal to recall .Note that the first set of results compares cases using their score , which is just their joint log probabilities ( divided by the number of characters , and thus expressed as negative cross - entropy rates ) .","label":"Uses","metadata":{},"score":"52.897648"}
{"text":"The probability that the sentence is subjective is then set into the variable subjProb .This is a bit tricky because we have to determine if the category quote ( meaning \" subjective \" ) is the first - best or second - best response and pull out the correct probability .","label":"Uses","metadata":{},"score":"52.959003"}
{"text":"The probability that the sentence is subjective is then set into the variable subjProb .This is a bit tricky because we have to determine if the category quote ( meaning \" subjective \" ) is the first - best or second - best response and pull out the correct probability .","label":"Uses","metadata":{},"score":"52.959003"}
{"text":"Each data file should be downloaded and then unpacked .They are distributed in tarred / gzipped format .For instance , here 's the result of unpacking the review polarity data which was downloaded to the directory in which untarring is performed : .","label":"Uses","metadata":{},"score":"52.970764"}
{"text":"Each data file should be downloaded and then unpacked .They are distributed in tarred / gzipped format .For instance , here 's the result of unpacking the review polarity data which was downloaded to the directory in which untarring is performed : .","label":"Uses","metadata":{},"score":"52.970764"}
{"text":"\\ ] .Term \\(G(h_\\theta ) \\ ) is called the ' ' regularization term ' ' , measuring the complexity of any rule \\(h_\\theta\\ .\\ ) Exact definitions of the empirical risk term and the regularization term may differ in various classification methods .","label":"Uses","metadata":{},"score":"53.018135"}
{"text":"Users may then reason about this web of trust to form opinions about content producers with whom they have never interacted before .These opinions will then determine whether content is accepted .The process of forming opinions is called trust propagation .","label":"Uses","metadata":{},"score":"53.119953"}
{"text":"The first line of the report indicates the name of the categories .In this case , it 's plot for \" objective \" sentences ( drawn from plot summaries ) and quote for \" subjective \" sentences ( drawn from user - review snippets ) .","label":"Uses","metadata":{},"score":"53.155846"}
{"text":"Traditional Classification Statistics .The report continues with a range of standard statistics that have been applied to classification problems : .The most popular statistic here is the kappa statistic , which we present in all three forms : \" standard \" , adjusted for \" bias \" , and adjusted for \" prevalence \" .","label":"Uses","metadata":{},"score":"53.27939"}
{"text":"Traditional Classification Statistics .The report continues with a range of standard statistics that have been applied to classification problems : .The most popular statistic here is the kappa statistic , which we present in all three forms : \" standard \" , adjusted for \" bias \" , and adjusted for \" prevalence \" .","label":"Uses","metadata":{},"score":"53.27939"}
{"text":"Given a set of training data , the SVM classifier finds a hyperplane with the largest possible margin ; that is , it tries finds the hyperplane such that each training point is correctly classified and the hyperplane is as far as possible from the points closest to it .","label":"Uses","metadata":{},"score":"53.39252"}
{"text":"We set the tuning parameter ex post facto ( after the fact ) .This is sometimes called an oracle parameter setting .Only rarely does a research paper make its tuning and evaluation methodology clear .Experiment with our results sensitivity by changing the minimum or maximum number of sentences in the summary or the subjectivty threshold .","label":"Uses","metadata":{},"score":"53.39679"}
{"text":"The conditional entropy statistic tells us how many additional bits we 'd need on average to encode the classification result given the reference category .This number will be 0.0 if there is perfect classification .The mutual information statistic just presents the response entropy minus the conditional entropy .","label":"Uses","metadata":{},"score":"53.485916"}
{"text":"The conditional entropy statistic tells us how many additional bits we 'd need on average to encode the classification result given the reference category .This number will be 0.0 if there is perfect classification .The mutual information statistic just presents the response entropy minus the conditional entropy .","label":"Uses","metadata":{},"score":"53.485916"}
{"text":"That movie was not very good . ' ' Diverging from Pang , we also added negation tags to bigrams .+ Negation tagging did not appear to have a significant effect on the data .For all the classifiers , the results from negation tagged data were almost the same as the results from the raw data .","label":"Uses","metadata":{},"score":"53.657066"}
{"text":"In particular , note that it calculates the 10 percent of the data on which to test and then as each sentence is encountered , it is added as a case to the evaluator using the BaseClassifierEvaluator.handle(Classified ) .Evaluation cases consist of the reference result , in this case category , and the input , in this case sentences[j ] .","label":"Uses","metadata":{},"score":"53.721603"}
{"text":"This means that if a statistical classifier requires approximately 100 positive training examples per category for learning sufficiently accurate models , then we can only solve the classification problem for only 1 % of the categories even if we use the entire set of 800,000 pages as training examples .","label":"Uses","metadata":{},"score":"53.801292"}
{"text":"For movie reviews we focus on two types of classification problem : .Subjective ( opinion ) vs. Objective ( fact ) sentences .Positive ( favorable ) vs. Negative ( unfavorable ) movie reviews .How is it Done ?The high - level idea is to use LingPipe 's language classification framework to do two classification tasks : separating subjective from objective sentences , and separating positive from negative movie reviews .","label":"Uses","metadata":{},"score":"53.905556"}
{"text":"To form contrast examples from spatial rationales ( such as the one shown in purple on the figure skater at the top of this page ) , we \" mask out \" the regions of the image that the annotator chose as a rationale .","label":"Uses","metadata":{},"score":"53.922752"}
{"text":"To avoid that , users may organiz ... \" .Using mobile devices , such as smart phones , people may create and distribute different types of digital content ( e.g. , photos , videos ) .One of the problems is that digital content , being easy to create and replicate , may likely swamp users rather than informing them .","label":"Uses","metadata":{},"score":"54.4827"}
{"text":"Finally , the method of tagging all words up to the next punctuation mark is suspect .Only a few words after the not are actually negated , and these often occur after a comma or other punctuation mark .+ Reviews are split into a beginning , middle , and end , so to see if one section carries more sentiment than another , we split the reviews into a first quarter , a middle half , and a last quarter and tagged the words in each section .","label":"Uses","metadata":{},"score":"54.484467"}
{"text":"Unigram and n - gram representations of text are common choices in opinion mining .However , unigrams can not capture important expressions like \" could have been better \" , which are ... \" .The problem addressed in this paper is to predict a user 's numeric rating in a product review from the text of the review .","label":"Uses","metadata":{},"score":"54.579407"}
{"text":"Note : The basic subjectivity demo must be run first to create the model file that will be used by the hierarchical model .The hierarchical classifier is run just like the other demos , either through the Ant target hierarchical ( with the same system property setting the data directory ) , or by the command : . java -cp \" sentimentDemo.jar ; . /lingpipe-4.1.0 . jar \" PolarityHierarchical POLARITY_DIR .","label":"Uses","metadata":{},"score":"54.615227"}
{"text":"The value of a feature in a given document is simply the number of times it appears in that document .There was no significant difference for SVMs and applying TF - IDF did not provide any improvement from using frequency for either .","label":"Uses","metadata":{},"score":"54.823208"}
{"text":"Confusion Matrix reference \\ response , plot , quote plot,458,42 quote,37,463 ... .This matrix represents the count of all reference / response pairs .The reference categories are read down the left and the response categories along the top .For this demo , the reference is the \" gold standard \" defined by the database curators and the response is the first - best category produced by the classifier we just trained .","label":"Uses","metadata":{},"score":"54.83966"}
{"text":"Confusion Matrix reference \\ response , plot , quote plot,458,42 quote,37,463 ... .This matrix represents the count of all reference / response pairs .The reference categories are read down the left and the response categories along the top .For this demo , the reference is the \" gold standard \" defined by the database curators and the response is the first - best category produced by the classifier we just trained .","label":"Uses","metadata":{},"score":"54.83966"}
{"text":"See section 4.3 of our paper for details on annotations for the Public Figures task .Our results for this task are shown below .Note that , with homogeneous rationales , our accuracy is higher than the baseline for both Males and Females , and with individual rationales , our accuracy is higher than the baseline for Males .","label":"Uses","metadata":{},"score":"54.90756"}
{"text":"Human Language Technology and Empirical Methods in Natural Language Processing ( HLT / EMNLP ' 05 ) , 2005 .[ 7 ] T. Wilson , P. Hoffmann , S. Somasundaran , J. Kessler , J. Wiebe , Y. Choi , C. Cardie , E. Riloff , and S. Patwardhan , \" Opinionfinder : A System for Subjectivity Analysis , \" Proc .","label":"Uses","metadata":{},"score":"54.946255"}
{"text":"42nd Ann .Meeting Assoc .Computational Linguistics ( ACL ' 04 ) , pp .271 - 278 , 2004 .[5 ] L. Lee and B. Pang , \" Seeing Stars : Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales , \" Proc .","label":"Uses","metadata":{},"score":"55.46077"}
{"text":"Finally , we construct a classifier with the given set of categories , n - gram length and boundedness .Training .The run method simply calls training then evaluation : .We consider training in this section and evaluation in the next .","label":"Uses","metadata":{},"score":"55.4784"}
{"text":"We propose two types of visual attribute rationales - homogeneous , in which attributes are named generally as rationales for each class , and individual , in which attributes are named as rationales on a per - image basis ( more analogous to the spatial rationales ) .","label":"Uses","metadata":{},"score":"55.538784"}
{"text":"+ Using the most frequent unigrams is an extremely simple method of feature selection , and in this case , not a particularly robust one , since feature selection should look for words that identify a given class .Choosing frequent words does not discriminate between the two classes and will select common words like ' ' the ' ' and ' ' it ' ' , which likely are weak sentiment indicators .","label":"Uses","metadata":{},"score":"55.60279"}
{"text":"/lib / commons - math-1.1 . jar \" PolarityBasic POLARITY_DIR . or through Ant with the target polarity and a system property determining the polarity directory 's location : .This produces the following output after running for about a minute and a quarter on my desktop machine : .","label":"Uses","metadata":{},"score":"55.640007"}
{"text":"( Like the other statistics , this is thoroughly explained in the class documentation . )Even for a two - way classification problem , these reports provide some interesting insight into the classificatioin problem .Here 's the initial piece of the report for the category of objective sentences , plot : .","label":"Uses","metadata":{},"score":"55.742195"}
{"text":"( Like the other statistics , this is thoroughly explained in the class documentation . )Even for a two - way classification problem , these reports provide some interesting insight into the classificatioin problem .Here 's the initial piece of the report for the category of objective sentences , plot : .","label":"Uses","metadata":{},"score":"55.742195"}
{"text":"What this means is that the joint probabilities assigned to cases perform very well at ranking plots versus quotes ( 92 % accuracy ) , but not very good at ranking confidence overall .For instance , in one case , there might be a score of -1.2 for plot and -1.4 for quote , whereas in another there might be a score of -1.9 for plot and -2.1 for quote .","label":"Uses","metadata":{},"score":"55.797855"}
{"text":"What this means is that the joint probabilities assigned to cases perform very well at ranking plots versus quotes ( 92 % accuracy ) , but not very good at ranking confidence overall .For instance , in one case , there might be a score of -1.2 for plot and -1.4 for quote , whereas in another there might be a score of -1.9 for plot and -2.1 for quote .","label":"Uses","metadata":{},"score":"55.797855"}
{"text":"Cross - validation performs multiple divisions of the data into training and test sets and then averages the results in order to bring down evaluation variance in order to tighten confidence intervals .It can be run just like the other methods either from the command line using class PolarityWhole or from Ant using the target whole .","label":"Uses","metadata":{},"score":"55.897324"}
{"text":"It can be run just like the other methods either from the command line using class PolarityWhole or from Ant using the target whole .Appendix 1 : Confidence Intervals .As usual , we compute 95 % confidence intervals for a given accuracy of p over N trials using the binomial distribution bionmial(p , N ) , which is just the distribution corresponding to N independent trials each with a p chance of success .","label":"Uses","metadata":{},"score":"56.29796"}
{"text":"Annotator Rationales for Visual Recognition .To appear , Proceedings of the International Conference on Computer Vision ( ICCV ) , Barcelona , Spain , November 2011 .[Paper ][Poster ] [ Video ] .Citations .[ 1 ] O. Zaidan , J. Eisner , and C. Piatko .","label":"Uses","metadata":{},"score":"56.311264"}
{"text":"Although we 've used 8-gram character language model classifiers , it 's easy to plug - and - play any of LingPipe 's other classifiers .Modifying the Existing Classes .Naive Bayes can be evaluated by importing the relevant classes and setting the classifier in the constructor : .","label":"Uses","metadata":{},"score":"56.342575"}
{"text":"Experiments and Results .We explore the utility of our approach with three datasets .Central to our approach are the human annotators , who provide rationales that should give insight into their class decision for an image .We use Mechanical Turk ( MTurk ) to gather most of our annotations in order to create large datasets with a variety of annotation styles .","label":"Uses","metadata":{},"score":"56.440796"}
{"text":"This approach may also be helpful for the basic character - level models , but they 're usually more robust with respect to smoothing than token models , which is just another reason why we prefer them for most applications .Cross - Validating Factory Implementation .","label":"Uses","metadata":{},"score":"56.965942"}
{"text":"186 - 194 , 2004 .[ 11 ] M.A. Hearst , \" Direction - Based Text Interpretation as an Information Access Refinement , \" Text - Based Intelligent Systems : Current Research and Practice in Information Extraction and Retrieval , P.Jacobs , ed . , Lawrence Erlbaum Assoc . , 1992 .","label":"Uses","metadata":{},"score":"56.977158"}
{"text":"Even if both decisions were right ( both were indeed plots ) , the ranked scores suffer .Usually , the conditional one - versus - all scores will be much better .These use the conditional probabilities assigned to answers to rank output .","label":"Uses","metadata":{},"score":"57.10717"}
{"text":"Even if both decisions were right ( both were indeed plots ) , the ranked scores suffer .Usually , the conditional one - versus - all scores will be much better .These use the conditional probabilities assigned to answers to rank output .","label":"Uses","metadata":{},"score":"57.10717"}
{"text":"Inspecting the Code .The source for this demo is in the file src / PolarityHierarchical . java .It 's identical to the first demo in the way that it steps through data , so we do n't repeat that code here .","label":"Uses","metadata":{},"score":"57.32247"}
{"text":"2006 ) , with a total of approximately 800,000 documents and a vocabulary of over 4 million unique words .Figure 1 shows the power - law correspondence between the category size ( in terms of the number of documents ) on the X - axis and the number of same - sized categories on the Y - axis .","label":"Uses","metadata":{},"score":"58.298874"}
{"text":"For bigrams , it harmed performance by around 5\\% in most cases , and for unigrams , it was not helpful .If reviews end up not actually following the model specified or if the model has no bearing on where the relevant data is , position tagging will be harmful because it increases the dimensionality of the input without increasing the information content .","label":"Uses","metadata":{},"score":"58.347122"}
{"text":"B Pang , L Lee , S Vaithyanathan ( 2002 ) .Thumbs up ?Sentiment Classification using Machine Learning Techniques , Proceeding of the ACM Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 79 - 86 .","label":"Uses","metadata":{},"score":"58.397514"}
{"text":"This involves the use of rationales to form \" contrast examples \" ( positive / negative training examples that are \" less positive \" or \" less negative \" due to the removal of the rationale ) .To use these contrast examples in an SVM , we introduce into the SVM an additional margin that separates positive examples from positive contrast examples , and negative examples from negative contrast examples , along with the normal SVM margin that divides positive and negative examples .","label":"Uses","metadata":{},"score":"58.715996"}
{"text":"The score was simply the average of the two accuracies .+ Across the board , the classifiers has a harder time with the Yelp dataset as compared to the movie dataset , performing between 56.0\\% and 75.2\\% .The respective lowest and highest performing configurations scored at 67.0\\% and 84.0\\% on the movie dataset .","label":"Uses","metadata":{},"score":"58.749283"}
{"text":"Recall that the process models are normalized for a given input length and do not model boundaries of strings differently than other positions .Training .The run method simply calls training then evaluation : .We consider training in this section and evaluation in the next .","label":"Uses","metadata":{},"score":"58.902264"}
{"text":"Uncompiled naive Bayes takes almost ten minutes to complete the classification demo .The act of compiling them to a file actually precomputes almost all of the probabilities ; reading them back in computes the suffix tree backoffs .The compiled classifiers should work in exactly the same way as the classifier from which they were compiled .","label":"Uses","metadata":{},"score":"59.140884"}
{"text":"Assuming the data is in the directory POLARITY_DIR and the sentimentDemo.jar file exists , the demo may be run from the command line ( all one line , with colon ( : ) classpath separators replacing the semicolons ( ; ) for linux ) : . java -cp \" sentimentDemo.jar ; .","label":"Uses","metadata":{},"score":"59.285004"}
{"text":"Human Language Technology and Empirical Methods in Natural Language Processing ( HLT / EMNLP ' 05 ) , pp .34 - 35 , 2005 .[ 8 ] A.B. Goldberg and X. Zhu , \" Seeing Stars When There Are n't Many Stars : Graph - Based Semi - Supervised Learning for Sentiment Categorization , \" Proc .","label":"Uses","metadata":{},"score":"59.308773"}
{"text":"Sentiment analysis involves classifying opinions in text into categories like \" positive \" or \" negative \" often with an implicit category of \" neutral \" .A classic sentiment application would be tracking what bloggers are saying about a brand like Toyota .","label":"Uses","metadata":{},"score":"59.4329"}
{"text":"/lib / commons - math-1.1 . jar \" SubjectivityBasic POLARITY_DIR . or through Ant with the target subjectivity and a system property determining the polarity directory 's location : .This produces the following output after about 45 seconds of chugging away on my desktop : .","label":"Uses","metadata":{},"score":"59.439228"}
{"text":"The cutsize is induced by the unknown adversarial labeling of the graph nodes .In deriving our characterization , ... \" .We show that the mistake bound for predicting the nodes of an arbitrary weighted graph is characterized ( up to logarithmic factors ) by the cutsize of a random spanning tree of the graph .","label":"Uses","metadata":{},"score":"59.69835"}
{"text":"These disparities suggest evidence that the movie dataset does not satisfy the conditional independence assumption .+ One key decision in a bag - of - words feature set is which words to include .Using more words provides more information , but harms the performance of the classifiers , and words that appear only infrequently in the training data may not present accurate information due to the law of small numbers .","label":"Uses","metadata":{},"score":"59.869907"}
{"text":"We 'll end with some words of warning on efficiency and performance .If you use naive Bayes or token n - gram models , you should probably compile them first .Uncompiled naive Bayes takes almost ten minutes to complete the classification demo .","label":"Uses","metadata":{},"score":"59.88321"}
{"text":"However , unigrams can not capture important expressions like \" could have been better \" , which are essential for prediction models of ratings .N - grams of words , on the other hand , capture such phrases , but typically occur too sparsely in the training set and thus fail to yield robust predictors .","label":"Uses","metadata":{},"score":"59.908302"}
{"text":"This code raises the possibility of either an IOException from the I / O or a ClassNotFoundException from reading in the actual classifier from the object intput stream .Training .The next step is to train the polarity classifier .It does this in exactly the same way as in the basic polarity demo using the method train ( ) .","label":"Uses","metadata":{},"score":"60.001137"}
{"text":"+ Given a large ensemble of classifiers , an easy way to combine them is with a simple majority voting scheme .This tends to eliminate weaknesses that exist in only one classifier , but can also eliminate strengths that exist in only one classifier .","label":"Uses","metadata":{},"score":"60.05799"}
{"text":"These results suggest that the limited information conveyed in adjectives is not representative of the full review itself .+ As in the motivating example for the use of POS tagging , it was in the case of the verb use of ' ' love ' ' ( ' ' I love this movie ' ' ) that conveyed sentimental information , rather than the adjective use of the word .","label":"Uses","metadata":{},"score":"60.366013"}
{"text":"The category array is initialized using the directory names under txt_sentoken , which in this case are \" pos \" and \" neg \" .We set the n - gram length to the constant 8 ; this could obviously be set with a command - line argument if desired .","label":"Uses","metadata":{},"score":"60.43329"}
{"text":"The category array is initialized using the directory names under txt_sentoken , which in this case are \" pos \" and \" neg \" .We set the n - gram length to the constant 8 ; this could obviously be set with a command - line argument if desired .","label":"Uses","metadata":{},"score":"60.43329"}
{"text":"Three classification algorithms were studied : Naive Bayes , maximum entropy , and support vector machines , with a standard bag - of - features framework .Pang et al tried different features and their combinations , e.g. unigram , bigram , POS , position .","label":"Uses","metadata":{},"score":"60.498116"}
{"text":"In layman 's terms , we 're not particularly confident about our results for the basic polarity evaluation .If we had 2000 tests rather than 200 ( ten times as much data ) , that number would be + /- 0.014 , a factor of sqrt(10 ) less due to 10 times the amount of data .","label":"Uses","metadata":{},"score":"60.713684"}
{"text":"In layman 's terms , we 're not particularly confident about our results for the basic polarity evaluation .If we had 2000 tests rather than 200 ( ten times as much data ) , that number would be + /- 0.014 , a factor of sqrt(10 ) less due to 10 times the amount of data .","label":"Uses","metadata":{},"score":"60.713684"}
{"text":"Evaluation .The evaluation is set up similarly to the subjectivity demo with a slight twist : . if ( !String review .String subjReview .Classification classification .evaluator.addClassification(category , .System.out.println ( ) ; .As in previous examples , the code that remains the same is greyed out .","label":"Uses","metadata":{},"score":"60.763412"}
{"text":"Evaluation cases consist of the reference result , in this case category , and the input , in this case sentences[j ] .Because the evaluator has a handle on the classifier , it just runs the classifier over the input and records the results .","label":"Uses","metadata":{},"score":"60.77807"}
{"text":"Note : The basic subjectivity demo must be run first to create the model file that will be used by the hierarchical model .The hierarchical classifier is run just like the other demos , either through the Ant target hierarchical ( with the same system property setting the data directory ) , or by the command : . java -cp \" sentimentDemo.jar ; .","label":"Uses","metadata":{},"score":"61.068825"}
{"text":"For each training file , a test is done to see if it is a training file .If it is , then the text is read from the file using the LingPipe utility method Files.readFromFile , and then used to train the classifier for the specified category .","label":"Uses","metadata":{},"score":"61.26169"}
{"text":"For each training file , a test is done to see if it is a training file .If it is , then the text is read from the file using the LingPipe utility method Files.readFromFile , and then used to train the classifier for the specified category .","label":"Uses","metadata":{},"score":"61.26169"}
{"text":"Typically , naive Bayes as used in classifiers is smoothed using something like add - one ( Laplace ) smoothing .This is what Pang and Lee do for their naive Bayes baseline .The way to implement add - one smoothing over LingPipe 's naive Bayes implementation is to collect all of the tokens during the first training pass in a set .","label":"Uses","metadata":{},"score":"61.37696"}
{"text":"45 - 52 , 2006 .[ 9 ] K. Shimada and T. Endo , \" Seeing Several Stars : A Rating Inference Task for a Document Containing Several Evaluation Criteria , \" Proc . 12th Pacific - Asia Conf .Advances in Knowledge Discovery and Data Mining ( PAKDD ' 08 ) , pp .","label":"Uses","metadata":{},"score":"61.749123"}
{"text":"This is telling us that even though we 're not particularly confident about our decisions on average , the ranking is in fact useful .In fact , this tells us that by setting a threshold other than 0.50 for classification , we could achieve an 86.4 % f - measure on this task .","label":"Uses","metadata":{},"score":"61.86161"}
{"text":"\\ ) NaÃ¯ve Bayes is \" naÃ¯ve \" in that it assumes conditional independence between all feature values in a feature vector .Which this assumption is clearly violated to text classification , NaÃ¯ve Bayes nevertheless produces fairly accurate classification rules in many cases .","label":"Uses","metadata":{},"score":"62.009575"}
{"text":"However , Pang 's motivation for limiting the number of features was for improve testing performance , but our classifiers and processors were fast enough that this was not particularly noticeable .+ On average , limiting the number of features from 16165 to 2633 , as in the original Pang paper , caused accuracy to drop by 5.2\\% , 4.0\\% , and 2.8\\% for Naive Bayes , Maximum Entropy , and SVM , respectively .","label":"Uses","metadata":{},"score":"62.37815"}
{"text":"Appendix 1 : Confidence Intervals .As usual , we compute 95 % confidence intervals for a given accuracy of p over N trials using the binomial distribution bionmial(p , N ) , which is just the distribution corresponding to N independent trials each with a p chance of success .","label":"Uses","metadata":{},"score":"62.460358"}
{"text":"The number of characters ( here set to 128 ) , determines the amount of penalty per character for unknown words .It 's also possible to experiment with token n - grams .And , of course , each of these classifiers has a few parameters to tweak .","label":"Uses","metadata":{},"score":"62.5807"}
{"text":"\\ )A commonly used loss functions is the 0/1-loss , which returns 0 if the prediction matches the true label , and 1 otherwise .In this case , the risk is equal to the probability that the classification rule will misclassify an example .","label":"Uses","metadata":{},"score":"62.66723"}
{"text":"Although we 've used 8-gram character language model classifiers , it 's easy to plug - and - play any of LingPipe 's other classifiers .Modifying the Existing Classes .Naive Bayes can be evaluated by importing the relevant classes and setting the classifier in the constructor : . import com.aliasi.classify .","label":"Uses","metadata":{},"score":"63.26493"}
{"text":"Warning : Oracle Evaluation .We set the tuning parameter ex post facto ( after the fact ) .This is sometimes called an oracle parameter setting .Only rarely does a research paper make its tuning and evaluation methodology clear .","label":"Uses","metadata":{},"score":"63.379986"}
{"text":"In order to play nicely with the rest of LingPipe , we should really define our hierarchical classifier to implement the interface classify .Classifier .This is actually very straightfoward , but we did n't want to confuse the earlier code with tricky Java particulars like inner class interface implementations .","label":"Uses","metadata":{},"score":"63.544678"}
{"text":"In order to play nicely with the rest of LingPipe , we should really define our hierarchical classifier to implement the interface classify .Classifier .This is actually very straightfoward , but we did n't want to confuse the earlier code with tricky Java particulars like inner class interface implementations .","label":"Uses","metadata":{},"score":"63.544678"}
{"text":"There are 42 cases that were labeled as plots in the gold standard , but were misclassified as quotes by the classifier .On the next row , there are 463 cases that were labeled quotes in the gold standard that were correctly classified as quotes by our classifier .","label":"Uses","metadata":{},"score":"63.70127"}
{"text":"There are 42 cases that were labeled as plots in the gold standard , but were misclassified as quotes by the classifier .On the next row , there are 463 cases that were labeled quotes in the gold standard that were correctly classified as quotes by our classifier .","label":"Uses","metadata":{},"score":"63.70127"}
{"text":"Finally , we consider the Public Figures dataset [ 3 ] , which contains 58,797 images of 200 people , and 73 attributes .We use it to test our attribute rationales for the attractiveness test , and leverage the binary attribute classifier outputs kindly shared by the authors to define a 1 , ... , a V .","label":"Uses","metadata":{},"score":"63.816574"}
{"text":"Consider the basic polarity evaluation , for which accuracy is 81.5 % , or 0.815 over 200 cases .This leads to a deviation of .This is a huge deviation , primarily because there are only 200 test cases .A 95 % confidence interval is roughly plus or minus 1.6 deviations , or about + /- 0.044 .","label":"Uses","metadata":{},"score":"63.858288"}
{"text":"Consider the basic polarity evaluation , for which accuracy is 81.5 % , or 0.815 over 200 cases .This leads to a deviation of .This is a huge deviation , primarily because there are only 200 test cases .A 95 % confidence interval is roughly plus or minus 1.6 deviations , or about + /- 0.044 .","label":"Uses","metadata":{},"score":"63.858288"}
{"text":"Running the Subjectivity Classifier .Assuming the data is in the directory POLARITY_DIR and the sentimentDemo.jar file exists ( if it does n't , run ant jar to create it ) , the demo may be run from the command line : . java -cp \" sentimentDemo.jar ; .","label":"Uses","metadata":{},"score":"65.27351"}
{"text":"Run this way , the subjectivity classifier still uses character 8-grams , but the polarity classifier uses naive Bayes with no character language model smoothing .The number of characters ( here set to 128 ) , determines the amount of penalty per character for unknown words .","label":"Uses","metadata":{},"score":"65.312065"}
{"text":"INDEX TERMS .Mood , Sentiment analysis , Data mining , Algorithm design and analysis , Predictive models , sentiment analysis , Mood , Sentiment analysis , Data mining , Algorithm design and analysis , Predictive models , affect detection , Mining methods and algorithms .","label":"Uses","metadata":{},"score":"65.7089"}
{"text":"+ Our testbed supported testing various parameters : frequency vs. presence of features vs. term frequency - inverse document frequency , unigrams vs. bigrams vs. both , number of features , and type of feature tagging .The types of feature tagging were negation , part of speech ( POS ) , and position .","label":"Uses","metadata":{},"score":"65.95422"}
{"text":"Although he did n't directly serve on my committee , Shai Ben - David got me started on the theoretical aspects of this work , and chapter 4 grew out of work I co - authored with him .I was also fortunate to have a great academic family .","label":"Uses","metadata":{},"score":"66.7426"}
{"text":"Cross - Validating Factory Implementation .For those who are comfortable reading Java code based on factory interfaces , there 's an implementation of cross - validation that evaluates several different kinds of models in the file src / PolarityWhole . java .","label":"Uses","metadata":{},"score":"66.81224"}
{"text":"Just do n't be fooled by overtrained a posteriori results on a test set .You 're unlikely to be called out on this in an ACL paper , but the results are rarely achievable in practice .We 'll end with some words of warning on efficiency and performance .","label":"Uses","metadata":{},"score":"67.104576"}
{"text":"If you want to see results for other slices , just swap out the 9 for any digit between 0 and 8 .Evaluation .The evaluation code follows the same structure as the training code : . if ( !+ + numTests ; .","label":"Uses","metadata":{},"score":"67.621826"}
{"text":"If you want to see results for other slices , just swap out the 9 for any digit between 0 and 8 .Evaluation .The evaluation code follows the same structure as the training code : . if ( !+ + numTests ; .","label":"Uses","metadata":{},"score":"67.621826"}
{"text":"Classification classification .evaluator.addClassification(category , .System.out.println ( ) ; .As in previous examples , the code that remains the same is greyed out .The new code creates a classifier evaluator with a null classifier .This is because we do n't actually have an implementation of the BaseClassifier interface ( see the next section for an illustration of how to create one ) .","label":"Uses","metadata":{},"score":"67.94957"}
{"text":"+ While the Naive Bayes classifier seems very simple , it is observed to have high predictive power ; in our tests , it performed competitively with the more sophisticated classifiers we used .The Bayes classifier can also be implemented very efficiently .","label":"Uses","metadata":{},"score":"68.07618"}
{"text":"This does illustrate another aspect of the nearly limitless fiddling that 's possible with these kinds of models .Evaluation .The evaluation is set up similarly to the subjectivity demo with a slight twist : . if ( !String review .","label":"Uses","metadata":{},"score":"68.51415"}
{"text":"This produces the following output after running for about a minute and a quarter on my desktop machine : .This result is very encouraging .We 've set LingPipe up with its default recommended n - gram length , 8 , and the resulting classification accuracy is within .014 of the best accuracy reported in Pang , Lee and Vaithyanathan 's Lee 's 2002 Thumbs up ? paper from EMNLP .","label":"Uses","metadata":{},"score":"68.57762"}
{"text":"My first thanks must go to Fernando Pereira .He was a wonderful advisor , and every aspect of this thesis has benefitted from his insight .At times I was a difficult , even unruly graduate student , and Fernando had patience with all my ideas , whether good or bad .","label":"Uses","metadata":{},"score":"68.840645"}
{"text":"My first thanks must go to Fernando Pereira .He was a wonderful advisor , and every aspect of this thesis has benefitted from his insight .At times I was a difficult , even unruly graduate student , and Fernando had patience with all my ideas , whether good or bad .","label":"Uses","metadata":{},"score":"68.840645"}
{"text":"+ Mostly out of curiosity , we wanted to see how our test configurations will perform when training on the movie dataset and testing on the Yelp dataset , an external out - of - domain dataset .We preprocessed the Yelp dataset such that it matched the format of the movie dataset and selected 1000 of each of the 1 - 5 star rating reviews .","label":"Uses","metadata":{},"score":"69.08971"}
{"text":"Running the Subjectivity Classifier .Assuming the data is in the directory POLARITY_DIR and the sentimentDemo.jar file exists ( if it does n't , run ant jar to create it ) , the demo may be run from the command line : . java -cp \" sentimentDemo.jar ; . /lingpipe-4.1.0 . jar \" SubjectivityBasic POLARITY_DIR . or through Ant with the target subjectivity and a system property determining the polarity directory 's location : .","label":"Uses","metadata":{},"score":"69.157425"}
{"text":"+ In an effort to preserve the potential value of negation information while using dead - simple features , we tagged words between those expressing negation and the next punctuation mark with a postfix ' ' \\_NOT . ' ' This distinguishes sentences like ' '","label":"Uses","metadata":{},"score":"70.51379"}
{"text":"+ The ineffectiveness of negation tagging probably comes from a few sources .First , it increases the number of uncommon features , which , as discussed previously , harms effectiveness and cancels out the increase in semantic awareness .Second , the presence of a \" not \" does not always indicate negation .","label":"Uses","metadata":{},"score":"70.84106"}
{"text":"Stepping through the Code .The code for the subjectivity classifier in src / SubjectivityBasic .java is almost identical to that of the polarity classifier , so we only focus on a few differences in this section .Splitting the Training File .","label":"Uses","metadata":{},"score":"70.88237"}
{"text":"Stepping through the Code .The code for the subjectivity classifier in src / SubjectivityBasic .java is almost identical to that of the polarity classifier , so we only focus on a few differences in this section .Splitting the Training File .","label":"Uses","metadata":{},"score":"70.88237"}
{"text":"The compiled classifiers should work in exactly the same way as the classifier from which they were compiled .The second word of warning is that naive Bayes as implemented here is n't accurate for this task .Typically , naive Bayes as used in classifiers is smoothed using something like add - one ( Laplace ) smoothing .","label":"Uses","metadata":{},"score":"71.03733"}
{"text":"Georgios Paltoglou , Michael Thelwall , \" Seeing Stars of Valence and Arousal in Blog Posts \" , IEEE Transactions on Affective Computing , vol.4 , no . 1 , pp .116 - 123 , Jan.-March 2013 , doi:10.1109/T - AFFC.2012.36 .","label":"Uses","metadata":{},"score":"71.104614"}
{"text":"15th Int'l Joint Conf .Artificial Intelligence ( IJCAI ) , 1997 .[ 38 ] A.L. Prodromidis and S.J. Stolfo , \" A Comparative Evaluation of Meta - Learning Strategies over Large and Distributed Data Sets , \" Proc . 16th Int'l Conf .","label":"Uses","metadata":{},"score":"72.39695"}
{"text":"+ Maximum Entropy is a general - purpose machine learning technique that provides the least biased estimate possible based on the given information .In other words , \" it is maximally noncommittal with regards to missing information \" [ src].","label":"Uses","metadata":{},"score":"73.8777"}
{"text":"For females , we only beat the baselines with 100 training examples using our annotations .See section 5.2 of our paper for details and analysis on the baselines , experiment setup , and results for the Hot or Not task .","label":"Uses","metadata":{},"score":"74.70697"}
{"text":".. the sign of the scores would incorrectly suggest that not helpful is less negative than not very helpful .Consider the following example : not so helpful vs. not so bad .In a unigram - based regression model each unigram gets a weight indicating i .. by NicolÃ² Cesa - bianchi , Claudio Gentile , Fabio Vitale , Giovanni Zappella , 2010 . \" ...","label":"Uses","metadata":{},"score":"74.78017"}
{"text":"Although a movie review and a Yelp review will differ in specialized vocabulary , audience , tone , etc . , the ways that people convey sentiment ( e.g. I loved it ! ) may not differ entirely .We wished to explore how training classifiers in one domain might generalize to neighbor domains .","label":"Uses","metadata":{},"score":"74.78752"}
{"text":"Shown below is the annotation interface used to gather annotations from MTurk ( click to enlarge ) : .See section 4.2 of our paper for details on annotations for the Hot or Not task .Our results for this task are shown below .","label":"Uses","metadata":{},"score":"75.09395"}
{"text":"+ Interestingly , for Naive Bayes , the positive and negative tests performed very differently between presence and frequency tests .By comparison , SVMs exhibited an average aggregate difference of 0.7\\% .These results provide evidence that training on presence rather than frequency yields models with less bias .","label":"Uses","metadata":{},"score":"75.32771"}
{"text":"+ However , when restricting from all features down to 16165 , the results were a wash .Naive Bayes did vaguely worse , Maximum Entropy remained unchanged , and SVMs did vaguely better .These results suggest that uncommon features do not carry much sentiment information .","label":"Uses","metadata":{},"score":"75.51755"}
{"text":"In addition to Fernando , this thesis was shaped by a great committee .Having Ben Taskar as committee chairman has given me the perfect excuse to interrupt his workday with new , ostensibly - thesis - related machine learning ideas .","label":"Uses","metadata":{},"score":"75.905975"}
{"text":"This serves as a rough way to disambiguate words that may hold different meanings in different contexts .For example , it would distinguish the different uses of \" love \" in ' ' I love this movie ' ' versus ' '","label":"Uses","metadata":{},"score":"76.13728"}
{"text":"We 've set LingPipe up with its default recommended n - gram length , 8 , and the resulting classification accuracy is within .014 of the best accuracy reported in Pang , Lee and Vaithyanathan 's Lee 's 2002 Thumbs up ? paper from EMNLP .","label":"Uses","metadata":{},"score":"76.48868"}
{"text":"Clearly , this assumption does not hold .+ We found a huge difference between results of Naive Bayes and Maximum Entropy for positive testing accuracy and negative testing accuracy .Maximum Entropy , which makes no unfounded assumptions about the data , gave very similar results for positive tests and negative tests with a 0.2\\% difference on average .","label":"Uses","metadata":{},"score":"77.17061"}
{"text":"Of the 116 men , we took 74 as attractive , 42 unattractive ; of the 84 women , we took 76 as attractive and just 8 as unattractive .For ease of explaining the slightly more complex and time - consuming task , we asked friends unfamiliar with the project to provide rationales ( rather than using MTurk ) .","label":"Uses","metadata":{},"score":"80.93239"}
{"text":"String resultCategory . if ( resultCategory.equals(category ) ) .The code rendered in grey is the same as in the training loop described in the last section .The remaining code begins by setting the number of tests and number of correct answer counters to zero .","label":"Uses","metadata":{},"score":"81.002846"}
{"text":"String resultCategory . if ( resultCategory.equals(category ) ) .The code rendered in grey is the same as in the training loop described in the last section .The remaining code begins by setting the number of tests and number of correct answer counters to zero .","label":"Uses","metadata":{},"score":"81.002846"}
{"text":"Hot or Not .We collected 1000 image / rating pairs of both men and women , requiring each to have been rated at least 100 times ( for a robust \" hotness \" label ) .This dataset is an excellent testbed , since we expect rationales are valuable to better learn such subjective clasification tasks .","label":"Uses","metadata":{},"score":"82.46544"}
{"text":"However , it turns out that word disambiguation is a much more complicated problem , as POS says nothing to distinguish between the meaning of cold in ' ' I was a bit cold during the movie ' ' and ' '","label":"Uses","metadata":{},"score":"82.980606"}
